Coling 2008: Companion volume ? Posters and Demonstrations, pages 153?156
Manchester, August 2008
A Toolchain for Grammarians
Bruno Guillaume
LORIA
INRIA Nancy Grand-Est
Bruno.Guillaume@loria.fr
Joseph Le Roux
LORIA
Nancy Universit?e
Joseph.Leroux@loria.fr
Jonathan Marchand
LORIA
Nancy Universit?e
Jonathan.Marchand@loria.fr
Guy Perrier
LORIA
Nancy Universit?e
Guy.Perrier@loria.fr
Kar
?
en Fort
LORIA
INRIA Nancy Grand-Est
Karen.Fort@loria.fr
Jennifer Planul
LORIA
Nancy Universit?e
Jennifer.Planul@loria.fr
Abstract
We present a chain of tools used by gram-
marians and computer scientists to develop
grammatical and lexical resources from
linguistic knowledge, for various natural
languages. The developed resources are
intended to be used in Natural Language
Processing (NLP) systems.
1 Introduction
We put ourselves from the point of view of re-
searchers who aim at developing formal grammars
and lexicons for NLP systems, starting from lin-
guistic knowledge. Grammars have to represent all
common linguistic phenomena and lexicons have
to include the most frequent words with their most
frequent uses. As everyone knows, building such
resources is a very complex and time consuming
task.
When one wants to formalize linguistic knowl-
edge, a crucial question arises: which mathemat-
ical framework to choose? Currently, there is no
agreement on the choice of a formalism in the sci-
entific community. Each of the most popular for-
malisms has its own advantages and drawbacks. A
good formalism must have three properties, hard to
conciliate: it must be sufficiently expressive to rep-
resent linguistic generalizations, easily readable by
linguists and computationally tractable. Guided
by those principles, we advocate a recent formal-
ism, Interaction Grammars (IGs) (Perrier, 2003),
the goal of which is to synthesize two key ideas,
expressed in two kinds of formalisms up to now:
using the resource sensitivity of natural languages
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
as a principle of syntactic composition, which is
a characteristic feature of Categorial Grammars
(CG) (Retor?e, 2000), and viewing grammars as
constraint systems, which is a feature of unifica-
tion grammars such as LFG (Bresnan, 2001) or
HPSG (Pollard and Sag, 1994).
Researchers who develop large lexicons and
grammars from linguistic knowledge are con-
fronted to the contradiction between the necessity
to choose a specific grammatical framework and
the cost of developing resources for this frame-
work. One of the most advanced systems de-
voted to such a task is LKB (Copestake, 2001).
LKB allows grammars and lexicons to be devel-
oped for different languages, but only inside the
HPSG framework, or at most a typed feature struc-
ture framework. Therefore, all produced resources
are hardly re-usable for other frameworks. Our
goal is to design a toolchain that is as much as pos-
sible re-usable for other frameworks than IG.
Our toolchain follows the following architecture
(see Figure 1):
? First, for building grammars, we use XMG
(Section 3.1) which translates the source
grammar into an object grammar.
? IGs that we have developed with XMG are
all lexicalized. Therefore, the object grammar
has to be anchored in a lexicon (Section 3.2)
in order to produce the anchored grammar.
? Then, when analyzing a sentence, we start
with a lexical disambiguation module (Sec-
tion 3.3).
? The resulting lexical selections, presented in
the compact form of an automaton, are finally
sent to the LEOPAR parser (Section 3.4).
153
LEOPAR
source grammar
XMG
object grammar lexicons
anchoring
input sentence
lexical disambiguation
output parse trees
anchored grammar
automaton
parsing
Figure 1: Toolchain architecture
2 Interaction Grammars
IGs (Perrier, 2003) are a grammatical formalism
based on the notion of polarity. Polarities express
the resource sensitivity of natural languages by
modeling the distinction between saturated and un-
saturated syntactic structures. Syntactic composi-
tion is represented as a chemical reaction guided
by the saturation of polarities. In a more precise
way, syntactic structures are underspecified trees
equipped with polarities expressing their satura-
tion state. They are superposed under the con-
trol of polarities in order to saturate them. In
CG, Tree Adjoining Grammars (TAGs) and De-
pendency Grammars, syntactic composition can
also be viewed as a mechanism for saturating po-
larities, but this mechanism is less expressive be-
cause node merging is localized at specific places
(root nodes, substitution nodes, foot nodes, ad-
junction nodes . . .). In IGs, tree superposition is
a more flexible way of realizing syntactic compo-
sition. Therefore, it can express sophisticated con-
straints on the environment in which a polarity has
to be saturated. From this angle, IGs are related
to Unification Grammars, such as HPSG, because
tree superposition is a kind of unification, but with
an important difference: polarities play an essen-
tial role in the control of unification.
3 Description of the Toolchain
3.1 The XMG Grammar Compiler
The first piece of software in our toolchain is
XMG
1
(Duchier et al, 2004), a tool used to de-
velop grammars. XMG addresses the issue of de-
signing wide-coverage grammars: it is based on a
distinction between source grammar, written by a
human, and object grammar, used in NLP systems.
XMG provides a high level language for writing
source grammars and a compiler which translates
those grammars into operational object grammars.
XMG is particularly adapted to develop lexical-
ized grammars. In those grammars, parsing a sen-
tence amounts to combining syntactical items at-
tached to words. In order to have an accurate lan-
guage model, it may be necessary to attach a huge
number of syntactical items to some words (verbs
and coordination words, in particular) that describe
the various usages of those words. In this context,
a grammar is a collection of items representing
syntactical behaviors. Those items, although dif-
ferent from each other, often share substructures
(for instance, almost all verbs have a substruc-
ture for subject verb agreement). That is to say,
if a linguist wants to change the way subject-verb
agreement is modeled, (s)he would have to mod-
ify all the items containing that substructure. This
is why designing and maintaining strongly lexical-
ized grammars is a difficult task.
The idea behind the so-called metagrammati-
cal approach is to write only substructures (called
fragments) and then add rules that describe the
combinations (expressed with conjunctions, dis-
junctions and unifications) of those fragments to
obtain complete items.
Fragments may contain syntactic, morpho-
syntactic and semantic pieces of information. An
object grammar is a set of structures containing
syntactic and semantic information, that can be an-
chored using morpho-syntactic information stored
in the interface of the structure (see Section 3.2).
During development and debugging stages, por-
tions of the grammar can be evaluated indepen-
dently. The grammar can be split into various mod-
ules that can be shared amongst grammars. Fi-
nally, graphical tools let the users explore the in-
heritance hierarchy and the partial structures be-
fore complete evaluation.
1
XMG is freely available under the CeCILL license at
http://sourcesup.cru.fr/xmg
154
XMG is also used to develop TAGs (Crabb?e,
2005) and it can be easily extended to other gram-
matical frameworks based on tree representations.
3.2 Anchoring the Object Grammar with a
Lexicon
The tool described in the previous section builds
the set of elementary trees of the grammar. The
toolchain includes a generic anchoring mechanism
which allows to use formalism independent lin-
guistic data for the lexicon part.
Each structure produced by XMG comes with
an interface (a two-level feature structure) which
describes morphological and syntactical con-
straints used to select words from the lexicon. Du-
ally, in the lexicon, each inflected form of the natu-
ral language is described by a set of two-level fea-
ture structures that contain morphological and syn-
tactical information.
If the interface of an unanchored tree unifies
with some feature structure associated with w in
the lexicon, then an anchored tree is produced for
the word w.
The toolchain also contains a modularized lexi-
con manager which aims at easing the integration
of external and formalism independent resources.
The lexicon manager provides several levels of lin-
guistic description to factorize redundant data. It
also contains a flexible compilation mechanism to
improve anchoring efficiency and to ease lexicon
debugging.
3.3 Lexical Disambiguation
Neutralization of polarities is the key mechanism
in the parsing process as it is used to control syn-
tactic composition. This principle can also be used
to filter lexical selections. For a input sentence, a
lexical selection is a choice of an elementary tree
from the anchored grammar for each word of the
sentence.
Indeed, the number of possible lexical selec-
tions may present an exponential complexity in
the length of the sentence. A way of filter-
ing them consists in abstracting some information
from the initial formalism F to a new formalism
F
abs
. Then, parsing in F
abs
allows to eliminate
wrong lexical selections at a minimal cost (Boul-
lier, 2003). (Bonfante et al, 2004) shows that po-
larities allow original methods of abstraction.
Following this idea, the lexical disambiguation
module checks the global neutrality of every lex-
ical selection for each polarized feature: a set of
trees bearing negative and positive polarities can
only be reduced to a neutral tree if the sum of the
negative polarities for each feature equals the sum
of its positive polarities.
Counting the sum of positive and negative fea-
tures can be done in a compact way by using an au-
tomaton. This automaton structure allows to share
all paths that have the same global polarity bal-
ance (Bonfante et al, 2004).
3.4 The LEOPAR Parser
The next piece of software in our toolchain is a
parser based on the IGs formalism
2
. In addition
to a command line interface, the parser provides
an intuitive graphical user interface. Parsing can
be highly customized in both modes. Besides, the
processed data can be viewed at each stage of the
analysis via the interface so one can easily check
the behavior of the grammar and the lexicons in
the parsing process.
The parsing can also be done manually: one first
chooses a lexical selection of the sentence given
by the lexer and then proceeds to the analysis by
neutralizing nodes from the selection. This way,
the syntactic composition can be controlled by the
user.
4 Results
Our toolchain has been used first to produce a large
coverage French IG. Most of the usual syntactical
constructions of French are covered. Some non
trivial constructions covered by the grammar are,
for instance: coordination, negation (in French,
negation is expressed with two words with com-
plex placement rules), long distance dependencies
(with island constraints). The object grammar con-
tains 2,074 syntactic structures which are produced
by 455 classes in the source grammar.
The French grammar has been tested on the
French TSNLP (Test Suite for the Natural Lan-
guage Processing) (Lehmann et al, 1996); this test
suite contains around 1,300 grammatical sentences
and 1,600 ungrammatical ones. The fact that our
grammar is based on linguistic knowledge ensures
a good coverage and greatly limits overgeneration:
88% of the grammatical sentences are correctly
parsed and 85% of the ungrammatical sentences
are rejected by our grammar.
2
LEOPAR is freely available under the CeCILL license at
http://www.loria.fr/equipes/calligramme/
leopar
155
A few months ago, we started to build an En-
glish IG. The modularity of the toolchain was an
advantage to build this grammar by abstracting the
initial grammar and then specifying the abstract
kernel for English. The English TSNLP has been
used to test the new grammar: 85% of the gram-
matical sentences are correctly parsed and 84%
of the ungrammatical sentences are rejected. It is
worth noting that those scores are obtained with a
grammar that is still being developed .
5 Future work
The toolchain we have presented here aims at pro-
ducing grammars and lexicons with large cover-
age from linguistic knowledge. This justifies the
choice of discarding statistical methods in the first
stage of the toolchain development: in the two
steps of lexical disambiguation and parsing, we
want to keep all possible solutions without dis-
carding even the less probable ones. Now, in a
next future, we have the ambition of using the
toolchain for parsing large raw corpora in differ-
ent languages.
For French, we have a large grammar and a large
lexicon, which are essential for such a task. The in-
troduction of statistics in the two modules of lexi-
cal disambiguation and parsing will contribute to
computational efficiency. Moreover, we have to
enrich our parsing strategies with robustness. We
also ambition to integrate semantics into grammars
and lexicons.
Our experience with English is a first step to take
multi-linguality into account. The crucial point
is to make our grammars evolve towards an even
more multi-lingual architecture with an abstract
kernel, common to different languages, and differ-
ent specifications of this kernel for different lan-
guages, thus following the approach of the Gram-
matical Framework (Ranta, 2004).
Finally, to make the toolchain evolve towards
multi-formalism, it is first necessary to extend
XMG for more genericity; there is no fundamental
obstacle to this task. Many widespread formalisms
can then benefit from our original methods of lex-
ical disambiguation and parsing, based on polari-
ties. (Kahane, 2006) presents the polarization of
several formalisms and (Kow, 2007) shows that
this way is promising.
References
Bonfante, G., B. Guillaume, and G. Perrier. 2004. Po-
larization and abstraction of grammatical formalisms
as methods for lexical disambiguation. In CoL-
ing?2004, 2004, pages 303?309, Geneva, Switzer-
land.
Boullier, P. 2003. Supertagging: A non-statistical
parsing-based approach. In IWPT 03, pages 55?65,
Nancy, France.
Bresnan, J. 2001. Lexical-Functional Syntax. Black-
well Publishers, Oxford.
Copestake, A. 2001. Implementing Typed Feature
Structure Grammars. CSLI Publications.
Crabb?e, B. 2005. Repr?esentation informatique de
grammaires fortement lexicalis?ees : application `a la
grammaire d?arbres adjoints. Phd thesis, Universit?e
Nancy 2.
Duchier, D., J. Le Roux, and Y. Parmentier. 2004.
The metagrammar compiler : A NLP Application
with a Multi-paradigm Architecture. In Second
International Mozart/Oz Conference - MOZ 2004,
Charleroi, Belgium.
Kahane, S. 2006. Polarized unification grammars.
In 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 137?144,
Sydney, Australia.
Kow, E. 2007. Surface realisation: ambiguity and de-
terminism. Phd thesis, Universit?e Nancy 2.
Lehmann, S., S. Oepen, S. Regnier-Pros, K. Netter,
V. Lux, J. Klein, K. Falkedal, F. Fouvry, D. Estival,
E. Dauphin, H. Compagnion, J. Baur, L. Balkan, and
D. Arnold. 1996. TSNLP ? Test Suites for Natu-
ral Language Processing. In CoLing 1996, Kopen-
hagen.
Perrier, G. 2003. Les grammaires d?interaction. Ha-
bilitation thesis, Universit?e Nancy 2.
Pollard, C.J. and I.A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ranta, A. 2004. Grammatical Framework: A Type-
Theoretical Grammar Formalism. Journal of Func-
tional Programming, 14(2):145?189.
Retor?e, C. 2000. The Logic of Categorial Grammars.
ESSLI?2000, Birmingham.
156
Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 17?24,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
PrepLex: a lexicon of French prepositions for parsing
Kare?n Fort
Calligramme and TALARIS projects
LORIA/INRIA Lorraine / Nancy, France
Karen.Fort@loria.fr
Bruno Guillaume
Calligramme project
LORIA/INRIA Lorraine / Nancy, France
Bruno.Guillaume@loria.fr
Abstract
PrepLex is a lexicon of French prepositions
which provides all the syntactic information
needed for parsing. It was built by compar-
ing and merging several authoritative lexical
sources. This lexicon also includes infor-
mation about the prepositions or classes of
prepositions that appear in French verb sub-
categorization frames. This resource has
been developed as a first step in making cur-
rent French preposition lexicons available
for effective natural language processing.
1 Introduction
When defining lexical entry classes according to cat-
egories, an obvious distinction appears between two
types of classes. First, the closed classes, compris-
ing elements which can be exhaustively enumerated,
for example pronouns or determiners. Second, open
classes for which it is impossible to list all the el-
ements (for example, they may vary according to
the domain). The four main open classes are nouns,
verbs, adjectives and adverbs. The lexicon construc-
tion methodology has to be adapted according to the
type of class that is being dealt with.
The status of the class of prepositions is difficult
to determine. A priori, prepositions may seem to be
a closed class, with elements which can be enumer-
ated. In practice, however, a comparison of the dif-
ferent available resources shows that it is not an easy
task to exhaustively list prepositions. Besides, they
represent more than 14% of French lemma tokens.1
1see for example, on a newspaper corpus:
A complete lexicon for parsing applications
should contain subcategorization information for
predicative words (Briscoe and Carroll, 1993; Car-
roll and Fang, 2004). This subcategorization infor-
mation often refers to prepositions in the description
of their arguments. Arguments are commonly used
with a particular preposition (for example compter
sur [count on]) or a set of semantically linked prepo-
sitions (such as aller [go] LOC, where LOC can be
any locative preposition).
For deep parsing, we need to distinguish between
indirect complements, required by the verb, and
adjuncts which do not appear in the verb valence.
The following two examples (1a) and (1b) have
the same surface structure, in which the two
preposition uses for avec can only be distinguished
semantically: in the first case, it introduces an
oblique complement, whereas in the second case,
it introduces an adjunct. This issue can be solved
using finer-grained semantic information.
1a. Jean se bat avec Paul
[Jean fights against Paul]
1b. Jean se bat avec courage
[Jean fights with courage]
This distinction leads us to allow two different
preposition uses and therefore causes lexical ambi-
guity. In order to limit this ambiguity, it is important
for a lexicon to identify the prepositions which can
have both functions (we will call these ?argument?
prepositions).
https://www.kuleuven.be/ilt/blf/
rechbaselex kul.php\#freq (Selva et al, 2002)
17
Our work aims at providing the community with
a lexicon that can be directly used by a parser. We
focused on syntactic aspects and extended the work
to some semantic elements, like semantically linked
sets of prepositions (as LOC). The generated lexicon
is freely available and is expected to be integrated
into larger resources for French, whether existing or
under development.
Section 2 describes the sources and the compar-
ative methodology we used. Section 3 details the
results of the comparison. Section 4 explains how
the lexicon was created from the above-mentioned
results. Finally, Section 5 shows an example of use
of the lexicon in a parsing application.
2 Methodology
In order to use prepositions for parsing, we need
a large list, containing both garden-variety preposi-
tions and prepositions that appear in verb subcatego-
rization frames.
2.1 Using syntactic lexicons
Obviously, some lexicons already exist which pro-
vide interesting lists of prepositions. This is the
case of Lefff (Sagot et al, 2006), which contains
a long list of prepositions. However, the syntactic
part of the lexicon is still under development and
it provides only few prepositions in verb subcate-
gorization frames. Besides, some prepositions in
Lefff are obsolete or rare. The French-UNL dic-
tionary (Se?rasset and Boitet, 2000) also contains
prepositions, but its coverage is quite limited and
the quality of its entries is not homogeneous. Other
sources present prepositions in verb subcategoriza-
tion frames, but the lists are not quite consistent.
We thus collected, as a first step, prepositions
from a certain number of resources, lexicons and
dictionaries for the garden-variety list, and syntactic
lexicons for the argument prepositions list. Two re-
sources belong to both categories, Lefff and French-
UNL dictionary:
? Lefff (Lexique des Formes Fle?chies du
Franc?ais/French inflected form lexicon (Sagot
et al, 2006)) is a large coverage (more than
110,000 lemmas) French morphological and
syntactic lexicon (see table 1 for an example of
a Lefff syntactic entry).
In its latest public version, 2.2.1, Lefff con-
tains 48 simple prepositions and 164 multiword
prepositions. It also provides information on
verb subcategorization frames, which contain
14 argument prepositions.
? UNL (Universal Networking Lan-
guage (Se?rasset and Boitet, 2000)), is a
French to disambiguated English dictionary for
machine translation, which contains syntactic
information in its French part (see table 1 for a
UNL example entry).
UNL has limited coverage (less than 27,000
lemmas), but it provides, in the English part,
semantic information that we will consider us-
ing in the near future. UNL contains 48 simple
prepositions, among which 12 appear in verb
subcategorization frames.
2.2 Using reference sources
We then completed the list of prepositions using
manually built resources, including lexicons, dictio-
naries and grammars:
? The Grevisse (Grevisse, 1997) grammar, in its
paper version, allowed us to check some intu-
itions concerning the obsolescence or usage of
some prepositions.
? The TLFi (Tre?sor de la langue franc?aise in-
formatise?), that we consulted through the CN-
RTL2, and that offers a slightly different list of
prepositions. In particular, it contains the forms
voici and voila`, that are seldom quoted in the
other available resources.
? Finally, the PrepNet (Saint-Dizier, 2006)
prepositions database was used to check the
completeness of our list as well as the semantic
information provided by other sources.
2.3 Using verb valence dictionaries
We then looked for a way to enrich the list of prepo-
sitions appearing in verb subcategorization frames
in Lefff and UNL, using resources that focus more
particularly on verbs:
2see: http://www.cnrtl.fr
18
Lefff entry for dialoguer avec [to talk to]
dialoguer: suj:sn|sinf|scompl,obja:(a`-sn|avec-sn),objde:(de-sn|de-scompl|de-sinf)
UNL entry for dialoguer avec [to talk to]
[dialoguer] {AUX(AVOIR),CAT(CATV),GP1(AVEC),VAL1(GN)} "have_talks";
DICOVALENCE entry for dialoguer avec [to talk to]
VAL$ dialoguer: P0 PP<avec>
VTYPE$ predicator simple
VERB$ DIALOGUER/dialoguer
NUM$ 29730
EG$ le de?le?gue? des e?tudiants a dialogue? avec le directeur de l?e?cole
TR$ spreken, zich onderhouden, een gesprek hebben, onderhandelen
P0$ qui, je, nous, elle, il, ils, on, celui-ci, ceux-ci
PP_PR$ avec
PP$ qui, lui_ton, eux, celui-ci, ceux-ci, l?un l?autre
LCCOMP$ nous dialoguons, je dialogue avec toi
SynLex entry for adapter avec [to adapt to]
adapter ?<suj:sn,obj:sn,obl:avec-sn>?
Table 1: Description of some entries with the preposition avec [with] in valence dictionaries
? DICOVALENCE, a valence dictionary of
French, formerly known as PROTON (van den
Eynde and Mertens, 2002), which has been
based on the pronominal approach. In version
1.1, this dictionary details the subcategoriza-
tion frames of more than 3,700 verbs (table 1
gives an example of a DICOVALENCE entry).
We extracted the simple and multiword prepo-
sitions it contains (i.e. more than 40), as well
as their associated semantic classes.
? We completed this argument prepositions list
with information gathered from SynLex (Gar-
dent et al, 2006), a syntactic lexicon cre-
ated from the LADL lexicon-grammar ta-
bles (Gross, 1975) (see table 1 for a SynLex
entry).
Using these sources, we conducted a systematic
study of each preposition, checking its presence
in each source, whether in verb subcategorization
frames or not, as well as its associated semantic
class(es). We then grouped the prepositions that ap-
pear both as lexical entries and in verb subcatego-
rization frames.
As multiword prepositions show specific charac-
teristics (in particular, their number) and raise partic-
ular issues (segmentation), we processed them sepa-
rately, using the same methodology.
3 Source comparison results
3.1 Simple prepositions
We thus listed 85 simple prepositions, among which
24 appear in verb subcategorization frames (see ta-
ble 2).
It is noticeable that the different sources use quite
different representations of syntactic information as
shown in table 1. Lefff offers a condensed vision
of verbs, in which valence patterns are grouped into
one single entry, whereas SynLex uses a flatter rep-
resentation without disjunction on syntactic cate-
gories for argument realization or for optional argu-
ments. To summarize, we could say that DICOVA-
LENCE lies somewhere between Lefff and SynLex,
since it uses disjunctive representation but has a finer
description of syntactic information and hence splits
many entries which are collapsed in Lefff.
3.2 Multiword prepositions
We obtained a list of 222 multiword prepositions,
among which 18 appear in verb subcategorization
frames (see table 3). It is to be noticed that only
DICOVALENCE and SynLex contain multiword
prepositions in verb subcategorization frames. As
for Lefff, it provides an impressive list of multiword
19
Lexicons Subcategorization frames
Lefff TLFi Grevisse PrepNet UNL Lefff DVa SynLex UNL
a` X X X loc 319 895 (18 loc) 887 (70 loc) 246
apre`s X X X loc X 2 12 1
aussi X
avec X X X X X 35 193 (1 loc) 611 (1 loc) 49
chez X X X loc X 9 (5 loc) 1
comme X X 14 11 10 3
de X X X deloc X 310 888 1980 282
(117 deloc) (69 deloc)
depuis X X X deloc X 2 1
derrie`re X X X loc X 3
devers X X X
dixit X
emmi X
entre X X X loc X 19 (3 loc) 4
hormis X X X X X
jusque X X X X 7 (7 loc)
le`s X X X
moyennant X X X X X
par X X X loc X 3 38 (4 loc) 73 8
parmi X X X loc X 7 (3 loc) 7
passe? X X
selon X X X X X 1 1
voici X X
Table 2: Some simple prepositions in different sources
aDICOVALENCE
prepositions (more than 150) which represents an
excellent basis for our work.
4 Lexicon construction
The first selection criterion we applied to build the
lexicon is that a preposition should appear in at least
one source among the above-mentioned ones. Also,
we consider a preposition to be an argument prepo-
sition if it appears in at least one verb subcategoriza-
tion frame.
4.1 Manual filtering
We then filtered the prepositions according to very
simple criteria. In particular, we identified some
prepositions to be removed as they were:
? erroneous, this is the case, for example, of
aussi (adverb rather than preposition), which is
present in the UNL dictionary as a preposition,
? obsolete or very rare, like emmi (from TLFi),
devers (from Lefff, TLFi, Grevisse) or comme
de (from DICOVALENCE).
We also checked the semantic features given in
the sources and removed erroneous ones, like avec
as locative in SynLex and DICOVALENCE.
4.2 Some remarks
Some sources include as prepositions forms that are
not universally considered to be prepositions in lin-
guistics. This is the case, in particular, for:
? comme, which is not present in the three refer-
ence sources (Grevisse, TLFi and PrepNet) as
it is ambiguous and can also be used as a con-
junction,
20
Lexicons Subcategorization frames
Lefff TLFi Grevisse PrepNet UNL Lefff DVa SynLex UNL
a` cause de X X X
a` la faveur de X X
a` partir de X X deloc 1
afin de X X X X
au nord de loc
au vu de X
aupre`s de X X X loc 27 (1 loc) 35
comme de 1
conforme?ment a` X X
d?avec X 1 6
d?entre X
en faveur de X X X 13
face a` X X 2
il y a X
jusqu?a` X loc X 10 (2 loc)
jusqu?en X
jusqu?ou` X
loin de X X loc
par suite de X
pour comble de X
pre`s de X X loc
quant a` X X X
tout au long de X X
vis-a`-vis de X X X 1
Table 3: Some multiword prepositions in different sources
aDICOVALENCE
? il y a or y compris, which only appear in Lefff,
? d?avec, which only appears in Grevisse and
verb subcategorization frames in DICOVA-
LENCE and SynLex.
We decided to keep those forms in the lexicon for
practical reasons, keeping the parsing application in
mind.
Moreover, even if its coverage is quite large, the
created lexicon is obviously not exhaustive. In
this respect, some missing entries should be added,
namely:
? prepositions from the DAFLES (Selva et al,
2002), like, for example, au de?triment de,
? prepositions appearing in reference grammars,
like question, in Grammaire me?thodique du
franc?ais (Riegel et al, 1997),
? some locative prepositions (and, through
metonymy, time prepositions) that are pre-
fixed by jusqu?, for example jusqu?aupre`s de.
This elided form of jusque should probably
be treated separately, as a preposition modi-
fier. The same goes for de`s, followed by a
time preposition (or a locative one, through
metonymy).
However, it is to be noticed that none of these
missing prepositions appear in verb subcategoriza-
tion frames.
This filtering process also allowed us to iden-
tify some issues, in particular elisions in multiword
21
forms, like afin de, afin d?, or contractions like face
a`, face au or a` partir de, a` partir du, which will be
processed in the segmentation step.
Others, like le`s, which is only used in toponyms
in dashed forms (e.g. Bathele?mont-le`s-Bauzemont),
will be processed during named entity segmentation.
4.3 Results
We obtained a list of 49 simple prepositions, of
which 23 appear in verb subcategorization frames
in at least one source and are therefore considered to
be argument prepositions (see table 4).
We also obtain a list of more than 200 multi-
word prepositions, among which 15 appear in verb
subcategorization frames in at least one source and
are therefore considered to be argument prepositions
(see table 5).
For the time being, we limited the semantic in-
formation in the lexicon to loc (locative) and deloc
(source), but we intend to extend those categories to
those used in DICOVALENCE (time, quantity, man-
ner). We have already added those to the preposi-
tions database that is being populated.
We also referred to the sources to add the cat-
egories of the arguments introduced by argument
prepositions.
PrepLex is currently distributed in a text format
suitable both for hand-editing and for integration in
a parser or other natural language processing tools.
In the format we propose, syntactic information is
described via feature structures. These feature struc-
tures are always recursive structures of depth 2. The
external level describes the structure in terms of ?ar-
guments? whereas the internal level gives a finer
syntactic description of either the head or of each
argument. This format aims at being modular and at
defining some ?classes? that share redundant infor-
mation. In the case of prepositions, the skeleton of
the feature structure used by all entries is:
Prep : [
head [cat=prep, prep=#, funct=#]
comp [cat=#, cpl=@]
]
When instantiated for a particular preposition, 3
feature values are to be provided (written with ?#?
in the above description) and the last parametrized
feature (written with @) is optional. When they are
in the head sub-structure, features are referred to by
their names whereas, in other cases, a prefix notation
is used.
a` [prep=a|LOC; funct=aobj|loc|adj;
comp.cat=np|sinf; comp.cpl=void|ceque]
apre`s [prep=apres|LOC; funct=obl|loc|adj;
comp.cat=np]
avec [prep=avec; funct=obl|adj;
comp.cat=np]
a`_travers [prep=a_travers; funct=obl|adj;
comp.cat=np]
Technically, the only difficult part is to decide
how to represent semantic classes of prepositions
like LOC. Here, we chose to define the whole set
of argument prepositions as well as all the semantic
classes (noted in uppercase) as possible atomic val-
ues for the prep feature. We then used the disjunc-
tion a|LOC to indicate that the preposition a` can be
used, either as a specific preposition or as a locative
preposition.
Additionally, we decided to add to the lexicon in-
formation about the sources in which the preposition
appears, in order to allow filtering for some specific
applications. In the case of argument prepositions,
we also added information about the preposition?s
frequency in the source, as well as a relevant exam-
ple.
We also decided to add corpus-based frequencies
to the lexicon. Thus, for each preposition, we pro-
vide its frequency per 1000 words, either as found in
the DAFLES (Selva et al, 2002), from a newspaper
corpus composed of Le Monde and Le Soir (1998),
or as extracted directly from Le Monde (1998) with
a simple grep command, without tagging.
5 Using the lexicon in a NLP system
We briefly expose some parsing problems related to
prepositions.
5.1 Segmentation issues
The first issue that appears when integrating preposi-
tions in a parsing system is that of segmentation. In
particular, contractions have to be processed specif-
ically so that au is identified as the equivalent of
a` le. The same goes for de, which can appear in
some multiword prepositions and can be elided as
d?. However, these phenomena are not specific to
prepositions. They can be addressed either in the
lexicon (for example Lefff explicitly contains both
22
Lexicons Subcategorization frames
Lefff TLFi Grevisse PrepNet UNL PrepLex Lefff DV SynLex UNL PrepLex
44 69 55 36 46 49 14 24 18 11 23
Table 4: Total number of simple prepositions by source
Lexicons Subcategorization frames
Lefff TLFi Grevisse PrepNet UNL PrepLex Lefff DV SynLex UNL PrepLex
166 11 77 89 2 206 0 16 4 0 15
Table 5: Total number of multiword prepositions by source
au cours de and au cours d?), or during the segmen-
tation step.
We decided on the second solution as it improves
lexicon maintainability.
An issue that is more directly linked to multiword
prepositions is that of segmentation ambiguities. For
example, in the following two sentences (2a) and
(2b) the group of words au cours de is a multiword
preposition in the first case, but it has to be decom-
posed in the second one. Other multiword preposi-
tions can never be decomposed, for example y com-
pris.
This highlights the fact that segmentation is am-
biguous and that it is necessary to be able to keep
the segmentation ambiguity through the whole pars-
ing process.
2a. Il a beaucoup travaille? au cours de cette anne?e
[He worked hard during the year]
2b. Il a beaucoup travaille? au cours de M. Durand
[He worked hard in Mr Durand?s course]
5.2 Adjunct prepositions vs argument
prepositions
In deep parsing we have to distinguish between
prepositions introducing a verb argument and prepo-
sitions introducing adjuncts. However, we have
seen that this distinction often relies on semantics
and that parsing should leave the two possibilities
open. Precise information about argument preposi-
tions and verb subcategorizations eliminates many
of these ambiguities.
6 Conclusion
We created a list of French prepositions for parsing
applications by comparing various lexicons and dic-
tionaries. We hence focused on syntactic aspects.
Manual filtering was used to eliminate obsolete or
rare prepositions, as well as a number of errors.
The resulting lexicon contains more than 250 French
prepositions, among which 49 are simple preposi-
tions.
In syntactic lexicons, subcategorization frames
describe prepositions introducing arguments. Prepo-
sitions appearing in verbal valence frames are called
?argument prepositions?. We identified 40 of them.
The produced lexicon is freely available. 3 It will
be developed further. In particular, some other in-
formation sources will be incorporated. This is the
case for the verbs constructions fields from the TFLi
which contain prepositions, that can be considered
as argument prepositions. We plan to use this infor-
mation to improve the lexicon.
We are also populating a database with this lexical
information. 3 This will help us ensure a better main-
tenance of the lexicon and will allow enrichment of
the entries, in particular with examples and associ-
ated verbs. We are adding corpus-based frequencies
to this database.
A more ambitious task would be to enrich the lex-
icon with fine-grained semantic information (more
detailed than the general classes loc, deloc, . . . ).
Many interesting linguistic studies have been con-
ducted on prepositions, including cross-lingual ap-
proaches. However, most of them are limited to de-
tailing the semantics of a small number of preposi-
tions; with the exceptions of PrepNet (Saint-Dizier,
2006) for French prepositions and TPP (Litkowski
and Hargraves, 2005) (The Preposition Project) for
English. It is now necessary to transform those re-
sources in order to make them directly usable by nat-
ural language processing systems.
3http://loriatal.loria.fr/Resources.html
23
References
Ted Briscoe and John A. Carroll. 1993. Generalized
probabilistic LR parsing of natural language (corpora)
with unification-based grammars. Computational Lin-
guistics, 19(1):25?59.
John A. Carroll and Alex C. Fang. 2004. The automatic
acquisition of verb subcategorisations and their impact
on the performance of an HPSG parser. In Proceed-
ings of the 1st International Joint Conference on Nat-
ural Language Processing (IJCNLP), pages 107?114,
Sanya City, China.
Claire Gardent, Bruno Guillaume, Guy Perrier, and In-
grid Falk. 2006. Extraction d?information de sous-
cate?gorisation a` partir des tables du LADL. In Pro-
ceedings of TALN 06, pages 139?148, Leuven.
Maurice Grevisse. 1997. Le Bon Usage ? Gram-
maire franc?aise, e?dition refondue par Andre? Goosse.
DeBoeck-Duculot, Paris ? Leuven, 13th edition.
Maurice Gross. 1975. Me?thodes en syntaxe. Hermann.
Ken Litkowski and Orin Hargraves. 2005. The preposi-
tion project. In Proc. of the ACL Workshop on Prepo-
sitions.
Martin Riegel, Jean-Christophe Pellat, and Rene? Rioul.
1997. Grammaire me?thodique du franc?ais. PUF, 3rd
edition.
Benoit Sagot, Lionel Cle?ment, ?Eric Villemonte de la
Clergerie, and Pierre Boullier. 2006. The Lefff 2
syntactic lexicon for French: architecture, acquisition,
use. In Proc. of LREC 06, Genoa, Italy.
Patrick Saint-Dizier. 2006. PrepNet: a Multilingual Lex-
ical Description of Prepositions. In Proc. of LREC 06,
Genoa, Italy, pages 877?885. European Language Re-
sources Association (ELRA).
Thierry Selva, Serge Verlinde, and Jean Binon. 2002.
Le DAFLES, un nouveau dictionnaire pour apprenants
du franc?ais. In Proc. of EURALEX?2002 (European
Association for Lexicography), Copenhagen.
Gilles Se?rasset and Christian Boitet. 2000. On UNL as
the future ?html of the linguistic content? and the reuse
of existing NLP components in UNL-related applica-
tions with the example of a UNL-French deconverter.
In Proceedings of COLING 2000, Saarbru?cken.
Karel van den Eynde and Piet Mertens, 2002. La valence:
l?approche pronominale et son application au lexique
verbal, pages 63?104. Cambridge University Press,
13th edition.
24
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 142?145,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Towards a Methodology for Named Entities Annotation
Kar?n Fort
INIST / LIPN
2 all?e du Parc de Brabois,
54500 Vandoeuvre-l?s-Nancy, France
karen.fort@inist.fr
Maud Ehrmann
XRCE
6 Chemin de Maupertuis,
38240 Meylan, France
Ehrmann@xrce.xerox.com
Adeline Nazarenko
LIPN, Universit? Paris 13 & CNRS
99 av. J.B. Cl?ment,
93430 Villetaneuse, France
nazarenko@lipn.univ-paris13.fr
Abstract
Today, the named entity recognition task is
considered as fundamental, but it involves
some specific difficulties in terms of anno-
tation. Those issues led us to ask the fun-
damental question of what the annotators
should annotate and, even more important,
for which purpose. We thus identify the
applications using named entity recogni-
tion and, according to the real needs of
those applications, we propose to seman-
tically define the elements to annotate. Fi-
nally, we put forward a number of method-
ological recommendations to ensure a co-
herent and reliable annotation scheme.
1 Introduction
Named entity (NE) extraction appeared in the mid-
dle of the 1990s with the MUC conferences (Mes-
sage Understanding Conferences). It has now be-
come a successful Natural Language Processing
(NLP) task that cannot be ignored. However, the
underlying corpus annotation is still little studied.
The issues at stake in manual annotation are cru-
cial for system design, be it manual design, ma-
chine learning, training or evaluation. Manual an-
notations give a precise description of the expected
results of the target system. Focusing on manual
annotation issues led us to examine what named
entities are and what they are used for.
2 Named Entities Annotation: practice
and difficulties
Named entity recognition is a well-established
task (Nadeau and Sekine, 2007). One can recall its
evolution according to three main directions. The
first corresponds to work in the ?general? field,
0This work was partly realised as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for innova-
tion.
with the continuation of the task defined by MUC
for languages other than English, with a revised set
of categories, mainly with journalistic corpora1.
The second direction relates to work in ?special-
ized? domains, with the recognition of entities in
medicine, chemistry or microbiology, like gene
and protein names in specialized literature2. The
last direction, spanning the two previous ones, is
disambiguation.
For each of those evaluation campaigns, cor-
pora were built and annotated manually. They
are generally used to develop automatic annotation
tools. ?To Develop? is to be understood in a broad
sense: the goal is to describe what automatic sys-
tems should do, to help writing the symbolic rules
they are based on, to learn those rules or decision
criteria automatically, and, finally, to evaluate the
results obtained by comparing them with a gold
standard. The annotation process brings into play
two actors, an annotator and a text. The text anno-
tation must follow precise guidelines, satisfy qual-
ity criteria and support evaluation.
In the general field, the MUC, CoNLL and
ACE evaluation campaigns seem to have paid at-
tention to the process of manual NE annotation,
with the definition of annotation guidelines and
the calculation of inter-annotator (but not intra-
annotator) agreement, using a back-and-forth pro-
cess between annotating the corpus and defining
the annotation guidelines. Nevertheless, some as-
pects of the annotation criteria remained problem-
atic, caused mainly by ?different interpretations
of vague portions of the guidelines? (Sundheim,
1995). In the fields of biology and medicine, texts
from specialized databases (PubMed and Med-
Line3) were annotated. Annotation guidelines
1See the evaluation campaigns MET, IREX, CoNNL,
ACE, ESTER and HAREM (Ehrmann, 2008, pp. 19-21).
2See the evaluation campaigns BioCreAtIvE (Kim et al,
2004) and JNLPBA (Hirschman et al, 2005).
3www.ncbi.nlm.nih.gov/pubmed, http://medline.cos.com
142
were vague about the annotation of NEs 4, and few
studies measured annotation quality. For the GE-
NIA (Kim et al, 2003), PennBioIE (Kulick et al,
2004) or GENETAG (Tanabe et al, 2005) corpora,
no inter- or intra-annotator agreement is reported.
If NE annotation seems a well-established prac-
tice, it involves some difficulties.
As regards general language corpora, those dif-
ficulties are identified (Ehrmann, 2008). The first
one relates to the choice of annotation categories
and the determination of what they encompass.
Indeed, beyond the ?universal? triad defined by
the MUC conferences (ENAMEX, NUMEX and
TIMEX), the inventory of categories is difficult to
stabilize. For ENAMEX, although it may be ob-
vious that the name of an individual such as Kofi
Annan is to be annotated using this category, what
to do with the Kennedys, Zorro, the Democrats or
Santa Claus? For the other categories, it is just
as difficult to choose the granularity of the cat-
egories and to determine what they encompass.
Another type of difficulty relates to the selection
of the mentions to be annotated as well as the de-
limitation of NE boundaries. Let us consider the
NE ?Barack Obama? and the various lexemes that
can refer to it: Barack Obama, Mr Obama, the
President of the United States, the new president,
he. Should we annotate proper nouns only, or also
definite descriptions that identify this person, even
pronouns which, contextually, could refer to this
NE? And what to do with the various attributes
that go with this NE (Mr and president)? Coordi-
nation and overlapping phenomena can also raise
problems for the annotators. Finally, another dif-
ficulty results from phenomena of referential plu-
rality, with homonyms NEs (Java place and Java
language) and metonyms (England as a geograph-
ical place, a government or sport team).
Our experience in microbiology shows that
these difficulties are even more acute in special-
ized language. We carried out an annotation ex-
periment on an English corpus of PubMed notices.
The main difficulty encountered related to the
distinction required between proper and common
nouns, the morphological boundary between the
two being unclear in those fields where common
nouns are often reclassified as ?proper nouns?, as
is demonstrated by the presence of these names
4(Tanabe et al, 2005) notes that ?a more detailed defi-
nition of a gene/protein name, as well as additional annota-
tion rules, could improve inter-annotator agreement and help
solve some of the tagging inconsistencies?.
in nomenclatures (small, acid-soluble spore pro-
tein A is an extreme case) or acronymisation phe-
nomena (one finds for example across the outer
membrane (OM)). In those cases, annotators were
instructed to refer to official lists, such as Swiss-
Prot5, which requires a significant amount of time.
Delimiting the boundaries of the elements to be
annotated also raised many questions. One can
thus choose to annotate nifh messenger RNA if it is
considered that the mention of the state messenger
RNA is part of the determination of the reference,
or only nifh, if it is considered that the proper noun
is enough to build the determination. Selecting se-
mantic types was also a problem for the annota-
tors, in particular for mobile genetic elements, like
plasmids or transposons. Indeed, those were to be
annotated in taxons but not in genes whereas they
are chunks of DNA, therefore parts of genome. A
particularly confusing directive for the annotators
was to annotate the acronym KGFR as a proper
noun and the developed form keratinocyte growth
Factor receptor as a common noun. This kind of
instruction is difficult to comprehend and should
have been documented better.
These problems result in increased annotation
costs, too long annotation guidelines and, above
all, a lot of indecision for the annotators, which
induces inconsistencies and lower-quality annota-
tion. This led us to consider the issue of what the
annotators must annotate (semantic foundations of
NE) and, above all, why.
3 What to Annotate?
3.1 Various Defining Criteria
Ehrmann (2008) proposes a linguistic analysis
of the notion of NE, which is presented as an
NLP ?creation?. In the following paragraphs, we
take up the distinction introduced in LDC (2004):
NE are ?mentions? refering to domain ?entities?,
those mentions relate to different linguistic cate-
gories: proper nouns (?Rabelais?), but also pro-
nouns (?he?), and in a broader sense, definite de-
scriptions (?the father of Gargantua?). Several
defining criteria for NE can be identified.
Referential Unicity One of the main charac-
teristics of proper nouns is their referential be-
haviour: a proper noun refers to a unique refer-
ential entity, even if this unicity is contextual. We
consider that this property is essential in the usage
of NEs in NLP.
5http://www.expasy.org/sprot/
143
Referential Autonomy NEs are also au-
tonomous from the referential point of view. It
is obvious in the case of proper nouns, which are
self-sufficient to identify the referent, at least in a
given communication situation (Eurotunnel). The
case of definite descriptions (The Channel Tunnel
operator) is a bit different: they can be used to
identify the referent thanks to external knowledge.
Denominational Stability Proper nouns are
also stable denominations. Even if some varia-
tions may appear (A. Merkel/Mrs Merkel), they
are more regular and less numerous than for other
noun phrases6.
Referential Relativity Interpretation is always
carried out relatively to a domain model, that can
be implicit in simple cases (for example, a country
or a person) but has to be made explicit when the
diversity in entities to consider increases.
3.2 Different Annotation Perspectives
The defining criteria do not play the same role
in all applications. In some cases (indexing and
knowledge integration), we focus on referential
entities which are designated by stable and non-
ambiguous descriptors. In those cases, the NEs
to use are proper nouns or indexing NEs and they
should be normalized to identify variations that
can appear despite their referential stability. For
this type of application, the main point is not to
highlight all the mentions of an entity in a doc-
ument, but to identify which document mentions
which entity. Therefore, precision has to be fa-
vored over recall. On the other hand, in the tasks
of information extraction and domain modelling, it
is important to identify all the mentions, including
definite descriptions (therefore, coreference rela-
tions between mentions that are not autonomous
enough from a referential point of view are also
important to identify).
As it is impossible to identify the mentions of all
the referential entities, the domain model defines
which entities are ?of interest? and the boundary
between what has to be annotated or not. For
instance, when a human resources director is in-
terested in the payroll in the organization, s/he
thinks in terms of personnel categories and not
in terms of the employees as individuals. This
appears in the domain model: the different cate-
gories of persons (technicians, engineers, etc.) are
6A contrario, this explains the importance of synonyms
identification in domains where denominations are not stable
(like, for instance, in genomics).
modelled as instances attached to the concept CAT-
OF-EMPLOYEES and the individuals are not rep-
resented. On the opposite, when s/he deals with
employees? paychecks and promotion, s/he is in-
terested in individuals. In this case, the model
should consider the persons as instances and the
categories of personnel as concepts.
Domain modelling implies making explicit
choices where texts can be fuzzy and mix points
of view. It is therefore impossible to annotate the
NEs of a text without refering to a model. In the
case of the above experiment, as it is often the
case, the model was simply described by a list of
concepts: the annotators had to name genes and
proteins, but also their families, compositions and
components.
4 Annotation methodology
Annotation guidelines As the targeted annota-
tion depends on what one wants to annotate and
how it will be exploited, it is important to provide
annotators with guidelines that explain what must
be annotated rather than how it should be anno-
tated. Very often, feasibility constraints overcome
semantic criteria,7 which confuses annotators. Be-
sides, it is important to take into consideration the
complexity of the annotation task, without exclud-
ing the dubious annotations or those which would
be too difficult to reproduce automatically. On the
contrary, one of the roles of manual annotation
is to give a general idea of the task complexity.
The annotators must have a clear view of the tar-
get application. This view must be based on an
explicit reference model, as that of GENIA, with
precise definitions and explicit modelling choices.
Examples can be added for illustration but they
should not replace the definition of the goal. It
is important that annotators understand the under-
lying logic of annotation. It helps avoiding mis-
understandings and giving them a sense of being
involved and committed.
Annotation tools Although there exists many
annotation tools, few are actually available, free,
downloadable and usable. Among those tools are
Callisto, MMAX2, Knowtator or Cadixe8 which
was used in the reported experiment. The features
7"In [src homology 2 and 3], it seems excessive to require
an NER program to recognize the entire fragment, however,
3 alone is not a valid gene name." (Tanabe et al, 2005).
8http://callisto.mitre.org, http://mmax2.sourceforge.net,
http://knowtator.sourceforge.net, http://caderige.imag.fr
144
and the annotation language expressivity must be
adapted to the targeted annotation task: is it suf-
ficient to type the textual segments or should they
also be related? is it possible/necessary to have
concurrent or overlapping annotations? In our ex-
periment on biology, for instance, although the an-
notators had the possibility to mention their un-
certainty by adding an attribute to the annotations,
they seldom did so, because it was not easy to do
using the provided interface.
Annotation evaluation Gut and Bayerl (2004)
distinguishes the inter-annotator agreement, which
measures the annotation stability, and the intra-
annotation agreement that gives an idea on how
reproducible an annotation is. The inter- and intra-
annotator agreements do not have to be measured
on the whole corpus, but quite early in the annota-
tion process, so that the annotation guidelines can
be modified. Another way to evaluate annotation
relies on annotator introspection. Annotators are
asked to auto-evaluate the reliability of their an-
notations and their (un)certainty attributes can be
used afterwards to evaluate the overall quality of
the work. Since we did not have several anno-
tators working independently on our biology cor-
pus, we asked them to indicate the uncertainty of
their annotations on a carefully selected sample
corpus. 25 files were extracted out of the 499 texts
of our corpus (5%). This evaluation required only
few hours of work and it enabled to better qualify
and quantity annotation confidence. The annota-
tors declared that around 20% of the total number
of annotation tags were "uncertain". We observed
that more than 75% of these uncertain tags were
associated to common nouns of type bacteria and
that uncertainty was very often (77%) linked to the
fact that distinguishing common and proper nouns
was difficult.
More generally, a good annotation methodology
consists in having several annotators working in-
dependently on the same sample corpus very early
in the process. It allows to quickly identify the dis-
agreement causes. If they can be solved, new rec-
ommendations are added to the annotation guide-
lines. If not, the annotation task might be simpli-
fied and the dubious cases eliminated.
5 Conclusion and Prospects
In the end, two main points must be considered for
a rigorous and efficient NE annotation in corpus.
First, as for the content, it is important to focus,
not on how to annotate, but rather on what to anno-
tate, according to the final application. Once spec-
ified what is to be annotated, one has to be cau-
tious in terms of methodology and consider from
the very beginning of the campaign, the evaluation
of the produced annotation.
We intend to apply this methodology to other
annotation campaigns of the project we participate
in. As those campaigns cover terminology and se-
mantic relations extraction, we will have to adapt
our method to those applications.
References
Maud Ehrmann. 2008. Les entit?s nomm?es, de la
linguistique au TAL : statut th?orique et m?thodes
de d?sambigu?sation. Ph.D. thesis, Univ. Paris 7.
Ulrike Gut and Petra Saskia Bayerl. 2004. Measuring
the reliability of manual annotations of speech cor-
pora. In Proc. of Speech Prosody, pages 565?568,
Nara, Japan.
Lynette Hirschman, Alexander Yeh, Christian
Blaschke, and Alfonso Valencia. 2005. Overview
of biocreative: critical assessment of information
extraction for biology. BMC Bioinformatics, 6(1).
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Ge-
nia corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19:180?182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduc-
tion to the bio-entity recognition task at JNLPBA.
In Proc. of JNLPBA COLING 2004 Workshop, pages
70?75.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
and Lyle Ungar. 2004. Integrated annotation for
biomedical information extraction. In HLT-NAACL
2004 Workshop: Biolink. ACL.
LDC. 2004. ACE (Automatic Content Extraction)
english annotation guidelines for entities. Livrable
version 5.6.1 2005.05.23, Linguistic Data Consor-
tium.
David Nadeau and Satoshi Sekine. 2007. A survey
of named entity recognition and classification. Lin-
guisticae Investigaciones, 30(1):3?26.
B. Sundheim. 1995. Overview of results of the MUC-6
evaluation. In Proc. of the 6th Message Understand-
ing Conference. Morgan Kaufmann Publishers.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne
Matten, and John Wilbur1. 2005. Genetag: a tagged
corpus for gene/protein named entity recognition.
Bioinformatics, 6.
145
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 56?63,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Influence of Pre-annotation on POS-tagged Corpus Development
Kare?n Fort
INIST CNRS / LIPN
Nancy / Paris, France.
karen.fort@inist.fr
Beno??t Sagot
INRIA Paris-Rocquencourt / Paris 7
Paris, France.
benoit.sagot@inria.fr
Abstract
This article details a series of carefully de-
signed experiments aiming at evaluating
the influence of automatic pre-annotation
on the manual part-of-speech annotation
of a corpus, both from the quality and the
time points of view, with a specific atten-
tion drawn to biases. For this purpose, we
manually annotated parts of the Penn Tree-
bank corpus (Marcus et al, 1993) under
various experimental setups, either from
scratch or using various pre-annotations.
These experiments confirm and detail the
gain in quality observed before (Marcus et
al., 1993; Dandapat et al, 2009; Rehbein
et al, 2009), while showing that biases do
appear and should be taken into account.
They finally demonstrate that even a not
so accurate tagger can help improving an-
notation speed.
1 Introduction
Training a machine-learning based part-of-speech
(POS) tagger implies manually tagging a signifi-
cant amount of text. The cost of this, in terms of
human effort, slows down the development of tag-
gers for under-resourced languages.
One usual way to improve this situation is to
automatically pre-annotate the corpus, so that the
work of the annotators is limited to the validation
of this pre-annotation. This method proved quite
efficient in a number of POS-annotated corpus de-
velopment projects (Marcus et al, 1993; Danda-
pat et al, 2009), allowing for a significant gain
not only in annotation time but also in consistency.
However, the influence of the pre-tagging quality
on the error rate in the resulting annotated corpus
and the bias introduced by the pre-annotation has
been little examined. This is what we propose to
do here, using different parts of the Penn Treebank
to train various instances of a POS tagger and ex-
periment on pre-annotation. Our goal is to assess
the impact of the quality (i.e., accuracy) of the
POS tagger used for pre-annotating and to com-
pare the use of pre-annotation with purely manual
tagging, while minimizing all kinds of biases. We
quantify the results in terms of error rate in the re-
sulting annotated corpus, manual annotation time
and inter-annotator agreement.
This article is organized as follows. In Sec-
tion 2, we mention some related work, while Sec-
tion 3 describes the experimental setup, followed
by a discussion on the obtained results (Section 4)
and a conclusion.
2 Related Work
2.1 Pre-annotation for POS Tagging
Very few manual annotation projects give details
about the campaign itself. One major exception is
the Penn Treebank project (Marcus et al, 1993),
that provided detailed information about the man-
ual annotation methodology, evaluation and cost.
Marcus et al (1993) thus showed that manual tag-
ging took twice as long as correcting pre-tagged
text and resulted in twice the inter-annotator dis-
agreement rate, as well as an error rate (using a
gold-standard annotation) about 50% higher. The
pre-annotation was done using a tagger trained on
the Brown Corpus, which, due to errors introduced
by an automatic mapping of tags from the Brown
tagset to the Penn Treebank tagset, had an error
rate of 7?9%. However, they report neither the in-
fluence of the training of the annotators on the po-
tential biases in correction, nor that of the quality
of the tagger on the correction time and the ob-
tained quality.
Dandapat et al (2009) went further and showed
that, for complex POS-tagging (for Hindi and
Bangla), pre-annotation of the corpus allows for
a gain in time, but not necessarily in consis-
56
tency, which depends largely on the pre-tagging
quality. They also noticed that untrained annota-
tors were more influenced by pre-annotation than
the trained ones, who showed ?consistent perfor-
mance?. However, this very complete and inter-
esting experiment lacked a reference allowing for
an evaluation of the quality of the annotations. Be-
sides, it only took into account two types of pre-
tagging quality, high accuracy and low accuracy.
2.2 Pre-annotation in Other Annotation
Tasks
Alex et al (2008) led some experiments in the
biomedical domain, within the framework of a
?curation? task of protein-protein interaction. Cu-
ration consists in reading through electronic ver-
sion of papers and entering retrieved information
into a template. They showed that perfectly pre-
annotating the corpus leads to a reduction of more
than 1/3 in curation time, as well as a better recall
from the annotators. Less perfect pre-annotation
still leads to a gain in time, but less so (a little less
than 1/4th). They also tested the effect of higher
recall or precision of pre-annotation on one anno-
tator (curator), who rated recall more positively
than precision. However, as they notice, this result
can be explained by the curation style and should
be tested on more annotators.
Rehbein et al (2009) led quite thorough ex-
periments on the subject, in the field of semantic
frame assignment annotation. They asked 6 an-
notators to annotate or correct frame assignment
using a task-specific annotation tool. Here again,
pre-annotation was done using only two types of
pre-tagging quality, state-of-the-art and enhanced.
The results of the experiments are a bit disappoint-
ing as they could not find a direct improvement of
annotation time using pre-annotation. The authors
reckon this might be at least partly due to ?an inter-
action between time savings from pre-annotation
and time savings due to a training effect.? For
the same reason, they had to exclude some of the
annotation results for quality evaluation in order
to show that, in line with (Marcus et al, 1993),
quality pre-annotation helps increasing annotation
quality. They also found that noisy and low qual-
ity pre-annotation does not overall corrupt human
judgment.
On the other hand, Fort et al (2009) claim that
pre-annotation introduces a bias in named entity
annotation, due to the preference given by anno-
tators to what is already annotated, thus prevent-
ing them from noticing entities that were not pre-
annotated. This particular type of bias should not
appear in POS-tagging, as all the elements are to
be annotated, but a pre-tagging could influence
the annotators, preventing them from asking them-
selves questions about a specific pre-annotation.
In a completely different field, Barque et
al. (2010) used a series of NLP tools, called
MACAON, to automatically identify the central
component and optional peripheral components of
dictionary definitions. This pre-processing gave
disappointing results as compared to entirely man-
ual annotation, as it did not allow for a significant
gain in time. The authors consider that the bad
results are due to the quality of the tool that they
wish to improve as they believe that ?an automatic
segmentation of better quality would surely yield
some gains.?
Yet, the question remains: is there a quality
threshold for pre-annotation to be useful? and if
so, how can we evaluate it? We tried to answer
at least part of these questions for a quite simple
task for which data is available: POS-tagging in
English.
3 Experimental Setup
The idea underlying our experiments is the follow-
ing. We split the Penn Treebank corpus (Marcus et
al., 1993) in a usual manner, namely we use Sec-
tions 2 to 21 to train various instances of a POS
tagger, and Section 23 to perform the actual ex-
periments. In order to measure the impact of the
POS tagger?s quality, we trained it on subcorpora
of increasing sizes, and pre-annotated Section 23
with these various POS taggers. Then, we man-
ually annotated parts of Section 23 under various
experimental setups, either from scratch or using
various pre-annotations, as explained below.
3.1 Creating the Taggers
We used the MElt POS tagger (Denis and Sagot,
2009), a maximum-entropy based system that is
able to take into account both information ex-
tracted from a training corpus and information ex-
tracted from an external morphological lexicon.1
It has been shown to lead to a state-of-the-art POS
tagger for French. Trained on Sections 2 to 21
1MElt is freely available under LGPL license, on the web
page of its hosting project (http://gforge.inria.
fr/projects/lingwb/) .
57
of the Penn Treebank (MEltALLen ), and evaluated
on Section 23, MElt exhibits a 96.4% accuracy,
which is reasonably close to the state-of-the-art
(Spoustova? et al (2009) report 97.4%). Since it is
trained without any external lexicon, MEltALLen is
very close to the original maximum-entropy based
tagger (Ratnaparkhi, 1996), which has indeed a
similar 96.6% accuracy.
We trained MElt on increasingly larger parts of
the POS-tagged Penn Treebank,2 thus creating dif-
ferent taggers with growing degrees of accuracy
(see table 1). We then POS-tagged the Section 23
with each of these taggers, thus obtaining for each
sentence in Section 23 a set of pre-annotations,
one from each tagger.
Tagger Nb train. sent. Nb tokens Acc. (%)
MElt10en 10 189 66.5
MElt50en 50 1,254 81.6
MElt100en 100 2,774 86.7
MElt500en 500 12,630 92.1
MElt1000en 1,000 25,994 93.6
MElt5000en 5,000 126,376 95.8
MElt10000en 10,000 252,416 96.2
MEltALLen 37,990 944,859 96.4
Table 1: Accuracy of the created taggers evaluated
on Section 23 of the Penn Treebank
3.2 Experiments
We designed different experimental setups to
evaluate the impact of pre-annotation and pre-
annotation accuracy on the quality of the resulting
corpus. The subparts of Section 23 that we used
for these experiments are identified by sentence
ids (e.g., 1?100 denotes the 100 first sentences in
Section 23).
Two annotators were involved in the experi-
ments. They both have a good knowledge of lin-
guistics, without being linguists themselves and
had only little prior knowledge of the Penn Tree-
bank POS tagset. One of them had previous exper-
tise in POS tagging (Annotator1). It should also
be noticed that, though they speak fluent English,
they are not native speakers of the language. They
were asked to keep track of their annotation time,
noting the time it took them to annotate or correct
each series of 10 sentences. They were also asked
to use only a basic text editor, with no macro or
specific feature that could help them, apart from
2More precisely, MEltien is trained on the i first sentences
of the overall training corpus, i.e. Sections 2 to 21.
the usual ones, like Find, Replace, etc. The set
of 36 tags used in the Penn Treebank and quite
a number of particular cases is a lot to keep in
mind. This implies a heavy cognitive load in short-
term memory, especially as no specific interface
was used to help annotating or correcting the pre-
annotations.
It was demonstrated that training improves
the quality of manual annotation in a significant
way as well as allows for a significant gain in
time (Marcus et al, 1993; Dandapat et al, 2009;
Mikulova? and S?te?pa?nek, 2009). In particular, Mar-
cus et al (1993) observed that it took the Penn
Treebank annotators 1 month to get fully efficient
on the POS-tagging correction task, reaching a
speed of 20 minutes per 1,000 words. The speed of
annotation in our experiments cannot be compared
to this, as our annotators only annotated and cor-
rected small samples of the Penn Treebank. How-
ever, the annotators? speed and correctness did
improve with practice. As explained below, we
took this learning curve into account, as previous
work (Rehbein et al, 2009) showed it has an sig-
nificant impact on the results.
Also, during each experiment, sentences were
annotated sequentially. Moreover, the experiments
were conducted in the order we describe them be-
low. For example, both annotators started their
first annotation task (sentences 1?100) with sen-
tence 1.
We conducted the following experiments:
1. Impact of the pre-annotation accuracy on
precision and inter-annotator agreement:
In this experiment, we used sentences 1?
400 with random pre-annotation: for each
sentence, one pre-annotation is randomly
selected among its possible pre-annotations
(one for each tagger instance). The aim of
this is to eliminate the bias caused by the an-
notators? learning curve. Annotation time for
each series of 10 consecutive sentences was
gathered, as well as precision w.r.t. the refer-
ence and inter-annotator agreement (both an-
notators annotated sentences 1?100 and 301?
400, while only one annotated 101?200 and
the other 201?300).
2. Impact of the pre-annotation accuracy on
annotation time: This experiment is based
on sentences 601?760, with pre-annotation.
We divided them in series of 10 sentences.
58
For each series, one pre-annotation is se-
lected (i.e., the pre-annotation produced by
one of the 8 taggers), in such a way that each
pre-annotation is used for 2 series. We mea-
sured the manual annotation time for each se-
ries and each annotator.
3. Bias induced by pre-annotation: In this
experiment, both annotators annotated sen-
tences 451?500 fully manually.3 Later,
they annotated sentences 451?475 with the
pre-annotation from MEltALLen (the best tag-
ger) and sentences 476?500 with the pre-
annotation from MElt50en (the second-worst
tagger). We then compared the fully man-
ual annotations with those based on pre-
annotations to check if and how they diverge
from the Penn Treebank ?gold-standard?; we
also compared annotation times, in order to
get a confirmation of the gain in time ob-
served in previous experiments.
4 Results and Discussion
4.1 Impact of the Pre-annotation Accuracy
on Precision and Inter-annotator
Agreement
The quality of the annotations created during ex-
periment 1 was evaluated using two methods.
First, we considered the original Penn Treebank
annotations as reference and calculated a simple
precision as compared to this reference. Figure 1
gives an overview of the obtained results (note that
the scale is not regular).
However, this is not sufficient to evaluate the
quality of the annotation as, actually, the reference
annotation is not perfect (see below). We therefore
evaluated the reliability of the annotation, calcu-
lating the inter-annotator agreement between An-
notator1 and Annotator2 on the 100-sentence se-
ries they both annotated. We calculated this agree-
ment on some of the subcorpora using pi, aka Car-
letta?s Kappa (Carletta, 1996)4. The results of this
are shown in table 2.
3During this manual annotation step (with no pre-
annotation), we noticed that the annotators used the
Find/Replace all feature of the text editor to fasten
the tagging of some obvious tokens like the or Corp., which
partly explains that the first groups of 10 sentences took
longer to annotate. Also, as no specific interface was use to
help annotating, a (very) few typographic errors were made,
such as DET instead of DT.
4For more information on the terminology issue, refer to
the introduction of (Artstein and Poesio, 2008).
Subcorpus pi
1-100 0.955
301-400 0.963
Table 2: Inter-annotator agreement on subcorpora
The results show a very good agreement accord-
ing to all scales (Krippendorff, 1980; Neuendorf,
2002; Krippendorff, 2004) as pi is always superior
to 0.9. Besides, it improves with training (from
0.955 at the beginning to 0.963 at the end).
We also calculated pi on the corpus we used to
evaluate the pre-annotation bias (Experiment 3).
The results of this are shown in table 3.
Subcorpus Nb sent. pi
No pre-annotation 50 0.947
MElt50en 25 0.944
MEltALLen 25 0.983
Table 3: Inter-annotator agreement on subcorpora
used to evaluate bias
Here again, the results are very good, though a
little bit less so than at the beginning of the mixed
annotation session. They are almost perfect with
MEltALLen .
Finally, we calculated pi throughout Experi-
ment 2. The results are given in Figure 2 and,
apart from a bizarre peak at MElt50en, they show a
steady progression of the accuracy and the inter-
annotator agreement, which are correlated. As for
the MElt50en peak, it does not appear in Figure 1, we
therefore interpret it as an artifact.
4.2 Impact of the Pre-annotation Accuracy
on Annotation Time
Before discussing the results of Experiment 2, an-
notation time measurements during Experiment 3
confirm that using a good quality pre-annotation
(say, MEltALLen ) strongly reduces the annotation
time as compared with fully manual annotation.
For example, Annotator1 needed an average time
of approximately 7.5 minutes to annotate 10 sen-
tences without pre-annotation (Experiment 3),
whereas Experiment 2 shows that it goes down to
approximately 2.5 minutes when using MEltALLen
pre-annotation. For Annotator2, the correspond-
ing figures are respectively 11.5 and 2.5 minutes.
Figure 3 shows the impact on the pre-annotation
type on annotation times. Surprisingly, only the
worst tagger (MElt10en) produces pre-annotations
that lead to a significantly slower annotation. In
59
Figure 1: Accuracy of annotation
other words, a 96.4% accurate pre-annotation does
not significantly speed up the annotation process
with respect to a 81.6% accurate pre-annotation.
This is very interesting, since it could mean that
the development of a POS-annotated corpus for a
new language with no POS tagger could be drasti-
cally sped up. Annotating approximately 50 sen-
tences could be sufficient to train a POS tagger
such as MElt and use it as a pre-annotator, even
though its quality is not yet satisfying.
One interpretation of this could be the follow-
ing. Annotation based on pre-annotations involves
two different tasks: reading the pre-annotated sen-
tence and replacing incorrect tags. The reading
task takes a time that does not really depends on
the pre-annotation quality. But the correction task
takes a time that is, say, linear w.r.t. the num-
ber of pre-annotation errors. Therefore, when the
number of pre-annotation errors is below a cer-
tain level, the correction task takes significantly
less time than the reading task. Therefore, be-
low this level, variations in the pre-annotation er-
ror rate do not lead to significant overall annota-
tion time. Apparently, this threshold is between
66.5% and 81.6% pre-annotation accuracy, which
can be reached with a surprisingly small training
corpus.
4.3 Bias Induced by Pre-annotation
We evaluated both the bias induced by a pre-
annotation with the best tagger, MEltALLen , and the
one induced by one of the least accurate taggers,
MElt50en. The results are given in table 4 and 5, re-
spectively.
They show a very different bias according to
the annotator. Annotator2?s accuracy raises from
94.6% to 95.2% with a 81.6% accuracy tagger
(MElt50en) and from 94.1% to 97.1% with a 96.4%
accuracy tagger (MEltALLen ). Therefore, Annota-
tor2, whose accuracy is less than that of Annota-
tor1 under all circumstances (see figure 1), seems
to be positively influenced by pre-annotation,
whether it be good or bad. The gain is however
much more salient with the best pre-annotation
(plus 3 points).
As for Annotator1, who is the most accurate an-
notator (see figure 1), the results are more surpris-
ing as they show a significant degradation of ac-
curacy, from 98.1 without pre-annotation to 95.8
with pre-annotation using MElt50en, the less accu-
rate tagger. Examining the actual results allowed
us to see that, first, Annotator1 non pre-annotated
version is better than the reference, and second,
the errors made in the pre-annotated version with
MElt50en are so obvious that they can only be due to
a lapse in concentration.
The results, however, remain stable with pre-
annotation using the best tagger (from 98.4 to
98.2), which is consistent with the results obtained
by Dandapat et al (2009), who showed that bet-
ter trained annotators are less influenced by pre-
annotation and show stable performance.
When asked about it, both annotators say
they felt they concentrated more without pre-
60
Figure 2: Annotation accuracy and pi depending on the type of pre-annotation
Annotator No pre-annotation with MEltALLen
Annotator1 98.4 98.2
Annotator2 94.1 97.1
Table 4: Accuracy with or without pre-annotation
with MEltALLen (sentences 451-475)
Annotator No pre-annotation with MElt50en
Annotator1 98.1 95.8
Annotator2 94.6 95.2
Table 5: Accuracy with or without pre-annotation
with MElt50en (sentences 476-500)
annotation. It seems that the rather good results
of the taggers cause the attention of the annotators
to be reduced, even more so as the task is repeti-
tive and tedious. However, annotators also had the
feeling that fully manual annotation could be more
subject to oversights.
These impressions are confirmed by the com-
parison of the contingency tables, as can be seen
from Tables 6, 7 and 8 (in these tables, lines cor-
respond to tags from the annotation and columns
to reference tags; only lines containing at least
one cell with 2 errors or more are shown, with
all corresponding columns). For example, Anno-
tator1 makes more random errors when no pre-
annotation is available and more systematic er-
rors when MEltALLen pre-annotations are used (typ-
ically, JJ instead of VBN, i.e., adjective instead of
past participle, which corresponds to a systematic
trend in MEltALLen ?s results).
JJ VBN
JJ 36 4
(Annotator 1)
JJ NN NNP NNPS VB VBN
JJ 36 4
NN 1 68 2
NNP 24 2
(Annotator 2)
Table 6: Excerpts of the contingency tables for
sentences 451?457 (512 tokens) with MEltALLen
pre-annotation
IN JJ NN NNP NNS RB VBD VBN
JJ 30 2 2
NNS 1 2 40
RB 2 16
VBD 1 17 2
WDT 2
(Annotator 1)
JJ NN RB VBN
JJ 28 3
NN 2 75 1
RB 2 16
VBN 2 10
(Annotator 2)
Table 7: Excerpts of the contingency tables for
sentences 476?500 (523 tokens) with MElt50en pre-
annotation
61
Figure 3: Annotation time depending on the type of pre-annotation
CD DT JJ NN NNP NNS
CD 30 2
JJ 2 72
NN 2 148
NNS 3 68
(Annotator 1)
CD DT IN JJ JJR NN NNP NNS RB VBN
IN 104 2
JJ 2 61 2 1 9
NN 1 4 145
NNPS 2
NNS 1 2 68
RBR 2
(Annotator 2)
Table 8: Excerpts of the contingency tables for
sentences 450?500 (1,035 tokens) without pre-
annotation
5 Conclusion and Further Work
The series of experiments we detailed in this arti-
cle confirms that pre-annotation allows for a gain
in quality, both in terms of accuracy w.r.t. a ref-
erence and in terms of inter-annotator agreement,
i.e., reliability. We also demonstrated that this
comes with biases that should be identified and
notified to the annotators, so that they can be extra
careful during correction. Finally, we discovered
that a surprisingly small training corpus could be
sufficient to build a pre-annotation tool that would
help drastically speeding up the annotation.
This should help developing taggers for under-
resourced languages. In order to check that, we
intend to use this method in a near future to de-
velop a POS tagger for Sorani Kurdish.
We also want to experiment on other, more
precision-driven, annotation tasks, like complex
relations annotation or definition segmentation,
that are more intrinsically complex and for which
there exist no automatic tool as accurate as for
POS tagging.
Acknowledgments
This work was partly realized as part of the
Quaero Programme5, funded by OSEO, French
State agency for innovation.
References
Beatrice Alex, Claire Grover, Barry Haddow, Mijail
Kabadjov, Ewan Klein, Michael Matthews, Stu-
art Roebuck, Richard Tobin, and Xinglong Wang.
2008. Assisted Curation: Does Text Mining Really
Help? In Pacific Symposium on Biocomputing.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555?596.
Lucie Barque, Alexis Nasr, and Alain Polgue`re. 2010.
From the Definitions of the Tre?sor de la Langue
Franc?aise to a Semantic Database of the French Lan-
guage. In Proceedings of the 14th EURALEX Inter-
national Congress, Leeuwarden.
Jean Carletta. 1996. Assessing Agreement on Classi-
fication Tasks: the Kappa Statistic. Computational
Linguistics, 22:249?254.
5http://quaero.org/
62
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex Linguistic
Annotation - No Easy Way Out! a Case from Bangla
and Hindi POS Labeling Tasks. In Proceedings of
the third ACL Linguistic Annotation Workshop.
Pascal Denis and Beno??t Sagot. 2009. Coupling an An-
notated Corpus and a Morphosyntactic Lexicon for
State-of-the-art POS Tagging with Less Human Ef-
fort. In Proceedings of PACLIC 2009, Hong-Kong,
China.
Kare?n Fort, Maud Ehrmann, and Adeline Nazarenko.
2009. Vers une me?thodologie d?annotation des
entite?s nomme?es en corpus ? In Actes de la
16e`me Confe?rence sur le Traitement Automatique
des Langues Naturelles 2009 Traitement Automa-
tique des Langues Naturelles 2009, Senlis, France.
Klaus Krippendorff, 1980. Content Analysis: An Intro-
duction to Its Methodology, chapter 12. Sage, Bev-
erly Hills, CA.
Klaus Krippendorff, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Marie Mikulova? and Jan S?te?pa?nek. 2009. Annotation
Quality Checking and its Implications for Design
of Treebank (in Building the Prague Czech-English
Dependency Treebank). In Proceedings of the Eight
International Workshop on Treebanks and Linguistic
Theories, volume 4-5, Milan, Italy, December.
Kimberly Neuendorf. 2002. The Content Analysis
Guidebook. Sage, Thousand Oaks CA.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Proceedings
of International Conference on Empirical Methods
in Natural Language Processing, pages 133?142.
Ines Rehbein, Josef Ruppenhofer, and Caroline
Sporleder. 2009. Assessing the Benefits of Partial
Automatic Pre-labeling for Frame-semantic Anno-
tation. In Proceedings of the Third Linguistic Anno-
tation Workshop, pages 19?26, Suntec, Singapore,
August. Association for Computational Linguistics.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised
Training for the Averaged Perceptron POS Tagger.
In EACL ?09: Proceedings of the 12th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 763?771, Morristown,
NJ, USA.
63
Proceedings of the Fifth Law Workshop (LAW V), pages 92?100,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Proposal for an Extension of Traditional Named Entities:
From Guidelines to Evaluation, an Overview
Cyril Grouin?, Sophie Rosset?, Pierre Zweigenbaum?
Kar?n Fort?,? , Olivier Galibert?, Ludovic Quintard?
?LIMSI?CNRS, France ?INIST?CNRS, France ?LIPN, France ?LNE, France
{cyril.grouin,sophie.rosset,pierre.zweigenbaum}@limsi.fr
karen.fort@inist.fr, {olivier.galibert,ludovic.quintard}@lne.fr
Abstract
Within the framework of the construction of a
fact database, we defined guidelines to extract
named entities, using a taxonomy based on an
extension of the usual named entities defini-
tion. We thus defined new types of entities
with broader coverage including substantive-
based expressions. These extended named en-
tities are hierarchical (with types and compo-
nents) and compositional (with recursive type
inclusion and metonymy annotation). Human
annotators used these guidelines to annotate a
1.3M word broadcast news corpus in French.
This article presents the definition and novelty
of extended named entity annotation guide-
lines, the human annotation of a global corpus
and of a mini reference corpus, and the evalu-
ation of annotations through the computation
of inter-annotator agreements. Finally, we dis-
cuss our approach and the computed results,
and outline further work.
1 Introduction
Within the framework of the Quaero project?a mul-
timedia indexing project?we organized an evalu-
ation campaign on named entity extraction aiming
at building a fact database in the news domain, the
first step being to define what kind of entities are
needed. This campaign focused on broadcast news
corpora in French. While traditional named enti-
ties include three major classes (persons, locations
and organizations), we decided to extend the cov-
erage of our campaign to new types of entities and
to broaden their main parts-of-speech from proper
names to substantives, this extension being neces-
sary for ever-increasing knowledge extraction from
documents. We thus produced guidelines to specify
the way corpora had to be annotated, and launched
the annotation process.
In this paper, after covering related work (Sec-
tion 2), we describe the taxonomy we created (Sec-
tion 3) and the annotation process and results (Sec-
tion 4), including the corpora we gathered and the
tools we developed to facilitate annotation. We then
present inter-annotator agreement measures (Sec-
tion 5), outline limitations (Section 6) and conclude
on perspectives for further work (Section 7).
2 Related work
2.1 Named entity definitions
Named Entity recognition was first defined as recog-
nizing proper names (Coates-Stephens, 1992). Since
MUC-6 (Grishman and Sundheim, 1996; SAIC,
1998), named entities have been proper names
falling into three major classes: persons, locations
and organizations.
Proposals were made to sub-divide these entities
into finer-grained classes. The ?politicians? sub-
class was proposed for the ?person? class by (Fleis-
chman and Hovy, 2002) while the ?cities? subclass
was added to the ?location? class by (Fleischman,
2001; Lee and Lee, 2005).
The CONLL conference added a miscellaneous
type that includes proper names falling outside the
previous classes. Some classes have thus sometimes
been added, e.g. the ?product? class by (Bick, 2004;
Galliano et al, 2009).
92
Specific entities are proposed and handled in
some tasks: ?language? or ?shape? for question-
answering systems in specific domains (Rosset et
al., 2007), ?email address? or ?phone number? to
process electronic messages (Maynard et al, 2001).
Numeric types are also often described and used.
They include ?date?, ?time?, and ?amount? types
(?amount? generally covers money and percentage).
In specific domains, entities such as gene, protein,
are also handled (Ohta, 2002), and campaigns are or-
ganized for gene detection (Yeh et al, 2005). At the
same time, extensions of named entities have been
proposed: (Sekine, 2004) defined a complete hierar-
chy of named entities containing about 200 types.
2.2 Named Entities and Annotation
As for any other kind of annotation, some aspects are
known to lead to difficulties in obtaining coherence
in the manual annotation process (Ehrmann, 2008;
Fort et al, 2009). Three different classes of prob-
lems are distinguished: (1) selecting the correct cat-
egory in cases of ambiguity, where one entity can
fall into several classes, depending on the context
(?Paris? can be a town or a person name); (2) detect-
ing the boundaries (in a person designation, is only
the proper name to be annotated or the trigger ?Mr?
too?) and (3) annotating metonymies (?France? can
be a sports team, a country, etc.).
In the ACE Named Entity task (Doddington et al,
2004), a complex task, the obtained inter-annotator
agreement was 0.86 in 2002 and 0.88 in 2003. Some
tasks obtain better agreement. Desmet and Hoste
(2010) described the Named Entity annotation real-
ized within the Sonar project, where Named Entity
are clearly simpler. They follow the MUC Named
Entity definition with the subtypes as proposed
by ACE. The agreement computed over the Sonar
Dutch corpus ranges from 0.91 to 0.97 (kappa val-
ues) depending of the emphasized elements (span,
main type, subtype, etc.).
3 Taxonomy
3.1 Guidelines production
Having in mind the objective of building a fact
database through the extraction of named entities
from texts, we defined a richer taxonomy than those
used in other information extraction works.
Following (Bonneau-Maynard et al, 2005; Alex
et al, 2010), the annotation guidelines were first
written from December 2009 to May 2010 by three
researchers managing the manual annotation cam-
paign. During guidelines production, we evaluated
the feasibility of this specific annotation task and the
usefulness of the guidelines by annotating a small
part of the target corpus. Then, these guidelines
were delivered to the annotators. They consist of a
description of the objects to annotate, general anno-
tation rules and principles, and more than 250 pro-
totypical and real examples extracted from the cor-
pus (Rosset et al, 2010). Rules are important to set
the general way annotations must be produced. Ad-
ditionally, examples are essential for human annota-
tors to grasp the annotation rationale more easily.
Indeed, while producing the guidelines, we knew
that the given examples would never cover all possi-
ble cases because of the specificity of language and
of the ambiguity of formulations and situations de-
scribed in corpora, as shown in (Fort et al, 2009).
Nevertheless, guidelines examples must be consid-
ered as a way to understand the final objective of
the annotation work. Thanks to numerous meetings
from May to November 2010, we gathered feedback
from the annotators (four annotators plus one anno-
tation manager). This feedback allowed us to clarify
and extend the guidelines in several directions. The
guidelines are 72 pages long and consist of 3 major
parts: general description of the task and the prin-
ciples (25% of the overall document), presentation
of each type of named entity (57%), and a simpler
?cheat sheet? (18%).
3.2 Definition
We decided to use the three general types of
named entities: name (person, location, organi-
zation) as described in (Grishman and Sundheim,
1996; SAIC, 1998), time (date and duration), and
quantity (amount). We then included named entities
extensions proposed by (Sekine, 2004; Galliano et
al., 2009) (respectively products and functions) and
we extended the definition of named entities to ex-
pressions which are not composed of proper names
(e.g., phrases built around substantives). The ex-
tended named entities we defined are both hierar-
chical and compositional. For example, type pers
(person) is split into two subtypes, pers.ind (indi-
93
Person Function
pers.ind (individual
person)
pers.coll (group of
persons)
func.ind (individual
function)
func.coll (collectivity
of functions)
Location Product
administrative
(loc.adm.town,
loc.adm.reg,
loc.adm.nat,
loc.adm.sup)
physical
(loc.phys.geo,
loc.phys.hydro,
loc.phys.astro)
facilities
(loc.fac),
oronyms
(loc.oro),
address
(loc.add.phys,
loc.add.elec)
prod.object
(manufac-
tured object)
prod.serv
(transporta-
tion route)
prod.fin
(financial
products)
prod.doctr
(doctrine)
prod.rule
(law)
prod.soft
(software)
prod.art prod.media prod.award
Organization Time
org.adm (administra-
tion)
org.ent (services)
Amount
amount (with unit or general object), includ-
ing duration
time.date.abs
(absolute date),
time.date.rel (relative
date)
time.hour.abs
(absolute hour),
time.hour.rel (relative
hour)
Table 1: Types (in bold) and subtypes (in italic)
vidual person) and pers.coll (collective person), and
pers entities are composed of several components,
among which are name.first and name.last.
3.3 Hierarchy
We used two kinds of elements: types and compo-
nents. The types with their subtypes categorize a
named entity. While types and subtypes were used
before (ACE, 2000; Sekine, 2004; ACE, 2005; Gal-
liano et al, 2009), we consider that structuring the
contents of an entity (its components) is important
too. Components categorize the elements inside a
named entity.
Our taxonomy is composed of 7 main types
(person, function, location, product, organization,
amount and time) and 32 subtypes (Table 1). Types
and subtypes refer to the general category of a
named entity. They give general information about
the annotated expression. Almost each type is then
specified using subtypes that either mark an opposi-
tion between two major subtypes (individual person
vs. collective person), or add precisions (for exam-
ple for locations: administrative location, physical
location, etc.).
This two-level representation of named entities,
with types and components, constitutes a novel ap-
proach.
Types and subtypes To deal with the intrinsic am-
biguity of named entities, we defined two specific
transverse subtypes: 1. other for entities with a dif-
ferent subtype than those proposed in the taxon-
omy (for example, prod.other for games), and 2. un-
known when the annotator does not know which sub-
type to use.
Types and subtypes constitute the first level of an-
notation. They refer to a general segmentation of
the world into major categories. Within these cate-
gories, we defined a second level of annotation we
call components.
Components Components can be considered as
clues that help the annotator to produce an anno-
tation: either to determine the named entity type
(e.g. a first name is a clue for the pers.ind named
entity subtype), or to set the named entity bound-
aries (e.g. a given token is a clue for the named en-
tity, and is within its scope, while the next token is
not a clue and is outside its scope). Components are
second-level elements, and can never be used out-
side the scope of a type or subtype element. An en-
tity is thus composed of components that are of two
kinds: transverse components and specific compo-
nents (Table 2). Transverse components can be used
in several types of entities, whereas specific compo-
nents can only be used in one type of entity.
94
Transverse components
name (name of the entity), kind (hyperonym of the entity), qualifier (qualifying adjective), demonym
(inhabitant or ethnic group name), demonym.nickname (inhabitant or ethnic group nickname), val
(a number), unit (a unit), extractor (an element in a series), range-mark (range between two values),
time-modifier (a time modifier).
pers.ind loc.add.phys time.date.abs/rel amount
name.last, name.first,
name.middle, pseudonym,
name.nickname, title
address-number, po-box,
zip-code,
other-address-component
week, day, month, year,
century, millennium,
reference-era
object
prod.award
award-cat
Table 2: Transverse and specific components
3.4 Composition
Another original point in this work is the compo-
sitional nature of the annotations. Entities can be
compositional for three reasons: (i) a type contains a
component; (ii) a type includes another type, used as
a component; and (iii) in cases of metonymy. Dur-
ing the Ester II evaluation campaign, there was an
attempt to use compositionality in named entities for
two categories: persons and functions, where a per-
son entity could contain a function entity.
<pers.hum> <func.pol> pr?sident </func.pol>
<pers.hum> Chirac </pers.hum> </pers.hum>
Nevertheless, the Ester II evaluation did not take
this inclusion into account and only focused on
the encompassing annotation (<pers.hum> pr?sident
Chirac </pers.hum>). We drew our inspiration from
this experience, and allowed the annotators and the
systems to use compositionality in the annotations.
Cases of inclusion can be found in the function
type (Figure 1), where type func.ind, which spans
the whole expression, includes type org.adm, which
spans the single word ?budget?. In this case, we con-
sider that the designation of this function (?ministre
du budget?) includes both the kind (?ministre?) and
nouveau
qualifier
ministre
kind
du Budget
name
org.adm
func.ind
, Fran?ois
name.first
Baroin
name.last
pers.ind
Figure 1: Multi-level annotation of entity types (red tags)
and components (blue tags): new minister of budget ,
Fran?ois Baroin.
the name (?budget?) of the ministry, which itself is
typed as is relevant (org.adm). Recursive cases of
embedding can be found when a subtype includes
another named entity annotated with the same sub-
type (org.ent in Figure 2).
le collectif
kind
des associations
kind
des droits de l' Homme
name
prod.rule
au Sahara
name
loc.phys.geo
loc.adm.sup
org.ent
org.ent
Figure 2: Recursive embedding of the same subtype:
Collective of the Human Rights Organizations in Sahara.
Cases of metonymy include strict metonymy (a
term is substituted with another one in a relation
of contiguity) and antonomasia (a proper name is
used as a substantive or vice versa). In such cases,
the entity must be annotated with both types, first
(inside) with the intrinsic type of the entity, then
(outside) with the type that corresponds to the re-
sult of the metonymy. Basically, country names
correspond to ?national administrative? locations
(loc.adm.nat) but they can also designate the admin-
istration (org.adm) of the country (Figure 3).
depuis
time-modifier
plusieurs
val
mois
unit
amount
time.date.rel
, la Russie
name
loc.adm.nat
org.adm
Figure 3: Annotation with a metonymic use of country
?Russia? as its government: for several months , Russia...
95
3.5 Boundaries
Our definition of the scope of entities excludes rel-
ative clauses, subordinate clauses, and interpolated
clauses: the annotation of an entity must end before
these clauses. If an interpolated clause occurs inside
an entity, its annotation must be split. Moreover, two
distinct persons sharing the same last name must be
annotated as two separate entities (Figure 4); we in-
tend to use relations between entities to gather these
segments in the next step of the project.
depuis
utmi-oefrl
vifr-eua
il n.s,etui
utmi-oefrl
Rprveu
utmi-strl
vifr-eua
Figure 4: Separate (coordinated) named entities.
4 Annotation process
4.1 Corpus
We managed the annotation of a corpus of about one
hundred hours of transcribed speech from several
French-speaking radio stations in France and Mo-
rocco. Both news and entertainment shows were
transcribed, including dialogs, with speaker turns.1
Once annotated, the corpus was split into a de-
velopment corpus: one file from a French radio sta-
tion;2 a training corpus: 188 files from five French
stations3 and one Moroccan station;4 and a test cor-
pus: 18 files from two French stations already stud-
ied in the training corpus5 and from unseen sources,
both radio6 and television,7 in order to evaluate the
robustness of systems. These data have been used in
the 2011 Quaero named entity evaluation campaign.
1Potential named entities may be split across several seg-
ments or turns.
2News from France Culture.
3News from France Culture (refined language), France Info
(news with short news headlines), France Inter (generalist radio
station), Radio Classique (classical music and economic news),
RFI (international radio broadcast out of France).
4News from RTM (generalist French speaking radio).
5News from France Culture, news and entertainment from
France Inter.
6A popular entertainment show from Europe 1.
7News from Arte (public channel with art and culture),
France 2 (public generalist channel), and TF1 (private gener-
alist popular channel).
This corpus allows us to perform different evalua-
tions, depending of the knowledge the systems have
of the source (source seen in the training corpus vs.
unseen source), the kind of show (news vs. enter-
tainment), the language style (popular vs. refined),
and the type of media (radio vs. television).
4.2 Tools for annotators
To perform our test annotations (see Section 2.2),
we developed a very simple annotation tool as an in-
terface based on XEmacs. We provided the human
annotators with this tool and they decided to use it
for the campaign, despite the fact that it is very sim-
ple and that we told them about other, more generic,
annotation tools such as GATE8 or Glozz.9 This is
probably due to the fact that apart from being very
simple to install and use, it has interesting features.
The first feature is the insertion of annotations
using combinations of keyboard shortcuts based on
the initial of each type, subtype and component
name. For example, combination F2 key + initial
keys is used to annotate a subtype (pers.ind, etc.),
F3 + keys for a transverse component (name, kind,
etc.), F4 + keys for a specific component (name.first,
etc.), and F5 to delete the annotation selected with
the cursor (both opening and closing tags).
The second feature is boundary management: if
the annotator puts the cursor over the token to anno-
tate, the annotation tool will handle the boundaries
of this token; opening and closing tags will be in-
serted around the token.
However, it presents some limitations: tags are
inserted in the text (which makes visualization more
complex, especially for long sentences or in cases
of multiple annotations on the same entity), no per-
sonalization is offered (tags are of only one color),
and there is no function to express annotator uncer-
tainty (the user must choose among several possible
tags the one that fits the best;10 while producing the
guidelines, we did not consider it could be of inter-
est: as a consequence, no uncertainty management
was implemented). Therefore, this tool allows users
to insert tags rapidly into a text, but it offers no exter-
nal resources, as real annotation tools (e.g. GATE)
often do.
8http://gate.ac.uk/
9http://www.glozz.org/
10Uncertainty can be found in cases of lack of context.
96
These simplistic characteristics combined with a
fast learning curve allow the annotators to rapidly
annotate the corpora. Annotators were allowed not
to annotate the transverse component name (only if
it was the only component in the annotated phrase,
e.g. ?Russia? in Figure 3, blue tag) and to annotate
events, even though we do not focus on this type
of entity as of yet. We therefore also provided a
normalization tool which adds the transverse com-
ponent name in these instances, and which removes
event annotations.
4.3 Corpus annotation
Global annotation It took four human annotators
two months and a half to annotate the entire corpus
(10 man-month). These annotators were hired grad-
uate students (MS in linguistics). The overall corpus
was annotated in duplicate. Regular comparisons of
annotations were performed and allowed the anno-
tators to develop a methodology, which was subse-
quently used to annotate the remaining documents.
Mini reference corpus To evaluate the global an-
notation, we built a mini reference corpus by ran-
domly selecting 400 sentences from the training cor-
pus and distributing them into four files. These files
were annotated by four graduate human annotators
from two research institutes (Figure 5) with two hu-
mans per institute, in about 10 hours per annotator.
	








Figure 5: Creation of mini reference corpus and compu-
tation of inter-annotator agreement. Institute 1 = LIMSI?
CNRS, Institute 2 = INIST?CNRS
First, we merged the annotations of each file
within a given institute (1.5h per pair of annotators),
then merged the results across the two institutes
(2h). Finally, we merged the results with the anno-
tations of the hired annotators (8h). We thus spent
about 90 hours to annotate and merge annotations in
this mini reference corpus (0.75 man-month).
4.4 Annotation results
Our broadcast news corpus includes 1,291,225
tokens, among which there are 954,049 non-
punctuation tokens. Its annotation contains 113,885
named entities and 146,405 components (Table 3),
i.e. one entity per 8.4 non-punctuation tokens, and
one component per 6.5 non-punctuation tokens.
There is an average of 6 annotations per line.
PPPPPPPPInf.
Data
Training Test
# shows 188 18
# lines 43,289 5,637
# words 1,291,225 108,010
# entity types 113,885 5,523
# distinct types 41 32
# components 146,405 8,902
# distinct comp. 29 22
Table 3: Statistics on annotated corpora.
5 Inter-Annotator Agreement
5.1 Procedure
During the annotation campaign, we measured sev-
eral criteria on a regular basis: inter-annotator agree-
ment and disagreement. We used them to correct er-
roneous annotations, and mapped these corrections
to the original annotations. We also used these mea-
sures to give the annotators feedback on the en-
countered problems, discrepancies, and residual er-
rors. Whereas we performed these measurements all
along the annotation campaign, this paper focuses
on the final evaluation on the mini reference corpus.
5.2 Metrics
Because human annotation is an interpretation pro-
cess (Leech, 1997), there is no ?truth? to rely on. It
is therefore impossible to really evaluate the validity
of an annotation. All we can and should do is to eval-
uate its reliability, i.e. the consistency of the anno-
tation across annotators, which is achieved through
computation of the inter-annotator agreement (IAA).
97
The best way to compute it is to use one of
the Kappa family coefficients, namely Cohen?s
Kappa (Cohen, 1960) or Scott?s Pi (Scott, 1955),
also known as Carletta?s Kappa (Carletta, 1996),11
as they take chance into account (Artstein and Poe-
sio, 2008). However, these coefficients imply a
comparison with a ?random baseline? to establish
whether the correlation between annotations is sta-
tistically significant. This baseline depends on the
number of ?markables?, i.e. all the units that could
be annotated.
In the case of named entities, as in many others,
this ?random baseline? is known to be difficult?if
not impossible?to identify (Alex et al, 2010). We
wish to analyze this in more detail, to see how we
could actually compute these coefficients and what
information it would give us about the annotation.
Markables Annotators Both institutes
F = 0.84522 F = 0.91123
U1: n-grams
? = 0.84522 ? = 0.91123
pi = 0.81687 pi = 0.90258
U2: n-grams ? 6
? = 0.84519 ? = 0.91121
pi = 0.81685 pi = 0.90257
U3: NPs
? = 0.84458 ? = 0.91084
pi = 0.81628 pi = 0.90219
U4: Ester entities
? = 0.71300 ? = 0.82607
pi = 0.71210 pi = 0.82598
U5: Pooling
? = 0.71300 ? = 0.82607
pi = 0.71210 pi = 0.82598
Table 4: Inter-Annotator Agreements (? stands for Co-
hen?s Kappa, pi for Scott?s Pi, and F for F-measure). IAA
values were computed by taking as the reference the hired
annotators? annotation or that obtained by merging from
both institutes (see Figure 5).
In the present case, we could consider that, poten-
tially, all the noun phrases can be annotated (row U3
in Table 4, based on the PASSAGE campaign (Vil-
nat et al, 2010)). Of course, this is a wrong approx-
imation as named entities are not necessarily noun
phrases (e.g., ?? partir de l?automne prochain?, from
next autumn).
We could also consider all n-grams of tokens in
the corpus (row U1). However, it would be more
11For more details on terminology issues, we refer to the in-
troduction of (Artstein and Poesio, 2008).
relevant to limit their size. For a maximum size of
six, we get the results shown in row U2. All this, of
course, is artificial, as the named entity annotation
process is not random.
To obtain results that are closer to reality, we
could use numbers of named entities from previous
named entity annotation campaigns (row U4 based
on the Ester II campaign (Galliano et al, 2009)), but
as we consider here a largely extended version of
those, the results would again be far from reality.
Another solution is to consider as ?markables? all
the units annotated by at least one of the annotators
(row U5). In this particular case, units not annotated
by any of the annotators (i.e. silence) are overlooked.
The lowest IAA will be the one computed with
this last solution, while the highest IAA will be
equal to the F-measure (i.e. the measure computed
with all the markables as shown in row U1 in Ta-
ble 4). We notice that the first two solutions (U1
and U2 with n-grams) are not acceptable because
they are far from reality; even extended named en-
tities are sparse annotations, and just considering
all tokens as ?markables? is not suitable. The last
three ones seem to be more relevant because they
are based on an observed segmentation on similar
data. Still, the U3 solution (NPs) overrates the num-
ber of markables because not all noun phrases are
extended named entities. Although the U4 solution
(Ester entities) is based on the same corpus used for
a related task, it underrates the number of markables
because that task produced 16.3 times less annota-
tions. Finally the U5 solution (pooling) gives the
lower bound for the ? estimation which is an in-
teresting information but may easily undervalue the
quality of the annotation.
As (Hripcsak and Rothschild, 2005) showed, in
our case ? tends towards the F-measure when the
number of negative cases tends towards infinity. Our
results show that it is hard to build a justifiable hy-
pothesis on the number of markables which is larger
than the number of actually annotated entities while
keeping ? significantly under the F-measure. But
building no hypothesis leads to underestimating the
? value.
This reinforces the idea of using the F-measure
as the main inter-annotator agreement measure for
named entity annotation tasks.
98
6 Limitations
We used syntax to define some components (e.g. a
qualifier is an adjective) and to set the scope of en-
tities (e.g. stop at relative clauses). Nevertheless,
this syntactic definition cannot fit all named enti-
ties, which are mainly defined according to seman-
tics: the phrase ?dans les mois qui viennent? (?in
the coming months?) expresses an entity of type
time.date.rel where the relative clause ?qui vien-
nent? is part of the entity and contributes the time-
modifier component.
The distinction between some types of entities
may be fuzzy, especially for the organizations (is
the Social Security an administrative organization or
a company?) and for context-dependent annotations
(is lemonde.fr a URL, a media, or a company?). As a
consequence, some entity types might be converted
into specific components in a future revision, e.g. the
func type could become a component of the pers
type, where it would become a description of the
function itself instead of the person who performs
this function (Figure 6).
depuisptm
-its
oftrlits
vaienr
tn.pl,num
Rpeulits
depuisptm
oftr
vaienr
tn.pl,num
Rpeulits
Figure 6: Possible revision: current annotation (left),
transformation of func from entity to component (right).
7 Conclusion and perspectives
In this paper, we presented an extension of the tra-
ditional named entity categories to new types (func-
tions, civilizations) and new coverage (expressions
built over a substantive). We created guidelines
that were used by graduate annotators to annotate
a broadcast news corpus.
The organizers also annotated a small part of the
corpus to build a mini reference corpus. We evalu-
ated the human annotations with our mini-reference
corpus: the actual computed ? is between 0.71 et
0.85 which, given the complexity of the task, seems
to indicate a good annotation quality. Our results are
consistent with other studies (Dandapat et al, 2009)
in demonstrating that human annotators? training is
a key asset to produce quality annotations.
We also saw that guidelines are never fixed, but
evolve all along the annotation process due to feed-
back between annotators and organizers; the rela-
tionship between guidelines producers and human
annotators evolved from ?parent? to ?peer? (Akrich
and Boullier, 1991). This evolution was observed
during the annotation development, beyond our ex-
pectations. These data have been used for the 2011
Quaero Named Entity evaluation campaign.
Extensions and revisions are planned. Our first
goal is to add a new type of named entity for all
kinds of events; guidelines are being written and hu-
man annotation tests are ongoing. We noticed that
some subtypes are more difficult to disambiguate
than others, especially org.adm and org.ent (defi-
nition and examples in the guidelines are not clear
enough). We shall make decisions about this kind
of ambiguity, either by merging these subtypes or by
reorganizing the distinctions within the organization
type. We also plan to link the annotated entities us-
ing relations; further work is needed to define more
precisely the way we will perform these annotations.
Moreover, the taxonomy we defined was applied to
a broadcast news corpus, but we intend to use it in
other corpora. The annotation of an old press corpus
was performed according to the same process. Its
evaluation will start in the coming months.
Acknowledgments
We thank all the annotators who did such a great
work on this project, as well as Sabine Barreaux
(INIST?CNRS) for her work on the reference cor-
pus.
This work was partly realized as part of the
Quaero Programme, funded by Oseo, French State
agency for innovation and by the French ANR Etape
project.
References
ACE. 2000. Entity detection and tracking,
phase 1, ACE pilot study. Task definition.
http://www.nist.gov/speech/tests/ace/phase1/doc/summary-
v01.htm.
ACE. 2005. ACE (Automatic Con-
tent Extraction) English annotation guide-
lines for entities version 5.6.1 2005.05.23.
http://www.ldc.upenn.edu/Projects/ACE/docs/English-
Entities-Guidelines_v5.6.1.pdf.
99
Madeleine Akrich and Dominique Boullier. 1991. Le
mode d?emploi, gen?se, forme et usage. In Denis
Chevallier, editor, Savoir faire et pouvoir transmettre,
pages 113?131. ?d. de la MSH (collection Ethnologie
de la France, Cahier 6).
Beatrice Alex, Claire Grover, Rongzhou Shen, and Mijail
Kabadjov. 2010. Agile Corpus Annotation in Prac-
tice: An Overview of Manual and Automatic Annota-
tion of CVs. In Proc. of the Fourth Linguistic Annota-
tion Workshop, pages 29?37, Uppsala, Sweden. ACL.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4):555?596.
Eckhard Bick. 2004. A named entity recognizer for dan-
ish. In LREC?04.
H?l?ne Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Seman-
tic Annotation of the French Media Dialog Corpus. In
InterSpeech, Lisbon.
Jean Carletta. 1996. Assessing Agreement on Classifi-
cation Tasks: the Kappa Statistic. Computational Lin-
guistics, 22:249?254.
Sam Coates-Stephens. 1992. The analysis and acquisi-
tion of proper names for the understanding of free text.
Computers and the Humanities, 26:441?456.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37?46.
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex Linguistic An-
notation - No Easy Way Out! A Case from Bangla
and Hindi POS Labeling Tasks. In Proc. of the Third
Linguistic Annotation Workshop, Singapour. ACL.
Bart Desmet and V?ronique Hoste. 2010. Towards a
balanced named entity corpus for dutch. In LREC.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ACE) program tasks, data, and evaluation. In Proc. of
LREC.
Maud Ehrmann. 2008. Les entit?s nomm?es, de la lin-
guistique au TAL : statut th?orique et m?thodes de
d?sambigu?sation. Ph.D. thesis, Univ. Paris 7 Diderot.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proc. of
COLING, volume 1, pages 1?7. ACL.
Michael Fleischman. 2001. Automated subcategoriza-
tion of named entities. In Proc. of the ACL 2001 Stu-
dent Research Workshop, pages 25?30.
Kar?n Fort, Maud Ehrmann, and Adeline Nazarenko.
2009. Towards a Methodology for Named Entities An-
notation. In Proceeding of the 3rd ACL Linguistic An-
notation Workshop (LAW III), Singapore.
Sylvain Galliano, Guillaume Gravier, and Laura
Chaubard. 2009. The ESTER 2 evaluation campaign
for the rich transcription of French radio broadcasts.
In Proc of Interspeech 2009.
Ralph Grishman and Beth Sundheim. 1996. Message
Understanding Conference - 6: A brief history. In
Proc. of COLING, pages 466?471.
George Hripcsak and Adam S. Rothschild. 2005. Tech-
nical brief: Agreement, the f-measure, and reliability
in information retrieval. JAMIA, 12(3):296?298.
Seungwoo Lee and Gary Geunbae Lee. 2005. Heuris-
tic methods for reducing errors of geographic named
entities learned by bootstrapping. In IJCNLP, pages
658?669.
Geoffrey Leech. 1997. Introducing corpus annotation.
In Geoffrey Leech Roger Garside and Tony McEnery,
editors, Corpus annotation: Linguistic information
from computer text corpora, pages 1?18. Longman,
London.
Diana Maynard, Valentin Tablan, Cristian Ursu, Hamish
Cunningham, and Yorick Wilks. 2001. Named en-
tity recognition from diverse text types. In Recent Ad-
vances in NLP 2001 Conference, Tzigov Chark.
Tomoko Ohta. 2002. The genia corpus: An annotated
research abstract corpus in molecular biology domain.
In Proc. of HLTC, pages 73?77.
Sophie Rosset, Olivier Galibert, Gilles Adda, and Eric
Bilinski. 2007. The LIMSI participation to the QAst
track. In Working Notes for the CLEF 2007 Workshop,
Budapest, Hungary.
Sophie Rosset, Cyril Grouin, and Pierre Zweigenbaum.
2010. Entit?s nomm?es : guide d?annotation Quaero,
November. T3.2, presse ?crite et orale.
SAIC. 1998. Proceedings of the seventh message under-
standing conference (MUC-7).
William A Scott. 1955. Reliability of Content Analysis:
The Case of Nominal Scale Coding. Public Opinion
Quaterly, 19(3):321?325.
Satoshi Sekine. 2004. Definition, dictionaries and tagger
of extended named entity hierarchy. In Proc. of LREC.
Anne Vilnat, Patrick Paroubek, Eric Villemonte de la
Clergerie, Gil Francopoulo, and Marie-Laure Gu?not.
2010. Passage syntactic representation: a minimal
common ground for evaluation. In Proc. of LREC.
Alex Yeh, Alex Morgan, Marc Colosimo, and Lynette
Hirschman. 2005. BioCreAtIvE task 1A: gene men-
tion finding evaluation. BMC Bioinformatics, 6(1).
100
Proceedings of BioNLP Shared Task 2011 Workshop, pages 65?73,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP Shared Task 2011 ? Bacteria Gene Interactions and Renaming
Julien Jourde1, Alain-Pierre Manine2, Philippe Veber1, Kare?n Fort3, Robert Bossy1,
Erick Alphonse2, Philippe Bessie`res1
1Mathe?matique, Informatique et 2PredictiveDB 3LIPN ? Universite? Paris-Nord/
Ge?nome ? Institut National de la 16, rue Alexandre Parodi CNRS UMR7030 and
Recherche Agronomique F75010 Paris, France INIST CNRS UPS76 ? F54514
MIG INRA UR1077 {apmanine,alphonse} Vand?uvre-le`s-Nancy, France
F78352 Jouy-en-Josas, France @predictivedb.com karen.fort@inist.fr
forename.lastname@jouy.inra.fr
Abstract
We present two related tasks of the BioNLP
Shared Tasks 2011: Bacteria Gene Renam-
ing (Rename) and Bacteria Gene Interactions
(GI). We detail the objectives, the corpus spec-
ification, the evaluation metrics, and we sum-
marize the participants? results. Both issued
from PubMed scientific literature abstracts,
the Rename task aims at extracting gene name
synonyms, and the GI task aims at extracting
genic interaction events, mainly about gene
transcriptional regulations in bacteria.
1 Introduction
The extraction of biological events from scientific
literature is the most popular task in Information Ex-
traction (IE) challenges applied to molecular biol-
ogy, such as in LLL (Ne?dellec, 2005), BioCreative
Protein-Protein Interaction Task (Krallinger et al,
2008), or BioNLP (Demner-Fushman et al, 2008).
Since the BioNLP 2009 shared task (Kim et al,
2009), this field has evolved from the extraction of a
unique binary interaction relation between proteins
and/or genes towards a broader acceptation of bio-
logical events including localization and transforma-
tion (Kim et al, 2008). In the same way, the tasks
Bacteria Gene Interactions and Bacteria Gene Re-
naming deal with the extraction of various molecu-
lar events capturing the mechanisms relevant to gene
regulation in prokaryotes. The study of bacteria has
numerous applications for health, food and indus-
try, and overall, they are considered as organisms
of choice for the recent integrative approaches in
systems biology, because of their relative simplicity.
Compared to eukaryotes, they allow easier and more
in-depth analysis of biological functions and of their
related molecular mechanisms.
Processing literature on bacteria raises linguis-
tic and semantic specificities that impact text anal-
ysis. First of all, gene renaming is a frequent phe-
nomenon, especially for model bacteria. Hence, the
abundance of gene synonyms that are not morpho-
logical variants is high compared to eukaryotes. The
history of bacterial gene naming has led to drastic
amounts of homonyms and synonyms which are of-
ten missing (or worse, erroneous) in gene databases.
In particular, they often omit old gene names that
are no longer used in new publications, but that are
critical for exhaustive bibliography search. Poly-
semy makes the situation even worse, as old names
frequently happen to be reused to denote different
genes. A correct and complete gene synonym table
is crucial to biology studies, for instance when inte-
grating large scale experimental data using distinct
nomenclatures. Indeed this information can save a
lot of bibliographic research time. The Rename Task
is a new task in text-mining for biology that aims at
extracting explicit mentions of renaming relations.
It is a critical step in gene name normalization that
is needed for further extraction of biological events
such as genic interactions.
Regarding stylistics, gene and protein interactions
are not formulated in the same way for eukary-
otes and prokaryotes. Descriptions of interactions
and regulations in bacteria include more knowledge
about their molecular actors and mechanisms, com-
pared to the literature on eukaryotes. Typically in
bacteria literature, the genic regulations are more
65
likely expressed by direct binding of the protein,
while in eukaryote literature, non-genic agents re-
lated to environmental conditions are much more
frequent. The bacteria GI Task is based on (Manine
et al, 2010) which is a semantic re-annotation of the
LLL challenge corpus (Ne?dellec, 2005), where the
description of the GI events in a fine-grained rep-
resentation includes the distinction between expres-
sion, transcription and other action events, as well as
different transcription controls (e.g. regulon mem-
bership, promoter binding). The entities are not only
protein agent and gene target but extend to families,
complexes and DNA sites (binding sites, promoters)
in order to better capture the complexity of the reg-
ulation at a molecular level. The task consists in re-
lating the entities with the relevant relations.
2 Rename Task Description
The goal of the Rename task is illustrated by Figure
1. It consists in predicting renaming relations be-
tween text-bound gene names given as input. The
only type of event is Renaming where both argu-
ments are of type Gene. The event is directed, the
former and the new names are distinguished. Genes
and proteins were not distinguished because of the
high frequency of metonymy in renaming events.
The relation to predict between genes is a Renam-
ing of a former gene name into a new one. In the
example of Figure 1, YtaA, YvdP and YnzH are the
former names of three proteins renamed CotI, CotQ
and CotU, respectively.
Figure 1: Examples of relations to be extracted.
2.1 Rename Task corpus
The Rename Task corpus is a set of 1,836 PubMed
references of bacterial genetic and genomic studies,
including title and abstract. A first set of 23,000 doc-
uments was retrieved, identifying the presence of the
bacterium Bacillus subtilis in the text and/or in the
MeSH terms. B. subtilis documents are particularly
rich in renaming mentions. Many genes were re-
named in the middle of the nineties, so that the new
names matched those of the Escherichia coli homo-
logues. The 1,843 documents the most susceptible
to mention renaming were automatically filtered ac-
cording to two non exclusive criteria:
1. Either the document mentions at least two gene
synonyms as recorded in the fusion of seven B.
subtilis gene nomenclatures. This led to a set
of 703 documents.
2. Or the document contains a renaming expres-
sion from a list that we manually designed and
tested (e.g. rename, also known as). It is an ex-
tension of a previous work by (Weissenbacher,
2004). A total of 1,140 new documents not in-
cluded in the first set match this criteria.
About 70% of the documents (1,146) were kept in
the training data set. The rest was split into the de-
velopment and test sets, containing 246 and 252 doc-
uments respectively. Table 1 gives the distribution
of genes and renaming relations per corpus. Gene
names were automatically annotated in the docu-
ments with the nomenclature of B. subtilis. Gene
names involved in renaming acts were manually cu-
rated. Among the 21,878 gene mentions in the three
corpus, 680 unique names are involved in renaming
relations which represents 891 occurrences of genes.
Training + Dev. Test
Documents (1,146 + 246) 1,392 252 (15%)
Gene names 18,503 3,375 (15%)
Renamings 373 88 (24%)
Table 1: Rename Task corpus content.
2.2 Rename Task annotation and guidelines
Annotation procedure The corpus was annotated
in a joint effort of MIG/INRA and INIST/CNRS.
The reference annotation of the Rename Task cor-
pus was done in two steps, a first annotation step
by science information professionals of INIST with
MIG initial specifications, a second checking step by
people at MIG. Two annotators and a project man-
ager were in charge of the task at INIST. The docu-
ments were annotated using the Cadixe editor1. We
1http://caderige.imag.fr/Articles/
CADIXEXML-Annotation.pdf
66
provided to them detailed annotation guidelines that
were largely modified in the process. A subset of
100 documents from the first set of 703 was anno-
tated as a training session. This step was used to re-
fine the guidelines according to the methodology de-
scribed in (Bonneau-Maynard et al, 2005). Several
inter-annotator agreements coefficients were com-
puted to measure the discrepancy between annota-
tors (Fort et al, 2009). With a kappa and pi scores
(for more details on those, see (Artstein and Poesio,
2008)), the results can be considered satisfactory.
The manual analysis of the 18 discrepancies led to
enrich the annotation guidelines. The first hundreds
of documents of the second set did not mention any
renaming, leading to concentrate the annotation ef-
forts on the first set. These documents actually con-
tained renamings, but nearly exclusively concerning
other kinds of biological entities (protein domains,
molecules, cellular ultrastructures, etc.).
Guidelines In order to simplify the task, only
short names of gene/protein/groups in B. subtilis
were considered. Naming conventions set short
names of four letters long with an upper case let-
ter at the end for all genes (e.g. gerE) and the same
names with the upper case of the initial letter (e.g.
GerE) and long names for the proteins (e.g. Spore
germination protein gerE). But many irregular gene
names exist (e.g. tuf), which are considered as well.
It also happens that gene or protein name lists are
abbreviated by factorization to form a sequence. For
instance queCDEF is the abbreviation of the list of
gene names queC, queD, queE and queF. Such ag-
gregations are acceptable gene names as well. In any
case, these details were not needed by the task par-
ticipants since the corpus was provided with tagged
gene names.
Most renaming relations involve couples of the
same type, genes, proteins or aggregations. Only
18 relations link mixed couples of genes and pro-
teins. In case of ambiguity, annotators would consult
international gene databases and an internal INRA
database to help them determine whether a given
couple of names were actually synonyms.
Multiple occurrences of the same renaming rela-
tion were annotated independently, and had to be
predicted. The renaming pairs are directed, the for-
mer and the new forms have to be distinguished.
When the renaming order was not explicit in the
document, the rule was to annotate by default the
first member of the couple as the new form, and the
second one as the former form. Figure 2 presents the
most common forms of renaming.
Figure 2: Common types of relations to be extracted.
Revised annotations INIST annotations were
systematically checked by two experts in Bioinfor-
matics from INRA. Mainly, encoding relations (e.g.
the gene encoding sigma K (sigK)) that are not re-
naming cases were purged. Given the number of
ambiguous annotations, we designed a detailed ty-
pology in order to justify acceptance or rejection
decisions in seven different sub-cases hereafter pre-
sented. Three positive relations figure in Table 2,
where the underlined names are the former names
and the framed names are the new ones. Explicit re-
naming relations occur in 261 sentences, synonymy-
like relations in 349 sentences, biological proof-
based relations in 76 sentences.
Explicit renaming relation is the easiest positive
case to identify. In the example, the aggregation of
gene names ykvJKLM is clearly renamed by the au-
thors as queCDEF. Although the four genes are con-
Explicit renaming
PMID 15767583 : Genetic analysis of ykvJKLM mu-
tants in Acinetobacter confirmed that each was essen-
tial for queuosine biosynthesis, and the genes were re-
named queCDEF .
Implicit renaming
PMID 8002615 : Analysis of a suppressor mutation
ssb ( kinC ) of sur0B20 (spo0A) mutation in Bacil-
lus subtilis reveals that kinC encodes a histidine pro-
tein kinase.
Biological proof
PMID 1744050 : DNA sequencing established that
spoIIIF and spoVB are a single monocistronic locus
encoding a 518-amino-acid polypeptide with features
of an integral membrane protein.
Table 2: Positive examples of the Rename Task.
67
catenated, there is no evidence mentioned of them
acting as an operon. Furthermore, despite the con-
text involving mutants of Acinetobacter, the aggre-
gation belongs correctly to B. subtilis.
Implicit renaming is an asymmetric relation
since one of the synonyms is intended to replace the
other one in future uses. The example presents two
renaming relations between former names ssb and
spo0A, and new names kinC and sur0B20, respec-
tively. The renaming relation between ssb and kinC
has a different orientation due to additional informa-
tion in the reference. Like in the preceding example,
the renaming is a consequence of a genetic mutation
experiment. Mutation names represent an important
transversal issue that is discussed below.
Biological proof is a renaming relation induced
by an explicit scientific conclusion while the renam-
ing is not, as in the example where experiments re-
veal that two loci spoIIIF and spoVB are in fact the
same one and then become synonyms. Terms such
as ?allelic to? or ?identical to? usually qualify such
conclusions. Predicting biological proof-based rela-
tions requires some biological modeling.
The next three cases are negative (Table 3). Un-
derlined gene and protein names are involved in a
relation which is not a renaming relation.
Protein encoding relation occurs between a gene
and the protein it codes for. Some mentions may
look like renaming relations. The example presents
the gene yeaC coding for MoxR. No member of the
couple is expected to replace the other one.
Homology measures the similarity between gene
or protein sequences. Most of the homology men-
tions involve genes or proteins from different species
Protein encoding
PMID 8969499: The putative products of ORFs yeaB
(Czd protein), yeaC (MoxR), yebA (CNG-channel and
cGMP-channel proteins from eukaryotes),
Genetic homology
PMID 10619015 : Dynamic movement of the ParA-
like Soj protein of B. subtilis and its dual role in nu-
cleoid organization and developmental regulation.
Operon | Regulon | Family
PMID 3127379 : Three promoters direct transcription
of the sigA (rpoD) operon in Bacillus subtilis.
Table 3: Negative examples of the Rename Task.
(orthologues). The others compare known gene or
protein sequences of the same species (paralogues).
This may be misleading since the similarity men-
tion may look like biological proof-based relations,
as between ParA and Soj in Table 3.
Operon, regulon or family renaming involves
objects that may look like genes, proteins or sim-
ple aggregations of gene or protein names but that
are perceptibly different. The objects represent more
than one gene or protein and the renaming does not
necessarily affect all of them. More problematic,
their name may be the same as one of the genes or
proteins they contain, as in the example where sigA
and rpoD are operons but are also known as gene
names. Here, sigA (and so rpoD) represents at least
two different genes. For the sake of clarity, oper-
ons, regulons and families are rejected, even if all
the genes are clearly named, as in an aggregation.
The last point concerns mutation which are fre-
quent in Microbiology for revealing gene pheno-
types. They carry information about the original
gene names (e.g., rvtA11 is a mutant name created
by adding 11 to rvtA). But partial names cannot be
partially annotated, that is to say, the original part
(rvtA) should not be annotated in the mutation name
(rvtA11). Most of these names are local names, and
should not be annotated because of their restricted
scope. It may happen so that the mutation name
is registered as a synonym in several international
databases. To avoid inconsistencies, all renamings
involving a mutation referenced in a database were
accepted, and only biological proof-based and ex-
plicit renamings involving a strict non-null unrefer-
enced mutation (a null mutation corresponds to a to-
tal suppression of a gene) were accepted.
2.3 Rename Task evaluation procedure
The evaluation of the Rename task is given in terms
of recall, precision and F-score of renaming rela-
tions. Two set of scores are given: the first set is
computed by enforcing strict direction of renaming
relations, the second set is computed with relaxed
direction. Since the relaxed score takes into ac-
count renaming relations even if the arguments are
inverted, it will necessarily be greater or equal than
the strict score. The participant score is the relaxed
score, the strict score is given for information. Re-
laxed scores are informative with respect to the ap-
68
plication goal. The motivation of the Rename task
is to keep bacteria gene synonyms tables up to date.
The choice of the canonical name among synonyms
for denoting a gene is done by the bacteriology com-
munity, and it may be independent of the anteriority
or novelty of the name. The annotation of the ref-
erence corpus showed that the direction was not al-
ways decidable, even for a human reader. Thus, it
would have been unfair to evaluate systems on the
basis of unsure information.
2.4 Results of the Rename Task participants
Final submissions were received from three teams,
the University of Turku (Uturku), the University of
Concordia (Concordia) and the Bibliome team from
MIG/INRA. Their results are summarized in Table
4. The ranking order is given by the overall F-score
for relations with relaxed argument order.
Team Prec. Recall F-score
Univ. of Turku 95.9 79.6 87.0
Concordia Univ. 74.4 65.9 69.9
INRA 57.0 73.9 64.4
Table 4: Participant scores at the Rename Task.
Uturku achieved the best F-score with a very high
precision and a high recall. Concordia achieved the
second F-score with balanced precisions and recalls.
Bibliome is five points behind with a better recall
but much lower precision. Both UTurku and Con-
cordia predictions rely on dependencies (Charniak-
Johnson and Stanford respectively, using McClosky
model), whereas Bibliome predictions rely on bag of
words. This demonstrates the high value of depen-
dency parsing for this task, in particular for the pre-
cision of predictions. We notice that UTurku system
uses machine learning (SVM) and Concordia uses
rules based on trigger words. The good results of
UTurku confirms the hypothesis that gene renam-
ing citations are highly regular in scientific litera-
ture. The most frequently missed renamings belong
to the Biological Proof category (see Table 2). This
is expected because the renaming is formulated as a
reasoning where the conclusion is only implicit.
2.5 Discussion
The very high score of Uturku method leads us to
conclude that the task can be considered as solved
by a linguistic-based approach. Whereas Bib-
liome used an extensive nomenclature considered
as exhaustive and sentence filtering using a SVM,
Uturku used only two nomenclatures in synergy but
with more sophisticated linguistic-based methods,
in particular syntactic analyses. Bibliome methods
showed that a too high dependence to nomenclatures
may decrease scores if they contain compromised
data. However, the use of an extensive nomencla-
ture as done by Bibliome may complement Uturku
approach and improve recall. It is also interesting
that both systems do not manage renamings cross-
ing sentence boundaries.
The good results of the renaming task will be ex-
ploited to keep synonym gene lists up to date with
extensive bibliography mining. In particular this
will contribute to enriching SubtiWiki, a collabora-
tive annotation effort on B. subtilis (Flo?rez et al,
2009; Lammers et al, 2010).
3 Gene Interactions Task description
The goal of the Bacteria GI Task is illustrated by
Figure 3. The genes cotB and cotC are related to
their two promoters, not named here, by the rela-
tion PromoterOf. The protein GerE is related to
these promoters by the relation BindTo. As a con-
sequence, GerE is related to cotB and cotC by an In-
teraction relation. According to (Kim et al, 2008),
the need to define specialized relations replacing one
unique and general interaction relation was raised in
(Manine et al, 2009) for extracting genic interac-
tions from text. An ontology describes relations and
entities (Manine et al, 2008) catching a model of
gene transcription to which biologists implicitly re-
fer in their publications. Therefore, the ontology is
mainly oriented towards the description of a struc-
tural model of genes, with molecular mechanisms
of their transcription and associated regulations.
The corpus roughly contains three kinds of genic
Figure 3: Examples of relations to be extracted.
69
interaction mentions, namely regulations, regulon
membership and binding. The first case corresponds
to interactions the mechanism of which is not explic-
itly given in the text. The mention only tells that the
transcription of a given gene is influenced by a given
protein, either positively (activation), negatively (in-
hibition) or in an unspecified way. The second kind
of genic interaction mention (regulon membership)
basically conveys the same information, using the
regulon term/concept. The regulon of a gene is the
set of genes that it controls. In that case, the interac-
tion is expressed by saying that a gene is a member
of some regulon. The third and last kind of mention
provides with more mechanistic details on a regula-
tion, since it describes the binding of a protein near
the promoter of a target gene. This motivates the in-
troduction of Promoter and Site entities, which cor-
respond to DNA regions. It is thus possible to extract
the architecture of a regulatory DNA region, linking
a protein agent to its gene target (see Figure 3).
The set of entity types is divided into two main
groups, namely 10 genic entities and 3 kinds of ac-
tion (Table 5). Genic entities represent biological
objects like a gene, a group of genes or a gene prod-
uct. In particular, a GeneComplex annotation corre-
sponds to an operon, which is a group of genes that
are contiguous in the genome and under the control
of the same promoter. The annotation GeneFamily
is used to denote either genes involved in the same
biological function or genes with sequence homolo-
gies. More importantly, PolymeraseComplex anno-
tations correspond to the protein complex that is re-
sponsible for the transcription of genes. This com-
plex includes several subunits (components), com-
bined with a sigma factor, that recognizes specific
promoters on the DNA sequence.
The second group of entities are phrases express-
ing either molecular processes (e.g. sequestration,
dephosphorylation, etc.) or the molecular state of
the bacteria (e.g. presence, activity or level of a pro-
tein). They represent some kind of action that can
be performed on a genic entity. Note that transcrip-
tion and expression events were tagged as specific
actions, because they play a specific part in certain
relations (see below).
The annotation of entities and actions was pro-
vided to the participants, and the task consisted in
extracting the relations listed in Table 6.
Name Example
Gene cotA
GeneComplex sigX-ypuN
GeneFamily class III heat shock genes
GeneProduct yvyD gene product
Protein CotA
PolymeraseComplex SigK RNA polymerase
ProteinFamily DNA-binding protein
Site upstream site
Promoter promoter regions
Regulon regulon
Action activity | level | presence
Expression expression
Transcription transcription
Table 5: List of molecular entities and actions in GI.
Name Example
ActionTarget expression of yvyD
Interaction ComK negatively regulates
degR expression
RegulonDependence sigmaB regulon
RegulonMember yvyD is member of sigmaB
regulon
BindTo GerE adheres to the pro-
moter
SiteOf -35 sequence of the pro-
moter
PromoterOf the araE promoter
PromoterDependence GerE-controlled promoter
TranscriptionFrom transcription from the up-
stream site
TranscriptionBy transcription of cotD by
sigmaK RNA polymerase
Table 6: List of relations in GI.
The relations are binary and directed, and rely the
entities defined above. The three kinds of interac-
tions are represented with an Interaction annotation,
linking an agent to its target. The other relations
provide additional details on the regulation, like ele-
mentary components involved in the reaction (sites,
promoters) and contextual information (mainly pro-
vided by the ActionTarget relations). A formal def-
inition of relations and relation argument types can
be found on the Bacteria GI Task Web page.
3.1 Bacteria Gene Interactions corpus
The source of the Bacteria GI Task corpus is a set
of PubMed abstracts mainly dealing with the tran-
70
scription of genes in Bacillus subtilis. The semantic
annotation, derived from the ontology of (Manine et
al., 2008), contains 10 molecular entities, 3 different
actions, and 10 specialized relations. This is applied
to 162 sentences from the LLL set (Ne?dellec, 2005),
which are provided with manually checked linguis-
tic annotations (segmentation, lemmatization, syn-
tactic dependencies). The corpus was split into 105
sentences for training, 15 for development and 42
for test. Table 7 gives the distribution of the entities
and actions per corpus and Table 8 gives the distri-
bution of the relations per corpus.
3.2 Annotation procedures and guidelines
The semantic annotation scheme was developed by
two annotators through a series of independent an-
notations of the corpus, followed by reconciliation
steps, which could involve concerted modifications
(Manine et al, 2010). As a third and final stage, the
Entity or action Train. + Dev. Test
Documents (105+15) 120 42
Protein 219 85
Gene 173 56
Transcription 53 21
Promoter 49 10
Action 45 22
PolymeraseComplex 43 14
Expression 29 6
Site 22 8
GeneComplex 19 4
ProteinFamily 12 3
Regulon 11 2
GeneProduct 10 3
GeneFamily 6 5
Table 7: Distribution of entities and actions in GI.
Relation Train. + Dev. Test
Interaction 208 64
ActionTarget 173 47
PromoterOf 44 8
BindTo 39 4
PromoterDependence 36 4
TranscriptionBy 36 8
SiteOf 23 6
RegulonMember 17 2
TranscriptionFrom 14 2
RegulonDependence 12 1
Table 8: Distribution of relations in GI.
corpus was reviewed and the annotation simplified
to make it more appropriate to the contest. The final
annotation contains 748 relations distributed in nine
categories, 146 of them belonging to the test set.
The annotation scheme was generally well suited
to accurately represent the meaning of the sentences
in the corpus, with one notable exception. In the cor-
pus, there is a common phrasing telling that a pro-
tein P regulates the transcription of a gene G by a
given sigma factor S. In that case, the only anno-
tated interactions are between the couples (P, G) and
(S, G). This representation is not completely satis-
factory, and a ternary relation involving P, S and G
would have been more adequate.
Additional specific rules were needed to cope
with linguistic issues. First, when the argument of a
relation had coreferences, the relation was repeated
for each maximally precise coreference of the argu-
ment. Second, in case of a conjunction like ?sig-
maA and sigmaX holoenzymes?, there should ide-
ally be two entities (namely ?sigmaA holoenzyme?
and ?sigmaX holoenzyme?); however, this is not
easy to represent using the BioNLP format. In this
situation, we grouped the two entities into a single
one. These cases were rare and unlikely affected the
feasibility of the task, since entities were provided
in the test set.
3.3 Gene Interactions evaluation procedure
The training and development corpora with the ref-
erence annotations were made available to partici-
pants by December, 1st on the BioNLP shared Task
pages together with evaluation software. The test
corpus with the entity annotations has been made
available by March, 1st. The participants sent the
predicted annotations to the BioNLP shared Task
organizers by March, 10th. The evaluation results
were computed and provided to the participants and
on the Web site the same day. The participants are
evaluated and ranked according to two scores: F-
score for all event types together, and F-score for
the Interaction event type. In order for a predicted
event to count as a hit, both arguments must be the
same as in the reference in the right order and the
event type must be the same as in the reference.
71
3.4 Results of GI Task participants
There was only one participant, whose results are
shown in Tables 9 and 10. Some relations were
not significantly represented in the test set and thus
the corresponding results should be considered with
caution. This is the case for RegulonMember and
TranscriptionFrom, only represented two times each
in the test. The lowest recall, 17%, obtained for the
SiteOf relation is explained by its low representa-
tion in the corpus: most of the test errors come from
a difficult sentence with coreferences.
The recall of 56% for the Interaction relation cer-
tainly illustrates the heterogeneity of this category,
gathering mentions of interactions at large, as well
as precise descriptions of gene regulations. For in-
stance, Figure 4 shows a complex instance where all
of the interactions were missed. Surprisingly, we
also found false negatives in rather trivial examples
(?ykuD was transcribed by SigK RNA polymerase
from T4 of sporulation.?). Uturku used an SVM-
based approach for extraction, and it is thus delicate
to account for the false negatives in a simple and
concise way.
Event U. Turku scores
Global Precision 85
Global Recall 71
Global F-score 77
Interaction Precision 75
Interaction Recall 56
Interaction F-score 64
Table 9: University of Turku global scores.
Event Prec. Rec. F-score
Global 85 71 77
ActionTarget 94 92 93
BindTo 75 75 75
Interaction 75 56 64
PromoterDependence 100 100 100
PromoterOf 100 100 100
RegulonDependence 100 100 100
RegulonMember 100 50 67
SiteOf 100 17 29
TranscriptionBy 67 50 57
TranscriptionFrom 100 100 100
Table 10: University of Turku scores for each relation.
Figure 4: Examples of three missed interactions.
3.5 Discussion
The GI corpus was previously used in a relation
extraction work (Manine et al 2009) based on In-
ductive Logic Programming (Muggleton and Raedt,
1994). However a direct comparison of the results
is not appropriate here since the annotations were
partially revised, and the evaluation setting was dif-
ferent (leave-one-out in Manine?s work, test set in
the challenge).
Nevertheless, we note similar tendencies if we
compare relative results between relations. In partic-
ular, it was also found in Manine?s paper that SiteOf,
TranscriptionBy and Interaction are the most diffi-
cult relations to extract. It is also worth to mention
that both approaches rely on syntactic dependencies,
and use the curated dependencies provided in the
corpus. Interestingly, the approach by the University
of Turku reports a slightly lower F-measure with de-
pendencies calculated by the Charniak parser (about
1%, personal communication). This information is
especially important in order to consider a produc-
tion setting.
4 Conclusion
The quality of results for both challenges suggests
that current methods are mature enough to be used
in semi-automatic strategies for genome annotation,
where they could efficiently assist biological experts
involved in collaborative annotation efforts (Lam-
mers et al, 2010). However, the false positive rate,
notably for the Interaction relation, is still too high
for the extraction results to be used as a reliable
source of information without a curation step.
Acknowlegments
We thank Franc?oise Tisserand and Bernard Talercio
(INIST) for their work on the Rename corpus, and
the QUAERO Programme funded by OSEO (French
agency for innovation) for its support.
72
References
Artstein R., Poesio M. (2008). Inter-coder agreement
for Computational Linguistics. Computational Lin-
guistics, 34(4):555-96.
Bjo?rne J., Heimonen J., Ginter F., Airola A., Pahikkala
T., Salakoski T. (2009). Extracting complex biological
events with rich graph-based feature sets. BioNLP?09
Proc. Workshop Current Trends in Biomedical Natural
Language Processing: Shared Task, pp. 10-18.
Bonneau-Maynard H., Rosset S., Ayache C., Kuhn A.,
Mostefa D. (2005). Semantic annotation of the French
Media Dialog Corpus. Interspeech-2005, pp. 3457-60.
Demner-Fushman D., Ananiadou S., Cohen K.B., Pestian
J., Tsujii J., Webber B. (2008). Themes in biomedical
natural language processing: BioNLP08. BMC Bioin-
formatics, 9(Suppl. 11):S1.
Flo?rez L.A., Roppel S.F., Schmeisky A.G., Lammers
C.R., Stu?lke J. (2009). A community-curated con-
sensual annotation that is continuously updated: The
Bacillus subtilis centred wiki SubtiWiki. Database,
2009:bap012.
Fort K., Franc?ois C., Ghribi M. (2010). ?Evaluer des an-
notations manuelles disperse?es : les coefficients sont-
ils suffisants pour estimer l?accord inter-annotateurs ?
17e Conf. Traitement Automatique des Langues Na-
turelles (TALN 2010).
Kim J.D., Ohta T., Tsujii J. (2008) Corpus annotation for
mining biomedical events from literature. BMC Bioin-
formatics, 9:10.
Kim J.D., Ohta T., Pyysalo S., Kano Y., Tsujii J. (2009).
Overview of BioNLP?09 shared task on event ex-
traction. BioNLP?09 Proc. Workshop Current Trends
in Biomedical Natural Language Processing: Shared
Task, pp. 1-9.
Krallinger M., Leitner F., Rodriguez-Penagos C., Va-
lencia A. (2008). Overview of the protein-protein in-
teraction annotation extraction task of BioCreative II.
Genome Biology, 9(Suppl. 2):S4.
Lammers C.R., Flo?rez L.A., Schmeisky A.G., Roppel
S.F., Ma?der U., Hamoen L., Stu?lke J. (2010). Con-
necting parts with processes: SubtiWiki and Subti-
Pathways integrate gene and pathway annotation for
Bacillus subtilis. Microbiology, 156(3):849-59.
Manine A.P., Alphonse E., Bessie`res P. (2008). Informa-
tion extraction as an ontology population task and its
application to genic interactions. 20th IEEE Int. Conf.
Tools with Artificial Intelligence (ICTAI?08), pp. 74-
81.
Manine A.P., Alphonse E., Bessie`res P. (2009). Learn-
ing ontological rules to extract multiple relations of
genic interactions from text. Int. J. Medical Informat-
ics, 78(12):e31-8.
Manine A.P., Alphonse E., Bessie`res P. (2010). Extrac-
tion of genic interactions with the recursive logical the-
ory of an ontology. Lecture Notes in Computer Sci-
ences, 6008:549-63.
Muggleton S., Raedt L.D. (1994) Inductive Logic Pro-
gramming: Theory and methods. J. Logic Program-
ming, 19-20:629-79.
Ne?dellec C. (2005). Learning Language in Logic ? Genic
Interaction Extraction Challenge. Proc. 4th Learning
Language in Logic Workshop (LLL?05), pp. 31-7.
Weissenbacher, D. (2004). La relation de synonymie en
Ge?nomique. RECITAL 2004 Conference.
73
