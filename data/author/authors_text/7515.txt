Detection of Question-Answer Pairs in Email Conversations
Lokesh Shrestha and Kathleen McKeown
Columbia University
Computer Science Deparment
New York, NY 10027,
USA,
lokesh@cs.columbia.edu, kathy@cs.columbia.edu
Abstract
While sentence extraction as an approach to
summarization has been shown to work in docu-
ments of certain genres, because of the conver-
sational nature of email communication where
utterances are made in relation to one made
previously, sentence extraction may not capture
the necessary segments of dialogue that would
make a summary coherent. In this paper, we
present our work on the detection of question-
answer pairs in an email conversation for the
task of email summarization. We show that var-
ious features based on the structure of email-
threads can be used to improve upon lexical
similarity of discourse segments for question-
answer pairing.
1 Introduction
In this paper, we discuss work on the detection of
question and answer pairs in email threads, i.e., co-
herent exchanges of email messages among several
participants. Email is a written medium of asyn-
chronous multi-party communication. This means
that, as in face-to-face spoken dialog, the email
thread as a whole is a collaborative effort with inter-
action among the discourse participants. However,
unlike spoken dialog, the discourse participants are
not physically co-present, so that the written word is
the only channel of communication. Furthermore,
replies do not happen immediately, so that respon-
ders need to take special precautions to identify rel-
evant elements of the discourse context (for exam-
ple, by quoting previous messages). Thus, email is
a distinct linguistic genre that poses its own chal-
lenges to summarization.
With the increasing popularity of email as a
means of communication, an increasing number of
meetings are scheduled, events planned, issues re-
solved, and questions answered through emails. As
the number of emails in one?s mailbox increases, in-
Regarding ?acm home/bjarney?, on Apr 9,
2001, Muriel Danslop wrote:
Two things: Can someone be responsible for
the press releases for Stroustrup?
Responding to this on Apr 10, 2001, Theresa
Feng wrote:
I think Phil, who is probably a better writer
than most of us, is writing up something for
dang and Dave to send out to various ACM
chapters. Phil, we can just use that as our
?press release?, right?
In another subthread, on Apr 12, 2001, Kevin
Danquoit wrote:
Are you sending out upcoming events for this
week?
Figure 1: Sample summary obtained with sentence
extraction
formation from past conversations becomes increas-
ingly inaccessible and difficult to manage. For ex-
ample, a number of emails can be used in schedul-
ing a meeting, and a search for information on the
meeting may retrieve all the intermediate emails,
thus hindering one?s access to the required informa-
tion. Access to required information could be dra-
matically improved by querying summaries of email
conversations
While summarization of email conversations
seems a natural way to improve upon current
methods of email management, research on email
summarization is in early stages. Consider an
example summary of a thread of email conver-
sation produced by a sentence extraction based
email thread summarization system developed at
Columbia (Rambow et al, 2004) shown in Figure 1.
While this summary does include an answer to the
first question, it does not include answers to the
two questions posed subsequently even though the
answers are present in the thread. This example
demonstrates one of the inadequacies of sentence
extraction based summarization modules: namely,
the absence of discourse segments that would have
made the summaries more readable and complete.
A summarization module that includes answers to
questions posed in extractive summaries, then, be-
comes very useful.
Further, questions are a natural means of resolv-
ing any issue. This is especially so of email conver-
sations through which most of our issues, whether
professional or personal, get resolved. And, the
asynchronous nature of email conversation makes it
possible for users to pursue several questions in par-
allel. In fact, in our corpus of email exchanges, we
found that about 20% of all email threads focus pri-
marily on a question-answer exchange, whether one
question is posed and multiple people respond or
whether multiple questions are posed and multiple
responses given. For these type of email exchanges,
a summary that can highlight the main question(s)
asked and the response(s) given would be useful.
Being able to distinguish questions pertaining to dif-
ferent issues in an email thread and being able to
associate the answers with their questions is a ne-
cessity in order to generate this type of summary.
In this paper, we present our work on the detec-
tion of question and answer pairs in email conver-
sations. The question-answer detection system we
present will ultimately serve as one component of a
full email summarization system, providing a por-
tion of summary content. We developed one ap-
proach for the detection of questions in email mes-
sages, and a separate approach to detect the corre-
sponding answers. These are described in turn be-
low.
2 Previous and Related Work
(Muresan et al, 2001) describe work on summariz-
ing individual email messages using machine learn-
ing approaches to learn rules for salient noun phrase
extraction. In contrast, our work aims at summariz-
ing whole threads and at capturing the interactive
nature of email.
(Nenkova and Bagga, 2003) present work on gen-
erating extractive summaries of threads in archived
discussions. A sentence from the root message and
from each response to the root is extracted using
ad-hoc algorithms crafted by hand. This approach
works best when the subject of the root email best
describes the ?issue? of the thread, and when the
root email does not discuss more than one issue. In
our work, we do not make any assumptions about
the nature of the email, and try to learn strategies to
link question and answer segments for summariza-
tion.
(Newman and Blitzer, 2003) also address the
problem of summarizing archived discussion lists.
They cluster messages into topic groups, and then
extract summaries for each cluster. The summary of
a cluster is extracted using a scoring metric based on
sentence position, lexical similarity of a sentence to
cluster centroid, and a feature based on quotation,
among others. Because the summaries are extrac-
tive in nature, this approach still suffers from the
possibility of incomplete summaries.
(Lam et al, 2002) present work on email summa-
rization by exploiting the thread structure of email
conversation and common features such as named
entities and dates. They summarize the message
only, though the content of the message to be sum-
marized is ?expanded? using the content from its an-
cestor messages. The expanded message is passed
to a document summarizer which is used as a black
box to generate summaries. Our work, in contrast,
aims at summarizing the whole thread, and we are
precisely interested in changing the summarization
algorithm itself, not in using a black box summa-
rizer.
In addition, there has been some work on summa-
rizing meetings. As discussed in Section 1, email is
different in important respects from multi-party di-
alog. However, some important aspects are related.
(Zechner and Lavie, 2001), for example, presents
a spoken dialogue summarization system that takes
into consideration local cross-speaker coherence by
linking question answer pairs, and uses this infor-
mation to generate extract based summaries with
complete question-answer regions. While we have
used a similar question detection approach, our ap-
proach to answer detection is different. We get back
to this in Section 4.
(Rambow et al, 2004) show that sentence ex-
traction techniques can work for summarizing email
threads, but profit from email-specific features. In
addition, they show that the presentation of the sum-
mary should take into account the dialogic structure
of email communication. However, since their ap-
proach does not try to detect question and answer
pairs, the extractive summaries suffer from the pos-
sibility of incomplete summaries.
3 Automatic Question Detection
While the detection of questions in email messages
is not as difficult a problem as in speech conversa-
tions where features such as the question mark char-
acter are absent, relying on the use of question mark
character for identifying questions in email mes-
sages is not adequate. The need for special attention
in detecting questions in email messages arises due
to three reasons. First, the use of informal language
means users might use the question mark character
in cases other than questions (for example, to de-
note uncertainty) and may overlook using a question
mark after a question. Second, a question may be
stated in a declarative form, as in, ?I was wondering
if you are free at 5pm today.? Third, not every ques-
tion, whether in an interrogative form or in a declar-
ative form, is meant to be answered. For example,
rhetorical questions are used for purposes other than
to obtain the information the question asked, and are
not required to be associated with answer segments.
We used supervised rule induction for the de-
tection of interrogative questions. Training exam-
ples were extracted from the transcribed SWITCH-
BOARD corpus annotated with DAMSL tags.1
This particular corpus was chosen not only be-
cause an adequate number of training examples
could be extracted from the manual annotations,
but also because of the use of informal language
in speech that is also characteristic of email con-
versation. Utterances with DAMSL tags ?sv?
(speech act ?statement-opinion?) and ?sd? (speech
act ?statement-non-opinion?) were used to extract
5,000 negative examples. Similarly, utterances
with tags ?qy? (?yes-no-question?), ?qw? (?Wh-
question?), and ?qh? (?rhetorical-question?) were
used to extract 5,000 positive examples. Each utter-
ance was then represented by a feature vector which
included the following features:
? POS tags for the first five terms (shorter utter-
ances were padded with dummies)
? POS tags for the last five terms (shorter utter-
ances were padded with dummies)
? length of the utterance
1From the Johns Hopkins University LVCSR
Summer Workshop 1997, available from
http://www.colorado.edu/ling/jurafsky/ws97/
Scheme Ripper Ripper+
Recall 0.56 0.72
Precision 0.96 0.96
F1-score 0.70 0.82
Table 1: Test results for detection of questions in
interrogative form
? POS-bigrams from a list of 100 most discrimi-
nating POS-bigrams list.
The list of most discriminating POS-bigrams was
obtained from the training data by following the
same procedure that (Zechner and Lavie, 2001)
used.
We then used Ripper (Cohen, 1996) to learn
rules for question detection. Like many learning
programs, Ripper takes as input the classes to be
learned, a set of feature names and possible values,
and training data specifying the class and feature
values for each training example. In our case, the
training examples are the speech acts extracted from
the SWITCHBOARD corpus as described above.
Ripper outputs a classification model for predicting
the class (i.e., whether a speech act is a question
or not) of future examples; the model is expressed
as an ordered set of if-then rules. For testing, we
manually extracted 300 questions in interrogative
form and 300 statements in declarative form from
the ACM corpus.2 We show our test results with re-
call, precision and F1-score3 in Table 1 on the first
column.
While the test results show that the precision was
very good, the recall score could be further im-
proved. Upon further investigation on why the re-
call was so low, we found that unlike the positive
examples we used in our training data, most of the
questions in the test data that were missed by the
rules learned by Ripper started with a declarative
phrase. For example, both ?I know its on 108th, but
after that not sure, how exactly do we get there??,
and ?By the way, are we shutting down clic?? be-
gin with declarative phrases and were missed by the
Ripper learned rules. Following this observation,
2More information on the ACM corpus will be provided in
Section 4.1. At the time of the development of the question
detection module the annotations were not available to us, so
we had to manually extract the required test speech acts.
3F1-score = 2PRP+R , where P=Precision and R=Recall
we manually updated our question detection mod-
ule to break a speech act that was not initially pre-
dicted as question into phrases separated by comma
characters. Then we applied the rules on the first
phrase of the speech act and if that failed on the last
phrase. For example, the rules would fail on the
phrase ?I know its on 108th?, but would be able to
classify the phrase ?how exactly do we get there?
as a question. In doing this we were able to increase
the recall score to 0.72, leading to a F1-score of 0.82
as shown in Table 1 in the second column.
4 Automatic Answer Detection
While the automatic detection of questions in email
messages is relatively easier than the detection of
the same in speech conversations, the asynchronous
nature of email conversations makes detection and
pairing of question and answer pairs a more diffi-
cult task. Whereas in speech a set of heuristics can
be used to identify answers as shown by (Zechner
and Lavie, 2001), such heuristics cannot be readily
applied to the task of question and answer pairing in
email conversations. First, more than one topic can
be discussed in parallel in an email thread, which
implies that questions relating to more than a single
topic can be pursued in parallel. Second, even when
an email thread starts with the discussion of a single
topic, the thread may eventually be used to initiate a
different topic just because the previous topic?s list
of recipients closely matched those required for the
newly initiated topic. Third, because of the use of
?Reply? and ?ReplyAll? functions in email clients,
a user may be responding to an issue posed earlier
in the thread while using one of the email messages
subsequent to the message posing the issue to ?reply
back? to that issue. So, while it may seem from the
structure of the email thread that a person is reply-
ing back to a certain email, the person may actually
be replying back to an email earlier in the thread.
This implies that when several persons answer a
question, there may be answers which appear sev-
eral emails after the email posing the question. Fi-
nally, the fact that the question and its correspond-
ing answers may have few words in common fur-
ther complicates answer detection. This is possible
when a person uses the context of the email conver-
sation to ask questions and make answers, and the
semantics of such questions and answers have to be
interpreted with respect to the context they appear
in. Such context is readily available for a reader
through the use of quoted material from past email
messages. All of these make the task of detecting
and linking question and answer pairs in email con-
versations a complicated task. However, this task
is not as complicated a task as automatic question
answering where the search space for candidate an-
swers is much wider and more sophisticated mea-
sures than those based on lexical similarity have to
be employed.
Our approach to automatic answer detection in
email conversations is based on the observation that
while a number of issues may be pursued in paral-
lel, users tend to use separate paragraphs to address
separate issues in the same email message. While
a more complicated approach to segmentation of
email messages could be possible, we have used this
basic observation to delineate discourse segments in
email messages. Further, because discourse seg-
ments contain more lexical context than their in-
dividual sentences, our approach detects associa-
tions between pairs of discourse segments rather
than pairs of sentences.
We now present our machine learning approach
to automatic detection of question and answer pairs.
4.1 Corpus
Our corpus consists of approximately 300 threads of
about 1000 individual email messages sent during
one academic year among the members of the board
of the student organization of the ACM at Columbia
University. The emails dealt mainly with planning
events of various types, though other issues were
also addressed. On average, each thread contained
3.25 email messages, with all threads containing at
least two messages, and the longest thread contain-
ing 18 messages. Threads were constructed from
the individual email messages using the ?In-Reply-
To? header information to link parent and child
email messages.
Two annotators (DB and GR) each were asked
to highlight and link question and answer pairs in
the corpus. Our work presented here is based on
the work these annotators had completed at the time
of this writing. GR has completed work on 200
threads of which there are 81 QA threads (threads
with question and answer pairs), 98 question seg-
ments, and 142 question and answer pairs. DB has
completed work on 138 threads of which there are
62 QA threads, 72 question segments, and 92 ques-
tion and answer pairs. We consider a segment to
be a question segment if a sentence in that segment
has been highlighted as a question. Similarly, we
consider a segment to be an answer segment if a
sentence in that segment has been paired with a
question to form a question and answer pair. The
kappa statistic (Carletta, 1996) for identifying ques-
tion segments is 0.68, and for linking question and
answer segments given a question segment is 0.81.
4.2 Features
For each question segment in an email message,
we make a list of candidate answer segments.
This is basically just a list of original (content
that is not quoted from past emails)4 segments in
all the messages in the thread subsequent to the
message of the question segment. Let the thread in
consideration be called t, the container message of
the question segment be called mq, the container
message of the candidate answer segment be called
ma, the question segment be called q, and the
candidate answer segment be called a. For each
question and candidate answer pair, we compute
the following sets of features:
4.2.1 Some Standard Features
(a) number of non stop words in segment q and seg-
ment a;
(b) cosine similarity5 and euclidean distance6 be-
tween segment q and a;
4.2.2 Features derived from the structure of
the thread t
(c) the number of intermediate messages between
mq and ma in t;
(d) the ratio of the number of messages in t sent ear-
4While people do use quoted material to respond to specific
segments of past emails, a phenomenon more common is dis-
cussion lists, because the occurrence of such is rare in the ACM
corpus we decided not to use them as a feature.
5
cosine sim(x, y) =
?N
i=1(cxi ? cyi)??N
j=1 c2xj ?
?N
j=1 c2xj
where cxi is the count of word i in segment x, and cyi is thecount of word i in segment y.
6
euclidean dis(x, y) =
????
N?
i=1
(c2xi ? c2yi)
where cxi is the count of word i in segment x, and cyi is thecount of word i in segment y.
lier than mq and all the messages in t, and similarly
for ma;
(e) whether a is the first segment in the list of candi-
date answer segments of q (this is true if a segment
is the first segment in the first message sent in reply
to mq);
4.2.3 Features based on the other candidate
answer segments of q
(f) number of candidate answer segments of q and
the number of candidate answer segments of q after
a (a segment x is considered to be after another seg-
ment y if x is from a message sent later than that of
y, or if x appears after y in the same message);
(g) the ratio of the number of candidate answer seg-
ments before a and the number of all candidate an-
swer segments (a segment x is considered to be be-
fore another segment y if x is from a message sent
earlier than that of y, or if x appears before y in the
same message); and
(h) whether q is the most similar segment of a
among all segments from ancestor messages of ma
based on cosine similarity (the list of ancestor mes-
sages of a message is computed by recursively fol-
lowing the ?In-Reply-To? header information that
points to the parent message of a message).
While the contribution of a single feature to the
classification task may not be intuitively apparent,
we hope that a combination of a subset of these
features, in some way, would help us in detecting
question-answer pairs. For example, when the num-
ber of candidate answer segments for a question
segment is less than or equal to two, feature (e) may
be the best contributor to the classification task. But,
when the number of candidate answer segments is
high, a collection of some features may be the best
contributor.
We categorized each feature vector for the pair q
and a as a positive example if a has been marked as
an answer and linked with q.
4.3 Training Data
We computed four sets of training data. Two for
each of the annotators separately, which we call DB
and GR according to the label of their respective
annotator. One taking the union of the annotators,
which we call Union, and another taking the inter-
section, which we call Inter. For the first two sets,
we collected the threads that had at least one ques-
tion and answer pair marked by the respective anno-
tator. For each question that has an answer marked
Data Set DB GR Union Inter
datapoints 259 355 430 181
positives 89 139 168 59
questions 72 98 118 52
threads 62 81 97 46
Table 2: Summary of training data: number of in-
stances
Data Set Precision Recall F1-score
DB 0.6 0.27 0.372
GR 0.629 0.439 0.517
Union 0.61 0.429 0.503
Inter 0.571 0.407 0.475
Table 3: Baseline results
(some of the highlighted questions do not have cor-
responding answers in the thread), we computed a
list of feature vectors as described above with all of
its candidate answer segments. For the union set,
we collected all the threads that had question and
answer pairs marked by either annotator, and com-
puted the feature vectors for each such question seg-
ment. A feature vector was categorized positive if
any of the two annotators have marked the respec-
tive candidate answer segment as an answer. For
the intersection set, we collected all the threads that
had question and answer pairs marked by both an-
notators. Here, a feature vector was labelled posi-
tive only if both the annotators marked the respec-
tive candidate answer segment as an answer. Ta-
ble 2 summarizes the information on the four sets
of training data.
4.4 Experiments and Results
This section describes experiments using Ripper to
automatically induce question and candidate answer
pair classifiers, using the features described in Se-
cion 4.2. We obtained the results presented here us-
ing five-fold cross-validation.
Table 3 shows the precision, recall and F1-score
for the four datasets using the cosine similarity fea-
ture only. We use these results as the baseline
against which we compare the results for the full
set of features shown in Table 4. While precision
using the full feature set is comparable to that of
the baseline measure, we get a significant improve-
ment on recall with the full feature set. The base-
Data Set Precision Recall F1-score
DB 0.69 0.652 0.671
GR 0.68 0.612 0.644
Union 0.698 0.619 0.656
Inter 0.6 0.508 0.55
Table 4: Summary of results
line measure predicts that the candidate answer seg-
ment whose similarity with the question segment is
above a certain threshold will be an actual answer
segment. Our results suggest that lexical similarity
cannot alone capture the rules associated with ques-
tion and answer pairing, and that the use of various
features based on the structure of the thread of email
conversations can be used to improve upon lexical
similarity of discourse segments. Further, while the
results do not seem to suggest a clear preference for
the data set DB over the data set GR (this could be
explained by their high kappa score of 0.81), taking
the union of the two datasets does seem to be bet-
ter than taking the intersection of the two datasets.
This could be because the intersection greatly re-
duces the number of positive data points from what
is available in the union, and hence makes the learn-
ing of rules more difficult with Inter.
Finally, on observing that some questions had at
most 2 candidate answers, and others had quite a
few, we investigated what happens when we divide
the data set Union into two data sets, one for ques-
tion segments with 2 or less candidate answer seg-
ments which we call the data set Union a, and the
other with the rest of the data set which we call the
data set Union b. Union a has, on average, 1.5 can-
didate answer segments, while Union b has 5.7. We
show the results of this experiment with the full fea-
ture set in Table 5. Our results show that it is much
easier to learn rules for questions with the data set
Union a, which we show in the first row, than other-
wise. We compare our results for the baseline mea-
sure of predicting the majority class, which we show
in the second row, to demonstrate that the results
obtained with the dataset Union a were not due to
majority class prediction. While the results for the
other subset, Union b, which we show in the third
row, compare well with the results for Union, when
the results for the data sets Union a and Union b
are combined, which we show in the fourth row,
we achieve better results than without the splitting,
Data Set Precision Recall F1-score positives datapoints
Union a 0.879 0.921 0.899 63 79
Baseline for Union a 0.797 1.0 0.887 63 79
Union b 0.631 0.619 0.625 105 351
Combined 0.728 0.732 0.730 168 430
Union 0.698 0.619 0.656 168 430
Table 5: Summary of results with the split data set compared with a baseline and the unsplit data set
shown in the last row.
5 Conclusion and Future Work
We have presented an approach to detect question-
answer pairs with good results. Our approach is
the first step towards a system which can highlight
question-answer pairs in a generated summary. Our
approach works well with interrogative questions,
but we have not addressed the automatic detection
of questions in the declarative form and rhetorical
questions. People often pose their requests in a
declarative form in order to be polite among other
reasons. Such requests could be detected with their
use of certain key phrases some of which include
?Please let me know...?, ?I was wondering if...?, and
?If you could....that would be great.?. And, because
rhetorical questions are used for purposes other than
to obtain the information the question asked, such
questions do not require to be paired with answers.
The automatic detection of these question types are
still under investigation.
Further, while the approach to the detection of
question-answer pairs in threads of email conver-
sation we have presented here is quite effective, as
shown by our results, the use of such pairs of dis-
course segments for use in summarization of email
conversations is an area of open research to us.
As we discussed in Section 1, generation of sum-
maries for email conversations that are devoted to
question-answer exchanges and that integrate iden-
tified question-answer pairs as part of a full sum-
mary is also needed.
6 Acknowledgements
We are grateful to Owen Rambow for his helpful ad-
vice. We also thank Andrew Rosenberg for his dis-
cussion on kappa statistic as it relates to the ACM
corpus. This work was supported by the National
Science Foundation under the KDD program. Any
opinions, findings, and conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
National Science Foundation.
References
Jean Carletta. 1996. Assessing agreement on clas-
sification tasks: The kappa statistic. Computa-
tional Linguistics, 22(2):249?254.
William Cohen. 1996. Learning trees and rules
with set-valued features. In Fourteenth Confer-
ence of the American Association of Articial In-
telligence. AAAI.
Derek Lam, Steven L. Rohall, Chris Schmandt, and
Mia K. Stern. 2002. Exploiting e-mail structure
to improve summarization. In ACM 2002 Confer-
ence on Computer Supported Cooperative Work
(CSCW2002), Interactive Posters, New Orleans,
LA.
Smaranda Muresan, Evelyne Tzoukermann, and Ju-
dith Klavans. 2001. Combining Linguistic and
Machine Learning Techniques for Email Sum-
marization. In Proceedings of the CoNLL 2001
Workshop at the ACL/EACL 2001 Conference.
Ani Nenkova and Amit Bagga. 2003. Facilitating
email thread access by extractive summary gen-
eration. In Proceedings of RANLP, Bulgaria.
Paula Newman and John Blitzer. 2003. Summariz-
ing archived discussions: a beginning. In Pro-
ceedings of Intelligent User Interfaces.
Owen Rambow, Lokesh Shrestha, John Chen, and
Christy Lauridsen. 2004. Summarizaing email
threads. In Proceedings of HLT-NAACL 2004:
Short Papers.
Klaus Zechner and Alon Lavie. 2001. Increasing
the coherence of spoken dialogue summaries by
cross-speaker information linking. In Proceed-
ings of the NAACL-01 Workshop on Automatic
Summarization, Pittsburgh, PA.
Summarizing Email Threads
Owen Rambow Lokesh Shrestha John Chen Chirsty Lauridsen
Columbia University Columbia University Microsoft Research Asia Columbia University
New York, NY, USA New York, NY, USA Beijing, China New York, NY, USA
rambow@cs.columbia.edu, lokesh@cs.columbia.edu
v-johnc@msrchina.research.microsoft.com, christy@columbia.edu
Abstract
Summarizing threads of email is different from
summarizing other types of written communi-
cation as it has an inherent dialog structure. We
present initial research which shows that sen-
tence extraction techniques can work for email
threads as well, but profit from email-specific
features. In addition, the presentation of the
summary should take into account the dialogic
structure of email communication.
1 Introduction
In this paper, we discuss work on summarizing email
threads, i.e., coherent exchanges of email messages
among several participants.1 Email is a written medium
of asynchronous multi-party communication. This means
that, unlike for example news stories but as in face-to-face
spoken dialog, the email thread as a whole is a collabo-
rative effort with interaction among the discourse partici-
pants. However, unlike spoken dialog, the discourse par-
ticipants are not physically co-present, so that the writ-
ten word is the only channel of communication. Fur-
thermore, replies do not happen immediately, so that re-
sponses need to take special precautions to identify rele-
vant elements of the discourse context (for example, by
citing previous messages). Thus, email is a distinct lin-
guistic genre that poses its own challenges to summariza-
tion.
In the approach we propose in this paper, we follow
the paradigm used for other genres of summarization,
namely sentence extraction: important sentences are ex-
tracted from the thread and are composed into a summary.
Given the special characteristics of email, we predict that
certain email-specific features can help in identifying rel-
evant sentences for extraction. In addition, in presenting
the extracted summary, special ?wrappers? ensure that
1The work reported in this paper was funded under the KDD
program. We would like to thank three anonymous reviewers
for very insightful and helpful comments.
the reader can reconstruct the interactional aspect of the
thread, which we assume is crucial for understanding the
summary. We acknowledge that other techniques should
also be explored for email summarization, but leave that
to separate work.
2 Previous and Related Work
Muresan et al (2001) describe work on summarizing in-
dividual email messages using machine learning ap-
proaches to learn rules for salient noun phrase extraction.
In contrast, our work aims at summarizing whole threads
and at capturing the interactive nature of email.
Nenkova and Bagga (2003) present work on generat-
ing extractive summaries of threads in archived discus-
sions. A sentence from the root message and from each
response to the root extracted using ad-hoc algorithms
crafted by hand. This approach works best when the sub-
ject of the root email best describes the ?issue? of the
thread, and when the root email does not discuss more
than one issue. In our work, we do not make any assump-
tions about the nature of the email, and learn sentence
extraction strategies using machine learning.
Newman and Blitzer (2003) also address the problem
of summarizing archived discussion lists. They cluster
messages into topic groups, and then extract summaries
for each cluster. The summary of a cluster is extracted
using a scoring metric based on sentence position, lexical
similarity of a sentence to cluster centroid, and a feature
based on quotation, among others. While the approach is
quite different from ours (due to the underlying clustering
algorithm and the absence of machine learning to select
features), the use of email-specific features, in particular
the feature related to quoted material, is similar.
Lam et al (2002) present work on email summariza-
tion by exploiting the thread structure of email conver-
sation and common features such as named entities and
dates. They summarize the message only, though the con-
tent of the message to be summarized is ?expanded? us-
ing the content from its ancestor messages. The expanded
message is passed to a document summarizer which is
used as a black box to generate summaries. Our work, in
contrast, aims at summarizing the whole thread, and we
are precisely interested in changing the summarization al-
gorithm itself, not in using a black box summarizer.
In addition, there has been some work on summarizing
meetings. As discussed in Section 1, email is different
in important respects from (multi-party) dialog. How-
ever, some important aspects are related. Zechner (2002),
for example, presents a meeting summarization system
which uses the MMR algorithm to find sentences that are
most similar to the segment and most dissimilar to each
other. The similarity weights in the MMR algorithm are
modified using three features, including whether a sen-
tence belongs to a question-answer pair. The use of the
question-answer pair detection is an interesting proposal
that is also applicable to our work. However, overall most
of the issues tackled by Zechner (2002) are not relevant
to email summarization.
3 The Data
Our corpus consists of 96 threads of email sent during
one academic year among the members of the board of
the student organization of the ACM at Columbia Uni-
versity. The emails dealt mainly with planning events of
various types, though other issues were also addressed.
On average, each thread contained 3.25 email messages,
with all threads containing at least two messages, and the
longest thread containing 18 messages.
Two annotators each wrote a summary of the thread.
We did not provide instructions about how to choose con-
tent for the summaries, but we did instruct the annotators
on the format of the summary; specifically, we requested
them to use the past tense, and to use speech-act verbs and
embedded clauses (for example, Dolores reported she?d
gotten 7 people to sign up instead of Dolores got 7 peo-
ple to sign up). We requested the length to be about 5%
to 20% of the original text length, but not longer than 100
lines.
Writing summaries is not a task that competent native
speakers are necessarily good at without specific train-
ing. Furthermore, there may be many different possible
summary types that address different needs, and differ-
ent summaries may satisfy a particular need. Thus, when
asking native speakers to write thread summaries we can-
not expect to obtain summaries that are similar.
We then used the hand-written summaries to identify
important sentences in the threads in the following man-
ner. We used the sentence-similarity finder SimFinder
(Hatzivassiloglou et al, 2001) in order to rate the sim-
ilarity of each sentence in a thread to each sentence in
the corresponding summary. SimFinder uses a combi-
nation of lexical and linguistic features to assign a sim-
ilarity score to an input pair of texts. We excluded sen-
tences that are being quoted, as well as signatures and
the like. For each sentence in the thread, we retained
the highest similarity score. We then chose a threshold;
sentences with SimFinder scores above this threshold are
then marked as ?Y?, indicating that they should be part
of a summary, while the remaining sentences are marked
?N?. About 26% of sentences are marked ?Y?. All sen-
tences from the email threads along with their classifica-
tion constitutes our data. For annotator DB, we have 1338
sentences, of which 349 are marked ?Y?, for GR (who
has annotated a subset of the threads that DB has anno-
tated) there are 1296 sentences, of which 336 are marked
?Y?. Only 193 sentences are marked ?Y? using the sum-
maries of both annotators, reflecting the difference in the
summaries written by the two annotators. The kappa for
the marking of the sentences is 0.29 (recall that this only
indirectly reflects annotator choice). Thus, our expecta-
tion that human-written summaries will show great vari-
ation was borne out; we discuss these differences further
in Section 5.
4 Features for Sentence Extraction
We start out with features that are not specific to email.
These features consider the thread as a single text. We
call this feature set basic. Each sentence in the email
thread is represented by a feature vector. We shall call
the sentence in consideration s, the message in which the
sentence appears m, the thread in which the sentence ap-
pears t, and the entire corpus c. (We omit some features
we use for lack of space.)
? thread line num: The absolute position of s in t.
? centroid sim: Cosine similarity of s?s TF-IDF vec-
tor (excluding stop words) with t?s centroid vector.
The centroid vector is the average of the TF-IDF
vectors of all the sentences in t. The IDF compo-
nent is derived from the ACM Corpus.
? centroid sim local: Same as centroid sim except
that the inverse document frequencies are derived
from the thread.
? length: The number of content terms in s.
? tfidfsum: Sum of the TF-IDF weights of content
terms in s. IDF weights are derived from c.
? tfidfavg: Average TF-IDF weight of the content
terms in s. IDF weights are derived from c.
? t rel pos: Relative position of s in t: the number of
sentences preceding s divided by the total number of
sentences in t. All messages in a thread are ordered
linearly by the time they were sent.
? is Question: Whether s is a question, as determined
by punctuation.
Ann. Feature set ctroid basic basic+ full
DB Recall 0.255 0.315 0.370 0.421
DB Precision 0.298 0.553 0.584 0.607
DB F-measure 0.272 0.401 0.453 0.497
GR Recall 0.291 0.217 0.193 0.280
GR Precision 0.333 0.378 0.385 0.475
GR F-measure 0.311 0.276 0.257 0.352
Figure 1: Results for annotators DB and GR using differ-
ent feature sets
We now add two features that take into account the di-
vision of the thread into messages and the resulting dia-
log structure. The union of this feature set with basic is
called basic+.
? msg num: The ordinality of m in t (i.e., the absolute
position of m in t).
? m rel pos: Relative position of s in m: the number
of sentences preceding s divided by the total number
of sentences in m.
Finally, we add features which address the specific
structure of email communication. The full feature set
is called full.
? subject sim: Overlap of the content words of the
subject of the first message in t with the content
words in s.
? num of res: Number of direct responses to m.
? num Of Recipients: Number of recipients of m.
? fol Quote: Whether s follows a quoted portion in m.
5 Experiments and Results
This section describes experiments using the machine
learning program Ripper (Cohen, 1996) to automatically
induce sentence classifiers, using the features described
in Section 4. Like many learning programs, Ripper takes
as input the classes to be learned, a set of feature names
and possible values, and training data specifying the class
and feature values for each training example. In our case,
the training examples are the sentences from the threads
as described in Section 3. Ripper outputs a classifica-
tion model for predicting the class (i.e., whether a sen-
tence should be in a summary or not) of future exam-
ples; the model is expressed as an ordered set of if-then
rules. We obtained the results presented here using five-
fold cross-validation. In this paper, we only evaluate the
results of the machine learning step; we acknowledge the
need for an evaluation of the resulting summaries using
DB only GR only avg max
Recall 0.421 0.280 0.212 0.268
Precision 0.607 0.475 0.406 0.444
F-measure 0.497 0.352 0.278 0.335
Figure 2: Results for combining two annotators (last two
columns) using full feature set
word/string based similarity metric and/or human judg-
ments and leave that to future publications.
We show results for the two annotators and different
feature sets in Figure 1. First consider the results for
annotator DB. Recall that basic includes only standard
features that can be used for all text genres, and consid-
ers the thread a single text. basic+ takes the breakdown
of the thread into messages into account. full also uses
features that are specific to email threads. We can see
that by using more features than the baseline set basic,
performance improves. Specifically, using email-specific
features improves the performance over the basic base-
line, as we expected. We also give a second baseline,
ctroid, which we determined by choosing the top 20% of
sentences most similar to the thread centroid. All results
using Ripper improve on this baseline.
If we perform exactly the same experiments on the
summaries written by annotator GR, we obtain the re-
sults shown in the bottom half of Figure 1. The results are
much worse, and the centroid-based baseline outperforms
all but the full feature set. We leave to further research
an explanation of why this may be the case; we speculate
that GR,as an annotator, is less consistent in her choice of
material than is DB when forming a summary. Thus, the
machine learner has less regularity to learn from. How-
ever, we take this difference as evidence for the claim that
one should not expect great regularity in human-written
summaries.
Finally, we investigated what happens when we com-
bine the data from both sources, DB and GR. Using
SimFinder, we obtained two scores for each sentence, one
that shows the similarity to the most similar sentence in
DB?s summary, and one that shows the similarity to the
most similar sentence in GR?s summary. We can com-
bine these two scores and then use the combined score in
the same way that we used the score from a single anno-
tator. We explore two ways of combining the scores: the
average, and the maximum. Both ways of combining the
scores result in worse scores than either annotator on his
or her own; the average is worse than the maximum (see
Figure 2). We interpret these results again as meaning
that there is little convergence in the human-written sum-
maries, and it may be advantageous to learn from one
particular annotator. (Of course, another option might be
to develop and enforce very precise guidelines for the an-
1 IF centroid sim local ? 0.32215 AND thread line num ? 4 AND isQuestion = 1
AND tfidfavg ? 0.212141 AND tfidfavg ? 0.301707 THEN Y.
2 IF centroid sim ? 0.719594 AND numOfRecipients ? 8 THEN Y.
3 IF centroid sim local ? 0.308202 AND thread line num ? 4 AND tfidfmax ? 0.607829
AND m rel pos ? 0.363636 AND t rel pos ? 0.181818 THEN Y.
4 IF subject sim ? 0.333333 tfidfsum ? 2.83636 tfidfsum ? 2.64262 tfidfmax ? 0.675917 THEN Y.
5 ELSE N.
Figure 3: Sample rule set generated from DB data (simplified for reasons of space)
Regarding ?acm home/bjarney?, on Apr 9, 2001, Muriel
Danslop wrote: Two things: Can someone be responsible
for the press releases for Stroustrup?
Responding to this on Apr 10, 2001, Theresa Feng wrote:
I think Phil, who is probably a better writer than most of
us, is writing up something for dang and Dave to send out
to various ACM chapters. Phil, we can just use that as our
?press release?, right?
In another subthread, on Apr 12, 2001, Kevin Danquoit
wrote: Are you sending out upcoming events for this
week?
Figure 4: Sample summary obtained with the rule set in
Figure 3
notators as to the contents of the summaries.)
A sample rule set obtained from DB data is shown in
Figure 3. Some rules are intuitively appealing: for ex-
ample, rule 1 states that questions at the beginning of a
thread that are similar to entire thread should be retained,
and rule 2 states that sentence which are very similar to
the thread and which have a high number of recipients
should be retained. However, some rules show signs of
overfitting, for example rule 1 limits the average TF-IDF
values to a rather narrow band. Hopefully, more data
will alleviate the overfitting problem. (The data collec-
tion continues.)
6 Postprocessing Extracted Sentences
Extracted sentences are sent to a module that wraps these
sentences with the names of the senders, the dates at
which they were sent, and a speech act verb. The speech
act verb is chosen as a function of the structure of the
email thread in order to make this structure more appar-
ent to the reader. Further, for readability, the sentences
are sorted by the order in which they were sent. An ex-
ample can be seen in Figure 4. Note that while the initial
question is answered in the following sentence, two other
questions are left unanswered in this summary (the an-
swers are in fact in the thread).
7 Future Work
In future work, we will perform a qualitative error anal-
ysis and investigate in more detail what characteristics
of DB?s summaries lead to better extractive summaries.
We can use this insight to instruct human annotators, and
to improve the automatic extraction. We intend to learn
predictors for some other thread aspects such as thread
category and question-answer pairs, and then use these
as input to the sentence extraction procedure. For ex-
ample, identifying question-answer pairs appears to be
important for generating ?complete? summaries, as illus-
trated by the sample summary. We also intend to perform
an evaluation based on human feedback.
References
William Cohen. 1996. Learning trees and rules with
set-valued features. In Fourteenth Conference of the
American Association of Artificial Intelligence. AAAI.
Vasileios Hatzivassiloglou, Judith Klavans, Melissa Hol-
combe, Regina Barzilay, Min-Yen Kan, and Kathleen
McKeown. 2001. SimFinder: A flexible cluster-
ing tool for summarization. In Proceedings of the
NAACL Workshop on Automatic Summarization, Pitts-
burgh, PA.
Derek Lam, Steven L. Rohall, Chris Schmandt, and
Mia K. Stern. 2002. Exploiting e-mail structure to
improve summarization. In ACM 2002 Conference on
Computer Supported Cooperative Work (CSCW2002),
Interactive Posters, New Orleans, LA.
Smaranda Muresan, Evelyne Tzoukermann, and Judith
Klavans. 2001. Combining Linguistic and Ma-
chine Learning Techniques for Email Summarization.
In Proceedings of the CoNLL 2001 Workshop at the
ACL/EACL 2001 Conference.
Ani Nenkova and Amit Bagga. 2003. Facilitating email
thread access by extractive summary generation. In
Proceedings of RANLP, Bulgaria.
Paula Newman and John Blitzer. 2003. Summarizing
archived discussions: a beginning. In Proceedings of
Intelligent User Interfaces.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Computational Linguistics, 28(4):447?485.
