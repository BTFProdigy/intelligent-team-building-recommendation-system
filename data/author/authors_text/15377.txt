Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 89?98, Dublin, Ireland, August 23-29 2014.
Group Non-negative Matrix Factorization with Natural Categories for
Question Retrieval in Community Question Answer Archives
Guangyou Zhou, Yubo Chen, Daojian Zeng, and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,yubo.chen,djzeng,jzhao}@nlpr.ia.ac.cn
Abstract
Community question answering (CQA) has become an important service due to the popularity of
CQA archives on the web. A distinctive feature is that CQA services usually organize questions
into a hierarchy of natural categories. In this paper, we focus on the problem of question re-
trieval and propose a novel approach, called group non-negative matrix factorization with natural
categories (GNMFNC). This is achieved by learning the category-specific topics for each cate-
gory as well as shared topics across all categories via a group non-negative matrix factorization
framework. We derive an efficient algorithm for learning the factorization, analyze its complex-
ity, and provide proof of convergence. Experiments are carried out on a real world CQA data set
from Yahoo! Answers. The results show that our proposed approach significantly outperforms
various baseline methods and achieves the state-of-the-art performance for question retrieval.
1 Introduction
Community question answering (CQA) such as Yahoo! Answers
1
and Quora
2
, has become an important
service due to the popularity of CQA archives on the web. To make use of the large-scale questions and
their answers, it is critical to have functionality of helping users to retrieve previous answers (Duan et
al., 2008). Typically, such functionality is achieved by first retrieving the historical questions that best
match a user?s queried question, and then using answers of these returned questions to answer the queried
question. This is what we called question retrieval in this paper.
The major challenge for question retrieval, as for most information retrieval tasks, is the lexical gap
between the queried questions and the historical questions in the archives. For example, if a queried ques-
tion contains the word ?company? but a relevant historical question instead contains the word ?firm?, then
there is a mismatch and the historical question may not be easily distinguished from an irrelevant one.
To solve the lexical gap problem, most researchers focused on translation-based approaches since the
relationships between words (or phrases) can be explicitly modeled through word-to-word (or phrases)
translation probabilities (Jeon et al., 2005; Riezler et al., 2007; Xue et al., 2008; Lee et al., 2008; Bern-
hard and Gurevych, 2009; Zhou et al., 2011; Singh, 2012). However, these existing methods model the
relevance ranking without considering the category-specific and shared topics with natural categories, it
is not clear whether this information is useful for question retrieval.
A distinctive feature of question-answer pairs in CQA is that CQA services usually organize questions
into a hierarchy of natural categories. For example, Yahoo! Answers contains a hierarchy of 26 categories
at the first level and more than 1262 subcategories at the leaf level. When a user asks a question, the user
is typically required to choose a category label for the question from a predefined hierarchy. Questions in
the predefined hierarchy usually share certain generic topics while questions in different categories have
their specific topics. For example, questions in categories ?Arts & Humanities? and ?Beauty & Style?
may share the generic topic of ?dance? but they also have the category-specific topics of ?poem? and
?wearing?, respectively.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
1
http://answers.yahoo.com/
2
http://www.quora.com/
89
Inspired by the above observation, we propose a novel approach, called group non-negative matrix
factorization with natural categories (GNMFNC). GNMFNC assumes that there exists a set of category-
specific topics for each of the category, and there also exists a set of shared topics for all of the categories.
Each question in CQA is specified by its category label, category-specific topics, as well as shared topics.
In this way, the large-scale question retrieval problem can be decomposed into small-scale subproblems.
In GNMFNC, questions in each category are represented as a term-question matrix. The term-question
matrix is then approximated as the product of two matrices: one matrix represents the category-specific
topics as well as the shared topics, and the other matrix denotes the question representation based on
topics. An objective function is defined to measure the goodness of prediction of the data with the
model. Optimization of the objective function leads to the automatic discovery of topics as well as
the topic representation of questions. Finally, we calculate the relevance ranking between the queried
questions and the historical questions in the latent topic space.
Past studies by (Cao et al., 2009; Cao et al., 2010; Ming et al., 2010; Cai et al., 2011; Ji et al., 2012;
Zhou et al., 2013) confirmed a significant retrieval improvement by adding the natural categories into
various existing retrieval models. However, all these previous work regarded natural categories indi-
vidually without considering the relationships among them. On the contrary, this paper can effectively
capture the relationships between the shared aspects and the category-specific individual aspects with
natural categories via a group non-negative matrix factorization framework. Also, our work models the
relevance ranking in the latent topic space rather than using the existing retrieval models. To date, no at-
tempts have been made regarding group non-negative matrix factorization in studies of question retrieval,
which remains an under-explored area.
The remainder of this paper is organized as follows. Section 2 describes our proposed group non-
negative matrix factorization with natural categories for question retrieval. Section 3 presents the exper-
imental results. In Section 4, we conclude with ideas for future research.
2 Group Non-negative Matrix Factorization with Natural Categories
2.1 Problem Formulation
In CQA, all questions are usually organized into a hierarchy of categories. When a user asks a question,
the user is typically required to choose a category label for the question from a predefined hierarchy of
categories. Hence, each question in CQA has a category label. Suppose that we are given a question col-
lection D in CQA archive with size N , containing terms from a vocabulary V with size M . A question
d is represented as a vector d ? R
M
where each entry denotes the weight of the corresponding term,
for example tf-idf is used in this paper. Let C = {c
1
, c
2
, ? ? ? , c
P
} denote the set of categories (subcat-
egories) of question collection D, where P is the number of categories (subcategories). The question
collection D is organized into P groups according to their category labels and can be represented as
D = {D
1
,D
2
, ? ? ? ,D
P
}. D
p
= {d
(p)
1
, ? ? ? ,d
(p)
N
p
} ? R
M?N
p
is the term-question matrix corresponding
to category c
p
, in which each row stands for a term and each column stands for a question. N
p
is the
number of questions in category c
p
such that
?
P
p=1
N
p
= N .
LetU
?
p
= [U
s
,U
p
] ? R
M?(K
s
+K
p
)
be the term-topic matrix corresponding to category c
p
, where K
s
is the number of shared topics, K
p
is the number of category-specific topics corresponding to category
c
p
, and p ? [1, P ]. Term-topic matrix U
s
can be represented as U
s
= [u
(s)
1
, ? ? ? ,u
(s)
K
s
] ? R
M?K
s
, in
which each column corresponds to a shared topic. While the term-topic matrix U
p
can be represented
as U
p
= [u
(p)
1
, ? ? ? ,u
(p)
K
p
] ? R
M?K
p
. The total number of topics in the question collection D is K =
K
s
+ PK
p
. Let V
p
= [v
(p)
1
, ? ? ? ,v
(p)
N
p
] ? R
(K
s
+K
p
)?N
p
be the topic-question matrix corresponding to
category c
p
, in which each column denotes the question representation in the topic space. We also denote
V
T
p
= [H
T
p
,W
T
p
], where H
p
? R
K
s
?N
p
and W
p
? R
K
p
?N
p
correspond to the coefficients of shared
topicsU
s
and category-specific topicsU
p
, respectively.
Thus, given a question collection D = {D
1
,D
2
, ? ? ? ,D
P
} together with the category labels C =
{c
1
, c
2
, ? ? ? , c
P
}, our proposed GNMFNC amounts to modeling the question collection D with P group
90
simultaneously, arriving at the following objective function:
O =
P
?
p=1
{
?
p
?
?
D
p
? [U
s
,U
p
]V
p
?
?
2
F
+ R(U
s
,U
p
)
}
(1)
where ?
p
, ?D
p
?
?2
F
. R(U
s
,U
p
) is a regularization term used to penalize the ?similarity? between the
shared topics and category-specific topics throughU
s
andU
p
.
In this paper, we aim to ensure that matrix U
s
captures only shared topics and matrix U
p
captures
only the category-specific topics. For example, if matricesU
s
andU
p
are mutually orthogonal, we have
U
T
s
U
p
= 0. To impose this constraint, we attempt to minimize the sum-of-squares of entries of the
matrix U
T
s
U
p
(e.g., ?U
T
s
U
p
?
2
F
which uniformly optimizes each entry of U
T
s
U
p
). With this choice, the
regularization term of R(U
s
,U
p
) is given by
R(U
s
,U
p
) =
P
?
p=1
?
p
?
?
U
T
s
U
p
?
?
2
F
+
P
?
l=1,l?=p
?
l
?
?
U
T
p
U
l
?
?
2
F
(2)
where ?
p
and ?
l
are the regularization parameters, ?p ? [1, P ], ?l ? [1, P ].
Learning the objective function in equation (1) involves the following optimization problem:
min
U
s
,U
p
,V
p
?0
L = O + ?
1
?
?
U
T
s
1
M
? 1
K
s
?
?
2
F
+ ?
2
?
?
U
T
p
1
M
? 1
K
p
?
?
2
F
+ ?
3
?
?
V
p
1
N
p
? 1
K
s
+K
p
?
?
2
F
(3)
where ?
1
, ?
2
and ?
3
are the shrinkage regularization parameters. Based on the shrinkage methodology,
we can approximately satisfy the normalization constraints for each column of [U
s
,U
p
] and V
T
p
by
guaranteeing the optimization converges to a stationary point.
2.2 Learning Algorithm
We present the solution to the GNMFNC optimization problem in equation (3) as the following theorem.
The theoretical aspects of the optimization are presented in the next subsection.
Theorem 2.1. UpdatingU
s
,U
p
andV
p
using equations (4)?(6) corresponds to category c
p
will mono-
tonically decrease the objective function in equation (3) until convergence.
U
s
? U
s
?
[
?
P
p=1
?
p
D
p
H
T
p
]
[
?
P
p=1
?
p
[U
s
,U
p
]V
p
H
T
p
+ ?
p
U
p
U
T
p
U
s
]
(4)
U
p
? U
p
?
[
?
p
D
p
W
T
p
]
[
?
p
[U
s
,U
p
]V
p
W
T
p
+ ?
p
U
s
U
T
s
U
p
+
?
P
l=1,l?=p
?
l
U
l
U
T
l
U
p
]
(5)
V
p
? V
p
?
[
?
p
D
T
p
[U
s
,U
p
]
]
[
?
p
V
T
p
[U
s
,U
p
]
T
[U
s
,U
p
]
]
(6)
where operator ? is element-wise product and
[?]
[?]
is element-wise division.
Based on Theorem 2.1, we note that multiplicative update rules given by equations (4)?(6) are ob-
tained by extending the updates of standard NMF (Lee and Seung, 2001). A number of techniques can
be used here to optimize the objective function in equation (3), such as alternating least squares (Kim
and Park, 2008), the active set method (Kim and Park, 2008), and the projected gradients approach (Lin,
2007). Nonetheless, the multiplicative updates derived in this paper have reasonably fast convergence
behavior as shown empirically in the experiments.
2.3 Theoretical Analysis
In this subsection, we give the theoretical analysis of the optimization, convergence and computational
complexity.
91
Without loss of generality, we only show the optimization ofU
s
and formulate the Lagrange function
with constraints as follows:
L(U
s
) = O + ?
1
?
?
U
T
s
1
M
? 1
K
s
?
?
2
F
+ Tr(?
s
U
T
s
)
(7)
where Tr(?) denotes the trace of a matrix, ?
s
? R
K
s
?K
s
is the Lagrange multiplier for the nonnegative
constraintU
s
? 0.
The partial derivative of L(U
s
) w.r.t. U
s
is
?
U
s
L(U
s
) = ?2
P
?
p=1
?
p
D
p
H
T
p
+ 2
P
?
p=1
?
p
[U
s
,U
p
]V
p
H
T
p
+ 2
P
?
p=1
?
p
U
p
U
T
p
U
s
+ 2?
1
U
s
? 2?
1
+ ?
s
(8)
Using the Karush-Kuhn-Tucker (KKT) (Boyd and Vandenberghe, 2004) condition ?
s
?U
s
= 0, we
obtain
?
U
s
L(U
s
) ?U
s
=
{
?
?
P
p=1
?
p
D
p
H
T
p
+
?
P
p=1
?
p
[U
s
,U
p
]V
p
H
T
p
+
?
P
p=1
?
p
U
p
U
T
p
U
s
+ ?
1
U
s
? ?
1
}
?U
s
= 0 (9)
After normalization ofU
s
, the terms ?
1
U
s
and ?
1
are in fact equal. They can be safely ignored from
the above formula without influencing convergence. This leads to the updating rule for U
s
in equation
(4). Following the similar derivations as shown above, we can obtain the updating rules for the rest
variablesU
p
andV
p
in GNMFNC optimization, as shown in equations (5) and (6).
2.3.1 Convergence Analysis
In this subsection, we prove the convergence of multiplicative updates given by equations (4)?(6). We
first introduce the definition of auxiliary function as follows.
Definition 2.1. F(X,X
?
) is an auxiliary function for L(X) if L(X) ? F(X,X
?
) and equality holds if
and only if L(X) = F(X,X).
Lemma 2.1. (Lee and Seung, 2001) If F is an auxiliary function for L, L is non-increasing under the
update
X
(t+1)
= argmin
X
F(X,X
(t)
)
Proof. By Definition 2.1, L(X
(t+1)
) ? F(X
(t+1)
,X
(t)
) ? F(X
(t)
,X
(t)
) = L(X
(t)
)
Theorem 2.2. Let L(U
(t+1)
s
) denote the sum of all terms in L that containU
(t+1)
s
, the following function
is an auxiliary function for L(U
(t+1)
s
)
F(U
(t+1)
s
,U
(t)
s
) = L(U
(t)
s
) + (U
(t+1)
s
?U
(t)
s
)?
U
(t)
s
L(U
(t)
s
) +
1
2
(U
(t+1)
s
?U
(t)
s
)
2
P(U
(t)
s
)
(10)
P(U
(t)
s
) =
?
ij
[
?
P
p=1
?
p
[U
(t)
s
,U
p
]V
p
W
T
p
+ ?
p
U
p
U
T
p
U
(t)
s
+ ?
1
U
(t)
s
]
ij
?
ij
[U
(t)
s
]
ij
where ?
U
(t)
s
L(U
(t)
s
) is the first-order derivative of L(U
(t)
s
) with respect toU
(t)
s
. Theorem 2.2 can be
proved similarly to (Lee and Seung, 2001) by validating L(U
(t+1)
s
) ? F(U
(t+1)
s
,U
(t)
s
), L(U
(t+1)
s
) =
F(U
(t+1)
s
,U
(t+1)
s
), and the Hessian matrix ??
U
(t+1)
s
F(U
(t+1)
s
,U
(t)
s
) ? 0. Due to limited space, we
omit the details of the validation.
92
addition multiplication division overall
GNMFNC:U
s
P (3MN
p
K
s
+MN
p
K
p
+MK
2
s
) P (3MN
p
K
s
+MN
p
K
p
+MK
2
s
) MK
s
O(PMN
p
K
max
)
GNMFNC:U
p
3MN
p
K
p
+MN
p
K
s
+ PM
2
K
?
3MN
p
K
p
+MN
p
K
s
+ PM
2
K
?
MK
p
O(PMRK
?
)
GNMFNC:V
p
3MN
p
K
?
3MN
p
K
?
N
p
K
?
O(MN
p
K
?
)
Table 1: Computational operation counts for each iteration in GNMFNC.
Based on Theorem 2.2, we can fixU
(t)
s
and minimize F(U
(t+1)
s
,U
(t)
s
) with respect toU
(t+1)
s
. When
setting ?
U
(t+1)
s
F(U
(t+1)
s
,U
(t)
s
) = 0, we get the following updating rule
U
(t+1)
s
? U
(t)
s
?
[
?
P
p=1
?
p
D
p
H
T
p
+ ?
1
]
[
?
P
p=1
?
p
[U
(t)
s
,U
p
]V
p
W
T
p
+ ?
p
U
p
U
T
p
U
(t)
s
+ ?
1
U
(t)
s
]
(11)
which is consistent with the updating rule derived from the KKT conditions aforementioned.
By Lemma 2.1 and Theorem 2.2, we have L(U
(0)
s
) = F(U
(0)
s
,U
(0)
s
) ? F(U
(1)
s
,U
(0)
s
) ?
F(U
(1)
s
,U
(1)
s
) = L(U
(1)
s
) ? ? ? ? ? L(U
(Iter)
s
), where Iter is the number of iterations. Therefore,
U
s
is monotonically decreasing. Since the objective function L is lower bounded by 0, the correctness
and convergence of Theorem 2.1 is validated.
2.3.2 Computational Complexity
In this subsection, we discuss the time computational complexity of the proposed algorithm GNMFNC.
Besides expressing the complexity of the algorithm using big O notation, we also count the number of
arithmetic operations to provide more details about running time. We show the results in Table 1, where
K
max
= max{K
s
,K
p
}, K
?
= K
s
+ K
p
and R = max{M,N
p
}.
Suppose the multiplicative updates stop after Iter iterations, the time cost of multiplicative updates
then becomes O(Iter ? PMRK
?
). We set Iter = 100 empirically in rest of the paper. Therefore, the
overall running time of GNMFNC is linear with respect to the size of word vocabulary, the number of
questions and categories.
2.4 Relevance Ranking
The motivation of incorporating matrix factorization into relevance ranking is to learn the word rela-
tionships and reduce the ?lexical gap? (Zhou et al., 2013a). To do so, given a queried question q with
category label c
p
from Yahoo! Answers, we first represent it in the latent topic space as v
q
,
v
q
= argmin
v?0
?q? [U
s
,U
p
]v?
2
2
(12)
where vector q is the tf-idf representation of queried question q in the term space.
For each historical question d (indexed by r) in question collection D, with representation v
d
= r-th
column ofV, we compute its similarity with queried question v
q
as following
s
topic
(q, d) =
< v
q
,v
d
>
?v
q
?
2
? ?v
d
?
2
(13)
The latent topic space score s
topic
(q, d) is combined with the conventional term matching score
s
term
(q, d) for final relevance ranking. There are several ways to conduct the combination. Linear
combination is a simple and effective way. The final relevance ranking score s(q, d) is:
s(q, d) = ?s
topic
(q, d) + (1? ?)s
term
(q, d) (14)
where ? ? [0, 1] is the parameter which controls the relative importance of the latent topic space score
and term matching score. s
term
(q, d) can be calculated with any of the conventional relevance models
such as BM25 (Robertson et al., 1994) and LM (Zhai and Lafferty, 2001).
93
3 Experiments
3.1 Data Set and Evaluation Metrics
We collect the data set from Yahoo! Answers and use the getByCategory function provided in Yahoo!
Answers API
3
to obtain CQA threads from the Yahoo! site. More specifically, we utilize the resolved
questions and the resulting question repository that we use for question retrieval contains 2,288,607 ques-
tions. Each resolved question consists of four parts: ?question title?, ?question description?, ?question
answers? and ?question category?. We only use the ?question title? and ?question category? parts, which
have been widely used in the literature for question retrieval (Cao et al., 2009; Cao et al., 2010). There
are 26 first-level categories in the predefined natural hierarchy, i.e., each historical question is categorized
into one of the 26 categories. The categories include ?Arts & Humanities?, ?Beauty & Style?, ?Business
& Finance?, etc.
In order to evaluate our approach, we randomly select 2,000 questions as queried questions from the
above data collection to construct the validation/test sets, and the remaining data collection as training
set. Note that we select the queried questions in proportion to the number of questions and categories
against the whole distribution to have a better control over a possible imbalance. To obtain the ground-
truth, we employ the Vector Space Model (VSM) (Salton et al., 1975) to retrieve the top 10 results and
obtain manual judgements. The top 10 results don?t include the queried question itself. Given a returned
result by VSM, an annotator is asked to label it with ?relevant? or ?irrelevant?. If a returned result
is considered semantically equivalent to the queried question, the annotator will label it as ?relevant?;
otherwise, the annotator will label it as ?irrelevant?. Two annotators are involved in the annotation
process. If a conflict happens, a third person will make judgement for the final result. In the process
of manually judging questions, the annotators are presented only the questions. As a result, there are in
total 20,000 judged question pairs. We randomly split the 2,000 queried questions into validation/test
sets, each has 1,000/1,000 queried questions. We use the validation set for parameter tuning and the test
set for evaluation.
Evaluation Metrics: We evaluate the performance of question retrieval using the following metrics:
Mean Average Precision (MAP) and Precision@N (P@N). MAP rewards methods that return relevant
questions early and also rewards correct ranking of the results. P@N reports the fraction of the top-N
questions retrieved that are relevant. We perform a significant test, i.e., a t-test with a default significant
level of 0.05.
There are several parameters used in the paper, we tune these parameters on the validation set.
Specifically, we set the number of category-specific topics per category and the number of shared
topics in GNMFNC as (K
s
,K
p
) = {(5, 2), (10, 4), (20, 8), (40, 16), (80, 32)}, resulting in K =
{57, 114, 228, 456, 912} total number of topics. (Note that the total number of topics in GNMFNC
is K
s
+ 26 ?K
p
, where 26 is the number of categories in the first-level predefined natural hierarchy
4
).
Finally, we set (K
s
,K
p
) = (20, 8) and K = 228 empirically as this setting yields the best performance.
For regularization parameters ?
p
and ?
l
, it is difficult to directly tune on the validation set, we present
an alternative way by adding a common factor a to look at the objective function of optimization problem
in equation (3) on the training data. In other words, we set ?
p
=
a
K
s
?K
p
and ?
l
=
a
K
p
?K
l
. Therefore, we
tune the parameters ?
p
and ?
l
by alternatively adjusting the common factor a via grid search. As a result,
we set a = 100, resulting in ?
p
= ?
l
= 0.625 in the following experiments. The trade-off parameter ?
in the linear combination is set from 0 to 1 in steps of 0.1 for all methods. We set ? = 0.6 empirically.
For shrinkage regularization parameters, we empirically set ?
1
= ?
2
= ?
3
= 1.
3.2 Question Retrieval Results
In this experiment, we present the experimental results for question retrieval on the test data set. Specif-
ically, for our proposed GNMFNC, we combine the latent topic matching scores with the term matching
scores given by BM25 and LM, denoted as ?BM25+GNMFNC? and ?LM+GNMFNC?. Table 2 shows
3
http://developer.yahoo.com/answers
4
Here we do not use the leaf categories because we find that it is not possible to run GNMFNC with such large number of
topics on the current machines, and we will leave it for future work.
94
Table 2: Comparison with different methods
for question retrieval.
# Methods MAP P@10
1 BM25 0.243 0.225
2 LM 0.286 0.232
3 (Jeon et al., 2005) 0.327 0.235
4 (Xue et al., 2008) 0.341 0.238
5 (Zhou et al., 2011) 0.365 0.243
6 (Singh, 2012) 0.354 0.240
7 (Cao et al., 2010) 0.358 0.242
8 (Cai et al., 2011) 0.331 0.236
9 BM25+GNMFNC 0.369 0.248
10 LM+GNMFNC 0.374 0.251
Table 3: Comparison of matrix factoriza-
tions for question retrieval.
# Methods MAP P@10
1 BM25 0.243 0.225
2 BM25+NMF 0.325 0.235
3 BM25+CNMF 0.344 0.239
4 BM25+GNMF 0.361 0.242
5 BM25+GNMFNC 0.369 0.248
6 LM 0.286 0.232
7 LM+NMF 0.337 0.237
8 LM+CNMF 0.352 0.240
9 LM+GNMF 0.365 0.243
10 LM+GNMFNC 0.374 0.251
the main retrieval performances under the evaluation metrics MAP, P@1 and P@10. Row 1 and row
2 are the baseline systems, which model the relevance ranking using BM25 (Robertson et al., 1994)
and language model (LM) (Zhai and Lafferty, 2001) in the term space. Row 3 is word-based transla-
tion model (Jeon et al., 2005), and row 4 is word-based translation language model (TRLM) (Xue et
al., 2008). Row 5 is phrase-based translation model (Zhou et al., 2011), and row 6 is the entity-based
translation model (Singh, 2012). Row 7 to row 11 explore the natural categories for question retrieval.
In row 7, Cao et al. (2010) employed the natural categories to compute the local and global relevance
with different model combination, here we use the combination VSM + TRLM for comparison because
this combination obtains the superior performance than others. In row 8, Cai et al. (2011) proposed a
category-enhanced TRLM for question retrieval. There are some clear trends in the results of Table 2:
(1) BM25+GNMFNC and LM+GNMFNC perform significantly better than BM25 and LM respec-
tively (t-test, p-value < 0.05, row 1 vs. row 9; row 2 vs. row 10), indicating the effective of GNMFNC.
(2) BM25+GNMFNC and LM+GNMFNC perform better than translation methods, some improve-
ments are statistical significant (t-test, p-value < 0.05, row 3 and row 4 vs. row 9 and row 10). The
reason may be that GNMFNC models the relevance ranking in the latent topic space, which can also
effectively solve the the lexical gap problem.
(3) Capturing the shared aspects and the category-specific individual aspects with natural categories
in the group modeling framework can significantly improve the performance of question retrieval (t-test,
p-value < 0.05, row 7 and row 8 vs. row 9 and row 10).
(4) Natural categories are useful and effectiveness for question retrieval, no matter in the group mod-
eling framework or existing retrieval models (row 3? row 6 vs. row 7?row 10).
3.3 Comparison of Matrix Factorizations
We note that our proposed GNMFNC is related to non-negative matrix factorization (NMF) (Lee and
Seung, 2001) and its variants, we introduce three baselines. The first baseline is NMF, which is trained
on the whole training data. The second baseline is CNMF, which is trained on each category without
considering the shared topics. The third baseline is GNMF (Lee and Choi, 2009; Wang et al., 2012),
which is similar to our GNMFNC but there are no constraints on the category-specific topics to prevent
them from capturing the information from the shared topics.
NMF and GNMF are trained on the training data with the same parameter settings in section 4.1 for
fair comparison. For CNMF, we also train the model on the training data with the same parameter settings
in section 4.1, except parameter K
s
, as there exists no shared topics in CNMF.
Table 3 shows the question retrieval performance of NMF families on the test set, obtained with the
best parameter settings determined by the validation set. From the results, we draw the following obser-
vations:
(1) All of these methods can significantly improve the performance in comparison to the baseline
BM25 and LM (t-test, p-value < 0.05).
(2) GNMF and GNMFNC perform significantly better than NMF and CNMF respectively (t-test, p-
value < 0.05), indicating the effectiveness of group matrix factorization framework, especially the use
of shared topics.
95
0 20 40 60 80 1000.41
0.42
0.43
0.44
0.45
0.46
0.47
0.48
0.49
0.5
Iteration number
Obje
ctive 
funct
ion v
alue
Figure 1: Convergence curve of GNMFNC.
-4 -3 -2 -1 0 1 2 3 40.414
0.416
0.418
0.42
0.422
0.424
0.426
0.428
0.43
Log10a
Conv
erged
 obje
ctive 
funct
ion v
alue
Figure 2: Objective function value vs. factor a.
(3) GNMFNC performs significantly better than GNMF (t-test, p-value < 0.05, row 4 vs. row 5; row
9 vs. row 10), indicating the effectiveness of the regularization term on the category-specific topics to
prevent them from capturing the information from the shared topics.
From the experimental results reported above, we can conclude that our proposed GNMFNC is useful
for question retrieval with high accuracies. To the best of our knowledge, it is the first time to investigate
the group matrix factorization for question retrieval.
3.4 Convergence Behavior
In subsection 2.3.1, we have shown that the multiplicative updates given by equations (4)?(6) are con-
vergent. Here, we empirically show the convergence behavior of GNMFNC.
Figure 1 shows the convergence curve of GNMFNC on the training data set. From the figure, y-axis is
the value of objective function and x-axis denotes the iteration number. We can see that the multiplicative
updates for GNMFNC converge very fast, usually within 80 iterations.
3.5 Regularization Parameters Selection
One success of this paper is to use regularized constrains on the category-specific topics to prevent them
from capturing the information from the shared topics. It is necessary to give an in-depth analysis of
the regularization parameters used in the paper. Consider the regularization term used in equation (2),
each element in U
T
s
U
p
and U
T
p
U
l
has a value between 0 and 1 as each column of U
s
, U
p
and U
l
is
normalized. Therefore, it is appropriate to normalize the term having ?U
T
s
U
p
?
2
F
by K
s
K
p
since there
are K
s
?K
p
elements inU
T
s
U
p
. Similarly, ?U
T
p
U
l
?
2
F
is normalized by K
l
K
p
. Note that K
l
= K
p
and
l ?= p. As discussed in subsection 4.1, we present an alternative way by adding a common factor a and
set ?
p
=
a
K
s
?K
p
and ?
l
=
a
K
p
?K
l
. The common factor a is used to adjust a trade-off between the matrix
factorization errors and the mutual orthogonality, which cannot directly tune on the validation set. Thus,
we look at the objective function of optimization problem in equation (3) on the training data and find
the optimum value for a.
Figure 2 shows the objective function value vs. common factor a, where y-axis denotes the converged
objective function value, and x-axis denotes Log
10
a . We can see that the optimum value of a is 100.
Therefore, the common factor a can be fixed at 100 for our data set used in the paper, resulting in
?
p
= ?
l
= 0.625. Note that the optimum value of (K
s
,K
p
) are set as (20, 8) in subsection 4.1. Due to
limited space, we do not give an in-depth analysis for other parameters.
4 Conclusion and Future Work
In this paper, we propose a novel approach, called group non-negative matrix factorization with natural
categories (GNMFNC). The proposed method is achieved by learning the category-specific topics for
each category as well as shared topics across all categories via a group non-negative matrix factorization
framework. We derive an efficient algorithm for learning the factorization, analyze its complexity, and
96
provide proof of convergence. Experiments show that our proposed approach significantly outperforms
various baseline methods and achieves state-of-the-art performance for question retrieval.
There are some ways in which this research could be continued. First, the optimization of GNMFNC
can be decomposed into many sub-optimization problems, a natural avenue for future research is to
reduce the running time by executing the optimization in a distributed computing environment (e.g.,
MapReduce (Dean et al., 2004)). Second, another combination approach will be used to incorporate the
latent topic match score as a feature in a learning to rank model, e.g., LambdaRank (Burges et al., 2007).
Third, we will try to investigate the use of the proposed approach for other kinds of data sets with larger
categories, such as categorized documents from ODP project.
5
Acknowledgments
This work was supported by the National Natural Science Foundation of China (No. 61333018 and
No. 61303180), the Beijing Natural Science Foundation (No. 4144087), CCF Opening Project of Chi-
nese Information Processing, and also Sponsored by CCF-Tencent Open Research Fund. We thank the
anonymous reviewers for their insightful comments.
References
D. Bernhard and I. Gurevych. 2009. Combining lexical semantic resources with question & answer archives for
translation-based answer finding. In Proceedings of ACL, pages 728-736.
S. Boyd and L. Vandenberghe. 2004. Convex Optimization. Cambridge university press.
C. Boutsidis and E. Gallopoulos. 2008. SVD based initialization: a head start for nonnegative matrix factorization.
Pattern Recognition, 41(4):1350-1362.
C. Burges, R. Ragno, and Q. Le. 2007. Learning to rank with nonsmooth cost function. In Proceedings of NIPS.
L. Cai, G. Zhou, K. Liu, and J. Zhao. 2011. Learning the latent topics for question retrieval in community QA. In
Proceedings of IJCNLP.
X. Cao, G. Cong, B. Cui, C. Jensen, and C. Zhang. 2009. The use of categorization information in language
models for question retrieval. In Proceedings of CIKM, pages 265-274.
X. Cao, G. Cong, B. Cui, and C. Jensen. 2010. A generalized framework of exploring category information for
question retrieval in community question answer archives. In Proceedings of WWW.
J. Dean, S. Ghemanwat, and G. Inc. 2004. Mapreduce: simplified data processing on large clusters. In Proceed-
ings of OSDI.
H. Duan, Y. Cao, C. Lin, and Y. Yu. 2008. Searching questions by identifying questions topics and question focus.
In Proceedings of ACL, pages 156-164.
J. Jeon, W. Croft, and J. Lee. 2005. Finding similar questions in large question and answer archives. In Proceed-
ings of CIKM, pages 84-90.
Z. Ji, F. Xu, and B. Wang. 2012. A category-integrated language model for question retrieval in community
question answering. In Proceedings of AIRS, pages 14-25.
H. Kim and H. Park. 2008. Non-negative matrix factorization based on alternating non-negativity constrained
least squares and active set method. SIAM J Matrix Anal Appl, 30(2):713-730.
A. Langville, C. Meyer, R. Albright, J. Cox, and D. Duling. 2006. Initializations for the nonnegative matrix
factorization. In Proceedings of KDD.
J. Lee, S. Kim, Y. Song, and H. Rim. 2008. Bridging lexical gaps between queries and questions on large online
Q&A collections with compact translation models. In Proceedings of EMNLP, pages 410-418.
D. Lee and H. Seung. 2001. Algorithms for non-negative matrix factorization. In Proceedings of NIPS.
5
http://www.dmoz.org/
97
H. Lee and S. Choi. 2009. Group nonnegative matrix factorization for eeg classification. In Proceedings of
AISTATS, pages 320-327.
C. Lin. 2007. Projected gradient methods for nonnegative matrix factorization. Neural Comput, 19(10):2756-
2779.
Z. Ming, T. Chua, and G. Cong. 2010. Exploring domain-specific term weight in archived question search. In
Proceedings of CIKM, pages 1605-1608.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proceedings of ACL, pages 464-471.
S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. 1994. Okapi at trec-3. In Proceedings
of TREC, pages 109-126.
G. Salton, A. Wong, and C. Yang. 1975. A vector space model for automatic indexing. Communications of the
ACM, 18(11):613-620.
A. Singh. 2012. Entity based q&a retrieval. In Proceedings of EMNLP-CoNLL, pages 1266-1277.
Q. Wang, Z. Cao, J. Xun, and H. Li. 2012. Group matrix factorizaiton for scalable topic modeling. In Proceedings
of SIGIR.
X. Xue, J. Jeon, and W. Croft. 2008. Retrieval models for question and answer archives. In Proceedings of SIGIR,
pages 475-482.
C. Zhai and J. Lafferty. 2001. A study of smooth methods for language models applied to ad hoc information
retrieval. In Proceedings of SIGIR, pages 334-342.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-based translation model for question retrieval in community
question answer archives. In Proceedings of ACL, pages 653-662.
G. Zhou, F. Liu, Y. Liu, S. He, and J. Zhao. 2013. Statistical machine translation improves question retrieval in
community question answering via matrix factorization. In Proceedings of ACL, pages 852-861.
G. Zhou, Y. Chen, D. Zeng, and J. Zhao. 2013. Toward faster and better retrieval models for question search. In
Proceedings of CIKM, pages 2139-2148.
98
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1331?1340, Dublin, Ireland, August 23-29 2014.
Sentiment Classification with Graph Co-Regularization
Guangyou Zhou, Jun Zhao, and Daojian Zeng
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,jzhao,djzeng}@nlpr.ia.ac.cn
Abstract
Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or neg-
ative) of user-generated sentiment data (e.g., reviews, blogs). To obtain sentiment classifica-
tion with high accuracy, supervised techniques require a large amount of manually labeled data.
The labeling work can be time-consuming and expensive, which makes unsupervised (or semi-
supervised) sentiment analysis essential for this application. In this paper, we propose a novel
algorithm, called graph co-regularized non-negative matrix tri-factorization (GNMTF), from the
geometric perspective. GNMTF assumes that if two words (or documents) are sufficiently close
to each other, they tend to share the same sentiment polarity. To achieve this, we encode the
geometric information by constructing the nearest neighbor graphs, in conjunction with a non-
negative matrix tri-factorization framework. We derive an efficient algorithm for learning the
factorization, analyze its complexity, and provide proof of convergence. Our empirical study on
two open data sets validates that GNMTF can consistently improve the sentiment classification
accuracy in comparison to the state-of-the-art methods.
1 Introduction
Recently, sentiment classification has gained a wide interest in natural language processing (NLP) com-
munity. Methods for automatically classifying sentiments expressed in products and movie reviews can
roughly be divided into supervised and unsupervised (or semi-supervised) sentiment analysis. Super-
vised techniques have been proved promising and widely used in sentiment classification (Pang et al.,
2002; Pang and Lee, 2008; Liu, 2012). However, the performance of these methods relies on manually
labeled training data. In some cases, the labeling work may be time-consuming and expensive. This
motivates the problem of learning robust sentiment classification via unsupervised (or semi-supervised)
paradigm.
A traditional way to perform unsupervised sentiment analysis is the lexicon-based method (Turney,
2002; Taboada et al., 2011). Lexicon-based methods employ a sentiment lexicon to determine overall
sentiment orientation of a document. However, it is difficult to define a universally optimal sentiment
lexicon to cover all words from different domains (Lu et al., 2011a). Besides, most semi-automated
lexicon-based methods yield unsatisfactory lexicons, with either high coverage and low precision or
vice versa (Ng et al., 2006). Thus it is challenging for lexicon-based methods to accurately identify
the overall sentiment polarity of users generated sentiment data. Recently, Li et al. (2009) proposed a
constrained non-negative matrix tri-factorization (CNMTF) approach to sentiment classification, with
a domain-independent sentiment lexicon as prior knowledge. Experimental results show that CNMTF
achieves state-of-the-art performance.
From the geometric perspective, the data points (words or documents) may be sampled from a distribu-
tion supported by a low-dimensional manifold embedded in a high-dimensional space (Cai et al., 2011).
This geometric structure, meaning that two words (or documents) sufficiently close to each other tend to
share the same sentiment polarity, should be preserved during the matrix factorization. Research studies
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
1331
have shown that learning performance can be significantly enhanced in many real applications (e.g., text
mining, computer vision, etc.) if the geometric structure is exploited (Roweis and Saul, 2000; Tenen-
baum et al., 2000). However, CNMTF fails to exploit the geometric structure, it is not clear whether this
geometric information is useful for sentiment classification, which remains an under-explored area. This
paper is thus designed to fill the gap.
In this paper, we propose a novel algorithm, called graph co-regularized non-negative matrix tri-
factorization (GNMTF). We construct two affinity graphs to encode the geometric information under-
lying the word space and the document space, respectively. Intuitively, if two words or documents are
sufficiently close to each other, they tend to share the same sentiment polarity. Taking these two graphs
as co-regularization for the non-negative matrix tri-factorization, leading to the better sentiment polarity
prediction which respects to the geometric structures of the word space and document space. We also de-
rive an efficient algorithm for learning the tri-factorization, analyze its complexity, and provide proof of
convergence. Empirical study on two open data sets shows encouraging results of the proposed method
in comparison to state-of-the-art methods.
The remainder of this paper is organized as follows. Section 2 introduces the basic concept of matrix
tri-factorization. Section 3 describes our graph co-regularized non-negative matrix tri-factorization (GN-
MTF) for sentiment classification. Section 4 presents the experimental results. Section 5 introduces the
related work. In section 6, we conclude the paper and discuss future research directions.
2 Preliminaries
2.1 Non-negative Matrix Tri-factorization
Li et al. (2009) proposed a matrix factorization based framework for unsupervised (or semi-supervised)
sentiment analysis. The proposed framework is built on the orthogonal non-negative matrix tri-
factorization (NMTF) (Ding et al., 2006). In these models, a term-document matrixX = [x
1
, ? ? ? ,x
n
] ?
R
m?n
is approximated by three factor matrices that specify cluster labels for words and documents by
solving the following optimization problem:
min
U,H,V?0
O =
?
?
X?UHV
T
?
?
2
F
+ ?
1
?
?
U
T
U? I
?
?
2
F
+ ?
2
?
?
V
T
V ? I
?
?
2
F
(1)
where ?
1
and ?
2
are the shrinkage regularization parameters, U = [u
1
, ? ? ? ,u
k
] ? R
m?k
+
is the word-
sentiment matrix, V = [v
1
, ? ? ? ,v
n
] ? R
n?k
+
is the document-sentiment matrix, and k is the number of
sentiment classes for documents. Our task is polarity sentiment classification (positive or negative), i.e.,
k = 2. For example,V
i1
= 1 (orU
i1
= 1) represents that the sentiment polarity of document i (or word
i) is positive, andV
i2
= 1 (orU
i2
= 1) represents that the sentiment polarity of document i (or word i)
is negative. V
i?
= 0 (orU
i?
= 0) represents unknown, i.e., the document i (or word i) is neither positive
or negative. H ? R
k?k
+
provides a condensed view of X; ? ? ?
F
is the Frobenius norm and I is a k ? k
identity matrix with all entries equal to 1. Based on the shrinkage methodology, we can approximately
satisfy the orthogonality constraints forU andV by preventing the second and third terms from getting
too large.
2.2 Constrained NMTF
Lexical knowledge in the form of the polarity of words in the lexicon can be introduced in matrix tri-
factorization. By partially specifying word polarity viaU, the lexicon influences the sentiment prediction
V over documents. Following the literature (Li et al., 2009), let U
0
represent lexical prior knowledge
about sentiment words in the lexicon, e.g., if word i is positive (U
0
)
i1
= 1 while if it is negative
(U
0
)
i2
= 1, and if it does not exist in the lexicon (U
0
)
i?
= 0. Li et al. (2009) also investigated that we
had a few documents manually labeled for the purpose of capturing some domain-specific connotations.
LetV
0
denote the manually labeled documents, if the document expresses positive sentiment (V
0
)
ii
= 1,
and (V
0
)
i2
= 1 for negative sentiment. Therefore, the semi-supervised learning with lexical knowledge
can be written as:
min
U,H,V?0
O + ?Tr
[
(U?U
0
)
T
C
u
(U?U
0
)
]
+ ?Tr
[
(V ?V
0
)
T
C
v
(V ?V
0
)
]
(2)
1332
where Tr(?) denotes the trace of a matrix, ? > 0 and ? > 0 are the parameters which control the
contribution of lexical prior knowledge and manually labeled documents. C
u
? {0, 1}
m?m
is a diagonal
matrix whose entry C
u
ii
= 1 if the category of the i-th word is known and C
u
ii
= 0 otherwise. C
v
?
{0, 1}
n?n
is a diagonal matrix whose entry C
v
ii
= 1 if the category of the i-th document is labeled and
C
v
ii
= 0 otherwise.
3 Graph Co-regularized Non-negative Matrix Tri-factorization
In this section, we introduce our proposed graph co-regularized non-negative matrix tri-factorization
(GNMTF) algorithm which avoids this limitation by incorporating the geometrically based co-
regularization.
3.1 Model Formulation
Based on the manifold assumption (Belkin and Niyogi, 2001), if two documents x
i
and x
j
are sufficiently
close to each other in the intrinsic geometric of the documents distribution, then their sentiment polarity
v
i
and v
j
should be close. In order to model the geometric structure, we construct a document-document
graphG
v
. In the graph, nodes represent documents in the corpus and edges represent the affinity between
the documents. The affinity matrixW
v
? R
n?n
of the graph G
v
is defined as
W
v
ij
=
{
cos(x
i
,x
j
) if x
i
? N
p
(x
j
) or x
j
? N
p
(x
i
)
0 otherwise
(3)
where N
p
(x
i
) represents the p-nearest neighbors of document x
i
. Many matrices, e.g., 0-1 weighting,
textual similarity and heat kernel weighting (Belkin and Niyogi, 2001), can be used to obtain nearest
neighbors of a document, and further define the affinity matrix. Since W
v
ij
in our paper is only for
measuring the closeness, we only use the simple textual similarity and do not treat the different weighting
schemes separately due to the limited space. For further information, please refer to (Cai et al., 2011).
Preserving the geometric structure in the document space is reduced to minimizing the following loss
function:
R
v
=
1
2
n
?
i,j=1
?
?
v
i
? v
j
?
?
2
2
W
v
ij
=
n
?
i=1
v
T
i
v
i
D
v
ii
?
n
?
i,j=1
v
T
i
v
j
W
v
ij
= Tr(V
T
D
v
V)? Tr(V
T
W
v
V) = Tr(V
T
L
v
V)
(4)
whereD
v
? R
n?n
is a diagonal matrix whose entries are column (or row, sinceD
v
is symmetric) sums
ofW
v
,D
v
ii
=
?
n
j=1
W
v
ij
, and L
v
= D
v
?W
v
is the Laplacian matrix (Chung, 1997) of the constructed
graph G
v
.
Similarly to document-document geometric structure, if two words w
i
= [x
i1
, ? ? ? ,x
in
] and w
j
=
[x
j1
, ? ? ? ,x
jn
] are sufficiently close to each other in the intrinsic geometric of the words distribution,
then their sentiment polarity u
i
and u
j
should be close. In order to model the geometric structure in the
word space, we construct a word-word graph G
u
. In the graph, nodes represent distinct words and edges
represent the affinity between words. The affinity matrixW
u
? R
m?m
of the graph G
u
is defined as
W
u
ij
=
{
cos(w
i
,w
j
) ifw
i
? N
p
(w
j
) orw
j
? N
p
(w
i
)
0 otherwise
(5)
where N
p
(w
j
) represents the p-nearest neighbor of word w
j
. Here, we represent a term w
j
as a docu-
ment vector [x
j1
, ? ? ? ,x
jn
]. To measure the closeness of two words, a common way is to calculate the
similarity of their vector representations. Although there are several ways (e.g., co-occurrence infor-
mation, semantic similarity computed by WordNet, Wikipedia, or search engine have been empirically
studied in NLP literature (Hu et al., 2009)) to define the affinity matrixW
u
, we do not treat the different
ways separately and leave this investigation for future work.
Preserving the geometric structure in the word space is reduced to minimizing the following loss
function:
R
u
=
1
2
m
?
i,j=1
?
?
u
i
? u
j
?
?
2
2
W
u
ij
= Tr(U
T
L
u
U) (6)
1333
where L
u
= D
u
?W
u
is the Laplacian matrix of the constructed graph G
u
, and D
u
? R
m?m
is a
diagonal matrix whose entries areD
u
ii
=
?
m
j=1
W
u
ij
.
Finally, we treat unsupervised (or semi-supervised) sentiment classification as a clustering problem,
employing lexical prior knowledge and partial manually labeled data to guide the learning process. More-
over, we introduce the geometric structures from both document and word sides as co-regularization.
Therefore, our proposed unsupervised (or semi-supervised) sentiment classification framework can be
mathematically formulated as solving the following optimization problem:
min
U,H,V?0
L =
?
?
X?UHV
T
?
?
2
F
+ ?
1
?
?
U
T
U? I
?
?
2
F
+ ?
2
?
?
V
T
V ? I
?
?
2
F
+ ?Tr
[
(U?U
0
)
T
C
u
(U?U
0
)
]
+ ?Tr(U
T
L
u
U)
+ ?Tr
[
(V ?V
0
)
T
C
v
(V ?V
0
)
]
+ ?Tr(V
T
L
v
V)
(7)
where ? > 0 and ? > 0 are parameters which control the contributions of document space and word
space geometric information, respectively. With the optimization results, the sentiment polarity of a new
document x
i
can be easily inferred by f(x
i
) = argmax
j?{p, n}
V
ij
.
3.2 Learning Algorithm
We present the solution to the GNMTF optimization problem in equation (7) as the following theorem.
The theoretical aspects of the optimization are presented in the next subsection.
Theorem 3.1. Updating U, H and V using equations (8)?(10) will monotonically decrease the objec-
tive function in equation (7) until convergence.
U? U ?
[
XVH
T
+ ?
1
U+ ?C
u
U
0
+ ?W
u
U
]
[
UHV
T
VH
T
+ ?
1
UU
T
U+ ?C
u
U+ ?D
u
U
]
(8)
H? H ?
[
U
T
XV
]
[
U
T
UHV
T
V
]
(9)
V? V ?
[
X
T
UH+ ?
2
V + ?C
v
V
0
+ ?W
v
V
]
[
VH
T
U
T
UH+ ?
2
VV
T
V + ?C
v
V + ?D
v
V
]
(10)
where operator ? is element-wise product and
[?]
[?]
is element-wise division.
Based on Theorem 3.1, we note that the multiplicative update rules given by equations (8)?(10) are
obtained by extending the updates of standard NMTF (Ding et al., 2006). A number of techniques can
be used here to optimize the objective function in equation (7), such as alternating least squares (Kim
and Park, 2008), the active set method (Kim and Park, 2008), and the projected gradients approach (Lin,
2007). Nonetheless, the multiplicative updates derived in this paper has reasonably fast convergence
behavior as shown empirically in the experiments.
3.3 Theoretical Analysis
In this subsection, we give the theoretical analysis of the optimization, convergence and computational
complexity. Without loss of generality, we only show the optimization ofU and formulate the Lagrange
function with constraints as follows:
L(U) =
?
?
X?UHV
T
?
?
2
F
+ ?
1
?
?
U
T
U? I
?
?
2
F
+ ?Tr
[
(U?U
0
)
T
C
u
(U?U
0
)
]
+ Tr(?U
T
)
(11)
where ? is the Lagrange multiplier for the nonnegative constraintU ? 0.
The partial derivative of L(U) w.r.t. U is
?
U
L(U) = ?2XVH
T
+ 2UHV
T
VH
T
+ 2?
1
UU
T
U? 2?
1
U
+ 2?C
u
U? 2?C
u
U
0
+ 2?D
u
U? 2?W
u
U+?
1334
Using the Karush-Kuhn-Tucker (KKT) (Boyd and Vandenberghe, 2004) condition ??U = 0, we can
obtain
?
U
L(U) ?U =
[
UHV
T
VH
T
+ ?
1
UU
T
U+ ?C
u
U+ ?D
u
U
]
?U
?
[
XVH
T
+ ?
1
U+ ?C
u
U
0
+ ?W
u
U
]
?U = 0
This leads to the update rule in equation (8). Following the similar derivations as shown above, we
can obtain the updating rules for all the other variables H and V in GNMTF optimization, as shown in
equations (9) and (10).
3.3.1 Convergence Analysis
In this subsection, we prove the convergence of multiplicative updates given by equations (8)?(10). We
first introduce the definition of auxiliary function as follows.
Definition 3.1. F(Y,Y
?
) is an auxiliary function for L(Y) if L(Y) ? F(Y,Y
?
) and equality holds if
and only if L(Y) = F(Y,Y).
Lemma 3.1. (Lee and Seung, 2001) If F is an auxiliary function for L, L is non-increasing under the
updateY
(t+1)
= argmin
Y
F(Y,Y
(t)
)
Proof. By Definition 3.1, L(Y
(t+1)
) ? F(Y
(t+1)
,Y
(t)
) ? F(Y
(t)
,Y
(t)
) = L(Y
(t)
)
Theorem 3.2. Let function
F(U
ij
,U
(t)
ij
) = L(U
(t)
ij
) + L
?
(U
(t)
ij
)(U
ij
?U
(t)
ij
)
+
[
UHV
T
VH
T
+ ?
1
UU
T
U+ ?C
u
U+ ?D
u
U
]
ij
U
ij
(
U
ij
?U
(t)
ij
)
(12)
be a proper auxiliary function for L(U
ij
), where L
?
(U
ij
) = [?
U
L(U)]
ij
is the first-order derivatives
of L(U
ij
) with respect toU
ij
.
Theorem 3.2 can be proved similarly to (Ding et al., 2006). Due to limited space, we omit the details
of the validation. Based on Lemmas 3.1 and Theorem 3.2, the update rule for U can be obtained by
minimizing F(U
(t+1)
ij
,U
(t)
ij
). When setting ?
U
(t+1)
ij
F(U
(t+1)
ij
,U
(t)
ij
), we can obtain
U
(t+1)
ij
= U
(t)
ij
[
XVH
T
+ ?
1
U+ ?C
u
U
0
+ ?W
u
U
]
ij
[
UHV
T
VH
T
+ ?
1
UU
T
U+ ?C
u
U+ ?D
u
U
]
ij
By Lemma 3.1 and Theorem 3.2, we have L(U
(0)
) = F(U
(0)
,U
(0)
) ? F(U
(1)
,U
(0)
) ?
F(U
(1)
,U
(1)
) = L(U
(1)
) ? ? ? ? ? L(U
(Iter)
), where Iter denotes the number of iteration number.
Therefore, U is monotonically decreasing. Since the objective function L is lower bounded by 0, the
correctness and convergence of Theorem 3.1 is validated.
3.3.2 Time Complexity Analysis
In this subsection, we discuss the time computational complexity of the proposed algorithm GNMTF.
Besides expressing the complexity of the algorithm using big O notation, we also count the number of
arithmetic operations to provide more details about running time. We show the results in Table 1, where
m ? k and n ? k.
Based on the updating rules summarized in Theorem 3.1, it it not hard to count the arithmetic operators
of each iteration in GNMTF. It is important to note thatC
u
is a diagonal matrix, the nonzero elements on
each row of C
u
is 1. Thus, we only need zero addition and mk multiplications to compute C
u
U. Simi-
larly, forC
u
U
0
,C
v
V,C
v
V
0
,D
u
U andD
v
V, we also only need zero addition and mk multiplications
for each of them. Besides, we also note thatW
u
is a sparse matrix, if we use a p-nearest neighbor graph,
the average nonzero elements on each row of W
u
is p. Thus, we only need mpk additions and mpk
multiplications to compute W
u
U. Similarly, for W
v
V, we need the same operation counts as W
u
U.
Suppose the multiplicative updates stop after Iter iterations, the time cost of multiplicative updates then
becomes O(Iter ? mnk). Therefore, the overall running time of GNMTF is similar to the standard
NMTF and CNMTF.
1335
addition multiplication division overall
GNMTF:U 2k
3
+ (2m+ n)k
2
+m(n+ p)k 2k
3
+ (2m+ n)k
2
+m(n+ p+ 7)k mk O(mnk)
GNMTF:H 2k
3
+ (m+ n+ 2)k
2
+mnk 2k
3
+ (m+ n+ 1)k
2
+mnk k
2
O(mnk)
GNMTF:V 2k
3
+ (2n+m)k
2
+ n(m+ p)k 2k
3
+ (2n+m)k
2
+ n(m+ p+ 7)k nk O(mnk)
Table 1: Computational operation counts for each iteration in GNMTF.
4 Experiments
4.1 Data Sets
Sentiment classification has been extensively studied in the literature. Among these, a large majority
proposed experiments performed on the benchmarks made of Movies Reviews (Pang et al., 2002) and
Amazon products (Blitzer et al., 2007).
Movies data This data set has been widely used for sentiment analysis in the literature (Pang et
al., 2002), which consists of 1000 positive and 1000 negative reviews drawn from the IMDB archive of
rec.arts.movies.reviews.newsgroups.
Amazon data This data set is heterogeneous, heavily unbalanced and large-scale, a smaller ver-
sion has been released. The reduced data set contains 4 product types: Kitchen, Books, DVDs, and
Electronics (Blitzer et al., 2007). There are 4000 positive and 4000 negative reviews.
1
For these two data sets, we select 8000 words with highest document-frequency to generate the vo-
cabulary. Stopwords
2
are removed and a normalized term-frequency representation is used. In order to
construct the lexical prior knowledge matrixU
0
, we use the sentiment lexicon generated by (Hu and Liu,
2004). It contains 2,006 positive words (e.g., ?beautiful?) and 4,783 negative words (e.g., ?upset?).
4.2 Unsupervised Sentiment Classification
Our first experiment is to explore the benefits of incorporating the geometric information in the unsu-
pervised paradigm (that is C
v
= 0). Therefore, the third part in equation (7) will be ignored. For this
unsupervised paradigm of GNMTF, we empirically set ? = ? = ? = 1, ?
1
= ?
2
= 1, Iter = 100 and
run GNMTF 10 repeated times to remove any randomness caused by the random initialization. Due to
limited space, we do not present the impacts of the parameters on the learning model. Now we compare
our proposed GNMTF with the following four categories of methods:
(1) Lexicon-Based Methods (LBM in short): Taboada et al. (2011) proposed to incorporate intensifi-
cation and negation to refine the sentiment score for each document. This is the state-of-the-art lexicon-
based method for unsupervised sentiment classification.
(2) Document Clustering Methods: We choose the most representative cluster methods, K-means,
NMTF, Information-Theoretic Co-clustering (ITCC) (Dhillon et al., 2003), and Euclidean Co-clustering
method (ECC) (Cho et al., 2004). We set the number of clusters as two in these methods. Note that all
these methods do not make use of the sentiment lexicon.
(3) Constrained NMTF (CNMTF in short): Li et al. (2009) incorporated the sentiment lexicon into
NMTF as a domain-independent prior constraint.
(4) Graph co-regularized Non-negative Matrix Tri-factorization (GNMTF in short): It is a new algo-
rithm proposed in this paper. We use cosine similarity for constructing the p-nearest neighbor graph for
its simplicity. The number of nearest neighbor p is set to 10 empirically both on document and word
spaces.
4.2.1 Sentiment Classification Results
The experimental results are reported in Table 2. We perform a significant test, i.e., a t-test with a default
significant level of 0.05. From Table 2, we can see that (1) Both CNMTF and GNMTF consider the
lexical prior knowledge from off-the-shelf sentiment lexicon and achieve better performance than NMTF.
This suggests the importance of the lexical prior knowledge in learning the sentiment classification (row
1
The data set can be freely downloaded from http://www.cs.jhu.edu/ mdredze/datasets/sentiment/.
2
http://truereader.com/manuals/onix/stopwords1.html
1336
# Methods Movies Amazon
1 LBM 0.632 0.580
2 K-means 0.543 (-8.9%) 0.535 (-4.5%)
3 NMTF 0.561 (-7.1%) 0.547 (-3.3%)
4 ECC 0.678 (+4.6%) 0.642 (+6.2%)
5 ITCC 0.714 (+8.2%) 0.655 (+7.5%)
6 CNMTF 0.695 (+6.3%) 0.658 (+7.8%)
7 GNMTF 0.736 (+10.4%) 0.705 (+12.5%)
Table 2: Sentiment classification accuracy of unsupervised paradigm on the data sets. Improvements of
K-means, NMTF, ITCC, ECC, CNMTF and GNMTF over baseline LBM are shown in parentheses.
0 20 40 60 80 1000.2
0.25
0.3
0.35
0.4
0.45
0.5
(a) Movies data
Objec
tive fu
nction
 value
0 20 40 60 80 1000.46
0.48
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
(b) Amazon data
Objec
tive fu
nction
 value
Figure 1: Convergence curves of GNMTF on both data sets.
3 vs. row 6 and row 7); (2) Regardless of the data sets, our GNMTF significantly outperforms state-of-
the-art CNMTF and achieves the best performance. This shows the superiority of geometric information
and graph co-regularization framework (row 4 vs. row 5, the improvements are statistically significant at
p < 0.05).
4.2.2 Convergence Behavior
In subsection 3.3.1, we have shown that the multiplicative updates given by equations (8)?(10) are
convergent. Here, we empirically show the convergence behavior of GNMTF.
Figure 1 shows the convergence curves of GNMTF on Movies and Amazon data sets. From the figure,
y-axis is the value of objective function and x-axis denotes the iteration number. We can see that the
multiplicative updates for GNMTF converge very fast, usually within 50 iterations.
4.3 Semi-supervised Sentiment Classification
In this subsection, we describe our proposed GNMTF with a few labeled documents. For this semi-
supervised paradigm of GNMTF, we empirically set Iter = 100, ?
1
= ?
2
= 2, ? = ? = ? = ? = 1 and
p = 10 on document and word spaces and also run 10 repeated times to remove any randomness caused
by the random initialization. Due to limited space, we do not give an in-depth parameter analysis. For
CNMTF, we set ? = ? = 1 for fair comparison. We also compare our proposed GNMTF with some
representative semi-supervised approaches described in (Li et al., 2009): (1) Semi-supervised learning
with local and global consistency (Consistency Method in short) (Zhou et al., 2004); (2) Semi-supervised
learning using gaussian fields and harmonic functions (GFHF in short) (Zhu et al., 2003). Besides,
we also compare the results of our proposed GNMTF with the representative supervised classification
method: support vector machine (SVM), which has been widely used in sentiment classification (Pang
et al., 2002).
The results are presented in Figure 2. From the figure, we can see that GNMTF outperforms other
methods over the entire range of number of labeled documents on both data sets. By this observation,
we can conclude that taking the geometric information can still improve the sentiment classification
accuracy in semi-supervised paradigm.
1337
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
(a) Movies data
Senti
ment
 class
ificati
on ac
curac
y
SVMConsistency MethodGFHFCNMTFGNMTF 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.55
0.6
0.65
0.7
0.75
0.8
0.85
(b) Amazon data
Senti
ment
 class
ificati
on ac
curac
y
SVMConsistency MethodGFHFCNMTFGNMTF
Figure 2: Sentiment classification accuracy vs. different percentage of labeled documents, where x-axis
denotes the number of documents labeled as a fraction of the original labeled documents.
5 Related Work
Sentiment classification has gained widely interest in NLP community, we point the readers to recent
books (Pang and Lee, 2008; Liu, 2012) for an in-depth survey of literature on sentiment analysis.
Methods for automatically classifying sentiments expressed in products and movie reviews can
roughly be divided into supervised and unsupervised (or semi-supervised) sentiment analysis. Super-
vised techniques have been proved promising and widely used in sentiment classification (Pang et al.,
2002; Pang and Lee, 2008; Liu, 2012). However, the performance of these methods relies on manually
labeled training data. In some cases, the labeling work may be time-consuming and expensive. This
motivates the problem of learning robust sentiment classification via unsupervised (or semi-supervised)
paradigm.
The most representative way to perform semi-supervised paradigm is to employ partial labeled data to
guide the sentiment classification (Goldberg and Zhu, 2006; Sindhwani and Melville, 2008; Wan, 2009;
Li et al., 2011). However, we do not have any labeled data at hand in many situations, which makes
the unsupervised paradigm possible. The most representative way to perform unsupervised paradigm
is to use a sentiment lexicon to guide the sentiment classification (Turney, 2002; Taboada et al., 2011)
or learn sentiment orientation via a matrix factorization clustering framework (Li et al., 2009; ?; Hu
et al., 2013). In contrast, we perform sentiment classification with the different model formulation and
learning algorithm, which considers both word-level and document-level sentiment-related contextual
information (e.g., the neighboring words or documents tend to share the same sentiment polarity) into
a unified framework. The proposed framework makes use of the valuable geometric information to
compensate the problem of lack of labeled data for sentiment classification. In addition, some researchers
also explored the matrix factorization techniques for other NLP tasks, such as relation extraction (Peng
and Park, 2013) and question answering (Zhou et al., 2013)
Besides, many studies address some other aspects of sentiment analysis, such as cross-domain senti-
ment classification (Blitzer et al., 2007; Pan et al., 2010; Hu et al., 2011; Bollegala et al., 2011; Glorot
et al., 2011), cross-lingual sentiment classification (Wan, 2009; Lu et al., 2011b; Meng et al., 2012) and
imbalanced sentiment classification (Li et al., 2011), which are out of scope of this paper.
6 Conclusion and Future Work
In this paper, we propose a novel algorithm, called graph co-regularized non-negative matrix tri-
factorization (GNMTF), from a geometric perspective. GNMTF assumes that if two words (or docu-
ments) are sufficiently close to each other, they tend to share the same sentiment polarity. To achieve
this, we encode the geometric information by constructing the nearest neighbor graphs, in conjunction
with a non-negative matrix tri-factorization framework. We derive an efficient algorithm for learning
the factorization, analyze its complexity, and provide proof of convergence. Our empirical study on two
open data sets validates that GNMTF can consistently improve the sentiment classification accuracy in
comparison to state-of-the-art methods.
1338
There are some ways in which this research could be continued. First, some other ways should be
considered to construct the graphs (e.g., hyperlinks between documents, synonyms or co-occurrences
between words). Second, we will try to extend the proposed framework for other aspects of sentiment
analysis, such as cross-domain or cross-lingual settings.
Acknowledgments
This work was supported by the National Natural Science Foundation of China (No. 61303180 and
No. 61272332), the Beijing Natural Science Foundation (No. 4144087), CCF Opening Project of Chi-
nese Information Processing, and also Sponsored by CCF-Tencent Open Research Fund. We thank the
anonymous reviewers for their insightful comments.
References
M. Belkin and P. Niyogi. 2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. In
Proceedings of NIPS, pages 585-591.
J. Blitzer, M. Dredze and F. Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: domain adaptation
for sentiment classification. In Proceedings of ACL, pages 440-447.
D. Bollegala, D. Weir, and J. Carroll. 2011. Using multiples sources to construct a sentiment sensitive thesaurus.
In Proceedings of ACL, pages 132-141.
S. Boyd and L. Vandenberghe. 2004. Convex Optimization. Cambridge university press.
D. Cai, X. He, J. Han, and T. Huang. 2011. Graph regularized non-negative matrix factorization for data represen-
tation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8): 1548-1560.
H. Cho, I. Dhillon, Y. Guan, and S. Sra. 2004. Minimum sum squared residue co-clutering of gene expression
data. In Proceedings of SDM, pages 22-24.
F. Chung. 1997. Spectral graph theory. Regional Conference Series in Mathematics, Volume 92.
I. Dhillon, S. Mallela, and D. Modha. 2003. Information-theoretic Co-clustering. In Proceedings of KDD, pages
89-98.
C. Ding, T. Li, W. Peng, and H. Park. 2006. Orthogonal non-negative matrix tri-factorization for clustering. In
Proceedings of KDD, pages 126-135.
X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain adaptation for larage-scale sentiment classification: a deep
learning approach. In Proceedings of ICML.
A. Goldberg and X. Zhu. 2006. Seeing stars when there aren?t many stars: graph-based semi-supervised learning
for sentiment categorization. In Proceedings of NAACL Workshop.
Y. He, C. Lin and H. Alani. 2011. Automatically extracting polarity-bearing topics for cross-domain sentiment
classification. In Proceedings of ACL, pages 123-131.
M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD.
X. Hu, J. Tang, H. Gao, and H. Liu. 2013. Unsupervised sentiment analysis with emotional signals. In Proceedings
of WSDM.
X. Hu, N. Sun, C. Zhang, and T. Chua. 2009. Exploiting internal and external semantics for the clustering of short
texts using world knowldge. In Proceedings of CIKM, pages 919-928.
H. Kim and H. Park. 2008. Non-negative matrix factorization based on alternating non-negativity constrained
least squares and active set method. SIAM J Matrix Anal Appl, 30(2):713-730.
D. Lee and H. Seung. 2001. Algorithms for non-negative matrix factorization. In Proceedings of NIPS.
S. Li, Z. Wang, G. Zhou, and S. Lee. 2011. Semi-supervised learning for imbalanced sentiment classification. In
Proceedings of IJCAI, pages 1826-1831.
1339
T. Li, Y. Zhang, and V. Singhwani. 2009. A non-negative matrix tri-factorization approach to sentiment classifica-
tion with lexical prior knowledge. In Proceedings of ACL, pages 244-252.
C. Lin. 2007. Projected gradient methods for nonnegative matrix factorization. Neural Comput, 19(10):2756-
2779.
B. Liu. 2012. Sentiment analysis and opinion mining. Morgan & Claypool Publishers.
B. Lu, C. Tan, C. Cardie, and B. Tsou. 2011. Joint bilingual sentiment classification with unlabeled parallel
corpora. In Proceedings of ACL, pages 320-330.
Y. Lu, M. Castellanos, U. Dayal, and C. Zhai. 2011. Automatic construction of a context-aware sentiment lexicon:
an optimization approach. In Proceedings of WWW, pages 347-356.
X. Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and H. Wang. 2012. Cross-lingual mixture model for sentiment
classification. In Proceedings of ACL, pages 572-581.
V. Ng, S. Dasgupta, and S. Arifin. 2006. Examing the role of linguistic knowlege sources in the automatic
identification and classificaton of reviews. In Proceedings of ACL.
S. Pan, X. Ni, J. Sun, Q. Yang, and Z. Chen. 2010. Cross-domain sentiment classification via spectral feature
alignment. In Proceedings of WWW.
B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Informaiton
Retrieval, pages 1-135.
B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques.
In Proceedings of EMNLP, pages 79-86.
S. Riedel, L. Yao, A. McCallum, and B. Marlin. 2013. Relation extraction with matrix factorization and universal
schemas. In Proceedings of NAACL.
W. Peng and D. Park. 2011. Generative adjective sentiment dictionary for social media sentiment analysis using
constrained nonnegative matrix factorization. In Proceedings of ICWSM.
S. Roweis and L. Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323-2326.
V. Sindhwani and P. Melville. 2008. Document-word co-regulariztion for semi-supervised sentiment analysis. In
Proceedings of ICDM, pages 1025-1030.
J. Tenenbaum, V. Silva, and J. Langford. 2000. A global geometric framework for nonlinear dimensionality
reduction. Science, 290(5500):2319-2323.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and M. Stede. 2011. Lexicon-based methods for sentiment analysis.
Computational Linguistics.
P. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of
reviews. In Proceedings of ACL, pages 417-424.
X. Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of ACL, pages 235-243.
D. Zhou, Q. Bousquet, T. Lal, J. Weston, and B. Scholkopf. 2004. Learning with local and global consistency. In
Proceedings of NIPS.
G. Zhou, F. Liu, Y. Liu, S. He, and J. Zhao. 2013. Statistical machine translation improves question retrieval in
community question answering via matrix factorization. In Proceedings of ACL, pages 852-861.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic
functions. In Proceedings of ICML.
1340
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2335?2344, Dublin, Ireland, August 23-29 2014.
Relation Classification via Convolutional Deep Neural Network
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{djzeng,kliu,swlai,gyzhou,jzhao}@nlpr.ia.ac.cn
Abstract
The state-of-the-art methods used for relation classification are primarily based on statistical ma-
chine learning, and their performance strongly depends on the quality of the extracted features.
The extracted features are often derived from the output of pre-existing natural language process-
ing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders
the performance of these systems. In this paper, we exploit a convolutional deep neural network
(DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as
input without complicated pre-processing. First, the word tokens are transformed to vectors by
looking up word embeddings
1
. Then, lexical level features are extracted according to the given
nouns. Meanwhile, sentence level features are learned using a convolutional approach. These
two level features are concatenated to form the final extracted feature vector. Finally, the fea-
tures are fed into a softmax classifier to predict the relationship between two marked nouns. The
experimental results demonstrate that our approach significantly outperforms the state-of-the-art
methods.
1 Introduction
The task of relation classification is to predict semantic relations between pairs of nominals and can
be defined as follows: given a sentence S with the annotated pairs of nominals e
1
and e
2
, we aim
to identify the relations between e
1
and e
2
(Hendrickx et al., 2010). There is considerable interest in
automatic relation classification, both as an end in itself and as an intermediate step in a variety of NLP
applications.
The most representative methods for relation classification use supervised paradigm; such methods
have been shown to be effective and yield relatively high performance (Zelenko et al., 2003; Bunescu
and Mooney, 2005; Zhou et al., 2005; Mintz et al., 2009). Supervised approaches are further divided
into feature-based methods and kernel-based methods. Feature-based methods use a set of features that
are selected after performing textual analysis. They convert these features into symbolic IDs, which are
then transformed into a vector using a paradigm that is similar to the bag-of-words model
2
. Conversely,
kernel-based methods require pre-processed input data in the form of parse trees (such as dependency
parse trees). These approaches are effective because they leverage a large body of linguistic knowledge.
However, the extracted features or elaborately designed kernels are often derived from the output of pre-
existing NLP systems, which leads to the propagation of the errors in the existing tools and hinders the
performance of such systems (Bach and Badaskar, 2007). It is attractive to consider extracting features
that are as independent from existing NLP tools as possible.
To identify the relations between pairs of nominals, it is necessary to a skillfully combine lexical and
sentence level clues from diverse syntactic and semantic structures in a sentence. For example, in the
sentence ?The [fire]
e
1
inside WTC was caused by exploding [fuel]
e
2
?, to identify that fire and fuel are in a
This work is licenced under a Creative Commons Attribution 4.0 International License.Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
A word embedding is a distributed representation for a word. For example, Collobert et al. (2011) use a 50-dimensional
vector to represent a word.
2
http://en.wikipedia.org/wiki/Bag-of-words model
2335
Cause-Effect relationship, we usually leverage the marked nouns and the meanings of the entire sentence.
In this paper, we exploit a convolutional DNN to extract lexical and sentence level features for relation
classification. Our method takes all of the word tokens as input without complicated pre-processing,
such as Part-of-Speech (POS) tagging and syntactic parsing. First, all the word tokens are transformed
into vectors by looking up word embeddings. Then, lexical level features are extracted according to the
given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two
level features are concatenated to form the final extracted feature vector. Finally, the features are feed
into a softmax classifier to predict the relationship between two marked nouns.
The idea of extracting features for NLP using convolutional DNN was previously explored by Col-
lobert et al. (2011), in the context of POS tagging, chunking (CHUNK), Named Entity Recogni-
tion (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of Collobert
et al. (2011). In (Collobert et al., 2011), all of the tasks are considered as the sequential labeling prob-
lems in which each word in the input sentence is given a tag. However, our task, ?relation classification?,
can be considered a multi-class classification problem, which results in a different objective function.
Moreover, relation classification is defined as assigning relation labels to pairs of words. It is thus nec-
essary to specify which pairs of words to which we expect to assign relation labels. For that purpose, the
position features (PF) are exploited to encode the relative distances to the target noun pairs. To the best
of our knowledge, this work is the first example of using a convolutional DNN for relation classification.
The contributions of this paper can be summarized as follows.
? We explore the feasibility of performing relation classification without complicated NLP pre-
processing. A convolutional DNN is employed to extract lexical and sentence level features.
? To specify pairs of words to which relation labels should be assigned, position features are proposed
to encode the relative distances to the target noun pairs in the convolutional DNN.
? We conduct experiments using the SemEval-2010 Task 8 dataset. The experimental results demon-
strate that the proposed position features are critical for relation classification. The extracted lexical
and sentence level features are effective for relation classification. Our approach outperforms the
state-of-the-art methods.
2 Related Work
Relation classification is one of the most important topics in NLP. Many approaches have been explored
for relation classification, including unsupervised relation discovery and supervised classification. Re-
searchers have proposed various features to identify the relations between nominals using different meth-
ods.
In the unsupervised paradigms, contextual features are used. Distributional hypothesis theory (Harris,
1954) indicates that words that occur in the same context tend to have similar meanings. Accordingly, it is
assumed that the pairs of nominals that occur in similar contexts tend to have similar relations. Hasegawa
et al. (2004) adopted a hierarchical clustering method to cluster the contexts of nominals and simply
selected the most frequent words in the contexts to represent the relation between the nominals. Chen
et al. (2005) proposed a novel unsupervised method based on model order selection and discriminative
label identification to address this problem.
In the supervised paradigm, relation classification is considered a multi-classification problem, and re-
searchers concentrate on extracting more complex features. Generally, these methods can be categorized
into two types: feature-based and kernel-based. In feature-based methods, a diverse set of strategies
have been exploited to convert the classification clues (such as sequences and parse trees) into feature
vectors (Kambhatla, 2004; Suchanek et al., 2006). Feature-based methods suffer from the problem
of selecting a suitable feature set when converting the structured representation into feature vectors.
Kernel-based methods provide a natural alternative to exploit rich representations of the input classifica-
tion clues, such as syntactic parse trees. Kernel-based methods allow the use of a large set of features
without explicitly extracting the features. Various kernels, such as the convolution tree kernel (Qian et
2336
WordRepresentation
FeatureExtraction
Output W3x
Figure 1: Architecture of the neural network used
for relation classification.
WindowProcessing
max over timesConvolution
tanh W2x
W1
WF
PF
Sentence levelFeatures
Figure 2: The framework used for extracting sen-
tence level features.
al., 2008), subsequence kernel (Mooney and Bunescu, 2005) and dependency tree kernel (Bunescu and
Mooney, 2005), have been proposed to solve the relation classification problem. However, the methods
mentioned above suffer from a lack of sufficient labeled data for training. Mintz et al. (2009) proposed
distant supervision (DS) to address this problem. The DS method selects sentences that match the facts
in a knowledge base as positive examples. The DS algorithm sometimes faces the problem of wrong
labels, which results in noisy labeled data. To address the shortcoming of DS, Riedel et al. (2010) and
Hoffmann et al. (2011) cast the relaxed DS assumption as multi-instance learning. Furthermore, Taka-
matsu et al. (2012) noted that the relaxed DS assumption would fail and proposed a novel generative
model to model the heuristic labeling process in order to reduce the wrong labels.
The supervised method has been demonstrated to be effective for relation detection and yields rela-
tively high performance. However, the performance of this method strongly depends on the quality of the
designed features. With the recent revival of interest in DNN, many researchers have concentrated on us-
ing Deep Learning to learn features. In NLP, such methods are primarily based on learning a distributed
representation for each word, which is also called a word embeddings (Turian et al., 2010). Socher et al.
(2012) present a novel recursive neural network (RNN) for relation classification that learns vectors in
the syntactic tree path that connects two nominals to determine their semantic relationship. Hashimoto
et al. (2013) also use an RNN for relation classification; their method allows for the explicit weighting
of important phrases for the target task. As mentioned in Section 1, it is difficult to design high quality
features using the existing NLP tools. In this paper, we propose a convolutional DNN to extract lexical
and sentence level features for relation classification; our method effectively alleviates the shortcomings
of traditional features.
3 Methodology
3.1 The Neural Network Architecture
Figure 1 describes the architecture of the neural network that we use for relation classification. The
network takes an input sentence and discovers multiple levels of feature extraction, where higher levels
represent more abstract aspects of the inputs. It primarily includes the following three components: Word
Representation, Feature Extraction and Output. The system does not need any complicated syntactic or
semantic preprocessing, and the input of the system is a sentence with two marked nouns. Then, the
word tokens are transformed into vectors by looking up word embeddings. In succession, the lexical and
sentence level features are respectively extracted and then directly concatenated to form the final feature
vector. Finally, to compute the confidence of each relation, the feature vector is fed into a softmax
classifier. The output of the classifier is a vector, the dimension of which is equal to the number of
predefined relation types. The value of each dimension is the confidence score of the corresponding
relation.
2337
Features Remark
L1 Noun 1
L2 Noun 2
L3 Left and right tokens of noun 1
L4 Left and right tokens of noun 2
L5 WordNet hypernyms of nouns
Table 1: Lexical level features.
3.2 Word Representation
In the word representation component, each input word token is transformed into a vector by looking
up word embeddings. Collobert et al. (2011) reported that word embeddings learned from significant
amounts of unlabeled data are far more satisfactory than the randomly initialized embeddings. In relation
classification, we should first concentrate on learning discriminative word embeddings, which carry more
syntactic and semantic information, using significant amounts of unlabeled data. Unfortunately, it usually
takes a long time to train the word embeddings
3
. However, there are many trained word embeddings that
are freely available (Turian et al., 2010). A comparison of the available word embeddings is beyond
the scope of this paper. Our experiments directly utilize the trained embeddings provided by Turian et
al.(2010).
3.3 Lexical Level Features
Lexical level features serve as important cues for deciding relations. The traditional lexical level features
primarily include the nouns themselves, the types of the pairs of nominals and word sequences between
the entities, the quality of which strongly depends on the results of existing NLP tools. Alternatively,
this paper uses generic word embeddings as the source of base features. We select the word embeddings
of marked nouns and the context tokens. Moreover, the WordNet hypernyms
4
are adopted as MVRNN
(Socher et al., 2012). All of these features are concatenated into our lexical level features vector l. Table
1 presents the selected word embeddings that are related to the marked nouns in the sentence.
3.4 Sentence Level Features
As mentioned in section 3.2, all of the tokens are represented as word vectors, which have been demon-
strated to correlate well with human judgments of word similarity. Despite their success, single word
vector models are severely limited because they do not capture long distance features and semantic com-
positionality, the important quality of natural language that allows humans to understand the meanings
of a longer expression. In this section, we propose a max-pooled convolutional neural network to offer
sentence level representation and automatically extract sentence level features. Figure 2 shows the frame-
work for sentence level feature extraction. In the Window Processing component, each token is further
represented as Word Features (WF) and Position Features (PF) (see section 3.4.1 and 3.4.2). Then, the
vector goes through a convolutional component. Finally, we obtain the sentence level features through a
non-linear transformation.
3.4.1 Word Features
Distributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tend
to have similar meanings. To capture this characteristic, the WF combines a word?s vector representation
and the vector representations of the words in its context. Assume that we have the following sequence
of words.
S : [People]
0
have
1
been
2
moving
3
back
4
into
5
[downtown]
6
The marked nouns are associated with a label y that defines the relation type that the marked pair contains.
Each word is also associated with an index into the word embeddings. All of the word tokens of the
sentence S are then represented as a list of vectors (x
0
,x
1
, ? ? ? ,x
6
), where x
i
corresponds to the word
3
Collobert et al. (2011) proposed a pairwise ranking approach to train the word embeddings, and the total training time for
an English corpus (Wikipedia) was approximately four weeks.
4
http://sourceforge.net/projects/supersensetag/
2338
embedding of the i-th word in the sentence. To use a context size of w, we combine the size w windows
of vectors into a richer feature. For example, when we take w = 3, the WF of the third word ?moving?
in the sentence S is expressed as [x
2
,x
3
,x
4
]. Similarly, considering the whole sentence, the WF can be
represented as follows:
{[x
s
,x
0
,x
1
], [x
0
,x
1
,x
2
], ? ? ? , [x
5
,x
6
,x
e
]}
5
3.4.2 Position Features
Relation classification is a very complex task. Traditionally, structure features (e.g., the shortest depen-
dency path between nominals) are used to solve this problem (Bunescu and Mooney, 2005). Apparently,
it is not possible to capture such structure information only through WF. It is necessary to specify which
input tokens are the target nouns in the sentence. For this purpose, PF are proposed for relation classi-
fication. In this paper, the PF is the combination of the relative distances of the current word to w
1
and
w
2
. For example, the relative distances of ?moving? in sentence S to ?people? and ?downtown? are 3
and -3, respectively. In our method, the relative distances also are mapped to a vector of dimension d
e
(a
hyperparameter); this vector is randomly initialized. Then, we obtain the distance vectors d
1
and d
2
with
respect to the relative distances of the current word to w
1
and w
2
, and PF = [d
1
,d
2
]. Combining the WF
and PF, the word is represented as [WF,PF]
T
, which is subsequently fed into the convolution component
of the algorithm.
3.4.3 Convolution
We will see that the word representation approach can capture contextual information through combina-
tions of vectors in a window. However, it only produces local features around each word of the sentence.
In relation classification, an input sentence that is marked with target nouns only corresponds to a re-
lation type rather than predicting label for each word. Thus, it might be necessary to utilize all of the
local features and predict a relation globally. When using neural network, the convolution approach is a
natural method to merge all of the features. Similar to Collobert et al. (2011), we first process the output
of Window Processing using a linear transformation.
Z = W
1
X (1)
X ? R
n
0
?t
is the output of the Window Processing task, where n
0
= w? n, n (a hyperparameter) is the
dimension of feature vector, and t is the token number of the input sentence. W
1
? R
n
1
?n
0
, where n
1
(a
hyperparameter) is the size of hidden layer 1, is the linear transformation matrix. We can see that the
features share the same weights across all times, which greatly reduces the number of free parameters to
learn. After the linear transformation is applied, the output Z ? R
n
1
?t
is dependent on t. To determine
the most useful feature in the each dimension of the feature vectors, we perform a max operation over
time on Z.
m
i
= maxZ(i, ?) 0 ? i ? n
1
(2)
where Z(i, ?) denote the i-th row of matrix Z. Finally, we obtain the feature vector m =
{m
1
,m
2
, ? ? ? ,m
n
1
}, the dimension of which is no longer related to the sentence length.
3.4.4 Sentence Level Feature Vector
To learn more complex features, we designed a non-linear layer and selected hyperbolic tanh as the
activation function. One useful property of tanh is that its derivative can be expressed in terms of the
function value itself:
d
dx
tanhx = 1? tanh
2
x (3)
It has the advantage of making it easy to compute the gradient in the backpropagation training procedure.
Formally, the non-linear transformation can be written as
g = tanh(W
2
m) (4)
5
x
s
and x
e
are special word embeddings that correspond to the beginning and end of the sentence, respectively.
2339
W2
? R
n
2
?n
1
is the linear transformation matrix, where n
2
(a hyperparameter) is the size of hidden
layer 2. Compared with m ? R
n
1
?1
, g ? R
n
2
?1
can be considered higher level features (sentence level
features).
3.5 Output
The automatically learned lexical and sentence level features mentioned above are concatenated into a
single vector f = [l, g]. To compute the confidence of each relation, the feature vector f ? R
n
3
?1
(n
3
equals n
2
plus the dimension of the lexical level features) is fed into a softmax classifier.
o = W
3
f (5)
W
3
? R
n
4
?n
3
is the transformation matrix and o ? R
n
4
?1
is the final output of the network, where n
4
is equal to the number of possible relation types for the relation classification system. Each output can
be then interpreted as the confidence score of the corresponding relation. This score can be interpreted
as a conditional probability by applying a softmax operation (see Section 3.6).
3.6 Backpropagation Training
The DNN based relation classification method proposed here could be stated as a quintuple ? =
(X,N,W
1
,W
2
,W
3
)
6
. In this paper, each input sentence is considered independently. Given an in-
put example s, the network with parameter ? outputs the vector o, where the i-th component o
i
contains
the score for relation i. To obtain the conditional probability p(i|x, ?), we apply a softmax operation over
all relation types:
p(i|x, ?) =
e
o
i
n
4
?
k=1
e
o
k
(6)
Given all our (suppose T ) training examples (x
(i)
; y
(i)
), we can then write down the log likelihood of the
parameters as follows:
J (?) =
T
?
i=1
log p(y
(i)
|x
(i)
, ?) (7)
To compute the network parameter ?, we maximize the log likelihood J(?) using a simple optimization
technique called stochastic gradient descent (SGD). N,W
1
,W
2
and W
3
are randomly initialized and
X is initialized using the word embeddings. Because the parameters are in different layers of the neural
network, we implement the backpropagation algorithm: the differentiation chain rule is applied through
the network until the word embedding layer is reached by iteratively selecting an example (x, y) and
applying the following update rule.
? ? ? + ?
? log p(y|x, ?)
??
(8)
4 Dataset and Evaluation Metrics
To evaluate the performance of our proposed method, we use the SemEval-2010 Task 8 dataset (Hen-
drickx et al., 2010). The dataset is freely available
7
and contains 10,717 annotated examples, including
8,000 training instances and 2,717 test instances. There are 9 relationships (with two directions) and
an undirected Other class. The following are examples of the included relationships: Cause-Effect,
Component-Whole and Entity-Origin. In the official evaluation framework, directionality is taken into
account. A pair is counted as correct if the order of the words in the relationship is correct. For example,
both of the following instances S
1
and S
2
have the relationship Component-Whole.
S
1
: The [haft]
e
1
of the [axe]
e
2
is make ? ? ? ? Component-Whole(e
1
,e
2
)
S
2
: This [machine]
e
1
has two [units]
e
2
? ? ? ? Component-Whole(e
2
,e
1
)
6
N represents the word embeddings of WordNet hypernyms.
7
http://docs.google.com/View?id=dfvxd49s 36c28v9pmw
2340
?# Window size
1 2 3 4 5 6 7
F1
72
74
76
78
80
82
# Hidden layer 1
0 100 200 300 400 500 600
F1
72
74
76
78
80
82
# Hidden layer 2
0 100 200 300 400 500 600
F1
72
74
76
78
80
82
Figure 3: Effect of hyperparameters.
However, these two instances cannot be classified into the same category because Component-
Whole(e
1
,e
2
) and Component-Whole(e
2
,e
1
) are different relationships. Furthermore, the official rank-
ing of the participating systems is based on the macro-averaged F1-scores for the nine proper relations
(excluding Other). To compare our results with those obtained in previous studies, we adopt the macro-
averaged F1-score and also account for directionality into account in our following experiments
8
.
5 Experiments
In this section, we conduct three sets of experiments. The first is to test several variants via cross-
validation to gain some understanding of how the choice of hyperparameters impacts upon the perfor-
mance. In the second set of experiments, we make comparison of the performance among the convolu-
tional DNN learned features and various traditional features. The goal of the third set of experiments is
to evaluate the effectiveness of each extracted feature.
5.1 Parameter Settings
In this section, we experimentally study the effects of the three parameters in our proposed method:
the window size in the convolutional component w, the number of hidden layer 1, and the number of
hidden layer 2. Because there is no official development dataset, we tuned the hyperparameters by trying
different architectures via 5-fold cross-validation.
In Figure 3, we respectively vary the number of hyper parameters w, n
1
and n
2
and compute the F1.
We can see that it does not improve the performance when the window size is greater than 3. Moreover,
because the size of our training dataset is limited, the network is prone to overfitting, especially when
using large hidden layers. From Figure 3, we can see that the parameters have a limited impact on the
results when increasing the numbers of both hidden layers 1 and 2. Because the distance dimension has
little effect on the result (this is not illustrated in Figure 3), we heuristically choose d
e
= 5. Finally,
the word dimension and learning rate are the same as in Collobert et al. (2011). Table 2 reports all the
hyperparameters used in the following experiments.
Hyperparameter Window size Word dim. Distance dim. Hidden layer 1 Hidden layer 2 Learning rate
Value w = 3 n = 50 d
e
= 5 n
1
= 200 n
2
= 100 ? = 0.01
Table 2: Hyperparameters used in our experiments.
5.2 Results of Comparison Experiments
To obtain the final performance of our automatically learned features, we select seven approaches as com-
petitors to be compared with our method in Table 3. The first five competitors are described in Hendrickx
et al. (2010), all of which use traditional features and employ SVM or MaxEnt as the classifier. These
systems design a series of features and take advantage of a variety of resources (WordNet, ProBank,
and FrameNet, for example). RNN represents recursive neural networks for relation classification, as
8
The corpus contains a Perl-based automatic evaluation tool.
2341
Classifier Feature Sets F1
SVM POS, stemming, syntactic patterns 60.1
SVM word pair, words in between 72.5
SVM POS, stemming, syntactic patterns, WordNet 74.8
MaxEnt POS, morphological, noun compound, thesauri, Google n-grams, WordNet 77.6
SVM POS, prefixes, morphological, WordNet, dependency parse, Levin classed, ProBank,
FrameNet, NomLex-Plus, Google n-gram, paraphrases, TextRunner
82.2
RNN - 74.8
POS, NER, WordNet 77.6
MVRNN - 79.1
POS, NER, WordNet 82.4
Proposed word pair, words around word pair, WordNet 82.7
Table 3: Classifier, their feature sets and the F1-score for relation classification.
proposed by Socher et al. (2012). This method learns vectors in the syntactic tree path that connect two
nominals to determine their semantic relationship. The MVRNN model builds a single compositional
semantics for the minimal constituent, including both nominals as RNN (Socher et al., 2012). It is almost
certainly too much to expect a single fixed transformation to be able to capture the meaning combination
effects of all natural language operators. Thus, MVRNN assigns a matrix to every word and modifies the
meanings of other words instead of only considering word embeddings in the recursive procedure.
Table 3 illustrates the macro-averaged F1 measure results for these competing methods along with the
resources, features and classifier used by each method. Based on these results, we make the following
observations:
(1) Richer feature sets lead to better performance when using traditional features. This improvement
can be explained by the need for semantic generalization from training to test data. The quality of
traditional features relies on human ingenuity and prior NLP knowledge. It is almost impossible to
manually choose the best feature sets.
(2) RNN and MVRNN contain feature learning procedures; thus, they depend on the syntactic tree used
in the recursive procedures. Errors in syntactic parsing inhibit the ability of these methods to learn
high quality features. RNN cannot achieve a higher performance than the best method that uses
traditional features, even when POS, NER and WordNet are added to the training dataset. Compared
with RNN, the MVRNN model can capture the meaning combination effectively and achieve a higher
performance.
(3) Our method achieves the best performance among all of the compared methods. We also perform
a t-test (p 6 0.05), which indicates that our method significantly outperforms all of the compared
methods.
5.3 The Effect of Learned Features
Feature Sets F1
Lexical L1 34.7
+L2 53.1
+L3 59.4
+L4 65.9
+L5 73.3
Sentence WF 69.7
+PF 78.9
Combination all 82.7
Table 4: Score obtained for various sets of features on for the test set. The bottom portion of the table
shows the best combination of lexical and sentence level features.
In our method, the network extract lexical and sentence level features. The lexical level features pri-
marily contain five sets of features (L1 to L5). We performed ablation tests on the five sets of features
from the lexical part of Table 4 to determine which type of features contributed the most. The results are
2342
presented in Table 4, from which we can observe that our learned lexical level features are effective for
relation classification. The F1-score is improved remarkably when new features are added. Similarly, we
perform experiment on the sentence level features. The system achieves approximately 9.2% improve-
ments when adding PF. When all of the lexical and sentence level features are combined, we achieve the
best result.
6 Conclusion
In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence
level features for relation classification. In the network, position features (PF) are successfully proposed
to specify the pairs of nominals to which we expect to assign relation labels. The system obtains a
significant improvement when PF are added. The automatically learned features yield excellent results
and can replace the elaborately designed features that are based on the outputs of existing NLP tools.
Acknowledgments
This work was sponsored by the National Basic Research Program of China (No. 2014CB340503) and
the National Natural Science Foundation of China (No. 61272332, 61333018, 61202329, 61303180).
This work was supported in part by Noah?s Ark Lab of Huawei Tech. Co. Ltd. We thank the anonymous
reviewers for their insightful comments.
References
Nguyen Bach and Sameer Badaskar. 2007. A review of relation extraction. Literature review for Language and
Statistics II.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In
Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language
Processing, pages 724?731.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zhengyu Niu. 2005. Unsupervised feature selection for relation
extraction. In Proceedings of the International Joint Conference on Natural Language Processing, pages 262?
267.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Zellig Harris. 1954. Distributional structure. Word, 10(23):146?162.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering relations among named entities from
large corpora. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages
415?422.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. 2013. Simple customization
of recursive neural networks for semantic relation classification. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing, pages 1372?1376.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid
?
O. S?eaghdha, Sebastian Pad?o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification
of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic
Evaluation, SemEval ?10, pages 33?38.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies - Volume 1, pages 541?
550.
Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for
extracting relations. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics
on Interactive poster and demonstration sessions.
2343
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2, pages 1003?1011.
Raymond J Mooney and Razvan C Bunescu. 2005. Subsequence kernels for relation extraction. In Advances in
neural information processing systems, pages 171?178.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian. 2008. Exploiting constituent depen-
dencies for tree kernel-based semantic relation extraction. In Proceedings of the 22nd International Conference
on Computational Linguistics, pages 697?704.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without
labeled text. In Proceedings of the 2010 European conference on Machine learning and knowledge discovery
in databases: Part III, pages 148?163.
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1201?1211.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard Weikum. 2006. Combining linguistic and statistical analysis
to extract relations from web documents. In Proceedings of the 12th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 712?717.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for
relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers - Volume 1, pages 721?729.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394.
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. The
Journal of Machine Learning Research, 3:1083?1106.
GuoDong Zhou, Su Jian, Zhang Jie, and Zhang Min. 2005. Exploring various knowledge in relation extraction.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427?434.
2344
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653?662,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Phrase-Based Translation Model for Question Retrieval in Community
Question Answer Archives
Guangyou Zhou, Li Cai, Jun Zhao?, and Kang Liu
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,lcai,jzhao,kliu}@nlpr.ia.ac.cn
Abstract
Community-based question answer (Q&A)
has become an important issue due to the pop-
ularity of Q&A archives on the web. This pa-
per is concerned with the problem of ques-
tion retrieval. Question retrieval in Q&A
archives aims to find historical questions that
are semantically equivalent or relevant to the
queried questions. In this paper, we propose
a novel phrase-based translation model for
question retrieval. Compared to the traditional
word-based translation models, the phrase-
based translation model is more effective be-
cause it captures contextual information in
modeling the translation of phrases as a whole,
rather than translating single words in isola-
tion. Experiments conducted on real Q&A
data demonstrate that our proposed phrase-
based translation model significantly outper-
forms the state-of-the-art word-based transla-
tion model.
1 Introduction
Over the past few years, large scale question and
answer (Q&A) archives have become an important
information resource on the Web. These include
the traditional Frequently Asked Questions (FAQ)
archives and the emerging community-based Q&A
services, such as Yahoo! Answers1, Live QnA2, and
Baidu Zhidao3.
?Correspondence author: jzhao@nlpr.ia.ac.cn
1http://answers.yahoo.com/
2http://qna.live.com/
3http://zhidao.baidu.com/
Community-based Q&A services can directly re-
turn answers to the queried questions instead of a
list of relevant documents, thus provide an effective
alternative to the traditional adhoc information re-
trieval. To make full use of the large scale archives
of question-answer pairs, it is critical to have func-
tionality helping users to retrieve historical answers
(Duan et al, 2008). Therefore, it is a meaningful
task to retrieve the questions that are semantically
equivalent or relevant to the queried questions. For
example in Table 1, given questionQ1,Q2 can be re-
turned and their answers will then be used to answer
Q1 because the answer ofQ2 is expected to partially
satisfy the queried question Q1. This is what we
called question retrieval in this paper.
The major challenge for Q&A retrieval, as for
Query:
Q1: How to get rid of stuffy nose?
Expected:
Q2: What is the best way to prevent a cold?
Not Expected:
Q3: How do I air out my stuffy room?
Q4: How do you make a nose bleed stop quicker?
Table 1: An example on question retrieval
most information retrieval models, such as vector
space model (VSM) (Salton et al, 1975), Okapi
model (Robertson et al, 1994), language model
(LM) (Ponte and Croft, 1998), is the lexical gap (or
lexical chasm) between the queried questions and
the historical questions in the archives (Jeon et al,
2005; Xue et al, 2008). For example in Table 1, Q1
and Q2 are two semantically similar questions, but
they have very few words in common. This prob-
653
lem is more serious for Q&A retrieval, since the
question-answer pairs are usually short and there is
little chance of finding the same content expressed
using different wording (Xue et al, 2008). To solve
the lexical gap problem, most researchers regarded
the question retrieval task as a statistical machine
translation problem by using IBM model 1 (Brown
et al, 1993) to learn the word-to-word translation
probabilities (Berger and Lafferty, 1999; Jeon et al,
2005; Xue et al, 2008; Lee et al, 2008; Bernhard
and Gurevych, 2009). Experiments consistently re-
ported that the word-based translation models could
yield better performance than the traditional meth-
ods (e.g., VSM. Okapi and LM). However, all these
existing approaches are considered to be context in-
dependent in that they do not take into account any
contextual information in modeling word translation
probabilities. For example in Table 1, although nei-
ther of the individual word pair (e.g., ?stuffy?/?cold?
and ?nose?/?cold?) might have a high translation
probability, the sequence of words ?stuffy nose? can
be easily translated from a single word ?cold? in Q2
with a relative high translation probability.
In this paper, we argue that it is beneficial to cap-
ture contextual information for question retrieval.
To this end, inspired by the phrase-based statistical
machine translation (SMT) systems (Koehn et al,
2003; Och and Ney, 2004), we propose a phrase-
based translation model (P-Trans) for question re-
trieval, and we assume that question retrieval should
be performed at the phrase level. This model learns
the probability of translating one sequence of words
(e.g., phrase) into another sequence of words, e.g.,
translating a phrase in a historical question into an-
other phrase in a queried question. Compared to the
traditional word-based translation models that ac-
count for translating single words in isolation, the
phrase-based translation model is potentially more
effective because it captures some contextual infor-
mation in modeling the translation of phrases as a
whole. More precise translation can be determined
for phrases than for words. It is thus reasonable to
expect that using such phrase translation probabili-
ties as ranking features is likely to improve the ques-
tion retrieval performance, as we will show in our
experiments.
Unlike the general natural language translation,
the parallel sentences between questions and an-
swers in community-based Q&A have very different
lengths, leaving many words in answers unaligned
to any word in queried questions. Following (Berger
and Lafferty, 1999), we restrict our attention to those
phrase translations consistent with a good word-
level alignment.
Specifically, we make the following contribu-
tions:
? we formulate the question retrieval task as a
phrase-based translation problem by modeling
the contextual information (in Section 3.1).
? we linearly combine the phrase-based transla-
tion model for the question part and answer part
(in Section 3.2).
? we propose a linear ranking model framework
for question retrieval in which different models
are incorporated as features because the phrase-
based translation model cannot be interpolated
with a unigram language model (in Section
3.3).
? finally, we conduct the experiments on
community-based Q&A data for question re-
trieval. The results show that our proposed ap-
proach significantly outperforms the baseline
methods (in Section 4).
The remainder of this paper is organized as fol-
lows. Section 2 introduces the existing state-of-the-
art methods. Section 3 describes our phrase-based
translation model for question retrieval. Section 4
presents the experimental results. In Section 5, we
conclude with ideas for future research.
2 Preliminaries
2.1 Language Model
The unigram language model has been widely used
for question retrieval on community-based Q&A
data (Jeon et al, 2005; Xue et al, 2008; Cao et al,
2010). To avoid zero probability, we use Jelinek-
Mercer smoothing (Zhai and Lafferty, 2001) due to
its good performance and cheap computational cost.
So the ranking function for the query likelihood lan-
guage model with Jelinek-Mercer smoothing can be
654
written as:
Score(q, D) =
?
w?q
(1 ? ?)Pml(w|D) + ?Pml(w|C)
(1)
Pml(w|D) =
#(w,D)
|D|
, Pml(w|C) =
#(w,C)
|C|
(2)
where q is the queried question, D is a document, C
is background collection, ? is smoothing parameter.
#(t,D) is the frequency of term t in D, |D| and |C|
denote the length of D and C respectively.
2.2 Word-Based Translation Model
Previous work (Berger et al, 2000; Jeon et al, 2005;
Xue et al, 2008) consistently reported that the word-
based translation models (Trans) yielded better per-
formance than the traditional methods (VSM, Okapi
and LM) for question retrieval. These models ex-
ploit the word translation probabilities in a language
modeling framework. Following Jeon et al (2005)
and Xue et al (2008), the ranking function can be
written as:
Score(q, D) =
?
w?q
(1??)Ptr(w|D)+?Pml(w|C) (3)
Ptr(w|D) =
?
t?D
P (w|t)Pml(t|D), Pml(t|D) =
#(t,D)
|D|
(4)
where P (w|t) denotes the translation probability
from word t to word w.
2.3 Word-Based Translation Language Model
Xue et al (2008) proposed to linearly mix two dif-
ferent estimations by combining language model
and word-based translation model into a unified
framework, called TransLM. The experiments show
that this model gains better performance than both
the language model and the word-based translation
model. Following Xue et al (2008), this model can
be written as:
Score(q, D) =
?
w?q
(1 ? ?)Pmx(w|D) + ?Pml(w|C)
(5)
Pmx(w|D) = ?
?
t?D
P (w|t)Pml(t|D)+(1??)Pml(w|D)
(6)
D:                      ?  for good cold home remedies ? document
E:                  [for,    good,    cold,    home remedies] segmentation
F:            [for1,    best2,    stuffy nose3,    home remedy4] translation
M:                     (1?3?2?1?3?4?4?2) permutation
q:                     best home remedy for stuffy nose queried question
Figure 1: Example describing the generative procedure
of the phrase-based translation model.
3 Our Approach: Phrase-Based
Translation Model for Question
Retrieval
3.1 Phrase-Based Translation Model
Phrase-based machine translation models (Koehn
et al, 2003; D. Chiang, 2005; Och and Ney,
2004) have shown superior performance compared
to word-based translation models. In this paper,
the goal of phrase-based translation model is to
translate a document4 D into a queried question
q. Rather than translating single words in isola-
tion, the phrase-based model translates one sequence
of words into another sequence of words, thus in-
corporating contextual information. For example,
we might learn that the phrase ?stuffy nose? can be
translated from ?cold? with relative high probabil-
ity, even though neither of the individual word pairs
(e.g., ?stuffy?/?cold? and ?nose?/?cold?) might have
a high word translation probability. Inspired by the
work of (Sun et al, 2010; Gao et al, 2010), we
assume the following generative process: first the
document D is broken into K non-empty word se-
quences t1, . . . , tK , then each t is translated into a
new non-empty word sequence w1, . . . ,wK , and fi-
nally these phrases are permutated and concatenated
to form the queried questions q, where t and w de-
note the phrases or consecutive sequence of words.
To formulate this generative process, let E
denote the segmentation of D into K phrases
t1, . . . , tK , and let F denote the K translation
phrases w1, . . . ,wK ?we refer to these (ti,wi)
pairs as bi-phrases. Finally, letM denote a permuta-
tion of K elements representing the final reordering
step. Figure 1 describes an example of the genera-
tive procedure.
Next let us place a probability distribution over
rewrite pairs. Let B(D,q) denote the set of E,
4In this paper, a document has the same meaning as a histor-
ical question-answer pair in the Q&A archives.
655
F , M triples that translate D into q. Here we as-
sume a uniform probability over segmentations, so
the phrase-based translation model can be formu-
lated as:
P (q|D) ?
?
(E,F,M)?
B(D,q)
P (F |D,E) ? P (M |D,E, F ) (7)
As is common practice in SMT, we use the maxi-
mum approximation to the sum:
P (q|D) ? max
(E,F,M)?
B(D,q)
P (F |D,E) ? P (M |D,E, F ) (8)
Although we have defined a generative model for
translatingD into q, our goal is to calculate the rank-
ing score function over existing q andD, rather than
generating new queried questions. Equation (8) can-
not be used directly for document ranking because
q and D are often of very different lengths, leav-
ing many words in D unaligned to any word in q.
This is the key difference between the community-
based question retrieval and the general natural lan-
guage translation. As pointed out by Berger and Laf-
ferty (1999) and Gao et al (2010), document-query
translation requires a distillation of the document,
while translation of natural language tolerates little
being thrown away.
Thus we attempt to extract the key document
words that form the distillation of the document, and
assume that a queried question is translated only
from the key document words. In this paper, the
key document words are identified via word align-
ment. We introduce the ?hidden alignments? A =
a1 . . . aj . . . aJ , which describe the mapping from a
word position j in queried question to a document
word position i = aj . The different alignment mod-
els we present provide different decompositions of
P (q, A|D). We assume that the position of the key
document words are determined by the Viterbi align-
ment, which can be obtained using IBM model 1 as
follows:
A? = argmax
A
P (q, A|D)
= argmax
A
{
P (J |I)
J
?
j=1
P (wj |taj )
}
=
[
argmax
aj
P (wj |taj )
]J
j=1
(9)
Given A?, when scoring a given Q&A pair, we re-
strict our attention to those E, F , M triples that are
consistent with A?, which we denote as B(D,q, A?).
Here, consistency requires that if two words are
aligned in A?, then they must appear in the same bi-
phrase (ti,wi). Once the word alignment is fixed,
the final permutation is uniquely determined, so we
can safely discard that factor. Thus equation (8) can
be written as:
P (q|D) ? max
(E,F,M)?B(D,q,A?)
P (F |D,E) (10)
For the sole remaining factor P (F |D,E), we
make the assumption that a segmented queried ques-
tion F = w1, . . . ,wK is generated from left to
right by translating each phrase t1, . . . , tK indepen-
dently:
P (F |D,E) =
K
?
k=1
P (wk|tk) (11)
where P (wk|tk) is a phrase translation probability,
the estimation will be described in Section 3.3.
To find the maximum probability assignment ef-
ficiently, we use a dynamic programming approach,
somewhat similar to the monotone decoding algo-
rithm described in (Och, 2002). We define ?j to
be the probability of the most likely sequence of
phrases covering the first j words in a queried ques-
tion, then the probability can be calculated using the
following recursion:
(1) Initialization:
?0 = 1 (12)
(2) Induction:
?j =
?
j?<j,w=wj?+1...wj
{
?j?P (w|tw)
}
(13)
(3) Total:
P (q|D) = ?J (14)
3.2 Phrase-Based Translation Model for
Question Part and Answer Part
In Q&A, a document D is decomposed into (q?, a?),
where q? denotes the question part of the historical
question in the archives and a? denotes the answer
part. Although it has been shown that doing Q&A
retrieval based solely on the answer part does not
perform well (Jeon et al, 2005; Xue et al, 2008),
the answer part should provide additional evidence
about relevance and, therefore, it should be com-
bined with the estimation based on the question part.
656
In this combined model, P (q|q?) and P (q|a?) are cal-
culated with equations (12) to (14). So P (q|D) will
be written as:
P (q|D) = ?1P (q|q?) + ?2P (q|a?) (15)
where ?1 + ?2 = 1.
In equation (15), the relative importance of ques-
tion part and answer part is adjusted through ?1 and
?2. When ?1 = 1, the retrieval model is based
on phrase-based translation model for the question
part. When ?2 = 1, the retrieval model is based on
phrase-based translation model for the answer part.
3.3 Parameter Estimation
3.3.1 Parallel Corpus Collection
In Q&A archives, question-answer pairs can be con-
sidered as a type of parallel corpus, which is used for
estimating the translation probabilities. Unlike the
bilingual machine translation, the questions and an-
swers in a Q&A archive are written in the same lan-
guage, the translation probability can be calculated
through setting either as the source and the other as
the target. In this paper, P (a?|q?) is used to denote
the translation probability with the question as the
source and the answer as the target. P (q?|a?) is used
to denote the opposite configuration.
For a given word or phrase, the related words
or phrases differ when it appears in the ques-
tion or in the answer. Following Xue et
al. (2008), a pooling strategy is adopted. First,
we pool the question-answer pairs used to learn
P (a?|q?) and the answer-question pairs used to
learn P (q?|a?), and then use IBM model 1 (Brown
et al, 1993) to learn the combined translation
probabilities. Suppose we use the collection
{(q?, a?)1, . . . , (q?, a?)m} to learn P (a?|q?) and use the
collection {(a?, q?)1, . . . , (a?, q?)m} to learn P (q?|a?),
then {(q?, a?)1, . . . , (q?, a?)m, (a?, q?)1, . . . , (a?, q?)m} is
used here to learn the combination translation prob-
ability Ppool(wi|tj).
3.3.2 Parallel Corpus Preprocessing
Unlike the bilingual parallel corpus used in SMT,
our parallel corpus is collected from Q&A archives,
which is more noisy. Directly using the IBM model
1 can be problematic, it is possible for translation
model to contain ?unnecessary? translations (Lee et
al., 2008). In this paper, we adopt a variant of Tex-
tRank algorithm (Mihalcea and Tarau, 2004) to iden-
tify and eliminate unimportant words from parallel
corpus, assuming that a word in a question or an-
swer is unimportant if it holds a relatively low sig-
nificance in the parallel corpus.
Following (Lee et al, 2008), the ranking algo-
rithm proceeds as follows. First, all the words in
a given document are added as vertices in a graph
G. Then edges are added between words if the
words co-occur in a fixed-sized window. The num-
ber of co-occurrences becomes the weight of an
edge. When the graph is constructed, the score of
each vertex is initialized as 1, and the PageRank-
based ranking algorithm is run on the graph itera-
tively until convergence. The TextRank score of a
word w in document D at kth iteration is defined as
follows:
Rkw,D = (1? d) + d ?
?
?j:(i,j)?G
ei,j
?
?l:(j,l)?G ej,l
Rk?1w,D
(16)
where d is a damping factor usually set to 0.85, and
ei,j is an edge weight between i and j.
We use average TextRank score as threshold:
words are removed if their scores are lower than the
average score of all words in a document.
3.3.3 Translation Probability Estimation
After preprocessing the parallel corpus, we will cal-
culate P (w|t), following the method commonly
used in SMT (Koehn et al, 2003; Och, 2002) to ex-
tract bi-phrases and estimate their translation proba-
bilities.
First, we learn the word-to-word translation prob-
ability using IBM model 1 (Brown et al, 1993).
Then, we perform Viterbi word alignment according
to equation (9). Finally, the bi-phrases that are con-
sistent with the word alignment are extracted using
the heuristics proposed in (Och, 2002). We set the
maximum phrase length to five in our experiments.
After gathering all such bi-phrases from the train-
ing data, we can estimate conditional relative fre-
quency estimates without smoothing:
P (w|t) = N(t,w)
N(t)
(17)
where N(t,w) is the number of times that t is
aligned to w in training data. These estimates are
657
source stuffy nose internet explorer
1 stuffy nose internet explorer
2 cold ie
3 stuffy internet browser
4 sore throat explorer
5 sneeze browser
Table 2: Phrase translation probability examples. Each
column shows the top 5 target phrases learned from the
word-aligned question-answer pairs.
useful for contextual lexical selection with sufficient
training data, but can be subject to data sparsity is-
sues (Sun et al, 2010; Gao et al, 2010). An alter-
nate translation probability estimate not subject to
data sparsity is the so-called lexical weight estimate
(Koehn et al, 2003). Let P (w|t) be the word-to-
word translation probability, and let A be the word
alignment between w and t. Here, the word align-
ment contains (i, j) pairs, where i ? 1 . . . |w| and
j ? 0 . . . |t|, with 0 indicating a null word. Then we
use the following estimate:
Pt(w|t, A) =
|w|
?
i=1
1
|{j|(j, i) ? A}|
?
?(i,j)?A
P (wi|tj)
(18)
We assume that for each position inw, there is ei-
ther a single alignment to 0, or multiple alignments
to non-zero positions in t. In fact, equation (18)
computes a product of per-word translation scores;
the per-word scores are the averages of all the trans-
lations for the alignment links of that word. The
word translation probabilities are calculated using
IBM 1, which has been widely used for question re-
trieval (Jeon et al, 2005; Xue et al, 2008; Lee et al,
2008; Bernhard and Gurevych, 2009). These word-
based scores of bi-phrases, though not as effective
in contextual selection, are more robust to noise and
sparsity.
A sample of the resulting phrase translation ex-
amples is shown in Table 2, where the top 5 target
phrases are translated from the source phrases ac-
cording to the phrase-based translation model. For
example, the term ?explorer? used alone, most likely
refers to a person who engages in scientific explo-
ration, while the phrase ?internet explorer? has a
very different meaning.
3.4 Ranking Candidate Historical Questions
Unlike the word-based translation models, the
phrase-based translation model cannot be interpo-
lated with a unigram language model. Following
(Sun et al, 2010; Gao et al, 2010), we resort to
a linear ranking framework for question retrieval in
which different models are incorporated as features.
We consider learning a relevance function of the
following general, linear form:
Score(q, D) = ?T ??(q, D) (19)
where the feature vector ?(q, D) is an arbitrary
function that maps (q, D) to a real value, i.e.,
?(q, D) ? R. ? is the corresponding weight vec-
tor, we optimize this parameter for our evaluation
metrics directly using the Powell Search algorithm
(Paul et al, 1992) via cross-validation.
The features used in this paper are as follows:
? Phrase translation features (PT):
?PT (q, D,A) = logP (q|D), where P (q|D)
is computed using equations (12) to (15), and
the phrase translation probability P (w|t) is
estimated using equation (17).
? Inverted Phrase translation features (IPT):
?IPT (D,q, A) = logP (D|q), where P (D|q)
is computed using equations (12) to (15) ex-
cept that we set ?2 = 0 in equation (15), and
the phrase translation probability P (w|t) is es-
timated using equation (17).
? Lexical weight feature (LW):
?LW (q, D,A) = logP (q|D), here P (q|D)
is computed by equations (12) to (15), and the
phrase translation probability is computed as
lexical weight according to equation (18).
? Inverted Lexical weight feature (ILW):
?ILW (D,q, A) = logP (D|q), here P (D|q)
is computed by equations (12) to (15) except
that we set ?2 = 0 in equation (15), and the
phrase translation probability is computed as
lexical weight according to equation (18).
? Phrase alignment features (PA):
?PA(q, D,B) =
?K
2 |ak ? bk?1 ? 1|,
where B is a set of K bi-phrases, ak is the start
position of the phrase in D that was translated
658
into the kth phrase in queried question, and
bk?1 is the end position of the phrase in D
that was translated into the (k ? 1)th phrase in
queried question. The feature, inspired by the
distortion model in SMT (Koehn et al, 2003),
models the degree to which the queried phrases
are reordered. For all possible B, we only
compute the feature value according to the
Viterbi alignment, B? = argmaxB P (q, B|D).
We find B? using the Viterbi algorithm, which is
almost identical to the dynamic programming
recursion of equations (12) to (14), except that
the sum operator in equation (13) is replaced
with the max operator.
? Unaligned word penalty features (UWP):
?UWP (q, D), which is defined as the ratio be-
tween the number of unaligned words and the
total number of words in queried questions.
? Language model features (LM):
?LM (q, D,A) = logPLM (q|D), where
PLM (q|D) is the unigram language model
with Jelinek-Mercer smoothing defined by
equations (1) and (2).
? Word translation features (WT):
?WT (q, D) = logP (q|D), where P (q|D) is
the word-based translation model defined by
equations (3) and (4).
4 Experiments
4.1 Data Set and Evaluation Metrics
We collect the questions from Yahoo! Answers and
use the getByCategory function provided in Yahoo!
Answers API5 to obtain Q&A threads from the Ya-
hoo! site. More specifically, we utilize the resolved
questions under the top-level category at Yahoo!
Answers, namely ?Computers & Internet?. The re-
sulting question repository that we use for question
retrieval contains 518,492 questions. To learn the
translation probabilities, we use about one million
question-answer pairs from another data set.6
In order to create the test set, we randomly se-
lect 300 questions for this category, denoted as
5http://developer.yahoo.com/answers
6The Yahoo! Webscope dataset Yahoo answers com-
prehensive questions and answers version 1.0.2, available at
http://reseach.yahoo.com/Academic Relations.
?CI TST?. To obtain the ground-truth of ques-
tion retrieval, we employ the Vector Space Model
(VSM) (Salton et al, 1975) to retrieve the top 20 re-
sults and obtain manual judgements. The top 20 re-
sults don?t include the queried question itself. Given
a returned result by VSM, an annotator is asked to
label it with ?relevant? or ?irrelevant?. If a returned
result is considered semantically equivalent to the
queried question, the annotator will label it as ?rel-
evant?; otherwise, the annotator will label it as ?ir-
relevant?. Two annotators are involved in the anno-
tation process. If a conflict happens, a third person
will make judgement for the final result. In the pro-
cess of manually judging questions, the annotators
are presented only the questions. Table 3 provides
the statistics on the final test set.
#queries #returned #relevant
CI TST 300 6,000 798
Table 3: Statistics on the Test Data
We evaluate the performance of our approach us-
ing Mean Average Precision (MAP). We perform
a significant test, i.e., a t-test with a default signif-
icant level of 0.05. Following the literature, we set
the parameters ? = 0.2 (Cao et al, 2010) in equa-
tions (1), (3) and (5), and ? = 0.8 (Xue et al, 2008)
in equation (6).
4.2 Question Retrieval Results
We randomly divide the test questions into five
subsets and conduct 5-fold cross-validation experi-
ments. In each trial, we tune the parameters ?1 and
?2 with four of the five subsets and then apply it to
one remaining subset. The experiments reported be-
low are those averaged over the five trials.
Table 4 presents the main retrieval performance.
Row 1 to row 3 are baseline systems, all these meth-
ods use word-based translation models and obtain
the state-of-the-art performance in previous work
(Jeon et al, 2005; Xue et al, 2008). Row 3 is simi-
lar to row 2, the only difference is that TransLM only
considers the question part, while Xue et al (2008)
incorporates the question part and answer part. Row
4 and row 5 are our proposed phrase-based trans-
lation model with maximum phrase length of five.
Row 4 is phrase-based translation model purely
based on question part, this model is equivalent to
659
# Methods Trans Prob MAP
1 Jeon et al (2005) Ppool 0.289
2 TransLM Ppool 0.324
3 Xue et al (2008) Ppool 0.352
4 P-Trans (?1 = 1, l = 5) Ppool 0.366
5 P-Trans (l = 5) Ppool 0.391
Table 4: Comparison with different methods for question
retrieval.
setting ?1 = 1 in equation (15). Row 5 is the phrase-
based combination model which linearly combines
the question part and answer part. As expected,
different parts can play different roles: a phrase to
be translated in queried questions may be translated
from the question part or answer part. All these
methods use pooling strategy to estimate the transla-
tion probabilities. There are some clear trends in the
result of Table 4:
(1) Word-based translation language model
(TransLM) significantly outperforms word-based
translation model of Jeon et al (2005) (row 1 vs. row
2). Similar observations have been made by Xue et
al. (2008).
(2) Incorporating the answer part into the models,
either word-based or phrase-based, can significantly
improve the performance of question retrieval (row
2 vs. row 3; row 4 vs. row 5).
(3) Our proposed phrase-based translation model
(P-Trans) significantly outperforms the state-of-the-
art word-based translation models (row 2 vs. row 4
and row 3 vs. row 5, all these comparisons are sta-
tistically significant at p < 0.05).
4.3 Impact of Phrase Length
Our proposed phrase-based translation model, due to
its capability of capturing contextual information, is
more effective than the state-of-the-art word-based
translation models. It is important to investigate the
impact of the phrase length on the final retrieval per-
formance. Table 5 shows the results, it is seen that
using the longer phrases up to the maximum length
of five can consistently improve the retrieval per-
formance. However, using much longer phrases in
the phrase-based translation model does not seem to
produce significantly better performance (row 8 and
row 9 vs. row 10 are not statistically significant).
# Systems MAP
6 P-Trans (l = 1) 0.352
7 P-Trans (l = 2) 0.373
8 P-Trans (l = 3) 0.386
9 P-Trans (l = 4) 0.390
10 P-Trans (l = 5) 0.391
Table 5: The impact of the phrase length on retrieval per-
formance.
Model # Methods Average MAP
P-Trans (l = 5) 11 Initial 69 0.38012 TextRank 24 0.391
Table 6: Effectiveness of parallel corpus preprocessing.
4.4 Effectiveness of Parallel Corpus
Preprocessing
Question-answer pairs collected from Yahoo! an-
swers are very noisy, it is possible for translation
models to contain ?unnecessary? translations. In this
paper, we attempt to identify and decrease the pro-
portion of unnecessary translations in a translation
model by using TextRank algorithm. This kind of
?unnecessary? translation between words will even-
tually affect the bi-phrase translation.
Table 6 shows the effectiveness of parallel corpus
preprocessing. Row 11 reports the average number
of translations per word and the question retrieval
performance when only stopwords 7 are removed.
When using the TextRank algorithm for parallel cor-
pus preprocessing, the average number of transla-
tions per word is reduced from 69 to 24, but the
performance of question retrieval is significantly im-
proved (row 11 vs. row 12). Similar results have
been made by Lee et al (2008).
4.5 Impact of Pooling Strategy
The correspondence of words or phrases in the
question-answer pair is not as strong as in the bilin-
gual sentence pair, thus noise will be inevitably in-
troduced for both P (a?|q?) and P (q?|a?).
To see how much the pooling strategy benefit the
question retrieval, we introduce two baseline meth-
ods for comparison. The first method (denoted as
P (a?|q?)) is used to denote the translation probabil-
ity with the question as the source and the answer as
7http://truereader.com/manuals/onix/stopwords1.html
660
Model # Trans Prob MAP
P-Trans (l = 5)
13 P (a?|q?) 0.387
14 P (q?|a?) 0.381
15 Ppool 0.391
Table 7: The impact of pooling strategy for question re-
trieval.
the target. The second (denoted as P (a?|q?)) is used
to denote the translation probability with the answer
as the source and the question as the target. Table 7
provides the comparison. From this Table, we see
that the pooling strategy significantly outperforms
the two baseline methods for question retrieval (row
13 and row 14 vs. row 15).
5 Conclusions and Future Work
In this paper, we propose a novel phrase-based trans-
lation model for question retrieval. Compared to
the traditional word-based translation models, the
proposed approach is more effective in that it can
capture contextual information instead of translating
single words in isolation. Experiments conducted
on real Q&A data demonstrate that the phrase-
based translation model significantly outperforms
the state-of-the-art word-based translation models.
There are some ways in which this research could
be continued. First, question structure should be
considered, so it is necessary to combine the pro-
posed approach with other question retrieval meth-
ods (e.g., (Duan et al, 2008; Wang et al, 2009;
Bunescu and Huang, 2010)) to further improve the
performance. Second, we will try to investigate the
use of the proposed approach for other kinds of data
set, such as categorized questions from forum sites
and FAQ sites.
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 60875041 and
No. 61070106). We thank the anonymous reviewers
for their insightful comments. We also thank Maoxi
Li and Jiajun Zhang for suggestion to use the align-
ment toolkits.
References
A. Berger and R. Caruana and D. Cohn and D. Freitag and
V. Mittal. 2000. Bridging the lexical chasm: statistical
approach to answer-finding. In Proceedings of SIGIR,
pages 192-199.
A. Berger and J. Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of SIGIR, pages
222-229.
D. Bernhard and I. Gurevych. 2009. Combining lexical
semantic resources with question & answer archives
for translation-based answer finding. In Proceedings
of ACL, pages 728-736.
P. F. Brown and V. J. D. Pietra and S. A. D. Pietra and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263-311.
R. Bunescu and Y. Huang. 2010. Learning the relative
usefulness of questions in community QA. In Pro-
ceedings of EMNLP, pages 97-107.
X. Cao and G. Cong and B. Cui and C. S. Jensen. 2010.
A generalized framework of exploring category infor-
mation for question retrieval in community question
answer archives. In Proceedings of WWW.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL.
H. Duan and Y. Cao and C. Y. Lin and Y. Yu. 2008.
Searching questions by identifying questions topics
and question focus. In Proceedings of ACL, pages
156-164.
J. Gao and X. He and J. Nie. 2010. Clickthrough-based
translation models for web search: from word models
to phrase models. In Proceedings of CIKM.
J. Jeon and W. Bruce Croft and J. H. Lee. 2005. Find-
ing similar questions in large question and answer
archives. In Proceedings of CIKM, pages 84-90.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into text. In Proceedings of EMNLP, pages 404-
411.
P. Koehn and F. Och and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL,
pages 48-54.
J. -T. Lee and S. -B. Kim and Y. -I. Song and H. -C. Rim.
2008. Bridging lexical gaps between queries and ques-
tions on large online Q&A collections with compact
translation models. In Proceedings of EMNLP, pages
410-418.
F. Och. 2002. Statistical mahcine translation: from sin-
gle word models to alignment templates. Ph.D thesis,
RWTH Aachen.
F. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computa-
tional Linguistics, 30(4):417-449.
661
J. M. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of
SIGIR.
W. H. Press and S. A. Teukolsky and W. T. Vetterling
and B. P. Flannery. 1992. Numerical Recipes In C.
Cambridge Univ. Press.
S. Robertson and S. Walker and S. Jones and M.
Hancock-Beaulieu and M. Gatford. 1994. Okapi at
trec-3. In Proceedings of TREC, pages 109-126.
G. Salton and A. Wong and C. S. Yang. 1975. A vector
space model for automatic indexing. Communications
of the ACM, 18(11):613-620.
X. Sun and J. Gao and D. Micol and C. Quirk. 2010.
Learning phrase-based spelling error models from
clickthrough data. In Proceedings of ACL.
K. Wang and Z. Ming and T-S. Chua. 2009. A syntactic
tree matching approach to finding similar questions in
community-based qa services. In Proceedings of SI-
GIR, pages 187-194.
X. Xue and J. Jeon and W. B. Croft. 2008. Retrieval
models for question and answer archives. In Proceed-
ings of SIGIR, pages 475-482.
C. Zhai and J. Lafferty. 2001. A study of smooth meth-
ods for language models applied to ad hoc information
retrieval. In Proceedings of SIGIR, pages 334-342.
662
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1556?1565,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Exploiting Web-Derived Selectional Preference to Improve Statistical
Dependency Parsing
Guangyou Zhou, Jun Zhao?, Kang Liu, and Li Cai
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,jzhao,kliu,lcai}@nlpr.ia.ac.cn
Abstract
In this paper, we present a novel approach
which incorporates the web-derived selec-
tional preferences to improve statistical de-
pendency parsing. Conventional selectional
preference learning methods have usually fo-
cused on word-to-class relations, e.g., a verb
selects as its subject a given nominal class.
This paper extends previous work to word-
to-word selectional preferences by using web-
scale data. Experiments show that web-scale
data improves statistical dependency pars-
ing, particularly for long dependency relation-
ships. There is no data like more data, perfor-
mance improves log-linearly with the number
of parameters (unique N-grams). More impor-
tantly, when operating on new domains, we
show that using web-derived selectional pref-
erences is essential for achieving robust per-
formance.
1 Introduction
Dependency parsing is the task of building depen-
dency links between words in a sentence, which has
recently gained a wide interest in the natural lan-
guage processing community. With the availabil-
ity of large-scale annotated corpora such as Penn
Treebank (Marcus et al, 1993), it is easy to train
a high-performance dependency parser using super-
vised learning methods.
However, current state-of-the-art statistical de-
pendency parsers (McDonald et al, 2005; McDon-
ald and Pereira, 2006; Hall et al, 2006) tend to have
?Correspondence author: jzhao@nlpr.ia.ac.cn
lower accuracies for longer dependencies (McDon-
ald and Nivre, 2007). The length of a dependency
from word wi to word wj is simply equal to |i ? j|.
Longer dependencies typically represent the mod-
ifier of the root or the main verb, internal depen-
dencies of longer NPs or PP-attachment in a sen-
tence. Figure 1 shows the F1 score1 relative to the
dependency length on the development set by using
the graph-based dependency parsers (McDonald et
al., 2005; McDonald and Pereira, 2006). We note
that the parsers provide very good results for adja-
cent dependencies (96.89% for dependency length
=1), while the dependency length increases, the ac-
curacies degrade sharply. These longer dependen-
cies are therefore a major opportunity to improve the
overall performance of dependency parsing. Usu-
ally, these longer dependencies can be parsed de-
pendent on the specific words involved due to the
limited range of features (e.g., a verb and its mod-
ifiers). Lexical statistics are therefore needed for
resolving ambiguous relationships, yet the lexical-
ized statistics are sparse and difficult to estimate di-
rectly. To solve this problem, some information with
different granularity has been investigated. Koo et
al. (2008) proposed a semi-supervised dependency
parsing by introducing lexical intermediaries at a
coarser level than words themselves via a cluster
method. This approach, however, ignores the se-
lectional preference for word-to-word interactions,
such as head-modifier relationship. Extra resources
1Precision represents the percentage of predicted arcs of
length d that are correct, and recall measures the percentage
of gold-standard arcs of length d that are correctly predicted.
F1 = 2? precision ? recall/(precision + recall)
1556
1 5 10 15 20 25 300.7
0.75
0.8
0.85
0.9
0.95
1
Dependency Length
F1 S
core
 (%)
MST1MST2
Figure 1: F score relative to dependency length.
beyond the annotated corpora are needed to capture
the bi-lexical relationship at the word-to-word level.
Our purpose in this paper is to exploit web-
derived selectional preferences to improve the su-
pervised statistical dependency parsing. All of our
lexical statistics are derived from two kinds of web-
scale corpus: one is the web, which is the largest
data set that is available for NLP (Keller and Lap-
ata, 2003). Another is a web-scale N-gram corpus,
which is a N-gram corpus with N-grams of length 1-
5 (Brants and Franz, 2006), we call it Google V1 in
this paper. The idea is very simple: web-scale data
have large coverage for word pair acquisition. By
leveraging some assistant data, the dependency pars-
ing model can directly utilize the additional informa-
tion to capture the word-to-word level relationships.
We address two natural and related questions which
some previous studies leave open:
Question I: Is there a benefit in incorporating
web-derived selectional preference features for sta-
tistical dependency parsing, especially for longer de-
pendencies?
Question II: How well do web-derived selec-
tional preferences perform on new domains?
For Question I, we systematically assess the value
of using web-scale data in state-of-the-art super-
vised dependency parsers. We compare dependency
parsers that include or exclude selectional prefer-
ence features obtained from web-scale corpus. To
the best of our knowledge, none of the existing stud-
ies directly address long dependencies of depen-
dency parsing by using web-scale data.
Most statistical parsers are highly domain depen-
dent. For example, the parsers trained on WSJ text
perform poorly on Brown corpus. Some studies have
investigated domain adaptation for parsers (Mc-
Closky et al, 2006; Daume? III, 2007; McClosky et
al., 2010). These approaches assume that the parsers
know which domain it is used, and that it has ac-
cess to representative data in that domain. How-
ever, in practice, these assumptions are unrealistic
in many real applications, such as when processing
the heterogeneous genre of web texts. In this paper
we incorporate the web-derived selectional prefer-
ence features to design our parsers for robust open-
domain testing.
We conduct the experiments on the English Penn
Treebank (PTB) (Marcus et al, 1993). The results
show that web-derived selectional preference can
improve the statistical dependency parsing, partic-
ularly for long dependency relationships. More im-
portantly, when operating on new domains, the web-
derived selectional preference features show great
potential for achieving robust performance (Section
4.3).
The remainder of this paper is divided as follows.
Section 2 gives a brief introduction of dependency
parsing. Section 3 describes the web-derived selec-
tional preference features. Experimental evaluation
and results are reported in Section 4. Finally, we dis-
cuss related work and draw conclusion in Section 5
and Section 6, respectively.
2 Dependency Parsing
In dependency parsing, we attempt to build head-
modifier (or head-dependent) relations between
words in a sentence. The discriminative parser we
used in this paper is based on the part-factored
model and features of the MSTParser (McDonald et
al., 2005; McDonald and Pereira, 2006; Carreras,
2007). The parsing model can be defined as a con-
ditional distribution p(y|x;w) over each projective
parse tree y for a particular sentence x, parameter-
ized by a vector w. The probability of a parse tree
is
p(y|x;w) = 1
Z(x;w)
exp
{
?
??y
w ??(x, ?)
}
(1)
where Z(x;w) is the partition function and ? are
part-factored feature functions that include head-
1557
modifier parts, sibling parts and grandchild parts.
Given the training set {(xi, yi)}Ni=1, parameter es-
timation for log-linear models generally resolve
around optimization of a regularized conditional
log-likelihood objective w? = argminwL(w)
where
L(w) = ?C
N
?
i=1
logp(yi|xi;w) +
1
2
||w||2 (2)
The parameter C > 0 is a constant dictating the
level of regularization in the model. Since objec-
tive function L(w) is smooth and convex, which is
convenient for standard gradient-based optimization
techniques. In this paper we use the dual exponenti-
ated gradient (EG)2 descent, which is a particularly
effective optimization algorithm for log-linear mod-
els (Collins et al, 2008).
3 Web-Derived Selectional Preference
Features
In this paper, we employ two different feature sets:
a baseline feature set3 which draw upon ?normal?
information source, such as word forms and part-of-
speech (POS) without including the web-derived se-
lectional preference4 features, a feature set conjoins
the baseline features and the web-derived selectional
preference features.
3.1 Web-scale resources
All of our selectional preference features described
in this paper rely on probabilities derived from unla-
beled data. To use the largest amount of data possi-
ble, we exploit web-scale resources. one is web, N-
gram counts are approximated by Google hits. An-
other we use isGoogle V1 (Brants and Franz, 2006).
This N-gram corpus records how often each unique
sequence of words occurs. N-grams appearing 40
2http://groups.csail.mit.edu/nlp/egstra/
3This kind of feature sets are similar to other feature sets in
the literature (McDonald et al, 2005; Carreras, 2007), so we
will not attempt to give a exhaustive description.
4Selectional preference tells us which arguments are plau-
sible for a particular predicate, one way to determine the se-
lectional preference is from co-occurrences of predicates and
arguments in text (Bergsma et al, 2008). In this paper, the
selectional preferences have the same meaning with N-grams,
which model the word-to-word relationships, rather than only
considering the predicates and arguments relationships.
obj
detdet
root
obj
mod
subj
Figure 2: An example of a labeled dependency tree. The
tree contains a special token ?$? which is always the root
of the tree. Each arc is directed from head to modifier and
has a label describing the function of the attachment.
times or more (1 in 25 billion) are kept, and appear
in the n-gram tables. All n-grams with lower counts
are discarded. Co-occurrence probabilities can be
calculated directly from the N-gram counts.
3.2 Web-derived N-gram features
3.2.1 PMI
Previous work on noun compounds bracketing
has used adjacency model (Resnik, 1993) and de-
pendency model (Lauer, 1995) to compute associa-
tion statistics between pairs of words. In this pa-
per we generalize the adjacency and dependency
models by including the pointwise mutual informa-
tion (Church and Hanks, 1900) between all pairs of
words in the dependency tree:
PMI(x, y) = log p(?x y?)
p(?x?)p(?y?)
(3)
where p(?x y?) is the co-occurrence probabilities.
When use the Google V1 corpus, this probabilities
can be calculated directly from the N-gram counts,
while using the Google hits, we send the queries to
the search engine Google5 and all the search queries
are performed as exact matches by using quotation
marks.6
The value of these features is the PMI, if it is de-
fined. If the PMI is undefined, following the work
of (Pitler et al, 2010), we include one of two binary
features:
p(?x y?) = 0 or p(?x?) ? p(?y?) = 0
Besides, we also consider the trigram features be-
5http://www.google.com/
6Google only allows automated querying through the
Google Web API, this involves obtaining a license key, which
then restricts the number of queries to a daily quota of 1000.
However, we obtained a quota of 20,000 queries per day by
sending a request to api-support@google.com for research pur-
poses.
1558
PMI(?hit with?)
xi-word=?hit?, xj-word=?with?, PMI(?hit with?)
xi-word=?hit?, xj-word=?with?, xj-pos=?IN?, PMI(?hit with?)
xi-word=?hit?, xi-pos=?VBD?, xj-word=?with?, PMI(?hit with?)
xi-word=?hit?, b-pos=?ball?, xj-word=?with?, PMI(?hit with?)
xi-word=?hit?, xj-word=?with?, PMI(?hit with?), dir=R, dist=3
. . .
Table 1: An example of the N-gram PMI features and the conjoin features with the baseline.
tween the three words in the dependency tree:
PMI(x, y, z) = log p(?x y z?)
p(?x y?)p(?y z?)
(4)
This kinds of trigram features, for example in MST-
Parser, which can directly capture the sibling and
grandchild features.
We illustrate the PMI features with an example
of dependency parsing tree in Figure 2. In deciding
the dependency between the main verb hit and its ar-
gument headed preposition with, an example of the
N-gram PMI features and the conjoin features with
the baseline are shown in Table 1.
3.2.2 PP-attachment
Propositional phrase (PP) attachment is one of
the hardest problems in English dependency pars-
ing. An English sentence consisting of a subject, a
verb, and a nominal object followed by a preposi-
tional phrase is often ambiguous. Ambiguity resolu-
tion reflects the selectional preference between the
verb and noun with their prepositional phrase. For
example, considering the following two examples:
(1) John hit the ball with the bat.
(2) John hit the ball with the red stripe.
In sentence (1), the preposition with depends on the
main verb hit; but in sentence (2), the prepositional
phrase is a noun attribute and the preposition with
needs to depends on the word ball. To resolve this
kind of ambiguity, there needs to measure the attach-
ment preference. We thus have PP-attachment fea-
tures that determine the PMI association across the
preposition word ?IN?7:
PMIIN (x, z) = log
p(?x IN z?)
p(x)
(5)
7Here, the preposition word ?IN? (e.g., ?with?, ?in?, . . .) is
any token whose part-of-speech is IN
N-gram feature templates
hw, mw, PMI(hw,mw)
hw, ht, mw, PMI(hw,mw)
hw, mw, mt, PMI(hw,mw)
hw, ht, mw, mt, PMI(hw,mw)
. . .
hw, mw, sw
hw, mw, sw, PMI(hw, mw, sw)
hw, mw, gw
hw, mw, gw, PMI(hw, mw, gw)
Table 2: Examples of N-gram feature templates. Each
entry represents a class of indicator for tuples of informa-
tion. For example, ?hw, mw? reprsents a class of indi-
cator features with one feature for each possible combi-
nation of head word and modifier word. Abbreviations:
hw=head word, ht= head POS. st, gt=likewise for sibling
and grandchild.
PMIIN (y, z) = log
p(?y IN z?)
p(y)
(6)
where the word x and y are usually verb and noun,
z is a noun which directly depends on the preposi-
tion word ?IN?. For example in sentence (1), we
would include the features PMIwith(hit, bat) and
PMIwith(ball, bat). If both PMI features exist and
PMIwith(hit, bat) > PMIwith(ball, bat), indicating
to our dependency parsing model that the preposi-
tion word with depends on the verb hit is a good
choice. While in sentence (2), the features include
PMIwith(hit, stripe) and PMIwith(ball, stripe).
3.3 N-gram feature templates
We generate N-gram features by mimicking the
template structure of the original baseline features.
For example, the baseline feature set includes indi-
cators for word-to-word and tag-to-tag interactions
between the head and modifier of a dependency. In
the N-gram feature set, we correspondingly intro-
duce N-gram PMI for word-to-word interactions.
1559
The N-gram feature set for MSTParser is shown
in Table 2. Following McDonald et al (2005),
all features are conjoined with the direction of
attachment as well as the distance between the two
words creating the dependency. In between N-gram
features, we include the form of word trigrams
and PMI of the trigrams. The surrounding word
N-gram features represent the local context of the
selectional preference. Besides, we also present
the second-order feature templates, including the
sibling and grandchild features. These features are
designed to disambiguate cases like coordinating
conjunctions and prepositional attachment. Con-
sider the examples we have shown in section 3.2.2,
for sentence (1), the dependency graph path feature
ball ? with ? bat should have a lower weight
since ball rarely is modified by bat, but is often
seen through them (e.g., a higher weight should be
associated with hit ? with ? bat). In contrast,
for sentence (2), our N-gram features will tell us
that the prepositional phrase is much more likely
to attach to the noun since the dependency graph
path feature ball ? with ? stripe should have a
high weight due to the high strength of selectional
preference between ball and stripe.
Web-derived selectional preference features
based on PMI values are trickier to incorporate
into the dependency parsing model because they
are continuous rather than discrete. Since all the
baseline features used in the literature (McDonald et
al., 2005; Carreras, 2007) take on binary values of 0
or 1, there is a ?mis-match? between the continuous
and binary features. Log-linear dependency parsing
model is sensitive to inappropriately scaled feature.
To solve this problem, we transform the PMI
values into a more amenable form by replacing the
PMI values with their z-score. The z-score of a
PMI value x is x??? , where ? and ? are the mean
and standard deviation of the PMI distribution,
respectively.
4 Experiments
In order to evaluate the effectiveness of our proposed
approach, we conducted dependency parsing exper-
iments in English. The experiments were performed
on the Penn Treebank (PTB) (Marcus et al, 1993),
using a standard set of head-selection rules (Yamada
and Matsumoto, 2003) to convert the phrase struc-
ture syntax of the Treebank into a dependency tree
representation, dependency labels were obtained via
the ?Malt? hard-coded setting.8 We split the Tree-
bank into a training set (Sections 2-21), a devel-
opment set (Section 22), and several test sets (Sec-
tions 0,9 1, 23, and 24). The part-of-speech tags for
the development and test set were automatically as-
signed by the MXPOST tagger10, where the tagger
was trained on the entire training corpus.
Web page hits for word pairs and trigrams are ob-
tained using a simple heuristic query to the search
engine Google.11 Inflected queries are performed
by expanding a bigram or trigram into all its mor-
phological forms. These forms are then submitted as
literal queries, and the resulting hits are summed up.
John Carroll?s suite of morphological tools12 is used
to generate inflected forms of verbs and nouns. All
the search terms are performed as exact matches by
using quotation marks and submitted to the search
engines in lower case.
We measured the performance of the parsers us-
ing the following metrics: unlabeled attachment
score (UAS), labeled attachment score (LAS) and
complete match (CM), which were defined by Hall
et al (2006). All the metrics are calculated as mean
scores per word, and punctuation tokens are consis-
tently excluded.
4.1 Main results
There are some clear trends in the results of Ta-
ble 3. First, performance increases with the order
of the parser: edge-factored model (dep1) has the
lowest performance, adding sibling and grandchild
relationships (dep2) significantly increases perfor-
mance. Similar observations regarding the effect of
model order have also been made by Carreras (2007)
and Koo et al (2008).
Second, note that the parsers incorporating the N-
gram feature sets consistently outperform the mod-
els using the baseline features in all test data sets,
regardless of model order or label usage. Another
8http://w3.msi.vxu.se/ nivre/research/MaltXML.html
9We removed a single 249-word sentence from Section 0 for
computational reasons.
10http://www.inf.ed.ac.uk/resources/nlp/local doc/MXPOST.html
11http://www.google.com/
12http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html.
1560
Sec dep1 +hits +V1 dep2 +hits +V1 dep1-L +hits-L +V1-L dep2-L +hits-L +V1-L
00 90.39 90.94 90.91 91.56 92.16 92.16 90.11 90.69 90.67 91.94 92.47 92.42
01 91.01 91.60 91.60 92.27 92.89 92.86 90.77 91.39 91.39 91.81 92.38 92.37
23 90.82 91.46 91.39 91.98 92.64 92.59 90.30 90.98 90.92 91.24 91.83 91.77
24 89.53 90.15 90.13 90.81 91.44 91.41 89.42 90.03 90.02 90.30 90.91 90.89
Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:
dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from
the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are
scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
finding is that the N-gram features derived from
Google hits are slightly better than Google V1 due
to the large N-gram coverage, we will discuss later.
As a final note, all the comparisons between the inte-
gration of N-gram features and the baseline features
in Table 3 are mildly significant using the Z-test of
Collins et al (2005) (p < 0.08).
Type Systems UAS CM
D
Yamada and Matsumoto (2003) 90.3 38.7
McDonald et al (2005) 90.9 37.5
McDonald and Pereira (2006) 91.5 42.1
Corston-Oliver et al (2006) 90.9 37.5
Hall et al (2006) 89.4 36.4
Wang et al (2007) 89.2 34.4
Carreras et al (2008) 93.5 -
GoldBerg and Elhadad (2010)? 91.32 40.41
Ours 92.64 46.61
C
Nivre and McDonald (2008)? 92.12 44.37
Martins et al (2008)? 92.87 45.51
Zhang and Clark (2008) 92.1 45.4
S
Koo et al (2008) 93.16 -
Suzuki et al (2009) 93.79 -
Chen et al (2009) 93.16 47.15
Table 4: Comparison of our final results with other best-
performing systems on the whole Section 23. Type
D, C and S denote discriminative, combined and semi-
supervised systems, respectively. ? These papers were
not directly reported the results on this data set, we im-
plemented the experiments in this paper.
To put our results in perspective, we also com-
pare them with other best-performing systems in Ta-
ble 4. To facilitate comparisons with previous work,
we only use Section 23 as the test data. The re-
sults show that our second order model incorpo-
rating the N-gram features (92.64) performs better
than most previously reported discriminative sys-
tems trained on the Treebank. Carreras et al (2008)
reported a very high accuracy using information of
constituent structure of TAG grammar formalism,
while in our system, we do not use such knowl-
edge. When compared to the combined systems, our
system is better than Nivre and McDonald (2008)
and Zhang and Clark (2008), but a slightly worse
than Martins et al (2008). We also compare our
method with the semi-supervised approaches, the
semi-supervised approaches achieved very high ac-
curacies by leveraging on large unlabeled data di-
rectly into the systems for joint learning and decod-
ing, while in our method, we only explore the N-
gram features to further improve supervised depen-
dency parsing performance.
Table 5 shows the details of some other N-gram
sources, where NEWS: created from a large set of
news articles including the Reuters and Gigword
(Graff, 2003) corpora. For a given number of unique
N-gram, using any of these sources does not have
significant difference in Figure 3. Google hits is
the largest N-gram data and shows the best perfor-
mance. The other two are smaller ones, accuracies
increase linearly with the log of the number of types
in the auxiliary data set. Similar observations have
been made by Pitler et al (2010). We see that the
relationship between accuracy and the number of N-
gram is not monotonic for Google V1. The reason
may be that Google V1 does not make detailed pre-
processing, containing many mistakes in the corpus.
Although Google hits is noisier, it has very much
larger coverage of bigrams or trigrams.
Some previous studies also found a log-linear
relationship between unlabeled data (Suzuki and
Isozaki, 2008; Suzuki et al, 2009; Bergsma et al,
2010; Pitler et al, 2010). We have shown that this
trend continues well for dependency parsing by us-
ing web-scale data (NEWS and Google V1).
13Google indexes about more than 8 billion pages and each
contains about 1,000 words on average.
1561
Corpus # of tokens ? # of types
NEWS 3.2B 1 3.7B
Google V1 1,024.9B 40 3.4B
Google hits13 8,000B 100 -
Table 5: N-gram data, with total number of words in the
original corpus (in billions, B). Following (Brants and
Franz, 2006; Pitler et al, 2010), we set the frequency
threshold to filter the data ?, and total number of unique
N-gram (types) remaining in the data.
1e4 1e5 1e6 1e7 1e8 1e991.9
92
92.1
92.2
92.3
92.4
92.5
92.6
92.7
Number of Unique N-grams
UAS
 Sco
re (%
)
NEWSGoogle V1Google hits
Figure 3: There is no data like more data. UAS accu-
racy improves with the number of unique N-grams but
still lower than the Google hits.
4.2 Improvement relative to dependency length
The experiments in (McDonald and Nivre, 2007)
showed a negative impact on the dependency pars-
ing performance from too long dependencies. For
our proposed approach, the improvement relative
to dependency length is shown in Figure 4. From
the Figure, it is seen that our method gives observ-
able better performance when dependency lengths
are larger than 3. The results here show that the
proposed approach improves the dependency pars-
ing performance, particularly for long dependency
relationships.
4.3 Cross-genre testing
In this section, we present the experiments to vali-
date the robustness the web-derived selectional pref-
erences. The intent is to understand how well the
web-derived selectional preferences transfer to other
sources.
The English experiment evaluates the perfor-
mance of our proposed approach when it is trained
1 10 20 300.75
0.8
0.85
0.9
0.95
1
Dependency Length
F1 S
core
 (%)
MST2MST2+N-gram
Figure 4: Dependency length vs. F1 score.
on annotated data from one genre of text (WSJ) and
is used to parse a test set from a different genre: the
biomedical domain related to cancer (PennBioIE.,
2005) with 2,600 parsed sentences. We divided the
data into 500 for training, 100 for development and
others for testing. We created five sets of train-
ing data with 100, 200, 300, 400, and 500 sen-
tences respectively. Figure 5 plots the UAS ac-
curacy as function of training instances. WSJ is
the performance of our second-order dependency
parser trained on section 2-21; WSJ+N-gram is the
performance of our proposed approach trained on
section 2-21; WSJ+BioMed is the performance of
the parser trained on WSJ and biomedical data.
WSJ+BioMed+N-gram is the performance of our
proposed approach trained on WSJ and biomedical
data. The results show that incorporating the web-
scale N-gram features can significantly improve the
dependency parsing performance, and the improve-
ment is much larger than the in-domain testing pre-
sented in Section 4.1, the reason may be that web-
derived N-gram features do not depend directly on
training data and thus work better on new domains.
4.4 Discussion
In this paper, we present a novel method to im-
prove dependency parsing by using web-scale data.
Despite the success, there are still some problems
which should be discussed.
(1) Google hits is less sparse than Google V1
in modeling the word-to-word relationships, but
Google hits are likely to be noisier than Google V1.
It is very appealing to carry out a correlation anal-
1562
100 150 200 250 300 350 400 450 50080
81
82
83
84
85
86
87
88
UAS
 Sco
re (%
)
WSJWSJ+N-gramWSJ+BioMedWSJ+BioMed+N-gram
Figure 5: Adapting a WSJ parser to biomedical text.
WSJ: performance of parser trained only on WSJ;
WSJ+N-gram: performance of our proposed approach
trained only on WSJ; WSJ+BioMed: parser trained on
WSJ and biomedical text; WSJ+BioMed+N-gram: our
approach trained on WSJ and biomedical text.
ysis to determine whether Google hits and Google
V1 are highly correlated. We will leave it for future
research.
(2) Veronis (2005) pointed out that there had been
a debate about reliability of Google hits due to the
inconsistencies of page hits estimates. However, this
estimate is scale-invariant. Assume that when the
number of pages indexed by Google grows, the num-
ber of pages containing a given search term goes to
a fixed fraction. This means that if pages indexed
by Google doubles, then so do the bigrams or tri-
grams frequencies. Therefore, the estimate becomes
stable when the number of indexed pages grows un-
boundedly. Some details are presented in Cilibrasi
and Vitanyi (2007).
5 Related Work
Our approach is to exploit web-derived selectional
preferences to improve the dependency parsing. The
idea of this paper is inspired by the work of Suzuki
et al (2009) and Pitler et al (2010). The former uses
the web-scale data explicitly to create more data for
training the model; while the latter explores the web-
scale N-grams data (Lin et al, 2010) for compound
bracketing disambiguation. Our research, however,
applies the web-scale data (Google hits and Google
V1) to model the word-to-word dependency rela-
tionships rather than compound bracketing disam-
biguation.
Several previous studies have exploited the web-
scale data for word pair acquisition. Keller and
Lapata (2003) evaluated the utility of using web
search engine statistics for unseen bigram. Nakov
and Hearst (2005) demonstrated the effectiveness of
using search engine statistics to improve the noun
compound bracketing. Volk (2001) exploited the
WWWas a corpus to resolve PP attachment ambigu-
ities. Turney (2007) measured the semantic orienta-
tion for sentiment classification using co-occurrence
statistics obtained from the search engines. Bergsma
et al (2010) created robust supervised classifiers
via web-scale N-gram data for adjective ordering,
spelling correction, noun compound bracketing and
verb part-of-speech disambiguation. Our approach,
however, extends these techniques to dependency
parsing, particularly for long dependency relation-
ships, which involves more challenging tasks than
the previous work.
Besides, there are some work exploring the word-
to-word co-occurrence derived from the web-scale
data or a fixed size of corpus (Calvo and Gel-
bukh, 2004; Calvo and Gelbukh, 2006; Yates et al,
2006; Drabek and Zhou, 2000; van Noord, 2007)
for PP attachment ambiguities or shallow parsing.
Johnson and Riezler (2000) incorporated the lex-
ical selectional preference features derived from
British National Corpus (Graff, 2003) into a stochas-
tic unification-based grammar. Abekawa and Oku-
mura (2006) improved Japanese dependency pars-
ing by using the co-occurrence information derived
from the results of automatic dependency parsing of
large-scale corpora. However, we explore the web-
scale data for dependency parsing, the performance
improves log-linearly with the number of parameters
(unique N-grams). To the best of our knowledge,
web-derived selectional preference has not been suc-
cessfully applied to dependency parsing.
6 Conclusion
In this paper, we present a novel method which in-
corporates the web-derived selectional preferences
to improve statistical dependency parsing. The re-
sults show that web-scale data improves the de-
pendency parsing, particularly for long dependency
relationships. There is no data like more data,
performance improves log-linearly with the num-
1563
ber of parameters (unique N-grams). More impor-
tantly, when operating on new domains, the web-
derived selectional preferences show great potential
for achieving robust performance.
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 60875041 and
No. 61070106), and CSIDM project (No. CSIDM-
200805) partially funded by a grant from the Na-
tional Research Foundation (NRF) administered by
the Media Development Authority (MDA) of Singa-
pore. We thank the anonymous reviewers for their
insightful comments.
References
T. Abekawa and M. Okumura. 2006. Japanese depen-
dency parsing using co-occurrence information and a
combination of case elements. In Proceedings of ACL-
COLING.
S. Bergsma, D. Lin, and R. Goebel. 2008. Discriminative
learning of selectional preference from unlabeled text.
In Proceedings of EMNLP, pages 59-68.
S. Bergsma, E. Pitler, and D. Lin. 2010. Creating robust
supervised classifier via web-scale N-gram data. In
Proceedings of ACL.
T. Brants and Alex Franz. 2006. The Google Web 1T
5-gram Corpus Version 1.1. LDC2006T13.
H. Calvo and A. Gelbukh. 2004. Acquiring selec-
tional preferences from untagged text for prepositional
phrase attachment disambiguation. In Proceedings of
VLDB.
H. Calvo and A. Gelbukh. 2006. DILUCT: An open-
source Spanish dependency parser based on rules,
heuristics, and selectional preferences. In Lecture
Notes in Computer Science 3999, pages 164-175.
X. Carreras. 2007. Experiments with a higher-order pro-
jective dependency parser. In Proceedings of EMNLP-
CoNLL, pages 957-961.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In Proceedings of CoNLL.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.
2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC
No. LDC2000T43.Linguistic Data Consortium.
W. Chen, D. Kawahara, K. Uchimoto, and Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP,
pages 570-579.
K. W. Church and P. Hanks. 1900. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22-29.
R. L. Cilibrasi and P. M. B. Vitanyi. 2007. The Google
similarity distance. IEEE Transaction on Knowledge
and Data Engineering, 19(3):2007. pages 370-383.
M. Collins, A. Globerson, T. Koo, X. Carreras, and P.
L. Bartlett. 2008. Exponentiated gradient algorithm
for conditional random fields and max-margin markov
networks. Journal of Machine Learning Research,
pages 1775?1822.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of ACL, pages 531-540.
S. Corston-Oliver, A. Aue, Kevin. Duh, and E. Ringger.
2006. Multilingual dependency parsing using bayes
point machines. In Proceedings of NAACL.
H. Daume? III. 2007. Frustrating easy domain adaptation.
In Proceedings of ACL.
E. F. Drabek and Q. Zhou. 2000. Using co-occurrence
statistics as an information source for partial parsing of
Chinese. In Proceedings of Second Chinese Language
Processing Workshop, ACL, pages 22-28.
Y. GoldBerg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proceedings of NAACL, pages 742-750.
D. Graff. 2003. English Gigaword, LDC2003T05.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discrimina-
tive classifier for deterministic dependency parsing. In
Proceedings of ACL, pages 316-323.
M. Johnson and S. Riezler. 2000. Exploiting auxiliary
distribution in stochastic unification-based garmmars.
In Proceedings of NAACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
of ACL, pages 595-603.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459-484.
M. Lapata and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1), pages 1-30.
M. Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
ACL.
D. K. Lin, H. Church, S. Ji, S. Sekine, D. Yarowsky, S.
Bergsma, K. Patil, E. Pitler, E. Lathbury, V Rao, K.
Dalwani, and S. Narsale. 2010. New tools for web-
scale n-grams. In Proceedings of LREC.
M.P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. Computational Linguistics.
1564
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proceedings
of EMNLP, pages 157-166.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of ACL.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic Domain Adapatation for Parsing. In Proceed-
ings of NAACL-HLT.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81-88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, pages 91-98.
P. Nakov and M. Hearst. 2005. Search engine statis-
tics beyond the n-gram: application to noun compound
bracketing. In Proceedings of CoNLL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL, pages 950-958.
G. van Noord. 2007. Using self-trained bilexical pref-
erences to improve disambiguation accuracy. In Pro-
ceedings of IWPT, pages 1-10.
PennBioIE. 2005. Mining the bibliome project, 2005.
http:bioie.ldc.upenn.edu/.
E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Us-
ing web-scale N-grams to improve base NP parsing
performance. In Proceedings of COLING, pages 886-
894.
P. Resnik. 1993. Selection and information: a class-
based approach to lexical relationships. Ph.D. thesis,
University of Pennsylvania.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of EMNLP, pages 551-560.
J. Suzuki and H. Isozaki. 2008. Semi-supervised sequen-
tial labeling and segmentation using giga-word scale
unlabeled data. In Proceedings of ACL, pages 665-
673.
P. D. Turney. 2003. Measuring praise and criticism:
Inference of semantic orientation from association.
ACM Transactions on Information Systems, 21(4).
J. Veronis. 2005. Web: Google adjusts its counts. Jean
Veronis? blog: http://aixtal.blogsplot.com/2005/03/
web-google-adjusts-its-count.html.
M. Volk. 2001. Exploiting the WWW as corpus to re-
solve PP attachment ambiguities. In Proceedings of
the Corpus Linguistics.
Q. I. Wang, D. Lin, and D. Schuurmans. 2007. Simple
training of dependency parsers via structured boosting.
In Proceedings of IJCAI, pages 1756-1762.
Yamada and Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proceedings
of IWPT, pages 195-206.
A. Yates, S. Schoenmackers, and O. Etzioni. 2006. De-
tecting parser errors using web-based semantic filters.
In Proceedings of EMNLP, pages 27-34.
Y. Zhang and S. Clark. 2008. A tale of two parsers: in-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of EMNLP, pages 562-571.
1565
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 852?861,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Statistical Machine Translation Improves Question Retrieval in
Community Question Answering via Matrix Factorization
Guangyou Zhou, Fang Liu, Yang Liu, Shizhu He, and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,fliu,liuyang09,shizhu.he,jzhao}@nlpr.ia.ac.cn
Abstract
Community question answering (CQA)
has become an increasingly popular re-
search topic. In this paper, we focus on the
problem of question retrieval. Question
retrieval in CQA can automatically find
the most relevant and recent questions that
have been solved by other users. However,
the word ambiguity and word mismatch
problems bring about new challenges for
question retrieval in CQA. State-of-the-art
approaches address these issues by implic-
itly expanding the queried questions with
additional words or phrases using mono-
lingual translation models. While use-
ful, the effectiveness of these models is
highly dependent on the availability of
quality parallel monolingual corpora (e.g.,
question-answer pairs) in the absence of
which they are troubled by noise issue.
In this work, we propose an alternative
way to address the word ambiguity and
word mismatch problems by taking advan-
tage of potentially rich semantic informa-
tion drawn from other languages. Our pro-
posed method employs statistical machine
translation to improve question retrieval
and enriches the question representation
with the translated words from other lan-
guages via matrix factorization. Experi-
ments conducted on a real CQA data show
that our proposed approach is promising.
1 Introduction
With the development of Web 2.0, community
question answering (CQA) services like Yahoo!
Answers,1 Baidu Zhidao2 and WkiAnswers3 have
attracted great attention from both academia and
industry (Jeon et al, 2005; Xue et al, 2008;
Adamic et al, 2008; Wang et al, 2009; Cao et al,
2010). In CQA, anyone can ask and answer ques-
tions on any topic, and people seeking information
are connected to those who know the answers. As
answers are usually explicitly provided by human,
they can be helpful in answering real world ques-
tions.
In this paper, we focus on the task of question
retrieval. Question retrieval in CQA can automati-
cally find the most relevant and recent questions
(historical questions) that have been solved by
other users, and then the best answers of these his-
torical questions will be used to answer the users?
queried questions. However, question retrieval is
challenging partly due to the word ambiguity and
word mismatch between the queried questions
and the historical questions in the archives. Word
ambiguity often causes the retrieval models to re-
trieve many historical questions that do not match
the users? intent. This problem is also amplified
by the high diversity of questions and users. For
example, depending on different users, the word
?interest? may refer to ?curiosity?, or ?a charge
for borrowing money?.
Another challenge is word mismatch between
the queried questions and the historical questions.
The queried questions may contain words that are
different from, but related to, the words in the rele-
vant historical questions. For example, if a queried
question contains the word ?company? but a rele-
vant historical question instead contains the word
?firm?, then there is a mismatch and the historical
1http://answers.yahoo.com/
2http://zhidao.baidu.com/
3http://wiki.answers.com/
852
English Chinese
word ambiguity
How do I get a loan ?(w?)??(r?h?)?(c?ng)
from a bank? ??(y?nh?ng)??(d?iku?n)?
How to reach the ??(r?h?)??(qi?nw?ng)
bank of the river? ??(h??n)?
word mismatch
company ??(g?ngs?)
firm ??(g?ngs?)
rheum ??(g?nm?o)
catarrh ??(g?nm?o)
Table 1: Google translate: some illustrative examples.
question may not be easily distinguished from an
irrelevant one.
Researchers have proposed the use of word-
based translation models (Berger et al, 2000;
Jeon et al, 2005; Xue et al, 2008; Lee et al,
2008; Bernhard and Gurevych, 2009) to solve
the word mismatch problem. As a principle ap-
proach to capture semantic word relations, word-
based translation models are built by using the
IBM model 1 (Brown et al, 1993) and have
been shown to outperform traditional models (e.g.,
VSM, BM25, LM) for question retrieval. Be-
sides, Riezler et al (2007) and Zhou et al (2011)
proposed the phrase-based translation models for
question and answer retrieval. The basic idea is
to capture the contextual information in model-
ing the translation of phrases as a whole, thus
the word ambiguity problem is somewhat allevi-
ated. However, all these existing studies in the
literature are basically monolingual approaches
which are restricted to the use of original language
of questions. While useful, the effectiveness of
these models is highly dependent on the availabil-
ity of quality parallel monolingual corpora (e.g.,
question-answer pairs) in the absence of which
they are troubled by noise issue. In this work,
we propose an alternative way to address the word
ambiguity and word mismatch problems by taking
advantage of potentially rich semantic information
drawn from other languages. Through other lan-
guages, various ways of adding semantic informa-
tion to a question could be available, thereby lead-
ing to potentially more improvements than using
the original language only.
Taking a step toward using other languages, we
propose the use of translated representation by al-
ternatively enriching the original questions with
the words from other languages. The idea of im-
proving question retrieval with statistical machine
translation is based on the following two observa-
tions: (1) Contextual information is exploited dur-
ing the translation from one language to another.
For example in Table 1, English words ?interest?
and ?bank? that have multiple meanings under
different contexts are correctly addressed by us-
ing the state-of-the-art translation tool ??Google
Translate.4 Thus, word ambiguity based on con-
textual information is naturally involved when
questions are translated. (2) Multiple words that
have similar meanings in one language may be
translated into an unique word or a few words in a
foreign language. For example in Table 1, English
words such as ?company? and ?firm? are trans-
lated into ??? (g?ngs?)?, ?rheum? and ?catarrh?
are translated into ???(g?nm?o)? in Chinese.
Thus, word mismatch problem can be somewhat
alleviated by using other languages.
Although Zhou et al (2012) exploited bilin-
gual translation for question retrieval and obtained
the better performance than traditional monolin-
gual translation models. However, there are two
problems with this enrichment: (1) enriching
the original questions with the translated words
from other languages increases the dimensionality
and makes the question representation even more
sparse; (2) statistical machine translation may in-
troduce noise, which can harm the performance of
question retrieval. To solve these two problems,
we propose to leverage statistical machine transla-
tion to improve question retrieval via matrix fac-
torization.
The remainder of this paper is organized as fol-
lows. Section 2 describes the proposed method
by leveraging statistical machine translation to im-
prove question retrieval via matrix factorization.
Section 3 presents the experimental results. In sec-
tion 4, we conclude with ideas for future research.
4http://translate.google.com/translate t
853
2 Our Approach
2.1 Problem Statement
This paper aims to leverage statistical machine
translation to enrich the question representation.
In order to address the word ambiguity and word
mismatch problems, we expand a question by
adding its translation counterparts. Statistical ma-
chine translation (e.g., Google Translate) can uti-
lize contextual information during the question
translation, so it can solve the word ambiguity and
word mismatch problems to some extent.
Let L = {l1, l2, . . . , lP } denote the language
set, where P is the number of languages con-
sidered in the paper, l1 denotes the original lan-
guage (e.g., English) while l2 to lP are the for-
eign languages. Let D1 = {d(1)1 , d(1)2 , . . . , d(1)N }
be the set of historical question collection in origi-
nal language, where N is the number of historical
questions in D1 with vocabulary size M1. Now
we first translate each original historical question
from language l1 into other languages lp (p ?
[2, P ]) by Google Translate. Thus, we can ob-
tain D2, . . . , DP in different languages, and Mp is
the vocabulary size of Dp. A question d(p)i in Dp
is simply represented as a Mp dimensional vector
d(p)i , in which each entry is calculated by tf-idf.
The N historical questions in Dp are then repre-
sented in a Mp ? N term-question matrix Dp =
{d(p)1 ,d
(p)
2 , . . . ,d
(p)
N }, in which each row corre-
sponds to a term and each column corresponds to
a question.
Intuitively, we can enrich the original ques-
tion representation by adding the translated words
from language l2 to lP , the original vocabu-
lary size is increased from M1 to ?Pp=1 Mp.
Thus, the term-question matrix becomes D =
{D1,D2, . . . ,DP } and D ? R(
?P
p=1 Mp)?N .
However, there are two problems with this enrich-
ment: (1) enriching the original questions with the
translated words from other languages makes the
question representation even more sparse; (2) sta-
tistical machine translation may introduce noise.5
To solve these two problems, we propose to
leverage statistical machine translation to improve
question retrieval via matrix factorization. Figure
1 presents the framework of our proposed method,
where qi represents a queried question, and qi is a
vector representation of qi.
5Statistical machine translation quality is far from satis-
factory in real applications.
??
??
??
??
 
HistoricalQuestionCollectionRepresentation
 
QueryRepresentation
Figure 1: Framework of our proposed approach
for question retrieval.
2.2 Model Formulation
To tackle the data sparseness of question represen-
tation with the translated words, we hope to find
two or more lower dimensional matrices whose
product provides a good approximate to the orig-
inal one via matrix factorization. Previous stud-
ies have shown that there is psychological and
physiological evidence for parts-based representa-
tion in the human brain (Wachsmuth et al, 1994).
The non-negative matrix factorization (NMF) is
proposed to learn the parts of objects like text
documents (Lee and Seung, 2001). NMF aims
to find two non-negative matrices whose product
provides a good approximation to the original ma-
trix and has been shown to be superior to SVD in
document clustering (Xu et al, 2003; Tang et al,
2012).
In this paper, NMF is used to induce the reduced
representation Vp of Dp, Dp is independent on
{D1,D2, . . . ,Dp?1,Dp+1, . . . ,DP }. When ig-
noring the coupling between Vp, it can be solved
by minimizing the objective function as follows:
O1(Up,Vp) = minUp?0,Vp?0 ?Dp ?UpVp?
2
F (1)
where ? ? ?F denotes Frobenius norm of a matrix.
Matrices Up ? RMp?K and Vp ? RK?N are the
reduced representation for terms and questions in
the K dimensional space, respectively.
To reduce the noise introduced by statistical ma-
chine translation, we assume that Vp from lan-
guage Dp (p ? [2, P ]) should be close to V1
854
from the original language D1. Based on this as-
sumption, we minimize the distance between Vp
(p ? [2, P ]) and V1 as follows:
O2(Vp) = minVp?0
P?
p=2
?Vp ?V1?2F (2)
Combining equations (1) and (2), we get the fol-
lowing objective function:
O(U1, . . . ,UP ;V1, . . . ,VP ) (3)
=
P?
p=1
?Dp ?UpVp?2F +
P?
p=2
?p?Vp ?V1?2F
where parameter ?p (p ? [2, P ]) is used to adjust
the relative importance of these two components.
If we set a small value for ?p, the objective func-
tion behaves like the traditional NMF and the im-
portance of data sparseness is emphasized; while a
big value of ?p indicatesVp should be very closed
to V1, and equation (3) aims to remove the noise
introduced by statistical machine translation.
By solving the optimization problem in equa-
tion (4), we can get the reduced representation of
terms and questions.
minO(U1, . . . ,UP ;V1, . . . ,VP ) (4)
subject to : Up ? 0,Vp ? 0, p ? [1, P ]
2.3 Optimization
The objective function O defined in equation (4)
performs data sparseness and noise removing si-
multaneously. There are 2P coupling components
in O, and O is not convex in both U and V to-
gether. Therefore it is unrealistic to expect an al-
gorithm to find the global minima. In the follow-
ing, we introduce an iterative algorithm which can
achieve local minima. In our optimization frame-
work, we optimize the objective function in equa-
tion (4) by alternatively minimizing each compo-
nent when the remaining 2P ? 1 components are
fixed. This procedure is summarized in Algorithm
1.
2.3.1 Update of MatrixUp
Holding V1, . . . ,VP and U1, . . . ,Up?1,Up+1,
. . . ,UP fixed, the update of Up amounts to the
following optimization problem:
min
Up?0
?Dp ?UpVp?2F (5)
Algorithm 1 Optimization framework
Input: Dp ? Rmp?N , p ? [1, P ]
1: for p = 1 : P do
2: V(0)p ? RK?N ? random matrix
3: for t = 1 : T do  T is iteration times
4: U(t)p ? UpdateU(Dp,V(t?1)p )
5: V(t)p ? UpdateV(Dp,U(t)p )
6: end for
7: returnU(T )p , V(T )p
8: end for
Algorithm 2 Update Up
Input: Dp ? RMp?N , Vp ? RK?N
1: for i = 1 : Mp do
2: u?(p)?i = (VpVTp )?1Vpd?(p)i
3: end for
4: returnUp
Let d?(p)i = (d(p)i1 , . . . , d(p)iK )T and u?(p)i =
(u(p)i1 , . . . , u
(p)
iK )T be the column vectors whose en-
tries are those of the ith row of Dp and Up re-
spectively. Thus, the optimization of equation (5)
can be decomposed into Mp optimization prob-
lems that can be solved independently, with each
corresponding to one row of Up:
min
u?(p)i ?0
?d?(p)i ?VTp u?
(p)
i ?22 (6)
for i = 1, . . . ,Mp.
Equation (6) is a standard least squares prob-
lems in statistics and the solution is:
u?(p)?i = (VpVTp )?1Vpd?
(p)
i (7)
Algorithm 2 shows the procedure.
2.3.2 Update of MatrixVp
Holding U1, . . . ,UP and V1, . . . ,Vp?1,Vp+1,
. . . ,VP fixed, the update of Vp amounts to the
optimization problem divided into two categories.
if p ? [2, P ], the objective function can be writ-
ten as:
min
Vp?0
?Dp ?UpVp?2F + ?p?Vp ?V1?2F (8)
if p = 1, the objective function can be written
as:
min
Vp?0
?Dp ?UpVp?2F + ?p?Vp?2F (9)
855
Let d(p)j be the jth column vector of Dp, and
v(p)j be the jth column vector of Vp, respectively.
Thus, equation (8) can be rewritten as:
min
{v(p)j ?0}
N?
j=1
?d(p)j ?Upv
(p)
j ?22+
N?
j=1
?p?v(p)j ?v
(1)
j ?22
(10)
which can be decomposed into N optimization
problems that can be solved independently, with
each corresponding to one column of Vp:
min
v(p)j ?0
?d(p)j ?Upv
(p)
j ?22+?p?v
(p)
j ?v
(1)
j ?22 (11)
for j = 1, . . . , N .
Equation (12) is a least square problem with L2
norm regularization. Now we rewrite the objective
function in equation (12) as
L(v(p)j ) = ?d
(p)
j ?Upv
(p)
j ?22 + ?p?v
p
j ? v
(1)
j ?22
(12)
where L(v(1)j ) is convex, and hence has a unique
solution. Taking derivatives, we obtain:
?L(v(p)j )
?v(p)j
= ?2UTp (d(p)j ?Upv
(p)
j )+2?p(v
(p)
j ?v
(1)
j )
(13)
Forcing the partial derivative to be zero leads to
v(p)?j = (UTpUp + ?pI)?1(UTp d
(p)
j + ?pv
(1)
j )
(14)
where p ? [2, P ] denotes the foreign language rep-
resentation.
Similarly, the solution of equation (9) is:
v(p)?j = (UTpUp + ?pI)?1UTp d
(p)
j (15)
where p = 1 denotes the original language repre-
sentation.
Algorithm 3 shows the procedure.
2.4 Time Complexity Analysis
In this subsection, we discuss the time complex-
ity of our proposed method. The optimization
u?(p)i using Algorithm 2 should calculate VpVTp
and Vpd?(p)i , which takes O(NK2 + NK) op-
erations. Therefore, the optimization Up takes
O(NK2 + MpNK) operations. Similarly, the
time complexity of optimization Vi using Algo-
rithm 3 is O(MpK2 + MpNK).
Another time complexity is the iteration times
T used in Algorithm 1 and the total number of
Algorithm 3 Update Vp
Input: Dp ? RMp?N , Up ? RMp?K
1: ?? (UTpUp + ?pI)?1
2: ?? UTpDp
3: if p = 1 then
4: for j = 1 : N do
5: v(p)j ? ??j , ?j is the jth column of ?
6: end for
7: end if
8: returnV1
9: if p ? [2, P ] then
10: for j = 1 : N do
11: v(p)j ? ?(?j + ?pv(1)j )
12: end for
13: end if
14: returnVp
languages P , the overall time complexity of our
proposed method is:
P?
p=1
T ?O(NK2 + MpK2 + 2MpNK) (16)
For each language Dp, the size of vocabulary
Mp is almost constant as the number of questions
increases. Besides, K ? min(Mp, N), theoreti-
cally, the computational time is almost linear with
the number of questions N and the number of lan-
guages P considered in the paper. Thus, the pro-
posed method can be easily adapted to the large-
scale information retrieval task.
2.5 Relevance Ranking
The advantage of incorporating statistical machine
translation in relevance ranking is to reduce ?word
ambiguity? and ?word mismatch? problems. To
do so, given a queried question q and a historical
question d from Yahoo! Answers, we first trans-
late q and d into other foreign languages (e.g., Chi-
nese, French etc.) and get the corresponding trans-
lated representation qi and di (i ? [2, P ]), where
P is the number of languages considered in the pa-
per. For queried question q = q1, we represent it
in the reduced space:
vq1 = argminv?0 ?q1 ?U1v?
2
2 + ?1?v?22 (17)
where vector q1 is the tf-idf representation of
queried question q1 in the term space. Similarly,
for historical question d = d1 (and its tf-idf repre-
sentation d1 in the term space) we represent it in
the reduced space as vd1 .
856
The relevance score between the queried ques-
tion q1 and the historical question d1 in the re-
duced space is, then, calculated as the cosine sim-
ilarity between vq1 and vd1 :
s(q1, d1) =
< vq1 ,vd1 >
?vq1?2 ? ?vd1?2
(18)
For translated representation qi (i ? [2, P ]), we
also represent it in the reduced space:
vqi = argminv?0 ?qi?Uiv?
2
2+?i?v?vq1?22 (19)
where vector qi is the tf-idf representation of qi
in the term space. Similarly, for translated rep-
resentation di (and its tf-idf representation di in
the term space) we also represent it in the reduced
space as vdi . The relevance score s(qi, di) be-
tween qi and di in the reduced space can be cal-
culated as the cosine similarity between vqi and
vdi .
Finally, we consider learning a relevance func-
tion of the following general, linear form:
Score(q, d) = ?T ??(q, d) (20)
where feature vector ?(q, d) =
(sV SM (q, d), s(q1, d1), s(q2, d2), . . . , s(qP , dP )),
and ? is the corresponding weight vector, we
optimize this parameter for our evaluation metrics
directly using the Powell Search algorithm (Paul
et al, 1992) via cross-validation. sV SM (q, d) is
the relevance score in the term space and can be
calculated using Vector Space Model (VSM).
3 Experiments
3.1 Data Set and Evaluation Metrics
We collect the data set from Yahoo! Answers and
use the getByCategory function provided in Ya-
hoo! Answers API6 to obtain CQA threads from
the Yahoo! site. More specifically, we utilize
the resolved questions and the resulting question
repository that we use for question retrieval con-
tains 2,288,607 questions. Each resolved ques-
tion consists of four parts: ?question title?, ?ques-
tion description?, ?question answers? and ?ques-
tion category?. For question retrieval, we only use
the ?question title? part. It is assumed that ques-
tion title already provides enough semantic infor-
mation for understanding the users? information
needs (Duan et al, 2008). There are 26 categories
6http://developer.yahoo.com/answers
Category #Size Category # Size
Arts & Humanities 86,744 Home & Garden 35,029
Business & Finance 105,453 Beauty & Style 37,350
Cars & Transportation 145,515 Pet 54,158
Education & Reference 80,782 Travel 305,283
Entertainment & Music 152,769 Health 132,716
Family & Relationships 34,743 Sports 214,317
Politics & Government 59,787 Social Science 46,415
Pregnancy & Parenting 43,103 Ding out 46,933
Science & Mathematics 89,856 Food & Drink 45,055
Computers & Internet 90,546 News & Events 20,300
Games & Recreation 53,458 Environment 21,276
Consumer Electronics 90,553 Local Businesses 51,551
Society & Culture 94,470 Yahoo! Products 150,445
Table 2: Number of questions in each first-level
category.
at the first level and 1,262 categories at the leaf
level. Each question belongs to a unique leaf cat-
egory. Table 2 shows the distribution across first-
level categories of the questions in the archives.
We use the same test set in previous work (Cao
et al, 2009; Cao et al, 2010). This set contains
252 queried questions and can be freely down-
loaded for research communities.7
The original language of the above data set is
English (l1) and then they are translated into four
other languages (Chinese (l2), French (l3), Ger-
man (l4), Italian (l5)), thus the number of language
considered is P = 5) by using the state-of-the-art
translation tool ??Google Translate.
Evaluation Metrics: We evaluate the perfor-
mance of question retrieval using the following
metrics: Mean Average Precision (MAP) and
Precision@N (P@N). MAP rewards methods that
return relevant questions early and also rewards
correct ranking of the results. P@N reports the
fraction of the top-N questions retrieved that are
relevant. We perform a significant test, i.e., a t-
test with a default significant level of 0.05.
We tune the parameters on a small development
set of 50 questions. This development set is also
extracted from Yahoo! Answers, and it is not in-
cluded in the test set. For parameter K, we do an
experiment on the development set to determine
the optimal values among 50, 100, 150, ? ? ? , 300 in
terms of MAP. Finally, we set K = 100 in the ex-
periments empirically as this setting yields the best
performance. For parameter ?1, we set ?1 = 1
empirically, while for parameter ?i (i ? [2, P ]),
we set ?i = 0.25 empirically and ensure that?
i ?i = 1.
7http://homepages.inf.ed.ac.uk/gcong/qa/
857
# Methods MAP P@10
1 VSM 0.242 0.226
2 LM 0.385 0.242
3 Jeon et al (2005) 0.405 0.247
4 Xue et al (2008) 0.436 0.261
5 Zhou et al (2011) 0.452 0.268
6 Singh (2012) 0.450 0.267
7 Zhou et al (2012) 0.483 0.275
8 SMT + MF (P = 2, l1, l2) 0.527 0.284
9 SMT + MF (P = 5) 0.564 0.291
Table 3: Comparison with different methods for
question retrieval.
3.2 Question Retrieval Results
Table 3 presents the main retrieval performance.
Row 1 and row 2 are two baseline systems, which
model the relevance score using VSM (Cao et al,
2010) and language model (LM) (Zhai and Laf-
ferty, 2001; Cao et al, 2010) in the term space.
Row 3 and row 6 are monolingual translation mod-
els to address the word mismatch problem and
obtain the state-of-the-art performance in previ-
ous work. Row 3 is the word-based translation
model (Jeon et al, 2005), and row 4 is the word-
based translation language model, which linearly
combines the word-based translation model and
language model into a unified framework (Xue et
al., 2008). Row 5 is the phrase-based translation
model, which translates a sequence of words as
whole (Zhou et al, 2011). Row 6 is the entity-
based translation model, which extends the word-
based translation model and explores strategies to
learn the translation probabilities between words
and the concepts using the CQA archives and a
popular entity catalog (Singh, 2012). Row 7 is
the bilingual translation model, which translates
the English questions from Yahoo! Answers into
Chinese questions using Google Translate and ex-
pands the English words with the translated Chi-
nese words (Zhou et al, 2012). For these previ-
ous work, we use the same parameter settings in
the original papers. Row 8 and row 9 are our pro-
posed method, which leverages statistical machine
translation to improve question retrieval via ma-
trix factorization. In row 8, we only consider two
languages (English and Chinese) and translate En-
glish questions into Chinese using Google Trans-
late in order to compare with Zhou et al (2012).
In row 9, we translate English questions into other
four languages. There are some clear trends in the
result of Table 3:
(1) Monolingual translation models signifi-
cantly outperform the VSM and LM (row 1 and
row 2 vs. row 3, row 4, row 5 and row 6).
(2) Taking advantage of potentially rich seman-
tic information drawn from other languages via
statistical machine translation, question retrieval
performance can be significantly improved (row 3,
row 4, row 5 and row 6 vs. row 7, row 8 and row 9,
all these comparisons are statistically significant at
p < 0.05).
(3) Our proposed method (leveraging statisti-
cal machine translation via matrix factorization,
SMT + MF) significantly outperforms the bilin-
gual translation model of Zhou et al (2012) (row
7 vs. row 8, the comparison is statistically signifi-
cant at p < 0.05). The reason is that matrix factor-
ization used in the paper can effectively solve the
data sparseness and noise introduced by the ma-
chine translator simultaneously.
(4) When considering more languages, ques-
tion retrieval performance can be further improved
(row 8 vs. row 9).
Note that Wang et al (2009) also addressed the
word mismatch problem for question retrieval by
using syntactic tree matching. We do not compare
with Wang et al (2009) in Table 3 because pre-
vious work (Ming et al, 2010) demonstrated that
word-based translation language model (Xue et
al., 2008) obtained the superior performance than
the syntactic tree matching (Wang et al, 2009).
Besides, some other studies attempt to improve
question retrieval with category information (Cao
et al, 2009; Cao et al, 2010), label ranking (Li et
al., 2011) or world knowledge (Zhou et al, 2012).
However, their methods are orthogonal to ours,
and we suspect that combining the category infor-
mation or label ranking into our proposed method
might get even better performance. We leave it for
future research.
3.3 Impact of the Matrix Factorization
Our proposed method (SMT +MF) can effectively
solve the data sparseness and noise via matrix fac-
torization. To further investigate the impact of
the matrix factorization, one intuitive way is to
expand the original questions with the translated
words from other four languages, without consid-
ering the data sparseness and noise introduced by
machine translator. We compare our SMT + MF
with this intuitive enriching method (SMT + IEM).
Besides, we also employ our proposed matrix fac-
torization to the original question representation
(VSM + MF). Table 4 shows the comparison.
858
# Methods MAP P@10
1 VSM 0.242 0.226
2 VSM + MF 0.411 0.253
3 SMT + IEM (P = 5) 0.495 0.280
4 SMT + MF (P = 5) 0.564 0.291
Table 4: The impact of matrix factorization.
(1) Our proposed matrix factorization can sig-
nificantly improve the performance of question re-
trieval (row 1 vs. row2; row3 vs. row4, the
improvements are statistically significant at p <
0.05). The results indicate that our proposed ma-
trix factorization can effectively address the issues
of data spareness and noise introduced by statisti-
cal machine translation.
(2) Compared to the relative improvements of
row 3 and row 4, the relative improvements of row
1 and row 2 is much larger. The reason may be
that although matrix factorization can be used to
reduce dimension, it may impair the meaningful
terms.
(3) Compared to VSM, the performance of
SMT + IEM is significantly improved (row 1
vs. row 3), which supports the motivation that
the word ambiguity and word mismatch problems
could be partially addressed by Google Translate.
3.4 Impact of the Translation Language
One of the success of this paper is to take ad-
vantage of potentially rich semantic information
drawn from other languages to solve the word am-
biguity and word mismatch problems. So we con-
struct a dummy translator (DT) that translates an
English word to itself. Thus, through this trans-
lation, we do not add any semantic information
into the original questions. The comparison is pre-
sented in Table 5. Row 1 (DT + MF) represents
integrating two copies of English questions with
our proposed matrix factorization. From Table 5,
we have several different findings:
(1) Taking advantage of potentially rich seman-
tic information drawn from other languages can
significantly improve the performance of question
retrieval (row 1 vs. row 2, row 3, row 4 and row 5,
the improvements relative to DT + MF are statisti-
cally significant at p < 0.05).
(2) Different languages contribute unevenly for
question retrieval (e.g., row 2 vs. row 3). The
reason may be that the improvements of leverag-
ing different other languages depend on the qual-
ity of machine translation. For example, row 3
# Methods MAP
1 DT + MF (l1, l1) 0.352
2 SMT + MF (P = 2, l1, l2) 0.527
3 SMT + MF (P = 2, l1, l3) 0.553
4 SMT + MF (P = 2, l1, l4) 0.536
5 SMT + MF (P = 2, l1, l5) 0.545
6 SMT + MF (P = 3, l1, l2, l3) 0.559
7 SMT + MF (P = 4, l1, l2, l3, l4) 0.563
8 SMT + MF (P = 5, l1, l2, l3, l4, l5) 0.564
Table 5: The impact of translation language.
Method Translation MAP
SMT + MF (P = 2, l1, l2) Dict 0.468GTrans 0.527
Table 6: Impact of the contextual information.
is better than row 2 because the translation qual-
ity of English-French is much better than English-
Chinese.
(3) Using much more languages does not seem
to produce significantly better performance (row 6
and row 7 vs. row 8). The reason may be that in-
consistency between different languages may exist
due to statistical machine translation.
3.5 Impact of the Contextual Information
In this paper, we translate the English questions
into other four languages using Google Translate
(GTrans), which takes into account contextual in-
formation during translation. If we translate a
question word by word, it discards the contextual
information. We would expect that such a transla-
tion would not be able to solve the word ambiguity
problem.
To investigate the impact of contextual infor-
mation for question retrieval, we only consider
two languages and translate English questions
into Chinese using an English to Chinese lexicon
(Dict) in StarDict8. Table 6 shows the experi-
mental results, we can see that the performance is
degraded when the contextual information is not
considered for the translation of questions. The
reason is that GTrans is context-dependent and
thus produces different translated Chinese words
depending on the context of an English word.
Therefore, the word ambiguity problem can be
solved during the English-Chinese translation.
4 Conclusions and Future Work
In this paper, we propose to employ statistical ma-
chine translation to improve question retrieval and
8StarDict is an open source dictionary software, available
at http://stardict.sourceforge.net/.
859
enrich the question representation with the trans-
lated words from other languages via matrix fac-
torization. Experiments conducted on a real CQA
data show some promising findings: (1) the pro-
posed method significantly outperforms the pre-
vious work for question retrieval; (2) the pro-
posed matrix factorization can significantly im-
prove the performance of question retrieval, no
matter whether considering the translation lan-
guages or not; (3) considering more languages can
further improve the performance but it does not
seem to produce significantly better performance;
(4) different languages contribute unevenly for
question retrieval; (5) our proposed method can
be easily adapted to the large-scale information re-
trieval task.
As future work, we plan to incorporate the ques-
tion structure (e.g., question topic and question fo-
cus (Duan et al, 2008)) into the question represen-
tation for question retrieval. We also want to fur-
ther investigate the use of the proposed method for
other kinds of data set, such as categorized ques-
tions from forum sites and FAQ sites.
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 61070106, No.
61272332 and No. 61202329), the National High
Technology Development 863 Program of China
(No. 2012AA011102), the National Basic Re-
search Program of China (No. 2012CB316300),
We thank the anonymous reviewers for their in-
sightful comments. We also thank Dr. Gao Cong
for providing the data set and Dr. Li Cai for some
discussion.
References
L. Adamic, J. Zhang, E. Bakshy, and M. Ackerman.
2008. Knowledge sharing and yahoo answers: ev-
eryone knows and something. In Proceedings of
WWW.
A. Berger, R. Caruana, D. Cohn, D. Freitag, and V.Mit-
tal. 2000. Bridging the lexical chasm: statistical ap-
proach to answer-finding. In Proceedings of SIGIR,
pages 192-199.
D. Bernhard and I. Gurevych. 2009. Combining
lexical semantic resources with question & answer
archives for translation-based answer finding. In
Proceedings of ACL, pages 728-736.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263-311.
X. Cao, G. Cong, B. Cui, C. Jensen, and C. Zhang.
2009. The use of categorization information in lan-
guage models for question retrieval. In Proceedings
of CIKM, pages 265-274.
X. Cao, G. Cong, B. Cui, and C. Jensen. 2010. A
generalized framework of exploring category infor-
mation for question retrieval in community question
answer archives. In Proceedings of WWW, pages
201-210.
H. Duan, Y. Cao, C. Y. Lin, and Y. Yu. 2008. Searching
questions by identifying questions topics and ques-
tion focus. In Proceedings of ACL, pages 156-164.
C. L. Lawson and R. J. Hanson. 1974. Solving least
squares problems. Prentice-Hall.
J. -T. Lee, S. -B. Kim, Y. -I. Song, and H. -C. Rim.
2008. Bridging lexical gaps between queries and
questions on large online Q&A collections with
compact translation models. In Proceedings of
EMNLP, pages 410-418.
W. Wang, B. Li, and I. King. 2011. Improving ques-
tion retrieval in community question answering with
label ranking. In Proceedings of IJCNN, pages 349-
356.
D. D. Lee and H. S. Seung. 2001. Algorithms for
non-negative matrix factorization. In Proceedings
of NIPS.
Z. Ming, K. Wang, and T. -S. Chua. 2010. Prototype
hierarchy based clustering for the categorization and
navigation of web collections. In Proceedings of SI-
GIR, pages 2-9.
J. Jeon, W. Croft, and J. Lee. 2005. Finding similar
questions in large question and answer archives. In
Proceedings of CIKM, pages 84-90.
C. Paige and M. Saunders. 1982. LSQR: an algo-
rithm for sparse linear equations and sparse least
squares. ACM Transaction on Mathematical Soft-
ware, 8(1):43-71.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B.
P. Flannery. 1992. Numerical Recipes In C. Cam-
bridge Univ. Press.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal,
and Y. Liu. 2007. Statistical machine translation for
query expansion in answer retrieval. In Proceedings
of ACL, pages 464-471.
A. Singh. 2012. Entity based q&a retrieval. In Pro-
ceedings of EMNLP-CoNLL, pages 1266-1277.
J. Tang, X. Wang, H. Gao, X. Hu, and H. Liu. 2012.
Enriching short text representation in microblog for
clustering. Front. Comput., 6(1):88-101.
860
E. Wachsmuth, M. W. Oram, and D. I. Perrett. 1994.
Recognition of objects and their component parts:
responses of single units in the temporal cortex of
teh macaque. Cerebral Cortex, 4:509-522.
K. Wang, Z. Ming, and T-S. Chua. 2009. A syntac-
tic tree matching approach to find similar questions
in community-based qa services. In Proceedings of
SIGIR, pages 187-194.
B. Wang, X. Wang, C. Sun, B. Liu, and L. Sun. 2010.
Modeling semantic relevance for question-answer
pairs in web social communities. In Proceedings of
ACL, pages 1230-1238.
W. Xu, X. Liu, and Y. Gong. 2003. Document cluster-
ing based on non-negative matrix factorization. In
Proceedings of SIGIR, pages 267-273.
X. Xue, J. Jeon, and W. B. Croft. 2008. Retrieval mod-
els for question and answer archives. In Proceedings
of SIGIR, pages 475-482.
C. Zhai and J. Lafferty. 2001. A study of smooth meth-
ods for language models applied to ad hoc informa-
tion retrieval. In Proceedings of SIGIR, pages 334-
342.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-
based translation model for question retrieval in
community question answer archives. In Proceed-
ings of ACL, pages 653-662.
G. Zhou, K. Liu, and J. Zhao. 2012. Exploiting bilin-
gual translation for question retrieval in community-
based question answering. In Proceedings of COL-
ING, pages 3153-3170.
G. Zhou, Y. Liu, F. Liu, D. Zeng, and J. Zhao. 2013.
Improving Question Retrieval in Community Ques-
tion Answering Using World Knowledge. In Pro-
ceedings of IJCAI.
861
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 104?109,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Inference for Heterogeneous Dependency Parsing
Guangyou Zhou and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,jzhao}@nlpr.ia.ac.cn
Abstract
This paper is concerned with the problem
of heterogeneous dependency parsing. In
this paper, we present a novel joint infer-
ence scheme, which is able to leverage
the consensus information between het-
erogeneous treebanks in the parsing phase.
Different from stacked learning meth-
ods (Nivre and McDonald, 2008; Martins
et al, 2008), which process the depen-
dency parsing in a pipelined way (e.g., a
second level uses the first level outputs), in
our method, multiple dependency parsing
models are coordinated to exchange con-
sensus information. We conduct experi-
ments on Chinese Dependency Treebank
(CDT) and Penn Chinese Treebank (CTB),
experimental results show that joint infer-
ence can bring significant improvements
to all state-of-the-art dependency parsers.
1 Introduction
Dependency parsing is the task of building depen-
dency links between words in a sentence, which
has recently gained a wide interest in the natu-
ral language processing community and has been
used for many problems ranging from machine
translation (Ding and Palmer, 2004) to question
answering (Zhou et al, 2011a). Over the past few
years, supervised learning methods have obtained
state-of-the-art performance for dependency pars-
ing (Yamada and Matsumoto, 2003; McDonald
et al, 2005; McDonald and Pereira, 2006; Hall
et al, 2006; Zhou et al, 2011b; Zhou et al,
2011c). These methods usually rely heavily on
the manually annotated treebanks for training the
dependency models. However, annotating syntac-
?(with) ??(eyes) ??(cast) ??(Hongkong )   BA                      NN                 VV                         NR
?(with) ??(eyes) ??(cast) ??(Hongkong )      p                       n                      v                            ns
Figure 1: Different grammar formalisms of syn-
tactic structures between CTB (upper) and CDT
(below). CTB is converted into dependency gram-
mar based on the head rules of (Zhang and Clark,
2008).
tic structure, either phrase-based or dependency-
based, is both time consuming and labor intensive.
Making full use of the existing manually annotated
treebanks would yield substantial savings in data-
annotation costs.
In this paper, we present a joint inference
scheme for heterogenous dependency parsing.
This scheme is able to leverage consensus in-
formation between heterogenous treebanks dur-
ing the inference phase instead of using individual
output in a pipelined way, such as stacked learning
methods (Nivre and McDonald, 2008; Martins et
al., 2008). The basic idea is very simple: although
heterogenous treebanks have different grammar
formalisms, they share some consensus informa-
tion in dependency structures for the same sen-
tence. For example in Figure 1, the dependency
structures actually share some partial agreements
for the same sentence, the two words ?eyes? and
?Hongkong? depend on ?cast? in both Chinese
Dependency Treebank (CDT) (Liu et al, 2006)
and Penn Chinese Treebank (CTB) (Xue et al,
2005). Therefore, we would like to train the de-
pendency parsers on individual heterogenous tree-
bank and jointly parse the same sentences with
consensus information exchanged between them.
The remainder of this paper is divided as fol-
104
Treebank1 Treebank2Parser1 Parser2
 consensus information exchangeJoint inference test data
Figure 2: General joint inference scheme of het-
erogeneous dependency parsing.
lows. Section 2 gives a formal description of
the joint inference for heterogeneous dependency
parsing. In section 3, we present the experimental
results. Finally, we conclude with ideas for future
research.
2 Our Approach
The general joint inference scheme of heteroge-
neous dependency parsing is shown in Figure 2.
Here, heterogeneous treebanks refer to two Chi-
nese treebanks: CTB and CDT, therefore we have
only two parsers, but the framework is generic
enough to integrate more parsers. For easy expla-
nation of the joint inference scheme, we regard a
parser without consensus information as a base-
line parser, a parser incorporates consensus infor-
mation called a joint parser. Joint inference pro-
vides a framework that accommodates and coordi-
nates multiple dependency parsing models. Sim-
ilar to Li et al (2009) and Zhu et al (2010),
the joint inference for heterogeneous dependency
parsing consists of four components: (1) Joint In-
ference Model; (2) Parser Coordination; (3) Joint
Inference Features; (4) Parameter Estimation.
2.1 Joint Inference Model
For a given sentence x, a joint dependency parsing
model finds the best dependency parsing tree y?
among the set of possible candidate parses Y(x)
based on a scoring function Fs:
y? = argmax
y?Y(x)
Fs(x, y) (1)
Following (Li et al, 2009), we will use dk to de-
note the kth joint parser, and also use the notation
Hk(x) for a list of parse candidates of sentence
x determined by dk. The sth joint parser can be
written as:
Fs(x, y) = Ps(x, y) +
?
k,k ?=s
?k(y,Hk(x)) (2)
where Ps(x, y) is the score function of the sth
baseline model, and each?k(y,Hk(x)) is a partial
consensus score function with respect to dk and is
defined over y andHk(x):
?k(y,Hk(x)) =
?
l
?k,lfk,l(y,Hk(x)) (3)
where each fk,l(y,Hk(x)) is a feature function
based on a consensus measure between y and
Hk(x), and ?k,l is the corresponding weight pa-
rameter. Feature index l ranges over all consensus-
based features in equation (3).
2.2 Parser Coordination
Note that in equation (2), though the baseline score
function Ps(x, y) can be computed individually,
the case of ?k(y,Hk(x)) is more complicated. It
is not feasible to enumerate all parse candidates
for dependency parsing. In this paper, we use a
bootstrapping method to solve this problem. The
basic idea is that we can use baseline models? n-
best output as seeds, and iteratively refine joint
models? n-best output with joint inference. The
joint inference process is shown in Algorithm 1.
Algorithm 1 Joint inference for multiple parsers
Step1: For each joint parser dk, perform inference with
a baseline model, and memorize all dependency parsing
candidates generated during inference in Hk(x);
Step2: For each candidate in Hk(x), we extract subtrees
and store them inH?k(x). First, we extract bigram-subtreesthat contain two words. If two words have a dependency
relation, we add these two words as a subtree into H?k(x).Similarly, we can extract trigram-subtrees. Note that the
dependency direction is kept. Besides, we also store the
?ROOT? word of each candidate in H?k(x);
Step3: Use joint parsers to re-parse the sentence x with
the baseline features and joint inference features (see sub-
section 2.3). For joint parser dk, consensus-based features
of any dependency parsing candidate are computed based
on current setting of H?s(x) for all s but k. New depen-
dency parsing candidates generated by dk in re-parsing are
cached in H??k(x);
Step4: Update all Hk(x) with H??k(x);
Step5: Iterate from Step2 to Step4 until a preset iteration
limit is reached.
In Algorithm 1, dependency parsing candidates
of different parsers can be mutually improved. For
example, given two parsers d1 and d2 with candi-
dates H1 and H2, improvements on H1 enable d2
to improve H2, and H1 benefits from improved
H2, and so on.
We can see that a joint parser does not en-
large the search space of its baseline model, the
only change is parse scoring. By running a com-
plete inference process, joint model can be applied
to re-parsing all candidates explored by a parser.
105
Thus Step3 can be viewed as full-scale candidates
reranking because the reranking scope is beyond
the limited n-best output currently cached inHk.
2.3 Joint Inference Features
In this section we introduce the consensus-based
feature functions fk,l(y,Hk(x)) introduced in
equation (3). The formulation can be written as:
fk,l(y,Hk(x)) =
?
y??Hk(x)
P (y?|dk)Il(y, y?) (4)
where y is a dependency parse of x by using parser
ds (s ?= k), y? is a dependency parse in Hk(x)
and P (y?|dk) is the posterior probability of depen-
dency parse y? parsed by parser dk given sentence
x. Il(y, y?) is a consensus measure defined on y
and y? using different feature functions.
Dependency parsing model P (y?|dk) can be
predicted by using the global linear models
(GLMs) (e.g., McDonald et al (2005); McDonald
and Pereira (2006)). The consensus-based score
functions Il(y, y?) include the following parts:
(1) head-modifier dependencies. Each head-
modifier dependency (denoted as ?edge?) is a tu-
ple t =< h,m, h ? m >, so Iedge(y, y?) =?
t?y ?(t, y?).
(2) sibling dependencies: Each sibling de-
pendency (denoted as ?sib?) is a tuple t =<
i, h,m, h ? i ? m >, so Isib(y, y?) =?
t?y ?(t, y?).
(3) grandparent dependencies: Each grand-
parent dependency (denoted as ?gp?) is a tuple
t =< h, i,m, h ? i ? m >, so Igp(y, y?) =?
<h,i,m,h?i?m>?y ?(t, y?).
(4) root feature: This feature (denoted as
?root?) indicates whether the multiple depen-
dency parsing trees share the same ?ROOT?, so
Iroot(y, y?) =
?
<ROOT>?y ?(< ROOT >, y?).
?(?, ?) is a indicator function??(t, y?) is 1 if
t ? y? and 0 otherwise, feature index l ?
{edge, sib, gp, root} in equation (4). Note that
< h,m, h ? m > and < m,h,m ? h > are
two different edges.
In our joint model, we extend the baseline fea-
tures of (McDonald et al, 2005; McDonald and
Pereira, 2006; Carreras, 2007) by conjoining with
the consensus-based features, so that we can learn
in which kind of contexts the different parsers
agree/disagree. For the third-order features (e.g.,
grand-siblings and tri-siblings) described in (Koo
et al, 2010), we will discuss it in future work.
2.4 Parameter Estimation
The parameters are tuned to maximize the depen-
dency parsing performance on the development
set, using an algorithm similar to the average per-
ceptron algorithm due to its strong performance
and fast training (Koo et al, 2008). Due to lim-
ited space, we do not present the details. For more
information, please refer to (Koo et al, 2008).
3 Experiments
In this section, we describe the experiments
to evaluate our proposed approach by using
CTB4 (Xue et al, 2005) and CDT (Liu et al,
2006). For the former, we adopt a set of head-
selection rules (Zhang and Clark, 2008) to convert
the phrase structure syntax of treebank into a de-
pendency tree representation. The standard data
split of CTB4 from Wang et al (2007) is used. For
the latter, we randomly select 2,000 sentences for
test set, another 2,000 sentences for development
set, and others for training set.
We use two baseline parsers, one trained on
CTB4, and another trained on CDT in the ex-
periments. We choose the n-best size of 16 and
the best iteration time of four on the development
set since these settings empirically give the best
performance. CTB4 and CDT use two different
POS tag sets and transforming from one tag set
to another is difficult (Niu et al, 2009). To over-
come this problem, we use Stanford POS Tagger1
to train a universal POS tagger on the People?s
Daily corpus,2 a large-scale Chinese corpus (ap-
proximately 300 thousand sentences and 7 mil-
lion words) annotated with word segmentation and
POS tags. Then the POS tagger produces a uni-
versal layer of POS tags for both the CTB4 and
CDT. Note that the word segmentation standards
of these corpora (CTB4, CDT and People?s Daily)
slightly differs; however, we do not consider this
problem and leave it for future research.
The performance of the parsers is evaluated us-
ing the following metrics: UAS, DA, and CM,
which are defined by (Hall et al, 2006). All the
metrics except CM are calculated as mean scores
per word, and punctuation tokens are consistently
excluded.
We conduct experiments incrementally to eval-
uate the joint features used in our first-order and
second-order parsers. The first-order parser
1http://nlp.stanford.edu/software/tagger.shtml
2http://www.icl.pku.edu.cn
106
? Features CTB4 CDTUAS CM UAS CM
dep1
baseline 86.6 42.5 75.4 16.6
+ edge 88.01 (?1.41) 44.28 (?1.78) 77.10 (?1.70) 17.82 (?1.22)
+ root 87.22 (?0.62) 43.03 (?0.53) 75.83 (?0.43) 16.81 (?0.21)
+ both 88.19 (?1.59) 44.54 (?2.04) 77.16 (?1.76) 17.90 (?1.30)
CTB4 + CDT 87.32 43.08 75.91 16.89
dep2
baseline 88.38 48.81 77.52 19.70
+ edge 89.17 (?0.79) 49.73 (?0.92) 78.44 (?0.92) 20.85 (?1.15)
+ sib 88.94 (?0.56) 49.26 (?0.45) 78.02 (?0.50) 20.13 (?0.43)
+ gp 88.90 (?0.52) 49.11 (?0.30) 77.97 (?0.45) 20.06 (?0.36)
+ root 88.61 (?0.23) 48.88 (?0.07) 77.65 (?0.13) 19.88 (?0.18)
+ all 89.62 (?1.24) 50.15 (?1.34) 79.01 (?1.49) 21.11 (?1.41)
CTB4 + CDT 88.91 49.13 78.03 20.12
Table 1: Dependency parsing results on the test set with different joint inference features. Abbreviations:
dep1/dep2 = first-order parser and second-order parser; baseline = dep1 without considering any joint
inference features; +* = the baseline features conjoined with the joint inference features derived from the
heterogeneous treebanks; CTB4 + CDT = we simply concatenate the two corpora and train a dependency
parser, and then test on CTB4 and CDT using this single model. Improvements of joint models over
baseline models are shown in parentheses.
Type Systems ? 40 Full
D
dep2 90.86 88.38
MaltParser 87.1 85.8
Wang et al (2007) 86.6 -
C
MSTMalt? 90.55 88.82
Martins et al (2008)? 90.63 88.84
Surdeanu et al (2010)? 89.40 86.63
H Zhao et al (2009) 88.9 86.1Ours 91.48 89.62
S Yu et al (2008) - 87.26Chen et al (2009) 92.34 89.91
Chen et al (2012) - 91.59
Table 2: Comparison of different approach on
CTB4 test set using UAS metric. MaltParser =
Hall et al (2006); MSTMalt=Nivre and McDon-
ald (2008). Type D = discriminative dependency
parsers without using any external resources; C =
combined parsers (stacked and ensemble parsers);
H = discriminative dependency parsers using ex-
ternal resources derived from heterogeneous tree-
banks, S = discriminative dependency parsers us-
ing external unlabeled data. ? The results on CTB4
were not directly reported in these papers, we im-
plemented the experiments in this paper.
(dep1) only incorporates head-modifier depen-
dency part (McDonald et al, 2005). The second-
order parser (dep2) uses the head-modifier and
sibling dependency parts (McDonald and Pereira,
2006), as well as the grandparent dependency
part (Carreras, 2007; Koo et al, 2008). Table 1
shows the experimental results.
As shown in Table 1, we note that adding more
joint inference features incrementally, the depen-
dency parsing performance is improved consis-
tently, for both treebanks (CTB4 or CDT). As a
final note, all comparisons between joint models
and baseline models in Table 1 are statistically sig-
nificant.3 Furthermore, we also present a base-
line method called ?CTB4 + CDT? for compari-
son. This method first tags both CTB4 and CDT
with the universal POS tagger trained on the Peo-
ple?s Daily corpus, then simply concatenates the
two corpora and trains a dependency parser, and
finally tests on CTB4 and CDT using this single
model. The comparisons in Table 1 tell us that
very limited information is obtained without con-
sensus features by simply taking a union of the
dependencies and their contexts from the two tree-
banks.
To put our results in perspective, we also com-
pare our second-order joint parser with other best-
performing systems. ?? 40? refers to the sentence
with the length up to 40 and ?Full? refers to all
the sentences in test set. The results are shown
in Table 2, our approach significantly outperforms
many systems evaluated on this data set. Chen
et al (2009) and Chen et al (2012) reported a
very high accuracy using subtree-based features
and dependency language model based features
derived from large-scale data. Our systems did not
use such knowledge. Moreover, their technique is
orthogonal to ours, and we suspect that combin-
ing their subtree-based features into our systems
might get an even better performance. We do not
present the comparison of our proposed approach
3We use the sign test at the sentence level. All the com-
parisons are significant at p < 0.05.
107
Type Systems UAS DA
D
Duan et al (2007) 83.88 84.36
Huang and Sagae (2010) 85.20 85.52
Zhang and Nivre (2011) 86.0 -
C Zhang and Clark (2008) - 86.21Bohnet and Kuhn (2012) 87.5 -
H Li et al (2012) 86.44 -Ours 85.88 86.52
S Chen et al (2009) - 86.70
Table 3: Comparison of different approaches on
CTB5 test set. Abbreviations D, C, H and S are as
in Table 2.
Treebanks #Sen # Better # NoChange # Worse
CTB4 355 74 255 26
CDT 2,000 341 1,562 97
Table 4: Statistics on joint inference output on
CTB4 and CDT development set.
with the state-of-the-art methods on CDT because
there is little work conducted on this treebank.
Some researchers conducted experiments on
CTB5 with a different data split: files 1-815 and
files 1,001-1,136 for training, files 886-931 and
1,148-1,151 for development, files 816-885 and
files 1,137-1,147 for testing. The development
and testing sets were also performed using gold-
standard assigned POS tags. We report the experi-
mental results on CTB5 test set in Table 4. Our re-
sults are better than most systems on this data split,
except Zhang and Nivre (2011), Li et al (2012)
and Chen et al (2009).
3.1 Additional Results
To obtain further information about how depen-
dency parsers benefit from the joint inference, we
conduct an initial experiment on CTB4 and CDT.
From Table 4, we find that out of 355 sentences on
the development set of CTB4, 74 sentences ben-
efit from the joint inference, while 26 sentences
suffer from it. For CDT, we also find that out of
2,000 sentences on the development set, 341 sen-
tences benefit from the joint inference, while 97
sentences suffer from it. Although the overall de-
pendency parsing results is improved, joint infer-
ence worsens dependency parsing result for some
sentences. In order to obtain further information
about the error sources, it is necessary to investi-
gate why joint inference gives negative results, we
will leave it for future work.
4 Conclusion and Future Work
We proposed a novel framework of joint infer-
ence, in which multiple dependency parsing mod-
els were coordinated to search for better depen-
dency parses by leveraging the consensus infor-
mation between heterogeneous treebanks. Exper-
imental results showed that joint inference signif-
icantly outperformed the state-of-the-art baseline
models.
There are some ways in which this research
could be continued. First, recall that the joint in-
ference scheme involves an iterative algorithm by
using bootstrapping. Intuitively, there is a lack of
formal guarantee. A natural avenue for further re-
search would be the use of more powerful algo-
rithms that provide certificates of optimality; e.g.,
dual decomposition that aims to develop decod-
ing algorithms with formal guarantees (Rush et
al., 2010). Second, we would like to combine our
heterogeneous treebank annotations into a unified
representation in order to make dependency pars-
ing results comparable across different annotation
guidelines (e.g., Tsarfaty et al (2011)).
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 61070106, No.
61272332 and No. 61202329), the National High
Technology Development 863 Program of China
(No. 2012AA011102), the National Basic Re-
search Program of China (No. 2012CB316300),
We thank the anonymous reviewers and the prior
reviewers of ACL-2012 and AAAI-2013 for their
insightful comments. We also thank Dr. Li Cai for
providing and preprocessing the data set used in
this paper.
References
B. Bohnet and J. Kuhn. 2012. The best of both worlds-
a graph-based completion model for transition-
based parsers. In Proceedings of EACL.
X. Carreras. 2007. Experiments with a Higher-order
Projective Dependency Parser. In Proceedings of
EMNLP-CoNLL, pages 957-961.
W. Chen, D. Kawahara, K. Uchimoto, and Torisawa.
2009. Improving Dependency Parsing with Subtrees
from Auto-Parsed Data. In Proceedings of EMNLP,
pages 570-579.
W. Chen, M. Zhang, and H. Li. 2012. Utilizing depen-
dency language models for graph-based dependency
parsing models. In Proceedings of ACL.
Y. Ding and M. Palmer. 2004. Synchronous depen-
dency insertion grammars: a grammar formalism
for syntax based statistical MT. In Proceedings of
108
the Workshop on Recent Advances in Dependency
Grammar, pages 90-97.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic Mod-
els for Action-based Chinese Dependency Parsing.
In Proceedings of ECML/PKDD.
J. M. Eisner. 2000. Bilexical Grammars and Their
Cubic-Time Parsing Algorithm. Advanced in Prob-
abilistic and Other Parsing Technologies, pages 29-
62.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative
Classifier for Deterministic Dependency Parsing. In
Proceedings of ACL, pages 316-323.
L. Huang and K. Sagae. 2010. Dynamic Programming
for Linear-Time Incremental Parsing. In Proceed-
ings of ACL, pages 1077-1086.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
Semi-Supervised Dependency Parsing. In Proceed-
ings of ACL.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D.
Sontag. 2010. Dual Decomposition for Parsing with
Non-Projective Head Automata. In Proceedings of
EMNLP.
M. Li, N. Duan, D. Zhang, C.-H. Li, and M. Zhou.
2009. Collaborative Decoding: Partial Hypothesis
Re-ranking Using Translation Consensus Between
Decoders. In Proceedings of ACL, pages 585-592.
Z. Li, T. Liu, and W. Che. 2012. Exploiting multiple
treebanks for parsing with Quasi-synchronous gram-
mars. In Proceedings of ACL.
T. Liu, J. Ma, and S. Li. 2006. Building a Dependency
Treebank for Improving Chinese Parser. Journal of
Chinese Languages and Computing, 16(4):207-224.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking Dependency Parsers. In Proceed-
ings of EMNLP, pages 157-166.
R. McDonald and F. Pereira. 2006. Online Learning of
Approximate Dependency Parsing Algorithms. In
Proceedings of EACL, pages 81-88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line Large-margin Training of Dependency Parsers.
In Proceedings of ACL, pages 91-98.
Z. Niu, H. Wang, and H. Wu. 2009. Exploiting Het-
erogeneous Treebanks for Parsing. In Proceedings
of ACL, pages 46-54.
J. Nivre and R. McDonld. 2008. Integrating Graph-
based and Transition-based Dependency Parsing. In
Proceedings of ACL, pages 950-958.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On Dual Decomposition and Linear Program-
ming Relation for Natural Language Processing. In
Proceedings of EMNLP.
M. Surdeanu and C. D. Manning. 2010. Ensemble
Models for Dependency Parsing: Cheap and Good?
In Proceedings of NAACL.
R. Tsarfaty, J. Nivre, and E. Andersson. 2011. Eval-
uating Dependency Parsing: Robust and Heuristics-
Free Cross-Annotation Evaluation. In Proceedings
of EMNLP.
J.-N Wang, J-.S. Chang, and K.-Y. Su. 1994. An Au-
tomatic Treebank Conversion Algorithm for Corpus
Sharing. In Proceedings of ACL, pages 248-254.
Q. I. Wang, D. Lin, and D. Schuurmans. 2007. Sim-
ple Training of Dependency Parsers via Structured
Boosting. In Proceedings of IJCAI, pages 1756-
1762.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2005.
The Penn Chinese Treebank: Phrase Structure An-
notation of a Large Corpus. Natural Language En-
gineering, 10(4):1-30.
Yamada and Matsumoto. 2003. Statistical Sependency
Analysis with Support Vector Machines. In Pro-
ceedings of IWPT, pages 195-206.
D. H. Younger. 1967. Recognition and Parsing of
Context-Free Languages in Time n3. Information
and Control, 12(4):361-379, 1967.
K. Yu, D. Kawahara, and S. Kurohashi. 2008. Chi-
nese Dependency Parsing with Large Scale Auto-
matically Constructed Case Structures. In Proceed-
ings of COLING, pages 1049-1056.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing Using
Beam-Search. In Proceedings of EMNLP, pages
562-571.
Y. Zhang and J. Nivre. 2011. Transition-based De-
pendency Parsing with Rich Non-local Features. In
Proceedings of ACL, pages 188-193.
H. Zhao, Y. Song, C. Kit, and G. Zhou. 2009. Cross
Language Dependency Parsing Using a Bilingual
Lexicon. In Proceedings of ACL, pages 55-63.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-
Based Translation Model for Question Retrieval in
Community Question Answer Archives. In Pro-
ceedings of ACL, pages 653-662.
G. Zhou, J. Zhao, K. Liu, and L. Cai. 2011. Exploit-
ing Web-Derived Selectional Preference to Improve
Statistical Dependency Parsing. In Proceedings of
ACL, pages 1556-1565.
G. Zhou, L. Cai, K. Liu, and J. Zhao. 2011. Improving
Dependency Parsing with Fined-Grained Features.
In Proceedings of IJCNLP, pages 228-236.
M. Zhu, J. Zhu, and T. Xiao. 2010. Heterogeneous
Parsing via Collaborative Decoding. In Proceedings
of COLING, pages 1344-1352.
109
