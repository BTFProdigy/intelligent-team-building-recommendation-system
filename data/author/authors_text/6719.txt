Proceedings of the 12th Conference of the European Chapter of the ACL, pages 675?682,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Semi-Supervised Polarity Lexicon Induction
Delip Rao?
Department of Computer Science
Johns Hopkins University
Baltimore, MD
delip@cs.jhu.edu
Deepak Ravichandran
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA
deepakr@google.com
Abstract
We present an extensive study on the prob-
lem of detecting polarity of words. We
consider the polarity of a word to be ei-
ther positive or negative. For example,
words such as good, beautiful , and won-
derful are considered as positive words;
whereas words such as bad, ugly, and sad
are considered negative words. We treat
polarity detection as a semi-supervised la-
bel propagation problem in a graph. In
the graph, each node represents a word
whose polarity is to be determined. Each
weighted edge encodes a relation that ex-
ists between two words. Each node (word)
can have two labels: positive or negative.
We study this framework in two differ-
ent resource availability scenarios using
WordNet and OpenOffice thesaurus when
WordNet is not available. We report our
results on three different languages: En-
glish, French, and Hindi. Our results in-
dicate that label propagation improves sig-
nificantly over the baseline and other semi-
supervised learning methods like Mincuts
and Randomized Mincuts for this task.
1 Introduction
Opinionated texts are characterized by words or
phrases that communicate positive or negative sen-
timent. Consider the following example of two
movie reviews1 shown in Figure 1. The posi-
tive review is peppered with words such as enjoy-
able, likeable, decent, breathtakingly and the negative
?Work done as a summer intern at Google Inc.
1Source: Live Free or Die Hard,
rottentomatoes.com
Figure 1: Movie Reviews with positive (left) and
negative (right) sentiment.
comment uses words like ear-shattering, humorless,
unbearable. These terms and prior knowledge of
their polarity could be used as features in a su-
pervised classification framework to determine the
sentiment of the opinionated text (E.g., (Esuli and
Sebastiani, 2006)). Thus lexicons indicating po-
larity of such words are indispensable resources
not only in automatic sentiment analysis but also
in other natural language understanding tasks like
textual entailment. This motivation was seen in
the General Enquirer effort by Stone et al (1966)
and several others who manually construct such
lexicons for the English language.2 While it is
possible to manually build these resources for a
language, the ensuing effort is onerous. This mo-
tivates the need for automatic language-agnostic
methods for building sentiment lexicons. The im-
portance of this problem has warranted several ef-
forts in the past, some of which will be reviewed
here.
We demonstrate the application of graph-based
semi-supervised learning for induction of polar-
ity lexicons. We try several graph-based semi-
2The General Inquirer tries to classify English words
along several dimensions, including polarity.
675
supervised learning methods like Mincuts, Ran-
domized Mincuts, and Label Propagation. In par-
ticular, we define a graph with nodes consisting
of the words or phrases to be classified either as
positive or negative. The edges between the nodes
encode some notion of similarity. In a transduc-
tive fashion, a few of these nodes are labeled us-
ing seed examples and the labels for the remaining
nodes are derived using these seeds. We explore
natural word-graph sources like WordNet and ex-
ploit different relations within WordNet like syn-
onymy and hypernymy. Our method is not just
confined to WordNet; any source listing synonyms
could be used. To demonstrate this, we show
the use of OpenOffice thesaurus ? a free resource
available in several languages.3
We begin by discussing some related work in
Section 2 and briefly describe the learning meth-
ods we use, in Section 3. Section 4 details our
evaluation methodology along with detailed ex-
periments for English. In Section 5 we demon-
strate results in French and Hindi, as an example
of how the method could be easily applied to other
languages as well.
2 Related Work
The literature on sentiment polarity lexicon induc-
tion can be broadly classified into two categories,
those based on corpora and the ones using Word-
Net.
2.1 Corpora based approaches
One of the earliest work on learning polarity
of terms was by Hatzivassiloglou and McKeown
(1997) who deduce polarity by exploiting con-
straints on conjoined adjectives in the Wall Street
Journal corpus. For example, the conjunction
?and? links adjectives of the same polarity while
?but? links adjectives of opposite polarity. How-
ever the applicability of this method for other im-
portant classes of sentiment terms like nouns and
verbs is yet to be demonstrated. Further they as-
sume linguistic features specific to English.
Wiebe (2000) uses Lin (1998a) style distribu-
tionally similar adjectives in a cluster-and-label
process to generate sentiment lexicon of adjec-
tives.
In a different work, Riloff et al (2003) use man-
ually derived pattern templates to extract subjec-
tive nouns by bootstrapping.
3http://www.openoffice.org
Another corpora based method due to Turney
and Littman (2003) tries to measure the semantic
orientation O(t) for a term t by
O(t) =
?
ti?S+
PMI(t, ti) ?
?
tj?S?
PMI(t, tj)
where S+ and S? are minimal sets of polar terms
that contain prototypical positive and negative
terms respectively, and PMI(t, ti) is the point-
wise mutual information (Lin, 1998b) between
the terms t and ti. While this method is general
enough to be applied to several languages our aim
was to develop methods that exploit more struc-
tured sources like WordNet to leverage benefits
from the rich network structure.
Kaji and Kitsuregawa (2007) outline a method
of building sentiment lexicons for Japanese us-
ing structural cues from HTML documents. Apart
from being very specific to Japanese, excessive de-
pendence on HTML structure makes their method
brittle.
2.2 WordNet based approaches
These approaches use lexical relations defined in
WordNet to derive sentiment lexicons. A sim-
ple but high-precision method proposed by Kim
and Hovy (2006) is to add all synonyms of a po-
lar word with the same polarity and its antonyms
with reverse polarity. As demonstrated later, the
method suffers from low recall and is unsuitable in
situations when the seed polar words are too few ?
not uncommon in low resource languages.
In line with Turney?s work, Kamps et. al. (2004)
try to determine sentiments of adjectives in Word-
Net by measuring relative distance of the term
from exemplars, such as ?good? and ?bad?. The
polarity orientation of a term t is measured as fol-
lows
O(t) = d(t, good) ? d(t, bad)d(good, bad)
where d(.) is a WordNet based relatedness mea-
sure (Pedersen et al, 2004). Again they report re-
sults for adjectives alone.
Another relevant example is the recent work by
Mihalcea et. al. (2007) on multilingual sentiment
analysis using cross-lingual projections. This is
achieved by using bridge resources like dictionar-
ies and parallel corpora to build sentence subjec-
tivity classifiers for the target language (Roma-
nian). An interesting result from their work is that
676
only a small fraction of the lexicon entries pre-
serve their polarities under translation.
The primary contributions of this paper are :
? An application of graph-based semi-
supervised learning methods for inducing
sentiment lexicons from WordNet and other
thesauri. The label propagation method
naturally allows combining several relations
from WordNet.
? Our approach works on all classes of words
and not just adjectives
? Though we report results for English, Hindi,
and French, our methods can be easily repli-
cated for other languages where WordNet is
available.4 In the absence of WordNet, any
thesaurus listing synonyms could be used.
We present one such result using the OpenOf-
fice thesaurus ? a freely available multilin-
gual resource scarcely used in NLP literature.
3 Graph based semi-supervised learning
Most natural language data has some structure that
could be exploited even in the absence of fully an-
notated data. For instance, documents are simi-
lar in the terms they contain, words could be syn-
onyms of each other, and so on. Such informa-
tion can be readily encoded as a graph where the
presence of an edge between two nodes would in-
dicate a relationship between the two nodes and,
optionally, the weight on the edge could encode
strength of the relationship. This additional infor-
mation aids learning when very few annotated ex-
amples are present. We review three well known
graph based semi-supervised learning methods ?
mincuts, randomized mincuts, and label propaga-
tion ? that we use in induction of polarity lexicons.
3.1 Mincuts
A mincut of a weighted graph G(V,E) is a par-
titioning the vertices V into V1 and V2 such that
sum of the edge weights of all edges between V1
and V2 is minimal (Figure 2).
Mincuts for semi-supervised learning proposed
by Blum and Chawla (2001) tries to classify data-
points by partitioning the similarity graph such
that it minimizes the number of similar points be-
ing labeled differently. Mincuts have been used
4As of this writing, WordNet is available for more than 40
world languages (http://www.globalwordnet.org)
Figure 2: Semi-supervised classification using
mincuts
in semi-supervised learning for various tasks, in-
cluding document level sentiment analysis (Pang
and Lee, 2004). We explore the use of mincuts for
the task of sentiment lexicon learning.
3.2 Randomized Mincuts
An improvement to the basic mincut algorithm
was proposed by Blum et. al. (2004). The deter-
ministic mincut algorithm, solved using max-flow,
produces only one of the several possible mincuts.
Some of these cuts could be skewed thereby nega-
tively effecting the results. As an extreme example
consider the graph in Figure 3a. Let the nodes with
degree one be labeled as positive and negative re-
spectively, and for the purpose of illustration let
all edges be of the same weight. The graph in Fig-
ure 3a. can be partitioned in four equal cost cuts ?
two of which are shown in (b) and (c). The min-
Figure 3: Problem with mincuts
cut algorithm, depending on the implementation,
will return only one of the extreme cuts (as in (b))
while the desired classification might be as shown
in Figure 3c.
The randomized mincut approach tries to ad-
dress this problem by randomly perturbing the ad-
jacency matrix by adding random noise.5 Mincut
is then performed on this perturbed graph. This is
5We use a Gaussian noise N (0, 1).
677
repeated several times and unbalanced partitions
are discarded. Finally the remaining partitions are
used to deduce the final classification by majority
voting. In the unlikely event of the voting result-
ing in a tie, we refrain from making a decision thus
favoring precision over recall.
3.3 Label propagation
Another semi-supervised learning method we use
is label propagation by Zhu and Ghahramani
(2002). The label propagation algorithm is a trans-
ductive learning framework which uses a few ex-
amples, or seeds, to label a large number of un-
labeled examples. In addition to the seed exam-
ples, the algorithm also uses a relation between the
examples. This relation should have two require-
ments:
1. It should be transitive.
2. It should encode some notion of relatedness
between the examples.
To name a few, examples of such relations in-
clude, synonymy, hypernymy, and similarity in
some metric space. This relation between the ex-
amples can be easily encoded as a graph. Thus ev-
ery node in the graph is an example and the edge
represents the relation. Also associated with each
node, is a probability distribution over the labels
for the node. For the seed nodes, this distribution
is known and kept fixed. The aim is to derive the
distributions for the remaining nodes.
Consider a graph G(V,E,W ) with vertices V ,
edges E, and an n ? n edge weight matrix W =
[wij ], where n = |V |. The label propagation algo-
rithm minimizes a quadratic energy function
E = 12
?
(i, j) ? E
wij(yi ? yj)2
where yi and yj are the labels assigned to the
nodes i and j respectively.6 Thus, to derive the
labels at yi, we set ??yiE = 0 to obtain the follow-
ing update equation
yi =
?
(i,j)?E
wijyj
?
(i,j)?E
wij
In practice, we use the following iterative algo-
rithm as noted by Zhu and Ghahramani (2002). A
6For binary classification yk ? {?1, +1}.
n ? n stochastic transition matrix T is derived by
row-normalizing W as follows:
Tij = P (j ? i) =
wij
?n
k=1 wkj
where Tij can be viewed as the transition probabil-
ity from node j to node i. The algorithm proceeds
as follows:
1. Assign a n ? C matrix Y with the initial as-
signment of labels, where C is the number of
classes.
2. Propagate labels for all nodes by computing
Y = TY
3. Row-normalize Y such that each row adds up
to one.
4. Clamp the seed examples in Y to their origi-
nal values
5. Repeat 2-5 until Y converges.
There are several points to be noted. First, we add
a special label ?DEFAULT? to existing set of la-
bels and set P (DEFAULT | node = u) = 1 for all
unlabeled nodes u. For all the seed nodes s with
class label Lwe define P (L | node = s) = 1. This
ensures nodes that cannot be labeled at all7 will re-
tain P (DEFAULT) = 1 thereby leading to a quick
convergence. Second, the algorithm produces a
probability distribution over the labels for all un-
labeled points. This makes this method specially
suitable for classifier combination approaches. For
this paper, we simply select the most likely label
as the predicted label for the point. Third, the al-
gorithm eventually converges. For details on the
proof for convergence we refer the reader to Zhu
and Ghahramani (2002).
4 Evaluation and Experiments
We use the General Inquirer (GI)8 data for eval-
uation. General Inquirer is lexicon of English
words hand-labeled with categorical information
along several dimensions. One such dimension is
called valence, with 1915 words labeled ?Positiv?
(sic) and 2291 words labeled ?Negativ? for words
with positive and negative sentiments respectively.
Since we want to evaluate the performance of the
7As an example of such a situation, consider a discon-
nected component of unlabeled nodes with no seed in it.
8http://www.wjh.harvard.edu/?inquirer/
678
algorithms alone and not the recall issues in us-
ing WordNet, we only consider words from GI that
also occur in WordNet. This leaves us the distri-
bution of words as enumerated in Table 1.
PoS type No. of Positives No. of Negatives
Nouns 517 579
Verbs 319 562
Adjectives 547 438
Table 1: English evaluation data from General In-
quirer
All experiments reported in Sections 4.1 to 4.5
use the data described above with a 50-50 split
so that the first half is used as seeds and the sec-
ond half is used for test. Note that all the exper-
iments described below did not involve any pa-
rameter tuning thus obviating the need for a sepa-
rate development test set. The effect of number of
seeds on learning is described in Section 4.6.
4.1 Kim-Hovy method and improvements
Kim and Hovy (2006) enrich their sentiment lexi-
con from WordNet as follows. Synonyms of a pos-
itive word are positive while antonyms are treated
as negative. This basic version suffers from a very
poor recall as shown in the Figure 4 for adjectives
(see iteration 1). The recall can be improved for a
slight trade-off in precision if we re-run the above
algorithm on the output produced at the previous
level. This could be repeated iteratively until there
is no noticeable change in precision/recall. We
consider this as the best possible F1-score pro-
duced by the Kim-Hovy method. The classwise
F1 for this method is shown in Table 2. We use
these scores as our baseline.
Figure 4: Kim-Hovy method
PoS type P R F1
Nouns 92.59 21.43 34.80
Verbs 87.89 38.31 53.36
Adjectives 92.95 31.71 47.28
Table 2: Precision/Recall/F1-scores for Kim-
Hovy method
4.2 Using prototypes
We now consider measuring semantic orientation
from WordNet using prototypical examples such
as ?good? and ?bad? similar to Kamps et al
(2004). Kamps et. al., report results only for
adjectives though their method could be used for
other part-of-speech types. The results for us-
ing prototypes are listed in Table 3. Note that
the seed data was fully unused except for the ex-
amples ?good? and ?bad?. We still test on the
same test data as earlier for comparing results.
Also note that the recall need not be 100 in this
case as we refrain from making a decision when
d(t,good) = d(t,bad).
PoS type P R F1
Nouns 48.03 99.82 64.86
Verbs 58.12 100.00 73.51
Adjectives 57.35 99.59 72.78
Table 3: Precision/Recall/F1-scores for prototype
method
4.3 Using mincuts and randomized mincuts
We now report results for mincuts and random-
ized mincuts algorithm using the WordNet syn-
onym graph. As seen in Table 4, we only observed
a marginal improvement (for verbs) over mincuts
by using randomized mincuts.
But the overall improvement of using graph-
based semi-supervised learning methods over the
Kim-Hovy and Prototype methods is quite signifi-
cant.
4.4 Using label propagation
We extract the synonym graph from WordNet with
an edge between two nodes being defined iff one
is a synonym of the other. When label propaga-
tion is performed on this graph results in Table
5 are observed. The results presented in Tables
2-5 need deeper inspection. The iterated Kim-
Hovy method suffers from poor recall. However
both mincut methods and the prototype method by
679
P R F1
Nouns
Mincut 68.25 100.00 81.13
RandMincut 68.32 99.09 80.08
Verbs
Mincut 72.34 100.00 83.95
RandMincut 73.06 99.02 84.19
Adjectives
Mincut 73.78 100.00 84.91
RandMincut 73.58 100.00 84.78
Table 4: Precision/Recall/F1-scores using mincuts
and randomized mincuts
PoS type P R F1
Nouns 82.55 58.58 58.53
Verbs 81.00 85.94 83.40
Adjectives 84.76 64.02 72.95
Table 5: Precision/Recall/F1-scores for Label Pro-
pogation
Kamps et. al., have high recall as they end up
classifying every node as either positive or nega-
tive. Note that the recall for randomized mincut
is not 100 as we do not make a classification de-
cision when there is a tie in majority voting (refer
Section 3.2). Observe that the label propagation
method performs significantly better than previ-
ous graph based methods in precision. The rea-
son for lower recall is attributed to the lack of con-
nectivity between plausibly related nodes, thereby
not facilitating the ?spread? of labels from the la-
beled seed nodes to the unlabeled nodes. We ad-
dress this problem by adding additional edges to
the synonym graph in the next section.
4.5 Incorporating hypernyms
The main reason for low recall in label propaga-
tion is that the WordNet synonym graph is highly
disconnected. Even nodes which are logically re-
lated have paths missing between them. For exam-
ple the positive nouns compliment and laud belong
to different synonym subgraphs without a path
between them. But incorporating the hypernym
edges the two are connected by the noun praise.
So, we incorporated hypernyms of every node to
improve connectivity. Performing label propaga-
tion on this combined graph gives much better re-
sults (Table 6) with much higher recall and even
slightly better precision. In Table 6., we do not
report results for adjectives as WordNet does not
define hypernyms for adjectives. A natural ques-
PoS type P R F1
Nouns 83.88 99.64 91.08
Verbs 85.49 100.00 92.18
Adjectives N/A N/A N/A
Table 6: Effect of adding hypernyms
tion to ask is if we can use other WordNet relations
too. We will defer this until section 6.
4.6 Effect of number of seeds
The results reported in Sections 4.1 to 4.5 fixed
the number of seeds. We now investigate the per-
formance of the various methods on the number
of seeds used. In particular, we are interested in
performance under conditions when the number of
seeds are few ? which is the motivation for using
semi-supervised learning in the first place. Fig-
ure 5 presents our results for English. Observe that
Label Propagation performs much better than our
baseline even when the number of seeds is as low
as ten. Thus label propagation is especially suited
when annotation data is extremely sparse.
One reason for mincuts performing badly with
few seeds is because they generate degenrate cuts.
5 Adapting to other languages
In order to demonstrate the ease of adaptability of
our method for other languages, we used the Hindi
WordNet9 to derive the adjective synonym graph.
We selected 489 adjectives at random from a list
of 10656 adjectives and this list was annotated by
two native speakers of the language. The anno-
tated list was then split 50-50 into seed and test
sets. Label propagation was performed using the
seed list and evaluated on the test list. The results
are listed in Table 7.
Hindi P R F1
90.99 95.10 93.00
Table 7: Evaluation on Hindi dataset
WordNet might not be freely available for all
languages or may not exist. In such cases build-
ing graph from an existing thesaurus might also
suffice. As an example, we consider French. Al-
though the French WordNet is available10 , we
9http://www.cfilt.iitb.ac.in/wordnet/webhwn/
10http://www.illc.uva.nl/EuroWordNet/consortium-
ewn.html
680
Figure 5: Effect of number of seeds on the F-score for Nouns, Verbs, and Adjectives. The X-axis is
number of seeds and the Y-axis is the F-score.
found the cost prohibitive to obtain it. Observe
that if we are using only the synonymy relation in
WordNet then any thesaurus can be used instead.
To demonstrate this, we consider the OpenOffice
thesaurus for French, that is freely available. The
synonym graph of French adjectives has 9707 ver-
tices and 1.6M edges. We manually annotated a
list of 316 adjectives and derived seed and test sets
using a 50-50 split. The results of label propaga-
tion on such a graph is shown in Table 8.
French P R F1
73.65 93.67 82.46
Table 8: Evaluation on French dataset
The reason for better results in Hindi compared
to French can be attributed to (1) higher inter-
annotator agreement (? = 0.7) in Hindi compared
that in French (? = 0.55).11 (2) The Hindi ex-
periment, like English, used WordNet while the
French experiment was performed on graphs de-
rived from the OpenOffice thesaurus due lack of
freely available French WordNet.
11We do not have ? scores for English dataset derived from
the Harvard Inquirer project.
6 Conclusions and Future Work
This paper demonstrated the utility of graph-based
semi-supervised learning framework for building
sentiment lexicons in a variety of resource avail-
ability situations. We explored how the struc-
ture of WordNet could be leveraged to derive
polarity lexicons. The paper combines, for the
first time, relationships like synonymy and hyper-
nymy to improve label propagation results. All
of our methods are independent of language as
shown in the French and Hindi cases. We demon-
strated applicability of our approach on alterna-
tive thesaurus-derived graphs when WordNet is
not freely available, as in the case of French.
Although our current work uses WordNet and
other thesauri, in resource poor situations when
only monolingual raw text is available we can per-
form label propagation on nearest neighbor graphs
derived directly from raw text using distributional
similarity methods. This is work in progress.
We are also currently working on the possibil-
ity of including WordNet relations other than syn-
onymy and hypernymy. One relation that is in-
teresting and useful is antonymy. Antonym edges
cannot be added in a straight-forward way to the
681
graph for label propagation as antonymy encodes
negative similarity (or dissimilarity) and the dis-
similarity relation is not transitive.
References
[Blum and Chawla2001] Avrim Blum and Shuchi
Chawla. 2001. Learning from labeled and un-
labeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning, pages
19?26.
[Blum et al2004] Blum, Lafferty, Rwebangira, and
Reddy. 2004. Semi-supervised learning using ran-
domized mincuts. In Proceedings of the ICML.
[Esuli and Sebastiani2006] Andrea Esuli and Fabrizio
Sebastiani. 2006. Determining term subjectivity
and term orientation for opinion mining. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 193?200.
[Hatzivassiloglou and McKeown1997] Vasileios Hatzi-
vassiloglou and Kathleen McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Pro-
ceedings of the ACL, pages 174?181.
[Kaji and Kitsuregawa2007] Nobuhiro Kaji and Masaru
Kitsuregawa. 2007. Building lexicon for sentiment
analysis from massive collection of HTML docu-
ments. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1075?1083.
[Kamps et al2004] Jaap Kamps, Maarten Marx, R. ort.
Mokken, and Maarten de Rijke. 2004. Using
WordNet to measure semantic orientation of adjec-
tives. In Proceedings of LREC-04, 4th International
Conference on Language Resources and Evaluation,
volume IV.
[Kim and Hovy2006] Soo-Min Kim and Eduard H.
Hovy. 2006. Identifying and analyzing judgment
opinions. In Proceedings of the HLT-NAACL.
[Lin1998a] Dekang Lin. 1998a. Automatic retrieval
and clustering of similar words. In Proceedings of
COLING, pages 768?774.
[Lin1998b] Dekang Lin. 1998b. An information-
theoretic definition of similarity. In Proceedings
of the 15th International Conference in Machine
Learning, pages 296?304.
[Mihalcea et al2007] Rada Mihalcea, Carmen Banea,
and Janyce Wiebe. 2007. Learning multilingual
subjective language via cross-lingual projections. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 976?
983.
[Pang and Lee2004] Bo Pang and Lillian Lee. 2004.
A sentimental education: Sentiment analysis using
subjectivity summarization based on minimum cuts.
In Proceedings of the ACL, pages 271?278.
[Pedersen et al2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. Word-
net::similarity - measuring the relatedness of con-
cepts. In Proceeding of the HLT-NAACL.
[Riloff et al2003] Ellen Riloff, Janyce Wiebe, and
Theresa Wilson. 2003. Learning subjective nouns
using extraction pattern bootstrapping. In Proceed-
ings of the 7th Conference on Natural Language
Learning, pages 25?32.
[Stone et al1966] Philip J. Stone, Dexter C. Dunphy,
Marshall S. Smith, and Daniel M. Ogilvie. 1966.
The General Inquirer: A Computer Approach to
Content Analysis. MIT Press.
[Turney and Littman2003] Peter D. Turney and
Michael L. Littman. 2003. Measuring praise and
criticism: Inference of semantic orientation from
association. ACM Transactions on Information
Systems, 21(4):315?346.
[Wiebe2000] Janyce M. Wiebe. 2000. Learning sub-
jective adjectives from corpora. In Proceedings of
the 2000 National Conference on Artificial Intelli-
gence. AAAI.
[Zhu and Ghahramani2002] Xiaojin Zhu and Zoubin
Ghahramani. 2002. Learning from labeled and un-
labeled data with label propagation. Technical Re-
port CMU-CALD-02-107, Carnegie Mellon Univer-
sity.
682
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 199?202,
Prague, June 2007. c?2007 Association for Computational Linguistics
JHU1 : An Unsupervised Approach to Person Name Disambiguation
using Web Snippets
Delip Rao Nikesh Garera David Yarowsky
Dept. of Computer Science
Johns Hopkins University
Baltimore, MD 21218
{delip, ngarera, yarowsky}@cs.jhu.edu
Abstract
This paper presents an approach to person
name disambiguation using K-means clus-
tering on rich-feature-enhanced document
vectors, augmented with additional web-
extracted snippets surrounding the polyse-
mous names to facilitate term bridging. This
yields a significant F-measure improvement
on the shared task training data set. The pa-
per also illustrates the significant divergence
between the properties of the training and
test data in this shared task, substantially
skewing results. Our system optimized on
F0.2 rather than F0.5 would have achieved
top performance in the shared task.
1 Introduction
Being able to automatically distinguish between
John Doe, the musician, and John Doe, the actor, on
the Web is a task of significant importance with ap-
plications in IR and other information management
tasks. Mann and Yarowsky (2004) used bigograph-
ical data annotated with named entitities and per-
form fusion of extracted information across multiple
documents. Bekkerman and McCallum (2005) stud-
ied the problem in a social network setting exploit-
ing link topology to disambiguate namesakes. Al-
Kamha and Embley (2004) used a combination of
attributes (like zipcodes, state, etc.), links, and page
similarity to derive the name clusters while Wan et.
al. (2005) used lexical features and named entities.
2 Approaches
Our framework focuses on the K-means clustering
model using both bag of words as features and vari-
ous augumented feature sets. We experimented with
several similarity functions and chose Pearson?s cor-
relation coefficient1 as the distance measure for clus-
tering. The weights for the features were set to the
term frequency of their respective words in the doc-
ument.2
2.1 Submitted system: Clustering using Web
Snippets
We queried the Google search engine with the
target person names and extracted up to the top
one thousand results. For each result we also
extracted the snippet associated with it. An example
is shown below in Figure 2.1. As can be seen the
Figure 1: Google snippet for ?Dekang Lin?
snippets contain high quality, low noise features that
could be used to improve the performance of the
system. Each snippet was treated as a document and
1This performs better than the standard measures like Eu-
clidean and Cosine with K-means clustering on this data.
2We found that using TF weights instead of TF-IDF weights
gives a better performance on this task.
199
clustered along with the supplied documents. This
process is illustrated in Figure 2. The following
example illustrates how these web snippets can
improve performance by lexical transitivity. In
this hypothetical example, a short test document
contains a Canadian postal code (T6G 2H1) not
found in any of the training documents. However,
there may exist an additional web page not in the
training or test data which contains both this term
and also overlap with other terms in the training data
(e.g. 492-9920), serving as an effective transitive
bridge between the two.
Training Document 1 492-9920, not(T6G 2H1)
Web Snippet 2 both 492-9920, T6G 2H1
Test Document 3 T6G 2H1, not(492-9920)
Thus K-means clustering is likely to cluster the
three documents above together while without this
transitive bridge the association between training
and test documents is much less strong. The final
clustering of the test data is simply a projection with
the training documents and web snippets removed.
Projection of test documents
Initial clusters of web snippets + test documents
Web snippet document
Test document
Figure 2: Clustering using Web Snippets
2.2 Baselines
In this section we describe several trivial baselines:
1. Singletons: A clustering where each cluster
has only one document hence number of clus-
ters is same as the number of documents.
2. One Cluster: A clustering with only one clus-
ter containing all documents.
3. Random: A clustering scheme which parti-
tions the documents uniformly at random into
K clusters, where the value of K were the op-
timal K on the training and test data.
These results are summarized in Table 1. Note that
all average F-scores mentioned in this table and the
rest of the paper are microaverages obtained by av-
eraging the purity and invese purity over all names
and then calculating the F-score.
Train Test
Baseline F0.2 F0.5 F0.2 F0.5
Singletons .676 .511 .843 .730
One Cluster .688 .638 .378 .327
Random .556 .493 .801 .668
Table 1: Baseline performance
2.3 K-means on Bag of Words model
The standard unaugumented Bag of Words model
achieves F0.5 of 0.666 on training data, as shown
in Table 2.
2.4 Part of speech tag features
We then consider only terms that are nouns (NN,
NNP) and adjectives (JJ) with the intuition that
most of the content bearing words and descriptive
words that disambiguate a person would fall in these
classes. The result then improves to 0.67 on the
training data.
2.5 Rich features
Another variant of this system, that we call Rich-
Feats, gives preferential weighting to terms that are
immediately around all variants of the person name
in question, place names, occupation names, and
titles. For marking up place names, occupation
names, and titles we used gazetteer3 lookup with-
out explicit named entity disambiguation. The key-
words that appeared in the HTML tag <META ..>
were also given higher weights. This resulted in an
F0.5 of 0.664.
2.6 Snippets from the Web
The addition of web snippets as described in Sec-
tion 2.1 yeilds a significant F0.5 improvement to
0.72.
3Totalling 19646 terms, gathered from publicly available re-
sources on the web. Further details are available on request.
200
2.7 Snippets and Rich features
This is a combination of the models mentioned in
Sections 2.5 and 2.6. This model combination re-
sulted in a slight degradation of performance over
snippets by themselves on the training data but a
slight improvement on test data.
Model K F0.2 F0.5
Vanilla BOW 10% 0.702 0.666
BOW + PoS 10% 0.706 0.670
BOW + RichFeats 10% 0.700 0.664
Snippets 10 0.721 0.718
Snippets + RichFeats 10 0.714 0.712
Table 2: Performance on Training Data
3 Selection of Parameters
The main parameter for K-means clustering is
choosing the number of clusters, K. We optimized
K over the training data varying K from 10%,
20%,? ? ?,100% of the number of documents as well
as varying absolute K values from 10, 20, ? ? ? to 100
documents.4 The evaluation score of F-measure can
be highly sensitive to this parameter K, as shown
in Table 3. The value of K that gives the best F-
measure on training set using vanilla bag of words
(BOW) model is K = 10%, however we see in Ta-
ble 3 that this value of K actually performs much
worse on the test data as compared to other K val-
ues.
4 Training/Test discrepancy and
re-evaluation using cross validation on
test data
Table 4 compares cluster statistics between the train-
ing and test data. This data was derived from Artiles
et. al (2007). The large difference between aver-
age number of clusters in training and test sets in-
dicates that the parameter K, optimized on training
set cannot be transferred to test set as these two sets
belong to a very different distribution. This can be
emprically seen in Table 3 where applying the best
K on training results in a significant performance
4We discard the training and test documents that have no text
content, thus the absolute value K = 10 and percentage value K
= 10% can result in different K?s, even if name had originally
100 documents to begin with.
drop on test set given this divergence when param-
eters are optimized for F0.5 (although performance
does transfer well when parameters are optimized on
F0.2). This was observed in our primary evaluation
system which was optimized for F0.5 and resulted in
a low official score of F0.5 = .53 and F0.2 = .65.
Train Test
K F0.2 F0.5 F0.2 F0.5
10% .702 .666 .527 .600
20% .716 .644 .617 .630
30% .724 .631 .683 .676
40% .724 .618 .728 .705
50% .732 .614 .762 .724
60% .731 .601 .798 .747
70% .730 .593 .832 .766
80% .732 .586 .855 .773
90% .714 .558 .861 .764
100% .670 .502 .843 .730
Table 3: Selecting the optimal parameter on training
data and application to test data
Thus an interesting question is to measure per-
formance when parameters are chosen on data shar-
ing the distributional character of the test data rather
than the highly divergent training set. To do this, we
used a standard 2-fold cross validation to estimate
clustering parameters from a held-out, alternate-half
portion of the test data5, which more fairly repre-
sents the character of the other half of the test data
than does the very different training data. We di-
vide the test set into two equal halves (taking first
fifteen names alphabetically in one set and the rest
in another). We optimize K on the first half, test
on the other half and vice versa. We report the two
K-values and their corresponding F-measures in Ta-
ble 5 and we also report the average in order to com-
pare it with the results on the test set obtained using
K optimized on training. Further, we also report
what would be oracle best K, that is, if we optimize
K on the entire test data 6. We can see in Table 5
that how optimizing K on a devlopment set with
5This also prevents overfitting as the two halves for training
and testing are disjoint.
6By oracle best K we mean the K obtained by optimizing
over the entire test data. Note that, the oracle best K is just
for comparison because it would be unfair to claim results by
optimizing K on the entire test set, all our claimed results for
different models are based on 2-fold cross validation.
201
same distribution as test set can give us F-measure
in the range of 77%, a significant increase as com-
pared to the F-measure obtained by optimizing K on
given training data. Further, Table 5, also indicates
results by a custom clustering method, that takes the
best K-means clustering using vanilla bag of words
model, retains the largest cluster and splits all the
other clusters into singleton clusters. This method
gives an improved 2-fold F-measure score over the
simple bag of words model, implying that most of
the namesakes in test data have one (or few) domi-
nant cluster and a lot of singleton clusters. Table 6
shows a full enumeration of model variance under
this cross validated test evaluation. POS and Rich-
Feats yield small gains, and a best F0.5 performance
of .776.
Data set cluster size # of clusters
Mean Variance Mean Variance
Train 5.4 144.0 10.8 146.3
Test 3.1 26.5 45.9 574.1
Table 4: Cluster statistics from the test and training
data
Data set K F0.2 F0.5
F0.5 Best K on train 10% .702 .666
F0.2 Best K on train 10 .707 .663
Best K on train 10% .527 .560
applied to test 10 .540 .571
2Fold on Test 80 .847 .748
80% .862 .793
.854* .771*
2Fold on Single 80 .847 .749
Largest Cluster 80 .866 .795
.856* .772*
Oracle on Test 80 .858 .774
Table 5: Comparision of training and test results us-
ing Vanilla Bag-of-words model. The values indi-
cated with * represent the average value.
5 Conclusion
We presented a K-means clustering approach for the
task of person name disambiguation using several
augmented feature sets including HTML meta fea-
tures, part-of-speech-filtered features, and inclusion
of additional web snippets extracted from Google
to facilitate term bridging. The latter showed sig-
nificant empirical gains on the training data. Best
Model K F0.2 F0.5
Vanilla BOW 80/ .847/.862 .749/.793
80% Avg = .854 Avg = .771
BOW + PoS 80%/ .844/.865 .749/.795
80% Avg = .854 Avg = .772
BOW 80%/ .847/.868 .754/.798
RichFeats 80% Avg = .858 Avg = .776
Snippets 50%/ .842/.875 .746/.800
50% Avg = .859 Avg = .773
Snippets + 40%/ .836/.874 .750/.798
RichFeats 50% Avg = .855 Avg = .774
Table 6: Performance on 2Fold Test Data
performance on test data, when parameters are op-
timized for F0.2 on training (Table 3), yielded a top
performing F0.2 of .855 on test data (and F0.5=.773
on test data). We also explored the striking discrep-
ancy between training and test data characteristics
and showed how optimizing the clustering param-
eters on given training data does not transfer well
to the divergent test data. To control for similar
training and test distributional characteristics, we re-
evaluated our test results estimating clustering pa-
rameters from alternate held-out portions of the test
set. Our models achieved cross validated F0.5 of .77-
.78 on test data for all feature combinations, further
showing the broad strong performance of these tech-
niques.
References
Reema Al-Kamha and David W. Embley. 2004. Grouping
search-engine returned citations for person-name queries. In
Proceedings of the 6th annual ACM international workshop
on Web information and data management, pages 96?103.
Javier Artiles, Julio Gonzalo, and Felisa Verdejo. 2007. Eval-
uation: Establishing a benchmark for the web people search
task. In Proceedings of Semeval 2007, Association for Com-
putational Linguistics.
Ron Bekkerman and Andrew McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In Pro-
ceedings of the 14th international conference on World Wide
Web, pages 463?470.
Gideon S. Mann and David Yarowsky. 2004. Unsupervised
personal name disambiguation. In Proceedings of the sev-
enth conference on Natural language learning (CONLL),
pages 33?40.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong Ding. 2005.
Person resolution in person search results: Webhawk. In
Proceedings of the 14th ACM international conference on
Information and knowledge management, pages 163?170.
202
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 58?65,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Ranking and Semi-supervised Classification
on Large Scale Graphs Using Map-Reduce
Delip Rao
Dept. of Computer Science
Johns Hopkins University
delip@cs.jhu.edu
David Yarowsky
Dept. of Computer Science
Johns Hopkins University
yarowsky@cs.jhu.edu
Abstract
Label Propagation, a standard algorithm
for semi-supervised classification, suffers
from scalability issues involving memory
and computation when used with large-
scale graphs from real-world datasets. In
this paper we approach Label Propagation
as solution to a system of linear equations
which can be implemented as a scalable
parallel algorithm using the map-reduce
framework. In addition to semi-supervised
classification, this approach to Label Prop-
agation allows us to adapt the algorithm to
make it usable for ranking on graphs and
derive the theoretical connection between
Label Propagation and PageRank. We pro-
vide empirical evidence to that effect using
two natural language tasks ? lexical relat-
edness and polarity induction. The version
of the Label Propagation algorithm pre-
sented here scales linearly in the size of
the data with a constant main memory re-
quirement, in contrast to the quadratic cost
of both in traditional approaches.
1 Introduction
Natural language data often lend themselves to a
graph-based representation. Words can be linked
by explicit relations as in WordNet (Fellbaum,
1989), and documents can be linked to one an-
other via hyperlinks. Even in the absence of such a
straightforward representation it is possible to de-
rive meaningful graphs such as the nearest neigh-
bor graphs, as done in certain manifold learning
methods, e.g. Roweis and Saul (2000); Belkin and
Niyogi (2001). Typically, these graphs share the
following properties:
? They are edge-weighted.
? The edge weight encodes some notion of re-
latedness between the vertices.
? The relation represented by edges is at least
weakly transitive. Examples of such rela-
tions include, ?is similar to?, ?is more gen-
eral than?, and so on. It is important that the
relations selected are transitive for the graph-
based learning methods using random walks.
Such graphs present several possibilities for
solving natural language problems involving rank-
ing, classification, and clustering. Graphs have
been successfully employed in machine learning
in a variety of supervised, unsupervised, and semi-
supervised tasks. Graph based algorithms perform
better than their counterparts as they capture the
latent structure of the problem. Further, their ele-
gant mathematical framework allows simpler anal-
ysis to gain a deeper understanding of the prob-
lem. Despite these advantages, implementations
of most graph-based learning algorithms do not
scale well on large datasets from real world prob-
lems in natural language processing. With large
amounts of unlabeled data available, the graphs
can easily grow to millions of nodes and most ex-
isting non-parallel methods either fail to work due
to resource constraints or find the computation in-
tractable.
In this paper we describe a scalable implemen-
tation of Label Propagation, a popular random
walk based semi-supervised classification method.
We show that our framework can also be used for
ranking on graphs. Our parallel formulation shows
a theoretical connection between Label Propaga-
tion and PageRank. We also confirm this em-
pirically using the lexical relatedness task. The
58
proposed Parallel Label Propagation scales up lin-
early in the data and the number of processing ele-
ments available. Also, the main memory required
by the method does not grow with the size of the
graph.
The outline of this paper is as follows: Section 2
introduces the manifold assumption and explains
why graph-based learning algorithms perform bet-
ter than their counterparts. Section 3 motivates
the random walk based approach for learning on
graphs. Section 4 introduces the Label Propaga-
tion method by Zhu et al (2003). In Section 5 we
describe a method to scale up Label Propagation
using Map-Reduce. Section 6 shows how Label
Propagation could be used for ranking on graphs
and derives the relation between Label Propaga-
tion and PageRank. Parallel Label Propagation is
evaluated on ranking and semi-supervised classifi-
cation problems in natural language processing in
Section 8. We study scalability of this algorithm in
Section 9 and describe related work in the area of
parallel algorithms and machine learning in Sec-
tion 10.
2 Manifold Assumption
The training data D can be considered as a collec-
tion of tuples D = (X ,Y) where Y are the labels
and X are the features, and the learned modelM
is a surrogate for an underlying physical process
which generates the data D. The data D can be
considered as a sampling from a smooth surface or
a manifold which represents the physical process.
This is known as the manifold assumption (Belkin
et al, 2005). Observe that even in the simple case
of Euclidean data (X = {x : x ? R
d
}) as shown
in Figure 1, points that lie close in the Euclidean
space might actually be far off on the manifold.
A graph, as shown in Figure 1c, approximates the
structure of the manifold which was lost in vector-
ized algorithms operating in the Euclidean space.
This explains the better performance of graph al-
gorithms for learning as seen in the literature.
3 Distance measures on graphs
Most learning tasks on graphs require some notion
of distance or similarity to be defined between the
vertices of a graph. The most obvious measure of
distance in a graph is the shortest path between the
vertices, which is defined as the minimum number
of intervening edges between two vertices. This is
also known as the geodesic distance. To convert
this distance measure to a similarity measure, we
take the reciprocal of the shortest-path length. We
refer to this as the geodesic similarity.
Figure 2: Shortest path distances on graphs ignore
the connectivity structure of the graph.
While shortest-path distances are useful in
many applications, it fails to capture the following
observation. Consider the subgraph of WordNet
shown in Figure 2. The term moon is con-
nected to the terms religious leader
and satellite.
1
Observe that both
religious leader and satellite are
at the same shortest path distance from moon.
However, the connectivity structure of the graph
would suggest satellite to be more similar
than religious leader as there are multiple
senses, and hence multiple paths, connecting
satellite and moon.
Thus it is desirable to have a measure that cap-
tures not only path lengths but also the connectiv-
ity structure of the graph. This notion is elegantly
captured using random walks on graphs.
4 Label Propagation: Random Walk on
Manifold Graphs
An efficient way to combine labeled and unla-
beled data involves construction of a graph from
the data and performing a Markov random walk
on the graph. This has been utilized in Szummer
and Jaakkola (2001), Zhu et. al. (2003), and Azran
(2007). The general idea of Label Propagation in-
volves defining a probability distribution F over
the labels for each node in the graph. For labeled
nodes, this distribution reflects the true labels and
the aim is to recover this distribution for the unla-
beled nodes in the graph.
Consider a graph G(V,E,W ) with vertices V ,
edges E, and an n ? n edge weight matrix W =
1
The religious leader sense of moon is due to Sun
Myung Moon, a US religious leader.
59
(a) (b) (c)
Figure 1: Manifold Assumption [Belkin et al, 2005]: Data lies on a manifold (a) and points along the
manifold are locally similar (b).
[w
ij
], where n = |V |. The Label Propagation al-
gorithm minimizes a quadratic energy function
E =
1
2
?
(i, j) ? E
w
ij
(F
i
? F
j
)
2
(1)
The general recipe for using random walks
for classification involves constructing the graph
Laplacian and using the pseudo-inverse of the
Laplacian as a kernel (Xiao and Gutman, 2003).
Given a weighted undirected graph, G(V,E,W ),
the Laplacian is defined as follows:
L
ij
=
?
?
?
d
i
if i = j
?w
ij
if i is adjacent to j
0 otherwise
(2)
where d
i
=
?
j
w
ij
.
It has been shown that the pseudo-inverse of the
Laplacian L is a kernel (Xiao and Gutman, 2003),
i.e., it satisfies the Mercer conditions. However,
there is a practical limitation to this approach. For
very large graphs, even if the graph Laplacians are
sparse, their pseudo-inverses are dense matrices
requiring O(n
2
) space. This can be prohibitive in
most computing environments.
5 Parallel Label Propagation
In developing a parallel algorithm for Label
Propagation we instead take an alternate approach
and completely avoid the use of inverse Lapla-
cians for the reasons stated above. Our approach
follows from the observation made from Zhu et
al.?s (2003) Label Propagation algorithm:
Observation: In a weighted graph G(V,E,W )
with n = |V | vertices, minimization of Equation
(1) is equivalent to solving the following system
of linear equations.
?
(i, j) ? E
w
ij
F
i
=
?
(i, j) ? E
w
ij
F
j
(3)
?
c ? classes(i)
F
i
(c) = 1 ?i, j ? V.
We use this observation to derive an iterative
Label Propagation algorithm that we will later par-
allelize. Consider a weighted undirected graph
G(V,E,W ) with the vertex set partitioned into V
L
and V
U
(i.e., V = V
L
?V
U
) such that all vertices in
V
L
are labeled and all vertices in V
U
are unlabeled.
Typically only a small set of vertices are labeled,
i.e., |V
U
|  |V
L
|. Let F
u
denote the probability
distribution over the labels associated with vertex
u ? V . For v ? V
L
, F
v
is known, and we also
add a ?dummy vertex? v
?
to the graph G such that
w
vv
?
= 1 and F
v
?
= F
v
. This is equivalent to the
?clamping? done in (Zhu et al, 2003). Let V
D
be
the set of dummy vertices.
Algorithm 1: Iterative Label Propogation
repeat
forall v ? (V ? V
D
) do
F
v
=
?
(v,u)?E
w
uv
F
v
Row normalize F
v
.
end
until convergence or maxIterations
Observe that every iteration of Algorithm 1 per-
forms certain operations on each vertex of the
graph. Further, these operations only rely on
local information (from neighboring vertices of
the graph). This leads to the parallel algorithm
(Algorithm 2) implemented using the map-reduce
model. Map-Reduce (Dean and Ghemawat, 2004)
is a paradigm for implementing distributed algo-
rithms with two user supplied functions ?map? and
?reduce?. The map function processes the input
key/value pairs with the key being a unique iden-
60
tifier for a node in the graph and the value corre-
sponds to the data associated with the node. The
mappers run on different machines operating on
different parts of the data and the reduce function
aggregates results from various mappers.
Algorithm 2: Parallel Label Propagation
map(key, value):
begin
d = 0
neighbors = getNeighbors(value);
foreach n ? neighbors do
w = n.weight();
d += w ? n.getDistribution();
end
normalize(d);
value.setDistribution(d);
Emit(key, value);
end
reduce(key, values): Identity Reducer
Algorithm 2 represents one iteration of Algo-
rithm 1. This is run repeatedly until convergence
or for a specified number of iterations. The al-
gorithm is considered to have converged if the la-
bel distributions associated with each node do not
change significantly, i.e.,
?
?
?
?
F
(i+1)
? F
(i)
?
?
?
?
2
< 
for a fixed  > 0.
6 Label Propagation for Ranking
Graph ranking is applicable in a variety of prob-
lems in natural language processing and informa-
tion retrieval. Given a graph, we would like to
rank the vertices of a graph with respect to a node,
called the pivot node or query node. Label Prop-
agation and its variants (Szummer and Jaakkola,
2001; Zhu et al, 2003; Azran, 2007) have been
traditionally used for semi-supervised classifica-
tion. Our view of Label Propagation (via Algo-
rithm 1) suggests a way to perform ranking on
graphs.
Ranking on graphs can be performed in the Par-
allel Label Propagation framework by associating
a single point distribution with all vertices. The
pivot node has a mass fixed to the value 1 at all it-
erations. In addition, the normalization step in Al-
gorithm 2 is omitted. At the end of the algorithm,
the mass associated with each node determines its
rank.
6.1 Connection to PageRank
It is interesting to note that Algorithm 1 brings
out a connection between Label Propagation and
PageRank (Page et al, 1998). PageRank is a ran-
dom walk model that allows the random walk to
?jump? to its initial state with a nonzero proba-
bility (?). Given the probability transition matrix
P = [P
rs
], where P
rs
is the probability of jumping
from node r to node s, the weight update for any
vertex (say v) is derived as follows
v
t+1
= ?v
t
P + (1? ?)v
0
(4)
Notice that when ? = 0.5, PageRank is reduced
to Algorithm 1, by a constant factor, with the ad-
ditional (1? ?)v
0
term corresponding to the con-
tribution from the ?dummy vertices? V
D
in Algo-
rithm 1.
We can in fact show that Algorithm 1 reduces to
PageRank as follows:
v
t+1
= ?v
t
P + (1? ?)v
0
? v
t
P +
(1? ?)
?
v
0
= v
t
P + ?v
0
(5)
where ? =
(1??)
?
. Thus by setting the edge
weights to the dummy vertices to ?, i.e., ?(z, z
?
) ?
E and z
?
? V
D
, w
zz
?
= ?, Algorithm 1, and hence
Algorithm 2, reduces to PageRank. Observe that
when ? = 1 we get the original Algorithm 1.
We?ll refer to this as the ??-correction?.
7 Graph Representation
Since Parallel Label Propagation algorithm uses
only local information, we use the adjacency list
representation (which is same as the sparse adja-
cency matrix representation) for the graph. This
representation is important for the algorithm to
have a constant main memory requirement as no
further lookups need to be done while comput-
ing the label distribution at a node. The interface
definition for the graph is listed in Appendix A.
Often graph data is available in an edge format,
as <source, destination, weight> triples. We use
another map-reduce step (Algorithm 3) to convert
that data to the form shown in Appendix A.
8 Evaluation
We evaluate the Parallel Label Propagation algo-
rithm for both ranking and semi-supervised clas-
sification. In ranking our goal is to rank the ver-
tices of a graph with respect to a given node called
the pivot/query node. In semi-supervised classi-
fication, we are given a graph with some vertices
61
Algorithm 3: Graph Construction
map(key, value):
begin
edgeEntry = value;
Node n(edgeEntry);
Emit(n.id, n);
end
reduce(key, values):
begin
Emit(key, serialize(values));
end
labeled and would like to predict labels for the re-
maining vertices.
8.1 Ranking
To evaluate ranking, we consider the problem
of deriving lexical relatedness between terms.
This has been a topic of interest with applica-
tions in word sense disambiguation (Patwardhan
et al, 2005), paraphrasing (Kauchak and Barzilay,
2006), question answering (Prager et al, 2001),
and machine translation (Blatz et al, 2004), to
name a few. Following the tradition in pre-
vious literature we evaluate on the Miller and
Charles (1991) dataset. We compare our rankings
with the human judegments using the Spearman
rank correlation coefficient. The graph for this
task is derived from WordNet, an electronic lex-
ical database. We compare Algorithm 2 with re-
sults from using geodesic similarity as a baseline.
As observed in Table 1, the parallel implemen-
tation in Algorithm 2 performs better than rank-
ing using geodesic similarity derived from short-
est path lengths. This reinforces the motivation of
using random walks as described in Section 3.
Method Spearman
Correlation
Geodesic (baseline) 0.28
Parallel Label 0.36
Propagation
Table 1: Lexical-relatedness results: Comparison
with geodesic similarity.
We now empirically verify the equivalence of
the ?-corrected Parallel Label Propagation and
PageRank established in Equation 4. To do this,
we use ? = 0.1 in the PageRank algorithm and
set ? =
(1??)
?
= 9 in the ?-corrected Parallel La-
bel Propagation algorithm. The results are seen in
Table 2.
Method Spearman
Correlation
PageRank (? = 0.1) 0.39
Parallel Label 0.39
Propagation (? = 9)
Table 2: Lexical-relatedness results: Comparision
of PageRank and ?-corrected Parallel Label Prop-
agation
8.2 Semi-supervised Classification
Label Propagation was originally developed as a
semi-supervised classification method. Hence Al-
gorithm 2 can be applied without modification.
After execution of Algorithm 2, every node v in
the graph will have a distribution over the labels
F
v
. The predicted label is set to argmax
c?classes(v)
F
v
(c).
To evaluate semi-supervised classification we
consider the problem of learning sentiment polar-
ity lexicons. We consider the polarity of a word to
be either positive or negative. For example, words
such as good, beautiful , and wonderful are consid-
ered as positive sentiment words; whereas words
such as bad, ugly, and sad are considered negative
sentiment words. Learning such lexicons has ap-
plications in sentiment detection and opinion min-
ing. We treat sentiment polarity detection as a
semi-supervised Label Propagation problem in a
graph. In the graph, each node represents a word
whose polarity is to be determined. Each weighted
edge encodes a relation that exists between two
words. Each node (word) can have two labels:
positive or negative. It is important to note that La-
bel Propagation, and hence Algorithms 1&2, sup-
port multi-class classification but for the purpose
of this task we have two labels. The graph for the
task is derived from WordNet. We use the Gen-
eral Inquirer (GI)
2
data for evaluation. General
Inquirer is lexicon of English words hand-labeled
with categorical information along several dimen-
sions. One such dimension is called valence, with
1915 words labeled ?Positiv? (sic) and 2291 words
labeled ?Negativ? for words with positive and neg-
ative sentiments respectively. We used a random
20% of the data as our seed labels and the rest
as our unlabeled data. We compare our results
2
http://www.wjh.harvard.edu/?inquirer/
62
(a) (b)
Figure 3: Scalability results: (a) Scaleup (b) Speedup
(F-scores) with another scalable previous work by
Kim and Hovy (Kim and Hovy, 2006) in Table 2
for the same seed set. Their approach starts with a
few seeds of positive and negative terms and boot-
straps the list by considering all synonyms of pos-
itive word as positive and antonyms of positive
words as negative. This procedure is repeated mu-
tatis mutandis for negative words in the seed list
until there are no more words to add.
Method Nouns Verbs Adjectives
Kim & Hovy 34.80 53.36 47.28
Parallel Label 58.53 83.40 72.95
Propagation
Table 3: Polarity induction results (F-scores)
The performance gains seen in Table 3 should
be attributed to the Label Propagation in general
as the previous work (Kim and Hovy, 2006) did
not utilize a graph based method.
9 Scalability experiments
We present some experiments to study the scala-
bility of the algorithm presented. All our experi-
ments were performed on an experimental cluster
of four machines to test the concept. The machines
were Intel Xeon 2.4 GHz with 1Gb main memory.
All performance measures were averaged over 20
runs.
Figure 3a shows scaleup of the algorithm which
measures how well the algorithm handles increas-
ing data sizes. For this experiment, we used all
nodes in the cluster. As observed, the increase in
time is at most linear in the size of the data. Fig-
ure 3b shows speedup of the algorithm. Speedup
shows how well the algorithm performs with in-
crease in resources for a fixed input size. In
this case, we progressively increase the number of
nodes in the cluster. Again, the speedup achieved
is linear in the number of processing elements
(CPUs). An appealing factor of Algorithm 2 is that
the memory used by each mapper process is fixed
regardless of the size of the graph. This makes the
algorithm feasible for use with large-scale graphs.
10 Related Work
Historically, there is an abundance of work in par-
allel and distributed algorithms for graphs. See
Grama et al (2003) for survey chapters on the
topic. In addition, the emergence of open-source
implementations of Google?s map-reduce (Dean
and Ghemawat, 2004) such as Hadoop
3
has made
parallel implementations more accessible.
Recent literature shows tremendous interest in
application of distributed computing to scale up
machine learning algorithms. Chu et al (2006)
describe a family of learning algorithms that fit
the Statistical Query Model (Kearns, 1993). These
algorithms can be written in a special summation
form that is amenable to parallel speed-up. Exam-
ples of such algorithms include Naive Bayes, Lo-
gistic Regression, backpropagation in Neural Net-
works, Expectation Maximization (EM), Princi-
pal Component Analysis, and Support Vector Ma-
chines to name a few. The summation form can be
easily decomposed so that the mapper can com-
pute the partial sums that are then aggregated by a
reducer. Wolfe et al (2008) describe an approach
to estimate parameters via the EM algorithm in a
setup aimed to minimize communication latency.
The k-means clustering algorithm has been an
archetype of the map-reduce framework with sev-
eral implementations available on the web. In
3
http://hadoop.apache.org/core
63
addition, the Netflix Million Dollar Challenge
4
generated sufficient interest in large scale cluster-
ing algorithms. (McCallum et al, 2000), describe
algorithmic improvements to the k-means algo-
rithm, called canopy clustering, to enable efficient
parallel clustering of data.
While there is earlier work on scalable map-
reduce implementations of PageRank (E.g., Gle-
ich and Zhukov (2005)) there is no existing liter-
ature on parallel algorithms for graph-based semi-
supervised learning or the relationship between
PageRank and Label Propagation.
11 Conclusion
In this paper, we have described a parallel algo-
rithm for graph ranking and semi-supervised clas-
sification. We derived this by first observing that
the Label Propagation algorithm can be expressed
as a solution to a set of linear equations. This is
easily expressed as an iterative algorithm that can
be cast into the map-reduce framework. This al-
gorithm uses fixed main memory regardless of the
size of the graph. Further, our scalability study re-
veals that the algorithm scales linearly in the size
of the data and the number of processing elements
in the cluster. We also showed how Label Prop-
agation can be used for ranking on graphs and
the conditions under which it reduces to PageR-
ank. We evaluated our implementation on two
learning tasks ? ranking and semi-supervised clas-
sification ? using examples from natural language
processing including lexical-relatedness and senti-
ment polarity lexicon induction with a substantial
gain in performance.
A Appendix A: Interface definition for
Undirected Graphs
In order to guarantee the constant main memory
requirement of Algorithm 2, the graph represen-
tation should encode for each node, the complete
information about it?s neighbors. We represent
our undirected graphs in the Google?s Protocol
Buffer format.
5
Protocol Buffers allow a compact,
portable on-disk representation that is easily ex-
tensible. This definition can be compiled into effi-
cient Java/C++ classes.
The interface definition for undirected graphs is
listed below:
4
http://www.netflixprize.com
5
Implementation available at
http://code.google.com/p/protobuf/
package graph;
message NodeNeighbor {
required string id = 1;
required double edgeWeight = 2;
repeated double labelDistribution = 3;
}
message UndirectedGraphNode {
required string id = 1;
repeated NodeNeighbor neighbors = 2;
repeated double labelDistribution = 3;
}
message UndirectedGraph {
repeated UndirectedGraphNode nodes = 1;
}
References
Arik Azran. 2007. The rendezvous algorithm: Multi-
class semi-supervised learning with markov random
walks. In Proceedings of the International Confer-
ence on Machine Learning (ICML).
Micheal. Belkin, Partha Niyogi, and Vikas Sindhwani.
2005. On manifold regularization. In Proceedings
of AISTATS.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceeding of
COLING.
Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,
Gary R. Bradski, Andrew Y. Ng, and Kunle Oluko-
tun. 2006. Map-reduce for machine learning on
multicore. In Proceedings of Neural Information
Processing Systems.
Jeffrey Dean and Sanjay Ghemawat. 2004. Map-
reduce: Simplified data processing on large clusters.
In Proceedings of the symposium on Operating sys-
tems design and implementation (OSDI).
Christaine Fellbaum, editor. 1989. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
D. Gleich and L. Zhukov. 2005. Scalable comput-
ing for power law graphs: Experience with parallel
pagerank. In Proceedings of SuperComputing.
Ananth Grama, George Karypis, Vipin Kumar, and An-
shul Gupta. 2003. Introduction to Parallel Comput-
ing (2nd Edition). Addison-Wesley, January.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of HLT-NAACL.
Michael Kearns. 1993. Efficient noise-tolerant learn-
ing from statistical queries. In Proceedings of the
Twenty-Fifth Annual ACM Symposium on Theory of
Computing (STOC).
64
Soo-Min Kim and Eduard H. Hovy. 2006. Identifying
and analyzing judgment opinions. In Proceedings of
HLT-NAACL.
Andrew McCallum, Kamal Nigam, and Lyle H. Un-
gar. 2000. Efficient clustering of high-dimensional
data sets with application to reference matching.
In Knowledge Discovery and Data Mining (KDD),
pages 169?178.
G. Miller and W. Charles. 1991. Contextual correlates
of semantic similarity. In Language and Cognitive
Process.
Larry Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1998. The pagerank citation ranking:
Bringing order to the web. Technical report, Stan-
ford University, Stanford, CA.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2005. Senserelate::targetword - A gen-
eralized framework for word sense disambiguation.
In Proceedings of ACL.
John M. Prager, Jennifer Chu-Carroll, and Krzysztof
Czuba. 2001. Use of wordnet hypernyms for an-
swering what-is questions. In Proceedings of the
Text REtrieval Conference.
M. Szummer and T. Jaakkola. 2001. Clustering and
efficient use of unlabeled examples. In Proceedings
of Neural Information Processing Systems (NIPS).
Jason Wolfe, Aria Haghighi, and Dan Klein. 2008.
Fully distributed EM for very large datasets. In Pro-
ceedings of the International Conference in Machine
Learning.
W. Xiao and I. Gutman. 2003. Resistance distance and
laplacian spectrum. Theoretical Chemistry Associa-
tion, 110:284?289.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
the International Conference on Machine Learning
(ICML).
65
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 120?123,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
?-extension Hidden Markov Models and Weighted Transducers for
Machine Transliteration
Balakrishnan Vardarajan
Dept. of Electrical and Computer Engineering
Johns Hopkins University
bvarada2@jhu.edu
Delip Rao
Dept. of Computer Science
Johns Hopkins University
delip@cs.jhu.edu
Abstract
We describe in detail a method for translit-
erating an English string to a foreign
language string evaluated on five differ-
ent languages, including Tamil, Hindi,
Russian, Chinese, and Kannada. Our
method involves deriving substring align-
ments from the training data and learning a
weighted finite state transducer from these
alignments. We define an ?-extension Hid-
den Markov Model to derive alignments
between training pairs and a heuristic to
extract the substring alignments. Our
method involves only two tunable parame-
ters that can be optimized on held-out data.
1 Introduction
Transliteration is a letter by letter mapping of one
writing system to another. Apart from the obvi-
ous use in writing systems, transliteration is also
useful in conjunction with translation. For exam-
ple, machine translation BLEU scores are known
to improve when named entities are transliterated.
This engendered several investigations into auto-
matic transliteration of strings, named entities in
particular, from one language to another. See
Knight and Graehl(1997) and later papers on this
topic for an overview.
Hidden Markov Model (HMM) (Rabiner,
1989) is a standard sequence modeling tool used
in various problems in natural language process-
ing like machine translation, speech recognition,
part of speech tagging and information extraction.
There have been earlier attempts in using HMMs
for automatic transliteration. See (Abdul Jaleel
and Larkey, 2003; Zhou et al, 2008) for exam-
ple. In this paper, we define an ?-extension Hid-
den Markov Model that allows us to align source
and target language strings such that the charac-
ters in the source string may be optionally aligned
to the ? symbol. We also introduce a heuristic that
allows us to extract high quality sub-alignments
from the ?-aligned word pairs. This allows us to
define a weighted finite state transducer that pro-
duces transliterations for an English string by min-
imal segmentation.
The overview of this paper is as follows: Sec-
tion 2 introduces ?-extension Hidden Markov
Model and describes our alignment procedure.
Section 3 describes the substring alignment
heuristic and our weighted finite state transducer
to derive the final n-best transliterations. We con-
clude with a result section describing results from
the NEWS 2009 shared task on five different lan-
guages.
2 Learning Alignments
The training data D is given as pairs of strings
(e, f) where e is the English string with the cor-
responding foreign transliteration f . The English
string e consists of a sequence of English letters
(e1, e2, . . . , eN ) while f = (f1, f2, . . . , fM ) .
We represent E as the set of all English symbols
and F as the set of all foreign symbols.1 We also
assume both languages have a special null symbol
?, that is ? ? E and ? ? F .
Our alignment model is a Hidden Markov
Model H(X,Y,S,T,Ps), where
? X is the start state and Y is the end state.
? S is the set of emitting states with S = |S|.
The emitting states are indexed from 1 to S.
The start state X is indexed as state 0 and the
end state Y is indexed as state S + 1.
? T is an (S + 1) ? (S + 1) stochastic matrix
with T = [tij] for i ? {0, 1, . . . , S} and j ?
{1, 2, . . . , S + 1}.
1Alphabets and diacritics are treated as separate symbols.
120
? Ps = [pef ] is an |E| ? |F| matrix of joint
emission probabilities with pef = P (e, f |s)
?s ? S .
We define s? to be an ?-extension of a string of
characters s = (c1, c2, . . . , ck) as the string ob-
tained by pumping an arbitrary number of ? sym-
bols between any two adjacent characters cl and
cl+1. That is, s? = (di1 , . . . , di2 , . . . , dik) where
dij = cj and dl = ? for im < l < im+1 where
1 ? l < k. Observe that there are countably infi-
nite ?-extensions for a given string s since an arbi-
trary number of ? symbols can be inserted between
characters cm and cm+1. Let T (s) denote the set
of all possible ?-extensions for a given string s.
For a given pair of strings (u, v), we define a
joint ?-extension of (u, v) as the pair (u?, v?) s.t. u? ?
T (u) and v? ? T (v) with |u?| = |v?| and ?i s.t.
u?i = v?i = ?. Due to this restriction, there are finite
?-extensions for a pair (u, v) with the length of u?
and v? bounded above by |u| + |v|. 2 Let J(u, v)
denote the set of all joint ?-extensions of (u, v).
Given a pair of strings (e, f) with e =
(e1, e2, . . . , eN ) and f = (f1, f2, . . . , fM ), we
compute the probability ?(e, f, s?) that they are
transliteration pairs ending in state s? as
?(e, f, s?) =
?
(e?,?f)?J(e,f)
?
0=s0,...,s|e?|=s?
t0,s1
|e?|
?
i=1
tsi,si+1P (e?i, f?i|si)
In order to compute the probability Q(e, f) of a
given transliteration pair, the final state has to be
the end state S + 1. Hence
Q(e, f) =
S
?
s=1
?(e, f, s)ts,S+1 (1)
We also write the probability ?(e, f, s?) that they
are transliteration pairs starting in state s? as
?(e, f, s?) =
?
(e?,?f)?J(e,f)
?
s?=s0,...,s|e?|+1=S+1
ts0,s1
|e?|
?
i=1
tsi,si+1P (e?i, f?i|si)
Again noting that the start state of the HMM
H is 0, we have Q(e, f) =
S
?
s=1
?(e, f, s)t0,s. We
2|u?| = |v?| > |u| + |v| would imply ?i s.t. u?i = v?i = ?
which contradicts the definition of joint ?-extension.
denote a subsequence of a string u as umn =
(un, un+1, . . . , um) . Using these definitions, we
can define ?(ei1, f
j
1 , s) as
?
?
?
?
?
?
?
?
?
1 i = j = 0, s = 0
0 i = j = 0, s 6= 0
t0,sP (e1, f1|s) i = j = 1
PS
s?=1 ts?,s?(ei1, f
j?1
1 , s?)P (?, fj |s) i = 1, j > 1
PS
s?=1 ts?,s?(ei?11 , f
j
1 , s?)P (ei, ?|s) i > 1, j = 1
Finally for i > 1 and j > 1,
?(ei1, f j1 , s) =
?
s??S
ts?,s[?(ei1, f j?11 , s?)P (?, fj |s)+
?(ei?11 , f
j
1 , s?)P (ei, ?|s)+
?(ei?11 , f
j?1
1 , s?)P (ei, fj|s)]
Similarly the recurrence for ?(eNi , fMj , s)
?
?
?
?
?
ts,S+1 i = N + 1,
j = M + 1
PS
s?=1 ts,s??(eNi , fMj+1, s?)P (?, fj |s?) i = N, j < M
PS
s?=1 ts,s??(eNi+1, fMj , s?)P (ei, ?|s?) i < N, j = M
For i < N and j < M , ?(eNi , fMj , s) =
?
s??S
ts,s?[?(eNi , fMj+1, s?)P (?, fj |s?)+
?(eNi+1, fMj , s?)P (ei, ?|s?)+
?(eNi+1, fMj+1, s?)P (ei, fj|s?)]
In order to proceed with the E.M. estimation
of the parameters T and Ps , we collect the
soft counts c(e, f |s) for emission probabilities by
looping over the training data D as shown in Fig-
ure 1.
Similarly the soft counts ct(s?, s) for the tran-
sition probabilities are estimated as shown in Fig-
ure 2.
Finally the probabilities P (e, f |s) and tij are re-
estimated as
P? (e, f |s) = c(e, f |s)?
e?E,f?F c(e, f |s)
(2)
t?s?,s =
ct(s?, s)
?
s ct(s?, s)
(3)
We can also compute the most probable align-
ment (e?, f? ) between the two strings e and f as
121
c(e, f |s) =
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?
s?
?(ei?11 , f
j?1
1 , s?)ts?,sP (ei, fj |s)?(eNi , fMj , s)1(ei = e, fj = f)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?
s?
?(ei?11 , f
j
1 , s?)ts?,sP (ei, ?|s)?(eNi , fMj , s)1(ei = e, fj = f)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?
s?
?(ei1, f j?11 , s?)ts?,sP (?, fj |s)?(eNi , fMj , s)1(ei = e, fj = f)
Figure 1: EM soft count c(e, f |s) estimation.
ct(s?, s) =
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?(ei?11 , f
j?1
1 , s?)ts?,sP (ei, fj|s)?(eNi , fMj , s)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?(ei?11 , f
j
1 , s?)ts?,sP (ei, ?|s)?(eNi , fMj , s)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?(ei1, f j?11 , s?)ts?,sP (?, fj|s)?(eNi , fMj , s)
+
?
(e,f)?D
1
Q(e, f)?(e
N
1 , fM1 , s?)ts?,S+11(s = S + 1)
Figure 2: EM soft count ct(s?, s) estimation.
122
arg max
(e?,?f)?J(e,f)
?
0=s0,...,s|e?|+1=S+1
t0,s1
|e?|
?
i=1
tsi,si+1P (e?i, f?i|si)
The pair (e?, f?) is considered as an alignment be-
tween the training pair (e, f).
3 Transduction of the Transliterated
Output
Given an alignment (e?, f?), we consider all possi-
ble sub-alignments (e?ji , f?
j
i ) as pairs of substrings
obtained from (e?, f? ) such that e?i 6= ?, f?i 6= ?,
e?j+1 6= ? and f?j+1 6= ? . We extract all pos-
sible sub-alignments of all the alignments from
the training data. Let A be the bag of all sub-
alignments obtained from the training data. We
build a weighted finite state transducer that trans-
duces any string in E+ to F+ using these sub-
alignments.
Let (u,v) be an element of A. From the train-
ing data D, observe that A can have multiple re-
alizations of (u,v). Let N(u,v) be the number
of times (u,v) is observed in A. The empirical
probability of transducing string u to v is simply
P (v|u) = N(u,v)?
v:(u,v?)?A N(u,v?)
For every pair (u,v) ? A , we also compute the
probability of transliteration from the HMM H as
Q(u,v) from Equation 1.
We construct a finite state transducer Fu,v that
accepts only u and emits v with a weight wu,v
defined as
wu,v = ? log(P (v|u))?? log(Q(u,v))+? (4)
Finally we construct a global weighted finite
state transducer F by taking the union of all the
Fu,v and taking its closure.
F =
?
?
?
(u,v)?A
Fu,v
?
?
+
(5)
The weight ? is typically sufficiently high so
that a new english string is favored to be broken
into fewest possible sub-strings whose translitera-
tions are available in the training data.
We tune the weights ? and ? by evaluating the
accuracy on the held-out data. The n-best paths
in the weighted finite state transducer F represent
our n-best transliterations.
4 Results
We evaluated our system on the standard track data
provided by the NEWS 2009 shared task orga-
nizers on five different languages ? Tamil, Hindi,
Russian, and Kannada was derived from (Ku-
maran and Kellner, 2007) and Chinese from (Li et
al., 2004). The results of this evaluation on the test
data is shown in Table 1. For a detailed description
Language Top-1 mean MRR
Accuracy F1 score
Tamil 0.327 0.870 0.458
Hindi 0.398 0.855 0.515
Russian 0.506 0.901 0.609
Chinese 0.450 0.755 0.514
Kannada 0.235 0.817 0.353
Table 1: Results on NEWS 2009 test data.
of the evaluation measures used we refer the read-
ers to NEWS 2009 shared task whitepaper (Li et
al., 2009).
5 Conclusion
We described a system for automatic translitera-
tion of pairs of strings from one language to an-
other using ?-extension hidden markov models and
weighted finite state transducers. We evaluated
our system on all the languages for the NEWS
2009 standard track. The system presented is lan-
guage agnostic and can be trained for any language
pair within a few minutes on a single core desktop
computer.
References
Nasreen Abdul Jaleel and Leah Larkey. 2003. Statistical transliteration for english-arabic
cross language information retrieval. In Proceedings of the twelfth international con-
ference on Information and knowledge management, pages 139?146.
Kevin Knight and Jonathan Graehl. 1997. Machine transliteration. In Computational Lin-
guistics, pages 128?135.
A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration.
In SIGIR ?07: Proceedings of the 30th annual international ACM SIGIR conference
on Research and development in information retrieval, pages 721?722, New York, NY,
USA. ACM.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine
transliteration. In ACL ?04: Proceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, page 159, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine. 2009. Whitepaper of
news 2009 machine transliteration shared task. In Proceedings of ACL-IJCNLP 2009
Named Entities Workshop (NEWS 2009).
Lawrence Rabiner. 1989. A tutorial on hidden markov models and selected applications in
speech recognition. In Proceedings of the IEEE, pages 257?286.
Yilu Zhou, Feng Huang, and Hsinchun Chen. 2008. Combining probability models and web
mining models: a framework for jproper name transliteration. Information Technology
and Management, 9(2):91?103.
123
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277?285,
Beijing, August 2010
Entity Disambiguation for Knowledge Base Population
?Mark Dredze and ?Paul McNamee and ?Delip Rao and ?Adam Gerber and ?Tim Finin
?Human Language Technology Center of Excellence, Center for Language and Speech Processing
Johns Hopkins University
?University of Maryland ? Baltimore County
mdredze,mcnamee,delip,adam.gerber@jhu.edu, finin@umbc.edu
Abstract
The integration of facts derived from information extraction
systems into existing knowledge bases requires a system to
disambiguate entity mentions in the text. This is challeng-
ing due to issues such as non-uniform variations in entity
names, mention ambiguity, and entities absent from a knowl-
edge base. We present a state of the art system for entity dis-
ambiguation that not only addresses these challenges but also
scales to knowledge bases with several million entries using
very little resources. Further, our approach achieves perfor-
mance of up to 95% on entities mentioned from newswire
and 80% on a public test set that was designed to include
challenging queries.
1 Introduction
The ability to identify entities like people, orga-
nizations and geographic locations (Tjong Kim
Sang and De Meulder, 2003), extract their at-
tributes (Pasca, 2008), and identify entity rela-
tions (Banko and Etzioni, 2008) is useful for sev-
eral applications in natural language processing
and knowledge acquisition tasks like populating
structured knowledge bases (KB).
However, inserting extracted knowledge into a
KB is fraught with challenges arising from nat-
ural language ambiguity, textual inconsistencies,
and lack of world knowledge. To the discern-
ing human eye, the ?Bush? in ?Mr. Bush left
for the Zurich environment summit in Air Force
One.? is clearly the US president. Further con-
text may reveal it to be the 43rd president, George
W. Bush, and not the 41st president, George H.
W. Bush. The ability to disambiguate a polyse-
mous entity mention or infer that two orthograph-
ically different mentions are the same entity is
crucial in updating an entity?s KB record. This
task has been variously called entity disambigua-
tion, record linkage, or entity linking. When per-
formed without a KB, entity disambiguation is
called coreference resolution: entity mentions ei-
ther within the same document or across multi-
ple documents are clustered together, where each
cluster corresponds to a single real world entity.
The emergence of large scale publicly avail-
able KBs like Wikipedia and DBPedia has spurred
an interest in linking textual entity references to
their entries in these public KBs. Bunescu and
Pasca (2006) and Cucerzan (2007) presented im-
portant pioneering work in this area, but suffer
from several limitations including Wikipedia spe-
cific dependencies, scale, and the assumption of
a KB entry for each entity. In this work we in-
troduce an entity disambiguation system for link-
ing entities to corresponding Wikipedia pages de-
signed for open domains, where a large percent-
age of entities will not be linkable. Further, our
method and some of our features readily general-
ize to other curated KB. We adopt a supervised
approach, where each of the possible entities con-
tained within Wikipedia are scored for a match to
the query entity. We also describe techniques to
deal with large knowledge bases, like Wikipedia,
which contain millions of entries. Furthermore,
our system learns when to withhold a link when
an entity has no matching KB entry, a task that
has largely been neglected in prior research in
cross-document entity coreference. Our system
produces high quality predictions compared with
recent work on this task.
2 Related Work
The information extraction oeuvre has a gamut of
relation extraction methods for entities like per-
sons, organizations, and locations, which can be
classified as open- or closed-domain depending
on the restrictions on extractable relations (Banko
and Etzioni, 2008). Closed domain systems ex-
tract a fixed set of relations while in open-domain
systems, the number and type of relations are un-
bounded. Extracted relations still require process-
ing before they can populate a KB with facts:
namely, entity linking and disambiguation.
277
Motivated by ambiguity in personal name
search, Mann and Yarowsky (2003) disambiguate
person names using biographic facts, like birth
year, occupation and affiliation. When present
in text, biographic facts extracted using regular
expressions help disambiguation. More recently,
the Web People Search Task (Artiles et al, 2008)
clustered web pages for entity disambiguation.
The related task of cross document corefer-
ence resolution has been addressed by several
researchers starting from Bagga and Baldwin
(1998). Poesio et al (2008) built a cross document
coreference system using features from encyclo-
pedic sources like Wikipedia. However, success-
ful coreference resolution is insufficient for cor-
rect entity linking, as the coreference chain must
still be correctly mapped to the proper KB entry.
Previous work by Bunescu and Pasca (2006)
and Cucerzan (2007) aims to link entity men-
tions to their corresponding topic pages in
Wikipedia but the authors differ in their ap-
proaches. Cucerzan uses heuristic rules and
Wikipedia disambiguation markup to derive map-
pings from surface forms of entities to their
Wikipedia entries. For each entity in Wikipedia,
a context vector is derived as a prototype for the
entity and these vectors are compared (via dot-
product) with the context vectors of unknown en-
tity mentions. His work assumes that all entities
have a corresponding Wikipedia entry, but this as-
sumption fails for a significant number of entities
in news articles and even more for other genres,
like blogs. Bunescu and Pasca on the other hand
suggest a simple method to handle entities not in
Wikipedia by learning a threshold to decide if the
entity is not in Wikipedia. Both works mentioned
rely on Wikipedia-specific annotations, such as
category hierarchies and disambiguation links.
We just recently became aware of a system
fielded by Li et al at the TAC-KBP 2009 eval-
uation (2009). Their approach bears a number
of similarities to ours; both systems create candi-
date sets and then rank possibilities using differing
learning methods, but the principal difference is in
our approach to NIL prediction. Where we simply
consider absence (i.e., the NIL candidate) as an-
other entry to rank, and select the top-ranked op-
tion, they use a separate binary classifier to decide
whether their top prediction is correct, or whether
NIL should be output. We believe relying on fea-
tures that are designed to inform whether absence
is correct is the better alternative.
3 Entity Linking
We define entity linking as matching a textual en-
tity mention, possibly identified by a named en-
tity recognizer, to a KB entry, such as a Wikipedia
page that is a canonical entry for that entity. An
entity linking query is a request to link a textual
entity mention in a given document to an entry in
a KB. The system can either return a matching en-
try or NIL to indicate there is no matching entry.
In this work we focus on linking organizations,
geo-political entities and persons to a Wikipedia
derived KB.
3.1 Key Issues
There are 3 challenges to entity linking:
Name Variations. An entity often has multiple
mention forms, including abbreviations (Boston
Symphony Orchestra vs. BSO), shortened forms
(Osama Bin Laden vs. Bin Laden), alternate
spellings (Osama vs. Ussamah vs. Oussama),
and aliases (Osama Bin Laden vs. Sheikh Al-
Mujahid). Entity linking must find an entry de-
spite changes in the mention string.
Entity Ambiguity. A single mention, like
Springfield, can match multiple KB entries, as
many entity names, like people and organizations,
tend to be polysemous.
Absence. Processing large text collections vir-
tually guarantees that many entities will not ap-
pear in the KB (NIL), even for large KBs.
The combination of these challenges makes
entity linking especially challenging. Consider
an example of ?William Clinton.? Most read-
ers will immediately think of the 42nd US pres-
ident. However, the only two William Clintons in
Wikipedia are ?William de Clinton? the 1st Earl
of Huntingdon, and ?William Henry Clinton? the
British general. The page for the 42nd US pres-
ident is actually ?Bill Clinton?. An entity link-
ing system must decide if either of the William
Clintons are correct, even though neither are ex-
act matches. If the system determines neither
278
matches, should it return NIL or the variant ?Bill
Clinton?? If variants are acceptable, then perhaps
?Clinton, Iowa? or ?DeWitt Clinton? should be
acceptable answers?
3.2 Contributions
We address these entity linking challenges.
Robust Candidate Selection. Our system is
flexible enough to find name variants but suffi-
ciently restrictive to produce a manageable can-
didate list despite a large-scale KB.
Features for Entity Disambiguation. We de-
veloped a rich and extensible set of features based
on the entity mention, the source document, and
the KB entry. We use a machine learning ranker
to score each candidate.
Learning NILs. We modify the ranker to learn
NIL predictions, which obviates hand tuning and
importantly, admits use of additional features that
are indicative of NIL.
Our contributions differ from previous efforts
(Bunescu and Pasca, 2006; Cucerzan, 2007) in
several important ways. First, previous efforts de-
pend on Wikipedia markup for significant perfor-
mance gains. We make no such assumptions, al-
though we show that optional Wikipedia features
lead to a slight improvement. Second, Cucerzan
does not handle NILs while Bunescu and Pasca
address them by learning a threshold. Our ap-
proach learns to predict NIL in a more general
and direct way. Third, we develop a rich fea-
ture set for entity linking that can work with any
KB. Finally, we apply a novel finite state machine
method for learning name variations. 1
The remaining sections describe the candidate
selection system, features and ranking, and our
novel approach learning NILs, followed by an
empirical evaluation.
4 Candidate Selection for Name Variants
The first system component addresses the chal-
lenge of name variants. As the KB contains a large
number of entries (818,000 entities, of which 35%
are PER, ORG or GPE), we require an efficient se-
lection of the relevant candidates for a query.
Previous approaches used Wikipedia markup
for filtering ? only using the top-k page categories
1http://www.clsp.jhu.edu/ markus/fstrain
(Bunescu and Pasca, 2006) ? which is limited to
Wikipedia and does not work for general KBs.
We consider a KB independent approach to selec-
tion that also allows for tuning candidate set size.
This involves a linear pass over KB entry names
(Wikipedia page titles): a naive implementation
took two minutes per query. The following sec-
tion reduces this to under two seconds per query.
For a given query, the system selects KB entries
using the following approach:
? Titles that are exact matches for the mention.
? Titles that are wholly contained in or contain
the mention (e.g., Nationwide and Nationwide In-
surance).
? The first letters of the entity mention match the
KB entry title (e.g., OA and Olympic Airlines).
? The title matches a known alias for the entity
(aliases described in Section 5.2).
? The title has a strong string similarity score
with the entity mention. We include several mea-
sures of string similarity, including: character
Dice score > 0.9, skip bigram Dice score > 0.6,
and Hamming distance <= 2.
We did not optimize the thresholds for string
similarity, but these could obviously be tuned to
minimize the candidate sets and maximize recall.
All of the above features are general for any
KB. However, since our evaluation used a KB
derived from Wikipedia, we included a few
Wikipedia specific features. We added an entry if
its Wikipedia page appeared in the top 20 Google
results for a query.
On the training dataset (Section 7) the selection
system attained a recall of 98.8% and produced
candidate lists that were three to four orders of
magnitude smaller than the KB. Some recall er-
rors were due to inexact acronyms: ABC (Arab
Banking; ?Corporation? is missing), ASG (Abu
Sayyaf; ?Group? is missing), and PCF (French
Communist Party; French reverses the order of the
pre-nominal adjectives). We also missed Interna-
tional Police (Interpol) and Becks (David Beck-
ham; Mr. Beckham and his wife are collectively
referred to as ?Posh and Becks?).
279
4.1 Scaling Candidate Selection
Our previously described candidate selection re-
lied on a linear pass over the KB, but we seek
more efficient methods. We observed that the
above non-string similarity filters can be pre-
computed and stored in an index, and that the skip
bigram Dice score can be computed by indexing
the skip bigrams for each KB title. We omitted
the other string similarity scores, and collectively
these changes enable us to avoid a linear pass over
the KB. Finally we obtained speedups by serving
the KB concurrently2. Recall was nearly identical
to the full system described above: only two more
queries failed. Additionally, more than 95% of
the processing time was consumed by Dice score
computation, which was only required to cor-
rectly retrieve less than 4% of the training queries.
Omitting the Dice computation yielded results in
a few milliseconds. A related approach is that of
canopies for scaling clustering for large amounts
of bibliographic citations (McCallum et al, 2000).
In contrast, our setting focuses on alignment vs.
clustering mentions, for which overlapping parti-
tioning approaches like canopies are applicable.
5 Entity Linking as Ranking
We select a single correct candidate for a query
using a supervised machine learning ranker. We
represent each query by a D dimensional vector
x, where x ? RD, and we aim to select a sin-
gle KB entry y, where y ? Y , a set of possible
KB entries for this query produced by the selec-
tion system above, which ensures that Y is small.
The ith query is given by the pair {xi, yi}, where
we assume at most one correct KB entry.
To evaluate each candidate KB entry in Y we
create feature functions of the form f(x, y), de-
pendent on both the example x (document and en-
tity mention) and the KB entry y. The features
address name variants and entity disambiguation.
We take a maximum margin approach to learn-
ing: the correct KB entry y should receive a
higher score than all other possible KB entries
y? ? Y, y? 6= y plus some margin ?. This learning
2Our Python implementation with indexing features and
four threads achieved up to 80? speedup compared to naive
implementation.
constraint is equivalent to the ranking SVM algo-
rithm of Joachims (2002), where we define an or-
dered pair constraint for each of the incorrect KB
entries y? and the correct entry y. Training sets pa-
rameters such that score(y) ? score(y?) + ?. We
used the library SVMrank to solve this optimiza-
tion problem.3 We used a linear kernel, set the
slack parameter C as 0.01 times the number of
training examples, and take the loss function as
the total number of swapped pairs summed over
all training examples. While previous work used
a custom kernel, we found a linear kernel just as
effective with our features. This has the advan-
tage of efficiency in both training and prediction 4
? important considerations in a system meant to
scale to millions of KB entries.
5.1 Features for Entity Disambiguation
200 atomic features represent x based on each
candidate query/KB pair. Since we used a lin-
ear kernel, we explicitly combined certain fea-
tures (e.g., acroynym-match AND known-alias) to
model correlations. This included combining each
feature with the predicted type of the entity, al-
lowing the algorithm to learn prediction functions
specific to each entity type. With feature combina-
tions, the total number of features grew to 26,569.
The next sections provide an overview; for a de-
tailed list see McNamee et al (2009).
5.2 Features for Name Variants
Variation in entity name has long been recog-
nized as a bane for information extraction sys-
tems. Poor handling of entity name variants re-
sults in low recall. We describe several features
ranging from simple string match to finite state
transducer matching.
String Equality. If the query name and KB en-
try name are identical, this is a strong indication of
a match, and in our KB entry names are distinct.
However, similar or identical entry names that
refer to distinct entities are often qualified with
parenthetical expressions or short clauses. As
an example, ?London, Kentucky? is distinguished
3www.cs.cornell.edu/people/tj/svm_light/svm_rank.html
4Bunescu and Pasca (2006) report learning tens of thou-
sands of support vectors with their ?taxonomy? kernel while
a linear kernel represents all support vectors with a single
weight vector, enabling faster training and prediction.
280
from ?London, Ontario?, ?London, Arkansas?,
?London (novel)?, and ?London?. Therefore,
other string equality features were used, such as
whether names are equivalent after some transfor-
mation. For example, ?Baltimore? and ?Baltimore
City? are exact matches after removing a common
GPE word like city; ?University of Vermont? and
?University of VT? match if VT is expanded.
Approximate String Matching. Many entity
mentions will not match full names exactly. We
added features for character Dice, skip bigram
Dice, and left and right Hamming distance scores.
Features were set based on quantized scores.
These were useful for detecting minor spelling
variations or mistakes. Features were also added if
the query was wholly contained in the entry name,
or vice-versa, which was useful for handling ellip-
sis (e.g., ?United States Department of Agricul-
ture? vs. ?Department of Agriculture?). We also
included the ratio of the recursive longest com-
mon subsequence (Christen, 2006) to the shorter
of the mention or entry name, which is effective at
handling some deletions or word reorderings (e.g.,
?Li Gong? and ?Gong Li?). Finally, we checked
whether all of the letters of the query are found in
the same order in the entry name (e.g., ?Univ Wis-
consin? would match ?University of Wisconsin?).
Acronyms. Features for acronyms, using dic-
tionaries and partial character matches, enable
matches between ?MIT? and ?Madras Institute of
Technology? or ?Ministry of Industry and Trade.?
Aliases. Many aliases or nicknames are non-
trivial to guess. For example JAVA is the
stock symbol for Sun Microsystems, and ?Gin-
ger Spice? is a stage name of Geri Halliwell. A
reasonable way to do this is to employ a dictio-
nary and alias lists that are commonly available
for many domains5.
FST Name Matching. Another measure of sur-
face similarity between a query and a candidate
was computed by training finite-state transducers
similar to those described in Dreyer et al (2008).
These transducers assign a score to any string pair
by summing over all alignments and scoring all
5We used multiple lists, including class-specific lists (i.e.,
for PER, ORG, and GPE) lists extracted from Freebase (Bol-
lacker et al, 2008) and Wikipedia redirects. PER, ORG, and
GPE are the commonly used terms for entity types for peo-
ple, organizations and geo-political regions respectively.
contained character n-grams; we used n-grams of
length 3 and less. The scores are combined using a
global log-linear model. Since different spellings
of a name may vary considerably in length (e.g.,
J Miller vs. Jennifer Miller) we eliminated the
limit on consecutive insertions used in previous
applications.6
5.3 Wikipedia Features
Most of our features do not depend on Wikipedia
markup, but it is reasonable to include features
from KB properties. Our feature ablation study
shows that dropping these features causes a small
but statistically significant performance drop.
WikiGraph statistics. We added features de-
rived from the Wikipedia graph structure for an
entry, like indegree of a node, outdegree of a node,
and Wikipedia page length in bytes. These statis-
tics favor common entity mentions over rare ones.
Wikitology. KB entries can be indexed with hu-
man or machine generated metadata consisting of
keywords or categories in a domain-appropriate
taxonomy. Using a system called Wikitology,
Syed et al (2008) investigated use of ontology
terms obtained from the explicit category system
in Wikipedia as well as relationships induced from
the hyperlink graph between related Wikipedia
pages. Following this approach we computed top-
ranked categories for the query documents and
used this information as features. If none of the
candidate KB entries had corresponding highly-
ranked Wikitology pages, we used this as a NIL
feature (Section 6.1).
5.4 Popularity
Although it may be an unsafe bias to give prefer-
ence to common entities, we find it helpful to pro-
vide estimates of entity popularity to our ranker
as others have done (Fader et al, 2009). Apart
from the graph-theoretic features derived from the
Wikipedia graph, we used Google?s PageRank to
by adding features indicating the rank of the KB
entry?s corresponding Wikipedia page in a Google
query for the target entity mention.
6Without such a limit, the objective function may diverge
for certain parameters of the model; we detect such cases and
learn to avoid them during training.
281
5.5 Document Features
The mention document and text associated with a
KB entry contain context for resolving ambiguity.
Entity Mentions. Some features were based on
presence of names in the text: whether the query
appeared in the KB text and the entry name in the
document. Additionally, we used a named-entity
tagger and relation finder, SERIF (Boschee et al,
2005), identified name and nominal mentions that
were deemed co-referent with the entity mention
in the document, and tested whether these nouns
were present in the KB text. Without the NE anal-
ysis, accuracy on non-NIL entities dropped 4.5%.
KB Facts. KB nodes contain infobox attributes
(or facts); we tested whether the fact text was
present in the query document, both locally to a
mention, or anywhere in the text. Although these
facts were derived from Wikipedia infoboxes,
they could be obtained from other sources as well.
Document Similarity We measured similarity
between the query document and the KB text in
two ways: cosine similarity with TF/IDF weight-
ing (Salton and McGill, 1983); and using the Dice
coefficient over bags of words. IDF values were
approximated using counts from the Google 5-
gram dataset as by Klein and Nelson (2008).
Entity Types. Since the KB contained types
for entries, we used these as features as well as
the predicted NE type for the entity mention in
the document text. Additionally, since only a
small number of KB entries had PER, ORG, or
GPE types, we also inferred types from Infobox
class information to attain 87% coverage in the
KB. This was helpful for discouraging selection
of eponymous entries named after famous enti-
ties (e.g., the former U.S. president vs. ?John F.
Kennedy International Airport?).
5.6 Feature Combinations
To take into account feature dependencies we cre-
ated combination features by taking the cross-
product of a small set of diverse features. The
attributes used as combination features included
entity type; a popularity based on Google?s rank-
ings; document comparison using TF/IDF; cov-
erage of co-referential nouns in the KB node
text; and name similarity. The combinations were
cascaded to allow arbitrary feature conjunctions.
Thus it is possible to end up with a feature kbtype-
is-ORG AND high-TFIDF-score AND low-name-
similarity. The combined features increased the
number of features from roughly 200 to 26,000.
6 Predicting NIL Mentions
So far we have assumed that each example has a
correct KB entry; however, when run over a large
corpus, such as news articles, we expect a signifi-
cant number of entities will not appear in the KB.
Hence it will be useful to predict NILs.
We learn when to predict NIL using the SVM
ranker by augmenting Y to include NIL, which
then has a single feature unique to NIL answers.
It can be shown that (modulo slack variables) this
is equivalent to learning a single threshold ? for
NIL predictions as in Bunescu and Pasca (2006).
Incorporating NIL into the ranker has several
advantages. First, the ranker can set the thresh-
old optimally without hand tuning. Second, since
the SVM scores are relative within a single exam-
ple and cannot be compared across examples, set-
ting a single threshold is difficult. Third, a thresh-
old sets a uniform standard across all examples,
whereas in practice we may have reasons to favor
a NIL prediction in a given example. We design
features for NIL prediction that cannot be cap-
tured in a single parameter.
6.1 NIL Features
Integrating NIL prediction into learning means
we can define arbitrary features indicative of NIL
predictions in the feature vector corresponding to
NIL. For example, if many candidates have good
name matches, it is likely that one of them is cor-
rect. Conversely, if no candidate has high entry-
text/article similarity, or overlap between facts
and the article text, it is likely that the entity is
absent from the KB. We included several features,
such as a) the max, mean, and difference between
max and mean for 7 atomic features for all KB
candidates considered, b) whether any of the can-
didate entries have matching names (exact and
fuzzy string matching), c) whether any KB en-
try was a top Wikitology match, and d) if the top
Google match was not a candidate.
282
Micro-Averaged Macro-Averaged
Best Median All Features Best Features Best Median All Features Best Features
All 0.8217 0.7108 0.7984 0.7941 0.7704 0.6861 0.7695 0.7704
non-NIL 0.7725 0.6352 0.7063 0.6639 0.6696 0.5335 0.6097 0.5593
NIL 0.8919 0.7891 0.8677 0.8919 0.8789 0.7446 0.8464 0.8721
Table 1: Micro and macro-averaged accuracy for TAC-KBP data compared to best and median reported performance.
Results are shown for all features as well as removing a small number of features using feature selection on development data.
7 Evaluation
We evaluated our system on two datasets: the
Text Analysis Conference (TAC) track on Knowl-
edge Base Population (TAC-KBP) (McNamee and
Dang, 2009) and the newswire data used by
Cucerzan (2007) (Microsoft News Data).
Since our approach relies on supervised learn-
ing, we begin by constructing our own training
corpus.7 We highlighted 1496 named entity men-
tions in news documents (from the TAC-KBP doc-
ument collection) and linked these to entries in
a KB derived from Wikipedia infoboxes. 8 We
added to this collection 119 sample queries from
the TAC-KBP data. The total of 1615 training ex-
amples included 539 (33.4%) PER, 618 (38.3%)
ORG, and 458 (28.4%) GPE entity mentions. Of
the training examples, 80.5% were found in the
KB, matching 300 unique entities. This set has a
higher number of NIL entities than did Bunescu
and Pasca (2006) (10%) but lower than the TAC-
KBP test set (43%).
All system development was done using a train
(908 examples) and development (707 examples)
split. The TAC-KBP and Microsoft News data
sets were held out for final tests. A model trained
on all 1615 examples was used for experiments.
7.1 TAC-KBP 2009 Experiments
The KB is derived from English Wikipedia pages
that contained an infobox. Entries contain basic
descriptions (article text) and attributes. The TAC-
KBP query set contains 3904 entity mentions for
560 distinct entities; entity type was only provided
for evaluation. The majority of queries were for
organizations (69%). Most queries were missing
from the KB (57%). 77% of the distinct GPEs
in the queries were present in the KB, but for
7Data available from www.dredze.com
8http://en.wikipedia.org/wiki/Help:Infobox
PERs and ORGs these percentages were signifi-
cantly lower, 19% and 30% respectively.
Table 1 shows results on TAC-KBP data us-
ing all of our features as well a subset of features
based on feature selection experiments on devel-
opment data. We include scores for both micro-
averaged accuracy ? averaged over all queries
? and macro-averaged accuracy ? averaged over
each unique entity ? as well as the best and me-
dian reported results for these data (McNamee
and Dang, 2009). We obtained the best reported
results for macro-averaged accuracy, as well as
the best results for NIL detection with micro-
averaged accuracy, which shows the advantage of
our approach to learning NIL. See McNamee et
al. (2009) for additional experiments.
The candidate selection phase obtained a re-
call of 98.6%, similar to that of development data.
Missed candidates included Iron Lady, which
refers metaphorically to Yulia Tymoshenko, PCC,
the Spanish-origin acronym for the Cuban Com-
munist Party, and Queen City, a former nickname
for the city of Seattle, Washington. The system re-
turned a mean of 76 candidates per query, but the
median was 15 and the maximum 2772 (Texas). In
about 10% of cases there were four or fewer can-
didates and in 10% of cases there were more than
100 candidate KB nodes. We observed that ORGs
were more difficult, due to the greater variation
and complexity in their naming, and that they can
be named after persons or locations.
7.2 Feature Effectiveness
We performed two feature analyses on the TAC-
KBP data: an additive study ? starting from a
small baseline feature set used in candidate selec-
tion we add feature groups and measure perfor-
mance changes (omitting feature combinations),
and an ablative study ? starting from all features,
remove a feature group and measure performance.
283
Class All non-NIL NIL
Baseline 0.7264 0.4621 0.9251
Acronyms 0.7316 0.4860 0.9161
NE Analysis 0.7661 0.7181 0.8022
Google 0.7597 0.7421 0.7730
Doc/KB Text Similarity 0.7313 0.6699 0.7775
Wikitology 0.7318 0.4549 0.9399
All 0.7984 0.7063 0.8677
Table 2: Additive analysis: micro-averaged accuracy.
Table 2 shows the most significant features in
the feature addition experiments. The baseline
includes only features based on string similarity
or aliases and is not effective at finding correct
entries and strongly favors NIL predictions. In-
clusion of features based on analysis of named-
entities, popularity measures (e.g., Google rank-
ings), and text comparisons provided the largest
gains. The overall changes are fairly small,
roughly ?1%; however changes in non-NIL pre-
cision are larger.
The ablation study showed considerable redun-
dancy across feature groupings. In several cases,
performance could have been slightly improved
by removing features. Removing all feature com-
binations would have improved overall perfor-
mance to 81.05% by gaining on non-NIL for a
small decline on NIL detection.
7.3 Experiments on Microsoft News Data
We downloaded the evaluation data used in
Cucerzan (2007)9: 20 news stories from MSNBC
with 642 entity mentions manually linked to
Wikipedia and another 113 mentions not having
any corresponding link to Wikipedia.10 A sig-
nificant percentage of queries were not of type
PER, ORG, or GPE (e.g., ?Christmas?). SERIF
assigned entity types and we removed 297 queries
not recognized as entities (counts in Table 3).
We learned a new model on the training data
above using a reduced feature set to increase
speed.11 Using our fast candidate selection sys-
tem, we resolved each query in 1.98 seconds (me-
dian). Query processing time was proportional to
9http://research.microsoft.com/en-us/um/people/silviu/WebAssistant/TestData/
10One of the MSNBC news articles is no longer available
so we used 759 total entities.
11We removed Google, FST and conjunction features
which reduced system accuracy but increased performance.
Num. Queries Accuracy
Total Nil All non-NIL NIL
NIL 452 187 0.4137 0.0 1.0
GPE 132 20 0.9696 1.00 0.8000
ORG 115 45 0.8348 0.7286 1.00
PER 205 122 0.9951 0.9880 1.00
All 452 187 0.9469 0.9245 0.9786
Cucerzan (2007) 0.914 - -
Table 3: Micro-average results for Microsoft data.
the number of candidates considered. We selected
a median of 13 candidates for PER, 12 for ORG
and 102 for GPE. Accuracy results are in Table
3. The high results reported for this dataset over
TAC-KBP is primarily because we perform very
well in predicting popular and rare entries ? both
of which are common in newswire text.
One issue with our KB was that it was derived
from infoboxes in Wikipedia?s Oct 2008 version
which has both new entities, 12 and is missing en-
tities.13 Therefore, we manually confirmed NIL
answers and new answers for queries marked as
NIL in the data. While an exact comparison is not
possible (as described above), our results (94.7%)
appear to be at least on par with Cucerzan?s sys-
tem (91.4% overall accuracy).With the strong re-
sults on TAC-KBP, we believe that this is strong
confirmation of the effectiveness of our approach.
8 Conclusion
We presented a state of the art system to disam-
biguate entity mentions in text and link them to
a knowledge base. Unlike previous approaches,
our approach readily ports to KBs other than
Wikipedia. We described several important chal-
lenges in the entity linking task including han-
dling variations in entity names, ambiguity in en-
tity mentions, and missing entities in the KB, and
we showed how to each of these can be addressed.
We described a comprehensive feature set to ac-
complish this task in a supervised setting. Impor-
tantly, our method discriminately learns when not
to link with high accuracy. To spur further re-
search in these areas we are releasing our entity
linking system.
122008 vs. 2006 version used in Cucerzan (2007) We
could not get the 2006 version from the author or the Internet.
13Since our KB was derived from infoboxes, entities not
having an infobox were left out.
284
References
Javier Artiles, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search: results of the first evalu-
ation and the plan for the second. In WWW.
Amit Bagga and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Conference on Computational
Linguistics (COLING).
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Association for Computational Linguistics.
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In SIGMOD Management of Data.
E. Boschee, R. Weischedel, and A. Zamanian. 2005.
Automatic information extraction. In Conference
on Intelligence Analysis.
Razvan C. Bunescu and Marius Pasca. 2006. Using
encyclopedic knowledge for named entity disam-
biguation. In European Chapter of the Assocation
for Computational Linguistics (EACL).
Peter Christen. 2006. A comparison of personal name
matching: Techniques and practical issues. Techni-
cal Report TR-CS-06-02, Australian National Uni-
versity.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on wikipedia data. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Empirical Methods in
Natural Language Processing (EMNLP).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling Wikipedia-based named entity dis-
ambiguation to arbitrary web text. In WikiAI09
Workshop at IJCAI 2009.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Knowledge Discovery
and Data Mining (KDD).
Martin Klein and Michael L. Nelson. 2008. A com-
parison of techniques for estimating IDF values to
generate lexical signatures for the web. In Work-
shop on Web Information and Data Management
(WIDM).
Fangtao Li, Zhicheng Zhang, Fan Bu, Yang Tang,
Xiaoyan Zhu, and Minlie Huang. 2009. THU
QUANTA at TAC 2009 KBP and RTE track. In Text
Analysis Conference (TAC).
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Conference
on Natural Language Learning (CONLL).
Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In
Knowledge Discovery and Data Mining (KDD).
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track.
In Text Analysis Conference (TAC).
Paul McNamee, Mark Dredze, Adam Gerber, Nikesh
Garera, Tim Finin, James Mayfield, Christine Pi-
atko, Delip Rao, David Yarowsky, and Markus
Dreyer. 2009. HLTCOE approaches to knowledge
base population at TAC 2009. In Text Analysis Con-
ference (TAC).
Marius Pasca. 2008. Turning web text and search
queries into factual knowledge: hierarchical class
attribute extraction. In National Conference on Ar-
tificial Intelligence (AAAI).
Massimo Poesio, David Day, Ron Artstein, Jason Dun-
can, Vladimir Eidelman, Claudio Giuliano, Rob
Hall, Janet Hitzeman, Alan Jern, Mijail Kabadjov,
Stanley Yong, Wai Keong, Gideon Mann, Alessan-
dro Moschitti, Simone Ponzetto, Jason Smith, Josef
Steinberger, Michael Strube, Jian Su, Yannick Ver-
sley, Xiaofeng Yang, and Michael Wick. 2008. Ex-
ploiting lexical and encyclopedic resources for en-
tity disambiguation: Final report. Technical report,
JHU CLSP 2007 Summer Workshop.
Gerard Salton and Michael McGill. 1983. Introduc-
tion to Modern Information Retrieval. McGraw-
Hill Book Company.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Confer-
ence on Natural Language Learning (CONLL).
Zareen Syed, Tim Finin, and Anupam Joshi. 2008.
Wikipedia as an ontology for describing documents.
In Proceedings of the Second International Confer-
ence on Weblogs and Social Media. AAAI Press.
285
Coling 2010: Poster Volume, pages 1050?1058,
Beijing, August 2010
Streaming Cross Document Entity Coreference Resolution
Delip Rao and Paul McNamee and Mark Dredze
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University
delip,mcnamee,mdredze@jhu.edu
Abstract
Previous research in cross-document en-
tity coreference has generally been re-
stricted to the offline scenario where the
set of documents is provided in advance.
As a consequence, the dominant approach
is based on greedy agglomerative cluster-
ing techniques that utilize pairwise vec-
tor comparisons and thus require O(n2)
space and time. In this paper we ex-
plore identifying coreferent entity men-
tions across documents in high-volume
streaming text, including methods for uti-
lizing orthographic and contextual infor-
mation. We test our methods using several
corpora to quantitatively measure both the
efficacy and scalability of our streaming
approach. We show that our approach
scales to at least an order of magnitude
larger data than previous reported meth-
ods.
1 Introduction
A key capability for successful information ex-
traction, topic detection and tracking, and ques-
tion answering is the ability to identify equiva-
lence classes of entity mentions. An entity is a
real-world person, place, organization, or object,
such as the person who serves as the 44th pres-
ident of the United States. An entity mention is
a string which refers to such an entity, such as
?Barack Hussein Obama?, ?Senator Obama? or
?President Obama?. The goal of coreference res-
olution is to identify and connect all textual entity
mentions that refer to the same entity.
The first step towards this goal is to identify all
references within the same document, or within
document coreference resolution. A document of-
ten has a leading canonical reference to the entity
(?Barack Obama?) followed by additional expres-
sions for the same entity (?President Obama.?)
An intra-document coreference system must first
identify each reference, often relying on named
entity recognition, and then decide if these refer-
ences refer to a single individual or multiple enti-
ties, creating a coreference chain for each unique
entity. Feature representations include surface
form similarity, lexical context of mentions, po-
sition in the document and distance between ref-
erences. A variety of statistical learning meth-
ods have been applied to this problem, including
use of decision trees (Soon et al, 2001; Ng and
Cardie, 2002), graph partitioning (Nicolae and
Nicolae, 2006), maximum-entropy models (Luo
et al, 2004), and conditional random fields (Choi
and Cardie, 2007).
Given pre-processed documents, in which enti-
ties have been identified and entity mentions have
been linked into chains, we seek to identify across
an entire document collection all chains that re-
fer to the same entity. This task is called cross
document coreference resolution (CDCR). Sev-
eral of the challenges associated with CDCR dif-
fer from the within document task. For example,
it is unlikely that the same document will discuss
John Phillips the American football player and
John Phillips the musician, but it is quite proba-
ble that documents discussing each will appear in
the same collection. Therefore, while matching
entities with the same mention string can work
well for within document coreference, more so-
phisticated approaches are necessary for the cross
document scenario where a one-entity-per-name
assumption is unreasonable.
One of the most common approaches to both
within document and cross document corefer-
ence resolution has been based on agglomerative
clustering, where vectors might be bag-of-word
contexts (Bagga and Baldwin, 1998; Mann and
1050
Yarowsky, 2003; Gooi and Allan, 2004; Chen
and Martin, 2007). These algorithms creates a
O(n2) dependence in the number of mentions ?
for within document ? and documents ? for cross
document. This is a reasonable limitation for
within document, since the number of references
will certainly be small; we are unlikely to en-
counter a document with millions of references.
In contrast to the small n encountered within a
document, we fully expect to run a CDCR sys-
tem on hundreds of thousands or millions of doc-
uments. Most previous approaches cannot handle
collections of this size.
In this work, we present a new method for
cross document coreference resolution that scales
to very large corpora. Our algorithm operates in
a streaming setting, in which documents are pro-
cessed one at a time and only a single time. This
creates a linear (O(n)) dependence on the num-
ber of documents in the collection, allowing us
to scale to millions of documents and millions
of unique entities. Our algorithm uses stream-
ing clustering with common coreference similar-
ity computations to achieve large scale. Further-
more, our method is designed to support both
name disambiguation and name variation.
In the next section, we give a survey of related
work. In Section 3 we detail our streaming setup,
giving a description of the streaming algorithm
and presenting efficient techniques for represent-
ing clusters over streams and for computing simi-
larity. Section 4 describes the data sets on which
we evaluate our methods and presents results. We
conclude with a discussion and description of on-
going work.
2 Related Work
Traditional approaches to cross document coref-
erence resolution have first constructed a vector
space representation derived from local (or global)
contexts of entity mentions in documents and then
performed some form of clustering on these vec-
tors. This is a simple extension of Firth?s distribu-
tional hypothesis applied to entities (Firth, 1957).
We describe some of the seminal work in this area.
Some of the earliest work in CDCR was by
Bagga and Baldwin (1998). Key contributions
of their research include: promotion of a set-
theoretic evaluation measure, B-CUBED; intro-
duction of a data set based on 197 New York
Times articles which mention a person named
John Smith; and, use of TF/IDF weighted vec-
tors and cosine similarity in single-link greedy ag-
glomerative clustering.
Mann and Yarowsky (2003) extended Bagga
and Baldwin?s work and contributed several inno-
vations, including: use of biographical attributes
(e.g., year of birth, occupation), and evaluation us-
ing pseudonames. Pseudonames are sets of artifi-
cially conflated names that are used as an efficient
method for producing a set of gold-standard dis-
ambiguations.1 Mann and Yarowsky used 4 pairs
of conflated names in their evaluation. Their sys-
tem did not perform as well on named entities with
little available biographic information.
Gooi and Allan (2004) expanded on the use
of pseudonames by semi-automatically creating
a much larger evaluation set, which they called
the ?Person-X? corpus. They relied on automated
named-entity tagging and domain-focused text re-
trieval. This data consisted of 34,404 documents
where a single person mention in each document
was rewritten as ?Person X?. Besides their novel
construction of a large-scale resource, they in-
vestigated several minor variations in clustering,
namely (a) use of Kullback-Leibler divergence as
a distance measure, (b) use of 55-word snippets
around entity mentions (vs. entire documents or
extracted sentences), and (c) scoring clusters us-
ing average-link instead of single- or complete-
link.
Finally, in more recent work, Chen and Martin
(2007) explore the CDCR task in both English and
Chinese. Their work focuses on use of both lo-
cal, and document-level noun-phrases as features
in their vector-space representation.
There have been a number of open evaluations
of CDCR systems. For example, the Web People
Search (WePS) workshops (Artiles et al, 2008)
have created a task for disambiguating personal
names from HTML pages. A set of ambiguous
names is chosen and each is submitted to a popular
web search engine. The top 100 pages are then
manually clustered.We discuss several other data
1See Sanderson (2000) for use of this technique in word
sense disambiguation.
1051
sets in Section 4.2
All of the papers mentioned above focus on dis-
ambiguating personal names. In contrast, our sys-
tem can also handle organizations and locations.
Also, as was mentioned earlier, we are commit-
ted to a scenario where documents are presented
in sequence and entities must be disambiguated
instantly, without the benefit of observing the en-
tire corpus. We believe that such a system is bet-
ter suited to highly dynamic environments such as
daily news feeds, blogs, and tweets. Additionally,
a streaming system exposes a set of known entity
clusters after each document is processed instead
of waiting until the end of the stream.
3 Approach
Our cross document coreference resolution sys-
tem relies on a streaming clustering algorithm
and efficient calculation of similarity scores. We
assume that we receive a stream of corefer-
ence chains, along with entity types, as they
are extracted from documents. We use SERIF
(Ramshaw and Weischedel, 2005), a state of the
art document analysis system which performs
intra-document coreference resolution. BBN de-
veloped SERIF to address information extraction
tasks in the ACE program and it is further de-
scribed in Pradhan et al (2007).
Each unique entity is represented by an entity
cluster c, comprised of entity chains from many
documents that refer to the same entity. Given
an entity coreference chain e, we identify the best
known entity cluster c. If a suitable entity cluster
is not found, a new entity cluster is formed.
An entity cluster is selected for a given corefer-
ence chain using several similarity scores, includ-
ing document context, predicted entity type, and
orthographic similarity between the entity men-
tion and previously discovered references in the
entity cluster. An efficient implementation of the
similarity score allows the system to identify the
top k most likely mentions without considering all
m entity clusters. The final output of our sys-
tem is a collection of entity clusters, each con-
taining a list of coreference chains and their doc-
uments. Additionally, due to its streaming nature,
2We preferred other data sets to the WePS data in our
evaluation because it is not easily placed in temporal order.
the system can be examined at any time to produce
this information based on only the documents that
have been processed thus far.
In the next sections, we describe both the clus-
tering algorithm and efficient computation of the
entity similarity scores.
3.1 Clustering Algorithm
We use a streaming clustering algorithm to cre-
ate entity clusters as follows. We observe a set
of points from a potentially infinite set X , one at
a time, and would like to maintain a fixed number
of clusters while minimizing the maximum cluster
radius, defined as the radius of the smallest ball
containing all points of the cluster. This setup is
well known in the theory and information retrieval
community and is referred to as the dynamic clus-
tering problem (Can and Ozkarahan, 1987).
Others have attempted to use an incremen-
tal clustering approach, such as Gooi and Al-
lan (2004) (who eventually prefer a hierarchi-
cal clustering approach), and Luo et al (2004),
who use a Bell tree approach for incrementally
clustering within document entity mentions. Our
work closely follows the Doubling Algorithm of
Charikar et al (1997), which has better perfor-
mance guarantees for streaming data. Streaming
clustering means potentially linear performance in
the number of observations since each document
need only be examined a single time, as opposed
to the quadratic cost of agglomerative clustering.3
The Doubling Algorithm consists of two stages:
update and merge. Update adds points to existing
clusters or creates new clusters while merge com-
bines clusters to prevent the clusters from exceed-
ing a fixed limit. New clusters are created accord-
ing to a threshold set using development data. We
selected a threshold of 0.5 since it worked well in
preliminary experiments. Since the number of en-
tities grows with time, we have skipped the merge
step in our initial experiments so as not to limit
cluster growth.
We use a dynamic caching scheme which backs
the actual clusters in a disk based index, but re-
3It is possible to implement hierarchical agglomerative
clustering in O(n logm) time where n is the number of
points and m in the number of clusters. However this is still
superlinear and expensive in situations where m continually
increases like in streaming coreference resolution.
1052
1 2 3 4 5 6log(Rank)0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
log(
Fre
que
ncy
)
PERLOCORG
Figure 1: Frequency vs. rank for 567k people,
136k organizations, and 25k locations in the New
York Times Annotated Corpus (Sandhaus, 2008).
tains basic cluster information in memory (see be-
low). Doing so improves paging performance as
observed in Omiecinski and Scheuermann (1984).
Motivated by the Zipfian distribution of named en-
tities in news sources (Figure 1), we organize our
cluster store using an LRU policy, which facili-
tates easy access to named entities that were ob-
served in the recent past. We obtain additional
performance gains by hashing the clusters based
on the constituent mention string (details below).
This allows us to quickly retrieve a small but re-
lated number of clusters, k. It is always the case
that k << m, the current number of clusters.
3.2 Candidate Cluster Selection
As part of any clustering algorithm, each new item
must be compared against current clusters. As we
see more documents, the number of unique clus-
ters (entities) grows. Therefore, we need efficient
methods to select candidate clusters.
To select the top candidate clusters, we obtain
those that have high orthographic similarity with
the head name mention in the coreference chain e.
We compute this similarity using the dice score on
either word unigrams or character skip bigrams.
For each entity mention string associated with a
cluster c, we generate all possible n-grams using
one of the above two policies. We then index the
cluster by each of its n-grams in a hash maintained
in memory. In addition, we keep the number of n-
grams generated for each cluster.
When given a new head mention e for a coref-
erence chain, we generate all of the n-grams and
look up clusters that contain these n-grams using
the hash. We then compute the dice score:
dice(e, c) = |{ngram(e)} ? {ngram(c)}||{ngram(e)} ? {ngram(c)}| ,
where {ngram(e)} are the set of n-grams in entity
mention e and {ngram(c)} are the set of n-grams
for all entity mentions in cluster c. Note that we
can calculate the numerator (the intersection) by
looking up the n-grams of e in the hash and count-
ing matches with c. The denominator is equivalent
to the number of n-grams unique to e and to c plus
the number that are shared. The number that are
shared is the intersection. The number unique to
e is the total number of n-grams in e minus the in-
tersection. The final term, the number unique to c,
is computed by taking the total number of n-grams
in c (a single integer stored in memory) minus the
intersection.
Through this strategy, we can select only those
clusters that have the highest orthographic simi-
larity to e without requiring the cluster contents,
which may not be stored in memory. In our exper-
iments, we evaluate settings where we select all
candidates with non-zero score and a pruned set of
the top k dice score candidates. We also include
in the n-gram list known aliases to facilitate or-
thographically dissimilar, but reasonable matches
(e.g., IBM or ?Big Blue? for ?International Busi-
ness Machines, Inc.?).4
For further efficiency, we keep separate caches
for each named entity type.5 We then select the
appropriate cache based on the automatically de-
termined type of the named entity provided by the
named entity tagger, which also prevents spurious
matches of non-matching entity types.
3.3 Similarity Metric
After filtering by orthographic information to
quickly obtain a small set of candidate clusters,
a full similarity score is computed for the current
4We generated alias lists for entities from Freebase.
5Persons (PER), organizations (ORG), and locations
(LOC).
1053
entity coreference chain and each retrieved can-
didate cluster. These computations require infor-
mation about each cluster, so the cluster?s suffi-
cient statistics are loaded using the LRU cache de-
scribed above.
We define several similarity metrics between
coreference chains and clusters to deal with both
name variation and disambiguation. For name
variation, we define an orthographic similarity
metric to match similar entity mention strings. As
before, we use word unigrams and character skip
bigrams. For each of these methods, we compute
a similarity score as dice(e, c) and select the high-
est scoring cluster.
To address name disambiguation, we use two
types of context from the document. First, we use
lexical features represented as TF/IDF weighted
vectors. Second, we consider topic features, in
which each word in a document is replaced with
the topic inferred from a topic model. This yields
a distribution over topics for a given document.
We use an LDA (Blei et al, 2003) model trained
on the New York Times Annotated Corpus (Sand-
haus, 2008). We note that LDA can be computed
over streams (Yao et al, 2009).
To compare context vectors we use cosine sim-
ilarity, where the cluster vector is the average of
all document vectors assigned to the cluster. Note
that the filtering step in Section 3.2 returns only
those candidates with some orthographic similar-
ity with the coreference chain, so a similarity met-
ric that uses context only is still restricted to ortho-
graphically similar entities.
Finally, we consider a combination of ortho-
graphic and context similarity as a linear combi-
nation of the two metrics as:
score(e, c) = ? dice(e, c) + (1? ?)cosine(e, c) .
We set ? = 0.8 based on initial experiments.
4 Evaluation
We used several corpora to evaluate our meth-
ods, including two data sets commonly used in the
coreference community. We also created a new
test set using artificially conflated names. And fi-
nally to test scalability, we ran our algorithm over
a large text collection that, while it did not have
Attribute smith nytac ace08 kbp09
Total Documents 197 1.85M 10k 1.2M
Annotated Docs 197 19,360 415 **
Annotated Entities 35 200 3,943 **
Table 1: Data sets used in our experiments. For
the kbp09 data we did not have annotations.
ground truth entity clusters, was useful for com-
puting other performance statistics. Properties for
each data set are given in Table 1.
4.1 John Smith corpus
Bagga and Baldwin (1998) evaluated their disam-
biguation system on a set of 197 articles from the
New York Times that mention a person named
?John Smith?. This data exhibits no name variants
and is strictly a disambiguation task. We include
this data (smith) to allow comparison to previous
work.
4.2 NYTAC Pseudo-name corpus
To study the effects of word sense ambiguity
and disambiguation several researchers have ar-
tificially conflated dissimilar words together and
then attempted to disambiguate them (Sanderson,
2000). The obvious advantage is cheaply obtained
ground truth for disambiguation.
The same trick has also been employed in per-
son name disambiguation (Mann and Yarowsky,
2003; Gooi and Allan, 2004). We adopt the same
method on a somewhat larger scale using annota-
tions from the New York Times Annotated Corpus
(NYTAC) (Sandhaus, 2008), which annotates doc-
uments based on whether or not they mention an
entity. The NYTAC data contains documents from
20 years of the New York Times and contains rich
metadata and document-level annotations that in-
dicate when an entity is mentioned in the docu-
ment using a standard lexicon of entities. (Note
that mention strings are not tagged.) Using these
annotations we created a set of 100 pairs of con-
flated person names.
The names were selected to be medium fre-
quency (i.e., occurring in between 50 and 200 ar-
ticles) and each pair matches in gender. The first
50 pairs are for names that are topically similar,
for example, Tim Robbins and Tom Hanks (both
actors); Barbara Boxer and Olympia Snowe (both
1054
smith nytac ace08
Approach P R F P R F P R F
Baseline 1.000 0.178 0.302 1.000 0.010 0.020 1.000 0.569 0.725
ExactMatch 0.233 1.000 0.377 0.563 0.897 0.692 0.977 0.697 0.814
Ortho 0.603 0.629 0.616 0.611 0.784 0.687 0.975 0.694 0.811
BoW 0.956 0.367 0.530 0.930 0.249 0.349 0.989 0.589 0.738
Topic 0.847 0.592 0.697 0.815 0.244 0.363 0.983 0.605 0.750
Ortho+BoW 0.603 0.634 0.618 0.801 0.601 0.686 0.976 0.691 0.809
Ortho+Topic 0.603 0.634 0.618 0.800 0.591 0.680 0.975 0.704 0.819
Table 2: Best B3 performance on the smith, nytac, and ace08 test sets.
US politicians). We imagined that this would be
a more challenging subset because of presumed
lexical overlap. The second set of 50 name pairs
were arbitrarily conflated. We sub-selected the
data to ensure that no two entities in our collec-
tion co-occur in the same document and this left
us with 19,360 documents for which ground-truth
was known. In each document we rewrote the
conflated name mentions using a single gender-
neutral name; any middle initials or names were
discarded.
4.3 ACE 2008 corpus
The NIST ACE 2008 (ace08) evaluation studied
several related technologies for information ex-
traction, including named-entity recognition, re-
lation extraction, and cross-document coreference
for person names in both English and Arabic. Ap-
proximately 10,000 documents from several gen-
res (predominantly newswire) were given to par-
ticipants, who were expected to cluster person and
organization entities across the entire collection.
However, only a selected set of about 400 docu-
ments were annotated and used to evaluate sys-
tem performance. Baron and Freedman (2008)
describe their work in this evaluation, which in-
cluded a separate task for within-document coref-
erence.
4.4 TAC-KBP 2009 corpus
The NIST TAC 2009 Knowledge Base Popula-
tion track (kbp09) (McNamee and Dang, 2009)
conducted an evaluation of a system?s ability to
link entity mentions to corresponding Wikipedia-
derived knowledge base nodes. The TAC-KBP
task focused on ambiguous person, organization,
and geo-political entities mentioned in newswire,
and required systems to cope with name variation
(e.g., ?Osama Bin Laden? / ?Usama Bin Laden?
or ?Mark Twain? / ?Samuel Clemens?) as well as
name disambiguation. Furthermore, the task re-
quired detection of when no appropriate KB entry
exists, which is a departure from the conventional
disambiguation problem. The collection contains
over 1.2 million documents, primarily newswire.
Wikipedia was used as a surrogate knowledge
base, and it has been used in several previous stud-
ies (e.g., Cucerzan (2007)). This task is closely re-
lated to CDCR, as mentions that are aligned to the
same knowledge base entry create a coreference
cluster. However, there are no actual CDCR anno-
tations for this corpus, though we used it nonethe-
les as a benchmark corpus to evaluate speed and
to demonstrate scalability.
5 Discussion
5.1 Accuracy
In Table 2 we report cross document coreference
resolution performance for a variety of experi-
mental conditions using the B3 method, which
includes precision, recall, and calculated F?=1
values. For each of the three evaluation corpora
(smith, nytac, and ace08) we report values for two
baseline methods and for similarity metrics us-
ing different types of features. The first baseline,
called Baseline, places each coreference chain in
its own cluster while the second baseline, called
ExactMatch, merges all mentions that match ex-
actly orthographically into the same cluster.
Use of name similarity scores as features (in ad-
dition to their use for candidate cluster selection)
is indicated by rows labeled Ortho. Use of lexi-
cal features is indicated by BoW and use of topic
model features by Topic.
Using topic models as features was more help-
ful than lexical contexts on the smith corpus.
1055
#coref chains Baseline ExactMatch Ortho BOW Topics Ortho+BOW Ortho+Topics
1K
10K
20K
30K
40K
50K
60K
70K
80K
90K
100K
110K
120K
130K
140K
1000 702 699 925 857 699 697
10000 4563 4530 7964 7956 4514 4518
20000 8234 8202 15691 15073 8159 8163
30000 11745 11682 23138 21878 11608 11611
40000 15041 14964 30900 28500 14869 14863
50000 18110 18016 38248 34758 17910 17903
60000 20450 20377 44735 40081 20241 20228
70000 22845 22780 51190 45722 22615 22603
80000 25062 25026 57440 51104 24832 24818
90000 27389 27358 64140 56581 27145 27126
100000 29797 29782 71034 62228 29546 29511
110000 32147 32139 77705 67853 31882 31840
120000 34567 34589 84309 73397 34284 34235
130000 36817 36874 90465 78676 36543 36486
140000 38826 38901 96225 83525 38539 38482
0
37500
75000
112500
150000
1K 20K 40K 60K 80K 100K 120K 140K
#
 
o
f
 
c
l
u
s
t
e
r
s
 
p
r
o
d
u
c
e
d
# of entity chains seen
Baseline ExactMatch
Ortho BOW
Topics Ortho+BOW
Ortho+Topics
Figure 2: Number of clusters produced vs. num-
ber of entity chains observed in the stream. Num-
ber of entity chains is proportional to the number
of documents.
When used alone topic beats BoW, but in com-
bination with the ortho features performance is
equivalent. For both nytac and ace08 heavy re-
liance on orthographic similarity proved hard to
beat. On the ace08 corpus Baron and Freedman
(2008) report B3 F-scores of 83.2 for persons and
67.8 for organizations, and our streaming results
appear to be comparable to their offline method.
The cluster growth induced by the various mea-
sures can be seen in Figure 2. The two base-
line methods, Baseline and ExactMatch, provide
bounds on the cluster growth with all other meth-
ods falling in between.
5.2 Hashing Strategies for Candidate
Selection
Table 3 containsB3 F-scores when different hash-
ing strategies are employed for candidate selec-
tion. The trend appears to be that stricter match-
ing outperforms fuzzier matching; full mentions
tended to beat words, which beat use of the char-
acter bigrams. This agrees with the results de-
scribed in the previous section, which show heavy
reliance on orthographic similarity.
5.3 Timing Results
Figure 3 shows how processing time increases
with the number of entities observed in the ace08
#chains 1 5 10 20
1000 2 0 0 1
2000 2 0 0 1
3000 2 0 0 1
4000 2 0 0 1
5000 2 2 0 4
6000 2 2 0 4
7000 2 2 0 4
8000 2 2 0 4
9000 2 2 0 4
10000 2 2 0 4
11000 2 2 0 4
12000 2 2 0 5
13000 2 2 0 6
14000 2 2 0 6
15000 2 2 0 6
16000 2 2 0 6
17000 2 2 0 6
18000 2 2 0 6
19000 2 2 1 7
20000 2 2 1 7
21000 2 2 2 7
22000 2 2 2 7
23000 2 2 3 7
24000 2 2 3 7
25000 2 2 3 7
26000 2 2 3 7
27000 2 2 4 8
28000 2 2 4 9
29000 2 2 5 10
30000 2 2 8 11
31000 2 2 8 12
32000 2 2 8 13
33000 2 2 8 14
34000 2 2 8 15
35000 2 2 8 16
36000 2 2 8 17
37000 2 2 8 18
38000 2 2 9 19
39000 2 2 9 20
40000 2 2 9 21
41000 2 2 10 22
42000 2 2 10 23
43000 2 2 11 24
44000 2 2 12 25
45000 2 2 12 26
46000 2 2 13 27
47000 2 3 14 28
48000 2 3 15 29
49000 2 4 16 30
50000 2 5 17 32
51000 2 6 18 34
52000 2 7 19 36
53000 2 7 20 38
54000 2 7 21 40
55000 2 8 22 41
56000 2 8 23 43
57000 2 9 24 45
58000 2 10 25 47
59000 2 10 26 48
60000 2 11 27 50
61000 2 12 28 52
62000 2 13 29 54
63000 2 13 30 56
64000 2 14 31 58
65000 2 15 32 60
66000 3 16 33 62
67000 3 17 34 63
68000 3 18 35 65
69000 3 19 36 67
70000 3 19 37 68
71000 4 20 38 70
72000 4 21 39 72
73000 4 22 40 73
74000 5 23 41 75
75000 6 24 42 77
76000 6 25 43 79
77000 6 26 44 81
78000 6 27 45 83
79000 7 28 46 85
80000 7 29 47 87
81000 8 30 48 89
82000 8 31 49 91
83000 9 32 50 93
84000 9 33 51 95
85000 9 34 52 97
86000 9 35 53 99
87000 10 36 54 101
88000 10 37 55 102
89000 10 38 56 104
90000 11 39 57 106
91000 13 40 58 108
92000 14 41 59 111
93000 15 42 60 113
94000 15 43 61 115
95000 16 44 62 117
96000 17 45 63 119
97000 17 46 64 121
98000 18 47 65 123
99000 19 48 66 125
100000 20 49 67 127
101000 21 50 68 129
102000 21 51 69 131
103000 22 52 70 133
104000 23 53 71 136
105000 24 54 72 138
106000 25 55 73 140
107000 25 56 74 142
108000 26 57 75 144
109000 27 58 76 146
110000 28 59 77 148
111000 28 60 78 150
112000 29 61 79 153
113000 30 62 80 156
114000 31 63 81 158
115000 32 64 83 161
116000 33 66 84 163
117000 34 67 85 165
118000 35 68 86 167
119000 36 69 87 169
120000 37 70 88 171
121000 38 71 90 174
122000 39 72 92 177
123000 40 73 93 179
124000 41 74 94 181
125000 42 75 95 183
126000 43 76 96 186
127000 44 77 99 188
128000 45 78 100 191
129000 46 79 101 196
130000 47 80 102 198
131000 48 81 103 201
132000 49 82 104 204
133000 50 83 105 207
134000 51 84 106 210
135000 52 85 107 214
136000 53 86 109 217
137000 54 87 110 219
138000 55 89 112 222
139000 56 90 113 225
140000 57 91 115 228
141000 58 92 117 231
142000 59 93 118 234
143000 62 94 120 237
143442 62 94 121 238
1
10
100
1000
1000 25000 49000 73000 97000 121000
T
i
m
e
 
(
s
e
c
s
)
# of chains processed
1 5 10 20
Figure 3: Elapsed processing time as a function of
bounding the number of candidate clusters consid-
ered for an entity. When fewer candidates are con-
sidered, clustering decisions can be made much
faster.
document stream. We experimented with using an
upper bound on the number of candidate clusters
to consider for an entity.
Figure 4 compares the efficiency of using three
different methods for candidate cluster identifica-
tion. The most restrictive hashing strategy, using
exact mention strings, is the most efficient, fol-
lowed by the use of words, then the use of charac-
ter skip bigrams. This makes intuitive sense ? the
strictest matching reduces the number of candi-
date clusters that have to be considered when pro-
cessing an entity.6
The ace08 corpus contained over 10,000 doc-
uments and is one of the largest CDCR test sets.
In Figure 5 we show how processing time grows
when processing the kbp09 corpus. Doubling the
number of entities processed increases the runtime
by about a factor of 5. The curve is not linear
due to the increasing number of entity cluster?s
that must be considered. Future work will exam-
ine how to keep the number of clusters considered
constant over time, such as ignoring older entities.
6 Conclusion
We have presented a new streaming cross doc-
ument coreference resolution system. Our ap-
proach is substantially faster than previous sys-
6In the limit, if names were unique, hashing on strings
would completely solve the CDCR problem and processing
an entity would be O(1)
1056
smith nytac ace08
Approach bigrams words mention bigrams words mention bigrams words mention
Ortho 0.382 0.553 0.616 0.120 0.695 0.687 0.540 0.797 0.811
BoW 0.480 0.530 0.467 0.344 0.339 0.349 0.551 0.700 0.738
Topic 0.697 0.661 0.579 0.071 0.620 0.363 0.544 0.685 0.750
Ortho+BoW 0.389 0.554 0.618 0.340 0.691 0.686 0.519 0.783 0.809
Ortho+Topic 0.398 0.555 0.618 0.120 0.477 0.680 0.520 0.776 0.819
Table 3: B3 F-scores using different hashing strategies for candidate selection. Name/cluster similarity
could be based on character skip bigrams, words appear in names, or exact matching of mention.
#chains mention string word bigram
1000 2 1 1
2000 2 2 5
3000 2 2 6
4000 2 4 7
5000 3 5 9
6000 4 6 11
7000 5 6 13
8000 5 7 15
9000 6 7 17
10000 6 7 19
11000 7 9 21
12000 7 10 23
13000 7 11 25
14000 9 13 27
15000 9 14 29
16000 10 15 31
17000 10 16 33
18000 11 17 36
19000 12 18 39
20000 12 19 42
21000 13 21 44
22000 13 23 47
23000 13 24 50
24000 14 26 53
25000 15 28 56
26000 16 30 60
27000 16 32 64
28000 17 34 68
29000 17 36 71
30000 18 39 75
31000 19 40 79
32000 21 43 83
33000 22 45 88
34000 23 47 92
35000 24 49 96
36000 26 51 100
37000 26 53 104
38000 27 55 108
39000 27 57 112
40000 28 59 117
41000 29 63 121
42000 30 66 125
43000 31 70 129
44000 33 73 133
45000 34 77 137
46000 35 80 142
47000 36 84 146
48000 36 87 150
49000 38 90 154
50000 39 94 158
51000 40 98 162
52000 41 101 167
53000 42 104 171
54000 44 107 175
55000 45 111 179
56000 46 115 183
57000 48 119 187
58000 49 122 192
59000 49 126 196
60000 50 130 200
61000 51 134 204
62000 52 139 208
63000 53 143 212
64000 56 146 217
65000 57 150 222
66000 59 155 227
67000 60 158 232
68000 62 163 237
69000 62 167 242
70000 63 170 247
71000 65 173 252
72000 66 178 257
73000 67 183 262
74000 67 186 267
75000 68 191 273
76000 69 194 277
77000 70 198 282
78000 70 202 287
79000 71 207 292
80000 72 211 297
81000 73 215 302
82000 73 219 307
83000 74 224 312
84000 74 229 317
85000 75 233 322
86000 76 238 328
87000 77 243 333
88000 77 246 338
89000 78 250 343
90000 78 254 348
91000 79 258 353
92000 80 262 358
93000 80 268 363
94000 81 273 368
95000 82 277 373
96000 82 281 378
97000 83 284 384
98000 84 288 389
99000 84 293 394
100000 85 297 400
101000 85 300 406
102000 85 303 413
103000 85 307 419
104000 85 310 425
105000 86 314 431
106000 87 319 438
107000 89 325 444
108000 90 330 450
109000 91 334 456
110000 91 339 463
111000 92 343 469
112000 92 348 476
113000 93 352 482
114000 94 357 489
115000 95 362 495
116000 95 366 501
117000 96 369 507
118000 96 373 513
119000 96 377 520
120000 97 382 526
121000 97 387 532
122000 100 392 539
123000 100 395 546
124000 101 399 552
125000 101 403 558
126000 102 408 564
127000 102 412 571
128000 103 417 577
129000 104 421 583
130000 104 425 589
131000 105 430 596
132000 106 435 602
133000 108 441 609
134000 109 446 615
135000 111 452 622
136000 112 457 628
137000 113 461 634
138000 113 467 640
139000 114 472 648
140000 115 479 655
141000 116 484 662
142000 117 489 669
143000 118 494 676
143442 118 497 679
1
10
100
1000
1000 25000 49000 73000 97000 121000
T
i
m
e
 
(
s
e
c
s
)
# of coref chains processed
mention string word bigram
Figure 4: Comparison of three hashing strategies
for identifying candidate clusters for a given en-
tity. The more restrictive strategies lead to faster
processing as fewer candidates are considered.
tems, and our experiments have demonstrated
scalability to an order of magnitude larger data
than previously published evaluations. Despite its
speed and simplicity, we still obtain competitive
results on a variety of data sets as compared with
batch systems. In future work, we plan to investi-
gate additional similarity metrics that can be com-
puted efficiently, as well as experiments on web
scale corpora.
References
Artiles, Javier, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search: results of the first evalua-
tion and the plan for the second. In World Wide Web
(WWW).
Bagga, Amit and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Conference on Computational
Linguistics (COLING).
Baron, Alex and Marjorie Freedman. 2008. Who
#coref chains processed Time (secs)
1K 1.5
100K 10
200K 40
400K 120
600K 700
900K 920
1.1M 1200
1
10
100
1000
10000
1K 100K 200K 400K 600K 900K 1.1M
T
i
m
e
 
(
s
e
c
s
)
# of coref chains processed
Figure 5: The number of coreference chains pro-
cessed over time in the kbp09 corpus. The pro-
cessing of over 1 million coreference chains is at
least an order of magnitude larger than previous
systems reported.
is Who and What is What: Experiments in cross-
document co-reference. In Empirical Methods in
Natural Language Processing (EMNLP).
Blei, D.M., A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Research (JMLR), 3:993?1022.
Can, F. and E. Ozkarahan. 1987. A dynamic clus-
ter maintenance system for information retrieval. In
Conference on Research and Development in Infor-
mation Retrieval (SIGIR).
Charikar, Moses, Chandra Chekuri, Toma?s Feder, and
Rajeev Motwani. 1997. Incremental clustering and
dynamic information retrieval. In ACM Symposium
on Theory of Computing (STOC).
Chen, Ying and James Martin. 2007. Towards ro-
bust unsupervised personal name disambiguation.
In Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Choi, Y. and C. Cardie. 2007. Structured local training
and biased potential functions for conditional ran-
dom fields with application to coreference resolu-
tion. In North American Chapter of the Association
1057
for Computational Linguistics (NAACL), pages 65?
72.
Cucerzan, Silviu. 2007. Large-scale named entity
disambiguation based on wikipedia data. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 708?716.
Firth, J.R. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis, pages 1?32.
Oxford: Philological Society.
Gooi, Chung Heong and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
North American Chapter of the Association for
Computational Linguistics (NAACL).
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Association for Computational Linguistics (ACL).
Mann, Gideon S. and David Yarowsky. 2003. Unsu-
pervised personal name disambiguation. In Confer-
ence on Natural Language Learning (CONLL).
McNamee, Paul and Hoa Dang. 2009. Overview of
the TAC 2009 knowledge base population track. In
Text Analysis Conference (TAC).
Ng, V. and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Asso-
ciation for Computational Linguistics (ACL), pages
104?111.
Nicolae, C. and G. Nicolae. 2006. Bestcut: A
graph algorithm for coreference resolution. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 275?283. Association for Compu-
tational Linguistics.
Omiecinski, Edward and Peter Scheuermann. 1984. A
global approach to record clustering and file reorga-
nization. In Conference on Research and Develop-
ment in Information Retrieval (SIGIR).
Pradhan, S.S., L. Ramshaw, R. Weischedel,
J. MacBride, and L. Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events
in ontonotes. In International Conference on
Semantic Computing (ICSC).
Ramshaw, L. and R. Weischedel. 2005. Information
extraction. In IEEE ICASSP.
Sanderson, Mark. 2000. Retrieving with good sense.
Information Retrieval, 2(1):45?65.
Sandhaus, Evan. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics.
Yao, L., D. Mimno, and A. McCallum. 2009. Effi-
cient methods for topic model inference on stream-
ing document collections. In Knowledge discovery
and data mining (KDD).
1058
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 514?518,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Typed Graph Models for Semi-Supervised Learning of Name Ethnicity
Delip Rao
Dept. of Computer Science
Johns Hopkins University
delip@cs.jhu.edu
David Yarowsky
Dept. of Computer Science
Johns Hopkins University
yarowsky@cs.jhu.edu
Abstract
This paper presents an original approach to
semi-supervised learning of personal name
ethnicity from typed graphs of morphophone-
mic features and first/last-name co-occurrence
statistics. We frame this as a general solu-
tion to an inference problem over typed graphs
where the edges represent labeled relations be-
tween features that are parameterized by the
edge types. We propose a framework for
parameter estimation on different construc-
tions of typed graphs for this problem us-
ing a gradient-free optimization method based
on grid search. Results on both in-domain
and out-of-domain data show significant gains
over 30% accuracy improvement using the
techniques presented in the paper.
1 Introduction
In the highly relational world of NLP, graphs are
a natural way to represent relations and constraints
among entities of interest. Even problems that are
not obviously graph based can be effectively and
productively encoded as a graph. Such an encoding
will often be comprised of nodes, edges that repre-
sent the relation, and weights on the edges that could
be a metric or a probability-based value, and type
information for the nodes and edges. Typed graphs
are a frequently-used formalism in natural language
problems including dependency parsing (McDonald
et al, 2005), entity disambiguation (Minkov and Co-
hen, 2007), and social networks to just mention a
few.
In this paper, we consider the problem of iden-
tifying a personal attribute such as ethnicity from
only an observed first-name/last-name pair. This has
important consequences in targeted advertising and
personalization in social networks, and in gathering
intelligence for business and government research.
We propose a parametrized typed graph framework
for this problem and perform the hidden attribute in-
ference using random walks on typed graphs. We
also propose a novel application of a gradient-free
optimization technique based on grid search for pa-
rameter estimation in typed graphs. Although, we
describe this in the context of person-attribute learn-
ing, the techniques are general enough to be applied
to various typed graph based problems.
2 Data for Person-Ethnicity Learning
Name ethnicity detection is a particularly challeng-
ing (and practical) problem in Nigeria given that
it has more than 250 ethnicities1 with minor vari-
ations. We constructed a dictionary of Nigerian
names and their associated ethnicity by crawling
baby name sites and other Nigerian diaspora web-
sites (e.g. onlinenigeria.com) to compile a name dic-
tionary of 1980 names with their ethnicity. We re-
tained the top 4 ethnicities ? Yoruba, Igbo, Efik
Ibibio, and Benin Edo2. In addition we also crawled
Facebook to identify Nigerians from different com-
munities. There are more details to this dataset that
1https://www.cia.gov/library/publications/the-world-
factbook/geos/ni.html
2Although the Hausa-Fulani is a populous community from
the north of Nigeria, we did not include it as our dictionary had
very few Hausa-Fulani names. Further, Hausa-Fulani names are
predominantly Arabic or Arabic derivatives and stand out from
the rest of the ethnic groups, making their detection easier.
514
will be made available with the data itself for future
research.
3 Random Walks on Typed Graphs
Consider a graph G = (V,E), with edge set E de-
fined on the vertices in V . A typed graph is one
where every vertex v in V has an associated type
tv ? TV . Analogously, we also use edge types
TE ? TV ? TV . Some examples of typed edges
and vertices used in this paper are shown in Table 1.
These will be elaborated further in Section 4.
Vertices POSITIONAL BIGRAM, BIGRAM,
TRIGRAM, FIRST NAME, LAST NAME, . . .
Edges POSITION (POSITIONAL BIGRAM? BIGRAM),
32BACKOFF (TRIGRAM? BIGRAM),
CONCURRENCE (FIRST NAME? LAST NAME),
. . .
Table 1: Example types for vertices and edges in the
graph for name morpho-phonemics
With every edge type te ? TE we associate a real-
valued parameter ? ? [0, 1]. Thus our graph is pa-
rameterized by a set of parameters ? with |?| =
|TE |. We will need to learn these parameters from
the training data; more on this in Section 5. We re-
lax the estimation problem by forcing the graph to
be undirected. This effectively reduces the number
of parameters by half.
We now have a weighted graph with a weight
matrix W(?). The probability transition matrix
P(?) for the random walk is derived by noting
P(?) = D(?)?1W(?) where D(?) is the diagonal
weighted-degree matrix, i.e, dii(?) =
?
j wij(?).
From this point on, we rely on standard label-
propagation based semi-supervised classification
techniques (Zhu et al, 2003; Baluja et al, 2008;
Talukdar et al, 2008) that work by spreading proba-
bility mass across the edges in the graph. While tra-
ditional label propagation methods proceed by con-
structing graphs using some kernel or arbitrary sim-
ilarity measures, our method estimates the appro-
priate weight matrix from training data using grid
search.
4 Graph construction
Our graphs have two kinds of nodes ? nodes we want
to classify ? called target nodes and feature nodes
which correspond to different feature types. Some
of the target nodes can optionally have label infor-
mation, these are called seed nodes and are excluded
from evaluation. Every feature instance has its own
node and an edge exists between a target node and
a feature node if the target node instantiates the fea-
ture. Features are not independent. For example the
trigram aba also indicates the presence of the bi-
grams ab and ba . We encode this relationship
between features by adding typed edges. For in-
stance, in the previous case, a typed edge (32BACK-
OFF) is added between the trigram aba and the bi-
gram ab representing the backoff relation. In the
absence of these edges between features, our graph
would have been bipartite. We experimented with
three kinds of graphs for this task:
First name/Last name (FN LN) graph
As a first attempt, we only considered first and last
names as features generated by a name. The name
we wish to classify is treated as a target node. There
are two typed relations 1) between the first and last
name, called CONCURRENCE, where the first and
last names occur together and 2) Where an edge,
SHARED NAME, exists between two first (last)
names if they share a last (first) name. Hence there
are only two parameters to estimate here.
Figure 1: A part of the First name/Last name graph:
Edges indicate co-occurrence or a shared name.
Character Ngram graph
The ethnicity of personal names are often indi-
cated by morphophonemic features of the individ-
ual?s given/first or family/last names. For exam-
ple, the last names Polanski, Piotrowski, Soszyn-
ski, Sikorski with the suffix ski indicate Polish de-
scent. Instead of writing suffix rules, we generate
character n-gram features from names ranging from
515
Figure 2: A part of the character n-gram graph: Ob-
serve how the suffix osun contributes to the inference
of adeosun as a Yoruba name even though it was never
seen in training. The different colors on the edges rep-
resent edge types whose weights are estimated from the
data.
bigrams to 5-grams and all orders in-between. We
further distinguish n-grams that appear in the begin-
ning (corresponding to prefixes), middle, and end
(corresponding to suffixes). Thus the last name,
mosun in the graph is connected to the follow-
ing positional trigrams mos-BEG , osu-MID ,
sun-END besides positional n-grams of other or-
ders. The positional trigram mos-BEG connected
to the position-independent trigram mos using the
typed edge POSITION. Further, the trigram mos
is connected to the bigrams mo and os using
a 32BACKOFF edge. The resulting graph has
four typed relations ? 32BACKOFF, 43BACKOFF,
45BACKOFF, and POSITION ? and four corre-
sponding parameters to be estimated.
Combined graph
Finally, we consider the union of the character n-
gram graph and the FirstName-LastName graph. Ta-
ble 2 lists some summary statistics for the various
graphs.
#Vertices #Edges Avg. degree
FN LN 22.8K 137.2K 3.6
CHAR. NGRAM 282.6K 1.2M 8.7
COMBINED 282.6K 1.3M 9.2
Table 2: Graphs for person name ethnicity classification
5 Grid Search for Parameter Estimation
The typed graph we constructed in the previous sec-
tion has as many parameters as the number of edge
types, i.e, |?| = |TE |. We further constrain the val-
ues taken by the parameters to be in the range [0, 1].
Note that there is no loss of representation in doing
so, as arbitrary real-valued weights on edges can be
normalized to the range [0, 1]. Our objective is to
find a set of values for ? that maximizes the classi-
fication accuracy. Towards that effect, we quantize
the range [0, 1] into k equally sized bins and con-
vert this to a discrete-valued optimization problem.
While this is an approximation, our experience finds
that relative values of the various ?i ? ? are more
important than the absolute values for label propa-
gation.
Figure 3: Grid search on a unit 2-simplex with k = 4.
The complexity of this search procedure is O(kn)
for k bins and n parameters. For problems with
small number of parameters, like ours (n = 4 or
n = 2 depending on the graph model), and with
fewer bins this search is still tractable although com-
putationally expensive. We set k = 4; this results
in 256 combinations to be searched at most and we
evaluate each combination in parallel on a cluster.
Clearly, this exhaustive search works only for prob-
lems with few parameters. However, grid search can
still be used in problems with large number of edge
types using one of the following two techniques: 1)
Randomly sample with replacement from a Dirichlet
distribution with same order as the number of bins.
Evaluate using parameter values from each sample
on the development set. Select the parameter values
that result in highest accuracy on the development
set from a large number of samples. 2) Perform a
516
coarse grained search first using a small k on the
range [0, 1] and use that result to shrink the search
range. Perform grid search again on this smaller
range. We simply search exhaustively given the na-
ture of our problem.
6 Experiments & Results
We evaluated our three different model variants un-
der two settings: 1) When only a weak prior from
the dictionary data is present; we call this ?out-of-
domain? since we don?t use any labels from Face-
book and 2) when both the dictionary prior and some
labels from the Facebook data is present; we call this
?in-domain?. The results are reported using 10-fold
cross-validation. In addition to the proposed typed
graph models, we show results from a smoothed-
Na??ve Bayes implementation and two standard base-
lines 1) where labels are assigned uniformly at ran-
dom (UNIFORM) and 2) where labels are assigned
according the empirical prior distribution (PRIOR).
The baseline accuracies are shown in Table 3.
Out-of-domain In-domain
UNIFORM 25.0 25.0
PRIOR 42.6 42.6
Na??ve Bayes 75.1 77.2
Table 3: Ethnicity-classification accuracy from baseline
classifiers.
We performed similar in-domain and out-of-
domain experiments for each of the graph models
proposed in Section 4 and list the results in Table 4,
without using grid search.
Out-of-domain In-domain
FN LN 57.6 60.2
CHAR. NGRAM 73.2 76.8
%gain over FN LN 27% 27.6%
COMBINED 77.1 78.7
%gain over CHAR. NGRAM 5.3% 2.5%
Table 4: Ethnicity-classification accuracy without grid
search
Some points to note about the results reported in
Table 4: 1) These results were obtained without us-
ing parameters from the grid search based optimiza-
tion. 2) The character n-gram graph model performs
better than the first-name/last-name graph model by
itself, as expected due to the smoothing induced by
the backoff edge types. 3) The combination of first-
name/last-name graph and the n-gram improves ac-
curacy by over 30%.
Table 5 reports results from using parameters es-
timated using grid search. The parameter estimation
was done on a development set that was not used
in the 10-fold cross-validation results reported in the
table. Observe that the parameters estimated via grid
search always improved performance of label prop-
agation.
Out-of-domain In-domain
FN LN 59.1 61.4
CHAR. NGRAM 76.7 78.5
COMBINED 78.6 80.1
Improvements by grid search (c.f., Table 4)
FN LN 2.6% 2%
CHAR. NGRAM 4.8% 2.2%
COMBINED 1.5% 1.7%
Table 5: Ethnicity-classification accuracy with grid
search
7 Conclusions
We considered the problem of learning a person?s
ethnicity from his/her name as an inference prob-
lem over typed graphs, where the edges represent la-
beled relations between features that are parameter-
ized by the edge types. We developed a framework
for parameter estimation on different constructions
of typed graphs for this problem using a gradient-
free optimization method based on grid search. We
also proposed alternatives to scale up grid search for
large problem instances. Our results show a sig-
nificant performance improvement over the baseline
and this performance is further improved by param-
eter estimation resulting over 30% improvement in
accuracy using the conjunction of techniques pro-
posed for the task.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi Jing,
Jay Yagnik, Shankar Kumar, Deepak Ravichandran,
and Mohamed Aly. 2008. Video suggestion and dis-
covery for youtube: taking random walks through the
view graph. In Proceeding of the 17th international
conference on World Wide Web.
Jonathan Chang, Itamar Rosenn, Lars Backstrom, and
Cameron Marlow. 2010. epluribus: Ethnicity on so-
517
cial networks. In Proceedings of the International
Conference in Weblogs and Social Media (ICWSM).
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing.
Association for Computational Linguistics.
Einat Minkov and William Cohen. 2007. Learning to
rank typed graph walks: local and global approaches.
In Proceedings of the 9th WebKDD and 1st SNA-KDD
2007 workshop on Web mining and social network
analysis, New York, NY, USA. ACM.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas?ca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In Proceedings of the Inter-
national Conference in Machine Learning, pages 912?
919.
518
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 41?48
Manchester, August 2008
Affinity Measures based on the Graph Laplacian
Delip Rao
Dept. of Computer Science
Johns Hopkins University
delip@cs.jhu.edu
David Yarowsky
Dept. of Computer Science
Johns Hopkins University
yarowsky@cs.jhu.edu
Chris Callison-Burch
Dept. of Computer Science
Johns Hopkins University
ccb@cs.jhu.edu
Abstract
Several language processing tasks can be
inherently represented by a weighted graph
where the weights are interpreted as a
measure of relatedness between two ver-
tices. Measuring similarity between ar-
bitary pairs of vertices is essential in solv-
ing several language processing problems
on these datasets. Random walk based
measures perform better than other path
based measures like shortest-path. We
evaluate several random walk measures
and propose a new measure based on com-
mute time. We use the psuedo inverse
of the Laplacian to derive estimates for
commute times in graphs. Further, we
show that this pseudo inverse based mea-
sure could be improved by discarding the
least significant eigenvectors, correspond-
ing to the noise in the graph construction
process, using singular value decomposi-
tion.
1 Introduction
Natural language data lend themselves to a graph
based representation. Words could be linked by
explicit relations as in WordNet (Fellbaum, 1989)
or documents could be linked to one another via
hyperlinks. Even in the absence of such a straight-
forward representation it is possible to derive
meaningful graphs such as the nearest neighbor
graphs as done in certain manifold learning meth-
ods (Roweis and Saul, 2000; Belkin and Niyogi,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
2001). All of these graphs share the following
properties:
? They are edge-weighted.
? The edge weight encodes some notion of re-
latedness between the vertices.
? The relation represented by edges is at least
weakly transitive. Examples of such rela-
tions include, ?is similar to?, ?is more general
than?, and so on. It is important that the re-
lations selected are transitive for the random
walk to make sense.
Such graphs present several possibilities in solv-
ing language problems on the data. One such task
is, given two vertices in the graph we would like
to know how related the two vertices are. There
is an abundance of literature on this topic, some
of which will be reviewed here. Finding similarity
between vertices in a graph could be an end in it-
self, as in the lexical similarity task, or could be a
stage before solving other problems like clustering
and classification.
2 Contributions of this paper
The major contributions of this paper are
? A comprehensive evaluation of various ran-
dom walk based measures
? Propose a new similarity measure based on
commute time.
? An improvement to the above measure by
eliminating noisy features via singular value
decomposition.
41
3 Problem setting
Consider an undirected graph G(V,E,W) with
vertices V , edges E, and W = [w
ij
] be the sym-
metric adjacency weight matrix with w
ij
as the
weight of the edge connecting vertices i and j. The
weight, w
ij
= 0 for vertices i and j that are not
neighbors and when w
ij
> 0 it is interpreted as an
indication of relatedness between i and j. In our
case, we consider uniformly weighted graphs, i.e,
w
ij
= 1 for neighbors but this need not be the case.
Let n = |V | be the order of the graph. We define
a relation sim : V ? V ? R
+
such that sim(i, j)
is the relatedness between vertices i and j. There
are several ways to define sim; the ones explored
in this paper are:
? sim
G
(i, j) is the reciprocal of the shortest
path length between vertices i and j. Note
that this is not a random walk based mea-
sure but a useful baseline for comparison pur-
poses.
? sim
B
(i, j) is the probability of a random walk
from vertex i to vertex j using all paths of
length less than m.
? sim
P
(i, j) is the probability of a random walk
from vertex i to vertex j defined via a pager-
ank model.
? sim
C
(i, j) is a function of the commute time
between vertex i and vertex j.
4 Data and Evaluation
We evaluate each of the similarity measure we
consider by using a linguistically motivated task
of finding lexical similarity. Deriving lexical
relatedness between terms has been a topic of
interest with applications in word sense disam-
biguation (Patwardhan et al, 2005), paraphras-
ing (Kauchak and Barzilay, 2006), question an-
swering (Prager et al, 2001), and machine trans-
lation (Blatz et al, 2004) to name a few. Lex-
ical relatedness between terms could be derived
either from a thesaurus like WordNet or from
raw monolingual corpora via distributional simi-
larity (Pereira et al, 1993). WordNet is an inter-
esting graph-structured thesaurus where the ver-
tices are the words and the edges represent rela-
tions between the words. For the purpose of this
work, we only consider relations like hypernymy,
hyponymy, and synonymy. The importance of this
problem has generated copious literature in the
past ? see (Pedersen et al, 2004) or (Budanitsky
and Hirst, 2006) for a detailed review of various
lexical relatedness measures on WordNet. Our fo-
cus in this paper is not to derive the best similar-
ity measure for WordNet but to use WordNet and
the lexical relatedness task as a method to evalu-
ate the various random walk based similarity mea-
sures. Following the tradition in previous litera-
ture we evaluate on the Miller and Charles (1991)
dataset. This data consists of 30 word-pairs along
with human judgements which is a real value be-
tween 1 and 4. For every measure we consider,
we derive similarity scores and compare with the
human judgements using the Spearman rank cor-
relation coefficient.
5 Graph construction
For the purpose of evaluation of the random walk
measures, we construct a graph for every pair of
words for which similarity has to be computed.
This graph is derived from WordNet as follows:
? For each word w in the pair (w
1
, w
2
):
? Add an edge between w and all of its
parts of speech. For example, if the word
is coast, add edges between coast and
coast#noun and coast#verb.
? For each word#pos combination,
add edges to all of its senses (For
example, coast#noun#1 through
coast#noun#4.
? For each word sense, add edges to all of
its hyponyms
? For each word sense, add edges to all of
its hypernyms recursively.
In this paper we consider uniform weights on all
edges as our main aim is to illustrate the differ-
ent random walk measures rather than fine tune the
graph construction process.
6 Shortest path based measure
The most obvious measure of distance in a graph is
the shortest path between the vertices which is de-
fined as the minimum number of intervening edges
between two vertices. This is also known as the
geodesic distance. To convert this distance mea-
sure to a similarity measure, we take the recipro-
cal of the shortest-path length. We refer to this as
the geodesic similarity. This is not a random walk
42
Figure 1: Shortest path distances on graphs
measure but will serve as an important baseline for
our work. As can be observed from Table 1, the
Method Spearman correlation
Geodesic 0.275
Table 1: Similarity using shortest-path measure.
correlation is rather poor for the shortest path mea-
sure.
7 Why are shortest path distances bad?
While shortest-path distances are useful in many
applications, it fails to capture the following obser-
vation. Consider the subgraph of WordNet shown
in Figure 1. The term moon is connected to the
terms religious leader and satellite
1
.
Observe that both religious leader and
satellite are at the same shortest path dis-
tance from moon. However, the connectivity
structure of the graph would suggest satellite
to be ?more? similar than religious leader
as there are multiple senses, and hence multiple
paths, connecting satellite and moon.
Thus it is desirable to have a measure that cap-
tures not only path lengths but also the connectiv-
ity structure of the graph. This notion is elegantly
captured using random walks on graphs.
7.1 Similarity via Random walks
A random walk is a stochastic process that consists
of a sequence of discrete steps taken at random de-
fined by a distribution. Random walks have inter-
esting connections to Brownian motion, heat diffu-
sion and have been used in semi-supervised learn-
ing ? for example, see (Zhu et al, 2003). Certain
properties of random walks are defined for ergodic
processes only
2
. In our work, we assume these
1
The religious leader sense of moon is due to Sun
Myung Moon, a US religious leader.
2
A stochastic process is ergodic if the underlying Markov
chain is irreducible and aperiodic. A Markov chain is irre-
hold true as the graphs we deal with are connected,
undirected, and non-bipartite.
7.1.1 Bounded length walks
As our first random walk measure, we consider
the bounded length walk ? i.e., all random walks of
length less than or equal to a bound m. We derive
a probability transition matrix P from the weight
matrix W as follows:
P = D
?1
W
where, D is a diagonal matrix with d
ii
=
?
n
j = 1
w
ij
. Observe that:
? p
ij
= P[i, j] ? 0, and
?
?
n
j = 1
p
ij
= 1
Hence p
ij
can be interpreted as the probability
of transition from vertex i to vertex j in one step. It
is easy to observe that P
k
gives the transition prob-
ability from vertex i to vertex j in k steps. This
leads to the following similarity measure:
S = P + P
2
+ P
3
+ ...+ P
m
Observe that S[i, j] derives the total probability of
transition from vertex i to vertex j in at most m
steps
3
. Given S, we can derive several measures of
similarity:
1. Bounded Walk: S[i, j]
2. Bounded Walk Cosine: dot product of
rowvectors S
i
and S
j
.
When we evaluate these measures on the Miller-
Charles data the results shown in Table 2. are ob-
served. For this experiment, we consider all walks
that are at most 20 steps long, i.e., m = 20. Ob-
serve that these results are significantly better than
the Geodesic similarity based on shortest-paths.
ducible if there exists a path between any two states and it is
aperiodic if the GCD of all cycle lengths is one.
3
The matrix S is row normalized to ensure that the entries
can be interpreted as probabilities.
43
Method Spearman correlation
Bounded Walk 0.346
Bounded Walk Cosine 0.365
Table 2: Similarity using bounded random walks
(m = 20).
7.1.2 How many paths are sufficient?
In the previous experiment, we arbitrarily fixed
m = 20. However, as observed in Figure 2. , be-
yond a certain value the choice of m does not affect
the result as the random walk converges to its sta-
tionary distribution. The choice of m depends on
Figure 2: Effect of m in Bounded walk
the amount of computation available. A reason-
ably large value of m (m > 10) should be suffi-
cient for most purposes and one could use lower
values of m to derive an approximation for this
measure. One could derive an upper bound on the
value of m using the mixing time of the underlying
Markov chain (Aldous and Fill, 2001).
7.1.3 Similarity via pagerank
Pagerank (Page et al, 1998) is the celebrated ci-
tation ranking algorithm that has been applied to
several natural language problems from summa-
rization (Erkan and Radev, 2004) to opinion min-
ing (Esuli and Sebastiani, 2007) to our task of
lexical relatedness (Hughes and Ramage, 2007).
Pagerank is yet another random walk model with a
difference that it allows the random walk to ?jump?
to its initial state with a nonzero probability (?).
Given the probability transition matrix P as defined
above, a stationary distribution vector for any ver-
tex (say i) could be derived as follows:
1. Let e
i
be a vector of all zeros with e
i
(i) = 1
2. Let v
0
= e
i
3. Repeat until ?v
t
? v
t?1
?
F
< ?
? v
t+1
= ?v
t
P + (1? ?)v
0
? t = t+ 1
4. Assign v
t+1
as the stationary distribution for
vertex i.
Armed with the stationary distribution vectors for
vertices i and j, we define pagerank similarity ei-
ther as the cosine of the stationary distribution vec-
tors or the reciprocal Jensen-Shannon (JS) diver-
gence
4
between them. Table 3. shows results on
the Miller-Charles data. We use ? = 0.1, the best
value on this data. Observe that these results are
Method Spearman correlation
Pagerank JS-Divergence 0.379
Pagerank Cosine 0.393
Table 3: Similarity via pagerank (? = 0.1).
better than the best bounded walk result. We fur-
ther note that our results are different from that
of (Hughes and Ramage, 2007) as they use exten-
sive feature engineering and weight tuning during
the graph generation process that we have not been
able to reproduce. Hence for simplicity we stuck to
a simpler graph generation process. Nevertheless,
the result in Table 3. is still useful as we are in-
terested in the performance of the various spectral
similarity measures rather than achieving the best
performance on the lexical relatedness task. The
graphs we use in all methods are identical making
comparisons across methods possible.
7.2 Similarity via Hitting Time
Given a graph with the transition probability ma-
trix P as defined above, the hitting time between
vertices i and j, denoted as h(i, j), is defined as
the expected number of steps taken by a random
walker to first encounter vertex j starting from ver-
tex i. This can be recursively defined as follows:
h(i, j) =
?
?
?
1 +
?
k : w
ik
> 0
p
ik
h(k, j) if i 6= j
0 if i = j
(1)
4
The Jensen-Shannon divergence between two distribu-
tions p and q is defined as D(p ? a)+D(q ? a), where D(. ?
.) is the Kullback-Liebler divergence and a = (p + q)/2.
Note that unlike KL-divergence this measure is symmetric.
See (Lin, 1991) for additional details.
44
The lower the hitting times of two vertices, the
more similar they are. It can be easily verified
that hitting time is not a symmetric relation hence
graph theory literature suggests another symmet-
ric measure ? the commute time.
5
The commute
time, c(i, j), is the expected number of steps taken
to leave vertex i, reach vertex j, and return back to
i. Thus,
c(i, j) = h(i, j) + h(j, i) (2)
Observe that, the commute time is a metric in that
it is positive definite, symmetric, and satisifies tri-
angle inequality. Hence, commute time could be
used as a distance measure as well. We derive a
similarity measure from this distance measure us-
ing the following lemma.
Lemma 1. For every edge (i, j), c(i, j) ? 2l
where l = |E|, the number of edges.
Proof. This can be easily observed by defining a
Markov chain on the edges with probability tran-
sition matrix Q with 2l states, such that Q
e
1
e
2
=
1/degree(e
1
? e
2
). Since this matrix is doubly
stochastic, the stationary distribution on this chain
will be uniform with a probability 1/2l. Now
c(i, j) = h(i, j)+h(j, i), is the expected time for a
walk to start at i, visit j, and return back to i. When
the stationary probability at each edge is 1/2l, this
expected time evaluates to 2l. Hence the commute
time can be at most 2l.
This lemma allows us to define a similarity mea-
sure as follows:
sim
C
(i, j) = 1?
c(i, j)
2l
(3)
Observe that the measure defined in Equation 3 is
a metric and further its range is defined in [0, 1].
We now only need a way to compute the commute
times to use Equation 3. One could compute the
hitting times and hence the commute times from
the Equations 1 and 2 using dynamic program-
ming, akin to shortest paths in graphs. In this pa-
per, we instead choose to derive commute times
via the graph Laplacian. This also allows us to
handle ?noise? in the graph construction process
which cannot be taken care by naive dynamic pro-
gramming.
5
Note that distance measures, in general, need not be sym-
metric but we interpret distance as proximity which mandates
symmetry.
Chandra et. al. (1989) show that the commute
time between two vertices is equal to the resis-
tance distance between them. Resistance distance,
as proposed by Klein and Randic (1993), is the
effective resistance between two vertices in the
electrical network represented by the graph, where
the edges have resistance 1/w
ij
. Xiao and Gut-
man (2003), show the relation between resistance
distances in graphs to the Laplacian spectrum, thus
enabling a way to derive commute times from the
graph Laplacian in closed form.
We now introduce graph Laplacians, which are
interesting in their own right besides being related
to commute time. The Laplacian of a graph could
be viewed as a discrete version of the Laplace-
Beltrami operator on Riemannian manifolds. It is
defined as
L = D ? W
The graph Laplacian has interesting properties and
a wide range of applications, in semi-supervised
learning (Zhu et al, 2003), non-linear dimension-
ality reduction (Roweis and Saul, 2000; Belkin and
Niyogi, 2001), and so on. See (Chung, 1997) for
a thorough introduction on Laplacians and their
properties. We depend on the fact that L is:
1. symmetric (since D and W are for undirected
graphs)
2. positive-semidefinite : since it is symmet-
ric, all of the eigenvalues are real and by
the Greshgorin circle theorem, the eigenval-
ues must also be non-negative and hence L is
positive-semidefinite.
Throughout this paper we use normalized Lapla-
cians as defined below:
L = D
?1/2
LD
?1/2
= I ? D
?1/2
WD
?1/2
The normalized Laplacians preserve all properties
of the Laplacian by construction.
As noted in Xiao and Gutman (2003), the re-
sistance distances can be derived from the gener-
alized Moore-Penrose pseudo-inverse of the graph
Laplacian(L
?
) ? also called the inverse Laplacian.
Like Laplacians, their pseudo inverse counterparts
are also symmetric, and positive semi-definite.
Lemma 2. L
?
is symmetric
Proof. The Moore-Penrose pseudo-inverse is de-
fined as L
?
= (L
T
L)
?1
L
T
. From this definition,
it is clear that (L
?
)
T
= (L
T
)
?
. By the symmetry
45
property of graph Laplacians, L
T
= L. Hence,
(L
?
)
T
= L
?
.
Lemma 3. L
?
is positive semi-definite
Proof. We make use of the following properties
from (Chung, 1997):
? The Laplacian, L, is positive semi-definite
(also shown above).
? If the Eigen-decomposition of L is Q?Q
T
,
then the Eigen-decomposition of the pseudo-
inverse L
?
is Q?
?1
Q
T
. If any of the eigenval-
ues of L is zero then the corresponding eigen-
value for L
?
is also zero.
Since L is positive semi-definite, and the eigen-
values of L
?
have the same sign as L, the pseudo
inverse L
?
has to be positive semi-definite.
Lemma 4. The inverse Laplacian is a gram matrix
Proof. To prove this, we use the fact that the
Laplacian Matrix is symmetric and positive semi-
definite. Hence by Cholesky decomposition we
can write L = UU
T
.
Therefore L
?
= (U
T
)
?
U
?
= (U
?
)
T
(U
?
).
Hence L
?
is a matrix of dot-products or a gram-
matrix.
Thus, from Lemmas 2, 3 and 4, the inverse
Laplacian L
?
is a valid Kernel.
7.2.1 Similarity measures from the Laplacian
The pseudo inverse of the Laplacian allows us
to compute the following similarity measures.
1. Since L
?
is a kernel, L
?
ij
can be interpreted a
similarity value of vertices i and j.
2. Commute time: This is due to (Aldous and
Fill, 2001). The commute time, c(i, j) ?
(L
?
ii
+ L
?
jj
? 2L
?
ij
). This allows us to derive
similarities using Equation 3.
Evaluating the above measures with the Miller-
Charles data yields results shown in Table 4.
Again, these results are better than the other ran-
dom walk methods compared in the paper.
Method Spearman correlation
L
?
ij
0.469
Commute Time (sim
C
) 0.520
Table 4: Similarity via inverse Laplacian.
7.2.2 Noise in the graph construction process
The graph construction process outlined in Sec-
tion 5 is not necessarily the best one. In fact, any
method that constructs graphs from existing data
incorporates ?noise? or extraneous features. These
could be spurious edges between vertices, miss-
ing edges, or even improper edge weights. It is
however impossible to know any of this a priori
and some noise is inevitable. The derivation of
commute times via the pseudo inverse of a noisy
Laplacian matrix makes it even worse because the
pseudo inverse amplifies the noise in the original
matrix. This is because the largest singular value
of the pseudo inverse of a matrix is equal to the in-
verse of the smallest singular value of the original
matrix. A standard technique in signal processing
and information retrieval to eliminate noise or han-
dle missing values is to use singular value decom-
position (Deerwester et al, 1990). We apply SVD
to handle noise in the graph construction process.
For a given matrix A, SVD decomposes A into
three matrices U, S, and V such that A = USV
T
,
where S is a diagonal matrix of eigenvalues of A,
and U and V are orthonormal matrices containing
the left and the right eigenvectors respectively. The
top-k eigenvectors and eigenvalues are computed
using the iterative method by Lanczos-Arnoldi (us-
ing LAPACK) and the product of these matrices
represents a ?smoothed? version of the original
Laplacian. The pseudo inverse is then computed
on this smooth Laplacian. Table 5., shows the im-
provements obtained by discarding bottom 20% of
the eigenvalues.
Method Original After SVD
L
?
ij
0.469 0.472
Commute Time (sim
C
) 0.520 0.542
Table 5: Denoising graph Laplacian via SVD
Figure 3. shows the dependence on the num-
ber of eigenvalues selected. As can be observed in
both curves there is a reduction in performance by
adding the last few eigenvectors and hence may be
safely discarded. This observation is true in other
text processing tasks like document clustering or
classification using Latent Semantic Indexing.
8 Related Work
Apart from the related work cited throughout this
paper, we would also like to note the paper by Yen
46
Figure 3: Noise reduction via SVD.
et al(2007) on using sigmoid commute time kernel
on a graph for document clustering but our work
differs in that our goal was to study various ran-
dom walk measures rather than a specific task and
we provide a new similarity measure (ref. Eqn
3) based on an upper bound on the commute time
(Lemma 1). Our work also suggests a way to han-
dle noise in the graph construction process.
9 Conclusions and Future Work
This paper presented an evaluation of random
walk based similarity measures on weighted undi-
rected graphs. We provided an intuitive explana-
tion of why random walk based measures perform
better than shortest-path or geodesic measures,
and backed it with empirical evidence. The ran-
dom walk measures we consider include bounded
length walks, pagerank based measures, and a new
measure based on the commute times in graphs.
We derived the commute times via pseudo inverse
of the graph Laplacian. This enables a new method
of graph similarity using SVD that is robust to the
noise in the graph construction process. Further,
the inverse Laplacian is also interesting in that it is
a kernel by itself and could be used for other tasks
like word clustering, for example.
Acknowledgements
The authors would like to thank David Smith and
Petros Drineas for useful discussions and to Fan
Chung for the wonderful book on Spectral Graph
theory.
References
Aldous and Fill. 2001. Reversible Markov Chains and
Random Walks on Graphs. In preparation.
Belkin, Mikhail and Partha Niyogi. 2001. Laplacian
eigenmaps and spectral techniques for embedding
and clustering. In Proceedings of the NIPS.
Blatz, John, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto San-
chis, and Nicola Ueffing. 2004. Confidence estima-
tion for machine translation. In Proceeding of the
COLING.
Budanitsky, Alexander and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Chandra, Ashok, Prabhakar Raghavan, Walter Ruzzo,
Roman Smolensky, and Prasoon Tiwari. 1989. The
electrical resistance of a graph captures its commute
and cover times. In Proceedings of the STOC.
Chung, Fan. 1997. Spectral graph theory. In CBMS:
Conference Board of the Mathematical Sciences, Re-
gional Conference Series.
Deerwester, Scott, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41.
Erkan, G?unes and Dragomir Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence Re-
search (JAIR), 22:457?479.
Esuli, Andrea and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of the ACL, pages 424?431.
Fellbaum, Christaine, editor. 1989. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Hughes, Thad and Daniel Ramage. 2007. Lexical
semantic relatedness with random graph walks. In
Proceedings of the EMNLP.
Kauchak, David and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
HLT-NAACL.
Klein, D. and M. Randic. 1993. Resistance distance.
Journal of Mathematical Chemistry, 12:81?95.
Lin, Jianhua. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1).
Miller, G. and W. Charles. 1991. Contextual correlates
of semantic similarity. In Language and Cognitive
Process.
Page, Larry, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1998. The pagerank citation ranking:
Bringing order to the web. Technical report, Stan-
ford University, Stanford, CA.
47
Patwardhan, Siddharth, Satanjeev Banerjee, and Ted
Pedersen. 2005. Senserelate:: Targetword-A gen-
eralized framework for word sense disambiguation.
In Proceedings of the ACL.
Pedersen, Ted, Siddharth Patwardhan, and Jason
Michelizzi. 2004. Wordnet::similarity - measuring
the relatedness of concepts. In Proceedings of the
AAAI.
Pereira, Fernando, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the ACL.
Prager, John M., Jennifer Chu-Carroll, and Krzysztof
Czuba. 2001. Use of wordnet hypernyms for an-
swering what-is questions. In Proceedings of the
Text REtrieval Conference.
Roweis, Sam and Lawrence Saul. 2000. Nonlinear di-
mensionality reduction by locally linear embedding.
Science, 290:2323?2326.
Xiao, W. and I. Gutman. 2003. Resistance distance and
laplacian spectrum. Theoretical Chemistry Associa-
tion, 110:284?289.
Yen, Luh, Francois Fouss, Christine Decaestecker, Pas-
cal Francq, and Marco Saerens. 2007. Graph nodes
clustering based on the commute-time kernel. In
Proceedings of the PAKDD.
Zhu, Xiaojin, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the
ICML.
48
