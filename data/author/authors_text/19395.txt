Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1886?1896, Dublin, Ireland, August 23-29 2014.
Learning the Taxonomy of Function Words for Parsing
Dongchen Li, Xiantao Zhang, Dingsheng Luo and Xihong Wu
Key Laboratory of Machine Perception and Intelligence
Speech and Hearing Research Center
Peking University, Beijing, China
{lidc,zhangxt,dsluo,wxh}@cis.pku.edu.cn
Abstract
Completely data-driven grammar training is prone to over-fitting. Human-defined word class
knowledge is useful to address this issue. However, the manual word class taxonomy may be
unreliable and irrational for statistical natural language processing, aside from its insufficient
linguistic phenomena coverage and domain adaptivity. In this paper, a formalized representation
of function word subcategorization is developed for parsing in an automatic manner. The function
word classification representing intrinsic features of syntactic usages is used to supervise the
grammar induction, and the structure of the taxonomy is learned simultaneously. The grammar
learning process is no longer a unilaterally supervised training by hierarchical knowledge, but
an interactive process between the knowledge structure learning and the grammar training. The
established taxonomy implies the stochastic significance of the diversified syntactic features.
The experiments on both Penn Chinese Treebank and Tsinghua Treebank show that the proposed
method improves parsing performance by 1.6% and 7.6% respectively over the baseline.
1 Introduction
Probabilistic context-free grammar (PCFG) is widely used in the fields of speech recognition, machine
translation, information retrieval, etc. It takes the empirical rules and probabilities from a Treebank.
However, due to the context-free assumption, PCFG does not always perform well (Klein and Man-
ning, 2003). For instance, it assumes adverbs, including temporal adverbs, degree adverbs and negation
adverbs, to share the same distribution, whereas the distinction would provide useful indication for dis-
ambiguating the syntactic structure of the context.
It arose that the manual word classification in linguistic research was used to enrich PCFG and im-
prove the performance. However, from the point of view of statistical natural language processing, there
are some drawbacks for manual classification. Firstly, Linguistic phenomena covered by the manual
refinement may be limited by the linguistic observations of human. Secondly, the evidence of manual
refinement is often based on a particular corpus or specific sources of knowledge acquisition. As a result,
its adaptivity to different domains or genres may be insufficient. As for function words, due to the ambi-
guity and complexity in syntactic grammar, it is more difficult to develop formalized representation than
for content words. There are diversified standards for grammar refinement. Consequently, the word clas-
sification or category refinement can be conducted in distinct manners, while each of them is reasonable
in some sense. A delicate hierarchical classification inevitably involves in multiple dividing standards.
However, the word sets under distinct dividing standards may be overlapping. The problems come up
that how to choose the set of the multiple standards to cooperate to build the taxonomy, and how to de-
cide the priority of each standard. Regarding that the manual method is hard to overcome critical issues,
manual taxonomy for function words may not be reliable for statistical natural language processing.
This article attempts to address these issues in a data-driven manner. we first manually construct a
cursory and flat classification of function words. A hierarchical split-merge approach is employed to
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1886
introduce our classification, and the PCFG training procedure is supervised to alleviate the over-fitting
issue. The priorities of the subcategorization standards are determined by the measurement of effec-
tiveness for parsing in a greedy manner in the hierarchical classification. And the hierarchical structure
of the classification is learned by data-driven approach in the course of grammar induction, so as to fit
the practical usages in the Treebank. Accordingly, the grammar learning process is no longer a unilat-
erally supervised training by hierarchical knowledge, but an interactive process between the knowledge
representation induction and the grammar training. That is, the grammar induction is supervised by the
knowledge and the structure of the taxonomy is learned simultaneously. These two processes are iterated
for several rounds and the hierarchical structure of the function word taxonomy is constructed. In each
round, the induced grammar could benefit from the optimized taxonomy during the learning process. The
category split in the early rounds take more priorities than in the late ones. Thus, the learned taxonomy
implies the stochastic significance of the series of the syntactic features.
Experiments on Penn Chinese Treebank Fifth Edition (CTB5.0) (Xue et al., 2002) and Tsinghua Chi-
nese Treebank (TCT) (Zhou, 2004) are performed. The results show that the induced grammars with
refined conjunction categories gain parsing performance improvement by 1.6% on CTB and by 7.6% on
TCT. During the training process, a taxonomy of function words is learned, which reflects their practical
usages in the corpus.
The rest of this paper is organized as follows. We first review related work on category refinement
for parsing. Then we describe our manually defined categories of function words in Section 3. The
hierarchical state-split approach for introducing the the categories are presented in Section 4, and our
taxonomy learning method is described in Section 5. In Section 7, experimental comparison is conducted
among various methods on granularity choosing. And conclusions of this research are drawn in last
section.
2 Related Work
A variety of techniques have been proposed to enrich PCFG in either manual (Klein and Manning, 2003;
Zhang and Clark, 2011) or automatic (Petrov, 2009; Cohen et al., 2012) manner.
2.1 Automatic Refinement of Function Words for Parsing
One way of grammar refinement is data-driven state-split methods (Matsuzaki et al., 2005; Prescher,
2005). The part-of-speech and syntactic tags in the grammar are automatically split to encode the kinds
of linguistic distinctions exhibited in the Treebank. The hierarchical state-split approach (Petrov et al.,
2006) started from a bare-bones Treebank derived grammar, and iteratively refined it in a split-merge-
smooth cycle with the EM-based parameter re-estimation. It achieved state of the art accuracies for many
languages including English, Chinese and German.
One tag is usually heterogeneous, in the sense that its word set can be of multiple different types.
Nevertheless, the automatic process tries to split the tags through a greedy data-driven manner, where
multiple distinctive information is used simultaneously when dividing tags. Thus the refined tags are
not intuitively interpretable. Meanwhile, considering that the EM algorithm usually gets stuck at a sub-
optimal configuration, this data-driven method suffers from the risk of over-fitting. As shown in their
experiments, there is little to be gained from splitting the closed part-of-speech classes (e.g. DT, CC, IN)
or the nonterminal ADJP.
To alleviate the risk of over-fitting, we employ the human-defined knowledge to constrain the splitting
process in this research. Based on the state-split model, our approach aims to reach a compromise
between manual and automatic refinement approaches.
2.2 Manual Refinement of Function Words for Parsing
The other way to refine the annotation for training a parser is incorporating knowledge base. Semantic
knowledge of content words has been proved to be effective in alleviate the data sparsity. Some re-
searches utilized semantic knowledge in WordNet (Miller, 1995; Fellbaum, 1999) for English parsing
(Fujita et al., 2010; Agirre et al., 2008), and Xiong et al. (2005; Lin et al. (2009) improved Chinese pars-
1887
ing by incorporating semantic knowledge in HowNet (Dong and Dong, 2003; Dong and Dong, 2006).
While WordNet and Hownet contain word classification for content words, Li et al. (2014b; Li et al.
(2014a) have focused on exploiting manual classification for conjunction in parsing.
Klein and Manning (2003) examined the annotation in Penn English Treebank, manually split the ma-
jority of the part-of-speech (POS) tags. For the function words, they split the tag ?IN? into subordinating
conjunctions, complementizers and prepositions, and appended
?
BE to all forms of ?be? and
?
HAVE to
all forms of ?have?. Conjunction tags are also marked to indicate whether they were ?But?, ?but? or
?&?. The experimental results showed that the split tags of function words surprisingly make much con-
tribution to the overall improved parsing accuracy. Levy and Manning (2003) transferred this work to
Penn Chinese Treebank. They found that, in some cases, certain adverbs such as ?however (,)? and
?especially (c??)? preferred IP modification and could help disambiguate IP coordination from VP
coordination. To capture this point, they marked those adverbs possessing an IP grandparent. However,
these manual refinement methods seems to split the tags in a rough way, which might account for a mod-
est accuracy achieved. Some existing work used heuristic rules to simply split the tags of function words
(Klein and Manning, 2003; Levy and Manning, 2003). They demonstrated that many function words
stood out to be helpful in predicting the syntactic structure and syntactic label.
3 Manual Tabular Subcategories of Function Words
When subcategorizing function words, in this section, we manually list various grammatical distinctions
that are commonly made in traditional and generative grammar in a fairly flat taxonomy. The grammar
training procedure learns by using our manual taxonomy as a starting point, and constructs a reasonable
and subtle hierarchical strucutre based on the distribution of function words usages in the corpus.
Based on some existing knowledge base (Xia, 2000; Xue et al., 2000; Zhu et al., 1995; Wang and
Yu, 2003) and previous research work (Li et al., 2014b), we investigate and summarize the usage of
function words, and come up with a hierarchical subcategories. The taxonomy of the function words is
represented in a tree structure, where each subcategory of a function word corresponds to a node in the
taxonomy, the nonterminals are subcategories and the terminals are words.
For the convenience and consistence, our manual classification just gives a rough and broad taxonomy.
It is labor-intensive and error-prone of classifying the function words manually to produce a consistent
output. Fine-grained hierarchical structure is not obligatory, but would be harmful if inappropriately clas-
sified, as it may mislead the learning process. To avoid this kind of risk, the elaboration is saved, rather
than introducing unnecessary bias. The learning process would perform the hierarchical classification
according to the distribution in the corpus.
For instance, the distinction within conjunctions is intricate. Conjunctions are the words that are
called ?connective words? in traditional Chinese grammar books. In Penn Chinese Treebank, they are
tagged as coordinating conjunctions (CC), subordinating conjunctions (CS), or adverbs (AD) according
to their syntactic distribution. CC conjoins two equivalent constituents (noun phrases, clauses, etc.),
each of which has approximately the same function as the whole construction. CS precedes a subordi-
nating clause, whereas conjunctive adverbs often appear in the main clause and pair with a subordinating
conjunction (e.g., if (XJ)/CS ... then (?)/AD). However, in Chinese, it is often hard to tell the sub-
ordinating clause from the main clause in the compound statement. As a result, in the prospective of
linguistic computing, the confusion is that, CS and conjunctive adverbs both precedes the subordinating
clauses or main clauses, while CC connects two phrases or precedes the main clause. In our scheme,
we simply conflates the CC, CS and conjunctive adverbs together. This result in a general ?conjunction?
category, within which we just enumerate all the possible uses of the conjunctions. As a result, the struc-
ture of our human-defined taxonomy is fairly flat, as briefly shown in Figure 1 and Figure 2. Our scheme
releases our hands from the confusing situations, by leaving them to our data-driven method described in
the following section. Figure 1 and Figure 2 abbreviate the manual classification and their corresponding
examples.
Many prepositions in Chinese are evolved from verbs, thus the linguistic characteristics of preposi-
tions are somewhat similar to verbs. Therefore, this paper divides the preposition word set according to
1888
Subordinating Conjunction
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Coordination: bothQ
Progression: not only?=
Transition: although?,
Preference: rather than??
Cause: becausedu
Condition
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Assumption: ifXJ
Universalization: whatever??
Unnecessary Condition: sinceQ,
Insufficient Condition: although=?
Sufficient Condition: as long as??
Necessary Condition: only if?k
Equality: unless??
... ...
Figure 1: Abbreviated Hierarchical subcategories of subordinating conjunctions with examples.
Adverb
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Conjunctive Adverb
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Transition
?
?
?
?
?
?
?
Preference: would rather?X
In case: lest??
Otherwise: or else?K
However: but%
Result
?
?
?
Therefore: so??
Then, As a result: as soon?
So that: so that?B
Progression
?
?
?
?
?
?
?
Furthermore: but also?
In addition: moreover,	
Later: subsequently? 
As well: likewise?
...
Adjunct Adverb
?
?
?
Frequency Adverbs: for many times?g
Degree Adverbs: very4?
...
...
Figure 2: Abbreviated Hierarchical subcategories of adverbs with examples.
the types of their associated arguments: ?benefactive?, such as ??(for)? and ??(to)?, marks the ben-
eficiary of an action; ?locative?, such as ?3(in)?, marks adverbials that indicate the place of the event;
?direction?, such as ? ?(towards)? and ? d(from)?, marks adverbials that answer the questions ?from
where?? and ?to where??; ?temporal?, such as ?3(on)?, marks temporal or aspectual adverbials that
answer the question ?when??, and so on.
4 Refining Grammar with Hierarchical Category Refinement
In this section, we choose the appropriate granularity in a data-driven manner based on the split-merge
learning method in Section 2.1. Our approach first initializes the categories with the most general sub-
categories in the taxonomy and then splits the categories through the hypernym-hyponym relation in the
1889
taxonomy. Data-driven method is used to merge the overly refined subcategories.
The top category in the taxonomy is used as the starting annotations of POS tags. As we cannot
predict which layer should be the most adequate one, we try to avoid applying any priori restriction on
the refinement granularity, and start with the most general tags.
With the hierarchical knowledge, it turns out to be a critical issue that which granularity should be
used to refine the tags for parsing. We intend to take neither too coarse subcategories nor too fine ones in
the hierarchical knowledge for parsing. Instead, it would be our advantage to split the tags with the very
granularity where needed, rather than splitting them all to one specific granularity in the taxonomy.
For example, ?Conjunctive Adverbs? are divided into three subcategories in our taxonomy as shown
in Figure 2. The evidence for the refinement may occur in very rare case, and certainly some of the
context of the different subcategories are quite the same. Splitting symbols with the same context is
not only unnecessary, but potentially harmful, since it unreasonably fragments observations of other
symbols.behavior.
In this paper, the hierarchical subcategory knowledge is used to refine grammars by supervising the
automatic hierarchical state-split approach. In the split stage in each cycle, the function word subcategory
is split along the hierarchy of the knowledge, instead of being randomly split and classified automatically.
In this way, we try to alleviate the over-fitting of the greedy data-driven approach, and a new set of
knowledge-related tags are generated. In the following step, we retreat some of the split subcategories to
their more general layer according to its likelihood loss of merging them. In this way, we try to avoid the
excessive refinement in our hierarchical knowledge without sufficient data support.
There are two issues that we have to consider in this process: a) how to deal with the polysemous
words, and b) how to deal with the multi-branch situation other than binary branch in the taxonomy.
Regarding to the polysemous words, they occur mostly in two situation for function words. Some are the
polysemous words which can be taken as conjunctions or auxiliary words, while the others can be taken
as preposition or adverbs. Fortunately there is no ambiguity for a word given its POS tag, so we could
neglect this situation in the split process when training. We demonstrated the solution for the multiple
branches in the Section 5.
5 Learning the Taxonomy of Function Words
There are multiple subcategorization criterions for building function word taxonomy, and it is diffi-
culty for human to rank the ordering in the classification process. This section represents the method
of learning the taxonomy of the function words in data-driven manner. Based on the manual tabular
classification, the similar word classes are conflated to express the data distribution.
The multiple branches in the taxonomy are intractable for the original split-merge method, because it
splits every category into two and merges half of them for efficiency. If we follow this scheme in our
training process, it would be difficult to deal with the multi-branch situation in the taxonomy, because
how to choose the first two to split among the multiple branches is another challenge. It is an equally
difficult problem for us to binarize the taxonomy by hand comparing to directly choosing the granularity.
It would be our advantage to binarize the taxonomy by a data-driven method. For automatic binariza-
tion, a straightforward approach is to measure the utility of traversing all the plausible ways of cutting all
the branches into two sets individually and use the best one. Then we can deal with the divided two sets
in the same manner recursively. However, not only is this impractical, requiring an entire training phase
for each possible binarization scheme which is exponentially expensive, but it assumes the contributions
of multiple binarizations in different branches are independent. In fact, extra sub-symbols may need to
be added to several nonterminals before they can cooperate to pass information along the parse tree.
Therefore, we go in the opposite direction, and propose an extended version of split-merge learning
to handle the multiple branches in the taxonomy. That is, we split each state into all the subcategories in
the lower layer in the taxonomy even if it has multiple branches, train, and then measure for every two
sibling subcategories in the same layer the loss in likelihood incurred when merging them. If this loss is
small, the new division of these two subcategories does not carry enough useful information and can be
merged back. Contrary to the gain in likelihood for splitting, the loss in likelihood for merging can be
1890
efficiently approximated (Petrov et al., 2006).
More specifically, we assume transitivity in merging multiple subcategories in one layer. Figure 3
gives an illustration. After the split stage, the category A has been split into subcategories A-1, A-2, ...
to A-7. Then we compute the loss in likelihood of the training data by merging back each pair of two
subcategories through A-1 to A-7. If the loss is lower than a certain threshold
1
set for each round of
merge, this pair of newly split subcategories will be merged. We only show the sibling ones for brevity in
this example. Assume the losses of merging these pairs (A-1, A-2), (A-2, A-3), (A-3, A-4) and (A-4, A-
5) are below the threshold ?. Thus, A-1, A-2, A-3, A-4 and A-5 are merged to X-1 due to the transitivity
of the connected points, where X-1 is the automatically generated subcategory which contains the five
conflated subcategories as its descendants. At the meantime, A-6 and A-7 still remain. This scheme is an
approximation because it merges subcategories that should be merged with the same subcategory. But it
will leave the split of this instances to the next round when more evidence on interaction with other more
refined subcategories is given.
(a) Refined subcategories before the merge stage (b) Refined subcategories after the merge stage
Figure 3: Illustration of merging the subcategories for multiple branches in the taxonomy. Where ? is
a certain threshold below which this pair of subcategories will be merged, and X is the automatically
generated subcategory which contains the conflated subcategories as its descendants.
After merging in each round, the hierarchical knowledge is reshaped to fit the practical usage in the
Treebank. The split-merge cycles allow us to progressively increase the complexity of the hierarchical
knowledge, and the more useful distinctions are represented as the higher level in the taxonomy, which
gives priority to the most useful distinctions in return by supervising the grammar induction. Figure 4
demonstrates the transformation of the hierarchical structure from the tabular classification. Along this
road, the training scheme is not a unilateral training, but an interactive process between the knowledge
representation learning and the grammar training. Our learning process exerts a mutual effect to both the
induced grammar and the optimized structure of the hierarchical knowledge. In this way, the set of di-
viding standards are chosen iteratively according to their syntactic features. The more effective divisions
are conducted in the early stages. In the following stages, the divisions which interact with previous
divisions to give the most effective disambiguating information are adopted. The final taxonomy are
built based on manual classification in data-driven approach, and the hierarchical structure are optimized
and rational in the perspective of actual data distribution. Figure 4 illustrates a concrete instance of the
procedure of learning the taxonomy. On one hand, this procedure provides a more rational hierarchical
subcategorization structure according to data distribution. On the other hand, the order of the division
criterions represents the priorities the grammar induction takes for each criterion. The structure in the
higher levels of the taxonomy are determined by the dominant syntactic characteristics. And the division
in the later iterations are on the basis of minor distinctive characteristics.
6 Experiments and Results
6.1 Data Set
We present experimental results on both CTB5.0 (All traces and functional tags were stripped.) and TCT.
We ran experiments on CTB5.0 using the standard data allocation: files from CHTB 001.fid to
CHTB 270.fid, and files from CHTB 400.fid to CHTB 1151.fid were used as training set. The develop-
ment set includes files from CHTB 301.fid to CHTB 325.fid, and the test set includes files CHTB 271.fid
1
In practice, instead of setting a predefined threshold for merging, we merge a specific number of the newly split subcate-
gories.
1891
AA-7A-6A-5A-4A-3A-2A-1
(a) First round of category split
A
A-7A-6X-1
X-1
A-5A-4A-3A-2A-1
(b) First round of category merge
A
A-7A-6X-1
A-5A-4A-3A-2A-1
(c) Second round of category split
A
A-7A-6X-1
X-3X-2
X-2
A-2A-1
X-3
A-5A-4A-3
(d) Second round of category merge
A
A-7A-6X-1
X-3
A-5A-4A-3
X-2
A-2A-1
(e) Third round of category split
A
A-7A-6X-1
X-3
X-4A-3
X-2
A-2A-1
X-4
A-5A-4
(f) Third round of category merge
Figure 4: Iteration of grammar induction and taxonomy structure learning
to CHTB 300.fid. Experiments on TCT use the data set as in CIPS-SIGHAN-ParsEval-2012 (Zhou,
2012). We have parsed on the segmented text in the Treebank, namely, no use of gold POS-tags, use
of gold segmentations, and full-length sentences. This is the same as for other 5 parsers in Table 1 for
comparison. All the experiments were carried out after six cycles of split-merge.
1892
6.2 Final Results
The final results are shown in Table 1. Our final parsing performance is higher than both the manual
annotation method (Levy and Manning, 2003) and the data-driven method (Petrov, 2009).
Parser Precision Recall F
1
Levy(2003) 78.40 79.20 78.80
Petrov(2009) 84.82 81.93 83.33
Lin(2009) 86.00 83.10 84.50
Qian(2012) 84.57 83.68 84.13
Zhang(2013) 84.42 84.43 84.43
This paper 86.55 83.41 84.95
Table 1: Our final parsing performance compared with the best previous work on CTB5.0.
On test set TCT, the method achieves the best precision, recall and F-measure in the CIPS-SIGHAN-
ParsEval-2012 competition, and table 2 compares our results with the system of Beijing Information
Science and Technology University (BISTU) which got the second place in the competition.
Parser Precision Recall F
1
BISTU 70.10 68.08 69.08
This paper 76.81 76.66 76.74
Table 2: Our final parsing performance compared with the best previous works on TCT.
Given the manual labor required for generating the taxonomy (and in languages where there is a
taxonomy, determining whether it is suitable), this first study focuses on a language where there is quite
a bit of under- and over-specification in the Treebanks? tag sets. So this work is only implemented on
Chinese. We regard it as future work to transfer this approach to other languages.
6.3 Analysis
The outline of constructing the taxonomy of function words are as follows. Firstly, the function words are
manually subcategorized in a rough and cursory way. When dealing with subcategories hard to resolve
their relation of subordination, we simply treat them as siblings in the tree in a rather flat stricture, and
leave the elaboration of exquisite clustering to the algorithms. The data-driven approach in Section 4 au-
tomatically choose the appropriate granularity of refinement for our grammar. Moreover, the split-merge
learning for multiple branches in the hierarchical subcategories in Section 5 exploits the relationship be-
tween the sibling nodes in the same layer, making use of the Treebank data to adjust and optimize the
hierarchy.
During the split-merge process, the hierarchical subcategories are learned to fit the data, which is
a transformation of our manually defined hierarchy. The transformed hierarchy is just the route map
of subcategories employed in our model. As abbreviated in Figure 5 and Figure 6, many distinctions
between word sets of the subcategories have been exploited by our approach, and the learned taxonomy
is interpretable. For instance, It shows that the learned structure of the taxonomy is reasonable.
6.4 Comparison with Previous Work
Although the taxonomy of function words are learned in the grammar training process, the grammar is
trained on the Treebank in supervised manner. Thus, this work is not directly relevant with unsupervised
grammar induction literature (Headden III et al., 2009; Berant et al., 2007; Mare?cek and
?
Zabokrtsk`y,
2014).
1893
Subordinating Conjunction
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Coordination: bothQ
X
{
Progression: not only?=
Transition: although?,
X
{
Preference: rather than??
Cause: becausedu
Condition
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Universalization: ??
Equality: ??
X
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
X
?
?
?
?
?
X
{
Assumption: XJ
Sufficient Condition: ??
Necessary Condition: ?k
X
{
Unnecessary Condition: Q,
Insufficient Condition: =?
...
Figure 5: Abbreviated automatically learned hierarchical subcategories of subordinating conjunctions
with examples. Where ?X? represents the automatically generated subcategory.
Conjunctive Adverb
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Transition
?
?
?
?
?
?
?
?
?
Preference: would rather?X
X
{
In case: lest??
Otherwise: or else?K
However: but%
Result
?
?
?
?
?
X
{
Therefore: so??
So that: so that?B
Then, As a result: as soon?
Progression
?
?
?
?
?
?
?
?
?
X
?
?
?
?
?
X
{
Furthermore: but also?
In addition: moreover,	
Later: subsequently? 
As well: likewise?
...
Figure 6: Abbreviated automatically learned hierarchical subcategories of adverbs with examples.
Lin et al. (2009) and Li et al. (2014b) presented ideas of using either hierarchical semantic knowledge
from HowNet for content words or grammar knowledge for subordinating conjunctions. They introduced
hierarchical subcategory knowledge in a different stage. They split the original Treebank categories
in split-merge process according to the data, and then find a method to map the subcategories to the
node in the taxonomy, and constrain their further splitting. Comparing to their work, our approach is
more delicate, which is splitting the categories according to the knowledge, and learning the knowledge
structure according to data during the training course. Lin et al. (2009) incorporated semantic knowledge
of content words into the data-driven method. It would be promising if this work stacks with the content
word knowledge. However, the work with content word knowledge have to handle the polysemous words
in the semantic taxonomy, so they split the categories according to the data, and then find a way to map
the subcategories to the node in the taxonomy, and constrain their further splitting. It is our goal to make
these two methods compatible with each other.
Incorporating word formation knowledge achieved higher parsing accuracy according to Zhang and
1894
Clark (2011). However, they ran their experiment on gold POS-tags and a different data set split, which
is different form the setup of work in Table 1 including this work. They also presented their result on
automatically assigned POS-tags and the same data set split as in the work in Table 1 to facilitate the
performance comparison. It gave F
1
score of 81.45% for sentences with less than 40 words and 78.3%
for all sentences, significantly lower than Petrov and Klein (2007).
Zhang et al. (2013) exhaustively exploited character-level syntactic structures for words, and achieved
84.43% on F
1
measure. They placed more emphasis on the word-formation of content words, which
our model highlights the value of the function words. The complementary intuitions make it possible to
integrate these approaches together in the future work.
7 Conclusion
This paper presents an approach for inducing finer syntactic categories while learning the taxonomy for
function words. It used linguistic insight to guide the state-split process, and the hierarchical structure
representing syntactic features of function word usages was established during the grammar training
process. Empirical evidence has been provided that automatically subcategorizing function words con-
tributes to high parsing performance. The induced grammar supervised by the taxonomy outperformed
pervious approaches, which benefited from both the knowledge and the data-driven method. The pro-
posed approach for learning the structure of the taxonomy could be generalized to construct semantic
knowledge base.
Acknowledgments
This work was supported in part by the National Basic Research Program of China (973 Program) under
grant 2013CB329304, the Research Special Fund for Public Welfare Industry of Health under grant
201202001, the Key National Social Science Foundation of China under grant 12&ZD119, the National
Natural Science Foundation of China under grant 91120001.
References
Eneko Agirre, Timothy Baldwin, and David Martinez. 2008. Improving parsing and pp attachment performance
with sense information. Proceedings of ACL-08: HLT, pages 317?325.
Jonathan Berant, Yaron Gross, Matan Mussel, Ben Sandbank, Eytan Ruppin, and Shimon Edelman. 2007. Boost-
ing unsupervised grammar induction by splitting complex sentences on function words. In Proceedings of the
Boston University Conference on Language Development.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2012. Spectral learning of latent-
variable pcfgs. In Proceedings of the 50th annual meeting of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 223?231. Association for Computational Linguistics.
Zhendong Dong and Qiang Dong. 2003. Hownet-a hybrid language and knowledge resource. In Proceedings
of the international conference on natural language processing and knowledge engineering, pages 820?824.
IEEE.
Zhengdong Dong and Qiang Dong. 2006. HowNet and the computation of meaning. World Scientific Publishing
Co. Pte. Ltd.
Christiane Fellbaum. 1999. WordNet. Wiley Online Library.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki Tanaka. 2010. Exploiting semantic information for hpsg
parse selection. Research on language and computation, 8(1):1?22.
William P Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, pages 101?109.
Association for Computational Linguistics.
Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st annual
meeting on Association for Computational Linguistics-Volume 1, pages 423?430. Association for Computational
Linguistics.
1895
Roger Levy and Christopher D Manning. 2003. Is it harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st annual meeting on Association for Computational Linguistics-Volume 1, pages 439?
446. Association for Computational Linguistics.
Dongchen Li, Xiantao Zhang, and Xihong Wu. 2014a. Improved parsing with taxonomy of conjunctions. In 2014
IEEE China Summit & International Conference on Signal and Information Processing. IEEE.
Dongchen Li, Xiantao Zhang, and Xihong Wu. 2014b. Learning grammar with explicit annotations for subordi-
nating conjunctions in chinese. In Proceedings of the 52th annual meeting of the Association for Computational
Linguistics Student Research Workshop. Association for Computational Linguistics.
Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu, and Huisheng Chi. 2009. Refining grammars for parsing
with hierarchical semantic knowledge. In Proceedings of the 2009 conference on empirical methods in natural
language processing: Volume 3-Volume 3, pages 1298?1307. Association for Computational Linguistics.
David Mare?cek and Zden?ek
?
Zabokrtsk`y. 2014. Dealing with function words in unsupervised dependency parsing.
In Computational Linguistics and Intelligent Text Processing, pages 250?261. Springer.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005. Probabilistic cfg with latent annotations. In Pro-
ceedings of the 43rd annual meeting on Association for Computational Linguistics, pages 75?82. Association
for Computational Linguistics.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human language technologies
2007: the conference of the North American chapter of the Association for Computational Linguistics, pages
404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st international conference on computational linguistics and the 44th
annual meeting of the Association for Computational Linguistics, pages 433?440. Association for Computa-
tional Linguistics.
Slav Orlinov Petrov. 2009. Coarse-to-Fine natural language processing. Ph.D. thesis, University of California.
Detlef Prescher. 2005. Inducing head-driven pcfgs with latent heads: Refining a tree-bank grammar for parsing.
In Machine Learning: ECML 2005, pages 292?304. Springer.
Hui Wang and Shiwen Yu. 2003. The semantic knowledge-base of contemporary chinese and its applications
in wsd. In Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17, pages
112?118. Association for Computational Linguistics.
Fei Xia. 2000. The part-of-speech tagging guidelines for the penn chinese treebank (3.0). Technical report.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the penn chinese treebank
with semantic knowledge. In Natural language processing?IJCNLP 2005, pages 70?81. Springer.
Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony Kroch. 2000. The bracketing guidelines for the penn chinese
treebank (3.0). Technical report.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer. 2002. Building a large-scale annotated chinese corpus. In
Proceedings of the 19th international conference on computational linguistics-Volume 1, pages 1?8. Association
for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search.
Computational linguistics, 37(1):105?151.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2013. Chinese parsing exploiting characters. 51st
annual meeting of the Association for Computational Linguistics.
Qiang Zhou. 2004. Annotation scheme for chinese treebank. Journal of Chinese information processing, 18(4):1?
8.
Qiang Zhou. 2012. Evaluation report of the third chinese parsing evaluation: Cips-sighan-parseval-2012. In
Proceedings of the second CIPS-SIGHAN joint conference on Chinese language processing, pages 159?167.
Xuefeng Zhu, Shiwen Yu, and Hui Wang. 1995. The development of contemporary chinese grammatical knowl-
edge base and its applications. International journal of asian language processing, 5(1,2):39?41.
1896
Proceedings of the ACL 2014 Student Research Workshop, pages 48?55,
Baltimore, Maryland USA, June 22-27 2014.
c?2014 Association for Computational Linguistics
Learning Grammar with Explicit Annotations for Subordinating
Conjunctions
Dongchen Li, Xiantao Zhang and Xihong Wu
Key Laboratory of Machine Perception and Intelligence
Speech and Hearing Research Center
Peking University, Beijing, China
{lidc,zhangxt,wxh}@cis.pku.edu.cn
Abstract
Data-driven approach for parsing may suf-
fer from data sparsity when entirely un-
supervised. External knowledge has been
shown to be an effective way to alleviate
this problem. Subordinating conjunctions
impose important constraints on Chinese
syntactic structures. This paper proposes a
method to develop a grammar with hierar-
chical category knowledge of subordinat-
ing conjunctions as explicit annotations.
Firstly, each part-of-speech tag of the sub-
ordinating conjunctions is annotated with
the most general category in the hierar-
chical knowledge. Those categories are
human-defined to represent distinct syn-
tactic constraints, and provide an appropri-
ate starting point for splitting. Secondly,
based on the data-driven state-split ap-
proach, we establish a mapping from each
automatic refined subcategory to the one
in the hierarchical knowledge. Then the
data-driven splitting of these categories is
restricted by the knowledge to avoid over
refinement. Experiments demonstrate that
constraining the grammar learning by the
hierarchical knowledge improves parsing
performance significantly over the base-
line.
1 Introduction
Probabilistic context-free grammars (PCFGs) un-
derlie most of the high-performance parsers
(Collins, 1999; Charniak, 2000; Charniak and
Johnson, 2005; Zhang and Clark, 2009; Chen and
Kit, 2012; Zhang et al, 2013). However, a naive
PCFG which simply takes the empirical rules and
probabilities off of a Treebank does not perform
well (Klein and Manning, 2003; Levy and Man-
ning, 2003; Bansal and Klein, 2012), because
its context-freedom assumptions are too strong in
some cases (e.g. it assumes that subject and ob-
ject NPs share the same distribution). Therefore,
a variety of techniques have been developed to en-
rich PCFG (Klein and Manning, 2005; Matsuzaki
et al, 2005; Zhang and Clark, 2011; Shindo et al,
2012).
Hierarchical state-split approach (Petrov et al,
2006; Petrov and Klein, 2007; Petrov and Klein,
2008a; Petrov and Klein, 2008b; Petrov, 2009)
refines and generalizes the original grammars in
a data-driven manner, and achieves state-of-the-
art performance. Starting from a completely
markovized X-Bar grammar, each category is split
into two subcategories. EM is initialized with this
starting point and used to climb the highly non-
convex objective function of computing the joint
likelihood of the observed parse trees. Then a
merging step applies a likelihood ratio test to re-
verse the least useful half part of the splits. Learn-
ing proceeds by iterating between those two steps
for six rounds. Spectral learning of latent-variable
PCFGs (Cohen et al, 2012; Bailly et al, ; Co-
hen et al, 2013b; Cohen et al, 2013a) is an-
other effective manner of state-split approach that
provides accurate and consistent parameter esti-
mates. However, this two complete data-driven
approaches are likely to be hindered by the over-
fitting issue.
Incorporating knowledge (Zhang et al, 2013;
Wu et al, 2011) to refine the categories in train-
ing a parser has been proved to remedy the
weaknesses of probabilistic context-free grammar
(PCFG). The knowledge contains content words
semantic resources base (Fujita et al, 2010; Agirre
et al, 2008; Lin et al, 2009), named entity cues
(Li et al, 2013) and so on. However, they are
limited in that they do not take into account the
knowledge about subordinating conjunctions.
Subordinating conjunctions are important in-
dications for different syntactic structure, espe-
48
cially for Chinese. For example, the subordinating
conjunction ???? (no matter what) is typically
ahead of a sentence with pros and cons of the sit-
uation; on the contrary, a sufficient condition of-
ten occurs after the subordinating conjunction ?X
J? (if). Those two cases are of distinct syntac-
tic structure. Figure 1 demonstrates that although
the sequences of the part-of-speech of the input
words are similar, these two subordinating con-
junctions exert quite different syntactic constraints
to the following clauses.
IP
VP
VP
VA
??
succeed
ADVP
AD
?
not
CC
??
or
VP
VA
??
succeed
ADVP
CS
??
No
matter
(a) ???? (no matter what) is typically ahead of a sentence
with pros and cons of the situation.
IP
IP
VP
VP
VP
VA
??
succeed
ADVP
AD
?
don?t
ADVP
AD
??
still
NP
PN
\
you
ADVP
CS
XJ
if
(b) ?XJ? (if) often precedes a sufficient condition.
Figure 1: Different types of subordinating con-
junctions indicate distinct syntactic structure.
Based on the hierarchical state-split approach,
this paper proposes a data-oriented model super-
vised by our hierarchical subcategories of subordi-
nating conjunctions. In order to constrain the auto-
matic subcategory refinement, we firstly establish
the mapping between the automatic clustered sub-
categories and the predefined subcategories. Then
we employ a knowledge criterion to supervise the
hierarchical splitting of these subordinating con-
junction subcategories by the automatic state-split
approach, which can alleviate over-fitting. The ex-
periments are carried out on Penn Chinese Tree-
bank and Tsinghua Treebank, which verify that
the refined grammars with refined subordinating
conjunction categories can improve parsing per-
formance significantly.
The rest of this paper is organized as follows.
We first describe our hierarchical subcategories of
subordinating conjunction. Section 3 illustrates
the constrained grammar learning process in de-
tails. Section 4 presents the experimental evalua-
tion and the comparison with other approaches.
2 Hierarchical Subcategories of
Subordinating Conjunction
The only tag ?CS? for all the various subordinat-
ing conjunctions is too coarse to indicate the in-
tricate subordinating relationship. The words in-
dicating different grammatical features share the
same tag ?CS?, such as transition relationship,
progression relationship, preference relationship,
purpose relationship and condition relationship. In
each case, the context is different, and the subor-
dinating conjunction is an obvious indication for
the parse disambiguation for the context. The ex-
isting resources for computational linguistic, like
HowNet (Dong and Dong, 2003) and Cilin (Mei
et al, 1983), have classified all subordinating con-
junctions as one category, which is too coarse to
capture the syntactic implication.
To make use of the indication, we subdivide the
subordinating conjunctions according to its gram-
matical features in our scheme. Subordinating
conjunctions indicating each relationship is further
subdivided into two subcategories: one is used be-
fore the principal clause, the other is before the
subordinate clause. For example, the conjunc-
tions representing cause and effect contains ?be-
cause? and ?so?, where ?because? should mod-
ify the cause, and ?so? should modify the effect.
In addition, we found that there are several cases
in the conditional clause. Accordingly, we sub-
divide the conditional subordinating conjunctions
into seven types: assumption, universalization,
49
SubordinatingConjunction
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Transition
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
LatterOf?Transition
?? %
Formerof?Transition
?? ?,
Progression
{
FormerOf?Progression
?? ?
LatterOf?Progression
?? $?
Preference
{
LatterOf?Preference
?? ?X
FormerOf?Preference
?? ??
LogicCoordination
?
?
?
LatterOfTheCoordinationq
Logic?And
?? ??
Condition
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Assumption XJ
Universalization??
UnnecessaryConditionQ,
InsufficientCondition=?
SufficientCondition??
NecessaryCondition?k
Equality??
Purpose
{
LatterOf?Purpose
?? ?B
FormerOf?Purpose
?? ??
CauseAndEffect
{
Causedu
EffectJ
Figure 2: Hierarchical subcategories of subordinating conjunctions with examples.
equality, sufficient condition, necessary condition,
sufficient but unnecessary condition and necessary
but insufficient condition (concession). The de-
tailed hierarchical subcategories of subordinating
conjunctions are displayed in Figure 2.
3 Parsing with Hierarchical Categories
The automatic state-split approach is designed to
refine all symbols together through a data-driven
manner, which takes the over-fitting risk. Instead
of splitting and merging all symbols together auto-
matically, we employ a knowledge-based criterion
with hierarchical refinement knowledge to con-
straint the splitting of these new refined tags for
subordinating conjunctions.
At the beginning, we produce a good starting
annotation with the top subcategories in the hi-
erarchical subcategories, which is of great use to
constraining the automatic splitting process. As
demonstrated in Figure 4, our parser is trained on
the good initialization with the automatic hierar-
chical state-split process, and gets improvements
compared with the original training data. For ex-
ample, as shown in Figure 2, the category for
%(but) and ?Cause? for du(because) is anno-
tated as the top category ?Transition? and ?Cause
And Effect? respectively.
However, during this process, only the most
general hypernyms are used as the semantic rep-
resentation of words, and the lower subcategory
knowledge in the hierarchy is not explored. Thus,
we further constraint the split of the subordinating
conjunctions subcategories to be consistent with
the hierarchical subcategories to alleviate the over-
fitting issue. The top class is only used as the start-
ing annotations of POS tags to reduce the search
space for EM in our method. It is followed by the
hierarchical state-split process to further refine the
starting annotations based on the hierarchical sub-
categories.
3.1 Mapping from Automatic Subcategories
to Predefined Subcategories
With the initialization proposed above, the auto-
matically split-merge approach produces a series
of refined categories for each tag. We restrict each
automatically refined subcategory of subordinat-
ing conjunctions to correspond to a special node
50
Figure 3: A schematic figure for the hierarchical state-split process of the tag ?CS?. Each subcategory
of this tag has its own word set, and corresponds to one layer at the appropriate level in the hierarchical
subcategories.
in the hierarchical subcategories, as a hyponym
of ?CS?. The hierarchical subcategories are em-
ployed in the hierarchical state-split process to im-
pose restrictions on the subcategory refinement.
First of all, it is necessary to establish the map-
ping from each subcategory in the data-driven hi-
erarchical subcategories to the subcategory in the
predefined hierarchical subcategories. We trans-
fer the method for semantic-related labels (Lin et
al., 2009) to our case here. The mapping is imple-
mented with the word set related to each automati-
cally refined granularity of clustered subordinating
conjunctions and the node at the special level in
the subcategory knowledge. The schematic in Fig-
ure 3 demonstrates this supervised splitting pro-
cess for CS. The left part of this figure is the word
sets of automatic clustered subcategories of the
CS, which is split hierarchically. As expressed
by the lines, each subcategory corresponds to one
node in the right part of this figure, which is our hi-
erarchical subcategory knowledge of subordinat-
ing conjunctions.
As it is shown in Figure 3, the original tag ?CS?
treats all the words it produces as its word set.
Upon splitting each coarse category into two more
specific subcategories, its word set is also cut into
two subsets accordingly, through forcedly divid-
ing each word in the word set into one subcategory
which is most probable for this word in the lex-
ical grammar. And each automatic refinement is
mapped to the most specific subcategory (that is to
say, the lowest node) that contains the entirely cor-
responding word set in the human-defined knowl-
edge. On this basis, the new knowledge-based cri-
terion is introduced to enrich and generalize these
subcategories, with the purpose of fitting the re-
finement to the subcategory knowledge rather than
the training data.
3.2 Knowledge-based Criterion for
Subordinating Conjunctions Refinement
With the mapping between the automatic refined
subcategories and the human-defined hierarchical
subcategory knowledge, we could supervise the
automatic state refinement by the knowledge.
Instead of being merged by likelihood, a
knowledge-based criterion is employed, to decide
whether or not to go back to the upper layer in
the hierarchical subcategories and thus remove the
new subcategories of these tags. The criterion is
that, we assume that the bottom layer in the hi-
erarchical subcategories is special enough to ex-
press the distinction of the subordinating conjunc-
tions. If the subcategories of the subordinating
conjunctions has gone beyond the bottom layer,
then the new split subcategories are deemed to be
unnecessary and should be merged back. That is
to say, once the parent layer of this new subcate-
gory is mapped onto the most special subcategory,
it should be removed immediately. As illustrated
51
Treebank Train Dataset Develop Dataset Test Dataset
CTB5 Articles 1-270 Articles 400-1151, 301-325 Articles 271-300
TCT 16000 sentences 800 sentences 758 sentences
Table 1: Data allocation of our experiment.
in Figure 3, if the node has no hyponym, this sub-
category has been specialized enough according to
the knowledge, and thus the corresponding subcat-
egory will stop splitting.
By introducing a knowledge-based criterion,
the issue is settled whether or not to further split
subcategories from the perspective of predefined
knowledge. To investigate the effectiveness of the
presented approach, several experiments are con-
ducted on both Penn Chinese Treebank and Ts-
inghua Treebank. They reveal that the subcategory
knowledge of subordinating conjunctions is effec-
tive for parsing.
4 Experiments
4.1 Experimental Setup
We present experimental results on both Chinese
Treebank (CTB) 5.0 (Xue et al, 2002) (All traces
and functional tags were stripped.) and Tsinghua
Treebank (TCT) (Zhou, 2004). All the experi-
ments were carried out after six cycles of split-
merge.
The data set alocation is described in Table 1.
We use the EVALB parseval reference imple-
mentation (Sekine, 1997) for scoring. Statistical
significance was checked by Bikel?s randomized
parsing evaluation comparator (Bikel, 2000).
4.2 Parsing Performance with Hierarchical
Subcategories
We presented a flexible approach which refines
the subordinating conjunctions in a hierarchy fash-
ion where the hierarchical layers provide different
granularity of specificity. To facilitate the compar-
isons, we set up 6 experiments on CTB5.0 with
different strategies of choosing the subcategory
layers in the hierarchical subcategory knowledge:
? baseline: Training without hierarchical sub-
category knowledge
? top: Choosing the top layer in hierarchi-
cal subcategories (using ?Transition?, ?Con-
dition? , ?Purpose? and so on)
? bottom: Choosing the bottom layer in hierar-
chical subcategories (the most specified sub-
categories)
? word: Substituting POS tag with the word it-
self
? knowledge criterion: Automatically choos-
ing the appropriate layer through the knowl-
edge criterion
Figure 4: Comparison of parsing performance for
each model in the split-merge cycles.
Figure 4 shows the F
1
scores of the last 4 cy-
cles in the 6 split-merge cycles. The results are
just as expectation, through which we can tell that
the ?top? model performs slightly better than the
baseline owing to a better start point of the state-
splitting. This result confirms the value of our
initial explicit annotations. While the ?bottom?
model doesn?t improve the performance due to
excessive refinement and causes over-fitting, the
?word? model behaves even worse for the same
reason. In the 5th split-merge cycle, the ?knowl-
edge criterion? model picks the appropriate layer
52
in hierarchical subcategories and achieves the best
result.
We also test our method on TCT. Table 2 com-
pares the accuracies of the baseline, initialization
with top subcategories and the ?knowledge cri-
terion? model, and confirms that the subcategory
knowledge helps parse disambiguation.
Parser P R F
1
baseline 74.40 74.28 74.34
top 75.12 75.17 75.14
knowledge criterion 76.18 76.27 76.22
Table 2: Our parsing performance with different
criterions on TCT.
4.3 Final Results
Our final results are achieved using the ?knowl-
edge criterion? model. As we can see from the
table 3, our final parsing performance is higher
than the unlexicalized parser (Levy and Manning,
2003; Petrov, 2009) and the parsing system in
Qian and Liu (2012), but falls short of the systems
using semantic knowledge of Lin et al (2009) and
exhaustive word formation knowledge of Zhang et
al. (2013).
Parser P R F
1
Levy(2003) 78.40 79.20 78.80
Petrov(2009) 84.82 81.93 83.33
Qian(2012) 84.57 83.68 84.13
Zhang(2013) 84.42 84.43 84.43
Lin(2009) 86.00 83.10 84.50
This paper 85.93 82.87 84.32
Table 3: Our final parsing performance compared
with the best previous works on CTB5.0.
The improvement on the hierarchical state-split
approach verifies the effectiveness of the subcat-
egory knowledge of subordinating conjunctions
for alleviating over-fitting. And the subcategory
knowledge could be integrated with the knowl-
edge base employed in Lin et al (2009) and Zhang
et al (2013) to contribute more on parsing accu-
racy improvement.
5 Conclusion
In this paper, we present an approach to constrain
the data-driven state-split method by hierarchi-
cal subcategories of subordinating conjunctions,
which appear as explicit annotations in the gram-
mar. The parsing accuracy is improved by this
method owing to two reasons. Firstly, the most
general hypernym of subordinating conjunctions
exerts an initial restrict to the following splitting
step. Secondly, the splitting process is confined
by a knowledge-based criterion with the human-
defined hierarchical subcategories to avoid over
refinement.
Acknowledgments
We thank Baidu for travel and conference sup-
port for this paper. We thank Meng Zhang and
Dingsheng Luo for their valuable advice. This
work was supported in part by the National Ba-
sic Research Program of China (973 Program) un-
der grant 2013CB329304, the Research Special
Fund for Public Welfare Industry of Health under
grant 201202001, the Key National Social Science
Foundation of China under grant 12&ZD119, the
National Natural Science Foundation of China un-
der grant 91120001.
References
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and pp attachment perfor-
mance with sense information. Proceedings of ACL-
08: HLT, pages 317?325.
Rapha?el Bailly, Xavier Carreras P?erez, Franco M
Luque, and Ariadna Julieta Quattoni. Unsupervised
spectral learning of wcfg as low-rank matrix com-
pletion. Association for Computational Linguistics.
Mohit Bansal and Daniel Klein. 2012. An all-
fragments grammar for simple and accurate parsing.
Technical report, DTIC Document.
Bikel. 2000. Dan bikel?s random-
ized parsing evaluation comparator. In
http://www.cis.upenn.edu/dbikel/software.html.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd annual meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
53
American chapter of the association for computa-
tional Linguistics conference, pages 132?139. Asso-
ciation for Computational Linguistics.
Xiao Chen and Chunyu Kit. 2012. Higher-order con-
stituent parsing and parser combination. In Pro-
ceedings of the 50th annual meeting of the Associ-
ation for Computational Linguistics: Short papers-
Volume 2, pages 1?5. Association for Computational
Linguistics.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable pcfgs. In Proceedings of the 50th an-
nual meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 223?231.
Association for Computational Linguistics.
Shay B Cohen, Giorgio Satta, and Michael Collins.
2013a. Approximate pcfg parsing using tensor de-
composition. In Proceedings of NAACL-HLT, pages
487?496.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2013b. Experiments with
spectral learning of latent-variable pcfgs. In Pro-
ceedings of NAACL-HLT, pages 148?157.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Zhendong Dong and Qiang Dong. 2003. Hownet-a hy-
brid language and knowledge resource. In Proceed-
ings of the international conference on natural lan-
guage processing and knowledge engineering, pages
820?824. IEEE.
Sanae Fujita, Francis Bond, Stephan Oepen, and
Takaaki Tanaka. 2010. Exploiting semantic infor-
mation for hpsg parse selection. Research on lan-
guage and computation, 8(1):1?22.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st annual meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Dan Klein and Christopher D Manning. 2005. Parsing
and hypergraphs. In New developments in parsing
technology, pages 351?372. Springer.
Roger Levy and Christopher D Manning. 2003. Is
it harder to parse chinese, or the chinese treebank?
In Proceedings of the 41st annual meeting on As-
sociation for Computational Linguistics-Volume 1,
pages 439?446. Association for Computational Lin-
guistics.
Dongchen Li, Xiantao Zhang, and Xihong Wu. 2013.
Improved chinese parsing using named entity cue.
In Proceeding of the 13th international conference
on parsing technology, pages 45?53.
Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu,
and Huisheng Chi. 2009. Refining grammars for
parsing with hierarchical semantic knowledge. In
Proceedings of the 2009 conference on empirical
methods in natural language processing: Volume 3-
Volume 3, pages 1298?1307. Association for Com-
putational Linguistics.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd annual meeting on Associ-
ation for Computational Linguistics, pages 75?82.
Association for Computational Linguistics.
Jia-Ju Mei, YM Li, YQ Gao, et al 1983. Chinese
thesaurus (tong-yi-ci-ci-lin).
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human language tech-
nologies 2007: the conference of the North Amer-
ican chapter of the Association for Computational
Linguistics, pages 404?411.
Slav Petrov and Dan Klein. 2008a. Discriminative
log-linear grammars with latent variables. Advances
in neural information processing systems, 20:1153?
1160.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing.
In Proceedings of the conference on empirical meth-
ods in natural language processing, pages 867?876.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
international conference on computational linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 433?440. As-
sociation for Computational Linguistics.
Slav Orlinov Petrov. 2009. Coarse-to-Fine natural
language processing. Ph.D. thesis, University of
California.
Xian Qian and Yang Liu. 2012. Joint chinese word
segmentation, pos tagging and parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 501?
511. Association for Computational Linguistics.
Collins Sekine. 1997. Evalb bracket scoring program.
In http://nlp.cs.nyu.edu/evalb/.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing.
In Proceedings of the 50th annual meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 440?448. Association for
Computational Linguistics.
54
Xihong Wu, Meng Zhang, and Xiaojun Lin. 2011.
Parsing-based chinese word segmentation integrat-
ing morphological and syntactic information. In
Proceedings of 7th international conference on nat-
ural language processing and knowledge engineer-
ing (NLP-KE), pages 114?121. IEEE.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of the 19th international confer-
ence on computational linguistics-Volume 1, pages
1?8. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the chinese treebank using a global dis-
criminative model. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
162?171. Association for Computational Linguis-
tics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational linguistics, 37(1):105?151.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
51st annual meeting of the Association for Compu-
tational Linguistics.
Qiang Zhou. 2004. Annotation scheme for chinese
treebank. Journal of Chinese information process-
ing, 18(4):1?8.
55
