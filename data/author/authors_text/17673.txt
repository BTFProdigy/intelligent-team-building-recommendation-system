Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1701?1712,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Combining String and Context Similarity
for Bilingual Term Alignment from Comparable Corpora
Georgios Kontonatsios
1,2
Ioannis Korkontzelos
1,2
Jun?ichi Tsujii
3
Sophia Ananiadou
1,2
National Centre for Text Mining, University of Manchester, Manchester, UK
1
School of Computer Science, University of Manchester, Manchester, UK
2
Microsoft Research Asia, Beijing, China
3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
Automatically compiling bilingual dictio-
naries of technical terms from comparable
corpora is a challenging problem, yet with
many potential applications. In this paper,
we exploit two independent observations
about term translations: (a) terms are of-
ten formed by corresponding sub-lexical
units across languages and (b) a term and
its translation tend to appear in similar lex-
ical context. Based on the first observa-
tion, we develop a new character n-gram
compositional method, a logistic regres-
sion classifier, for learning a string similar-
ity measure of term translations. Accord-
ing to the second observation, we use an
existing context-based approach. For eval-
uation, we investigate the performance of
compositional and context-based methods
on: (a) similar and unrelated languages,
(b) corpora of different degree of compa-
rability and (c) the translation of frequent
and rare terms. Finally, we combine the
two translation clues, namely string and
contextual similarity, in a linear model and
we show substantial improvements over
the two translation signals.
1 Introduction
Bilingual dictionaries of technical terms are re-
sources useful for various tasks, such as computer-
aided human translation (Dagan and Church,
1994; Fung and McKeown, 1997), Statistical Ma-
chine Translation (Och and Ney, 2003) and Cross-
Language Information Retrieval (Ballesteros and
Croft, 1997). In the last two decades, researchers
have focused on automatically compiling bilingual
term dictionaries either from parallel (Smadja et
al., 1996; Van der Eijk, 1993) or comparable cor-
pora (Rapp, 1999; Fung and Yee, 1998). While
parallel corpora contain the same sentences in two
languages, comparable corpora consist of bilin-
gual pieces of text that share some features, only,
such as topic, domain, or time period. Comparable
corpora can be constructed more easily than paral-
lel corpora. Freely available, up-to-date, on-line
resources (e.g., Wikipedia) can be employed.
In this paper, we exploit two different sources
of information to extract bilingual terminology
from comparable corpora: the compositional and
the contextual clue. The compositional clue is
the hypothesis that the representations of a term
in any pair of languages tend to consist of cor-
responding lexical or sub-lexical units, e.g., pre-
fixes, suffices and morphemes. In order to cap-
ture associations of textual units across languages,
we investigate three different character n-gram ap-
proaches, namely a Random Forest (RF) classifier
(Kontonatsios et al., 2014), Support Vector Ma-
chines with an RBF kernel (SVM-RBF) and a Lo-
gistic Regression (LogReg) classifier. Whilst the
previous approaches take as an input monolingual
features and then try to find cross-lingual map-
pings, our proposed method (LogReg classifier)
considers multilingual features, i.e., tuples of co-
occurring n-grams.
The contextual clue is the hypothesis that mu-
tual translations of a term tend to occur in similar
lexical context. Context-based approaches are un-
supervised methods that compare the context dis-
tributions of a source and a target term. A bilin-
gual seed dictionary is used to map context vec-
tor dimensions of two languages. Li and Gaussier
(2010) suggested that the seed dictionary can be
used to estimate the degree of comparability of a
bilingual corpus. Given a seed dictionary, the cor-
pus comparability is the expectation of finding for
each word of the source corpus, its translation in
the target part of the corpus. The performance of
context-based methods has been shown to depend
on the frequency of terms to be translated and the
1701
corpus comparability. In this work, we use an ex-
isting distributional semantics approach to locate
term translations.
Furthermore, we hypothesise that the compo-
sitional and contextual clue are orthogonal, since
the former considers the internal structure of terms
while the latter exploits the surrounding lexical
context. Based on the above hypothesis, we com-
bine the two translation clues in a linear model.
For experimentation, we construct compara-
ble corpora for four language pairs (English-
Spanish, English-French, English-Greek and
English-Japanese) of the biomedical domain.
We choose this domain because a large propor-
tion of the medical terms tends to composition-
ally translate across languages (Lovis et al., 1997;
Namer and Baud, 2007). Additionally, given the
vast amount of newly introduced terms (neolo-
gisms) in the medical domain (Pustejovsky et al.,
2001), term alignment methods are needed in or-
der to automatically update existing resources.
We investigate the following aspects of term
alignment: (a) the performance of compositional
methods on closely related and on distant lan-
guages, (b) the performance of context vectors and
compositional methods when translating frequent
or rare terms, (c) the degree to which the corpus
comparability affects the performance of context-
based and compositional methods (d) the improve-
ments that we can achieve when we combine the
compositional and context clue.
Our experiments show that the performance of
compositional methods largely depends on the dis-
tance between the two languages. The perfor-
mance of the context-based approach is greatly
affected by corpus-specific parameters (the fre-
quency of occurrence of the terms to be translated
and the degree of corpora comparability). It is also
shown that the combination of compositional and
contextual methods performs better than each of
the clues, separately. Combined systems can be
deployed in application environments with differ-
ent language pairs, comparable corpora and seeds
dictionaries.
The LogReg, dictionary extraction method de-
scribed in this paper is freely available
1
.
1
http://personalpages.manchester.
ac.uk/postgrad/georgios.kontonatsios/
Software/LogReg-TermAlign.tar.gz
2 Related Work
Context-based methods (Fung and Yee, 1998;
Rapp, 1999) adapt the Distributional Hypothesis
(Harris, 1954), i.e., words that occur in similar
lexical context tend to have the same meaning, in
a multilingual environment. They represent the
context of each term t as a context vector, usu-
ally following the bag-of-words model. Each di-
mension of the vector corresponds to a context
word occurring within a predefined window, while
the corresponding value is computed by a corre-
lation metric, e.g., Log-Likelihood Ratio (Morin
et al., 2007; Chiao and Zweigenbaum, 2002) or
Point-wise Mutual Information (Andrade et al.,
2010). A general bilingual dictionary is then used
to translate/project the target context vectors into
the source language. As a result, the source and
target context vectors become directly compara-
ble. In a final step, candidate translations are being
ranked according to a distance metric, e.g., cosine
similarity (Tamura et al., 2012) or Jaccard index
(Zanzotto et al., 2010; Apidianaki et al., 2012).
Whilst context-based methods have become a
common practise for bilingual dictionary extrac-
tion from comparable corpora, nonetheless, their
performance is subject to various factors, one of
which is the quality of the comparable corpus. Li
and Gaussier (2010) introduced the corpus com-
parability metric and showed that it is related to
the performance of context vectors. The higher
the corpus comparability is, the higher the perfor-
mance of context vectors is. Furthermore, context
vector approaches are sensitive to the frequency of
terms. For frequent terms, distributional seman-
tics methods exhibit robust performance since the
corresponding context is more informative. Chiao
and Zweigenbaum (2002) reported an accuracy of
91% for the top 20 candidates when translating
terms that occur 100 times or more. However,
the performance of context vectors drastically de-
creases for lower frequency terms (Kontonatsios et
al., 2014; Morin and Daille, 2010).
Our work is more closely related to a second
class of term alignment methods that exploits the
internal structure of terms between a source and
a target language. Compositional translation al-
gorithms are based on the principal of composi-
tionality (Keenan and Faltz, 1985), which claims
that the translation of the whole is a function of
the translation of its parts. Lexical (Morin and
Daille, 2010; Daille, 2012; Robitaille et al., 2006;
1702
Tanaka, 2002) and sub-lexical (Delpech et al.,
2012) compositional algorithms are knowledge-
rich approaches that proceed in two steps, namely
generation and selection. In the generation step,
an input source term is segmented into basic trans-
lation units: words (lexical compositional meth-
ods) or morphemes (sub-lexical methods). Then
a pre-compiled, seed dictionary of words or mor-
phemes is used to translate the components of the
source term. Finally, a permutation function gen-
erates candidate translations using the list of the
translated segments. In the selection step, candi-
date translations are ranked according to their fre-
quency (Morin and Daille, 2010; Robitaille et al.,
2006) or their context similarity with the source
term (Tanaka, 2002). The performance of the
compositional translation algorithms is bound to
the coverage of the seed dictionary (Daille, 2012).
Delpech et al. (2012) noted that 30% of untrans-
lated terms were due to the low coverage of the
seed dictionary.
Kontonatsios et al. (2014) introduced a Random
Forest (RF) classifier that learns correspondences
of character n-grams between a source and target
language. Unlike lexical and sub-lexical compo-
sitional methods, a RF classifier does not require
a bilingual dictionary of translation units. The
model is able to automatically build correlation
paths between source and target sub-lexical seg-
ments that best discriminate translation from non-
translation pairs. However, being a supervised
method, it still requires a seed bilingual dictio-
nary of technical terms for training. The RF classi-
fier was previously applied on an English-Spanish
comparable corpus and it was shown to signifi-
cantly outperform context-based approaches.
3 Methods
In this section we describe the character n-gram
models, the context vector method and the hybrid
system. The lexicon induction task is formalised
as a two-class classification problem. Given a pair
of terms in a source and a target language, the out-
put is a prediction of whether the terms are mutual
translations are not. Furthermore, each term align-
ment method implements a ranking function that
calculates a similarity score between a source and
a target term. The methods rank target terms ac-
cording to the similarity score and select the top N
ranked terms as candidate translations. The rank-
ing functions will be discussed in the following
subsections.
3.1 Character n-gram models
Let s be a source term containing p character n-
grams (s={s
1
, s
2
, ..., s
p
} s
i
? S, ?i ? [1, p])
and t a target term of q n-grams (t={t
1
, t
2
, ..., t
q
}
t
i
? T , ?i ? [1, q]). We extract charac-
ter n-grams by considering any contiguous, non-
linguistically motivated sequence of characters
that occurs within a window size of [2 ? 5]
2
) for
English, French and Greek. For Japanese, uni-
grams are included (window size of [1 ? 5] be-
cause Japanese terms often contain Kanji (Chi-
nese) characters.
Given the two lists of source and target n-grams,
our objective is to find an underlying relationship
between S and T that best discriminates trans-
lation from non-translation pairs. The RF clas-
sifier was previously shown to exhibit such be-
haviour (Kontonatsios et al., 2014). An RF clas-
sifier (Breiman, 2001) is a collection of decision
trees voting for the most popular class. For a pair
of source and target terms ?s, t?, the RF method
creates feature vectors of a fixed size 2r, i.e., first
order feature space. The first r features are ex-
tracted from the source term, while the last r fea-
tures from the target term. Each feature has a
boolean value (0 or 1) that designates the pres-
ence/absence of the corresponding n-gram in the
input instance.
The ability of the RF to detect latent associa-
tions between S and T relies on the decision trees.
The internal nodes of a decision tree represent the
n-gram features that are linked together in the tree-
hierarchy. Each leaf node of the trees is labelled as
translation or non-translation indicating whether
the parent path of n-gram features is positively or
negatively associated. The classification margin
that we use to rank the candidate translations is
given by a margin function (Breiman, 2001):
mg(X,Y ) = av(I(x) = 1)?av(I(x)) = 0) (1)
where x is an instance ?s, t?, y ? Y = {0, 1} the
class label, I(?) : (s, t) ?? {0, 1} is the indicator
function of a decision tree and av(I(?)) the aver-
age number of trees voting for the same class la-
bel. In our experiments, we used the same settings
as the ones reported in Kontonatsios et al. (2014).
2
we have experiments with larger and narrower window
sizes but this setting resulted in better translation accuracy
1703
We used 140 decision trees and log
2
|2q| + 1 ran-
dom features. For training an RF model, we used
the WEKA platform (Hall et al., 2009).
The second class of machine learning algo-
rithms that we investigate is Support Vector Ma-
chines (SVMs). The simplest version of SVMs
is a linear classifier (linear-SVM) that tries to
place a hyperplane, a decision boundary, that sepa-
rates translation from non-translation instances. A
linear-SVM is a feature agnostic method since the
model only exploits the position of the vectors in
the hyperspace to achieve class separation (Hastie
et al., 2009).
The first order feature representation used with
the RF classifier does not model associations be-
tween S and T . Hence, intuitively, a first or-
der feature space is not linearly separable, i.e.,
there exists no decision boundary that divides the
data points into translations and non-translations.
3
. To solve non-linear classification problems,
SVMs employ non-linear kernels. A kernel func-
tion projects input instances into a higher dimen-
sional space to discover non-linear associations
between the initial features. In this new, projected
feature space, the SVM attempts to define a sep-
arating plane. For training an non-linear SVM on
the first order feature space, we used the LIBSVM
package (Chang and Lin, 2011) with a radial ba-
sis function (RBF) kernel. For ranking candidate
translations, we used the decision value given by
LIBSVM which represents the distance between
an instance and the hyperplane. To translate a
source term, the method ranks candidate transla-
tions by decision value and suggests as best trans-
lation the candidate with the maximum distance
(maximum margin).
While the first order models try to find cross-
lingual mappings between monolingual features,
our proposed method follows a different approach.
It models cross-lingual links between the source
and target character n-grams and uses them as
second order features to train a linear classifier.
A second order feature is a tuple of n-grams in
S and T , respectively, that co-occur in a train-
ing, translation instance. Second order feature
3
We applied a linear-SVM with the first order feature
representation on the four comparable corpora for English-
French, English-Spanish, English-Greek and English-
Japanese. In all cases, the best accuracies achieved were close
to zero. Additionally, the ranked list of candidate translations
was the same for all source terms. Hence, we can empiri-
cally suggest that the linear-SVM cannot exploit a first order
feature space.
values are boolean. Given a translation instance
?s, t? of p source and q target n-grams, there are
p?q second order features. For dimensionality re-
duction, we consider as second order features the
most frequent out of all possible first order feature
combinations, only. Experiments indicate that a
large number of features needs to be considered
to achieve robust performance. To cope with the
high dimensional second order space, we use LI-
BLINEAR (Fan et al., 2008), which is designed
to solve large-scale, linear classifications prob-
lems. LIBLINEAR implements two linear clas-
sification algorithms: LogReg and linear-SVM.
Both models solve the same optimisation problem,
i.e., determine the optimal separating plane, but
they adopt different loss functions. Since LIBLIN-
EAR does not support decision value estimations
for the linear-SVM, we only experimented with
LogReg. Similarly to SVM-RBF, LogReg ranks
candidate translations by classification margin.
3.2 Context vectors
We follow a standard approach to calculate context
similarity of source and target terms (Rapp, 1999;
Morin and Daille, 2010; Morin and Prochasson,
2011a; Delpech et al., 2012). Context vectors
of candidate terms in the source and target lan-
guage are populated after normalising each bilin-
gual corpus, separately. Normalisation consists
of stop-word filtering, tokenisation, lemmatisa-
tion and Part-of-Speech (PoS) tagging. For En-
glish, Spanish and French we used the TreeTagger
(Schmid, 1994) while for Greek we used the ILSP
toolkit (Papageorgiou et al., 2000). The Japanese
corpus was segmented and PoS-tagged using Ju-
man (Kurohashi and Kawahara, 2005).
In succession, monolingual context vectors are
compiled by considering all lexical units that oc-
cur within a window of 3 words before or af-
ter a term (a seven-word window). Only lexical
units (seeds) that occur in a bilingual dictionary
are retained The values in context vectors are Log-
Likelihood Ratio associations (Dunning, 1993) of
the term and a seed lexical unit occurring in it. In
a second step, we use the translations in the seed
dictionary to map target context vectors into the
source vector space. If there are several transla-
tions for a term, they are all considered with equal
weights. Finally, candidate translations are ranked
in descending order of the cosine of the angle be-
tween the mapped target vectors and the source
1704
Trainingcorpus
Testcorpus
character n-grammodel context vectors
hybrid model
Annotate Annotate
Train Project
seed termdictionary seed worddictionary
Figure 1: Architecture of the hybrid term align-
ment system.
vector.
3.3 Hybrid term alignment system
Figure 1 illustrates a block diagram of our term
alignment system. We use two bilingual seed dic-
tionaries: (a) a dictionary of term translation pairs
to train the n-gram models and (b) a dictionary of
word-to-word correspondences to translate target
context vectors. The n-gram and context vector
methods are used separately to score term pairs.
The n-gram model computes the value of the com-
positional clue while the context vector estimates
the score of the contextual clue. The hybrid model
combines both methods by using the correspond-
ing scores as features to train a linear classifier.
For this, we used a linear-SVM of the LIBSVM
package with default values for all parameters.
4 Data
Following previous research (Prochasson and
Fung, 2011; Irvine and Callison-Burch, 2013;
Klementiev et al., 2012), we construct compara-
ble biomedical corpora using Wikipedia as a freely
available resource.
Starting with a list of 4K biomedical English
terms (query-terms), we collected 4K English
Wikipedia articles, by matching query-terms to the
topic signatures of articles. Then, we followed
the Wikipedia interlingual links to retrieve the-
matically related articles in each target language.
Since not all English articles contain links for all
four target languages (Spanish, French, Greek and
Japanese), we used a different list of query-terms
for each language pair. Corpora were randomly
divided into training and testing parts. For train-
ing we used 3K documents and for testing the re-
maining 1K. Table 1 shows the size of corpora in
terms of numbers of source (SW) and target words
(TW).
4.1 Seed dictionaries
As shown in Figure 1, the term alignment methods
require two seed bilingual dictionaries: a term and
a word dictionary. The character n-gram models
rely on a bilingual term dictionary to learn asso-
ciations of n-grams that appear often in technical
terms. The dictionary may contain both single-
word and multi-word terms. For English-Spanish
and English-French we used UMLS (Bodenreider,
2004) while for English-Japanese we used an elec-
tronic dictionary of medical terms (Denshika and
Kenkyukai, 1991).
An English-Greek biomedical dictionary was
not available at the time of conducting these ex-
periments, thus we automatically compiled a dic-
tionary from a parallel corpus. For this, we trained
a standard Statistical Machine Translation system
(Koehn et al., 2007) on EMEA (Tiedemann, 2009),
a biomedical parallel corpus containing sentence-
aligned documents from the European Medicines
Agency. Then, we extracted all English-Greek
pairs for which: (a) the English sequence was
listed in UMLS and (b) the translation probability
was equal or higher to 0.7.
The sizes of the seed term dictionaries vary sig-
nificantly, e.g., 500K entries for English-French
but only 20K entries for English-Greek. How-
ever, the character n-gram models require a rela-
tively small portion of the corresponding dictio-
nary to converge. In the reported experiments,
we used 10K translation pairs as positive, train-
ing instances. In addition, we generated an equal
number of pseudo-negative instances by randomly
matching non-translation terms.
Morin and Prochasson (2011b) showed that the
translation accuracy of context vectors is higher
when using bilingual dictionaries that contain both
general language entries and technical terms rather
than general or domain-specific dictionaries, sep-
1705
Training corpus Test Corpus
# SW # TW # SW # TW
en-fr 4.8M 2.2M 1.9M 1.1M
en-es 4.9M 2.5M 1.8M 0.9M
en-el 10.2M 2.4M 3.3M 1.3M
en-jpn 5.3M 2.4M 2.3M 1.2M
Table 1: Statistics of the English-French (en-
fr), Engish-Spanish (en-es), English-Greek (en-
el) and English-Japanese (en-jpn) Wikipedia com-
parable corpora. SW: source words, TW: target
words
Corpus Seed words
Comparability in dictionary
en-fr 0.71 66K
en-es 0.75 40K
en-el 0.68 22K
en-jpn 0.49 57K
Table 2: Corpus comparability and number of fea-
tures of the seed word dictionaries
arately. In a mixed dictionary, lexical units are
either single-word technical terms, such as ?dis-
ease? and ?patient?, or general language words,
such as ?occur? and ?high?. Note that we have
already compiled a seed term dictionary for each
pair of languages. Following the suggestion of
Morin and Prochasson (2011b), we attempt to en-
rich the seed term dictionaries with general lan-
guage entries. For this, we extracted bilingual
word dictionaries for English-Spanish, English-
French and English-Greek by applying GIZA++
(Och and Ney, 2003) on the EMEA corpus. We
then concatenated the word with the term dictio-
naries to obtain enhanced seeds for the three lan-
guage pairs. For English-Japanese, we only used
the term dictionary to translate the target context
vectors.
Once the word dictionaries have been compiled,
we compute the corpus comparability measure. Li
and Gaussier (2010) define corpus comparability
as the percentage of words that can be translated
bi-directionally, given a seed dictionary.
Table 2 shows corpus comparability scores of
the four corpora accompanied with the number
of English, single words in the seed dictionar-
ies. It can be observed that seed dictionary sizes
are not necessarily proportional to the correspond-
ing corpus comparability scores. As expected, for
English-Japanese, corpus comparability is low be-
cause the dictionary contains single-word terms,
only. The English-Spanish dictionary is smaller
than the English-French but achieved higher cor-
pus comparability, i.e., a higher percentage of
words can be bi-directionally translated using the
corresponding seed dictionary. A possible ex-
planation is that the comparable corpora were
constructed using different lists of query-terms.
Hence, the query-terms used for English-Spanish
retrieved a more coherent corpus. The resulting
values of corpus comparability indicate that the
context vectors will perform the best for English-
Spanish while for English-Japanese the perfor-
mance is expected to be substantially lower.
4.2 Training and evaluation datasets
For evaluation, we construct a test dataset of
single-word terms, in particular nouns or adjec-
tives. The dataset contains 1K terms that occur
more frequently than 20 but not more than 200
times and are listed in the English part of the
UMLS. In order to extract candidate translations,
we considered all nouns or adjectives that occur
at least 5 times in the target part of the corpus.
Furthermore, we do not constraint the evaluation
datasets only to those terms whose corresponding
translation occurs in the corpus.
The hybrid model that combines the composi-
tional and context clue, is based on a two-feature
model. Therefore, the model converges using only
a few hundred instances. For training a hybrid
model, we used 1K translation instances that oc-
curred in the training comparable corpora. Sim-
ilarly, to the character n-gram models, pseudo-
negative instances were generated by randomly
coupling non-translation terms. The ratio of posi-
tive to negative instances is 1 : 1.
5 Experiments
In this section, we present three experiments con-
ducted to evaluate the character n-gram, con-
text vector and hybrid methods. Firstly, we
examine the performance of the n-gram mod-
els on closely related language pairs (English-
French, English-Spanish), on a distant language
pair (English-Greek) and on an unrelated language
pair (English-Japanese). English and Greek are
not unrelated because they are members of the
same language family, but also not closely re-
lated because they use different scripts. Secondly,
1706
we compare the character n-gram methods against
context vectors when translating frequent or rare
terms and on comparable corpora of similar lan-
guage pairs (English-French, English-Spanish) but
of different corpus comparability scores. Thirdly,
we evaluate the hybrid method on all four com-
parable corpora and investigate the improvement
margin of combining the contextual with the com-
positional clue.
As evaluation metrics, we adopt the top-N
translation accuracy, following most previous ap-
proaches (Rapp, 1999; Chiao and Zweigenbaum,
2002; Morin et al., 2007; Tamura et al., 2012). The
top-N translation accuracy is defined as the per-
centage of source terms for which a given method
has output the correct translation among the top N
candidate translations.
5.1 Character n-gram models
In the first experiment, we investigate the perfor-
mance of the character n-gram models consider-
ing an increasing number of features. The features
were sorted in order of decreasing frequency of oc-
currence. Starting from the top of the list, more
features were incrementally added and translation
accuracy was recorded.
Figure 2 shows the top-20 translation accu-
racy of single-word terms on an increasing num-
ber of first and second order features. With re-
gards to the first order models (Subfigure 2a),
the Random Forest (RF) classifier outperforms
our baseline method (SVM-RBF) for all four lan-
guage pairs. The largest margin between RF and
SVM-RBF can be observed for the English-Greek
dataset while for closely related language pairs,
i.e., English-French and English-Spanish, the mar-
gin is smaller. Furthermore, it can be noted that
using only a small number of first order features,
1K features (500 for the source and 500 for the
target language, both n-gram models reach a sta-
ble performance.
In contrast to the first order models, the Lo-
gReg classifier requires a large number of sec-
ond order features to achieve a robust performance
(Subfigure 2b). Starting from 100K features, the
translation accuracy continuously increases. The
best performance is observed for a total number
of 4M second order features when considering
the English-French, English-Spanish and English-
Greek datasets. For English-Japanese, the best
performance is achieved for 2M features. Beyond
this point, translation accuracy decreases slightly.
After feature selection is performed, we directly
compare all the character n-gram models. Table 3
summarises performance achieved by the LogReg,
RF and SVM-RBF models. It can be noted that
LogReg and RF performed similarly for closely
related languages (no statistically significant dif-
ferences were observed) while both methods out-
performed the SVM-RBF. However, for English-
Greek and English-Japanese, LogReg achieved
a statistically significant improvement over the
translation accuracy of RF and SVM-RBF. Lo-
gReg outperformed RF by 7% for English-Greek,
while for English-Japanese the improvement was
10% and 17% percent for top-1 and top-20 accu-
racy, respectively. Finally, it can be observed that
the more distant the language pair is, the lower the
performance.
5.2 N-gram methods and context vectors
In this experiment, we compare the n-gram meth-
ods against context vectors with regards to two pa-
rameters: (a) the frequency of source terms to be
translated and (b) corpus comparability. English-
French and English-Spanish are similar language
pairs but the corresponding corpora are of dif-
ferent corpus comparability scores. To investi-
gate how performance is affected by term occur-
rence frequency, we compiled an additional test
dataset of 1K rare English terms in the frequency
range [10, 20]. Our intuition is, that character n-
gram methods will perform similarly for all set-
tings since character n-grams are corpus indepen-
dent features.
We compare (a) the character n-gram models
(LogReg, RF and SVM-RBF) with (b) the con-
text vector method (context) and (c) an upper
bound. The latter represents the percentage of
source terms for which a reference translation ac-
tually occurs in the target corpus. Hence, the up-
per bound is the maximum performance achiev-
able according to the reference evaluation.
Figure 3a shows the top-20 translation accu-
racy for high and medium frequency terms, within
the frequency range [20, 200]. Context vectors
achieved a robust performance of 52% and 45%
for English-Spanish and English-French, respec-
tively. The difference in corpus comparability
can explain this 7% margin between these perfor-
mances. As shown in Table 2, the corpus com-
parability scores for English-Spanish and English-
1707
0
0.1
0.2
0.3
0.4
0.5
0.6
100 200 400 600 800 1000
trans
lation
 accu
racy 
@ 20
# first order features
RF (en-jpn)SVM-RBF (en-jpn)RF (en-el)SVM-RBF (en-el)
RF (en-fr)SVM-RBF (en-fr)RF (en-es)SVM-RBF (en-es)
(a) First order n-gram models
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
100 200 300 400 500 1000 2000 3000 4000
trans
lation
 accu
racy 
@ 20
# second order features (x10^3)
en-jpn en-el en-fr en-es
(b) Second order n-gram model
Figure 2: Top-20 translation accuracy of models trained on (a) first and (b) second order features
English-French English-Spanish English-Greek English-Japanese
acc@1 acc@20 acc@1 acc@20 acc@1 acc@20 acc@1 acc@20
LogReg 0.45 0.61 0.42 0.62 0.3 0.48 0.25 0.41
RF 0.47 0.58 0.43 0.59 0.23 0.41 0.15 0.24
SVM-RBF 0.38 0.51 0.33 0.53 0.1 0.25 0.06 0.16
Table 3: Top-1 (acc@1) and top-20 (acc@20) translation accuracy of LogReg, RF and SVM-RBF
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
en-fr en-es%
 top
-20 
tran
slati
on a
ccur
acy
LogRegRFSVM-RBF
Contextupper bound
(a) Test terms with frequency [20, 200]
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
en-fr en-es%
 top
-20 
tran
slati
on a
ccur
acy
LogRegRFSVM-RBF
Contextupper bound
(b) Test terms with frequency [10, 20]
Figure 3: Top-20 translation accuracy of terms in the frequency range of [10, 200] and [10, 20]
French are 0.75 and 0.71, respectively. In contrast
to context vectors, the character n-gram methods
performed comparably.
A second factor that affects the performance of
context vectors, is the frequency of the terms to
be translated. The translation of rare terms has
been shown to be a challenging case for context
vectors. For example, Morin and Daille (2010)
reported low accuracy (21% for the top-20 can-
didates) of context vectors for terms occurring 20
times or less. In our experiments, Figure 3b illus-
trates accuracies achieved for less frequent terms
([10, 20]). The performance of context vectors is
significantly lower, 26% for English-Spanish and
21% for English-French. Furthermore, the trans-
lation accuracy of the n-gram methods decreases
slightly (? 5% to 8%). This can be explained
by the decrease of the upper bound for lower fre-
quency terms (? 3% to 6%).
5.3 Combining internal and contextual
similarity
We have hypothesised that the compositional and
contextual clue are orthogonal, i.e., they convey
1708
 0.1 0.2
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9
en-fr en-es en-el en-jpn%
 top
-1 tr
ansl
atio
n ac
cura
cy
LogRegLogReg+ContextRFRF+Context
SVM-RBFSVM-RBF+ContextContextupper bound
(a) Top-20 accuracy (acc@20)
 0.1 0.2
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9
en-fr en-es en-el en-jpn%
 top
-1 tr
ansl
atio
n ac
cura
cy
LogRegLogReg+ContextRFRF+Context
SVM-RBFSVM-RBF+ContextContextupper bound
(b) Top-1 accuracy (acc@1)
Figure 4: Overall performance. Top-20 and top-1 translation accuracy
different and possibly complimentary information.
To investigate this intuition, we evaluate the hybrid
model on all four comparable corpora, for term oc-
currence frequencies in [20, 200].
Figure 4a illustrates top-20 translation accu-
racy scores for (a) the character n-gram models,
(b) the context vector method and (c) the hy-
brid models, i.e., LogReg+Context, RF+Context,
SVM-RBF+Context. We observe that the com-
bination of the compositional and contextual clue
improved the performance of all methods. The hy-
brid model largely improved the performance of
the SVM-RBF (? 14% to 20%). With regards
to the combined signals the translation accuracy
of LogReg and RF increased by ? 4% for the
English-Japanese corpus and ? 8% for all other
corpora.
For the top 1 candidate translation, we observe
in Figure 4 smaller improvements achieved by the
hybrid model in comparison to the top-20 accu-
racy. Interestingly, the RF classifier performed
slightly better on its own for English-French,
English-Spanish and English-Japanese. This in-
dicates that the hybrid method ranks more correct
translations in the top 20 candidates but it does not
always assign the best score to the correct answer.
6 Discusion and Future work
In this paper, we investigated a compositional
and a context-based approach useful for compil-
ing bilingual dictionaries of terms automatically
from comparable corpora. Compositional transla-
tion methods exploit the internal structure of terms
across languages while context-based approaches
investigate the surrounding lexical context.
We proposed a character n-gram composi-
tional method, i.e., a Logistic Regression clas-
sifier, which uses a multilingual representation,
i.e., source and target terms. Experimental evi-
dence showed that the LogReg classifier signifi-
cantly outperformed the baseline methods on dis-
tant languages. For closely related languages, Lo-
gReg performed comparably to an existing n-gram
method based on a Random Forest classifier.
Furthermore, we compared the n-gram models
against a context-based approach under different
corpus-specific parameters: (a) corpus compara-
bility, which is relevant to the seed dictionary, and
(b) the occurrence frequency of the terms to be
translated. It was shown that the performance of
n-gram methods was not affected by different pa-
rameter settings. Only small fluctuations were ob-
served, since the n-gram methods are based on
corpus-independent features, only. In contrast,
the context-based method was affected by corpus
comparability scores. The corresponding transla-
tion accuracy declined significantly for rare terms.
Finally, we hypothesised that the n-gram and
context-based methods provide complimentary in-
formation. To test this hypothesis, we developed a
hybrid method that combines compositional and
contextual similarity scores as features in a lin-
ear classifier. The hybrid model achieved signif-
icantly better top-20 translation accuracy than the
two methods separately but minor improvements
were observed in terms of top-1 accuracy.
As future work, we plan to improve the qual-
ity of the extracted dictionary further by exploiting
additional translation signals. For example, previ-
ous works (Schafer and Yarowsky, 2002; Klemen-
tiev et al., 2012) have reported that the temporal
and topic similarity are clues that indicate transla-
tion equivalence. It would be interesting to investi-
gate the contribution of different clues for various
1709
experimental parameters, e.g., domain, distance of
languages, types of comparable corpora.
Acknowledgements
The authors would like to thank Dr. Danushka
Bollegala for providing feedback on this paper
and the three anonymous reviewers for their useful
comments and suggestions. This work was funded
by the European Community?s Seventh Frame-
work Program (FP7/2007-2013) [grant number
318736 (OSSMETER)].
References
Daniel Andrade, Tetsuya Nasukawa, and Jun?ichi Tsu-
jii. 2010. Robust measurement and comparison of
context similarity for finding translation pairs. In
Proceedings of the 23rd International Conference
on Computational Linguistics, pages 19?27. Asso-
ciation for Computational Linguistics.
Marianna Apidianaki, Nikola Ljube?sic, and Darja
Fi?ser. 2012. Disambiguating vectors for bilin-
gual lexicon extraction from comparable corpora.
In Eighth Language Technologies Conference, pages
10?15.
Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In ACM SIGIR
Forum, volume 31, pages 84?91. ACM.
Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical termi-
nology. Nucleic acids research, 32(suppl 1):D267?
D270.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45:5?32.
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for Candidate Translational Equivalents in
Specialized, Comparable Corpora. In International
Conference on Computational Linguistics.
Ido Dagan and Ken Church. 1994. Termight: Identi-
fying and translating technical terminology. In Pro-
ceedings of the fourth conference on Applied natural
language processing, pages 34?40. Association for
Computational Linguistics.
Emmanuel Morin B??eatrice Daille. 2012. Revising the
compositional method for terminology acquisition
from comparable corpora. COLING 2012, 1810.
Estelle Delpech, B?eatrice Daille, Emmanuel Morin,
and Claire Lemaire. 2012. Extraction of domain-
specific bilingual lexicon from comparable corpora:
Compositional translation and ranking. In COLING,
pages 745?762.
Igakuyo Denshika and Jisho Kenkyukai. 1991.
250,000 medical term dictionary (in japanese).
Nichigai Associates, Inc.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational lin-
guistics, 19(1):61?74.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Pascale Fung and Kathleen McKeown. 1997. A
technical word-and term-translation aid using noisy
parallel corpora across language groups. Machine
Translation, 12(1-2):53?87.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 414?420. Association for Computational Lin-
guistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
Z.S. Harris. 1954. Distributional structure. Word.
Trevor Hastie, Robert Tibshirani, Jerome Friedman,
T Hastie, J Friedman, and R Tibshirani. 2009. The
elements of statistical learning, volume 2. Springer.
Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of NAACL-
HLT, pages 518?523.
Edward L Keenan and Leonard M Faltz. 1985.
Boolean semantics for natural language, volume 23.
Springer.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 130?140. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
1710
pages 177?180. Association for Computational Lin-
guistics.
G. Kontonatsios, I. Korkontzelos, J. Tsujii, and S. Ana-
niadou. 2014. Using a random forest classifier
to compile bilingual dictionaries of technical terms
from comparable corpora. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, volume 2:
Short Papers, pages 111?116. Association for Com-
putational Linguistics.
Sadao Kurohashi and Daisuke Kawahara. 2005.
Japanese morphological analysis system juman ver-
sion 5.1 manual.
Bo Li and Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 644?652. Association for Computational
Linguistics.
Christian Lovis, R Baud, PA Michel, JR Scherrer, and
AM Rassinoux. 1997. Building medical dictionar-
ies for patient encoding systems: A methodology. In
Artificial Intelligence in Medicine, pages 373?380.
Springer.
Emmanuel Morin and B?eatrice Daille. 2010. Com-
positionality and lexical alignment of multi-word
terms. Language Resources and Evaluation, 44(1-
2):79?95.
Emmanuel Morin and Emmanuel Prochasson. 2011a.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34. Association for Computational Lin-
guistics.
Emmanuel Morin and Emmanuel Prochasson. 2011b.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34, Portland, Oregon, June. Association
for Computational Linguistics.
Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable corpora.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 664?
671, Prague, Czech Republic, June. Association for
Computational Linguistics.
Fiammetta Namer and Robert Baud. 2007. Defin-
ing and relating biomedical terms: towards a cross-
language morphosemantics-based system. Interna-
tional Journal of Medical Informatics, 76(2):226?
233.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Harris Papageorgiou, Prokopis Prokopidis, Voula
Giouli, and Stelios Piperidis. 2000. A unified pos
tagging architecture and its application to greek. In
Proceedings of the 2nd Language Resources and
Evaluation Conference, pages 1455?1462, Athens,
June. European Language Resources Association.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned compara-
ble documents. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1327?1335. Association for Computational
Linguistics.
James Pustejovsky, Jose Castano, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym-meaning pairs from
medline databases. Studies in health technology and
informatics, (1):371?375.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 519?526. Associ-
ation for Computational Linguistics.
Xavier Robitaille, Yasuhiro Sasaki, Masatsugu
Tonoike, Satoshi Sato, and Takehito Utsuro. 2006.
Compiling french-japanese terminologies from the
web. In EACL.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In proceedings of the 6th con-
ference on Natural language learning-Volume 20,
pages 1?7. Association for Computational Linguis-
tics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49. Manch-
ester, UK.
Frank Smadja, Kathleen R McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Compu-
tational linguistics, 22(1):1?38.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 24?36. Associa-
tion for Computational Linguistics.
Takaaki Tanaka. 2002. Measuring the similarity be-
tween compound nouns in different languages us-
ing non-parallel corpora. In Proceedings of the
19th international conference on Computational
linguistics-Volume 1, pages 1?7. Association for
Computational Linguistics.
1711
J?org Tiedemann. 2009. News from opus-a collection
of multilingual parallel corpora with tools and in-
terfaces. In Recent Advances in Natural Language
Processing, volume 5, pages 237?248.
Pim Van der Eijk. 1993. Automating the acquisition of
bilingual terminology. In Proceedings of the sixth
conference on European chapter of the Association
for Computational Linguistics, pages 113?119. As-
sociation for Computational Linguistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ?10, pages 1263?1271, Stroudsburg, PA,
USA. Association for Computational Linguistics.
1712
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 111?116,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Using a Random Forest Classifier to Compile Bilingual Dictionaries of
Technical Terms from Comparable Corpora
Georgios Kontonatsios
1,2
Ioannis Korkontzelos
1,2
Jun?ichi Tsujii
3
Sophia Ananiadou
1,2
National Centre for Text Mining, University of Manchester, Manchester, UK
1
School of Computer Science, University of Manchester, Manchester, UK
2
Microsoft Research Asia, Beijing, China
3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
We describe a machine learning approach,
a Random Forest (RF) classifier, that is
used to automatically compile bilingual
dictionaries of technical terms from com-
parable corpora. We evaluate the RF clas-
sifier against a popular term alignment
method, namely context vectors, and we
report an improvement of the translation
accuracy. As an application, we use the
automatically extracted dictionary in com-
bination with a trained Statistical Machine
Translation (SMT) system to more accu-
rately translate unknown terms. The dic-
tionary extraction method described in this
paper is freely available
1
.
1 Background
Bilingual dictionaries of technical terms are im-
portant resources for many Natural Language
Processing (NLP) tasks including Statistical Ma-
chine Translation (SMT) (Och and Ney, 2003) and
Cross-Language Information Retrieval (Balles-
teros and Croft, 1997). However, manually cre-
ating and updating such resources is an expensive
process. In addition to this, new terms are con-
stantly emerging. Especially in the biomedical
domain, which is the focus of this work, there is
a vast number of neologisms, i.e., newly coined
terms, (Pustejovsky et al., 2001).
Early work on bilingual lexicon extraction
focused on clean, parallel corpora providing
satisfactory results (Melamed, 1997; Kay and
R?oscheisen, 1993). However, parallel corpora are
expensive to construct and for some domains and
language pairs are scarce resources. For these rea-
sons, the focus has shifted to comparable corpora
1
http://personalpages.manchester.
ac.uk/postgrad/georgios.kontonatsios/
Software/RF-TermAlign.tar.gz
that are more readily available, more up-to-date,
larger and cheaper to construct than parallel data.
Comparable corpora are collections of monolin-
gual documents in a source and target language
that share the same topic, domain and/or docu-
ments are from the same period, genre and so
forth.
Existing methods for bilingual lexicon extrac-
tion from comparable corpora are mainly based
on the same principle. They hypothesise that a
word and its translation tend to appear in simi-
lar lexical context (Fung and Yee, 1998; Rapp,
1999; Morin et al., 2007; Chiao and Zweigen-
baum, 2002). Context vector methods are reported
to achieve robust performance on terms that occur
frequently in the corpus. Chiao and Zweigenbaum
(2002) achieved a performance of 94% accuracy
on the top 20 candidates when translating high fre-
quency, medical terms (frequency of 100 or more).
In contrast, Morin and Daille (2010) reported an
accuracy of 21% for multi-word terms occurring
20 times or less, noting that translating rare terms
is a challenging problem for context vectors.
Kontonatsios et al. (2013) introduced an RF
classifier that is able to automatically learn as-
sociation rules of textual units between a source
and target language. However, they applied their
method only on artificially constructed datasets
containing an equal number of positive and neg-
ative instances. In the case of comparable cor-
pora, the datasets are highly unbalanced (given n,
m source and target terms respectively, we need to
classify n?m instances). In this work, we incor-
porate the classification margin into the RF model,
to allow the method to cope with the skewed dis-
tribution of positive and negative instances that oc-
curs in comparable corpora.
Our proposed method ranks candidate transla-
tions using the classification margin and suggests
as the best translation the candidate with the max-
imum margin. We evaluate our method on an
111
English-Spanish comparable corpus of Wikipedia
articles that are related to the medical sub-domain
of ?breast cancer?. Furthermore, we show that dic-
tionaries extracted from comparable corpora can
be used to dynamically augment an SMT sys-
tem in order to better translate Out-of-Vocabulary
(OOV) terms.
2 Methodology
A pair of terms in a source and target language is
represented as a feature vector where each dimen-
sion corresponds to a unique character n-gram.
The value of each dimension is 0 or 1 and desig-
nates the occurrence of the corresponding n-gram
in the input terms. The feature vectors that we
use contain 2q dimensions where the first q dimen-
sions correspond to the n-gram features extracted
from the source terms and the last q dimensions to
those from the target terms. In the reported experi-
ments, we use the 600 (300 source and 300 target)
most frequently occurring n-grams.
The underlying mechanism that allows the RF
method to learn character gram mappings between
terms of a source and target language is the de-
cision trees. A node in the decision tree is a
unique character n-gram. The nodes are linked
through the branches of the trees and therefore the
two sub-spaces of q source and q target charac-
ter grams are combined. Each decision tree in the
forest is constructed as follows: every node is split
by considering |?| random n-gram features of the
initial feature set ?, and a decision tree is fully
grown. This process is repeated |? | times and con-
structs |? | decision trees. We tuned the RF clas-
sifier using 140 random trees where we observed
a plateau in the classification performance. Fur-
thermore, we set the number of random features
using |?| = log
2
|?|+ 1 as suggested by Breiman
(2001).
The classification margin that we use to rank
the candidate translations is calculated by simply
subtracting the average number of trees predicting
that the input terms are not translations from the
average number of decision trees predicting that
the terms are mutual translations. A larger classi-
fication margin means that more decision trees in
the forest classify an instance as a translation pair.
For training an RF model, we use a bilingual
dictionary of technical terms. When the dictionary
lists more than one translation for an English term,
we randomly select only one. Negative instances
are created by randomly matching non-translation
pairs of terms. We used an equal number of posi-
tive and negative instances for training the model.
Starting from 20, 000 translation pairs we gener-
ated a training dataset of 40, 000 positive and neg-
ative instances.
2.1 Baseline method
The context projection method was first pro-
posed by (Fung and Yee, 1998; Rapp, 1999) and
since then different variations have been suggested
(Chiao and Zweigenbaum, 2002; Morin et al.,
2007; Andrade et al., 2010; Morin and Prochas-
son, 2011). Our implementation more closely
follows the context vector method introduced by
(Morin and Prochasson, 2011).
As a preprocessing step, stop words are re-
moved using an online list
2
and lemmatisation
is performed using TreeTagger (Schmid, 1994) on
both the English and Spanish part of the compa-
rable corpus. Afterwards, the method proceeds
in three steps. Firstly, for each source and target
term of the comparable corpus, i.e., i, we collect
all lexical units that: (a) occur within a window
of 3 words around i (a seven-word window) and
(b) are listed in the seed bilingual dictionary. The
lexical units that satisfy the above two conditions
are the dimensions of the context vectors. Each
dimension has a value that indicates the correla-
tion between the context lexical unit and the term
i. In our approach, we use the log-likelihood ra-
tio. In the second step, the seed dictionary is used
to translate the lexical units of the Spanish context
vectors. In this way the Spanish and English vec-
tors become comparable. When several transla-
tions are listed in the seed dictionary, we consider
all of them. In the third step, we compute the con-
text similarity, i.e., distance metric, between the
vector of an English term to be translated with ev-
ery projected, Spanish context vector. For this we
use the cosine similarity.
3 Experiments
In this section, we evaluate the two dictionary ex-
traction methods, namely context vectors and RF,
on a comparable corpus of Wikipedia articles.
For the evaluation metric, we use the top-k
translation accuracy
3
and the mean reciprocal
2
http://members.unine.ch/jacques.savoy/clef/index.html
3
the percentage of English terms whose top k candidates
contain a correct translation
112
rank (MRR)
4
as in previous approaches (Chiao
and Zweigenbaum, 2002; Chiao and Zweigen-
baum, 2002; Morin and Prochasson, 2011; Morin
et al., 2007; Tamura et al., 2012). As a refer-
ence list, we use the UMLS metathesaurus
5
. In
addition to this, considering that in several cases
the dictionary extraction methods retrieved syn-
onymous translations that do not appear in the ref-
erence list, we manually inspected the answers.
Finally, unlike previous approaches (Chiao and
Zweigenbaum, 2002), we do not restrict the test
list only to those English terms whose Spanish
translations are known to occur in the target cor-
pus. In such cases, the performance of dictionary
extraction methods have been shown to achieve a
lower performance (Tamura et al., 2012).
3.1 Data
We constructed a comparable corpus of Wikipedia
articles. For this, we used Wikipedia?s search en-
gine
6
and submitted the queries ?breast cancer?
and ?c?ancer de mama? for English and Spanish
respectively. From the returned list of Wikipedia
pages, we used the 1, 000 top articles for both lan-
guages.
The test list contains 1, 200 English single-word
terms that were extracted by considering all nouns
that occur more than 10 but not more than 200
times and are listed in UMLS. For the Spanish part
of the corpus, we considered all nouns as candi-
date translations (32, 347 in total).
3.2 Results
Table 1 shows the top-k translation accuracy and
the MRR of RF and context vectors.
Acc
1
Acc
10
Acc
20
MRR
RF 0.41 0.57 0.59 0.47
Cont.
Vectors 0.1 0.21 0.26 0.11
Table 1: top-k translation accuracy and MRR of
RF and context vectors on 1, 200 English terms
We observe that the proposed RF method
achieves a considerably better top-k translation ac-
4
MRR =
1
|Q|
?
Q
i=1
1
rank
i
where |Q| is the number of
English terms for which we are extracting translations and
rank
i
is the position of the first correct translation from re-
turned list of candidates
5
nlm.nih.gov/research/umls
6
http://en.wikipedia.org/wiki/Help:Searching
curacy and MRR than the baseline method. More-
over, we segmented the 1, 200 test terms into 7
frequency ranges
7
, from high-frequency to rare
terms. Figure 1 shows the translation accuracy at
top 20 candidates for the two methods. We note
Figure 1: Translation accuracy of top 20 candi-
dates on different frequency ranges
that for high frequency terms, i.e. [100,200] range,
the performance achieved by the two methods is
similar (53% and 52% for the RF and context vec-
tors respectively). However, for lower frequency
terms, the translation accuracy of the context vec-
tors continuously declines. This confirms that con-
text vectors do not behave robustly for rare terms
(Morin and Daille, 2010). In contrast, the RF
slightly fluctuates over different frequency ranges
and presents approximately the same translation
accuracy for both frequent and rare terms.
4 Application
As an application of our method, we use the pre-
viously extracted dictionaries to on-line augment
the phrase table of an SMT system and observe
the translation performance on test sentences that
contain OOV terms. For the translation probabil-
ities in the phrase table, we use the distance met-
ric given by the dictionary extraction methods i.e.,
classification margin and cosine similarity of RF
and context vectors respectively, normalised by
the uniform probability (if a source term has m
candidate translations, we normalise the distance
metric by dividing by m as in (Wu et al., 2008) .
4.1 Data and tools
We construct a parallel, sentence-aligned corpus
from the biomedical domain, following the pro-
cess described in (Wu et al., 2011; Yepes et al.,
2013). The parallel corpus comprises of article ti-
tles indexed by PubMed in both English and Span-
ish. We collect 120K parallel sentences for train-
7
each frequency range contains 100 randomly sampled
terms
113
ing the SMT and 1K sentences for evaluation. The
test sentences contain 1, 200 terms that do not ap-
pear in the training parallel corpus. These terms
occur in the Wikipedia comparable corpus. Hence,
the previously extracted dictionaries list a possible
translation. Using the PubMed parallel corpus, we
train Moses (Koehn et al., 2007), a phrase-based
SMT system.
4.2 Results
We evaluated the translation performance of the
SMT that uses the dictionary extracted by the RF
against the following baselines: (i) Moses using
only the training parallel data (Moses), (ii) Moses
using the dictionary extracted by context vectors
(Moses+context vector). The evaluation metric is
BLEU (Papineni et al., 2002).
Table 2 shows the BLEU score achieved by the
SMT systems when we append the top-k transla-
tions to the phrase table.
BLEU
on top-k translations
1 10 20
Moses 24.22 24.22 24.22
Moses+
RF 25.32 24.626 24.42
Moses+
Context Vectors 23.88 23.69 23.74
Table 2: Translation performance when adding
top-k translations to the phrase table
We observe that the best performance is
achieved by the RF when we add the top 1 trans-
lation with a total gain of 1.1 BLEU points over
the baseline system. In contrast, context vec-
tors decreased the translation performance of the
SMT system. This indicates that the dictionary ex-
tracted by the context vectors is too noisy and as
a result the translation performance dropped. Fur-
thermore, it is noted that the augmented SMT sys-
tems achieve the highest performance for the top 1
translation while for k greater than 1, the transla-
tion performance decreases. This behaviour is ex-
pected since the target language model was trained
only on the training Spanish sentences of the par-
allel corpus. Hence, the target language model
does not have a prior knowledge of the OOV trans-
lations and as a result it cannot choose the correct
translation among k candidates.
To further investigate the effect of the language
model on the translation performance of the aug-
mented SMT systems, we conducted an oracle ex-
periment. In this ideal setting, we assume a strong
language model, that is trained on both training
and test Spanish sentences of the parallel corpus,
in order to assign a higher probability to a correct
translation if it exists in the deployed dictionary.
As we observe in Table 3, a strong language model
can more accurately select the correct translation
among top-k candidates. The dictionary extracted
by the RF improved the translation performance
by 2.5 BLEU points for the top-10 candidates and
context vectors by 0.45 for the top-20 candidates.
BLEU
on top-k translations
1 10 20
Moses 28.85 28.85 28.85
Moses+
RF 30.98 31.35 31.2
Moses+
Context Vectors 28.18 29.17 29.3
Table 3: Translation performance when adding
top-k translations to the phrase table. SMT sys-
tems use a language model trained on training and
test Spanish sentences of the parallel corpus.
5 Discussion
In this paper, we presented an RF classifier that
is used to extract bilingual dictionaries of techni-
cal terms from comparable corpora. We evaluated
our method on a comparable corpus of Wikipedia
articles. The experimental results showed that our
proposed method performs robustly when translat-
ing both frequent and rare terms.
As an application, we used the automatically
extracted dictionary to augment the phrase table of
an SMT system. The results demonstrated an im-
provement of the overall translation performance.
As future work, we plan to integrate the RF clas-
sifier with context vectors. Intuitively, the two
methods are complementary considering that the
RF exploits the internal structure of terms while
context vectors use the surrounding lexical con-
text. Therefore, it will be interesting to investigate
how we can incorporate the two feature spaces in
a machine learner.
114
6 Acknowledgements
This work was funded by the European Commu-
nity?s Seventh Framework Program (FP7/2007-
2013) [grant number 318736 (OSSMETER)].
References
Daniel Andrade, Tetsuya Nasukawa, and Jun?ichi Tsu-
jii. 2010. Robust measurement and comparison of
context similarity for finding translation pairs. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 19?
27, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In ACM SIGIR
Forum, volume 31, pages 84?91. ACM.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45:5?32.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for Candidate Translational Equivalents in
Specialized, Comparable Corpora. In International
Conference on Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 414?420. Association for Computational Lin-
guistics.
Martin Kay and Martin R?oscheisen. 1993. Text-
translation alignment. computational Linguistics,
19(1):121?142.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Georgios Kontonatsios, Ioannis Korkontzelos, Jun?ichi
Tsujii, and Sophia Ananiadou. 2013. Using ran-
dom forest to recognise translation equivalents of
biomedical terms across languages. In Proceed-
ings of the Sixth Workshop on Building and Using
Comparable Corpora, pages 95?104. Association
for Computational Linguistics, August.
I. Dan Melamed. 1997. A portable algorithm for map-
ping bitext correspondence. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 305?312. Association for
Computational Linguistics.
Emmanuel Morin and B?eatrice Daille. 2010. Com-
positionality and lexical alignment of multi-word
terms. Language Resources and Evaluation, 44(1-
2):79?95.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34, Portland, Oregon, June. Association
for Computational Linguistics.
Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable corpora.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 664?
671, Prague, Czech Republic, June. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
James Pustejovsky, Jose Castano, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym-meaning pairs from
medline databases. Studies in health technology and
informatics, (1):371?375.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 519?526. Associ-
ation for Computational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49. Manch-
ester, UK.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 24?36. Associa-
tion for Computational Linguistics.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume
1, pages 993?1000. Association for Computational
Linguistics.
115
Cuijun Wu, Fei Xia, Louise Deleger, and Imre Solti.
2011. Statistical machine translation for biomedical
text: are we there yet? In AMIA Annual Sympo-
sium Proceedings, volume 2011, page 1290. Ameri-
can Medical Informatics Association.
Antonio Jimeno Yepes,
?
Elise Prieur-Gaston, and
Aur?elie N?ev?eol. 2013. Combining medline and
publisher data to create parallel corpora for the auto-
matic translation of biomedical text. BMC bioinfor-
matics, 14(1):146.
116
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43?48,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extending an interoperable platform to facilitate the creation
of multilingual and multimodal NLP applications
Georgios Kontonatsios?, Paul Thompson?, Riza Theresa Batista-Navarro?,
Claudiu Miha?ila??, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
131 Princess Street, Manchester M1 7DN, UK
{kontonag,batistar,thompsop,mihailac,
korkonti,ananiads}@cs.man.ac.uk
Abstract
U-Compare is a UIMA-based workflow
construction platform for building natu-
ral language processing (NLP) applica-
tions from heterogeneous language re-
sources (LRs), without the need for pro-
gramming skills. U-Compare has been
adopted within the context of the META-
NET Network of Excellence, and over
40 LRs that process 15 European lan-
guages have been added to the U-Compare
component library. In line with META-
NET?s aims of increasing communication
between citizens of different European
countries, U-Compare has been extended
to facilitate the development of a wider
range of applications, including both mul-
tilingual and multimodal workflows. The
enhancements exploit the UIMA Subject
of Analysis (Sofa) mechanism, that allows
different facets of the input data to be rep-
resented. We demonstrate how our cus-
tomised extensions to U-Compare allow
the construction and testing of NLP appli-
cations that transform the input data in dif-
ferent ways, e.g., machine translation, au-
tomatic summarisation and text-to-speech.
1 Introduction
Currently, there are many repositories that con-
tain a range of NLP components, e.g., OpenNLP1,
Stanford CoreNLP2, JULIE NLP Toolsuite3 and
NaCTeM software tools4. The ability to chain
components from these repositories into pipelines
is a prerequisite to facilitate the development of
?The authors have contributed equally to the development
of this work and production of the manuscript.
1http://opennlp.sourceforge.net/projects.html
2http://nlp.stanford.edu/software/corenlp.shtml
3http://www.julielab.de/Resources/Software/NLP Tools.html
4http://nactem.ac.uk/software.php
complex NLP applications. Combining together
heterogeneous components is not, however, al-
ways straightforward. The various components
used in a pipeline may be implemented using dif-
ferent programming languages, may have incom-
patible input/output formats, e.g., stand-off or in-
line annotations, or may require or produce incom-
patible data types, e.g., a particular named entity
recogniser (NER) may require specific types of
syntactic constituents as input, making it impor-
tant to choose the right type of syntactic parser to
run prior to the NER. Thus, the tools required to
build a new application may not be interoperable
with each other, and considerable extra work may
be required to make the tools talk to each other.
The Unstructured Information Management Ar-
chitecture (UIMA) (Ferrucci and Lally, 2004) was
created as a means to alleviate such problems. It
is a framework that facilitates the straightforward
combination of LRs, i.e., tools and corpora, into
workflow applications. UIMA is an OASIS stan-
dard that enables interoperability of LRs by defin-
ing a standard workflow metadata format and stan-
dard input/output representations.
U-Compare (Kano et al, 2011) is a graphical
NLP workflow construction platform built on top
of UIMA. It facilitates the rapid construction, test-
ing and evaluation of NLP workflows using drag-
and-drop actions within its graphical user inter-
face (GUI). U-Compare enhances interoperabil-
ity among UIMA-compliant LRs, by defining a
common and sharable Type System, i.e., a hier-
archy of annotation types, which models a wide
range of NLP data types, e.g., sentence, token,
part-of-speech tag, named entity and discourse
annotations. The aim is for all components in
U-Compare?s library to be compliant with this
type system. In the context of META-NET, U-
Compare?s library has been extended with 46 new
LRs supporting 15 European languages, all of
which are compliant with the same type system.
43
This makes U-Compare the world?s largest repos-
itory of type system-compatible LRs, allowing
users to seamlessly combine together resources to
create a range of NLP applications.
Previously, U-Compare was able to support the
development of a wide range of monolingual lex-
ical, syntactic and semantic processing tasks ap-
plications that enriched textual input documents
by adding annotations of various types. However,
not all NLP applications operate in this way; some
workflows transform the input data to create new
?views? of the input data. The META-NET project
aims to ensure equal access to information by all
European citizens. This aim implies the devel-
opment of both multilingual applications, which
transform input data from one language into an-
other, or multimodal applications, in which text
may be transformed into speech, or vice versa.
U-Compare has been extended in several ways
to support the construction of these more complex
workflow types. Specifically, information about
both the original and transformed data, together
with annotations associated with each view, can
now be visualised in a straightforward manner.
The changes support two new categories of work-
flow. Firstly, workflows that produce two or more
textual views of an input text are useful not only
for multilingual applications, such as those that
carry out machine translation, but also applica-
tions that transform the input text in other ways,
such as those that produce a summary of an in-
put text. Secondly, workflows that output audio as
well as textual views, e.g., text-to-speech applica-
tions, are also supported.
2 Related work
Over the past few years, an increasing num-
bers of researchers have begun to create and dis-
tribute their own workflow construction architec-
tures (Ferrucci and Lally, 2004; Cunningham et
al., 2002; Grishman et al, 1997; Scha?fer, 2006)
or platforms (Kano et al, 2011; Rak et al, 2012;
Ogrodniczuk and Karagiozov, 2011; Savova et al,
2010) that allow the rapid development of NLP ap-
plications.
GATE (Cunningham et al, 2002) is a workflow
construction framework that has been used to de-
velop several types of NLP applications, including
summarisation systems. It facilitates the develop-
ment of a wide range of NLP applications by pro-
viding a collection of components that can process
various languages, together with Java libraries that
handle character encoding for approximately 100
languages. However, GATE does not formally de-
fine any standards to model multilingual or mul-
timodal applications, but rather aims to boost the
development process of NLP applications.
TIPSTER (Grishman et al, 1997) is a generic
framework for the development of NLP applica-
tions. TIPSTER provides multilingual function-
alities by associating text segments of a paral-
lel document with one or more languages. This
allows language-dependent NLP components to
process only the appropriate mono-lingual sub-
documents. However, TIPSTER does not provide
explicit guidelines regarding the annotation types
and attributes that are produced by components.
This lack of a common and sharable system of
annotation types discourages interoperability be-
tween LRs. However, TIPSTER does not provide
a mechanism that facilitates the development of
multilingual or multimodal NLP applications.
Heart of Gold (Scha?fer, 2006) is an XML-
based workflow construction architecture that en-
ables interoperability of tools developed in dif-
ferent programming languages to be combined
into pipelines. Heart of Gold contains a rich li-
brary of shallow and deep parsing components
supporting several languages, e.g., English, Ger-
man, Japanese and Greek. Nonetheless, Heart of
Gold does not specifically support the construction
of multilingual or multimodal workflows.
In contrast to the other frameworks introduced
above, UIMA (Ferrucci and Lally, 2004) provides
an abstract-level mechanism that can be used to
support the development of workflows that carry
out transformations of the input data. This mech-
anism is called the Subject of Analysis or Sofa.
Multiple Sofas can be linked with an input file,
each of which stores different data and associ-
ated annotations. This mechanism can thus be ex-
ploited to represent alternative ?views? of the in-
put data, such as a source text and its translation.
The data stored in different Sofas is not restricted
to textual information; it can also correspond to
other modalities, such as audio data. This makes
the Sofa mechanism equally suitable for storing
the output of text-to-speech workflows. Our ex-
tensions to U-Compare are thus implemented by
reading and displaying the contents of different
types of Sofas.
The Sofa mechanism has previously been
44
under-exploited by UIMA developers, despite its
power in allowing more complex NLP workflows
to be constructed. Indeed, no other existing
UIMA-based platform (Kano et al, 2011; Rak et
al., 2012; Savova et al, 2010; Hahn et al, 2008)
has demonstrated the use of Sofas to construct
multilingual or multimodal applications. Thus, to
our knowledge, our enhancements to U-Compare
constitute the first attempt to make the construc-
tion of workflows that carry out transformations of
input data more readily available to UIMA users,
without the need for programming skills.
3 METANET4U Components in
U-Compare
The two dozen national and many regional lan-
guages of Europe present linguistic barriers that
can severely limit the free flow of goods, infor-
mation and services. The META-NET Network
of Excellence was created to respond to this is-
sue. Consisting of 60 research centres from 34
countries, META-NET has aimed to stimulate a
concerted, substantial and continent-wide effort to
push forward language technology research and
engineering, in order to ensure equal access to
information and knowledge for all European cit-
izens.
META-NET?s aims are dependent on the ready
availability of LRs that can carry out NLP and
text mining (TM) on a range of European lan-
guages. Such resources constitute the building
blocks for constructing language technology ap-
plications that can help European citizens to gain
easy access to the information they require. One
of the major outcomes of META-NET has been
the development of META-SHARE, an open, dis-
tributed facility for sharing and exchange of LRs
in a large number of European languages.
Within the context of META-NET, interoper-
ability of LRs is clearly of utmost importance, to
expedite the process of developing new NLP ap-
plications. In order to provide a concrete demon-
stration of the utility and power of promoting in-
teroperability within META-SHARE, one of the
sub-projects of META-NET, i.e., METANET4U,
has carried out a pilot study on interoperability,
making use of the UIMA framework and the U-
Compare platform. It is in this context that a set
of 46 new LRs, available in META-SHARE, were
wrapped as UIMA components and made avail-
able in U-Compare. Of these components, 37 op-
erate on one or more specific languages other than
English and 4 are language-independent. Table 1
shows the full set of categories of UIMA com-
ponents created during the METANET4U project,
together with the languages supported.
Several of these new components output mul-
tiple Sofas, i.e., two machine translation compo-
nents, two automatic summarisation components
and a text-to-speech component. It is hoped that
our U-Compare extensions will help to stimulate
the development of a greater number of related
UIMA components, and thus promote a new level
of complexity for future UIMA workflows.
Component Function Supported Languages
Language Identifier 54 modern languages
Paragraph breaker pt, mt
Sentence splitter en, pt ,mt, es, ca, ast,cy, gl, it
Tokeniser en, pt, mt, es, ca, ast,cy, gl, it, fr
Morph. Analyser en, pt, es, ca, ast,cy, gl, it, ro, eu, fr
POS Tagger en, es, ca, cy, gl, it,pt, ro, eu, fr, mt
Syntactic chunker en, es, ca, gl,ast, ro, fr
NP chunker ro
Segmenter ro, en
FDG Parser ro
Dependency Parser en, es, ca, gl, ast
Discourse Parser ro
NER Languageindependent
Summariser ro, en
Machine translation es?{gl,pt,ca}en?es, eu?es
Table 1: METANET4U UIMA components
4 Enhancements to U-Compare
In UIMA, an artefact, i.e., raw text, audio, im-
age, video, and its annotations, e.g., part-of-
speech tags, are represented in a standard format,
namely the Common Analysis Structure (CAS).
A CAS can contain any number of smaller sub-
CASes, i.e., Sofas, that carry different artefacts
with their linked annotations. Figure 1 illustrates
the different types of Sofas that are created by the
three types of workflows that we will demonstrate.
Firstly, for a machine translation workflow, at least
45
Multi-lingualMulti-modalWorkflowsDocuments aZ ??
CAS
?CASCAS
SOFA SOFA
SOFA SOFA
SOFA SOFA
Figure 1: UIMA based multilingual and multi-
modal workflow architecture
two CAS views, i.e., Sofas, are created, the first
corresponding to the text in the source language,
and the other Sofas corresponding to the transla-
tion(s) of the source text into target language(s).
The second type of workflow, i.e., automatic sum-
marisation, is related to the former workflow, in
that the two Sofas produced by the workflow are
both textual, one containing the input text and one
containing a summary of the original text. The
third type of workflow is different, in that a Sofa
containing audio data is used to represent the out-
put of a multimodal workflow.
Two specific extensions have been made to U-
Compare to handle both textual and audio So-
fas. When the output of a workflow consists of
multiple textual views (Sofas), the default anno-
tation viewer is automatically split to allow mul-
tiple views of the text to be displayed and side-
by-side. This can be useful, e.g., to allow careful
comparison of a source text and target translation
in a machine translation workflow. To handle au-
dio Sofas, we have developed a new, customised
viewer that can visualise and play audio data. The
visualisation consists of a graphical display of the
waveform, power information and spectrogram, as
well as segmentation of the audio data into re-
gions (such as individual tokens) and transcrip-
tions, if such information is present in the audio
Sofa. The viewer makes use the open-source li-
brary Java Speech Toolkit (JSTK)5.
5 Workflow applications
In order to provide a practical demonstration of
the enhanced capabilities of U-Compare, we show
5http://code.google.com/p/jstk
three different workflows that transform the input
data in different ways, namely translation, auto-
matic summarisation and speech synthesis. In this
section, we provide brief details of these work-
flows.
5.1 Machine translation
The University of Manchester has created UIMA
wrapper components corresponding to different
modules of Apertium (Corb??-Bellot et al, 2005), a
free rule-based machine translation engine. These
components consist of a morphological analyser,
POS tagger and translator. The three components
must be run in sequence to carry out translation,
although the first two components can be used
in other workflows to carry out monolingual
analyses. The UIMA components currently
handle a subset of the 27 languages dealt with
by the complete Apertium system, corresponding
to the languages of the METANET4U partners,
i.e., English?Spanish, Galician?Spanish,
Portuguese?Spanish, Catalan?Spanish and
Basque?Spanish. However, additional language
pairs can be added straightforwardly. Our sample
workflow includes as its initial component the
Language Identifier from the Romanian Academy
Research Institute for Artificial Intelligence
(RACAI), to automatically detect the language of
the text in the input Sofa. The subsequent compo-
nents in the workflow are the Apertium modules.
The workflow demonstrates how heterogeneous
components from different research groups can
be combined into workflows to create new NLP
applications. A sample output from running the
workflow is shown in Figure 2. The input text
was detected as English by the RACAI Language
Identifier. The English text was subsequently
analysed by the morphological analyser and POS
Tagger, and translated to Spanish by the translator.
Figure 2 illustrates the side-by-side display of the
contents of the two Sofas.
5.2 Automatic summarisation
Automatic summarisation for Romanian text can
be carried out by creating a workflow consisting
of two components developed by the Universitatea
?Alexandru Ioan Cuza? din Ias?i (UAIC). Firstly,
a segmenter (UAICSeg) splits the input text into
fragments, which are in turn used as input to the
summariser component (UAICSum). The length
of the output summary (percentage of the whole
document) is parameterised. As can be seen in
46
Figure 2: Translation of English text to Spanish
Figure 3: Summarisation of Romanian text
Figure 3, the output of this workflow is displayed
using the same parallel Sofa viewer. In this case,
the full text is displayed in the left-hand pane and
the summary is shown in the right-hand pane.
5.3 Speech synthesis
The Universitat Polite`cnica de Catalunya (UPC)
developed a speech synthesiser component that
is based around their Ogmios text-to-speech sys-
tem (Bonafonte et al, 2006). The UIMA com-
ponent version of this tool generates separate text
and audio Sofas; the former stores the textual to-
kens and textual representations of their pronun-
ciations, whilst the latter stores the start and end
time offsets of each of the tokens in the audio file,
together with their transcriptions. Fig. 4 shows
how the textual Sofa information is displayed in
U-Compare?s default annotation viewer, whilst the
audio Sofa information is shown in the new au-
dio visualiser mentioned above. The three differ-
ent types of visual information are displayed be-
low each other, and the segments (tokens) of the
audio file, together with their transcriptions, are
displayed at the bottom of the window. A ?Play?
button allows either the complete file or a selected
segment to be played.
6 Conclusions
The requirements of META-NET have motivated
several new enhancements to the U-Compare plat-
form, which, to our knowledge, make it the first
UIMA-based workflow construction platform that
is fully geared towards the development of NLP
applications that support a wide range of European
languages. The 46 new UIMA-wrapped LRs that
have been made available through U-Compare,
supporting 15 different European languages and
all compliant with the same type system, mean
that the improved U-Compare is essentially a hub
of multilingual resources, which can be freely and
flexibly combined to create new workflows. In
47
Figure 4: Speech Synthesis
addition, our enhancements to U-Compare mean
that various types of multilingual and multimodal
workflows can now be created with the minimum
effort. These enhancements are intended to make
U-Compare more attractive to users, and to help
stimulate the development of a new generation of
more complex UIMA-based NLP applications. As
future work, we intend to extend the library of
components that output multiple Sofas, and further
extend the functionalities of U-Compare to handle
other data modalities, e.g., video.
Acknowledgements
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; MetaNet4U project (ICT PSP Pro-
gramme) [grant number 270893]; and Engineer-
ing and Physical Sciences Research Council [grant
numbers EP/P505631/1, EP/J50032X/1].
References
A. Bonafonte, P. Agu?ero, J. Adell, J. Pe?rez, and
A. Moreno. 2006. Ogmios: The upc text-to-speech
synthesis system for spoken translation. In TC-
STAR Workshop on Speech-to-Speech Translation,
pages 199?204.
A. Corb??-Bellot, M. Forcada, S. Ortiz-Rojas, J. Pe?rez-
Ortiz, G. Ram??rez-Sa?nchez, F. Sa?nchez-Mart??nez,
I. Alegria, A. Mayor, and K. Sarasola. 2005.
An open-source shallow-transfer machine transla-
tion engine for the romance languages of Spain. In
Proceedings of the 10th Conference of the EAMT,
pages 79?86.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an architecture for devel-
opment of robust HLT applications.
D. Ferrucci and A. Lally. 2004. Building an ex-
ample application with the unstructured information
management architecture. IBM Systems Journal,
43(3):455?475.
R. Grishman, B. Caid, J. Callan, J. Conley, H. Corbin,
J. Cowie, K. DiBella, P. Jacobs, M. Mettler, B. Og-
den, et al 1997. TIPSTER text phase ii architecture
design version 2.1 p 19 june 1996.
U. Hahn, E. Buyko, R. Landefeld, M. Mu?hlhausen,
M. Poprat, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the JULIE lab UIMA compo-
nent repository. In LREC?08 Workshop ?Towards
Enhanced Interoperability for Large HLT Systems:
UIMA for NLP?, pages 1?7, Marrakech, Morocco,
May.
Y. Kano, M. Miwa, K. Cohen, L. Hunter, S. Ananiadou,
and J. Tsujii. 2011. U-compare: A modular nlp
workflow construction and evaluation system. IBM
Journal of Research and Development, 55(3):11.
M. Ogrodniczuk and D. Karagiozov. 2011. Atlas - the
multilingual language processing platform. Proce-
samiento de Lenguaje Natural, 47(0):241?248.
R. Rak, A. Rowley, W. Black, and S. Ananiadou.
2012. Argo: an integrative, interactive, text mining-
based workbench supporting curation. Database:
The Journal of Biological Databases and Curation,
2012.
G. Savova, J. Masanz, P. Ogren, J. Zheng, S. Sohn,
K. Kipper-Schuler, and C. Chute. 2010. Mayo clin-
ical text analysis and knowledge extraction system
(ctakes): architecture, component evaluation and ap-
plications. Journal of the American Medical Infor-
matics Association, 17(5):507?513.
U. Scha?fer. 2006. Middleware for creating and com-
bining multi-dimensional nlp markup. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 81?84. ACL.
48
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 79?88,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Towards a Better Understanding of Discourse:
Integrating Multiple Discourse Annotation Perspectives Using UIMA
Claudiu Miha?ila??, Georgios Kontonatsios?, Riza Theresa Batista-Navarro?,
Paul Thompson?, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
{mihailac,kontonag,batistar,thompsop,
korkonti,ananiads}@cs.man.ac.uk
Abstract
There exist various different discourse an-
notation schemes that vary both in the
perspectives of discourse structure consid-
ered and the granularity of textual units
that are annotated. Comparison and inte-
gration of multiple schemes have the po-
tential to provide enhanced information.
However, the differing formats of cor-
pora and tools that contain or produce
such schemes can be a barrier to their
integration. U-Compare is a graphical,
UIMA-based workflow construction plat-
form for combining interoperable natu-
ral language processing (NLP) resources,
without the need for programming skills.
In this paper, we present an extension
of U-Compare that allows the easy com-
parison, integration and visualisation of
resources that contain or output annota-
tions based on multiple discourse anno-
tation schemes. The extension works by
allowing the construction of parallel sub-
workflows for each scheme within a single
U-Compare workflow. The different types
of discourse annotations produced by each
sub-workflow can be either merged or vi-
sualised side-by-side for comparison. We
demonstrate this new functionality by us-
ing it to compare annotations belonging
to two different approaches to discourse
analysis, namely discourse relations and
functional discourse annotations. Integrat-
ing these different annotation types within
an interoperable environment allows us to
study the correlations between different
types of discourse and report on the new
insights that this allows us to discover.
?The authors have contributed equally to the development
of this work and production of the manuscript.
1 Introduction
Over the past few years, there has been an increas-
ing sophistication in the types of available natural
language processing (NLP) tools, with named en-
tity recognisers being complemented by relation
and event extraction systems. Such relations and
events are not intended to be understood in isola-
tion, but rather they are arranged to form a coher-
ent discourse. In order to carry out complex tasks
such as automatic summarisation to a high degree
of accuracy, it is important for systems to be able
to analyse the discourse structure of texts automat-
ically. To facilitate the development of such sys-
tems, various textual corpora containing discourse
annotations have been made available to the NLP
community. However, there is a large amount of
variability in the types of annotations contained
within these corpora, since different perspectives
on discourse have led to the development of a
number of different annotation schemes.
Corpora containing discourse-level annotations
usually treat the text as a sequence of coherent tex-
tual zones (e.g., clauses and sentences). One line
of research has been to identify which zones are
logically connected to each other, and to charac-
terise these links through the assignment of dis-
course relations. There are variations in the com-
plexity of the schemes used to annotate these dis-
course relations. For example, Rhetorical Struc-
ture Theory (RST) (Mann and Thompson, 1988)
defines 23 types of discourse relations that are
used to structure the text into complex discourse
trees. Whilst this scheme was used to enrich the
Penn TreeBank (Carlson et al, 2001), the Penn
Discourse TreeBank (PDTB) (Prasad et al, 2008)
used another scheme to identify discourse rela-
tions that hold between pairs of text spans. It cate-
gorises the relations into types such as ?causal?,
?temporal? and ?conditional?, which can be ei-
ther explicit or implicit, depending on whether or
79
not they are represented in text using overt dis-
course connectives. In the biomedical domain, the
Biomedical Discourse Relation Bank (BioDRB)
(Prasad et al, 2011) annotates a similar set of re-
lation types, whilst BioCause focusses exclusively
on causality (Miha?ila? et al, 2013).
A second line of research does not aim to link
textual zones, but rather to classify them accord-
ing to their specific function in the discourse. Ex-
amples of functional discourse annotations include
whether a particular zone asserts new information
into the discourse or represents a speculation or
hypothesis. In scientific texts, knowing the type
of information that a zone represents (e.g., back-
ground knowledge, hypothesis, experimental ob-
servation, conclusion, etc.) allows for automatic
isolation of new knowledge claims (Sa?ndor and de
Waard, 2012). Several annotation schemes have
been developed to classify textual zones accord-
ing to their rhetorical status or general informa-
tion content (Teufel et al, 1999; Mizuta et al,
2006; Wilbur et al, 2006; de Waard and Pan-
der Maat, 2009; Liakata et al, 2012a). Related
to these studies are efforts to capture information
relating to discourse function at the level of events,
i.e., structured representations of pieces of knowl-
edge which, when identified, facilitate sophisti-
cated semantic searching (Ananiadou et al, 2010).
Since there can be multiple events in a sentence
or clause, the identification of discourse informa-
tion at the event level can allow for a more de-
tailed analysis of discourse elements than is possi-
ble when considering larger units of text. Certain
event corpora such as ACE 2005 (Walker, 2006)
and GENIA-MK (Thompson et al, 2011) have
been annotated with various types of functional
discourse information.
It has previously been shown that considering
several functional discourse annotation schemes in
parallel can be beneficial (Liakata et al, 2012b),
since each scheme offers a different perspective.
For a common set of documents, the cited study
analysed and compared functional discourse an-
notations at different levels of textual granular-
ity (i.e., sentences, clauses and events), showing
how the different schemes could complement each
other in order to lay the foundations for a possible
future harmonisation of the schemes. The results
of this analysis provide evidence that it would be
useful to carry out further such analyses involv-
ing other such schemes, including an investiga-
tion of how discourse relations and functional dis-
course annotations could complement each other,
e.g., which types of functional annotations occur
within the arguments of discourse relations. There
are, however, certain barriers to carrying out such
an analysis. For example, a comparison of an-
notation schemes would ideally allow the differ-
ent types of annotations to be visualised simul-
taneously or seamlessly merged together. How-
ever, the fact that annotations in different corpora
are encoded using different formats (e.g., stand-off
or in-line) and different encoding schemes means
that this can be problematic.
A solution to the challenges introduced above is
offered by the Unstructured Information Manage-
ment Architecture (UIMA) (Ferrucci and Lally,
2004), which defines a common workflow meta-
data format facilitating the straightforward combi-
nation of NLP resources into a workflow. Based
on the interoperability of the UIMA framework,
numerous researchers distribute their own tools as
UIMA-compliant components (Kano et al, 2011;
Baumgartner et al, 2008; Hahn et al, 2008;
Savova et al, 2010; Gurevych et al, 2007; Rak
et al, 2012b). However, UIMA is only intended
to provide an abstract framework for the interop-
erability of language resources, leaving the actual
implementation to third-party developers. Hence,
UIMA does not explicitly address interoperability
issues of tools and corpora.
U-Compare (Kano et al, 2011) is a UIMA-
based workflow construction platform that pro-
vides a graphical user interface (GUI) via which
users can rapidly create NLP pipelines using a
drag-and-drop mechanism. Conforming to UIMA
standards, U-Compare components and pipelines
are compatible with any UIMA application via a
common and sharable type system (i.e., a hier-
archy of annotation types). In defining this type
system, U-Compare promotes interoperability of
tools and corpora, by exhaustively modelling a
wide range of NLP data types (e.g., sentences, to-
kens, part-of-speech tags, named entities). This
type system was recently extended to include dis-
course annotations to model three discourse phe-
nomena, namely causality, coreference and meta-
knowledge (Batista-Navarro et al, 2013).
In this paper, we describe our extensions to U-
Compare, supporting the integration and visuali-
sation of resources annotated according to mul-
tiple discourse annotation schemes. Our method
80
decomposes pipelines into parallel sub-workflows,
each linked to a different annotation scheme.
The resulting annotations produced by each sub-
workflow can be either merged within a single
document or visualised in parallel views.
2 Related work
Previous studies have shown the advantages of
comparing and integrating different annotation
schemes on a corpus of documents (Guo et al,
2010; Liakata et al, 2010; Liakata et al, 2012b).
Guo et al (2010) compared three different dis-
course annotation schemes applied to a corpus
of biomedical abstracts on cancer risk assess-
ment and concluded that two of the schemes pro-
vide more fine-grained information than the other
scheme. They also revealed a subsumption rela-
tion between two schemes. Such outcomes from
comparing schemes are meaningful for users who
wish to select the most appropriate scheme for an-
notating their data. Liakata et al (2012) under-
line that different discourse annotation schemes
capture different dimensions of discourse. Hence,
there might be complementary information across
different schemes. Based on this hypothesis, they
provide a comparison of three annotation schemes,
namely CoreSC (Liakata et al, 2012a), GENIA-
MK (Thompson et al, 2011) and DiscSeg (de
Waard, 2007), on a corpus of three full-text pa-
pers. Their results showed that the categories in
the three schemes can complement each other. For
example, the values of the Certainty Level dimen-
sion of the GENIA-MK scheme can be used to as-
sign confidence values to the Conclusion, Result,
Implication and Hypothesis categories of CoreSC
and DiscSeg. In contrast to previous studies, our
proposed approach automatically integrates mul-
tiple annotation schemes. The proposed mecha-
nism allows users to easily compare, integrate and
visualise multiple discourse annotation schemes
in an interoperable NLP infrastructure, i.e., U-
Compare.
There are currently a number of freely-available
NLP workflow infrastructures (Ferrucci and Lally,
2004; Cunningham et al, 2002; Scha?fer, 2006;
Kano et al, 2011; Grishman, 1996; Baumgartner
et al, 2008; Hahn et al, 2008; Savova et al, 2010;
Gurevych et al, 2007; Rak et al, 2012b). Most
of the available infrastructures support the devel-
opment of standard NLP applications, e.g., part-
of-speech tagging, deep parsing, chunking, named
entity recognition and several of them allow the
representation and analysis of discourse phenom-
ena (Kano et al, 2011; Cunningham et al, 2002;
Savova et al, 2010; Gurevych et al, 2007). How-
ever, none of them has demonstrated the integra-
tion of resources annotated according to multiple
annotation schemes within a single NLP pipeline.
GATE (Cunningham et al, 2002) is an open
source NLP infrastructure that has been used for
the development of various language processing
tasks. It is packaged with an exhaustive number
of NLP components, including discourse analy-
sis modules, e.g., coreference resolution. Further-
more, GATE offers a GUI environment and wrap-
pers for UIMA-compliant components. However,
GATE implements a limited workflow manage-
ment mechanism that does not support the execu-
tion of parallel or nested workflows. In addition to
this, GATE does not promote interoperability of
language resources since it does not define any hi-
erarchy of NLP data types and components do not
formally declare their input/output capabilities.
In contrast to GATE, UIMA implements a more
sophisticated workflow management mechanism
that supports the construction of both parallel
and nested pipelines. In this paper, we exploit
this mechanism to integrate multiple annotation
schemes in NLP workflows. cTAKES (Savova
et al, 2010) and DKPro (Gurevych et al, 2007)
are two repositories containing UIMA-compliant
components that are tuned for the medical and
general domain, respectively. However, both of
these repositories support the representation of
only one discourse phenomenon, i.e., coreference.
Argo (Rak et al, 2012a; Rak et al, 2012b) is a
web-based platform that allows multiple branch-
ing and merging of UIMA pipelines. It incorpo-
rates several U-Compare components and conse-
quently, supports the U-Compare type system.
3 A UIMA architecture for processing
multiple annotation schemes
In UIMA, a document, together with its associated
annotations, is represented as a standardised data
structure, namely the Common Analysis Struc-
ture (CAS). Each CAS can contain any number
of nested sub-CASes, i.e., Subjects of Analysis
(Sofas), each of which can associate a different
type of annotation with the input document. In
this paper, we employ this UIMA mechanism to
allow the integration and comparison of multiple
81
Collection of Documents
Multi-SofaReader
Parallel Annotation Viewer Annotation Merger
ComparingSchemes Integrating Schemes
ComponentC_1
Sofa S_1
ComponentC_2
Sofa S_2
ComponentC_N-1
Sofa S_N-1
ComponentC_N
Sofa S_N
sub-workflows
Figure 1: Integrating annotations from multiple
annotation schemes in UIMA workflows
annotation schemes in a single U-Compare work-
flow. Assume that we have a corpus of documents
which has been annotated according to n different
schemes, S1, S2, ..., Sn?1, Sn. Also, assume that
we will use a library of m text analysis compo-
nents, C1, C2, ..., Cm?1, Cm, to enrich the corpus
with further annotations.
Our implemented architecture is illustrated in
Figure 1. Using multiple Sofas, we are able to split
a UIMA workflow into parallel sub-workflows.
Starting from a Multi-Sofa reader, we create n
sub-workflows, i.e., Sofas, each of which is linked
to a particular scheme for a different annotation
type. Each sub-workflow can then apply the anal-
ysis components that are most suitable for pro-
cessing the annotations from the corresponding
scheme.
U-Compare offers two different modes for visu-
alising corpora that have been annotated accord-
ing to multiple schemes. In the comparison mode,
the default annotation viewer is automatically split
to allow annotations from different schemes to be
displayed side-by-side. The second type of visu-
alisation merges the annotations produced by the
parallel sub-workflows into a single view. The
most appropriate view may depend on the prefer-
ences of the user and the task at hand, e.g., iden-
tifying similarities, differences or complementary
information between different schemes.
4 Application Workflows
In this section, we demonstrate two workflow ap-
plications that integrate multiple discourse anno-
tation schemes. The first workflow exploits U-
Compare?s comparison mode to visualise in par-
allel functional discourse annotations from two
schemes, namely, CoreSC (Liakata et al, 2012a)
and GENIA-MK (Thompson et al, 2011). The
second application integrates functional discourse
annotations in the ACE 2005 corpus with dis-
course relations obtained by an automated tool.
4.1 Visualising functional discourse
annotations from different schemes
The purpose of this workflow application is to re-
veal the different interpretations given by two dis-
course annotation schemes applied to a biomed-
ical corpus of three full-text papers (Liakata et
al., 2012b). The pipeline contains two read-
ers that take as input the annotations (in the
BioNLP Shared Task stand-off format) from the
two schemes and map them to U-Compare?s
type system. In this way, the annotations be-
come interoperable with existing components in
U-Compare?s library. U-Compare detects that the
workflow contains two annotation schemes and
automatically creates two parallel sub-workflows
as explained earlier. Furthermore, we configure
the workflow to use the comparison mode. There-
fore, the annotation viewer will display the two
different types of annotations based on the input
schemes side-by-side. Figure 2 illustrates the par-
allel viewing of a document annotated according
to both the CoreSC (left-hand side) and GENIA-
MK (right-hand side) annotation schemes. The
CoreSC scheme assigns a single category per sen-
tence. The main clause in the highlighted sen-
tence on the left-hand side constitutes the hypoth-
esis that transcription factors bind to exon-1. Ac-
cordingly, as can be confirmed from the annota-
tion table on the far right-hand side of the figure,
the (Hyp)othesis category has been assigned to the
sentence.
In the GENIA-MK corpus, the different pieces
of information contained within the sentence have
been separately annotated as structured events.
One of these events corresponds to the hypothe-
sis, but this is not the only information expressed:
information about a previous experimental out-
come from the authors, i.e., that exon1 is impli-
cated in CCR3 transcription, is annotated as a sep-
82
Figure 2: Comparing discourse annotations schemes in U-Compare. The pipeline uses two Sofas corre-
sponding to the CoreSC (left panel) and GENIA-MK (right panel) schemes.
arate event. Since functional discourse informa-
tion is annotated directly at the event level in the
GENIA-MK corpus, the bind event is considered
independently from the other event as represent-
ing an Analysis. Furthermore, the word hypoth-
esized is annotated as a cue for this categorisa-
tion. There are several ways in which the an-
notations of the two schemes can be seen to be
complementary to each other. For example, the
finer-grained categorisation of analytical informa-
tion in the CoreSC scheme could help to determine
that the analytical bind event in the GENIA-MK
corpus specifically represents a hypothesis, rather
than, e.g., a conclusion. Conversely, the event-
based annotation in the GENIA-MK corpus can
help to determine exactly which part of the sen-
tence represents the hypothesis. Furthermore, the
cue phrases annotated in the GENIA-MK corpus
could be used as additional features in a system
trained to assign CoreSC categories. Although in
this paper we illustrate only the visualisation of
different types of functional discourse annotations,
it is worth noting that U-Compare provides sup-
port for further processing. Firstly, unlike annota-
tion platforms such as brat (Stenetorp et al, 2012),
U-Compare allows for analysis components to be
integrated into workflows in a straightforward and
user-interactive manner. If, for example, it is of in-
terest to determine the tokens (and the correspond-
ing parts-of-speech) which frequently act as cues
in Analysis events, syntactic analysis components
(e.g., tokenisers and POS taggers) can be incorpo-
rated via a drag-and-drop mechanism. Also, U-
Compare allows the annotations to be saved in a
computable format using the provided Xmi Writer
CAS Consumer component. This facilitates fur-
ther automatic comparison of annotations.
4.2 Integrating discourse relations with
functional discourse annotations
To demonstrate the integration of annotations orig-
inating from two completely different perspectives
on discourse, we have created a workflow that
merges traditional discourse relations with func-
tional discourse annotations in a general domain
corpus. For this application, we used the ACE
2005 corpus, which consists of 599 documents
coming from broadcast conversation, broadcast
news, conversational telephone speech, newswire,
weblog and usenet newsgroups. This corpus
contains event annotations which have been en-
riched by attributes such as polarity (positive or
negative), modality (asserted or other), generic-
ity (generic or specific) and tense (past, present,
future or unspecified). We treat the values of
these attributes as functional discourse annota-
tions, since they provide further insight into the
interpretation of the events. We created a compo-
nent that reads the event annotations in the corpus
and maps them to U-Compare?s type system.
To obtain discourse relation annotations (which
are not available in the ACE corpus) we em-
ployed an end-to-end discourse parser trained
on the Penn Discourse TreeBank (Lin et al,
2012). It outputs three general types of anno-
tations, namely, explicit relations, non-explicit
relations and attribution spans. Explicit rela-
tions (i.e., those having overt discourse connec-
tives) are further categorised into the following 16
PDTB level-2 types: Asynchronous, Synchrony,
Cause, Pragmatic cause, Contrast, Concession,
Conjunction, Instantiation, Restatement, Alterna-
tive, List, Condition, Pragmatic condition, Prag-
matic contrast, Pragmatic concession and Excep-
83
Figure 3: Integrating different discourse annotation schemes in U-Compare.
tion. Non-explicit relations, on the other hand,
consist of EntRel and NoRel types, in addition to
the same first 11 explicit types mentioned above.
We created a workflow consisting of the ACE
corpus reader and the discourse parser (available
in U-Compare as a UIMA web service). This al-
lowed us to merge traditional discourse relations
with event-based functional discourse annotations,
and to visualise them in the same document (Fig-
ure 3). Furthermore, with the addition of the
Xmi Writer CAS Consumer in the workflow, the
merged annotations can be saved in a computable
format for further processing, allowing users to
perform deeper analyses on the discourse annota-
tions. This workflow has enabled us to gain some
insights into the correlations between functional
discourse annotations and discourse relations.
5 Correlations between discourse
relations and functional discourse
annotations
Based on the merged annotation format described
in the previous section, we computed cases in
which at least one of the arguments of a discourse
relation also contains an event. Figure 4 is a
heatmap depicting the correlations between differ-
ent types of discourse relations and the attribute
values of ACE events that co-occur with these re-
lations. The darker the colour, the smaller the ratio
of the given discourse relation co-occurring with
the specified ACE event attribute value. For in-
stance, the Cause relation co-occurs mostly with
positive events (over 95%) and the correspond-
ing cell is a very light shade of green. These are
discussed and exemplified below. In the exam-
ples, the following marking convention is used:
discourse connectives are capitalised, whilst argu-
ments are underlined. Event triggers are shown in
bold, and cues relating to functional discourse cat-
egories are italicised.
For all discourse relation types, at least 50% of
co-occurring events are assigned the specific value
of the Genericity attribute. Specific events are
those that describe a specific occurrence or situ-
ation, rather than a more generic situation. In gen-
eral, this high proportion of specific events is to be
expected. The types of text contained within the
corpus, consisting largely of news and transcrip-
tions of conversions, would be expected to intro-
duce a large amount of information about specific
events.
For two types of discourse relations, i.e. Condi-
tion and Concession, there are more or less equal
numbers of specific and generic events. The na-
ture of these relation types helps to explain these
proportions. Conditional relations often describe
how a particular, i.e., specific, situation will hold
if some hypothetical situation is true. Since hypo-
thetical situations do not denote specific instances,
they will usually be labelled as generic. Con-
cessions, meanwhile, usually describe how a spe-
cific situation holds, even though another (more
generic) situation would normally hold, that would
be inconsistent with this. For the Instantiation re-
lation category, it may once again be expected that
similar proportions of generic and specific events
would co-occur within their arguments, since an
instantiation describes a specific instance of a
more generic situation. However, contrary to these
84
AlternativeAsynchronousAttributionCauseConcessionConditionConjunctionContrastEntRelInstantiationListNoRelRestatementSynchronous
GenericSpecific AssertedOther NegativePositive FuturePast PresentUnspecif
ied
0
1
0.5
0.25
0.75
Genericity Modality Polarity Tense0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
Figure 4: Heatmap showing the distribution of correlations between discourse relations and event-based
functional discourse categories. A darker shade indicates a smaller percentage of instances of a discourse
relation co-occurring with an event attribute.
expectations, the ratio of specific to generic events
is 3:1. A reason for this is that discourse argu-
ments corresponding to the description of a spe-
cific instance may contain several different events,
as illustrated in Example (1).
(1) Toefting has been convicted before. In
1999 he was given a 20-day suspended sentence
for assaulting a fan who berated him for
playing with German club Duisburg.
In terms of the Modality attribute, most dis-
course relations correlate with definite, asserted
events. Simillarly to the Genericity attribute, this
can be largely explained by the nature of the texts.
However, there are two relation types, i.e., Condi-
tion and Consession, which have particularly high
proportions of co-occurring events whose modal-
ity is other. Events that are assigned this attribute
value correspond to those that are not described as
though they are real occurrences. This includes,
e.g., speculated or hypothetical events. The fact
that Condition relations are usually hypothetical
in nature explain why 76% of events that co-occur
with such relations are assigned the other value
for the Modality attribute. Example (2) illustrates
a sentence containing this relation type.
(2) And I?ve said many times, IF we all
agreed on everything, everybody would want to
marry Betty and we would really be in a mess,
wouldn?t we, Bob.
An even higher proportion of Concession re-
lations co-occurs with events whose modality is
other. Example (3) helps to explain this. In the
first clause (the generic situation), the mention of
minimising civilian casualties is only described as
an effort, rather than a definite situation. The hedg-
ing of this generic situation is necessary in order to
concede that the more specific situation described
in the second clause could actually be true, i.e.,
that a large number of civilians have already been
killed. Due to the nature of news reporting, which
may come from potentially unreliable sources, the
killed event in this second clause is also hedged,
through the use of the word reportedly.
(3) ALTHOUGH the coalition leaders have
repeatedly assured that every effort would be
made to minimize civilian casualties in the
current Iraq war, at least 130 Iraqi civilians have
been reportedly killed since the war started five
days ago.
Almost 96% of events that co-occur with argu-
ments of discourse relations have positive polarity.
Indeed, for eight relation types, 100% of the cor-
responding events are positive. This can partly be
explained by the fact that, in texts reporting news,
85
there is an emphasis on reporting events that have
happened, rather than events that did not happen.
It can, however, be noted that events that co-occur
with certain discourse relation types have a greater
likelihood of having negative polarity. These rela-
tions include Contrast (9% of events having neg-
ative polarity) and Cause (5% negative events).
Contrasts can include comparisons of positive and
negative situations, as in Example (4), whilst for
Causes, it can sometimes be relevant to state that
a particular situation caused a specific event not to
take place, as shown in Example (5).
(4) The message from the Israeli government
is that its soldiers are not targeting journalists,
BUT that journalists who travel to places where
there could be live fire exchange between
Israeli forces and Palestinian gunmen have a
responsibility to take greater precautions.
(5) His father didn?t want to invade Iraq, BE-
CAUSE of all these problems they?re having
now.
For most relation types, around 60% of their co-
occurring events are annotated as describing past
tense situations. This nature of newswire and con-
versations mean that this is largely to be expected,
since they normally report mainly on events that
have already happened. The proportion of events
assigned the future tense value is highest when
they co-occur with discourse relations of type Al-
ternative. In this relation type, it is often the case
that one of the arguments describes a possible fu-
ture alternative to a current situation, as the case in
Example (6). This possible information pattern for
Alternative relations, where one of the arguments
represents a currently occurring situation, would
also help to explain why, even though very few
events in general are annotated as present tense,
almost 10% of events that co-occur with Alter-
native relations describe events that are currently
ongoing. As for events whose Tense value is un-
specified, two of the most common discourse re-
lation types with which they occur are Condition
and Concession. As exemplified above, Condition
relations are often hypothetical in nature, meaning
that no specific tense can be assigned. The generic
argument of a Concession relation can also remain
unmarked for tense. As in Example (3), it is not
clear whether the effort to minimise civilian casu-
alties has already been initiated, or will be initiated
in the future.
(6) Saddam wouldn?t be destroying missiles
UNLESS he thought he was going to be
destroyed if he didn?t.
6 Conclusions
Given the level of variability in existing discourse-
annotated corpora, it is meaningful for users to
identify the relative merits of different schemes.
In this paper, we have presented an extension of
the U-Compare infrastructure that facilitates the
comparison, integration and visualisation of doc-
uments annotated according to different annota-
tion schemes. U-Compare constructs multiple and
parallel annotation sub-workflows nested within a
single workflow, with each sub-workflow corre-
sponding to a distinct scheme. We have applied
the implemented method to visualise the similar-
ities and differences of two functional discourse
annotation schemes, namely CoreSC and GENIA-
MK. To demonstrate the integration of multiple
schemes in U-Compare, we developed a work-
flow that merged event annotations from the ACE
2005 corpus (which include certain types of func-
tional discourse information) with discourse rela-
tions obtained by an end-to-end parser. Moreover,
we have analysed the merged annotations obtained
by this workflow and this has allowed us to iden-
tify various correlations between the two different
types of discourse annotations.
Based on the intuition that there is comple-
mentary information across different types of dis-
course annotations, we intend to examine how the
integration of multiple discourse schemes, e.g.,
features obtained by merging annotations, affects
the performance of machine learners for discourse
analysis.
7 Acknowledgements
We are grateful to Dr. Ziheng Lin (Na-
tional University of Singapore) for providing us
with the discourse parser used for this work.
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; Engineering and Physical Sciences Re-
search Council [grant numbers EP/P505631/1,
EP/J50032X/1]; and MRC Text Mining and
Screening (MR/J005037/1).
86
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381 ? 390.
Riza Theresa B. Batista-Navarro, Georgios Kontonat-
sios, Claudiu Miha?ila?, Paul Thompson, Rafal Rak,
Raheel Nawaz, Ioannis Korkontzelos, and Sophia
Ananiadou. 2013. Facilitating the analysis of dis-
course phenomena in an interoperable NLP plat-
form. In Computational Linguistics and Intelligent
Text Processing, volume 7816 of Lecture Notes in
Computer Science, pages 559?571. Springer Berlin
Heidelberg, March.
William A. Baumgartner, Kevin Bretonnel Cohen, and
Lawrence Hunter. 2008. An open-source frame-
work for large-scale, flexible evaluation of biomedi-
cal text mining systems. Journal of biomedical dis-
covery and collaboration, 3:1+, January.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure Theory.
In Proceedings of the Second SIGdial Workshop on
Discourse and Dialogue - Volume 16, SIGDIAL ?01,
pages 1?10, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In In Recent Advanced in Language
Processing, pages 168?175.
Anita de Waard and Henk Pander Maat. 2009. Epis-
temic segment types in biology research articles. In
Proceedings of the Workshop on Linguistic and Psy-
cholinguistic Approaches to Text Structuring (LPTS
2009).
Anita de Waard. 2007. A pragmatic structure for re-
search articles. In Proceedings of the 2nd inter-
national conference on Pragmatic web, ICPW ?07,
pages 83?89, New York, NY, USA. ACM.
David Ferrucci and Adam Lally. 2004. Building an
example application with the unstructured informa-
tion management architecture. IBM Systems Jour-
nal, 43(3):455?475.
Ralph Grishman. 1996. TIPSTER Text Phase II archi-
tecture design version 2.1p 19 june 1996. In Pro-
ceedings of the TIPSTER Text Program: Phase II,
pages 249?305, Vienna, Virginia, USA, May. Asso-
ciation for Computational Linguistics.
Yufan Guo, Anna Korhonen, Maria Liakata,
Ilona Silins Karolinska, Lin Sun, and Ulla Ste-
nius. 2010. Identifying the information structure of
scientific abstracts: An investigation of three differ-
ent schemes. In Proceedings of the 2010 Workshop
on Biomedical Natural Language Processing, pages
99?107. Association for Computational Linguistics.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the GSCL.
Udo Hahn, Ekaterina Buyko, Rico Landefeld, Matthias
Mu?hlhausen, Michael Poprat, Katrin Tomanek, and
Joachim Wermter. 2008. An overview of JCoRe,
the JULIE lab UIMA component repository. In
LREC?08 Workshop ?Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP?,
pages 1?7, Marrakech, Morocco, May.
Yoshinobu Kano, Makoto Miwa, Kevin Cohen,
Lawrence Hunter, Sophia Ananiadou, and Jun?ichi
Tsujii. 2011. U-Compare: A modular NLP work-
flow construction and evaluation system. IBM Jour-
nal of Research and Development, 55(3):11.
Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin Batchelor. 2010. Corpora for the concep-
tualisation and zoning of scientific papers. In Pro-
ceedings of LREC, volume 10.
Maria Liakata, Shyamasree Saha, Simon Dobnik,
Colin Batchelor, and Dietrich Rebholz-Schuhmann.
2012a. Automatic recognition of conceptualization
zones in scientific articles and two life science appli-
cations. Bioinformatics, 28(7):991?1000.
Maria Liakata, Paul Thompson, Anita de Waard, Ra-
heel Nawaz, Henk Pander Maat, and Sophia Ana-
niadou. 2012b. A three-way perspective on scien-
tic discourse annotation for knowledge extraction.
In Proceedings of the ACL Workshop on Detecting
Structure in Scholarly Discourse (DSSD), pages 37?
46, July.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, FirstView:1?34, 10.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text, 8(3):243?281.
Claudiu Miha?ila?, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. BioCause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14(1):2, January.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468 ? 487. Re-
cent Advances in Natural Language Processing for
Biomedical Applications Special Issue.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse Tree-
Bank 2.0. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios
87
Piperidis, and Daniel Tapias, editors, In Proceedings
of the 6th International Conference on language Re-
sources and Evaluation (LREC), pages 2961?2968.
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The biomedical
discourse relation bank. BMC Bioinformatics,
12(1):188.
Rafal Rak, Andrew Rowley, and Sophia Ananiadou.
2012a. Collaborative development and evaluation
of text-processing workflows in a UIMA-supported
web-based workbench.
Rafal Rak, Andrew Rowley, William Black, and Sophia
Ananiadou. 2012b. Argo: an integrative, in-
teractive, text mining-based workbench supporting
curation. Database: The Journal of Biological
Databases and Curation, 2012.
A?gnes Sa?ndor and Anita de Waard. 2012. Identifying
claimed knowledge updates in biomedical research
articles. In Proceedings of the Workshop on De-
tecting Structure in Scholarly Discourse, ACL ?12,
pages 10?17, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Guergana Savova, James Masanz, Philip Ogren, Jiap-
ing Zheng, Sunghwan Sohn, Karin Kipper-Schuler,
and Christopher Chute. 2010. Mayo clini-
cal text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and
applications. Journal of the American Medical In-
formatics Association, 17(5):507?513.
Ulrich Scha?fer. 2006. Middleware for creating and
combining multi-dimensional nlp markup. In Pro-
ceedings of the 5th Workshop on NLP and XML:
Multi-Dimensional Markup in Natural Language
Processing, pages 81?84. ACL.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions Session at EACL 2012, Avignon, France, April.
Association for Computational Linguistics.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, EACL ?99,
pages 110?117, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12(1):393.
Christopher Walker. 2006. ACE 2005 Multilingual
Training Corpus.
W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.
2006. New directions in biomedical text annota-
tion: definitions, guidelines and corpus construction.
BMC Bioinformatics, 7(1):356.
88
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 95?104,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using a Random Forest Classifier to recognise translations of biomedical
terms across languages
Georgios Kontonatsios1,2 Ioannis Korkontzelos1,2 Jun?ichi Tsujii3 Sophia Ananiadou1,2
National Centre for Text Mining, University of Manchester, Manchester, UK1
School of Computer Science, University of Manchester, Manchester, UK2
Microsoft Research Asia, Beijing, China3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
We present a novel method to recognise
semantic equivalents of biomedical terms
in language pairs. We hypothesise that
biomedical term are formed by seman-
tically similar textual units across lan-
guages. Based on this hypothesis, we
employ a Random Forest (RF) classifier
that is able to automatically mine higher
order associations between textual units
of the source and target language when
trained on a corpus of both positive and
negative examples. We apply our method
on two language pairs: one that uses the
same character set and another with a dif-
ferent script, English-French and English-
Chinese, respectively. We show that
English-French pairs of terms are highly
transliterated in contrast to the English-
Chinese pairs. Nonetheless, our method
performs robustly on both cases. We eval-
uate RF against a state-of-the-art align-
ment method, GIZA++, and we report a
statistically significant improvement. Fi-
nally, we compare RF against Support
Vector Machines and analyse our results.
1 Introduction
Given a term in a source language and term in a
target language the task of this paper is to classify
this pair as a translation or not. We investigate the
performance of the proposed classifier by apply-
ing it on a balanced classification problem, i.e. our
experimental datasets contain an equal number of
positive and negative examples. The proposed
classification model can be used as a component of
a larger system that automatically compiles bilin-
gual dictionaries of technical terms across lan-
guages. Bilingual dictionaries of terms are impor-
tant resources for many Natural Language Pro-
cessing (NLP) applications including Statistical
Machine Translation (SMT) (Feng et al, 2004;
Huang and Vogel, 2002; Wu et al, 2008), Cross-
Language Information Retrieval (Ballesteros and
Croft, 1997) and Question Answering systems
(Al-Onaizan and Knight, 2002). Especially in the
biomedical domain, manually creating and more
importantly updating such resources is an expen-
sive process, due to the vast amount of neologisms,
i.e. newly introduced terms (Pustejovsky et al,
2001). The UMLS metathesaurus which is one the
most popular hub of multilingual resources in the
biomedical domain, contains technical terms in 21
languages that are linked together using a con-
cept identifier. In Spanish, the second most popu-
lar language in UMLS, only 16.44% of the 7.6M
English terms are covered while other languages
fluctuate between 0.0052% (for Hebrew terms) to
3.26% (for Japanese terms). Hence, these lex-
ica are far for complete and methods that semi-
automatically (i.e., in a post-processing step, cu-
rators can manually remove erroneous dictionary
entries) discover pairs of terms across languages
are needed to enrich such multilingual resources.
Our method can be applied to parallel, aligned cor-
pora, where we expect approximately the same,
balanced classification problem. However, in
comparable corpora the search space of candidate
alignments is of vast size, i.e., quadratic the the
size of the input data. To cope with this heavily
unbalanced classification problem, we would need
to narrow down the number of negative instances
before classification.
We hypothesise that there are language in-
dependent rules that apply to biomedical terms
across many languages. Often the same or simi-
lar textual units (e.g., morphemes and suffixes) are
concatenated to realise the same terms in different
languages. For example, Table 1 illustrates how
a morpheme expressing pain (ache in English) is
used to realise the same terms in English, Chinese
and French. The realisations of the term ?head-
95
English Morpheme: -ache Chinese Morpheme: ? French Morpheme: -mal
head-ache ?-? mal de te?te
back-ache ?-? mal au dos
ear-ache ??-? mal d?oreille
Table 1: An example of English, Chinese and French terms consisting of the same morphemes
ache? is expected to consist of the units for ?head?
and ?ache? regardless of the language of realisa-
tion. Hence, knowing the translations of ?head?
and ?ache? allows the reconstruction ?headache?
in a target language.
In our method, we use a Random Forest (RF) clas-
sifier (Breiman, 2001) to learn the underlying rules
according to which terms are being constructed
across languages. An RF is an ensemble of De-
cision Trees voting for the most popular class. RF
classifiers are popular in the biomedical domain
for various tasks: classification of microarray data
(D??az-Uriarte and De Andres, 2006), compound
classification in cheminformatics (Svetnik et al,
2003), classification of microRNA data (Jiang et
al., 2007) and protein-protein interactions in Sys-
tems Biology (Chen and Liu, 2005). In NLP, RF
classifiers have been used for: Language Mod-
elling (Xu and Jelinek, 2004) and semantic pars-
ing (Nielsen and Pradhan, 2004). To the best of
the authors? knowledge, this is the first attempt to
employ RF for identifying translation equivalents
of biomedical terms.
We prefer RF over other traditional machine learn-
ing approaches such as Support Vector Machines
(SVMs) for a number of reasons. Firstly, RF is
able to automatically construct correlation paths
from the feature space, i.e. decision rules that cor-
respond to the translation rules that we intend
to capture. Secondly, RF is considered one of
the most accurate classifier available (D??az-Uriarte
and De Andres, 2006; Jiang et al, 2007). Finally,
RF is reported to cope well with datasets where the
number of features is larger than the number of ob-
servations (D??az-Uriarte and De Andres, 2006). In
our dataset, the number of features is almost four
times more than that of the observations.
We represent pairs of terms using character gram
features (i.e., first order features). Such shal-
low features have been proven effective in a num-
ber of NLP applications including: Named En-
tity Recognition (Klein et al, 2003), Multilin-
gual Named Entity Transliteration (Klementiev
and Roth, 2006; Freitag and Khadivi, 2007) and
predicting authorship (Stamatatos, 2006). In ad-
dition, by selecting character n-grams instead of
word n-grams, one avoids to segment words in
Chinese which has been proven to be a challenging
topic (Sproat and Emerson, 2003). We evaluate
our proposed method on two datasets of biomed-
ical terms (English-French and English-Chinese)
that contain equal numbers of positive and neg-
ative instances. RF achieves higher classifica-
tion performance than baseline methods. To boost
SVM?s performance further, we used a second or-
der feature space to represent the data. It consists
of pairs of character grams that co-occur in trans-
lation pairs. In the second order feature space, the
performance of SVMs improved significantly.
The rest of the paper is structured as follows. In
Section 2, we present previous approaches in iden-
tifying translation equivalents of terms or named
entities. In Section 3, we define the classifica-
tion problem, we formulate the RF classifier and
we discuss the first and second order feature space
that we use to represent pairs of terms. In Sec-
tion 4, we show that RF achieves superior classi-
fication performance. In Section 5, we overview
our method and we discuss how it can be used to
compile large-scale bilingual dictionaries of terms
from comparable corpora.
2 Related Work
In this section, we review previous approaches
that exploit the internal structure of sequences to
align terms or named entities across languages.
(Klementiev and Roth, 2006; Freitag and Khadivi,
2007) use character gram features, similar to the
feature space that we propose in this paper, to train
discriminative, supervised models. Klementiev
and Roth (2006) introduce a supervised Percep-
tron model for English and Russian named enti-
ties. They construct a character gram feature space
as follows: firstly, they extract all distinct charac-
ter grams from both source and target named en-
tity. Then, they pair character grams of the source
named entity with character grams of the corre-
sponding target named entity into features. In or-
96
der to reduce the number of features, they link
only those character grams whose position offsets
in the source and target sequence differs by -1, 0
or 1. Freitag and Khadivi (2007) employ the same
character gram feature space but they do not con-
straint the included character-grams to their rela-
tive position offsets in the source and target se-
quence. The boolean features are defined for ev-
ery distinct character-grams observed in the data
of length k or shorter. Using this feature space
they train an Averaged Perceptron model, able to
incorporate an arbitrary number of features in the
input vectors, for English and Arabic named en-
tities. The above character gram based methods
mainly focused on aligning named entities of the
general domain, i.e. person names, locations, or-
ganizations, etc., that are transliterated, i.e. present
phonetic similarities, across languages.
SMT-based approaches built on top of existing
SMT frameworks to identify translation pairs of
terms (Tsunakawa et al, 2008; Wu et al, 2008).
Tsunakawa et al (2008), align terms between
a source language Ls and a target language Lt
using a pivot language Lp. They assume that
two bilingual dictionaries exist: from Ls to Lp
and from Lp to Lt. Then, they train GIZA++
(Och and Ney, 2003) on both directions and they
merge the resulting phrase tables into one table
between Ls and Lt, using grow-diag-final heuris-
tics (Koehn et al, 2007). Wu et al (2008), use
morphemes instead of words as translation units
to train a phrase based SMT system for technical
terms in English and Chinese. The use of shorter
lexical fragments, e.g. lemmas, stems and suf-
fixes, as translation units has reportedly reduced
the Out-Of-Vocabulary problem (Virpioja et al,
2007; Popovic and Ney, 2004; Oflazer and El-
Kahlout, 2007).
Hybrid methods exploit that a term or a named en-
tity can be translated in various ways across lan-
guages (Shao and Ng, 2004; Feng et al, 2004; Lu
and Zhao, 2006). For instance, person names are
usually translated by transliteration (i.e., words
exhibiting pronunciation similarities across lan-
guages, are likely to be mutual translations) while
technical terms are likely to be translated by
meaning (i.e., the same semantic units are used to
generate the translation of the term in the target
language). The resulting hybrid systems were re-
ported to perform at least as well as existing SMT
systems (Feng et al, 2004).
Lepage and Denoual (2005) presented an analog-
ical learning machine translation system as part
of the IWSLT task (Eck and Hori, 2005) that re-
quires no training process and it is able to achieve
state-of-the art performance. The core method
of their system models relationships between se-
quences of characters, e.g., sentences, phrases or
words, across languages using proportional analo-
gies, i.e., [a : b = c : d], ?a is to b as c is to d?, and
is able to solve unknown analogical equations,
i.e., [x : y = z :?] (Lepage, 1998). Analogical
learning has been proven effective in translating
unseen words (Langlais and Patry, 2007). Further-
more, analogical learning is reported to achieve a
better precision but a lower recall than a phrase-
based machine translation system when translating
medical terms (Langlais et al, 2009).
3 Methodology
Let em = (e1, ? ? ? , em) be an English term
consisting of m translation units and fn =
(f1, ? ? ? , fn) a French or Chinese term consist-
ing of n units. As translation units, we con-
sider character grams. We define a function f :
(em, fn) ?? {0, 1}:
f(em, fn) =
{
1, if em translates into fn
0, otherwise
The function can be learned by training a Random
Forest (RF) classifier1. Let N be the number of
training instances, |?| the total number of features,
i.e. the number of dimensions of the feature space,
|? | a predefined number of random decision trees
and |?| a predefined number of random features.
An RF classifier is defined as a collection of fully
grown decision tree classifiers, ?i(X) (Breiman,
2001):
RF = {?1(X), ? ? ? , ?? (X)}, X = (e
m, chn)
(1)
A pair of terms is classified as a translation pair
if the majority of the trees is voting for this class
label. Let I(?i(X)) be the vote of the ith tree
in the forest and avj?{0,1} the average number of
votes for class labels 0 (translation) and 1 (non-
translation). The function f of ? decision trees
can be written as the majority function:
f(em, chn) = Maj (I(?1(X)), ? ? ? , I(?? (X)))
=
?
1
2
??
1 I(?i(X)) + 1/2(?1)
r
?
?
(2)
1The WEKA implementation (Hall et al, 2009) of RF was
used for all experiments of this paper.
97
The majority function returns 1 if the majority
of I(?i(X)) is 1, or returns 0 if the majority of
I(?i(X)) is 0. Adding or subtracting 1/2 controls
whether a tie is resolved towards 1 or 0, respec-
tively. In RF ties are resolved randomly. To rep-
resent this, the negative unit (?1) is raised to a
randomly chosen positive integer r ? N+.
We tuned the RF classifier using 140 random
trees and |?| = log2 |?|+ 1 features as suggested
in Breiman (Breiman, 2001).
The RF mechanism that triggers term construction
rules across languages lies in the decision trees.
A RF grows a decision tree by selecting the most
informative feature, i.e. corresponding to the
lowest entropy, out of ? random features. For
each selected feature, a node is created and this
process is repeated for all ? random features of
the unprunned decision trees. In other words, the
process starts with the most informative feature
and builds association rules between all random
features. These are the construction rules that
we are interested in. Figure 1 illustrates a path
in one of the decision trees of an RF classifier
taken from the experiments we conducted on
the English-Chinese dataset. In only one of
thousands of branches of the forest, the classifier
is able to partially trigger the construction rule of
kinase, a type of enzyme, between English and
Chinese. The translation rule correctly associates
the English n-grams kin and as with their Chinese
translation ??. In addition, the translation rule
contains both positive and negative associations
between features. The English n-grams ing and
or are negatively correlated with the term kinase.
3.1 Feature Engineering
Each pair of terms is represented as a feature vec-
tor of character n-grams. We further define two
types of character n-gram features, namely first
order and second order. First order character n-
grams are boolean features that designate the oc-
currence of a corresponding character gram of pre-
defined length in the input term. These features are
monolingual, extracted separately from the source
and target term. The RF classifier is shown to ben-
efit from only monolingual features and achieves
the best observed performance. In contrast, SVMs
were shown not to perform well using the first or-
der feature space because they cannot directly as-
sociate the source with the target character grams.
To enhance the performance of SVMs, we con-
structed a second order feature space that contains
associations between first order features. A sec-
ond order feature is a tuple of a source and a tar-
get character gram that co-occur in one or more
translation pairs. Table 2 illustrates an example.
Second order character n-grams are multilingual
features and are defined over true translation pairs.
For this reason, we extract second order features
from the training data only.
In all experiments, the features were sorted in de-
creasing order of frequency of occurrence. We
trained a RF and two SVM classifiers, namely
linear-SVM and RBF-SVM, using a gradually in-
creasing number of features, always starting from
the top of the list. SMT frameworks cannot be
trained on an increasing number of features be-
cause each training instance needs to correspond
to at least one known translation unit (i.e., first or-
der features). Therefore, GIZA++ is trained on the
complete set of translation units.
4 Experiments
In this section, we discuss the employed datasets
of biomedical terms in English-French and
English-Chinese and three baseline methods. We
compare and discuss RF and SVMs trained on the
first order and second order features. Finally, we
report results of all classification methods evalu-
ated on the same datasets.
4.1 Datasets
For our experiments, we used an online bilin-
gual dictionary2 for English-Chinese terms and the
UMLS metathesaurus3 for English-French terms.
The former contains 31, 700 entries while the lat-
ter is a much larger dictionary containing 84, 000
entries. For training, we used the same number of
instances for both language pairs (i.e., 21, 000 en-
tries) in order not to bias the performance towards
the larger English-French dataset. The remain-
ing instances were used for testing (i.e., 10, 7000
and 63, 000 English-Chinese and English-French
respectively). In the case where a source term cor-
responded to more that one target terms according
to the seed dictionary, we randomly selected only
one translation. Negative instances were created
by randomly matching non-translation pairs of
terms. Since we are dealing with a balanced clas-
2www2.chkd.cnki.net/kns50/
3nlm.nih.gov/research/umls
98
Figure 1: Example of a term construction rule as a branch in a decision tree.
Input pair of English-French terms : (e1, e2, e3, f1, f2, f3)
English first order French first order Second order
?1(e1, e2) ?1(f1, f2) ?1(e1e2, f1f2), ?1(e1e2, f2f3)
?1(e2, e3) ?1(f2, f3) ?1(e2e3, f1f2), ?1(e2e3, f2f3)
Table 2: Example of first and second order features using a predefined n-gram size of 2.
sification problem, we created as many negative
instances as the positive ones in all our datasets.
In all experiments we performed a 3-fold cross-
validation.
4.2 Baselines
We evaluated RF against three classification meth-
ods, namely SVMs, GIZA++ and a Levenshtein
distance-based classifier.
SVMs coordinate a hyperplane in the hyperspace
defined by the features to best separate the posi-
tive and negative instances, i.e. aligned from non-
aligned pairs. In contrast to RF, SVMs do not sup-
port building association rules between features,
i.e., translation units, which in our task seems to be
a deficiency. SVMs produce one final association
rule, i.e. the classification boundary which sepa-
rates positive from negative examples. Its abil-
ity to distinguish aligned from non-aligned pair
of terms depends on how separable the two clus-
ters are. We evaluated several settings for the
SVM classifier. Apart from the default linear ker-
nel function, we applied a radial basis function,
i.e. RBF-SVM. RBF-SVM uses the kernel trick to
project the instances in a higher dimensional space
to better separate the two clusters. While tuning
the SVM?s classification cost C, we observed op-
timal performance for a value of 100. Secondly,
we seeded the association rules of translation units
to the SVM classifier by creating a second or-
der feature space, discussed in detail in section
3.1. We employed the LIBSVM implementation
(Chang and Lin, 2011) of SVMs using both the
linear and RBF kernels.
The second baseline method is GIZA++, an
open source implementation of the 5 IBM-models
(Brown et al, 1993). GIZA++ is traditionally
trained on a bilingual, parallel corpus of aligned
sentences and estimates the probability P (s|t) of a
source translation unit (typically a word), s, given
a target unit t. To apply GIZA++ on our dataset,
we consider the list of terms as parallel sentences.
GIZA++, trained on a list of terms, estimates
the alignment probability of English-Chinese and
English-French textual units, i.e. character n-
grams. Each entry i, j in the translation table
is the probability P (si|tj), where si and tj are
the source and target character n-grams in row i
and column j, respectively. Further details about
training a SMT toolkit for aligning technical terms
can be found in (Tsunakawa et al, 2008; Freitag
and Khadivi, 2007; Wu et al, 2008). After train-
ing GIZA++ we estimate the posterior probabil-
ity P (cfn|em) that a test, Chinese or French term
cfn = {cf1, ? ? ? , cfn} is aligned with a given En-
glish term em = {e1, ? ? ? , em} as follows:
p(cfn|em) = n?m
n?
i=1
m?
j=1
P (cfi|ej) (3)
A threshold ? was defined to classify a pair of
terms into translations or non-translations:
f(em, cfn) =
{
1, if p(cfn|em) ? ?
0, otherwise
(4)
We experimented with different values of ?
(greedy search) and we selected a value that max-
imizes classification performance.
In order to estimate how phonetically similar the
two language pairs are, we employed a third base-
99
(a) English-French dataset (b) English-Chinese dataset
Figure 2: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the first
order dataset
line method that uses the Edit/Levenshtein dis-
tance of pairs of terms to classify instances as
translations or not. The Levenshtein distance is
defined as the minimum edit operations, i.e., inser-
tion, deletions and substitution, required to trans-
form one sequence of characters to another. We
cannot directly calculate the Levenshtein distance
between English-Chinese pairs of terms since the
two languages are using different scripts. There-
fore, before we applied the Levenshtein distance-
based classifier, we converted the Chinese terms
to their pinyin form, i.e., Romanization system of
Chinese characters. As with GIZA++, we selected
a threshold ? that maximizes the performance of
the classifier.
4.3 Results
We hypothesise that a RF classifier is able to form
association paths between first order features. We
also have the theoretical intuition that SVM clas-
sifiers are not able to form such association paths.
As a result, we expect limited performance on the
first order feature set, because it does not contain
any associations among character grams.
Figure 2 shows the F-Score achieved by RF, linear-
SVM, RBF-SVM, GIZA++ and Levenshtein/Edit
distance-based classifier on the English-French
and English-Chinese datasets. RF and SVMs are
trained on an increasing number of features. The
behaviour of the classifiers is approximately the
same in both datasets. Performance is greater on
the English-French dataset since English is more
similar to French than to Chinese.
We also observe that linear-SVM and RBF-SVM
do not behave consistently. RBF-SVM?s perfor-
mance quickly climbs to a maximum and after-
wards it declines while linear-SVM?s performance
is constantly increasing until it balances to a very
high error rate, almost corresponding to random
classification. The linear-SVM classifier performs
poorly using first order features only, indicating
that this feature space is non-linearly separable,
i.e. there exists no hyperplane that separates trans-
lation from non-translation instances. Contrary,
RBF-SVM is able to construct a higher dimen-
sional space by applying the kernel trick so as
to take full advantage of a small number of fre-
quent and informative first order features. In this
higher dimensional space of few but informative
first order features, the RBF-SVM classifier coor-
dinates a hyperplane that effectively separates pos-
itive from negative instances. However, increas-
ing the number of features introduces noise that
affects the performance.
The RF is able to profit from larger sets of first
order features; thus, its performance is continu-
ously increasing until it stabilises at 6, 000 fea-
tures. The branches of the decision trees are shown
to manage features correctly to construct most of
the translation rules. Increasing the size of the fea-
ture space minimises the classification error, be-
cause more translation rules that generalize well
on unseen data are constructed.
The bilingual dictionary that we use for our
experiments contains heterogeneous biomedical
terms of diverse semantic categories. For ex-
ample, our data-set contains common medical
terms such as Intellectual Products (e.g. Pain
Management, prise en charge de la douleur, ?
???) or complex biological concepts such
as Enzymes (e.g. homogentisate 1,2-dioxygenase,
100
(a) English-French dataset (b) English-Chinese dataset
Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second
order dataset
English-French pairs English-Chinese pairs
P R F1 P R F1
GIZA++ 0.901 0.826 0.862 0.907 0.742 0.816
Levenshtein Distance 0.762 0.821 0.791 0.501 0.990 0.668
SVM -RBFsecond-order 0.946 0.884 0.914 0.750 0.899 0.818
Linear-SVMsecond-order 0.866 0.887 0.8763 0.765 0.893 0.824
RFfirst-order 0.962 0.874 0.916 0.779 0.940 0.851
Table 3: Best observed performance of RF, SVM and GIZA++ and Levenshtein Distance
acide homogentisique-oxydase, ???1,2-??
?). Therefore, we would expect poor perfor-
mance of the supervised methods using only a
small portion of the total set of first order features
due to the high diversity of the terms. For exam-
ple the morpheme ache/ mal/ ? is more frequent
in Disease or Syndrome named entities rather than
Enzyme named entities. However, the results indi-
cate that RF can generalize well on heterogeneous
terms. Figure 2 shows that the RF classifier out-
performs SMT based methods, using only 1000
features.
The Levenshtein distance-based classifier per-
forms considerably better in the English-French
dataset than in English-Chinese. In fact, its best
performance for the English-Chinese dataset is
achieved when classifying every pair of terms as
a translation, i.e. 100% recall but 50% precision.
In a second experiment, we attempted to explore
whether the performance of SVMs can be im-
proved by providing cross-language association
features. We employed the second order feature
set discussed in subsection 3.1. We used a constant
number of 6, 000 first order features, the num-
ber of features that achieved maximum F-Score
for RF in the previous experiment. Besides these
first order features, we added an increasing num-
ber of second order ones. Figure 3 shows the F-
Score curves of the RF, linear-SVM, RBF-SVM,
GIZA++ and Levenshtein distance using this fea-
ture space.
We observe that second order features improved
the performance of both SVMs considerably. In
contrast to the previous experiment, the two SVMs
present consistent bevaviour. Interestingly, the
performance of the RF slightly decreased when
using a small number of second order features.
A possible explanation of this behaviour is that
the second order associative features added noise,
since the RF had already formed the association
rules from first order features. In addition, for m
English and n Chinese or French first order fea-
tures there were m ? n possible combinations of
second order features as explained in Subsection
3.1. Hence, there was a large number of second
order features that we excluded from the train-
ing process. Consequently, decision tree branches
were populated with incomplete association rules
while the RF was able to form these associa-
tions automatically. Nevertheless, as more sec-
ond order features were added, more association
rules were explored and the RF performance in-
101
creased. Table 3 summarises the highest perfor-
mance achieved by the RF, SVMs, GIZA++ and
Levenshtein distance all trained and tested on the
same dataset. The resulting performance of the RF
compared with GIZA++ is statistically significant
(p < 0.0001) in all experiments. Comparing the
RF with the SVMs, we note that in the English-
French dataset, the performance of the SVM-RBF
is approximately the same with the performance
of our proposed method. However, this comes
with a cost. Firstly, SVMs can possibly achieve
a comparable performance to the RF when us-
ing multilingual, second order features. In con-
trast, our experiments show that RF benefit from
monolingual, first order features only. Secondly,
SVMs need a large number of additional multi-
lingual features, (6.000 second order features or
more) to perform similarly to RF. As a conse-
quence, the resulting models of the SVM classi-
fiers are more complex. We measured the aver-
age time needed by the two classifiers to decide
for a single pair of terms. The RF is approx-
imately 30 times faster than SVMs (on average
0.010 and 0.292 seconds, respectively). Finally,
in the English-Chinese dataset the RF performed
significantly better than both SVMs.
5 Discussion And Future Work
In this paper, we presented a novel classification
method that uses Random Forest (RF) to recognise
translations of biomedical terms across languages.
Our approach is based on the hypothesis that in
many languages, there exist some rules for com-
bining textual units, e.g. n-grams, to form biomed-
ical terms. Based on this assumption, we de-
fined a first order feature space of character grams
and demonstrated that an RF classifier is able to
discover such cross language translation rules for
terms. We experimented with two diverse lan-
guage pairs: English-French and English-Chinese.
In the former case, pairs of terms exhibit high pho-
netic similarity while in the latter case they do not.
Our results showed that the proposed method per-
forms robustly in both cases and achieves a signif-
icantly better performance than GIZA++. We also
evaluated Support Vector Machines (SVM) clas-
sifiers on the same first order feature space and
showed that they fail to form translation rules in
both language pairs, possibly because it cannot
associate first order features with each other suc-
cessfully. We attempted to boost the performance
of the SVM classifier by adding association evi-
dence of textual units to the features. We extracted
second order features from the training data and
we defined a new feature set consisting of both first
order and second order features. In this feature
space, the performance of the SVMs improved sig-
nificantly.
In addition to this, we observe from the reported
experiments that RF achieves a better F-Score per-
formance than GIZA++ in all datasets. Nonethe-
less, GIZA++ presents a better precision (but
lower recall) in one dataset, i.e., English/Chinese.
Based on this observation we plan to investigate
the performance of a hybrid system combining RF
with MT approaches.
One trivial approach to apply the proposed method
for compiling large-scale bilingual dictionaries of
terms from comparable corpora would be to di-
rectly classify all possible pairs of terms into
translations or non-translations. However, in
comparable corpora, the size of the search space
is quadratic to the input data. Therefore, the clas-
sification task is much more challenging since the
distribution of positive and negative instances is
highly skewed. To cope with the vast search space
of comparable corpora, we plan to incorporate
context-based approaches with the RF classifica-
tion method. Context-based approaches, such as
distributional vector similarity (Fung and McKe-
own, 1997; Rapp, 1995; Koehn and Knight, 2002;
Haghighi et al, 2008), can be used to limit the
number of candidate translations by filtering out
pairs of terms with low contextual similarity.
Finally, the proposed method can be also used to
online augment the phrase table of Statistical Ma-
chine Translation (SMT) in order to better han-
dle the Out-of-Vocabulary problem i.e. inability
to translate textual units that consist of one or
more words and do not occur in the training data
(Habash, 2008).
Acknowledgements
The work described in this paper is partially
funded by the European Community?s Seventh
Framework Program (FP7/2007-2013) under grant
agreement no. 318736 (OSSMETER).
102
References
Y. Al-Onaizan and K. Knight. 2002. Translating
named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 400?408. Association for Computational Lin-
guistics.
L. Ballesteros and W.B. Croft. 1997. Phrasal trans-
lation and query expansion techniques for cross-
language information retrieval. In ACM SIGIR Fo-
rum, volume 31, pages 84?91. ACM.
L. Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5?32.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
C.C. Chang and C.J. Lin. 2011. Libsvm: a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3):27.
X.W. Chen and M. Liu. 2005. Prediction of protein?
protein interactions using random decision forest
framework. Bioinformatics, 21(24):4394?4400.
R. D??az-Uriarte and S.A. De Andres. 2006. Gene se-
lection and classification of microarray data using
random forest. BMC bioinformatics, 7(1):3.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In Proc. of the In-
ternational Workshop on Spoken Language Transla-
tion, pages 1?22.
D. Feng, Y. Lv, and M. Zhou. 2004. A new approach
for english-chinese named entity alignment. In Em-
pirical Methods in Natural Language Processing,
pages 372?379.
D. Freitag and S. Khadivi. 2007. A sequence align-
ment model based on the averaged perceptron. In
Conference on Empirical methods in Natural Lan-
guage Processing, pages 238?247.
P. Fung and K. McKeown. 1997. A technical word-
and term-translation aid using noisy parallel cor-
pora across language groups. Machine Translation,
12(1):53?87.
N. Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 57?60. Association for
Computational Linguistics.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons
from monolingual corpora. Proceedings of ACL-08:
HLT, pages 771?779.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
F. Huang and S. Vogel. 2002. Improved named en-
tity translation and bilingual named entity extrac-
tion. In International Conference on Multimodal In-
teraction, pages 253?258. IEEE.
P. Jiang, H. Wu, W. Wang, W. Ma, X. Sun, and Z. Lu.
2007. Mipred: classification of real and pseudo
microrna precursors using random forest prediction
model with combined features. Nucleic acids re-
search, 35(suppl 2):W339?W344.
D. Klein, J. Smarr, H. Nguyen, and C.D. Manning.
2003. Named entity recognition with character-level
models. In Proceedings of the seventh conference
on Natural language learning at HLT-NAACL, pages
180?183. Association for Computational Linguis-
tics.
A. Klementiev and D. Roth. 2006. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 817?
824. Association for Computational Linguistics.
P. Koehn and K. Knight. 2002. Learning a transla-
tion lexicon from monolingual corpora. In Proceed-
ings of the ACL-02 workshop on Unsupervised lex-
ical acquisition-Volume 9, pages 9?16. Association
for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating unknown words by analogical learning. In
Proceedings of EMNLP-CoNLL, pages 877?886.
Philippe Langlais, Franc?ois Yvon, and Pierre Zweigen-
baum. 2009. Improvements in analogical learning:
application to translating multi-terms of the medical
domain. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 487?495. Association
for Computational Linguistics.
Yves Lepage. 1998. Solving analogies on words: an
algorithm. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 728?734. Association for Computational Lin-
guistics.
M. Lu and J. Zhao. 2006. Multi-feature based chinese-
english named entity extraction from comparable
corpora. pages 131?141.
103
R.D. Nielsen and S. Pradhan. 2004. Mixing weak
learners in semantic parsing. In Empirical Methods
in Natural Language Processing.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional linguistics, 29(1):19?51.
K. Oflazer and I.D. El-Kahlout. 2007. Exploring
different representational units in english-to-turkish
statistical machine translation. In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 25?32. Association for Computational
Linguistics.
Maja Popovic and Hermann Ney. 2004. Towards the
Use of Word Stems and Suffixes for Statistical Ma-
chine Translation. In 4th International Conference
on Language Resources and Evaluation (LREC),
pages 1585?1588, Lisbon,Portugal.
J. Pustejovsky, J. Castano, B. Cochran, M. Kotecki,
and M. Morrell. 2001. Automatic extraction
of acronym-meaning pairs from medline databases.
Studies in health technology and informatics,
(1):371?375.
R. Rapp. 1995. Identifying word translations in non-
parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguis-
tics, pages 320?322. Association for Computational
Linguistics.
L. Shao and H.T. Ng. 2004. Mining new word trans-
lations from comparable corpora. In Proceedings
of the 20th international conference on Computa-
tional Linguistics, page 618. Association for Com-
putational Linguistics.
R. Sproat and T. Emerson. 2003. The first international
chinese word segmentation bakeoff. In Proceedings
of the second SIGHAN workshop on Chinese lan-
guage processing-Volume 17, pages 133?143. Asso-
ciation for Computational Linguistics.
Efstathios Stamatatos. 2006. Ensemble-based author
identification using character n-grams. In In Proc.
of the 3rd Int. Workshop on Textbased Information
Retrieval, pages 41?46.
V. Svetnik, A. Liaw, C. Tong, J.C. Culberson, R.P.
Sheridan, and B.P. Feuston. 2003. Random forest:
a classification and regression tool for compound
classification and qsar modeling. Journal of chemi-
cal information and computer sciences, 43(6):1947?
1958.
T. Tsunakawa, N. Okazaki, and J. Tsujii. 2008.
Building bilingual lexicons using lexical translation
probabilities via pivot languages. In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, may.
S. Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sade-
niemi. 2007. Morphology-aware statistical machine
translation based on morphs induced in an unsu-
pervised manner. Machine Translation Summit XI,
2007:491?498.
X. Wu, N. Okazaki, T. Tsunakawa, and J. Tsujii. 2008.
Improving English-to-Chinese Translation for Tech-
nical Terms Using Morphological Information. In
AMTA-2008. MT at work: Proceedings of the Eighth
Conference of the Association for Machine Trans-
lation in the Americas, pages 202?211, Waikiki,
Hawai?i, October.
P. Xu and F. Jelinek. 2004. Random forests in lan-
guage modeling. In Empirical Methods in Natural
Language Processing, pages 325?332. Association
for Computational Linguistics.
104
