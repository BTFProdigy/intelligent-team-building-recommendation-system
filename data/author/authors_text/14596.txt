Extracting Multiword Expressions with A Semantic Tagger 
Scott S. L. Piao 
Dept. of Linguistics and MEL 
Lancaster University 
 s.piao@lancaster.ac.uk 
Paul Rayson 
Computing Department 
Lancaster University 
 paul@comp.lancs.ac.uk 
Dawn Archer 
Dept. of Linguistics and MEL 
Lancaster University 
d.archer@lancaster.ac.uk
Andrew Wilson 
Dept. of Linguistics and MEL 
Lancaster University 
 eiaaw@exchange.lancs.ac.uk 
Tony McEnery 
Dept. of Linguistics and MEL 
Lancaster University 
 amcenery@lancaster.ac.uk 
Abstract 
Automatic extraction of multiword 
expressions (MWE) presents a tough 
challenge for the NLP community 
and corpus linguistics. Although 
various statistically driven or knowl-
edge-based approaches have been 
proposed and tested, efficient MWE 
extraction still remains an unsolved 
issue. In this paper, we present our 
research work in which we tested 
approaching the MWE issue using a 
semantic field annotator. We use an 
English semantic tagger (USAS) de-
veloped at Lancaster University to 
identify multiword units which de-
pict single semantic concepts. The 
Meter Corpus (Gaizauskas et al, 
2001; Clough et al, 2002) built in 
Sheffield was used to evaluate our 
approach. In our evaluation, this ap-
proach extracted a total of 4,195 
MWE candidates, of which, after 
manual checking, 3,792 were ac-
cepted as valid MWEs, producing a 
precision of 90.39% and an esti-
mated recall of 39.38%. Of the ac-
cepted MWEs, 68.22% or 2,587 are 
low frequency terms, occurring only 
once or twice in the corpus. These 
results show that our approach pro-
vides a practical solution to MWE 
extraction. 
1 Introduction 
2 
Automatic extraction of Multiword ex-
pressions (MWE) is an important issue in the 
NLP community and corpus linguistics. An 
efficient tool for MWE extraction can be use-
ful to numerous areas, including terminology 
extraction, machine translation, bilin-
gual/multilingual MWE alignment, automatic 
interpretation and generation of language. A 
number of approaches have been suggested 
and tested to address this problem. However, 
efficient extraction of MWEs still remains an 
unsolved issue, to the extent that Sag et al 
(2001b) call it ?a pain in the neck of NLP?. 
In this paper, we present our work in 
which we approach the issue of MWE extrac-
tion by using a semantic field annotator. Spe-
cifically, we use the UCREL Semantic 
Analysis System (henceforth USAS), devel-
oped at Lancaster University to identify mul-
tiword units that depict single semantic 
concepts, i.e. multiword expressions. We have 
drawn from the Meter Corpus (Gaizauskas et 
al., 2001; Clough et al, 2002) a collection of 
British newspaper reports on court stories to 
evaluate our approach. Our experiment shows 
that it is efficient in identifying MWEs, in 
particular MWEs of low frequencies. In the 
following sections, we describe this approach 
to MWE extraction and its evaluation. 
Related Works 
Generally speaking, approaches to MWE 
extraction proposed so far can be divided into 
three categories: a) statistical approaches 
based on frequency and co-occurrence affin-
ity, b) knowledge?based or symbolic ap-
proaches using parsers, lexicons and language 
filters, and c) hybrid approaches combining 
different methods (Smadja 1993; Dagan and 
Church 1994; Daille 1995; McEnery et al 
1997; Wu 1997; Wermter et al 1997; Mi-
chiels and Dufour 1998; Merkel and Anders-
son 2000; Piao and McEnery 2001; Sag et al 
2001a, 2001b; Biber et al 2003). 
In practice, most statistical approaches use 
linguistic filters to collect candidate MWEs. 
Such approaches include Dagan and Church?s 
(1994) Termight Tool. In this tool, they first 
collect candidate nominal terms with a POS 
syntactic pattern filter, then use concordances 
to identify frequently co-occurring multiword 
units. In his Xtract system, Smadja (1993) 
first extracted significant pairs of words that 
consistently co-occur within a single syntactic 
structure using statistical scores called dis-
tance, strength and spread, and then exam-
ined concordances of the bi-grams to find 
longer frequent multiword units. Similarly, 
Merkel and Andersson (2000) compared fre-
quency-based and entropy based algorithms, 
each of which was combined with a language 
filter. They reported that the entropy-based 
algorithm produced better results. 
One of the main problems facing statistical 
approaches, however, is that they are unable 
to deal with low-frequency MWEs. In fact, 
the majority of the words in most corpora 
have low frequencies, occurring only once or 
twice. This means that a major part of true 
multiword expressions are left out by statisti-
cal approaches. Lexical resources and parsers 
are used to obtain better coverage of the lexi-
con in MWE extraction. For example, Wu 
(1997) used an English-Chinese bilingual 
parser based on stochastic transduction 
grammars to identify terms, including multi-
word expressions. In their DEFI Project, Mi-
chiels and Dufour (1998) used dictionaries to 
identify English and French multiword ex-
pressions and their translations in the other 
language. Wehrli (1998) employed a genera-
tive grammar framework to identify com-
pounds and idioms in their ITS-2 MT 
English-French system. Sag et al (2001b) 
introduced Head-driven Phrase Structure 
Grammar for analyzing MWEs. Like pure 
statistical approaches, purely knowledge-
based symbolic approaches also face prob-
lems. They are language dependent and not 
flexible enough to cope with complex struc-
tures of MWEs. As Sag et al (2001b) sug-
gest, it is important to find the right balance 
between symbolic and statistical approaches. 
In this paper, we propose a new approach 
to MWEs extraction using semantic field in-
formation. In this approach, multiword units 
depicting single semantic concepts are recog-
nized using the Lancaster USAS semantic 
tagger. We describe that system and the algo-
rithms used for identifying single and multi-
word units in the following section. 
3 
                                                          
Lancaster Semantic tagger 
The USAS system has been in develop-
ment at Lancaster University since 1990 1 . 
Based on POS annotation provided by the 
CLAWS tagger (Garside and Smith, 1997), 
USAS assigns a set of semantic tags to each 
item in running text and then attempts to dis-
ambiguate the tags in order to choose the 
most likely candidate in each context. Items 
can be single words or multiword expressions. 
The semantic tags indicate semantic fields 
which group together word senses that are 
related by virtue of their being connected at 
some level of generality with the same mental 
concept. The groups include not only syno-
nyms and antonyms but also hypernyms and 
hyponyms. 
The initial tagset was loosely based on 
Tom McArthur's Longman Lexicon of Con-
temporary English (McArthur, 1981) as this 
appeared to offer the most appropriate thesau-
rus type classification of word senses for this 
kind of analysis. The tagset has since been 
considerably revised in the light of practical 
tagging problems met in the course of the re-
search. The revised tagset is arranged in a 
hierarchy with 21 major discourse fields ex-
panding into 232 category labels. The follow-
ing list shows the 21 labels at the top level of 
the hierarchy (for the full tagset, see website: 
http://www.comp.lancs.ac.uk/ucrel/usas). 
 
1 This work is continuing to be supported by the Bene-
dict project, EU project IST-2001-34237. 
A general and abstract terms 
B the body and the individual 
C arts and crafts 
E emotion 
F food and farming 
G government and the public domain 
H architecture, buildings, houses and the 
home 
I money and commerce in industry 
K entertainment, sports and games 
L life and living things 
M movement, location, travel and trans-
port 
N numbers and measurement 
O substances, materials, objects and 
equipment 
P education 
Q linguistic actions, states and processes 
S social actions, states and processes 
T time 
W the world and our environment 
X psychological actions, states and 
processes 
Y science and technology 
Z names and grammatical words 
 
Currently, the lexicon contains just over 
37,000 words and the template list contains 
over 16,000 multiword units. These resources 
were created manually by extending and ex-
panding dictionaries from the CLAWS tagger 
with observations from large text corpora. 
Generally, only the base form of nouns and 
verbs are stored in the lexicon and a lemmati-
sation procedure is used for look-up. How-
ever, the base form is not sufficient in some 
cases. Stubbs (1996: 40) observes that ?mean-
ing is not constant across the inflected forms 
of a lemma?, and Tognini-Bonelli (2001: 92) 
notes that lemma variants have different 
senses. 
In the USAS lexicon, each entry consists 
of a word with one POS tag and one or more 
semantic tags assigned to it. At present, in 
cases where a word has more than one syntac-
tic tag, it is duplicated (i.e. each syntactic tag 
is given a separate entry).  
The semantic tags for each entry in the 
lexicon are arranged in approximate rank fre-
quency order to assist in manual post editing, 
and to allow for gross automatic selection of 
the common tag, subject to weighting by do-
main of discourse. 
In the multi-word-unit list, each template 
consists of a pattern of words and part-of-
speech tags. The semantic tags for each tem-
plate are arranged in rank frequency order in 
the same way as the lexicon. Various types of 
multiword expressions are included: phrasal 
verbs (e.g. stubbed out), noun phrases (e.g. ski 
boots), proper names (e.g. United States), true 
idioms (e.g. life of Riley).  
Figure 1 below shows samples of the actual 
templates used to identify these MWUs. Each 
of these example templates has only one se-
mantic tag associated with it, listed on the 
right-hand end of the template. However, the 
second example (ski boot) combines the 
clothing (B5) and sports (K5.1) fields into one 
tag. The pattern on the left of each template 
consists of a sequence of words joined to POS 
tags with the underscore character. The words 
and POS fields can include the asterisk wild-
card character to allow for inflectional vari-
ants and to write more powerful templates 
with wider coverage. USAS templates can 
match discontinuous MWUs, and this is illus-
trated by the first example, which includes 
optional intervening POS items marked 
within curly brackets. Thus this template can 
match stubbed out and stubbed the cigarette 
out. ?Np? is used to match simple noun 
phrases identified with a noun-phrase chun-
ker. 
 
stub*_* {Np/P*/R*} out_RP    O4.6- 
ski_NN1 boot*_NN*          B5/K5.1 
United_* States_N*              Z2 
life_NN1 of_IO Riley_NP1        K1 
 
Figure 1 Sample of USAS multiword templates 
 
As in the case of grammatical tagging, the 
task of semantic tagging subdivides broadly 
into two phases: Phase I (Tag assignment): 
attaching a set of potential semantic tags to 
each lexical unit and Phase II (Tag disam-
biguation): selecting the contextually appro-
priate semantic tag from the set provided by 
Phase I. USAS makes use of seven major 
techniques or sources of information in phase 
II. We will list these only briefly here, since 
they are described in more detail elsewhere 
(Garside and Rayson, 1997). 
  
1. POS tag. Some senses can be elimi-
nated by prior POS tagging. The CLAWS 
part-of-speech tagger is run prior to semantic 
tagging. 
2. General likelihood ranking for single-
word and MWU tags. In the lexicon and 
MWU list senses are ranked in terms of fre-
quency, even though at present such ranking 
is derived from limited or unverified sources 
such as frequency-based dictionaries, past 
tagging experience and intuition.  
3. Overlapping MWU resolution. Nor-
mally, semantic multi-word units take priority 
over single word tagging, but in some cases a 
set of templates will produce overlapping 
candidate taggings for the same set of words. 
A set of heuristics is applied to enable the 
most likely template to be treated as the pre-
ferred one for tag assignment.  
4. Domain of discourse. Knowledge of 
the current domain or topic of discourse is 
used to alter rank ordering of semantic tags in 
the lexicon and template list for a particular 
domain.  
5. Text-based disambiguation. It has 
been claimed (by Gale et al 1992) on the ba-
sis of corpus analysis that to a very large ex-
tent a word keeps the same meaning 
throughout a text. 
6. Contextual rules. The template 
mechanism is also used in identifying regular 
contexts in which a word is constrained to 
occur in a particular sense.  
7. Local probabilistic disambiguation. It 
is generally supposed that the correct seman-
tic tag for a given word is substantially de-
termined by the local surrounding context.  
 
After automatic tag assignment has been 
carried out, manual post-editing can take 
place, if desired, to ensure that each word and 
idiom carries the correct semantic classifica-
tion. 
From these seven disambiguation meth-
ods, our main interest in this paper is the third 
technique of overlapping MWU resolution. 
When more than one template match overlaps 
in a sentence, the following heuristics are ap-
plied in sequence: 
 
1. Prefer longer templates over shorter 
templates 
2. For templates of the same length, pre-
fer shorter span matches over longer 
span matches (a longer span indicates 
more intervening items for discon-
tinuous templates) 
3. If the templates do not apply to the 
same sequence of words, prefer the 
one that begins earlier in the sentence 
4. For templates matching the same se-
quence of words, prefer the one 
which contains the more fully defined 
template pattern (with fewer wild-
cards in the word fields) 
5. Prefer templates with a more fully de-
fined first word in the template 
6. Prefer templates with fewer wildcards 
in the POS tags 
 
These six rules were found to differentiate 
in all cases of overlapping MWU templates. 
Cases which failed to be differentiated indi-
cated that two (or more) templates in our 
MWU list were in fact identical, apart from 
the semantic tags and required merging to-
gether. 
4 Experiment of MWE extraction 
In order to test our approach of extracting 
MWEs using semantic information, we first 
tagged the newspaper part of the METER 
Corpus with the USAS tagger. We then col-
lected the multiword units assigned as a single 
semantic unit. Finally, we manually checked 
the results. 
The Meter Corpus chosen as the test data 
is a collection of court reports from the Brit-
ish Press Association (PA) and some leading 
British newspapers (Gaizauskas 2001; Clough 
et al, 2002). In our experiment, we used the 
newspaper part of the corpus containing 774 
articles with more than 250,000 words. It pro-
vides a homogeneous corpus (in the sense that 
the reports come from a restricted domain of 
court events) and is thus a good source from 
which to extract domain-specific MWEs. 
Another reason for choosing this corpus is 
that it has not been used in training the USAS 
system. As an open test, we assume the re-
sults of the experiment should reflect true ca-
pability of our approach for real-life 
applications. 
The current USAS tagger may assign mul-
tiple possible semantic tags for a term when it 
fails to disambiguate between them. As men-
tioned previously, the first one denotes the 
most likely semantic field of the term. There-
fore, in our experiment we chose the first tag 
when such situations arose. 
A major problem we faced in our experi-
ment is the definition of a MWE. Although it 
has been several years since people started to 
work on MWE extraction, we found that there 
is, as yet, no available ?clear-cut? definition 
for MWEs. We noticed various possible defi-
nitions have been suggested for MWE/MWU. 
For example, Smadja (1993) suggests a basic 
characteristic of collocations and multiword 
units is recurrent, domain-dependent and co-
hesive lexical clusters. Sag et el. (2001b) sug-
gest that MWEs can roughly be defined as 
?idiosyncratic interpretations that cross word 
boundaries (or spaces)?. Biber et al (2003) 
describe MWEs as lexical bundles, which 
they go on to define as combinations of words 
that can be repeated frequently and tend to be 
used frequently by many different speak-
ers/writers within a register. 
Although it is not difficult to interpret 
these deifications in theory, things became 
much more complicated when we undertook 
our practical checking of the MWE candi-
dates. Quite often, we experienced disagree-
ment between us about whether or not to 
accept a MWE candidate as a good one. In 
practice, we generally followed Biber et al?s 
definition, i.e. accept a candidate MWE as a 
good one if it can repeatedly co-occur in the 
corpus. 
Another difficulty we experienced relates 
to estimating recall. Because the MWEs in the 
METER Corpus are not marked-up, we could 
not automatically calculate the number of 
MWEs contained in the corpus. Conse-
quently, we had to manually estimate this fig-
ure. Obviously it is not practical to manually 
check though the whole corpus within the 
limited time allowed. Therefore, we had to 
estimate the recall on a sample of the corpus, 
as will be described in the following section. 
5 Evaluation 
In this section, we analyze the results of 
the MWE extraction in detail for a full 
evaluation of our approach to MWE extrac-
tion. 
Overall, after we processed the test corpus, 
the USAS tagger extracted 4,195 MWE can-
didates from the test corpus. After manually 
checking through the candidates, we selected 
3,792 as good MWEs, resulting in overall 
precision of 90.39%. 
As we explained earlier, due to the diffi-
culty of obtaining the total number of true 
MWEs in the entire test corpus, we had to 
estimate recall of the MWE extraction on a 
sample corpus. In detail, we first randomly 
selected fifty texts containing 14,711 words 
from the test corpus, then manually marked-
up good MWEs in the sample texts, finally 
counted the number of the marked-up MWUs. 
As a result, 1,511 good MWEs were found in 
the sample. Since the number of automatically 
extracted good MWEs in the sample is 595, 
the recall on the sample is calculated as fol-
lows: 
Recall=(595?1511)?100%=39.38%. 
Considering the homogenous feature of 
the test data, we assume this local recall is 
roughly approximate to the global recall of 
the test corpus. 
To analyze the performance of USAS in 
respect to the different semantic field catego-
ries, we divided candidates according to the 
assigned semantic tag, and calculated the pre-
cision for each of them. Table 1 lists these 
precisions, sorting the semantic fields by the 
number of MWE candidates (refer to section 
3 for definitions of the twenty-one main se-
mantic field categories). As shown in this ta-
ble, the USAS semantic tagger obtained 
precisions between 91.23% to 100.00% for 
each semantic field except for the field of 
?names and grammatical words? denoted by 
Z. As Z was the biggest field (containing 
45.39% of the total MWEs and 43.12% of the 
accepted MWEs), we examined these MWEs 
more closely. We discovered that numerous 
pairs of words are tagged as person names 
(Z1) and geographical names (Z2) by mistake, 
e.g. Blackfriars crown (tagged as Z1), stabbed 
Constance (tagged as Z2) etc. 
 
Semantic 
field 
Total 
MWEs 
Accepted 
MWEs 
Precision 
Z 1,904  1,635 85.87% 
T 497  459 92.35% 
A 351  328 93.44% 
M 254  241 94.88% 
N 227  211 92.95% 
S 180  177 98.33% 
B 131  128 97.71% 
G 118  110 93.22% 
X 114  104 91.23% 
I 74  72 97.30% 
Q 67  63 94.03% 
E 58  53 91.38% 
H 53  52 98.11% 
K 48  45 93.75% 
P 39  37 94.87% 
O 32  29 90.63% 
F 24  24 100.00% 
L 11  11 100.00% 
Y 6  6 100.00% 
C 5  5 100.00% 
W 2  2 100.00% 
Total 4,195 3,792 90.39% 
 
Table 1: Precisions for different semantic catego-
ries 
 
Another possible factor that affects the 
performance of the USAS tagger is the length 
of the MWEs. To observe the performance of 
our approach from this perspective, we 
grouped the MWEs by their lengths, and then 
checked precision for each of the categories. 
Table 2 shows the results (once again, they 
are sorted in descending order by MWE 
lengths). As we might expect, the number of 
MWEs decreases as the length increases. In 
fact, bi-grams alone constitute 80.52% and 
81.88% of the candidate and accepted MWEs 
respectively. The precision also showed a 
generally increasing trend as the MWE length 
increases, but with a major divergence of tri-
grams. One main type of error occurred on tri-
grams is that those with the structure of 
CIW(capital-initial word) + conjunction + 
CIW tend to be tagged as Z2 (geographical 
name). The table shows relatively high preci-
sion for longer MWEs, reaching 100% for 6-
grams. Because the longest MWEs extracted 
have six words, no longer MWEs could be 
examined.  
 
MWE 
length 
Total 
MWEs 
Accepted 
MWEs 
Precision 
2 3,378 3,105 91.92% 
3 700 575 82.14% 
4 95 91 95.44% 
5 18 17 94.44% 
6 4 4 100.00% 
Total 4,195 3,792 90.39% 
 
Table 2: Precisions for MWEs of different lengths 
 
As discussed earlier, purely statistical al-
gorithms of MWE extraction generally filter 
out candidates of low frequencies. However, 
such low-frequency terms in fact form major 
part of MWEs in most corpora. In our study, 
we attempted to investigate the possibility of 
extracting low frequency MWEs by using 
semantic field annotation. We divided MWEs 
into different frequency groups, then checked 
precision for each of the categories. Table 3 
shows the results, which are sorted by the 
candidate MWE frequencies. As we expected, 
69.46% of the candidate MWEs and 68.22% 
of the accepted MWEs occur in the corpus 
only once or twice. This means that, with a 
frequency filter of Min(f)=3, a purely statisti-
cal algorithm would exclude more than half of 
the candidates from the process. 
 
Freq. of 
MWE 
Total 
number 
Accepted 
MWEs 
Precision 
1 2,164  1,892 87.43% 
2 750  695 92.67% 
3 - 4 616 570 92.53% 
5 - 7 357 345 96.64% 
8 - 20 253 238 94.07% 
21 - 117 55 52 94.55% 
Total 4,195 3,792 90.39% 
 
Table 3: Precisions for MWEs with different fre-
quencies 
 
Table 3 also displays an interesting rela-
tionship between the precisions and the fre-
quencies. Generally, we would expect better 
precisions for MWEs of higher frequencies, 
as higher co-occurrence frequencies are ex-
pected to reflect stronger affinity between the 
words within the MWEs. By and large, 
slightly higher precisions were obtained for 
the latter groups of higher frequencies (5?7, 
8-20 and 21-117) than those for the preceding 
lower frequency groups, i.e. 94.07%-96.64% 
versus 87.43%-92.67%. Nevertheless, for the 
latter three groups of the higher frequencies 
(5-7, 8-20 and 21?117) the precision did not 
increase as the frequency increases, as we 
initially expected. 
When we made a closer examination of 
the error MWEs in this frequency range, we 
found that some frequent domain-specific 
terms are misclassified by the USAS tagger. 
For example, since the texts in the test corpus 
are newspaper reports of court stories, many 
law courts (e.g. Manchester crown court, 
Norwich crown court) are frequently men-
tioned throughout the corpus, causing high 
frequencies of such terms (f=20 and f=31 re-
spectively). Unfortunately, the templates used 
in the USAS tagger did not capture them as 
complete terms. Rather, fragments were as-
signed a Z1 person name tag (e.g. Manchester 
crown). A solution to this type of problem is 
to improve the multiword unit templates used 
in the USAS tagger. Other possible solutions 
may include incorporating a statistical algo-
rithm to help detect boundaries of complete 
MWEs. 
When we examined the error distribution 
within the semantic fields more closely, we 
found that most errors occurred within the Z 
and T categories (refer to Table 1). The errors 
occurring in these semantic field categories 
and their sub-divisions make up 76.18% of 
the total errors (403). Table 4 shows the error 
distribution across 14 sub-divisions (for defi-
nitions of these subdivisions, see: website: 
http://www.comp.lancs.ac.uk/ucrel/usas). No-
tice that the majority of errors are from four 
semantic sub-categories: Z1, Z2, Z3 and T1.3. 
Notice, also, that the first two of these ac-
count for 60.55% of the total errors. This 
shows that the main cause of the errors in the 
USAS tool is the algorithm and lexical entries 
used for identifying names - personal and 
geographical and, to a lesser extent, the algo-
rithm and lexical entries for identifying peri-
ods of time. If these components of the USAS 
can be improved, a much higher precision can 
be expected. 
In sum, our evaluation shows that our se-
mantic approach to MWE extraction is effi-
cient in identifying MWEs, in particular those 
of lower frequencies. In addition, a reasona-
bly wide lexical coverage is obtained, as indi-
cated by the recall of 39.38%, which is 
important for terminology building. Our ap-
proach provides a practical way for extracting 
MWEs on a large scale, which we envisage 
can be useful for both linguistic research and 
practical NLP applications. 
 
Stag  Err. Stag  Err. 
Z1:person names 119 T1.1.1:time-past 1 
Z2:geog. names 125 T1.1.2:time-present 1 
Z3:other names 16 T1.2:time-momentary 8 
Z4:discourse bin 3 T1.3:time-period 23 
Z5:gram. bin 2 T2:time-begin/end 2 
Z8:pronouns etc. 2 T3:time-age 1 
Z99:unmatched 2 T4:time-early/late 2 
 
Table 4: Errors for some semantic sub-divisions 
6 Conclusion 
In this paper, we have shown that it is a 
practical way to extract MWEs using seman-
tic field information. Since MWEs are lexical 
units carrying single semantic concepts, it is 
reasonable to consider the issue of MWE ex-
traction as an issue of identifying word bun-
dles depicting single semantic units. The main 
difficulty facing such an approach is that very 
few reliable automatic tools available for 
identifying lexical semantic units. However, a 
semantic field annotator, USAS, has been 
built in Lancaster University. Although it was 
not built aiming to the MWE extraction, we 
thought it might be very well suited for this 
purpose. Our experiment shows that the 
USAS tagger is indeed an efficient tool for 
MWE extraction. 
Nevertheless, the current semantic tagger 
does not provide a complete solution to the 
problem. During our experiment, we found 
that not all of the multiword units it collects 
are valid MWEs. An efficient algorithm is 
needed for distinguishing between free word 
combinations and relatively fixed, closely 
affiliated word bundles. 
References 
Douglas Biber, Susan Conrad and Viviana Cortes. 
2003. Lexical bundles in speech and writing: an 
initial taxonomy. In A. Wilson, P. Rayson and 
T. McEnery (eds.) Corpus Linguistics by the 
Lune: a festschrift for Geoffrey Leech, pp. 71-
92.  Peter Lang, Frankfurt. 
Paul Clough, Robert Gaizauskas and S. L. Piao. 
2002. Building and annotating a corpus for the 
study of journalistic text reuse. In Proceedings 
of the 3rd International Conference on Lan-
guage Resources and Evaluation (LREC-2002), 
pp. 1678-1691. Los Palmas de Gran Canaria, 
Spain. 
Ido Dagan, and Ken Church. 1994. Termight: 
identifying and translating technical terminol-
ogy. In Proceedings of the 4th Conference on 
Applied Natural Language Processing, pp. 34-
40. Stuttgard, German. 
B?atrice Daille. 1995. Combined approach for 
terminology extraction: lexical statistics and 
linguistic filtering. Technical paper. UCREL, 
Lancaster University. 
Robert Gaizauskas, Jonathan Foster, Yorick 
Wilks, John Arundel, Paul Clough and Scott 
Piao. 2001. The METER corpus: a corpus for 
analysing journalistic text reuse. In the Pro-
ceedings of the Corpus Linguistics 2001, pp: 
214-223. Lancaster, UK. 
William Gale, Kenneth Church, and David 
Yarowsky. 1992. One sense per discourse. In 
Proceedings of the 4th DARPA Speech and 
Natural Language Workshop, pp 233-7. 
Roger Garside and Nick Smith. 1997. A hybrid 
grammatical tagger: CLAWS4. In R. Garside, 
G. Leech and A. McEnery (eds.), Corpus 
Annotation: Linguistic Information from 
Computer Text Corpora, pp. 102-121. Long-
man, London. Roger Garside and Paul Rayson. 1997. Higher-
level annotation tools. In. Roger Garside, Geof-
frey Leech, and Tony McEnery (eds.) Corpus 
Annotation: Linguistic Information from Com-
puter Text Corpora, pp. 179 - 193. Longman, 
London.  
Tom McArthur. 1981. Longman Lexicon of 
Contemporary English. Longman, London. 
Tony McEnery, Lang? Jean-Marc, Oakes Michael 
and V?ronis Jean. 1997. The exploitation of 
multilingual annotated corpora for term extrac-
tion. In Garside Roger, Leech Geoffrey and 
McEnery Anthony (eds), Corpus annotation --- 
linguistic information from computer text cor-
pora, pp 220-230. London & New York, Long-
man. 
Magnus Merkel and Mikael Andersson. 2000. 
Knowledge-lite extraction of multi-word units 
with language filters and entropy thresholds. In 
Proceedings of 2000 Conference User-Oriented 
Content-Based Text and Image Handling 
(RIAO'00), pages 737--746, Paris, France. 
Archibald Michiels and Nicolas Dufour.1998. 
DEFI, a tool for automatic multi-word unit rec-
ognition, meaning assignment and translation 
selection. In Proceedings of the First Interna-
tional Conference on Language Resources & 
Evaluation, pp. 1179-1186. Granada, Spain. 
Scott Songlin Piao and Tony McEnery. 2001. 
Multi-word unit alignment in English-Chinese 
parallel corpora. In the Proceedings of the Cor-
pus Linguistics 2001, pp. 466-475. Lancaster, 
UK. 
Ivan A. Sag, Francis Bond, Ann Copestake and 
Dan Flickinger. 2001a. Multiword Expressions. 
LinGO Working Paper No. 2001-01. Stanford 
University, CA. 
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake and Dan Flickinger. 2001b. Multi-
word Expressions: A Pain in the Neck for NLP. 
LinGO Working Paper No. 2001-03. Stanford 
University, CA. 
Frank Smadja. 1993. Retrieving collocations from 
text: Xtract. Computational Linguistics 
19(1):143-177. 
Michael Stubbs. 1996. Text and corpus analysis: 
computer-assisted studies of language and cul-
ture. Blackwell, Oxford. 
Elena Tognini-Bonelli. 2001. Corpus linguistics at 
work. Benjamins, The Netherlands. 
Eric Wehrli. 1998. Translating idioms. In Pro-
ceedings of COLING-ACL ?98, Montreal, Can-
ada, Vol. 2, pp. 1388-1392. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics 23(3): 377-
401. 
 
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 2?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Measuring MWE Compositionality Using Semantic Annotation 
Scott S. L. Piao1, Paul Rayson1, Olga Mudraya2, Andrew Wilson2 and Roger Garside1 
 
1Computing Department 
Lancaster University 
Lancaster, UK 
{s.piao, p.rayson, r.garside}@lancaster.ac.uk 
2Dept. of Linguistics and EL 
Lancaster University 
Lancaster, UK 
{o.mudraya, a.wilson}@lancaster.ac.uk 
 
 
Abstract 
This paper reports on an experiment in 
which we explore a new approach to the 
automatic measurement of multi-word 
expression (MWE) compositionality. We 
propose an algorithm which ranks MWEs 
by their compositionality relative to a 
semantic field taxonomy based on the 
Lancaster English semantic lexicon (Piao 
et al, 2005a). The semantic information 
provided by the lexicon is used for meas-
uring the semantic distance between a 
MWE and its constituent words. The al-
gorithm is evaluated both on 89 manually 
ranked MWEs and on McCarthy et als 
(2003) manually ranked phrasal verbs. 
We compared the output of our tool with 
human judgments using Spearman?s 
rank-order correlation coefficient. Our 
evaluation shows that the automatic rank-
ing of the majority of our test data 
(86.52%) has strong to moderate correla-
tion with the manual ranking while wide 
discrepancy is found for a small number 
of MWEs. Our algorithm also obtained a 
correlation of 0.3544 with manual rank-
ing on McCarthy et als test data, which 
is comparable or better than most of the 
measures they tested. This experiment 
demonstrates that a semantic lexicon can 
assist in MWE compositionality meas-
urement in addition to statistical algo-
rithms. 
1 Introduction 
Over the past few years, compositionality and 
decomposability of MWEs have become impor-
tant issues in NLP research. Lin (1999) argues 
that ?non-compositional expressions need to be 
treated differently than other phrases in many 
statistical or corpus?based NLP methods?. Com-
positionality means that ?the meaning of the 
whole can be strictly predicted from the meaning 
of the parts? (Manning & Sch?tze, 2000). On the 
other hand, decomposability is a metric of the 
degree to which the meaning of a MWE can be 
assigned to its parts (Nunberg, 1994; Riehemann, 
2001; Sag et al, 2002). These two concepts are 
closely related. Venkatapathy and Joshi (2005) 
suggest that ?an expression is likely to be rela-
tively more compositional if it is decomposable?. 
While there exist various definitions for 
MWEs, they are generally defined as cohesive 
lexemes that cross word boundaries (Sag et al, 
2002; Copestake et al, 2002; Calzolari et al, 
2002; Baldwin et al, 2003), which include 
nominal compounds, phrasal verbs, idioms, col-
locations etc. Compositionality is a critical crite-
rion cutting across different definitions for ex-
tracting and classifying MWEs. While semantics 
of certain types of MWEs are non-compositional, 
like idioms ?kick the bucket? and ?hot dog?, 
some others can have highly compositional se-
mantics like the expressions ?traffic light? and 
?audio tape?. 
Automatic measurement of MWE composi-
tionality can have a number of applications. One 
of the often quoted applications is for machine 
translation (Melamed, 1997; Hwang & Sasaki, 
2005), in which non-compositional MWEs need 
special treatment. For instance, the translation of 
a highly compositional MWE can possibly be 
inferred from the translations of its constituent 
words, whereas it is impossible for non-
compositional MWEs, for which we need to 
identify the translation equivalent for the MWEs 
as a whole. 
In this paper, we explore a new method of 
automatically estimating the compositionality of 
MWEs using lexical semantic information, 
sourced from the Lancaster semantic lexicon 
(Piao et al, 2005a) that is employed in the 
USAS1 tagger (Rayson et al, 2004). This is a 
                                                 
1 UCREL Semantic Analysis System 
2
large lexical resource which contains nearly 
55,000 single-word entries and over 18,800 
MWE entries. In this lexicon, each MWE2 and 
the words it contains are mapped to their poten-
tial semantic categories using a semantic field 
taxonomy of 232 categories. An evaluation of 
lexical coverage on the BNC corpus showed that 
the lexical coverage of this lexicon reaches 
98.49% for modern English (Piao et al, 2004).  
Such a large-scale semantic lexical resource al-
lows us to examine the semantics of many 
MWEs and their constituent words conveniently 
without resorting to large corpus data. Our ex-
periment demonstrates that such a lexical re-
source provides an additional approach for auto-
matically estimating the compositionality of 
MWEs. 
One may question the necessity of measuring 
compositionality of manually selected MWEs. 
The truth is, even if the semantic lexicon under 
consideration was compiled manually, it does not 
exclusively consist of non-compositional MWEs 
like idioms. Built for practical discourse analysis, 
it contains many MWEs which are highly com-
positional but depict certain entities or semantic 
concepts. This research forms part of a larger 
effort to extend lexical resources for semantic 
tagging. Techniques are described elsewhere 
(e.g. Piao et al, 2005b) for finding new candi-
date MWE from corpora. The next stage of the 
work is to semi-automatically classify these can-
didates using an existing semantic field taxon-
omy and, to assist this task, we need to investi-
gate patterns of compositionality. 
2 Related Work  
In recent years, various approaches have been 
proposed to the analysis of MWE compositional-
ity. Many of the suggested approaches employ 
statistical algorithms. 
One of the earliest studies in this area was re-
ported by Lin (1999) who assumes that ?non-
compositional phrases have a significantly dif-
ferent mutual information value than the phrases 
that are similar to their literal meanings? and 
proposed to identify non-compositional MWEs 
in a corpus based on distributional characteristics 
of MWEs. Bannard et al (2003) tested tech-
niques using statistical models to infer the mean-
ing of verb-particle constructions (VPCs), focus-
                                                 
2 In this lexicon, many MWEs are encoded as templates, 
such as driv*_* {Np/P*/J*/R*} mad_JJ,  which represent 
variational forms of a single MWE, For further details, see 
Rayson et al, 2004.  
ing on prepositional particles. They tested four 
methods over four compositional classification 
tasks, reporting that, on all tasks, at least one of 
the four methods offers an improvement in preci-
sion over the baseline they used. 
McCarthy et al (2003) suggested that compo-
sitional phrasal verbs should have similar 
neighbours as for their simplex verbs. They 
tested various measures using the nearest 
neighbours of phrasal verbs and their simplex 
counterparts, and reported that some of the 
measures produced results which show signifi-
cant correlation with human judgments. Baldwin 
et al (2003) proposed a LSA-based model for 
measuring the decomposability of MWEs by ex-
amining the similarity between them and their 
constituent words, with higher similarity indicat-
ing the greater decomposability.  They evaluated 
their model on English noun-noun compounds 
and verb-particles by examining the correlation 
of the results with similarities and hyponymy 
values in WordNet. They reported that the LSA 
technique performs better on the low-frequency 
items than on more frequent items. Venkatapathy 
and Joshi (2005) measured relative composition-
ality of collocations having verb-noun pattern 
using a SVM (Support Vector Machine) based 
ranking function. They integrated seven various 
collocational and contextual features using their 
ranking function, and evaluated it against manu-
ally ranked test data. They reported that the SVM 
based method produces significantly better re-
sults compared to methods based on individual 
features. 
The approaches mentioned above invariably 
depend on a variety of statistical contextual in-
formation extracted from large corpus data. In-
evitably, such statistical information can be af-
fected by various uncontrollable ?noise?, and 
hence there is a limitation to purely statistical 
approaches. 
In this paper, we contend that a manually 
compiled semantic lexical resource can have an 
important part to play in measuring the composi-
tionality of MWEs. While any approach based on 
a specific lexical resource may lack generality, it 
can complement purely statistical approaches by 
importing human expert knowledge into the 
process. Particularly, if such a resource has a 
high lexical coverage, which is true in our case, 
it becomes much more useful for dealing with 
general English. It should be emphasized that we 
propose our semantic lexical-based approach not 
as a substitute for the statistical approaches. 
3
Rather we propose it as a potential complement 
to them.   
In the following sections, we describe our ex-
periment and explore this approach to the issue 
of automatic estimation of MWE compositional-
ity. 
3 Measuring MWE compositionality 
with semantic field information 
In this section, we propose an algorithm for 
automatically measuring MWE compositionality 
based on the Lancaster semantic lexicon. In this 
lexicon, the semantic field of each word and 
MWE is encoded in the form of semantic tags. 
We contend that the compositionality of a MWE 
can be estimated by measuring the distance be-
tween semantic fields of an MWE and its con-
stituent words based on the semantic field infor-
mation available from the lexicon. 
The lexicon employs a taxonomy containing 
21 major semantic fields which are further di-
vided into 232 sub-categories. 3  Tags are de-
signed to denote the semantic fields using letters 
and digits. For instance, tag N3.2 denotes the 
category of {SIZE} and Q4.1 denotes {media: 
Newspapers}. Each entry in the lexicon maps a 
word or MWE to its potential semantic field 
category/ies. More often than not, a lexical item 
is mapped to multiple semantic categories, re-
flecting its potential multiple senses. In such 
cases, the tags are arranged by the order of like-
lihood of meanings, with the most prominent one 
at the head of the list. For example, the word 
?mass? is mapped to tags N5, N3.5, S9, S5 and 
B2, which denote its potential semantic fields of 
{QUANTITIES},  {MEASUREMENT: 
WEIGHT}, {RELIGION AND SUPERNATU-
RAL}, {GROUPS AND AFFILIATION} and 
{HEALTH AND DISEASE}. 
 The lexicon provides direct access to the se-
mantic field information for large number of 
MWEs and their constituent words. Furthermore, 
the lexicon was analysed and classified manually 
by a team of linguists based on the analysis of 
corpus data and consultation of printed and elec-
tronic corpus-based dictionaries, ensuring a high 
level of consistency and accuracy of the semantic 
analysis.  
In our context, we interpret the task of measur-
ing the compositionality of MWEs as examining 
the distance between the semantic tag of a MWE 
and the semantic tags of its constituent words. 
                                                 
3 For the complete semantic tagset, see website: 
http://www.comp.lancs.ac.uk/ucrel/usas/ 
Given a MWE M and its constituent words wi (i 
= 1, .., n), the compositionality D can be meas-
ured by multiplying the semantic distance SD 
between M and each of its constituent words wi. 
In practice, the square root of the product is used 
as the score in order to reduce the range of actual 
D-scores, as shown below: 
 
(1)   ?
=
=
n
i
iwMSDMD
1
),()(  
 
where D-score ranges between [0, 1], with 1 in-
dicating the strongest compositionality and 0 the 
weakest compositionality. 
In the semantic lexicon, as the semantic in-
formation of function words is limited, they are 
classified into a single grammatical bin (denoted 
by tag Z5). In our algorithm, they are excluded 
from the measuring process by using a stop word 
list. Therefore, only the content constituent 
words are involved in measuring the composi-
tionality. Although function words may form an 
important part of many MWEs, such as phrasal 
verbs, because our algorithm solely relies on se-
mantic field information, we assume they can be 
ignored.  
 The semantic distance between a MWE and 
any of its constituent words is calculated by 
quantifying the similarity between their semantic 
field categories. In detail, if the MWE and a con-
stituent word do not share any of the major 21 
semantic domains, the SD is assigned a small 
value ?.4 If they do, three possible cases are con-
sidered: 
 
Case a. If they share the same tag, and the con-
stituent word has only one tag, then SD 
is one. 
Case b. If they share a tag or tags, but the con-
stituent words have multiple candidate 
tags, then SD is weighted using a vari-
able ? based on the position of the 
matched tag in the candidate list as well 
as the number of candidate tags. 
Case c. If they share a major category, but their 
tags fall into different sub-categories 
(denoted by the trailing digits following 
a letter), SD is further weighted using a 
                                                 
4 We avoid using zero here in order to avoid producing se-
mantic distance of zero indiscriminately when any one of 
the constituent words produces zero distance regardless of 
other constituent words. 
4
variable ? which reflects the difference 
of the sub-categories. 
With respect to weight ?, suppose L is the 
number of candidate tags of the constituent word 
under consideration, N is the position of the spe-
cific tag in the candidate list (the position starts 
from the top with N=1), then the weight ? is cal-
culated as 
 
(2)  
2
1
L
NL +?=? , 
 
where N=1, 2, ?, n and N<=L. Ranging between 
[1, 0), ? takes into account both the location of 
the matched tag in the candidate tag list and the 
number of candidate tags. This weight penalises 
the words having more candidate semantic tags 
by giving a lower value for their higher degree of 
ambiguity. As either L or N increases, the ?-
value decreases.       
Regarding the case c), where the tags share the 
same head letter but different digit codes, i.e. 
they are from the same major category but in 
different sub-categories, the weight ? is calcu-
lated based on the number of sub-categories they 
share. As we mentioned earlier, a semantic tag 
consists of an initial letter and some trailing dig-
its divided by points, e.g. S1.1.2 {RECIPROC-
ITY}, S1.1.3 {PARTICIPATION}, S1.1.4 {DE-
SERVE} etc. If we let T1, T2 be a pair of semantic 
tags with the same initial letters, which have ki 
and kj trailing digit codes (denoting the number 
of sub-division layers) respectively and share n 
digit codes from the left, or from the top layer, 
then ? is calculated as follows: 
 
(3)   
k
n=? ; 
(4)   . ),max( ji kkk =
 
where ? ranges between (0, 1). In fact, the cur-
rent USAS taxonomy allows only the maximum 
three layers of sub-division, therefore ? has one 
of three possible scores: 0.500 (1/2), 0.333 (1/3) 
and 0.666 (2/3). In order to avoid producing zero 
scores, if the pair of tags do not share any digit 
codes except the head letter, then n is given a 
small value of 0.5. 
Combining all of the weighting scores, the 
semantic distance SD in formula (1) is calculated 
as follows: 
 
(5)  ( )
??
?
?
??
?
?
?
=
?
?
=
=
.  then   c), if
;  then   b), if
1;  then   a), if
;   then   matches,   tagno if
,
1
1
n
i
ii
n
i
iiwMSD
??
?
?
 
where ? is given a small value of 0.001 for our 
experiment5. 
Some MWEs and single words in the lexicon 
are assigned with combined semantic categories 
which are considered to be inseparable, as shown 
below: 
petrol_NN1 station_NN1 M3/H1 
where the slash means that this MWE falls under 
the categories of M3 {VEHICLES AND TRANS-
PORTS ON LAND} and H1 {ARCHITECTURE 
AND KINDS OF HOUSES AND BUILDINGS} 
at the same time. For such cases, criss-cross 
comparisons between all possible tag pairs are 
carried out in order to find the optimal match 
between the tags of the MWE and its constituent 
words. 
By way of further explanation, the word 
?brush? as a verb has candidate semantic tags of 
B4 {CLEANING AND PERSONAL CARE} and 
A1.1.1 {GENERAL ACTION, MAKING} etc. On 
the other hand, the phrasal verb ?brush down? 
may fall under either B4 category with the sense 
of cleaning or G2.2 category {ETHICS} with the 
sense of reprimand. When we apply our algo-
rithm to it, we get the D-score of 1.0000 for the 
sense of cleaning, indicating a high degree of 
compositionality, whereas we get a low D-score 
of 0.0032 for the sense of reprimand, indicating 
a low degree of compositionality. Note that the 
word ?down? in this MWE is filtered out as it is 
a functional word. 
The above example has only a single constitu-
ent content word. In practice, many MWEs have 
more complex structures than this example. In 
order to test the performance of our algorithm, 
we compared its output against human judgments 
of compositionality, as reported in the following 
section. 
4 Manually Ranking MWEs for 
Evaluation 
In order to evaluate the performance of our 
tool against human judgment, we prepared a list 
                                                 
5 As long as ? is small enough, it does not affect the ranking 
of D-scores. 
5
of 89 MWEs6 and asked human raters to rank 
them via a website. The list includes six MWEs 
with multiple senses, and these were treated as 
separate MWE. The Lancaster MWE lexicon has 
been compiled manually by expert linguists, 
therefore we assume that every item in this lexi-
con is a true MWE, although we acknowledge 
that some errors may exist. 
Following McCarthy et al?s approach, we 
asked the human raters to assign each MWE a 
number ranging between 0 (opaque) and 10 
(fully compositional). Both native and non-native 
speakers are involved, but only the data from 
native speakers are used in this evaluation. As a 
result, three groups of raters were involved in the 
experiment.  Group 1 (6 people) rated MWEs 
with indexes of 1-30, Group 2 (4 people) rated 
MWEs with indexes of 31-59 and Group 3 (five 
people) rated MWEs with indexes of 6-89. 
In order to test the level of agreement between 
the raters, we used the procedures provided in 
the 'irr' package for R (Gamer, 2005). With this 
tool, the average intraclass correlation coefficient 
(ICC) was calculated for each group of raters 
using a two-way agreement model (Shrout & 
Fleiss, 1979). As a result, all ICCs exceeded 0.7 
and were significant at the 95% confidence level, 
indicating an acceptable level of agreement be-
tween raters. For Group 1, the ICC was 0.894 
(95% ci = 0.807 < ICC < 0.948), for Group 2 it 
was 0.9 (95% ci=0.783<ICC<0.956) and for 
Group 3 it was 0.886 (95% ci =  0.762 < ICC < 
0.948). 
Based on this test, we conclude that the man-
ual ranking of the MWEs is reliable and is suit-
able to be used in our evaluation. Source data for 
the human judgements is available from our 
website in spreadsheet form7. 
5 Evaluation 
In our evaluation, we focused on testing the 
performance of the D-score against human rat-
ers? judgment on ranking different MWEs by 
their degree of compositionality, as well as dis-
tinguishing the different degrees of composition-
ality for each sense in the case of multiple tags.  
The first step of the evaluation was to imple-
ment the algorithm in a program and run the tool 
on the 89 test MWEs we prepared. Fig. 1 illus-
trates the D-score distribution in a bar chart. As 
shown by the chart, the algorithm produces a 
widely dispersed distribution of D-scores across 
                                                 
6 Selected at random from the Lancaster semantic lexicon. 
7 http://ucrel.lancs.ac.uk/projects/assist/ 
the sample MWEs, ranging from 0.000032 to 
1.000000. For example, the tool assigned the 
score of 1.0 to the FOOD sense and 0.001 to the 
THIEF senses of ?tea leaf? successfully distin-
guishing the different degrees of compositional-
ity of these two senses. 
 
MWE Compositionality Distribution
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86
89 MWEs
D
-s
co
re
 
 
Fig 1: D-score distribution across 89 sample 
MWEs 
 
As shown in Fig. 1, some MWEs share the 
same scores, reflecting the limitation of the num-
ber of ranks that our algorithm can produce as 
well as the limited amount of semantic informa-
tion available from a lexicon. Nonetheless, the 
algorithm produced 45 different scores which 
ranked the MWEs into 45 groups (see the steps 
in the figure). Compared to the eleven scores 
used by the human raters, this provides a fine-
grained ranking of the compositionality.   
The primary issue in our evaluation is the ex-
tent to which the automatic ranking of the MWEs 
correlates with the manual ranking of them. As 
described in the previous section, we created a 
list of 89 manually ranked MWEs for this pur-
pose. Since we are mainly interested in the ranks 
rather than the actual scores, we examined the 
correlation between the automatic and manual 
rankings using Spearman?s correlation coeffi-
cient. (For the full ranking list, see Appendix). 
In the manually created list, each MWE was 
ranked by 3-6 human raters. In order to create a 
unified single test data of human ranking, we 
calculated the average of the human ranks for 
each MWE. For example, if two human raters 
give ranks 3 and 4 to a MWE, then its rank is 
(3+4)/2=3.5. Next, the MWEs are sorted by the 
averaged ranks in descending order to obtain the 
combined ranks of the MWEs. Finally, we sorted 
the MWEs by the D-score in the same way to 
obtain a parallel list of automatic ranks. For the 
calculation of Spearman?s correlation coefficient, 
if n MWEs are tied to a score (either D-score or 
the average manual ranks), their ranks were ad-
6
justed by dividing the sum of their ranks by the 
number of MWEs involved. Fig. 2 illustrates the 
correspondence between the adjusted automatic 
and manual rankings. 
 
Auto vs. Manual Ranks Comparison
(n=89, rho=0.2572)
0
20
40
60
80
100
0 20 40 60 80 100
auto ranks
m
an
ua
l r
an
ks
 
 
Fig. 2: Scatterplot of automatic vs. manual 
ranking. 
 
As shown in Fig. 2, the overall correlation seems 
quite weak. In the automatic ranking, quite a few 
MWEs are tied up to three ranks, illustrated by 
the vertically aligned points. The precise correla-
tion between the automatic and manual rankings 
was calculated using the function provided in R 
for Windows 2.2.1.  Spearman's rank correlation 
(rho) for these data was 0.2572 (p=0.01495), 
indicating a significant though rather weak posi-
tive relationship. 
In order to find the factors causing this weak 
correlation, we tested the correlation for those 
MWEs whose rank differences were less than 20, 
30, 40 and 50 respectively. We are interested to 
find out how many of them fall under each of the 
categories and which of their features affected 
the performance of the algorithm. As a result, we 
found 43, 54, 66 and 77 MWEs fall under these 
categories respectively, which yield different 
correlation scores, as shown in Table 1.  
 
numb of 
MWEs 
Percent 
(%) 
Rank 
diff 
rho-
score 
Sig. 
43 48.31 <20 0.9149 P<0.001 
54 60.67 <30 0.8321 P<0.001 
66 74.16 <40 0.7016 P<0.001 
77 86.52 <50 0.5084 P<0.001 
89 (total) 100.00 <=73 0.2572 P<0.02 
 
Table 1: Correlation coefficients corresponding 
different rank differences. 
 
As we expected, the rho decreases as the rank 
difference increases, but all of the four categories 
containing a total of 77 MWEs (86.52%) show 
reasonably high correlations, with the minimum 
score of 0.5084. 8 In particular, 66 of them 
(74.16%), whose ranking differences are less 
than 40, demonstrate a strong correlation with 
rho-score 0.7016, as illustrated by Fig. 3 
 
ScatterPlot of Auto vs. Man Ranks for 66 MWEs
(rank_diff < 40)
0
20
40
60
80
100
0 20 40 60 80 10auto ranks
m
an
 r
an
ks
0
 
 
Fig 3: ScatterPlot for 66 MWEs (rank_diff < 
40) which shows a strong correlation 
 
Our manual examination shows that the algo-
rithm generally pushes the highly compositional 
and non-compositional MWEs towards opposite 
ends of the spectrum of the D-score. For example, 
those assigned with score 1 include ?aid worker?, 
?audio tape? and ?unemployment figure?. On the 
other hand, MWEs such as ?tea leaf? (meaning 
thief), ?kick the bucket? and ?hot dog? are given 
a low score of 0.001. We assume these two 
groups of MWEs are generally treated as highly 
compositional and opaque MWEs respectively. 
However, the algorithm could be improved. A 
major problem found is that the algorithm pun-
ishes longer MWEs which contain function 
words. For example, ?make an appearance? is 
scored 0.000114 by the algorithm, but when the 
article ?an? is removed, it gets a higher score 
0.003608. Similarly, when the preposition ?up? 
is removed from ?keep up appearances?, it gets 
0.014907 compared to the original 0.000471, 
which would push up their rank much higher. To 
address this problem, the algorithm needs to be 
refined to minimise the impact of the function 
words to the scoring process. 
Our analysis also reveals that 12 MWEs with 
rank differences (between automatic and manual 
ranking) greater than 50 results in a degraded 
overall correlation. Table 2 lists these words, in 
which the higher ranks indicate higher composi-
tionality.  
 
                                                 
8 Salkind (2004: 88) suggests that r-score ranges 0.4~0.6, 
0.6~0.8 and 0.8~1.0 indicate moderate, strong and very 
strong correlations respectively. 
7
MWE Sem. Tag9 Auto 
rank 
Manual 
rank 
plough into A9- 53.5 3 
Bloody Mary F2 53.5 2 
pillow fight K6 26 80.5 
lollipop lady M3/S2 70 15 
cradle snatcher S3.2/T3/S2 73.5 17.5 
go bananas X5.2+++ 65 8.5 
make an appearance S1.1.3+ 2 58.5 
keep up appearances A8/S1.1.1 4 61 
sandwich course P1 69 11.5 
go bananas B2-/X1 68 10 
Eskimo roll M4 71.5 5 
in other words Z4 12.5 83 
 
Table 2: Twelve MWEs having rank differences 
greater than 50. 
 
Let us take ?pillow fight? as an example. The 
whole expression is given the semantic tag K6, 
whereas neither ?pillow? nor ?fight? as individ-
ual word is given this tag. In the lexicon, ?pil-
low? is classified as H5 {FURNITURE AND 
HOUSEHOLD FITTINGS} and ?fight? is as-
signed to four semantic categories including S8- 
{HINDERING}, X8+ {HELPING}, E3- {VIO-
LENT/ANGRY}, and K5.1 {SPORTS}. For this 
reason, the automatic score of this MWE is as 
low as 0.003953 on the scale of [0, 1]. On the 
contrary, human raters judged the meaning of 
this expression to be fairly transparent, giving it 
a high score of 8.5 on the scale of [0, 10]. Similar 
contrasts occurred with the majority of the 
MWEs with rank differences greater than 50, 
which are responsible for weakening the overall 
correlation. 
Another interesting case we noticed is the 
MWE ?pass away?. This MWE has two major 
senses in the semantic lexicon L1- {DIE} and 
T2- {END} which were ranked separately. Re-
markably, they were ranked in the opposite order 
by human raters and the algorithm. Human raters 
felt that the sense DIE is less idiomatic, or more 
compositional, than END, while the algorithm 
indicated otherwise. The explanation of this 
again lies in the semantic classification of the 
lexicon, where ?pass? as a single word contains 
the sense T2- but not L1-. Consequently, the 
automatic score for ?pass away? with the sense 
                                                 
                                                
9 Semantic tags occurring in Table 2: A8 (seem), A9 (giving 
possession), B2 (health and disease), F2 (drink), K6 (chil-
dren?s games and toys), M3 (land transport), M4 (swim-
ming), P1 (education), S1.1.1 (social actions), S1.1.3 (par-
ticipation), S2 (people), S3.2 (relationship), T3 (time: age), 
X1 (psychological actions), X5.2 (excited), Z4 (discourse 
bin) 
L1- is much lower (0.001) than that with the 
sense of T2- (0.007071). 
In order to evaluate our algorithm in compari-
son with previous work, we also tested it on the 
manual ranking list created by McCarthy et al
(2003).10 We found that 79 of the 116 phrasal 
verbs in that list are included in the Lancaster 
semantic lexicon. We applied our algorithm on 
those 79 items to compare the automatic ranks 
against the average manual ranks using the 
Spearman?s rank correlation coefficient (rho). As 
a result, we obtained rho=0.3544 with signifi-
cance level of p=0.001357. This result is compa-
rable with or better than most measures reported 
by McCarthy et al(2003). 
6 Discussion 
The algorithm we propose in this paper is dif-
ferent from previous proposed statistical methods 
in that it employs a semantic lexical resource in 
which the semantic field information is directly 
accessible for both MWEs and their constituent 
words. Often, typical statistical algorithms meas-
ure the semantic distance between MWEs and 
their constituent words by comparing their con-
texts comprising co-occurrence words in near 
context extracted from large corpora, such as 
Baldwin et als algorithm (2003). 
When we consider the definition of the com-
positionality as the extent to which the meaning 
of the MWE can be guessed based on that of its 
constituent words, a semantic lexical resource 
which maps MWEs and words to their semantic 
features provides a practical way of measuring 
the MWE compositionality. The Lancaster se-
mantic lexicon is one such lexical resource 
which allows us to have direct access to semantic 
field information of large number of MWE and 
single words. Our experiment demonstrates the 
potential value of such semantic lexical resources 
for the automatic measurement of MWE compo-
sitionality. Compared to statistical algorithms 
which can be affected by a variety of un-
controllable factors, such as size and domain of 
corpora, etc., an expert-compiled semantic lexi-
cal resource can provide much more reliable and 
?clean? lexical semantic information. 
However, we do not suggest that algorithms 
based on semantic lexical resources can substi-
tute corpus-based statistical algorithms. Rather, 
we suggest it as a complement to existing statis-
tical algorithms. As the errors of our algorithm 
 
10This list is available at website: 
http://mwe.stanford.edu/resources/  
8
reveal, the semantic information provided by the 
lexicon alone may not be rich enough for a very 
fine-grained distinction of MWE compositional-
ity. In order to obtain better results, this algo-
rithm needs to be combined with statistical tech-
niques. 
A limitation of our approach is language-
dependency. In order to port our algorithm to 
languages other than English, one needs to build 
similar semantic lexicon in those languages. 
However, similar semantic lexical resources are 
already under construction for some other lan-
guages, including Finnish and Russian (L?fberg 
et al, 2005; Sharoff et al, 2006), which will al-
low us to port our algorithm to those languages. 
7 Conclusion 
In this paper, we explored an algorithm based 
on a semantic lexicon for automatically measur-
ing the compositionality of MWEs. In our 
evaluation, the output of this algorithm showed 
moderate correlation with a manual ranking. We 
claim that semantic lexical resources provide 
another approach for automatically measuring 
MWE compositionality in addition to the exist-
ing statistical algorithms. Although our results 
are not yet conclusive due to the moderate scale 
of the test data, our evaluation demonstrates the 
potential of lexicon-based approaches for the 
task of compositional analysis. We foresee, by 
combining our approach with statistical algo-
rithms, that further improvement can be ex-
pected. 
8 Acknowledgement 
The work reported in this paper was carried 
out within the UK-EPSRC-funded ASSIST Pro-
ject (Ref. EP/C004574). 
References  
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, 
and Dominic Widdows. 2003. An Empirical Model 
of Multiword Expression Compositionality. In 
Proc. of the ACL-2003 Workshop on Multiword 
Expressions: Analysis, Acquisition and Treatment,  
pages 89-96, Sapporo, Japan. 
Colin Bannard, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Proc. of the ACL2003 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, pages 65?72, Sapporo. 
Nicoletta Calzolari, Charles Fillmore, Ralph Grish-
man, Nancy Ide, Alessandro Lenci, Catherine 
MacLeod, and Antonio Zampolli. 2002. Towards 
best practice for multiword expressions in compu-
tational lexicons. In Proc. of the Third Interna-
tional Conference on Language Resources and 
Evaluation (LREC 2002), pages 1934?1940, Las 
Palmas, Canary Islands. 
Ann Copestake, Fabre Lambeau, Aline Villavicencio, 
Francis Bond, Timothy Baldwin, Ivan A. Sag, and 
Dan Flickinger. 2002. Multiword expressions: Lin-
guistic precision and reusability. In Proc. of the 
Third International Conference on Language Re-
sources and Evaluation (LREC 2002), pages 1941?
1947, Las Palmas, Canary Islands. 
Matthias  Gamer. 2005. The irr Package: Various Co-
efficients of Interrater Reliability and Agreement. 
Version 0.61 of 11 October 2005.  Available from:   
cran.r-project.org/src/contrib/Descriptions/irr.html 
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of the 37th Annual 
Meeting of the ACL, pages 317?324, College Park, 
USA. 
Laura L?fberg, Scott Piao, Paul Rayson, Jukka-Pekka 
Juntunen, Asko Nyk?nen, and Krista Varantola. 
2005. A semantic tagger for the Finnish language. 
In Proc. of the Corpus Linguistics 2005 conference, 
Birmingham, UK. 
Christopher D. Manning and Hinrich Sch?tze. 2000. 
Foundations of Statistical Natural Language Proc-
essing. The MIT Press, Cambridge, Massachusetts. 
Diana McCarthy, Bill Keller, and John Carroll. 2003. 
Detecting a continuum of compositionality in 
phrasal verbs. In Proc. of the ACL-2003 Workshop 
on Multiword Expressions: Analysis, Acquisition 
and Treatment, pages 73?80, Sapporo, Japan. 
Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc. 
of the 2nd Conference on Empirical Methods in 
Natural Language Processing , Providence, USA. 
Geoffrey Nunberg, Ivan A. Sag, and Tom Wasow. 
1994. Idioms. Language, 70: 491?538. 
Scott S.L. Piao, Paul Rayson, Dawn Archer and Tony 
McEnery. 2004. Evaluating Lexical Resources for 
a Semantic Tagger. In Proc. of LREC-04, pages 
499?502, Lisbon, Portugal. 
Scott S.L. Piao, Dawn Archer, Olga Mudraya, Paul 
Rayson, Roger Garside, Tony McEnery and An-
drew Wilson. 2005a. A Large Semantic Lexicon 
for Corpus Annotation. In Proc. of the Corpus Lin-
guistics Conference 2005, Birmingham, UK. 
Scott S.L. Piao., Paul Rayson, Dawn Archer, Tony 
McEnery. 2005b. Comparing and combining a se-
mantic tagger and a statistical tool for MWE ex-
traction. Computer Speech and Language, 19, 4: 
378?397. 
9
Paul Rayson, Dawn Archer, Scott Piao, and Tony 
McEnery. 2004. The UCREL Semantic Analysis 
System. In Proc. of LREC-04 Workshop: Beyond 
Named Entity Recognition Semantic Labeling for 
NLP Tasks, pages 7?12, Lisbon, Portugal. 
Susanne Riehemann. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. the-
sis, Stanford University, Stanford. 
 Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake, and Dan Flickinger. 2002. Multiword 
Expressions: A Pain in the Neck for NLP. In Proc. 
of the 3rd International Conference on Intelligent 
Text Processing and Computational Linguistics 
(CICLing-2002), pages 1?15, Mexico City, Mexico. 
Neil J. Salkind. 2004. Statistics for People Who Hate 
Statistics. Sage: Thousand Oakes, US. 
Serge Sharoff, Bogdan Babych, Paul Rayson, Olga 
Mudraya and Scott Piao. 2006. ASSIST: Auto-
mated semantic assistance for translators. Proceed-
ings of EACL 2006, pages 139?142, Trento, Italy. 
Patrick E. Shrout and Joseph L. Fleiss. 1979. Intra-
class Correlations: Uses in Assessing Rater Reli-
ability. Psychological Bulletin (2), 420?428. 
Sriram Venkatapathy and Aravind K. Joshi. 2005. 
Measuring the relative compositionality of verb-
noun (V-N) collocations by integrating features. In 
Proc. of Human Language Technology Conference 
and Conference on Empirical Methods in Natural 
Language Processing (HLT/EMNLP 2005), pages 
899?906, Vancouver, Canada. 
 
Appendix: Manual vs. Automatic Ranks 
of Sample MWEs 
The table below shows the human and auto-
matic rankings of 89 sample MWEs. The MWEs 
are sorted in ascending order by manual average 
ranks. The top items are supposed to be the most 
compositional ones. For example, according to 
the manual ranking, facial expression is the most 
compositional MWE while tea leaf is the most 
opaque one. This table also shows that some 
MWEs are tied up with the same ranks. For the 
definitions of the full semantic tagset, see web-
site http://www.comp.lancs.ac.uk/ucrel/usas/. 
 
MWE Tag Sem tag Man 
rank 
Auto. 
rank 
facial expression B1 1 9 
aid worker S8/S2 2 4 
audio tape K3 3.5 4 
leisure activities K1 3.5 36.5 
advance warning T4/Q2.2 5 36.5 
living space H2 6 51 
in other words Z4 7 77.5 
unemployment fig-
ures 
I3.1/N5 8 4 
camera angle Q4.3 9.5 45 
pillow fight K6 9.5 64 
youth club S5/T3 11.5 4 
petrol station M3/H1 11.5 36.5 
palm tree L3 13 9 
rule book G2.1/Q4.1 14 4 
ball boy K5.1/S2.2 15 13 
goal keeper K5.1/S2 16.5 4 
kick in E3- 16.5 36.5 
ventilation shaft H2 18 47 
directory enquiries Q1.3 19 14 
phone box Q1.3/H1 21 18.5 
lose balance M1 21 53 
bend the rules A1.7 21 54.5 
big nose X7/X2.4 23 67 
quantity control N5/A1.7 24 11.5 
act of God S9 25 36.5 
air bag A15/M3 26 62.5 
mind stretching A12 27 59 
plain clothes B5 28 36.5 
keep up appearances A8/S1.1.1 29 86 
examining board P1 30 23 
open mind X6 31.5 49 
make an appearance S1.1.3+ 31.5 88 
cable television Q4.3 33 15 
king size N3.2 34 36.5 
action point X7 35 61 
keep tight rein on A1.7 36 28 
noughts and crosses K5.2 37 77.5 
tea leaf L3/F2 38 4 
single minded X5.1 39.5 77.5 
window dressing I2.2 39.5 77.5 
street girl G1.2/S5 42 36.5 
just over the horizon S3.2/S2.1 42 60 
pressure group T1.1.3 42 16.5 
air proof O4.1 44.5 57.5 
heart of gold S1.2.2 44.5 77.5 
lose heart X5.2 46 26 
food for thought X2.1/X5.1 47 89 
play part S8 48 68 
look down on S1.2.3 49 77.5 
arm twisting Q2.2 50 36.5 
take into account A1.8 51 69 
kidney bean F1 52 9 
come alive A3+ 53 52 
break new ground T3/T2 54 54 
make up to S1.1.2 55 65 
by virtue of C1 56.5 36.5 
snap shot A2.2 56.5 27 
pass away L1- 58 77.5 
long face E4.1 59 77.5 
bossy boots S1.2.3/S2 60 77.5 
plough into M1/A1.1.2 61 11.5 
kick in T2+ 62 50 
animal magnetism S1.2 63 55.5 
sixth former P1/S2 64 77.5 
pull the strings S7.1 65 62.5 
couch potato A1.1.1/S2 66 77.5 
think tank S5/X2.1 67 36.5 
come alive X5.2+ 68 24 
hot dog F1 69 77.5 
cheap shot G2.2-/Q2.2 70 66 
10
rock and roll K2 71 48 
bright as a button S3.2/T3/S2 72.5 87 
cradle snatcher X9.1+ 72.5 16.5 
alpha wave B1 74 77.5 
lollipop lady M3/S2 75 20 
pass away X5.2+ 76.5 57.5 
plough into T2- 76.5 36.5 
piece of cake P1 78.5 77.5 
sandwich course A12 78.5 21 
go bananas B2-/X1 80 22 
go bananas X5.2+++ 81.5 36.5 
go bananas E3- 81.5 25 
kick the bucket L1 83 77.5 
on the wagon F2 84 36.5 
Eskimo roll M4 85 18.5 
acid house K2 86 46 
plough into A9- 87 36.5 
Bloody Mary F2 88 36.5 
tea leaf G2.1-/S2mf 89 77.5 
 
11
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 465?473,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
TermWeighting Schemes for Latent Dirichlet Allocation
Andrew T. Wilson
Sandia National Laboratories
PO Box 5800, MS 1323
Albuquerque, NM 87185-1323, USA
atwilso@sandia.gov
Peter A. Chew
Moss Adams LLP
6100 Uptown Blvd. NE, Suite 400
Albuquerque, NM 87110-4489, USA
Peter.Chew@MossAdams.com
Abstract
Many implementations of Latent Dirichlet Al-
location (LDA), including those described in
Blei et al (2003), rely at some point on the
removal of stopwords, words which are as-
sumed to contribute little to the meaning of
the text. This step is considered necessary be-
cause otherwise high-frequency words tend to
end up scattered across many of the latent top-
ics without much rhyme or reason. We show,
however, that the ?problem? of high-frequency
words can be dealt with more elegantly, and
in a way that to our knowledge has not been
considered in LDA, through the use of appro-
priate weighting schemes comparable to those
sometimes used in Latent Semantic Indexing
(LSI). Our proposed weighting methods not
only make theoretical sense, but can also be
shown to improve precision significantly on a
non-trivial cross-language retrieval task.
1 Introduction
Latent Dirichlet Allocation (LDA) (Blei et al, 2003),
like its more established competitors Latent Seman-
tic Indexing (LSI) (Deerwester et al, 1990) and
Probabilistic Latent Semantic Indexing (PLSI) (Hof-
mann, 1999), is a model which is applicable to the
analysis of text corpora. It is claimed to differ from
LSI in that LDA is a generative Bayesianmodel (Blei
et al, 2003), although this may depend upon the
manner in which one approaches LSI (see for exam-
ple Chew et al (2010)). In LDA as applied to text
analysis, each document in the corpus is modeled as
a mixture over an underlying set of topics, and each
topic is modeled as a probability distribution over the
terms in the vocabulary.
As the newest among the above-mentioned tech-
niques, LDA is still in a relatively early stage of de-
velopment. It is also sufficiently different from LSI,
probably themost popular andwell-known compres-
sion technique for information retrieval (IR), that
many practitioners of LSI may perceive a ?barrier to
entry? to LDA. This in turn perhaps explains why no-
tions such as term weighting, which have been com-
monplace in LSI for some time (Dumais, 1991), have
not yet found a place in LDA. In fact, it is often as-
sumed that weighting is unnecessary in LDA. For
example, Blei et al (2003) contrast the use of tf-
idf weighting in both non-reduced space (Salton and
McGill, 1983) and LSI on the one hand with PLSI
and LDA on the other, where no mention is made of
weighting. Ramage et al (2008) propose a simple
term-frequency weighting scheme for tagged docu-
ments within the framework of LDA, although term
weighting is not their focus and their scheme is in-
tended to incorporate document tags into the same
model that represents the documents themselves.
In this paper, we produce evidence that term
weighting should be given consideration within
LDA. First and foremost, this is shown empiri-
cally through a non-trivial multilingual retrieval task
which has previously been used as the basis for
tests of variants of LSI. We also show that term
weighting allows one to avoid maintenance of stop-
lists, which can be awkward especially for multilin-
gual data. With appropriate term weighting, high-
frequency words (which might otherwise be elimi-
nated as stopwords) are assigned naturally to topics
465
by LDA, rather than dominating and being scattered
across many topics as happens with the standard uni-
form weighting. Our approach belies the usually
unstated, but widespread, assumption in papers on
LDA that the removal of stopwords is a necessary
pre-processing step (see e.g. Blei et al (2003); Grif-
fiths and Steyvers (2004)).
It might seem that to demonstrate this it would be
necessary to perform a test that directly compares the
results when stoplists are used to those when weight-
ing are used. However, we believe that stopwords
are highly ad-hoc to begin with. Assuming a vocab-
ulary of n words and a stoplist of x items, there are
(at least in theory)
(n
x
)
possible stoplists. To be sure
that no stoplist improves on a particular termweight-
ing scheme we would have to test every one of these.
In addition, our tests are with a multilingual dataset,
which raises the issue that a domain-appropriate sto-
plist for a particular corpus and language may not be
available. This is even more true if we pre-process
the dataset morphologically (for example, with stem-
ming). Therefore, rather than attempting a direct
comparison of this type, we take the position that it
is possible to sidestep the need for stoplists and to do
so in a non-ad-hoc way.
The paper is organized as follows. Section 2 de-
scribes the general framework of LDA, which has
only very recently been applied to cross-language
IR. In Section 3, we look at alternatives to the
?standard? uniform weighting scheme (i.e., lack of
weighting scheme) commonly used in LDA. Sec-
tion 4 discusses the framework we use for empiri-
cal testing of our hypothesis that a weighting scheme
would be beneficial. We present the results of this
comparison in Section 5 along with an impressionis-
tic comparison of the output of the different alterna-
tives. We conclude in Section 6.
2 Latent Dirichlet Allocation
Our IR framework is multilingual Latent Dirich-
let Allocation (LDA), first proposed by Blei et al
(2003) as a general Bayesian framework with initial
application to topicmodeling. It is only very recently
that variants of LDA have been applied to cross-
language IR: examples are Cimiano et al (2009) and
Ni et al (2009).
As an approach to topic modeling, LDA relies on
the idea that the tokens in a document are drawn in-
dependently from a set of topics where each topic is
a distribution over types (words) in the vocabulary.
The mixing coefficients for topics within each docu-
ment and weights for types in each topic can be spec-
ified a priori or learned from a training corpus. Blei
et al initially proposed a variational model (2003)
for learning topics from data. Griffiths and Steyvers
(2004) later developed a Markov chain Monte Carlo
approach based on collapsed Gibbs sampling.
In this model, the mixing weights for topics within
each document and the multinomial coefficients for
terms within each topic are hidden (latent) and must
be learned from a training corpus. Blei et al (2003)
proposed LDA as a general Bayesian framework and
gave a variational model for learning topics from
data. Griffiths and Steyvers (2004) subsequently de-
veloped a stochastic learning algorithm based on col-
lapsed Gibbs sampling. In this paper we will focus
on the Gibbs sampling approach.
2.1 Generative Document Model
The LDA algorithm models the D documents in a
corpus as mixtures of K topics where each topic is
in turn a distribution over W terms. Given ?, the
matrix of mixing weights for topics within each doc-
ument, and?, the matrix of multinomial coefficients
for each topic, we can use this formulation to de-
scribe a generative model for documents (Alg. 1).
Restating the LDA model in linear-algebraic
terms, we can say that the product of ? (theK ?W
column-stochastic topic-by-type matrix) and ? (the
D ? K column-stochastic topic-by-document ma-
trix) is the originalD?W term-by-documentmatrix.
In this sense, LDA computes a matrix factorization
of the term-by-document matrix in the sameway that
LSI or non-negative matrix factorization (NMF) do.
In fact, LDA is a special case of NMF, but unlike in
NMF, there is a unique factorization in LDA. We see
this as a feature recommending LDA above NMF.
Our objective is to reverse the generative model to
learn the contents of ? and ? given a training corpus
D, a number of topics K, and symmetric Dirichlet
prior distributions over both ? and ? with hyperpa-
rameters ? and ?, respectively.
466
for k = 1 toK do
Draw ?k ? Dirichlet(?)
end for
for d = 1 to D do
Draw ? ? Dirichlet(?)
Draw N ? Poisson(?)
for i = 1 to N do
Draw z ? Multinomial(?)
Draw w ? Multinomial(?(z))
end for
end for
Algorithm 1: Generative algorithm for LDA. This will
generate D documents with N tokens each. Each token
is drawn from one of K topics. The distributions over
topics and terms have Dirichlet hyperparameters ? and
? respectively. The Poisson distribution over the token
count may be replaced with any other convenient distri-
bution.
2.2 Learning Topics via Collapsed Gibbs
Sampling
Rather than learn ? and ? directly, we use collapsed
Gibbs sampling (Geman et al (1993), Chatterji and
Pachter (2004)) to learn the latent assignment of to-
kens to topics z given the observed tokens x.
The algorithm operates by repeatedly sampling
each zij from a distribution conditioned on the val-
ues of all other elements of z. This requires main-
taining counts of tokens assigned to topics globally
and within each document. We use the following no-
tation for these sums:
Nijk: Number of tokens of type wi in document dj
assigned to topic k
N?stijk : The sum Nijk with the contribution of token
xst excluded
We indicate summation over all values of an index
with (?).
Given the current state of z the conditional proba-
bility of zij is:
p(zij = k|z?ij , x, d, ?, ?) =
p(xij |?k) p(k|dj) ?
N?iji(?)k + ?
N?ij(?)(?)k + W?
N?ij(?)jk + ?
N(?)j(?) + T?
(1)
As Griffiths and Steyvers (2004) point out, this is
an intuitive result. The first term, p(xij |?k), indi-
cates the importance of term xij in topic k. The sec-
ond term, p(k|dj), indicates the importance of topic
k in document j. The sum of the terms is normalized
implicitly to 1 when we draw each new zij .
We sample a new value for zij for every token xij
during each iteration of Gibbs sampling. We run the
sampler for a burn-in period of a few hundred itera-
tions to allow it to reach its converged state and then
estimate ? and ? from z as follows:
?jk =
N(?)jk + ?
N(?)j(?) + T?
(2)
?ki =
Ni(?)k + ?
N(?)(?)k + W?
(3)
2.3 Classifying New Documents
In LSI, new documents not in the original training
set can be ?projected? into the semantic space of the
training set. The equivalent process in LDA is one
of classification: given a corpus D? of one or more
new documents we use the existing topics ? to com-
pute a maximum a posteriori estimate of the mixing
coefficients ??. This follows the same Monte Carlo
process of repeatedly resampling a set of token-to-
topic assignments z? for the tokens x? in the new doc-
uments. These new tokens are used to compute the
first term p(k|dj) in Eq. 1. We re-use the topic as-
signments z from the training corpus to compute the
second term p(xij |?k). Tokens with new types that
were not present in the vocabulary of the training
corpus do not participate in classification.
The resulting distribution ?? essentially encodes
how likely each new document is to relate to each of
the K topics. We can use this matrix to compute
pairwise similarities between any two documents
from either corpus (training or newly-classified).
Whereas in LSI it may make sense to compute sim-
ilarity between documents using the cosine met-
ric (since the ?dimensions? defining the space are
orthogonal), we compute similarities in LDA us-
ing either the symmetrized Kullback-Leibler (KL)
or Jensen-Shannon (JS) divergences (Kullback and
Leibler (1951), Lin (2002)) since these are methods
of measuring the similarity between probability dis-
tributions.
467
3 Term Weighting Schemes and LDA
The standard approach presented above assumes, ef-
fectively, that each token is equally important in cal-
culating the conditional probabilities. From both an
information-theoretic and a linguistic point of view,
however, it is clear that this is not the case. In En-
glish, a term such as ?the? which occurs with high
frequency in many documents does not contribute as
much to the meaning of each document as a lower-
frequency term such as ?corpus?. It is an axiom of
information theory that an event a?s information con-
tent (in bits) is equal to log2 1p(a) = ? log2 p(a).
Treating tokens as events, we can say that the in-
formation content of a particular token of type t is
? log2 p(t). Furthermore, as is well-known, we can
estimate p(t) from observed frequencies in a corpus:
it is simply the number of tokens of type t in the cor-
pus, divided by the total number of tokens in the cor-
pus. For high-probability terms such as ?the?, there-
fore, ? log2 p(t) is low. Our basic hypothesis is that
recalculating p(zij |z, x, ?, ?) to take the information
content of each token into account will improve the
results of LDA. Specifically, we have incorporated
a weighting term into Eq. 1 by replacing the counts
denoted N with weights denotedM .
p(zij = k|z?ij , x, d, ?, ?) ?
M?iji(?)k + ?
M?ij(?)(?)k + W?
M?ij(?)jk + ?
M(?)j(?) + T?
(4)
Here Mijk is the total weight of tokens of type i
in document j assigned to topic k instead of the total
number of tokens. All of the machinery for Gibbs
sampling and the estimation of ? and ? from z re-
mains unchanged.
We appeal to an urn model to explain the intuition
behind this approach. In the original LDA formula-
tion, each topic ? can be modeled as an urn contain-
ing a large number of balls of uniform size. Each
ball assumes one ofW different colors (one color for
each term in the vocabulary). The frequency of oc-
currence of each color in the urn is proportional to the
corresponding term?s weight in topic ?. We incor-
porate a term weighting scheme by making the size
of each ball proportional to the weight of its corre-
sponding term. This makes the probability of draw-
ing the ball for a termw proportional to both the term
weightm(w) and its multinomial weight ?w:
p(w|?, ?,m) = ?
w m(w)
?
w?W m(w)
(5)
We can now expand Eq. 4 to obtain a new sampling
equation for use with the Gibbs sampler.
p(zij = k|z?ij , x,d,m, ?, ?) =
m(xi)N?iji(?)k + ?
?
w m(w)N
?ij
w(?)k + W?
?
w m(w)N
?ij
wjk + ?
?
w m(w)Nwj(?) + T?
(6)
If all weightsm(w) = 1 this reduces immediately
to the standard LDA formulation in Eq. 1.
The information measure we describe above is
constant for a particular term across the entire cor-
pus, but it is possible to conceive of other, more so-
phisticated weighting schemes as well, for example
those where term weights vary by document. Point-
wise mutual information (PMI) is one such weight-
ing scheme which has a solid basis in information
theory and has been shown to work well in the con-
text of LSI (Chew et al, 2010). According to PMI,
the weight of a given term w in a given document
d is the pointwise mutual information of the term
and document, or? log2
p(w|d)
p(w) . Extending the LDA
model to accommodate PMI is straightforward. We
replace m(xi) and m(w) in Eq. 4 with m(xi, d) as
follows.
m(xi, d) = ? log2
p(xi|d)
p(xi)
= ? log2
#[tokens of type xi in d]
#[tokens of type xi]
(7)
It is possible for PMI of a term within a document
to be negative. When this happens, we clamp the
weight of the offending term to zero in that docu-
ment. In practice, we observe this only with com-
mon words (e.g. ?and?, ?in?, ?of?, ?that?, ?the? and
?to? in English) that are assigned very lowweight ev-
erywhere else in the corpus. This clamping does not
noticeably affect the results.
In the next sections, we describe tests which have
enabled us to evaluate empirically which of these
formulations works best in practice.
468
4 Testing Framework
In this paper, we chose to test our hypotheses with
the same cross-language retrieval task used in a num-
ber of previous studies of LSI (e.g. Chew and Abde-
lali (2007)). Briefly, the task is to train an IR model
on one particular multilingual corpus, then deploy
it on a separate multilingual corpus, using a docu-
ment in one language to retrieve related documents
in other languages. This task is difficult because of
the size of the datasets involved. Its usefulness be-
comes apparent when we consider the following two
use cases: a humanwishing (1) to use a search engine
to retrieve relevant documents in many languages re-
gardless of the language in which the query is posed;
or (2) to produce a clustering or visualization of doc-
uments according to their topics even when the doc-
uments are in different languages.
The training corpus consists of the text of the Bible
in 31,226 parallel chunks, corresponding generally
to verses, in Arabic, English, French, Russian and
Spanish. These data were obtained from the Un-
bound Bible project (Biola University (2006)). The
test data, obtained from http://www.kuran.gen.
tr/, is the text of the Quran in the same 5 languages,
in 114 parallel chunks corresponding to suras (chap-
ters). The task, in short, is to use the training data
to inform whatever linguistic, semantic, or statistical
model is being tested, and then to infer characteris-
tics of the test data in such a way that the test docu-
ments can automatically be matched with their trans-
lations in other languages. Though the documents
come from a specific domain (scriptural texts), what
is of interest is comparative results using different
weighting schemes, holding the datasets and other
settings constant. The training and test datasets are
large enough to allow statistically significant obser-
vations to be made, and if a significant difference is
observed between experiments using two settings, it
is to be expected that similar basic differences would
be observed with any other set of training and test
data. In any case, it should be noted that the Bible
and Quran were written centuries apart, and in differ-
ent original languages; we believe this contributes
to a clean separation of training and test data, and
makes for a non-trivial retrieval task.
In our framework, a term-by-document matrix is
formed from the Bible as a parallel verse-aligned
corpus. We employed two different approaches
to tokenization, one (word-based tokenization) in
which text was tokenized at every non-word char-
acter, and the other (unsupervised morpheme-based
tokenization) in which after word-based tokeniza-
tion, a further pre-processing step (based on Gold-
smith (2001)) was performed to add extra breaks at
everymorpheme. It is shown elsewhere (Chew et al,
2010) that this step leads to improved performance
with LSI. In each verse, all languages are concate-
nated together, allowing terms (either morphemes or
words) from all languages to be represented in every
verse. Cross-language homographs such as ?mien?
in English and French are treated as distinct terms
in our framework. Thus, if there are L languages,
D documents (each of which is translated into each
of the L languages), andW distinct linguistic terms
across all languages, then the term-by-document ma-
trix is of dimensionsW byD (notW byD?L); with
the Bible as a training corpus, the actual numbers in
our case are 160,345? 31,226. As described in Sec.
2.2, we use this matrix as the input to a collapsed
Gibbs sampling algorithm to learn the latent assign-
ment of tokens in all five languages to language-
independent topics, as well as the latent assignment
of language-independent topics to the multilingual
(parallel) documents. In general, we specified, arbi-
trarily but consistently across all tests, that the num-
ber of topics to be learned should be 200. Other pa-
rameters for the Gibbs sampler held constant were
the number of iterations for burn-in (200) and the
number of iterations for sampling (1).
To evaluate our different approaches to weighting,
we use classification as described in Sec. 2.3 to ob-
tain, for each document from the Quran test corpus,
a probability distribution across the topics learned
from the Bible. While in training we have D multi-
lingual documents, in testing we haveD? ?L docu-
ments, each in a specific language, for which a distri-
bution is computed. For theQuran data, this amounts
to 114 ? 5 = 570 documents. This is because our
goal is to match documents with their translations
in other languages using just the probability distri-
butions. For each source-language/target-language
pair L1 and L2, we obtain the similarity of each of
the 114 documents in L1 to each of the 114 doc-
uments in L2. We found that similarity here is
best computed using the Jensen-Shannon divergence
469
Tokenization
Weighting Scheme Word Morpheme
Unweighted 0.505 0.544
log p(w|L) 0.616 0.641
PMI 0.612 0.686
Table 1: Summary of comparison results. This table
shows the average precision at one document (P1) for
each of the tokenization and weighting schemes we eval-
uated. Detailed results are presented in Table 2.
(Lin, 2002) and so this measure was used in all
tests. Ultimately, the measure of how well a partic-
ular method performs is average precision at 1 doc-
ument (P1). Among the various measurements for
evaluating the performance of IR systems (Salton
and McGill (1983), van Rijsbergen (1979)), this is
a fairly standard measure. For a particular source-
target pair, this is the percentage (out of 114 cases)
where a document in L1 is most similar to its mate
in L2. With 5 languages, there are 25 source-target
pairs, and we can also calculate average P1 across
all language pairs. Here, we average across 114 ?
25 (or 2,850) cases. This is why even small differ-
ences in P1 can be statistically significant.
5 Results
First, we present a summary of our results in Table 1
which clearly demonstrates that it is better in LDA to
use some kind of weighting scheme rather than the
uniform weights in the standard LDA formulation
from Eq. 1. This is true whether tokenization is by
word or by morpheme. All increases from the base-
line precision at 1 document (0.505 and 0.544 re-
spectively), whether under log or PMIweighting, are
highly significant (p < 10?11). Furthermore, all in-
creases in precision when moving from word-based
to morphology-based tokenization are also highly
significant (p < 5 ? 10?5 without weighting, p <
5?10?3 with log-weighting, and p< 2?10?15 with
PMI weighting). The best result overall, where P1 is
0.686, is obtained with morphological tokenization
and PMI weighting (parallel to the results in (Chew
et al, 2010) with LSI), and again the difference be-
tween this result and its nearest competitor of 0.641
is highly significant (p < 3 ? 10?6). We return to
comment below on lack of an increase in P1 when
moving from log-weighting to PMI-weighting under
word-based tokenization.
These results can also be broken out by language
pair, as shown in Table 2. Here, it is apparent that
Arabic, and to a lesser extent Russian, are harder lan-
guages in the IR problem at hand. Our intuition is
that this is connected with the fact that these two lan-
guages have a more complex morphological struc-
ture: words are formed by a process of agglutination.
A consequence of this is that single Arabic and Rus-
sian tokens can less frequently be mapped to single
tokens in other languages, which appears to ?con-
fuse? LDA (and also, as we have found, LSI). The
complex morphology of Russian and Arabic is also
reflected in the type-token ratios for each language:
in our English Bible, there are 12,335 types (unique
words) and 789,744 tokens, a type-token ratio of
0.0156. The ratios for French, Spanish, Russian and
Arabic are 0.0251, 0.0404, 0.0843 and 0.1256 re-
spectively. Though the differences may not be ex-
plicable in purely statistical terms (there may be lin-
guistic factors at play which cannot be reduced to
statistics), it seems plausible that choosing a subop-
timal term-weighting scheme could exacerbate any
intrinsic problems of statistical imbalance. Consid-
ering this, it is interesting to note that the greatest
gains, when moving from unweighted LDA to ei-
ther form of weighted LDA, are often to be found
where Russian and/or Arabic are involved. This, to
us, shows the value of using a multilingual dataset
as a testbed for our different formulations of LDA:
it allows problems which may not be apparent when
working with a monolingual dataset to come more
easily to light.
We have mentioned that the best results are with
PMI and morphological tokenization, and also that
there is an increase in precision for many language of
the pairs when morphological (as opposed to word-
based) tokenization is employed. To us, the results
leave little doubt that both weighting and morpho-
logical tokenization are independently beneficial. It
appears, though, that morphology and weighting are
also complementary and synergistic strategies for
improving the results of LDA: for example, a subop-
timal approach in tokenization may at best place an
upper bound on the overall precision achievable, and
perhaps at worst undo the benefits of a good weight-
ing scheme. This may explain the one apparently
anomalous result, which is the lack of an increase in
470
Original Words Morphological Tokenization
EN ES RU AR FR EN ES RU AR FR
LDA
EN 1.000 0.500 0.447 0.132 0.816 1.000 0.500 0.658 0.211 0.640 EN
ES 0.649 1.000 0.307 0.175 0.781 0.605 1.000 0.482 0.175 0.737 ES
RU 0.430 0.316 1.000 0.149 0.430 0.553 0.421 1.000 0.272 0.553 RU
AR 0.070 0.149 0.114 1.000 0.096 0.123 0.105 0.228 1.000 0.114 AR
FR 0.781 0.693 0.421 0.175 1.000 0.693 0.640 0.667 0.211 1.000 FR
Log-WLDA
EN 1.000 0.518 0.518 0.228 0.658 1.000 0.675 0.561 0.219 0.754 EN
ES 0.558 1.000 0.605 0.254 0.763 0.711 1.000 0.570 0.289 0.860 ES
RU 0.605 0.615 1.000 0.298 0.702 0.684 0.667 1.000 0.289 0.728 RU
AR 0.404 0.430 0.526 1.000 0.439 0.430 0.439 0.535 1.000 0.404 AR
FR 0.667 0.667 0.658 0.281 1.000 0.711 0.667 0.561 0.289 1.000 FR
PMI-WLDA
EN 1.000 0.579 0.658 0.272 0.702 1.000 0.719 0.658 0.342 0.851 EN
ES 0.596 1.000 0.623 0.246 0.693 0.816 1.000 0.675 0.272 0.798 ES
RU 0.649 0.579 1.000 0.307 0.693 0.702 0.693 1.000 0.360 0.772 RU
AR 0.351 0.368 0.421 1.000 0.351 0.456 0.474 0.509 1.000 0.377 AR
FR 0.693 0.667 0.605 0.254 1.000 0.825 0.772 0.719 0.333 1.000 FR
Table 2: Full results for precision at one document for all combinations of LDA, Log-WLDA, PMI-WLDA, word
tokenization and morphological tokenization.
precision moving from log-WLDA to PMI-WLDA
under word-based tokenization: if word-based tok-
enization is suboptimal, PMI weighting cannot com-
pensate for that. Effectively, for best results, the
right strategies have to be pursued with respect both
to morphology and to weighting.
Finally, we can illustrate the differences between
weighted and unweighted LDA in another way. As
discussed earlier, each topic in LDA is a probabil-
ity distribution over terms. For each topic, we can
list the most probable terms in decreasing order of
probability; this gives a sense of what each topic
is ?about? and whether the groupings of terms ap-
pear reasonable. Since we use 200 topics, an ex-
haustive listing is impractical here, but in Table 3
we present some representative examples from un-
weighted LDA and PMI-WLDA that we judged to
be of interest. It appears to us that the groupings are
not perfect under either LDA or PMI-WLDA; under
both methods, we find examples of rather heteroge-
neous topics, whereas we would like each topic to be
semantically focused. Still, a comparison of the out-
put with LDA and PMI-WLDA sheds some light on
why PMI-WLDA makes it less necessary to remove
stopwords. Note that all words listed for the top two
topics under LDA would commonly be considered
stopwords. This might also be true of the words in
topic 1 for PMI-WLDA, but in the latter case, the
topic is actually one of themost semantically focused
in that the top words have a clear semantic connec-
tion to one another. This cannot be said of topics 1
and 2 in LDA. For one thing, many of the same terms
that appear in topic 1 reappear in topic 2, making the
two topics hard to distinguish from one another. Sec-
ondly, the terms have only a loose semantic connec-
tion to one another: ?the?, ?and?, and ?of? are all high-
frequency and likely to co-occur, but they are differ-
ent parts of speech and have very different functions
in English. One might say that topics 1 and 2 in LDA
are a rag-bag of high-frequency words, and it is un-
surprising that these topics do little to help charac-
terize documents in our cross-language IR task. The
same cannot be said of any of the top 5 topics in PMI-
WLDA.We believe this illustrates well, and at a fun-
damental level, why weighted forms of LDA work
better in practice than unweighted LDA.
6 Conclusion
We have conducted a series of experiments to evalu-
ate the effect of different weighting schemes on La-
tent Dirichlet Allocation. Our results demonstrate,
perhaps contrary to the conventional wisdom that
weighting is unnecessary in LDA, that weighting
schemes (and other pre-processing strategies) simi-
471
Weighting Scheme
LDA (no weighting) PMI-WLDA
Topic 1 2 3 4 5 1 2 3 4 5
Terms
the the vanit? as c?rcel under city coeur sat col?re
et de vanidad comme prison sous ville heart assis ira
and et vanity como ????? ??? ciudad coraz?n vent wrath
los of ???? ??? prison ??? ?????? ?????? wind anger
? and ????? un ??????? debajo ????? ?????? viento furor
y y aflicci?n a prisonniers ombre twelve ???? sentado ????
les de poursuite one ??????? bases douze ??? ????? fureur
? ? ?????? ??? bound basas doce ???? ????? ???
de la pr?dicateur une prisi?n sombra ???? ???? sitting ?????
of la ???? ???? prisoners dessous ?????? ??????? ??? contre
Table 3: Top 10 terms within top 5 topics for each of LDA and PMI-WLDA. Terms that appear twice within the same
topic (e.g. ?la? in LDA topic 2) are words from different languages with the same spelling (here Spanish and French).
lar to those commonly employed in other approaches
to IR (such as LSI) can significantly improve the
performance of a system. Our approach also runs
counter to the standard position in LDA that it is
necessary or desirable to remove stopwords as a pre-
processing step, and we have presented an alterna-
tive approach of applying an appropriate weighting
scheme within LDA. This approach is preferable be-
cause it is considerably less ad-hoc than the construc-
tion of stoplists. We have shown mathematically
how alternative weighting schemes can be incorpo-
rated into the Gibbs sampling model. We have also
demonstrated that, far from being arbitrary, the in-
troduction of weighting into the LDA model has a
solid and rational basis in information and probabil-
ity theory, just as the basic LDA model itself has.
In future work, we would like to explore further
enhancements to weighting in LDA. There are many
variants which can be considered: one example is
the incorporation of word order and context through
an n-gram model based on conditional probabilities.
We also aim to evaluate LDA against LSIwith a view
to establishingwhether one can be said to outperform
the other consistently in terms of precision, with ap-
propriate settings held constant. Finally, we would
like to determine whether other techniques which
have been shown to benefit LSI can also be usefully
brought to bear in LDA, just as we have shown here
in the case of term weighting.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Machine
Learning Research 3, pages 993?1022.
Sourav Chatterji and Lior Pachter. 2004. Multiple Or-
ganism Gene Finding by Collapsed Gibbs Sampling.
In RECOMB ?04: Proceedings of the eighth annual in-
ternational conference on Research in computational
molecular biology, pages 187?193, New York, NY,
USA. ACM.
Peter A. Chew and Ahmed Abdelali. 2007. Bene-
fits of the ?Massively Parallel Rosetta Stone?: Cross-
Language Information Retrieval with Over 30 Lan-
guages. In Association for Computational Linguistics,
editor, Proceedings of the 45th meeting of the Associ-
ation of Computational Linguistics, pages 872?879.
Peter A. Chew, Brett W. Bader, Stephen Helmreich,
Ahmed Abdelali, and Stephen J. Verzi. 2010.
An Information-Theoretic, Vector-Space-Model Ap-
proach to Cross-Language Information Retrieval.
Journal of Natural Language Engineering. Forthcom-
ing.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit Versus
Latent Concept Models for Cross-Language Informa-
tion Retrieval. In Proceedings of the 21st Inter-
national Joint Conference on Artificial Intelligence,
pages 1513?1518.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Susan T. Dumais. 1991. Improving the Retrieval of In-
formation from External Sources. Behavior Research
Methods, Instruments and Computers, 23(2):229?236.
472
Stuart Geman, Donald Geman, K. Abend, T. J. Harley,
and L. N. Kanal. 1993. Stochastic Relaxation, Gibbs
Distributions and the Bayesian Restoration of Images*.
Journal of Applied Statistics, 20(5):25?62.
J. Goldsmith. 2001. Unsupervised Learning of the Mor-
phology of a Natural Language. Computational Lin-
guistics, 27(2):153?198.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing Scientific Topics. In Proceedings of the Na-
tional Academy of Sciences USA, volume 101, pages
5228?5235.
Thomas Hofmann. 1999. Probablistic Latent Semantic
Indexing. In Proceedings of the 22nd Annual Interna-
tional SIGIR Conference, pages 53?57.
Solomon Kullback and Richard A. Leibler. 1951. On
Information and Sufficiency. Annals of Mathematical
Statistics, 22:49?86.
J. Lin. 2002. DivergenceMeasures based on the Shannon
Entropy. IEEE Transactions on Information Theory,
37(1):145?151, August.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining Multilingual Topics from Wikipedia. In
18th International World Wide Web Conference, pages
1155?1155, April.
Daniel Ramage, Paul Heymann, Christopher D. Man-
ning, and Hector Garcia-Molina. 2008. Clustering the
Tagged Web. In Second ACM International Confer-
ence on Web Search and Data Mining (WSDM 2009),
November.
G. Salton and M. McGill, editors. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill.
Biola University. 2006. The Unbound Bible.
http://www.unboundbible.com.
C.J. van Rijsbergen. 1979. Information Retrieval.
Butterworth-Heinemann.
473
