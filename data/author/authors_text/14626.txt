Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 16?25,
Beijing, August 2010
The Noisier the Better: Identifying Multilingual  Word Translations Using a Single Monolingual Corpus 
Reinhard Rapp University of Tarragona GRLMC 
reinhardrapp@gmx.de 
Michael Zock Laboratoire d?Informatique Fondamentale CNRS Marseille 
Michael.Zock@lif.univ-mrs.fr 
 
Abstract 
The automatic generation of dictionaries from raw text has previously been based on parallel or comparable corpora. Here we describe an approach requiring only a single monolingual corpus to generate bilingual dictionaries for several lan-guage pairs. A constraint is that all lan-guage pairs have their target language in common, which needs to be the lan-guage of the underlying corpus. Our ap-proach is based on the observation that monolingual corpora usually contain a considerable number of foreign words. As these are often explained via transla-tions typically occurring close by, we can identify these translations by look-ing at the contexts of a foreign word and by computing its strongest associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five ma-jor European languages. We also com-pare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that re-sults are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 
1 Introduction 
Established methods for the identification of word translations are based on parallel (Brown et al, 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al, 2004). The work using parallel corpora such as Europarl (Koehn, 
2005; Armstrong et al, 1998) or JRC Acquis (Steinberger et al, 2006) typically performs a length-based sentence alignment of the trans-lated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into prac-tice using a number of freely available open source tools such as Moses (Koehn et al, 2007) and Giza++ (Och & Ney, 2003).  However, parallel texts are a scarce resource for many language pairs (Rapp & Mart?n Vide, 2007), which is why methods based on compa-rable corpora have come into focus. One ap-proach is to extract parallel sentences from comparable corpora (Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach relates co-occurrence patterns between languages. Hereby the underlying assumption is that across languages there is a correlation between the co-occurrences of words which are translations of each other. If, for example, in a text of one lan-guage two words A and B co-occur more often than expected by chance, then in a text of an-other language those words which are the trans-lations of A and B should also co-occur more frequently than expected. However, to exploit this observation some bridge needs to be built between the two lan-guages. This can be done via a basic dictionary comprising some essential vocabulary. To put it simply, this kind of dictionary allows a (partial) word-by-word translation from the source to the target language,1 so that the result can be con-sidered as a pair of monolingual corpora. Deal-
                                                 1 Note that this translation can also be conducted at the level of co-occurrence vectors rather than at the text level. 
16
ing only with monolingual corpora means that the established methodology for computing similar words (see e.g. Pantel & Lin, 2002), which is based on Harris? (1954) distributional hypothesis, can be applied. It turns out that the most similar words between the two corpora effectively identify the translations of words. This approach based on comparable corpora considerably relieves the data acquisition bot-tleneck, but has the disadvantage that the results tend to lack accuracy in practice. As an alternative, there is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which has the advan-tage that it does not even require a corpus. A simple word list will suffice. However, this ap-proach works only for closely related languages, and has limited potential otherwise. We propose here to generate dictionaries on the basis of foreign word occurrences in texts. As far as we know, this is a method which has not been tried before. When doing so, a single monolingual corpus can be used for all source languages for which it contains a sufficient number of foreign words. A constraint is that the target language must always be the language of the monolingual corpus,2 which therefore all dictionaries have in common. 
2 Approach and Language Resources 
Starting from the observation that monolingual dictionaries typically include a considerable number of foreign words, the basic idea is to consider the most significant co-occurrences of a foreign word as potential translation candi-dates. This implies that the language of the un-derlying corpus must correspond to the target language, and that this corpus can be utilized for any source language for which word citations are well represented. As the use of foreign language words in texts depends on many parameters, including writer, text type, status of language and cultural back-ground, it is interesting to compare results when varying some of these parameters. However, due to the general scarceness of foreign word                                                  2 Although in principle it would also be possible to determine relations between foreign words from dif-ferent languages within a corpus, this seems not promising as the problem of data sparsity is likely to be prohibitive. 
citations our approach requires very large cor-pora. For this reason, we were only able to vary two parameters, namely language and text type. Some large enough corpora that we had at our disposal were the Gigaword Corpora from the Linguistic Data Consortium (Mendon?a et al, 2009a; Mendon?a et al, 2009b) and the WaCky Corpora described in Sharoff (2006), Baroni et al (2009), and Ferraresi et al (2010). From these, we selected the following for this study:  
? French WaCky Corpus (8.2 GB) 
? German WaCky Corpus (9.9 GB) 
? Italian WaCky Corpus (10.4 GB) 
? French Gigaword 2nd edition (5.0 GB) 
? Spanish Gigaword 2nd edition (6.8 GB)  The memory requirements shown for each cor-pus relate to ANSI coded text only versions. We derived these from the original corpora by re-moving linguistic annotation (for the WaCky corpora) and XML markup, and by converting the coding from UTF8 to ANSI. Both Gigaword corpora consist of news-ticker texts from several press agencies. News-ticker text is a text type closely related to news-paper text. It is usually carefully edited, and the vocabulary is geared towards easy understand-ing for the intended readership. This implies that foreign word citations are kept to a mini-mum. In contrast, the WaCky Corpora have been downloaded from the web and represent a great variety of text types and styles. Hence, not all texts can be expected to have been carefully edited, and mixes between languages are proba-bly more frequent than with newsticker text. As in this work English is the main source language, and as we have dealt with it as a tar-get language already in Rapp & Zock (2010), we do not use the respective English versions of these corpora here. We also do not use the Wikipedia XML Corpora (Denoyer et al, 2006) as these greatly vary in size for different lan-guages which makes comparisons across lan-guages somewhat problematic. In contrast, the sizes of the above corpora are within the same order of magnitude (1 billion words each), which is why we do not control for corpus size here. 
17
Concerning the number of foreign words within these corpora, we might expect that, given the status of English as the world?s pre-miere language, English foreign words should be the most frequent ones in our corpora. As French and Spanish are also prominent lan-guages, foreign words borrowed from them may be less frequent but should still be common, whereas borrowings from German and Italian are expected to be the least likely ones. From this point of view the quality of the results should vary accordingly. But of course there are many other aspects that are important, for ex-ample, relations between countries, cultural background, relatedness between languages, etc. As these are complex influences with intricate interactions, it is impossible to accurately an-ticipate the actual outcome. In other words, ex-perimental work is needed. Let us therefore de-scribe our approach. For identifying word translations within a corpus, we assume that the strongest association to a foreign word is likely to be its translation. This can be justified by typical usage patterns of foreign words often involving, for example, an explanation right after their first occurrence in a text. Associations between words can be com-puted in a straightforward manner by counting word co-occurrences followed by the applica-tion of an association measure on the co-occurrence counts. Co-occurrence counts are based on a text window comprising the 20 words on either side of a given foreign word. On the resulting counts we apply the log-likelihood ratio (Dunning, 1993). As explained by Dunning, this measure has the advantage to be applicable also on low counts, which is an important characteristic in our setting where the problem of data sparseness is particularly se-vere. This is also the reason why we chose a window size somewhat larger than the ones used in most other studies. Despite its simplicity this procedure of com-puting associations to foreign words is well suited for identifying word translations. As mentioned above, we assume that the strongest association to a foreign word is its best transla-tion. We did this for words from five languages (English, French, German, Italian, and Spanish). The results are shown in the next section. In 
order to be able to quantitatively evaluate the quality of our results, we counted for all source words of a language the number of times the expected target word obtained the strongest as-sociation score. Our expectations on what should count as a correct translation had been fixed before run-ning the experiments by creating a gold stan-dard for evaluation. We started from the list of 100 English words (nouns, adjectives and verbs) which had been introduced by Kent & Rosanoff (1910) in a psychological context. We translated these English words into each of the four target languages, namely French, German, Italian, and Spanish. As we are at least to some extent familiar with these languages, and as the Kent/Rosanoff vocabulary is fairly straightforward, we did this manually. In cases where we were aware of ambiguities, we tried to come up with a translation relating to what we assumed to be the most frequent of a word?s possible senses. In case of doubt we consulted a number of written bilingual dictionaries, the dict.leo.org dictionary website, and the transla-tion services provided by Google and Yahoo. For each word, we always produced only a sin-gle translation. In an attempt to provide a com-mon test set, the appendix shows the resulting list of word equations in full length for refer-ence by interested researchers. It should be noted that the concept of word equations is a simplification, as it does not take into account the fact that words tend to be am-biguous, and that ambiguities typically do not match across languages. Despite these short-comings we nevertheless use this concept. Let us give some justification.  Word ambiguities are omnipresent in any language. For example, the English word palm has two meanings (tree and hand) which are usually expressed by different words in other languages. However, for our gold standard we must make a choice. We can not include two or more translations in one word equation as this would contradict the principle that all words in a word equation should share their main sense.  Another problem is that, unless we work with dictionaries derived from parallel corpora, it is difficult to estimate how common a transla-tion is. But if we included less common transla-tions in our list, we would have to give their matches a smaller weight during evaluation. 
18
This, however, is difficult to accomplish accu-rately. This is why, despite their shortcomings, we use word equations in this work. Evaluation of our results involves comparing a predicted translation to the corresponding word in the gold standard. We consider the pre-dicted translation to be correct if there is a match, otherwise we consider it as false. While in principle possible, we do not make any finer distinctions concerning the quality of a match. A problem that we face in our approach is what we call the homograph trap. What we mean by this term is that a foreign word occur-ring in a corpus of a particular language may also be a valid word in this language, yet possi-bly with a different meaning. For example, if the German word rot (meaning red) occurs in an English corpus, its occurrences can not easily be distinguished from occurrences of the English word rot, which is a verb describing the process of decay. Having dealt with this problem in Rapp & Zock (2010) we will not elaborate on it here, rather we will suggest a workaround. The idea is to look only at a very restricted vocabulary, namely the words defined in our gold standard. There we have 100 words in each of the five languages, i.e. 500 words altogether. The ques-tion is how many of these words occur more often than once. Note, however, that apart from English (which was the starting point for the gold standard), repetitions can occur not only across languages but also within a language. For example, the Spanish word sue?o means both sleep and dream, which are distinct entries in the list. The following is a complete list of words showing either of these two types of repetitions, i.e. exact string matches (taking into account capitalization and accents): alto (4), bambino (2), Bible (2), bitter (2), casa (2), commando (2), corto (2), doux (2), duro (2), fruit (2), jus-tice (2), lento (2), lion (2), long (2), luna (2), mano (2), memoria (2), mouton (2), religion (2), sacerdote (2), sue?o (2), table (2), whisky (4). However, as is obvious from this list, these repetitions are due to common vocabulary of the languages, with whisky being a typical example. They are not due to incidental string identity of completely different words. So the latter is not a problem (i.e. causing the identification of wrong 
translations) as long as we do not go beyond the vocabulary defined in our gold standard. For this reason and because dealing with the full vocabulary of our (very large) corpora would be computationally expensive, we de-cided to replace in our corpora all words absent from the gold standard by a common designator for unknown words. Also, in our evaluations, for the target language vocabulary we only use the words occurring in the respective column of the gold standard. So far, we always computed translations to single source words. However, if we assume, for example, that we already have word equa-tions for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm. As sug-gested in Rapp & Zock (2010) this can be done by looking up the ranks of each of the four given words (i.e. the words occurring in a par-ticular word equation) within the association vector of a translation candidate, and by multi-plying these ranks. So for each candidate we obtain a product of ranks. We then assume that the candidate with the smallest product will be the best translation.3  Let us illustrate this by an example: If the given words are the variants of the word nerv-ous in English, French, German, and Spanish, i.e. nervous, nerveux, nerv?s, and nervioso, and if we want to find out their translation into Ital-ian, we would look at the association vectors of each word in our Italian target vocabulary. The association strengths in these vectors need to be inversely sorted, and in each of them we will look up the positions of our four given words. Then for each vector we compute the product of the four ranks, and finally sort the Italian vo-cabulary according to these products. We would then expect that the correct Italian translation, namely nervoso, ends up in the first position, i.e. has the smallest value for its product of ranks. 
                                                 3 Note that, especially in the frequent case of zero-co-occurrences, many words may have the same as-sociation strength, and rankings within such a group of words may be arbitrary within a wide range. To avoid such arbitrariness, it is advisable to assign all words within such a group the same rank, which is chosen to be the average rank within the group. 
19
In the next section, we will show the results for this algorithm in addition to those for single source language words. As a different matter, let us mention that for our above algorithm we do not need an explicit identification of what should count as a foreign word. We only need a list of words to be trans-lated, and a list of target language words con-taining the translation candidates from which to choose. Overlapping vocabulary is permitted. If the overlapping words have the same meaning in both languages, then there is no problem and the identification of the correct translation is rather trivial as co-occurrences of a word with itself tend to be frequent. However, if the over-lapping words have different meanings, then we have what we previously called a homogaph trap. In such (for small vocabularies very rare) cases, it would be helpful to be able to distin-guish the occurrences of the foreign words from those of the homograph. However, this problem essentially boils down to a word sense disam-biguation task (actually a hard case of it as the foreign word occurrences, and with them the respective senses, tend to be rare) which is be-yond the scope of this paper. 
3 Experimental Results and Evaluation 
We applied the following procedure on each of the five corpora: The language of the respective corpus was considered the target language, and the vocabulary of the respective column in the gold standard was taken to be the target lan-guage vocabulary.  
 Source Languages 
 DE EN FR ES IT all 
DE WaCky ? 54 22 18 20 48 
ES Giga 9 42 37 ? 29 56 
FR Giga 15 45 ? 20 14 49 
FR WaCky 27 59 ? 16 21 50 
IT WaCky 17 53 29 27 ? 56 
Average 17.0 50.6 29.3 20.3 21.0 51.8  Table 1: Number of correctly predicted translations for various corpora and source languages. Column all refers to the parallel use of all four source lan-guages using the product-of-ranks algorithm. 
The other languages are referred to as the source languages, and the corresponding columns of the gold standard contain the respective vocabu-laries. Using the algorithm described in the pre-vious section, for each source vocabulary the following procedure was conducted: For every source language word the target vocabulary was sorted according to the respective scores. The word obtaining the first rank was considered to be the predicted translation. This predicted translation was compared to the translation listed in the gold standard. If it matched, the prediction was counted as correct, otherwise as wrong. Table 1 lists the number of correct predic-tions for each corpus and for each source lan-guage. These results lead us to the following three conclusions:   1)  The noisier the better  We have only for one language (French) both a Gigaword and a WaCky corpus. The results based on the WaCky corpus are clearly better for all languages except Spanish. Alternatively, we can also look at the average performance for the five source languages among the three WaCky corpora, which is 30.3, and the analo-gous performance for the two Gigaword cor-pora, which is 26.4. These findings lend some support to our hypothesis that noisy web cor-pora are better suited for our purpose than care-fully edited newsticker corpora, which are probably more successful in avoiding foreign language citations  2)  English words are cited more often  In the bottom row, Table 1 shows for each of the five languages the scores averaged over all corpora. As hypothesized previously, we can take citation frequency as an indicator (among others) of the ?importance? of a language. And citation frequency can be expected to correlate with our scores. With 50.6, the average score for English is far better than for any other lan-guage, thereby underlining its special status among world languages. With an average score of 29.3 French comes next which confirms the hypothesis that it is another world language re-ceiving considerable attention elsewhere. Some-what surprising is the finding that Spanish can not keep up with French and obtains an average 
20
score of 20.3 which is even lower than the 21.0 for Italian. A possible explanation is the fact that we are only dealing with European lan-guages here, and that the cultural influence of the Roman Empire and Italy has been so con-siderable in Europe that it may well account for this. So the status of Spanish in the world may not be well reflected in our selection of corpora. Finally, the average score of 17.0 for German shows that it is the least cited language in our selection of languages. Bear in mind, though, that German is the only clearly Germanic lan-guage here, and that its vocabulary is very dif-ferent from that of the other languages. These are mostly Romanic in type, with English somewhere in between. Therefore, the little overlap in vocabulary might make it hard for French, Italian, and Spanish writers to under-stand and use German foreign words.   3)  Little improvement for several source words  The right column in Table 1 shows the scores if (using the product-of-ranks algorithm) four source languages are taken into account in par-allel. As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal. This contrasts with the findings described in Rapp & Zock (2010) where significant improvements could be achieved by increasing the number of source languages. So this casts some doubt on these. However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement. This is not the case here, where we try to improve on a score of around 50 for English. Remember that this is a somewhat conservative score as we count cor-rect but alternative translations, as errors. As this is already a performance much closer to the optimum, making further performance gains is more difficult. Therefore, perhaps we should take it as a success that the product-of-ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non-English languages was probably mostly detrimental. Having analyzed the quantitative results, to give a better impression of the strengths and weaknesses of our algorithm, for the (according to Table 1) best performing combination of cor-
pus and language pair, namely the French WaCky corpus, English as the source language and French as the target language, Table 2 shows some actual source words and their com-puted translations. 
 
  ESW    CF   ET  RE  CT 
cabbage 9 chou 1 chou blossom 25 fleur 73 commande carpet 39 tapis 1 tapis bitter 59 amer 1 amer hammer 67 marteau 1 marteau bread 82 pain 1 pain citizen 115 citoyen 1 citoyen bath 178 bain 1 bain butterfly 201 papillon 1 papillon eat 208 manger 1 manger butter 220 beurre 59 terre eagle 282 aigle 1 aigle cheese 527 fromage 1 fromage cold 539 froid 1 froid deep 585 profond 1 profond cottage 624 cabanon 1 cabanon earth 702 terre 53 tabac child 735 enfant 1 enfant bed 806 lit 2 table beautiful 923 beau 1 beau care 1267 soin 1 soin hand 1810 main 2 main city 2610 ville 1 ville girl 2673 fille 1 fille green 2861 vert 1 vert blue 2914 bleu 1 bleu hard 3615 dur 1 dur black 9626 noir 1 noir Bible 17791 Bible 1 Bible foot 23548 pied 8 siffler chair 24027 chaise 1 chaise fruit 38544 fruit 1 fruit  Table 2: Results for the language pair English ? French. The meaning of the columns is as follows: ESW = English source word; CF = corpus frequency of English source word; ET = expected translation according to gold standard; RE = computed rank of expected translation; CT = computed translation. 
4 Summary and Future Work 
In this paper we made an attempt to solve the difficult problem of identifying word trans-lations on the basis of a single monolingual cor-
21
pus, whereby the same corpus is used for sev-eral language pairs. The basic idea underlying our work is to look at foreign words, to compute their co-occurrence-based associations, and to consider these as translations of the respective words. Whereas Rapp & Zock (2010) dealt only with an English corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora. We were able to shed some light on criteria influencing per-formance, such as the selection of text type and the direction of a language pair. For example, it is more promising to look at occurrences of English words in a German corpus rather than the other way around. Because of the special status of English it is also advisable to use it as a pivot wherever possible. Perhaps surprisingly, the work may have im-plications regarding cognitive models of second language acquisition. The reason is that it de-scribes how to acquire the vocabulary of a new language from a mixed corpus. This is relevant as traditional foreign language teaching (involv-ing explanations in the native tongue and vo-cabulary learning using bilingual word lists) can be considered as providing such a mixed corpus. Regarding future work, let us outline a plan for the construction of a universal dictionary of all languages which are well enough represented on the web.4 There might be some chance for it, because the algorithm can be extended to work with standard search engines and is also suitable for a bootstrapping approach.  Let us start by assuming that we have a large matrix where the rows correspond to the union of the vocabularies of a considerable number of languages, and the columns correspond to these languages themselves. We presuppose no prior translation knowledge, so that the matrix is completely empty at the beginning (although prior knowledge could be useful for the iterative algorithm to converge). STEP 1: For each word in the vocabulary we perform a search via a search engine such as Google, preferably in an automated fashion via an application programming interface (API). Next, we retrieve as many documents as possi-
                                                 4 Note that this plan could also be adapted to other methodologies (such as Rapp, 1999), and may be more promising with these. 
ble, and separate them according to language.5 Then, for each language for which we have ob-tained the critical mass of documents, we apply our algorithm and compute the respective trans-lations. These are entered into the matrix. As we are interested in word equations, we assume that translations are symmetric. This means that each translation identified can be entered at two positions in the matrix. So at the end of step 1 we have for each word the translations into a number of other languages, but this number may still be small at this stage.  STEP 2: We now look at each row of the ma-trix and feed the words found within the same row into the product-of-ranks algorithm. We do not have to repeat the Google search, as step 1 already provided all documents needed. Be-cause when looking at several source words we have a better chance to find occurrences in our documents, this should give us translations for some more languages in the same row. But we also need to recompute the translations resulting from the previous step as some of them will be erroneous e.g.  for reasons of data sparseness or due to the homograph trap. STEP 3: Repeat step 2 until as many matrix cells as possible are filled with translations. We hope that with each iteration completeness and correctness improve, and that the process con-verges in such a way that the (multilingual) words in each row disambiguate each other, so that ultimately each row corresponds to an un-ambiguous concept. 
Acknowledgments 
Part of this research was supported by a Marie Curie Intra European Fellowship within the 7th European Community Framework Programme. We thank the WaCky group for making avail-able their excellent Web corpora, and the Lin-guistic Data Consortium for providing the Gi-gaword Corpora. We also wish to thank Lourdes Callau, Maria Dolores Jimenez Lopez, and Lilica Voicu for their support in acquiring the LDC corpora. 
                                                 5  If the language identification markup within the retrieved documents turns out to be unreliable (which is unfortunately often the case in practice), standard language identification software can be used. 
22
References 
Armstrong, Susan; Kempen, Masja; McKelvie, David; Petitpierre, Dominique; Rapp, Reinhard; Thompson, Henry (1998). Multilingual Corpora for Cooperation. Proceedings of the 1st Interna-tional Conference on Linguistic Resources and Evaluation (LREC), Granada, Vol. 2, 975?980.  
Baroni, Marco; Bernardini, Silvia; Ferraresi, Adri-ano,  Zanchetta, Eros (2009). The WaCky Wide Web: A collection of very large linguistically pro-cessed Web-crawled corpora. Journal of Language Resources and Evaluation 43 (3): 209-226. 
Brown, Peter; Cocke, John; Della Pietra, Stephen A.; Della Pietra, Vincent J.; Jelinek, Frederick; Laf-ferty, John D.; Mercer, Robert L.; Rossin, Paul S. (1990). A statistical approach to machine transla-tion. Computational Linguistics, 16(2), 79?85. 
Chiao, Yun-Chuang; Sta, Jean-David; Zwei-genbaum, Pierre (2004). A novel approach to im-prove word translations extraction from non-parallel, comparable corpora. In: Proceedings of the International Joint Conference on Natural Language Processing, Hainan, China. AFNLP. 
Denoyer, Ludovic; Gallinari, Pattrick (2006). The Wikipedia XML Corpus. SIGIR Forum, 40(1), 64?69. 
Dunning, T. (1993). Accurate methods for the sta-tistics of surprise and coincidence. Computational Linguistics, 19(1), 61?74.  
Ferraresi, Adriano; Bernardini, Silvia; Picci, Gio-vanni; Baroni, Marco (2010). Web corpora for bi-lingual lexicography: a pilot study of English/ French collocation extraction and translation. In Xiao, Richard (ed.): Using Corpora in Contrastive and Translation Studies. Newcastle: Cambridge Scholars Publishing. 
Fung, Pascale; McKeown, Kathy (1997). Finding terminology translations from non-parallel cor-pora. Proceedings of the 5th Annual Workshop on Very Large Corpora, Hong Kong, 192-202.  
Fung, Pascale; Yee, Lo Yuen (1998). An IR ap-proach for translating new words from nonparallel, comparable texts. In: Proceedings of COLING-ACL 1998, Montreal, Vol. 1, 414-420. 
Harris, Zelig S. (1954). Distributional structure. WORD, 10:146?162.  
Kent, Grace Helen; Rosanoff , A.J. (1910). A study of association in insanity. American Journal of In-sanity 67:317?390. Koehn, Philipp (2005). Europarl: A parallel corpus for statistical machine translation. Proceedings of MT Summit, Phuket, Thailand, 79?86.  
Koehn, Philipp; Hoang, Hieu; Birch, Alexandra; Callison-Burch, Chris; Federico, Marcello; Ber-toldi, Nicola; Cowan, Brooke; Shen, Wade; Moran, Christine; Zens, Richard; Dyer, Chris; Bo-jar, Ond?ej; Constantin, Alexandra; Herbst, Evan (2007). Moses: Open source toolkit for statistical machine translation. In: Proceedings of ACL, Pra-gue, demonstration session, 177?180.  
Koehn, Philipp; Knight, Kevin (2002). Learning a translation lexicon from monolingual corpora. In: Unsupervised Lexical Acquisition. Proceedings of the ACL SIGLEX Workshop, 9?16. 
Mendon?a, Angelo, Graff, David, DiPersio, Denise (2009a). French Gigaword Second Edition. Lingu-istic Data Consortium, Philadelphia. 
Mendon?a, Angelo, Graff, David, DiPersio, Denise (2009b). Spanish Gigaword Second Edition. Lin-guistic Data Consortium, Philadelphia. 
Munteanu, Dragos Stefan; Marcu, Daniel (2005). Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics 31(4), 477?504.  
Och, Franz Josef; Ney, Hermann (2003). A System-atic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1), 19?51. 
Pantel, Patrick; Lin, Dekang (2002). Discovering word senses from text. In: Proceedings of ACM SIGKDD, Edmonton, 613?619 
Rapp, Reinhard (1995). Identifying word translations in non-parallel texts. In: Proceedings of the 33rd Meeting of the Association for Computational Lin-guistics. Cambridge, Massachusetts, 320-322.  
Rapp, Reinhard. (1999). Automatic identification of word translations from unrelated English and German corpora. In: Proceedings of the 37th An-nual Meeting of the Association for Computational Linguistics 1999, College Park, Maryland. 519?526. 
Rapp, Reinhard; Mart?n Vide, Carlos (2007). Statis-tical machine translation without parallel corpora. In: Georg Rehm, Andreas Witt, Lothar Lemnitzer (eds.): Data Structures for Linguistic Resources and Applications. Proceedings of the Biennial GLDV Conference 2007. T?bingen: Gunter Narr Verlag. 231?240.  
Rapp, Reinhard; Zock, Michael (2010). Utilizing Citations of Foreign Words in Corpus-Based Dic-tionary Generation. Proceedings of NLPIX 2010. 
Sharoff, Serge (2006). Creating general-purpose cor-pora using automated search engine queries. In Marco Baroni and Silvia Bernardini (eds.): WaCky! Working papers on the Web as Corpus. Gedit, Bologna, http://wackybook.sslmit.unibo.it/ 
 
23
Steinberger, Ralf; Pouliquen, Bruno; Widiger, Anna; Ignat, Camelia; Erjavec, Toma?; Tufi?, Dan; Varga, D?niel (2006). The JRC-Acquis: A multi-lingual aligned parallel corpus with 20+ languages. Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006). Genoa, Italy.   
Wu, Dekai; Fung, Pascale (2005). Inversion trans-duction grammar constraints for mining parallel sentences from quasi-comparable corpora. Pro-ceedings of the Second International Joint Confer-ence on  Natural Language Processing (IJCNLP-2005). Jeju, Korea.   
Appendix: Gold Standard of 100 Word Equations 
 
 ENGLISH GERMAN FRENCH SPANISH ITALIAN 
1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  
anger baby bath beautiful bed Bible bitter black blossom blue boy bread butter butterfly cabbage care carpet chair cheese child citizen city cold command convenience cottage dark deep doctor dream eagle earth eat foot fruit girl green hammer hand handle hard head health heavy 
Wut Baby Bad sch?n Bett Bibel bitter schwarz Bl?te blau Junge Brot Butter Schmetterling Kohl Pflege Teppich Stuhl K?se Kind B?rger Stadt kalt Kommando Bequemlichkeit H?uschen dunkel tief Arzt Traum Adler Erde essen Fu? Frucht M?dchen gr?n Hammer Hand Griff hart Kopf Gesundheit schwer 
col?re b?b? bain beau lit Bible amer noir fleur bleu gar?on pain beurre papillon chou soin tapis chaise fromage enfant citoyen ville froid commande commodit? cabanon fonc? profond m?decin r?ve aigle terre manger pied fruit fille vert marteau main poign?e dur t?te sant? lourd 
furia beb? ba?o hermoso cama Biblia amargo negro flor azul chico pan mantequilla mariposa col cuidado alfombra silla queso ni?o ciudadano ciudad fr?o comando conveniencia casita oscuro profundo m?dico sue?o ?guila tierra comer pie fruta chica verde martillo mano manejar duro cabeza salud pesado 
rabbia bambino bagno bello letto Bibbia amaro nero fiore blu ragazzo pane burro farfalla cavolo cura tappeto sedia formaggio bambino cittadino citt? freddo comando convenienza casetta buio profondo medico sogno aquila terra mangiare piede frutta ragazza verde martello mano maniglia duro testa salute pesante 
24
45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  
high house hungry joy justice King lamp light lion long loud man memory moon mountain music mutton needle nervous ocean oven priest quick quiet red religion river rough salt scissors sheep short sickness sleep slow smooth soft soldier sour spider square stomach street sweet table thief thirsty tobacco whisky whistle white window wish woman work yellow 
hoch Haus hungrig Freude Gerechtigkeit K?nig Lampe Licht L?we lang laut Mann Ged?chtnis Mond Berg Musik Hammel Nadel nerv?s Ozean Backofen Priester schnell still rot Religion Fluss rau Salz Schere Schaf kurz Krankheit schlafen langsam glatt weich Soldat sauer Spinne Quadrat Magen Stra?e s?? Tisch Dieb durstig Tabak Whisky pfeifen wei? Fenster Wunsch Frau arbeiten gelb 
?lev? maison affam? joie justice roi lampe lumi?re lion long fort homme m?moire lune montagne musique mouton aiguille nerveux oc?an four pr?tre rapide tranquille rouge religion rivi?re rugueux sel ciseaux mouton courte maladie sommeil lent lisse doux soldat acide araign?e carr? estomac rue doux table voleur soif tabac whisky siffler blanc fen?tre d?sir femme travail jaune 
alto casa hambriento alegr?a justicia rey l?mpara luz le?n largo alto hombre memoria luna monta?a m?sica cordero aguja nervioso oc?ano horno sacerdote r?pido tranquilo rojo religi?n r?o ?spero sal tijeras oveja corto enfermedad sue?o lento liso suave soldado agrio ara?a cuadrado est?mago calle dulce mesa ladr?n sediento tabaco whisky silbar blanco ventana deseo mujer trabajo amarillo 
alto casa affamato gioia giustizia re lampada luce leone lungo alto uomo memoria luna montagna musica montone ago nervoso oceano forno sacerdote rapido tranquillo rosso religione fiume ruvido sale forbici pecora corto malattia dormire lento liscio morbido soldato acido ragno quadrato stomaco strada dolce tavolo ladro assetato tabacco whisky fischiare bianco finestra desiderio donna lavoro giallo 
25
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 43?51,
Beijing, August 2010
A Voting Mechanism for Named Entity Translation in English?
Chinese Question Answering 
Ling-Xiang Tang1, Shlomo Geva1, Andrew Trotman2, Yue Xu1 
 
1Faculty of Science and Technology 
Queensland University of Technology 
 {l4.tang,s.geva,yue.xu}@qut.edu.au
 
2Department of Computer Science 
University of Otago 
 
 
andrew@cs.otago.ac.nz
 
 
 
Abstract 
In this paper, we describe a voting 
mechanism for accurate named entity 
(NE) translation in English?Chinese 
question answering (QA). This mecha-
nism involves translations from three 
different sources: machine translation, 
online encyclopaedia, and web docu-
ments. The translation with the highest 
number of votes is selected. We evalu-
ated this approach using test collection, 
topics and assessment results from the 
NTCIR-8 evaluation forum. This 
mechanism achieved 95% accuracy in 
NEs translation and 0.3756 MAP in 
English?Chinese cross-lingual infor-
mation retrieval of QA.  
1  Introduction 
Nowadays, it is easy for people to access 
multi-lingual information on the Internet. Key 
term searching on an information retrieval (IR) 
system is common for information lookup. 
However, when people try to look for answers 
in a different language, it is more natural and 
comfortable for them to provide the IR system 
with questions in their own natural languages 
(e.g. looking for a Chinese answer with an 
English question: ?what is Taiji??). Cross-
lingual question answering (CLQA) tries to 
satisfy such needs by directly finding the cor-
rect answer for the question in a different lan-
guage.  
In order to return a cross-lingual answer, a 
CLQA system needs to understand the ques-
tion, choose proper query terms, and then ex-
tract correct answers. Cross-lingual informa-
tion retrieval (CLIR) plays a very important 
role in this process because the relevancy of 
retrieved documents (or passages) affects the 
accuracy of the answers. 
A simple approach to achieving CLIR is to 
translate the query into the language of the tar-
get documents and then to use a monolingual 
IR system to locate the relevant ones. How-
ever, it is essential but difficult to translate the 
question correctly. Currently, machine transla-
tion (MT) can achieve very high accuracy 
when translating general text. However, the 
complex phrases and possible ambiguities pre-
sent in a question challenge general purpose 
MT approaches. Out-of-vocabulary (OOV) 
terms are particularly problematic. So the key 
for successful CLQA is being able to correctly 
translate all terms in the question, especially 
the OOV phrases. 
In this paper, we discuss an approach for 
accurate question translation that targets the 
OOV phrases and uses a translation voting 
mechanism. This mechanism involves transla-
tions from three different sources: machine 
translation, online encyclopaedia, and web 
documents. The translation with the highest 
number of votes is selected. To demonstrate 
this mechanism, we use Google Translate 
43
(GT)1 as the MT source, Wikipedia as the en-
cyclopaedia source, and Google web search 
engine to retrieve Wikipedia links and relevant 
Web document snippets.  
English questions on the Chinese corpus for 
CLQA are used to illustrate of this approach. 
Finally, the approach is examined and evalu-
ated in terms of translation accuracy and re-
sulting CLIR performance using the test col-
lection, topics and assessment results from 
NTCIR-82. 
English Question Templates (QTs) 
who [is | was | were | will], what is the definition of, 
what is the [relationship | interrelationship | inter-
relationship]  [of | between], what links are there, 
what link is there, what [is | was | are | were | does | 
happened], when [is | was | were |  will | did | do],  
where [will | is | are | were], how [is | was | were | 
did], why [does | is | was | do | did | were | can | 
had], which [is | was | year], please list, describe 
[relationship | interrelationship | inter-relationship]  
[of | between], could you [please | EMPTY] give 
short description[s] to, who, where, what, which, 
how, describe, explain 
Chinese QT Counterparts 
???????, ??????, ??????, ????
??, ?????,?????, ?????, ?????, 
?????, ?????, ????, ????,????, 
????, ????, ????, ????, ????, ?
???, ????, ????, ????, ????, ??
??, ????, ????,????, ????, ???
?, ????, ????, ???,???, ???, ??
?, ???, ???,???, ???, ???, ???, 
???, ???, ???,???,???, ???,???,
???,??,??, ??,??, ??, ??, ??, ??, 
??, ??, ??, ??, ??, ??, ??, ??, ?
?,??, ?? 
Table 1. Question templates 
2 CLIR Issue and Related Work 
In CLIR, retrieving documents with a cross-
lingual query with out-of-vocabulary phrases 
has always been difficult. To resolve this prob-
lem, an external resource such as Web or 
Wikipedia is often used to discover the possi-
ble translation for the OOV term. Wikipedia 
and other Web documents are thought of as 
treasure troves for OOV problem solving be-
cause they potentially cover the most recent 
OOV terms. 
                                                 
1 http://translate.google.com. 
2 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html. 
The Web-based translation method was 
shown to be an effective way to solve the OOV 
phrase problem (Chen et al, 2000; Lu et al, 
2007; Zhang & Vines, 2004; Zhang et al, 
2005). The idea behind this method is that a 
term/phrase and its corresponding translation 
normally co-exist in the same document be-
cause authors often provide the new terms? 
translation for easy reading.  
In Wikipedia the language links provided 
for each entry cover most popular written lan-
guages, therefore, it was used to solve a low 
coverage issue on named entities in Eu-
roWordNet (Ferr?ndez et al, 2007); a number 
of research groups (Chan et al, 2007; Shi et 
al., 2008; Su et al, 2007; Tatsunori Mori, 
2007) employed Wikipedia to tackle OOV 
problems in the NTCIR evaluation forum. 
3 CLQA Question Analysis 
Questions for CLQA can be very complex. For 
example, ?What is the relationship between the 
movie "Riding Alone for Thousands of Miles" 
and ZHANG Yimou??. In this example, it is 
important to recognise two named entities 
("Riding Alone for Thousands of Miles" and 
?ZHANG Yimou?) and to translate them pre-
cisely.  
In order to recognise the NEs in the ques-
tion, first, English question template phrases in 
Table 1 are removed from question; next, we 
use the Stanford NLP POS tagger (The Stan-
ford Natural Language Processing Group, 
2010) to identify the named entities; then 
translate them accordingly. Chinese question 
template phrases are also pruned from the 
translated question at the end to reduce the 
noise words in the final query.  
There are three scenarios in which a term or 
phrase is considered a named entity. First, it is 
consecutively labelled NNP or NNPS (Univer-
sity of Pennsylvania, 2010). Second, term(s) 
are grouped by quotation marks.  For example, 
to extract a named entity from the example 
question above, three steps are needed: 
1. Remove the question template phrase 
?What is the relationship between? from 
the question. 
2. Process the remaining using the POS tag-
ger, giving ?the_DT movie_NN ``_`` Rid-
ing_NNP Alone_NNP for_IN Thou-
44
sands_NNS of_IN Miles_NNP 
``_``and_CC ZHANG_NNP Yimou_NNP 
?_.? 
3. ?Riding Alone for Thousands of Miles? is 
between two tags (``) and so is an entity, 
and the phrase ?ZHANG Yimou?, as indi-
cated by two consecutive NNP tags is also 
a named entity. 
Third, if a named entity recognised in the two 
scenarios above is followed in the question by 
a phrase enclosed in bracket pairs, this phrase 
will be used as a tip term providing additional 
information about this named entity. For in-
stance, in the question ?Who is David Ho (Da-i 
Ho)??, ?Da-i Ho? is the tip term of the named 
entity ?David Ho?.  
4 A Voting Mechanism for Named 
Entity Translation (VMNET) 
Observations have been made:  
? Wikipedia has over 100,000 Chinese en-
tries describing various up-to-date events, 
people, organizations, locations, and facts. 
Most importantly, there are links between 
English articles and their Chinese counter-
parts.  
? When people post information on the 
Internet, they often provide a translation 
(where necessary) in the same document.  
These pages contain bilingual phrase pairs.  
For example, if an English term/phrase is 
used in a Chinese article, it is often fol-
lowed by its Chinese translation enclosed 
in parentheses. 
? A web search engine such as Google can 
identify Wikipedia entries, and return 
popular bi-lingual web document snippets 
that are closely related to the query.  
? Statistical machine translation relying on 
parallel corpus such as Google Translate 
can achieve very high translation accuracy. 
Given these observations, there could be up 
to three different sources from which we can 
obtain translations for a named entity; the task 
is to find the best one.  
4.1 VMNET Algorithm 
A Google search on the extracted named entity 
is performed to return related Wikipedia links 
and bilingual web document snippets.   Then 
from the results of Web search and MT, three 
different translations could be acquired. 
Wikipedia Translation 
The Chinese equivalent Wikipedia pages 
could be found by following the language links 
in English pages. The title of the discovered 
Chinese Wikipedia page is then used as the 
Wikipedia translation.  
Bilingual Clue Text Translation 
 The Chinese text contained in the snippets 
returned by the search engine is processed for 
bilingual clue text translation. The phrase in a 
different language enclosed in parentheses 
which come directly after the named entity is 
used as a candidate translation. For example, 
from a web document snippet, ?YouTube - 
Sean Chen (???) dunks on Yao Ming??, ?
???? can be extracted and used as a candi-
date translation of ?Sean Chen?, who is a bas-
ket ball player from Taiwan. 
Machine Translation 
In the meantime, translations for the named 
entity and its tip term (if there is one) are also 
retrieved using Google Translate.  
Regarding the translation using Wikipedia, 
the number of results could be more than one 
because of ambiguity. So for a given named 
entity, we could have at least one, but possibly 
more than three candidate translations. 
With all possible candidate translations, the 
best one then can be selected. Translations 
from all three sources are equally weighted. 
Each translation contributes one vote, and the 
votes for identical translation are cumulated. 
The best translation is the one with the highest 
number of votes. In the case of a tie, the first 
choice of the best translation is the Wikipedia 
translation if only one Wiki-entry is found; 
otherwise, the priority for choosing the best is 
bilingual clue text translation, then machine 
translation. 
4.2 Query Generation with VMNET 
Because terms can have multiple meanings, 
ambiguity often occurs if only a single term is 
given in machine translation. A state-of-the-art 
MT toolkit/service could perform better if 
more contextual information is provided. So a 
better translation is possible if the whole sen-
tence is given (e.g. the question). For this rea-
45
son, the machine translation of the question is 
the whole query and not with the templates 
removed. 
However, issues arise: 1) how do we know 
if all the named entities in question are trans-
lated correctly? 2) if there is an error in named 
entity translation, how can it be fixed? Particu-
larly for case 2, the translation for the whole 
question is considered acceptable, except for 
the named entity translation part. We intend to 
keep most of the translation and replace the 
bad named entity translation with the good 
one. But finding the incorrect named entity 
translation is difficult because the translation 
for a named entity can be different in different 
contexts. The missing boundaries in Chinese 
sentences make the problem harder. To solve 
this, when a translation error is detected, the 
question is reformatted by replacing all the 
named entities with some nonsense strings 
containing special characters as place holders. 
These place holders remain unchanged during 
the translation process.  The good NE transla-
tions then can be put back for the nearly trans-
lated question.  
 Given an English question Q, the detailed 
steps for the Chinese query generation are as 
following: 
1. Retrieve machine translation Tmt for the 
whole question from Google Translate.   
2. Remove question template phrase from 
question. 
3. Process the remaining using the POS tag-
ger.  
4. Extract the named entities from the tagged 
words using the method discussed in Sec-
tion 3. 
5. Replace each named entity in question Q 
with a special string Si,(i =0,1,2,..) which 
makes nonsense in translation and is 
formed by a few non-alphabet characters. 
In our experiments, Si is created by joining 
a double quote character with a ^ character 
and the named entity id (a number, starting 
from 0, then increasing by 1 in order of 
occurrence of the named entity) followed 
by another double quote character. The fi-
nal Si, becomes ?^id?. The resulting ques-
tion is used as Qs.  
6. Retrieve machine translation Tqs for Qs 
from Google Translate.  Since Si consists 
of special characters, it remains unchanged 
in Tqs. 
7. Start the VMNET loop for each named 
entity. 
8. With an option set to return both English 
and Chinese results, Google the named en-
tity and its tip term (if there is one).  
9. If there are any English Wikipedia links in 
the top 10 search results, then retrieve 
them all. Else, jump to step 12. 
10. Retrieve all the corresponding Chinese 
Wikipedia articles by following the lan-
guages links in the English pages. If none, 
then jump to step 12. 
11. Save the title NETwiki(i) of each Chinese 
Wikipedia article  Wiki(i). 
12. Process the search results again to locate a 
bilingual clue text translation candidate - 
NETct, as discussed in Section 4.1. 
13. Retrieve machine translation NETmt, and 
NETtip for this named entity and its tip term 
(if there is one). 
14. Gather all candidate translations: NET-
wiki(*), NETct, NETtip, and NETmt  for vot-
ing. The translation with the highest num-
ber of votes is considered the best 
(NETbest). If there is a tie, NETbest is then 
assigned the translation with the highest 
priority. The priority order of candidate 
translation is NETwiki(0) (if 
sizeof(NETwiki(*))=1)  >  NETct  > NETmt. It 
means when a tie occurs and if there are 
more than one Wikipedia translation, all 
the Wikipedia translations are skipped. 
15. If Tmt does not contain NETbest, it is then 
considered a faulty translation. 
16. Replace Si  in Tqs with NETbest. 
17. If NETbest is different from any NETwiki(i) 
but can be found in the content of a 
Wikipedia article (Wiki(i)), then the corre-
sponding NETwiki(i)  is used as an addi-
tional query term, and appended to the fi-
nal Chinese query. 
18. Continue the VMNET loop and jump back 
to step 8 until no more named entities re-
main in the question. 
19. If Tmt was considered a faulty translation, 
use Tqs as the final translation of Q. Other-
wise, just use Tmt. The Chinese question 
template phrases are pruned from the 
translation for the final query generation.  
46
A short question translation example is 
given below: 
? For the question ?What is the relationship 
between the movie "Riding Alone for 
Thousands of Miles" and ZHANG Yi-
mou??, retrieving its Chinese translation  
from a MT service, we get the following: 
???????????????????
????. 
? The translation for the movie name "Rid-
ing Alone for Thousands of Miles" of 
?ZHANG Yimou? is however incorrect. 
? Since the question is also reformatted into 
?What is the relationship between the 
movie "^0" and ?^1???, machine transla-
tion returns a second translation:  ????
???????^ 0???^ 1?? 
? VMNET obtains the correct translations:  
????? and ???, for two named en-
tities "Riding Alone for Thousands of 
Miles" and ?ZHANG Yimou? respectively. 
? Replace the place holders with the correct 
translations in the second translation and 
give the final Chinese translation: ???
??????????????????
??? 
5 Information Retrieval 
5.1 Chinese Document Processing 
Approaches to Chinese text indexing vary: 
Unigrams, bigrams and whole words are all 
commonly used as tokens. The performance of 
various IR systems using different segmenta-
tion algorithms or techniques varies as well 
(Chen et al, 1997; Robert & Kwok, 2002). It 
was seen in prior experiments that using an 
indexing technique requiring no dictionary can 
have similar performance to word-based index-
ing (Chen, et al, 1997). Using bigrams that 
exhibit high mutual information and unigrams 
as index terms can achieve good results. Moti-
vated by indexing efficiency and without the 
need for Chinese text segmentation, we use 
both bigrams and unigrams as indexing units 
for our Chinese IR experiments. 
5.2 Weighting Model 
A slightly modified BM25 ranking function 
was used for document ordering.  
When calculating the inverse document fre-
quency, we use: 
           
 
 
   (1) 
where N is the number of documents in the 
corpus, and n is the document frequency of 
query term  . The retrieval status value of a 
document d with respect to query            
is given as: 
 
          
 
                 
                  ?       
      
     
 
          
 
   
           (2) 
where          is the term frequency of term 
   in document d;        is the length of 
document d in words and avgdl is the mean 
document length. The number of bigrams is 
included in the document length. The values of 
the tuneable parameters    and b used in our 
experiments are 0.7 and 0.3 respectively. 
6 CLIR Experiment 
6.1 Test Collection and Topics 
Table 2 gives the statistics of the test collection 
and the topics used in our experiments. The 
collection contains 308,845 documents in sim-
plified Chinese from Xinhua News. There are 
in total 100 topics consisting of both English 
and Chinese questions. This is a NTCIR-8 col-
lection for ACLIA task.  
Corpus #docs #topics 
Xinhua Chinese (simplified) 308,845 100 
Table 2. Statistics of test corpus and topics 
6.2 Evaluation Measures 
The evaluation of VMNET performance cov-
ers two main aspects: translation accuracy and 
CLIR performance.  
As we focus on named entity translation, the 
translation accuracy is measured using the pre-
cision of translated named entities at the topic 
level. So the translation precision -P is defined 
as: 
   
 
 
    (3) 
where c is the number of topics in which all 
the named entities are correctly translated; N is 
the number of topics evaluated. 
47
The effectiveness of different translation 
methods can be further measured by the result-
ing CLIR performance. In NTCIR-8, CLIR 
performance is measured using the mean aver-
age precision. The MAP values are obtained 
by running the ir4qa_eval2 toolkit with the 
assessment results 3  on experimental run 
s(NTCIR Project, 2010).  MAP is computed 
using only 73 topics due to an insufficient 
number of relevant document found for the 
other 27 topics (Sakai et al, 2010). This is the 
case for all NTCIR-8 ACLIA submissions and 
not our decision. 
It also must be noted that there are five top-
ics that have misspelled terms in their English 
questions. The misspelled terms in those 5 top-
ics are given in Table 3. It is interesting to see 
how different translations cope with misspelled 
terms and how this affects the CLIR result.  
Topic ID Misspelling Correction 
ACLIA2-CS-0024 Qingling Qinling 
ACLIA2-CS-0035 Initials D Initial D 
ACLIA2-CS-0066 Kasianov Kasyanov 
ACLIA2-CS-0074 
Northern 
Territories 
northern 
territories 
ACLIA2-CS-0075 Kashimir Kashmir 
Table 3. The misspelled terms in topics 
6.3 CLIR Experiment runs 
A few experimental runs were created for 
VMNET and CLIR system performance 
evaluation. Their details are listed in Table 7.  
Those with name *CS-CS* are the Chinese 
monolingual IR runs; and those with the name 
*EN-CS* are the English-to-Chinese CLIR 
runs. Mono-lingual IR runs are used for 
benchmarking our CLIR system performance.  
7 Results and Discussion 
7.1 Translation Evaluation 
The translations in our experiments using 
Google Translate reflect only the results re-
trieved at the time of the experiments because 
Google Translate is believed to be improved 
over time. 
The result of the final translation evaluation 
on the 100 topics is given in Table 4.  Google 
Translate had difficulties in 13 topics. If all 
                                                 
3 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html. 
thirteen named entities in those topics where 
Google Translate failed are considered OOV 
terms, the portion of topics with OOV phrases 
is relatively small. Regardless, there is an 8% 
improvement achieved by VMNET reaching 
95% precision.   
Method c N P 
Google Translate 87 100 87% 
VMNET 95 100 95% 
Table 4. Translation Evaluation Results 
There are in total 14 topics in which Google 
Translate or VMNET failed to correctly trans-
late all named entities. These topics are listed 
in Table 8. Interestingly, for topic (ACLIA2-
CS-0066) with the misspelled term 
?Kasianov?, VMNET still managed to find a 
correct translation (??????????
???????). This has to be attributed to 
the search engine?s capability in handling mis-
spellings. On the other hand, Google Translate 
was correct in its translation of ?Northern Ter-
ritories? of Japan, but VMNET incorrectly 
chose ?Northern Territory? (of Australia). For 
the rest of the misspelled phrases (Qingling, 
Initials D, Kashimir), neither Google Translate 
nor VMNET could pick the correct translation. 
7.2 IR Evaluation 
The MAP values of all experimental runs cor-
responding to each query processing technique 
and Chinese indexing strategy are given in Ta-
ble 5. The results of mono-lingual runs give 
benchmarking scores for CLIR runs.  
As expected, the highest MAP 0.4681 is 
achieved by the monolingual run VMNET-CS-
CS-01-T, in which the questions were manu-
ally segmented and all the noise words were 
removed.  
It is encouraging to see that the automatic 
run VMNET-CS-CS-02-T with only question 
template phrase removal has a slightly lower 
MAP 0.4419 than that (0.4488) of the best per-
formance CS-CS run in the NTCIR-8 evalua-
tion forum (Sakai, et al, 2010). 
If unigrams were used as the only indexing 
units, the MAP of VMNET-CS-CS-04-T 
dropped from 0.4681 to 0.3406. On the other 
hand, all runs using bigrams as indexing units 
either exclusively or jointly performed very 
well. The MAP of run VMNET-CS-CS-05-T 
using bigrams only is 0.4653, which is slightly 
48
lower than that of the top performer run 
VMNET-CS-CS-01-T, which used two forms 
of indexing units. However, retrieval perform-
ance could be maximised by using both uni-
grams and bigrams as indexing units. 
The highest MAP (0.3756) of a CLIR run is 
achieved by run VMNET-EN-CS-03-T, which 
used VMNET for translation. Comparing it to 
our manual run VMNET-CS-CS-01-T, there is 
around 9% performance degradation as a result 
of the influence of noise words in the ques-
tions, and the possible information loss or 
added noise due to English-to-Chinese transla-
tion, even though the named entities translation 
precision is relatively high. 
The best EN-CS CLIR run (MAP 0.4209)  
in all submissions to the NTCIR-8 ACLIA task 
used the same indexing technique (bigrams 
and unigrams) and ranking function (BM25) as 
run VMNET-EN-CS-03-T but with ?query 
expansion based on RSV? (Sakai, et al, 2010).  
The MAP difference 4.5% between the forum 
best run and our CLIR best run could suggest 
that using query expansion is an effective way 
to improve the CLIR system performance.  
Runs VMNET-EN-CS-01-T and VMNET-
EN-CS-04-T, that both used Google Translate 
provide direct comparisons with runs 
VMNET-EN-CS-02-T and VMNET-EN-CS-
03-T, respectively, which employed VMNET 
for translation. All runs using VMNET per-
formed better than the runs using Google 
Translate.  
Run Name MAP 
NTCIR-8 CS-CS BEST 0.4488 
VMNET-CS-CS-01-T 0.4681 
VMNET-CS-CS-02-T 0.4419 
VMNET-CS-CS-03-T 0.4189 
VMNET-CS-CS-04-T 0.3406 
VMNET-CS-CS-05-T 0.4653 
NTCIR-8 EN-CS BEST 0.4209 
VMNET-EN-CS-01-T 0.3161 
VMNET-EN-CS-02-T 0.3408 
VMNET-EN-CS-03-T 0.3756 
VMNET-EN-CS-04-T 0.3449 
Table 5. Results of all experimental runs 
The different performances between CLIR 
runs using Google Translate and VMENT is 
the joint result of the translation improvement 
and other translation differences. As shown in 
Table 8, VMNET found the correct transla-
tions for 8 more topics than Google Translate. 
It should be noted that there are two topics 
(ACLIA2-CS-0008 and ACLIA2-CS-0088) 
not included in the final CLIR evaluation (Sa-
kai, et al, 2010). Also, there is one phrase, 
?Kenneth Yen (K. T. Yen) (???)?, which 
VMNET couldn?t find the correct translation 
for, but it detected a highly associated term 
?Yulon - ?????, an automaker company in 
Taiwan; Kenneth Yen is the CEO of Yulon. 
Although Yulon is not a correct translation, it is 
still a good query term because it is then possi-
ble to find the correct answer for the question: 
?Who is Kenneth Yen??. However, this topic 
was not included in the NTCIR-8 IR4QA 
evaluation.  
Moreover, it is possible to have multiple 
explanations for a term. In order to discover as 
many question-related documents as possible, 
alternative translations found by VMNET are 
also used as additional query terms. They are 
shown in Table 6. For example, ?? is the 
Chinese term for DINK in Mainland China, 
but ??? is used in Taiwan. Furthermore, 
because VMNET gives the Wikipedia transla-
tion the highest priority if only one entry is 
found, a person?s full name is used in person 
name translation rather than the short com-
monly used name.  For example, Cheney (for-
mer vice president of U.S.) is translated into?
???? rather than just??.  
NE VMNET Wiki Title 
Princess Nori ???? ???? 
DINK ?? ??? 
BSE ??? ?????? 
Three Gorges Dam ???? ???? 
Table 6. Alternative translations 
The biggest difference, 3.07%, between 
runs that used different translation is from runs 
VMNET-EN-CS-03-T and VMNET-EN-CS-
04-T, which both pruned the question template 
phrase for simple query processing. Although 
the performance improvement is not obvious, 
the correct translations and the additional 
query terms found by VMNET are still very 
valuable. 
8 Conclusions 
General machine translation can already 
achieve very good translation results, but with 
our proposed approach we can further improve 
the translation accuracy. With a proper adjust-
49
ment of this approach, it could be used in a 
situation where there is a need for higher pre-
cision of complex phrase translation.  
The results from our CLIR experiments in-
dicate that VMNET is also capable of provid-
ing high quality query terms. A CLIR system 
can achieve good results for answer finding by 
using the VMNET for translation, simple in-
dexing technique (bigrams and unigrams), and 
plain question template phrase pruning.  
 
Run Name Indexing  
Units 
Query Processing 
VMNET-CS-CS-01-T U + B Manually segment the question and remove all the noise words  
VMNET-CS-CS-02-T U + B Prune the question template phrase 
VMNET-CS-CS-03-T U + B Use the whole question without doing any extra processing work 
VMNET-CS-CS-04-T U As VMNET-CS-CS-01-T 
VMNET-CS-CS-05-T B As VMNET-CS-CS-01-T 
VMNET-EN-CS-01-T U + B Use Google Translate on the whole question and use the entire translation 
as query 
VMNET-EN-CS-02-T U + B Use VMNET translation result without doing any further processing 
VMNET-EN-CS-03-T U + B As above, but prune the Chinese question template from translation 
VMNET-EN-CS-04-T U + B Use Google Translate  on  the whole question and prune the Chinese ques-
tion template phrase from the translation 
Table 7. The experimental runs. For indexing units, U means unigrams; B means bigrams. 
 
 
Topic ID Question with OOV Phrases  Correct  GT VMNET  
ACLIA2-CS-0002 What is the relationship between the movie 
"Riding Alone for Thousands of Miles" 
and ZHANG Yimou? 
????
? 
??????? ????? 
ACLIA2-CS-0008 Who is LI Yuchun? ??? ??? ??? 
ACLIA2-CS-0024 Why does Qingling build "panda corridor 
zone" 
?? ??? ??? 
ACLIA2-CS-0035 Please list the events related to the movie 
"Initials D". 
??? D ?? D??? ?? D??
? 
ACLIA2-CS-0036 Please list the movies in which Zhao Wei 
participated. 
?? ?? ?? 
ACLIA2-CS-0038 What is the relationship between Xia Yu 
and Yuan Quan. 
?? ??? ?? 
ACLIA2-CS-0048 Who is Sean Chen(Chen Shin-An)? ??? ???????? ??? 
ACLIA2-CS-0049 Who is Lung Yingtai? ??? ??? ??? 
ACLIA2-CS-0057 What is the disputes between China and 
Japan for the undersea natural gas field in 
the East China Sea? 
?? ????? ?? 
ACLIA2-CS-0066 What is the relationship between two Rus-
sian politicians, Kasianov and Putin? 
????
? 
Kasianov ???
?????
?????
???? 
ACLIA2-CS-0074 Where are Japan's Northern Territories 
located? 
???? ???? ??? 
ACLIA2-CS-0075 Which countries have borders in the Ka-
shimir region? 
???? Kashimir Kashimir 
ACLIA2-CS-0088 What is the relationship between the 
Golden Globe Awards and Broken-back 
Mountain? 
??? ????? ??? 
ACLIA2-CS-0089 What is the relationship between Kenneth 
Yen(K. T. Yen) and China? 
??? ????????
??? 
???? 
Table 8. The differences between Google Translate and VMNET translation of OOV 
phrases in which GT or VMNET was wrong. 
50
 References 
Chan, Y.-C., Chen, K.-H., & Lu, W.-H. (2007). 
Extracting and Ranking Question-Focused 
Terms Using the Titles of Wikipedia Articles. 
Paper presented at the NTCIR-6. 
Chen, A., He, J., Xu, L., Gey, F. C., & Meggs, J. 
(1997, 1997). Chinese text retrieval without 
using a dictionary. Paper presented at the SIGIR 
'97: Proceedings of the 20th annual international 
ACM SIGIR conference on Research and 
development in information retrieval. 
Chen, A., Jiang, H., & Gey, F. (2000). Combining 
multiple sources for short query translation in 
Chinese-English cross-language information 
retrieval. 17-23. 
Ferr?ndez, S., Toral, A., Ferr?ndez, ?., Ferr?ndez, 
A., & Mu?oz, R. (2007). Applying Wikipedia?s 
Multilingual Knowledge to Cross?Lingual 
Question Answering Natural Language 
Processing and Information Systems (pp. 352-
363). 
Lu, C., Xu, Y., & Geva, S. (2007). Translation 
disambiguation in web-based translation 
extraction for English?Chinese CLIR. 819-823. 
NTCIR Project. (2010). Tools. from 
http://research.nii.ac.jp/ntcir/tools/tools-en.html 
Robert, W. P. L., & Kwok, K. L. (2002). A 
comparison of Chinese document indexing 
strategies and retrieval models. ACM 
Transactions on Asian Language Information 
Processing (TALIP), 1(3), 225-268. 
Sakai, T., Shima, H., Kando, N., Song, R., Lin, C.-
J., Mitamura, T., et al (2010). Overview of 
NTCIR-8 ACLIA IR4QA. Paper presented at the 
Proceedings of NTCIR-8, to appear. 
Shi, L., Nie, J.-Y., & Cao, G. (2008). RALI 
Experiments in IR4QA at NTCIR-7. Paper 
presented at the NTCIR-7. 
Su, C.-Y., Lin, T.-C., & Wu, S.-H. (2007). Using 
Wikipedia to Translate OOV Terms on MLIR. 
Paper presented at the NTCIR-6. 
Tatsunori Mori, K. T. (2007). A method of Cross-
Lingual Question-Answering Based on Machine 
Translation and Noun Phrase Translation using 
Web documents. Paper presented at the NTCIR-
6. 
The Stanford Natural Language Processing Group. 
(2010). Stanford Log-linear Part-Of-Speech 
Tagger. from 
http://nlp.stanford.edu/software/tagger.shtml 
University of Pennsylvania. (2010). POS tags. from 
http://bioie.ldc.upenn.edu/wiki/index.php/POS_t
ags 
Zhang, Y., & Vines, P. (2004). Using the web for 
automated translation extraction in cross-
language information retrieval. Paper presented 
at the Proceedings of the 27th annual 
international ACM SIGIR conference on 
Research and development in information 
retrieval.  
Zhang, Y., Vines, P., & Zobel, J. (2005). Chinese 
OOV translation and post-translation query 
expansion in Chinese?English cross-lingual 
information retrieval. ACM Transactions on 
Asian Language Information Processing 
(TALIP), 4(2), 57-77. 
 
 
51
A Boundary-Oriented Chinese Segmentation Method Using N-
Gram Mutual Information 
Ling-Xiang Tang1, Shlomo Geva1, Andrew Trotman2, Yue Xu1 
 
1Faculty of Science and Technology 
Queensland University of Technology 
Brisbane, Australia 
{l4.tang,s.geva,yue.xu}@qut.edu.au 
2Department of Computer Science 
University of Otago 
Dunedin, New Zealand 
 andrew@cs.otago.ac.nz 
 
Abstract 
This paper describes our participation 
in the Chinese word segmentation task 
of CIPS-SIGHAN 2010. We imple-
mented an n-gram mutual information 
(NGMI) based segmentation algorithm 
with the mixed-up features from unsu-
pervised, supervised and dictionary-
based segmentation methods. This al-
gorithm is also combined with a simple 
strategy for out-of-vocabulary (OOV) 
word recognition. The evaluation for 
both open and closed training shows 
encouraging results of our system. The 
results for OOV word recognition in 
closed training evaluation were how-
ever found unsatisfactory. 
1 Introduction 
Chinese segmentation has been an interesting 
research topic for decades. Lots of delicate 
methods are used for providing good Chinese 
segmentation. In general, on the basis of the 
required human effort, Chinese word segmen-
tation approaches can be classified into two 
categories: supervised and unsupervised.  
Particularly, supervised segmentation meth-
ods can achieve a very high precision on the 
targeted knowledge domain with the help of 
training corpus?the manually segmented text 
collection. On the other hand, unsupervised 
methods are suitable for more general Chinese 
segmentation where there is no or limited 
training data available. The resulting segmen-
tation accuracy with unsupervised methods 
may not be very satisfying, but the human ef-
fort for creating the training data set is not ab-
solutely required. 
In the Chinese word segmentation task of 
CIPS-SIGHAN 2010, the focus is on the per-
formance of Chinese segmentation on cross-
domain text. There are in total two types of 
evaluations: closed and open. We participated 
in both closed and open training evaluation 
tasks and both simplified and traditional 
Chinse segmentation subtasks. For the closed 
training evaluation, the provided resource for 
system training is limited and using external 
resources such as trained segmentation soft-
ware, corpus, dictionaries, lexicons, etc are 
forbidden; especially, human-encoded rules 
specified in the segmentation algorithm are not 
allowed.  
For the bakeoff of this year, we imple-
mented a boundary-oriented NGMI-based al-
gorithm with the mixed-up features from su-
pervised, unsupervised and dictionary-based 
methods for the segmentation of cross-domain 
text. In order to detect words not in the training 
corpus, we also used a simple strategy for out-
of-vocabulary word recognition.  
2 A Boundary-Oriented Segmentation 
Method 
2.1 N-Gram Mutual Information 
It is a challenge to segment text that is out-of-
domain for supervised methods which are 
good at the segmentation for the text that has 
been seen segmented before. On the other hand, 
unsupervised segmentation methods could help 
to discover words even if they are not in vo-
cabularies. To conquer the goal of segmenting 
text that is out-of-domain and to take advan-
tage of the training corpus, we use n-gram mu-
tual information (NGMI)(Tang et al, 2009) ?
an unsupervised boundary-oriented segmenta-
tion method and make it trainable for cross-
domain text segmentation.  
As an unsupervised segmentation approach, 
NGMI is derived from the character-based mu-
tual information(Sproat & Shih, 1990), but 
unlike its ancestor it can additionally recognise 
words longer than two characters. Generally, 
mutual information is used to measure the as-
sociation strength of two adjoining entities 
(characters or words) in a given corpus. The 
stronger association, the more likely it is that 
they should be together. The association score 
MI for the adjacent two entities (x and y) is 
calculated as: 
           
        
 
       
 
       
 
     
     
        
 
(1) 
where freq(x) is the frequency of entity x oc-
curring in the given corpus; freq(xy) is the fre-
quency of entity xy (x followed by y) occurring 
in the corpus; N is the size of entities in the 
given corpus; p(x) is an estimate of the prob-
ability of entity x occurring in corpus, calcu-
lated as freq(x)/N. 
NGMI separates words by choosing the 
most probable boundaries in the unsegmented 
text with the help of a frequency table of n-
gram string patterns. Such a frequency table 
can be built from any selected text.  
The main concept of NGMI is to find the 
boundaries between words by combining con-
textual information rather than looking for just 
words. Any place between two Chinese char-
acters could be a possible boundary. To find 
the most rightful ones, boundary confidence 
(BC) is introduced to measure the confidence 
of having words separated correctly. In other 
words, BC measures the association level of 
the left and right characters around each possi-
ble boundary to decide whether the boundary 
should be actually placed. 
For any input string, suppose that we have: 
                           (2) 
The boundary confidence of a possible 
boundary ( | )  is defined as: 
                      (3) 
where L and R are the adjoining left and right 
segments with up to two characters from each 
side of the boundary ( | ) and            , 
        ; and NGMImin is calculated as: 
                             
                
                
                     
(4) 
Basically, NGMImin considers mutual informa-
tion of k (k=2, or k= 4) pairs of segments 
around the boundary; the one with the lowest 
value is used as the score of boundary confi-
dence. Those segment pairs used in NGMImin 
calculation are named adjoining segment pairs 
(ASPs). Each ASP consists of a pair of adjoin-
ing segments. 
For the boundary confidence of the bounda-
ries at the beginning or end of an input string, 
we can retrieve only one character from one 
side of the boundary. So for these two kinds of 
boundaries differently we have: 
                                     
                              
(5) 
                    
                                    
                                             (6) 
For any possible boundary the lower confi-
dence score it has, the more likely it is an ac-
tual boundary. A threshold then can be set to 
decide whether a boundary should be placed. 
So even without a lexicon, it is still probable to 
segment text with a certain precision which 
just simply means the suggested words are all 
out-of-vocabulary. Hence, NGMI can be sub-
sequently used for OOV word recognition. 
2.2 Supervised NGMI 
The idea of making NGMI trainable is to turn 
the segmented text into a word based fre-
quency table. It is a table that records only 
words, adjoining word pairs and their frequen-
cies. For example, given a piece of training 
text ? ?A B C E B C A B? (where A, B, C and 
E are n-gram Chinese words), its frequency 
table should look like the following: 
A|B 2 
B|C 2 
C|E 1 
C|A 1 
A 2 
B 3 
C 2 
E 1 
Also, when doing the boundary confidence 
computation, any substrings (children) of the 
words (parents) in this table are set to have the 
same frequency as their parents?.  
3 Segmentation System Design 
3.1 Frequency Table and Its Alignment 
In order to resolve ambiguity and also recog-
nise OOV terms, statistical information of n-
gram string patterns in test files should be col-
lected. There are in total two groups of fre-
quency information used in the segmentation. 
One is from the training data, recording the 
frequency information of the actual words and 
the adjoining word pairs; the other is from the 
unsegmented text, containing frequency infor-
mation of all possible n-gram patterns.  
However, the statistical data collected from 
the unsegmented test file contains many noise 
patterns. It is necessary to remove those noise 
patterns from the table to avoid negative im-
pact on the final BC computation. Therefore, 
an alignment of the pattern frequencies ob-
tained from the test file is performed to reduce 
noise.  
The frequency alignment is conducted in a 
few steps. First, build a frequency table of all 
string patterns for the unsegmented text includ-
ing those having a frequency of one. Second, 
the frequency table is sorted by the frequency 
and the length of the patterns. Longer patterns 
have a higher ranking than the shorter ones; for 
patterns of same length the ones having higher 
frequency are ranked higher than those having 
lower. Next, starting from the beginning of the 
table where the longest and the most frequent 
pattern have the highest ranking, retrieve one 
record each time and remove from the table all 
its sub-patterns which have the same frequency 
as its parent?s.  
After such a frequency alignment is done, 
two frequency tables are merged into one and 
ready for the final boundary confidence calcu-
lation. 
3.2 Segmentation  
In the training and the system testing stages, 
the segmentation results using boundary confi-
dence alone for word disambiguation were 
found unsatisfactory. Trying to achieve as high 
performance as possible, the overall word 
segmentation for the bakeoff is done by using 
a hybrid algorithm which is a combination of 
NGMI for general word segmentation, and the 
backward maximum match (BMM) method for 
the final word disambiguation.  
Since it is common for a Chinese document 
containing various types of characters: Chi-
nese, digit, alphabet and characters from other 
languages, segmentation needs to be consid-
ered for two particular forms of Chinese 
words: 1) words containing non-Chinese char-
acters such as numbers or letters; and 2) words 
containing purely Chinese characters.  
In order to simplify the process of overall 
segmentation, boundaries are automatically 
added to the places in which Chinese charac-
ters precede non-Chinese characters. Addition-
ally, for words containing numbers or letters, 
we only search those begin with numbers or 
letters and end with Chinese character(s) 
against the given lexicons. If the search fails, 
the part with all non-Chinese characters re-
mains the same and a boundary is added be-
tween the non-Chinese character and the Chi-
nese character.   
For example, to segment a sentence 
?????????????????, it 
consists of there following main steps: 
? First, because of ?? |??, only ????
???? requires initial segmentation. 
? Next, find a matched word ??????? 
in a given lexicon. 
? Last, segment ??????. 
So the critical part of the segmentation algo-
rithm is to segment strings with purely Chinese 
characters.  
By already knowing the actual word infor-
mation (i.e. a vocabulary from the labelled 
training data), it can be set in our algorithm 
that when computing BCs each possible 
boundary is assigned with a score falling in 
one of the following four BC categories: 
? INSEPARATABLE 
? THRESHOLD 
? normal boundary confidence score 
? ABSOLUTE-BOUNDARY 
INSEPARATABLE means the characters 
around the possible boundary are a part of an 
actual word; ABSOLUTE-BOUNDARY 
means the adjoining segments pairs are not 
seen in any words or string patterns. 
THRESHOLD is a threshold value that is 
given to a possible boundary for which only 
one of ASPs can be found in the word pair ta-
ble, and its length is greater than two. 
After finishing all BC computations for an 
input string, it then can be broken down into 
segments separated by the boundaries having a 
BC score that is lower than or equals to the 
threshold value. For each segment, it can be 
checked if it is a word in the vocabulary or if it 
is an OOV term using an OOV judgement 
formula that will be discussed in Section 3.3. If 
a segment is not a word or an OOV term, it 
means there is an ambiguity in that segment. 
For example, given a sentence ???????, 
the substring ????? inside the sentence can 
be either segmented into ???  | ?? or ?? | 
???. 
To disambiguate it, a segment is divided 
into two chunks at the place having the lowest 
BC score. If one of the chunks is a word or 
OOV term, this two-chunk breaking-down op-
eration continues on the remaining non-word 
chunk until both divided chunks are words, or 
none of them is a word or an OOV term. After 
this recursive operation is finished, if there are 
still non-word chunks left they will be further 
segmented using the BMM method. 
The overall segmentation algorithm for an 
all-Chinese string can be summarised as fol-
lows: 
1) Compute BC for each possible boundary. 
2) Input string becomes segments that are 
separated by the boundaries having a 
low BC score (not higher than the 
threshold). 
3) For each remaining non-word segment 
resulting from step 2, it gets recursively 
broken down into two chunks at the 
place having the lowest BC among this 
segment based on the scores from step 1. 
This breaking-down-into-two-chunk 
loop continues on the non-word chunk if 
the other is a word or an OOV term; 
otherwise, all the remaining non-word 
chucks are further segmented using the 
backward maximum match method. 
3.3 OOV Word Recognition 
We use a simple strategy for OOV word detec-
tion. It is assumed that an n-gram string pattern 
can be qualified as an OOV word if it repeats 
frequently within only a short span of text or a 
few documents. So to recognise OOV words, 
the statistical data extracted from the unseg-
mented text needs to contain not only pattern 
frequency information but also document fre-
quency information. However, the documents 
in the test data are boundary-less. To obtain 
document frequencies for string patterns, we 
separate test files into a set of virtual docu-
ments by splitting them according size. The 
size of the virtual document (VDS) is adjust-
able.  
For a given non-word string pattern S, we 
then can compute its probability of being an 
OOV term by using: 
         
  
  
   (7) 
where tf is the term frequency of the string pat-
tern S; df is the virtual document frequency of 
the string pattern. Then S is considered an 
OOV candidate, if it satisfies: 
                    (8) 
where OOV_THRES is an adjustable threshold 
value used to filter out the patterns with lower 
probability of being OOV words. However, 
using this strategy could have side effects on 
the segmentation performance because not all 
the suggested OOV words could be correct. 
4 Experiments 
4.1 Experimental Environment 
OS GNU/Linux 2.6.32.11-99.fc12.x86_64 
CPU 
Intel(R) Core(TM)2 Duo CPU     
E6550  @ 2.33GHz 
MEM 8G memory  
BUILD DEBUG build without optimisation 
Table 1. Software and Hardware Environ-
ment. 
The information of operating system and 
hardware used in the experiments is given in 
Table 1. 
4.2 Parameters Settings 
Parameter Value 
N # of words in training corpus  
THRESHOLD log(1/N) 
VDS 10,000bytes 
OOV_THRES 2.3 
Table 2. System settings used in both closed 
and open training evaluation. 
Table 2 shows the parameters used in the sys-
tem for segmentation and OOV recognition. 
4.3 Closed and Open Training 
For both closed and open training evaluations, 
the algorithm and parameters used for segmen-
tation and OOV detection are exactly the same. 
This is true except for an extra dictionary - cc-
cedict(MDBG) being used in the open training 
evaluation. 
4.4 Segmentation Efficiency 
SUBTASK DOMAIN TIME 
simplified 
(closed) 
A 2m19.841s 
B 2m1.405s 
C 1m57.819s 
D 1m54.375s 
simplified 
(open) 
A 3m52.726s 
B 3m20.907s 
C 3m10.398s 
D 3m22.866s 
traditional 
(closed) 
A 2m33.448s 
B 2m56.056s 
C 3m7.103s 
D 3m14.286s 
traditional A 3m14.595s 
(open) B 3m41.634s 
C 3m53.839s 
D 4m10.099s 
Table 3. The execution time of segmenta-
tions for four different domains in both 
simplified and traditional Chinese subtasks. 
Table 3 shows the execution time of all tasks 
for generating the segmentation outputs. The 
execution time listed in the table includes the 
time for loading the training frequency table, 
building the frequency table from the test file, 
and producing the actual segmentation results. 
5 Evaluation 
5.1 Segmentation Results 
Simplified Chinese 
Task R P F1 ROOV RROOV RRIV 
A (c) 0.907 0.862 0.884 0.069 0.206 0.959 
A (o) 0.869 0.873 0.871 0.069 0.657 0.885 
B (c) 0.876 0.844 0.86 0.152 0.457 0.951 
B (o) 0.859 0.878 0.868 0.152 0.668 0.893 
C (c) 0.885 0.804 0.842 0.110 0.218 0.967 
C (o) 0.865 0.846 0.855 0.110 0.559 0.903 
D (c) 0.904 0.865 0.884 0.087 0.321 0.960 
D (o) 0.853 0.850 0.851 0.087 0.438 0.893 
Traditional Chinese 
Task R P F1 ROOV RROOV RRIV 
A (c) 0.864 0.789 0.825 0.094 0.105 0.943 
A (o) 0.804 0.722 0.761 0.094 0.234 0.863 
B (c) 0.868 0.85 0.859 0.094 0.316 0.926 
B (o) 0.789 0.736 0.761 0.094 0.35 0.834 
C (c) 0.871 0.815 0.842 0.075 0.115 0.932 
C (o) 0.811 0.74 0.774 0.075 0.254 0.856 
D (c) 0.875 0.834 0.854 0.068 0.169 0.926 
D (o) 0.811 0.753 0.781 0.068 0.235 0.853 
Table 4. The segmentation results for four 
domains in both closed and open training 
evaluations. (c) ? closed; (o) ? open;  A - Lit-
erature; B ? Computer; C ? Medicine; D ? 
Finance. ROOV is the OOV rate in the test 
file.  
In the Chinese word segmentation task of 
CIPS-SIGHAN 2010, the system performance 
is measured by five metrics: recall (R), preci-
sion (P), F-measure (F1), recall rate of OOV 
words (RROOV), and recall rate of words in vo-
cabulary (RRIV). 
The official results of our system for both 
open and closed training evaluation are given 
in Table 4. The recall rates, precision values, 
and F1-scores of all tasks show promising re-
sults of our system in the segmentation for 
cross-domain text. However, the gaps between 
our scores and the bakeoff bests also suggest 
that there is still plenty of room for perform-
ance improvements in our system. 
The OOV recall rates (RRoov) showed in 
Table 4 demonstrate that the OOV recognition 
strategy used in our system can achieve a cer-
tain level of OOV word discovery in closed 
training evaluation. The overall result for the 
OOV word recognition is not very satisfactory 
if comparing it with the best result from other 
bakeoff participants. But for the open training 
evaluation the OOV recall rate picked up sig-
nificantly, which indicates that the extra dic-
tionary - cc-cedict covers a fair amount of 
terms for various domains. 
5.2 Possible Further Improvements 
Due to finishing the implementation of our 
segmentation system in a short time, we be-
lieve that there might be many program bugs 
which had negative effects on our system and 
leaded to producing results not as expected.  In 
an analysis of the segmentation outputs, words 
starting with numbers were found incorrectly 
segmented because of the different encodings 
used in the training and test files for digits. 
Moreover, the disambiguation in breaking 
down a non-word segment which contains at 
least an n-gram word could lead to an all-
single-character-word segmentation. This 
should certainly be avoided. 
Also, the current OOV word recognition 
strategy may detect a few good OOV words, 
but also introduces incorrect segmentation 
consistently through the whole input text if 
OOV words are mistakenly identified. If this 
OOV word recognition used in our system can 
be further improved, it can help to alleviate the 
problem of performance deterioration. 
For the open training, if language rules can 
be encoded in both word segmentation and 
OOV word recognition, it certainly is another 
beneficial method to improve the overall preci-
sion and recall rate. 
6 Conclusions 
In this paper, we describe a novel hybrid 
boundary-oriented NGMI-based segmentation 
method, which combines a simple strategy for 
OOV word recognition. The evaluation results 
show reasonable performance of our system in 
cross-domain text segmentation even with the 
negative effects from system bugs and the 
OOV word recognition strategy. It is believed 
that the segmentation system can be improved 
by fixing the existing program bugs, and hav-
ing a better OOV word recognition strategy. 
Performance can also be further improved by 
incorporating language or domain specific 
knowledge into the system. 
References 
MDBG. CC-CEDICT download. from 
http://www.mdbg.net/chindict/chindict.php?pag
e=cc-cedict 
Sproat, Richard, and Chilin Shih. 1990. A statistical 
method for finding word boundaries in Chinese 
text. Computer Processing of Chinese &amp; 
Oriental Languages, 4(4): 336-351. 
Tang, Ling-Xiang, Shlomo Geva, Yue Xu, and 
Andrew Trotman. 2009. Word Segmentation for 
Chinese Wikipedia Using N-Gram Mutual 
Information. Paper presented at the 14th 
Australasian Document Computing Symposium 
(ADCS 2009).  
 
 
 
