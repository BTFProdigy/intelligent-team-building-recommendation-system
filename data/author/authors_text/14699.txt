Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 106?113,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
 
 
Exploiting Multi-Features to Detect Hedges and Their Scope in 
Biomedical Texts
 
Huiwei Zhou1, Xiaoyan Li2, Degen Huang3, Zezhong Li4, Yuansheng Yang5 
Dalian University of Technology 
Dalian, Liaoning, China 
{1zhouhuiwei, 3huangdg, 5yangys}@dlut.edu.cn 
2lixiaoyan@mail.dlut.edu.cn 
4lizezhonglaile@163.com 
Abstract 
In this paper, we present a machine learning 
approach that detects hedge cues and their 
scope in biomedical texts. Identifying hedged 
information in texts is a kind of semantic 
filtering of texts and it is important since it 
could extract speculative information from 
factual information. In order to deal with the 
semantic analysis problem, various evidential 
features are proposed and integrated through a 
Conditional Random Fields (CRFs) model. 
Hedge cues that appear in the training dataset 
are regarded as keywords and employed as an 
important feature in hedge cue identification 
system. For the scope finding, we construct a 
CRF-based system and a syntactic 
pattern-based system, and compare their 
performances. Experiments using test data 
from CoNLL-2010 shared task show that our 
proposed method is robust. F-score of the 
biological hedge detection task and scope 
finding task achieves 86.32% and 54.18% in 
in-domain evaluations respectively. 
1. Introduction 
Identifying sentences in natural language texts 
which contain unreliable or uncertain information 
is an increasingly important task of information 
extraction since the extracted information that 
falls in the scope of hedge cues cannot be 
presented as factual information. Szarvas et al 
(2008) report that 17.69% of the sentences in the 
abstracts section of the BioScope corpus and 
22.29% of the sentences in the full papers section 
contain hedge cues. Light et al (2004) estimate 
that 11% of sentences in MEDLINE abstracts 
contain speculative fragments. Szarvas (2008) 
reports that 32.41% of gene names mentioned in 
the hedge classification dataset described in 
Medlock and Briscoe (2007) appear in a 
speculative sentence. Many Wikipedia articles 
contain a specific weasel tag which mark 
sentences as non-factual (Ganter and Strube, 
2009). 
There are some Natural Language Processing 
(NLP) researches that demonstrate the benefit of 
hedge detection experimentally in several 
subjects, such as the ICD-9-CM coding of 
radiology reports and gene named Entity 
Extraction (Szarvas, 2008), question answering 
systems (Riloff et al, 2003), information 
extraction from biomedical texts (Medlock and 
Briscoe, 2007). 
The CoNLL-2010 Shared Task (Farkas et al, 
2010) ?Learning to detect hedges and their scope 
in natural language text? proposed two tasks 
related to speculation research. Task 1 aimed to 
identify sentences containing uncertainty and 
Task 2 aimed to resolve the in-sentence scope of 
hedge cues. We participated in both tasks. 
In this paper, a machine learning system is 
constructed to detect sentences in texts which 
contain uncertain or unreliable information and to 
find the scope of hedge cues. The system works 
in two phases: in the first phase uncertain 
sentences are detected, and in the second phase 
in-sentence scopes of hedge cues are found. In the 
uncertain information detecting phase, hedge 
cues play an important role. The sentences that 
contain at least one hedge cue are considered as 
uncertain, while sentences without cues are 
considered as factual. Therefore, the task of 
uncertain information detection can be converted 
into the task of hedge cue identification. Hedge 
cues that appear in the training dataset are 
collected and used as keywords to find hedges. 
Furthermore, the detected keywords are 
employed as an important feature in hedge cue 
identification system. In addition to keywords, 
various evidential features are proposed and 
integrated through a machine learning model. 
Finding the scope of a hedge cue is to determine 
at sentence level which words are affected by the 
106
  
hedge cue. In the scope finding phase, we 
construct a machine learning-based system and a 
syntactic pattern-based system, and compare their 
performances. 
For the learning algorithm, Conditional random 
fields (CRFs) is adopted relying on its flexible 
feature designs and good performance in 
sequence labeling problems as described in 
Lafferty et al (2001). The main idea of CRFs is 
to estimate a conditional probability distribution 
over label sequences, rather than over local 
directed label sequences as with Hidden Markov 
Models (Baum and Petrie, 1966) and Maximum 
Entropy Markov Models (McCallum et al, 
2000). 
Evaluation is carried out on the CoNLL-2010 
shared task (Farkas et al, 2010) dataset in which 
sentences containing uncertain information are 
annotated. For the task of detecting uncertain 
information, uncertain cues are annotated. And 
for the task of finding scopes of hedge cues, 
hedge cues and their scope are annotated as 
shown in sentence (a): hedge cue indicate that, 
and its scope indicate that dhtt is widely 
expressed at low levels during all stages of 
Drosophila development are annotated. 
 
(a)Together, these data <xcope 
id="X8.74.1"><cue ref="X8.74.1" 
type="speculation">indicate that</cue> dhtt 
is widely expressed at low levels during all 
stages of Drosophila development</xcope>. 
2. Related Work 
In the past few years, a number of studies on 
hedge detection from NLP perspective have been 
proposed. Elkin et al (2005) exploited 
handcrafted rule-based negation/uncertainty 
detection modules to detect the negation or 
uncertainty information. However, their detection 
modules were hard to develop due to the lack of 
standard corpora that used for evaluating the 
automatic detection and scope resolution. Szarvas 
et al (2008) constructed a corpus annotated for 
negations, speculations and their linguistic scopes. 
It provides a common resource for the training, 
testing and comparison of biomedical NLP 
systems. 
Medlock and Briscoe (2007) proposed an 
automatic classification of hedging in biomedical 
texts using weakly supervised machine learning. 
They started with a very limited amount of 
annotator-labeled seed data. Then they iterated 
and acquired more training seeds without much 
manual intervention. The best classifier using 
their model achieved 0.76 precision/recall 
break-even-point (BEP). Further, Medlock 
(2008) illuminated the hedge identification task 
including annotation guidelines, theoretical 
analysis and discussion. He argued for separation 
of the acquisition and classification phases in 
semi-supervised machine learning method and 
presented a probabilistic acquisition model. In 
probabilistic model he assumed bigrams and 
single terms as features based on the intuition that 
many hedge cues are bigrams and single terms 
and achieves a peak performance of around 0.82 
BEP.  
Morante and Daelemans (2009) presented a 
meta-learning system that finds the scope of 
hedge cues in biomedical texts. The system 
worked in two phases: in the first phase hedge 
cues are identified, and in the second phase the 
full scopes of these hedge cues are found. The 
performance of the system is tested on three 
subcorpora of the BioScope corpus. In the hedge 
finding phase, the system achieves an F-score of 
84.77% in the abstracts subcorpus. In the scope 
finding phase, the system with predicted hedge 
cues achieves an F-score of 78.54% in the 
abstracts subcorpus. 
The research on detecting uncertain 
information is not restricted to analyze 
biomedical documents. Ganter and Strube (2009) 
investigated Wikipedia as a source of training 
data for the automatic hedge detection using word 
frequency measures and syntactic patterns. They 
showed that the syntactic patterns worked better 
when using the manually annotated test data, 
word frequency and distance to the weasel tag 
was sufficient when using Wikipedia weasel tags 
themselves. 
3. Identifying Hedge Cues 
Previous studies (Light et al, 2004) showed that 
the detection of hedging could be solved 
effectively by looking for specific keywords 
which were useful for deciding whether a 
sentence was speculative. Szarvas (2008) reduces 
the number of keyword candidates without 
excluding helpful keywords for hedge 
classification. Here we also use a simple 
keyword-based hedge cue detection method. 
3.1 Keyword-based Hedge Cue Detection 
In order to recall as many hedge cues as possible, 
107
  
all hedge cues that appear in the training dataset 
are used as keywords. Hedge cues are represented 
by one or more tokens. The list of all hedge cues 
in the training dataset is comprised of 143 cues. 
90 hedge cues are unigrams, 24 hedge cues are 
bigrams, and the others are trigrams, four-grams 
and five-grams. Besides, hedge cues that appear 
in the training dataset and their synonyms in 
WordNet 1  are also selected as keywords for 
hedge cue detection. The complete list of them 
contains 438 keywords, 359 of which are 
unigrams. Many tokens appear in different grams 
cues, such as possibility appears in five-grams 
cue cannot rule out the possibility, four-gram cue 
cannot exclude the possibility, trigrams cue raise 
the possibility and unigram cue possibility. To 
find the complete cues, keywords are matched 
through a maximum matching method (MM) (Liu 
et al, 1994). For example, though indicate and 
indicate that are both in keywords list, indicate 
that is extracted as a keyword in sentence (a) 
through MM. 
3.2 CRF-based Hedge Cue Detection 
Candidate cues are extracted based on keywords 
list in keyword-based hedge cue detection stage. 
But the hedge cue is extremely ambiguous, so 
CRFs are applied to correct the false 
identification results that occurred in the 
keyword-based hedge cue detection stage. The 
extracted hedge cues are used as one feature for 
CRFs-based hedge cue detection. 
A CRF identifying model is generated by 
applying a CRF tool to hedge cue labeled 
sequences. Firstly, hedge cue labeled sentences 
are transformed into a set of tokenized word 
sequences with IOB2 labels: 
 
B-cue Current token is the beginning of a 
hedge cue 
I-cue Current token is inside of  a hedge cue 
O Current token is outside of any hedge 
cue  
 
For sentence (a) the system assigns the B-cue 
tag to indicate, the I-cue tag to that and the O tag 
to the rest of tokens as shown in Figure1. 
The hedge cues that are found by 
keyword-based method is also given IOB2 labels 
feature as shown in Figure1. 
                                                          
1
 Available at http://wordnet.princeton.edu/ 
 
 
 
 
 
 
 
 
 
 
Text 
? 
these 
data 
indicate 
that 
dhtt 
is 
... 
Keyword Labels Feature 
...       
O   
O 
B 
I 
O 
O 
...                            
Cue Labels  
...       
O   
O 
B-cue 
I-cue 
O 
O 
...                          
 
Figure 1: Example of Cues labels and Keywords 
labels Feature 
 
Diverse features including keyword feature are 
employed to our CRF-based hedge cue detection 
system. 
 
(1) Word Features 
? Word (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
Where Word (0) is the current word, Word (-1) 
is the first word to the left, Word (1) is the first 
word to the right, etc. 
 
(2) Stem Features 
The motivation for stemming in hedge 
identification is that distinct morphological forms 
of hedge cues are used to convey the same 
semantics (Medlock, 2008). In our method, 
GENIA Tagger2 (Tsuruoka et al, 2005) is applied 
to get stem features. 
? Stem (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
where Stem (0) is the stem for the current word, 
Stem(-1) is the first stem to the left, Stem (1) is the 
first stem to the right, etc. 
 
(3) Part-Of-Speech Features  
Since most of hedge cues in the training dataset 
are verbs, auxiliaries, adjectives and adverbs. 
Therefore, Part-of-Speech (POS) may provide 
useful evidence about the hedge cues and their 
boundaries. GENIA Tagger is also used to 
generate this feature.  
? POS (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
where POS (0) is the current POS, POS (-1) is 
the first POS to the left, POS (1) is the first POS 
to the right, etc. 
 
(4) Chunk Features 
Some hedge cues are chunks consisting of more 
than one token. Chunk features may contribute to 
the hedge cue boundaries. We use GENIA 
Tagger to get chunk features for each token. The 
                                                          
2
 Available at 
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/ 
108
  
chunk features include unigram, bigram, and 
trigram types, listed as follows: 
? Chunk (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
? Chunk (i?1)+Chunk(i) (i =?1,0,+1,+2) 
? Chunk (i?2) + Chunk (i?1)+Chunk (i) (i= 
0,+1,+2) 
where Chunk (0) is the chunk label for the 
current word, Chunk (?1) is the chunk label for 
the first word to the left , Chunk (1) is the chunk 
label for the first word to the right, etc. 
 
(5) Keyword Features 
Keyword labels feature is an important feature. 
? Keyword (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, 
+n) 
where Keyword (0) is the current keyword label, 
Keywords (-1) is the keyword label for the first 
keyword to the left, Keywords (1) is the keyword 
label for the first keyword to the right, etc. 
Feature sets can be easily redefined by 
changing the window size n. The relationship of 
the window size and the F-score observed in our 
experiments will be reported in Section 5. 
4. Hedge Scope Finding 
In this task, a CRFs classifier is applied to predict 
for all the tokens in the sentence whether a token 
is the first token of the scope sequence (F-scope), 
the last token of the scope sequence (L-scope), or 
neither (None). For sentence (a) in Section 1, the 
classifier assigns F-scope to indicate, L-scope to 
benchmarks, and None to the rest of the tokens. 
Only sentences that assigned cues in the first 
phase are selected for hedge scope finding. 
Besides, a syntactic pattern-based system is 
constructed, and compared with the CRF-based 
system. 
4.1 CRF-based System 
The features that used in CRF-based hedge cue 
detection systems are also used for scope finding 
except for the keyword features. The features are: 
 
(1) Word Features 
? Word (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
 
(2) Stem Features 
? Stem (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
(3) Part-Of-Speech Features  
? POS (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
 
(4) Chunk Features 
The chunk features include unigram, bigram, 
and trigram types, listed as follows: 
 
? Chunk (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, +n) 
? Chunk (i?1)+Chunk(i) (i =?1,0,+1,+2) 
? Chunk (i?2) + Chunk (i?1)+Chunk (i) (i= 
0,+1,+2) 
 
(5) Hedge cues Features 
Hedge cues labels that are doped out in Task 1 
are selected as an important feature. 
 
? Hedge cues (i) (i=-n, ?, ?2, ?1, 0, +1, +2, ?, 
+n) 
where Hedge cues (0) is the cue label for the 
current word, Hedge cues (?1) is the cue label for 
the first word to the left , Hedge cues (1) is the 
cue label for the first word to the right, etc. 
The scope of the sequence must be consistent 
with the hedge cues. That means that the number 
of the F-scope and L-scope must be the same with 
the hedge cues. However, sometimes their 
number predicted by classifier is not same. 
Therefore, we need to process the output of the 
classifier to get the complete sequence of the 
scope. The following post processing rules are 
adapted. 
 
? If the number of F-scope, L-scope and hedge 
cue is the same, the sequence will start at the 
token predicted as F-scope, and end at the 
token predicted as L-scope. 
? If one token has been predicted as F-scope 
and none has been predicted as L-scope, the 
sequence will start at the token predicted as 
F-scope and end at the end of the sentence. 
Since when marking the scopes of keywords, 
linguists always extend the scope to the biggest 
syntactic unit possible. 
? If one token has been predicted as L-scope 
and none has been predicted as F-scope, the 
sequence will start at the hedge cue and end at 
the token predicted as L-scope. Since scopes 
must contain their cues. 
? If one token has been predicted as F-scope 
and more than one has been predicted as 
L-scope, the sequence will end at the first token 
predicted as L-scope. Statistics from prediction 
on CoNLL-2010 Shared Task evaluation data 
show that 20 sentences are in this case. And the 
scope of 6 sentences extends to the first 
L-scope, and the scope of 3 sentences end at 
the last L-scope, the others are predicted 
mistakenly. Our system prediction and 
gold-standard annotation are shown in sentence 
(b1) and (b2) respectively. 
109
  
 
(b1) our system annotation: 
dRas85DV12 <xcope id="X3.64.1"><cue 
ref="X3.64.1" type="speculation">may</cue> 
be more potent than dEGFR?</xcope> because 
dRas85DV12 can activate endogenous PI3K 
signaling</xcope> [16]. 
 
(b2) gold-standard annotation: 
dRas85DV12 <xcope id="X3.64.1"><cue 
ref="X3.64.1" 
type="speculation">may</cue> be more 
potent than dEGFR?</xcope> because 
dRas85DV12 can activate endogenous PI3K 
signaling [16]. 
 
? If one token has been predicted as L-scope 
and more than one has been predicted as 
F-scope, the sequence will start at the first 
token predicted as F-scope. 
? If an L-scope is predicted before an F-scope, 
the sequence will start at the token predicted as 
F-scope, and finished at the end of the sentence.  
4.2 Syntactic Pattern-based System 
Hedge scopes usually can be determined on the 
basis of syntactic patterns dependent on the cue. 
Therefore, a syntactic pattern-based system is 
also implemented for hedge scope finding. When 
the sentence is predicted as uncertain, the toolkit 
of Stanford Parser3 (Klein and Manning, 2003) is 
utilized to parse the sentence into a syntactic tree, 
which can release a lot of information about the 
grammatical structure of sentences that is 
beneficial for the finding of hedge scope. For 
sentence (c) the Stanford Parser gives the 
syntactic tree as showed in Figure 2. 
 
(c) This <xcope id="X*.*.*"><cue ref="X*.*.*" 
type="speculation"> may </cue> represent a 
viral illness</xcope>. 
It is obvious to see from the syntactic tree, all 
the words of the parsed sentence concentrate at 
the places of leaves. We use the following rules to 
find the scope. 
? If the tag of the word is ?B-cue?, it is predicted 
as F-scope. 
? If the POS of the hedge cue is verbs and 
auxiliaries, the L-scope is signed at the end of the 
clause. 
? If the POS of the hedge cue is attributive 
                                                          
3
 Available at 
http://nlp.stanford.edu/software/lex-parser.shtml  
adjectives, the L-scope is signed at the following 
noun phrase.  
? If the POS of the hedge cue is prepositions, the 
L-scope is signed at the following noun phrase. 
? If none of the above rules apply, the scope of a 
hedge cue starts with the hedge cue and ends at 
the following clause. 
 
 
 
Figure 2: Syntactic tree parsed by Stanford 
Parser 
5. Experiments and Discussion 
We evaluate our method using CoNLL-2010 
shared task dataset. The evaluation of uncertain 
information detection task is carried out using the 
sentence-level F-score of the uncertainty class. 
As mentioned in Section 1, Task 1 is converted 
into the task of hedge cues identification. 
Sentences can be classified as certain or uncertain 
according to the presence or absence of a few 
hedge cues within the sentences. In task of 
finding in-sentence scopes of hedge cues, a scope 
is correct if all the tokens in the sentence have 
been assigned the correct scope class for a 
specific hedge signal. 
5.1 Detecting Uncertain Information 
In the CoNLL-2010 Shared Task 1, our 
in-domain system obtained the F-score of 85.77%. 
Sentence-level results of in-domain systems 
under the condition n=3 (window size) are 
summarized in Table 1.  
 
System Prec. Recall F-score 
Keyword-based 41.15 99.24 58.18 
CRF-based system 
(without keyword 
features) 
88.66 80.13 84.18 
CRF-based system 
+ keyword features 
86.21 84.68 85.44 
CRF-based system 86.49 85.06 85.77 
110
  
+ keyword features 
+ MM 
 
Table 1: Official in-domain results for Task 1 
(n=3) 
 
The keyword-based system extracts hedge cues 
through maximum matching method (MM). As 
can be seen in Table 1, the system achieves a high 
recall (99.24%). This can be explained that 
almost all of the hedge cues in the test dataset are 
in the keywords list. However, it also brings 
about the low precision since not all potential 
speculative keywords convey real speculation. So 
the keyword-based method can be combined with 
our CRF-based method to get better performance. 
All the CRF-based systems in Table 1 
significantly outperform the keyword-based 
system, since the multi-features achieve a high 
precision. And the result with keyword features is 
better than the result without it. The keyword 
features improve the performance by recalling 39 
true positives. In addition, further improvement is 
achieved by using Maximum Matching method 
(MM). 
In the test dataset, there should be a few hedge 
cues not in the training dataset. And the 
additional resources besides the manually labeled 
data are allowed for in-domain predictions. 
Therefore, the synonyms of the keywords can be 
used for in-domain systems. The synonyms of the 
keywords are added to the keywords list, and are 
expected to improve detecting performance. The 
synonyms are obtained from WordNet. 
Table 2 shows the relationship between the 
window size and the sentence-level results. This 
table shows the results with and without 
synonyms. Generally, the results with synonyms 
are better than the results without them. With 
respect to window size, the wider the window 
size, the better precision can be achieved. 
However, large window size leads to low recall 
which is probably because of data sparse. The 
best F-score 86.32 is obtained when the window 
size is +/-4. 
 
Window 
size 
Synonym
s 
 
Prec. Recall F-score 
without 
synonyms 
85.27 86.46 85.86 1 
with 
synonyms 
85.66 86.20 85.93 
without 
synonyms 
86.35 85.70 86.02 2 
with 86.14 84.94 85.53 
without 
synonyms 
86.49 85.06 85.77 3 
with 
synonyms 
86.69 84.94 85.81 
without 
synonyms 
86.34 84.81 85.57 4 
with 
synonyms 
87.21 85.44 86.32 
 
Table 2: Sentence-level results relative to 
synonyms and window size for speculation 
detection 
5.2 Finding Hedge Scope 
In the CoNLL-2010 Shared Task 2, our 
in-domain system obtained the F-score of 44.42%. 
Table 3 shows the scope finding results. For 
in-domain scope finding system, we use the 
hedge cues extracted by the submitted CRF-based 
in-domain system (the best result 85.77 in Table 
1). The result of the syntactic pattern-based 
system is not ideal probably due to the syntactic 
parsing errors and limited annotation rules. 
 
System Prec. Recall F-score 
syntactic pattern-based 44.31 42.59 43.45 
CRF-based 45.32 43.56 44.42 
 
Table 3: Official in-domain results for Task 2 
 
Through analyzing the false of our scope 
finding system, we found that many of our false 
scope were caused by such scope as sentence (d1) 
shows. Our CRF-based system signed the 
L-scope to the end of sentence mistakenly. The 
incorrectly annotation of our system and 
gold-standard annotation are shown in sentence 
(d1) and (d2) respectively. So an additional rule is 
added to our CRF-based system to correct the 
L-scope. The rule is: 
? If one token has been predicted as L-scope, 
and if the previous token is ?)?, or ?]?, the 
L-scope will be modified just before the 
paired token ?(? or ?[?. 
 
(d1) The incorrectly predicted version: 
These factors were <cue ref="X1.178.1" 
type="speculation">presumed</cue> to be 
pathogenic</xcope> (85).  
(d2) Gold-standard annotation: 
These factors were <cue ref="X1.178.1" 
type="speculation">presumed</cue> to be 
pathogenic (85) </xcope>. 
 
111
  
F-score is reached to 51.83 by combining this 
additional rule with the submitted CRF-based 
in-domain system as shown in Table 4. 
 
TP FP FN Prec. Recall F-score 
525 468 508 52.87 50.82 51.83 
 
Table 4: Official in-domain results for Task 2 
 
Several best results of Task 1 are exploited to 
investigate the relationship between the window 
size and the scope finding results. From the 
results of Table 5, we can see that the case of n=4 
gives the best precision, recall and F-score. And 
the case of n=2 and the case of n=3 based on the 
same task 1 system have a very similar score. 
With respect to the different systems of Task 1, in 
principle, the higher the F-score of Task 1, the 
better the performance of Task 2 can be expected. 
However, the result is somewhat different from 
the expectation. The best F-score of Task 2 is 
obtained under the case F-score (task 1) =86.02. 
This indicates that it is not certain that Task 2 
system based on the best Task 1 result gives the 
best scope finding performance.  
 
F-score 
(Task 1) 
Window 
size 
Prec. Recall F-score 
86.32 4 
3 
2 
54.32 
52.59 
52.90 
51.69 
50.05 
50.34 
52.98 
51.29 
51.59 
86.02 4 
3 
2 
54.85 
53.13 
53.13 
52.57 
50.92 
50.92 
53.68 
52.00 
52.00 
85.86 4 
3 
2 
54.19 
52.50 
52.50 
52.57 
50.92 
50.92 
53.37 
51.70 
51.70 
 
Table 5: Scope finding results relative to the 
results of task 1 and window size 
 
In the case that scopes longer than n (window 
size) words, the relevant cue will thus not fall into 
the +/-n word window of the L-scope and all 
hedge cue features will be O tag. The hedge cue 
features will be useless for detecting L-scopes. 
Taking into account the importance of hedge cue 
features, the following additional features are 
also incorporated to capture hedge cue features. 
 
? Distance to the closest preceding hedge cue 
? Distance to the closest following hedge cue 
? Stem of the closest preceding hedge cue 
? Stem of the closest following hedge cue  
? POS of the closest preceding hedge cue 
? POS of the closest following hedge cue 
 
Table 6 shows the results when the additional 
hedge cue features are used. The results with 
additional hedge cue feature set are constantly 
better than the results without them. In most of 
cases, the improvement is significant. The best 
F-score 54.18% is achieved under the case 
F-score (task 1) =86.02 and n=4. 
 
F-score 
(Task 1) 
Window 
size 
Prec. Recall F-score 
86.32 4 
3 
2 
54.73 
54.22 
53.41 
52.08 
51.60 
50.82 
53.37 
52.88 
52.08 
86.02 4 
3 
2 
55.35 
54.75 
53.94 
53.05 
52.47 
51.69 
54.18 
53.58 
52.79 
85.86 4 
3 
2 
54.49 
53.79 
53.09 
52.86 
52.18 
51.50 
53.66 
52.97 
52.29 
 
Table 6: Scope finding results relative to the 
results of Task 1 and window size with additional 
cue features 
 
The upper-bound results of CRF-based system 
assuming gold-standard annotation of hedge cues 
are show in Table 7. 
 
TP FP FN Prec. Recall F-score 
618 427 415 59.14 59.83 59.48 
 
Table 7: Scope finding result with gold-standard 
hedge signals 
 
A comparative character analysis of syntactic 
pattern-based method and CRF-based method 
will be interesting, which can provide insights 
leading to better methods in the future. 
6. Conclusion 
In this paper, we have exploited various useful 
features evident to detect hedge cues and their 
scope in biomedical texts. For hedge detection 
task, keyword-based system is integrated with 
CRF-based system by introducing keyword 
features to CRF-based system. Our experimental 
results show that the proposed method improves 
the performance of CRF-based system by the 
additional keyword features. Our system has 
achieved a state of the art F-score 86.32% on the 
sentence-level evaluation. For scope finding task, 
112
  
two different systems are established: CRF-based 
and syntactic pattern-based system. CRF-based 
system outperforms syntactic pattern-based 
system due to its evidential features. 
In the near future, we will improve the hedge 
cue detection performance by investigating more 
implicit information of potential keywords. On 
the other hand, we will study on how to improve 
scope finding performance by integrating 
CRF-based and syntactic pattern-based scope 
finding systems. 
References 
Leonard E. Baum, and Ted Petrie. 1966. Statistical 
inference for probabilistic functions of finite state 
Markov chains. Annals of Mathematical 
Statistics, 37(6):1554?1563. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, 
Casey S. Husser, William Carruth, Larry R. 
Bergstrom, and Dietlind L. Wahner-Roedler. 2005. 
A controlled trial of automated classification of 
negation from clinical notes. BMC Medical 
Informatics and Decision Making, 5(13). 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos 
Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of CoNLL-2010: Shared Task, 
2010, pages 1?12. 
Viola Ganter, and Michael Strube. 2009. Finding 
hedges by chasing weasels: Hedge detection using 
wikipedia tags and shallow linguistic features. In 
Proceedings of the ACL-IJCNLP 2009 
Conference Short Papers, pages 173?176. 
Dan Klein, and Christopher D. Manning. 2003. 
Accurate unlexicalized parsing. In Proceedings of 
the 41st Meeting of the Association for 
Computational Linguistics, pages 423?430. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: 
Probabilistic models for segmenting and labeling 
sequence data. In Proceedings of the Eighteenth 
International Conference on Machine 
Learning, pages 282?289. 
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The language of bioscience: facts, 
speculations, and statements in between. In 
HLT-NAACL 2004 Workshop: BioLINK 2004, 
Linking Biological Literature, Ontologies and 
Databases, pages 17?24. 
Yuan Liu, Qiang Tan, and Kunxu Shen. 1994. The 
word segmentation rules and automatic word 
segmentation methods for Chinese information 
processing. QingHua University Press and 
GuangXi Science and Technology Press. 
Andrew McCallum, Dayne Freitag, and Fernando 
Pereira. 2000. Maximum entropy Markov models 
for information extraction and segmentation. In 
Proceedings of ICML 2000, pages 591?598. 
Ben Medlock. 2008. Exploring hedge identification in 
biomedical literature. Journal of Biomedical 
Informatics, 41(4):636?654. 
Ben Medlock, and Ted Briscoe. 2007. Weakly 
supervised learning for hedge classification in 
scientific literature. In Proceedings of ACL-07, 
pages 992?999. 
Roser Morante, and Walter Daelemans. 2009. 
Learning the scope of hedge cues in biomedical 
texts. In Proceedings of the Workshop on 
BioNLP, ACL 2009, pages 28?36. 
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. 
Learning subjective nouns using extraction pattern 
bootstrapping. In Proceedings of the 7th 
Conference on Computational Natural 
Language Learning, pages 25?32. 
Gy?rgy Szarvas. 2008. Hedge classification in 
biomedical texts with a weakly supervised selection 
of keywords. In Proceedings of ACL: HLT, pages 
281?289. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, 
and J?nos Csirik. 2008. The BioScope corpus: 
biomedical texts annotated for uncertainty, negation 
and their scopes. In Proceedings of BioNLP 2008: 
Current Trends in Biomedical Natural 
Language, pages 38?45. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
Jun?ichi Tsujii. 2005. Developing a robust 
part-of-speech tagger for biomedical text. In 
Advances in Informatics 2005, pages 382?392.
 
113
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 66?70,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Combining Syntactic and Semantic Features by SVM for Unrestricted
Coreference Resolution
Huiwei Zhou1, Yao Li2, Degen Huang3, Yan Zhang4, Chunlong Wu5, Yuansheng Yang6
Dalian University of Technology
Dalian, Liaoning, China
{1zhouhuiwei,3huangdg,6yangys}@dlut.edu.cn
2tianshanyao@mail.dlut.edu.cn
4zhangyan zyzy@yeah.net
5wuchunlong@gmail.com
Abstract
The paper presents a system for the CoNLL-
2011 share task of coreference resolution. The
system composes of two components: one for
mentions detection and another one for their
coreference resolution. For mentions detec-
tion, we adopted a number of heuristic rules
from syntactic parse tree perspective. For
coreference resolution, we apply SVM by ex-
ploiting multiple syntactic and semantic fea-
tures. The experiments on the CoNLL-2011
corpus show that our rule-based mention iden-
tification system obtains a recall of 87.69%,
and the best result of the SVM-based corefer-
ence resolution system is an average F-score
50.92% of the MUC, B-CUBED and CEAFE
metrics.
1 Introduction
Coreference resolution, defined as finding the dif-
ferent mentions in a document which refer to the
same entity in reality, is an important subject in Nat-
ural Language Processing. In particular, coreference
resolution is a critical component of information ex-
traction systems (Chinchor and Nancy, 1998; Sund-
heim and Beth, 1995) and a series of coreference
resolution tasks have been introduced and evaluated
from MUC (MUC-6, 1995). Some machine learning
approaches have been applied to coreference resolu-
tion (Soon et al, 2001; Ng and Cardie, 2002; Bengt-
son and Roth, 2008; Stoyanov et al, 2009). Soon
et al(2001) use a decision tree classifier to decide
whether two mentions in a document are coreferen-
t. Bergsma and Lin (2006) exploit an effective fea-
ture of gender and number to a pronoun resolution
system and improve the performance significantly,
which is also appeared in our feature set. Howev-
er, automatic coreference resolution is a hard task
since it needs both syntactic and semantic knowl-
edge and some intra-document knowledge. To im-
prove the performance further, many deep knowl-
edge resources like shallow syntactic and seman-
tic knowledge are exploited for coreference resolu-
tion (Harabagiu et al, 2001; McCallum and Well-
ner, 2004; Denis and Baldridge, 2007; Ponzetto and
Strube, 2005; Versley, 2007; Ng, 2007). In order to
make use of more syntactic information, Kong et al
(2010) employ a tree kernel to anaphoricity determi-
nation for coreference resolution and show that ap-
plying proper tree structure in corefernce resolution
can achieve a good performance.
The CoNLL-2011 Share Task (Pradhan et
al., 2011) ?Modeling Unrestricted Coreference in
OntoNotes? proposes a task about unrestricted
coreference resolution, which aims to recognize
mentions and find coreference chains in one docu-
ment. We participate in the closed test.
In this paper, we exploit multi-features to a
coreference resolution system for the CONLL-2011
Share Task, including flat features and a tree struc-
ture feature. The task is divided into two steps in
our system. In the first step, we adopt some heuristic
rules to recognize mentions which may be in a coref-
erence chain; in the second step, we exploit a num-
ber of features to a support vector machine (SVM)
classifier to resolute unrestricted coreference. The
experiments show that our system gets a reasonable
result.
The rest of the paper is organized as follows. In
66
Section 2, we describe in detail how our system does
the work of coreference resolution, including how
we recognize mentions and how we mark the coref-
erence chains. The experimental results are dis-
cussed in Section 3. Finally in Section 4, we give
some conclusion.
2 The Coreference Resolution System
The task of coreference resolution is divided into
two steps in our system: mentions detection and
coreference resolution. In the first step, we use some
heuristic rules to extract mentions which may re-
fer to an entity. In the second step, we make up
mention-pairs with the mentions extracted in the
first step, and then classify the mention-pairs in-
to two groups with an SVM model: Coreferent or
NotCoreferent. Finally we get several coreference
chains in a document according to the result of clas-
sification. Each coreference chain stands for one en-
tity.
2.1 Rule-based Identification of Mentions
The first step for coreference resolution is to identify
mentions from a sequence of words. We have tried
the machine-learning method detecting the bound-
ary of a mention. But the recall cannot reach a high
level, which will lead to bad performance of coref-
erence resolution. So we replace it with a rule-based
method. After a comprehensive study, we find that
mentions are always relating to pronouns, named en-
tities, definite noun phrases or demonstrative noun
phrases. So we adopt the following 5 heuristic rules
to extract predicted mentions:
1. If a word is a pronoun, then it is a mention.
2. If a word is a possessive pronoun or a posses-
sive, then the smallest noun phrase containing
this word is a mention.
3. If a word string is a named entity, then it is a
mention.
4. If a word string is a named entity, then the s-
mallest noun phrase containing it is a mention.
5. If a word is a determiner (a, an, the, this, these,
that, etc.), then all the noun phrase beginning
with this word is a mention.
2.2 Coreference Resolution with
Multi-Features
The second step is to mark the coreference chain us-
ing the model trained by an SVM classifier. We ex-
tract the marked mentions from the training data and
take mention-pairs in one document as instances to
train the SVM classifier like Soon et al(2001) . The
mentions with the same coreference id form the pos-
itive instances while those between the nearest posi-
tive mention-pair form the negative instance with the
second mention of the mention-pair.
The following features are commonly used in
NLP processes, which are also used in our system:
? i-NamedEntity/j-NamedEntity: the named en-
tity the mention i/j belongs to
? i-SemanticRole/j-SemanticRole: the semantic
role the mention i/j belongs to which
? i-POSChain/j-POSChain: the POS chain of the
mention i/j
? i-Verb/j-Verb: the verb of the mention i/j
? i-VerbFramesetID/j-VerbFramesetID: the verb
frameset ID of the mention i/j, which works to-
gether with i/j-Verb
All the 5 kinds of features above belong to a sin-
gle mention. For mention-pairs, there are another 4
kinds of features as below:
? StringMatch: after cutting the articles, 1 if the
two mentions can match completely, 2 if one is
a substring of the other, 3 if they partly match,
4 else.
? IsAlias: after cutting the articles, 1 if one men-
tion is the name alias or the abbreviation of the
other one, 0 else
? Distance: it is the number of sentences between
two mentions, 0 if the two mentions are from
one sentenci-Verb/j-Verb: the verb of the men-
tion i/j
? SpeakerAgreement: 1 if both the speakers of
the two mentions are unknown, 2 if both the
two mentions come from the same speaker, 3 if
the mentions comes from different speakers.
67
All of the 14 simple and effective features above
are applied in the baseline system, which use the
same method with our system. But coreference res-
olution needs more features to make full use of the
intra-documental knowledge, so we employ the fol-
lowing 3 kinds of features to our system to catch
more information about the context.
? i-GenderNumber/j-GenderNumber (GN): 7
values: masculine, feminine, neutral, plu-
ral, ?rst-person singular, ?rst-person plural,
second-person.
? SemanticRelation (SR): the semantic relation
in WordNet between the head words of the t-
wo mentions: synonym, hyponym, no relation,
unknown.
? MinimumTree (MT): a parse tree represents the
syntactic structure of a sentence, but corefer-
ence resolution needs the overall context in a
document. So we add a super root to the forest
of all the parse trees in one document, and then
we get a super parse tree. The minimum tree
(MT) of a mention-pair in a super parse tree is
the minimum sub-tree from the common par-
ent mention to the two mentions, just like the
method uesd by Zhou(2009). And the similari-
ty of two trees is calculated using a convolution
tree kernel (Collins and Duffy, 2001), which
counts the number of common sub-trees.
We try all the features in our system, and get some
interesting results which is given in Experiments and
Results Section.
3 Experiments and Results
Our experiments are all carried out on CONLL-2011
share task data set (Pradhan et al, 2007).
The result of mention identification in the first
step is evaluated through mention recall. And the
performance of coreference resolution in the second
step is measured using the average F1-measures of
MUC, B-CUBED and CEAFE metrics (Recasens et
al., 2010). All the evaluations are implemented us-
ing the scorer downloaded from the CONLL-2011
share task website 1 .
1http://conll.bbn.com/index.php/software.html
3.1 Rule-based Identification of Mentions
The mention recall of our system in the mention i-
dentification step reaches 87.69%, which can result
in a good performance of the coreference resolution
step. We also do comparative experiments to inves-
tigate the effect of our rule-based mention identifica-
tion. The result is shown in Table 1. The CRF-based
method in Table 1 is to train a conditional random
field (CRF) model with 6 basic features, including
Word, Pos, Word ID, Syntactic parse label, Named
entity, Semantic role.
Method Recall Precision F-score
Rule-based 87.69 32.16 47.06
CRF-based 59.66 50.06 54.44
Table 1: comparative experiments of CRF-based and
rule-based methods of mention identification(%)
Table 1 only shows one kind of basic machine-
learning methods performs not so well as our rule-
based method in recall measure in mention iden-
tification, but the F1-measure of the CRF-based
method is higher than that of the rule-based method.
In our system, the mention identification step should
provide as many anaphoricities as possible to the
coreference resolution step to avoid losing corefer-
ent mentions, which means that the higher the recal-
l of mention identification is, the better the system
performs.
3.2 Coreference Resolution with
Multi-Features
In the second step of our system, SVM-LIGHT-
TK1.2 implementation is employed to coreference
resolution. We apply the polynomial kernel for
the flat features and the convolution tree kernel for
the minimum tree feature to the SVM classifier, in
which the parameter d of the polynomial kernel is
set to 3 (polynomial (a ? b + c)d) and the combin-
ing parameter r is set to 0.2 (K = tree? forest ?
kernel ? r + vector ? kernel). All the other pa-
rameters are set to the default value. All the exper-
iments are done on the broadcast conversations part
of CoNLL-2011 corpus as the calculating time of
SVM-LIGHT-TK1.2 is so long.
Experimental result using the baseline method
with the GenderNumber feature added is shown in
68
d=? MUC B3 CEAFE AVE
2 47.49 61.14 36.15 48.26
3 51.37 62.82 38.26 50.82
Table 2: parameter d in polynomial kernel in coreference
resolution using the baseline method with the GN fea-
ture(%)
Talbe 2. The result shows that the parameter d in
polynomial kernel plays an important role in our
coreference resolution system. The score when d is
3 is 2.56% higher than when d is 2, but the running
time becomes longer, too.
r=? MUC B3 CEAFE AVE
1 31.41 45.08 22.72 33.07
0.25 34.15 46.87 23.63 34.88
0 51.37 62.82 38.26 50.82
Table 3: combining parameter r (K = tree ? forest ?
kernel ? r + vector? kernel) in coreference resolution
using the baseline with the GN and MT features(%)
In Table 3, we can find that the lower the combin-
ing parameter r is, the better the system performs,
which indicates that the MT feature plays a negative
role in our system. There are 2 possible reasons for
that: the MT structure is not proper for our coref-
erence resolution system, or the simple method of
adding a super root to the parse forest of a document
is not effective.
Method MUC B3 CEAFE AVE
baseline 42.19 58.12 33.6 44.64
+GN 51.37 62.82 38.26 50.82
+GN+SR 49.61 64.18 38.13 50.64
+GN 50.97 62.53 37.96 50.49
+SEMCLASS
Table 4: effect of GN and SR features in coreference res-
olution using no MT feature (%)
Table 4 shows the effect of GenderNumber fea-
ture and SemanticRelation feature, and the last item
is the method using the SemanticClassAgreement-
Feature (SEMCLASS) used by (Soon et al, 2001)
instead of the SR feature of our system. The GN fea-
ture significantly improves the performance of our
system by 6.18% of the average score, which may
be greater if we break up the gender and number
feature into two features. As the time limits, we
haven?t separated them until the deadline of the pa-
per. The effect of the SR feature is not as good as
we think. The score is lower than the method with-
out SR feature, but is higher than the method using
SEMCLASS feature. The decreasing caused by S-
R feature may be due to that the searching depth in
WordNet is limited to one to shorten running time.
To investigate the performance of the second step,
we do an experiment for the SVM-based corefer-
ence resolution using just all the anaphoricities as
the mention collection input. The result is shown in
Table 5. As the mention collection includes no in-
correct anaphoricity, any mistake in coreference res-
olution step has double effect, which may lead to a
relatively lower result than we expect.
MUC B3 CEAFE AVE
65.55 58.77 39.96 54.76
Table 5: using just all the anaphoricities as the mention
collection input in coreference resolution step (%)
In the three additional features, only the GN fea-
ture significantly improves the performance of the
coreference resolution system, the result we finally
submitted is to use the baseline method with GN fea-
ture added. The official result is shown in Table 6.
The average score achieves 50.92%.
MUC B3 CEAFE AVE
48.96 64.07 39.74 50.92
Table 6: official result in CoNLL-2011 Share Task using
baseline method with GN feature added (%)
4 Conclusion
This paper proposes a system using multi-features
for the CONLL-2011 share task. Some syntactic and
semantic information is used in our SVM-based sys-
tem. The best result (also the official result) achieves
an average score of 50.92%. As the MT and S-
R features play negative roles in the system, future
work will focus on finding a proper tree structure
for the intra-documental coreference resolution and
combining the parse forest of a document into a tree
to make good use of the convolution tree kernel.
69
References
A. McCallum and B. Wellner. 2004. Conditional models
of identity uncertainty with application to noun coref-
erence. In Advances in Neural Information Processing
Systems (NIPS), 2004.
Chinchor, Nancy A. 1998. Overview of MUC-7/MET-2.
In Proceedings of the Seventh Message Understanding
Conference (MUC-7).
Eric Bengtson, Dan Roth. 2008. Understanding the Val-
ue of Features for Coreference Resolution Proceed-
ings of the 2008 Conferenceon Empirical Methods in
Natural Language Processing, pages294C303.
Fang Kong, Guodong Zhou, Longhua Qian, Qiaoming
Zhu. 2010. Dependency-driven Anaphoricity Deter-
mination for Coreference Resolution Proceedings of
the 23rd International Conferenceon Computational
Linguistics (Coling2010), pages599C607.
Guodong Zhou, Fang Kong. 2009. Global Learning of
Noun Phrase Anaphoricity in Coreference Resolution
via Label Propagation. In Proceedings of the 2009
Coreference on Empirical Methods in Natural Lan-
guage Processing, pages 978-986, 2009.
M. Collins, N.Duffy. 2001. Convolution Kernels for Nat-
ural Language Resolution NIPS? 2001.
Marta Recasens, Llu?s Mrquez, Emili Sapena, M. Antnia
Mart?, Mariona Taul, Vronique Hoste, Massimo Poe-
sio, Yannick Versley 2010. SemEval-2010 Task 1:
Coreference Resolutionin Multiple Languages In Pro-
ceeding SemEval 2010 Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, 2010.
MUC-6. 1995. Coreference task definition (v2.3, 8 Sep
95) In Proceedings of the Sixth Message Understand-
ing Conference (MUC-6), pages 335-344.
P.Denis, J.Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of HLT/NAACL, 2007.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings
of ACL, 2002.
V. Ng. 2007. Shallow semantics for coreference resolu-
tion. In Proceedings of IJCAI, 2007.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen
Riloff. 2009. Conundrums in Noun Phrase Corefer-
ence Resolution: Making Sense of the State-of-the-Art
Proceeding ACL ?09 Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL.
W.Soon,H.Ng,and D.Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrase. Com-
putational Linguistics, 27(4):521-544,2001.
S. M.Harabagiu,R.C.Bunescu,and S.J. Maiorano. 2001.
Text and knowledge mining for coreference resolution.
In Proceedings of NAACL, 2001.
S.Ponzetto, M.Strube. 2005. Semantic role labeling for
coreference resolution. In Proceedings of EACL, Italy,
April 2005.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011).
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In International Conference on Seman-
tic Computing, 2007.
Shane Bergsma, Dekang Lin. 2006. Bootstrapping Path-
Based Pronoun Resolution. In Proceedings of the 21st
International Conference on Computational Linguis-
tics, 2006.
Sundheim, Beth M. 1995. Overview of results of the
MUC-6 evaluation. In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6), pages 13-
31.
Y.Versley. 2007. Antecedent selection techniques for
high-recall coreference resolution. In Proceedings of
EMNLP/CoNLL, 2007.
70
