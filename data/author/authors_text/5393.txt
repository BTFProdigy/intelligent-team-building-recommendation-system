Extracting semantic lusters from the alignment of definitions 
Gerardo SIERRA 
Institute de Ingenierfa, UNAM 
Apdo. Postal 70-472 
Mdxico 04510, D.F. 
gsm @pumas.iingen.unanunx 
John McNAUGHT 
Department of Language Engineering, UMIST 
PO Box 88 
Manchester M60 IQD, UK 
John.McNaught @unlist.ac.uk 
Abstract 
Through tile alignment of definitions fronl 
two or more dilTerent sources, it is 
possible to retrieve pairs of words that can 
be used indistinguishably in the same 
sentence without changing tile meaning of 
the concept. As lexicographic work 
exploits common defining schemes, such 
as genus and dilTerentia, a concept is 
simihu'ly defined by different dictionaries. 
The dilTerence in words used between two 
lexicographic sources lets us extend lhe 
lexical knowledge base, so that clustering 
is available through merging two or more 
dictionaries into a single database and 
then using an approlwiate alignment 
techlaique. Since aligmnent starts from thc 
same entry of two dictionaries, clustering 
is l~lster than any other technique. 
Tile algorithm introduced here is analogy- 
based, and starts from calculating the 
Levenshtein distance, which is a variation 
o1' the edit distance, and allows us to align 
the definitions. As a measure of similarity, 
the concept el' longest collocation couple 
is introduced, which is the basis of 
clustering similar words. The process 
iterates, replacing similar pairs of words 
in tile definitions until no new clusters are 
found. 
Introduction 
Clustering methods to identify semantically 
similar words are usually divided in relation- 
based and distribution-based approaches 
\[Hirawaka, Xu and Haase 1996\]. Relation-based 
clustering methods rely on the relations in a 
semantic network or ontology to judge the 
similarity between two concepts, either by 
measuring the shortest length that connects two 
concepts in the hierarchical net \[Agirrc and 
Rigau 199611, oi" by comparing tile information 
content shared by the members unde," tile same 
cluster \[Morris and Hirst 1991, Resnik 1997\]. 
ltowever, even although these ontologies 
describe a huge number of members for a 
cluster, few words of a category may be 
interchangeable in the same context and then 
used as members of tile same cluster. This 
means that not all words in a category arc 
necessary. 
Conversely, distribution-based clustering 
methods depend on pure statistical analysis of 
the lexical occurrences ill running texts. A relier 
drawback is that distribution-based methods 
require us to process a large amount of data in 
order to get more reliable results. Moreover, tile 
use el hu'ge corpora is not always practical, due 
to economic, time or capabilities factors. Gao 
11199711 states that tile problem for statistical 
alignment algorilhms, such as those based on tile 
facts described by Gale and Church \[1991\], is
the low frequency of words that occur in parallel 
corpora. The consequences for lacking hu'ge 
corpora include results based on low-frequency 
words, which are quite unrepresentative for 
clustering. 
From a methodological point of view, there is, in 
addition to the above two approaches, a little 
known approach called the analogy-based 
approach. This employs an inferential process 
and is used ill computatkmal linguistics and 
artificial intelligence as an alternative to current 
rule-based linguistic models. 
1 Analogy-based clustering 
Jones \[11996\] suggests corpus alignment as a 
feasible analogy-based approach, ill order to 
align two sentences in tile same language, 
Waterman \[1996\] uses a technique for 
measuring tile similarity between lexical strings, 
named edit distance. This matches tile words of 
795 
two sentences in linear order and determines 
their correspondence. For example, given the 
followiug two definitions for alkalimeter: 
? An apparatus for determining the 
concentration o1' alkalis in solution \[CED\] 
? An instrument for ascertaining the amount 
of alkali in a solution \[OED2\] 
Alignment may identil'y which words in these 
definitions are equivalents of each other. A 
quick observation of the sentences lets us 
identify three pairs of words: (apparatus, 
instrument), (determining, ascertaining) and 
(concentration, amount). 
The appeal of using definitions as corpora for 
alignment is l'ounded on two reasons. Firstly, 
dictionaries contain all necessary information as 
a knowledge base for extracting keywords 
\[Boguraev and Pustejovsky 1996\]. Secondly, it 
is much easier to find the sentences for aligning, 
since definitions are distinguished by entry 
headword. 
Taking into account Waterman's tudies, we 
propose an analogy-based method to identify 
automatically semantic lusters. The difference 
in words used between two or more 
lexicographic definitions enables us to infer 
paradigms by merging the dictionary definitions 
into a single database and then using our own 
alignment technique. 
2 Clustering algorithm 
Tile overall structure of the clustering algorithn\] 
is shown in figure l, and its description is given 
below. 
2.1 Processing definitions 
Our algorithms are used in an overall system 
called "onomasiological search system" (OSS), 
whose aim is to allow the user to find terms by 
giving a description o1' a concept. Lexicographic 
and terminological definitions constitute the 
main lexical resources. Our algorithms cluster 
words that are used in the same context, thus 
operate on pairs of definitions for a same entry 
word, drawn fi'om two different dictionaries. If 
dictionary 1 does not have an entry word that 
exists in dictionary J, then this entry word is 
omitted from consideration. In order to balance 
the number of strings when an entry word in the 
dictionary 1 has two or more senses, the entry 
word in dictionary J is repeated as many times 
as necessary to equal the number of senses of 
dictionary I. We thus derive two files I and J 
containing an equal number of strings S~ and S 2, 
respectively. Each string consists of an entry 
term followed by its definition, the definition 
giving only one sense of the entry term. For each 
string S 1 there is a string S 2. 
Our experiments focus on 314 terms for 
measuring instruments extracted with their 
definitions from CED \[199411 and OED2 \[1994\], 
resulting in 387 strings from each dictionary. 
match S t and S 2 
/ S  1 and S 2 
/ definitions 
t 
processing 
H 
/ 
/ 
/ 
> 
ste ln l l ler  
calculate Levenshtein 
distance H align S~ and S 2 
I 
"'-I J replace strings ~ identify bindings 
<c lusters  ),~__~ cluster bindings 
Figure 1 Clustering algorithm 
stoplist \[ 
,L 
I find Ice @0 
796 
The strings consist ()1' the entry term and the 
definition, so that etymology, part of speech, 
inl'lected t'orms ol' the cntry term, examples and 
other inl'ormation were deleted. Subject-field 
labels, such as 'astronomy' and 'meteorology', 
were preserved, either in full or slightly 
abbreviated, as they are helpful to resolve which 
sense o1' a word to choose, and usually constitute 
a l'undamental property of the concept. 
It should be noted that none of the 387 strings 
suffered any additional transformation, apart 
l'rom a few cases in order to complete a 
del'inition when it had been broken in two pm'ts 
by the dictionary editor, such as when a core 
meaning appears just once at the beginning of 
several subsequent senses. Althongh some 
abbreviations ('U.S.A.'), initials of proper 
names ('C.T.R. Wilson') and possessives ( :un s 
rays') will come out as two or more words al'ter 
deleting punctuation marks and therefore can 
alter the efficiency el' the algorithm, they were 
preserved to observe their effect. 
2.2 Aligning definitions 
In order to compare two strings of woMs, we use 
the Levenshtein distance \[Levenshtein 196611, a
similar method to the edit distance. This method 
measures the edit transl'ormations that change 
one string into other. The Levenshtein distance 
arrangcs the strings in a matrix, with the words 
el' Sj heading the columns and those of S 2 
heading the rows. A null word is inserted at the 
beginning of each string S~ and S 2, in position 
i=0,.j=0. The matrix is filled with the costs of 
insertion, deletion and substitution using the 
l'ollowing formtfla ? 
D(ai,  b i_, ) + Di,,. ,, (bi) 
D(a i ,b j )=  rain D(ai_j,bi)+Di,,.,.(ai) 
D(aH,  b /< ) + D ,I, (ai, b j) 
Where the cost of insertion. D~,,.,(), is 1. and the 
cost of substitution. D,,i,(), is 0 or 1, according to 
whether a~ and bj differ or not. 
Our experimental results have shown that the 
application of the Levenshtein distance using 
stem forms gives better matches than nsing full 
forms. Therefore, we shall fill the matrix with 
the cost for the stem l'orms, although the strings 
preserve the fnll forms both l'or the following 
steps and in the output table. We used the 
stmnming algorithm or' Porter \[1980\], which 
removes endings l'ronl words. 
Building on the Levenshtein distance, Wagner 
and Fisher \[1974\] propose a dynamic 
t~rogramming method to align the elements of 
two strings. Their procedure to return the 
ordered pairs of the alignment starts with the last 
cell of the matrix with cost\[n\]\[m\] and works 
back until either i or j equals 0, according to 
which o1' its neighbours a cell was derived l'rom. 
I1' it is derived either from the previous 
horizontal or vertical cell (\[i-l\]\[j\] or \[i\]lj-l\] 
respectively) then the difference in cost is.just 1, 
otherwise it is derived l'rom the diagonal. 
2.3 Extracting triplets 
The alignment gives us a list of triplets formed 
by ~.ll, J,l~, cost\[i\]\[j\]), in decreasing order 
according to cost\[i\]\[jl, where./.)' I, and ./\]~ arc full 
forms from the strings S~ and S e, respectively. 
There are three possible pairings of words: 
"Equal couple" is defined as the pair (1-\[i, .ffj) of 
full forms such that the corresponding stem 
forms are equal (,s.'/' I = 4)" 
"Matched couple" is a pair (/.)~i, .Oj) such that .sf~ 
# .ff~. This couple represents a potential pair ot' 
similar words. 
"Null couple" is a pair (.g, .g) such that ,s:/I ()r 4 
is missing. 
With respect to the Levcnshtein distance, the 
equal couple means these words do not need any 
change to make both equal, while for the 
matched couple we shall replace one word with 
the other progressively, and for the null couple 
we must either insert one word into the given 
string or delete it from the given string. 
The purpose of clustering is to match different 
pairs of words (matched couples), thus neither 
pairs of equal words (equal couples) nor pairs 
with a null word (null couples) are relevant. 
2.4 Measuring similarity 
As a measure of the similarity between a 
matched couple, we quantify the surrounding 
equal couples above and below it. This concept 
is similar to the "longest common subsequence" 
of two strings suggested by Wagner and Fisher 
\[1974\], which is del'ined as the common 
subsequence of two strings having maximal 
length, although in our case both strings differ 
by the single matched couple. By analogy, we 
use longest collocation couple, henceforth 
797 
abbreviated lcc, since we refer to couples instead 
of a single string. Besides, the word 
"collocation" is more representative for a pair o1' 
words and their neighbourhood, being the core 
of two longest common subsequences. We 
define longest collocation couple as the maximal 
sequence of pairs of words formed by equal 
couples surrounding a matched couple. 
Given the alignment of the strings S~ and S 2 
consisting of a list of triplets formed by (ffi., ff ,  
cost\[ill/\]), in decreasing order according to 
cost\[i\]\[j\], where.ff I, and fl~ are, respectively, full 
fomas l'rom S~ and $2, the lcc is the longest 
consecutive sequence of triplets (~i., f~, 
cost\[i\]\[j\]) formed by one matched couple, such 
that it meets 3 conditions: 
? The cost dilTerence between the first triplet 
and the last triplet is 1. 
? There is no null couple. 
? The matched couple is neither the first nor 
the last triplet. 
By these conditions, only the matched couple 
becomes the core el' a Icc: we constrain a 
matched couple 1o be between two or more 
equal couples, and eliminate the possibility that 
the matched couple appears at the beginning or 
end o1' a phrase. 
As a result, we get a new triplet Off, .\[f~, Icco), 
where (If, J\[~) is the matched couple and lcc,a is 
the length of the longest collocation couple. As 
an example, for the definitions of "dynameter" 
in table 1, there is only one matched couple, 
"determining-measuring", whose lcc is 9 (the 
extent o1' the Icc is indicated by arrows). 
telescopes 
of 
power 
magnifying 
the 
determining 
for 
inslrulnent 
an 
dynameter 
,/,/; 
telescope 
a 
of 
power 
magnifying 
the 
measuring 
for 
instrument 
An 
dynameter 
cost\[il\[jl 
2 
2 
1 
1 
I 
1 
1 
0 
0 
0 
0 
Table 1 Triplets for "dynameter"  
<- 
?U> 
II 
?o 
?J 
<-- 
Ranking all triplets found by lcc in decreasing 
order, we observe that the greater the value o1' 
lcc, the greater the similarity between the words 
of the matched couple. 
2.5 Removing flmetion words 
So far, function words and other noise words 
will also be clustered by our algorithms, in 
general, such words interfere in the 
identification of clusters and can give more 
wrong than good results. We use a stoplist to 
automatically identify any pair of words where a 
non-relevant word appears and exclude it, on the 
grounds that they are not very useful words for 
clustering. Thus, when the program comes 
across a matched pair of different words in a 
context and il' that matched pair contains a word 
from the stoplist, then the pair is rejected. 
Essentially, this is the same thing as using a 
tagger and looking at the tags as well as the 
words, since one would not want to choose a 
noun pairing with a determiner or a relative. 
By inspection, we observe that, after stoplist 
discrimination, the best potential clusters are 
found at higher values ot' Icc. Our experimental 
results show us that a length of lcc equal to 5 is a 
reliable threshold. Although there are also good 
matches for values equal to 4 and 3, the majority 
of these are duplicates of higher values. 
2.6 Clustering 
We introduce the terln binding to represent a
candidate cluster, i.e. two words that may be 
used in the same context without changing the 
meaning o1' a definition. A binding is a matched 
couple (J.l~, .\[/'9 formed by the full forms .\[f~ and 
ft;, after stoplist discrimination, drawn t'rom the 
strings S, and S~, respectively, in such a way that 
the stem forms are equivalent, in a determined 
context, according to a determined threshold'. 
The threshold associated with a binding is the 
length of the lcc, and we consider only bindings 
of matched couples where lcc >_ 5. 
Each binding can be considered as an initial 
cluster. Clusters represent sets o1' words that are 
used with the same meaning in particular 
contexts. In a consecutive sequence of bindings, 
it may happen that a stem form occurs in two or 
more dilTerent bindings. In this case, one can 
cluster all bindings with a common stem form 
according to the transitive property. 
in order to cluster bindings, we use an algorithm 
consisting o1' three loops. First, it assigns a 
cluster number to each binding, so those 
bindings with a common word have the same 
cluster number. Secondly, it clusters bindings 
with the same cluster number, but removes 
798 
duplicate stem forms in tile same cluster. 
Thirdly, it checks if it is possible to inerge new 
clusters with those of previous cycles. This 
process will typically result in a set of 
overlapping clusters, reflecting the natm'al state 
where concepts may belong to more than one 
conceptual class. 
2.7 Cycling 
As bindings represent pairs of words such that 
the stem forms can be substituted in a particular 
context without changing the meaning, sJi = aJ~ 
we can replace any of the full formsf? with the 
full l'orms ffj according to each binding, so that 
the corresponding definition preserves the same 
meaning. After substituting bindings, we 
observe that several pairs of words will now 
typically present a high lcc score, even those 
pairs of words which initially did not yield 
matches with any word. It is then advantageous 
to replace thus the bindings in the definitions 
alld to repeat the entire process until no new 
clusters are found. The first cycle runs from the 
reading o1' definitkms up to merging of clusters. 
All subsequent cycles will start by replacing 
retained bindings in the definitions, thus each 
subsequent cycle works with new data. 
3 Experimental results 
The current clustering algorithm was developed 
by analysing definitions on the following basis: 
? Language dictionaries. The use of language 
dictionaries has been preferred because there 
are enough to extract data from. As they are 
in machine-readable form, it is possible to 
copy definitions, avoiding likely mistakes 
while typewriting. 
? Corpus on 314 "measuring instruments". 
This domain has the advantage that it is easy 
to search for the terms that correspond to it, 
as they usually end in "-meter", "-scope" or 
"-graph". As a conscqueuce o1' applying the 
clustering program to the 387 strings, it is 
evident that the maiority of clusters were 
related to "measure" and "instrument". 
? Alignment of two strings. We have shown 
that two sources of data (pairs of del'inition) 
are sufficient for clustering to yield good 
results. 
? No manipulation el'data. After ktentification 
of the term and the definitions, these were 
truncated to 200 characters and punctuation 
marks were removed. No words in 
definitions were replaced or moved, to "tidy 
up" the data, before being submitted to the 
main process. 
? Stemming algorithm. The stemmer 
algorithm presents both overstemming and 
understemming, but nevertheless the 
clustering program yiekts good results. 
? Stoplist discrimination. The stoplist has 
been used as a tagger, i.e. as a filter to avoid 
matching words with dil'ferent parts ot' 
speech. 
? Bindings for Ice _> 5. The best clusters have 
been observed for bindings with lcc> 5, and 
the results presented m'e good. 
Table 2 presents ome cluster results after two 
cycles of the clustering procedure starting from 
the Levcnshtein distance. In addition to these 
clusters, 14 other clusters of two or three 
elements were obtained. 
I. apparalus inslrumcnt telescope 
2. analyse ascerlaining determining estimating 
location measuring recording lakins testing 
3. amotmt concenlration intensity percentage 
proportion rate salinity strength 
Table 2 Cluster results for "measuring 
illstFunlents" 
The procedure then stops, as no more matched 
words with lcc _> 5 have been found for our data. 
The following sections analyse variations of 
these considerations. 
3.1 Using multiple resources 
General language dictionaries present the 
advantage of using well-established 
lexicographic riteria to normalise definitions. 
These criteria, as for example the use of 
analytical definitious by genus and differentia, 
have been nowadays implemented by 
terminological or specialiscd ictionaries, with 
the addition of a richer vocabulary and the 
identification of properties that are not always 
considered relevant in other resources. 
Unfortunately, these are more oriented to a 
specific domain, so that it is sometimes 
necessary to search in two or more resources to 
compile the data. 
We used many online lexical resources, some of 
them available on the lnternct. This allowed us 
to easily use different databases to extract 
799 
Enhancing automatic term recognition through recognition of variation  
Goran Nenadi?*  
Department of Computation 
UMIST 
Manchester, UK, M60 1QD 
G.Nenadic@umist.ac.uk 
Sophia Ananiadou* 
Computer Science 
University of Salford 
Salford, UK, M5 4WT  
S.Ananiadou@salford.ac.uk 
John McNaught* 
Department of Computation 
UMIST 
Manchester, UK, M60 1QD 
J.McNaught@umist.ac.uk 
 
                                                     
* Co-affiliation: National Centre for Text Mining, Manchester, UK 
Abstract 
Terminological variation is an integral part of the 
linguistic ability to realise a concept in many ways, 
but it is typically considered an obstacle to 
automatic term recognition (ATR) and term 
management. We present a method that integrates 
term variation in a hybrid ATR approach, in which 
term candidates are recognised by a set of 
linguistic filters and termhood assignment is based 
on joint frequency of occurrence of all term 
variants. We evaluate the effectiveness of 
incorporating specific types of term variation by 
comparing it to the performance of a baseline 
method that treats term variants as separate terms. 
We show that ATR precision is enhanced by 
considering joint termhoods of all term variants, 
while recall benefits by the introduction of new 
candidates through consideration of different 
variation types. On a biomedical test corpus we 
show that precision can be increased by 20?70% 
for the top ranked terms, while recall improves 
generally by 2?25%. 
1 Introduction 
Terminological processing has long been 
recognised as one of the crucial aspects of 
systematic knowledge acquisition and of many 
NLP applications (IR, IE, corpus querying, etc.). 
However, term variation has been under-discussed 
and is rarely accounted for in such applications.  
When naming a new concept, scientists and 
specialists usually follow some predefined term 
formation patterns, a process which does not 
exclude the usage of term variations or alternative 
names for concepts. Term variations are very 
frequent: approximately one third of term 
occurrences are variants (Jacquemin, 2001). They 
occur not only in text, but also in controlled, 
manually curated terminological resources (e.g. 
UMLS (NLM, 2004)).  
The task of an automatic term recognition (ATR) 
system is not only to suggest the most likely 
candidate terms from text, but also to correlate 
them with synonymous term variants. In this paper, 
we briefly present an analysis of term variation 
phenomena, whose results are subsequently 
incorporated into a corpus-based ATR method in 
order to enhance its performance.  
The paper is organised as follows. In Section 2, 
we analyse the main types of term variation, and 
briefly examine how existing ATR systems treat 
them. Our approach to incorporating variants into 
ATR is presented in Section 3. In Section 4, we 
evaluate our approach by comparing it to a 
baseline method (the method without variation re-
cognition), and we conclude the paper in Section 5. 
2 Background 
Terms are linguistic units that are assigned to 
concepts and used by domain specialists to 
describe and refer to specific concepts in a domain. 
In this sense, terms are preferred designators of 
concepts. In text, however, concepts are frequently 
denoted by different surface realisations of 
preferred terms, which we denote as their term 
variants. Consequently, a concept can be 
linguistically represented using any of the surface 
forms that are variants of the corresponding 
preferred term. We consider the following types of 
term variation: 
 
(i) orthographic: e.g. usage of hyphens and slashes 
(amino acid and amino-acid), lower and upper 
cases (NF-KB and NF-kb), spelling variations 
(tumour and tumor), different Latin/Greek 
transcriptions (oestrogen and estrogen), etc. 
(ii) morphological: the simplest variations are 
related to inflectional phenomena (e.g. singular, 
plural). Derivational transformations can lead to 
variants in some cases (cellular gene and cell 
gene), but not always (activated factor vs. 
activating factor); 
(iii) lexical: genuine lexical synonyms, which may 
be interchangeably used (carcinoma and 
cancer, haemorrhage and blood loss); 
(iv) structural: e.g. possessive usage of nouns 
using prepositions (clones of human and human 
clones), prepositional variants (cell in blood, 
cell from blood), term coordinations (adrenal 
glands and gonads); 
(v) acronyms and abbreviations: very frequent 
term variation phenomena in technical 
sublanguages, especially in biomedicine; 
sometimes they may be even preferred terms 
(DNA for deoxyribonucleic acid).  
 
Note that variation types (i) ? (iii) affect 
individual constituents, while (iv) and (v) involve 
variation in structure of the preferred term. In any 
case, they do not ?change? the meaning as they 
refer to the same concept. Daille et al (1996) and 
Jacquemin (1999, 2001) further identified types of 
variation that modified the meaning of terms.  
Although many authors mention the problems 
related to term variation, few have dealt with 
linking the corresponding term variants. Also, the 
recognition of variants is typically performed as a 
separate operation, and not as part of ATR.  
The simplest technique to handle some types of 
term variation (e.g. morphological) is based on 
stemming: if two term forms share a stemmed 
representation, they are considered as mutual 
variants (Jacquemin and Tzoukermann, 1999; 
Ananiadou et al, 2000). However, stemming may 
result in ambiguous denotations related to ?over-
stemming? (i.e. resulting in the conflation of terms 
which are not real variants) and ?under-stemming? 
(i.e. resulting in the failure to link real term 
variants).  
Other approaches to the recognition of term 
variants use preferred terms and known synonyms 
from existing term dictionaries and approximate 
string matching techniques to link or generate 
different term variants (Krauthammer et al, 2001; 
Tsuruoka and Tsujii, 2003).  
Jacquemin (2001) presents a rule-based system, 
FASTR, which supports several hundred meta-
rules dealing with morphological, syntactic (i.e. 
structural) and semantic term variation. Term 
variation recognition is based on the 
transformation of basic term structures into variant 
structures. However, the variants recognised by 
FASTR are more conceptual variants than 
terminological ones, as non-terminological units 
(such as verb phrases, extended insertions, etc.) are 
also linked to terms in order to improve indexing 
and retrieval.   
 
3 Incorporating term variation into ATR 
Our approach to ATR combines the C-value 
method (Frantzi et al, 2000) with the recognition 
of term variation, which is incorporated as an 
integral part of the term extraction process.  
C-value is a hybrid approach combining term 
formation patterns with corpus-based statistical 
measures. Term formation patterns act as linguistic 
filters to a POS tagged corpus: filtered sequences 
are considered as potential realisations of domain 
concepts (term candidates). They are subsequently 
assigned termhoods (i.e. likelihood to represent 
terms) according to a statistical measure. The 
measure amalgamates four corpus-based 
characteristics of a term candidate, namely its 
frequency of occurrence, its frequency of 
occurrence as a form nested within other candidate 
terms, the number of candidate terms inside which 
it is nested, and the number of words it contains.   
The original C-value method treats term variants 
that correspond to the same concept as separate 
term candidates. Consequently, by providing 
separate frequencies of occurrence for individual 
variants instead of a single frequency of 
occurrence calculated for a term candidate unifying 
all variants, the corpus-based measures and 
termhoods are distributed across different variants. 
Therefore, we aim at enhancing the statistical 
evaluation of termhoods through conflation of 
different surface representations of a given term, 
and through joint frequencies of occurrence of all 
equivalent surface forms that correspond to a 
single concept.  
In order to conflate equivalent surface 
expressions, we carry out linguistic normalisation 
of individual term candidates (see examples in 
Table 1). Firstly, each term candidate is mapped to 
a specific canonical representative (CR) by 
semantically isomorphic transformations. Then, we 
establish an equivalence relation, where two term 
candidates are related iff they share the same CR. 
The partitions of this relation are denoted as 
synterms: a synterm contains surface term 
representations sharing the same CR.  
 
synterm canonical representative 
human cancers 
cancer in humans 
human?s cancer 
human carcinoma } human cancer 
Table 1: Term normalisation examples 
 
Our aim is to form synterms prior to the syntactic 
estimation of termhoods for term candidates. 
Therefore, after the extraction of individual term 
candidates, we subsequently normalise them in 
order to generate synterms, where the 
normalisation is performed according to the 
typology of variations described in Section 2. More 
precisely, we consider separately the normalisation 
of variations that affect term candidate constituents 
and variations that involve structural changes. The 
general architecture of our ATR approach is 
presented in Figure 1. 
 
 
P O S  tagger 
 
In flec tiona l n orm alisa tio n  
S tructura l  n orm alisation  
O rthographic  no rm alisa tion  
E x trac ted  syn term s 
Inp u t d ocu m ents 
T erm h ood  es tim ation  
E xtrac tio n  of term  cand id ates  
A cron ym  acq uis ition  
 
Figure 1: The architecture of the ATR process 
 
3.1 Normalising term constituent variation 
In the case of variations that do not affect the 
structure of terms, the formation of CRs is based 
on a POS tagger (for inflectional variation) and 
simple heuristics (for orthographic normalisation). 
For example, different transcriptions of 
neoclassical combining forms are treated by 
replacements of specific character combinations 
(ae ? e, ph ? f) in such forms (and only in such 
forms). Inflectional normalisation is based on POS 
tagging: a canonical term candidate form is a 
singular form containing no possessives (Down?s 
syndrome ? down syndrome). 
In order to address lexical variants, one can use 
dictionaries of synonyms where the preferred terms 
are used for normalisation purposes ({hepatic 
microsomes, liver microsomes} ? liver 
microsomes). In experiments reported here, we did 
not attempt to normalise lexical variation. 
 
3.2 Normalising term structure variation 
Variations affecting term structure are less frequent 
but more complex. Here we consider two types of 
term variation: prepositional term candidates and 
coordinated term candidates (for a detailed analysis 
of these variations see (Nenadic et al, 2004)). 
Prepositional term candidates are normalised by 
transformation into corresponding expressions 
without prepositions. Using prepositions of, in, for 
and by as anchors, we generate semantically 
isomorphic CRs by inversion. For example, the 
candidate nuclear factor of activated T cell is 
transformed into activated T cell nuclear factor. 
Here is a simplified example of a rule describing 
the transformation of a term candidate that 
contains the preposition of: 
 
if  structure of  term candidate is   
   (A|N)1* N1 Prep(of) (A|N)2* N2  
then   CR = (A|N)2* N2 (A|N)1* N1 
 
In order to address the problems of determining 
the boundaries of term constituents in text (to the 
right and left of prepositions), for each 
prepositional term candidate we generate all 
possible nested candidates? and their corresponding 
CRs. For example, for the candidate regulation of 
gene expression, we generate both gene regulation 
and gene expression regulation. Since this 
approach also generates a number of false 
candidates, additional heuristics are used to 
enhance precision, such as removing adverbials 
and determiners, using a stop list of 
terminologically irrelevant prepositional 
expressions (e.g. number of ..., list of ..., case of ..., 
in presence of ...), etc.  
A similar approach is used for the recognition of 
coordinated term candidates: coordinating 
conjunctions (and, or, but not, as well as, etc.) are 
used as anchors, and when a coordinating structure 
is recognised in text, the corresponding CRs of the 
candidate terms involved are generated.  
We differentiate between head coordination 
(where term heads are coordinated, e.g. adrenal 
glands and gonads) and argument coordination 
(where term arguments/modifiers are coordinated, 
e.g. SMRT and Trip-1 mRNAs). 
The recognition and extraction of coordinated 
terms is highly ambiguous even for human 
specialists, since coordinated terms and term 
conjunctions share the same structures (see Table 
2). Also, similar patterns cover both argument and 
head coordinations, which makes it difficult to 
extract coordinated constituents (i.e. terms). Not 
only is the recognition of term coordinations and 
their subtypes ambiguous, but also internal 
boundaries of coordinated terms are blurred. In a 
separate study, we have shown that 
morphosyntactic features are insufficient both for 
the successful recognition of coordinations and for 
the extraction of coordinated terms: in many cases, 
the correct interpretation and decoding of term 
coordinations is only possible with sufficient 
background knowledge (Nenadic et al, 2004). 
                                                     
? Each constituent extracted from a nested pre-
positional term candidate has to follow a pattern used 
for the extraction of individual candidate terms. 
 
example adrenal  glands and gonads 
head 
coordination [adrenal [glands and gonads]] 
term  
conjunction  [adrenal glands] and [gonads] 
Table 2: Ambiguities within coordinated structures 
In order to address the problems of structural 
ambiguities and boundaries of coordinated terms, 
we also generate all possible nested coordination 
expressions and corresponding term candidates. 
For example, from a candidate coordination viral 
gene expression and replication we generate two 
pairs of coordinated term candidates: 
 
viral gene expression  and  viral gene replication 
viral gene expression  and  viral replication 
 
Patterns for the extraction of term candidates 
from coordinations have been acquired semi-
manually for a subset of term coordinations. For 
each pattern, we define a procedure for the 
extraction of coordinated term candidates and 
generation of the corresponding CRs (see Table 3 
for examples). The generated candidates from 
coordinated structures are subsequently treated as 
individual term candidates. 
 
3.3 Normalising acronym variation  
We treat acronym extraction as part of the ATR 
process (see Figure 1). In (Nenadic et al, 2002) we 
suggested a simple procedure for acquiring 
acronyms and their expanded forms (EFs), which 
was mainly based on using orthographic and 
syntactic features of contexts where acronyms 
were introduced. The model is based on three types 
of patterns: acronym patterns (defining common 
internal acronym structures and forms), definition 
patterns (based on syntactic patterns which 
describe typical contexts where acronyms are 
introduced in text) and matching patterns (the set 
of matching rules between acronyms and their 
corresponding EFs).  
Acronyms also exhibit variation (e.g. RAR alpha, 
RAR-alpha, RARA, RARa, RA receptor alpha etc. 
are all acronyms for retinoic acid receptor alpha). 
Therefore, in addition to extracting acronyms, we 
further gather all acronym variants and their EFs, 
and we map them into a single CR. Since in this 
paper acronyms are taken as term variants, we  
?replace? acronym occurrences by the CR of their 
EFs. In order to bypass the problem of acronym 
ambiguity, we replace/normalise only acronyms 
that are introduced in a given document. 
 
(N|A)1 & (N|A)2 (N+)3 
candidate1 = (N|A)2 (N+)3 
candidate2 = (N|A)1 nested(N+3 ) 
 
e.g.   B and T cell antigen    
          T cell antigen    
          B cell antigen, B antigen 
N1 & N2 A3 N+4 
candidate1 = N2 A3 N+4 
candidate2 = N1 A3 N+4 
 
e.g.  function or surface antigenic profile 
     surface antigenic profile   
     function antigenic profile 
N+1 N2 & (N|A)3 
candidate1 = N+1 N2  
candidate2 = nested(N+1) (N|A)3  
 
e.g.  breast cancer therapy and prevention 
     breast cancer therapy  
     breast caner prevention, breast  prevention 
N+1 (A+)2  A3 &  A4 
candidate1 = N+1 (A+)2 A3  
candidate2 = N+1 (A+)2 A4  
 
e.g.  RNA polymerases II and III 
      RNA polymerasis II 
      RNA polymerasis III 
Table 3: Examples of patterns used for the 
extraction of term candidates from coordinations  
(nested denotes the generation of all possible 
linearly nested substrings)  
3.4 Calculating termhoods with variants 
Term variants sharing the same CR are grouped 
together into synterms, and the calculation of C-
values (i.e. termhoods) is performed for the whole 
synterm rather than for individual term candidates. 
The main reason for doing this is to avoid the 
distribution of frequencies of occurrence of term 
candidates across different variants, as these 
frequencies have a significant impact on estimating 
termhoods. Instead of providing separate 
frequencies of occurrence and obtaining termhoods 
for individual term candidates, we provide a single 
frequency of occurrence and joint termhood 
calculated for a synterm, which unifies all variants. 
Similarly to the estimation of C-values for 
individual term candidates (Frantzi et al, 2000), 
the formula for calculating the termhoods for 
synterms is as follows: 
 
 
??
??
?
?
??
=
?
?
nestednot  is CR),(||log
nested is CR,))(||
1)((||log  )value(-C
2
2
CRfCR
bfTCRfCRc CRTbCR
 
where c denotes a synterm whose elements share a 
canonical representative (denoted as CR in the 
formula), f(CR) corresponds to the cumulative 
frequency with which all term candidates from the 
synterm c occur in a given corpus, |CR| denotes 
the average length of the term candidates (the 
number of constituents), and TCR is a set of all 
synterms whose CRs contain the given CR as a 
nested substring. 
This approach ensures that all term variants are 
naturally dealt with jointly, thus supporting the fact 
that they denote the same concept. As a 
consequence, we expect that precision would be 
enhanced by considering joint frequencies of 
occurrence and termhoods for all variants of 
candidate terms, while recall would benefit by the 
introduction of new candidates through 
consideration of different variation types. 
 
4 Evaluation and discussion 
In order to assess the effectiveness of incorporating 
specific types of term variation into ATR, we 
compared the performance of the baseline C-value 
method (without considering variations) with the 
approach including recognition and conflation of 
term variants. Here we are not interested in an 
absolute measure of the ATR performance, but 
rather in the comparison of results obtained 
through handling different variation types.  
We conducted two sets of experiments: in the 
first experiment, we analysed the incorporation of 
term candidates resulting from considering term 
variations individually, while, in the second, we 
experimented with the integration of combined 
variations in the ATR process. 
The evaluation was carried out using the GENIA 
corpus (GENIA, 2004), which contains 2,000 
abstracts in the biomedical domain with 76,592 
manually marked occurrences of terms. These 
occurrences (which include different term variants) 
correspond to 29,781 different, unique terms. Each 
occurrence of a term in the corpus (except 
occurrences of acronyms) is linked to the 
corresponding ?normalised? term (typically a 
singular form), while coordinated terms are 
identified, marked and normalised within term 
coordinations. A third of occurrences of GENIA 
terms are affected by inflectional variations, and 
almost half of GENIA terms have inflectional 
variants appearing in the corpus. On the other 
hand, only 0.5% of terms contain a preposition, 
while 2% of all term occurrences are coordinated, 
involving 9% of distinct GENIA terms (for a 
detailed analysis of GENIA terms see (Nenadic et 
al., 2004)). 
We used the list of GENIA terms as a gold 
standard for the evaluation. Since our ATR method 
produces a ranked list of suggested synterms, we 
considered precision at fixed rank cut-offs 
(intervals): precision was calculated as the ratio 
between the number of correctly recognised terms 
and the total number of entities recognised in a 
given interval (where an interval included all terms 
from the top ranked synterms).? The baseline 
method (original C-value) was treated in the same 
way, as term candidates suggested by the original 
C-value could be seen as singleton synterms. In 
order to estimate the influence on recall, we also 
used all variants from suggested synterms.  
The incorporation of individual variations 
affecting term constituents into ATR had 
considerable positive effects, especially on the 
most frequently occurring terms (see Figures 2a 
and 2b): for some intervals, inflectional variants, 
for example, improved precision by almost 50%. 
Similarly, the integration of acronyms improved 
precision, in particular for frequent terms (up to 
70%), as acronyms are typically introduced for 
such terms. As one would expect, the combined 
constituent-level variations further improved 
interval precisions compared both to the baseline 
method and individual variations (see Figure 2c). 
However, the incorporation of structural variants 
(in particular for prepositional terms) negatively 
influenced precision compared to the baseline 
method, as many false candidates were introduced.  
In order to assess the quality of extracted 
prepositional term candidates, we evaluated a set 
of the 117 most frequently occurring candidates 
with prepositions: 80% of suggested expressions 
were deemed relevant by domain experts, although 
they were not included in the gold GENIA 
standard (such as expression of genes or binding of 
NF kappa B). Still, the recognition of prepositional 
term candidates is difficult as they are infrequent 
and there are no clear morphosyntactic cues that 
can differentiate between terminologically relevant 
and irrelevant prepositional phrases. 
The incorporation of coordinated term candidates 
had only marginal influence on precision, mainly 
because they were not frequent in the GENIA 
corpus. Furthermore, simple term conjunctions 
                                                     
? It was an open question whether to count the 
recognition of each term form (e.g. singular and plural 
forms, an acronym and its EF, prepositional and non-
prepositional forms) separately (i.e. as two positive 
?hits?) or as one positive ?hit? (see also (Church, 
1995)). Since the evaluation of the baseline method 
(original C-value) typically counts such hits separately, 
we decided to follow this approach, and consequently 
count all positive hits from synterms.  
 
were far more frequent than term coordinations, 
which made their extraction highly ambiguous. 
Still, using only the patterns from Table 3, we have 
correctly extracted 35.76% of all GENIA 
coordinated terms, with more than a half of all 
suggested candidates being found among those that 
appeared exclusively in coordinations. However, 
these patterns also generated a number of false 
coordination expressions, and consequently a 
number of false term candidates. 
The integration of term variants was also useful 
for re-ranking of true positive term candidates: the 
combined rank was typically higher than the 
separate ranks of term variants. Furthermore, some 
terms, not suggested by the baseline method at all, 
were ranked highly when variants were conflated 
(for example, the term T-lymphocyte was 
recognised only as a coordinated term candidate, 
while replication of HIV-1 was extracted only by 
considering prepositional term candidates). In 
order to estimate the overall influence on recall of 
ATR, we used all terms from the respective 
synterms (see Table 4 for the detailed results). In 
general, the incorporation of inflectional variants 
increased recall by ?, while acronyms improved 
recall by almost ? when only the most frequent 
terms were considered. It is interesting that 
acronym acquisition can further improve recall by 
extracting variants that have more complex internal 
structures (such as EFs containing prepositions 
(REA = repressor of estrogen activity) and/or 
coordinations (SMRT = silencing mediator of 
retinoic and thyroid receptor)). Prepositional and 
coordination candidate terms had some influence 
on recall, in particular as they increased the 
likelihood of some candidates to be suggested as 
terms. Low recall of term coordinations may be 
increased by adding more patterns (which would 
probably negatively affect precision).  
Summarising, experiments performed on the 
GENIA corpus have shown that the incorporation 
of term variations into the ATR process resulted in 
significantly better precision and recall. In general, 
acronyms and inflectional unification are the most 
important variation types (at least in the domain of 
biomedicine). Individually, they increased 
precision by 20?70% for the top ranked synterm 
intervals, while recall is generally improved, in 
some cases up to 25%. Other term variations had 
only marginal influence on the performance, 
mainly because they were infrequent in the test 
corpus (compared to the total number of term 
occurrences, and not only with regard to specific 
individual candidates, but also in general). For 
these variations, larger-scale corpora may show 
their stronger influence.  
0
1
2
50 100 150 250 500 100
0
150
0
300
0
500
0
100
00 all
prepositions inflectional acronyms
Figure 2a: Comparison of interval ATR precision 
of the baseline method (=1) to ATR precisions with 
integrated recognition of individual term variants 
(terms with frequency > 5) 
0
1
2
50 100 150 250 500 100
0
150
0
300
0
500
0
100
00 all
prepositions inflectional acronyms
Figure 2b: Comparison of interval ATR precision 
of the baseline method (=1) to ATR precisions with 
integrated recognition of individual term variants 
(terms with frequency > 0) 
0
1
2
50 100 150 250 500 100
0
150
0
300
0
500
0
100
00 all
inflectional infl & acro all
Figure 2c: Comparison of interval ATR precision 
of the baseline method (=1) to ATR precisions with 
integrated recognition of combined term variants 
(terms with frequency > 0) 
 
term sets prep. coord. infl. acro. 
freq. ? 5 +5.30% +12.42% +17.52% +60.49%
freq. > 0 +2.36% +2.53% +25.25% +8.52% 
Table 4: Improvement in recall when variations 
are considered as an integral part of ATR 
 
5 Conclusion 
In this paper we discussed possibilities for the 
extraction and conflation of different types of 
variation of term candidates. We demonstrated that 
the incorporation of treatment of term variation 
enhanced the performance of an ATR system, and 
that tackling term variation phenomena was an 
essential step for ATR. In our case, precision was 
boosted by considering joint frequencies of 
occurrence and termhoods for all candidate terms 
from candidate synterms, while recall benefited 
from the introduction of new candidates through 
consideration of different variation types. Although 
we experimented with a biomedical corpus, our 
techniques are general and can be applied to other 
domains.  
Variations affecting single term candidate 
constituents are the most frequent phenomena, and 
also straightforward for implementation as part of 
an ATR process. The conflation of such term 
candidate variants can be further tuned for a 
specific domain by using lists of combining forms 
and affixes. The incorporation of acronyms had a 
significant high positive effect, in particular on 
more frequent terms (since acronyms are 
introduced for terms that are used more 
frequently). 
However, more complex structural phenomena 
had a moderate positive influence on recall, but, in 
general, the negative effect on precision. The main 
reason for such performances is structural and 
terminological ambiguity of these expressions, in 
addition to their low frequency of occurrence 
(compared to the total number of term 
occurrences). For handling such complex variants, 
a knowledge-intensive and domain-specific 
approach is needed, as coordinated term candidates 
or candidates with prepositions need to be 
additionally semantically analysed in order to 
suggest more reliable term candidates, and to 
introduce fewer false candidates.  
Apart  from being useful for boosting precision 
and recall, the integration of term variation into 
ATR is particularly important for smaller corpora 
(where linking related occurrences is vital for 
successful terminology management) as well as for 
many text-mining tasks (such as IR, IE, term or 
document clustering and classification, etc.). 
Finally, as future work, we plan to investigate 
more knowledge intensive, domain-specific 
treatment of prepositional and coordinated terms, 
as well as pronominal term references. 
6 Acknowledgements 
This research has been partially supported by the 
JISC-funded National Centre for Text Mining 
(NaCTeM), Manchester, UK. 
References  
S. Ananiadou, S. Albert and D. Schuhmann. 2000. 
Evaluation of Automatic Term Recognition of 
Nuclear Receptors from Medline. Genome 
Informatics Series, vol. 11. 
K.W. Church. 1995. One Term or Two? Proc. of 
SIGIR-95, pp. 310-318. 
B. Daille, B. Habert and C. Jacquemin. 1996. 
Empirical Observation of Term Variation and 
Principles for Their Description. Terminology 
3(2), pp. 197?258. 
K. Frantzi, S. Ananiadou and H. Mima. 2000. 
Automatic Recognition of Multi-Word Terms: 
the C/NC value method. International Journal of 
Digital Libraries, vol. 3:2, pp. 115?130. 
C. Jacquemin. 1999. Syntagmatic and paradigmatic 
representations of term variation. Proc. of 37th 
Annual Meeting of ACL, pp. 341?348. 
C. Jacquemin. 2001. Spotting and Discovering 
Terms through NLP. MIT Press, Cambridge MA. 
C. Jacquemin and E. Tzoukermann. 1999. NLP for 
Term Variant Extraction: A Synergy of 
Morphology, Lexicon and Syntax, in T. 
Strzalkowski (ed.), Natural Language 
Information Retrieval, Kluwer, pp. 25-74 
M. Krauthammer, A. Rzhetsky, P. Morozov, and 
C. Friedman. 2001. Using BLAST for 
identifying gene and protein names in journal 
articles. Gene, 259(1?2): pp. 245?52. 
GENIA. 2004. GENIA resources. Available at 
http://www.tsujii.is.u-tokyo.ac.jp/~Genia/ 
G. Nenadic, I. Spasic and S. Ananiadou. 2002. 
Automatic Acronym Acquisition and Term 
Variation Management within Domain-Specific 
Texts. Proc. of LREC 2002, pp. 2155?2162. 
G. Nenadic, I. Spasic and S. Ananiadou. 2004. 
Mining Biomedical Abstracts: What?s in a 
Term? Proc. of IJC-NLP, pp. 247-254.  
NLM. 2004. National Library of Medicine, Unified 
Medical Language System. 
Y. Tsuruoka and J. Tsujii. 2003. Probabilistic 
Term Variant Generator for Biomedical Terms. 
Proc. of 26th Annual ACM SIGIR Conference. 
Proceedings of the Linguistic Annotation Workshop, pages 33?40,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Annotation Type System for a Data-Driven NLP Pipeline
Udo Hahn Ekaterina Buyko Katrin Tomanek
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30, 07743 Jena, Germany
{hahn|buyko|tomanek}@coling-uni-jena.de
Scott Piao John McNaught Yoshimasa Tsuruoka Sophia Ananiadou
NaCTeM and School of Computer Science
University of Manchester
{scott.piao|john.mcnaught|yoshimasa.tsuruoka|sophia.ananiadou}@manchester.ac.uk
Abstract
We introduce an annotation type system for
a data-driven NLP core system. The specifi-
cations cover formal document structure and
document meta information, as well as the
linguistic levels of morphology, syntax and
semantics. The type system is embedded in
the framework of the Unstructured Informa-
tion Management Architecture (UIMA).
1 Introduction
With the maturation of language technology, soft-
ware engineering issues such as re-usability, in-
teroperability, or portability are getting more and
more attention. As dozens of stand-alone compo-
nents such as tokenizers, stemmers, lemmatizers,
chunkers, parsers, etc. are made accessible in vari-
ous NLP software libraries and repositories the idea
sounds intriguing to (re-)use them on an ?as is? basis
and thus save expenditure and manpower when one
configures a composite NLP pipeline.
As a consequence, two questions arise. First, how
can we abstract away from the specific code level of
those single modules which serve, by and large, the
same functionality? Second, how can we build NLP
systems by composing them, at the abstract level
of functional specification, from these already ex-
isting component building blocks disregarding con-
crete implementation matters? Yet another burning
issue relates to the increasing availability of multiple
metadata annotations both in corpora and language
processors. If alternative annotation tag sets are cho-
sen for the same functional task a ?data conversion?
problem is created which should be solved at the ab-
stract specification level as well (Ide et al, 2003).
Software engineering methodology points out that
these requirements are best met by properly identi-
fying input/output capabilities of constituent compo-
nents and by specifying a general data model (e.g.,
based on UML (Rumbaugh et al, 1999)) in or-
der to get rid of the low-level implementation (i.e.,
coding) layer. A particularly promising proposal
along this line of thought is the Unstructured Infor-
mation Management Architecture (UIMA) (Ferrucci
and Lally, 2004) originating from IBM research ac-
tivities.1 UIMA is but the latest attempt in a series
of proposals concerned with more generic NLP en-
gines such as ATLAS (Laprun et al, 2002) or GATE
(Cunningham, 2002). These frameworks have in
common a data-driven architecture and a data model
based on annotation graphs as an adaptation of the
TIPSTER architecture (Grishman, 1997). They suf-
fer, however, from a lack of standards for data ex-
change and abstraction mechanisms at the level of
specification languages.
This can be achieved by the definition of a com-
mon annotation scheme. We propose an UIMA
schema which accounts for a significant part of the
complete NLP cycle ? from the collection of doc-
uments and their internal formal structure, via sen-
tence splitting, tokenization, POS tagging, and pars-
ing, up until the semantic layer (still excluding dis-
course) ? and which aims at the implementation-
independent specification of a core NLP system.
1Though designed for any sort of unstructured data (text,
audio and video data), we here focus on special requirements
for the analysis of written documents.
33
2 Related work
Efforts towards the design of annotation schemata
for language resources and their standardization
have a long-standing tradition in the NLP commu-
nity. In the very beginning, this work often fo-
cused exclusively on subdomains of text analysis
such as document structure meta-information, syn-
tactic or semantic analysis. The Text Encoding Ini-
tiative (TEI)2 provided schemata for the exchange
of documents of various genres. The Dublin Core
Metadata Initiative3 established a de facto standard
for the Semantic Web.4 For (computational) lin-
guistics proper, syntactic annotation schemes, such
as the one from the Penn Treebank (Marcus et al,
1993), or semantic annotations, such as the one un-
derlying ACE (Doddington et al, 2004), are increas-
ingly being used in a quasi standard way.
In recent years, however, the NLP community is
trying to combine and merge different kinds of an-
notations for single linguistic layers. XML formats
play a central role here. An XML-based encod-
ing standard for linguistic corpora XCES (Ide et al,
2000) is based on CES (Corpus Encoding Standard)
as part of the EAGLES Guidelines.5 Work on TIGER
(Brants and Hansen, 2002) is an example for the li-
aison of dependency- and constituent-based syntac-
tic annotations. New standardization efforts such as
the Syntactic Annotation Framework (SYNAF) (De-
clerck, 2006) aim to combine different proposals and
create standards for syntactic annotation.
We also encounter a tendency towards multiple
annotations for a single corpus. Major bio-medical
corpora, such as GENIA (Ohta et al, 2002) or
PennBioIE,6 combine several layers of linguistic
information in terms of morpho-syntactic, syntac-
tic and semantic annotations (named entities and
events). In the meantime, the Annotation Compat-
ibility Working Group (Meyers, 2006) began to con-
centrate its activities on the mutual compatibility of
annotation schemata for, e.g., POS tagging, tree-
banking, role labeling, time annotation, etc.
The goal of these initiatives, however, has never
been to design an annotation scheme for a complete
2http://www.tei-c.org
3http://dublincore.org
4http://www.w3.org/2001/sw
5http://www.ilc.cnr.it/EAGLES96/
6http://bioie.ldc.upenn.edu
NLP pipeline as needed, e.g., for information ex-
traction or text mining tasks (Hahn and Wermter,
2006). This lack is mainly due to missing standards
for specifying comprehensive NLP software archi-
tectures. The MEANING format (Pianta et al, 2006)
is designed to integrate different levels of morpho-
syntactic annotations. The HEART OF GOLD mid-
dleware (Scha?fer, 2006) combines multidimensional
mark-up produced by several NLP components. An
XML-based NLP tool suite for analyzing and anno-
tating medical language in an NLP pipeline was also
proposed by (Grover et al, 2002). All these propos-
als share their explicit linkage to a specific NLP tool
suite or NLP system and thus lack a generic annota-
tion framework that can be re-used in other develop-
mental environments.
Buitelaar et al developed in the context of an in-
formation extraction project an XML-based multi-
layered annotation scheme that covers morpho-
syntactic, shallow parsing and semantic annotation
(Buitelaar et al, 2003). Their scheme borrows con-
cepts from object-oriented programming (e.g., ab-
stract types, polymorphism). The object-oriented
perspective already allows the development of a
domain-independent schema and extensions of core
types without affecting the base schema. This
schema is comprehensive indeed and covers a sig-
nificant part of advanced NLP pipelines but it is also
not connected to a generic framework.
It is our intention to come full circle within a
general annotation framework. Accordingly, we
cover a significant part of the NLP pipeline from
document meta information and formal document
structure, morpho-syntactic and syntactic analysis
up to semantic processing. The scheme we propose
is intended to be compatible with on-going work
in standardization efforts from task-specific annota-
tions and to adhere to object-oriented principles.
3 Data-Driven NLP Architecture
As the framework for our specification efforts, we
adopted the Unstructured Information Management
Architecture (UIMA) (Ferrucci and Lally, 2004). It
provides a formal specification layer based on UML,
as well as a run-time environment for the interpreta-
tion and use of these specifications. This dualism is
going to attract more and more researchers as a basis
34
for proper NLP system engineering.
3.1 UIMA-based Tool Suite
UIMA provides a platfrom for the integration
of NLP components (ANALYSIS ENGINES in the
UIMA jargon) and the deployment of complex
NLP pipelines. It is more powerful than other
prominent software systems for language engineer-
ing (e.g., GATE, ATLAS) as far as its pre- and
post-processing facilities are concerned ? so-called
COLLECTION READERS can be developed to handle
any kind of input format (e.g., WWW documents,
conference proceedings), while CONSUMERS, on
other hand, deal with the subsequent manipulation
of the NLP core results (e.g., automatic indexing).
Therefore, UIMA is a particularly suitable architec-
ture for advanced text analysis applications such as
text mining or information extraction.
We currently provide ANALYSIS ENGINES for
sentence splitting, tokenization, POS tagging, shal-
low and full parsing, acronym detection, named
entity recognition, and mapping from named enti-
ties to database term identifiers (the latter is mo-
tivated by our biological application context). As
we mainly deal with documents taken from the bio-
medical domain, our collection readers process doc-
uments from PUBMED,7 the most important liter-
ature resource for researchers in the life sciences.
PUBMED currently provides more than 16 million
bibliographic references to bio-medical articles. The
outcomes of ANALYSIS ENGINES are input for var-
ious CONSUMERS such as semantic search engines
or text mining tools.
3.2 Common Analysis System
UIMA is based on a data-driven architecture. This
means that UIMA components do not exchange or
share code, they rather exchange data only. The
components operate on common data referred to
as COMMON ANALYSIS SYSTEM (CAS)(Go?tz and
Suhre, 2004). The CAS contains the subject of anal-
ysis (document) and provides meta data in the form
of annotations. Analysis engines receive annotations
through a CAS and add new annotations to the CAS.
An annotation in the CAS then associates meta data
with a region the subject of the analysis occupies
7http://www.pubmed.gov
(e.g., the start and end positions in a document).
UIMA defines CAS interfaces for indexing, ac-
cessing and updating the CAS. CASes are modelled
independently from particular programming lan-
guages. However, JCAS, an object-oriented inter-
face to the CAS, was developed for JAVA. CASes are
crucial for the development and deployment of com-
plex NLP pipelines. All components to be integrated
in UIMA are characterized by abstract input/output
specifications, so-called capabilities. These speci-
fications are declared in terms of descriptors. The
components can be integrated by wrappers conform-
ing with the descriptors. For the integration task, we
define in advance what kind of data each component
may manipulate. This is achieved via the UIMA
annotation type system. This type system follows
the object-oriented paradigm. There are only two
kinds of data, viz. types and features. Features spec-
ify slots within a type, which either have primitive
values such as integers or strings, or have references
to instances of types in the CAS. Types, often called
feature structures, are arranged in an inheritance hi-
erarchy.
In the following section, we propose an ANNO-
TATION TYPE SYSTEM designed and implemented
for an UIMA Tool Suite that will become the back-
bone for our text mining applications. We distin-
guish between the design and implementation lev-
els, talking about the ANNOTATION SCHEME and
the TYPE SYSTEM, respectively.
4 Annotation Type System
The ANNOTATION SCHEME we propose currently
consists of five layers: Document Meta, Document
Structure & Style, Morpho-Syntax, Syntax and Se-
mantics. Accordingly, annotation types fall into five
corresponding categories. Document Meta and Doc-
ument Structure & Style contain annotations about
each document?s bibliography, organisation and lay-
out. Morpho-Syntax and Syntax describe the results
of morpho-syntactic and syntactic analysis of texts.
The results of lemmatisation, stemming and decom-
position of words can be represented at this layer, as
well. The annotations from shallow and full parsing
are represented at the Syntax layer. The appropri-
ate types permit the representation of dependency-
and constituency-based parsing results. Semantics
35
uima.tcas.Annotation
+begin: uima.cas.Integer
+end: uima.cas.Integer
Annotation
+componentId: uima.cas.String
+confidence: uima.cas.Double
Descriptor
pubmed.ManualDescriptor
+MeSHList: uma.cas.FSArray = MeSHHeading
+...
AutoDescriptor
+...
Header
+docType: uima.cas.String
+source: uima.cas.String
+docID: uima.cas.String
+language: uima.cas.String
+copyright: uima.cas.String
+authors: uima.cas.FSArray = AuthorInfo
+title: uima.cas.String
+pubTypeList: uima.cas.FSArray = PubType
+...
pubmed.Header
+citationStatus: uima.cas.String {...}
ManualDescriptor
+keywordList: uima.cas.FSArray = Keyword
+...
PubType
+name: uima.cas.Sting
Journal
+ISSN: uima.cas.String
+volume: uima.cas.String
+journalTitle: uima.cas.String
+impactFactor: uima.cas.String
Keyword
+name: uima.cas.String
+source: uima.cas.String
Token
+posTag: uima.cas.FSArray  = POSTag
+lemma: Lemma
+feats: GrammaticalFeats
+stemmedForm: StemmedForm
+depRelList: uima.cas.FSArray = DependencyRelation
+orthogr: uima.cas.FSArray = String
POSTag
+tagsetId: uima.cas.String
+language: uima.cas.String
+value: uima.cas.String
Lemma
+value: String
Acronym
Abbreviation
+expan: String
StemmedForm
+value: String
GrammaticalFeats
+language: uima.cas.String
DiscontinuousAnnotation
+value: FSArray = Annotation
PennPOSTag
NounFeats
+...
...
+...
...
+...
DependencyRelation
+head: Token
+projective: uima.cas.Boolean
+label: uima.cas.String
Relation
DepRelationSet...
Chunk
PhraseChunk
PTBConstituent
+formFuncDisc: uima.cas.String
+gramRole: uima.cas.String
+adv: uima.cas.String
+misc: uima.cas.String
+map: Constituent
+tpc: uima.cas.Boolean
+nullElement: uima.cas.String
+ref: Constituent
Constituent
+parent: Constituent
+head: Token
+cat: uima.cas.String
GENIAConstituent
+syn: uima.cas.String 
...
+...
...
+...
NP ...PP
Entity
+dbEntry: uima.cas.FSArray = DBEntry
+ontologyEntry: uima.cas.FSArray = OntologyEntry
+specificType: uima.cas.String
BioEntity
Cytokine
Organism VariationGene
...
LexiconEntry OntologyEntryDBEntry
ResourceEntry
+source: uima.cas.String
+entryId: uima.cas.String
+version: uima.cas.String
Zone 
Title TextBody Paragraph Figure
+caption: Caption
Section
+title: Title
+depth: uima.cas.Integer
Misc ... PersonOrganization
MUCEntity
...
2
3
4
5 6
1
CAS Core
Figure 1: Multi-Layered UIMA Annotation Scheme in UML Representation. 1: Basic Feature Structure and
Resource Linking. 2: Document Meta Information. 3: Morpho-Syntax. 4: Syntax. 5: Document Structure
& Style. 6: Semantics.
36
currently covers information about named entities,
events and relations between named entities.
4.1 Basic Feature Structure
All types referring to different linguistic lay-
ers derive from the basic type Annotation,
the root type in the scheme (cf. Figure 1-
1). The Annotation type itself derives infor-
mation from the default UIMA annotation type
uima.tcas.Annotation and, thus, inherits the
basic annotation features, viz. begin and end (mark-
ing spans of annotations in the subject of analysis).
Annotation extends this default feature structure
with additional features. The componentId marks
which NLP component actually computed this an-
notation. This attribute allows to manage multiple
annotations of the same type The unique linkage be-
tween an analysis component and an annotation item
is particularly relevant in cases of parallel annota-
tions. The component from which the annotation
originated also assigns a specific confidence score
to its confidence feature. Each type in the scheme is
at least supplied with these four slots inherited from
their common root type.
4.2 Document Meta Information
The Document Meta layer (cf. Figure 1-2) describes
the bibliographical and content information of a doc-
ument. The bibliographical information, often re-
trieved from the header of the analyzed document,
is represented in the type Header. The source
and docID attributes yield a unique identifier for
each document. We then adopted some Dublin Core
elements, e.g., language, title, docType. We dis-
tinguish between domain-independent information
such as language, title, document type and domain-
dependent information as relevant for text mining
in the bio-medical domain. Accordingly, the type
pubmed.Header was especially created for the
representation of PUBMED document information.
A more detailed description of the document?s pub-
lication data is available from types which specialize
PubType such as Journal. The latter contains
standard journal-specific attributes, e.g., ISSN, vol-
ume, journalTitle.
The description of the document?s content of-
ten comes with a list of keywords, informa-
tion assigned to the Descriptor type. We
clearly distinguish between content descriptors man-
ually provided by an author, indexer or cura-
tor, and items automatically generated by text
analysis components after document processing.
While the first kind of information will be stored
in the ManualDescriptor, the second one
will be represented in the AutoDescriptor.
The generation of domain-dependent descriptors is
also possible; currently the scheme contains the
pubmed.ManualDescriptor which allows to
assign attributes such as chemicals and genes.
4.3 Document Structure & Style
The Document Structure & Style layer (cf. Figure 1-
5) contains information about the organization and
layout of the analyzed documents. This layer en-
ables the marking-up of document structures such
as paragraphs, rhetorical zones, figures and tables,
as well as typographical information, such as italics
and special fonts. The focus of modeling this layer is
on the annotation of scientific documents, especially
in the life sciences. We adopted here the SCIXML8
annotation schema, which was especially developed
for marking-up scientific publications. The Zone
type refers to a distinct division of text and is the par-
ent type for various subtypes such as TextBody,
Title etc. While it seems impossible to predict all
of the potential formal text segments, we first looked
at types of text zones frequently occurring in sci-
entific documents. The type Section, e.g., repre-
sents a straightforward and fairly standard division
of scientific texts into introduction, methods and re-
sults sections. The divisions not covered by current
types can be annotated with Misc. The annotation
of tables and figures with corresponding types en-
ables to link text and additional non-textual infor-
mation, an issue which is gaining more and more
attention in the text mining field.
4.4 Morpho-Syntax
The Morpho-Syntax layer (cf. Figure 1-3) represents
the results of morpho-syntactic analysis such as to-
kenization, stemming, POS tagging. The small-
est annotation unit is Token which consists of five
attributes, including its part-of-speech information
8http://www.cl.cam.ac.uk/?aac10/
escience/sciborg.html
37
(posTag), stemmedForm, lemma, grammatical fea-
tures (feats), and orthographical information (or-
thogr).
With respect to already available POS tagsets,
the scheme allows corresponding extensions of
the supertype POSTag to, e.g., PennPOSTag
(for the Penn Tag Set (Marcus et al, 1993)) or
GeniaPOSTag (for the GENIA Tag Set (Ohta et
al., 2002)). The attribute tagsetId serves as a unique
identifier of the corresponding tagset. The value of
the POS tag (e.g., NN, VVD, CC) can be stored in
the attribute value. The potential values for the in-
stantiation of this attribute are always restricted to
the tags of the associated tagset. These constraints
enforce formal control on annotation processes.
As for morphologically normalized lexical items,
the Lemma type stores the canonical form of a lexi-
cal token which can be retrieved from a lexicon once
it is computed by a lemmatizer. The lemma value,
e.g., for the verb ?activates? would be ?activate?. The
StemmedForm represents a base form of a text to-
ken as produced by stemmers (e.g., ?activat-? for the
noun ?activation?).
Due to their excessive use in life science docu-
ments, abbreviations, acronyms and their expanded
forms have to be considered in terms of appropriate
types, as well. Accordingly, Abbreviation and
Acronym are defined, the latter one being a child
type of the first one. The expanded form of a short
one can easily be accessed from the attribute expan.
Grammatical features of tokens are represented
in those types which specialize the supertype
GrammaticalFeats. Its child types, viz.
NounFeats, VerbFeats, AdjectiveFeats,
PronounFeats (omitted from Figure 1-3) cover
the most important word categories. Attributes
of these types obviously reflect the properties
of particular grammatical categories. While
NounFeats comes with gender, case and num-
ber only, PronounFeats must be enhanced with
person. A more complex feature structure is asso-
ciated with VerbFeats which requires attributes
such as tense, person, number, voice and aspect. We
adapted here specifications from the TEI to allow
compatibility with other annotation schemata.
The type LexiconEntry (cf. Figure 1-1) en-
ables a link to the lexicon of choice. By designing
this type we achieve much needed flexibility in link-
ing text snaps (e.g., tokens, simplex forms, multi-
word terms) to external resources. The attributes
entryId and source yield, in combination, a unique
identifier of the current lexicon entry. Resource ver-
sion control is enabled through an attribute version.
Text annotations often mark disrupted text spans,
so-called discontinuous annotations. In coordinated
structures such as ?T and B cell?, the annotator
should mark two named entities, viz. ?T cell? and ?B
cell?, where the first one results from the combina-
tion of the disjoint parts ?T? and ?cell?. In order to
represent such discontinous annotations, we intro-
duced the type DiscontinuousAnnotation
(cf. Figure 1-1) which links through its attribute
value spans of annotations to an annotation unit.
4.5 Syntax
This layer of the scheme provides the types and at-
tributes for the representation of syntactic structures
of sentences (cf. Figure 1-4). The results from shal-
low and full parsing can be stored here.
Shallow parsing (chunking) aims at dividing
the flow of text into phrases (chunks) in a non-
overlapping and non-recursive manner. The type
Chunk accounts for different chunk tag sets by sub-
typing. Currently, the scheme supports Phrase-
Chunks with subtypes such as NP, VP, PP, or ADJP
(Marcus et al, 1993).
The scheme also reflects the most popular full
parsing approaches in NLP, viz. constituent-based
and dependency-based approaches. The results
from constituent-based parsing are represented in
a parse tree and can be stored as single nodes in
the Constituent type. The tree structure can
be reconstructed through links in the attribute par-
ent which stores the id of the parent constituent.
Besides the attribute parent, Constituent holds
the attributes cat which stores the complex syntac-
tic category of the current constituent (e.g., NP, VP),
and head which links to the head word of the con-
stituent. In order to account for multiple annota-
tions in the constituent-based approach, we intro-
duced corresponding constituent types which spe-
cialize Constituent. This parallels our approach
which we advocate for alternatives in POS tagging
and the management of alternative chunking results.
Currently, the scheme supports three differ-
ent constituent types, viz. PTBConstituent,
38
GENIAConstituent (Miyao and Tsujii, 2005)
and PennBIoIEConstituent. The attributes
of the type PTBConstituent cover the com-
plete repertoire of annotation items contained in
the Penn Treebank, such as functional tags for
form/function dicrepancies (formFuncDisc), gram-
matical role (gramRole), adverbials (adv) and mis-
cellaneous tags (misc). The representation of null
elements, topicalized elements and gaps with corre-
sponding references to the lexicalized elements in a
tree is reflected in attributes nullElement, tpc, map
and ref, respectively. GENIAConstituent and
PennBIoIEConstituent inherit from PTB-
Constituent all listed attributes and provide, in
the case of GENIAConstituent , an additional
attribute syn to specify the syntactic idiosyncrasy
(coordination) of constituents.
Dependency parsing results are directly linked to
the token level and are thus referenced in the Token
type. The DependencyRelation type inherits
from the general Relation type and introduces
additional features which are necessary for describ-
ing a syntactic dependency. The attribute label char-
acterizes the type of the analyzed dependency rela-
tion. The attribute head indicates the head of the
dependency relation attributed to the analyzed to-
ken. The attribute projective relates to the property
of the dependency relation whether it is projective
or not. As different dependency relation sets can be
used for parsing, we propose subtyping similar to
the constituency-based parsing approaches. In order
to account for alternative dependency relation sets,
we aggregate all possible annotations in the Token
type as a list (depRelList).
4.6 Semantics
The Semantics layer comprises currently the repre-
sentation of named entities, particularly for the bio-
medical domain. The entity types are hierarchically
organized. The supertype Entity (cf. Figure 1-
6) links annotated (named) entities to the ontologies
and databases through appropriate attributes, viz. on-
tologyEntry and sdbEntry. The attribute specific-
Type specifies the analyzed entity in a more detailed
way (e.g., Organism can be specified through
the species values ?human?, ?mouse?, ?rat?, etc.)
The subtypes are currently being developed in the
bio-medical domain and cover, e.g., genes, pro-
teins, organisms, diseases, variations. This hierar-
chy can easily be extended or supplemented with
entities from other domains. For illustration pur-
poses, we extended it here by MUC (Grishman
and Sundheim, 1996) entity types such as Person,
Organization, etc.
This scheme is still under construction and will
soon also incorporate the representation of relation-
ships between entities and domain-specific events.
The general type Relation will then be extended
with specific conceptual relations such as location,
part-of, etc. The representation of events will be
covered by a type which aggregates pre-defined re-
lations between entities and the event mention. An
event type such as InhibitionEventwould link
the text spans in the sentence ?protein A inhibits
protein B? in attributes agent (?protein A?), patient
(?protein B?), mention (?inhibits?).
5 Conclusion and Future work
In this paper, we introduced an UIMA annotation
type system which covers the core functionality
of morphological, syntactic and semantic analysis
components of a generic NLP system. It also in-
cludes type specifications which relate to the formal
document format and document style. Hence, the
design of this scheme allows the annotation of the
entire cycle of (sentence-level) NLP analysis (dis-
course phenomena still have to be covered).
The annotation scheme consists mostly of core
types which are designed in a domain-independent
way. Nevertheless, it can easily be extended with
types which fit other needs. The current scheme sup-
plies an extension for the bio-medical domain at the
document meta and structure level, as well as on the
semantic level. The morpho-syntactic and syntactic
levels provide types needed for the analysis of the
English language. Changes of attributes or attribute
value sets will lead to adaptations to other natural
languages.
We implemented the scheme as an UIMA type
system. The formal specifications are implemented
using the UIMA run-time environment. This direct
link of formal and implementational issues is a ma-
jor asset using UIMA unmatched by any previous
specification approach. Furthermore, all annotation
results can be converted to the XMI format within
39
the UIMA framework. XMI, the XML Metadata In-
terchange format, is an OMG9 standard for the XML
representation of object graphs.
The scheme also eases the representation of an-
notation results for the same task with alternative
and often competitive components. The identifica-
tion of the component which provided specific an-
notations can be retrieved from the attribute com-
ponentId. Furthermore, the annotation with alterna-
tive and multiple tag sets is supported as well. We
have designed for each tag set a type representing
the corresponding annotation parameters. The inher-
itance trees at almost all annotation layers support
the parallelism in annotation process (e.g., tagging
may proceed with different POS tagsets).
The user of the scheme can restrict the potential
values of the types or attributes. The current scheme
makes use of the customization capability for POS
tagsets, for all attributes of constituents and chunks.
This yields additional flexibility in the design and,
once specified, an increased potential for automatic
control for annotations.
The scheme also enables a straightforward con-
nection to external resources such as ontologies,
lexicons, and databases as evidenced by the corre-
sponding subtypes of ResourceEntry (cf. Figure
1-1). These types support the specification of a re-
lation between a concrete text span and the unique
item addressed in any of these resources.
With these considerations in mind, we strive for
the elaboration of a common standard UIMA type
system for NLP engines. The advantages of such a
standard include an easy exchange and integration
of different NLP analysis engines, the facilitation
of sophisticated evaluation studies (where, e.g., al-
ternative components for NLP tasks can be plugged
in and out at the spec level), and the reusability of
single NLP components developed in various labs.
Acknowledgments. This research was funded by the EC?s 6th Framework Programme
(4th call) within the BOOTStrep project under grant FP6-028099.
References
S. Brants and S. Hansen. 2002. Developments in the TIGER
annotation scheme and their realization in the corpus. In
Proc. of the 3rd LREC Conference, pages 1643?1649.
P. Buitelaar, T. Declerck, B. Sacaleanu, ?S. Vintar, D. Raileanu,
and C. Crispi. 2003. A multi-layered, XML-based approach
9http://www.omg.org
to the integration of linguistic and semantic annotations. In
Proc. of EACL 2003 Workshop NLPXML-03.
H. Cunningham. 2002. GATE, a general architecture for text
engineering. Computers and the Humanities, 36:223?254.
T. Declerck. 2006. SYNAF: Towards a standard for syntactic
annotation. In Proc. of the 5th LREC Conference.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program. In Proc. of the 4th LREC
Conference, pages 837?840.
D. Ferrucci and A. Lally. 2004. UIMA: an architectural ap-
proach to unstructured information processing in the corpo-
rate research environment. Natural Language Engineering,
10(3-4):327?348.
T. Go?tz and O. Suhre. 2004. Design and implementation of the
UIMA Common Analysis System. IBM Systems Journal,
43(3):476?489.
R. Grishman and B. Sundheim. 1996. Message Understand-
ing Conference ? 6: A brief history. In Proc. of the 16th
COLING, pages 466?471.
R. Grishman. 1997. Tipster architecture design document,
version 2.3. Technical report, Defense Advanced Research
Projects Agency (DARPA), U.S. Departement of Defense.
C. Grover, E. Klein, M. Lapata, and A. Lascarides. 2002.
XML-based NLP tools for analysing and annotating medi-
cal language. In Proc. of the 2nd Workshop NLPXML-2002,
pages 1?8.
U. Hahn and J. Wermter. 2006. Levels of natural language pro-
cessing for text mining. In S. Ananiadou and J. McNaught,
editors, Text Mining for Biology and Biomedicine, pages 13?
41. Artech House.
N. Ide, P. Bonhomme, and L. Romary. 2000. XCES: An XML-
based standard for linguistic corpora. In Proc. of the 2nd
LREC Conference, pages 825?830.
N. Ide, L. Romary, and E. de la Clergerie. 2003. International
standard for a linguistic annotation framework. In Proc. of
the HLT-NAACL 2003 SEALTS Workshop, pages 25?30.
C. Laprun, J. Fiscus, J. Garofolo, and S. Pajot. 2002. A prac-
tical introduction to ATLAS. In Proc. of the 3rd LREC Con-
ference, pages 1928?1932.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The PENN
TREEBANK. Computational Linguistics, 19(2):313?330.
A. Meyers. 2006. Annotation compatibility working group re-
port. In Proc. of the COLING-ACL 2006 Workshop FLAC
2006?, pages 38?53.
Y. Miyao and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proc. of the
ACL 2005, pages 83 ? 90.
T. Ohta, Y. Tateisi, and J.-D. Kim. 2002. The GENIA corpus:
An annotated research abstract corpus in molecular biology
domain. In Proc. of the 2nd HLT, pages 82?86.
E. Pianta, L. Bentivogli, C. Girardi, and B. Magnini. 2006.
Representing and accessing multilevel linguistic annotation
using the MEANING format. In Proc. of the 5th EACL-2006
Workshop NLPXML-2006, pages 77?80.
J. Rumbaugh, I. Jacobson, and G. Booch. 1999. The Unified
Modeling Language Reference Manual. Addison-Wesley.
U. Scha?fer. 2006. Middleware for creating and combining
multi-dimensional NLP markup. In Proc. of the 5th EACL-
2006 Workshop NLPXML-2006, pages 81?84.
40
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 761?768
Manchester, August 2008
Event Frame Extraction Based on a Gene Regulation Corpus 
Yutaka Sasaki 1    Paul Thompson 1    Philip Cotter 1    John McNaught 1, 2 
Sophia Ananiadou1, 2 
 
1 School of Computer Science, University of Manchester 
2  National Centre for Text Mining 
MIB, 131 Princess Street, Manchester, M1 7DN, United Kingdom 
Yutaka.Sasaki@manchester.ac.uk 
 
 Abstract 
This paper describes the supervised ac-
quisition of semantic event frames  based 
on a corpus of biomedical abstracts, in 
which the biological process of E. coli 
gene regulation has been linguistically 
annotated by a group of biologists in the 
EC research project "BOOTStrep". Gene 
regulation is one of the rapidly advancing 
areas for which information extraction 
could boost research. Event frames are an 
essential linguistic resource for extraction 
of information from biological literature.  
This paper presents a specification for 
linguistic-level annotation of gene regu-
lation events, followed by novel methods 
of automatic event frame extraction from 
text.  The event frame extraction per-
formance has been evaluated with 10-
fold cross validation.  The experimental 
results show that a precision of nearly 
50% and a recall of around 20% are 
achieved.  Since the goal of this paper is 
event frame extraction, rather than event 
instance extraction, the issue of low re-
call could be solved by applying the 
methods to a larger-scale corpus. 
1 Introduction 
This paper describes the automatic extraction of 
linguistic event frames based on a corpus of 
MEDLINE abstracts that has been annotated 
with gene regulation events by a group of do-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
main experts. Annotation is centred on both 
verbs and nominalised verbs that describe rele-
vant events. For each event, semantic arguments 
that occur within the same sentence are marked 
and labelled with semantic roles and named en-
tity (NE) types. 
The focus of the paper is the extraction of 
event frames on the basis of the annotated corpus 
using machine learning techniques. Event frames 
are linguistic specifications concerning the be-
haviour of verbs and nominalised verbs, in terms 
of the number and types of semantic arguments 
with which they typically co-occur in texts. Our 
eventual goal is to exploit such information to 
improve information extraction. Event frame ex-
traction is different to event instance extraction 
(or template filling). Our event frames are des-
tined for incorporation in the BOOTStrep 
BioLexicon to support identification of relevant 
event instances and  discovery of event instance 
participants by NLP systems. 
2 Background 
There are several well-established, large-scale 
repositories of semantic frames for general lan-
guage, e.g., VerbNet (Kipper-Schuler, 2005), 
PropBank (Palmer et al, 2005) and FrameNet 
(Rupenhoffer et al 2006). These all aim to char-
acterise verb behaviour in terms of the semantic 
arguments with which verbs occur but differ in 
how they represent semantic arguments and 
groupings of verbs.  
In VerbNet, the semantic roles of arguments 
come from frame-independent roles, e.g. Agent, 
Patient, Location and Instrument.  
In contrast, PropBank and FrameNet use a 
mixture of role types: some are common amongst 
a number of frames; others are specific to par-
ticular frames.  
Whilst FrameNet and VerbNet differ in their 
treatment of semantic roles, they both specify  
761
semantic frames that correspond to groups of 
verbs with similar behaviour. However, frames 
in PropBank correspond to individual verbs. 
   Biology-specific extensions have been at-
tempted both for PropBank (Wattarujeekrit et al, 
2004) and FrameNet (Dolbey et al, 2006). How-
ever, to our knowledge, there has been no such 
attempt at extending VerbNet into the biological 
domain. 
In common with VerbNet, our work is focus-
sed on producing event frames that use a set of 
frame-independent semantic roles. However, we 
adopt a smaller set of roles tailored to the domain. 
This use of frame-independent roles allows lin-
guistic generalisations to be captured more easily 
(Cohen and Hunter, 2006). Also, the use of such 
roles is more suitable for direct exploitation by 
NLP systems (Zaphirain et al, 2008).  
Unlike VerbNet, we aim to produce a set of 
frames that are verb-specific (rather than frames 
that apply to groups of verbs). Verb-specific 
frames are able to provide more detailed argu-
ment specifications?particularly important in 
the biomedical field, where phrases that identify 
information such as location, manner, timing and 
condition are essential for correct interpretation 
of events (Tsai et al 2007).  
3 Annotated corpus 
To aid semantic event frame extraction, we need 
a corpus annotated with event-level information.  
Several already exist for biology.  Some target 
extraction of PropBank-style frames (e.g. Chou 
et al (2006), Kulick et al (2004)). The corpus 
produced by Kim et al (2008) uses frame-
independent roles. However, only a few semantic 
argument types are annotated.  
The target of our event frame extraction is a 
set of semantic frames which specify all potential 
arguments of gene regulation events. For this 
purpose, we had to produce our own annotated 
corpus, using a larger set of event-independent 
semantic roles than Kim et al (2008). Our roles 
had to cover sufficiently wide scope to allow an-
notation and characterization of all instantiated 
arguments of relevant events within texts. To our 
knowledge, this makes our scheme unique within 
the biomedical field. 
In contrast to many other comparable re-
sources, annotated events are centred on both 
verbs and nominalised verbs, such as transcrip-
tion and control. Nominalised verbs play an im-
portant and possibly dominant role in biological 
texts (Cohen and Hunter, 2006). Our own corpus 
confirms this, in that the nominalised verb ex-
pression is the most commonly annotated word 
on which gene regulation events are centred. By 
annotating events centred on nominalised verbs 
in a similar way to verbs, it becomes possible to 
extract separate event frames for nominalised 
verbs. This enables their potentially idiosyncratic 
behaviour to be accounted for.  
Role Name Description Example (bold = semantic argument, italics = focussed verb)  
AGENT Drives/instigates event The narL gene product activates the nitrate reductase operon 
THEME a) Affected by/results from event 
b) Focus of events describing states 
recA protein was induced by UV radiation 
The FNR protein resembles CRP 
MANNER Method/way in which event is car-
ried out 
cpxA gene increases the levels of csgA transcription by dephosphoryla-
tion of CpxR 
INSTRUMENT Used to carry out event EnvZ functions through OmpR to control NP porin gene expression in 
Escherichia coli K-12. 
LOCATION Where complete event takes place Phosphorylation of OmpR modulates expression of the ompF and ompC 
genes in Escherichia coli 
SOURCE Start point of event A transducing lambda phage was isolated from a strain harboring a 
glpD??lacZ fusion  
DESTINATION End point of event Transcription of gntT is activated by binding of the cyclic AMP (cAMP)-
cAMP receptor protein (CRP) complex to a CRP binding site 
TEMPORAL Situates event in time w.r.t another 
event 
The Alp protease activity is detected in cells after introduction of plas-
mids carrying the alpA gene 
CONDITION Environmental conditions/changes 
in conditions 
Strains carrying a mutation in the crp structural gene fail to repress ODC 
and ADC activities in response to increased cAMP 
RATE Change of level or rate marR mutations elevated inaA expression by  10-  to 20-fold over that of 
the wild-type. 
DESCRIPTIVE-
AGENT 
Provides descriptive information 
about the AGENT of the event 
It is likely that HyfR acts as a formate-dependent regulator of the hyf 
operon 
DESCRIPTIVE-
THEME 
Provides descriptive information 
about the AGENT of the event 
The FNR protein resembles CRP. 
PURPOSE Purpose/reason for the event occur-
ring 
The fusion strains were used to study the regulation of the cysB gene by 
assaying the fused lacZ gene product 
Table 1. Semantic Roles 
762
Our annotated corpus consists of 677 MED-
LINE abstracts on E. Coli. Within them, a total 
of 4770 gene regulation events have been anno-
tated. 
3.1 Semantic Roles 
Based on the observations of Tsai et al(2007) 
regarding the most important types of informa-
tion specified for biomedical events, together 
with detailed examination of a large number of 
relevant events within our corpus, in discussion 
with biologists, we defined a set of 13 frame-
independent semantic roles that are suitable for 
the domain.   
 Certain roles within the set are domain-
independent, and are based on those used in 
VerbNet, e.g. AGENT, THEME, and LOCA-
TION. To these, we have added a number of do-
main-dependent roles, e.g. CONDITION and 
MANNER. The size of the role set attempts to 
balance the need for a sufficiently wide-ranging 
set of roles with the need for one that is as small 
and general as possible, to reduce the burden on 
annotators, whilst also helping to ensure consis-
tency across extracted verb frames. The full set 
of semantic roles used is shown in Table 1.  
3.2  Named Entity Categorisation 
 Although our semantic roles are rather general, 
the annotation scheme allows more detailed in-
formation about semantic arguments to be en-
coded in the corpus through the assignment of 
named entity (NE) tags. Unlike other corpus pro-
jects, we do not annotate all entities within each 
abstract, but just those entities that occur as se-
mantic arguments of annotated gene regulation 
events. 
Our set of NE tags goes beyond the traditional 
view of NEs,  in that labelling is extended to in-
clude events represented by nominalised verbs 
(e.g. repression). A total of 61 NE classes have 
been defined as being relevant to the gene regu-
lation field, which are divided into four entity-
specific super-classes (DNA, PROTEIN, EX-
PERIMENTAL and ORGANISMS) and one 
event-specific super-class (PROCESSES). The 
NEs within each of these classes are hierarchi-
cally-structured. Table 2 provides definitions of 
each of these five super-classes. The NEs corre-
spond to classes in the Gene Regulation Ontol-
ogy (Splendiani et al 2007), which has been de-
veloped as part of the BOOTStrep project in 
which this work has been carried out. The Gene 
Regulation Ontology integrates parts of other 
established bio-ontologies, such as Gene Ontol-
ogy (Ashburner et al, 2000) and Sequence On-
tology (Eilbeck,2005). 
3.3 Annotation process 
Annotation was carried out over a period of three 
months by seven PhD students with experience 
in gene regulation and with native or near-native 
competence in English. 
 Prior to annotation, each abstract was auto-
matically processed. Firstly, linguistic pre-
processing (i.e. morphological analysis, POS 
tagging and syntactic chunking)1 was carried out.  
 Secondly, all occurrences from a list of 700 
biologically relevant verbs were automatically 
marked. Annotators then considered each marked 
verb within an abstract. If the verb denoted a 
gene regulation event, annotators then: 
a. Identified all semantic arguments of the 
verb within the sentence 
b. Assigned a semantic role to each identi-
fied argument 
                                                 
1 Each abstract to be annotated is first pre-processed with 
the GENIA tagger (Tsuruoka et al 2005). 
NE class Definition 
DNA 
Entities chiefly composed of nucleic 
acids and their structural or positional 
references. This includes the physical 
structure of all DNA-based entities 
and the functional roles associated 
with regions thereof. 
PROTEIN 
Entities chiefly composed of amino 
acids and their positional references. 
This includes the physical structure 
and functional roles associated with 
each type. 
EXPERIMENTAL 
Both physical and methodological 
entities, either used, consumed or 
required for a reaction to take place. 
ORGANISMS 
Entities representing individuals or 
collections of living things and their 
component parts. 
PROCESSES A set of event classes used to label biological processes described in text.  
Table 2. Description of NE super-classes  Table 3. Most commonly annotated verbs and 
nominalised verbs 
Word Count Type 
expression 409 NV 
encode 351 V 
transcription 125 NV 
bind 110 V 
require 100 V 
express 93 V 
regulate 91 V 
synthesis 90 NV 
contain 80 V 
induce 78 V 
763
c. If appropriate, assigned named entity 
categories to (parts of) the semantic ar-
gument span 
d. If the argument corresponded to a nomi-
nalised verb, repeated steps a?c to iden-
tify its own arguments. 
Syntactic chunks were made visible to annota-
tors. In conjunction with annotation guidelines, 
the chunks were used to help ensure consistency 
of annotated semantic arguments. For example, 
the guidelines state that semantic arguments 
should normally consist of complete (and pref-
erably single) syntactic chunks.  The annotation 
was performed using a customised version of 
WordFreak (Morton and LaCivita, 2003), a Java-
based linguistic annotation tool.  
3.4  Corpus statistics 
The corpus is divided into 2 parts, i.e. 
1) 597 abstracts, each annotated by a single 
annotator, containing a total of 3612 
events, 
2) 80 pairs of double-annotated documents, 
allowing checking of inter-annotator 
agreement and consistency, and contain-
ing 1158 distinct events.  
 
 In the corpus, 277 distinct verbs were annotated 
as denoting gene regulation events, of which 73 
were annotated 10 times or more. In addition, 
annotation has identified 135 relevant nominal-
ised verbs, of which 22 were annotated 10 times 
or more. The most commonly annotated verbs 
and nominalised verbs are shown in Table 3.  
3.5 Inter-annotator agreement 
Inter-annotator agreement statistics for the 80 
pairs of duplicate-annotated abstracts are shown 
in Table 4.  
The figures shown in Table 4 are direct 
agreement rates. Whilst the Kappa statistic is 
very familiar for calculating inter-annotator 
agreement, we follow Wilbur et al (2006) and 
Pyysalo (2007) in choosing not to use it, because 
it is not appropriate or possible to calculate it for 
all of the above statistics. For instance: 
 
1. For some tasks, like annotation of events and 
arguments spans, deciding how to calculate 
random agreement is not clear. 
2. The Kappa statistic assumes that annotation 
categories are discrete and mutually exclu-
sive. This is not the case for the NE catego-
ries, which are hierarchically structured.   
 
 Table 4 shows that, in terms of identifying 
events  (i.e. determining which verbs denote gene 
regulation events), agreement between annotators 
is reached about half the time. The main reason 
for this relatively low figure is that reaching a 
consensus on the specific types of events to be 
annotated under the heading of ?gene regulation? 
required a large amount of discussion. Thus, par-
ticularly towards the start of the annotation phase, 
annotators tended to either under- or over-
annotate the events. 
Greater amounts of consistency seem to be 
achievable for other sub-tasks of the annotation, 
with agreement rates for the identification and 
subsequent labelling of semantic arguments be-
ing achieved in around three quarters of cases.  
Comparable, but slightly lower rates of agree-
ment were achieved in the identification of NEs. 
In terms of assigning categories to them, the 
agreement rate for exact category matches is a 
little lower (62%). However, if we relax the 
matching conditions by exploiting the hierarchi-
cal structure of the NE categories (i.e. if we 
count as a match the cases where the category 
assigned by one annotator was the ancestor of the 
category assigned by the other annotator), then 
the agreement increases by around 11%.  
The large number of NE categories (61), 
makes the decision of the most appropriate cate-
gory rather complex; this was verified by the an-
notators themselves. Based on this, we will con-
sider the use of a more coarse-grained scheme 
when carrying out further annotation of this type. 
However, in the current corpus, the hierarchical 
structuring of the NE categories means that it 
would be possible to use a smaller set of catego-
ries by mapping the specific categories to more 
general ones.   
4 Corpus Format 
For the purposes of event frame extraction, the 
annotations in the corpus were converted to an 
XML-style inline format consisting of three dif-
ferent types of element: 
 
Table 4. Inter-annotator agreement rates  
AGREEMENT RATE VALUE 
Event identification 0.49 
Argument identification (partial span match) 0.73 
Semantic role assignment 0.78 
NE identification (partial span match) 0.68 
NE category assignment (exact) 0.62 
NE category assignment (including parent) 0.65 
NE category assignment (including ancestors) 0.73 
  
764
EVENT ? surrounds text spans (i.e. verb 
phrases and nominalised verbs) on which 
events are centred. 
SLOT ? surrounds spans corresponding to se-
mantic arguments (i.e. slots) of events.  The 
head verb/nominalised verb of the event is also 
treated as a SLOT, with role type Verb. The 
eventid attribute links each slot with its respec-
tive event, whilst the Role attribute indicates 
the semantic role assigned to the slot.  
NE ? surrounds text spans annotated as named 
entities. The cat attribute stores the NE cate-
gory assigned. 
 
Where there are several annotations over some 
text span, elements are embedded inside each 
other. If more than one annotation begins at a 
particular offset, then the ordering of the embed-
ding is fixed, so that SLOT elements are embed-
ded inside EVENT elements, and that NE ele-
ments are embedded inside SLOT elements. An 
example of the annotation for the sentence "TaqI 
restriction endonuclease has been subcloned 
downstream from an inducible phoA promoter" 
is shown below: 
 
<SLOT argid="4" eventid="5" Role="Theme">  
<NE cat="ENZYME">TaqI restriction endonucle-
ase</NE></SLOT> <EVENT id="5"> 
has been <SLOT argid="6" eventid="5" 
Role="Verb">subcloned </SLOT></EVENT>  
<SLOT argid="8" eventid="5" 
Role="Location">downstream from  
<NE cat="PROMOTER">an inducible phoA pro-
moter</NE></SLOT>. 
 
The EVENT created over the VP chunk has 
been subcloned has been annotated as having 2 
semantic arguments (SLOTs), i.e. a THEME,  
TaqI restriction endonuclease and a LOCATION, 
i.e. downstream from an inducible phoA pro-
moter. A 3rd SLOT element corresponds to the 
head verb in the VP chunk. Named entity tags 
have also been assigned to the THEME span and 
part of the LOCATION span.  
5 Event Patterns and Event Frames 
This section defines event patterns and event 
frames.  Event patterns are syntactic patterns of 
sequences of surface words, NEs, and semantic 
roles, whilst event frames are the record-like data 
structures consisting of event slots and event slot 
values. 
5.1 Event Patterns 
Event patterns are fragments of event annotations 
in which semantic arguments are generalized to 
their semantic role and NE categories, if present. 
An event pattern is extracted for each unique 
event id within an abstract. An event annotation 
span begins with the earliest SLOT span, and 
ends with the latest SLOT assigned to the event. 
An example event span is as follows: 
 
<SLOT eventid="9" Role="Agent">  
<NE cat="OPERON"> transfer operon</NE></SLOT> 
<EVENT id="9"><SLOT eventid="9" Role="Verb"> 
expression </SLOT></EVENT></SLOT> of  
<SLOT eventid="9" Role="Theme">  
<NE cat="DNA_FRAGMENT"> F-like plasmids 
</NE></SLOT> 
 
For each event, each event span is generalized 
into an event pattern as follows:  
? ?Verb? role slots of the event are converted 
into a tuple consisting of the role type, part-
of-speech and surface form, i.e., 
[Verb:POS:verb].  
? Other semantic role slots and their NE slots 
for the event are generalized to tuples con-
sisting of the role and NE super class, i.e., 
[role:NE_super_class]. 
? Other XML tags are removed. 
 
The above example event span is thus general-
ized to the following event pattern: 
 
[Agent:DNA] [Verb:NN:expression] of [Theme:DNA]. 
 
5.2 Event frames 
Event frames are directly extracted from event 
patterns, and take the following general form: 
 
event_frame_name( 
     slot_name => slot_value, 
     ? 
     slot_name => slot_value). 
where 
? event_frame_name is the base form of the 
event verb or nominalized verb; 
? slot_names are  the names of the semantic 
roles within the event pattern; 
? slot_values are NE categories, if present 
within the event pattern. 
 
For example, the event frame corresponding to 
the event pattern shown in the previous section is 
as follows: 
expression( Agent=>DNA, 
            Theme=>DNA ). 
 
765
6 Event Frame Extraction 
Our event frame extraction is a fusion of sequen-
tial labelling based on Conditional Random 
Fields (CRF), and event pattern matching. Event 
frames are extracted in three steps.  Firstly, a 
CRF-based Named Entity Recognizer (NER) 
assigns biological NEs to word sequences. Sec-
ondly, a CRF-based semantic role labeller deter-
mines the semantic roles of word sequences with 
NE labels.  Thirdly, word sequences are com-
pared with event patterns derived from the cor-
pus.  Only those event frames whose semantic 
roles, NEs, and verb POS satisfy event pattern 
conditions will be extracted. 
6.1 Biological NER  
Since it is costly and time-consuming to create a 
large-scale training corpus annotated by biolo-
gists, we need to concede to use coarse-grained 
biological NE categories. That is, the NER com-
ponent is trained on the five NE super classes, 
i.e., Protein, DNA, Experimental, Organisms, 
and Processes. 
The NER models are trained by CRFs 
(Lafferty et al, 2001) using the standard IOB2 
labelling method.  That is, the label ``B-NE'' is 
given to the first token of the target NE sequence, 
?I-NE? to each remaining token in the target se-
quence,  and ``O'' to other tokens. 
Features used are as follows: 
? word feature 
- orthographic features: 
 the first letter and the last four letters of the 
word form, in which capital letters in a word are 
normalized to ?A?, lower case letters are normal-
ized to ?a?, and digits are replaced by ?0?. For 
example, the word form ?IL-2? is normalised to 
?AA-0?. 
- postfix features:  the last two and four let-
ters 
? POS feature 
 
We applied first-order CRFs using the above fea-
tures for the tokens within a window size of  ?2 
of the current token. 
6.2 Semantic Role Labelling  
First of all, each NE token sequence identified by 
B and I labels is merged into a single token with 
the NE category name. Then, the semantic role 
labelling models are trained by CRFs in a similar 
way to NER.  That is, the label ``B-Role'' is given 
to the first token of the target Role sequence, ?I-
Role? to each remaining token in the target se-
quence, and ?O? to other tokens. 
Features used here are as follows: 
? word feature 
?  base form feature 
? POS feature 
? NE feature 
 
The window size was ?2 of the current token. 
6.3 Event pattern matching  
When a new sentence is given, sequential label-
ling models decide NE and semantic role labels 
of tokenized input sentences. Then, the token 
sequences are converted into the following token 
sequences with POS, semantic role, and NE in-
formation (called augmented token sequences): 
 
1. Each token sequence labelled by IOB seman-
tic role labels is merged into a token labelled 
with the role. 
2. Verbs and nominalized verbs are converted 
to [Verb:POS:surface_form]. 
3. Tokens with semantic role label and NE su-
per-class are converted into the form 
[Role:NE_super_class]. 
4. Other tokens with O label are converted to 
surface tokens. 
 
Then, event patterns are generalized: 
5. Event patterns are modified so that elements 
corresponding to verbs and nominalized 
verbs will match any words with the same 
POS, e.g., [Verb:POS:*]. 
 
Finally, each event pattern is applied to aug-
mented token sequences one by one:  
6. By matching the generalized event patterns 
with augmented token sequences, i.e. when 
verbs or nominalized verbs and the surround-
ing semantic roles and NEs satisfy the event 
pattern conditions, then successfully unified 
event patterns are extracted as new event pat-
terns. 
7. The newly obtained event patterns are con-
verted into event frames in the same way as 
described in Section 5.2.  
7 Experimental Results 
The aim of this section is to evaluate semantic 
frame extraction performance, given a set of an-
notated training data. 
The annotated corpus was randomly separated 
into 10 document groups and their event patterns 
766
and event frames were segmented into 10 groups 
according to the document separation. 
We conducted 10-fold cross validation based 
on the 10 document groups.  Named entity rec-
ognizers and semantic role labellers were trained 
using 9 groups of annotated documents.  Event 
frames were then extracted from the remaining 
group of documents.  Micro-average precision 
and recall for the set of event frames extracted 
from all the folds were evaluated. 
Table 5 shows the event frame extraction per-
formance.  #TP, #FN, and #FP indicate the num-
ber of true positives, false negatives, and false 
positives, respectively.   
Named entity recognition performance was 
also evaluated (Table 6).  Since the training data 
size is small, the performance is between ap-
proximately 20-60% F-measure. However, this 
will not cause a problem for the event frame ex-
traction task.  This is because, if a particular 
event frame occurs multiple times in a corpus, it 
is sufficient to extract only a single occurrence of 
the event description. So, whilst the NE and se-
mantic role labelling may not be successful for 
all occurrences of the event frame, there is a 
good chance that at least one occurrence of the 
event will be realized in the text in such a way as 
to allow the labelling to be carried out success-
fully, thus allowing the extraction of an appro-
priate event frame.  
8 Discussion 
Linguistic-level event annotation of biological 
events is an inherently difficult task.  This is 
supported by the fact that the inter-annotator 
agreement level for the identification of events 
was 0.49 (see Table 4).  Therefore, in terms of 
event extraction performance, a precision of 
49.0% on 10-fold cross validation is almost 
comparable to human experts. The low recall of 
18.6% may not be an issue, as the recall is likely 
to improve with the size of the target corpus.   
The precision may additionally be underesti-
mated in the evaluation due to inconsistencies in 
the annotation.  We found that the average preci-
sion of our event frame extraction over 10 folds 
is around 30%, despite the fact that the precision 
of all event frames extracted from 10 folds is 
almost 50% compared with the annotated event 
frames in the whole corpus.  This happens be-
cause some events not annotated in a particular 
fold are annotated in the rest of corpus.  From 
this insight, our conjecture is that the true preci-
sion against the whole corpus would be some-
what higher (potentially 70-80%) if we were us-
ing an annotated corpus 10 times larger for the 
evaluation. 
The automatic NER performance was also 
comparable to human annotators. 
There are several approaches to the generation 
of information extraction patterns (e.g. Soderland 
et al, 1995; Califf et al, 1997; Kim and Moldo-
van, 1995).  Our event patterns are similar to in-
formation extraction rules used in conventional 
IE systems.  However, the goal of this paper is 
not event instance extraction but event (or se-
mantic) frame extraction. We also combined 
CRF-based NER and semantic role labelling 
tuned for gene regulation with event extraction 
from sentences so that the clues of gene regula-
tion event frames could be assigned automati-
cally to un-annotated text. 
9 Conclusion  
This paper has presented linguistic annotation of 
gene regulation events in MEDLINE abstracts, 
and automatic event frame extraction based on 
the annotated corpus. Semantic event frames are 
linguistic resources effective in bridging between 
domain knowledge and text in IE tasks. 
Although biological event annotations carried 
out by domain experts is a challenging task, ex-
perimental results on event frame extraction 
demonstrate a precision of almost 50%, which is 
close to the inter-annotator agreement rate of 
human annotators. 
The extracted event frames will be included in 
the BOOTStrep BioLexicon, which will be made 
available for research purposes. 
Acknowledgement 
This research is supported by EC IST project 
FP6-028099 (BOOTStrep), whose Manchester 
team is hosted by the JISC/BBSRC/EPSRC 
sponsored National Centre for Text Mining. 
 
Table 5. 10-fold cross validation results 
 Score #TP #FN #FP 
Recall  0.186 165 730  
Precision 0.490 165  172 
 
Table 6.  NE identification performance 
NE Type Recall Precision F 
DNA 0.627  0.660  0.643  
Protein 0.525  0.633  0.574  
Experimental 0.224  0.512  0.312  
Processes 0.125  0.337  0.182  
Organisms 0.412  0.599  0.488  
 
767
References 
Califf, Mary E. and Raymond J. Mooney (1997).  
Relational Learning of Pattern-Match Rules for In-
formation Extraction, In Proceedings of the ACL-
97 Workshop in Natural Language Learning, pp 9?
15. 
Chou, Wen-Chi., Richard T.H. Tsai, Ying-Shan Su, 
Wei Ku, Ting-Yi Sung and Wen-Lian Hsu (2006). 
A Semi-Automatic Method for Annotating a Bio-
medical Proposition Bank. In Proceedings of the 
Workshop on Frontiers in Linguistically Annotated 
Corpora 2006, pp 5?12. 
Cohen, K. Bretonnel and Laurence Hunter (2006). A 
critical review of PASBio's argument structures for 
biomedical verbs. BMC Bioinformatics 7 (Suppl. 3), 
S5.  
Dolbey, Andrew, Michael Ellsworth and Jan 
Scheffczykx (2006). BioFrameNet: A Domain-
Specific FrameNet Extension with Links to Bio-
medical Ontologies. In O. Bodenreider (Ed.), In 
Proceedings of KR-MED, pp 87?94. 
Eilbeck, Karen, Suzanna .E Lewis., Christopher J. 
Mungall, Mark Yandell, Lincoln Stein, Richard 
Durbin and Michael Ashburner. (2005) The Se-
quence Ontology: A tool for the unification of ge-
nome annotations. Genome Biology 6:R44 
Kim, Jin-Dong,  Tomoko Ohta and Jun?ichi Tsujii 
(2008).  Corpus annotation for mining biomedical 
events from literature. BMC Bioinformatics 9:10.   
Kim, Jun-Tae and Dan I. Moldovan (1995).  Acquisi-
tion of Linguistic Patterns for Knowledge-Based 
Information Extraction. IEEE Transaction on 
Knowledge and Data Engineering (IEEE TKDE), 
7(5), pp.713?724.   
Kipper-Schuler, Karen (2005). VerbNet: A broad-
coverage, comprehensive verb lexicon. PhD Thesis. 
Computer and Information Science Dept., Univer-
sity of Pennsylvania. Philadelphia, PA. 
Kulick Seth, Ann Bies, Mark Liberman, Mark Mandel,  
Ryan McDonald, Martha Palmer, Andrew Schein, 
and Lyle Ungar  (2004) Integrated Annotation for 
Biomedical Information Extraction. In HLT-
NAACL 2004 Workshop: BioLink 2004, Linking 
Biological Literature, Ontologies and Databases, 
pp 61?68.  
Lafferty John, Andrew McCallum and Fernando 
Pereira (2001).  Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labelling Se-
quence Data. In Proceedings of the Eighteenth In-
ternational Conference on    Machine Learning 
(ICML-2001), pp 282?289.  
Morton, Thomas and Jeremy LaCivita (2003). Word-
Freak: an open tool for linguistic annotation. In 
Proceedings of the 2003 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology, 
pp 17?18. 
Palmer Martha, Paul Kingsbury and Daniel Gildea 
(2005). The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics, 
31(1), pp 71?106. 
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari 
Bj?rne, Jorma Boberg, Jouni J?rvinen and  Tapio 
Salakoski (2007). BioInfer: a corpus for informa-
tion extraction in the biomedical domain?.  BMC 
Bioinformatics 8:50. 
Ruppenhofer, Josef, Michael Ellsworth, Miriam R.L. 
Petruck, Christopher R. Johnson, and Jan  
Scheffczyk (2006).   FrameNet II: Extended The-
ory and Practice. Available online at 
http://framenet.icsi.berkeley.edu/ 
Soderland, Steven, David Fisher, Jonathan Aseltine 
and  Wendy Lenert (1995). CRYSTAL: Inducing a 
Conceptual Dictionary, In Proceedings of The 13th 
International Joint Conference on Artificial Intelli-
gence (IJCAI-95). pp.1314?1319. 
The Gene Ontology Consortium. (2000). Gene Ontol-
ogy: tool for the unification of biology. Nature Ge-
netetics 25, pp 25?29. 
Tsai Richard T.H, Wen-Chi Chou, Ying-San Su, Yu-
Chun Lin, Chen-Lung Sung, Hong-Jie Dai, Irene 
T.H Yeh, Wei Ku, Ting-Yi Sung and Wen-Lian 
Hsu (2007). BIOSMILE: A semantic role labeling 
system for biomedical verbs using a maximum-
entropy model with automatically generated tem-
plate features, BMC Bioinformatics 8:325  
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun?ichi Tsujii (2005). Developing a Robust 
Part-of-Speech Tagger for Biomedical Text, In Ad-
vances in Informatics - 10th Panhellenic Confer-
ence on Informatics, pp 382?392. 
Wattarujeekrit, Tuangthong, Parantu K. Shah and 
Nigel Collier (2004). PASBio: predicate-argument 
structures for event extraction in molecular biology, 
BMC Bioinformatics 5:155. 
Wilbur, W.John, Andrey Rzhetsky, and Hagit Shatkay 
(2006). New Directions in Biomedical Text Anno-
tations: Definitions. Guidelines and Corpus Con-
struction. BMC Bioinformatics. 7:356 
Zapirain, Be?at, Eneko Agirre, Llu?s M?rquez (2008). 
A Preliminary Study on the Robustness and Generali-
zation of Role Sets for Semantic Role Labeling. In 
Alexander F. Gelbukh (Ed.), Computational Linguis-
tics and Intelligent Text Processing, 9th International 
Conference, CICLing 2008. 
 
768
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 63?70,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
How to Make the Most of NE Dictionaries in Statistical NER
Yutaka Sasaki2 Yoshimasa Tsuruoka2 John McNaught1,2 Sophia Ananiadou1,2
1 National Centre for Text Mining
2 School of Computer Science, University of Manchester
MIB, 131 Princess Street, Manchester, M1 7DN, UK
Abstract
When term ambiguity and variability are very
high, dictionary-based Named Entity Recogni-
tion (NER) is not an ideal solution even though
large-scale terminological resources are avail-
able. Many researches on statistical NER have
tried to cope with these problems. However,
it is not straightforward how to exploit exist-
ing and additional Named Entity (NE) dictio-
naries in statistical NER. Presumably, addi-
tion of NEs to an NE dictionary leads to bet-
ter performance. However, in reality, the re-
training of NER models is required to achieve
this. We have established a novel way to im-
prove the NER performance by addition of
NEs to an NE dictionary without retraining.
We chose protein name recognition as a case
study because it most suffers the problems re-
lated to heavy term variation and ambiguity.
In our approach, first, known NEs are identi-
fied in parallel with Part-of-Speech (POS) tag-
ging based on a general word dictionary and
an NE dictionary. Then, statistical NER is
trained on the tagger outputs with correct NE
labels attached. We evaluated performance of
our NER on the standard JNLPBA-2004 data
set. The F-score on the test set has been im-
proved from 73.14 to 73.78 after adding the
protein names appearing in the training data to
the POS tagger dictionary without any model
retraining. The performance further increased
to 78.72 after enriching the tagging dictionary
with test set protein names. Our approach
has demonstrated high performance in pro-
tein name recognition, which indicates how
to make the most of known NEs in statistical
NER.
1 Introduction
The accumulation of online biomedical informa-
tion has been growing at a rapid pace, mainly at-
tributed to a rapid growth of a wide range of repos-
itories of biomedical data and literature. The auto-
matic construction and update of scientific knowl-
edge bases is a major research topic in Bioinformat-
ics. One way of populating these knowledge bases
is through named entity recognition (NER). Unfortu-
nately, biomedical NER faces many problems, e.g.,
protein names are extremely difficult to recognize
due to ambiguity, complexity and variability. A fur-
ther problem in protein name recognition arises at
the tokenization stage. Some protein names include
punctuation or special symbols, which may cause to-
kenization to lose some word concatenation infor-
mation in the original sentence. For example, IL-2
and IL - 2 fall into the same token sequence IL
- 2 as usually dash (or hyphen) is designated as a
token delimiter.
Research into NER is centred around three ap-
proaches: dictionary-based, rule-based and machine
learning-based approaches. To overcome the usual
NER pitfalls, we have opted for a hybrid approach
combining dictionary-based and machine learning
approaches, which we call dictionary-based statisti-
cal NER approach. After identifying protein names
in text, we link these to semantic identifiers, such as
UniProt accession numbers. In this paper, we focus
on the evaluation of our dictionary-based statistical
NER.
2 Methods
Our dictionary-based statistical approach consists of
two components: dictionary-based POS/PROTEIN
tagging and statistical sequential labelling. First,
63
dictionary-based POS/PROTEIN tagging finds can-
didates for protein names using a dictionary. The
dictionary maps strings to parts of speech (POS),
where the POS tagset is augmented with a tag
NN-PROTEIN. Then, sequential labelling applies
to reduce false positives and false negatives in the
POS/PROTEIN tagging results. Expandability is
supported through allowing a user of the NER tool to
improve NER coverage by adding entries to the dic-
tionary. In our approach, retraining is not required
after dictionary enrichment.
Recently, Conditional Random Fields (CRFs)
have been successfully applied to sequence labelling
problems, such as POS tagging and NER, and have
outperformed other machine learning techniques.
The main idea of CRFs is to estimate a conditional
probability distribution over label sequences, rather
than over local directed label sequences as with Hid-
den Markov Models (Baum and Petrie, 1966) and
Maximum Entropy Markov Models (McCallum et
al., 2000). Parameters of CRFs can be efficiently
estimated through the log-likelihood parameter esti-
mation using the forward-backward algorithm, a dy-
namic programming method.
2.1 Training and test data
Experiments were conducted using the training and
test sets of the JNLPBA-2004 data set(Kim et al,
2004).
Training data The training data set used in
JNLPBA-2004 is a set of tokenized sentences with
manually annotated term class labels. The sentences
are taken from the Genia corpus (version 3.02) (Kim
et al, 2003), in which 2,000 abstracts were manu-
ally annotated by a biologist, drawing on a set of
POS tags and 36 biomedical term classes. In the
JNLPBA-2004 shared task, performance in extract-
ing five term classes, i.e., protein, DNA, RNA, cell
line, and cell type classes, were evaluated.
Test Data The test data set used in JNLPBA-2004
is a set of tokenized sentences extracted from 404
separately collected MEDLINE abstracts, where the
term class labels were manually assigned, following
the annotation specification of the Genia corpus.
2.2 Overview of dictionary-based statistical
NER
Figure 1 shows the block diagram of dictionary-
based statistical NER. Raw text is analyzed by
a POS/PROTEIN tagger based on a CRF tagging
Figure 1: Block diagram of dictionary-based statistical
NER
Figure 2: Block diagram of training procedure
model and dictionary, and then converted into to-
ken sequences. Strings in the text that match with
protein names in the dictionary will be tagged as
NN-PROTEIN depending on the context around the
protein names. Since it is not realistic to enumer-
ate all protein names in the dictionary, due to their
high variability of form, instead previously unseen
forms are predicted to be protein names by statisti-
cal sequential labelling. Finally, protein names are
identified from the POS/PROTEIN tagged token se-
quences via a CRF labelling model.
Figure 2 shows the block diagram of the train-
ing procedure for both POS/PROTEIN tagging and
sequential labelling. The tagging model is created
using the Genia corpus (version 3.02) and a dic-
tionary. Using the tagging model, MEDLINE ab-
stracts used for the JNLPBA-2004 training data set
are then POS/PROTEIN-tagged. The output token
sequences over these abstracts are then integrated
with the correct protein labels of the JNLPBA-2004
training data. This process results in the preparation
of token sequences with features and correct protein
labels. A CRF labelling model is finally generated
by applying a CRF tool to these decorated token se-
quences.
64
IL/NNP
-/- 2/CD
-/-
mediated/VVD
mediated/VVN
activation/NN
IL-2/NN-PROTEIN
IL-2/NN-PROTEIN
-/-
2/CD
mediated/VVN
mediated/VVD
mediate/VVP
mediate/VV
activation/NN
IL/NNP
IL-2-mediated activation ...
POS/PROTEIN tagging
Lexicon
Figure 3: Dictionary based approach
2.2.1 Dictionary-based POS/PROTEIN tagging
The dictionary-based approach is beneficial when
a sentence contains some protein names that con-
flict with general English words. Otherwise, if the
POS tags of sentences are decided without consider-
ing possible occurrences of protein names, POS se-
quences could be disrupted. For example, in ?met
proto-oncogene precursor?, met might be falsely
recognized as a verb by a non dictionary-based tag-
ger.
Given a sentence, the dictionary-based approach
extracts protein names as follows. Find all word se-
quences that match the lexical entries, and create a
token graph (i.e., trellis) according to the word order.
Estimate the score of every path using the weights of
node and edges estimated by training using Condi-
tional Random Fields. Select the best path.
Figure 3 shows an example of our dictionary-
based approach. Suppose that the input is ?IL-
2-mediated activation?. A trellis is created based
on the lexical entries in a dictionary. The se-
lection criteria for the best path are determined
by the CRF tagging model trained on the Genia
corpus. In this example, IL-2/NN-PROTEIN
-/- mediated/VVN activation/NN is se-
lected as the best path. Following Kudo et al (Kudo
et al, 2004), we adapted the core engine of the
CRF-based morphological analyzer, MeCab1, to our
POS/PROTEIN tagging task. MeCab?s dictionary
databases employ double arrays (Aoe, 1989) which
enable efficient lexical look-ups.
The features used were:
? POS
? PROTEIN
1http://sourceforge.net/project/showfiles.php?group id=177856/
? POS-PROTEIN
? bigram of adjacent POS
? bigram of adjacent PROTEIN
? bigram of adjacent POS-PROTEIN
During the construction of the trellis, white space
is considered as the delimiter unless otherwise stated
within dictionary entries. This means that unknown
tokens are character sequences without spaces.
2.2.2 Dictionary construction
A dictionary-based approach requires the dictio-
nary to cover not only a wide variety of biomedical
terms but also entries with:
? all possible capitalization
? all possible linguistic inflections
We constructed a freely available, wide-coverage
English word dictionary that satisfies these condi-
tions. We did consider the MedPost pos-tagger
package2 which contains a free dictionary that has
downcased English words; however, this dictionary
is not well curated as a dictionary and the number of
entries is limited to only 100,000, including inflec-
tions.
Therefore, we started by constructing an English
word dictionary. Eventually, we created a dictionary
with about 266,000 entries for English words (sys-
tematically covering inflections) and about 1.3 mil-
lion entries for protein names.
We created the general English part of the dictio-
nary from WordNet by semi-automatically adding
POS tags. The POS tag set is a minor modifica-
tion of the Penn Treebank POS tag set3, in that pro-
tein names are given a new POS tag, NN-PROTEIN.
Further details on construction of the dictionary now
follow.
Protein names were extracted from the BioThe-
saurus4. After selecting only those terms
clearly stated as protein names, 1,341,992 pro-
tein names in total were added to the dictionary.
2ftp://ftp.ncbi.nlm.nih.gov/pub/lsmith/MedPost/
3ftp://ftp.cis.upenn.edu/pub/treebank/
doc/tagguide.ps.gz
4http://pir.georgetown.edu/iprolink/
biothesaurus/
65
Nouns were extracted from WordNet?s noun list.
Words starting with lower case and upper case
letters were determined as NN and NNP, re-
spectively. Nouns in NNS and NNPS cate-
gories were collected from the results of POS
tagging articles from Plos Biology Journal5
with TreeTagger6.
Verbs were extracted from WordNet?s verb list. We
manually curated VBD, VBN, VBG and VBZ
verbs with irregular inflections based on Word-
Net. Next, VBN, VBD, VBG and VBZ forms
of regular verbs were automatically generated
from the WordNet verb list.
Adjectives were extracted from WordNet?s adjec-
tive list. We manually curated JJ, JJR and JJS
of irregular inflections of adjectives based on
the WordNet irregular adjective list. Base form
(JJ) and regular inflections (JJR, JJS) of adjec-
tives were also created based on the list of ad-
jectives.
Adverbs were extracted from WordNet?s adverb
list. Both the original and capitalised forms
were added as RB.
Pronouns were manually curated. PRP and PRP$
words were added to the dictionary.
Wh-words were manually curated. As a result,
WDT, WP, WP$ and WRB words were added
to the dictionary.
Words for other parts of speech were manually
curated.
2.2.3 Statistical prediction of protein names
Statistical sequential labelling was employed to
improve the coverage of protein name recognition
and to remove false positives resulting from the pre-
vious stage (dictionary-based tagging).
We used the JNLPBA-2004 training data, which
is a set of tokenized word sequences with
IOB2(Tjong Kim Sang and Veenstra, 1999) protein
labels. As shown in Figure 2, POSs of tokens re-
sulting from tagging and tokens of the JNLPBA-
2004 data set are integrated to yield training data for
sequential labelling. During integration, when the
single token of a protein name found after tagging
5http://biology.plosjournals.org/
6http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/DecisionTreeTagger.html/
corresponds to a sequence of tokens from JNLPBA-
2004, its POS is given as NN-PROTEIN1, NN-
PROTEIN2,..., according to the corresponding token
order in the JNLPBA-2004 sequence.
Following the data format of the JNLPBA-2004
training set, our training and test data use the IOB2
labels, which are ?B-protein? for the first token of
the target sequence, ?I-protein? for each remaining
token in the target sequence, and ?O? for other to-
kens. For example, ?Activation of the IL 2 precursor
provides? is analyzed by the POS/PROTEIN tagger
as follows.
Activation NN
of IN
the DT
IL 2 precursor NN-PROTEIN
provides VVZ
The tagger output is given IOB2 labels as follows.
Activation NN O
of IN O
the DT O
IL NN-PROTEIN1 B-protein
2 NN-PROTEIN2 I-protein
precursor NN-PROTEIN3 I-protein
provides VVZ O
We used CRF models to predict the IOB2 la-
bels. The following features were used in our ex-
periments.
? word feature
? orthographic features
? the first letter and the last four letters of
the word form, in which capital letters in
a word are normalized to ?A?, lower case
letters are normalized to ?a?, and digits are
replaced by ?0?, e.g., the word form of IL-
2 is AA-0.
? postfixes, the last two and four letters
? POS feature
? PROTEIN feature
The window size was set to ?2 of the current to-
ken.
3 Results and discussion
66
Table 1: Experimental Rusults
Tagging R P F
Full 52.91 43.85 47.96
(a) POS/PROTEIN tagging Left 61.48 50.95 55.72
Right 61.38 50.87 55.63
Sequential Labelling R P F
Full 63.23 70.39 66.62
(b) Word feature Left 68.15 75.86 71.80
Right 69.88 77.79 73.63
Full 77.17 67.52 72.02
(c) (b) + orthographic feature Left 82.51 72.20 77.01
Right 84.29 73.75 78.67
Full 76.46 68.41 72.21
(d) (c) + POS feature Left 81.94 73.32 77.39
Right 83.54 74.75 78.90
Full 77.58 69.18 73.14
(e) (d) + PROTEIN feature Left 82.69 73.74 77.96
Right 84.37 75.24 79.54
Full 79.85 68.58 73.78
(f) (e) + after adding protein names in the Left 84.82 72.85 78.38
training set to the dictionary Right 86.60 74.37 80.02
3.1 Protein name recognition performance
Table 1 shows our protein name recognition results,
showing the differential effect of various combina-
tions of strategies. Results are expressed accord-
ing to recall (R), precision (P), and F-measure (F),
which here measure how accurately our various ex-
periments determined the left boundary (Left), the
right boundary (Right), and both boundaries (Full)
of protein names. The baseline for tagging (row
(a)) shows the protein name detection performance
of our dictionary-based tagging using our large pro-
tein name dictionary, where no training for protein
name prediction was involved. The F-score of this
baseline tagging method was 47.96.
The baseline for sequential labelling (row (b))
shows the prediction performance when using only
word features where no orthographic and POS fea-
tures were used. The F-score of the baseline la-
belling method was 66.62. When orthographic fea-
ture was added (row (c)), the F-score increased by
5.40 to 72.02. When the POS feature was added
(row (d)), the F-score increased by 0.19 to 72.21.
Using all features (row (e)), the F-score reached
73.14. Surprisingly, adding protein names appear-
ing in the training data to the dictionary further im-
proved the F-score by 0.64 to 73.78, which is the
second best score for protein name recognition us-
ing the JNLPBA-2004 data set.
Table 2: After Dictionary Enrichment
Method R P F
Tagging Full 79.02 61.87 69.40
(+test set Left 82.28 64.42 72.26
protein names) Right 80.96 63.38 71.10
Labelling full 86.13 72.49 78.72
(+test set Left 89.58 75.40 81.88
protein names) Right 90.23 75.95 82.47
Tagging and labelling speeds were measured us-
ing an unloaded Linux server with quad 1.8 GHz
Opteron cores and 16GB memory. The dictionary-
based POS/PROTEIN tagger is very fast even
though the total size of the dictionary is more than
one million. The processing speed for tagging and
sequential labelling of the 4,259 sentences of the test
set data took 0.3 sec and 7.3 sec, respectively, which
means that in total it took 7.6 sec. for recognizing
protein names in the plain text of 4,259 sentences.
3.2 Dictionary enrichment
The advantage of the dictionary-based statistical ap-
proach is that it is versatile, as the user can easily
improve its performance with no retraining. We as-
sume the following situation as the ideal case: sup-
pose that a user needs to analyze a large amount of
text with protein names. The user wants to know
67
the maximum performance achievable for identify-
ing protein names with our dictionary-based statis-
tical recognizer which can be achieved by adding
more protein names to the current dictionary. Note
that protein names should be identified in context.
That is, recall of the NER results with the ideal dic-
tionary is not 100%. Some protein names in the ideal
dictionary are dropped during statistical tagging or
labelling.
Table 2 shows the scores after each step of dic-
tionary enrichment. The first block (Tagging) shows
the tagging performance after adding protein names
appearing in the test set to the dictionary. The sec-
ond block (Labelling) shows the performance of the
sequence labelling of the output of the first step.
Note that tagging and the sequence labelling mod-
els are not retrained using the test set.
3.3 Discussion
It is not possible in reality to train the recognizer
on target data, i.e., the test set, but it would be pos-
sible for users to add discovered protein names to
the dictionary so that they could improve the overall
performance of the recognizer without retraining.
Rule-based and procedural approaches are taken
in (Fukuda et al, 1998; Franzen et al, 2002). Ma-
chine learning-based approaches are taken in (Col-
lier et al, 2000; Lee et al, 2003; Kazama et al,
2002; Tanabe and Wilbur, 2002; Yamamoto et al,
2003; Tsuruoka, 2006; Okanohara et al, 2006).
Machine learning algorithms used in these studies
are Naive Bayes, C4.5, Maximum Entropy Models,
Support Vector Machines, and Conditional Random
Fields. Most of these studies applied machine learn-
ing techniques to tokenized sentences.
Table 3 shows the scores reported by other sys-
tems. Tsai et al (Tsai et al, 2006) and Zhou and
Su (Zhou and Su, 2004) combined machine learning
techniques and hand-crafted rules. Tsai et al (Tsai
et al, 2006) applied CRFs to the JNLPBA-2004
data. After applying pattern-based post-processing,
they achieved the best F-score (75.12) among those
reported so far. Kim and Yoon(Kim and Yoon, 2007)
also applied heuristic post-processing. Zhou and Su
(Zhou and Su, 2004) achieved an F-score of 73.77.
Purely machine learning-based approaches have
been investigated by several researchers. The
GENIA Tagger (Tsuruoka, 2006) is trained on
the JNLPBA-2004 Corpus. Okanohara et al
(Okanohara et al, 2006) employed semi-Markov
CRFs whose performance was evaluated against the
JNLPBA-2004 data set. Yamamoto et al (Ya-
mamoto et al, 2003) used SVMs for character-
based protein name recognition and sequential la-
belling. Their protein name extraction performance
was 69%. This paper extends the machine learning
approach with a curated dictionary and CRFs and
achieved high F-score 73.78, which is the top score
among the heuristics-free NER systems. Table 4
shows typical recognition errors found in the recog-
nition results that achieved F-score 73.78. In some
cases, protein name boundaries of the JNLPBA-
2004 data set are not consistent. It is also one of
the reasons for the recognition errors that the data
set contains general protein names, such as domain,
family, and binding site names as well as anaphoric
expressions, which are usually not covered by pro-
tein name repositories. Therefore, our impression on
the performance is that an F-score of 73.78 is suffi-
ciently high.
Furthermore, thanks to the dictionary-based ap-
proach, it has been shown that the upper bound per-
formance using ideal dictionary enrichment, with-
out any retraining of the models, has an F-score of
78.72.
4 Conclusions
This paper has demonstrated how to utilize known
named entities to achieve better performance in sta-
tistical named entity recognition. We took a two-
step approach where sentences are first tokenized
and tagged based on a biomedical dictionary that
consists of general English words and about 1.3 mil-
lion protein names. Then, a statistical sequence
labelling step predicted protein names that are not
listed in the dictionary and, at the same time, re-
duced false negatives in the POS/PROTEIN tagging
results. The significant benefit of this approach is
that a user, not a system developer, can easily en-
hance the performance by augmenting the dictio-
nary. This paper demonstrated that the state-of-
the-art F-score 73.78 on the standard JNLPBA-2004
data set was achieved by our approach. Further-
more, thanks to the dictionary-based NER approach,
the upper bound performance using ideal dictionary
enrichment, without any retraining of the models,
yielded F-score 78.72.
5 Acknowledgments
This research is partly supported by EC IST project
FP6-028099 (BOOTStrep), whose Manchester team
is hosted by the JISC/BBSRC/EPSRC sponsored
National Centre for Text Mining.
68
Table 3: Conventional results for protein name recognition
Authors R P F
Tsai et al(Tsai et al, 2006) 71.31 79.36 75.12
Our system 79.85 68.58 73.78
Zhou and Su(Zhou and Su, 2004) 69.01 79.24 73.77
Kim and Yoon(Kim and Yoon, 2007) 75.82 71.02 73.34
Okanohara et al(Okanohara et al, 2006) 77.74 68.92 73.07
Tsuruoka(Tsuruoka, 2006) 81.41 65.82 72.79
Finkel et al(Finkel et al, 2004) 77.40 68.48 72.67
Settles(Settles, 2004) 76.1 68.2 72.0
Song et al(Song et al, 2004) 65.50 73.04 69.07
Ro?ssler(Ro?ssler, 2004) 72.9 62.0 67.0
Park et al(Park et al, 2004) 69.71 59.37 64.12
References
J. Aoe, An Efficient Digital Search Algorithm by Using
a Double-Array Structure, IEEE Transactions on Soft-
ware Engineering, 15(9):1066?1077, 1989.
L.E. Baum and T. Petrie, Statistical inference for proba-
bilistic functions of finite state Markov chains, The An-
nals of Mathematical Statistics, 37:1554?1563, 1966.
J. Chang, H. Schutze, R. Altman, GAPSCORE: Finding
Gene and Protein names one Word at a Time, Bioin-
formatics, Vol. 20, pp. 216-225, 2004.
N. Collier, C. Nobata, J. Tsujii, Extracting the Names
of Genes and Gene Products with a Hidden Markov
Model, Proc. of the 18th International Conference
on Computational Linguistics (COLING?2000), Saar-
brucken, 2000.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malvina Nis-
sim, Gail Sinclair and Christopher Manning, Exploit-
ing Context for Biomedical Entity Recognition: From
Syntax to the Web, Proc. of the Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA-2004), pp. 88?91, 2004.
K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden,
and J. Koster, Protein Names and How to Find Them,
Int. J. Med. Inf., Vol. 67, pp. 49?61, 2002.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi,
Toward information extraction: identifying protein
names from biological papers, PSB, pp. 705-716,
1998.
J. Kazama, T. Makino, Y. Ohta, J. Tsujii, Tuning Support
Vector Machines for Biomedical Named Entity Recog-
nition, Proc. of ACL-2002 Workshop on Natural Lan-
guage Processing in the Biomedical Domain, pp. 1?8,
2002.
J.-D. Kim, T. Ohta, Y. Tateisi, J. Tsujii: GENIA corpus
- semantically annotated corpus for bio-textmining,
Bioinformatics 2003, 19:i180-i182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, Introduction to the Bio-Entity Recogni-
tion Task at JNLPBA, Proc. of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (JNLPBA-2004), pp. 70?75, 2004.
S. Kim, J. Yoon: Experimental Study on a Two Phase
Method for Biomedical Named Entity Recognition,
IEICE Transactions on Informaion and Systems 2007,
E90-D(7):1103?1120.
Taku Kudo and Kaoru Yamamoto and Yuuji Matsumoto,
Applying Conditional Random Fields to Japanese
Morphological Analysis, Proc. of Empirical Methods
in Natural Language Processing (EMNLP), pp. 230?
237, 2004.
J. Lafferty, A. McCallum, and F. Pereira, Conditional
Random Fields: Probabilistic Models for Segment-
ing and Labeling Sequence Data, Proc. of ICML-2001,
pp.282?289, 2001
K. J. Lee, Y. S. Hwang and H. C. Rim (2003), Two-Phase
Biomedical NE Recognition based on SVMs, Proc. of
ACL 2003 Workshop on Natural Language Processing
in Biomedicine, Sapporo, 2003.
McCallum A, Freitag D, Pereira F.: Maximum entropy
Markov models for information extraction and seg-
mentation, Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning, 2000:591-
598.
Daisuke, Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka and Jun?ichi Tsujii, Improving the Scalability of
Semi-Markov Conditional Random Fields for Named
Entity Recognition, Proc. of ACL 2006, Sydney, 2006.
Kyung-Mi Park, Seon-Ho Kim, Do-Gil Lee and
Hae-Chang Rim. Boosting Lexical Knowledge for
Biomedical Named Entity Recognition, Proc. of the
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA-2004), pp.
76-79, 2004.
Marc Ro?ssler, Adapting an NER-System for German to
the Biomedical Domain, Proc. of the Joint Workshop
on Natural Language Processing in Biomedicine and
its Applications (JNLPBA-2004), pp. 92?95, 2004.
Burr Settles, Biomedical Named Entity Recognition Us-
ing Conditional Random Fields and Novel Feature
69
Table 4: Error Analysis
False positives
Cause Correct extraction Identified term
1 dictionary - protein, binding sites
2 prefix word trans-acting factor common trans-acting factor
3 unknown word - ATTTGCAT
4 sequential labelling error - additional proteins
5 test set error - Estradiol receptors
False negatives
Cause Correct extraction Identified term
1 anaphoric (the) receptor, (the) binding sites -
2 coordination (and, or) transcription factors NF-kappa B and AP-1 transcription factors NF-kappa B
3 prefix word activation protein-1 protein-1
catfish STAT STAT
4 postfix word nuclear factor kappa B complex nuclear factor kappa B
5 plural protein tyrosine kinase(s) protein tyrosine kinase
6 family name, biding site, T3 binding sites -
and domain residues 639-656 -
7 sequential labelling error PCNA -
Chloramphenicol acetyltransferase -
8 test set error superfamily member -
Sets, Proc. of the Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applications
(JNLPBA-2004), pp. 104?1007, 2004.
Yu Song, Eunju Kim, Gary Geunbae Lee and Byoung-
kee Yi, POSBIOTM-NER in the shared task of
BioNLP/NLPBA 2004, Proc. of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (JNLPBA-2004), pp. 100-103, 2004.
L. Tanabe and W. J. Wilbur, Tagging Gene and Protein
Names in Biomedical Text, Bioinformatics, 18(8), pp.
1124?1132, 2002.
E.F. Tjong Kim Sang and J. Veenstra, Representing Text
Chunks,EACL-99, pp. 173-179, 1999.
Richard Tzong-Han Tsai, W.-C. Chou, S.-H. Wu, T.-Y.
Sung, J. Hsiang, and W.-L. Hsu, Integrating Linguistic
Knowledge into a Conditional Random Field Frame-
work to Identify Biomedical Named Entities, Expert
Systems with Applications, 30 (1), 2006.
Yoshimasa Tsuruoka, GENIA Tagger 3.0,
http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/tagger/, 2006.
K. Yamamoto, T. Kudo, A. Konagaya and Y. Matsumoto,
Protein Name Tagging for Biomedical Annotation in
Text, in Proc. of ACL-2003 Workshop on Natural Lan-
guage Processing in Biomedicine, Sapporo, 2003.
Guofeng Zhou and Jian Su, Exploring Deep Knowledge
Resources in Biomedical Name Recognition, Proceed-
ings of the Joint Workshop on Natural Language Pro-
cessing of Biomedicine and its Applications (JNLPBA-
2004), pp. 96-99, 2004.
70
Proceedings of the EACL 2009 Demonstrations Session, pages 61?64,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
  
Three BioNLP Tools Powered by a Biological Lexicon 
 
Abstract 
In this paper, we demonstrate three NLP 
applications of the BioLexicon, which is a 
lexical resource tailored to the biology 
domain. The applications consist of a 
dictionary-based POS tagger, a syntactic 
parser, and query processing for biomedical 
information retrieval.  Biological 
terminology is a major barrier to the 
accurate processing of literature within 
biology domain. In order to address this 
problem, we have constructed the 
BioLexicon using both manual and semi-
automatic methods. We demonstrate the 
utility of the biology-oriented lexicon 
within three separate NLP applications. 
1 Introduction 
Processing of biomedical text can frequently be 
problematic, due to the huge number of technical 
terms and idiosyncratic usages of those terms.  
Sometimes, general English words are used in 
different ways or with different meanings in 
biology literature. 
There are a number of linguistic resources 
that can be use to improve the quality of 
biological text processing.  WordNet (Fellbaum, 
1998) and the NLP Specialist Lexicon 1  are 
dictionaries commonly used within biomedical 
NLP. 
WordNet is a general English thesaurus which 
additionally covers biological terms. However, 
since WordNet is not targeted at the biology 
domain, many biological terms and derivational 
relations are missing.   
The Specialist Lexicon is a syntactic lexicon 
of biomedical and general English words, 
providing linguistic information about individual 
vocabulary items (Browne et al, 2003).  Whilst 
it contains a large number of biomedical terms, 
                                                 
1 http://SPECIALIST.nlm.hih.gov 
its focus is on medical terms. Therefore some 
biology-specific terms, e.g., molecular biology 
terms, are not the main target of the lexicon.  
In response to this, we have constructed the 
BioLexicon (Sasaki et al, 2008), a lexical 
resource tailored to the biology domain.  We will 
demonstrate three applications of the BioLexicon, 
in order to illustrate the utility of the lexicon 
within the biomedical NLP field.  
The three applications are: 
 
? BLTagger: a dictionary-based POS tagger 
based on the BioLexicon 
? Enju full parser enriched by the 
BioLexicon 
? Lexicon-based query processing for 
information retrieval 
2. Summary of the BioLexicon 
In this section, we provide a summary of the 
BioLexicon (Sasaki et al, 2008). It contains 
words belonging to four part-of-speech 
categories: verb, noun, adjective, and adverb.  
Quochi et al(2008) designed the database 
model of the BioLexicon which follows the 
Lexical Markup Framework (Francopoulo et al, 
2008).    
2.1 Entries in the Biology Lexicon 
The BioLexicon accommodates both general 
English words and terminologies. Biomedical 
terms were gathered from existing biomedical 
databases. Detailed information regarding the 
sources of biomedical terms can be found in  
(Rebholz-Schuhmann et al, 2008). The lexicon 
entries consist of the following: 
 
(1) Terminological verbs: 759 base forms (4,556 
inflections) of terminological verbs with 
automatically extracted verb 
subcategorization frames 
 
Yutaka Sasaki 1   Paul Thompson1   John McNaught 1, 2   Sophia Ananiadou1, 2 
 
1 School of Computer Science, University of Manchester 
2  National Centre for Text Mining 
MIB, 131 Princess Street, Manchester, M1 7DN, United Kingdom 
{Yutaka.Sasaki,Paul.Thompson,John.McNaught,Sophia.Ananiadou}@manchester.ac.uk 
61
(2)Terminological adjectives: 1,258 
terminological adjectives.   
(3) Terminological adverbs: 130 terminological 
adverbs. 
(4) Nominalized verbs: 1,771  nominalized verbs.   
(5) Biomedical terms: Currently, the BioLexicon 
contains biomedical terms in the categories of 
cell (842 entries, 1,400 variants), chemicals 
(19,637 entries, 106,302 variants), enzymes 
(4,016 entries, 11,674 variants), diseases 
(19,457 entries, 33,161 variants), genes and 
proteins (1,640,608 entries, 3,048,920 
variants), gene ontology concepts (25,219 
entries, 81,642 variants), molecular role 
concepts (8,850 entries, 60,408 variants), 
operons (2,672 entries, 3,145 variants), 
protein complexes (2,104 entries, 2,647 
variants), protein domains (16,940 entries, 
33,880 variants), Sequence ontology concepts 
(1,431 entries, 2,326 variants), species 
(482,992 entries, 669,481 variants), and 
transcription factors (160 entries, 795 
variants).   
In addition to the existing gene/protein names, 
70,105 variants of gene/protein names have been 
newly extracted from 15 million MEDLINE 
abstracts. (Sasaki et al, 2008) 
2.2. Comparison to existing lexicons 
This section focuses on the words and 
derivational relations of words that are covered 
by our BioLexicon but not by comparable 
existing resources. Figures 1 and 2 show the 
percentage of the terminological words and 
derivational relations (such as the word 
retroregulate and the derivational relation 
retroregulate ? retroregulation) in our lexicon 
that are also found in WorNet and the Specialist 
Lexicion. 
Since WordNet is not targeted at the biology 
domain, many biological terms and derivational 
relations are not included.   
Because the Specialist Lexicon is a 
biomedical lexicon and the target is broader than 
our lexicon, some biology-oriented words and 
relations are missing.  For example, the 
Specialist Lexicon includes the term retro-
regulator but not retro-regulate. This means that 
derivational relations of retro-regulate are not 
covered by the Specialist Lexicon.  
3. Application 1: BLTagger 
Dictionary-based POS tagging is advantageous 
when a sentence contains technical terms that 
conflict with general English words. If the POS 
tags are decided without considering possible 
occurrences of biomedical terms, then POS 
errors could arise.  
For example, in the protein name ?met proto-
oncogene precursor?, met might be incorrectly 
recognized as a verb by a non dictionary-based 
tagger.   
Input sentence: 
?IL-2-mediated activation of ??
IL/NP
IL-2/NN-BIOMED
-/-
2/CD
mediated/VVD
IL-2-mediated/UNKNOWN
IL/NP
2/CD
IL-2/NN-BIOMED
??????????
mediated/VVD
mediate/VVP
mediate/VV
of/IN
mediated/VVN
-/-
-/-
mediated/VVNdictionary-based tagging of/IN
Fig. 3 BLTagger example 
coverage
0
20
40
60
80
100
verb
noun
adjective
adverb
nom
inalization
adjetivial 
adverbal
Terminologies Derivational relations
 
Fig. 1  Comparison with WordNet 
coverage
0
20
40
60
80
100
verb
noun
adjective
adverb
nom
inalization
adjetivial 
adverbal
Terminologies Derivational relations
 
 
Fig. 2  Comparison with Specialist Lexicon 
62
In the dictionary, biomedical terms are given 
POS tag "NN-BIOMED". Given a sentence, the 
dictionary-based POS tagger works as follows.  
 
? Find all word sequences that match the 
lexical entries, and create a token graph (i.e., 
trellis) according to the word order.  
? Estimate the score of every path using the 
weights of the nodes and edges, through 
training using Conditional Random Fields.  
? Select the best path. 
 
Figure 3 shows an example of our dictionary-
based POS tagger BLTagger. 
Suppose that the input is ?IL-2-mediated 
activation of?. A trellis is created based on the 
lexical entries in the dictionary. The selection 
criteria for the best path are determined by the 
CRF tagging model trained on the Genia corpus 
(Kim et al, 2003). In this example,  
 
IL-2/NN-BIOMED -/- mediated/VVN 
activation/NN of/IN 
 
is selected as the best path.  
Following Kudo et al (2004), we adapted the 
core engine of the CRF-based morphological 
analyzer, MeCab2, to our POS tagging task.  
The features used were: 
 
? POS 
? BIOMED 
? POS-BIOMED 
? bigram of adjacent POS 
? bigram of adjacent BIOMED 
? bigram of adjacent POS-BIOMED 
 
During the construction of the trellis, white 
space is considered as the delimiter unless 
otherwise stated within dictionary entries. This 
means that unknown tokens are character 
sequences without spaces. 
As the BioLexicon associates biomedical 
semantic IDs with terms, the BLTagger attaches 
semantic IDs to the tokenizing/tagging results. 
4. Application 2: Enju full parser with the 
BioLexicon 
Enju (Miyao, et al, 2003) is an HPSG parser, 
which is tuned to the biomedical domain.  
Sentences are parsed based on the output of the 
                                                 
2 http://sourceforge.net/project/showfiles.php?group 
id=177856/ 
Stepp POS tagger, which is also tuned to the 
biomedical domain. 
To further tune Enju to the biology domain, 
(especially molecular biology), we have 
modified Enju to parse sentences based on the 
output of the BLTagger. 
As the BioLexicon contains many multi-word 
biological terms, the modified version of Enju 
parses token sequences in which some of the 
tokens are multi-word expressions.  This is 
effective when very long technical terms (e.g., 
more than 20 words) are present in a sentence. 
To use the dictionary-based tagging for 
parsing, unknown words should be avoided as 
much as possible. In order to address this issue, 
we added entries in WordNet and the Specialist 
Lexicion to the dictionary of BLTagger. 
The enhancement in the performance of Enju 
based on these changes is still under evaluation. 
However, we demonstrate a functional, modified 
version of Enju. 
5. Application 3: Query processing for IR 
It is sometimes the case that queries for 
biomedical IR systems contain long technical 
terms that should be handled as single multi-
word expressions.  
We have applied BLTagger to the TREC 2007 
Genomics Track data (Hersh et al, 2007).  The 
goal of the TREC Genomics Track 2007 was to 
generate a ranked list of passages for 36 queries 
that relate to biological events and processes.    
Firstly, we processed the documents with a 
conventional tokenizer and standard stop-word 
remover, and then created an index containing 
the words in the documents. Queries are 
processed with the BLTagger and multi-word 
expressions are used as phrase queries.  Passages 
are ranked with Okapi BM25 (Robertson et al, 
1995). 
Table 1 shows the preliminary Mean Average 
Precision (MAP) scores of applying the 
BLTagger to the TREC data set.   
By adding biology multi-word expressions 
identified by the BLTagger to query terms (row 
(a)), we were able to obtain a slightly better 
Passage2 score. As the BLTagger outputs 
semantic IDs which are defined in the 
BioLexicon, we tried to use these semantic IDs 
for query expansion (rows (b) and (d)).  However, 
the MAP scores degraded. 
63
6. Conclusions 
We have demonstrated three applications of the 
BioLexicon, which is a resource comprising 
linguistic information, targeted for use within 
bio-text mining applications.   
We have described the following three 
applications that will be useful for processing of 
biological literature. 
 
? BLTagger: dictionary-based POS tagger 
based on the BioLexicon 
? Enju full parser enriched by the 
BioLexicon 
? Lexicon-based query processing for 
information retrieval 
 
Our future work will include further intrinsic 
and extrinsic evaluations of the BioLexicon in 
NLP, including its  application to information 
extraction tasks in the biology domain. The 
BioLexicon is available for non-commercial 
purposes under the Creative Commons license. 
Acknowledgements 
This research has been supported by the EC IST 
project FP6-028099 (BOOTStrep), whose 
Manchester team is hosted by the 
JISC/BBSRC/EPSRC sponsored National Centre 
for Text Mining.   
References 
Browne, A.C., G. Divita, A.R. Aronson, and A.T. 
McCray. 2003. UMLS Language and Vocabulary 
Tools. In Proc. of AMIA Annual Symposium 2003, 
p.798. 
Dietrich Rebholz-Schuhmann, Piotr Pezik, Vivian Lee, 
Jung-Jae Kim, Riccardo del Gratta, Yutaka Sasaki, 
Jock McNaught, Simonetta Montemagni, Monica 
Monachini, Nicoletta Calzolari, Sophia Ananiadou, 
BioLexicon: Towards a Reference Terminological 
Resource in the Biomedical Domain, the 16th 
Annual International Conference on Intelligent 
Systems for Molecular Biology (ISMB-2008) 
(Poster), Toronto, Canada, 2008. 
(http://www.ebi.ac.uk/Rebholz-srv/BioLexicon/ 
BioLexicon_Poster_EBI_UoM_ILC.pdf) 
Fellbaum, C., editor. 1998. WordNet: An Electronic 
Lexical Database.  MIT Press, Cambridge, MA.. 
Francopoulo, G., M. George, N. Calzolari, M. 
Monachini, N. Bel, M. Pet, and C. Soria. 2006. 
Lexical Markup Framework (LMF). In Proc. of  
LREC 2006, Genova, Italy. 
Hersh, W., Aaron Cohen, Lynn Ruslen, and Phoebe 
Roberts, TREC 2007 Genomics Track Overview, 
TREC-2007, 2007. 
Kim, J-D., T. Ohta, Y. Tateisi, and J. Tsujii. 2003.  
GENIA Corpus - Semantically Annotated Corpus 
for Bio-Text Mining. Bioinformatics, 19:i180-i182. 
Kudo T., Yamamoto K., Matsumoto Y., Applying 
Conditional Random Fields to Japanese Mor- 
phological Analysis. In Proc. of Empirical 
Methods in Natural Language Processing 
(EMNLP-04), pp. 230?237, 2004. 
Lafferty, J., A. McCallum, and F. Pereira. 2001. 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labelling Sequence Data. In 
Proc. of the Eighteenth International Conference 
on Machine Learning (ICML-2001), pages 282-289.  
Miyao, Y. and J. Tsujii, 2003. Probabilistic modeling 
of argument structures including non-local 
dependencies. In Proc. of the Conference on 
Recent Advances in Natural Language Processing 
(RANLP 2003), pages 285-291. 
Quochi, V., Monachini, M., Del Gratta, R., Calzolari, 
N., A lexicon for biology and bioinformatics: the 
BOOTStrep experience. In Proc. of LREC 2008, 
Marrakech, 2008. 
Robertson, S.E., Walker S., Jones, S., Hancock-
Beaulieu M.M., and Gatford, M., 1995. Okapi at 
TREC-3. In Proc of Overview of the Third Text 
REtrieval Conference (TREC-3), pp. 109?126. 
Yutaka Sasaki, Simonetta Montemagni, Piotr Pezik, 
Dietrich Rebholz-Schuhmann, John McNaught, 
and Sophia Ananiadou, BioLexicon: A Lexical 
Resource for the Biology Domain, In Proc. of the 
Third International Symposium on Semantic 
Mining in Biomedicine (SMBM 2008), 2008. 
Table 1 Preliminary MAP scores for TREC Genomics Track 2007 data 
 
Query expansion method Passage2 MAP Aspect MAP Document MAP 
(a) BioLexicon terms 0.0702 0.1726 0.2158 
(b) BioLexicon terms 
 + semantic IDs 
0.0696 0.1673 0.2148 
(c) no query expansion  (baseline) 0.0683 0.1726 0.2183 
(d) semantic IDs 0.0677 0.1670 0.2177 
 
64
