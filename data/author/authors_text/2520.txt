Text Summarization Challenge 2 
Text summarization evaluation at NTCIR Workshop 3 
 
Manabu Okumura 
Tokyo Institute of Technology 
oku@pi.titech.ac.jp 
Takahiro Fukusima  
Otemon Gakuin University  
fukusima@res.otemon.ac.jp 
Hidetsugu Nanba  
Hiroshima City University 
nanba@its.hiroshima-cu.ac.jp 
Abstract 
  We describe the outline of Text Summarization 
Challenge 2 (TSC2 hereafter), a sequel text 
summarization evaluation conducted as one of the tasks 
at the NTCIR Workshop 3.  First, we describe briefly the 
previous evaluation, Text Summarization Challenge 
(TSC1) as introduction to TSC2.   Then we explain 
TSC2 including the participants, the two tasks in TSC2, 
data used, evaluation methods for each task, and brief 
report on the results. 
 
Keywords: automatic text summarization, 
summarization evaluation 
1 
2 
Introduction 
As research on automatic text summarization is being 
a hot topic in NLP, we also see the needs to discuss and 
clarify the issues on how to evaluate text summarization 
systems. SUMMAC in May 1998 as a part of TIPSTER 
(Phase III) project ([1], [2]) and Document 
Understanding Conference (DUC) ([3]) in the United 
States show the need and importance of the evaluation 
for text summarization. 
In Japan, Text Summarization Challenge (TSC1), a 
text summarization evaluation, the first of its kind, was 
conducted in the years of 1999 to 2000 as a part of the 
NTCIR Workshop 2.  It was realized in order for the 
researchers in the field to collect and share text data for 
summarization, and to make clearer the issues of 
evaluation measures for summarization of Japanese 
texts ([4],[5],[6]). TSC1 used newspaper articles and 
had two tasks for a set of single articles with intrinsic 
and extrinsic evaluations.  The first task (task A) was to 
produce summaries (extracts and free summaries) for 
intrinsic evaluations.  We used recall, precision and F-
measure for the evaluation of the extracts, and content-
based as well as subjective methods for the evaluation 
of the free summaries. 
The summarization rates for task A were as follows: 
10, 30, 50% for extracts and 20, 40% for free 
summaries. 
The second task (task B) was to produce summaries 
for information retrieval (relevance judgment) task. The 
measures for evaluation were recall, precision and F-
measure to indicate the accuracy of the task, as well as 
the time to indicate how long it takes to carry out the 
task. 
We also prepared human-produced summaries 
including key data for the evaluation.  In terms of genre, 
we used editorials and business news articles at TSC1?s 
dryrun, and editorials and articles on social issues at the 
formal run evaluation.   
As sharable data, we had summaries for 180 
newspaper articles by spring 2001.  For each article, we 
had the following seven types of summaries: important 
sentences (10, 30, 50%), important parts specified (20, 
40%), and free summaries (20, 40%). 
In comparison, TSC2 uses newspaper articles and 
has two tasks (single- and multi-document 
summarization) for two types of intrinsic evaluations. In 
the following sections, we describe TSC2 in detail.  
Two Tasks in TSC2 and its Schedule 
TSC2 has two tasks.  They are single document 
summarization (task A) and multi-document 
summarization  (task B). 
Task A: We ask the participants to produce 
summaries in plain text to be compared with human-
prepared summaries from single documents.  
Summarization rate is a rate between the number of 
characters in the summary and the total number of 
characters in the original article.  The rates are about 
20% and 40%.  This task is the same as task A-2 in 
TSC1. 
Task B: In this task, more than two (multiple) 
documents are summarized for the task. Given a set of 
documents, which has been gathered for a pre-defined 
topic, the participants produce summaries of the set in 
plain text format. The information that was used to 
produce the document set, such as queries, as well as 
summarization lengths are given to the participants. 
Two summarization lengths are specified, short and 
long summaries for one set of documents. 
The schedule of evaluations at TSC2 was as follows: 
dryrun was conducted in December 2001 and formal run 
was in May 2002.  The final evaluation results were 
reported to the participants by early July 2002. 
3 Data Used for TSC2 
We use newspaper articles from the Mainichi 
newspaper database of 1998, 1999. As key data (human 
prepared summaries), we prepare the following types of 
summaries. 
 
Extract-type summaries:  
We asked captioners who are well experienced in 
summarization to select important sentences from 
each article.  The summarization rates are 10%, 30%, 
and 50%. 
Abstract-type summaries:  
We asked the captioners to summarize the original 
articles in two ways.  The first is to choose important 
parts of the sentences recognized important in 
extract-type summaries (abstract-type type1).  The 
second is to summarize the original articles ?freely? 
without worrying about sentence boundaries, trying 
to obtain the main idea of the articles (abstract-type 
type2).  Both types of abstract-type summaries are 
used for task A.  The summarization rates are 20% 
and 40%. 
 
Both extract-type and abstract-type summaries are 
summaries from single articles. 
 
Summaries from more than two articles: 
Given a set of newspaper articles that has been 
selected based on a certain topic, the captioners 
produced free summaries (short and long summaries) 
for the set.  Topics are various, from kidnapping case 
to Y2K problem. 
4 Evaluation Methods for each task 
We use summaries prepared by human as key data 
for evaluation. The same two intrinsic evaluation 
methods are used for both tasks.  They are evaluation by 
ranking summaries and by measuring the degree of 
revisions.  Here are the details of the two methods. We 
use 30 articles for task A and 30 sets of documents (30 
topics) for task B at formal run evaluation.  
Unfortunately,  due to the limitation of the budget,  only  
an evaluator evaluates a system?s result for an article(or 
a set). 
4.1. Evaluation by ranking 
This is basically the same as the evaluation method 
used for TSC1 task A-2 (subjective evaluation). We ask 
human judges, who are experienced in producing 
summaries, to evaluate and rank the system summaries 
in terms of two points of views. 
1. Content: How much the system summary covers 
the important content of the original article.  
2. Readability: How readable the system summary is. 
The judges are given 4 types of summaries to be 
evaluated and rank them in 1 to 4 scale (1 is the best, 2 
for the second, 3 for the third best, and 4 for the worst). 
For task A, the first two types are human-produced 
abstract-type type1 and type2 summaries.  The third is 
system results, and the fourth is summaries produced by 
lead method. 
For task B, the first is human-produced free 
summaries of the given set of documents, and the 
second is system results.  The third is the results of the 
baseline system based on lead method where the first 
sentence of each document is used.  The fourth is the 
results of the benchmark system using Stein method 
([7]) whose procedure is as follows: 
1. Produce a summary for each document. 
2. Group the summaries into several clusters. The 
number of clusters is adjusted to be less than the 
half of the number of the documents. 
3. Choose the most representative summary as the 
summary of the cluster. 
4. Compute the similarity among the clusters and 
output the representative summaries in such order 
that the similarity of neighboring summaries is 
high. 
4.2. Evaluation by revision 
 It is a newly introduced evaluation method in TSC2 
to evaluate the summaries by measuring the degree of 
revision to system results.  The judges read the original 
documents and revise the system summaries in terms of 
the content and readability.  The revisions are made by 
one of three editing operations (insertion, deletion, 
replacement). The degree of the revision is computed 
based on the number of the operations and the number 
of revised characters. The revisers could be completely 
free in what they did, though they were instructed to do 
minimum revision. 
  As baseline for task A, lead-method results are used. 
As reference for task A, human produced summaries 
(abstract type1 and abstract type 2) are used. And as 
baseline, reference, and benchmark for task B, lead-
method results, human produced summaries that are 
different from the key data, and the results based on the 
Stein method are used respectively. 
  When more than half of the document needs to be 
revised, the judges can ?give up? revising the document. 
5 Participants 
We had 4 participating systems for Task A, and 5 
systems for Task B at dryrun.  We have 8 participating 
systems for Task A and 9 systems for Task B at formal 
run.  As group, we had 8 participating groups, which are 
all Japanese, of universities, governmental research 
institute or companies in Japan.  Table 1 shows the 
breakdown of the groups. 
 
University 6 
Governmental 
research institute  1 
Company 2 
Table 1  Breakdown of Participants 
(Please note that one group consists of a company and a 
university.) 
6 Results 
6.1. Results of Evaluation by ranking 
Table 2 shows the result of evaluation by ranking 
for task A and Table 3 shows the result of evaluation by 
ranking for task B.  Each score is the average of the 
scores for 30 articles for task A, and 30 topics for task B 
at formal run. 
 
System 
No 
Content 
20% 
Read- 
ability 
20% 
Content 
40% 
Read- 
ability
40% 
F0101 2.53 2.87 2.60 2.77 
F0102 2.67 2.97 2.50 2.77 
F0103 2.80 2.93 2.90 2.90 
F0104 2.77 2.73 2.80 2.90 
F0105 2.70 2.73 2.60 2.77 
F0106 2.73 2.57 2.63 2.67 
F0107 2.70 2.60 2.50 2.53 
F0108 2.40 2.83 2.60 2.77 
TF 3.30 3.30 3.20 3.10 
Human 2.33 2.20 2.10 2.03 
Table 2 Ranking evaluation (task A) 
 
In Tables 2 and 3, F01* and F02* are labels for the 
different systems involved, respectively.  In Table 2, 
?TF? indicates a baseline system based on term-
frequency method, and ?Human? indicates human-
produced summaries that are different from the key data 
used in ranking judgement. 
   In Table 3, ?Human? indicates human-produced 
summaries that are different from the key data used in 
ranking judgement. 
 
System No ContentShort 
Read- 
ability 
Short 
Content 
Long 
Read- 
ability 
Long 
F0201 2.70 3.17 2.50 3.23 
F0202 2.73 2.70 2.77 2.93 
F0203 2.60 2.33 2.97 3.03 
F0204 2.63 2.90 2.80 3.03 
F0205 2.53 3.10 2.73 3.30 
F0206 3.20 3.00 3.47 3.30 
F0207 2.40 2.87 2.63 3.27 
F0208 2.93 2.70 2.53 2.80 
F0209 2.83 2.73 2.53 2.87 
Human 2.00 2.17 1.83 2.33 
Table 3 Ranking evaluation (task B) 
 
   In Appendix A, we also show tables giving the 
fraction of time that each system beats the baseline, one 
human summary, or two human summaries for task A.  
In Appendix B,  we show tables giving the fraction of 
time that each system beats the baseline, the benchmark, 
or  human summary for task B. 
 
 
Content
20% 
Read- 
ability 
20% 
Content 
40% 
Read- 
ability 
40% 
Human 
(type 1)
1.58 1.61 1.67 1.69 
Human
(type 2)
1.50 1.57 1.42 1.55 
Baseline
(Lead) 
3.80 3.60 3.83 3.55 
Table 4 Ranking evaluation (task A, human and 
baseline) 
 
 
Content
Short 
Read- 
ability 
Short 
Content 
Long 
Read- 
ability 
Long 
Human 
(type 2)
1.65 2.38 1.82 2.38 
Baseline
(Lead) 
2.80 2.20 2.70 2.22 
Benchmark
(Stein) 
2.48 2.00 2.50 1.99 
Table 5 Ranking evaluation (task B, human, 
baseline, and benchmark) 
 
  In comparison with the system results (Table 2 and 
Table 3), the scores for the human summaries, the 
baseline systems, and the benchmark system(the 
summaries to be compared)  are shown in Table 4 and 
Table 5.  
6.2. Results of Evaluation by revision 
   Table 6 shows the result of evaluation by revision for 
task A at rate 40%, and Table 7 shows the result of 
evaluation by revision for task A at rate 20%.  Table 8 
shows the result of evaluation by revision for task B 
long, and Table 9 shows the result of evaluation by 
revision for task B short. All the tables show the 
evaluation results in terms of average number of 
revisions (editing operations) per document. 
 
Deletion Insertion Replacement 
System 
UIM RD IM RD C RD 
F0101 2.0  0.1  1.5  0.4  0.5  0.7 
F0102 1.6  0.4  1.5  0.4  0.4  0.8 
F0103 2.3  0.2  2.4  0.2  0.4  0.5 
F0104 2.4  0.4  2.7  0.5  0.4  0.5 
F0105 2.0  0.3  1.7  0.1  0.7  0.7 
F0106 2.8  0.2  2.3  0.4  0.3  0.6 
F0107 2.5  0.6  1.8  0.2  0.1  0.5 
F0108 2.0  0.4  2.4  0.1  0.4  0.6 
ld 2.9  0.1  0.7  0.1  0.4  0.1 
free 0.4  0.4  1.2  0.4  0.1  0.3 
part 0.7  0.6  0.9  0.3  0.1  0.4 
edit 0.3 0.1 0.4 0.3 0.1 0.2 
ALL 1.9  0.3  1.8  0.3  0.3  0.5 
Table 6 Evaluation by revision (task A 40%) 
 
   Please note that UIM stands for unimportant, RD for 
readability, IM for important, C for content in Tables 6 
to 9.  They mean the reason for the operations, e.g. 
?unimportant? is for deletion operation due to the part 
judged to be unimportant, and ?content? is for 
replacement operation due to excess and deficiency of 
content. 
  In Table 6 and Table 7, ?ld? means a baseline system 
using lead method, ?free? is free summaries produced by 
human (abstract type 2), and ?part? is human-produced 
(abstract type1) summaries, and these three are baseline 
and reference scores for task A. 
 
 
 
 
 
 
Deletion Insertion Replacement 
System
UIM RD IM RD C RD 
F0101 1.4 0.4 1.3 0.2  0.5  0.3 
F0102 1.2 0.4 1.0  0.0  0.4  0.5 
F0103 0.8 0.1 1.2  0.0  0.2  0.1 
F0104 0.8 0.1 1.2  0.1  0.1  0.2 
F0105 1.2 0.1 0.7  0.0  0.4  0.2 
F0106 2.1 0.2 1.7  0.1  0.1  0.2 
F0107 0.8 0.6 0.9  0.1  0.2  0.1 
F0108 1.4 0.1 1.1  0.1  0.2  0.6 
ld 1.9 0.1 1.3  0.0  0.0  0.0 
free 0.6 0.4 1.1  0.1  0.2  0.1 
part 0.7 0.3 1.1  0.1  0.1  0.2 
edit 0.2 0.1 0.5 0.1 0.2 0.2 
ALL 1.1 0.3 1.1  0.1  0.2  0.3 
Table 7 Evaluation by revision (task A 20%) 
 
Deletion Insertion Replacement 
System
UIM RD IM RD C RD 
F0201 3.8 0.7 7.2 1.4 1.1 0.9 
F0202 5.2 0.6 3.5 0.4 0.7 0.5 
F0203 5.1 0.6 3.8 0.5 0.9 0.6 
F0204 4.2 0.6 3.4 0.7 1.4 0.7 
F0205 8.1 0.6 5.4 1.7 3.0 1.3 
F0206 3.2 0.2 4.7 0.7 0.8 0.6 
F0207 7.0 1.1 4.1 1.1 1.1 1.1 
F0208 4.8 0.7 4.0 0.4 0.8 0.9 
F0209 4.6 0.5 3.9 0.5 0.5 0.5 
human 3.0 0.9 3.4 7.8 1.0 1.2 
ld 5.7 0.9 2.9 0.4 0.7 0.5 
stein 4.0 0.5 2.2 0.3 0.8 0.5 
edit 3.0 1.2 2.9 0.7 0.7 1.1 
ALL 4.9 0.7 4.0 1.3 1.1 0.8 
Table 8 Evaluation by revision (task B long) 
 
In Table 8 and Table 9, ?human? means human-
produced summaries which are different from the key 
data, and ?ld? means a baseline system using lead 
method, ?stein? means a benchmark system using Stein 
method, and these three are baseline,  reference,  and 
benchmark scores for task B. 
To determine the plausibility of the judges?  revision,  
the revised summaries were again evaluated with the 
evaluation methods in section 5.  In Tables 6 to 9, `edit? 
means the evaluation results for the revised summaries. 
We also measure as degree of revision the number of 
revised characters for the three editing operations, and 
the number of documents that are given up revising by 
the judges.  Please look at the detailed data at NTCIR 
Workshop 3 data booklet. 
 Figure 1 indicates how much the scores for content 
and readability vary for the summaries of the same 
summarization rate.  It shows that the readability scores 
tend to be higher than those for content, and it is 
especially clearer for 40% summarization. 
 
Deletion Insertion Replacement 
System UI
M RD IM RD C RD 
F0201 3.5 0.5 4.3 0.8 1.1 0.7 
F0202 3.5 0.4 2.4 0.2 0.7 0.2 
F0203 3.6 0.3 2.8 0.2 0.5 0.4 
F0204 2.7 0.5 2.3 0.2 1.2 0.7 
F0205 5.5 0.4 2.5 0.8 2.0 0.7 
F0206 2.0 0.4 3.4 0.6 0.4 0.4 
F0207 3.5 0.4 2.7 0.3 0.6 0.6 
F0208 2.4 0.5 2.3 0.4 0.2 0.3 
F0209 2.5 0.5 2.2 0.2 0.3 0.4 
human 1.9 0.8 2.4 2.0 0.9 0.7 
ld 2.8 0.7 2.4 0.2 0.5 0.4 
stein 3.0 0.3 1.8 0.2 0.4 0.3 
edit 2.2 0.8 2.5 0.6 1.0 1.2 
ALL 3.1 0.5 2.6 0.5 0.7 0.5 
 
-0.300
-0.200
-0.100
0.000
0.100
0.200
0.300
F0101
F0102
F0103
F0104
F0105
F0106
F0107
F0108
TF Hum
an
C20-C40
R20-R40
Figure 2 Score difference between 20% and 40% 
summarizations (Task A) 
 
 Figure 2 shows the differences in scores for the 
different summarization rates, i.e. 20% and 40% of task 
A.  ?C20-C40? means the score for content 20% minus 
the score for content 40%.  ?R20-R40? ?means the score 
for readability 20% minus the score for readability 40%.  
Table 9 Evaluation by revision (task B short) 
7 Discussion  Figure 2 tells us that the ranking scores for 20% 
summarization tend to be higher than those for 40%, 
and this is true with the baseline system and human 
summaries as well. 7.1. Discussion for Evaluation by ranking 
 Second, consider task B.  Figure 3 shows the 
differences in scores for content and readability for each 
system for task B. ?CS-RS? means the score for content 
short summaries minus the score for readability short 
summaries.   ?CL-RL? is computed in the same way for 
long summaries. 
 We here further look into how the participating 
systems perform by analysing the ranking results in 
terms of differences in scores for content and those for 
readability. 
 First, consider task A. Figure 1 shows the differences 
in scores for content and readability for each system.  
?C20-R20? means the score for content 20% minus the 
score for readability 20%.   ?C40-R40? means the score 
for content 40% minus the score for readability 40%.    
 
-0.800
-0.600
-0.400
-0.200
0.000
0.200
0.400
F0201
F0202
F0203
F0204
F0205
F0206
F0207
F0208
F0209
H
um
an CS-RS
CL-RL
 
 
-0.500
-0.400
-0.300
-0.200
-0.100
0.000
0.100
0.200
F0101
F0102
F0103
F0104
F0105
F0106
F0107
F0108
TF Hum
an C20-R20
C40-R40
Figure 1 Score difference between Content and 
Readability (Task A) 
Figure 3 Score difference between content and 
readability (Task B) 
 
Figure 3 shows, like Figure 1, that the scores for 
readability tend to be higher, thence, the differences are 
in minus values, than those for content for both short 
and long summaries.  In addition, the differences are 
larger than the differences we saw for task A, i.e. in 
Figure 1. 
 Figure 4 shows the differences in scores for the 
different summarization lengths, i.e. short and long 
summaries of task B.  ?CS-CL? means the score for 
content short summaries minus the score for content 
long summaries.  ?RS-RL? means the score for 
readability short summaries minus the score for 
readability long summaries. 
 Figure 4 tells us, unlike Figure2, the scores for short 
summaries tend to be lower than those for long 
summaries.  This tendency is very clear for the 
readability ranking scores.  
Figure 1 and 3 show that when we compare the 
ranking scores for content and readability summaries, 
the readability scores tend to be higher than those for 
content, which means that the evaluation for readability 
is worse than that for content.  Figure 2 and 4 shows 
contradicting tendencies.  Figure 2 indicates that short 
(20%) summaries are higher in ranking scores, i.e. 
worse in evaluation.  However, Figure 4 indicates the 
other way round. 
Intuitively longer summaries can have better 
readability since they have more words to deal with, and 
it is shown in Figure2.  However, it is not the case with 
task B ranking results.  Longer summaries had worse 
scores, especially in readability evaluation.  
 
-0.800
-0.600
-0.400
-0.200
0.000
0.200
0.400
0.600
F0201
F0202
F0203
F0204
F0205
F0206
F0207
F0208
F0209
H
um
an
CS-CL
RS-RL
 
Figure 4 Score difference between different 
summarization lengths (Task B) 
7.2. Discussion for Evaluation by revision 
 To determine the plausibility of the judges?  revision,  
the revised summaries were again evaluated with the 
evaluation methods in section 5.  As Tables 6 to 9 show, 
the degree of the revisions for the revised summaries is 
rather smaller than that for the original ones and is 
almost same as that for human summaries. 
Tables 10 and 11 show the results of evaluation by 
ranking for the revised summaries at task A and B 
respectively. Compared with Tables 2 to 5, Tables 10 
and 11 show that the scores for the revised summaries 
are rather smaller than those for the original ones and 
are almost same as those for human summaries. 
From these results,  the quality of the revised 
summaries is considered as same as that of human 
summaries. 
 
System No Content20% 
Read- 
ability 
20% 
Content 
40% 
Read- 
ability 
40% 
edit 2.37 2.43 2.33 2.33 
Table 10 Ranking evaluation (task A) 
 
System No ContentShort 
Read- 
ability 
Short 
Content 
Long 
Read- 
ability 
Long 
edit 1.93 2.23 2.13 2.50 
Table 11 Ranking evaluation (task B) 
8. Conclusions 
 We have described the outline of the Text 
Summarization Challenge 2.  In addition to the two 
evaluation runs, we held two round-table discussions, 
one right after dryrun, and the other after formal run.  At 
the second round-table discussion, it was pointed out 
that we might need to examine more closely the results 
of evaluation, especially the one by ranking.  
We are now starting the third evaluation (TSC3). 
Please see our web page[4]  for the details of the task.  
References 
[1] Proceedings of The Tipster Text Program Phase III, 
Morgan Kaufmann, 1999. 
[2] Mani, I., et al The TIPSTER SUMMAC Text 
Summarization Evaluation, Technical Report, MTR 
98W0000138,  The MITRE Corp.,  1998. 
[3] http://www-nlpir.nist.gov/projects/duc/. 
[4] http://oku-gw.pi.titech.ac.jp/tsc/index-en.html. 
[5] Takahiro Fukusima and Manabu Okumura, ?Text 
Summarization Challenge ?Text Summarization 
Evaluation at NTCIR Workshop 2?, In Proceedings of 
NTCIR Workshop 2, pp.45-50, 2001. 
[6] Takahiro Fukusima and Manabu Okumura, ?Text 
Summarization Challenge ? Text Summarization 
Evaluation in Japan?, North American Association for 
Computational Linguistics (NAACL2001), Workshop 
on Automatic Summarization, pp.51-59, 2001. 
[7] Gees C. Stein, Tomek Strazalkowski and G. Bowden 
Wise, ?Summarizing Multiple Documents using Text 
Extraction and Interactive Clustering?, Pacific 
Association for Computational Linguistics, pp.200-
208, 1999. 
Appendix A 
 
20%  
readability lead human humans
F101 0.767 0.100 0.033 
F102 0.667 0.100 0.033 
F103 0.667 0.100 0.033 
F104 0.733 0.133 0.067 
F105 0.833 0.233 0.100 
F106 0.867 0.233 0.133 
F107 0.733 0.233 0.200 
F108 0.833 0.067 0.033 
human 0.933 0.467 0.233 
tf 0.267 0.067 0.067 
    
20% 
content lead human humans
F101 0.867 0.200 0.167 
F102 0.900 0.200 0.100 
F103 0.800 0.067 0.033 
F104 0.767 0.067 0.033 
F105 0.933 0.200 0.067 
F106 0.900 0.200 0.100 
F107 0.800 0.167 0.133 
F108 1.000 0.267 0.167 
human 1.000 0.400 0.233 
tf 0.233 0.000 0.000 
    
40% 
readability lead human humans
F101 0.833 0.233 0.033 
F102 0.700 0.133 0.100 
F103 0.800 0.100 0.067 
F104 0.800 0.133 0.033 
F105 0.767 0.200 0.167 
F106 0.800 0.167 0.100 
F107 0.767 0.200 0.167 
F108 0.833 0.100 0.067 
human 0.867 0.467 0.300 
tf 0.400 0.100 0.100 
    
40% 
content lead human humans
F101 0.967 0.167 0.100 
F102 0.900 0.267 0.200 
F103 0.800 0.100 0.033 
F104 0.900 0.133 0.067 
F105 0.867 0.200 0.167 
F106 0.967 0.200 0.100 
F107 0.933 0.233 0.167 
F108 1.000 0.167 0.100 
human 0.967 0.300 0.267 
tf 0.367 0.000 0.000 
Appendix B 
 
short  
readability lead stein human 
F201 0.233 0.167 0.333 
F202 0.333 0.267 0.367 
F203 0.367 0.333 0.533 
F204 0.300 0.233 0.300 
F205 0.200 0.233 0.267 
F206 0.267 0.233 0.233 
F207 0.200 0.267 0.400 
F208 0.367 0.300 0.233 
F209 0.433 0.167 0.433 
human 0.667 0.600 0.533 
    
short  
content lead stein human 
F201 0.533 0.400 0.267 
F202 0.433 0.333 0.200 
F203 0.500 0.500 0.100 
F204 0.433 0.400 0.200 
F205 0.500 0.533 0.233 
F206 0.300 0.200 0.100 
F207 0.633 0.633 0.233 
F208 0.400 0.333 0.133 
F209 0.433 0.267 0.167 
human 0.700 0.700 0.467 
    
long  
readability lead stein human 
F201 0.167 0.167 0.267 
F202 0.367 0.333 0.300 
F203 0.300 0.267 0.367 
F204 0.233 0.267 0.333 
F205 0.300 0.100 0.233 
F206 0.133 0.100 0.233 
F207 0.200 0.233 0.200 
F208 0.333 0.300 0.333 
F209 0.267 0.300 0.367 
human 0.567 0.533 0.467 
    
long  
content lead stein human 
F201 0.500 0.500 0.400 
F202 0.533 0.300 0.167 
F203 0.433 0.300 0.100 
F204 0.333 0.400 0.233 
F205 0.567 0.367 0.300 
F206 0.200 0.067 0.167 
F207 0.567 0.500 0.233 
F208 0.433 0.533 0.200 
F209 0.567 0.533 0.267 
human 0.733 0.700 0.567 
 
Corpus and Evaluation Measures for Multiple Document Summarization
with Multiple Sources
Tsutomu HIRAO
NTT Communication Science Laboratories
hirao@cslab.kecl.ntt.co.jp
Takahiro FUKUSIMA
Otemon Gakuin University
fukusima@res.otemon.ac.jp
Manabu OKUMURA
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Chikashi NOBATA
Communication Research Laboratories
nova@crl.go.jp
Hidetsugu NANBA
Hiroshima City University
nanba@its.hiroshima-cu.ac.jp
Abstract
In this paper, we introduce a large-scale test collec-
tion for multiple document summarization, the Text
Summarization Challenge 3 (TSC3) corpus. We
detail the corpus construction and evaluation mea-
sures. The significant feature of the corpus is that it
annotates not only the important sentences in a doc-
ument set, but also those among them that have the
same content. Moreover, we define new evaluation
metrics taking redundancy into account and discuss
the effectiveness of redundancy minimization.
1 Introduction
It has been said that we have too much informa-
tion on our hands, forcing us to read through a great
number of documents and extract relevant informa-
tion from them. With a view to coping with this situ-
ation, research on automatic text summarization has
attracted a lot of attention recently and there have
been many studies in this field. There is a particular
need to establish methods for the automatic sum-
marization of multiple documents rather than single
documents.
There have been several evaluation workshops
on text summarization. In 1998, TIPSTER SUM-
MAC (Mani et al, 2002) took place and the Doc-
ument Understanding Conference (DUC)1 has been
held annually since 2001. DUC has included multi-
ple document summarization among its tasks since
the first conference. The Text Summarization Chal-
lenge (TSC)2 has been held once in one and a half
years as part of the NTCIR (NII-NACSIS Test Col-
lection for IR Systems) project since 2001. Multiple
document summarization was included for the first
time as one of the tasks at TSC2 (in 2002) (Okumura
et al, 2003). Multiple document summarization is
now a central issue for text summarization research.
1http://duc.nist.gov
2http://www.lr.pi.titech.ac.jp/tsc
In this paper, we detail the corpus construction
and evaluation measures used at the Text Summa-
rization Challenge 3 (TSC3 hereafter), where multi-
ple document summarization is the main issue. We
also report the results of a preliminary experiment
on simple multiple document summarization sys-
tems.
2 TSC3 Corpus
2.1 Guidelines for Corpus Construction
Multiple document summarization from multiple
sources, i.e., several newspapers concerned with the
same topic but with different publishers, is more dif-
ficult than single document summarization since it
must deal with more text (in terms of numbers of
characters and sentences). Moreover, it is peculiar
to multiple document summarization that the sum-
marization system must decide how much redun-
dant information should be deleted3.
In a single document, there will be few sentences
with the same content. In contrast, in multiple doc-
uments with multiple sources, there will be many
sentences that convey the same content with differ-
ent words and phrases, or even identical sentences.
Thus, a text summarization system needs to recog-
nize such redundant sentences and reduce the redun-
dancy in the output summary.
However, we have no way of measuring the ef-
fectiveness of such redundancy in the corpora for
DUC and TSC2. Key data in TSC2 was given as
abstracts (free summaries) whose number of char-
acters was less than a fixed number and, thus, it
is difficult to use for repeated or automatic evalu-
ation, and for the extraction of important sentences.
Moreover, in DUC, where most of the key data were
abstracts whose number of words was less than a
3It is true that we need other important techniques such as
those for maintaining the consistency of words and phrases that
refer to the same object, and for making the results more read-
able; however, they are not included here.
fixed number, the situation was the same as TSC2.
At DUC 2002, extracts (important sentences) were
used, and this allowed us to evaluate sentence ex-
traction. However, it is not possible to measure the
effectiveness of redundant sentences reduction since
the corpus was not annotated to show sentence with
same content. In addition, this is the same even if
we use the SummBank corpus (Radev et al, 2003).
In any case, because many of the current summa-
rization systems for multiple documents are based
on sentence extraction, we believe these corpora to
be unsuitable as sets of documents for evaluation.
On this basis, in TSC3, we assumed that the pro-
cess of multiple document summarization consists
of the following three steps, and we produce a cor-
pus for the evaluation of the system at each of the
three steps4.
Step 1 Extract important sentences from a given set
of documents
Step 2 Minimize redundant sentences from the re-
sult of Step 1
Step 3 Rewrite the result of Step 2 to reduce the
size of the summary to the specified number of
characters or less.
We have annotated not only the important sen-
tences in the document set, but also those among
them that have the same content. These are the cor-
pora for steps 1 and 2. We have prepared human-
produced free summaries (abstracts) for step 3.
In TSC3, since we have key data (a set of cor-
rect important sentences) for steps 1 and 2, we con-
ducted automatic evaluation using a scoring pro-
gram. We adopted an intrinsic evaluation by human
judges for step 3, which is currently under evalu-
ation. We provide details of the extracts prepared
for steps 1 and 2 and their evaluation measures in
the following sections. We do not report the overall
evaluation results for TSC3.
2.2 Data Preparation for Sentence Extraction
We begin with guidelines for annotating important
sentences (extracts). We think that there are two
kinds of extract.
1. A set of sentences that human annotators
judge as being important in a document set
(Fukusima and Okumura, 2001; Zechner,
1996; Paice, 1990).
4This is based on general ideas of a summarization system
and is not intended to impose any conditions on a summariza-
tion system.
Mainichi articles
Yomiuri articles
abstract
(a)
(b)
(c)
(d)
Doc. x
Doc. y
Figure 1: An example of an abstract and its sources.
2. A set of sentences that are suitable as a source
for producing an abstract, i.e., a set of sen-
tences in the original documents that corre-
spond to the sentences in the abstracts(Kupiec
et al, 1995; Teufel and Moens, 1997; Marcu,
1999; Jing and McKeown, 1999).
When we consider how summaries are produced,
it seems more natural to identify important seg-
ments in the document set and then produce sum-
maries by combining and rephrasing such informa-
tion than to select important sentences and revise
them as summaries. Therefore, we believe that sec-
ond type of extract is superior and thus we prepared
the extracts in that way.
However, as stated in the previous section, with
multiple document summarization, there may be
more than one sentence with the same content, and
thus we may have more than one set of sentences
in the original document that corresponds to a given
sentence in the abstract; that is to say, there may be
more than one key datum for a given sentence in the
abstract5.
we have two sets of sentences that correspond to
sentence   in the abstract.
(1)  of document  , or
(2) a combination of  and  of document 	
This means that  alone is able to produce   , and
  can also be produced by combining   and   (Fig-
ure 1).
We marked all the sentences in the original doc-
uments that were suitable sources for producing the
sentences of the abstract, and this made it possible
for us to determine whether or not a summariza-
tion system deleted redundant sentences correctly
at Step 2. If the system outputs the sentences in
the original documents that are annotated as cor-
responding to the same sentence in the abstract, it
5We use ?set of sentences? since we often find that more
than one sentence corresponds to a sentence in the abstract.
Table 1: Important Sentence Data.
Sentence ID of Abstract Set of Corresponding Sentences
1 
 

2 
Construction and Analysis of Japanese-English Broadcast News Corpus
with Named Entity Tags
Tadashi Kumano, Hideki Kashioka and Hideki Tanaka
ATR Spoken Language Translation Research Laboratories
2?2?2, Hikaridai, Keihanna Science City, Kyoto 619?0288, Japan
{tadashi.kumano, hideki.kashioka, hideki.tanaka}@atr.co.jp
Takahiro Fukusima
Otemon Gakuin University
1?15, Nishiai 2-chome, Ibaraki, Osaka 567?8502, Japan
fukusima@res.otemon.ac.jp
Abstract
We are aiming to acquire named entity
(NE) translation knowledge from non-
parallel, content-aligned corpora, by uti-
lizing NE extraction techniques. For this
research, we are constructing a Japanese-
English broadcast news corpus with NE
tags. The tags represent not only NE
class information but also coreference in-
formation within the same monolingual
document and between corresponding
Japanese-English document pairs. Anal-
ysis of about 1,100 annotated article pairs
has shown that if NE occurrence informa-
tion, such as classes, number of occur-
rence and occurrence order, is given for
each language, it may provide a good clue
for corresponding NEs across languages.
1 Introduction
Studies on named entity (NE) extraction are mak-
ing progress for various languages, such as En-
glish and Japanese. A number of evaluation work-
shops have been held, including the Message Under-
standing Conference (MUC)1 for English and other
languages, and the Information Retrieval and Ex-
traction Exercise (IREX)2 for Japanese. Extraction
accuracy for English has reached a nearly practi-
cal level (Marsh and Perzanowski, 1998). As for
Japanese, it is more difficult to find NE bound-
1http://www.itl.nist.gov/iaui/894.02/
related_projects/muc/
2http://nlp.cs.nyu.edu/irex/
aries, however, NE extraction is relatively accurate
(Sekine and Isahara, 2000).
Most of the past research on NE extraction used
monolingual corpora, but the application of NE ex-
traction techniques to bilingual (or multilingual) cor-
pora is expected to obtain NE translation pairs. We
are developing a Japanese-English machine trans-
lation system for documents including many NEs,
such as news articles or documents about current
topics. Translating NE correctly is indispensable for
conveying information correctly. NE translations,
however, are not listed in conventional dictionaries.
It is necessary to retrieve NE translation knowledge
from the latest bilingual documents.
When extracting translation knowledge from
bilingual corpora, using literally translated parallel
corpora, such as official documents written in sev-
eral languages makes it easier to get the desired in-
formation. However, not many of such corpora con-
tain the latest NEs. There are few Japanese-English
corpora which are translated literally. Therefore,
we decided to extract NE translation pairs from
content-aligned corpora, such as multilingual broad-
cast news articles including new NEs daily, which
are not literally translated.
Sentential alignment (Brown et al, 1991; Gale
and Church, 1993; Kay and Ro?scheisen, 1993; Ut-
suro et al, 1994; Haruno and Yamazaki, 1996) is
commonly used as a starting point for finding the
translations of words or expressions from bilingual
corpora. However, it is not always possible to cor-
respond non-parallel corpora in sentences. Past sta-
tistical methods for non-parallel corpora (Fung and
Yee, 1998) are not valid for finding translations of
words or expressions with low frequency. These
methods have a problem in covering NEs because
there are many NEs that appear only once in a cor-
pus. So we need a specialized method for extract-
ing NE translation pairs. Transliteration is used for
finding the translations of NE in the source language
from texts in the target language (Stalls and Knight,
1998; Goto et al, 2001; Al-Onazian and Knight,
2002). Transliteration is useful for the names of per-
sons and places; however, it is not applicable to all
sorts of NEs.
Content-aligned documents, such as a bilingual
news corpus, are made to convey the same top-
ics. Since NEs are the essential element of docu-
ment contents, content-aligned documents are likely
to share NEs pointing to the same objects. Con-
sequently, when extracting all NEs with NE class
information from each of a pair of bilingual docu-
ments separately by applying monolingual NE ex-
traction techniques, the distribution of the NEs in
each document may be similar enough to recognize
correspondences between the NE translation pairs.
A technique for finding bilingual NE correspon-
dences will have a wide range of applications other
than NE translation-pair extraction. For example,
? Bilingual NE correspondences have clues for
identifying corresponding parts in a pair of
noisy bilingual documents.
? The similarity of any two documents in dif-
ferent languages can be estimated by NE
translation-pair correspondence.
For this research, we obtained a Japanese-English
broadcast news corpus (Kumano et al, 2002) by
the Japanese broadcast company NHK3, and we are
manually tagging NEs in the corpus to analyze it
and to conduct NE translation-pair extraction exper-
iments.
The tag specifications are based on the IREX
NE task (Sekine and Isahara, 1999), the evaluation
workshop of Japanese NE extraction. We extended
the specifications to English NEs. In addition, coref-
erence information between NEs, within the same
monolingual document and between the correspond-
ing Japanese-English document pairs (henceforth,
3Nippon Hoso Kyokai (Japan Broadcasting Corporation)
(http://www.nhk.or.jp/englishtop/)
we call these in a language and across languages,
respectively), is added to each of the tagged NEs,
for NE translation-pair extraction studies.
In Section 2, we will introduce the bilingual cor-
pus used in this study and describe its characteris-
tics. Then, we will discuss tag design for NE extrac-
tion studies, and explain the tag specifications and
existing problems. The current status of corpus an-
notation under these specifications will also be in-
troduced. We analyzed an annotated part of the cor-
pus in terms of NE occurrence and translation. This
analysis will be shown in Section 3. In Section 4,
we will mention future plans for the extraction of
NE translation-pairs.
2 Constructing a Japanese-English
broadcast news corpus with NE tags
2.1 Characteristics of the NHK
Japanese-English broadcast news corpus
We are annotating an NHK broadcast news corpus
with NE tags. The corpus is composed of Japanese
news articles for domestic programs and English
news articles translated for international broadcast-
ing4 and domestic bilingual programs5.
Figure 1 shows an example of a Japanese news
article and its translation in English. The original
Japanese article and the translated English article
deal with the same topic, but they differ much in de-
tails. The difference arises from the following rea-
sons (Kumano et al, 2002).
Audience Content might be added or deleted, ac-
cording to the audience, especially for interna-
tional broadcasting.
Broadcasting date The broadcasting of English
news is often delayed compared to the origi-
nal Japanese news. The time expressions might
be changed sometimes or new facts might be
added to the articles.
News styles / languages Comparing news articles
of two languages reveals that they have differ-
ent presentation styles, for example, facts are
sometimes introduced in a different order. The
4NHK WORLD (http://www.nhk.or.jp/
nhkworld/)
5http://www.nhk.or.jp/englishtop/
program_list/
Original article in Japanese (and its literal translation in English by authors):
1: ????????????????????????????????????
????????????
(There was a strong earthquake at 6:42 this morning in Izu Islands, the site of recent
numerous earthquakes. An earthquake of a little less than five in seismic intensity was
observed at Shikine Island.)
2: ????????????????????????????????????
????????????????????
(In addition, an event of seismic intensity four was observed for Niijima and Kozu Is-
land, events seismic intensity three for Toshima Island and Miyake Island, and events of
seismic intensity two and one for various parts of Kanto Area and Shizuoka Prefecture.)
3: ???????????????????
(There is no risk of tsunamis resulting from this earthquake.)
4: ?????????????????????????????????????
????????????????????????????
(According to observations by the Meteorological Agency, the earthquake epicenter
was located in the sea at a depth of ten kilometers near Niijima and Kozu Island. The
magnitude of the earthquakes was estimated to be five point one.)
5: ????????????????????????????????????
????????????????????????????????????
??????????????????
(In Izu Islands, where seismic activity has been observed from the end of June, repeated
cycles of seismic activity and dormancy have been observed. On the 30th of the previ-
ous month, a single strong earthquake having seismic intensity of a little less than six
was observed at Miyake Island, while two earthquakes having seismic intensity of five
were also observed there.)
6: ????????????????????????????????????
????????????????????????????????????
????
(In a series of seismic events, seventeen earthquakes having seismic intensity over five
have been observed up to this point, including strong tremors with a seismic intensity
of a little less than six observed four times at Kozu Island, Niijima, and Miyake Island.)
Translated article in English:
1: A strong earthquake jolted
Shikine Island, one of the Izu
islands south of Tokyo, early
on Thursday morning.
2: The Meteorological Agency
says the quake measured five-
minus on the Japanese scale of
seven.
3: The quake affected other is-
lands nearby.
4: Seismic activity began in the
area in late July, and 17 quakes
of similar or stronger intensity
have occurred.
5: Officials are warning of more
similar or stronger earthquakes
around Niijima and Kozu Is-
lands.
6: Tokyo police say there have
been no reports of damage
from the latest quake.
Figure 1: An article pair in an NHK broadcast news corpus
difference is due to language and socio-cultural
backgrounds.
2.2 NE tag design
We designed NE tags for NE translation-pair extrac-
tion research and working efficiency for manual an-
notation. The specifications are shown below.
? It is desirable that NE recognition guidelines
be consistent with NE tags of existing corpora.
Past guidelines of MUC and IREX should be
respected because they were configured as a re-
sult of many discussions. Consistent guidelines
enable us to utilize existing annotated corpora
and systems designated for the corpora.
? Within each bilingual document pair, corefer-
ence between NEs in a language and across lan-
guages will be specified. When several NEs
exist for the same referent in a document, it
is not always possible to determine the actual
translation for each instance of the NEs from
the counterpart document, because our corpus
is not composed of literal translations. There-
fore, coreference between NEs in a language
should be marked so that the coreference across
languages can be assigned between NE groups
that have the same referent. Coreference be-
tween NE groups is sufficient for our purpose.
? Assignment of coreference in a language is lim-
ited between NEs only. Although NEs may
have the same referent with pronouns or non-
NE expressions, these elements are ignored to
avoid complicating the annotation work.
2.3 Tag specifications
1. The tag specifications conform to IREX NE
tag specifications (IREX Committee, 1999) (an
English description in (Sekine and Isahara,
1999)) as regards the markup form, NE classes,
and NE recognition guidelines.
Japanese:
????????<LOCATION ID=?1? COR=?2?>
(Izu Islands)
????</LOCATION>
?<DATE ID=?2? COR=?4?>
(today)
???</DATE><TIME ID=?3? COR=?5?>
(a.m.)
??
(6:42)
??????</TIME>????????<LOCATION ID=?4? COR=?1?>
(Shikine Island)
???</LOCATION>????????????? ? ? ?
English:
A strong earthquake jolted <LOCATION ID=?1? COR=?4?>
Shikine Island</LOCATION>, one of the <LOCATION ID=?2?
COR=?1?>Izu islands</LOCATION> south of <LOCATION ID=?3?>
Tokyo</LOCATION>, early on <DATE ID=?4? COR=?2?>Thursday
</DATE> <TIME ID=?5? COR=?3?>morning</TIME>. ? ? ?
Figure 2: An annotation example
NE Class Example
Named entities (in the narrow sense):
ORGANIZATION The Diet; IREX Committee
PERSON (Mr.) Obuchi; Wakanohana
LOCATION Japan; Tokyo; Mt. Fuji
ARTIFACT Pentium Processor; Nobel Prize
Temporal expressions:
DATE September 2, 1999; Yesterday
TIME 11 PM; midnight
Number expressions:
MONEY 100 yen; $12,345
PERCENT 10%; a half
Table 1: NE Classes
Eight NE classes were defined at the IREX NE
task ? the same 7 classes as MUC-7 (3 types
of named entities in the narrow sense, 2 types
of temporal expressions, and 2 types of number
expressions), and ARTIFACT (concrete objects
like commercial products and abstract objects
such as laws or intellectual properties). Table 1
shows a list of these.
2. IREX?s NE classes and NE recognition guide-
lines are applied to English for consistency be-
tween Japanese and English NEs. For English-
specific annotation, such as prepositions or de-
terminers in NE, the MUC-7 Named Entity
Task Definition (Chinchor, 1997) is consulted6.
3. The SGML markup form of the IREX tag is
extended by adding the following two tag at-
tributes, which represent coreference informa-
tion in a language, and across languages.
ID=?NE group ID? (mandatory)
Each NE is assigned an attribute ID and
an ID number as its value. All corefer-
ent NEs in each language document are
6The tag specifications of IREX NE and those of MUC-7 do
not differ radically, because IREX NE tags are designed based
on the discussions of MUC.
given the same ID number7. The same
ID number is assigned to NEs that have
different forms, such as the full name and
the first name or the official name and the
abbreviated form, in addition to NEs with
the same form. Basically, NE are assigned
the same ID number when they belong to
an NE class and have the identical surface
form8.
COR=?ID for corresponding NE groups in
the other language? (optional)
When there exists a corresponding NE
(group) belonging to the same NE class
in the other language, an attribute COR
is given to each NE (group) in both lan-
guages, and the ID number for the coun-
terpart is assigned as a value to each other.
Annotations by the specifications are illustrated
in Figure 2.
2.4 Current status of the corpus annotation
Annotators who have experience in translation work
and in the production of linguistic data are engaging
in the tag annotation. Plans call for a total of 2,000
article pairs to be annotated, and about 1,100 pairs
have been finished up to the present.
2.5 Problems
Some problems became obvious in the course of
discussions of tag specifications and tag annotation
work. They confuse annotators and make the result
inaccurate. Typical cases are shown below.
2.5.1 The granularity difference between
Japanese and English
In Japanese, a unit smaller than a morpheme may
be accepted as an NE according to IREX guidelines.
7ID numbers do not maintain uniqueness across the docu-
ments.
8There are some exceptions. See Section 2.5.3.
(last Sunday and this Sunday)
sensyuu-no nichiyou -to konsyuu-no nichiyou
J: ???<DATE ID=?1?>??</DATE>????<DATE ID=?2?>??</DATE>
E: <DATE COR=?1?>last Sunday</DATE> and <DATE COR=?2?>this Sunday</DATE>
Figure 3: Assignment of different group IDs with NEs having the same surface form
On the other hand, English does not accept any unit
smaller than a word by MUC-7 guidelines. Some
Japanese NEs cannot have a counterpart English
NE, even if they have a corresponding English ex-
pression because of the difference in the segmenta-
tion granularity. For example, ????? (amerika;
America)? in the Japanese morpheme ??????
(amerika-jin; America-people)? is treated as an NE,
while no NE can be tagged to ?American?, the En-
glish counterpart of ??????.?
2.5.2 Translation problems
NEs have the same problem that translation in
general has: What is the exact translation word(s)
for an expression?
? Semantically corresponding expressions may
not be assigned corresponding NE relations,
because they belong to different NE classes or
an expression in a language is not recognized
as an NE. For example, a non-NE word ???
(seifu; government)? which means Japanese
government in Japanese articles is often trans-
lated as the English NE: ?Japan.?
? A non-literal translation of an NE may cause
difficulty in recognizing corresponding rela-
tions. Correspondences for some expressions
cannot be decided with the information repre-
sented in documents: Relative temporal expres-
sions in Japanese are often translated as ab-
solute expressions in English and those corre-
spondences cannot be identified without con-
sulting the calendar; Money expressions are
generally converted to dollars and the ex-
change rate at the relative time is needed to
confirm correspondences. For example, we
found a translation pair of money expressions
????? (sanzen-oku-en; three hundred bil-
lion yen)? and ?three billion U-S dollars? in our
corpus, which constitutes a rough conversion
from yen into dollars when the articles were
produced.
2.5.3 Assigning NE group IDs
We defined NEs that have the identical surface
form and the same NE class to be coreferent and
assigned the same NE group ID, in order to make
coreference judgment easier. There are some cases
where we cannot apply this rule, especially to tem-
poral expressions or number expressions.
The example in Figure 3 shows the translation
pair ???????????? (last Sunday and this
Sunday)? and ?last Sunday and this Sunday? anno-
tated with NE tags. Japanese temporal expressions
?????? (last Sunday)? and ?????? (this
Sunday)? are translated into English as ?last Sun-
day? and ?this Sunday? respectively. When anno-
tating NE tags for this translation pair, only ???
(Sunday)? in those temporal expressions in Japanese
is regarded as an NE according to the IREX?s NE
specifications. This causes a problem in which the
two NEs of the same surface form that are assigned
the same NE class have different referents. Each of
them should assign correspondence to different NEs
in the counterpart: the former to ?last Sunday? and
the latter to ?this Sunday.?
Tentatively, we allowed a different NE group ID to
be assigned to an NE with the identical surface form
in an NE class, as shown in Figure 3. It would be
better reexamine the consistency of the NE tag spec-
ification between Japanese and English, and the ne-
cessity of coreference information for temporal ex-
pressions and number expressions.
3 Analysis
We conducted an elementary investigation into
1,096 pairs of annotated Japanese and English ar-
ticles.
3.1 Corpus size
Table 2 shows the content size of our corpus by
the number of sentences and the morphemes/words.
The content decreases significantly when translating
from Japanese to English. This fact points out that
NE class
Japanese English
tokens avr. per types avr. per tokens avr. per types avr. per( art. / sent.) ( art. / sent.) ( art. / sent.) ( art. / sent.)
Total 24,147 (22.03 / 4.13) 12,809 (11.69 / 2.19) 15,844 (14.46 / 2.03) 10,353 ( 9.45 / 1.32)
ORGANIZATION 5,160 ( 4.71 / 0.88) 2,558 ( 2.33 / 0.44) 2,882 ( 2.63 / 0.37) 1,863 ( 1.70 / 0.24)
PERSON 3,525 ( 3.22 / 0.60) 1,628 ( 1.49 / 0.28) 2,800 ( 2.55 / 0.36) 1,410 ( 1.29 / 0.18)
LOCATION 8,737 ( 7.97 / 1.49) 3,752 ( 3.42 / 0.64) 5,792 ( 5.28 / 0.74) 3,302 ( 3.01 / 0.42)
ARTIFACT 455 ( 0.42 / 0.08) 282 ( 0.26 / 0.05) 241 ( 0.22 / 0.03) 193 ( 0.18 / 0.02)
DATE 4,342 ( 3.96 / 0.74) 2,959 ( 2.70 / 0.51) 2,990 ( 2.73 / 0.38) 2,620 ( 2.39 / 0.34)
TIME 854 ( 0.78 / 0.15) 740 ( 0.68 / 0.13) 245 ( 0.22 / 0.03) 232 ( 0.21 / 0.03)
MONEY 577 ( 0.53 / 0.10) 462 ( 0.42 / 0.08) 517 ( 0.47 / 0.07) 375 ( 0.34 / 0.05)
PERCENT 497 ( 0.45 / 0.08) 428 ( 0.39 / 0.07) 377 ( 0.34 / 0.05) 358 ( 0.33 / 0.05)
Table 3: NE frequency
articles sentences morphemes/words(avr. per article) (avr. per sent.)
J 1,096 5,851 (5.34) 321,204 (54.90)E 7,815 (7.13) 181,180 (23.18)
Table 2: Corpus size
the content tends to be lost through the translation
process.
3.2 In-language characteristics of NE
occurrences
3.2.1 Frequency
The number of occurrences for each NE class is
listed in Table 3. The distribution of NE classes is
almost the same as that in the data for MUC-7 or
IREX.
By comparing the decrease in content (cf. Ta-
ble 2), the number of NE tokens also decreases for
translations. However, the degree of the NE de-
crease is less than that of the morphemes/words. It
is also remarkable that the number of NE types is
fairly well preserved. Notice that only a small num-
ber of tokens in the NE class TIME appear in En-
glish. The reason may be that detailed time infor-
mation may become less important for English ar-
ticles, which are intended for audiences outside of
Japan and broadcast later than the original Japanese
articles.
3.2.2 NE characteristics within NE groups
To examine the surface form distribution in the
same NE groups, we counted the number of mem-
bers ( freq) and sorts of surface form (sort) for each
NE group in each article. The probability that a
given member has a unique surface form in a group
NE class
Japanese English
freq sort uniq freq sort uniq
Average 1.89 1.10 0.131 1.53 1.14 0.332
ORG. 2.02 1.12 0.144 1.55 1.16 0.345
PERSON 2.17 1.12 0.121 1.99 1.49 0.655
LOCATION 2.33 1.14 0.114 1.75 1.07 0.105
ARTIFACT 1.61 1.05 0.072 1.25 1.05 0.216
DATE 1.47 1.08 0.175 1.14 1.03 0.200
TIME 1.15 1.02 0.098 1.06 1.01 0.182
MONEY 1.25 1.03 0.109 1.38 1.35 0.936
PERCENT 1.16 1.00 0.008 1.05 1.06 0.278
Table 4: Surface form distribution in the same NE
groups
that has two or more members (uniq) has also been
calculated as follows:
uniq = freq? 2Csort? 2freq? 1Csort? 1 =
sort ? 1
freq ? 1 ( freq ? 2).
Table 4 shows the values averaged for all the NE
groups that appeared in all articles.
In English, a repetition of the same expression is
not conventionally desirable. Therefore, pronouns
or paraphrases are used frequently. On the other
hand, Japanese does not have such a convention.
This difference is considered to be the reason for the
result shown in Table 4: freq in English is smaller
than that in Japanese, and sort in English is larger
than that in Japanese. As a result, uniq in English is
higher than that in Japanese. These tendencies differ
slightly according to the NE classes.
? The sort of English PERSON is notably large. In
English, the name of a person is usually first ex-
pressed in full, and after that, it tends to be ex-
pressed only by the family name. In Japanese,
only the family name is generally used from the
beginning, especially for well-known persons.
NE class
J? E J? E
token type token type
Average 0.742 0.639 0.842 0.786
ORGANIZATION 0.684 0.612 0.877 0.837
PERSON 0.881 0.777 0.938 0.898
LOCATION 0.799 0.673 0.833 0.753
ARTIFACT 0.701 0.628 0.925 0.912
DATE 0.717 0.656 0.761 0.742
TIME 0.207 0.184 0.596 0.591
MONEY 0.593 0.595 0.781 0.733
PERCENT 0.712 0.692 0.830 0.827
Table 5: Cross-language corresponding rate
NE class
Japanese English
freq sort uniq freq sort uniq
Average 2.19 1.14 0.134 1.64 1.17 0.342
ORG. 2.25 1.17 0.164 1.62 1.19 0.364
PERSON 2.45 1.14 0.110 2.07 1.53 0.645
LOCATION 2.77 1.19 0.117 1.94 1.10 0.112
ARTIFACT 1.80 1.06 0.075 1.27 1.05 0.222
DATE 1.60 1.10 0.167 1.17 1.04 0.211
TIME 1.30 1.04 0.106 1.07 1.01 0.250
MONEY 1.24 1.04 0.138 1.47 1.43 0.934
PERCENT 1.20 1.00 0.010 1.06 1.01 0.250
Table 6: Surface form distribution in the same NE
groups (only for those having cross-language corre-
spondences)
? The uniq of English MONEY is quite high. A
money expression in Japanese tends to be trans-
lated into English as both the original currency
(usually yen) and dollars.
? The freq of temporal and number expressions
are smaller than those of named entities in the
narrow sense.
3.3 Cross-language characteristics of NE
occurrences
3.3.1 Correspondence across languages
We calculated the rates for a given NE in a doc-
ument to have a corresponding NE in the counter-
part language. The units of NE correspondences we
used for these calculations are both NE token and
NE group (type). The results, shown in Table 5,
show that an NE that appeared in English will have
a Japanese NE correspondent with a high rate.
We also conducted the same survey as we did in
Table 4 for only NEs having cross-language corefer-
ences, whose results are shown in Table 6. A com-
parison of both results shows that the freq for only
NEs having cross-language coreferences is larger,
NE class
J? E J? E
All Corr. only All Corr. only
All NEs 0.291 0.774 0.483 0.774
Average 0.304 0.790 0.494 0.790
ORG. 0.269 0.808 0.568 0.809
PERSON 0.403 0.877 0.671 0.875
LOCATION 0.318 0.746 0.461 0.745
ARTIFACT 0.410 0.725 0.662 0.710
DATE 0.307 0.805 0.428 0.805
TIME 0.033 0.815 0.227 0.815
MONEY 0.170 0.829 0.407 0.829
PERCENT 0.509 0.903 0.658 0.903
Table 7: Preservation ratio of NE order
especially in Japanese. An NE occurring more times
in an article may have more important information
and is more likely to appear in the translation.
3.3.2 Preservation of NE order
We investigated how well the order of NEs oc-
curring in an article is preserved in the counterpart
language as follows:
1. In every article, we eliminated all NEs except
the first occurrence of every NE group.
2. We calculated the ratio between all of the pos-
sible NE pairs in the source language and those
translated into the target language with the
same order of occurrence.
Table 7 lists the average preservation ratios of the
NE order for all NEs (?All?) and for NEs having
corresponding NEs in the counterpart (?Corr. only?).
The scores labeled ?All NEs? express ratios for the
order of all NEs. The preservation ratio for each
NE class is listed below in the table. The NE or-
ders are preserved so well even for all NEs that they
can be used for determining cross-language corre-
spondences.
4 Conclusion
In this paper, in which we aimed to acquire NE trans-
lation knowledge, we described our construction of
a Japanese-English broadcast news corpus with NE
tags for NE translation-pair extraction. The tags rep-
resent NE characteristics and coreference informa-
tion in a language and across languages. Analysis
of the annotated 1,097 article pairs has shown that
if NE occurrence information, such as classes, num-
ber of occurrences and occurrence order, is given for
each language side, it may provide a good clue for
determining NE correspondence across languages.
Our future plans are listed below.
? The problems in Section 2.5 need to be reex-
amined from the point of view of what infor-
mation bilingual corpora should have for NE
translation-pair extraction research.
? The proposed analysis in Section 3 pointed out
that identifying coreferences in a language is
very important for achieving NE translation-
pair extraction. Richer coreference information
should be annotated in our corpus for coref-
erence identification studies. We are planning
to annotate coreference information for pro-
nouns and some other non-NE expressions, re-
ferring to the MUC-7 coreference task defini-
tion (Hirschman and Chinchor, 1997).
? Corpora with different characteristics, such as a
bilingual newspaper corpus, will be annotated
and analyzed.
Acknowledgments This research was supported
in part by the Telecommunications Advancement
Organization of Japan.
References
Yaser Al-Onazian and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
02), pages 400?408.
Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In Pro-
ceedings of the 29th Annual Meeting of the Association
for Computational Linguistics (ACL-91), pages 169?
176.
Nancy Chinchor. 1997. MUC-7 named entity task
definition. http://www.itl.nist.gov/iaui/
894.02/related_projects/muc/proceedings/
ne_task.html.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the 36th Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics
(COLING-ACL ?98), volume I, pages 414?420.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Isao Goto, Noriyoshi Uratani, and Terumasa Ehara.
2001. Cross-language information retrieval of proper
nouns using context information. In Proceedings of
the 6th Natural Language Processing Pacific Rim Sym-
posium (NLPRS 2001), pages 571?578.
Masahiko Haruno and Takefumi Yamazaki. 1996. High-
performance bilingual text alignment using statistical
and dictionary information. In Proceedings of the 34th
International Conference on Computational Linguis-
tics (ACL ?96), pages 131?138.
Lynette Hirschman and Nancy Chinchor. 1997.
MUC-7 coreference task definition. http:
//www.itl.nist.gov/iaui/894.02/related_
projects/muc/proceedings/co_task.html.
IREX Committee. 1999. Named entity extraction task
definition (version 990214). http://nlp.cs.nyu.
edu/irex/NE/df990214.txt. (In Japanese).
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121?142.
Tadashi Kumano, Isao Goto, Hideki Tanaka, Noriyoshi
Uratani, and Terumasa Ehara. 2002. A translation
aid system by retrieving bilingual news database. Sys-
tems and Computers in Japan, 33(8):19?29. (Original
written in Japanese is in Transactions of the Institute
of Electronics, Information and Communication Engi-
neers, J85-D-II(6):1175?1184. 2001).
Elaine Marsh and Dennis Perzanowski. 1998. MUC-7
evaluation of IE technology: Overview and results.
http://www.itl.nist.gov/iaui/894.02/
related_projects/muc/proceedings/muc_7_
proceedings/marsh_slides.pdf.
Satoshi Sekine and Hitoshi Isahara. 1999. IREX
project overview. http://nlp.cs.nyu.edu/
irex/Paper/irex-e.ps. (Original written in
Japanese is in Proceedings of the IREX Workshop,
pages 1?5).
Satoshi Sekine and Hitoshi Isahara. 2000. IREX: IR
and IE evaluation project in Japanese. In Proceed-
ings of the 2nd International Conference on Language
Resources and Evaluation (LREC-2000), pages 1475?
1480.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in Arabic text. In
Proceedings of the Workshop on Computational Ap-
proaches of the Semitic Languages, pages 34?41.
Takehito Utsuro, Hiroshi Ikeda, Masaya Yamane, Yuji
Matsumoto, and Makoto Nagao. 1994. Bilingual
text matching using bilingual dictionary and statistics.
In Proceedings of the 32th International Conference
on Computational Linguistics (ACL-94), pages 1076?
1082.
