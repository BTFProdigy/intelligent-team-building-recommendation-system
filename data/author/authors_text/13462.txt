Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 341?344,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
NCSU: Modeling Temporal Relations  with Markov Logic and Lexical Ontology 
 Eun Young Ha Alok Baikadi Carlyle Licata  James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {eha,abaikad,cjlicata,lester}@ncsu.edu     Abstract As a participant in TempEval-2, we ad-dress the temporal relations task consist-ing of four related subtasks. We take a su-pervised machine-learning technique us-ing Markov Logic in combination with rich lexical relations beyond basic and syntactic features. One of our two submit-ted systems achieved the highest score for the Task F (66% precision), untied, and the second highest score (63% precision) for the Task C, which tied with three other systems.  1 Introduction Time plays a key role in narrative. However, cor-rectly recognizing temporal order among events is a challenging task. As a follow-up to the first TempEval competition, TempEval-2 addresses this challenge. Among the three proposed tasks of TempEval-2, we address the temporal rela-tions task consisting of four subtasks: predicting temporal relations that hold between events and time expressions in the same sentence (Task C), events and the document creation time (Task D), main events in adjacent sentences (Task E), and main events and syntactically dominated events, such as those in subordinated clauses (Task F). We are primarily concerned with Task C, E, and F, because D is not relevant to our application domain.1 However, rather than eliminating Task D altogether, we build a very simple model for this task by using only those features that are shared with other task models (i.e., the document                                                 1 Our application domain concerns analysis of narrative stories written by middle school students, with the analysis being conducted a single story at a time. 
creation time data are not used because none of the other task models need them as features). It was expected that this approach would support more interesting comparisons with other systems that take a more sophisticated approach to the task. Further, we experiment with a joint model-ing technique to examine if the communication with other task models brings a boost to a per-formance of the simple model. Taking a supervised machine-learning ap-proach with Markov Logic (ML) (Richardson and Domingos, 2006), we constructed two systems, NCSU-INDI and NCSU-JOINT. NCSU-INDI con-sists of four independently trained classifiers, one for each task, whereas NCSU-JOINT models all four tasks jointly. The choice of ML as learn-ing technique for temporal relations is motivated both theoretically and practically. Theoretically, it is a statistical relational learning framework that does not make the i.i.d. assumption for the data. This is a desirable characteristic for com-plex problems such as temporal relation classifi-cation, as well as many other natural language problems, in which the features representing a given problem are often correlated with one an-other. Practically, ML allows us to build both individual and joint models in a uniform frame-work; individual models can be easily combined together into a joint model with a set of global formulae governing over them.  In previous work (Yoshikawa et al, 2009), ML was successfully applied to temporal relation classification task. Our approach is different from this work in two primary respects. First, we introduce new lexical relation features derived from English lexical ontologies. Second, our model addresses a new task introduced in Tem-pEval-2, which is to identify temporal relations between main and syntactically dominated events in the same sentence. We also employ phrase-based syntactic features (Bethard and 
341
Martin 2007) rather than dependency-based syn-tactic features. 2 Features We consider three types of features: basic, syn-tactic, and lexical relation features. Basic fea-tures represent the information directly available from the original data provided by the task orga-nizer; syntactic features are extracted from syn-tactic parses generated by Charniak parser (Charniak, 2000); and lexical semantic relations that are derived from two external lexical data-bases, VERBOCEAN (Chklovski and Pantel, 2004) and WordNet (Fellbaum, 1998). 2.1 Basic Features Basic features include the word tokens, stems of the words, and the manually annotated attributes of events and time expressions. In the TempEval-2 data, an event always consists of a single word token, but time expressions often consist of mul-tiple tokens. We treat each word in time expres-sions as a different feature. For example, two word features, ?this? and ?afternoon?, are ex-tracted from a given time expression ?this after-noon?. Stemming is done with the Porter Stemmer in NLTK (Loper and Bird, 2002). The value attributes of time expressions are treated as symbolic features, rather than being decomposed into actual integer values representing dates and times.  2.2 Syntactic Features Our syntactic features draw upon the features previously shown to be effective for temporal relation classification (Bethard and Martin, 2007), including the following: ? pos: the part-of-speech (pos) tags of the event and the time expression word to-kens, assigned by Charniak parser.  ? gov-prep: any prepositions governing the event or time expression (e.g., ?for? in ?for ten years?). ? gov-verb: the verb governing the event or time expression, similar to gov-prep. ? gov-verb-pos: the pos tag of the governing verb. We also investigate both full and partial syn-tactic paths between a pair of event and time ex-pressions, but including these features does not improve the classification results on our devel-opment data set. 
2.3 Lexical Relation Features VERBOCEAN is a graph of semantic relations between verbs. There are 22,306 relations be-tween 3,477 verbs that have been mined using Google searches for lexico-syntactic patterns. VERBOCEAN contains five different types of re-lations (Table 1). Verbs are stored in the lemma-tized forms and senses are not disambiguated. A connection between two verbs indicates that the relation holds between some senses of the verbs. VERBOCEAN?S database is presented as a list of verb pair relations, along with a confidence score. Both the transitive and symmetric closure over the relations were taken before storage in a SQLite database for queries. The transitive clo-sure was calculated using the Warshall algorithm (Agrawal and Jagadish, 1990). The confidence score for the new arc was calculated as the aver-age of the two constituents. The symmetric clo-sure was calculated using a simple pass. The confidence score is the same as the reflected edge for symmetric relations. A set of VER-BOCEAN features were calculated for each target event pair within each of the temporal relations tasks. Each verb was lemmatized using the WordNet lemmatizer in NLTK before being compared against the database. Rather than fo-cusing only on HAPPENS-BEFORE relation as in Mani et al (2006), we consider all five verb rela-tions in two different versions, unweighted and weighted. The unweighted version is a binary feature indicating the existence of an arc between the two target verbs in VERBOCEAN. In the weighted version, the existence of an arc is weighted by the associated confidence score.  In addition to VerbOcean, WordNet was used for its conceptual relations. WordNet is a large lexical database, which contains information on verbs, nouns, adjectives and adverbs, grouped into hierarchically organized cognitive synonym 
                                                2 Examples are taken from http://demo.patrickpantel.com/Content/Verbocean/. 
Relation Example  SIMILARITY ?? produce :: create STRENGTH ? wound :: kill ANTONYMY ? open :: close  ENABLEMENT  fight :: win HAPPENS-BEFORE ? buy :: own  Table 1: Semantic relations between verbs in VERBOCEAN (? and ? denotes symmetric and transitive closure, respectively, holds for the given relation)2 
342
sets (synsets). WordNet was accessed through the WordNetCorpusReader module of NLTK. For each target event pair within each of the temporal relations tasks, a semantic distance be-tween the associated tokens was computed using the path-similarity metric present within the API. The synset chosen was simply the first synset returned by the reader. Similar to the VER-BOCEAN features, we consider both unweighted and weighted versions of the feature.   3 The Systems ML is a probabilistic extension of first-order logic that allows formulae to be violated. It as-signs a weight to each formula, reflecting the strength of the constraint represented by the for-mula. A Markov logic network (MLN) is a set of weighted first-order clauses, which, together with constants, defines a Markov network.  We constructed two systems, NCSU-INDI and NCSU-JOINT using an off-the-shelf tool for ML (Riedel, 2008). 3.1 NCSU-INDI NCSU-INDI consists of four independently trained MLNs, one for each task. Each MLN is defined by a set of local formulae that are con-junctions of predicates representing the features. An example local formula used for Task C is  eventTimex(e, t)  eventWord(e, w)            relEventTimex(e, t, r)        (1)  If a pair of event e and time expression t exists and the event consists of a word token w, for-mula (1) assigns a temporal relation t to the given pair of e and t with some weights. For each task, the features described in Sec-tion 2 were examined on a held-out development data set (about 10% of the training data) for their effectiveness in predicting temporal relations and removed if they do not improve the results. Ta-ble 2 lists the features actually used for the tasks. Interestingly, none of the time expression fea-tures were effective on the development data. 3.2 NCSU-JOINT As well as the local formulae from the four local MLNs, a set of global formulae are added to NCSU-JOINT as hard constraints to ensure the consistency between the classification decisions of local MLNs. For example, formula (2) ensures that if an event e1 happens before the document creation time (dct) and another event e2 happens 
after dct, then e1 happens before e2 and vice ver-sa.  relDctEvent(e1,t,BEFORE) relDctEvent(e2,t, AFTER)                         relEvents(e1, e2, BEFORE)       (2)  A set of global constraints is defined between Tasks C and F, D and F, as well as D and E, re-spectively. 4 Results and Discussion The predicted outputs from our systems exhibit mixed results. NCSU-INDI achieves the highest precision score on the test data for Task F by a relatively large margin (6%) from the second-place system, as well as the second highest preci-sion score on Task C, tied with three other sys-tems. Given the encouraging result for Task F, we would preliminarily conclude that the VE-BOCEAN relations are effective predictors of temporal relations between main and syntacti-cally dominated events. However, the same sys-tem does not achieve the same level of accuracy 
Task Feature C  D E F event-word ? ? ?e2 ?e1,e2 Event event-stem ? ? ?e1.e2 ?e1,e2 event-polarity ? ? ?e1.e2 ?e1,e2 event-modal ? ? ?e1.e2 ?e1,e2 event-pos ? ? ?e1.e2 ?e2 event-tense ?  ?e1.e2 ?e1,e2 event-aspect ? ? ?e1.e2 ?e1,e2 
Event  Attribute 
event-class ? ? ?e1.e2 ?e1,e2 timex-word     Timex timex-stem     timex-type     Timex  Attribute timex-value     pos  ?e ?e1.e2  gov-prep ?e,t ?e ?e1.e2 ?e1,e2 gov-verb ?e,t ?e ?e1.e2 ?e1,e2 
Syntactic Parse 
gov-verb-pos ?e,t  ?e1.e2 ?e1,e2 verb-rel    ? Verb-Ocean verb-rel-w   ?  word-dist   ?  WordNet word-dist-w      Table 2: Features used for each task (subscripts e and t mean event and time expression, re-spectively. Subscripts e1 and e2 mean the first and the second main events for the Task E and the main and the syntactically dominated events for the Task F, respectively)   
343
for Task E, even though it is closely related to Task F. The major difference between the mod-els of Task E and F is that the Task E model uses weighted VERBOCEAN relations along with a WordNet feature, while the Task F model uses unweighted VERBOCEAN relations without the WordNet feature. We suspect these two features might negatively impact the classification deci-sions on the test data, even though they prelimi-narily appeared to be effective predictors on the development data.  NCSU-JOINT also yields mixed results. The performance on both Task D and F dramatically drops with the joint modeling approach, while there is a modest improvement on Task E. Man-ual examination of the results on the test data revealed that the majority of the relations in Task D and F were classified as OVERLAP, which may be due to overly strict global constraints; rather than violating global constraints, the system re-sorted to rather neutral predictions.  5 Conclusions Temporal event order recognition is a challeng-ing task. Using basic, syntactic, and lexical rela-tion features, we built two systems with ML: NCSU-INDI models each subtask independently, and NCSU-JOINT models all four tasks jointly. NCSU-INDI was most effective in predicting temporal relations between main events and syn-tactically dominated events (66% precision), as well as temporal relations between time expres-sions and events (63% precision). Future direc-tions include conducting a more rigorous exami-nation of the predictive power of the features, as well as the impact of global formulae for the joint model.  Acknowledgments This research was supported by the National Sci-ence Foundation under Grant IIS-0757535.  Any opinions, findings, and conclusions or recom-mendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.  
References  R. Agrawal, S. Dar, and H. V. Jagadish. 1990. Direct transitive closure algorithms: design and perform-ance evaluation. ACM Transactions on Database Systems, 15(3): 427-458. S. Bethard and J. H. Martin. 2007. CU-TMP: tempo-ral relation classification using syntactic and se-mantic features. In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, pages 129-132, Prague, Czech Republic. E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Lin-guistics conference, pages 132-139, Seattle, WA. Y. Cheng, M. Asahara, and Y. Matsumoto. 2007. NAIST.Japan: Temporal relation identification us-ing dependency parsed tree. In Proceedings of the 4th International Workshop on Semantic Evalua-tions, pages 245-248, Prague, Czech Republic. T. Chklovski and P. Pantel. 2004.VerbOcean: Mining the Web for Fine-Grained Semantic Verb Rela-tions. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 33-40, Barcelona, Spain. C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press. E. Loper and S. Bird. 2002. NLTK: The Natural Lan-guage Toolkit. In Proceedings of ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics, pages 62?69, Philadelphia, PA. I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and J. Pustejovsky. 2006. Machine learning of temporal relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-tational Linguistics, pages 753-760, Sydney, Aus-tralia. M. Richardson and P. Domingos. 2006. Markov Log-ic Networks. Machine Learning, 62(1): 107-136. 
S. Riedel. 2008. Improving the accuracy and effi-ciency of MAP inference for Markov Logic. In Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence, pages 468-475, Helsinki, Finland. K. Yoshikawa, S. Riedel, M. Asahara, and Y. Matsu-moto. 2009. Jointly Identifying Temporal Relations with Markov Logic. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Nat-ural Language Processing of the AFNLP, pages 405-413, Suntec, Singapore. 
Precision / Recall (%) System Task C Task D Task E Task F NCSU-INDI 63/63 68/68 48/48 66/66 NCSU-JOINT 62/62 21/21 51/51 25/25  Table 3: Accuracy of the systems on each task 
344
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 75?78,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Exploring the Effectiveness of Lexical Ontologies                                                    for Modeling Temporal Relations with Markov Logic 
  Eun Y. Ha, Alok Baikadi, Carlyle J. Licata, Bradford W. Mott, James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {eha,abaikad,cjlicata,bwmott,lester}@ncsu.edu     Abstract Temporal analysis of events is a central problem in computational models of dis-course. However, correctly recognizing temporal aspects of events poses serious challenges. This paper introduces a joint modeling framework and feature set for temporal analysis of events that utilizes Markov Logic. The feature set includes novel features derived from lexical on-tologies. An evaluation suggests that in-troducing lexical relation features im-proves the overall accuracy of temporal relation models. 1 Introduction Reasoning about the temporal aspects of events is a critical task in discourse understanding. Temporal analysis techniques contribute to a broad range of applications including question answering and document summarization, but temporal reasoning is complex. A recent series of shared task evaluation challenges proposed a framework with standardized sets of temporal analysis tasks, including identifying the temporal entities mentioned in text, such as events and time expressions, as well as identifying the tem-poral relations that hold between those temporal entities (Pustejovsky and Verhagen, 2009).   Our previous work (Ha et al, 2010) addressed modeling temporal relations between temporal entities and proposed a supervised machine-learning approach with Markov Logic (ML) (Richardson and Domingos, 2006). As novel fea-tures, we introduced two types of lexical rela-tions derived from VerbOcean (Chklovski and Pantel, 2004) and WordNet (Fellbaum, 1998). A 
preliminary evaluation showed the effectiveness of our approach. In this paper, we extend our previous work and conduct a more rigorous evaluation, focusing on the impact of joint opti-mization of the features and the effectiveness of the lexical relation features for modeling tempo-ral relations. 2 Related Work Recently, data-driven approaches to modeling temporal relations for written text have been gaining momentum. Boguraev and Ando (2005) apply a semi-supervised learning technique to recognize events and to infer temporal relations between time expressions and their anchored events. Mani et al (2006) model temporal rela-tions between events as well as between events and time expressions using maximum entropy classifiers. The participants of TempEval-1 in-vestigate a variety of techniques for temporal analysis of text (Verhagen et al, 2007).  While most data-driven techniques model temporal relations as local pairwise classifiers, this approach has the limitation that there is no systematic mechanism to ensure global consis-tencies among predicted temporal relations (e.g., if event A happens before event B and event B happens before event C, then A should happen before C). To avoid this drawback, a line of re-search has explored techniques for the global optimization of local classifier decisions. Cham-bers and Jurafsky (2008) add global constraints over local classifiers using Integer Linear Pro-gramming. Yoshikawa et al (2009) jointly model related temporal classification tasks using ML. These approaches are shown to improve the ac-curacy of temporal relation models. Our work is most closely related to Yoshikawa et al (2009) in that ML is used for joint model-
75
ing of temporal relations. We extend their work in three primary respects. First, we introduce new lexical relation features. Second, our model addresses a new task introduced in TempEval-2. Third, we employ phrase-based syntactic features (Bethard and Martin 2007) rather than depend-ency-based syntactic features. 3 Data and Tasks We use the TempEval-2 data for English for both training and testing of our temporal relation models. The data includes 162 news articles (to-taling about 53,000 tokens) as the training set and another 11 news articles as the test set. The corpus is labeled with events, time expressions, and temporal relations. Each labeled event and time expression is further annotated with seman-tic and syntactic attributes. Six types of temporal relations are considered: before, after, overlap, before-or-overlap, overlap-or-after, and vague. Consider the following example from the TempEval-2 data, marked up with a time expres-sion t1 and three events e1, e2, and e3, where e1 and e2 are the main events of the first and the second sentences, respectively, and e3 is syntac-tically dominated by e2. But a [minute and a half]t1 later, a pilot from a nearby flight [calls]e1 in. Ah, we just [saw]e2 an [explosion]e3 up ahead of us here about sixteen thousand feet or something like that. In the first sentence, t1 and e1 are linked by a temporal relation overlap. Temporal relation af-ter holds between the two consecutive main events: e1 occurs after e2. The main event e2 of the second sentence overlaps with e3, which is syntactically dominated by e2. In this paper, we focus on three subproblems of the temporal relation identification task as de-fined by TempEval-2: identifying temporal rela-tions between (1) events and time expressions in the same sentence (ET); (2) two main events in consecutive sentences (MM); and (3) two events in the same sentence when one syntactically dominates another (MS), which is a new task in-troduced in TempEval-2. 4 Features Surface features include the word tokens and stems of the words. In the TempEval-2 data, an event always consists of a single word token, but 
time expressions often consist of multiple tokens. We treat the entire string of words in a given time expression as a single feature. Semantic features are the semantic attributes of individual events and time expressions de-scribed in Section 3. In this work, we use the gold-standard values for these features that were manually assigned by human annotators in the training and the test data.  Syntactic features include three features adopted from Bethard and Martin (2007): gov-prep, any prepositions governing the event or time expression (e.g., ?for? in ?for ten years?); gov-verb, the verb governing the event or time expression; gov-verb-pos, the part-of-speech (pos) tag of the governing verb. We also consider the pos tag of the word in the event and the time expression. Lexical relations are the semantic relations be-tween two events derived from VerbOcean (Chklovski and Pantel, 2004) and WordNet (Fellbaum, 1998). VerbOcean contains five types of relations (similarity, strength, antonymy, en-ablement, and happens-before) that commonly occur between pairs of verbs. To overcome data sparseness, we expanded the original VerbOcean database by calculating symmetric and transitive closures of key relations. With WordNet, a se-mantic distance between the associated tokens of each target event pair was computed. 5 Modeling Temporal Relations with Markov Logic ML is a statistical relational learning framework that provides a template language for defining Markov Logic Networks (MLNs). A MLN is a set of weighted first-order clauses constituting a Markov network in which each ground formula represents a feature (Richardson and Domingos, 2006). Our MLN consists of a set of formulae com-bining two types of predicates: hidden and ob-served. Hidden predicates are those that are not directly observable during test time. A hidden predicate is defined for each task: relEventTimex (temporal relation between an event and a time expression), relMainEvents (temporal relation between two main events), and relMainSub (temporal relation between a main and a domi-nated event). Observed predicates are those that can be fully observed during test time and repre-sent each of the features described in Section 4.  The following is an example formula used in our MLN: 
76
eventTimex(d, e, t)  eventWord(d, e, w)            relEventTimex(d, e, t, r)       (1) The predicate eventTimex(d, e, t) represents the existence of a candidate pair of event e and time expression t in a document d. Given this candi-date pair, formula (1) assigns weights to a tem-poral relation r whenever it observes a word to-ken w in the given event from the training data. This formula is local because it considers only one hidden predicate (relEventTimex). In addition to local formulae, we also define a set of global formulae to ensure consistency be-tween local decisions: relEventTimex(d, e1, t, r1)  relEventTimex(d, e2, t, r2)  relMainSub(d, e1, e2, r3)           (2) Formula (2) is global because it jointly concerns more than one hidden predicate (relEventTimex and relMainSub) at the same time. This formula ensures consistency between the predicted tem-poral relations r1, r2, and r3 given a main event e1, a syntactically dominated event e2, and a time expression t shared by both of these events. Two additional global formulae (3) and (4) are simi-larly defined to ensure consistency as below.  relMainSub(d, e1, e2, r3)  relEventTimex(d, e2, t, r2)    relEventTimex(d, e1, t, r1)       (3) relMainSub(d, e1, e2, r3)  relEventTimex(d, e1, t, r1)    relEventTimex(d, e2, t, r2)       (4) 6 Evaluation To evaluate the proposed approach, we built and compared two models: one model (NoLex) used all of the features described in Section 4 except for the lexical relation features, and the other model (Full) included the full set of features. The features were generated using the Porter Stemmer and WordNet Lemmatizer in NLTK (Loper and Bird, 2002) and the Charniak Parser (Charniak, 2000). The semantic distance between two word tokens was computed using the path-similarity metric provided by NLTK. All of the models were constructed using Markov TheBeast (Riedel, 2008) The feature set was optimized for each task on a held-out development data set consisting of approximately 10% of the entire training set (Ta-ble 1). Our previous work (Ha et al, 2010) ob-served that a local optimization approach that selects for each individual task (i.e., each hidden predicate in the given MLN) in isolation from the other tasks could harm the overall accuracy of a joint model because of resulting inconsistencies 
among individual tasks. In the new experiment described in this section, features were selected for each task to improve overall accuracy of the joint model combining all three tasks, similar to Yoshikawa et al (2009).  Table 2 reports the resulting performance (F1 scores) of the models. To isolate the potential effects of global constraints, we first compare the accuracies of the Full and the NoLex model, av-eraged from a ten-fold cross validation on the training data before global constraints are added. Full achieves relative 12% and 3% improve-ments over NoLex for temporal relation between events and time expressions (ET) and between two main events (MM), respectively. The im-provement for MM was statistically significant (p<0.05) from a two-tailed paired t-test. Note that the ET task itself does not use lexical rela-tion features but still achieves an improved result in Full over NoLex. This is an effect of joint modeling. There is a slight degradation (relative 2%) in the accuracy for temporal relations be-tween main and syntactically dominated events (MS). Overall, Full achieves relative 5% im-provement over NoLex. A similar trend of per-formance improvement in Full over NoLex was observed when the global formulae were added to each model. The second column (Global Con-straints) of Table 2 compares the two models trained on the entire training set and tested on the test set after the global formulae were added. However, no statistical significance was found on these improvements. Compared to the state-
Task Feature ET MM MS event-word ? ? ? event-stem ? ? ? timex-word ?   
Surface Features 
timex-stem ?   event-polarity ? ? ? event-modal ? ? ? event-pos ? ?   ?* event-tense ? ? ? event-aspect ? ? ? event-class ? ? ? timex-type ?   
Semantic Attributes 
timex-value ?   pos ? ? ? gov-prep ? ? ? gov-verb ? ? ? 
Syntactic Features 
gov-verb-pos ? ? ? verb-rel  ? ? Lexical  Relations word-dist  ?   Table 1: Features used to model each task. *The feature is extracted only from the second event in the pair being compared. 
77
of-the-art results achieved by the TempEval-2 participants, Full achieves the same or better re-sults on all three addressed tasks. 7 Conclusions Temporal relations can be modeled with Markov Logic using a variety of features including lexi-cal ontologies. Three tasks relating to the Tem-pEval-2 data were addressed: predicting tempo-ral relations between (1) events and time expres-sions in the same sentence, (2) two main events in consecutive sentences, and (3) two events in the same sentence when one syntactically domi-nates the other. An evaluation suggests that util-izing lexical relation features within a joint mod-eling framework using Markov Logic achieves state-of-the-art performance. The results suggest a promising direction for future work. The proposed approach assumes events and time expressions are already marked in the data. To construct a fully automatic tempo-ral relation identification system, the approach needs to be extended to include models that rec-ognize events and time expressions in text as well as their semantic attributes. A data-driven approach similar to the one described in this pa-per may be feasible for this new modeling task. It will entail exploring a variety of features to fur-ther understand the complexity underlying the problem of temporal analysis of events. Acknowledgments This research was supported by the National Sci-ence Foundation under Grant IIS-0757535. References  S. Bethard and J. H. Martin. 2007. CU-TMP: Temporal relation classification using syntactic and semantic features. In Proceedings of the 4th Inter-national Workshop on Semantic Evaluations, pages 129-132, Prague, Czech Republic. B. Boguraev and R. K. Ando. 2005. TimeML-compliant text analysis for temporal reasoning. In Proceedings of the 19th International Joint Conference on Artificial intelligence, pages 997-1003, Edinburgh, Scotland.  N. Chambers and D. Jurafsky. 2008. Jointly combin-ing implicit constraints improves temporal order-ing. In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages 698-706, Honolulu, HI. E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American Chapter of the Association for Computational Lin-
Chapter of the Association for Computational Lin-guistics Conference, pages 132-139, Seattle, WA. T. Chklovski and P. Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb relations. In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 33-40, Barcelona, Spain. E. Ha, A. Baikadi, C. Licata, and J. Lester. 2010. NCSU: Modeling temporal relations with Markov Logic and lexical ontology. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 341-344, Uppsala, Sweden. E. Loper and S. Bird. 2002. NLTK: The natural lan-guage toolkit. In Proceedings of ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics, pages 62-69, Philadelphia, PA. J. Pustejovsky and M. Verhagen. 2009. SemEval-2010 task 13: Evaluating events, time expressions, and temporal relations. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 112-116, Boulder, CO. S. Riedel. 2008. Improving the accuracy and effi-ciency of MAP inference for Markov Logic. In Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence, pages 468-475, Helsinki, Finland. M. Richardson and P. Domingos. 2006. Markov Logic networks. Machine Learning, 62(1):107-136. M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, G. Katz, and J. Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 75-80, Prague, Czech Republic. K. Yoshikawa, S. Riedel, M. Asahara, and Y. Matsu-moto. 2009. Jointly identifying temporal relations with Markov Logic. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 405-413, Suntec, Singapore. 
No Global Constraints Global Constraints Task NoLex Full NoLex Full State-of-the-art Overall 0.60 0.63 (+5%) 0.59 0.61 (+3%) NA ET 0.52 0.58 (+12%) 0.62 0.65 (+5%) 0.63 MM 0.65 0.67 (+3%)* 0.52 0.56 (+8%) 0.55 MS 0.66 0.65 (- 2%) 0.66 0.66 (+0%) 0.66 Table 2. Performance comparison between mod-els in F1 score. *Statistical significance (p<0.05) 
78
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 297?305,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Dialogue Act Modeling in a Complex Task-Oriented Domain 
  Kristy Elizabeth Boyer Eun Young Ha Robert Phillips* Michael D. Wallis* Mladen A. Vouk James C. Lester  Department of Computer Science, North Carolina State University Raleigh, North Carolina, USA  *Dual affiliation with Applied Research Associates, Inc. Raleigh, North Carolina, USA  {keboyer,?eha,?rphilli,?mdwallis,?vouk,?lester}@ncsu.edu? Abstract 
Classifying the dialogue act of a user utterance is a key functionality of a dialogue management system. This paper presents a data-driven dialogue act classifier that is learned from a corpus of human textual dialogue. The task-oriented domain involves tutoring in computer programming exercises. While engaging in the task, students generate a task event stream that is separate from and in parallel with the dialogue. To deal with this complex task-oriented dialogue, we propose a vector-based representation that encodes features from both the dialogue and the hierarchically structured task for training a maximum likelihood classifier. This classifier also leverages knowledge of the hidden dialogue state as learned separately by an HMM, which in previous work has increased the accuracy of models for predicting tutorial moves and is hypothesized to improve the accuracy for classifying student utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream. 1 Introduction Two central challenges for dialogue systems are interpreting user utterances and selecting system dialogue moves. Recent years have seen an increased focus on data-driven techniques for addressing these challenging tasks (Bangalore et al, 2008; Frampton & Lemon, 2009; Hardy et al, 2006; Sridar et al, 2009; Young et al, 2009). Much of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which 
provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al, 2009; Stolcke et al, 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management models entirely from human-human corpora (Bangalore et al, 2008; Chotimongkol, 2008; Hardy et al, 2006). Our work adopts this approach for a corpus of human-human dialogue in a task-oriented tutoring domain. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact, in our case a computer program, by the user during the course of the dialogue. Our corpus consists of human-human textual dialogue utterances and a separate, parallel stream of user-generated task actions. We utilize structural features including task/subtask, speaker, and hidden dialogue state along with lexical and syntactic features to interpret user (student) utterances.  This paper makes three contributions. First, it addresses representational issues in creating a dialogue model that integrates task actions with hierarchical task/subtask structure. The task is captured within a separate synchronous event stream that exists in parallel with the dialogue. Second, this paper explores the performance of dialogue act classifiers using different lexical/syntactic and structural feature sets. This comparison includes one model trained entirely on lexical/syntactic features, an important step toward robust unsupervised dialogue act tagging 
297
(Sridhar et al, 2009). Finally, it investigates whether the addition of HMM and task/subtask features improves the performance of the dialogue act classifiers. The findings support this hypothesis for three student dialogue moves, each with important implications for tutorial dialogue.  2 Related Work A variety of modeling approaches have been investigated for statistical dialogue act classification, including sequential approaches and vector-based classifiers. Sequential approaches typically formulate dialogue as a Markov chain in which an observation depends on a finite number of preceding observations. HMM-based approaches make use of the Markov assumption in a doubly stochastic framework that allows fitting optimal dialogue act sequences using the Viterbi algorithm (Rabiner, 1989; Stolcke et al, 2000). Like this work, the approach reported here adopts a first-order Markov formulation to train an HMM on sequences of dialogue acts, but the prediction of this HMM is subsequently encoded in a feature vector for training a vector-based classifier. Vector-based approaches, such as maximum entropy modeling, also frequently take into account both lexical/syntactic and structural features. Lexical and syntactic cues are extracted from local utterance context, while structural features involve longer dialogue act sequences and, in task-oriented domains, task/subtask history. Work by Bangalore et al (2008) on learning the structure of human-human dialogue in a catalogue-ordering domain (also extended to the Maptask and Switchboard corpora) utilizes features including words, part of speech tags, supertags, and named entities, and structural features including dialogue acts and task/subtask labels. In order to perform incremental decoding of dialogue acts and task/subtask structure, they take a greedy approach that does not require the search of complete dialogue sequences. Our work also accomplishes left-to-right incremental interpretation with a greedy approach. Our feature vectors differ from the aforementioned work slightly with respect to lexical/syntactic features and notably in the addition of a set of structural features generated by a separately trained HMM, as described in Section 4.2.  Recent work has explored the use of lexical, syntactic, and prosodic features for online dialogue act tagging (Sridhar et al, 2009); that 
work explores the notion that structural (history) features could be omitted altogether from incremental left-to-right decoding, resulting in computationally inexpensive and robust dialogue act classification. Although our textual dialogue does not feature prosodic cues, we report on the use of lexical/syntactic features alone to perform dialogue act classification, a step toward a fully unsupervised approach.   Like Bangalore et al (2008), we treat task structure as an integral part of the dialogue model. Other work that has taken this approach includes the Amiti?s project, in which a dialogue manager for a financial domain was derived entirely from a human-human corpus (Hardy et al, 2006). The TRIPS dialogue system also closely integrated task and dialogue models, for example, by utilizing the task model to facilitate indirect speech act interpretation (Allen et al, 2001). Work on the Maptask corpus has modeled task structure in the form of conversational games (Wright Hastie et al, 2002). Recent work in task-oriented domains has focused on learning task structure with unsupervised approaches (Chotimongkol, 2008). Emerging unsupervised methods, such as for detecting actions in multi-party discourse, also implicitly capture a task structure (Purver et al, 2006).  Our domain differs from the task-oriented domains described above in that our dialogues center on the user creating a persistent artifact of intrinsic value through a separate, synchronous stream of task actions. To illustrate, consider a catalogue-ordering task in which one subtask is to obtain the customer?s name. The fulfillment of this subtask occurs entirely through the dialogue, and the resulting artifact (a completed order) is produced by the system. In contrast, our task involves the user constructing a solution to a computer programming problem. The fulfillment of this task occurs partially in the dialogue through tutoring, and partially in a separate synchronous stream of user-driven task actions about which the tutor must reason. The stream of user-driven task actions produces an artifact of value in itself (a functioning computer program), and that artifact is the subject of much of the dialogue. We propose a representation that integrates task actions and dialogue acts from these streams into a shared vector-based representation, and we investigate the use of the resulting structural, lexical, and syntactic features for dialogue act classification.  
298
3 Corpus and Annotation The corpus was collected during a controlled human-human tutoring study in which tutors and students worked through textual dialogue to solve an introductory computer programming problem. The dialogues were effective: on average, students exhibited significant learning and self-confidence gains (Boyer et al, 2009).   The corpus contains 48 dialogues each with a separate, synchronous task event stream as depicted in Excerpt 1 of the appendix. There is exactly one dialogue (tutoring session) per student. The corpus captures approximately 48 hours of dialogue and contains 1,468 student utterances and 3,338 tutor utterances. Because the dialogue was textual, utterance segmentation consisted of splitting at existing sentence boundaries when more than one dialogue act was present in the utterance. This segmentation was conducted manually by the principal dialogue act annotator.1  The corpus was manually annotated with dialogue act labels and task/subtask features. Lexical and syntactic features were extracted automatically. The remainder of this section describes the manual annotation. 3.1 Dialogue Act Annotation The dialogue act annotation scheme was inspired by schemes for conversational speech (Stolcke et al, 2000) and task-oriented dialogue (Core & Allen, 1997). It was also influenced by tutoring-specific tagsets (Litman & Forbes-Riley, 2006). Inter-rater reliability for the dialogue act tagging on 10% of the corpus selected via stratified (by tutor) random sampling was ?=0.80. The dialogue act tags, their relative frequencies, and their individual kappa scores from manual annotation are displayed in Table 1.  3.2 Task Annotation All task actions were generated by the student while implementing the solution to an introductory computer programming problem in Java. These task actions were recorded as a separate event stream in parallel with the dialogue corpus. This stream included 97,509 keystroke-level user task events, which were manually aggregated into task/subtask event clusters and annotated for subtask structure and then for correctness. A total of 3,793 aggregated                                                 1 Automatic segmentation is a challenging problem in itself and is left to future work. 
student subtask actions were identified through manual annotation. The task annotation scheme is hierarchical, reflecting the nested nature of the subtasks. A subset of this task annotation scheme is depicted in Figure 1. In the models reported in this paper, the 66 leaves of the task/subtask hierarchy were encoded in the input feature vectors.   Table 1. Student dialogue acts Student?Dialogue?Act? Rel.?Freq.? Human???ACKNOWLEDGMENT?(ACK)? .17? .90?REQUEST?FOR?FEEDBACK?(RF)? .20? .91?EXTRA?DOMAIN?(EX)? .08? .79?GREETING?(GR)? .04? .92?UNCERTAIN?FEEDBACK?WITH?ELABORATION?(UE)? .01? .53?UNCERTAIN?FEEDBACK?(U)? .02? .49?NEGATIVE?FEEDBACK?WITH?ELABORATION?(NE)? .01? .61?NEGATIVE?FEEDBACK?(N)? .05? .76?POSITIVE?FEEDBACK?WITH?ELABORATION?(PE)? .02? .43?POSITIVE?FEEDBACK?(P)? .09? .81?QUESTION?(Q)? .09? .85?STATEMENT?(S)? .16? .82?THANKS?(T)? .05? 1?
 Each group of task events that occurred between dialogue utterances was tagged, possibly with many subtask labels, by a human judge. The judge aggregated the raw task keystrokes and tagged the task/subtask hierarchy for each cluster. (Please see Excerpt 1 in the appendix.) A second judge tagged 20% of the corpus in a reliability study for which one-to-one subtask identification was not enforced, an approach that was intended to give judges maximum flexibility to cluster task actions and subsequently apply the tags. All unmatched subtask tags were treated as disagreements. The resulting kappa statistic at the leaves was ?= 0.58. However, we also observe that the sequential nature of the subtasks within the larger task produces an ordinal relationship between subtasks. For example, in Figure 1, the ?distance? between subtasks 1-a and 1-b can be thought of as ?less than? the distance between subtasks 1-a vs. 3-d because those subtasks are farther from each other within the larger task. The weighted Kappa statistic (Artstein & Poesio, 2008) takes into account such an ordinal relationship and its implicit distance function. The weighted Kappa is 
299
?weighted=0.86, which indicates acceptable inter-rater reliability on the task/subtask annotation. 
 Figure 1. Portion of task annotation scheme  Along with its tag for hierarchical subtask structure, each task event was also judged for correctness according to the requirements of the task as depicted in Table 2. The agreement statistic for correctness was calculated for task events on which the two judges agreed on subtask tag. The resulting unweighted agreement statistic for correctness was ?=0.80.  Table 2. Task correctness labels  Label? Description?CORRECT? Fully? satisfying? the? requirements? of?the? learning? task.? Does? not? require?tutorial?remediation.?BUGGY? Violating? the? requirements? of? the?learning?task.?Often?requires?tutorial?remediation.?INCOMPLETE? Not? violating,? but? not? yet? fully?satisfying,? the? requirements? of? the?learning? task.? May? require? tutorial?remediation.?DISPREFERRED? Technically? satisfying? the?requirements? of? the? learning? task,?but? not? adhering? to? its? pedagogical?intentions.? Usually? requires? tutorial?remediation.?4 Features The vector-based representation for training the dialogue act classifiers integrates several sources of features: lexical and syntactic features, and structural features that include dialogue act labels, task/subtask labels, and set of hidden dialogue state prediction features.   
4.1 Lexical and Syntactic Features Lexical and syntactic features were automatically extracted from the utterances using the Stanford Parser default tokenizer and part of speech (pos) tagger (De Marneffe et al, 2006). The parser created both phrase structure trees and typed dependencies for individual sentences. From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Typed dependencies between pairs of words were extracted from each sentence. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper & Bird, 2004). The pos features were extracted in a similar way. Unigram and bigram word and pos tags were included for feature selection in the classifiers.   4.2 Structural Features Structural features include the annotated dialogue acts, the annotated task/subtask labels, and attributes that represent the hidden dialogue state. Our previous work has found that a set of hidden dialogue states, which correspond to widely accepted notions of dialogue modes in tutoring, can be identified in an unsupervised fashion (without hand labeling of the modes) by HMMs trained on manually labeled dialogue acts and task/subtask features (Boyer et al, 2009). These HMMs performed significantly better than bigram models for predicting human tutor moves (Boyer et al, 2010), which indicates that the hidden dialogue state leveraged by the HMMs has predictive value even in the presence of ?true? (manually annotated) dialogue act labels. Therefore, we hypothesized that an HMM could also improve the performance of models to classify student dialogue acts. To explore this hypothesis, we trained an HMM utilizing the methodology described in (Boyer et al, 2009) and used it to generate hidden dialogue state predictions in the form of a probability distribution over possible user utterances at each step in the dialogue. This set of stochastic features was subsequently passed to the classifier as part of the input vector (Figure 2).  4.3 Input Vectors The features were combined into a shared vector-based representation for training the classifier. As depicted in Table 3, the components of the 
300
feature vector include binary existence vectors for lexical and syntactic features for the current (target) utterance as well as for three utterances of left context (this left context may include both tutor and student utterances, which are distinguished by a separate indicator for the speaker). The task/subtask and correctness history features encode the separate stream of task events. There is no one-to-one correspondence between these history features and the left-hand dialogue context, because several task events could have occurred between a pair of dialogue events (or vice versa). This distinction is indicated in the table by the representation of dialogue time steps as [t, t-1,?] and task history steps as [task(t), task(t-1),?]. In total, the feature vectors included 11,432 attributes that were made available for feature selection.  
 Figure 2. Generation of hidden dialogue state prediction features 5 Experiments This section describes the learning of maximum likelihood vector-based models for classification of user dialogue acts. In addition to investigating the accuracy of the overall model, we also performed experiments regarding the utility of feature types for discriminating between particular dialogue acts of interest.    The classifiers are based on logistic regression, which learns a discriminant for each pair of dialogue acts by assigning weights in a maximum likelihood fashion. 2  The logistic regression models were learned using the Weka machine learning toolkit (Hall et al, 2009). For                                                 2 In general, the model that maximizes likelihood also maximizes entropy under the same constraints (Berger et al, 1996).  
feature selection, we performed attribute subset evaluation with a best-first approach that greedily searched the space of possible features using a hill climbing approach with backtracking. The prediction accuracy of the classifiers was determined through ten-fold cross-validation on the corpus, and the results below are presented in terms of prediction accuracy (number of correct classifications divided by total number of classifications) as well as by the kappa statistic, which adjusts for expected agreement by chance.    Table 3. Feature vectors 
Feature?vector?f? Description?[wt,1,?wt,|w|, pt,1,?,pt,|p|, dt,1,?,dt,|d|, st,1,?,st,|s|] 
Binary?existence?vector?for?word?unigrams?&?bigrams,?pos?unigrams?&?bigrams,?dependency?types,?and?syntactic?nodes?for?current?target?utterance?t??[wt-k,1,?wt-k,|w|, pt-k,1,?,pt-k,|p|, dt-k,1,?,dt-k,|d|, st-k,1,?,st-k,|s|]  where k=1,?,3 
Binary?existence?vector?for?word?unigrams?&?bigrams,?pos?unigrams?&?bigrams,?dependency?types,?and?syntactic?nodes?for?three?utterances?of?left?context?
[p(o1),?,p(o|S|)] Probability?distribution?for?emission?symbols?in?predicted?next?hidden?state?as?generated?by?HMM??[dat-1, dat-2, dat-3] Dialogue?act?left?context??[spt-1,spt-2, spt-3]? Speaker?label?left?context?[tktask(t-1), tktask(t-2), tktask(t-3)] Three?steps?of?subtask?history?(each?level?of?hierarchy?represented?as?a?separate?feature)??[ctask(t-1), ctask(t-2), ctask(t-3)]  
Three?steps?of?task?correctness?history?
pt Indicator?for?whether?the?target?utterance?was?immediately?preceded?by?a?task?event? 5.1 Overall Classification Task The overall dialogue act classification model was trained to classify each utterance with respect to the thirteen dialogue acts (Table 1). For this task, the feature selection algorithm selected 63 attributes including some syntax, dependency, pos, and word attributes as well as dialogue act, speaker, and task/subtask features. No hidden dialogue state features or task correctness attributes were selected. The overall classification accuracy was 62.8%. This accuracy constitutes a 369% improvement over baseline chance of 17% (the relative frequency of the most frequently occurring dialogue act, ACK). An alternate nontrivial baseline is a bigram model on true dialogue acts (including speaker tags); this model?s accuracy was 36.8%. The 
301
overall kappa for the full classifier was ?=.57. The confusion matrix for this model is depicted in Figure 3.        In addition to the classifier described above, we experimented with classifiers that used only the lexical and syntactic features of each utterance. This approach is of interest in part because it avoids the error propagation that can happen when a model relies on a series of its own previous classifications as features. The classifier that used only the set of lexical and syntactic features achieved a prediction accuracy of 60.2% and ?=.53 using 85 attributes.   
 5.2 Binary Dialogue Act Classification In tutoring, some student dialogue acts are particularly important to identify because of their implications for the tutor?s response or for the student model. For example, a student?s REQUEST FOR FEEDBACK requires the tutor to assess the condition of the task, rather than to query the in-domain factual knowledge base. UNCERTAIN FEEDBACK is another dialogue act of high importance because identifying it allows the tutor to respond in an affectively advantageous way (Forbes-Riley & Litman, 2009).  To explore which features are useful for classifying particular dialogue acts, we constructed binary dialogue act classifiers, one for each dialogue act, by preprocessing the dialogue act labels from the set of thirteen down to TRUE or FALSE depending on whether the label of the utterance matched the target dialogue act for that specialized classifier. Table 4 displays the features that were selected for each binary classifier, along with the percent accuracy and kappa for each model. Note that for some dialogue acts the chance baseline is very high, and therefore even a model with high prediction accuracy achieves a low kappa.         As depicted in Table 4, for several dialogue act models, the feature selection algorithm retained subtask and HMM features.   
Table 4. Binary DA classifiers  
DA? #?Features?Selected? %?Correct? Model???
ACK? 51? Lexical/syntax,?HMM,?DA?history?(preceding=S),?speaker?history?(preceding=Tutor)?? .933? .75?RF? 42? Lexical/syntax,?DA?history,?preceded?by?subtask? .905? .72?
EX? 57? Dependency,?pos,?word,?HMM,?DA?history?(preceding=EX),?subtask? .939? .45?
GR? 11? Syntax,?pos,?word,?DA?(previous=EMPTY),?speaker,?subtask?? .998? .97?UE? 21? Dependency,?pos,?word,?subtask? .991? .33?U? 63? Syntax,?dependency,?pos,?word,?HMM,?subtask? .979? .21?
NE? 44? Dependency,?pos,?word,?HMM,?DA?history?(2?ago=UNCERTAIN),?subtask? .987? 0?N? 83? Lexical/syntax,?DA?history,?subtask? .966? .76?PE? 90? Dependency,?pos,?word,?HMM,?subtask? .976? .10?
P? 110? Dependency,?pos,?word,?HMM,?DA?history?(previous=REQUEST?FEEDBACK)? .945? .58?Q? 43? Syntax,?dep,?pos,?word,?HMM,?subtask? .940? .60?S? 92? Syntax,?pos,?word,?HMM,?DA?history?(previous=EMPTY?or?Q)? .901? .57?
T? 29? Syntax,?pos,?word,?DA?history?(previous=POSITIVE)?(3?ago=POSITIVE)? .992? .92?    In an experiment to quantify the utility of these features, it was found that for many dialogue acts, a binary dialogue act classifier that was trained using only lexical and syntactic features achieved the same or better classification accuracy than the model that was given all features (Figure 4). For comparison, the modified baseline model used the last three true dialogue acts (with speaker tags); this model achieved better than chance for four dialogue acts and achieved nearly as well as the full model for GREETING (GR). The models that were given all possible features for selection outperformed the lexical/syntax-only model for seven of the thirteen dialogue acts (GREETING (GR), REQUEST FOR FEEDBACK (RF), POSITIVE FEEDBACK (P), POSITIVE ELABORATED FEEDBACK (PE), UNCERTAIN ELABORATED FEEDBACK (UE), NEGATIVE FEEDBACK (N), and EXTRA-DOMAIN (EX)); however, it should be noted that none of these differences in performance is statistically reliable at the p=0.05 level.   
Figure 3. Confusion matrix 
302
 Figure 4. Kappa for binary DA classifiers by features available for selection 6 Discussion We have presented a maximum likelihood classifier that assigns dialogue act labels to user utterances from a corpus of human-human tutorial dialogue given a set of lexical, syntactic, and structural features. Overall, this classifier achieved 62.8% accuracy in ten-fold cross-validation on the corpus. This performance is on par with other automatic dialogue act tagging models, both sequential and vector-based, in task-oriented domains that do not feature complex, user-driven parallel tasks. In a catalogue ordering domain with an integrated task and dialogue model, Bangalore et al (2009) report 75% classification accuracy for user utterances using a maximum entropy classifier, a 275% improvement over baseline. Poesio & Mikheev (1998) report 54% classification accuracy by utilizing conversational game structure and speaker changes in the Maptask corpus, an improvement of 170% over baseline. Recent work on Maptask reports a classification accuracy of 65.7% using local utterance (such as lexical/syntactic) features alone, with prosodic cues yielding further slight improvement (Sridhar et al, 2009). This classifier is analogous to our lexical/syntactic feature model, which achieved 60.2% accuracy. The results of these models demonstrate that, consistent with the findings in other task-oriented domains, lexical/syntactic features are highly useful for classifying student dialogue moves in this complex task-oriented domain. Models trained using those lexical/syntactic features 
performed almost universally better (with the exception of the binary classifier for GREETING) than models that were given the same left context of true dialogue act tags.  It was hypothesized that leveraging both the hidden dialogue state and hierarchical subtask features would improve the performance of the classifiers. There is some evidence that the subtask structure was helpful for the overall classifier; however, no HMM features were kept during feature selection for the overall model. Of the binary models, approximately half performed better than the overall model in terms of true positive rate; of those, three did so by including HMM or task/subtask features among the selected attributes to differentiate different tones of student feedback. However, this difference in performance was not statistically reliable. This finding suggests that, given lexical and syntactic features which are strong predictors of dialogue acts, the hidden dialogue state as captured by an an HMM may not contribute significantly to the dialogue act classification task. 7 Conclusion and Future Work Dialogue modeling for complex task-oriented domains poses significant challenges. An effective dialogue model allows systems to detect user dialogue acts so that they can respond in a manner that maximizes the chance of success. Experiments with the data-driven classifiers presented in this paper demonstrate that lexical/syntactic features can effectively classify student dialogue acts in the task-oriented tutoring domain. For POSITIVE, NEGATIVE, and UNCERTAIN ELABORATED student feedback acts, which play a key role in tutorial dialogue system, the addition of hidden dialogue state features (as learned by an HMM) and task/subtask features (annotated manually) improve classification accuracy, but not statistically reliably.    The overarching goal of this work is to create a data-driven tutorial dialogue system that learns its behavior from corpora of effective human tutoring. The dialogue act classification models reported here constitute an important step toward that goal, by integrating the dialogue stream with a parallel user-driven task event stream. The next generation of data-driven systems should leverage models that capture the rich interplay between dialogue and task. Future work will focus on data-driven approaches to task recognition and tutorial planning. Additionally, as dialogue system research addresses 
303
increasingly complex task-oriented domains, it becomes increasingly important to investigate unsupervised approaches for dialogue act classification and task recognition.   Acknowledgements.  This work is supported in part by the North Carolina State University Department of Computer Science and the National Science Foundation through a Graduate Research Fellowship and Grants CNS-0540523, REC-0632450 and IIS-0812291. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References  Allen, J., Ferguson, G., & Stent, A. (2001). An architecture for more realistic conversational systems. Proceedings of the IUI, 1-8.  Artstein, R., & Poesio, M. (2008). Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4), 555-596.  Austin, J. L. (1962). How to do things with words. Oxford: Oxford University Press. Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259.  Berger, A. L., Pietra, V. J. D., & Pietra, S. A. D. (1996). A maximum entropy approach to natural language processing. Comp. Ling., 22(1), 71.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden markov models. Proceedings of NAACL-HLT, Short Papers, 49-52.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Leveraging hidden dialogue state to select tutorial moves. Proceedings of the 5th NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Los Angeles, California.  Chotimongkol, A. (2008). Learning the structure of task-oriented conversations from the corpus of in-domain dialogs. (Unpublished Ph.D. Dissertation). Carnegie Mellon University School of Computer Science. Core, M., & Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28?35.  De Marneffe, M. C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependency parses 
from phrase structure parses. Proceedings of LREC, Genoa, Italy.   Forbes-Riley, K., & Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.  Frampton, M., & Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.  Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009). The WEKA data mining software: An update. SIGKDD Explorations, 11(1) Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., & Wu, M. (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Comm., 48(3-4), 354-373.  Litman, D., & Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.  Loper, E., & Bird, S. (2004). NLTK: The natural language toolkit. Proceedings of the ACL Demonstration Session, Barcelona, Spain. 214-217.  Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3), 130-137.  Purver, M., Kording, K. P., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topic modelling for multi-party spoken discourse. Proceedings of the ACL, Sydney, Australia. , 44(1) 17.  Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286.  Sridhar, V. K. R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4), 407-422.  Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-Dykema, C., & Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Comp. Ling., 26(3), 339-373.  Wright Hastie, H., Poesio, M., & Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.  Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., & Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.   
304
    
Time Stamp Dialogue Stream Task Stream  2008-04-11 18:23:45 Student:  so do i have to manipulate the array this time? [Q]   2008-04-11 18:23:53 Tutor:  this time, we need to do two things [S]    2008-04-11 18:24:02 Tutor:  first, we need to create a new array to hold the changed values [S]    2008-04-11 18:24:28     i 2008-04-11 18:24:28     n 2008-04-11 18:24:28     t 2008-04-11 18:24:28     \sp 1-a-i BUGGY 2008-04-11 18:24:35     \del  2008-04-11 18:24:36     \sp  2008-04-11 18:24:36     d 2008-04-11 18:24:36     o 2008-04-11 18:24:36     u 2008-04-11 18:24:36     b 2008-04-11 18:24:37     l 2008-04-11 18:24:37     e 2008-04-11 18:24:37     \sp 2008-04-11 18:24:39     [] 
1-a-i CORRECT 
2008-04-11 18:24:40     \sp  2008-04-11 18:24:42     n 2008-04-11 18:24:42     e 2008-04-11 18:24:42     w 2008-04-11 18:24:43     \sp 2008-04-11 18:24:44     \del 2008-04-11 18:24:45     T 2008-04-11 18:24:46     \del 2008-04-11 18:24:54     T 2008-04-11 18:24:54     i 2008-04-11 18:24:54     m 2008-04-11 18:24:54     e 2008-04-11 18:24:54     s 2008-04-11 18:24:55     3 2008-04-11 18:24:57     ; 
1-a-ii CORRECT 
2008-04-11 18:25:11 Student:  good? [RF]    2008-04-11 18:25:14 Tutor:  good so far, yes [PF]    2008-04-11 18:25:29 Student:  so now i have to change parts of the times array right? [Q]    2008-04-11 18:25:34 Tutor:  not quite [LF]    2008-04-11 18:25:57 Tutor:  So, when you create a new object, like a String for example, you'd say something like  String s = new String() [S]    2008-04-11 18:25:59 Tutor:  right? [AQ]    2008-04-11 18:26:06 Student:  right [P]    2008-04-11 18:26:14 Tutor:  arrays are similar [S]         
    
Appendix 
Excerpt 1. Parallel synchronous dialogue and task event streams with annotations. (Note tutor dialogue acts: AQ=ASSESSING QUESTION, LF=LUKEWARM FEEDBACK, PF=POSITIVE FEEDBACK) 
305
