eBonsai: An integrated environment for annotating treebanks
Ichikawa Hiroshi, Noguchi Masaki, Hashimoto Taiichi, Tokunaga Takenobu, Tanaka Hozumi
Department of Computer Science, Tokyo Institute of Technology
Tokyo Meguro ?Ookayama 2-12-1, Japan
ichikawa@cl.cs.titech.ac.jp
Abstract
Syntactically annotated corpora (tree-
banks) play an important role in re-
cent statistical natural language pro-
cessing. However, building a large tree-
bank is labor intensive and time con-
suming work. To remedy this prob-
lem, there have been many attempts to
develop software tools for annotating
treebanks.
This paper presents an integrated en-
vironment for annotating a treebank,
called eBonsai. eBonsai helps annota-
tors to choose a correct syntactic struc-
ture of a sentence from outputs of a
parser, allowing the annotators to re-
trieve similar sentences in the treebank
for referring to their structures.
1 Introduction
Statistical approach has been a main stream of
natural language processing research for the last
decade. Particularly, syntactically annotated cor-
pora (treebanks), such as Penn Treebank (Marcus
et al, 1993), Negra Corpus (Skut et al, 1997)
and EDR Corpus (Jap, 1994), contribute to im-
prove the performance of morpho-syntactic anal-
ysis systems. It is notorious, however, that build-
ing a large treebank is labor intensive and time
consuming work. In addition, it is quite difficult
to keep quality and consistency of a large tree-
bank. To remedy this problem, there have been
many attempts to develop software tools for anno-
tating treebanks (Plaehn and Brants, 2000; Bird et
al., 2002).
This paper presents an integrated environment
for annotating treebanks, called eBonsai. Fig-
ure 1 shows a snapshot of eBonsai. eBonsai
first performs syntactic analysis of a sentence us-
ing a parser based on GLR algorithm (MSLR
parser) (Tanaka et al, 1993), and provides can-
didates of its syntactic structure. An annotator
chooses a correct structure from these candidates.
When choosing a correct structure, the annotator
can consult the system to retrieve already anno-
tated similar sentences to make the current deci-
sion. Integration of annotation and retrieval is a
significant feature of eBonsai.
To realize the tight coupling of annotation and
retrieval, eBonsai has been implemented as the
following two plug-in modules of an universal
tool platform: Eclipse (The Eclipse Foundation,
2001).
? Annotation plug-in module: This module
helps to choose a correct syntactic structure
from candidate structures.
? Retrieval plug-in module: This module re-
trieves similar sentences to a sentence in
question from already annotated sentences in
the treebank.
These two plug-in modules work cooperatively
in the Eclipse framework. For example, infor-
mation can be transferred easily between these
two modules in a copy-and-past manner. Further-
more, since they are implemented as Eclipse plug-
in modules, these functionalities can also inter-
act with other plug-in modules and Eclipse native
features such as CVS.
108
Figure 1: A snapshot of eBonsai
	

	

	
 
   


   	       
 
  
   
                    
 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 399?406,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Efficient sentence retrieval based on syntactic structure
Ichikawa Hiroshi, Hakoda Keita, Hashimoto Taiichi and Tokunaga Takenobu
Department of Computer Science, Tokyo Institute of Technology
{ichikawa,hokoda,taiichi,take}@cl.cs.titech.ac.jp
Abstract
This paper proposes an efficient method
of sentence retrieval based on syntactic
structure. Collins proposed Tree Kernel
to calculate structural similarity. However,
structual retrieval based on Tree Kernel
is not practicable because the size of the
index table by Tree Kernel becomes im-
practical. We propose more efficient al-
gorithms approximating Tree Kernel: Tree
Overlapping and Subpath Set. These algo-
rithms are more efficient than Tree Kernel
because indexing is possible with practical
computation resources. The results of the
experiments comparing these three algo-
rithms showed that structural retrieval with
Tree Overlapping and Subpath Set were
faster than that with Tree Kernel by 100
times and 1,000 times respectively.
1 Introduction
Retrieving similar sentences has attracted much
attention in recent years, and several methods
have been already proposed. They are useful for
many applications such as information retrieval
and machine translation. Most of the methods
are based on frequencies of surface information
such as words and parts of speech. These methods
might work well concerning similarity of topics or
contents of sentences. Although the surface infor-
mation of two sentences is similar, their syntactic
structures can be completely different (Figure 1).
If a translation system regards these sentences as
similar, the translation would fail. This is because
conventional retrieval techniques exploit only sim-
ilarity of surface information such as words and
parts-of-speech, but not more abstract information
such as syntactic structures.
He beats a dog with a
V DET NN P DET
NP
PP
NP
S
stick
N
VP
VP
NP
He knows the girl with a
V DET NN P DET
NP
PP
NP
S
ribbon
N
NP
VP
NP
Figure 1: Sentences similar in appearance but dif-
fer in syntactic structure
Collins et al (Collins, 2001a; Collins, 2001b)
proposed Tree Kernel, a method to calculate a sim-
ilarity between syntactic structures. Tree Kernel
defines the similarity between two syntactic struc-
tures as the number of shared subtrees. Retrieving
similar sentences in a huge corpus requires cal-
culating the similarity between a given query and
each of sentences in the corpus. Building an index
table in advance could improve retrieval efficiency,
but indexing with Tree Kernel is impractical due to
the size of its index table.
In this paper, we propose two efficient algo-
399
rithms to calculate similarity of syntactic struc-
tures: Tree Overlapping and Subpath Set. These
algorithms are more efficient than Tree Kernel be-
cause it is possible to make an index table in rea-
sonable size. The experiments comparing these
three algorithms showed that Tree Overlapping is
100 times faster and Subpath Set is 1,000 times
faster than Tree Kernel when being used for struc-
tural retrieval.
After briefly reviewing Tree Kernel in section 2,
in what follows, we describe two algorithms in
section 3 and 4. Section 5 describes experiments
to compare these three algorithms and discussion
on the results. Finally, we conclude the paper and
look at the future direction of our research in sec-
tion 6.
2 Tree Kernel
2.1 Definition of similarity
Tree Kernel is proposed by Collins et al (Collins,
2001a; Collins, 2001b) as a method to calculate
similarity between tree structures. Tree Kernel de-
fines similarity between two trees as the number
of shared subtrees. Subtree S of tree T is defined
as any tree subsumed by T , and consisting of more
than one node, and all child nodes are included if
any.
Tree Kernel is not always suitable because the
desired properties of similarity are different de-
pending on applications. Takahashi et al pro-
posed three types of similarity based on Tree Ker-
nel (Takahashi, 2002). We use one of the similar-
ity measures (equation (1)) proposed by Takahashi
et al
KC(T1, T2) = maxn1?N1, n2?N2
C(n1, n2) (1)
where C(n1, n2) is the number of shared subtrees
by two trees rooted at nodes n1 and n2.
2.2 Algorithm to calculate similarity
Collins et al (Collins, 2001a; Collins, 2001b)
proposed an efficient method to calculate Tree
Kernel by using C(n1, n2) as follows.
? If the productions at n1 and n2 are different
C(n1, n2) = 0
? If the productions at n1 and n2 are the
same, and n1 and n2 are pre-terminals, then
C(n1, n2) = 1
? Else if the productions at n1 and n2 are the
same and n1 and n2 are not pre-terminals,
C(n1, n2) =
nc(n1)
?
i=1
(1 + C(ch(n1, i), ch(n2, i)))
(2)
where nc(n) is the number of children of node n
and ch(n, i) is the i?th child node of n. Equa-
tion (2) recursively calculates C on its child node,
and calculating Cs in postorder avoids recalcula-
tion. Thus, the time complexity of KC(T1, T2) is
O(mn), where m and n are the numbers of nodes
in T1 and T2 respectively.
2.3 Algorithm to retrieve sentences
Neither Collins nor Takahashi discussed retrieval
algorithms using Tree Kernel. We use the follow-
ing simple algorithm. First we calculate the simi-
larity KC(T1, T2) between a query tree and every
tree in the corpus and rank them in descending or-
der of KC .
Tree Kernel exploits all subtrees shared by trees.
Therefore, it requires considerable amount of time
in retrieval because similarity calculation must be
performed for every pair of trees. To improve re-
trieval time, an index table can be used in general.
However, indexing by all subtrees is difficult be-
cause a tree often includes millions of subtrees.
For example, one sentence in Titech Corpus (Noro
et al, 2005) with 22 words and 87 nodes includes
8,213,574,246 subtrees. The number of subtrees
in a tree with N nodes is bounded above by 2N .
3 Tree Overlapping
3.1 Definition of similarity
When putting an arbitrary node n1 of tree T1 on
node n2 of tree T2, there might be the same pro-
duction rule overlapping in T1 and T2. We define
CTO(n1, n2) as the number of such overlapping
production rules when n1 overlaps n2 (Figure 2).
We will define CTO(n1, n2) more precisely.
First we define L(n1, n2) of node n1 of T1 and
node n2 of T2. L(n1, n2) represents a set of pairs
of nodes which overlap each other when putting
n1 on n2. For example in Figure 2, L(b11, b21) =
{(b11, b21), (d11, d21), (e11, e21), (g11, g21), (i11, j21)}.
L(n1, n2) is defined as follows. Here ni and mi
are nodes of tree Ti, ch(n, i) is the i?th child of
node n.
1. (n1, n2) ? L(n1, n2)
400
 (1) aT2
b
d e
g
j
g
i
a
b c
d
(2)
e
g
i
b
d e
g
j
a
b c
d e
g
i
a
b c
d e
g
i
(3)
g
i
CTO(b11,b21) = 2
a
g
i
b
d e
g
j
T1
a
CTO(g11,g21) = 1
11
11
11 11
11
11
11
21
21
21
21
21 21
22
21
11
11
11 11
11
11
11
21
21
21
21
21 21
22
21
11
11
11 11
11
11
11
21
21
21
21
21 21
22
21
Figure 2: Example of similarity calculation
2. If (m1,m2) ? L(n1, n2),
(ch(m1, i), ch(m2, i)) ? L(n1, n2)
3. If (ch(m1, i), ch(m2, i)) ? L(n1, n2),
(m1,m2) ? L(n1, n2)
4. L(n1, n2) includes only pairs generated by
applying 2. and 3. recursively.
CTO(n1, n2) is defined by using L(n1, n2) as
follows.
CTO(n1, n2)
=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(m1,m2)
?
?
?
?
?
?
?
?
?
m1 ? NT (T1)
? m2 ? NT (T2)
? (m1,m2) ? L(n1, n2)
? PR(m1) = PR(m2)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
(3)
where NT (T ) is a set of nonterminal nodes in tree
T , PR(n) is a production rule rooted at node n.
Tree Overlapping similarity STO(T1, T2) is de-
fined as follows by using CTO(n1, n2).
STO(T1, T2) = max
n1?NT (T1) n2?NT (T2)
CTO(n1, n2)
(4)
This formula corresponds to equation (1) of Tree
Kernel.
As an example, we calculate STO(T1, T2) in
Figure 2 (1). Putting b11 on b21 gives Figure 2 (2)
in which two production rules b ? d e and e ? g
overlap respectively. Thus, CTO(b11, b21) becomes
2. While overlapping g11 and g21 gives Figure 2 (3)
in which only one production rule g ? i overlaps.
Thus, CTO(g11, g21) becomes 1. Since there are no
other node pairs which gives larger CTO than 2,
STO(T1, T2) becomes 2.
Table 1: Example of the index table
p I[p]
a ? b c {a11}
b ? d e {b11, b21}
e ? g {e11, e21}
g ? i {g11, g21}
a ? g b {a21}
g ? j {g21}
3.2 Algorithm
Let us take an example in Figure 3 to explain the
algorithm. Suppose that T0 is a query tree and the
corpus has only two trees, T1 and T2.
The method to find the most similar tree to a
given query tree is basically the same as Tree Ker-
nel?s (section 2.2). However, unlike Tree Kernel,
Tree Overlapping-based retrieval can be acceler-
ated by indexing the corpus in advance. Thus,
given a tree corpus, we build an index table I[p]
which maps a production rule p to its occurrences.
Occurrences of production rules are represented
by their left-hand side symbols, and are distin-
guished with respect to trees including the rule and
401
(1) T0 a(2)
b
d e
g
j
g
i
(3)a
b c
d e
Score: 2 pt. Score: 1 pt.
a
b c
d e
g
i
a
b c
d e
a
b c
d e
aT2
b
d e
g
j
g
i
a
b c
d e
g
i
T101
01
01
01
01
11
11
11 11
11
11
11
21
21
21
21
21
22
21
01
01
01
01
01
11
11
11 11
11
11
11
01
01
01
01
0121
21
21
21
21
21
22
21
21
 
Figure 3: Example of Tree Overlapping-based retrieval
the position in the tree. I[p] is defined as follows.
I[p] =
?
?
?
?
?
m
?
?
?
?
?
?
?
T ? F
? m ? NT (T )
? p = PR(m)
?
?
?
?
?
(5)
where F is the corpus (here {T1, T2}) and the
meaning of other symbols is the same as the defi-
nition of CTO (equation (3)).
Table 1 shows an example of the index table
generated from T1 and T2 in Figure 3 (1). In Ta-
ble 1, a superscript of a nonterminal symbol iden-
tifies a tree, and a subscript identifies a position in
the tree.
By using the index table, we calculate C[n,m]
with the following algorithm.
for all (n,m) do C[n,m] := 0 end
foreach n in NT (T0) do
foreach m in I[PR(n)] do
(n?,m?) := top(n,m)
C[n?,m?] := C[n?,m?] + 1
end
end
where top(n,m) returns the upper-most pair of
overlapped nodes when node n and m overlap.
The value of top uniquely identifies a situation of
overlapping two trees. Function top(n,m) is cal-
culated by the following algorithm.
function top(n,m);
begin
(n?,m?) := (n,m)
while order(n?) = order(m?) do
n? := parent(n?)
m? := parent(m?)
end
return (n?,m?)
end
where parent(n) is the parent node of n, and
order(n) is the order of node n among its siblings.
Table 2 shows example values of top(n,m) gen-
erated by overlapping T0 and T1 in Figure 3. Note
that top maps every pair of corresponding nodes
in a certain overlapping situation to a pair of the
upper-most nodes of that situation. This enables
us to use the value of top as an identifier of a situ-
ation of overlap.
Table 2: Examples of top(n,m)
(n,m) top(n,m)
(a01, a11) (a01, a11)
(b01, b11) (a01, a11)
(c01, c11) (a01, a11)
Now C[top(n,m)] = CTO(n,m), therefore the
tree similarity between a query tree T0 and each
tree T in the corpus STO(T0, T )can be calculated
by:
STO(T0, T ) = max
n?NT (T0), m?NT (T )
C[top(n,m)]
(6)
3.3 Comparison with Tree Kernel
The value of STO(T1, T2) roughly corresponds to
the number of production rules included in the
largest sub-tree shared by T1 and T2. Therefore,
this value represents the size of the subtree shared
402
by both trees, like Tree Kernel?s KC , though the
definition of the subtree size is different.
One difference is that Tree Overlapping consid-
ers shared subtrees even though they are split by a
nonshared node as shown in Figure 4. In Figure 4,
T1 and T2 share two subtrees rooted at b and c, but
their parent nodes are not identical. While Tree
Kernel does not consider the superposition putting
node a on h, Tree Overlapping considers putting a
on h and assigns count 2 to this superposition.
 a
b c
f g
(3)
d e
h
b c
f gd e
a
b c
f gd e
h
b c
f gd e
STO(T1,T2) = 2
(1) T1 (2) T2
Figure 4: Example of counting two separated
shared subtrees as one
Another, more important, difference is that Tree
Overlapping retrieval can be accelerated by index-
ing the corpus in advance. The number of indexes
is bounded above by the number of production
rules, which is within a practical index size.
4 Subpath Set
4.1 Definition of similarity
Subpath Set similarity between two trees is de-
fined as the number of subpaths shared by the
trees. Given a tree, its subpaths is defined as a
set of every path from the root node to leaves and
their partial paths.
Figure 5 (2) shows all subpaths in T1 and T2 in
Figure 5(1). Here we denotes a path as a sequence
of node names such as (a, b, d). Therefore, Sub-
path Set similarity of T1 and T2 becomes 15.
4.2 Algorithm
Suppose T0 is a query tree, TS is a set of trees in
the corpus and P (T ) is a set of subpaths of T . We
can build an index table I[p] for each production
rule p as follows.
I[p] = {T |T ? TS ? p ? P (T )} (7)
Using the index table, we can calculate the num-
ber of shared subpaths by T0 and T , S[T ], by the
following algorithm:
for all T S[T ] := 0;
foreach p in P (T0) do
foreach T in I[p] do
S[T ] := S[T ] + 1
end
end
4.3 Comparison with Tree Kernel
As well as Tree Overlapping, Subpath Set retrieval
can be accelerated by indexing the corpus. The
number of indexes is bounded above by L ? D2
where L is the maximum number of leaves of trees
(the number of words in a sentence) and D is the
maximum depth of syntactic trees. Moreover, con-
sidering a subpath as an index term, we can use
existing retrieval tools.
Subpath Set uses less structural information
than Tree Kernel and Tree Overlapping. It does
not distinguish the order and number of child
nodes. Therefore, the retrieval result tends to be
noisy. However, Subpath Set is faster than Tree
Overlapping, because the algorithm is simpler.
5 Experiments
This section describes the experiments which were
conducted to compare the performance of struc-
ture retrieval based on Tree Kernel, Tree Overlap-
ping and Subpath Set.
5.1 Data
We conducted two experiments using different an-
notated corpora. Titech corpus (Noro et al, 2005)
consists of about 20,000 sentences of Japanese
newspaper articles (Mainiti Shimbun). Each sen-
tence has been syntactically annotated by hand.
Due to the limitation of computational resources,
we used randomly selected 2,483 sentences as a
data collection.
Iwanami dictionary (Nishio et al, 1994) is a
Japanese dictionary. We extracted 57,982 sen-
tences from glosses in the dictionary. Each sen-
tences was analyzed with a morphological an-
alyzer, ChaSen (Asahara et al, 1996) and the
MSLR parser (Shirai et al, 2000) to obtain syntac-
tic structure candidates. The most probable struc-
ture with respect to PGLR model (Inui et al, 1996)
was selected from the output of the parser. Since
they were not investigated manually, some sen-
tences might have been assigned incorrect struc-
tures.
5.2 Method
We conducted two experiments Experiment I and
Experiment II with different corpora. The queries
403
(1) aT2
b
d e
g
j
g
i
a
b c
d e
g
i
T1
(c),
(a,c),
(e,g,i),
(b,e,g,i),
(a,b,e,g,i)
(2) Subpaths of T1
Subpaths of T2SSS(T1,T2) = 15
(a), (b), (d), (e), (g), (i),
(a,b), (b,d), (b,e), (e,g), (g,i),
(a,b,d), (a, b, e), (b,e,g),
(a,b,e,g)
(j),
(a,g), (g,j),
(a,g,i), (e,g,j),
(b,e,g,j),
(a,b,e,g,j)
Figure 5: Example of subpaths
were extracted from these corpora. The algorithms
described in the preceding sections were imple-
mented with Ruby 1.8.2. Table 3 outlines the ex-
periments.
Table 3: Summary of experiments
Experiment I II
Target corpus Titech Corpus Iwanami dict.
Corpus size 2,483 sent. 57,982 sent.
No. of queries 100 1,000
CPU Intel Xeon PowerPC G5
(2.4GHz) (2.3GHz)
Memory 2GB 2GB
5.3 Results and discussion
Since we select a query from the target corpus,
the query is always ranked in the first place in the
retrieval result. In what follows, we exclude the
query tree as an answer from the result.
We evaluated the algorithms based on the fol-
lowing two factors: average retrieval time (CPU
time) (Table 4) and the rank of the tree which was
top-ranked in other algorithm (Table 5). For ex-
ample, in Experiment I of Table 5, the column
??5th? of the row ?TO/TK? means that there were
73 % of the cases in which the top-ranked tree by
Tree Kernel (TK) was ranked 5th or above by Tree
Overlapping (TO).
We consider Tree Kernel (TK) as the baseline
method because it is a well-known existing simi-
larity measure and exploits more information than
others. Table 4 shows that in both corpora, the
retrieval speed of Tree Overlapping (TO) is about
Table 4: Average retrieval time per query [sec]
Algorithm Experiment I Experiment II
TK 529.42 3796.1
TO 6.29 38.3
SS 0.47 5.1
100 times faster than that of Tree Kernel, and the
retrieval speed of Subpath Set (SS) is about 1,000
times faster than that of Tree Kernel. This re-
sults show we have successfully accelerated the
retrieval speed.
The retrieval time of Tree Overlapping, 6.29
and 38.3 sec./per query, seems be a bit long. How-
ever, we can shorten this time if we tune the im-
plementation by using a compiler-type language.
Note that the current implementation uses Ruby,
an interpreter-type language.
Comparing Tree Overlapping and Subpath Set
with respect to Tree Kernel (see rows ?TK/TO?
and ?TK/SS?), the top-ranked trees by Tree Kernel
are ranked in higher places by Tree Overlapping
than by Subpath Set. This means Tree Overlap-
ping is better than Subpath Set in approximating
Tree Kernel.
Although the corpus of Experiment II is 20
times larger than that of Experiment I, the figures
of Experiment II is better than that of Experiment I
in Table 5. This could be explained as follows.
In Experiment II, we used sentences from glosses
in the dictionary, which tend to be formulaic and
short. Therefore we could find similar sentences
easier than in Experiment I.
To summarize the results, when being used in
404
Table 5: The rank of the top-ranked tree by other
algorithm [%]
Experiment I
A/B ? 1st? ? 5th ? 10th
TO/TK 34.0 73.0 82.0
SS/TK 16.0 35.0 45.0
TK/TO 29.0 41.0 51.0
SS/TO 27.0 49.0 58.0
TK/SS 17.0 29.0 37.0
TO/SS 29.0 58.0 69.0
Experiment II
A/B ? 1st? ? 5th ? 10th
TO/TK 74.6 88.0 92.0
SS/TK 65.3 78.8 84.1
TK/TO 71.1 81.0 84.6
SS/TO 73.4 86.0 89.8
TK/SS 65.5 75.9 79.7
TO/SS 76.1 87.7 92.0
similarity calculation of tree structure retrieval,
Tree Overlapping approximates Tree Kernel bet-
ter than Subpath Set, while Subpath Set is faster
than Tree Overlapping.
6 Conclusion
We proposed two fast algorithms to retrieve sen-
tences which have a similar syntactic structure:
Tree Overlapping (TO) and Subpath Set (SS). And
we compared them with Tree Kernel (TK) to ob-
tain the following results.
? Tree Overlapping-based retrieval outputs
similar results to Tree Kernel-based retrieval
and is 100 times faster than Tree Kernel-
based retrieval.
? Subpath Set-based retrieval is not so good
at approximating Tree Kernel-based retrieval,
but is 1,000 times faster than Tree Kernel-
based retrieval.
Structural retrieval is useful for annotationg cor-
pora with syntactic information (Yoshida et al,
2004). We are developing a corpus annotation tool
named ?eBonsai? which supports human to anno-
tate corpora with syntactic information and to re-
trieve syntactic structures. Integrating annotation
and retrieval enables annotators to annotate a new
instance with looking back at the already anno-
tated instances which share the similar syntactic
structure with the current one. For such purpose,
Tree Overlapping and Subpath Set alorithms con-
tribute to speed up the retrieval process, thus make
the annotation process more efficient.
However, ?similarity? of sentences is affected
by semantic aspects as well as structural aspects.
The output of the algorithms do not always con-
form with human?s intuition. For example, the
two sentences in Figure 6 have very similar struc-
tures including particles, but they are hardly con-
sidered similar from human?s viewpoint. With this
respect, it is hardly to say which algorithm is su-
perior to others.
As a future work, we need to develop a method
to integrate both content-based and structure-
based similarity measures. To this end, we have
to evaluate the algorithms in real application envi-
ronments (e.g. information retrieval and machine
translation) because desired properties of similar-
ity are different depending on applications.
References
Asahara, M. and Matsumoto, Y., Extended Models and
Tools for High-performance Part-of-Speech Tagger.
Proceedings of COLING 2000, 2000.
Collins, M. and Duffy, N. Parsing with a Single Neu-
ron: Convolution Kernels for Natural Language
Problems. Technical report UCSC-CRL-01-01, Uni-
versity of California at Santa Cruz, 2001.
Collins, M. and Duffy, N. Convolution Kernels for Nat-
ural Language. In Proceedings of NIPS 2001, 2001.
Inui, K., Shirai, K., Tokunaga T. and Tanaka H., The In-
tegration of Statistics-based Techniques in the Anal-
ysis of Japanese Sentences. Special Interest Group
of Natural Language Processing, Information Pro-
cessing Society of Japan, Vol. 96, No. 114, 1996.
Nagao, M. A framework of a mechanical translation
between Japanese and English by analogy principle.
In Alick Elithorn and Ranan Banerji, editors, Artif-
ical and Human Intelligence, pages 173-180. Ams-
terdam, 1984.
Noro, T., Koike, C., Hashimoto, T., Tokunaga, T. and
Tanaka, H. Evaluation of a Japanese CFG Derived
from a Syntactically Annotated Corpus with respect
to Dependency Measures, The 5th Workshop on
Asian Language Resources, pp.9-16, 2005.
Nishio, M., Iwabuchi, E. and Mizutani, S. (ed.)
Iwanami Kokugo Jiten, Iwanamishoten, 5th Edition,
1994.
Shirai, K., Ueki, M. Hashimoto, T., Tokunaga, T. and
Tanaka, H., MSLR Parser Tool Kit - Tools for Natu-
ral Language Analysis. Journal of Natural Language
405
P ADJ NN P N
PP
PP
S
P
VP
NP
V
NP
(to) (young) (a teaching material company) (of) (man) (SBJ ) (came)(classroom)
P ADJ NN P N
PP
PP
S
P
VP
NP
V
NP
(to) (exploded) (bombshell) (of) (piece) (SBJ ) (hit)(head)
"A young man of a teaching material company came to the classroom"
"A piece of the exploded bombshell hit his head"
Query
Top- ranked
Figure 6: Example of a retrieved similar sentence
Processing, Vol. 7, No. 5, pp. 93-112, 2000. (in
Japanese)
Somers, H., McLean, I., Jones, D. Experiments in mul-
tilingual example-based generation. CSNLP 1994:
3rd conference on the Cognitive Science of Natural
Language Processing, Dublin, 1994.
Takahashi, T., Inui K., and Matsumoto, Y.. Methods
of Estimating Syntactic Similarity. Special Interest
Group of Natural Language Processing, Information
Processing Society of Japan, NL-150-7, 2002. (in
Japanese)
Yoshida, K., Hashimoto, T., Tokunaga, T. and Tanaka,
H.. Retrieving annotated corpora for corpus annota-
tion. Proceedings of 4th International Conference on
Language Resources and Evaluation: LREC 2004.
pp.1775 ? 1778. 2004.
406
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183?192,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training a Parser for Machine Translation Reordering
Jason Katz-Brown Slav Petrov Ryan McDonald Franz Och
David Talbot Hiroshi Ichikawa Masakazu Seno Hideto Kazawa
Google
{jasonkb|slav|ryanmcd|och|talbot|ichikawa|seno|kazawa}@google.com
Abstract
We propose a simple training regime that can
improve the extrinsic performance of a parser,
given only a corpus of sentences and a way
to automatically evaluate the extrinsic quality
of a candidate parse. We apply our method
to train parsers that excel when used as part
of a reordering component in a statistical ma-
chine translation system. We use a corpus of
weakly-labeled reference reorderings to guide
parser training. Our best parsers contribute
significant improvements in subjective trans-
lation quality while their intrinsic attachment
scores typically regress.
1 Introduction
The field of syntactic parsing has received a great
deal of attention and progress since the creation of
the Penn Treebank (Marcus et al, 1993; Collins,
1997; Charniak, 2000; McDonald et al, 2005;
Petrov et al, 2006; Nivre, 2008). A common?
and valid?criticism, however, is that parsers typi-
cally get evaluated only on Section 23 of the Wall
Street Journal portion of the Penn Treebank. This
is problematic for many reasons. As previously ob-
served, this test set comes from a very narrow do-
main that does not necessarily reflect parser perfor-
mance on text coming from more varied domains
(Gildea, 2001), especially web text (Foster, 2010).
There is also evidence that after so much repeated
testing, parsers are indirectly over-fitting to this set
(Petrov and Klein, 2007). Furthermore, parsing was
never meant as a stand-alone task, but is rather a
means to an end, towards the goal of building sys-
tems that can process natural language input.
This is not to say that parsers are not used in larger
systems. All to the contrary, as parsing technology
has become more mature, parsers have become ef-
ficient and accurate enough to be useful in many
natural language processing systems, most notably
in machine translation (Yamada and Knight, 2001;
Galley et al, 2004; Xu et al, 2009). While it has
been repeatedly shown that using a parser can bring
net gains on downstream application quality, it is of-
ten unclear how much intrinsic parsing accuracy ac-
tually matters.
In this paper we try to shed some light on this is-
sue by comparing different parsers in the context of
machine translation (MT). We present experiments
on translation from English to three Subject-Object-
Verb (SOV) languages,1 because those require ex-
tensive syntactic reordering to produce grammatical
translations. We evaluate parse quality on a num-
ber of extrinsic metrics, including word reordering
accuracy, BLEU score and a human evaluation of fi-
nal translation quality. We show that while there is
a good correlation between those extrinsic metrics,
parsing quality as measured on the Penn Treebank
is not a good indicator of the final downstream ap-
plication quality. Since the word reordering metric
can be computed efficiently offline (i.e. without the
use of the final MT system), we then propose to tune
parsers specifically for that metric, with the goal of
improving the performance of the overall system.
To this end we propose a simple training regime
1We experiment with Japanese, Korean and Turkish, but
there is nothing language specific in our approach.
183
which we refer to as targeted self-training (Sec-
tion 2). Similar to self-training, a baseline model
is used to produce predictions on an unlabeled data
set. However, rather than directly training on the
output of the baseline model, we generate a list of
hypotheses and use an external signal to select the
best candidate. The selected parse trees are added
to the training data and the model is then retrained.
The experiments in Section 5 show that this simple
procedure noticeably improves our parsers for the
task at hand, resulting in significant improvements
in downstream translation quality, as measured in a
human evaluation on web text.
This idea is similar in vein to McClosky. et al
(2006) and Petrov et al (2010), except that we use an
extrinsic quality metric instead of a second parsing
model for making the selection. It is also similar to
Burkett and Klein (2008) and Burkett et al (2010),
but again avoiding the added complexity introduced
by the use of additional (bilingual) models for can-
didate selection.
It should be noted that our extrinsic metric is com-
puted from data that has been manually annotated
with reference word reorderings. Details of the re-
ordering metric and the annotated data we used are
given in Sections 3 and 4. While this annotation re-
quires some effort, such annotations are much easier
to obtain than full parse trees. In our experiments
in Section 6 we show that we can obtain similar
improvements on downstream translation quality by
targeted self-training with weakly labeled data (in
form of word reorderings), as with training on the
fully labeled data (with full syntactic parse trees).
2 Targeted Self-Training
Our technique for retraining a baseline parser is an
extension of self-training. In standard parser self-
training, one uses the baseline parsing model to
parse a corpus of sentences, and then adds the 1-best
output of the baseline parser to the training data. To
target the self-training, we introduce an additional
step, given as Algorithm 1. Instead of taking the 1-
best parse, we produce a ranked n-best list of predic-
tions and select the parser which gives the best score
according to an external evaluation function. That
is, instead of relying on the intrinsic model score,
we use an extrinsic score to select the parse towards
Algorithm 1 Select parse that maximizes an extrin-
sic metric.
Input: baseline parser B
Input: sentence S
Input: function COMPUTEEXTRINSIC(parse P )
Output: a parse for the input sentence
Pn = {P1, . . . , Pn} ? n-best parses of S by B
maxScore = 0
bestParse = ?
for k = 1 to n do
extrinsicScore = COMPUTEEXTRINSIC(Pk)
if extrinsicScore > maxScore then
maxScore = extrinsicScore
bestParse = Pk
end if
end for
return bestParse
which to update. In the case of a tie, we prefer the
parse ranked most highly in the n-best list.
The motivation of this selection step is that good
performance on the downstream external task, mea-
sured by the extrinsic metric, should be predictive
of an intrinsically good parse. At the very least,
even if the selected parse is not syntactically cor-
rect, or even if it goes against the original treebank-
ing guidelines, it results in a higher extrinsic score
and should therefore be preferred.
One could imagine extending this framework by
repeatedly running self-training on successively im-
proving parsers in an EM-style algorithm. A recent
work by Hall et al (2011) on training a parser with
multiple objective functions investigates a similar
idea in the context of online learning.
In this paper we focus our attention on machine
translation as the final application, but one could en-
vision applying our techniques to other applications
such as information extraction or question answer-
ing. In particular, we explore one application of
targeted self-training, where computing the extrin-
sic metric involves plugging the parse into an MT
system?s reordering component and computing the
accuracy of the reordering compared to a reference
word order. We now direct our attention to the de-
tails of this application.
184
3 The MT Reordering Task
Determining appropriate target language word or-
der for a translation is a fundamental problem in
MT. When translating between languages with sig-
nificantly different word order such as English and
Japanese, it has been shown that metrics which ex-
plicitly account for word-order are much better cor-
related with human judgments of translation qual-
ity than those that give more weight to word choice,
like BLEU (Lavie and Denkowski, 2009; Isozaki et
al., 2010a; Birch and Osborne, 2010). This demon-
strates the importance of getting reordering right.
3.1 Reordering as a separately evaluable
component
One way to break down the problem of translat-
ing between languages with different word order
is to handle reordering and translation separately:
first reorder source-language sentences into target-
language word order in a preprocessing step, and
then translate the reordered sentences. It has been
shown that good results can be achieved by reorder-
ing each input sentence using a series of tree trans-
formations on its parse tree. The rules for tree
transformation can be manually written (Collins et
al., 2005; Wang, 2007; Xu et al, 2009) or auto-
matically learned (Xia and McCord, 2004; Habash,
2007; Genzel, 2010).
Doing reordering as a preprocessing step, sepa-
rately from translation, makes it easy to evaluate re-
ordering performance independently from the MT
system. Accordingly, Talbot et al (2011) present a
framework for evaluating the quality of reordering
separately from the lexical choice involved in trans-
lation. They propose a simple reordering metric
based on METEOR?s reordering penalty (Lavie and
Denkowski, 2009). This metric is computed solely
on the source language side. To compute it, one
takes the candidate reordering of the input sentence
and partitions it into a set C of contiguous spans
whose content appears contiguously in the same or-
der in the reference. The reordering score is then
computed as
?(esys, eref) = 1?
|C| ? 1
|e| ? 1 .
This metric assigns a score between 0 and 1 where 1
indicates that the candidate reordering is identical to
the reference and 0 indicates that no two words that
are contiguous in the candidate reordering are con-
tiguous in the reference. For example, if a reference
reordering is A B C D E, candidate reordering A
B E C D would get score 1?(3?1)/(5?1) = 0.5.
Talbot et al (2011) show that this reordering score
is strongly correlated with human judgment of trans-
lation quality. Furthermore, they propose to evalu-
ate the reordering quality of an MT system by com-
puting its reordering score on a test set consisting
of source language sentences and their reference re-
orderings. In this paper, we take the same approach
for evaluation, and in addition, we use corpora of
source language sentences and their reference re-
orderings for training the system, not just testing
it. We describe in more detail how the reference re-
ordering data was prepared in Section 4.1.
3.2 Reordering quality as predictor of parse
quality
Figure 1 gives concrete examples of good and bad
reorderings of an English sentence into Japanese
word order. It shows that a bad parse leads to a bad
reordering (lacking inversion of verb ?wear? and ob-
ject ?sunscreen?) and a low reordering score. Could
we flip this causality around, and perhaps try to iden-
tify a good parse tree based on its reordering score?
With the experiments in this paper, we show that in-
deed a high reordering score is predictive of the un-
derlying parse tree that was used to generate the re-
ordering being a good parse (or, at least, being good
enough for our purpose).
In the case of translating English to Japanese or
another SOV language, there is a large amount of
reordering required, but with a relatively small num-
ber of reordering rules one can cover a large pro-
portion of reordering phenomena. Isozaki et al
(2010b), for instance, were able to get impressive
English?Japanese results with only a single re-
ordering rule, given a suitable definition of a head.
Hence, the reordering task depends crucially on a
correct syntactic analysis and is extremely sensitive
to parser errors.
185
4 Experimental Setup
4.1 Treebank data
In our experiments the baseline training corpus is
the Wall Street Journal (WSJ) section of the Penn
Treebank (Marcus et al, 1993) using standard train-
ing/development/testing splits. We converted the
treebank to match the tokenization expected by our
MT system. In particular, we split tokens containing
hyphens into multiple tokens and, somewhat sim-
plistically, gave the original token?s part-of-speech
tag to all newly created tokens. In Section 6 we
make also use of the Question Treebank (QTB)
(Judge et al, 2006), as a source of syntactically an-
notated out-of-domain data. Though we experiment
with both dependency parsers and phrase structure
parsers, our MT system assumes dependency parses
as input. We use the Stanford converter (de Marneffe
et al, 2006) to convert phrase structure parse trees to
dependency parse trees (for both treebank trees and
predicted trees).
4.2 Reference reordering data
We aim to build an MT system that can accurately
translate typical English text that one finds on the
Internet to SOV langauges. To this end, we ran-
domly sampled 13595 English sentences from the
web and created Japanese-word-order reference re-
orderings for them. We split the sentences arbitrarily
into a 6268-sentence Web-Train corpus and a 7327-
sentence Web-Test corpus.
To make the reference alignments we used the
technique suggested by Talbot et al (2011): ask
annotators to translate each English sentence to
Japanese extremely literally and annotate which En-
glish words align to which Japanese words. Golden
reference reorderings can be made programmati-
cally from these annotations. Creating a large set
of reference reorderings is straightforward because
annotators need little special background or train-
ing, as long as they can speak both the source and
target languages. We chose Japanese as the target
language through which to create the English refer-
ence reorderings because we had access to bilingual
annotators fluent in English and Japanese.
Good parse
Reordered:
15 or greater of an SPF has that sunscreen Wear
Reordering score: 1.0 (matches reference)
Bad parse
Reordered:
15 or greater of an SPF has that Wear sunscreen
Reordering score: 0.78 (?Wear? is out of place)
Figure 1: Examples of good and bad parses and cor-
responding reorderings for translation from English to
Japanese. The good parse correctly identifies ?Wear? as
the main verb and moves it to the end of the sentence; the
bad parse analyses ?Wear sunscreen? as a noun phrase
and does not reorder it. This example was one of the
wins in the human evaluation of Section 5.2.
4.3 Parsers
The core dependency parser we use is an implemen-
tation of a transition-based dependency parser using
an arc-eager transition strategy (Nivre, 2008). The
parser is trained using the averaged perceptron algo-
rithm with an early update strategy as described in
Zhang and Clark (2008). The parser uses the fol-
lowing features: word identity of the first two words
on the buffer, the top word on the stack and the head
of the top word on the stack (if available); part-of-
speech identities of the first four words on the buffer
and top two words on the stack; dependency arc la-
bel identities for the top word on the stack, the left
and rightmost modifier of the top word on the stack,
and the leftmost modifier of the first word in the
buffer. We also include conjunctions over all non-
lexical features.
We also give results for the latent variable parser
(a.k.a. BerkeleyParser) of Petrov et al (2006). We
convert the constituency trees output by the Berke-
leyParser to labeled dependency trees using the same
procedure that is applied to the treebanks.
While the BerkeleyParser views part-of-speech
(POS) tagging as an integral part of parsing, our
dependency parser requires the input to be tagged
186
with a separate POS tagger. We use the TnT tag-
ger (Brants, 2000) in our experiments, because of
its efficiency and ease of use. Tagger and parser are
always trained on the same data.
For all parsers, we lowercase the input at train and
test time. We found that this improves performance
in parsing web text. In addition to general upper-
case/lowercase noisiness of the web text negatively
impacting scores, we found that the baseline case-
sensitive parsers are especially bad at parsing imper-
ative sentences, as discussed in Section 5.3.2.
4.4 Reordering rules
In this paper we focus on English to Japanese, Ko-
rean, and Turkish translation. We use a superset of
the reordering rules proposed by Xu et al (2009),
which flatten a dependency tree into SOV word or-
der that is suitable for all three languages. The rules
define a precedence order for the dependents of each
part of speech. For example, a slightly simplified
version of the precedence order of child labels for
a verbal head HEADVERB is: advcl, nsubj, prep,
[other children], dobj, prt, aux, neg, HEADVERB,
mark, ref, compl.
Alternatively, we could have used an automatic
reordering-rule learning framework like that of Gen-
zel (2010). Because the reordering accuracy met-
ric can be computed for any source/target language
pair, this would have made our approach language
completely independent and applicable to any lan-
guage pair. We chose to use manually written rules
to eliminate the variance induced by the automatic
reordering-rule learning framework.
4.5 MT system
We carried out all our translation experiments on a
state-of-the-art phrase-based statistical MT system.
During both training and testing, the system reorders
source-language sentences in a preprocessing step
using the above-mentioned rules. During decoding,
we used an allowed jump width of 4 words. In ad-
dition to the regular distance distortion model, we
incorporate a maximum entropy based lexicalized
phrase reordering model (Zens and Ney, 2006) as
a feature used in decoding.
Overall for decoding, we use between 20 to
30 features, whose weights are optimized using
MERT (Och, 2003). All experiments for a given lan-
guage pair use the same set of MERT weights tuned
on a system using a separate parser (that is neither
the baseline nor the experiment parser). This po-
tentially underestimates the improvements that can
be obtained, but also eliminates MERT as a pos-
sible source of improvement, allowing us to trace
back improvements in translation quality directly to
parser changes.2
For parallel training data, we use a custom collec-
tion of parallel documents. They come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. For all language pairs, we
trained on approximately 300 million source words
each.
5 Experiments Reordering Web Text
We experimented with parsers trained in three dif-
ferent ways:
1. Baseline: trained only on WSJ-Train.
2. Standard self-training: trained on WSJ-Train
and 1-best parse of the Web-Train set by base-
line parser.
3. Targeted self-training: trained on WSJ-Train
and, for each sentence in Web-Train, the parse
from the baseline parser?s 512-best list that
when reordered gives the highest reordering
score.3
5.1 Standard self-training vs targeted
self-training
Table 1 shows that targeted self-training on Web-
Train significantly improves Web-Test reordering
score more than standard self-training for both the
shift-reduce parser and for the BerkeleyParser. The
reordering score is generally divorced from the at-
tachment scores measured on the WSJ-Test tree-
bank: for the shift-reduce parser, Web-Test reorder-
ing score and WSJ-Test labeled attachment score
2We also ran MERT on all systems and the pattern of im-
provement is consistent, but sometimes the improvement is big-
ger or smaller after MERT. For instance, the BLEU delta for
Japanese is +0.0030 with MERT on both sides as opposed to
+0.0025 with no MERT.
3We saw consistent but diminishing improvements as we in-
creased the size of the n-best list.
187
Parser Web-Test reordering WSJ-Test LAS
Shift-reduce WSJ baseline 0.757 85.31%
+ self-training 1x 0.760 85.26%
+ self-training 10x 0.756 84.14%
+ targeted self-training 1x 0.770 85.19%
+ targeted self-training 10x 0.777 84.48%
Berkeley WSJ baseline 0.780 88.66%
+ self-training 1x 0.785 89.21%
+ targeted self-training 1x 0.790 89.32%
Table 1: English?Japanese reordering scores on Web-Test for standard self-training and targeted self-training on
Web-Train. Label ?10x? indicates that the self-training data was weighted 10x relative to the WSJ training data.
Bolded reordering scores are different from WSJ-only baseline with 95% confidence but are not significantly different
from each other within the same group.
English to BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.1777 0.1802 2.56 2.69 yes (at 95% level)
Korean 0.3229 0.3259 2.61 2.70 yes (at 90% level)
Turkish 0.1344 0.1370 2.10 2.20 yes (at 95% level)
Table 2: BLEU scores and human evaluation results for translation between three language pairs, varying only the
parser between systems. ?WSJ-only? corresponds to the baseline WSJ-only shift-reduce parser; ?Targeted? corre-
sponds to the Web-Train targeted self-training 10x shift-reduce parser.
(LAS) are anti-correlated, but for BerkeleyParser
they are correlated. Interestingly, weighting the self-
training data more seems to have a negative effect on
both metrics.4
One explanation for the drops in LAS is that some
parts of the parse tree are important for downstream
reordering quality while others are not (or only to
a lesser extent). Some distinctions between labels
become less important; for example, arcs labeled
?amod? and ?advmod? are transformed identically
by the reordering rules. Some semantic distinctions
also become less important; for example, any sane
interpretation of ?red hot car? would be reordered
the same, that is, not at all.
5.2 Translation quality improvement
To put the improvement of the MT system in terms
of BLEU score (Papineni et al, 2002), a widely used
metric for automatic MT evaluation, we took 5000
sentences from Web-Test and had humans gener-
ate reference translations into Japanese, Korean, and
4We did not attempt this experiment for the BerkeleyParser
since training was too slow.
Turkish. We then trained MT systems varying only
the parser used for reordering in training and decod-
ing. Table 2 shows that targeted self-training data
increases BLEU score for translation into all three
languages.
In addition to BLEU increase, a side-by-side hu-
man evaluation on 500 sentences (sampled from
the 5000 used to compute BLEU scores) showed
a statistically significant improvement for all three
languages (see again Table 2). For each sen-
tence, we asked annotators to simultaneously score
both translations from 0 to 6, with guidelines
that 6=?Perfect?, 4=?Most Meaning/Grammar?,
2=?Some Meaning/Grammar?, 0=?Nonsense?. We
computed confidence intervals for the average score
difference using bootstrap resampling; a difference
is significant if the two-sided confidence interval
does not include 0.
5.3 Analysis
As the divergence between the labeled attachment
score on the WSJ-Test data and the reordering score
on the WSJ-Test data indicates, parsing web text
188
Parser Click as N Click as V Imperative rate
case-sensitive shift-reduce WSJ-only 74 0 6.3%
case-sensitive shift-reduce + Web-Train targeted self-training 75 0 10.5%
case-insensitive shift-reduce WSJ-only 75 0 10.3%
case-insensitive shift-reduce + Web-Train targeted self-training 75 0 11.6%
Berkeley WSJ-only 35 35 11.9%
Berkeley + Web-Train targeted self-training 13 58 12.5%
(WSJ-Train) 1 0 0.7%
Table 3: Counts on Web-Test of ?click? tagged as a noun and verb and percentage of sentences parsed imperatively.
poses very different challenges compared to parsing
newswire. We show how our method improves pars-
ing performance and reordering performance on two
examples: the trendy word ?click? and imperative
sentences.
5.3.1 Click
The word ?click? appears only once in the train-
ing portion of the WSJ (as a noun), but appears many
times in our Web test data. Table 3 shows the distri-
bution of part-of-speech tags that different parsers
assign to ?click?. The WSJ-only parsers tag ?click?
as a noun far too frequently. The WSJ-only shift-
reduce parser refuses to tag ?click? as a verb even
with targeted self-training, but BerkeleyParser does
learn to tag ?click? more often as a verb.
It turns out that the shift-reduce parser?s stub-
bornness is not due to a fundamental problem of
the parser, but due to an artifact in TnT. To in-
crease speed, TnT restricts the choices of tags for
known words to previously-seen tags. This causes
the parser?s n-best lists to never hypothesize ?click?
as a verb, and self-training doesn?t click no matter
how targeted it is. This shows that the targeted self-
training approach heavily relies on the diversity of
the baseline parser?s n-best lists.
It should be noted here that it would be easy to
combine our approach with the uptraining approach
of Petrov et al (2010). The idea would be to use the
BerkeleyParser to generate the n-best lists; perhaps
we could call this targeted uptraining. This way, the
shift-reduce parser could benefit both from the gen-
erally higher quality of the parse trees produced by
the BerkeleyParser, as well as from the information
provided by the extrinsic scoring function.
5.3.2 Imperatives
As Table 3 shows, the WSJ training set contains
only 0.7% imperative sentences.5 In contrast, our
test sentences from the web contain approximately
10% imperatives. As a result, parsers trained exclu-
sively on the WSJ underproduce imperative parses,
especially a case-sensitive version of the baseline.
Targeted self-training helps the parsers to predict im-
perative parses more often.
Targeted self-training works well for generating
training data with correctly-annotated imperative
constructions because the reordering of main sub-
jects and verbs in an SOV language like Japanese
is very distinct: main subjects stay at the begin-
ning of the sentence, and main verbs are reordered
to the end of the sentence. It is thus especially easy
to know whether an imperative parse is correct or
not by looking at the reference reordering. Figure 1
gives an example: the bad (WSJ-only) parse doesn?t
catch on to the imperativeness and gets a low re-
ordering score.
6 Targeted Self-Training vs Training on
Treebanks for Domain Adaptation
If task-specific annotation is cheap, then it is rea-
sonable to consider whether we could use targeted
self-training to adapt a parser to a new domain as
a cheaper alternative to making new treebanks. For
example, if we want to build a parser that can reorder
question sentences better than our baseline WSJ-
only parser, we have these two options:
1. Manually construct PTB-style trees for 2000
5As an approximation, we count every parse that begins with
a root verb as an imperative.
189
questions and train on the resulting treebank.
2. Create reference reorderings for 2000 questions
and then do targeted self-training.
To compare these approaches, we created reference
reordering data for our train (2000 sentences) and
test (1000 sentences) splits of the Question Tree-
bank (Judge et al, 2006). Table 4 shows that both
ways of training on QTB-Train sentences give sim-
ilarly large improvements in reordering score on
QTB-Test. Table 5 confirms that this corresponds
to very large increases in English?Japanese BLEU
score and subjective translation quality. In the hu-
man side-by-side comparison, the baseline transla-
tions achieved an average score of 2.12, while the
targeted self-training translations received a score of
2.94, where a score of 2 corresponds to ?some mean-
ing/grammar? and ?4? corresponds to ?most mean-
ing/grammar?.
But which of the two approaches is better? In
the shift-reduce parser, targeted self-training gives
higher reordering scores than training on the tree-
bank, and in BerkeleyParser, the opposite is true.
Thus both approaches produce similarly good re-
sults. From a practical perspective, the advantage of
targeted self-training depends on whether the extrin-
sic metric is cheaper to calculate than treebanking.
For MT reordering, making reference reorderings is
cheap, so targeted self-training is relatively advanta-
geous.
As before, we can examine whether labeled at-
tachment score measured on the test set of the
QTB is predictive of reordering quality. Table 4
shows that targeted self-training raises LAS from
64.78?69.17%. But adding the treebank leads
to much larger increases, resulting in an LAS of
84.75%, without giving higher reordering score. We
can conclude that high LAS is not necessary to
achieve top reordering scores.
Perhaps our reordering rules are somehow defi-
cient when it comes to reordering correctly-parsed
questions, and as a result the targeted self-training
process steers the parser towards producing patho-
logical trees with little intrinsic meaning. To explore
this possibility, we computed reordering scores after
reordering the QTB-Test treebank trees directly. Ta-
ble 4 shows that this gives reordering scores similar
to those of our best parsers. Therefore it is at least
possible that the targeted self-training process could
have resulted in a parser that achieves high reorder-
ing score by producing parses that look like those in
the QuestionBank.
7 Related Work
Our approach to training parsers for reordering is
closely related to self/up-training (McClosky. et al,
2006; Petrov et al, 2010). However, unlike uptrain-
ing, our method does not use only the 1-best output
of the first-stage parser, but has access to the n-best
list. This makes it similar to the work of McClosky.
et al (2006), except that we use an extrinsic metric
(MT reordering score) to select a high quality parse
tree, rather than a second, reranking model that has
access to additional features.
Targeted self-training is also similar to the re-
training of Burkett et al (2010) in which they
jointly parse unannotated bilingual text using a mul-
tiview learning objective, then retrain the monolin-
gual parser models to include each side of the jointly
parsed bitext as monolingual training data. Our ap-
proach is different in that it doesn?t use a second
parser and bitext to guide the creation of new train-
ing data, and instead relies on n-best lists and an
extrinsic metric.
Our method can be considered an instance of
weakly or distantly supervised structured prediction
(Chang et al, 2007; Chang et al, 2010; Clarke et al,
2010; Ganchev et al, 2010). Those methods attempt
to learn structure models from related external sig-
nals or aggregate data statistics. This work differs
in two respects. First, we use the external signals
not as explicit constraints, but to compute an ora-
cle score used to re-rank a set of parses. As such,
there are no requirements that it factor by the struc-
ture of the parse tree and can in fact be any arbitrary
metric. Second, our final objective is different. In
weakly/distantly supervised learning, the objective
is to use external knowledge to build better struc-
tured predictors. In our case this would mean using
the reordering metric as a means to train better de-
pendency parsers. Our objective, on the other hand,
is to use the extrinsic metric to train parsers that are
specifically better at the reordering task, and, as a re-
sult, better suited for MT. This makes our work more
in the spirit of Liang et al (2006), who train a per-
190
Parser QTB-Test reordering QTB-Test LAS
Shift-reduce WSJ baseline 0.663 64.78%
+ treebank 1x 0.704 77.12%
+ treebank 10x 0.768 84.75%
+ targeted self-training 1x 0.746 67.84%
+ targeted self-training 10x 0.779 69.17%
Berkeley WSJ baseline 0.733 76.50%
+ treebank 1x 0.800 87.79%
+ targeted self-training 1x 0.775 80.64%
(using treebank trees directly) 0.788 100%
Table 4: Reordering and labeled attachment scores on QTB-Test for treebank training and targeted self-training on
QTB-Train.
English to QTB-Test BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.2379 0.2615 2.12 2.94 yes (at 95% level)
Table 5: BLEU scores and human evaluation results for English?Japanese translation of the QTB-Test corpus, varying
only the parser between systems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training
10x shift-reduce parser.
ceptron model for an end-to-end MT system where
the alignment parameters are updated based on se-
lecting an alignment from a n-best list that leads to
highest BLEU score. As mentioned earlier, this also
makes our work similar to Hall et al (2011) who
train a perceptron algorithm on multiple objective
functions with the goal of producing parsers that are
optimized for extrinsic metrics.
It has previously been observed that parsers of-
ten perform differently for downstream applications.
Miyao et al (2008) compared parser quality in the
biomedical domain using a protein-protein interac-
tion (PPI) identification accuracy metric. This al-
lowed them to compare the utility of extant depen-
dency parsers, phrase structure parsers, and deep
structure parsers for the PPI identification task. One
could apply the targeted self-training technique we
describe to optimize any of these parsers for the PPI
task, similar to how we have optimized our parser
for the MT reordering task.
8 Conclusion
We introduced a variant of self-training that targets
parser training towards an extrinsic evaluation met-
ric. We use this targeted self-training approach to
train parsers that improve the accuracy of the word
reordering component of a machine translation sys-
tem. This significantly improves the subjective qual-
ity of the system?s translations from English into
three SOV languages. While the new parsers give
improvements in these external evaluations, their in-
trinsic attachment scores go down overall compared
to baseline parsers trained only on treebanks. We
conclude that when using a parser as a component
of a larger external system, it can be advantageous
to incorporate an extrinsic metric into parser train-
ing and evaluation, and that targeted self-training is
an effective technique for incorporating an extrinsic
metric into parser training.
References
A. Birch and M. Osborne. 2010. LRscore for evaluating
lexical and reordering quality in MT. In ACL-2010
WMT.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP ?00.
D. Burkett and D. Klein. 2008. Two languages are better
than one (for syntactic parsing). In EMNLP ?08.
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unannotated
bilingual text. In CoNLL ?10.
191
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL
?07.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In ICML ?10.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In CoNLL ?10.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In ACL
?05.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL ?97.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ?06.
J. Foster. 2010. ?cba to check the spelling?: Investigat-
ing parser performance on discussion forum posts. In
NAACL ?10.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In HLT-NAACL ?04.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
COLING ?10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ?01.
N. Habash. 2007. Syntactic preprocessing for statistical
machine translation. In MTS ?07.
K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.
2011. Training dependency parsers by jointly optimiz-
ing multiple objectives. In EMNLP ?11.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010a. Automatic evaluation of translation quality for
distant language pairs. In EMNLP ?10.
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010b.
Head finalization: A simple reordering rule for SOV
languages. In ACL-2010 WMT.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
Bank: creating a corpus of parse-annotated questions.
In ACL ?06.
A. Lavie and M. Denkowski. 2009. The Meteor metric
for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL ?06.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
D. McClosky., E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL ?06.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
?05.
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsu-
jii. 2008. Task-oriented evaluation of syntactic parsers
and their representations. In ACL ?08.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL ?02.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
S. Petrov, P. Chang, and M. Ringgaard H. Alshawi. 2010.
Uptraining for accurate deterministic question parsing.
In EMNLP ?10.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalua-
tion framework for machine translation reordering. In
EMNLP-2011 WMT.
C. Wang. 2007. Chinese syntactic reordering for statisti-
cal machine translation. In EMNLP ?07.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. In
Coling ?04.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In NAACL-HLT ?09.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL ?01.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In NAACL-
06 WMT.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In EMNLP ?08.
192
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 557?562,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A joint inference of deep case analysis and zero subject generation for
Japanese-to-English statistical machine translation
Taku Kudo, Hiroshi Ichikawa, Hideto Kazawa
Google Japan
{taku,ichikawa,kazawa}@google.com
Abstract
We present a simple joint inference of
deep case analysis and zero subject gener-
ation for the pre-ordering in Japanese-to-
English machine translation. The detec-
tion of subjects and objects from Japanese
sentences is more difficult than that from
English, while it is the key process to gen-
erate correct English word orders. In addi-
tion, subjects are often omitted in Japanese
when they are inferable from the context.
We propose a new Japanese deep syntac-
tic parser that consists of pointwise proba-
bilistic models and a global inference with
linguistic constraints. We applied our new
deep parser to pre-ordering in Japanese-to-
English SMT system and show substantial
improvements in automatic evaluations.
1 Introduction
Japanese to English translation is known to be one
of the most difficult language pair for statistical
machine translation (SMT). It has been widely be-
lieved for years that the difference of word or-
ders, i.e., Japanese is an SOV language, while En-
glish is an SVO language, makes the English-to-
Japanese and Japanese-to-English translation dif-
ficult. However, simple, yet powerful pre-ordering
techniques have made this argument a thing of the
past (Isozaki et al, 2010b; Komachi et al, 2006;
Fei and Michael, 2004; Lerner and Petrov, 2013;
Wu et al, 2011; Katz-Brown and Collins, 2008;
Neubig et al, 2012; Hoshino et al, 2013). Pre-
ordering processes the source sentence in such a
way that word orders appear closer to their final
positions on the target side.
While many successes of English-to-Japanese
translation have been reported recently, the quality
improvement of Japanese-to-English translation is
still small even with the help of pre-ordering (Goto
et al, 2013). We found that there are two ma-
jor issues that make Japanese-to-English transla-
tion difficult. One is that Japanese subject and ob-
ject cannot easily be identified compared to En-
glish, while their detections are the key process
to generate correct English word orders. Japanese
surface syntactic structures are not always corre-
sponding to their deep structures, i.e., semantic
roles. The other is that Japanese is a pro-drop lan-
guage in which certain classes of pronouns may
be omitted when they are pragmatically inferable.
In Japanese-to-English translation, these omitted
pronouns have to be generated properly.
There are several researches that focused on the
pre-ordering with Japanese deep syntactic analysis
(Komachi et al, 2006; Hoshino et al, 2013) and
zero pronoun generation (Taira et al, 2012) for
Japanese-to-English translation. However, these
two issues have been considered independently,
while they heavily rely on one another.
In this paper, we propose a simple joint infer-
ence which handles both Japanese deep structure
analysis and zero pronoun generation. To the best
of our knowledge, this is the first study that ad-
dresses these two issues at the same time.
This paper is organized as follows. First, we de-
scribe why Japanese-to-English translation is dif-
ficult. Second, we show the basic idea of this
work and its implementation based on pointwise
probabilistic models and a global inference with
an integer linear programming (ILP). Several ex-
periments are employed to confirm that our new
model can improve the Japanese to English trans-
lation quality.
2 What makes Japanese-to-English
translation difficult?
Japanese syntactic relations between arguments
and predicates are usually specified by particles.
There are several types of particles, but we focus
on ? (ga), ? (wo) and ? (wa) for the sake of
557
Table 1: An example of difficult sentence for pars-
ing
Sentence: ?? ? ?? ? ???.
Gloss: today wa TOP liquor ga NOM can drink.
Translation: (I) can drink liquor today.
simplicity
1
.
? ga is usually a subject marker. However, it
becomes an object marker if the predicate has
a potential voice type, which is usually trans-
lated into can, be able to, want to, or would
like to.
? wo is an object marker.
? wa is a topic case marker. The topic can be
anything that a speaker wants to talk about. It
can be subject, object, location, time or any
other grammatical elements.
We cannot always identify Japanese subject and
object only by seeing the surface case markers ga,
wo and wa. Especially the topic case marker is
problematic, since there is no concept of topic in
English. It is necessary to get a deep interpretation
of topic case markers in order to develop accurate
Japanese-to-English SMT systems.
Another big issue is that Japanese subject (or
even an object) can be omitted when they can
pragmatically be inferable from the context. Such
a pronoun-dropping is not a unique phenomenon
in Japanese actually. For instance, Spanish also
allows to omit pronouns. However, the inflec-
tional suffix of Spanish verbs include a hint of the
person of the subject. On the other hand, infer-
ring Japanese subjects is more difficult than Span-
ish, since Japanese verbs usually do not have any
grammatical cues to tell the subject type.
Table 1 shows an example Japanese sentence
which cannot be parsed only with the surface
structure. The second token wa specifies the rela-
tion between?? (today) and??? (can drink).
Human can easily tell that the relation of them is
not a subject but an adverb (time). The topic case
marker wa implies that the time when the speaker
drinks liquor is the focus of this sentence. The
4th token ga indicates the relation between ??
(liquor) and ??? (can drink). Since the predi-
cate has a potential voice (can drink), the ga par-
ticle should be interpreted as an object here. In
1
Other case markers are less frequent than these three
markers
this sentence, the subject is omitted. In general, it
is unknown who speaks this sentence, but the first
person is a natural interpretation in this context.
Another tricky phenomenon is that detecting
voice type is not always deterministic. There
are several ways to generate a potential voice in
Japanese, but we usually put the suffix word??
(reru) or ??? (rareru) after predicates. How-
ever, these suffix words are also used for a passive
voice.
In summary, we can see that the following
four factors are the potential causes that make the
Japanese parsing difficult.
? Japanese voice type detection is not straight-
forward. reru or rareru are used either for
passive or potential voice.
? surface case ga changes its interpretation
from subject to object when the predicate has
a potential voice.
? topic case marker wa is used as a topic case
marker which doesn?t exist in English. Topic
is either subject, object or any grammatical
elements depending on the context.
? Japanese subject is often omitted when it is
inferable from the context. There is no cue to
tell the subject person in verb suffix (inflec-
tion) like in Spanish verbs
We should note that they are not always inde-
pendent issues. For instance, the deep case detec-
tion helps to tell the voice type, and vice versa.
Another note is that they are unique issues
observed only in Japanese-to-English translation.
In English-to-Japanese translation, it is accept-
able to generate Japanese sentences that do not
use Japanese topic markers wa. Also, generating
Japanese pronoun from English pronoun is accept-
able, although it sounds redundant and unnatural
for native speakers.
3 A joint inference of deep case analysis
and zero subject generation
3.1 Probabilistic model over
predicate-argument structures
Our deep parser runs on the top of a dependency
parse tree. First, it extracts all predicates and their
arguments from a dependency tree by using man-
ual rules over POS tags. Since our pre-ordering
system generates the final word orders from a
labeled dependency tree, we formalize our deep
558
parsing task as a simple labeling problem over de-
pendency links, where the label indicates the deep
syntactic roles between head and modifier.
We here define a joint probability over a predi-
cate and its arguments as follows:
P (p, z, v, A, S,D) (1)
where
? p: a predicate
? z: a zero subject candidate for p. z ? Z =
{I, you, we, it, he/she, imperative, already exists}
? v: voice type of the predicate p. v ? V =
{active, passive, potential}
? a
k
? A: k-th argument which modifies or is
modified by the predicate
2
.
? d
k
? D: deep case label which represents a
deep relation between a
k
and p. d ? { sub-
ject, object, other }, where other means that
deep case is neither subject nor object.
? s
k
? S: surface relation (surface case
marker) between a
k
and p.
We assume that a predicate p is independent
from other predicates in a sentence. This assump-
tion allows us to estimate the deep structures of p
separately, with no regard to which decisions are
made in other predicates.
An optimal zero subject label z, deep cases D,
and voice type v for a given predicate p can be
solved as the following optimization problem.
?z?, v?,
?
D? = argmax
z,v,D
P (p, z, v, A, S,D)
Since the inference of this joint probability is diffi-
cult, we decompose P (p, z, v, A, S,D) into small
independent sub models:
P (p, z, v, A, S,D) ?
P
z
(z|p,A, S)P
v
(v|p,A, S)
P
d
(D|p, v, A, S)P (p,A, S) (2)
We do not take the last term P (p,A, S) into con-
sideration, since it is constant for the optimization.
In the next sections, we describe how these proba-
bilities P
z
, P
d
, and P
v
are computed.
2
Generally, an argument modifies a predicate, but in rela-
tive clauses, a predicate modifies an argument
3.1.1 Zero subject model: P
z
(z|p,A, S)
This model estimates the syntactic zero subject
3
of the predicate p. For instance, z= Imeans that the
subject of p is omitted and its type is first person.
z=imperative means that we do not need to aug-
ment a subject because the predicate is imperative.
z=already exists means that a subject already ap-
pears in the sentence. A maximum entropy classi-
fier is used in our zero subject model, which takes
the contextual features extracted from p, A, and S.
3.1.2 Voice type model: P
v
(v|p,A, S)
This model estimates the voice type of a predicate.
We also use a maximum entropy classifier for this
model. This classifier is used only when the predi-
cate has the ambiguous suffix reru or rareru. If the
predicate does not have any ambiguous suffix, this
model returns pre-defined voice types with with
very high probabilities.
3.1.3 Deep case model: P
d
(D|p, v,A, S)
This model estimates the deep syntactic role be-
tween a predicate p and its arguments A. This
model helps to resolve the deep cases when their
surface cases are topic. We define P
d
as follows
after introducing an independent assumption over
predicate-argument structures:
P (D|p, v, A, S) ?
?
i
[max(p(d
i
|a
i
, p) ? m(s
i
, d
i
, v), ?)].
p(d|a, p) models the deep relation between p and
a. We use a maximum likelihood estimation for
p(d|a, p):
p(d = subj|a, p) =
freq(s = ga, a, active form of p)
freq(a, active form of p)
p(d = obj|a, p) =
freq(s = wo, a, active form of p)
freq(a, active form of p)
,
where freq(s = ga, a, active form of p) is the
frequency of how often an argument a and p ap-
pears with the surface case ga. The frequencies
are aggregated only when the predicate appear in
active voice. If the voice type is active, we can
safely assume that the surface cases ga and wo
correspond to subject and object respectively. We
compute the frequencies from a large amount of
auto-parsed data.
m(s, d, v) is a non-negative penalty variable de-
scribing how the deep case d generates the sur-
face case s depending on the voice type v. Since
3
Here syntactic subject means the subject which takes the
voice type into account.
559
the number of possible surface cases, deep cases,
and voice types are small, we define this penalty
manually by referring to the Japanese grammar
book (descriptive grammar research group, 2009).
We use these manually defined penalties in order
to put more importance on syntactic preferences
rather than those of semantics. Even if a predicate-
augment structure is semantically irrelevant, we
take this structure as long as it is syntactically cor-
rect in order to avoid SMT from generating liberal
translations.
? is a very small positive constant to avoid zero
probability.
3.2 Joint inference with linguistic constraints
Our initial model (2) assumes that zero subjects
and deep cases are generated independently. How-
ever, this assumption does not always capture
real linguistic phenomena. English is a subject-
prominent language in which almost all sentences
(or predicates) must have a subject. This implies
that it is more reasonable to introduce strong lin-
guistic constraints to the final solution for pre-
ordering, which are described as follows:
? Subject is a mandatory role. A subject must
be inferred either by zero subject or deep case
model
4
. When the voice type is passive, an
object role in D is considered as a syntactic
subject.
? A predicate can not have multiple subjects
and objects respectively.
These two constraints avoid the model from in-
ferring syntactically irrelevant solutions.
In order to find the result with the constraints
above, we formalize our model as an integer lin-
ear programming, ILP. Let {x
1
, , ..., x
n
} be bi-
nary variables, i.e., x
i
? {0, 1}. x
i
corresponds
to the binary decisions in our model, e.g., x
k
=
1 if d
i
= subj and v = active. Let {p
1
, ..., p
n
} be
probability vector corresponding to the binary de-
cisions. ILP can be formalized as a mathematical
problem, in which the objective function and the
constraints are linear:
{x?
1
, ..., x?
n
} = argmax
{x
1
,...,x
n
}?{0,1}
n
n
?
i=1
log(p
i
)x
i
s.t. linear constraints over {x
1
, .., x
n
}.
After taking the log of (2), our optimization model
can be converted into an ILP. Also, the constraints
4
imperative is also handled as an invisible subject
described above can be represented as linear equa-
tions over binary variablesX . We leave the details
of the representations to (Punyakanok et al, 2004;
Iida and Poesio, 2011).
3.3 Japanese pre-ordering with deep parser
We use a simple rule-based approach to make pre-
ordered Japanese sentences from our deep parse
trees, which is similar to the algorithms described
in (Komachi et al, 2006; Katz-Brown and Collins,
2008; Hoshino et al, 2013). First, we naively re-
verse all the bunsetsu-chunks
5
. Then, we move
a subject chunk just before its predicate. This
process converts SOV to SVO. When the subject
is omitted, we generate a subject with our deep
parser and insert it to a subject position in the
source sentence. There are three different ways
to generate a subject.
1. Generate real Japanese words (Insert ? (I),
??? (you).. etc)
2. Generate virtual seed Japanese words (Insert
1st person, 2nd person..., which are not in
the Japanese lexicon.)
3. Generate only a single virtual seed Japanese
word regardless of the subject type. (Insert
zero subject)
1) is the most aggressive method, but it causes
completely incorrect translations if the detection
of subject type fails. 2) and 3) is rather conser-
vative, since they leave SMT to generate English
pronouns.
We decided to use the following hybrid ap-
proach, since it shows the best performance in our
preliminary experiments.
? In the training of SMT, use 3).
? In decoding, use 1) if the input sentence only
has one predicate. Otherwise, use 3).
3.4 Examples of parsing results
Table 2 shows examples of our deep parser output.
It can be seen that our parser can correctly identify
the deep case of topic case markers wa.
5
bunsetsu is a basic Japanese grammatical unit consisting
of one content word and functional words.
560
Table 2: Examples of deep parser output
??? (today wa) {d=other} ?? (liquor ga) {d=obj} ??? (can drink) {v=potential, z=I}
????? (news ga) {d=subj} ????? (was broadcast) {v=passive, z=already exist}
???? (pasta wa) {d=obj} ?????? (ate+question) {v=active, z=you}
???? (you wa) {d=subj} ?????? (ate+question) {v=active, z=already exist}
4 Experiments
4.1 Experimental settings
We carried out all our experiments using a state-
of-the-art phrase-based statistical Japanese-to-
English machine translation system (Och, 2003)
with pre-ordering. During the decoding, we
use the reordering window (distortion limit) to 4
words. For parallel training data, we use an in-
house collection of parallel sentences. These come
from various sources with a substantial portion
coming from the web. We trained our system on
about 300M source words. Our test set contains
about 10,000 sentences randomly sampled from
the web.
The dependency parser we apply is an imple-
mentation of a shift-reduce dependency parser
which uses a bunsetsu-chunk as a basic unit for
parsing (Kudo and Matsumoto, 2002).
The zero subject and voice type models were
trained with about 20,000 and 5,000 manually an-
notated web sentences respectively. In order to
simplify the rating tasks for our annotators, we ex-
tracted only one candidate predicate from a sen-
tence for annotations.
We tested the following six systems.
? baseline: no pre-ordering.
? surface reordering : pre-ordering only with
surface dependency relations.
? independent deep reordering: pre-ordering
using deep parser without global linguistic
constraints.
? independent deep reordering + zero sub-
ject: pre-ordering using deep parser and zero
subject generation without global linguistic
constraints.
? joint deep reordering: pre-ordering using
our new deep parser with global linguistic
constraints.
? joint deep reordering + zero-subject: pre-
ordering using deep parser and zero subject
generation with global linguistic constraints.
Table 3: Results for different reordering methods
System BLEU RIBES
baseline (no reordering) 16.15 52.67
surface reordering 19.39 60.30
independent deep reordering 19.68 61.27
independent deep reordering + zero subj. 19.81 61.67
joint deep reordering 19.76 61.43
joint deep reordering + zero subj. 19.90 61.89
As translation metrics, we used BLEU (Pap-
ineni et al, 2002), as well as RIBES (Isozaki et
al., 2010a), which is designed for measuring the
quality of distant language pairs in terms of word
orders.
4.2 Results
Table 3 shows the experimental results for six pre-
reordering systems. It can be seen that the pro-
posed method with deep parser outperforms base-
line and naive reordering with surface syntactic
trees. The zero subject generation can also im-
prove both BLEU and RIBES scores, but the im-
provements are smaller than those with reordering.
Also, joint inference with global linguistics con-
straints outperforms the model which solves deep
syntactic analysis and zero subject generation in-
dependently.
5 Conclusions
In this paper, we proposed a simple joint inference
of deep case analysis and zero subject generation
for Japanese-to-English SMT. Our parser consists
of pointwise probabilistic models and a global in-
ference with linguistic constraints. We applied our
new deep parser to pre-ordering in Japanese-to-
English SMT system and showed substantial im-
provements in automatic evaluations.
Our future work is to enhance our deep parser so
that it can handle other linguistic phenomena, in-
cluding causative voice, coordinations, and object
ellipsis. Also, the current system is built on the
top of a dependency parser. The final output of our
deep parser is highly influenced by the parsing er-
rors. It would be interesting to develop a full joint
inference of dependency parsing and deep syntac-
tic analysis.
561
References
Japan descriptive grammar research group. 2009. Con-
temporary Japanese grammar book 2. Part 3. Case
and Syntax, Part 4. Voice. Kuroshio Publishers.
Xia Fei and McCord Michael. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proc. of ACL.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proc. of NTCIR.
Sho Hoshino, Yusuke Miyao, Katsuhito Sudoh, and
Masaaki Nagata. 2013. Two-stage pre-ordering
for japanese-to-english statistical machine transla-
tion. In Proc. IJCNLP.
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ilp solution to zero anaphora resolution. In Proc. of
ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proc. of EMNLP. Association for Compu-
tational Linguistics.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proc. of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR.
Jason Katz-Brown and Michael Collins. 2008. Syntac-
tic reordering in preprocessing for japanese ? en-
glish translation: Mit system description for ntcir-
7 patent translation task. In Proc. of the NTCIR-7
Workshop Meeting.
Mamoru Komachi, Masaaki Nagata, and Yuji Mat-
sumoto. 2006. Phrase reordering for statistical ma-
chine translation based on predicate-argument struc-
ture. In Proc. of the International Workshop on Spo-
ken Language Translation.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of CoNLL.
Uri Lerner and Slav Petrov. 2013. Source-side classi-
fier preordering for machine translation. In Proc. of
EMNLP.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proc. of EMNLP.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proc. of ACL.
Hirotoshi Taira, Katsuhito Sudoh, and Masaaki Na-
gata. 2012. Zero pronoun resolution can improve
the quality of je translation. In Proc. of Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proc. of IJCNLP.
562
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 12?21,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
A Lightweight Evaluation Framework for Machine Translation Reordering
David Talbot1 and Hideto Kazawa2 and Hiroshi Ichikawa2
Jason Katz-Brown2 and Masakazu Seno2 and Franz J. Och1
1 Google Inc. 2 Google Japan
1600 Amphitheatre Parkway Roppongi Hills Mori Tower
Mountain View, CA 94043 6-10-1 Roppongi, Tokyo 106-6126
{talbot, och}@google.com {kazawa, ichikawa}@google.com
{jasonkb, seno}@google.com
Abstract
Reordering is a major challenge for machine
translation between distant languages. Recent
work has shown that evaluation metrics that
explicitly account for target language word or-
der correlate better with human judgments of
translation quality. Here we present a simple
framework for evaluating word order indepen-
dently of lexical choice by comparing the sys-
tem?s reordering of a source sentence to ref-
erence reordering data generated from manu-
ally word-aligned translations. When used to
evaluate a system that performs reordering as
a preprocessing step our framework allows the
parser and reordering rules to be evaluated ex-
tremely quickly without time-consuming end-
to-end machine translation experiments. A
novelty of our approach is that the translations
used to generate the reordering reference data
are generated in an alignment-oriented fash-
ion. We show that how the alignments are
generated can significantly effect the robust-
ness of the evaluation. We also outline some
ways in which this framework has allowed our
group to analyze reordering errors for English
to Japanese machine translation.
1 Introduction
Statistical machine translation systems can perform
poorly on distant language pairs such as English
and Japanese. Reordering errors are a major source
of poor or misleading translations in such systems
(Isozaki et al, 2010). Unfortunately the stan-
dard evaluation metrics used by the statistical ma-
chine translation community are relatively insensi-
tive to the long-distance reordering phenomena en-
countered when translating between such languages
(Birch et al, 2010).
The ability to rapidly evaluate the impact of
changes on a system can significantly accelerate the
experimental cycle. In a large statistical machine
translation system, we should ideally be able to ex-
periment with separate components without retrain-
ing the complete system. Measures such as per-
plexity have been successfully used to evaluate lan-
guage models independently in speech recognition
eliminating some of the need for end-to-end speech
recognition experiments. In machine translation,
alignment error rate has been used with some mixed
success to evaluate word-alignment algorithms but
no standard evaluation frameworks exist for other
components of a machine translation system (Fraser
and Marcu, 2007).
Unfortunately, BLEU (Papineni et al, 2001) and
other metrics that work with the final output of a ma-
chine translation system are both insensitive to re-
ordering phenomena and relatively time-consuming
to compute: changes to the system may require the
realignment of the parallel training data, extraction
of phrasal statistics and translation of a test set. As
training sets grow in size, the cost of end-to-end ex-
perimentation can become significant. However, it is
not clear that measurements made on any single part
of the system will correlate well with human judg-
ments of the translation quality of the whole system.
Following Collins et al (2005a) and Wang (2007),
Xu et al (2009) showed that when translating from
English to Japanese (and to other SOV languages
such as Korean and Turkish) applying reordering as
12
a preprocessing step that manipulates a source sen-
tence parse tree can significantly outperform state-
of-the-art phrase-based and hierarchical machine
translation systems. This result is corroborated by
Birch et al (2009) whose results suggest that both
phrase-based and hierarchical translation systems
fail to capture long-distance reordering phenomena.
In this paper we describe a lightweight framework
for measuring the quality of the reordering compo-
nents in a machine translation system. While our
framework can be applied to any translation sys-
tem in which it is possible to derive a token-level
alignment from the input source tokens to the out-
put target tokens, it is of particular practical interest
when applied to a system that performs reordering
as a preprocessing step (Xia and McCord, 2004). In
this case, as we show, it allows for extremely rapid
and sensitive analysis of changes to parser, reorder-
ing rules and other reordering components.
In our framework we evaluate the reordering pro-
posed by a system separately from its choice of tar-
get words by comparing it to a reference reordering
of the sentence generated from a manually word-
aligned translation. Unlike previous work (Isozaki
et al, 2010), our approach does not rely on the sys-
tem?s output matching the reference translation lexi-
cally. This makes the evaluation more robust as there
may be many ways to render a source phrase in the
target language and we would not wish to penalize
one that simply happens not to match the reference.
In the next section we review related work on
reordering for translation between distant language
pairs and automatic approaches to evaluating re-
ordering in machine translation. We then describe
our evaluation framework including certain impor-
tant details of how our reference reorderings were
created. We evaluate the framework by analyz-
ing how robustly it is able to predict improvements
in subjective translation quality for an English to
Japanese machine translation system. Finally, we
describe ways in which the framework has facili-
tated development of the reordering components in
our system.
2 Related Work
2.1 Evaluating Reordering
The ability to automatically evaluate machine trans-
lation output has driven progress in statistical ma-
chine translation; however, shortcomings of the
dominant metric, BLEU (Papineni et al, 2001) , par-
ticularly with respect to reordering, have long been
recognized (Callison-burch and Osborne, 2006).
Reordering has also been identified as a major fac-
tor in determining the difficulty of statistical ma-
chine translation between two languages (Birch et
al., 2008) hence BLEU scores may be most unreli-
able precisely for those language pairs for which sta-
tistical machine translation is most difficult (Isozaki
et al, 2010).
There have been many results showing that met-
rics that account for reordering are better correlated
with human judgements of translation quality (Lavie
and Denkowski, 2009; Birch and Osborne, 2010;
Isozaki et al, 2010). Examples given in Isozaki et
al. (2010) where object and subject arguments are
reversed in a Japanese to English statistical machine
translation system demonstrate how damaging re-
ordering errors can be and it should therefore not
come as a surprise that word order is a strong pre-
dictor of translation quality; however, there are other
advantages to be gained by focusing on this specific
aspect of the translation process in isolation.
One problem for all automatic evaluation metrics
is that multiple equally good translations can be con-
structed for most input sentences and typically our
reference data will contain only a small fraction of
these. Equally good translations for a sentence may
differ both in terms of lexical choice and word or-
der. One of the potential advantages of designing a
metric that looks only at word order, is that it may,
to some extent, factor out variability along the di-
mension of the lexical choice. Previous work on au-
tomatic evaluation metrics that focus on reordering,
however, has not fully exploited this.
The evaluation metrics proposed in Isozaki et al
(2010) compute a reordering score by comparing
the ordering of unigrams and bigrams that appear
in both the system?s translation and the reference.
These scores are therefore liable to overestimate
the reordering quality of sentences that were poorly
translated. While Isozaki et al (2010) does propose
13
a work-around to this problem which combines the
reordering score with a lexical precision term, this
clearly introduces a bias in the metric whereby poor
translations are evaluated primarily on their lexical
choice and good translations are evaluated more on
the basis of their word order. In our experience
word order is particularly poor in those sentences
that have the lowest lexical overlap with reference
translations; hence we would like to be able to com-
pute the quality of reordering in all sentences inde-
pendently of the quality of their lexical choice.
Birch and Osborne (2010) are closer to our ap-
proach in that they use word alignments to induce a
permutation over the source sentence. They com-
pare a source-side permutation generated from a
word alignment of the reference translation with one
generated from the system?s using various permuta-
tion distances. However, Birch and Osborne (2010)
only demonstrate that these metrics are correlated
with human judgements of translation quality when
combined with BLEU score and hence take lexical
choice into account.
Birch et al (2010) present the only results we
are aware of that compute the correlation be-
tween human judgments of translation quality and
a reordering-only metric independently of lexical
choice. Unfortunately, the experimental set-up there
is somewhat flawed. The authors ?undo? reorderings
in their reference translations by permuting the ref-
erence tokens and presenting the permuted transla-
tions to human raters. While many machine trans-
lation systems (including our own) assume that re-
ordering and translation can be factored into sepa-
rate models, e.g. (Xia and McCord, 2004), and per-
form these two operations in separate steps, the lat-
ter conditioned on the former, Birch et al (2010) are
making a much stronger assumption when they per-
form these simulations: they are assuming that lexi-
cal choice and word order are entirely independent.
It is easy to find cases where this assumption does
not hold and we would in general be very surprised
if a similar change in the reordering component in
our system did not also result in a change in the lex-
ical choice of the system; an effect which their ex-
periments are unable to model.
Another minor difference between our evaluation
framework and (Birch et al, 2010) is that we use
a reordering score that is based on the minimum
number of chunks into which the candidate and ref-
erence permutations can be concatenated similar to
the reordering component of METEOR (Lavie and
Denkowski, 2009). As we show, this is better cor-
related with human judgments of translation quality
than Kendall?s ? . This may be due to the fact that
it counts the number of ?jumps? a human reader has
to make in order to parse the system?s order if they
wish to read the tokens in the reference word order.
Kendall?s ? on the other hand penalizes every pair
of words that are in the wrong order and hence has
a quadratic (all-pairs) flavor which in turn might ex-
plain why Birch et al (2010) found that the square-
root of this quantity was a better predictor of trans-
lation quality.
2.2 Evaluation Reference Data
To create the word-aligned translations from which
we generate our reference reordering data, we used
a novel alignment-oriented translation method. The
method (described in more detail below) seeks
to generate reference reorderings that a machine
translation system might reasonably be expected to
achieve. Fox (2002) has analyzed the extent to
which translations seen in a parallel corpus can be
broken down into clean phrasal units: they found
that most sentence pairs contain examples of re-
ordering that violate phrasal cohesion, i.e. the cor-
responding words in the target language are not
completely contiguous or solely aligned to the cor-
responding source phrase. These reordering phe-
nomena are difficult for current statistical transla-
tion models to learn directly. We therefore deliber-
ately chose to create reference data that avoids these
phenomena as much as possible by having a single
annotator generate both the translation and its word
alignment. Our word-aligned translations are cre-
ated with a bias towards simple phrasal reordering.
Our analysis of the correlation between reorder-
ing scores computed on reference data created from
such alignment-oriented translations with scores
computed on references generated from standard
professional translations of the same sentences sug-
gests that the alignment-oriented translations are
more useful for evaluating a current state-of-the-art
system. We note also that while prior work has con-
jectured that automatically generated alignments are
a suitable replacement for manual alignments in the
14
context of reordering evaluation (Birch et al, 2008),
our results suggest that this is not the case at least for
the language pair we consider, English-Japanese.
3 A Lightweight Reordering Evaluation
We now present our lightweight reordering evalu-
ation framework; this consists of (1) a method for
generating reference reordering data from manual
word-alignments; and (2) a reordering metric for
scoring a sytem?s proposed reordering against this
reference data; and (3) a stand-alone evaluation tool.
3.1 Generating Reference Reordering Data
We follow Birch and Osborne (2010) in using ref-
erence reordering data that consists of permuations
of source sentences in a test set. We generate these
from word alignments of the source sentences to
reference translations. Unlike previous work, how-
ever, we have the same annotator generate both the
reference translation and the word alignment. We
also explicitly encourage the translators to generate
translations that are easy to align even if this does
result in occasionally unnatural translations. For in-
stance in English to Japanese translation we require
that all personal pronouns are translated; these are
often omitted in natural Japanese. We insist that
all but an extremely small set of words (articles and
punctuation for English to Japanese) be aligned. We
also disprefer non-contiguous alignments of a sin-
gle source word and require that all target words be
aligned to at least one source token. In Japanese
this requires deciding how to align particles that
mark syntactic roles; we choose to align these to-
gether with the content word (jiritsu-go) of the cor-
responding constituent (bunsetsu). Asking annota-
tors to translate and perform word alignment on the
same sentence in a single session does not necessar-
ily increase the annotation burden over stand-alone
word alignment since it encourages the creation of
alignment-friendly translations which can be aligned
more rapidly. Annotators need little special back-
ground or training for this task, as long as they can
speak both the source and target languages.
To generate a permutation from word alignments
we rank the source tokens by the position of the first
target token to which they are aligned. If multiple
source tokens are aligned to a single target word
or span we ignore the ordering within these source
spans; this is indicated by braces in Table 2. We
place unaligned source words immediately before
the next aligned source word or at the end of the
sentence if there is none. Table 2 shows the ref-
erence reordering derived from various translations
and word alignments.
3.2 Fuzzy Reordering Score
To evaluate the quality of a system?s reordering
against this reference data we use a simple reorder-
ing metric related to METEOR?s reordering compo-
nent (Lavie and Denkowski, 2009) . Given the refer-
ence permutation of the source sentence ?ref and the
system?s reordering of the source sentence ?sys ei-
ther generated directly by a reordering component or
inferred from the alignment between source and tar-
get phrases used in the decoder, we align each word
in ?sys to an instance of itself in ?ref taking the first
unmatched instance of the word if there is more than
one. We then define C to be the number chunks of
contiguously aligned words. If M is the number of
words in the source sentence then the fuzzy reorder-
ing score is computed as,
FRS(?sys, ?ref) = 1?
C ? 1
M ? 1
. (1)
This metric assigns a score between 0 and 1 where
1 indicates that the system?s reordering is identical
to the reference. C has an intuitive interpretation as
the number of times a reader would need to jump in
order to read the system?s reordering of the sentence
in the order proposed by the reference.
3.3 Evaluation Tool
While the framework we propose can be applied to
any machine translation system in which a reorder-
ing of the source sentence can be inferred from the
translation process, it has proven particularly use-
ful applied to a system that performs reordering as
a separate preprocessing step. Such pre-ordering
approaches (Xia and McCord, 2004; Collins et al,
2005b) can be criticized for greedily committing to
a single reordering early in the pipeline but in prac-
tice they have been shown to perform extremely well
on language pairs that require long distance reorder-
ing and have been successfully combined with other
more integrated reordering models (Xu et al, 2009).
15
The performance of a parser-based pre-ordering
component is a function of the reordering rules and
parser; it is therefore desirable that these can be eval-
uated efficiently. Both parser and reordering rules
may be evaluated using end-to-end automatic met-
rics such as BLEU score or in human evaluations.
Parsers may also be evaluated using intrinsic tree-
bank metrics such as labeled accuracy. Unfortu-
nately these metrics are either expensive to compute
or, as we show, unpredictive of improvements in hu-
man perceptions of translation quality.
Having found that the fuzzy reordering score pro-
posed here is well-correlated with changes in human
judgements of translation quality, we established a
stand-alone evaluation tool that takes a set of re-
ordering rules and a parser and computes the re-
ordering scores on a set of reference reorderings.
This has become the most frequently used method
for evaluating changes to the reordering component
in our system and has allowed teams working on
parsing, for instance, to contribute significant im-
provements quite independently.
4 Experimental Set-up
We wish to determine whether our evaluation frame-
work can predict which changes to reordering com-
ponents will result in statistically significant im-
provements in subjective translation quality of the
end-to-end system. To that end we created a num-
ber of systems that differ only in terms of reorder-
ing components (parser and/or reordering rules). We
then analyzed the corpus- and sentence-level corre-
lation of our evaluation metric with judgements of
human translation quality.
Previous work has compared either quite separate
systems, e.g. (Isozaki et al, 2010), or systems that
are artificially different from each other (Birch et al,
2010). There has also been a tendency to measure
corpus-level correlation. We are more interested in
comparing systems that differ in a realistic manner
from one another as would typically be required in
development. We also believe sentence-level cor-
relation is more important than corpus-level corre-
lation since good sentence-level correlation implies
that a metric can be used for detailed analysis of a
system and potentially to optimize it.
4.1 Systems
We carried out all our experiments using a state-of-
the-art phrase-based statistical English-to-Japanese
machine translation system (Och, 2003). Dur-
ing both training and testing, the system reorders
source-language sentences in a preprocessing step
using a set of rules written in the framework pro-
posed by (Xu et al, 2009) that reorder an English
dependency tree into target word order. During de-
coding, we set the reordering window to 4 words.
In addition to the regular distance distortion model,
we incorporate a maximum entropy based lexical-
ized phrase reordering model (Zens and Ney, 2006).
For parallel training data, we use an in-house collec-
tion of parallel documents. These come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. We trained our system on
about 300 million source words.
The reordering rules applied to the English de-
pendency tree define a precedence order for the chil-
dren of each head category (a coarse-grained part of
speech). For example, a simplified version of the
precedence order for child labels of a verbal head
HEADVERB is: advcl, nsubj, prep, [other children],
dobj, prt, aux, neg, HEADVERB, mark, ref, compl.
The dependency parser we use is an implementa-
tion of a transition-based dependency parser (Nivre,
2008). The parser is trained using the averaged per-
ceptron algorithm with an early update strategy as
described in Zhang and Clark (2008).
We created five systems using different parsers;
here targeted self-training refers to a training pro-
cedure proposed by Katz-Brown et al (2011) that
uses our reordering metric and separate reference re-
ordering data to pick parses for self-training: an n-
best list of parses is generated for each English sen-
tence for which we have reference reordering data
and the parse tree that results in the highest fuzzy
reordering score is added to our parser?s training set.
Parsers P3, P4 and P5 differ in how that framework
is applied and how much data is used.
? P1 Penn Treebank, perceptron, greedy search
? P2 Penn Treebank, perceptron, beam search
? P3 Penn Treebank, perceptron, beam search,
targeted self-training on web data
16
? P4 Penn Treebank, perceptron, beam search,
targeted self-training on web data
? P5 Penn Treebank, perceptron, beam search,
targeted self-training on web data, case insen-
sitive
We also created five systems using the fifth parser
(P5) but with different sets of reordering rules:
? R1 No reordering
? R2 Reverse reordering
? R3 Head final reordering with reverse reorder-
ing for words before the head
? R4 Head final reordering with reverse reorder-
ing for words after the head
? R5 Superset of rules from (Xu et al, 2009)
Reverse reordering places words in the reverse of the
English order. Head final reordering moves the head
of each dependency after all its children. Rules in R3
and R4 overlap significantly with the rules for noun
and verb subtrees respectively in R5. Otherwise all
systems were identical. The rules in R5 have been
extensively hand-tuned while R1 and R2 are rather
naive. System P5R5 was our best performing system
at the time these experiments were conducted.
We refer to systems by a combination of parser
and reordering rules set identifiers, for instance, sys-
tem P2R5, uses parser P2 with reordering rules R5.
We conducted two subjective evaluations in which
bilingual human raters were asked to judge trans-
lations on a scale from 0 to 6 where 0 indicates
nonsense and 6 is perfect. The first experiment
(Parsers) contrasted systems with different parsers
and the second (Rules) varied the reordering rules.
In each case three bilingual evaluators were shown
the source sentence and the translations produced by
all five systems.
4.2 Meta-analysis
We perform a meta-analysis of the following metrics
and the framework by computing correlations with
the results of these subjective evaluations of transla-
tion quality:
1. Evaluation metrics: BLEU score on final trans-
lations, Kendall?s ? and fuzzy reordering score
on reference reordering data
2. Evaluation data: both manually-generated and
automatically-generated word alignments on
both standard professional and alignment-
oriented translations of the test sentences
The automatic word alignments were generated us-
ing IBM Model 1 in order to avoid directional biases
that higher-order models such as HMMs have.
Results presented in square parentheses are 95
percent confidence intervals estimated by bootstrap
resampling on the test corpus (Koehn, 2004).
Our test set contains 500 sentences randomly
sampled from the web. We have both professional
and alignment-friendly translations for these sen-
tences. We created reference reorderings for this
data using the method described in Section 3.1.
The lack of a broad domain and publically avail-
able Japanese test corpus makes the use of this non-
standard test set unfortunately unavoidable.
The human raters were presented with the source
sentence, the human reference translation and the
translations of the various systems simultaneously,
blind and in a random order. Each rater was allowed
to rate no more than 3 percent of the sentences and
three ratings were elicited for each sentence. Rat-
ings were a single number between 0 and 6 where 0
indicates nonsense and 6 indicates a perfectly gram-
matical translation of the source sentence.
5 Results
Table 2 shows four reference reorderings generated
from various translations and word alignments. The
automatic alignments are significantly sparser than
the manual ones but in these examples the refer-
ence reorderings still seem reasonable. Note how the
alignment-oriented translation includes a pronoun
(translation for ?I?) that is dropped in the slightly
more natural standard translation to Japanese.
Table 1 shows the human judgements of transla-
tion quality for the 10 systems (note that P5R5 ap-
pears in both experiments but was scored differently
as human judgments are affected by which other
translations are present in an experiment). There is a
clear ordering of the systems in each experiment and
17
1. Parsers Subjective Score (0-6) 2. Rules Subjective Score (0-6)
P1R5 2.173 [2.086, 2.260] P5R1 1.258 [1.191, 1.325]
P2R5 2.320 [2.233, 2.407] P5R2 1.825 [1.746, 1.905]
P3R5 2.410 [2.321, 2.499] P5R3 1.849 [1.767, 1.931]
P4R5 2.453 [2.366, 2.541] P5R4 2.205 [2.118, 2.293]
P5R5 2.501 [2.413, 2.587] P5R5 2.529 [2.441, 2.619]
Table 1: Human judgements of translation quality for 1. Parsers and 2. Rules.
Metric Sentence-level correlation
r ?
Fuzzy reordering 0.435 0.448
Kendall?s ? 0.371 0.450
BLEU 0.279 0.302
Table 6: Pearson?s correlation (r) and Spearman?s rank
correlation (?) with subjective translation quality at
sentence-level.
we see that both the choice of parser and reordering
rules clearly effects subjective translation quality.
We performed pairwise significance tests using
bootstrap resampling for each pair of ?improved?
systems in each experiment. Tables 3, 4 and 5
shows which pairs were judged to be statistically
significant improvements at either 95 or 90 percent
level under the different metrics. These tests were
computed on the same 500 sentences. All pairs
but one are judged to be statistically significant im-
provements in subjective translation quality. Sig-
nificance tests performed using the fuzzy reorder-
ing metric are identical to the subjective scores for
the Parsers experiment but differ on one pairwise
comparison for the Rules experiment. According to
BLEU score, however, none of the parser changes
are significant at the 95 percent level and only one
pairwise comparison (between the two most differ-
ent systems) was significant at the 90 percent level.
BLEU score appears more sensitive to the larger
changes in the Rules experiment but is still in dis-
agreement with the results of the human evaluation
on four pairwise comparisons.
Table 6 shows the sentence-level correlation of
different metrics with human judgments of transla-
tion quality. Here both the fuzzy reordering score
and Kendall?s ? are computed on the reference
reordering data generated as described in Section
3.1. Both metrics are computed by running our
Translation Alignment Sentence-level
r ?
Alignment-oriented Manual 0.435 0.448
Alignment-oriented Automatic 0.234 0.252
Standard Manual 0.271 0.257
Standard Automatic 0.177 0.159
Table 7: Pearson?s correlation (r) and Spearman?s rank
correlation (?) with subjective translation quality at the
sentence-level for different types of reordering reference
data: (i) alignment-oriented translation vs. standard, (ii)
manual vs. automatic alignment.
lightweight evaluation tool and involve no transla-
tion whatsoever. These lightweight metrics are also
more correlated with subjective quality than BLEU
score at the sentence level.
Table 7 shows how the correlation between fuzzy
reordering score and subjective translation quality
degrades as we move from manual to automatic
alignments and from alignment-oriented translations
to standard ones. The automatically aligned refer-
ences, in particular, are less correlated with subjec-
tive translation scores then BLEU; we believe this
may be due to the poor quality of word alignments
for languages such as English and Japanese due to
the long-distance reordering between them.
Finally we present some intrinsic evaluation met-
rics for the parsers used in the first of our experi-
ments. Table 8 demonstrates that certain changes
may not be best captured by standard parser bench-
marks. While the first four parser models improve
on the WSJ benchmarks as they improve subjective
translation quality the best parser according to sub-
jective translation qualtiy (P5) is actually the worst
under both metrics on the treebank data. We con-
jecture that this is due to the fact that P5 (unlike the
other parsers) is case insensitive. While this helps us
significantly on our test set drawn from the web, it
18
Standard / ManualSource              How Can I Qualify For A Mortgage Tax Deduction ?Reordering        A Mortgage {{ Tax Deduction }} For I Qualify How Can ?Translation                        ?? ??? ?? ? ?? ? ?? ? ?? ? ? ?? ?? ? ?? ?? ? ?Alignment          6,6,7_8,4,3,3,3,3,3,0,0,0,0,0,1,1,9,9
Alignment-oriented / ManualSource              How Can I Qualify For A Mortgage Tax Deduction ?Reordering        I How A Mortgage {{ Tax Deduction }} For Qualify Can ?Translation                         ? ? ?? ? ?? ?? ??? ? ?? ? ?? ? ??? ?? ? ?? ?? ? ?Alignment         2,2,0,0,0,6,6,6,7_8,4,3,3,3,1,1,1,1,1,9
Standard / AutomaticSource              We do not claim to cure , prevent or treat any disease .Reordering        any disease cure , prevent or treat claim to We do not .Translation           ???? ?? ? ?? ,  ?? ,            ??? ?? ? ?? ?? ?? ? ? ?? ?? ? .Alignment         10,11,,5,6,7,,8,9,,,4,,,,2,2,2,12
Alignment-oriented / AutomaticSource            We do not claim to cure , prevent or treat any disease .Reordering        We any disease cure , prevent or treat claim to do not .Translation              ? ? ? ???? ?? ? ?? ,           ?? ???? ?? ? ?? ? ?? ? ?? ? .Alignment          0,0,,10,11,,5,6,7,8,9,,,,3,4,2,2,12
Table 2: Reference reordering data generated via various methods: (i) alignment-oriented vs. standard translation, (ii)
manual vs. automatic word alignment
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 +** +** +** +** P5R1 +** +** +** +**
P2R5 +** +** +** P5R2 0 +** +**
P3R5 +** P5R3 +** +**
P4R5 0 P5R4 +**
Table 3: Pairwise significance in subjective evaluation (0 = not significant, * = 90 percent, ** = 95 percent).
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 +** +** +** +** P5R1 0 +** +** +**
P2R5 +** +** +** P5R2 +** +** +**
P3R5 +** +** P5R3 +** +**
P4R5 0 P5R4 +**
Table 4: Pairwise significance in fuzzy reordering score (0 = not significant, * = 90 percent, ** = 95 percent).
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 0 0 +* +* P5R1 +** +** +** +**
P2R5 0 0 0 P5R2 0 +** +**
P3R5 0 0 P5R3 0 +*
P4R5 0 P5R4 0
Table 5: Pairwise significance in BLEU score (0 = not significant, * = 90 percent, ** = 95 percent).
19
Parser Labeled attachment POS accuracy
P1 0.807 0.954
P2 0.822 0.954
P3 0.827 0.955
P4 0.830 0.955
P5 0.822 0.944
Table 8: Intrinsic parser metrics on WSJ dev set.
Figure 1: P1 and P5?s parse trees and automatic reorder-
ing (using R5 ruleset) and fuzzy score.
hurts parsing performance on cleaner newswire.
6 Discussion
We have found that in practice this evaluation frame-
work is sufficiently correlated with human judg-
ments of translation quality to be rather useful for
performing detailed error analysis of our English-to-
Japanese system. We have used it in the following
ways in simple error analysis sessions:
? To identify which words are most frequently re-
ordered incorrectly
? To identify systematic parser and/or POS errors
? To identify the worst reordered sentences
? To evaluate individual reordering rules
Figures 1 and 2 show pairs of parse trees together
with their resulting reorderings and scores against
Figure 2: P1 and P5?s parse trees and automatic reorder-
ing (using R5 ruleset) and fuzzy score.
the reference. These are typical of the parser er-
rors that impact reordering and which are correctly
identified by our framework. In related joint work
(Katz-Brown et al, 2011) and (Hall et al, 2011), it
is shown that the framework can be used to optimize
reordering components automatically.
7 Conclusions
We have presented a lightweight framework for eval-
uating reordering in machine translation and demon-
strated that this is able to accurately distinguish sig-
nificant changes in translation quality due to changes
in preprocessing components such as the parser or
reordering rules used by the system. The sentence-
level correlation of our metric with judgements of
human translation quality was shown to be higher
than other standard evaluation metrics while our
evaluation has the significant practical advantage of
not requiring an end-to-end machine translation ex-
periment when used to evaluate a separate reorder-
ing component. Our analysis has also highlighted
the benefits of creating focused evaluation data that
attempts to factor out some of the phenomena found
in real human translation. While previous work has
provided meta-analysis of reordering metrics across
quite independent systems, ours is we believe the
first to provide a detailed comparison of systems
20
that differ only in small but realistic aspects such as
parser quality. In future work we plan to use the
framework to provide a more comprehensive analy-
sis of the reordering capabilities of a broad range of
machine translation systems.
References
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 327?332,
Uppsala, Sweden, July.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting success in machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 745?
754, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A quantitative analysis of reordering phenom-
ena. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation, pages 197?205, Athens,
Greece, March.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for mt evaluation: evaluating reorder-
ing. Machine Translation, 24:15?26, March.
Chris Callison-burch and Miles Osborne. 2006. Re-
evaluating the role of bleu in machine translation re-
search. In In EACL, pages 249?256.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005a. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 531?540, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005b. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 531?540, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 304?3111, July.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33:293?303, September.
Keith Hall, Ryan McDonald, and Jason Katz-Brown.
2011. Training dependency parsers by jointly optimiz-
ing multiple objective functions. In Proc. of EMNLP
2011.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic evalu-
ation of translation quality for distant language pairs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 944?
952, Cambridge, MA, October. Association for Com-
putational Linguistics.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a Parser for Ma-
chine Translation Reordering. In Proc. of EMNLP
2011.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23(2-3):105?115.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Chao Wang. 2007. Chinese syntactic reordering for
statistical machine translation. In In Proceedings of
EMNLP, pages 737?745.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the 20th international con-
ference on Computational Linguistics, COLING ?04,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
245?253, Boulder, Colorado, June.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation, pages 55?63.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
21
