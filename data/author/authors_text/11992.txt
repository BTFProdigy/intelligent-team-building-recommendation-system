Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 124?127,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Analysis of Listening-oriented Dialogue for Building Listening Agents
Toyomi Meguro, Ryuichiro Higashinaka, Kohji Dohsaka, Yasuhiro Minami, Hideki Isozaki
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan
{meguro,rh,dohsaka,minami,isozaki}@cslab.kecl.ntt.co.jp
Abstract
Our aim is to build listening agents that
can attentively listen to the user and sat-
isfy his/her desire to speak and have him-
self/herself heard. This paper investigates
the characteristics of such listening-oriented
dialogues so that such a listening process
can be achieved by automated dialogue sys-
tems. We collected both listening-oriented
dialogues and casual conversation, and ana-
lyzed them by comparing the frequency of
dialogue acts, as well as the dialogue flows
using HiddenMarkovModels (HMMs). The
analysis revealed that listening-oriented dia-
logues and casual conversation have charac-
teristically different dialogue flows and that
it is important for listening agents to self-
disclose before asking questions and to utter
more questions and acknowledgment than in
casual conversation to be good listeners.
1 Introduction
Although task-oriented dialogue systems have been
actively researched over the years (Walker et al,
2001), systems that perform more flexible (less task-
oriented) dialogues such as chats are beginning to be
actively investigated from their social and entertain-
ment aspects (Bickmore and Cassell, 2001; Higuchi
et al, 2008).
This paper deals with dialogues in which one con-
versational participant attentively listens to the other
(hereafter, listening-oriented dialogue). Our aim is
to build listening agents that can implement such a
listening process so that a user can satisfy his/her
desire to speak and have him/herself heard. Such
agents would lead the user?s state of mind for the
better as in a therapy session, although we want our
listening agents to help users mentally in everyday
conversation. It should also be noted that the pur-
pose of the listening-oriented dialogue is to simply
listen to users, not to elicit information as in inter-
views.
L: The topic is ?travel?, so did you
travel during summer vacation?
(QUESTION)
S: I like traveling. (SELF-DISCLOSURE)
L: Oh! I see! (SYMPATHY)
Why do you like to travel? (QUESTION)
S: This summer, I just went back
to my hometown.
(SELF-DISCLOSURE)
I was busy at work, but I?m
planning to go to Kawaguchi
Lake this weekend.
(SELF-DISCLOSURE)
I like traveling because it is
stimulating.
(SELF-DISCLOSURE)
L: Going to unusual places
changes one?s perspective,
doesn?t it?
(SYMPATHY)
You said you?re going to go to
Kawaguchi Lake this weekend.
Is this travel?
(QUESTION)
Will you go by car or train? (QUESTION)
Figure 1: Excerpt of a typical listening-oriented di-
alogue. Dialogue acts corresponding to utterances
are shown in parentheses (See Section 3.1 for their
meanings). The dialogue was originally in Japanese
and was translated by the authors.
There has been little research on listening agents.
One exception is (Maatman et al, 2005), which
showed that systems can make the user have the
sense of being heard by using gestures, such as nod-
ding and shaking of the head. Although our work is
similar to theirs, the difference is that we focus more
on verbal communication instead of non-verbal one.
For the purpose of gaining insight into how to
build our listening agents, we collected listening-
oriented dialogues as well as casual conversation,
and compared them in order to reveal the charac-
teristics of the listening-oriented dialogue. Figure 1
shows an example of a typical listening-oriented di-
alogue. In the figure, the conversational participants
talk about travel with the listener (L), repeatedly ask-
ing the speaker (S) to make self-disclosure.
2 Approach
We analyze the characteristics of listening-oriented
dialogues by comparing them with casual conversa-
tion. Here, casual conversation means a dialogue
where conversational participants have no prede-
fined roles (i.e., listeners and speakers). In this
124
study, we collect dialogues in texts because we want
to avoid the particular problems of voice, such as
filled pauses and interruptions, although we plan to
deal with speech input in the future.
As a procedure, we first collect listening-oriented
dialogues and casual conversation using human sub-
jects. Then, we label the collected dialogues with
dialogue act tags (see Section 3.1 for details of the
tags) to facilitate the analysis of the data. In the anal-
ysis, we examine the frequency of the tags in each
type of dialogue. We also look into the difference of
dialogue flows by modeling each type of dialogue by
Hidden Markov Models (HMMs) and comparing the
obtained models. We employ HMMs because they
are useful for modeling sequential data especially
when the number of states is unknown. We check
whether the HMMs for the listening-oriented dia-
logue and casual conversation can be successfully
distinguished from each other to see if the listen-
ing process can be successfully modeled. We also
analyze the transitions between states in the created
HMMs to examine the dialogue flows. We note that
HMMs have been used to model task-oriented dia-
logues (Shirai, 1996) and casual conversation (Iso-
mura et al, 2006). In this study, we use HMMs to
model and analyze listening-oriented dialogues.
3 Data collection
We recruited 16 participants. Eight participated as
listeners and the other eight as speakers. The male-
to-female ratio was even. The participants were 21
to 29 years old. Each participant engaged in four di-
alogues: two casual conversations followed by two
listening-oriented dialogues with a fixed role of lis-
tener/speaker. In listening-oriented dialogue, the lis-
teners were instructed to make it easy for the speak-
ers to say what they wanted to say. When col-
lecting the casual conversation, listeners were not
aware that they would be listeners afterwards. Lis-
teners had never met nor talked to the speakers prior
to the data collection. The listeners and speakers
talked over Microsoft Live MessengerTMin different
rooms; therefore, they could not see each other.
In each conversation, participants chatted for 30
minutes about their favorite topic that they selected
from the topic list we prepared. The topics were
food, travel, movies, music, entertainers, sports,
health, housework and childcare, personal comput-
ers and the Internet, animals, fashion and games. Ta-
ble 1 shows the number of collected dialogues, utter-
ances and words in each utterance of listeners and
Listening Casual
# dialogues 16 16
# utterances 850 720
# words Listener 20.60 17.92
per utt. Speaker 26.46 21.44
Table 1: Statistics of collected dialogues.
speakers. Generally, utterances in listening-oriented
dialogue were longer than those in casual conversa-
tion, probably because the subjects explained them-
selves in detail to make themselves better under-
stood.
At the end of each dialogue, the participants
filled out questionnaires that asked for their sat-
isfaction levels of dialogue, as well as how well
they could talk about themselves to their conver-
sational partners on the 10-point Likert scale. The
analysis of the questionnaire results showed that, in
listening-oriented dialogue, speakers were having a
better sense of making themselves heard than in ca-
sual conversation (Welch?s pairwise t-test; p=0.016)
without any degradation in the satisfaction level of
dialogue. This indicates that the subjects were suc-
cessfully performing attentive listening and that it is
meaningful to investigate the characteristics of the
collected listening-oriented dialogues.
3.1 Dialogue act
We labeled the collected dialogues using the dia-
logue act tag set: (1) SELF-DISCLOSURE (disclo-
sure of one?s preferences and feelings), (2) INFOR-
MATION (delivery of objective information), (3) AC-
KNOWLEDGMENT (encourages the conversational
partner to speak), (4) QUESTION (utterances that ex-
pect answers), (5) SYMPATHY (sympathetic utter-
ances and praises) and, (6) GREETING (social cues
to begin/end a dialogue).
We selected these tags from the DAMSL tag set
(Jurafsky et al, 1997) that deals with general con-
versation and also from those used to label therapy
conversation (Ivey and Ivey, 2002). Since our work
is still preliminary, we selected only a small num-
ber of labels that we thought were important for
modeling utterances in our collected dialogues, al-
though we plan to incorporate other tags in the fu-
ture. We expected that self-disclosure would occur
quite often in our data because the subjects were to
talk about their favorite topics and the participants
would be willing to communicate about their expe-
riences and feelings. We also expected that the lis-
teners would sympathize often to make others talk
with ease. Note that sympathy has been found useful
to increase closeness between conversational partic-
125
Listener Speaker
Casual Listening Casual Listening
DISC 66.6% 44.5% 53.3% 57.3%
INFO 6.5% 1.4% 5.6% 5.2%
ACK 8.0% 12.3% 6.6% 6.9%
QUES 4.1% 25.8% 21.3% 14.0%
SYM 2.6% 3.7% 3.2% 3.3%
GR 10.9% 9.8% 7.2% 9.6%
OTHER 1.3% 2.5% 2.9% 3.7%
Table 2: Rates of dialogue act tags.
DISC INFO ACK QUES SYM GR
Increase 0 0 8 8 5 4
Decrease 8 8 0 0 3 4
Table 3: Number of listeners whose tags in-
creased/decreased in listening-oriented dialogue.
ipants (Reis and Shaver, 1998).
A single annotator, who is not one of the authors,
labeled each utterance using the seven tags (six di-
alogue act tags plus OTHER). As a result, 1,177
tags were labeled to the utterances in the listening-
oriented dialogues and 1,312 tags to those in casual
conversation. The numbers of tags and utterances do
not match because, in text dialogue, an utterance can
be long and may be annotated with several tags.
4 Analysis
4.1 Comparing the frequency of dialogue acts
We compared the frequency of the dialogue act tags
in listening-oriented dialogues and casual conversa-
tion. Table 2 shows the rates of the tags in each type
of dialogue. In the table, OTHER means the expres-
sions that did not fall into any of our six dialogue
acts, such as facial expressions and mistypes. Table
3 shows the number of listeners whose rates of tags
increased or decreased from casual conversation to
listening-oriented dialogue.
Compared to casual conversation, the rates of
SELF-DISCLOSURE and INFORMATION decreased
in the listening-oriented dialogue. On the other
hand, the rates of ACKNOWLEDGMENT and QUES-
TION increased. This means that the listeners tended
to hold the transmission of information and focused
on letting speakers self-disclose or deliver informa-
tion. It can also be seen that the speakers decreased
QUESTION to increase self-disclosure.
4.2 Modeling dialogue act sequences by HMM
We analyzed the flow of listening-oriented dialogue
and casual conversation by modeling their dialogue
act sequences using HMMs. We defined 14 obser-
vation symbols, corresponding to the seven tags for
a listener and the same number of tags for a speaker.
L:Greeting:0.483S:Greeting:0.39
L:Self-disclosure:0.107L:Question:0.456S:Ack:0.224 S:Self-disclosure:0.828
L:Self-disclosure:0.579L:Ack:0.132
0.1
0.58
0.13
0.38
0.83
0.41
0.55
0.51
?
? ?
?
Figure 2: Ergodic HMM for listening-oriented dia-
logue. Circled numbers represent state IDs.
We trained the following two types of HMMs for
each type of dialogue.
Ergodic HMM: Each state emits all 14 observation
symbols. All states are connected to each other.
Speaker HMM: Half the states in this HMM only
emit one speaker?s dialogue acts and the other
half emit other speaker?s dialogue acts. All
states are connected to each other.
The EM algorithm was used to train the HMMs.
To find the best fitting HMM with minimal states,
we trained 1,000 HMMs for each type of HMM by
increasing the number of states from one to ten and
training 100 HMMs for each number of states. This
was necessary because the HMMs severely depend
on the initial probabilities. From the 1,000 HMMs,
we chose the most fitting model using the MDL
(Minimum Description Length) criterion.
4.2.1 Distinguishing Dialogue Types
We performed an experiment to examine whether
the trained HMMs can distinguish listening-oriented
dialogues and casual conversation. For this exper-
iment, we used eight listening-oriented dialogues
and eight casual conversations to train HMMs and
made them classify the remaining 16 dialogues. We
found that Ergodic HMM can distinguish the dia-
logues with an accuracy of 87.5%, and the Speaker
HMM achieved 100% accuracy. This indicates that
we can successfully train HMMs for each type of
dialogue and that investigating the trained HMMs
would show the characteristics of each type of di-
alogue. In the following sections, we analyze the
HMMs trained using all 16 dialogues of each type.
4.2.2 Analysis of Ergodic HMM
Figure 2 shows the Ergodic HMM for listening-
oriented dialogue. It can be seen that the major flow
126
L:Greeting:0.888
L:Self-disclosure:0.445L:Question:0.492 S:Self-disclosure:0.835
L:Self-disclosure:0.556L:Ack:0.27
S:Greeting:0.98
S:Self-disclosure:0.125S:Ack:0.661
0.42 0.370.43
0.38
0.11
0.56 0.51
0.18 0.92
0.47
0.63
0.25
? ?
? ?
? ?
Figure 3: Speaker HMM for listening-oriented dia-
logue.
S2:Greeting:0.775
S2:Self-disclosure:0.523S2:Question:0.414S1:Self-disclosure:0.644S1:Question:0.26
S2:Self-disclosure:0.629S2:Ack:0.12
S1:Greeting:0.848
S1:Self-disclosure:0.662S1:Ack:0.135
0.45 0.350.45
0.45 0.110.16
0.32 0.42
0.43
0.1
0.740.51
0.12
0.76 0.15
? ?
? ?
? ?
Figure 4: Speaker HMM for casual conversation.
of dialogue acts are: 2? L?s question ? 3? S?s self-
disclosure ? 4? L?s self-disclosure ? 2? L?s ques-
tion. This flow indicates that listeners tend to self-
disclose before the next question, showing the cycle
of reciprocal self-disclosure. This indicates that lis-
tening agents would need to have the capability of
self-disclosure in order to become human-like lis-
teners.
4.2.3 Analysis of Speaker HMM
Figures 3 and 4 show the Speaker HMMs for
listening-oriented dialogue and casual conversation,
respectively. Here, L and S correspond to S1 and
S2. It can be clearly seen that the two HMMs
have very similar structures. From the probabili-
ties, states with the same IDs seem to correspond to
each other. When we compare state IDs 3 and 5, it
can be seen that, when speakers take the role of lis-
teners, they reduce self-disclosure while increasing
questions and acknowledgment. Questions seem to
have more importance in listening-oriented dialogue
than in casual conversation, indicating that listening
agents need to have a good capability of generating
questions. The agents would also need to explicitly
increase acknowledgment in their utterances. Note
that, compared to spoken dialogue, acknowledgment
has to be performed consciously in text-based dia-
logue. When we compare state ID 4, we see that
the speaker starts questioning in casual conversation,
whereas the speaker only self-discloses in listening-
oriented dialogue. This shows that, in our data, the
speakers are successfully concentrating on making
self-disclosure in listening-oriented dialogue.
5 Conclusion and Future work
We collected listening-oriented dialogue and ca-
sual conversation, and compared them to find the
characteristics of listening-oriented dialogues that
are useful for building automated listening agents.
Our analysis found that it is important for listen-
ing agents to self-disclose before asking questions
and that it is necessary to utter more questions and
acknowledgment than in casual conversation to be
good listeners. As future work, we plan to use a
more elaborate tag set to further analyze the dia-
logue flows. We also plan to extend the HMMs
to Partially Observable Markov Decision Processes
(POMDPs) (Williams and Young, 2007) to achieve
dialogue management of listening agents from data.
References
Timothy Bickmore and Justine Cassell. 2001. Relational
agents: A model and implementation of building user trust.
In Proc. ACM CHI, pages 396?403.
Shinsuke Higuchi, Rafal Rzepka, and Kenji Araki. 2008. A
casual conversation system using modality and word associ-
ations retrieved from the web?. In EMNLP, pages 382?390.
Naoki Isomura, Fujio Toriumi, and Kenichiro Ishii. 2006.
Evaluation method of non-task-oriented dialogue system by
HMM. In Proc. the 4th Symposium on Intelligent Media In-
tegration for Social Information Infrastructure, pages 149?
152.
Allen E. Ivey and Mary Bradford Ivey. 2002. Intentional Inter-
viewing and Counseling: Facilitating Client Development in
a Multicultural Society. Brooks/Cole Publishing Company.
Dan Jurafsky, Liz Shriberg, and Debra Biasca, 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function Annota-
tion Coders Manual.
Martijn Maatman, Jonathan Gratch, and Stacy Marsella. 2005.
Natural behavior of a listening agent. Lecture Notes in Com-
puter Science, 3661:25?36.
Harry T. Reis and Phillip Shaver. 1998. Intimacy as an inter-
personal process. In S. Duck, editor, Handbook of personal
relationships, pages 367?398. John Wiley & Sons Ltd.
Katsuhiko Shirai. 1996. Modeling of spoken dialogue with and
without visual information. In Proc. ICSLP, volume 1, pages
188?191.
Marilyn A. Walker, Rebecca Passonneau, and Julie E. Boland.
2001. Quantitative and qualitative evaluation of darpa com-
municator spoken dialogue systems. In Proc. ACL, pages
515?522.
Jason D. Williams and Steve Young. 2007. Partially observ-
able Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393?422.
127
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 761?769,
Beijing, August 2010
Controlling Listening-oriented Dialogue using Partially Observable
Markov Decision Processes
Toyomi Meguro?, Ryuichiro Higashinaka?, Yasuhiro Minami?, Kohji Dohsaka?
?NTT Communication Science Laboratories, NTT Corporation
?NTT Cyber Space Laboratories, NTT Corporation
meguro@cslab.kecl.ntt.co.jp
higashinaka.ryuichiro@lab.ntt.co.jp
{minami,dohsaka}@cslab.kecl.ntt.co.jp
Abstract
This paper investigates how to automat-
ically create a dialogue control compo-
nent of a listening agent to reduce the cur-
rent high cost of manually creating such
components. We collected a large number
of listening-oriented dialogues with their
user satisfaction ratings and used them to
create a dialogue control component using
partially observable Markov decision pro-
cesses (POMDPs), which can learn a pol-
icy to satisfy users by automatically find-
ing a reasonable reward function. A com-
parison between our POMDP-based com-
ponent and other similarly motivated sys-
tems using human subjects revealed that
POMDPs can satisfactorily produce a dia-
logue control component that can achieve
reasonable subjective assessment.
1 Introduction
Although task-oriented dialogue systems have
been actively researched (Hirshman, 1989; Fer-
guson et al, 1996; Nakano et al, 1999; Walker
et al, 2002), recently non-task-oriented functions
are starting to attract attention, and systems with-
out a specific task that deal with more casual di-
alogues, such as chats, are being actively investi-
gated from their social and entertainment aspects
(Bickmore and Cassell, 2001; Higashinaka et al,
2008; Higuchi et al, 2008).
In the same vein, we have been working on
listening-oriented dialogues in which one conver-
sational participant attentively listens to the other
(hereafter, listening-oriented dialogue). Our aim
is to build listening agents that can implement
such a listening process so that users can satisfy
their desire to speak and be heard. Figure 1 shows
an excerpt from a typical listening-oriented dia-
logue. In the literature, dialogue control compo-
nents for less (or non-) task-oriented dialogue sys-
tems, such as listening agents, have typically used
hand-crafted rules for dialogue control, which
can be problematic because completely covering
all dialogue states by hand-crafted rules is diffi-
cult when the dialogue has fewer task restrictions
(Wallace, 2004; Isomura et al, 2009).
To solve this problem, this paper aims to auto-
matically build a dialogue control component of a
listening agent using partially observable Markov
decision processes (POMDPs). POMDPs, which
make it possible to learn a policy that can max-
imize the averaged reward in partially observable
environments (Pineau et al, 2003), have been suc-
cessfully adopted in task-oriented dialogue sys-
tems for learning a dialogue control module from
data (Williams and Young, 2007). However, no
work has attempted to use POMDPs for less (or
non-) task-oriented dialogue systems, such as lis-
tening agents, because user goals are not as well-
defined as task-oriented ones, complicating the
finding of a reasonable reward function.
We apply POMDPs to listening-oriented dia-
logues by having the system learn a policy that si-
multaneously maximizes how well users feel that
they are being listened to (hereafter, user satis-
faction) and how smoothly the system generates
dialogues (hereafter, smoothness). This formu-
lation is new; no work has considered both user
satisfaction and smoothness using POMDPs. We
collected a large amount of listening-oriented di-
alogues and annotated them with dialogue acts
and also obtained subjective evaluation results for
them. From them, we calculated the rewards and
learned the POMDP policies. We evaluated the
dialogue-act tag sequences of our POMDPs using
human subjects.
761
Utterance Dialogue act
S: Good evening. GREETING
The topic is ?food,? nice to
meet you.
GREETING
L: Nice to meet you, too. GREETING
S: I had curry for dinner. S-DISC (sub: fact)
Do you like curry? QUESTION (sub: pref)
L: Yes, I do. SYMPATHY
S: Really? REPEAT
Me, too. SYMPATHY
L: Do you usually go out to eat? QUESTION (sub: habit)
S: No, I always cook at home. S-DISC (sub: habit)
I don?t use any special spices,
but I sometimes cook noodles
using soup and curry.
S-DISC (sub: habit)
L: That sounds good! S-DISC (sub: pref (pos-
itive))
Figure 1: Excerpt of a typical listening-oriented
dialogue. Dialogue topic is ?food.? Dialogue acts
corresponding to utterances are shown in paren-
theses (See Table 1 for meanings): S-DISC stands
for SELF-DISCLOSURE; PREF for PREFERENCE;
S for speaker; and L for listener. The dialogue
was translated from Japanese by the authors.
The next section introduces related work. Sec-
tion 3 describes our approach. Section 4 de-
scribes our collection of listening-oriented dia-
logues. This is followed in Section 5 by an evalua-
tion experiment that compared our POMDP-based
dialogue control with other similarly motivated
systems. The last section summarizes the main
points and mentions future work.
2 Related work
With increased attention on social dialogues and
senior peer counseling, work continues to emerge
on listening-oriented dialogues. One early work
is (Maatman et al, 2005), which showed that vir-
tual agents can give users the sense of being heard
using such gestures as nodding and head shak-
ing. Recently, Meguro et al (2009a) analyzed
the characteristics of listening-oriented dialogues.
They compared listening-oriented dialogues and
casual conversations between humans, revealing
that the two types of dialogues have significantly
different flows and that listeners actively ques-
tion with frequently inserted self-disclosures; the
speaker utterances were mostly concerned with
self-disclosure.
Shitaoka et al (2010) also investigated the
functions of listening agents, focusing on their
response generation components. Their system
takes the confidence score of speech recognition
into account and changes the system response ac-
cordingly; it repeats the user utterance or makes
an empathic utterance for high-confidence user ut-
terances and makes a backchannel when the con-
fidence is low. The system?s empathic utterances
can be ?I?m happy? or ?That?s too bad,? depend-
ing on whether a positive or negative expression
is included in the user utterances. Their system?s
response generation only uses the speech recogni-
tion confidence and the polarity of user utterances
as cues to choose its actions. Currently, it does
not consider the utterance content or the user in-
tention.
In order for listening agents to achieve high
smoothness, a switching mechanism between the
?active listening mode,? in which the system is
a listener, and the ?topic presenting mode,? in
which the system is a speaker, has been proposed
(Yokoyama et al, 2010; Kobayashi et al, 2010).
Here, the system uses a heuristic function to main-
tain a high user interest level and to keep the sys-
tem in an active listening mode. Dialogue con-
trol is done by hand-crafted rules. Our motivation
bears some similarity to theirs in that we want to
build a listening agent that gives users a sense of
being heard; however, we want to automatically
make such an agent from dialogue data.
POMDPs have been introduced for robot action
control (Pineau et al, 2003). Here, the system
learns to make suitable movements for complet-
ing a certain task. Over the years, POMDPs have
been actively studied for applications to spoken
dialogue systems. Williams et al (2007) suc-
cessfully used a POMDP for dialogue control in a
ticket-buying domain in which the objective was
to fix the departure and arrival places for tickets.
Recent work on POMDPs indicates that it is pos-
sible to train a dialogue control module in task-
oriented dialogues when the user goal is obvious.
In contrast, in this paper, we aim to verify whether
POMDPs can be applied to less task-oriented di-
alogues (i.e., listening-oriented dialogues) where
user goals are not as obvious.
In a recent study, Minami et al (2009) ap-
plied POMDPs to non-task-orientedman-machine
interaction. Their system learned suitable ac-
tion control of agents that can act smoothly by
obtaining rewards from the statistics of artifi-
cially generated data. Our work is different be-
cause we use real human-human dialogue data to
762
train POMDPs for dialogue control in listening-
oriented dialogues.
3 Approach
A typical dialogue system has utterance under-
standing, dialogue control, and utterance gen-
eration modules. The utterance understanding
module comprehends user natural-language utter-
ances, whose output (i.e., a user dialogue act) is
passed to the dialogue control module. The dia-
logue control module chooses the best system di-
alogue act at every dialogue point using the user
dialogue act as input. The utterance generation
module generates natural-language utterances and
says them to users by realizing the system dia-
logue acts as surface forms.
This paper focuses on the dialogue control
module of a listening agent. Since a listening-
oriented dialogue has a characteristic conversation
flow (Meguro et al, 2009a), focusing on this mod-
ule is crucial because it deals with the dialogue
flow. Our objective is to train from data a dialogue
control module that achieves a smooth dialogue
flow that makes users feel that they are being lis-
tened to attentively.
3.1 Dialogue control using POMDPs
The purpose of our dialogue control is to simulta-
neously create situations in which users feel lis-
tened to (i.e., user satisfaction) and to generate
smooth action sequences (i.e., smoothness). To
do this, we automatically and statistically train
the reward and the policy of the POMDP using a
large amount of listening-oriented dialogue data.
POMDP is a reinforcement learning framework
that can learn a policy to select an action sequence
that maximizes average future rewards. Setting a
reward is crucial in POMDPs.
For our purpose, we introduce two different re-
wards: one for user satisfaction and the other for
smoothness. Before creating a POMDP structure,
we used the dynamic Bayesian network (DBN)
structure (Fig. 2) to obtain the statistical structure
of the data and the two rewards.
The random values in the DBN are as follows:
so and sa are the dialogue state and action state,
o is a speaker observation, a is a listener action,
and d is a random variable for an evaluation score
that indicates the degree of the user being listened
to. This evaluation score can be obtained by ques-
'aa
o
s
r
o
s
'o
a
s
a
s
o
?
?
a
'a
o
s
d
o
s
'o
a
s
a
s
o
?
?
DBN structure
POMDP structure
Figure 2: DBN and POMDP structures employed
in this paper. Note that a in the POMDP is isolated
from other states because it is decided by a learned
policy.
tionnaires, and the variable is used for calculat-
ing a user satisfaction reward for the POMDP.
The DBN arcs in Fig. 2 define the emission and
transition probabilities. Pr(o?|s?o) is the emission
probability of o? given s?o. Pr(d|so) is the emis-
sion probability of d given so. Pr(s?o|so, a) is a
transition probability from so to s?o given a. The
DBN is trained using the EM algorithm. Using
the obtained variables, we calculate the two re-
ward functions as follows:
(1) Reward for user satisfaction This reward is
obtained from the d variable by
r1((so, ?), a) =
max?
d=min
d ? Pr(d|so, a),
where * is arbitrary sa and min and max are min-
imum and maximum evaluation scores.
(2) Reward for smoothness For smoothness,
we maximize the action predictive probability
given the history of actions and observations. The
probability is calculated from listening-oriented
dialogue data. sa is introduced for estimating the
predictive probability of action a and for selecting
a to maximize the predictive probability.
We set Pr(a|sa) = 1 when a = sa so that sa
corresponds one-on-one with a. Then, if at = sa
at time t is given, we obtain
Pr(at|o1, a1, . . . , at?1, ot)
=
?
s?a
Pr(at|s?a) Pr(s?a|o1, a1, . . . , at?1, ot)
= Pr(sa|o1, a1, . . . , ot?1, at?1, ot)
Consequently, maximizing the predictive proba-
bility of a equals maximizing that of sa. If we
763
set 1.0 to reward r2((?, sa), a) when sa = a, the
POMDP will generate actions that maximize their
predictive probabilities. We believe that this re-
ward should increase the smoothness of a system
action sequence since the sequence is generated
according to the statistics of human-human dia-
logues.
Converting a DBN into a POMDP The DBN
is converted into a POMDP (Fig. 2), while main-
taining the transition and output probabilities. We
convert d to r as described above.
The system is in a partially observed state.
Since the state is not known exactly, we use a dis-
tribution called ?belief state? bt with which we ob-
tain the average reward that will be gained in the
future at time t by:
Vt =
??
?=0
??
?
s
b?+t((so, sa))r((so, sa), a?+t),
where ? is a discount factor; namely, the future
reward is decreased by ? . A policy is learned by
value iteration so that the action that maximizes
Vt can be chosen. We define r((so, sa), a) as fol-
lows:
r((so, sa), a) = r1((so, ?), a) + r2((?, sa), a).
By balancing these two rewards, we can choose
an action that satisfies both user satisfaction and
smoothness.
4 Data collection
We collected listening-oriented dialogues using
human subjects who consisted of ten listeners
(five males and five females) and 37 speakers (18
males and 19 females). The listeners and speak-
ers ranged from 20 to 60 years old and were all
native Japanese speakers. Listeners and speakers
were matched to form a listener-speaker pair and
communicated over the Internet with our chat in-
terface. They used only text; they were not al-
lowed to use voice, video, or facial expressions.
The speakers chose their own listener and freely
participated in dialogues from 7:00 pm to mid-
night for a period of 15 days. One conversation
was restricted to about ten minutes. The subjects
talked about a topic chosen by the speaker. There
were 20 predefined topics: money, sports, TV and
radio, news, fashion, pets, movies, music, house-
work and childcare, family, health, work, hob-
bies, food, human relationships, reading, shop-
ping, beauty aids, travel, and miscellaneous. The
listeners were instructed to make it easy for the
speakers to say what the speakers wanted to say.
We collected 1260 listening-oriented dialogues.
4.1 Dialogue-act annotation
We labeled the collected dialogues using the
dialogue-act tag set shown in Table 1. We made
these tags by selecting, extending, and modifying
those from previous studies that concerned human
listening behaviors in some way (Meguro et al,
2009a; Jurafsky et al, 1997; Ivey and Ivey, 2002).
In our tag set, only question and self-disclosure
tags have sub-category tags. Two annotators (not
the authors) labeled each sentence of our collected
dialogues using these 32 tags. In dialogue-act an-
notation, since there can be several sentences in
one utterance, one annotator first split the utter-
ances into sentences, and then both annotators la-
beled each sentence with a single dialogue act.
4.2 Obtaining evaluation scores
POMDPs need evaluation scores (i.e., d) for dia-
logue acts (i.e., a) for training a reward function.
Therefore, we asked a third-party participant, who
was neither a listener nor a speaker in our dialogue
data collection, to evaluate the user satisfaction
levels of the collected dialogues. She rated each
dialogue in terms of how she would have felt ?be-
ing heard? after the dialogue if she had been the
speaker of the dialogue in question. She provided
ratings on the 7-point Likert scale for each dia-
logue. Since she rated the whole dialogue with a
single rating, we set the evaluation score of each
action within a dialogue using the evaluation score
for that dialogue.
We used a third-person?s evaluation and not the
original person?s to avoid the fact that the eval-
uative criterion is too different between humans;
identical evaluation scores from two people do
not necessarily reflect identical user satisfaction
levels. We highly valued the reliability and con-
sistency of the third-person scores. This way, at
least, we can train a policy that maximizes its av-
erage reward function for the rater, which we need
to verify first before considering adaptation to two
or more individuals.
5 Experiment
5.1 Experimental setup
The experiment followed three steps.
764
GREETING Greeting and confirmation of dialogue
theme. e.g., Hello. Let?s talk about
lunch.
INFORMATION Delivery of objective information. e.g.,
My friend recommended a restaurant.
SELF-
DISCLOSURE
Disclosure of preferences and feelings.
sub: fact e.g., I live in Tokyo.
sub: experience e.g., I had a hamburger for lunch.
sub: habit e.g., I always go out for dinner.
sub: preference e.g., I like hamburgers.
(positive)
sub: preference e.g., I don?t really like hamburgers.
(negative)
sub: preference e.g., Its taste is near my homemade
(neutral) taste.
sub: desire e.g., I want to try it.
sub: plan e.g., I?m going there next week.
sub: other
ACKNOWLEDGM-
ENT
Encourage the conversational partner to
speak. e.g., Well. Aha.
QUESTION Utterances that expect answers.
sub: information e.g., Please tell me how to cook it.
sub: fact e.g., What kind of curry?
sub: experience e.g., What did you have for dinner?
sub: habit e.g., Did you cook it yourself?
sub: preference e.g., Do you like it?
sub: desire e.g., Don?t you want to eat rice?
sub: plan e.g., What are you going to have for
dinner?
sub: other
SYMPATHY Sympathetic utterances and praises.
e.g., Me, too.
NON-SYMPATHY Negative utterances. e.g., Not really.
CONFIRMATION Confirm what the conversation partner
said. e.g., Really?
PROPOSAL Encourage the partner to act. e.g., Try
it.
REPEAT Repeat the partner?s utterance.
PARAPHRASE Paraphrase the partner?s utterance.
APPROVAL Broach or show goodwill toward the
partner. e.g., Absolutely!
THANKS Express thanks e.g., Thank you.
APOLOGY Express regret e.g., I?m sorry.
FILLER Filler between utterances. e.g., Uh. Let
me see.
ADMIRATION Express affection. e.g., Ha-ha.
OTHER Other utterances.
Table 1: Definition and example of dialogue acts
In the first step, we created our POMDP sys-
tem using our approach (See Section 3.1). We
also made five other systems for comparison that
we describe in Section 5.2. Each system outputs
dialogue-act tag sequences for evaluation. The
dialogue theme was ?food? because it was the
most frequent theme and accounted for 20% of
our data (See Table 2 for the statistics); we trained
our POMDP using the ?food? dialogues. We re-
stricted the dialogue topic to verify that our ap-
proach at least works with a small set. Since there
is no established measure for automatically eval-
uating a dialogue-act tag sequence, we evaluated
All Food (subset of All)
# dialogues 1260 250
# words 479881 94867
# utterances per dialogue 28.2 29.1
# dialogues per listener 126 25
# dialogues per speaker 34 6.8
# dialogue acts 67801 13376
inter-annotator agreement 0.57 0.55
Table 2: Statistics of collected dialogues and
dialogue-act annotation. Inter-annotator agree-
ment means agreement of dialogue-act annotation
using Cohen?s ?.
our dialogue control module using human subjec-
tive evaluations. However, this is very difficult to
do because dialogue control modules only output
dialogue acts, not natural language utterances.
In the second step, we recruited participants
who created natural language utterances from
dialogue-act tag sequences. In their creating dia-
logues, we provided them with situations to stim-
ulate their imaginations. Table 3 shows the situ-
ations, which were deemed common in everyday
Japanese life; we let the participants create utter-
ances that fit the situations. These situations were
necessary because, without restrictions, the evalu-
ation scores could be influenced by dialogue con-
tent rather than by dialogue flow.
For this dialogue-imagining exercise, we re-
cruited 16 participants (eight males and eight fe-
males) who ranged from 19 to 39 years old. Each
participant made twelve dialogues using two situ-
ations. For assigning the situations, we first cre-
ated four conditions: (1) a student and living with
family, (2) working and living with family, (3) a
student and living alone, and (4) working and liv-
ing alone. Then the participants were categorized
into one of these conditions on the basis of their
actual lifestyle and assigned two of the situations
matching the condition.
For each situation, each participant created six
imaginary dialogues from the six dialogue-act se-
quences output by the six systems: our POMDP
and the other five systems for comparison. This
process produced such dialogues as shown in
Figs. 5 and 6. The dialogue in Fig. 5 was made
from a dialogue-act tag sequence of a human-
human conversation using No. 1 of Table 3. The
dialogue in Fig. 6 was made from the sequence of
our POMDP using No. 2 of Table 3.
In the third step, we additionally recruited three
judges (one male and two females) to evalu-
765
ate the imagined 192 (16 ? 2 ? 6) dialogues.
The judges were neither the participants who
made dialogues nor those who rated the collected
listening-oriented dialogues. Six dialogues made
from one situation were randomly shown to the
judges one-by-one, who then filled out question-
naires to indicate their user satisfaction levels by
answering this question on a 7-point Likert scale:
?If you had been the speaker, would you have felt
that you were listened to??
5.2 Systems for comparison
We created our POMDP-based dialogue control
and five other systems for comparison.
POMDP We learned a policy based on our ap-
proach. We used ?food? dialogues (See Section
4), and the evaluation scores were those described
in Section 4.2. This system used the policy to
generate sequences of dialogue-act tags by sim-
ulation; user observations were generated based
on emission probability, and system actions were
generated based on the policy.
In this paper, the total number of observations
and actions was 33 because we have 32 dialogue-
act tags (See Table 1) plus a ?skip? tag. In learning
the policy, an observation and an actionmust indi-
vidually take turns, but our data can include mul-
tiple dialogue-act tags in one utterance. There-
fore, if there is more than one dialogue-act tag
in one utterance, a ?skip? is inserted between the
tags. The state numbers for So and Sa were 16
and 33, respectively. In this experiment, we set 10
to r2((?, sa), a).
EvenPOMDP We arranged a POMDP using
only the smoothness reward (hereafter, Even-
POMDP) by creating a POMDP system with a
fixed evaluation score; hence user satisfaction
is not incorporated in the reward. When using
fixed (even) evaluation scores for all dialogues,
the effect of the user satisfaction reward is de-
nied, and the system only generates highly fre-
quent sequences. We have EvenPOMDP to clarify
whether user satisfaction is necessary. The other
conditions are identical as in the POMDP system.
HMM We modeled our dialogue-act tag se-
quences using a Speaker HMM (SHMM) (Me-
guro et al, 2009a), which has been utilized to
model two-party listening-orienteddialogues. In a
SHMM, half the states emit listener dialogue acts,
Listener?
GREETING
Speaker?
GREETING
Listener?
QUESTION
Speaker?
S-DISC
Listener?
S-DISC
SYMPATHY
or
1
2
3
4
5
Figure 3: Structure of rule-based system
and the other half emit speaker dialogue acts. All
states are connected to each other. We modeled
the ?food? dialogues using an SHMM, and made
themodel generate themost probable dialogue-act
tag sequences. More specifically, first, a dialogue-
act tag was generated randomly based on the ini-
tial state. If the state was that of a listener, we
generated a maximum likelihood action and the
state was randomly transited based on the transi-
tion probability. If the state was that of a speaker,
we randomly generated an action based on the
emission probability and the state was transited
using the maximum likelihood transition proba-
bility.
Rule-based system This system creates
dialogue-act tag sequences using hand-crafted
rules that are based on the findings in (Meguro et
al., 2009a) and are realized as shown in Fig. 3.
A sequence begins at state 1? in Fig. 3, and one
dialogue act is generated at each state. At state
3?, a sub-category tag under QUESTION is chosen
randomly, and at state 4?, a matched sub-category
tag under SELF-DISCLOSURE is chosen. At
state 5?, the listener?s SELF-DISCLOSURE or
SYMPATHY is generated randomly.
Human dialogue sequence This system created
dialogue-act tag sequences by randomly choosing
dialogues between humans from the collected data
and used their annotated tag sequences.
Random This system simply created dialogue-
act tag sequences at random.
5.3 Experimental results
Figure 4 shows the average subjective evaluation
scores. Except between HMM and EvenPOMDP,
there was a significant difference (p<0.01) be-
tween all systems in a non-parametric multiple
comparison test (Steel-Dwass test). The dialogues
shown in Figs. 5 and 6 were generated by the sys-
tems. The dialogue in Fig. 5 was made from hu-
man dialogue sequences, and the one in Fig. 6 was
made from POMDP.
766
With whom What day What time What Where Who made
1 family weekday around 6:00 pm grilled salmon home mother
2 family weekend around 7:00 pm potato and meat home mother
3 co-workers weekday at noon boiled seaweed lunch box myself
... ... ... ... ... ... ...
32 friend weekday at noon hamburger school cafeteria N/A
Table 3: Dialogue situations relating to everyday Japanese life
We qualitatively analyzed the dialogues of each
system and observed the following characteristics:
POMDP At a dialogue?s beginning, the system
greets several times and shifts to a different phase
in which listeners ask questions and self-disclose
to encourage speakers to reciprocate.
Rule-based The output of this system seems
very natural and easy to read. The dialogue-act
tags followed reasonable rules, making it easier
for the participants to create natural utterances
from them.
Human conversation The dialogues between
humans were obviously natural before they were
changed to tags from the natural-language ut-
terances. However, human dialogues have ran-
domness, which makes it difficult for the partic-
ipants to create natural-language utterances from
the tags. Hence, the evaluation score for this sys-
tem was lower than the ?Rule-based.?
HMM, EvenPOMDP Since these systems con-
tinually output the same action tags, their output
was very unnatural. For example, greetings never
stopped because GREETING is most frequently
followed by GREETING in the data. These sys-
tems have no mechanism to stop this loop.
POMDP successfully avoided such continua-
tion because its actions have more varied rewards.
For example, GREETING is repeated in Even-
POMDP because its smoothness reward is high;
however, in POMDP, although the smoothness re-
ward remains high, its user satisfaction reward is
not that high. This is because greetings appear
in all dialogues and their user satisfaction reward
converges to the average. Therefore, such actions
as greetings do not get repeated in POMDP. In
POMDP, some states have high user satisfaction
rewards, and the POMDP policy generated actions
to move to such states.
Random Since this system has more variety
of tags than HMM, its evaluation scores out-
performed HMM, but were outperformed by
POMDP, which learned statistically from the data.
Rule-based
6.07
Human 
dialogue
5.22
POMDP
?Proposed?
3.76
Random
2.67
HMM
1.17
Even
POMDP 
1.16
0
1
2
3
4
5
6
7
A
v
e
r
a
g
e
d
 
E
v
a
l
u
a
t
i
o
n
 
S
c
o
r
e
s
Figure 4: System scores. Except between
POMDP and EvenPOMDP, significant differences
exist among all systems (p<0.01).
From our qualitative analysis, we found that
POMDP can generate more satisfying sequences
than HMM/EvenPOMDP because it does not fall
into the loop of frequent dialogue-act tag se-
quences. This suggests the usefulness of incor-
porating two kinds of rewards into the policy and
that our approach for setting a reward is promis-
ing.
However, with the proposed POMDP, unnatural
sequences remain; for example, the system sud-
denly output THANKS, as shown in Fig. 6. The
number of states may have been too small. We
plan to investigate what caused this in the future.
In our qualitative analysis, we observed that
randomness in dialogues might hold a clue for
improving evaluation scores. Therefore, we
measured the perplexity of each system output
using dialogue-act trigrams and obtained 72.8
for ?Random,? 27.4 for ?Human dialogue,? 7.4
for ?POMDP,? 3.2 for ?HMM,? 2.5 for ?Even-
POMDP,? and 1.7 for ?Rule-based.?
The perplexity of the human dialogues is less
than that of the random system, but humans also
exhibit a certain degree of freedom. On the other
hand, POMDP?s perplexity is less than the human
dialogues; they still have some freedom, which
probably led to their reasonable evaluation scores.
Considering that HMM and EvenPOMDP, which
continually output the same dialogue acts, had low
767
Utterance Dialogue act
S: Hello. GREETING
L: Nice to meet you GREETING
S: I had dinner at home today. S-DISC (sub: fact)
Do you like grilled salmon? QUESTION, PREF
L: Yes, I think so. SYMPATHY
I sometimes want to have a
fancy meal.
S-DISC (sub: de-
sire)
S: Deluxe. REPEAT
Me too. SYMPATHY
L: Do you usually do your own
cooking?
QUESTION (sub:
habit)
S: No, I don?t. S-DISC, HABIT
I always buy my meals at the
convenience store.
S-DISC (sub:
habit)
L: I like the lunch boxes of conve-
nience stores
S-DISC (sub: pref
(positive))
Figure 5: Excerpt of listening-oriented dialogue
that participant imagined from tag sequences of
human conversations. Dialogue was translated
from Japanese by the authors.
Utterance Dialogue act
L: Nice to meet you. GREETING
Where and who did you have
dinner with today?
QUESTION (sub:
fact)
S: I had ?niku-jaga? (meat and
beef) with my family at home.
S-DISC (sub: fact)
L: Oh. ADMIRATION
S: I think it is normal to eat with
your family at home.
S-DISC (sub: pref
(neutral))
L: Thanks. THANKS
Do you have any brothers or sis-
ters?
QUESTION (sub:
fact)
Soon, my brother and his wife
will visit my home.
S-DISC (sub: plan)
S: I see. SYMPATHY
L: I want to use expensive meat in
my ?niku-jaga.?
S-DISC (sub: de-
sire)
Oh. ADMIRATION
Please give me your recipe. QUESTION (sub:
information)
S: My friends claim that my
?niku-jaga? is as good as a
restaurant?s.
INFORMATION
L: I?d love to try it S-DISC (sub: de-
sire)
Figure 6: Excerpt of a listening-oriented dialogue
made from tag sequences of POMDP
evaluation scores, we conclude that randomness is
necessary in non-task-oriented dialogues and that
some randomness can be included with our ap-
proach. We do not discuss ?Rule-based? here be-
cause its tag sequence was meant to have small
perplexity.
6 Conclusion and Future work
This paper investigated the possibility of automat-
ically building a dialogue control module from di-
alogue data to create automated listening agents.
With a POMDP as a learning framework,
a dialogue control module was learned from
the listening-oriented dialogues we collected and
compared with five different systems. Our
POMDP system showed higher performance in
subjective evaluations than other statistically mo-
tivated systems, such as an HMM-based one, that
work by selecting the most likely subsequent ac-
tion in the dialogue data. When we investigated
the output sequences of our POMDP system, the
system frequently chose to self-disclose and ques-
tion, which corresponds to human listener be-
havior, as revealed in the literature (Meguro et
al., 2009a). This suggests that learning dialogue
control by POMDPs is achievable for listening-
oriented dialogues.
The main contribution of this paper is that
we successfully showed that POMDPs can be
used to train dialogue control policies for less
task-oriented dialogue systems, such as listening
agents, where the user goals are not as clear as
task-oriented ones. We also revealed that the re-
ward function can be learned effectively by our
formulation that simultaneously maximizes user
satisfaction and smoothness. Finding an appro-
priate reward function is a real challenge for less
task-oriented dialogue systems; this work has pre-
sented the first workable solution.
Much work still remains. Even though we
conducted an evaluation experiment by simula-
tion (i.e, offline evaluation), human dialogues ob-
viously do not necessarily proceed as in simula-
tions. Therefore, we plan to evaluate our sys-
tem using online evaluation, which also forces us
to implement utterance understanding and gener-
ation modules. We also want to incorporate the
idea of topic shift into our policy learning because
we observed in our data that listeners frequently
change topics to keep speakers motivated. We are
also considering adapting the system behavior to
users. Specifically, we want to investigate dia-
logue control that adapts to the personality traits
of users because it has been found that the flow
of listening-oriented dialogues differs depending
on the personality traits of users (Meguro et al,
2009b). Finally, although we only dealt with text,
we also want to extend our approach to speech and
other modalities, such as gestures and facial ex-
pressions.
768
References
Bickmore, Timothy and Justine Cassell. 2001. Rela-
tional agents: a model and implementation of build-
ing user trust. In Proc. SIGCHI conference on
human factors in computing systems (CHI), pages
396?403.
Ferguson, George, James F. Allen, and Brad Miller.
1996. TRAINS-95: towards a mixed-initiativeplan-
ning assistant. In Proc. Third Artificial Intelligence
Planning Systems Conference (AIPS), pages 70?77.
Higashinaka, Ryuichiro, Kohji Dohsaka, and Hideki
Isozaki. 2008. Effects of self-disclosure and em-
pathy in human-computer dialogue. In Proc. IEEE
Workshop on Spoken Language Technology (SLT),
pages 108?112.
Higuchi, Shinsuke, Rafal Rzepka, and Kenji Araki.
2008. A casual conversation system using modal-
ity and word associations retrieved from the web.
In Proc. 2008 conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
382?390.
Hirshman, Lynette. 1989. Overview of the DARPA
speech and natural language workshop. In Proc.
DARPA Speech and Natural Language Workshop
1989, pages 1?2.
Isomura, Naoki, Fujio Toriumi, and Kenichiro Ishii.
2009. Evaluation method of non-task-oriented di-
alogue system using HMM. IEICE Transactions on
Information and Systems, J92-D(4):542?551.
Ivey, Allen E. and Mary Bradford Ivey. 2002. In-
tentional Interviewing and Counseling: Facilitat-
ing Client Development in a Multicultural Society.
Brooks/Cole Publishing Company.
Jurafsky, Dan, Elizabeth Shriberg, and Debra Bi-
asca, 1997. Switchboard SWBD-DAMSL Shallow-
Discourse-Function Annotation Coders Manual.
Kobayashi, Yuka, Daisuke Yamamoto, Toshiyuki
Koga, Sachie Yokoyama, and Miwako Doi. 2010.
Design targeting voice interface robot capable of
active listening. In Proc. 5th ACM/IEEE inter-
national conference on Human-robot interaction
(HRI), pages 161?162,
Maatman, R. M., Jonathan Gratch, and Stacy Marsella.
2005. Natural behavior of a listening agent. Lecture
Notes in Computer Science, 3661:25?36.
Meguro, Toyomi, Ryuichiro Higashinaka, Kohji
Dohsaka, Yasuhiro Minami, and Hideki Isozaki.
2009a. Analysis of listening-oriented dialogue for
building listening agents. In Proc. 10th Annual SIG-
DIAL Meeting on Discourse and Dialogue (SIG-
DIAL), pages 124?127.
Meguro, Toyomi, Ryuichiro Higashinaka, Kohji
Dohsaka, Yasuhiro Minami, and Hideki Isozaki.
2009b. Effects of personality traits on listening-
oriented dialogue. In Proc. International Workshop
on Spoken Dialogue Systems Technology (IWSDS),
pages 104?107.
Minami, Yasuhiro, Akira Mori, Toyomi Meguro,
Ryuichiro Higashinaka, Kohji Dohsaka, and Eisaku
Maeda. 2009. Dialogue control algorithm for
ambient intelligence based on partially observable
markov decision processes. In Proc. International
Workshop on Spoken Dialogue Systems Technology
(IWSDS), pages 254?263.
Nakano, Mikio, Noboru Miyazaki, Jun ichi Hirasawa,
Kohji Dohsaka, and Takeshi Kawabata. 1999. Un-
derstanding unsegmented user utterances in real-
time spoken dialogue systems. In Proc. 37th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 200?207.
Pineau, Joelle., Geoff. Gordon, and Sebastian Thrun.
2003. Point-based value iteration: An anytime al-
gorithm for POMDPs. In Proc. International Joint
Conference on Artificial Intelligence (IJCAI), pages
1025?1032.
Shitaoka, Kazuya, Ryoko Tokuhisa, Takayoshi
Yoshimura, Hiroyuki Hoshino, and Narimasa
Watanabe. 2010. Active listening system for dia-
logue robot. In JSAI SIG-SLUD Technical Report,
volume 58, pages 61?66. (in Japanese).
Walker, Marilyn, Alex Rudnicky, John Aberdeen, Eliz-
abeth Owen Bratt, Rashmi Prasad, Salim Roukos,
Greg S, and Seneff Dave Stallard. 2002. DARPA
communicator evaluation: progress from 2000 to
2001. In Proc. International Conference on Spoken
Language Processing (ICSLP), pages 273?276.
Wallace, Richard S. 2004. The Anatomy of A.L.I.C.E.
A.L.I.C.E. Artificial Intelligence Foundation, Inc.
Williams, Jason D. and Steve Young. 2007. Par-
tially observable markov decision processes for spo-
ken dialog systems. Computer Speech & Language,
21(2):393?422.
Yokoyama, Sachie, Daisuke Yamamoto, Yuka
Kobayashi, and Miwako Doi. 2010. Development
of dialogue interface for elderly people ?switching
the topic presenting mode and the attentive listening
mode to keep chatting?. In IPSJ SIG Technical
Report, volume 2010-SLP-80, pages 1?6. (in
Japanese).
769
Coling 2010: Poster Volume, pages 400?408,
Beijing, August 2010
Learning to Model Domain-Specific Utterance Sequences for Extractive
Summarization of Contact Center Dialogues
Ryuichiro Higashinaka?, Yasuhiro Minami?, Hitoshi Nishikawa?,
Kohji Dohsaka?, Toyomi Meguro?, Satoshi Takahashi?, Genichiro Kikui?
? NTT Cyber Space Laboratories, NTT Corporation
? NTT Communication Science Laboratories, NTT Corporation
higashinaka.ryuichiro@lab.ntt.co.jp, minami@cslab.kecl.ntt.co.jp
nishikawa.hitoshi@lab.ntt.co.jp, {dohsaka,meguro}@cslab.kecl.ntt.co.jp
{takahashi.satoshi,kikui.genichiro}@lab.ntt.co.jp
Abstract
This paper proposes a novel extractive
summarization method for contact cen-
ter dialogues. We use a particular
type of hidden Markov model (HMM)
called Class Speaker HMM (CSHMM),
which processes operator/caller utterance
sequences of multiple domains simulta-
neously to model domain-specific utter-
ance sequences and common (domain-
wide) sequences at the same time. We
applied the CSHMM to call summariza-
tion of transcripts in six different con-
tact center domains and found that our
method significantly outperforms compet-
itive baselines based on the maximum
coverage of important words using integer
linear programming.
1 Introduction
In modern business, contact centers are becom-
ing more and more important for improving cus-
tomer satisfaction. Such contact centers typically
have quality analysts who mine calls to gain in-
sight into how to improve business productivity
(Takeuchi et al, 2007; Subramaniam et al, 2009).
To enable them to handle the massive number of
calls, automatic summarization has been utilized
and shown to successfully reduce costs (Byrd et
al., 2008). However, one of the problems in cur-
rent call summarization is that a domain ontology
is required for understanding operator/caller utter-
ances, which makes it difficult to port one summa-
rization system from domain to domain.
This paper describes a novel automatic sum-
marization method for contact center dialogues
without the costly process of creating domain on-
tologies. More specifically, given contact center
dialogues categorized into multiple domains, we
create a particular type of hidden Markov model
(HMM) called Class Speaker HMM (CSHMM)
to model operator/caller utterance sequences. The
CSHMM learns to distinguish sequences of indi-
vidual domains and common sequences in all do-
mains at the same time. This approach makes it
possible to accurately distinguish utterances spe-
cific to a certain domain and thereby has the po-
tential to generate accurate extractive summaries.
In Section 2, we review recent work on auto-
matic summarization, including its application to
contact center dialogues. In Section 3, we de-
scribe the CSHMM. In Section 4, we describe
our automatic summarization method in detail. In
Section 5, we describe the experiment we per-
formed to verify our method and present the re-
sults. In Section 6, we summarize and mention
future work.
2 Related Work
There is an abundance of research in automatic
summarization. It has been successfully applied to
single documents (Mani, 2001) as well as to mul-
tiple documents (Radev et al, 2004), and various
summarization methods, such as the conventional
LEAD method, machine-learning based sentence
selection (Kupiec et al, 1995; Osborne, 2002),
and integer linear programming (ILP) based sen-
tence extraction (Gillick and Favre, 2009), have
been proposed. Recent years have seen work on
summarizing broadcast news speech (Hori and
Furui, 2003), multi-party meetings (Murray et al,
2005), and contact center dialogues (Byrd et al,
2008). However, despite the large amount of pre-
vious work, little work has tackled the automatic
summarization of multi-domain data.
400
In the past few decades, contact center dia-
logues have been an active research focus (Gorin
et al, 1997; Chu-Carroll and Carpenter, 1999).
Initially, the primary aim of such research was
to transfer calls from answering agents to oper-
ators as quickly as possible in the case of prob-
lematic situations. However, real-time processing
of calls requires a tremendous engineering effort,
especially when customer satisfaction is at stake,
which led to recent work on the offline process-
ing of calls, such as call mining (Takeuchi et al,
2007) and call summarization (Byrd et al, 2008).
The work most related to ours is (Byrd et al,
2008), which maps operator/caller utterances to
an ontology in the automotive domain by using
support vector machines (SVMs) and creates a
structured summary by heuristic rules that assign
the mapped utterances to appropriate summary
sections. Our work shares the same motivation
as theirs in that we want to make it easier for
quality analysts to analyze the massive number of
calls. However, we tackle the problem differently
in that we propose a newmodeling of utterance se-
quences for extractive summarization that makes
it unnecessary to create heuristics rules by hand
and facilitates the porting of a summarization sys-
tem.
HMMs have been successfully applied to au-
tomatic summarization (Barzilay and Lee, 2004).
In their work, an HMM was used to model the
transition of content topics. The Viterbi decod-
ing (Rabiner, 1990) was performed to find con-
tent topics that should be incorporated into a sum-
mary. Their approach is similar to ours in that
HMMs are utilized to model topic sequences, but
they did not use data of multiple domains in creat-
ing their model. In addition, their method requires
training data (original articles with their reference
summaries) in order to find which content top-
ics should be included in a summary, whereas our
method requires only the raw sequences with their
domain labels.
3 Class Speaker HMM
A Class Speaker HMM (CSHMM) is an exten-
sion of Speaker HMM (SHMM), which has been
utilized to model two-party conversations (Me-
guro et al, 2009). In an SHMM, there are two
states, and each state emits utterances of one of
the two conversational participants. The states are
1:speaker1 2:speaker2
Speaker HMM for Class 1
3:speaker1 4:speaker2
Speaker HMM for Class 2
Figure 1: Topology of an ergodic CSHMM. Num-
bers before ?speaker1? and ?speaker2? denote state
IDs.
connected ergodically and the emission/transition
probabilities are learned from training data by
using the EM-algorithm. Although Meguro et
al., (2009) used SHMMs to analyze the flow of
listening-oriented dialogue, we extend their idea
to make it applicable to classification tasks, such
as dialogue segmentation.
A CSHMM is simply a concatenation of
SHMMs, each of which is trained by using ut-
terance sequences of a particular dialogue class.
After such SHMMs are concatenated, the Viterbi
algorithm is used to decode an input utterance
sequence into class labels by estimating from
which class each utterance has most likely to have
been generated. Figure 1 illustrates the basic
topology of a CSHMM where two SHMMs are
concatenated ergodically. When the most likely
state sequence for an input utterance sequence is
<1,3,4,2>, we can convert these state IDs into
their corresponding classes; that is, <1,2,2,1>,
which becomes the result of utterance classifica-
tion.
We have conceived three variations of CSHMM
as we describe below. They differ in how we treat
utterance sequences that appear commonly in all
classes and how we train the transition probabili-
ties between independently trained SHMMs.
3.1 Ergodic CSHMM
The most basic CSHMM is the ergodic CSHMM,
which is a simple concatenation of SHMMs in
an ergodic manner as shown in Fig. 1. For K
classes, K SHMMs are combined with the initial
and transition probabilities all set to equal. In this
CSHMM, the assignment of class labels solely de-
pends on the output distributions of each class.
3.2 Ergodic CSHMM with Common States
This type of CSHMM is the same as the ergodic
CSHMM except that it additionally has a SHMM
trained from all dialogues of all classes. There-
401
3:speaker1 4:speaker2
1:speaker1 2:speaker2
5:speaker1 6:speaker2
Speaker HMM for Class 1
Speaker HMM for Class 2
Speaker HMM for All Classes (Class 0)
Figure 2: CSHMM with common states.
Copy
Class
1
M1
M1M0
Retrain
Train
Class
k
M0
Mk
Mk
Retrain
Train
Class
K
M0
MK
MK
Retrain
Train
All 
Classes
M0
Train
+
M0
M1 Mk MK
AVG
Concatenate
M1+0 Mk+0 MK+0
M1M0 M0 Mk
M0 MK
M1+0 Mk+0 MK+0
Step 1
Step 2
Step 3
Step 2?
END
Mconcat
If the fitting has 
converged for 
all Mk+0
Split Mconcat into 
pairs again and 
retrain Mk+0
M1?MK become 
less likely to
output common 
sequences
Transition probabilities
of M0 are redistributed
between M0 and Mk
Figure 3: Three steps to create a CSHMM using
concatenated training.
fore, for K classes, this CSHMM has K + 1
SHMMs. Figure 2 shows the model topology.
This newly added SHMM works in a manner sim-
ilar to the background model (Reynolds et al,
2000) representing sequences that are common
to all classes. By having these common states,
common utterance sequences can be classified as
?common?, making it possible to avoid forcefully
classifying common utterance sequences into one
of the given classes.
Detecting common sequences is especially
helpfulwhen several classes overlap in nature. For
example, most dialogues commonly start and end
with greetings, and many calls at contact centers
commonly contain exchanges in which the opera-
tor requests personal information about the caller
for confirmation. Regarding the model topology
in Fig. 2, if the most likely state sequence by
the Viterbi decoding is <1,4,5,6,3,2>, we obtain
a class label sequence <1,2,0,0,2,1> where the
third and fourth utterances are classified as ?zero?,
meaning that they do not belong to any class.
3.3 CSHMM using Concatenated Training
The CSHMMs presented so far have two prob-
lems: one is that the order of utterances of differ-
ent classes cannot be taken into account because
of the equal transition probabilities. As a result,
the very merit of HMMs, their ability to model
time series data, is lost. The other is that the out-
put distributions of common states may be overly
broad because they are the averaged distributions
over all classes; that is, the best path determined
by the Viterbi decoding may not go through the
common states at all.
Our solution to these problems is to apply con-
catenated training (Lee, 1989), which has been
successfully used in speech recognition to model
phoneme sequences in an unsupervised manner.
The procedure for concatenated training is illus-
trated in Fig. 3 and has three steps.
step 1 Let Mk (Mk ? M, 1 ? k ? K) be the
SHMM trained using dialogues Dk where
Dk = {?dj|c(dj) = k}, and M0 be the
SHMM trained using all dialogues; i.e., D.
Here, K means the total number of classes
and c(dj) the class assigned to a dialogue dj .
step 2 Connect each Mk ? M with a copy of
M0 using equal initial and transition proba-
bilities (we call this connected model Mk+0)
and retrain Mk+0 with ?dj ? Dk where
c(dj) = k.
step 3 Merge all models Mk+0 (1 ? k ? K) to
produce one concatenated HMM (Mconcat).
Here, the output probabilities of the copies
of M0 are averaged over K when all models
are merged to create a combined model. If
the fitting of all Mk+0 models has converged
against the training data, exit this procedure;
otherwise, go to step 2 by connecting a copy
of M0 and Mk for all k. Here, the transi-
tion probabilities from M0 to Ml(l 6= k) are
summed and equally distributed between the
copied M0?s self-loop and transitions to the
states in Mk.
In concatenated training, the transition and output
probabilities can be optimized between M0 and
402
Contact
Center
Dialogues
Domain 1
Domain K
?
HMM for Domain 1 HMM for Domain K
HMM for All Domains
Model topic label 
sequences
INPUT: A dialogue in Domain k
Topic Model
Topic label sequence
L
S
A
/
L
D
A
A
s
s
i
g
n
 
t
o
p
i
c
l
a
b
e
l
s
Domain label sequence OUTPUT: summary
Viterbi decoding
Assign
topic labels
Select utterances labeled with Domain k
Class Speaker
HMM
?..
Utterance sequence
Feature sequence
Extract content words
as utterance features
Figure 4: Overview of our summarization
method.
Mk, meaning that the output probabilities of utter-
ance sequences that are common and also found
in Mk can be moved from Mk to M0. This makes
the distribution of Mk sharp (not broad/uniform),
making it likely to output only the utterances rep-
resentative of a class k. As regards M0, its distri-
bution of output probabilities can also be sharp-
ened for utterances that occur commonly in all
classes. This sharpening of distributions is likely
to be helpful for class discrimination.
4 Summarization Method
We apply CSHMMs to extractive summarization
of contact center dialogues because such dia-
logues are two-party, can be categorized into mul-
tiple classes by their call domains (e.g., inquiry
types), and are likely contain many overlapping
exchanges between an operator and a caller across
domains, such as greetings, the confirmation of
personal information, and other cliches in busi-
ness (e.g., name exchanges, thanking/apologizing
phrases, etc.), making them the ideal target for
CSHMMs.
In our method, summarization is performed by
decoding a sequence of utterances of a domain
DMk into domain labels and selecting those ut-
terances that have domain labels DMk. This
makes it possible to extract utterances that are
characteristic of DMk in relation to other do-
mains. Our assumption is that extracting charac-
teristic sequences of a given domain provides a
good summary for that domain because such se-
quences should contain important information ne-
cessitated by the domain.
Figure 4 outlines our extractive summarization
process. The process consists of a training phase
and a decoding phase as described below.
Training phase: Let D (d1 . . . dN ) be the entire
set of contact center dialogues, DMk (DMk ?
DM, 1 ? k ? K) the domain assigned to do-
main k, and Udi,1 . . .Udi,H the utterances in di.
Here, H is the number of utterances in di. From
D, we create two models: a topic model (TM )
and a CSHMM.
The topic model is used to assign a single topic
to each utterance so as to facilitate the training
of the CSHMM by reducing the dimensions of
the feature space. The same approach has been
taken in (Barzilay and Lee, 2004). The topic
model can be created by such techniques as prob-
abilistic latent semantic analysis (PLSA) (S?ingliar
and Hauskrecht, 2006) and latent Dirichlet alo-
cation (LDA) (Tam and Schultz, 2005). PLSA
models the latent topics of the documents and its
Baysian extension is LDA, which also models the
co-occurrence of topics using the Dirichlet prior.
We first derive features Fd1 . . . FdN for the dia-logues. Here, we assume a bag-of-words repre-
sentation for the features; therefore, Fdi is repre-sented as {< w1, c1 > . . . < wV , cV >}, where
V means the total number of content words in the
vocabulary and < wi, ci > denotes that a content
word wi appears ci times in a dialogue. Note that
we derive the features for dialogues, not for utter-
ances, because utterances in dialogue can be very
short, often consisting of only one or two words
and thus making it hard to calculate the word co-
occurrence required for creating a topic model.
From the features, we build a topic model that in-
cludes P(z|w), where w is a word and z is a topic.
Using the topic model, we can assign a single
topic label to every utterance in D by finding its
likely topic; i.e., argmax
z
?
w?words(Udi) P(z|w).
After labeling all utterances in D with topic la-
bels, we train a CSHMM that learns characteristic
topic label sequences in each domain as well as
common topic label sequences across domains.
Decoding phase: Let dj be the input dialogue,
DM(dj) (? DM ) the table for obtaining the do-
main label of dj , and Udj ,1 . . .Udj ,Hdj the utter-ances in dj, where Hdj is the number of the utter-ances. We use TM to map the utterances to topic
403
Domain # Tasks Sentences Characters
FIN 15 8.93 289.93
ISP 15 7.20 259.53
LGU 20 9.85 328.55
MO 15 10.07 326.20
PC 15 9.40 354.07
TEL 18 8.44 322.22
ALL 98 9.01 314.46
Table 1: Scenario statistics: the number of tasks
and averaged number of sentences/characters in a
task scenario in the six domains.
labels Tdj ,1 . . .Tdj ,Hdj and convert them into do-main label sequences DMdj ,1 . . .DMdj ,Hdj us-ing the trained CSHMM by the Viterbi decoding.
Then, we select Udj ,h (1 ? h ? Hdj ) whose cor-responding domain labelDMdj ,h equalsDM(dj)and output the selected utterances in the order of
appearance in the original dialogue as a summary.
5 Experiment
We performed an experiment to verify our sum-
marization method. We first collected simulated
contact center dialogues using human subjects.
Then, we compared our method with baseline sys-
tems. Finally, we analyzed the created summaries
to investigate what had been learned by our CSH-
MMs.
5.1 Dialogue Data
Since we do not have access to actual contact cen-
ter data, we recruited human subjects to collect
simulated contact center dialogues. A total of 90
participants (49 males and 41 females) took the
roles of operator or a caller and talked over tele-
phones in separate rooms. The callers were given
realistic scenarios that included their motivation
for a call as well as detailed instructions about
what to ask. The operators, who had experience
of working at contact centers, were given manuals
containing the knowledge of the domain and ex-
plaining how to answer questions in specific sce-
narios.
The dialogues took place in six different do-
mains: Finance (FIN), Internet Service Provider
(ISP), Local Government Unit (LGU), Mail Or-
der (MO), PC support (PC), and Telecommuni-
cation (TEL). In each domain, there were 15?20
tasks. Table 1 shows the statistics of the task sce-
narios used by the callers. We cannot describe the
details of each domain for lack of space, but ex-
MO task No. 3: It is becoming a good season for the
Japanese Nabe (pan) cuisine. You own a Nabe restau-
rant and it is going well. When you were searching on
the Internet, thinking of creating a new dish, you saw
that drop-shipped Shimonoseki puffer fish was on sale.
Since you thought the puffer fish cuisine would become
hot in the coming season, you decided to order it as a
trial. . . . You ordered a puffer fish set on the Internet,
but you have not received the confirmation email that
you were supposed to receive. . . . You decided to call
the contact center to make an inquiry, ask them whether
the order has been successful, and request them to send
you the confirmation email.
Figure 5: Task scenario in the MO domain. The
scenario was originally in Japanese and was trans-
lated by the authors.
amples of the tasks for FIN are inquiries about in-
surance, notifications of the loss of credit cards,
and applications for finance loans, and those for
ISP are inquiries about fees for Internet access, re-
quests to forward emails, and reissuance of pass-
words. Figure 5 shows one of the task scenarios
in the MO domain.
We collected data on two separate occasions us-
ing identical scenarios but different participants,
which gave us two sets of dialogue data. We used
the former for training our summarization sys-
tem and the latter for testing. We only use the
transcriptions in this paper so as to avoid partic-
ular problems of speech. All dialogues were in
Japanese. Tables 2 and 3 show the statistics of the
training data and the test data, respectively. As
can be seen from the tables, each dialogue is quite
long, which attests to the complexity of the tasks.
5.2 Training our Summarization System
For training our system, we first created a topic
model using LDA.We performed a morphological
analysis using ChaSen1 to extract content words
from each dialogue and made its bag-of-words
features. We defined content words as nouns,
verbs, adjectives, unknown words, and interjec-
tions (e.g., ?yes?, ?no?, ?thank you?, and ?sorry?).
We included interjections because they occur very
frequently in dialogues and often possess impor-
tant content, such as agreement and refusal, in
transactional communication. We use this defini-
tion of content words throughout the paper.
Then, using an LDA software package2, we
built a topic model. We tentatively set the number
1http://chasen-legacy.sourceforge.jp/
2http://chasen.org/?daiti-m/dist/lda/
404
Utterances/Dial. Characters/Utt.
Domain # dial. OPE CAL Both OPE CAL Both
FIN 59 75.73 72.69 148.42 17.44 7.54 12.59
ISP 64 55.09 53.17 108.27 20.11 8.03 14.18
LGU 76 58.28 50.55 108.83 12.83 8.55 10.84
MO 70 66.39 58.74 125.13 15.09 7.43 11.49
PC 56 89.34 77.80 167.14 15.48 6.53 11.31
TEL 66 75.58 63.97 139.55 12.74 8.24 10.67
ALL 391 69.21 61.96 131.17 15.40 7.69 11.76
Table 2: Training data statistics: Averaged num-
ber of utterances per dialogue and characters per
utterance for each domain. OPE and CAL denote
operator and caller, respectively. See Section 5.1
for the full domain names.
Utterances/Dial. Characters/Utt.
Domain # dial. OPE CAL Both OPE CAL Both
FIN 60 73.97 61.05 135.02 14.53 7.50 11.35
ISP 59 76.08 61.24 137.32 15.43 6.94 11.65
LGU 56 66.55 51.59 118.14 14.54 7.53 11.48
MO 47 75.53 64.87 140.40 10.53 6.79 8.80
PC 44 124.02 94.16 218.18 14.23 7.79 11.45
TEL 41 93.71 68.54 162.24 13.94 7.85 11.37
ALL 307 83.07 65.69 148.76 13.98 7.41 11.08
Table 3: Test data statistics.
of topics to 100. Using this topic model, we la-
beled all utterances in the training data using these
100 topic labels.
We trained seven different CSHMMs in all: one
ergodic CSHMM (ergodic0), three variants of er-
godic CSHMMs with common states (ergodic1,
ergodic2, ergodic3), and three variants of CSH-
MMs with concatenated training (concat1, con-
cat2, concat3). The difference within the variants
is in the number of common states. The numbers
0?3 after ?ergodic? and ?concat? indicate the num-
ber of SHMMs containing common states. For
example, ergodic3 has nine SHMMs (six SHMMs
for the six domains plus three SHMMs contain-
ing common states). Since more states would
enable more minute modeling of sequences, we
made such variants in the hope that common se-
quences could be more accurately modeled. We
also wanted to examine the possibility of creat-
ing sharp output distributions in common states
without the concatenated training by such minute
modeling. These seven CSHMMs make seven dif-
ferent summarization systems.
5.3 Baselines
Baseline-1: BL-TF We prepared two baseline
systems for comparison. One is a simple sum-
marizer based on the maximum coverage of high
term frequency (TF) content words. We call
this baseline BL-TF. This baseline summarizes a
dialogue by maximizing the following objective
function:
max
?
zi?Z
weight(wi) ? zi
where ?weight? returns the importance of a con-
tent word wi and zi is a binary value indicating
whether to include wi in the summary. Here,
?weight? returns the count of wi in a given dia-
logue. The maximization is done using ILP (we
used an off-the-shelf solver lp solve3) with the
following three constraints:
xi, zi ? {0, 1}
?
xi?X
lixi ? K
?
i
mijxi ? zj (?zj ? Z)
where xi is a binary value that indicates whether
to include the i-th utterance in the summary, li is
the length of the i-th utterance,K is the maximum
number of characters to include in a summary, and
mij is a binary value that indicates whether wi is
included in the j-th utterance. The last constraint
means that if a certain utterance is included in the
summary, all words in that utterance have to be
included in the summary.
Baseline-2: BL-DD Although BL-TF should be
a very competitive baseline because it uses the
state-of-the-art formulation as noted in (Gillick
and Favre, 2009), having only this baseline is
rather unfair because it does not make use of the
training data, whereas our proposed method uses
them. Therefore, we made another baseline that
learns domain-specific dictionaries (DDs) from
the training data and incorporates them into the
weights of content words of the objective function
of BL-TF. We call this baseline BL-DD. In this
baseline, the weight of a content word wi in a do-
main DMk is
weight(wi,DMk) =
log(P(wi|DMk))
log(P(wi|DM\DMk))
3http://lpsolve.sourceforge.net/5.5/
405
Metric ergodic0 ergodic1 ergodic2 ergodic3 concat1 concat2 concat3
PROPOSED
F 0.177 0.177 0.177 0.177 0.187?e0e1e2e3 0.198?+e0e1e2e3c1 0.199?+e0e1e2e3c1precision 0.145 0.145 0.145 0.145 0.161? 0.191?+ 0.195?+
recall 0.294 0.294 0.294 0.294 0.280? 0.259?+ 0.259?+
(Same-length) BL-TF
F 0.171 0.171 0.171 0.171 0.168 0.164 0.163
precision 0.132 0.132 0.132 0.132 0.135 0.140 0.140
recall 0.294 0.294 0.294 0.294 0.270 0.241 0.240
(Same-length) BL-DD
F 0.189 0.189 0.189 0.189 0.189 0.187 0.187
precision 0.155 0.155 0.155 0.155 0.162 0.170 0.172
recall 0.287 0.287 0.287 0.287 0.273 0.250 0.248
Compression Rate 0.42 0.42 0.42 0.42 0.37 0.30 0.30
Table 4: F-measure, precision, and recall averaged over all 307 dialogues (cf. Table 3) in the test
set for the proposed methods and baselines BL-TF and BL-DD configured to output the same-length
summaries as the proposed systems. The averaged compression rate for each proposed system is shown
at the bottom. The columns (ergodic0?concat3) indicate our methods as well as the character lengths
used by the baselines. Asterisks, ?+?, e0?e3, and c1?c3 indicate our systems? statistical significance by
the Wilcoxon signed-rank test (p<0.01) over BL-TF, BL-DD, ergodic0?3, and concat1?3, respectively.
Statistical tests for the precision and recall were only performed between the proposed systems and
their same-length baseline counterparts. Bold font indicates the best score in each row.
where P(wi|DMk) denotes the occurrence prob-
ability of wi in the dialogues of DMk , and
P(wi|DM\DMk) the occurrence probability of
wi in all domains except for DMk. This log like-
lihood ratio estimates how much a word is char-
acteristic of a given domain. Incorporating such
weights would make a very competitive baseline.
5.4 Evaluation Procedure
We made our seven proposed systems and two
baselines (BL-TF and BL-DD) output extractive
summaries for the test data. Since one of the
shortcomings of our proposedmethod is its inabil-
ity to set the compression rate, we made our sys-
tems output summaries first and made the baseline
systems output their summaries within the charac-
ter lengths of our systems? summaries.
We used scenario texts (See Fig. 5) as reference
data; that is, a dialogue dealing with a certain task
is evaluated using the scenario text for that task.
As an evaluation criterion, we used the F-measure
(F1) to evaluate the retrieval accuracy on the ba-
sis of the recall and precision of retrieved content
words. We used the scenarios as references be-
cause they contain the basic content exchanged
between an operator and a caller, the retrieval ac-
curacy of which should be important for quality
analysts.
We could have used ROUGE (Lin and Hovy,
2003), but we did not because ROUGE does not
correlate well with human judgments in conversa-
tional data (Liu and Liu, 2008). Another benefit of
using the F-measure is that summaries of varying
lengths can be compared.
5.5 Results
Table 4 shows the evaluation results for the pro-
posed systems and the baselines. It can be seen
that concat3 shows the best performance in F-
measure among all systems, having a statistically
better performance over all systems except for
concat2. The CSHMMs with concatenated train-
ing were all better than ergodic0?3. Here, the per-
formance (and output) of ergodic0?3 was exactly
the same. This happened because of the broad dis-
tributions in their common states; no paths went
through the common states and all paths went
through the SHMMs of the six domains instead.
The evaluation results in Table 4 may be rather
in favor of our systems because the summarization
lengths were set by the proposed systems. There-
fore, we performed another experiment to inves-
tigate the performance of the baselines with vary-
ing compression rates and compared their perfor-
mance with the proposed systems in F-measure.
We found that the best performance was achieved
by BL-DD when the compression rate was 0.4
with the F-measure of 0.191, which concat3 sig-
nificantly outperformed by the Wilcoxon signed-
rank test (p<0.01). Note that the performance
shown in Table 4 may seem low. However, we
found that the maximum recall is 0.355 (cal-
406
CAL1 When I order a product from you, I get a confir-
mation email
CAL2 Puffer fish
CAL3 Sets I have ordered, but I haven?t received
the confirmation email
OPE1 Order
OPE2 I will make a confirmation whether you have
ordered
OPE3 Ten sets of Shimonoseki puffer fish by drop-
ship
OPE4 ?Yoriai? (name of the product)
OPE5 Two kilos of bony parts of tiger puffer fish
OPE6 Baked fins for fin sake
OPE7 600 milliliter of puffer fish soy sauce
OPE8 And, grated radish and red pepper
OPE9 Your desired delivery date is the 13th of Febru-
ary
CAL4 Yes, all in small cases
CAL5 This is q in alphabet right?
CAL6 Hyphen g
CAL7 You mean that the order was successful
OPE10 Yes, it was Nomura at JDS call center
Figure 6: Example output of concat3 for MO task
No. 3 (cf Fig. 5). The utterances were translated
by the authors. The compression rate for this dia-
logue was 0.24.
culated by using summaries with no compres-
sion). This means that the maximum F-measure
we could attain is lower than 0.524 (when the pre-
cision is ideal with 1). This is because of the dif-
ferences between the scenarios and the actual di-
alogues. We want to pursue ways to improve our
evaluation methodology in the future.
Despite such issues in evaluation, from the re-
sults, we conclude that our extractive summa-
rization method is effective and that having the
common states and training CSHMMs with con-
catenated training are useful in modeling domain-
specific sequences of contact center dialogues.
5.6 Example of System Output
Figure 6 shows an example output of concat3 for
the scenario MO task No. 3 (cf. Fig. 5). Bold font
indicates utterances that were NOT included in the
summary of concat3?s same-length-BF-DD coun-
terpart. It is clear that sequences related to the
MO domain were successfully extracted. When
we look at the summary of BF-DD, we see such
utterances as ?Can I have your address from the
postcode? and ?Finally, can I have your email ad-
dress?, which are obvious cliches in contact center
dialogues. This indicates the usefulness of com-
mon states for ignoring such common exchanges.
6 Summary and Future Work
This paper proposed a novel extractive sum-
marization method for contact center dialogues.
We devised a particular type of HMM called
CSHMM, which processes operator/caller utter-
ance sequences of multiple domains simulta-
neously to model domain-specific utterance se-
quences and common sequences at the same time.
We trained a CSHMM using the transcripts of
simulated contact center dialogues and verified its
effectiveness for the summarization of calls.
There still remain several limitations in our ap-
proach. One is its inability to change the com-
pression rate, which we aim to solve in the next
step using the forward-backward algorithm (Ra-
biner and Juang, 1986). This algorithm can cal-
culate the posterior probability of each state at
each time frame given an input dialogue sequence,
enabling us to extract top-N domain-specific se-
quences. We also need to find the appropriate
topic number for the topic model. In our imple-
mentation, we used a tentative value of 100, which
may not be appropriate. In addition, we believe
the topic model and the CSHMM can be unified
because these models are fundamentally similar,
especially when LDA is employed. Model topolo-
gies may also have to be reconsidered. In our
CSHMM with concatenated training, the states in
domain-specific SHMMs are only connected to
the common states, which may be inappropriate
because there could be a case where a domain
changes from one to another without having a
common sequence. ApplyingCSHMMs to speech
and other NLP tasks is another challenge. As a
near-term goal, we aim to apply our method to the
summarization of meetings, where we will need to
extend our CSHMMs to deal with more than two
participants. Finally, we also want to build a con-
tact center dialogue agent by extending the CSH-
MMs to partially observableMarkov decision pro-
cesses (POMDPs) (Williams and Young, 2007) by
following the recent work on building POMDPs
from dialogue data in the dynamic Bayesian net-
work (DBN) framework (Minami et al, 2009).
Acknowledgments
We thank the members of the Spoken Dialog
System Group, especially Noboru Miyazaki and
Satoshi Kobashikawa, for their effort in dialogue
data collection.
407
References
Barzilay, Regina and Lillian Lee. 2004. Catching the drift:
Probabilistic content models, with applications to gener-
ation and summarization. In Proceedings of the Human
Language Technology Conference of the North American
Chapter of the Association for Computational Linguistics
(HLT-NAACL), pages 113?120.
Byrd, Roy J., Mary S. Neff, Wilfried Teiken, Youngja
Park, Keh-Shin F. Cheng, Stephen C. Gates, and Karthik
Visweswariah. 2008. Semi-automated logging of contact
center telephone calls. In Proceeding of the 17th ACM
conference on Information and knowledge management
(CIKM), pages 133?142.
Chu-Carroll, Jennifer and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational Lin-
guistics, 25(3):361?388.
Gillick, Dan and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10?18.
Gorin, Allen L., Giuseppe Riccardi, and Jerry H. Wright.
1997. How may I help you? Speech Communication,
23(1-2):113?127.
Hori, Chiori and Sadaoki Furui. 2003. A new approach to
automatic speech summarization. IEEE Transactions on
Multimedia, 5(3):368?378.
Kupiec, Julian, Jan Pedersen, and Francine Chen. 1995. A
trainable document summarizer. In Proceedings of the
18th annual international ACM SIGIR conference on Re-
search and development in information retrieval (SIGIR),
pages 68?73.
Lee, Kai-Fu. 1989. Automatic speech recognition: the de-
velopment of the SPHINX system. Kluwer Academic Pub-
lishers.
Lin, Chin-Yew and Eduard Hovy. 2003. Automatic evalua-
tion of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics on Human Language Technology (NAACL-HLT),
pages 71?78.
Liu, Feifan and Yang Liu. 2008. Correlation between
ROUGE and human evaluation of extractive meeting sum-
maries. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics on Human
Language Technologies (HLT), pages 201?204.
Mani, Inderjeet. 2001. Automatic summarization. John
Benjamins Publishing Company.
Meguro, Toyomi, Ryuichiro Higashinaka, Kohji Dohsaka,
Yasuhiro Minami, and Hideki Isozaki. 2009. Analysis of
listening-oriented dialogue for building listening agents.
In Proceedings of the SIGDIAL 2009 conference, pages
124?127.
Minami, Yasuhiro, Akira Mori, Toyomi Meguro, Ryuichiro
Higashinaka, Kohji Dohsaka, and Eisaku Maeda. 2009.
Dialogue control algorithm for ambient intelligence based
on partially observable Markov decision processes. In
Proceedings of the 1st international workshop on spoken
dialogue systems technology (IWSDS), pages 254?263.
Murray, Gabriel, Steve Renals, and Jean Carletta. 2005. Ex-
tractive summarization of meeting recordings. In Pro-
ceedings of the 9th European Conference on Speech
Communication and Technology (EUROSPEECH), pages
593?596.
Osborne, Miles. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Workshop
on Automatic Summarization, pages 1?8.
Rabiner, Lawrence R. and Biing-Hwang Juang. 1986. An
introduction to hiddenMarkov models. IEEE ASSP Mag-
azine, 3(1):4?16.
Rabiner, Lawrence R. 1990. A tutorial on hidden Markov
models and selected applications in speech recognition.
Readings in speech recognition, 53(3):267?296.
Radev, Dragomir R., Hongyan Jing, Ma?gorzata Stys?, and
Daniel Tam. 2004. Centroid-based summarization of
multiple documents. Information Processing & Manage-
ment, 40(6):919?938.
Reynolds, Douglas A., Thomas F. Quatieri, and Robert B.
Dunn. 2000. Speaker verification using adaptedGaussian
mixture models. Digital Signal Processing, 10(1-3):19 ?
41.
Subramaniam, L. Venkata, Tanveer A. Faruquie, Shajith Ik-
bal, Shantanu Godbole, and Mukesh K. Mohania. 2009.
Business intelligence from voice of customer. In Pro-
ceedings of the 2009 IEEE International Conference on
Data Engineering (ICDE), pages 1391?1402.
Takeuchi, Hironori, L Venkata Subramaniam, Tetsuya Na-
sukawa, Shourya Roy, and Sreeram Balakrishnan. 2007.
A conversation-mining system for gathering insights to
improve agent productivity. In Proceedings of the IEEE
International Conference on E-Commerce Technology
and the IEEE International Conference on Enterprise
Computing, E-Commerce, and E-Services, pages 465?
468.
Tam, Yik-Cheung and Tanja Schultz. 2005. Dynamic
language model adaptation using variational Bayes in-
ference. In Proceedings of the 9th European Confer-
ence on Speech Communication and Technology (EU-
ROSPEECH), pages 5?8.
S?ingliar, Tomas and Milos Hauskrecht. 2006. Noisy-OR
component analysis and its application to link analy-
sis. The Journal of Machine Learning Research, 7:2189?
2213.
Williams, JasonD. and Steve Young. 2007. Partially observ-
able Markov decision processes for spoken dialog sys-
tems. Computer Speech & Language, 21(2):393?422.
408
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 928?939, Dublin, Ireland, August 23-29 2014.
Towards an open-domain conversational system fully based on natural
language processing
Ryuichiro Higashinaka
1
, Kenji Imamura
1
, Toyomi Meguro
2
, Chiaki Miyazaki
1
Nozomi Kobayashi
1
, Hiroaki Sugiyama
2
, Toru Hirano
1
Toshiro Makino
1
, Yoshihiro Matsuo
1
1
NTT Media Intelligence Laboratories
2
NTT Communication Science Laboratories
{higashinaka.ryuichiro, imamura.kenji, meguro.toyomi, miyazaki.chiaki,
kobayashi.nozomi, sugiyama.hiroaki, hirano.tohru,
makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jp
Abstract
This paper proposes an architecture for an open-domain conversational system and evaluates an
implemented system. The proposed architecture is fully composed of modules based on natu-
ral language processing techniques. Experimental results using human subjects show that our
architecture achieves significantly better naturalness than a retrieval-based baseline and that its
naturalness is close to that of a rule-based system using 149K hand-crafted rules.
1 Introduction
Although task-oriented dialogue systems have been extensively researched over the decades (Walker
et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain
conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore
and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational
system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge
for a domain and create understanding and generation modules for that domain (Nakano et al., 2000).
However, for open-domain conversation, such preparation cannot be performed. Since it is difficult to
handle users? open-domain utterances, to create workable systems, conventional approaches have used
hand-crafted rules (Wallace, 2004). Although elaborate rules may work well, the problem with the rule-
based approach is the high cost and the dependence on individual skills of developers, which hinders
systematic development. Another problem with the rule-based approach is its low coverage; that is, the
inability to handle unexpected utterances.
The recent increase of web data has propelled the development of approaches that use data retrieved
from the web for open-domain conversation (Shibata et al., 2009; Ritter et al., 2011). The merit of such
retrieval-based approaches is that, owing to the diversity of the web, systems can retrieve at least some
responses for user input, which solves the coverage problem. However, this comes at the cost of utterance
quality. Since the web, especially Twitter, is inherently noisy, it is, in many cases, difficult to sift out
appropriate sentences from retrieval results.
In this paper, we propose an architecture for an open-domain conversational system. The proposed
architecture is fully composed of modules based on natural language processing (NLP) techniques. Our
stance is not just to hand-craft or to search the web for utterances, but to create a system that can fully
understand and generate utterances. We want to show that it is possible to build an open-domain conver-
sational system by combining NLP modules, which will open the way to a systematic development and
improvement. We describe our open-domain conversational system based on our architecture and present
results of an evaluation of its performance by human subjects. We compare our system with rule-based
and retrieval-based systems, and show that our architecture is a promising direction. In this work, we
regard the term open-domain conversation to be interchangeable with non-task-oriented dialogue, casual
conversation (Eggins and Slade, 2005), chat, or social dialogue (Bickmore and Cassell, 2000). We use
the term to denote that user input is not restricted in any way as in open-domain question answering
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
928
	
			
	

	
		
	

Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 18?27,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Modeling User Satisfaction Transitions in Dialogues from Overall Ratings
Ryuichiro Higashinaka?, Yasuhiro Minami?, Kohji Dohsaka?, and Toyomi Meguro?
? NTT Cyber Space Laboratories, NTT Corporation
? NTT Communication Science Laboratories, NTT Corporation
higashinaka.ryuichiro@lab.ntt.co.jp
{minami,dohsaka,meguro}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a novel approach
for predicting user satisfaction transitions
during a dialogue only from the ratings
given to entire dialogues, with the aim
of reducing the cost of creating refer-
ence ratings for utterances/dialogue-acts
that have been necessary in conventional
approaches. In our approach, we first
train hidden Markov models (HMMs) of
dialogue-act sequences associated with
each overall rating. Then, we combine
such rating-related HMMs into a single
HMM to decode a sequence of dialogue-
acts into state sequences representing to
which overall rating each dialogue-act is
most related, which leads to our rating pre-
dictions. Experimental results in two di-
alogue domains show that our approach
can make reasonable predictions; it signif-
icantly outperforms a baseline and nears
the upper bound of a supervised approach
in some evaluation criteria. We also
show that introducing states that represent
dialogue-act sequences that occur com-
monly in all ratings into an HMM signifi-
cantly improves prediction accuracy.
1 Introduction
In recent years, there has been intensive work
on the automatic evaluation of dialogues (Walker
et al, 1997; Mo?ller et al, 2008). Automatic
evaluation makes it possible to predict the per-
formance of dialogue systems without the costly
process of performing surveys with human sub-
jects, leading to a rapid improvement cycle for
dialogue systems. It is also useful for detect-
ing problematic situations in an ongoing dialogue
(Walker et al, 2002; Herm et al, 2008; Kim,
2007). In these studies, the typical approach is
to train a prediction model, such as a regression
or classification model, using features represent-
ing the whole or a part of a dialogue together with
human reference labels (e.g., reference ratings).
However, creating such reference labels by hand
can be extremely costly when we want to predict
user satisfaction transitions during a dialogue be-
cause we need to create reference labels after each
utterance/dialogue-act in the training data (Engel-
brecht et al, 2009).
This paper proposes a novel approach for pre-
dicting user satisfaction transitions during a dia-
logue only from the dialogues with overall rat-
ings. The approach makes it possible to avoid
creating reference labels for utterances/dialogue-
acts and only requires a single reference label for
each dialogue. More specifically, we predict the
user satisfaction rating after each dialogue-act in a
dialogue only by using dialogues with dialogue-
level (overall) user satisfaction ratings as train-
ing data. Our basic approach is to train hid-
den Markov models (HMMs) of dialogue-act se-
quences associated with each overall rating and
combine such rating-related HMMs into a single
HMM. We use this combined HMM to decode a
sequence of dialogue-acts by the Viterbi algorithm
(Rabiner, 1990) into state sequences that indicate
from which rating-related HMM each dialogue-act
is most likely to have been generated, leading to
our rating predictions for the dialogue-acts. This
paper experimentally examines the validity of our
approach and explores several model topologies
for possible improvement.
In Section 2, we review related work on auto-
matic evaluation of dialogues. In Section 3, we
describe our approach in detail. In Section 4, we
describe the experiment we performed to verify
our approach and present the results. In Section
5, we summarize and mention future work.
2 Related Work
Regression models are typically utilized for eval-
uating the quality of an entire dialogue. Most fa-
mously, the PARADISE framework (Walker et al,
1997) learns from data a linear regression model
that predicts dialogue-level user satisfaction from
various objective characteristics of a dialogue that
concern task success and dialogue costs. This
framework is widely used today and a number of
extensions have been proposed to improve the pre-
diction performance (Mo?ller et al, 2008); how-
18
ever, it is not aimed at predicting user satisfaction
transitions.
Classification models are widely employed to
detect problematic situations in an ongoing dia-
logue. Walker et al (2002) developed the Prob-
lematic Dialogue Predictor for the ?How May I
Help You? system (Gorin et al, 1997) to robustly
transfer problematic calls to human operators in
call routing tasks. They derive speech recogni-
tion, language understanding, and dialogue man-
agement features from the first few turns of a dia-
logue and apply a decision tree classifier to detect
problematic calls. For a similar task, Hirschberg
et al (2004) and Herm et al (2008) used prosodic
and emotional features. Kim (2007) recently pro-
posed an approach for online call quality monitor-
ing so that problematic calls can be transferred to
human operators as quickly as possible rather than
waiting for the first few turns.
N-grams andHMM-based approaches have also
been actively studied. Hara et al (2010) proposed
predicting the most likely user satisfaction level of
a dialogue by using N-grams of dialogues for each
satisfaction level in the music navigation domain.
Isomura et al (2009) used HMMs to evaluate the
naturalness of a dialogue in their interview system.
They trained HMMs that model dialogue-act se-
quences between human subjects and used them to
evaluate human-machine dialogues by the output
probabilities of the HMMs. Recently, there have
been approaches to predict user satisfaction tran-
sitions by evaluating the quality of individual ut-
terances in a dialogue. For example, Engelbrecht
et al (2009) predicted user satisfaction ratings af-
ter each user utterance by HMMs trained from
utterance-level features and utterance-level refer-
ence ratings.
The problem with these approaches is that they
require a lot of training data, especially when we
want to predict the quality of smaller units such
as utterances. Our aim is to reduce such cost.
Our work is similar to Engelbrecht?s work (Engel-
brecht et al, 2009) in that we useHMMs to predict
user satisfaction transitions during a dialogue but
different in that we only use dialogue-level ratings
to model dialogue-act-level user satisfaction tran-
sitions.
3 Approach
We aim to predict user satisfaction transitions only
from dialogues with overall ratings. More for-
mally, given a dialogue d
i
of a set of dialogues
D (= {d
1
. . .d
N
}), we want to predict the user
satisfaction rating after each dialogue-act in d
i
,
namely, r?(da(d
i
, 1)) . . .r?(da(d
i
, m
i
)), using D
with their dialogue-level ratings r(d
1
) . . . r(d
N
).
1:speaker1 2:speaker2
Speaker HMM for Rating 1
3:speaker1 4:speaker2
Speaker HMM for Rating 2
Figure 1: SHMMs connected ergodically. In the
figure, an oval marked with speaker1/speaker2
indicates a state that emits speaker1/speaker2?s
dialogue-acts. Arrows denote transitions and
numbers before speaker1/speaker2 are state IDs.
Boxes group together the states related to a partic-
ular overall rating.
Here, da(d
i
, l) denotes the l-th dialogue-act in d
i
,
N the total number of dialogues, and m
i
the total
number of dialogue-acts in d
i
.
Our basic idea is to train HMMs representing
dialogue-act sequences of dialogues for each over-
all rating and combine these rating-related HMMs
into a single HMM that can assign ratings for
dialogue-acts by estimating from which HMM
each dialogue-act has most likely to have been
generated by the Viterbi decoding. We use HMMs
because they can deal with sequences that evolve
over time and have been successfully utilized to
model and evaluate dialogue-act sequences (Shi-
rai, 1996; Isomura et al, 2009; Engelbrecht et
al., 2009). The generative feature of an HMM is
also useful when we want to build a probabilis-
tic dialogue manager that produces the most likely
dialogue-act sequences (Hori et al, 2008) or that
aims to maximize a reward function in partially
observable Markov decision processes (Williams
and Young, 2007; Minami et al, 2009).
When there are K levels of user satisfaction as
overall ratings, we create K HMMs each of which
is trained using the dialogue-act sequences in dia-
logues D
k
? D, where D
k
= {?d
i
, |r(d
i
) = k}.
We use the EM-algorithm to train HMMs. Here,
we assume that each HMM has two states, each
of which emits dialogue-acts of one of the con-
versational participants. This type of HMM is
called a speaker HMM (SHMM) and has been
successfully utilized to model two-party conversa-
tion (Meguro et al, 2009).
As an illustrative example, Fig. 1 shows two
SHMMs for ratings 1 and 2 that are connected
ergodically. We can simply use these connected
SHMMs (namely, states 1, 2, 3, and 4) to decode a
sequence of dialogue-acts into state sequences and
thereby obtain rating predictions. For example, if
the optimal state sequence obtained by the Viterbi
decoding is {4, 2, 1, 3, 2}, we can convert it into
ratings <2, 1, 1, 2, 1> using the ratings associated
with the states.
19
3:speaker1 4:speaker2
1:speaker1 2:speaker2
5:speaker1 6:speaker2
Speaker HMM for Rating 1
Speaker HMM for Rating 2
Speaker HMM for All Ratings
Figure 2: SHMMs with an additional SHMM
trained from all dialogues.
Introducing Common States: The simple er-
godic model may not be sufficient for appropri-
ately assigning ratings to input dialogue-act se-
quences because it is often the case that there
are dialogue-act sequences, such as greetings and
question-answer pairs, that commonly occur in ev-
ery dialogue. If we forcefully assign a rating for
such dialogue-act sequences, it may result in de-
grading the prediction accuracy. Therefore, in
addition to the simple ergodic model, we intro-
duce another SHMM that represents dialogue-act
sequences of dialogues for all ratings (see Fig.
2). This additional SHMM models dialogue-act
sequences that occur commonly in all dialogues
and it can simply be trained using all dialogues.
Hence, we call the states in this SHMM common
states. When this SHMM is added to the ergodic
model, it may be possible to reduce the possibil-
ity of our having to forcefully assign inappropriate
scores to common dialogue-act sequences. In this
model, when the optimal state sequence is {1, 4,
5, 6, 2}, the predicted ratings become <1, 2, 0, 0,
1>. Here, we assume that the SHMM for all rat-
ings corresponds to rating 0, which is reasonable
because common dialogue-acts should not affect
ratings. The obtained ratings can also be inter-
preted as <1, 2, 2, 2, 1> when we assume that
the rating of a dialogue-act is taken over from the
previous turn.
Using Concatenated Training: We have so far
presented two model topologies, one with K
SHMMs connected ergodically and the other with
K + 1 SHMMs having an additional SHMM rep-
resenting all ratings. However, we still have a
problem; that is, we need to find optimal transi-
tion probabilities between the SHMMs of different
ratings. Our solution is to use concatenated train-
ing (Lee, 1989). The procedure for concatenated
training is illustrated in Fig. 3 and has the follow-
ing three steps.
step 1 Train an SHMM M
k
(M
k
? M, 1 ?
k ? K) using dialogues D
k
, where D
k
=
Copy
Rating
1
M1
M1M0
Retrain
Train
Rating
k
M0
Mk
Mk
Retrain
Train
Rating
K
M0
MK
MK
Retrain
Train
All 
Ratings
M0
Train
+
M0
M1 Mk MK
AVG
Concatenate
M1+0 Mk+0 MK+0
M1M0 M0 Mk
M0 MK
M1+0 Mk+0 MK+0
Step 1
Step 2
Step 3
Step 2?
END
Mconcat
If the fitting has 
converged for 
all Mk+0
Split Mconcat into 
pairs again and 
retrain Mk+0
M1?MK become 
less likely to
output common 
sequences
Transition probabilities
of M0 are redistributed
between M0 and Mk
Figure 3: Three steps to combine SHMMs using
concatenated training.
{?d
i
|r(d
i
) = k}, and an SHMM M
0
using
all dialogues; i.e., D. Here, K means the
maximum level of user satisfaction and r(d
i
)
the rating assigned to d
i
.
step 2 Connect each M
k
? M with a copy of
M
0
using equal initial and transition proba-
bilities (we call this connected model M
k+0
)
and retrain M
k+0
with ?d
i
? D
k
, where
r(d
i
) = k.
step 3 Merge all models M
k+0
(1 ? k ? K) to
produce one concatenated HMM (M
concat
).
Here, the output probabilities of the copies
of M
0
are averaged over K when all models
are merged to create a combined model. If
the fitting of all M
k+0
models has converged
against the training data, exit this procedure;
otherwise, go to step 2 by connecting a copy
of M
0
and M
k
for all k. Here, the transi-
tion probabilities from M
0
to M
l
(l 6= k) are
summed and equally distributed between the
copied M
0
?s self-loop and transitions to the
states in M
k
.
In concatenated training, the transition and out-
put probabilities can be optimized between M
0
and M
k
, meaning that the output probabilities
of dialogue-act sequences that are common and
also found in M
k
can be moved from M
k
to
M
0
. This makes the distribution of M
k
sharp (not
broad/uniform), making it likely to output only
the dialogue-acts specific to a rating k. As re-
gards M
0
, its distribution of output probabilities
can also be sharpened for dialogue-acts that oc-
cur commonly in all ratings. This sharpening of
distributions is likely to be helpful in assigning
20
appropriate ratings to dialogue-act sequences. In
the next section, we experimentally examine how
these proposed HMMs perform in modeling and
predicting user satisfaction transitions in dialogue.
4 Experiment
To verify our approach, we first prepared dialogue
data. Then, we trained our HMMs and compared
them with a random baseline and an upper bound
that uses a supervised approach; that is, an HMM
is trained using reference labels on the dialogue-
act level.
4.1 Dialogue Data
We used dialogues in two domains; the animal
discussion (AD) domain and the attentive listen-
ing (AL) domain. All dialogues are in Japanese.
In both domains, the data we used were text dia-
logues. We did not use spoken dialogue data be-
cause we wanted to avoid particular problems of
voice, such as filled pauses and overlaps, although
we aim to deal with spoken dialogue in the future.
4.1.1 Animal Discussion
We used the dialogue data in the AD domain that
we previously collected (Higashinaka et al, 2008).
In this domain, the system and user talk about likes
and dislikes about animals via a text chat inter-
face. The data consist of 1000 dialogues between
a dialogue system and 50 human users. Each
user conversed with the system 20 times, includ-
ing two example dialogues at the beginning. All
user/system utterances have been annotated with
dialogue-acts. There are 29 dialogue-act types in-
cluding those related to self-disclosure, question,
response, and greetings. For example, a dialogue-
act DISC-P denotes one?s self-disclosure about a
proposition P. Here, P is either like(X,A) or
dislike(X,A) where X is a conversational par-
ticipant and A a certain animal. DISC-R denotes
one?s self-disclosure of a reason for a proposition.
See (Higashinaka et al, 2008) for the details of the
dialogue-acts.
For our experiment, we created two subsets of
the data. We first extracted 180 dialogues by
taking all 18 non-example dialogues for the ini-
tial ten users sorted by user ID (AD-SUB1; 4147
user dialogue-acts and 6628 system dialogue-
acts). Then, from AD-SUB1, we randomly ex-
tracted nine dialogues per user to form another
subset of 90 dialogues (AD-SUB2; 2050 user
dialogue-acts and 3290 system dialogue-acts). An
annotator, who was not one of the authors, la-
beled AD-SUB1 with dialogue-level user satis-
faction ratings and AD-SUB2 with utterance-level
ratings. More specifically, each dialogue/utterance
Utterance (dialogue-acts) Sm Cl Wi
SYS Do you like rabbits? (DA: Q-DISC-P) 6 6 6
USR I like rabbits. They are cute.
(DA: DISC-P, DISC-R)
SYS Indeed they are cute. (DA: REPEAT) 6 6 6
SYS Tell me why you like rabbits. 6 5 6
(DA: Q-DISC-R-OTHER)
USR I like them because they are small and
warm. (DA: DISC-P-R)
SYS You like them because they are warm. 7 5 7
(DA: REPEAT)
Overall rating for the dialogue 7 5 6
Figure 4: Excerpt of a dialogue with utterance-
level user satisfaction ratings for smoothness
(Sm), closeness (Cl), and willingness (Wi) in the
AD domain. SYS and USR denote system and
user, respectively. The dialogue was translated by
the authors.
was given three different user satisfaction rat-
ings related to ?Smoothness of the conversation?,
?Closeness perceived by the user towards the sys-
tem?, and ?Willingness to continue the conversa-
tion?. The ratings ranged from 1 to 7, where 1
is the worst and 7 the best (see Fig. 4 for exam-
ples of utterance-level and overall ratings given by
the annotator for an excerpt of a dialogue). In a
manner similar to (Evanini et al, 2008), we used a
third-person?s user satisfaction rating for the sake
of consistency.
For utterance-level ratings, the annotator care-
fully read each utterance and gave ratings after
each system utterance according to how she would
have felt after receiving each system utterance if
she had been the user in the dialogue. To make
the situation more realistic, she was not allowed
to look down at the dialogue after the current ut-
terance. At the beginning of a dialogue, the rat-
ings always started from four (neutral). When the
annotator gave dialogue-level ratings, she looked
through the entire dialogue and rated its quality
(smoothness, closeness, and willingness) accord-
ing to how she would have felt after having had
the dialogue in question.
4.1.2 Attentive Listening
We collected human-human listening-oriented di-
alogues in a manner similar to (Meguro et al,
2009). In this AL domain, a listener attentively
listens to the other in order to satisfy the speaker?s
desire to speak and to make himself/herself heard.
We collected such listening-oriented dialogues us-
ing a website where users taking the roles of lis-
teners and speakers were matched up to have con-
versations. There were ten listeners who always
stayed at the website and 37 speakers who could
talk to them anytime the listeners were available.
They were all paid for their participation. A con-
versation was done through a text-chat interface.
21
The use of facial and other non-linguistic expres-
sions were not allowed for analysis purposes. The
participants were instructed to end the conversa-
tion approximately after ten minutes. Within a
three-week period, each speaker was instructed to
have at least two conversations a day, resulting in
our collecting 1260 listening-oriented dialogues.
Two independent annotators labeled each utter-
ance with 40 dialogue-act types, including those
related to self-disclosure, question, internal argu-
ment, sympathy, and information giving. The
inter-annotator agreement was reasonable, with
0.57 in Cohen?s ?. Although we cannot describe
the full details of our dialogue-acts for lack of
space, we have dialogue-acts DISC-EVAL-POS for
one?s self-disclosure of his/her positive evalua-
tion towards a certain entity, DISC-EXP for one?s
self-disclosure of his/her experience, and SELF-Q-
DESIRE for one?s internal argument about his/her
desire (e.g., ?Have I ever wanted to go abroad??).
We used the dialogue-act annotation of one of the
annotators in this work.
An annotator gave dialogue-level user satis-
faction ratings to all 1260 dialogues (AL-ALL;
31779 speaker dialogue-acts and 28681 listener
dialogue-acts). Then, we made a subset of the
data by randomly selecting ten dialogues for
each of the ten listeners to obtain 100 dialogues
(AL-SUB1; 2453 speaker dialogue-acts and 2197
listener dialogue-acts). Finally, the annotator
gave utterance-level ratings to AL-SUB1. The
utterance-level ratings were given only after lis-
teners? utterances. The annotator gave three rat-
ings as in the AD domain; namely, smoothness,
closeness, and good listening. Instead of willing-
ness, we have a ?good listener? criterion asking
for how good the annotator thinks the listener is
from the viewpoint of attentive listening; for ex-
ample, how well the listener is making it easy for
the speaker to speak. All ratings ranged from 1 to
7. See Fig. 5 for a sample dialogue in the AL do-
main with utterance-level and overall ratings given
by the annotator.
4.2 Training HMMs
From the dialogue data and their dialogue-level
ratings, we created our proposed HMMs. We had
five topology variations:
ergodic0: The simple ergodic model with no ad-
ditional SHMM for all ratings. See Fig.
1 for the topology. This HMM has 7
SHMMs connected ergodically with equal
initial/transition probabilities.
ergodic1: The simple ergodic model with an ad-
ditional SHMM for all ratings. See Fig. 2
for the topology. This HMM has 8 (7 +
Utterance (dialogue-acts) Sm Cl GL
LIS You know, in spring, Japanese food tastes de-
licious. (DA: DISC-EVAL-POS)
5 5 5
SPK This time every year, I make a plan to go on
a healthy diet. But . . . (DA: DISC-HABIT)
LIS Uh-huh (DA: ACK) 6 5 6
SPK The temperature goes up suddenly!
(DA: INFO)
SPK It?s always too late! (DA: DISC-EVAL-NEG)
LIS Clothing worn gets less and less while not be-
ing able to lose weight. (DA: DISC-FACT)
6 6 6
SPK Well, people around me soon get used to my
body shape though. (DA: DISC-FACT)
Overall rating for the dialogue 7 7 7
Figure 5: Excerpt of a dialogue with utterance-
level user satisfaction ratings for smoothness
(Sm), closeness (Cl), and good listener (GL) in the
AL domain. SPK and LIS denote speaker and lis-
tener, respectively. Both the speaker and listener
are human.
1) SHMMs connected ergodically with equal
initial/transition probabilities.
ergodic2: Same as ergodic1 except that the num-
ber of common states is doubled so that com-
mon dialogue-act sequences can be more ac-
curately modeled. Note that without concate-
nated training, SHMMs for each rating may
also have sharp distributions for common se-
quences. One possible solution to avoid this
is to sharpen the distributions of common
states by increasing its number of states.
concat1: 8 (7 + 1) SHMMs combined using con-
catenated training. See Fig. 3 for the topol-
ogy.
concat2: Same as concat1 except that the number
of common states is doubled.
[See Appendices A and B for the actual examples
of the obtained models]
4.2.1 Baseline and Upper Bound
We created the following baseline (random) and
upper bound (supervised) models for comparison:
random: This outputs ratings 1?7 at random.
supervised: This is an HMM trained in a man-
ner similar to (Engelbrecht et al, 2009). This
model is the same as ergodic0 in topology but
different in that the initial, transition, and out-
put probabilities are trained in a supervised
manner using the dialogue-acts and dialogue-
act-level reference ratings in AD-SUB2 and
AL-SUB1. Since we only have ratings for
system/listener utterances in the corpora, in
order to make training data, we assumed that
the ratings for dialogue-acts corresponding
to user/speaker utterances were the same as
22
those after the previous system/listener utter-
ances. This model simulates the ideal situ-
ation where we possess user satisfaction rat-
ings for all dialogue-acts in the data.
4.3 Evaluation Procedure
We performed a ten-fold cross validation. We first
separated utterance-level labeled data (i.e., AD-
SUB2 or AL-SUB1) into 10 disjoint sets. Then,
for each set S, we used dialogue-level labeled
data (i.e., AD-SUB1 or AL-ALL) excluding S
for training HMMs. Here, ?supervised? only used
the utterance-level labeled data excluding S for
training. Then, we made the models (i.e., er-
godic0, ergodic1, ergodic2, concat1, concat2, ran-
dom and supervised) output rating sequences for
the dialogue-acts in S and evaluated them with the
reference ratings in S. We repeated this process
ten times to evaluate the overall performance.
Since utterance-level ratings are provided only
after system/listener utterances, we only evaluated
ratings after dialogue-acts corresponding to sys-
tem/listener utterances. When a system/listener
utterance contained multiple dialogue-acts, the
dialogue-acts were assumed to have the same rat-
ing as that utterance. When the output rating
sequences contain 0, which can be the case for
ergodic1?2 and concat1?2, the 0 is replaced by the
most previous non-zero rating. When 0 is found at
the beginning of a dialogue, it remained 0. Al-
though our reference ratings always started with
four (cf. Section 4.1.1), we did not use this in-
formation to fill initial zeros because we wanted
to evaluate the prediction accuracy when we do
not have any prior knowledge. Since some mod-
els may benefit from avoiding evaluating dialogue-
acts at the beginning because of these zeros, we
simply compared the rating sequences where all
models produced non-zero values. For exam-
ple, when we have three output rating sequences
<0,5,6,0,4>, <0,0,1,2,0>, and <1,2,3,4,5> for a
given dialogue-act sequence, the zeros that follow
non-zero values are first filled with their preceed-
ing values, and thereby we obtain <0,5,6,6,4>,
<0,0,1,2,2>, and <1,2,3,4,5>. Then, by cropping
the common non-zero span, we obtain <6,6,4>,
<1,2,2>, and <3,4,5>, and use these rating se-
quences for evaluation.
4.3.1 Evaluation Criteria
We used two kinds of evaluation criteria: one for
evaluating individual matches and the other for
evaluating distributions.
Evaluating Individual Matches: We used the
match rate and mean absolute error to evaluate the
matching of reference and hypothesis rating se-
quences. They are derived by the equations shown
below. In the equations, R (= {R
1
. . .R
L
}) and
H (= {H
1
. . .H
L
}) denote reference and hypoth-
esis rating sequences for a dialogue, respectively.
L is the length of R and H (Note that they have
the same length).
? Match Rate (MR)
MR(R, H) = 1L
L
?
i=1
match(R
i
, H
i
), (1)
where ?match? returns 1 or 0 depending on
whether a rating in R matches that in H .
? Mean Absolute Error (MAE)
MAE(R, H) = 1L
L
?
i=1
|R
i
? H
i
|. (2)
Evaluating Distributions: In generative mod-
els, it is important that the output distribution
matches that of the reference. Therefore, we ad-
ditionally use Kullback-Leibler divergence, match
rate per rating, and mean absolute error per rat-
ing. The Kullback-Leibler divergence evaluates
the shape of output distributions. The match rate
per rating and mean absolute error per rating eval-
uate how accurately each individual rating can
be predicted; namely, the accuracy for predict-
ing dialogue-acts with one rating is equally val-
ued with those for other ratings irrespective of the
distribution of ratings in the reference. It is im-
portant to use these metrics in the practical as well
as information theoretic sense because it is no use
predicting only easy-to-guess ratings; we should
be able to correctly predict rare but still important
cases. For example, rating 1 in human-human di-
alogue is quite rare; however, predicting it is very
important for detecting problematic situations in a
dialogue.
? Kullback-Leibler Divergence (KL)
KL(R,H) =
K
?
r=1
P(H, r) ? log(P(H, r)
P(R, r)), (3)
where K is the maximum user satisfaction rating
(i.e. 7 in this experiment),R andH denote the se-
quentially concatenated reference/hypothesis rat-
ing sequences of the entire dialogues, and P(?, r)
denotes the occurrence probability that a rating r
is found in an arbitrary rating sequence.
? Match Rate per rating (MR/r)
MR/r(R,H) = 1K
K
?
r=1
?
i?{i|R
i
=r}
match(R
i
,H
i
)
?
i?{i|R
i
=r}
1
,
(4)
23
Criterion random ergodic0 ergodic1 ergodic2 concat1 concat2 supervised
Smoothness
MR 0.142
e0e1
0.111 0.111 0.157
e0e1
0.153 0.199
e0e1r
0.275
c1e0e1e2r
MAE 1.988
e0e1
2.212 2.212 1.980 1.936
e0e1
1.870
e0e1
1.420
c1c2e0e1e2r
KL 0.287 0.699 0.699 0.562 0.280 0.369 0.162
MR/r 0.143 0.137 0.137 0.176 0.136 0.177 0.217
MAE/r 2.286 2.414 2.414 2.152 2.301 2.206 1.782
Closeness
MR 0.143 0.129 0.129 0.171
e0e1
0.174 0.189
e0e1
0.279
c1c2e0e1e2r
MAE 2.028 2.066 2.066 1.964 1.798
e0e1r
1.886 1.431
c1c2e0e1e2r
KL 0.195 0.449 0.449 0.261 0.138 0.263 0.092
MR/r 0.143 0.156 0.156 0.170 0.155 0.164 0.231
MAE/r 2.283 2.236 2.236 2.221 2.079 2.067 1.702
Willingness
MR 0.143
e0e1
0.112 0.112 0.180
e0e1
0.152 0.183
e0e1
0.283
c1c2e0e1e2r
MAE 2.005 2.133 2.133 1.962 1.801
e0e1r
1.882 1.403
c1c2e0e1e2r
KL 0.225 0.568 0.568 0.507 0.238 0.255 0.125
MR/r 0.143 0.152 0.152 0.192 0.181 0.167 0.224
MAE/r 2.286 2.258 2.258 2.107 1.958 2.164 1.705
Table 1: The match rate (MR), mean absolute error (MAE), Kullback-Leibler divergence (KL), match
rate per rating (MR/r) and mean absolute error per rating (MAE/r) for our proposed HMMs, the random
baseline, and the upper bound (supervised) for the AD domain. ?e0?e2?, ?c1?c2?, and ?r? indicate the sta-
tistical significance (p<0.01) over ergodic0?2, concat1?2, and random, respectively. Bold font indicates
the best value within each row (except for ?supervised?).
whereR
i
andH
i
denote ratings at i-th positions.
? Mean Absolute Error per rating (MAE/r)
MAE/r(R,H) = 1K
K
?
r=1
?
i?{i|R
i
=r}
|R
i
?H
i
|
?
i?{i|R
i
=r}
1
.
(5)
4.4 Evaluation Results
Tables 1 and 2 show the evaluation results for the
AD and AL domains, respectively. The MR and
MAE values are averaged over all dialogues. To
compare the means of the MR and MAE, we per-
formed a non-parametric multiple comparison test
[Steel-Dwass test (Dwass, 1960)]. We did not per-
form a statistical test for other criteria because it
was difficult to perform sample-wise comparison
for distributions. Naturally, ?supervised? is the
best performing model for all criteria in both do-
mains. Therefore, we focus on how much our pro-
posed models differ from the baseline (random)
and the upper bound (supervised).
In the AD domain, we find that ergodic0 and er-
godic1 performed rather poorly and concat1 and
concat2 performed fairly well, significantly out-
performing the random baseline. However, it is
also clear that we still need a great deal of im-
provement for our models to reach the level of
?supervised?. A promising sign is that concat2
is not significantly different from ?supervised? in
smoothness. Here, ergodic0 and ergodic1 re-
turned the exact same results. This means that the
state transition paths did not go through the com-
mon states at all in ergodic1, suggesting that the
common states in ergodic1 have very broad out-
put distributions and the optimal path could not
go through the common states, instead preferring
other states having sharper distributions. How-
ever, this phenomenon was rightly avoided by in-
troducing more common states as seen in the re-
sults for ergodic2; nonetheless, as the results for
concat1 and concat2 indicate, the transition prob-
abilities have to be trained appropriately to obtain
better results.
In the AL domain, although the tendency of
the evaluation results is the same as that for the
AD domain, concat2 is clearly the best perform-
ing model. It outperformed other models in al-
most all cases except for ?Good Listener? for
which concat1 performed better. In fact, the MR/r
and MAE/r of concat1 are quite close to those of
?supervised?, suggesting the potential of our ap-
proach.
Overall, although we still need further improve-
ment in order for our models to be closer to the
upper bound, we showed that we can, to some ex-
tent, predict user satisfaction transitions in a dia-
logue only from overall ratings of dialogues using
our proposed HMMs. We also showed that model
topologies and learning methods can make signif-
icant differences. Especially, we found the intro-
duction of common states to be crucial in making
appropriate models for prediction. Since our mod-
els, especially concat2, significantly outperformed
the baseline, we believe that our approach can be
one of the viable options for automatically predict-
ing user satisfaction transitions when there exist
only overall rating data.
5 Summary and Future Work
We presented a novel approach for modeling user
satisfaction transitions only from dialogues with
overall ratings. The experimental results show that
it is possible to predict user satisfaction transi-
24
Criterion random ergodic0 ergodic1 ergodic2 concat1 concat2 supervised
Smoothness
MR 0.143
e0e1e2
0.069 0.069 0.131
e0e1
0.173
e0e1
0.243
c1e0e1e2r
0.439
c1c2e0e1e2r
MAE 1.868
e0e1e2
2.519 2.519 2.433 1.687
e0e1e2r
1.594
e0e1e2r
0.802
c1c2e0e1e2r
KL 0.989 2.253 2.253 2.319 0.851 0.753 0.087
MR/r 0.141 0.118 0.118 0.156 0.161 0.167 0.231
MAE/r 2.289 2.500 2.500 2.492 2.093 2.077 1.868
Closeness
MR 0.143
e0e1
0.050 0.050 0.175
e0e1
0.158
e0e1
0.263
c1e0e1e2r
0.425
c1c2e0e1e2r
MAE 1.849
e0e1e2
2.357 2.357 2.316 1.778
e0e1e2
1.562
e0e1e2r
0.890
c1c2e0e1e2r
KL 1.022 2.137 2.137 2.220 1.155 0.909 0.109
MR/r 0.143 0.090 0.090 0.122 0.117 0.159 0.237
MAE/r 2.281 2.577 2.577 2.811 2.260 2.039 1.972
Good Listener
MR 0.143
e0e1
0.075 0.075 0.145
e0e1
0.199
e0e1
0.206
e0e1e2
0.422
c1c2e0e1e2r
MAE 1.890
e0e1e2
2.237 2.237 2.150 1.634
e0e1e2r
1.634
e0e1e2r
0.852
c1c2e0e1e2r
KL 0.945 1.738 1.738 1.782 0.924 0.824 0.087
MR/r 0.143 0.121 0.121 0.184 0.224 0.200 0.227
MAE/r 2.284 2.358 2.358 2.236 1.911 2.083 1.769
Table 2: Evaluation results for the AL domain. See Table 1 for the notations in the table.
tions to some extent by our approach and that in-
troducing common states and concatenated train-
ing can significantly improve prediction accuracy.
For improvement, we plan to explore new dialogic
features for emissions, different topologies, and
other optimization functions, such as discrimina-
tive ones. We also need to validate our approach
using dialogue-act recognition results instead of
hand-labeled dialogue-acts. We also want to ap-
ply our approach to sequence mining in dialogues
where we have categories instead of ratings for di-
alogues. It is also necessary to test whether our
HMMs can be generalized over different raters,
since user satisfaction ratings may differ greatly
among individuals. Although there remain such
issues, we believe we have presented a new di-
rection in automatic evaluation of dialogues and
the experimental results show that our approach is
promising.
References
Meyer Dwass. 1960. Some k-sample rank-order tests. In
Ingram Olkin et al, editor, Contributions to Probability
and Statistics, pages 198?202. Stanford University Press.
Klaus-Peter Engelbrecht, Florian Go?dde, Felix Hartard,
Hamed Ketabdar, and Sebastian Mo?ller. 2009. Model-
ing user satisfactionwith hidden Markov models. In Proc.
SIGDIAL, pages 170?177.
Keelan Evanini, Phillip Hunter, Jackson Liscombe, David
Suendermann, Krishna Dayanidhi, and Roberto Pierac-
cini. 2008. Caller experience: A method for evaluating
dialog systems and its automatic prediction. In Proc. SLT,
pages 129?132.
Allen L. Gorin, Giuseppe Riccardi, and Jerry H. Wright.
1997. How may I help you? Speech Communication,
23(1-2):113?127.
Sunao Hara, Norihide Kitaoka, and Kazuya Takeda. 2010.
Estimation method of user satisfaction using N-gram-
based dialog history model for spoken dialog system. In
Proc. LREC, pages 78?83.
Ota Herm, Alexander Schmitt, and Jackson Liscombe. 2008.
When calls go wrong: How to detect problematic calls
based on log-files and emotions? In Proc. INTER-
SPEECH, pages 463?466.
Ryuichiro Higashinaka, Kohji Dohsaka, and Hideki Isozaki.
2008. Effects of self-disclosure and empathy in human-
computer dialogue. In Proc. SLT, pages 109?112.
Julia Hirschberg, Diane Litman, and Marc Swerts. 2004.
Prosodic and other cues to speech recognition failures.
Speech Communication, 43:155?175.
Chiori Hori, Kiyonori Ohtake, Teruhisa Misu, Hideki Kash-
ioka, and Satoshi Nakamura. 2008. Dialog management
using weighted finite-state transducers. In Proc. INTER-
SPEECH, pages 211?214.
Naoki Isomura, Fujio Toriumi, and Kenichiro Ishii. 2009.
Evaluation method of non-task-oriented dialogue system
usingHMM. IEICE Transactions on Information and Sys-
tems, J92-D(4):542?551.
Woosung Kim. 2007. Online call quality monitoring for
automating agent-based call centers. In Proc. INTER-
SPEECH, pages 130?133.
Kai-Fu Lee. 1989. Automatic speech recognition: the de-
velopment of the SPHINX system. Kluwer Academic Pub-
lishers.
ToyomiMeguro, Ryuichiro Higashinaka,Kohji Dohsaka,Ya-
suhiro Minami, and Hideki Isozaki. 2009. Analysis of
listening-oriented dialogue for building listening agents.
In Proc. SIGDIAL, pages 124?127.
Yasuhiro Minami, Akira Mori, Toyomi Meguro, Ryuichiro
Higashinaka, Kohji Dohsaka, and Eisaku Maeda. 2009.
Dialogue control algorithm for ambient intelligence based
on partially observable Markov decision processes. In
Proc. IWSDS, pages 254?263.
Sebastian Mo?ller, Klaus-Peter Engelbrecht, and Robert
Schleicher. 2008. Predicting the quality and usability of
spoken dialogue services. Speech Communication, 50(8-
9):730?744.
Lawrence R. Rabiner. 1990. A tutorial on hidden Markov
models and selected applications in speech recognition.
Readings in speech recognition, 53(3):267?296.
Katsuhiko Shirai. 1996. Modeling of spoken dialogue with
and without visual information. In Proc. ICSLP, vol-
ume 1, pages 188?191.
Marilyn A. Walker, Diane Litman, Candace A. Kamm, and
Alicia Abella. 1997. PARADISE: A framework for evalu-
ating spoken dialogue agents. In Proc. EACL, pages 271?
280.
Marilyn A. Walker, Irene Langkilde-Geary, Helen Wright
Hastie, Jerry Wright, and Allen Gorin. 2002. Automat-
ically training a problematic dialogue predictor for a spo-
ken dialogue system. Journal of Artificial Intelligence Re-
search, 16(1):293?319.
Jason D. Williams and Steve Young. 2007. Partially ob-
servableMarkov decision processes for spokendialog sys-
tems. Computer Speech & Language, 21(2):393?422.
25
Appendix A. HMM obtained by concat2 for Willingness rating in the AD domain.
This HMM is the model obtained for one of the folds in the experiment. Square and oval states emit
a system?s dialogue-act and a user?s dialogue-act, respectively. Emissions (dialogue-acts) are shown in
each state as a table with their probabilities. Only the emissions and transitions over the probability of
0.1 are displayed for the sake of brevity. Here, ?pi? denotes initial probability.
ra
ti
ng
:0
ra
ti
ng
:1
ra
ti
ng
:2
ra
ti
ng
:3
ra
ti
ng
:4
ra
ti
ng
:5
ra
ti
ng
:6
ra
ti
ng
:7
SY
ST
EM
 (p
i: 
0.
00
)
A
CK
0.
35
D
IS
C-
A
G
RE
E-
P
0.
14
D
IS
C-
D
IS
A
G
RE
E-
P
0.
14
R
EP
EA
T
0.
10
SY
ST
EM
 (p
i: 
1.
00
)
D
IS
C
-R
-O
TH
ER
0.
17
Q-
DI
SC
-R
0.
23
Q-
DI
SC
-R
-O
TH
ER
0.
24
0.
35
U
SE
R
 (p
i: 
0.
00
)
A
CK
0.
14
D
IS
C
-R
-O
TH
ER
0.
300.
13
0.
37
U
SE
R
 (p
i: 
0.
00
)
D
IS
C
-P
0.
40
D
IS
C
-R
0.
26
0.
31
0.
61
0.
12
0.
49
0.
15
SY
ST
EM
 (p
i: 
0.
00
)
A
CK
0.
81
EM
P
0.
11
0.
27
0.
26
U
SE
R
 (p
i: 
0.
00
)
A
CK
0.
30
D
IS
C
-O
TH
ER
0.
14
O
TH
ER
0.
27
Q-
DI
SC
-P
0.
10
0.
33
0.
12
0.
26
0.
56
SY
ST
EM
 (p
i: 
0.
00
)
A
CK
0.
77
Q-
DI
SC
-P
0.
14
0.
21
0.
22
0.
22
0.
11
U
SE
R
 (p
i: 
0.
00
)
O
TH
ER
0.
29
Q-
DI
SC
-O
TH
ER
0.
20
Q-
DI
SC
-R
0.
23
0.
23
0.
23
0.
16
0.
59
SY
ST
EM
 (p
i: 
0.
00
)
D
IS
C-
A
G
RE
E-
P
0.
33
D
IS
C
-P
0.
13
G
O
O
D
BY
E
0.
11
Q-
DI
SC
-P
0.
22
Q-
DI
SC
-P
-O
PE
N
0.
18
0.
19
0.
22
U
SE
R
 (p
i: 
0.
00
)
D
IS
C
-P
0.
46
O
TH
ER
0.
26
Q-
DI
SC
-R
0.
18
0.
46
0.
62
0.
12
0.
21
SY
ST
EM
 (p
i: 
0.
00
)
D
IS
C-
D
IS
A
G
RE
E-
P
0.
46
D
IS
C
-P
0.
20
D
IS
C
-P
-R
0.
12
G
O
O
D
BY
E
0.
15
0.
54
0.
19
U
SE
R
 (p
i: 
0.
00
)
D
IS
C
-P
-R
0.
10
G
RE
ET
IN
G
0.
14
Q-
DI
SC
-O
TH
ER
0.
21
Q-
DI
SC
-R
0.
19
R
ES
0.
10
0.
22
0.
61
0.
200
.1
5
SY
ST
EM
 (p
i: 
0.
00
)
D
IS
C-
A
G
RE
E-
P
0.
28
D
IS
C
-R
0.
10
EM
P
0.
14
R
EP
EA
T
0.
17
0.
51
0.
15
0.
14
SY
ST
EM
 (p
i: 
0.
00
)
D
IS
C-
D
IS
A
G
RE
E-
P
0.
35
D
IS
C
-P
0.
13
D
IS
C
-R
0.
15
Q-
DI
SC
-P
-O
PE
N
0.
15
0.
25 0
.1
1
0.
32
0.
16
U
SE
R
 (p
i: 
0.
00
)
D
IS
C
-D
IS
A
G
R
EE
-O
TH
ER
0.
16
D
IS
C
-O
TH
ER
0.
27
D
IS
C
-P
-R
0.
14
EM
P
0.
18
R
EP
EA
T
0.
16
0.
17
0.
52
0.
17
0.
19
SY
ST
EM
 (p
i: 
0.
00
)
D
IS
C
-P
0.
14
D
IS
C
-R
0.
15
D
IS
C
-R
-O
TH
ER
0.
45
G
O
O
D
BY
E
0.
11
Q-
DI
SC
-P
0.
16
0.
16
0.
16
0.
18
U
SE
R
 (p
i: 
0.
00
)
A
CK
0.
31
D
IS
C
-O
TH
ER
0.
23
EM
P
0.
10
R
EP
EA
T
0.
10
0.
46
0.
51
0.
20
U
SE
R
 (p
i: 
0.
00
)
D
IS
C
-R
-O
TH
ER
0.
41
G
O
O
D
BY
E
0.
12
Q-
DI
SC
-O
TH
ER
0.
22
R
EP
EA
T
0.
12
0.
37
0.
16
0.
45
26
Appendix B. HMM obtained by concat1 for Good Listener rating in the AL domain.
This HMM is the model obtained for one of the folds in the experiment. Square and oval states emit a lis-
tener?s dialogue-act and a speaker?s dialogue-act, respectively. We find DICS-EVAL-NEG (self-disclosure
of one?s evaluation with a negative polarity) in the rating score 1 and DICS-EVAL-POS in the rating score
7, indicating that it may be better to make speakers talk about positive evaluations to be a good listener.
ra
ti
ng
:0
ra
ti
ng
:1
ra
ti
ng
:2
ra
ti
ng
:3
ra
ti
ng
:4
ra
ti
ng
:5
ra
ti
ng
:6
ra
ti
ng
:7
LI
ST
EN
ER
 (p
i: 
0.1
6)
G
RE
ET
IN
G
0.
13
D
IS
C-
FA
CT
0.
10
D
IS
C-
EV
A
L-
PO
S
0.
11
0.
22
SP
EA
K
ER
 (p
i: 
0.
41
)
G
RE
ET
IN
G
0.
13
D
IS
C-
FA
CT
0.
15
SY
N
PA
TH
Y
0.
15
0.
35
0.
32
0.
26
L
IS
TE
N
ER
 (p
i: 
0.
01
)
G
RE
ET
IN
G
0.
28
D
IS
C-
FA
CT
0.
19
IN
FO
0.
15
0.
12
0.
24
0.
32
SP
EA
K
ER
 (p
i: 
0.
14
)
G
RE
ET
IN
G
0.
23
Q-
FA
CT
0.
13
D
IS
C-
EV
A
L-
N
EG
0.
12
0.
33
0.
24
0.
12
0.
32
0.
33
L
IS
TE
N
ER
 (p
i: 
0.
00
)
D
IS
C-
FA
CT
0.
21
IN
FO
0.
18
0.
16
0.
25
0.
23
SP
EA
K
ER
 (p
i: 
0.
00
)
IN
FO
0.
22
Q-
FA
CT
0.
12
0.
36
0.
27
0.
14
0.
35
0.
24
L
IS
TE
N
ER
 (p
i: 
0.
00
)
D
IS
C-
FA
CT
0.
32
Q-
FA
CT
0.
24
TH
A
N
K
0.
10
Q-
IN
FO
0.
11
0.
15
0.
34
0.
18
SP
EA
K
ER
 (p
i: 
0.
00
)
D
IS
C-
FA
CT
0.
34
D
IS
C-
EV
A
L-
N
EG
0.
12
0.
33
0.
39
0.
13
0.
21
0.
27
L
IS
TE
N
ER
 (p
i: 
0.
04
)
G
RE
ET
IN
G
0.
23
SY
N
PA
TH
Y
0.
16
Q-
FA
CT
0.
14
0.
18
0.
38
0.
22
SP
EA
K
ER
 (p
i: 
0.
10
)
G
RE
ET
IN
G
0.
31
Q-
FA
CT
0.
10
D
IS
C-
EV
A
L-
N
EG
0.
15
0.
23
0.
24
0.
16
0.
41
0.
19
L
IS
TE
N
ER
 (p
i: 
0.
00
)
SY
N
PA
TH
Y
0.
17
D
IS
C-
EV
A
L-
PO
S
0.
20
0.
16
0.
23
0.
23
SP
EA
K
ER
 (p
i: 
0.
00
)
D
IS
C-
FA
CT
0.
27
SY
N
PA
TH
Y
0.
20
D
IS
C-
EV
A
L-
PO
S
0.
17
0.
39
0.
22
0.
15
0.
31
0.
32
L
IS
TE
N
ER
 (p
i: 
0.
00
)
SY
N
PA
TH
Y
0.
19
D
IS
C-
EV
A
L-
PO
S
0.
24
CO
N
FI
RM
0.
14
0.
21
0.
24
0.
22
SP
EA
K
ER
 (p
i: 
0.
00
)
SY
N
PA
TH
Y
0.
26
D
IS
C-
EV
A
L-
PO
S
0.
23
IN
FO
0.
25
D
IS
C-
EX
P
0.
11
0.
33
0.
21
0.
21
0.
29
0.
29
L
IS
TE
N
ER
 (p
i: 
0.
04
)
G
RE
ET
IN
G
0.
24
D
IS
C-
EV
A
L-
PO
S
0.
22
IN
FO
0.
11
CO
N
FI
RM
0.
13
0.
17
0.
24
0.
22
SP
EA
K
ER
 (p
i: 
0.
10
)
G
RE
ET
IN
G
0.
18
D
IS
C-
FA
CT
0.
26
D
IS
C-
EV
A
L-
PO
S
0.
16
IN
FO
0.
18
0.
38
0.
26
0.
12
0.
31
0.
30
27
Proceedings of the SIGDIAL 2013 Conference, pages 334?338,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Open-domain Utterance Generation for Conversational Dialogue Systems
using Web-scale Dependency Structures
Hiroaki Sugiyama?, Toyomi Meguro?, Ryuichiro Higashinaka??, Yasuhiro Minami?
?NTT Communication Science Laboratories
2-4, Hikari-dai, Seika-cho, Souraku-gun, Kyoto, Japan
??NTT Media Intelligence Laboratories
1-1, Hikari-no-Oka, Yokosuka-shi, Kanagawa, Japan
{sugiyama.hiroaki,meguro.toyomi,higashinaka.ryuichiro,minami.yasuhiro}@lab.ntt.co.jp
Abstract
Even though open-domain conversational
dialogue systems are required in many
fields, their development is complicated
because of the flexibility and variety of
user utterances. To address this flexibil-
ity, previous research on conversational di-
alogue systems has selected system utter-
ances from web articles based on surface
cohesion and shallow semantic coherence;
however, the generated utterances some-
times contain irrelevant sentences with re-
spect to the input user utterance. We pro-
pose a template-based approach that fills
templates with the most salient words in
a user utterance and with related words
that are extracted using web-scale depen-
dency structures gathered from Twitter.
Our open-domain conversational dialogue
system outperforms retrieval-based con-
ventional systems in chat experiments.
1 Introduction
The need for open-domain conversational dia-
logue systems continues to grow. Such systems
are beginning to be actively investigated from their
social and entertainment aspects (Shibata et al,
2009; Ritter et al, 2011; Wong et al, 2012);
conversational dialogues also have potential for
therapy purposes and for evoking a user?s uncon-
scious requests in task-oriented dialogues (Bick-
more and Cassell, 2001). However, developing
open-domain conversational dialogue systems is
difficult, since the huge variety of user utterances
makes it harder to build knowledge resources for
generating appropriate system responses. To ad-
dress this issue, previous research has selected sys-
tem utterances from web articles or microblogs on
the basis of surface cohesion and shallow seman-
tic coherence (Shibata et al, 2009; Jafarpour and
Burges, 2010; Wong et al, 2012); however, the se-
lected utterances sometimes contain sentences ir-
relevant to the user utterance since they originally
appeared in a different context.
To satisfy both web-scale topic coverage and
suppression of irrelevant sentences, we propose a
template-based approach that fills templates with
words related to the topic of the user utterance and
with words related to the topic-words. This ap-
proach enables us to generate a wide range of sys-
tem responses when we properly extract related
words. To obtain words related to topic-words,
we analyzed the dependency structures of a huge
number of sentences posted to such microblogs as
Twitter, where a large number and variety of sen-
tences are posted daily. This way, we can generate
a variety of appropriate system responses despite
wide variation in user utterances.
We develop a conversational dialogue system
that generates system utterances with our pro-
posed utterance generation approach and exam-
ine its effectiveness by chat experiments with real
users.
2 Related Work
To generate system utterances for conversational
dialogue systems, Ritter et al (2011) proposed a
statistical machine translation-based approach that
considers source-reply tweet pairs as a bilingual
corpus. They compared the following three ap-
proaches: IR-status, which retrieves reply tweets
whose associated source tweets most resemble
the user utterance (Jafarpour and Burges, 2010);
IR-response, which retrieves reply tweets that
are the most similar to the user utterance; and
their proposed SMT-based approach, named MT-
chat. They reported that MT-chat outperformed
the other approaches and that IR-response was su-
perior to IR-status. However, these approaches
used only the words, and not the structures, of user
utterances to generate system utterances.
Yoshino et al (2011) proposed a QA system
that answers questions about current events by re-
trieving, from news articles, descriptions contain-
ing similar dependency structures as those of the
user?s questions. Although this retrieval-based ap-
proach is effective for answering the user?s fac-
tual questions, it is insufficient to generate sub-
jective utterances for conversational dialogue sys-
tems since such systems are required to introduce
334
new topics or to respond with opinions related to
user utterances.
3 Open-domain Utterance Generation
Open-domain conversational dialogue systems
should be able to respond to any user utterance on
any topic. To achieve this, we adopt a template-
based approach that estimates the topic of the
user utterance, extracts words related to the topic-
words, and fills templates with these words. The
template-based approach resembles previous rule-
based approaches, but these dialogue systems had
difficulty achieving coverage for template fillers.
In contrast, our approach utilizes the dependency
structures of sentences gathered from microblogs
that have a wide range of topics, in order to ex-
tract the related words used in template-filling.
The dependency parser we use is a state-of-the-art
Japanese dependency parser that uses Conditional
Random Fields trained on text and blog posts, and
performs cascaded chunking until all dependen-
cies are found. This parser achieved 84.59% de-
pendency accuracy on a corpus of Japanese blog
posts (Imamura et al, 2007).
Microblog posts do not typically contain formu-
laic utterances such as greetings or back-channels.
Therefore, in addition to the template-filling ap-
proach, we adopt dialogue act based utterance
generation for the formulaic uttenances. Figure 1
illustrates the whole architecture of our system.
3.1 Topic-word-driven Template-based
Utterance Generation
Our topic-word-driven template-based approach
consists of the following three steps: topic estima-
tion, related word extraction, and template-filling
utterance generation.
3.1.1 Topic Estimation
We identify three types of potential topic in an in-
put user utterance: proper nouns, common nouns,
and predicates (verbs, adjectives, adjectival verbs,
and verbal nouns).
Proper Nouns We take the last proper noun
that appears in the user?s utterance as a poten-
tial topic. Since general Japanese morphologi-
cal analyzers cannot capture recent proper nouns,
we complement the proper noun dictionary entries
with Wikipedia entries1.
Common Nouns To identify potential topics
from common nouns, we calculate the inverse doc-
ument frequency (IDF) of each common noun (all
nouns except for proper, time-related, and verbal
ones) in the user?s utterance. We use a corpus of
microblog posts and treat each post as a document.
We adopt the word with the highest IDF as a po-
tential topic.
1https://github.com/nabokov/mecab-dic-overdrive
Related word Extracon 
TopicEsmaon 
Template-filling based U"erance Generaon 
User U"erance 
System U"erance 
Topic-word-driven  Template-based U"erance 
Topic-word 
Topic-related words 
DialogueControl 
Dialogue act Esmaon 
U"erance Generaon with Predicted Dialogue act 
User?s Dialogue act 
System?s Dialogue act 
Dialogue act based U"erance (only for greengs and  back-channel feedback) 
Topic-word-driven  Template-based  Approach 
Dialogue actbased Approach (Secondarily) 
Figure 1: System Architecture
Predicates We take the predicate that composes
a dependency in the highest layer of the depen-
dency structure as a potential topic. For example,
we adopt ?ask?, but not ?walk? from the utterance
?I asked the man walking on the street?.
3.1.2 Related word Extraction
To obtain topic-related words, a thesaurus or topic
model such as Latent Dirichlet Allocation are the
most popular approaches (Blei et al, 2003). How-
ever, these approaches return semantically simi-
lar words to input query words, which do not ef-
fectively introduce new information into the sys-
tem utterances. Therefore, we count the depen-
dencies between words in a huge number of sen-
tences gathered from microblogs, and utilize the
most frequently dependent words. This approach
enables us to extract adjectives related to proper
noun topics; for example, the adjectives beautiful,
good, clear, white, and huge are extracted for Mt.
Fuji. Since microblogs contain a huge number of
subjective posts, we expect the extracted words to
be subjective and suitable for conversational dia-
logue systems. In this work, we extract adjectives
for proper and common nouns, and nouns and their
case frames for predicates. Examples of extracted
words are shown in Table 2.
3.1.3 Template-filling Utterance Generation
We generate two types of system utterances using
manually defined templates: subjective sentences
with proper nouns and common nouns; and ques-
tions with predicates and their case frames.
Noun-driven Subjective Sentence Generation
We generate system utterances using the proper
and common nouns and their related adjectives.
Here, we adopt different templates for each word
type; proper nouns have explicit meanings, so ad-
jectives related to them are easily suited for any di-
alogue context. By contrast, since common nouns
are used in various contexts in microblogs, ad-
jectives related to common nouns may not fit the
dialogue context. Thus, we use ?suki? (?like?
in English), or ?nigate? (?don?t like? in English)
in the templates based on the proportion of posi-
tive/negative adjectives in the set of related words
for a common noun topic. Table 3 shows represen-
tative examples for each type. If the system gener-
335
ates subjective utterances as the system?s own im-
pression of the dialogue topic, the user will expect
the system to justify or explain its opinion; how-
ever, our system cannot answer that kind of ques-
tion. Thus, we define the templates using hedges
such as ?I hear that...? to avoid such questions.
The number of templates for proper nouns is eight,
and for common nouns is four for each polarity.
Predicate-driven Question Sentence Genera-
tion We generate question sentences using pred-
icates and their related nouns and case frames. To
elicit user utterances on a particular topic, we gen-
erate How/What/Where/When types of questions
as shown in Table 3. To select a question word,
we use the predicate types and the classes of the
related nouns. If the predicate type is adjective or
adjectival noun, we select ?how? for the question
word. If the predicate type is verbal noun or verb
and location class words appear in the related noun
phrase, we select ?where? for the question word;
the time class induces the question word ?when?.
When no proper noun is found in the topic-word,
we select ?what?. The number of templates for
proper nouns is three for each interrogative type.
3.2 Dialogue act based Utterance Generation
Our approach has difficulty generating appropriate
responses to formulaic utterances such as greet-
ings and back-channels. To address this weakness,
we adopt dialogue act based utterance generation
for these types of utterance. A dialogue act is an
abstract expression of a speaker?s intention (Stol-
cke et al, 2000); we used the 33 dialogue acts de-
fined in Meguro et al (2010).
Our dialogue act based approach estimates the
next dialogue act that the system should output
based on the user?s utterance, and generates a sys-
tem utterance based on the system?s predicted di-
alogue act if the dialogue act is greetings, sympa-
thy, non-sympathy, filler, or confirmation.
3.2.1 User?s Dialogue act Estimation
We collected 1,259 conversational dialogues from
47 human subjects and labeled each sentence of
the collected data using the 33 dialogue acts.
67,801 dialogue acts are contained in the corpus.
We estimated the 33 dialogue acts from user
utterances using a logistic regression model and
adopted 1- and 2-gram words and 3- and 4-gram
characters as model features. We trained our
model using 1,000 dialogues and evaluated it us-
ing 259 dialogues. The estimation accuracy was
about 61%, whereas the human annotation agree-
ment rate was about 59%.
3.2.2 Dialogue control Model and Utterance
Generation with Predicted Dialogue act
We developed a dialogue control model that esti-
mates the system?s next dialogue act based on the
user?s dialogue act. The model features are the
user?s current dialogue act vector, the system?s last
dialogue act vector, and the user?s last dialogue act
vector. Each dialogue act vector consists of a 33-
dimensional binary vector space. We used the dia-
logue corpus described above to train and evaluate
our model, which we trained with 1,000 dialogues
and evaluated using 259 dialogues. The estimation
accuracy was 31%, whereas the dialogue act an-
notation agreement rate between humans is 60%.
We exploited the fact that formulaic utterances can
pre-define corresponding utterances regardless of
the context. Table 4 shows example generated sen-
tences for each dialogue act.
4 Experiment
4.1 Experiment Setting
We recruited ten native Japanese-speaking partic-
ipants in their 20?s and 30?s (two males and eight
females) from outside of the authors? organiza-
tion, who have experience using chat systems (not
bots). Each participant chatted with the following
systems, provided subjective evaluation scores for
each system for each of the eight criteria shown in
Table 1 (2)-(10) using 7-point Likert scales, and at
the end ranked all the systems. We examined the
effectiveness of our proposed approach by com-
parison with the following six systems.
We built the following proposed systems with
about 150 M posts gathered from Twitter (ex-
cluding posts that contain ?@?, ?RT?, ?http? and
brackets, and posts that don?t contain any depen-
dency pairs). At the beginning of a dialogue or
the end of a conversation topic when the topic-
based approach didn?t generate system utterances,
the proposed approaches generated questions such
as ?What is your favorite movie?? to introduce
the next conversation topic. These questions were
gathered from utterances in the self-introduction
phase (about the five initial utterances) of each di-
alogue in our dialogue corpus. We manually se-
lected 109 questions that have no context from
179 questions gathered from our corpus, and chose
a question at random to generate each topic-
inductive question.
Proposed-All This approach used all found top-
ics: proper and common nouns, and predicates.
This approach is expected to be well-balanced
since it generates both content-focused utterances
and general WH-type questions.
Proposed-Nouns This approach used only
proper and common nouns, not predicates.
Proposed-Predicates This approach used only
predicates, not proper nor common nouns.
Retrieval-Self This approach resembles the IR-
response method in Ritter et al (2011). This ap-
proach chose the most similar posts to the user ut-
336
Prop.-All Prop.-Noun Prop.-Pred. Ret.-self Ret.-reply Human
(1) Number of superior prefs. vs. Prop.-All - 4 3 0?? 2? 9??
(2) Naturalness of dialogue flow 4.0 3.1?? 3.5 2.2?? 3.5 6.5??(3) Grammatical correctness 4.0 3.7 4.4 4.1 3.9 6.4??(4) Dialogue usefulness 3.7 2.9?? 3.9 2.7?? 3.5 6.1??(5) Ease of considering next utterance 3.5 3.4 4.4?? 2.4?? 3.3 5.7??(6) Variety of system utterances 4.3 4.0 4.2 2.9?? 4.0 5.5(7) User motivation 4.5 4.0? 4.7 3.7? 4.6 5.6??(8) System motivation that the user feels 4.7 4.1? 4.3 3.5?? 4.5 5.7?(9) Desire to chat again 3.7 2.8?? 3.3 2.0?? 3.1 5.7??
(10) Averaged score of all evaluation items 4.05 3.50?? 4.08 2.93?? 3.8? 5.9??
Table 1: System preferences and evaluation scores on 7-point Likert scale (?: p<.1, ??: p<.05)
terance from source posts using the Lucene2 infor-
mation retrieval library, which is an IDF-weighted
vector-space similarity. We built about 55 M
source-reply post pairs from Twitter.
Retrieval-Reply This approach is the same as
the IR-status method in Ritter et al (2011). It
chooses a reply post whose associated source posts
most resemble the user?s utterance.
Human As an upper-bound of these systems,
the user chats with a human using the same chat
interface used by the other systems.
Each dialogue took place over four minutes and
was conducted through a text chat interface, and
the orders of presentation of systems to partici-
pants was randomized. Since the humans have
to type their utterances and the systems can gen-
erate utterances much faster than typing, we set
the transition of the system utterances to about ten
seconds to avoid different response intervals be-
tween the systems and the humans. Table 5 shows
a dialogue example.
4.2 Results and Discussion
Table 1 shows that Proposed-All is ranked the
highest of all the automatic systems (1), and
achieves the best average evaluation scores (2)-
(10). Statistical analyses were performed using
the Binomial test for (1) and Welch?s t test for (2)
to (10). Proposed-All was ranked higher than the
retrieval-based approaches (10 of 10 participants
ranked Proposed-All higher than Retrieval-Self,
and 8 participants ranked Proposed-All higher
than Retrieval-Reply), but none of our three pro-
posed approaches was ranked significantly higher
than the others.
The evaluation scores also demonstrate the
characteristics of each approach. Proposed-Nouns
shows significantly low scores in dialogue flow
(2), dialogue usefulness (4), and system motiva-
tion (9). Since this approach is overly affected by
the nouns in the user utterances, users didn?t feel
that the system was actually thinking. Proposed-
Predicates shows a high score in ease of thinking
about the next utterance (5) since it generates WH-
type questions for which users can easily produce
answer utterances.
2http://lucene.apache.org
For conventional retrieval-based approaches,
contrary to Ritter et al (2011), Retrieval-Self
shows significantly lower scores in almost all
the evaluation items, and Retrieval-Reply shows
scores close to Proposed-All. These results re-
flect the retrieved corpus size, which is 40 times
larger than that of Ritter et al (2011). When
the retrieval performance improves, Retrieval-Self
returns posts that are too similar to user utter-
ances, while Retrieval-Reply can find appropri-
ate source posts. Retrieval-Reply shows almost
the same scores as Proposed-All for each single
evaluation metric, but Retrieval-Reply is inferior
to Proposed-All in the averaged evaluation items
(10). This is a reason why Retrieval-Reply is also
inferior in (1).
None of the systems approached human per-
formance. The users thought that the systems
were not able to respond to user utterances that
referred to the system itself, like personal ques-
tions; and that the systems didn?t understand user
utterances since the systems sometimes generate
a question that contains different but semantically
similar words to those used by the user, due to the
lack of thesaurus knowledge.
5 Conclusions
We proposed a novel open-domain utterance gen-
eration approach for a conversational dialogue
system that generates system utterances using
templates populated with topics and related words
extracted from a huge number of dependency
structures. Our chat experiments demonstrated
that our template-based approach generated sys-
tem utterances preferred over those produced
with retrieval-based approaches, and that WH-
type questions make it easy for users to produce
their next utterance. Our work also indicated
that template-based utterance generation, which is
considered a legacy approach, has potential when
the template-filling resource is huge. Future work
includes improving the data-driven topic selec-
tion in the proposed approach, the aggregation of
words with web-scale class structures like Tama-
gawa et al (2012), response generation for utter-
ances that describe the systems themselves, and
exploitation of information about the user to gen-
erate system utterances.
337
References
Timothy Bickmore and Justine Cassell. 2001. Re-
lational Agents: A Model and Implementation of
Building User Trust. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, pages 396?403.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Kenji Imamura, Genichiro Kikui, and Norihito Yasuda.
2007. Japanese Dependency Parsing Using Sequen-
tial Labeling for Semi-Spoken Language. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 225?228.
Sina Jafarpour and Christopher J.C. Burges. 2010. Fil-
ter, Rank, and Transfer the Knowledge: Learning
to Chat. Technical Report MSR-TR-2010-93, Mi-
crosoft.
Toyomi Meguro, Ryuichiro Higashinaka, Yasuhiro Mi-
nami, and Kohji Dohsaka. 2010. Controlling
Listening-oriented Dialogue using Partially Observ-
able Markov Decision Processes. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 761?769.
Alan Ritter, Colin Cherry, and William.B. Dolan.
2011. Data-Driven Response Generation in Social
Media. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 583?593.
Masahiro Shibata, Tomomi Nishiguchi, and Yoichi
Tomiura. 2009. Dialog System for Open-Ended
Conversation Using Web Documents. Informatica,
33:277?284.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-
abeth Shriberg, Rebecca Bates, Daniel Jurafsky,
Paul Taylor, Rachel Martin, Carol Van Ess-Dykema,
and Marie Meteer. 2000. Dialogue Act Mod-
eling for Automatic Tagging and Recognition of
Conversational Speech. Computational Linguistics,
26(3):339?373.
Susumu Tamagawa, Takeshi Morita, and Takahira Ya-
maguchi. 2012. Extracting Property Semantics
from Japanese Wikipedia. In Proceedings of the 8th
international conference on Active Media Technol-
ogy, pages 357?368.
Wilson Wong, Lawrence Cavedon, John Thangara-
jah, and Lin Padgham. 2012. Strategies for
Mixed-Initiative Conversation Management using
Question-Answer Pairs. In Proceedings of the 24th
International Conference on Computational Lin-
guistics, pages 2821?2834.
Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2011. Spoken Dialogue System based on In-
formation Extraction using Similarity of Predicate
Argument Structures. In Proceedings of the 12th
Annual Meeting of the Special Interest Group on
Discourse and Dialogue, pages 59?66.
Appendix
Topic-word Extracted topic-related words
Mt.Fuji beautiful, good, clear, white, huge (adj)Bread delicious, good, like, cute (adj)Eat I (?ga?: subject-case), rice (?wo?: target-case)together (?ni?: ?with?,? to?)
Table 2: Extracted topic-related words (translated
by authors)
Word type Template example
Proper nouns I hear that [Noun] is [adj], isn?t it?[Noun] ha [adj] deshitakke? (JP)General nouns Actually, I am weak in [Noun]...[Noun] ha chotto nigate desu... (JP)Adjectives and How does it [Noun]?adjectival nouns douiu huuni [Noun] nndesuka? (JP)Verbs and Where do you [Verb]?verbal nouns doko de [Verb] nndesuka? (JP)
Table 3: Templates for topic types (translated by
authors)
Dialogue act Example
Greetings Hello.
Sympathy Good./ Nice.
Non-sympathy Not so good..
Filler Uh.
Confirmation Really?/Oh, [Noun].
Table 4: Generated utterances for dialogue acts
(translated by authors)
Speaker Utterance
User Hello.System Hello. Do you like movies?User I like movies.System Good. I think movie is fun, isn?t it?User I like reading more than going to the movies.System Reading is also good.User I borrow books from my local library.System Nice. I hear that libraries are quiet.User Yes, they usually are.I used to study, write reports, and read books.
Table 5: Dialogues between user and Proposed-
All (translated by authors)
338
