Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 343?351,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Recognizing Implicit Discourse Relations in the Penn Discourse Treebank
Ziheng Lin, Min-Yen Kan and Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
{linzihen,kanmy,nght}@comp.nus.edu.sg
Abstract
We present an implicit discourse relation
classifier in the Penn Discourse Treebank
(PDTB). Our classifier considers the con-
text of the two arguments, word pair infor-
mation, as well as the arguments? internal
constituent and dependency parses. Our
results on the PDTB yields a significant
14.1% improvement over the baseline. In
our error analysis, we discuss four chal-
lenges in recognizing implicit relations in
the PDTB.
1 Introduction
In the field of discourse modeling, it is widely
agreed that text is not understood in isolation, but
in relation to its context. One focus in the study
of discourse is to identify and label the relations
between textual units (clauses, sentences, or para-
graphs). Such research can enable downstream
natural language processing (NLP) such as sum-
marization, question answering, and textual entail-
ment. For example, recognizing causal relations
can assist in answering why questions. Detect-
ing contrast and restatements is useful for para-
phrasing and summarization systems. While dif-
ferent discourse frameworks have been proposed
from different perspectives (Mann and Thompson,
1988; Hobbs, 1990; Lascarides and Asher, 1993;
Knott and Sanders, 1998; Webber, 2004), most ad-
mit these basic types of discourse relationships be-
tween textual units.
When there is a discourse connective (e.g., be-
cause) between two text spans, it is often easy to
recognize the relation between the spans, as most
connectives are unambiguous (Miltsakaki et al,
2005; Pitler et al, 2008). On the other hand, it is
difficult to recognize the discourse relations when
there are no explicit textual cues. We term these
cases explicit and implicit relations, respectively.
While the recognition of discourse structure has
been studied in the context of explicit relations
(Marcu, 1998) in the past, little published work
has yet attempted to recognize implicit discourse
relations between text spans.
Detecting implicit relations is a critical step
in forming a discourse understanding of text, as
many text spans do not mark their discourse re-
lations with explicit cues. Recently, the Penn Dis-
course Treebank (PDTB) has been released, which
features discourse level annotation on both explicit
and implicit relations. It provides a valuable lin-
guistic resource towards understanding discourse
relations and a common platform for researchers
to develop discourse-centric systems. With the
recent release of the second version of this cor-
pus (Prasad et al, 2008), which provides a cleaner
and more thorough implicit relation annotation,
there is an opportunity to address this area of work.
In this paper, we provide classification of im-
plicit discourse relations on the second version of
the PDTB. The features we used include contex-
tual modeling of relation dependencies, features
extracted from constituent parse trees and depen-
dency parse trees, and word pair features. We
show an accuracy of 40.2%, which is a significant
improvement of 14.1% over the majority baseline.
After reviewing related work, we first give an
overview of the Penn Discourse Treebank. We
then describe our classification methodology, fol-
lowed by experimental results. We give a detailed
discussion on the difficulties of implicit relation
classification in the PDTB, and then conclude the
paper.
2 Related Work
One of the first works that use statistical meth-
ods to detect implicit discourse relations is that
of Marcu and Echihabi (2002). They showed that
word pairs extracted from two text spans provide
clues for detecting the discourse relation between
343
the text spans. They used a set of textual patterns
to automatically construct a large corpus of text
span pairs from the web. These text spans were
assumed to be instances of specific discourse re-
lations. They removed the discourse connectives
from the pairs to form an implicit relation corpus.
From this corpus, they collected word pair statis-
tics, which were used in a Na??ve Bayes framework
to classify discourse relations.
Saito et al (2006) extended this theme, to show
that phrasal patterns extracted from a text span
pair provide useful evidence in the relation clas-
sification. For example, the pattern ?... should
have done ...? usually signals a contrast. The au-
thors combined word pairs with phrasal patterns,
and conducted experiments with these two feature
classes to recognize implicit relations between ad-
jacent sentences in a Japanese corpus.
Both of these previous works have the short-
coming of downgrading explicit relations to im-
plicit ones by removing the explicit discourse con-
nectives. While this is a good approach to auto-
matically create large corpora, natively implicit re-
lations may be signaled in different ways. The fact
that explicit relations are explicitly signaled indi-
cates that such relations need a cue to be unam-
biguous to human readers. Thus, such an artificial
implicit relation corpus may exhibit marked dif-
ferences from a natively implicit one. We validate
this claim later in this work.
Wellner et al (2006) used multiple knowledge
sources to produce syntactic and lexico-semantic
features, which were then used to automatically
identify and classify explicit and implicit dis-
course relations in the Discourse Graphbank (Wolf
and Gibson, 2005). Their experiments show that
discourse connectives and the distance between
the two text spans have the most impact, and
event-based features also contribute to the perfor-
mance. However, their system may not work well
for implicit relations alone, as the two most promi-
nent features only apply to explicit relations: im-
plicit relations do not have discourse connectives
and the two text spans of an implicit relation are
usually adjacent to each other.
The work that is most related to ours is the
forthcoming paper of Pitler et al (2009) on im-
plicit relation classification on the second ver-
sion of the PDTB. They performed classification
of implicit discourse relations using several lin-
guistically informed features, such as word polar-
ity, verb classes, and word pairs, showing perfor-
mance increases over a random classification base-
line.
3 Overview of the Penn Discourse
Treebank
The Penn Discourse Treebank (PDTB) is a dis-
course level annotation (Prasad et al, 2008) over
the one million word Wall Street Journal corpus.
The PDTB adopts the predicate-argument view of
discourse relations, where a discourse connective
(e.g., because) is treated as a predicate that takes
two text spans as its arguments. The argument
that the discourse connective structurally attaches
to is called Arg2, and the other argument is called
Arg1. The PDTB provides annotations for explicit
and implicit discourse relations. By definition, an
explicit relation contains an explicit discourse con-
nective. In the PDTB, 100 explicit connectives are
annotated. Example 1 shows an explicit Contrast
relation signaled by the discourse connective but.
The last line shows the relation type and the file in
the PDTB from which the example is drawn.
(1) Arg1: In any case, the brokerage firms are
clearly moving faster to create new ads than
they did in the fall of 1987.
Arg2: But it remains to be seen whether
their ads will be any more effective.
(Contrast - wsj 2201)
In the PDTB, implicit relations are constrained
by adjacency: only pairs of adjacent sentences
within paragraphs are examined for the existence
of implicit relations. When an implicit relation
was inferred by an annotator, he/she inserted an
implicit connective that best reflects the relation.
Example 2 shows an implicit relation, where the
annotator inferred a Cause relation and inserted an
implicit connective so (i.e., the original text does
not include so). The text in the box (he says)
shows the attribution, i.e., the agent that expresses
the arguments. The PDTB provides annotation for
the attributions and supplements of the arguments.
(2) Arg1: ?A lot of investor confidence comes
from the fact that they can speak to us,?
he says .
Arg2: [so] ?To maintain that dialogue is
absolutely crucial.?
(Cause - wsj 2201)
344
The PDTB provides a three level hierarchy of
relation tags for its annotation. The first level
consists of four major relation classes: Temporal,
Contingency, Comparison, and Expansion. For
each class, a second level of types is defined to pro-
vide finer semantic distinctions. A third level of
subtypes is defined for only some types to specify
the semantic contribution of each argument. Rela-
tion classes and types in the PDTB are reproduced
in the first two columns of Table 1.
We focus on implicit relation classification of
the Level 2 types in the PDTB, as we feel that
Level 1 classes are too general and coarse-grained
for downstream applications, while Level 3 sub-
types are too fine-grained and are only provided
for some types. Table 1 shows the distribution of
the 16 Level 2 relation types of the implicit rela-
tions from the training sections, i.e., Sections 2
? 21. As there are too few training instances for
Condition, Pragmatic Condition, Pragmatic Con-
trast, Pragmatic Concession, and Exception, we
removed these five types from further considera-
tion. We thus use the remaining 11 Level 2 types
in our work. The initial distribution and adjusted
distribution are shown in the last two columns of
the table. We see that the three predominant types
are Cause (25.63%), Conjunction (22.25%), and
Restatement (19.23%).
Level 1 Class Level 2 Type Training % Adjusted %
instances
Temporal Asynchronous 583 4.36 4.36
Synchrony 213 1.59 1.59
Contingency Cause 3426 25.61 25.63
Pragmatic 69 0.52 0.52
Cause
Condition 1 0.01 ?
Pragmatic 1 0.01 ?
Condition
Comparison Contrast 1656 12.38 12.39
Pragmatic 4 0.03 ?
Contrast
Concession 196 1.47 1.47
Pragmatic 1 0.01 ?
Concession
Expansion Conjunction 2974 22.24 22.25
Instantiation 1176 8.79 8.80
Restatement 2570 19.21 19.23
Alternative 158 1.18 1.18
Exception 2 0.01 ?
List 345 2.58 2.58
Total 13375
Adjusted total 13366
Table 1: Distribution of Level 2 relation types of
implicit relations from the training sections (Sec.
2 ? 21). The last two columns show the initial
distribution and the distribution after removing the
five types that have only a few training instances.
4 Methodology
Our implicit relation classifier is built using super-
vised learning on a maximum entropy classifier.
As such, our approach processes the annotated ar-
gument pairs into binary feature vectors suitable
for use in training a classifier. Attributions and
supplements are ignored from the relations, as our
system does not make use of them. We chose the
following four classes of features as they represent
a wide range of information ? contextual, syntac-
tic, and lexical ? that have been shown to be help-
ful in previous works and tasks. We now discuss
the four categories of features used in our frame-
work.
X'XM X'XMX'XM X'XM
X'XM X'XM X'XM
X X
XX
X'XM
Figure 1: Two types of discourse dependency
structures. Top: fully embedded argument, bot-
tom: shared argument.
Contextual Features. Lee et al (2006) showed
that there are a variety of possible dependencies
between pairs of discourse relations: independent,
fully embedded argument, shared argument, prop-
erly contained argument, pure crossing, and par-
tially overlapping argument. They argued that the
last three cases ? properly contained argument,
pure crossing, and partially overlapping argument
? can be factored out by appealing to discourse no-
tions such as anaphora and attribution. Moreover,
we also observed from the PDTB corpus that fully
embedded argument and shared argument are the
most common patterns, which are shown in Fig-
ure 1. The top portion of Figure 1 shows a case
where relation r
1
is fully embedded in Arg1 of re-
lation r
2
, and the bottom portion shows r
1
and r
2
sharing an argument. We model these two patterns
as contextual features. We believe that these dis-
course dependency patterns between a pair of ad-
jacent relations are useful in identifying the rela-
tions. For example, if we have three items in a list,
according to the PDTB binary predicate-argument
definitions, there will be a List relation between
345
the first item and the second item, and another List
relation between the previous List relation and the
third item, where the previous List relation is fully
embedded in Arg1 of the current List relation. As
we are using the gold standard argument segmen-
tation from the PDTB, we can extract and leverage
these dependency patterns. For each relation curr,
we use the previous relation prev and the next re-
lation next as evidence to fire six binary features,
as defined in Table 2.
Note that while curr is an implicit relation to
be classified, both prev and next can be implicit or
explicit relations. Pitler et al (2008) showed that
the type of a relation sometimes correlates to the
type of its adjacent relation. When the adjacent
relation is explicit, its type may be suggested by
its discourse connective. Thus we include another
two groups of contextual features representing the
connectives of prev and next when they are explicit
relations.
Fully embedded argument:
prev embedded in curr.Arg1
next embedded in curr.Arg2
curr embedded in prev.Arg2
curr embedded in next.Arg1
Shared argument:
prev.Arg2 = curr.Arg1
curr.Arg2 = next.Arg1
Table 2: Six contextual features derived from two
discourse dependency patterns. curr is the relation
we want to classify.
Constituent Parse Features. Research work
from other NLP areas, such as semantic role la-
beling, has shown that features derived from syn-
tactic trees are useful in semantic understanding.
Such features include syntactic paths (Jiang and
Ng, 2006) and tree fragments (Moschitti, 2004).
From our observation of the PDTB relations, syn-
tactic structure within one argument may constrain
the relation type and the syntactic structure of
the other argument. For example, the constituent
parse structure in Figure 2(a) usually signals an
Asynchronous relation when it appears in Arg2,
as shown in Example 3, while the structure in Fig-
ure 2(b) usually acts as a clue for a Cause relation
when it appears in Arg1, as shown in Example 4.
In both examples, the lexicalized parts of the parse
structure are bolded.
(3) Arg1: But the RTC also requires ?working?
capital to maintain the bad assets of thrifts
that are sold
Arg2: [subsequently] That debt would be
paid off as the assets are sold
(Asynchronous - wsj 2200)
(4) Arg1: It would have been too late to think
about on Friday.
Arg2: [so] We had to think about it ahead of
time.
(Cause - wsj 2201)
(a)
SBAR
IN
as
S
. . .
(b)
VP
MD VP
VB
have
VP
VBN
been
ADJP PP
Figure 2: (a) constituent parse in Arg2 of Example
3, (b) constituent parse in Arg1 of Example 4.
S-TPC-1
NP-SBJ
PRP
We
VP
VBD
had
NP
NP
DT
no
NN
operating
NNS
problems
ADVP
IN
at
DT
all
Figure 3: A gold standard subtree for Arg1 of an
implicit discourse relation from wsj 2224.
For Arg1 and Arg2 of each relation, we extract
the corresponding gold standard syntactic parse
trees from the corpus. As an argument can be a
single sentence, a clause, or multiple sentences,
this results in a whole parse tree, parts of a parse
tree, or multiple parse trees. From these parses,
we extract all possible production rules. Although
the structures shown in Figure 2 are tree frag-
ments, tree fragments are not extracted as produc-
tion rules act as generalization of tree fragments.
As an example, Figure 3 shows the parse tree for
Arg1 of an implicit discourse relation from the text
wsj 2224. As Arg1 is a clause, the extracted tree
346
is a subtree. We then collect all production rules
from this subtree, with function tags (e.g., SBJ)
removed from internal nodes. POS tag to word
production rules are collected as well. The result-
ing production rules include ones such as: S ?
NP VP, NP? PRP, PRP? ?We?, etc. Each pro-
duction rule is represented as three binary features
to check whether this rule appears in Arg1, Arg2,
and both arguments.
Dependency Parse Features. We also experi-
mented with features extracted from dependency
trees of the arguments. We used the Stanford de-
pendency parser (de Marneffe et al, 2006), which
takes in a constituent parse tree and produces a de-
pendency tree. Again, for an argument, we may
collect a whole dependency tree, parts of a tree,
or multiple trees, depending on the span of the ar-
gument. The reason for using dependency trees
is that they encode additional information at the
word level that is not explicitly present in the con-
stituent trees. From each tree, we collect all words
with the dependency types from their dependents.
Figure 4 shows the dependency subtree for the
same example in Figure 3, from which we col-
lect three dependency rules: ?had?? nsubj dobj,
?problems?? det nn advmod, ?at?? dep.
Note that unlike the constituent parse features
which are guaranteed to be accurate (as they are
extracted from the gold parses of the corpus), the
dependency parses occasionally contain errors. As
with the constituent parse features, each depen-
dency rule is represented as three binary features
to check whether it appears in Arg1, Arg2, and
both arguments.
We
had
nsubjproblemsdobj
no operatingdet nn atadvmod
alldep
Figure 4: A dependency subtree for Arg1 of an
implicit discourse relation from wsj 2224.
Lexical Features. Marcu and Echihabi (2002)
demonstrated that word pairs extracted from the
respective text spans are a good signal of the
discourse relation between arguments. Thus we
also consider word pairs as a feature class. We
stemmed and collected all word pairs from Arg1
and Arg2, i.e., all (w
i
, w
j
) where w
i
is a word
from Arg1 and w
j
a word from Arg2. Unlike their
study, we limit the collection of word pair statis-
tics to occurrences only in the PDTB corpus.
4.1 Feature Selection
For the collection of production rules, dependency
rules, and word pairs, we used a frequency cutoff
of 5 to remove infrequent features. From the im-
plicit relation dataset of the training sections (i.e.,
Sec. 2 ? 21), we extracted 11,113 production rules,
5,031 dependency rules, and 105,783 word pairs
in total. We applied mutual information (MI) to
these three classes of features separately, resulting
in three ranked lists. A feature f has 11 MI values
with all 11 types (for example,MI(f, Cause) and
MI(f,Restatement)), and we used the MI with
the highest value for a feature to select features. In
our experiments, the top features from the lists are
used in the training and test phases.
5 Experiments
We experimented with a maximum entropy clas-
sifier from the OpenNLP MaxEnt package using
various combinations of features to assess their ef-
ficacy. We used PDTB Sections 2 ? 21 as our train-
ing set and Section 23 as the test set, and only used
the implicit discourse relations.
In the PDTB, about 2.2% of the implicit rela-
tions are annotated with two types, as shown in
Example 7 in Section 6. During training, a relation
that is annotated with two types is considered as
two training instances, each with one of the types.
During testing, such a relation is considered one
test instance, and if the classifier assigns either of
the two types, we consider it as correct. Thus, the
test accuracy is calculated as the number of cor-
rectly classified test instances divided by the total
number of test instances.
In our work, we use the majority class as
the baseline, where all instances are classified as
Cause. This yields an accuracy of 26.1% on the
test set. A random baseline yields an even lower
accuracy of 9.1% on the test set.
5.1 Results and Analysis
To check the efficacy of the different feature
classes, we trained individual classifiers on all fea-
tures within a single feature class (Rows 1 to 4
in Table 3) as well as a single classifier trained
347
with all features from all feature classes (Row 5).
Among the four individual feature classes, produc-
tion rules and word pairs yield significantly better
performance over the baseline with p < 0.01 and
p < 0.05 respectively, while context features per-
form slightly better than the baseline.
# Production # Dependency # Word Context Acc.
rules rules pairs
R1 11,113 ? ? No 36.7%
R2 ? 5,031 ? No 26.0%
R3 ? ? 105,783 No 30.3%
R4 ? ? ? Yes 28.5%
R5 11,113 5,031 105,783 Yes 35.0%
Table 3: Classification accuracy with all features
from each feature class. Rows 1 to 4: individual
feature class; Row 5: all feature classes.
Interestingly, we noted that the performance
with all dependency rules is slightly lower than
the baseline (Row 2), and applying all feature
classes does not yield the highest accuracy (Row
5), which we suspected were due to noise. To con-
firm this, we employed MI to select the top 100
production rules and dependency rules, and the top
500 word pairs (as word pairs are more sparse).
We then repeated the same set of experiments, as
shown in Table 4 (Row 4 of this table is repeated
from Table 3 for consistency). With only the top
features, production rules, dependency rules, and
word pairs all gave significant improvement over
the baseline with p < 0.01. When we used all
feature classes, as in the last row, we obtained the
highest accuracy of 40.2%.
# Production # Dependency # Word Context Acc.
rules rules pairs
R1 100 ? ? No 38.4%
R2 ? 100 ? No 32.4%
R3 ? ? 500 No 32.9%
R4 ? ? ? Yes 28.5%
R5 100 100 500 Yes 40.2%
Table 4: Classification accuracy with top
rules/word pairs for each feature class. Rows 1
to 4: individual feature class; Row 5: all feature
classes.
Table 4 also validates the pattern of predictive-
ness of the feature classes: production rules con-
tribute the most to the performance individually,
followed by word pairs, dependency rules, and fi-
nally, context features. A natural question to ask is
whether any of these feature classes can be omit-
ted to achieve the same level of performance as
the combined classifier. To answer this question,
we conducted a final set of experiments, in which
we gradually added in feature classes in the or-
der of their predictiveness (i.e., production rules
 word pairs  dependency rules  context fea-
tures), with results shown in Table 5. These results
confirm that each additional feature class indeed
contributes a marginal performance improvement,
(although it is not significant) and that all feature
classes are needed for optimal performance.
# Production # Dependency # Word Context Acc.
rules rules pairs
R1 100 ? ? No 38.4%
R2 100 ? 500 No 38.9%
R3 100 100 500 No 39.0%
R4 100 100 500 Yes 40.2%
Table 5: Accuracy with feature classes gradually
added in the order of their predictiveness.
Note that Row 3 of Table 3 corresponds to
Marcu and Echihabi (2002)?s system which ap-
plies only word pair features. The difference is
that they used a Na??ve Bayes classifier while we
used a maximum entropy classifier. As we did
not implement their Na??ve Bayes classifier, we
compare their method?s performance using the re-
sult from Table 3 Row 3 with ours from Table 5
Row 4, which shows that our system significantly
(p < 0.01) outperforms theirs.
Level 2 Type Precision Recall F
1
Count in
test set
Asynchronous 0.50 0.08 0.13 13
Synchrony ? ? ? 5
Cause 0.39 0.76 0.51 200
Pragmatic Cause ? ? ? 5
Contrast 0.61 0.09 0.15 127
Concession ? ? ? 5
Conjunction 0.30 0.51 0.38 118
Instantiation 0.67 0.39 0.49 72
Restatement 0.48 0.27 0.35 190
Alternative ? ? ? 15
List 0.80 0.13 0.23 30
All (Micro Avg.) 0.40 0.40 0.40 780
Table 6: Recall, precision, F
1
, and counts for 11
Level 2 relation types. ??? indicates 0.00.
Table 6 shows the recall, precision, and F
1
mea-
sure for the 11 individual Level 2 relation types
in the final experiment set up (Row 4 from Ta-
ble 5). A point worth noting is that the classi-
fier labels no instances of the Synchrony, Prag-
matic Cause, Concession, and Alternative relation
types. The reason is that the percentages for these
four types are so small that the classifier is highly
skewed towards the other types. From the distribu-
tion shown in Table 1, there are just 4.76% training
data for these four types, but 95.24% for the re-
maining seven types. In fact, only 30 test instances
are labeled with these four types, as shown in the
last column of Table 6. As Cause is the most pre-
348
dominant type in the training data, the classifier
tends to label uncertain relations as Cause, thus
giving Cause high recall but low precision. We see
that the F measures correlate well with the train-
ing data frequency, thus we hypothesize that ac-
curacy may improve if more training data for low
frequency relations can be provided.
Our work differs from that of (Pitler et al, 2009)
in that our system performs classification at the
more fine-grained Level 2 types, instead of the
coarse-grained Level 1 classes. Their system ap-
plies a Na??ve Bayes classifier whereas our system
uses a maximum entropy classifier, and the sets of
features used are also different. In addition, the
data set of (Pitler et al, 2009) includes EntRel
and AltLex, which are relations in which an im-
plicit connective cannot be inserted between ad-
jacent sentences, whereas ours excludes EntRel
and AltLex.
6 Discussion: Why are implicit discourse
relations difficult to recognize?
In the above experiments, we have shown that by
using the four feature classes, we are able to in-
crease the classification accuracy from 26.1% of
the majority baseline to 40.2%. Although we feel
a 14.1 absolute percentage improvement is a solid
result, an accuracy of 40% does not allow down-
stream NLP applications to trust the output of such
a classification system.
To understand the difficulties of the task more
deeply, we analyzed individual training and val-
idation data pairs, from which we were able to
generalize four challenges to automated implicit
discourse relation recognition. We hope that this
discussion may motivate future work on implicit
discourse relation recognition.
Ambiguity. There is ambiguity among the rela-
tions. For example, we notice that a lot of Contrast
relations are mistakenly classified as Conjunction.
When we analyzed these relations, we observed
that Contrast and Conjunction in the PDTB anno-
tation are very similar to each other in terms of
words, syntax, and semantics, as Examples 5 and
6 show. In both examples, the same antonymous
verb pair is used (fell and rose), different subjects
are mentioned in Arg1 and Arg2 (net and revenue
in the first example, and net and sales in the sec-
ond), and these subjects are all compared to like
items from the previous year. Moreover, the im-
plicit discourse connective given by the annotators
is while in both cases, which is an ambiguous con-
nective as shown in (Miltsakaki et al, 2005).
(5) Arg1: In the third quarter, AMR said, net
fell to $137 million, or $2.16 a share, from
$150.3 million, or $2.50 a share.
Arg2: [while] Revenue rose 17% to $2.73
billion from $2.33 billion a year earlier.
(Contrast - wsj 1812)
(6) Arg1: Dow?s third-quarter net fell to $589
million, or $3.29 a share, from $632 million,
or $3.36 a share, a year ago.
Arg2: [while] Sales in the latest quarter rose
2% to $4.25 billion from $4.15 billion a year
earlier.
(Conjunction - wsj 1926)
Relation ambiguity may be ameliorated if an in-
stance is analyzed in context. However, according
to the PDTB annotation guidelines, if the annota-
tors could not disambiguate between two relation
types, or if they felt both equally reflect their un-
derstanding of the relation between the arguments,
they could annotate two types to the relation. In
the whole PDTB corpus, about 5.4% of the ex-
plicit relations and 2.2% of the implicit relations
are annotated with two relation types. Example 7
is such a case where the implicit connective mean-
while may be interpreted as expressing a Conjunc-
tion or Contrast relation.
(7) Arg1: Sales surged 40% to 250.17 billion
yen from 178.61 billion.
Arg2: [meanwhile] Net income rose 11% to
29.62 billion yen from 26.68 billion.
(Conjunction; Contrast - wsj 2242)
Inference. Sometimes inference and a knowl-
edge base are required to resolve the relation type.
In Example 8, to understand that Arg2 is a re-
statement of Arg1, we need a semantic mechanism
to show that either the semantics of Arg1 infers
that of Arg2 or the other way around. In the be-
low example, I had calls all night long infers I
was woken up every hour semantically, as shown
in: receive call(I) ? duration(all night) ?
woken up(I) ? duration(every hour).
(8) Arg1: ?I had calls all night long from the
States,? he said.
Arg2: ?[in fact] I was woken up every hour
? 1:30, 2:30, 3:30, 4:30.?
(Restatement - wsj 2205)
349
In fact, most relation types can be represented
using formal semantics (PDTB-Group, 2007), as
shown in Table 7, where |Arg1| and |Arg2| repre-
sent the semantics extracted from Arg1 and Arg2,
respectively. This kind of formal semantic reason-
ing requires a robust knowledge base, which is still
beyond our current technology.
Relation type Semantic representation
Cause |Arg1| ? |Arg2| ? |Arg2| ? |Arg1|
Concession A ? C ? B ? ?C
where A ? |Arg1|, B ? |Arg2|
Instantiation exemplify(|Arg2|, ?x.x ? E)
where E = extract(|Arg1|)
Restatement |Arg1| ? |Arg2| ? |Arg1| ? |Arg2|
Alternative |Arg1| ? |Arg2| ? |Arg1| ? |Arg2|
Table 7: Some examples of relation types with
their semantic representations, as taken from
(PDTB-Group, 2007).
Context. PDTB annotators adopted the Mini-
mality Principle in argument selection, according
to which they only included in the argument the
minimal span of text that is sufficient for the in-
terpretation of the relation. While the context is
not necessary to interpret the relation, it is usually
necessary to understand the meaning of the argu-
ments. Without an analysis of the context, Arg1
and Arg2 may seem unconnected, as the follow-
ing example shows, where the meaning of Arg1 is
mostly derived from its previous context (i.e., West
German ... technical reactions).
(9) Prev. Context: West German Economics
Minister Helmut Haussmann said, ?In my
view, the stock market will stabilize
relatively quickly. There may be one or other
psychological or technical reactions,
Arg1: but they aren?t based on
fundamentals.
Arg2: [in short] The economy of West
Germany and the EC European Community
is highly stable.?
(Conjunction - wsj 2210)
Sometimes the range of the context may eas-
ily extend to the whole text, which would require
a system to possess a robust context modeling
mechanism. In Example 10, in order to realize
the causal relation between Arg2 and Arg1, we
possibly need to read the whole article and under-
stand what was happening: the machinist union
was having a strike and the strike prevented most
of its union members from working.
(10) Arg1: And at the company?s Wichita, Kan.,
plant, about 2,400 of the 11,700 machinists
still are working, Boeing said.
Arg2: [because] Under Kansas
right-to-work laws, contracts cannot require
workers to be union members.
(Cause - wsj 2208)
World Knowledge. Sometimes even context
modeling is not enough. We may also need world
knowledge to understand the arguments and hence
to interpret the relation. In the following example,
from the previous sentence of Arg1, it is reported
that ?the Senate voted to send a delegation of con-
gressional staffers to Poland to assist its legisla-
ture?, and this delegation is viewed as a ?gift? in
Arg1. It is suggested in Arg2 that the Poles might
view the delegation as a ?Trojan Horse?. Here we
need world knowledge to understand that ?Trojan
Horse? is usually applied as a metaphor for a per-
son or thing that appears innocent but has harm-
ful intent, and hence understand that Arg2 poses a
contrasting view of the delegation as Arg1 does.
(11) Arg1: Senator Pete Domenici calls this
effort ?the first gift of democracy?.
Arg2: [but] The Poles might do better to
view it as a Trojan Horse.
(Contrast - wsj 2237)
These four classes of difficulties ? ambiguity
between relations, inference, contextual modeling,
and world knowledge ? show that implicit dis-
course relation classification needs deeper seman-
tic representations, more robust system design,
and access to more external knowledge. These ob-
stacles may not be restricted to recognizing im-
plicit relations, but are also applicable to other re-
lated discourse-centric tasks.
7 Conclusion
We implemented an implicit discourse relation
classifier and showed initial results on the recently
released Penn Discourse Treebank. The features
we used include the modeling of the context of re-
lations, features extracted from constituent parse
trees and dependency parse trees, and word pair
features. Our classifier achieves an accuracy of
40.2%, a 14.1% absolute improvement over the
baseline. We also conducted a data analysis and
discussed four challenges that need to be ad-
dressed in future to overcome the difficulties of
implicit relation classification in the PDTB.
350
References
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation (LREC
2006), pages 449?454.
Jerry R. Hobbs. 1990. Literature and cognition. In
CSLI Lecture Notes Number 21. CSLI Publications.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2006), pages 138?145, Sydney, Australia.
Alistair Knott and Ted Sanders. 1998. The classifica-
tion of coherence relations and their linguistic mark-
ers: An exploration of two languages. Journal of
Pragmatics, 30(2):135?175.
Alex Lascarides and Nicholas Asher. 1993. Temporal
interpretation, discourse relations and commonsense
entailment. Linguistics and Philosophy, 16(5):437?
493.
Alan Lee, Rashmi Prasad, Aravind Joshi, Nikhil Di-
nesh, and Bonnie Webber. 2006. Complexity of
dependencies in discourse: Are dependencies in dis-
course more complex than in syntax? In Proceed-
ings of the 5th International Workshop on Treebanks
and Linguistic Theories, Prague, Czech Republic,
December.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 368?375, Morristown, NJ, USA.
Daniel Marcu. 1998. A surface-based approach to
identifying discourse markers and elementary tex-
tual units in unrestricted texts. In Proceedings of the
COLING-ACL 1998 Workshop on Discourse Rela-
tions and Discourse Markers, pages 1?7, Montreal,
Canada, August.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Experi-
ments on sense annotations and sense disambigua-
tion of discourse connectives. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic The-
ories (TLT2005), Barcelona, Spain, December.
Alessandro Moschitti. 2004. A study on convolu-
tion kernels for shallow semantic parsing. In Pro-
ceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2004),
Barcelona, Spain.
PDTB-Group, 2007. The Penn Discourse Treebank 2.0
Annotation Manual. The PDTB Research Group,
December.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Eas-
ily identifiable discourse relations. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (COLING 2008), Manchester,
UK, August.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse
relations in text. To appear in Proceedings of the
Joint Conference of the 47th Annual Meeting of the
Association for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP 2009).
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse Treebank
2.0. In Proceedings of the 6th International Confer-
ence on Language Resources and Evaluation (LREC
2008).
Manami Saito, Kazuhide Yamamoto, and Satoshi
Sekine. 2006. Using phrasal patterns to iden-
tify discourse relations. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2006), pages 133?
136, New York, USA, June.
Bonnie Webber. 2004. D-LTAG: Extending lex-
icalized TAG to discourse. Cognitive Science,
28(5):751?779, September.
Ben Wellner, James Pustejovsky, Catherine Havasi,
Anna Rumshisky, and Roser Sauri. 2006. Clas-
sification of discourse coherence relations: An ex-
ploratory study using multiple knowledge sources.
In Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue, Sydney, Australia, July.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: a corpus-based analysis. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING 2004), pages
134?140, Morristown, NJ, USA.
351
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 25?32,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Timestamped Graphs: Evolutionary Models of Text for                                   
Multi-document Summarization 
  Ziheng Lin, Min-Yen Kan   
School of Computing   
National University of Singapore   
Singapore 177543   
{linzihen, kanmy}@comp.nus.edu.sg   
 
Abstract 
Current graph-based approaches to auto-
matic text summarization, such as Le-
xRank and TextRank, assume a static 
graph which does not model how the in-
put texts emerge. A suitable evolutionary 
text graph model may impart a better un-
derstanding of the texts and improve the 
summarization process. We propose a 
timestamped graph (TSG) model that is 
motivated by human writing and reading 
processes, and show how text units in this 
model emerge over time. In our model, 
the graphs used by LexRank and Tex-
tRank are specific instances of our time-
stamped graph with particular parameter 
settings. We apply timestamped graphs on 
the standard DUC multi-document text 
summarization task and achieve compara-
ble results to the state of the art.  
1 Introduction 
Graph-based ranking algorithms such as 
Kleinberg?s HITS (Kleinberg, 1999) or Google?s 
PageRank (Brin and Page, 1998) have been suc-
cessfully applied in citation network analysis and 
ranking of webpages. These algorithms essentially 
decide the weights of graph nodes based on global 
topological information. Recently, a number of 
graph-based approaches have been suggested for 
NLP applications. Erkan and Radev (2004) intro-
duced LexRank for multi-document text summari-
zation. Mihalcea and Tarau (2004) introduced 
TextRank for keyword and sentence extractions. 
Both LexRank and TextRank assume a fully con-
nected, undirected graph, with text units as nodes 
and similarity as edges.  After graph construction, 
both algorithms use a random walk on the graph to 
redistribute the node weights.  
Many graph-based algorithms feature an evolu-
tionary model, in which the graph changes over 
timesteps.  An example is a citation network whose 
edges point backward in time: papers (usually) 
only reference older published works. References 
in old papers are static and are not updated. Simple 
models of Web growth are exemples of this: they 
model the chronological evolution of the Web in 
which a new webpage must be linked by an incom-
ing edge in order to be publicly accessible and may 
embed links to existing webpages. These models 
differ in that they allow links in previously gener-
ated webpages to be updated or rewired. However, 
existing graph models for summarization ? 
LexRank and TextRank ? assume a static graph, 
and do not model how the input texts evolve. The 
central hypothesis of this paper is that modeling 
the evolution of input texts may improve the sub-
sequent summarization process. Such a model may 
be based on human writing/reading process and 
should show how just composed/consumed units of 
text relate to previous ones. By applying this 
model over a series of timesteps, we obtain a rep-
resentation of how information flows in the con-
struction of the document set and leverage this to 
construct automatic summaries. 
We first introduce and formalize our timestam-
ped graph model in next section.  In particular, our 
formalization subsumes previous works: we show 
in Section 3 that the graphs used by LexRank and 
TextRank are specific instances of our timestam-
ped graph.  In Section 4, we discuss how the result-
ing graphs are applied to automatic multi-
document text summarization: by counting node 
in-degree or applying a random walk algorithm to 
smooth the information flow. We apply these mod-
els to create an extractive summarization program 
25
and apply it to the standard Document Understand-
ing Conference (DUC) datasets. We discuss the 
resulting performance in Section 5. 
2 Timestamped Graph 
We believe that a proper evolutionary graph model 
of text should capture the writing and reading 
processes of humans. Although such human proc-
esses vary widely, when we limit ourselves to ex-
pository text, we find that both skilled writers and 
readers often follow conventional rhetorical styles 
(Endres-Niggemeyer, 1998; Liddy, 1991). In this 
work, we explore how a simple model of evolution 
affects graph construction and subsequent summa-
rization. In this paper, our work is only exploratory 
and not meant to realistically model human proc-
esses and we believe that deep understanding and 
inference of rhetorical styles (Mann and Thompson, 
1988) will improve the fidelity of our model.  Nev-
ertheless, a simple model is a good starting point.   
We make two simple assumptions: 
 
1: Writers write articles from the first sentence 
to the last; 
2: Readers read articles from the first sentence 
to the last. 
 
The assumptions suggest that we add sentences 
into the graph in chronological order: we add the 
first sentence, followed by the second sentence, 
and so forth, until the last sentence is added.  
These assumptions are suitable in modeling the 
growth of individual documents. However when 
dealing with multi-document input (common in 
DUC), our assumptions do not lead to a straight-
forward model as to which sentences should ap-
pear in the graph before others.  One simple way is 
to treat multi-document problems simply as multi-
ple instances of the single document problem, 
which evolve in parallel.  Thus, in multi-document 
graphs, we add a sentence from each document in 
the input set into the graph at each timestep. Our 
model introduces a skew variable to model this and 
other possible variations, which is detailed later. 
The pseudocode in Figure 1 summarizes how 
we build a timestamped graph for multi-document 
input set.  Informally, we build the graph itera-
tively, introducing new sentence(s) as node(s) in  
 
 
 
the graph at each timestep. Next, all sentences in 
the graph pick other previously unconnected ones 
to draw a directed edge to.  This process continues 
until all sentences are placed into the graph.  
Figure 2 shows this graph building process in 
mid-growth, where documents are arranged in col-
umns, with dx represents the xth document and sy 
represents the yth sentence of each document. The 
bottom shows the nth sentences of all m documents 
being added simultaneously to the graph. Each new 
node can either connect to a node in the existing 
graph or one of the other m-1 new nodes. Each 
existing node can connect to another existing node 
or to one of the m newly-introduced nodes. Note 
that this model differs from the citation networks 
in such that new outgoing edges are introduced to 
old nodes, and differs from previous models for 
Web growth as it does not require new nodes to 
have incoming edges. 
 
              
Figure 2: Snapshot of a timestamped graph. 
 
Figure 3 shows an example of the graph building 
process over three timesteps, starting from an 
empty graph. Assume that we have three docu-
ments and each document has three sentences. Let 
dxsy indicate the yth sentence in the xth document. 
At timestep 1, sentences d1s1, d2s1 and d3s1 are 
s1
s2 
s3 
. 
. 
. 
. 
. 
sn 
d1  d2  d3  ????  dm 
existing 
graph 
Figure 1: Pseudocode for a specific instance of a  
timestamped graph algorithm 
Input:  M, a cluster of m documents relating to a  
common event;  
Let: i = index to sentences, initially 1;  
G = the timestamped graph, initially empty. 
Step 1:  Add the ith sentence of all documents into G. 
Step 2:  Let each existing sentence in G choose and 
connect to one other existing sentence in G. 
The chosen sentence must be sentence which 
has not been previously chosen by this sentence 
in previous iterations. 
Step 3:  if there are no new sentences to add, break; 
else i++, goto Step 1. 
Output:  G, a timestamped graph. 
m new  
sentences 
26
added to the graph. Three edges are introduced to 
the graph, in which the edges are chosen by some 
strategy; perhaps by choosing the candidate sen-
tence by its maximum cosine similarity with the 
sentence under consideration.  Let us say that this 
process connects d1s1?d3s1, d2s1?d3s1 and 
d3s1?d2s1. At timestep 2, sentences d1s2, d2s2 and 
d3s2 are added to the graph and six new edges are 
introduced to the graph. At timestep 3, sentences 
d1s3, d2s3 and d3s3 are added to the graph, and nine 
new edges are introduced. 
 
 
(a) Timestep 1           (b) Timestep 2            (c) Timestep 3 
 
Figure 3: An example of the growth of a  
timestamped graph. 
 
The above illustration is just one instance of a 
timestamped graph with specific parameter settings.  
We generalize and formalize the timestamped 
graph algorithm as follows: 
 
Definition: A timestamped graph algorithm 
tsg(M) is a 9-tuple (d, e, u, f, ?, t, i, s, ?) that speci-
fies a resulting algorithm that takes as input the set 
of texts M and outputs a graph G, where: 
 
d  specifies the direction of the edges, d?{f, b, u}; 
e  is the number of edges to add for each vertex  
in G at each timestep, e?? +; 
u  is 0 or 1, where 0 and 1 specifies unweighted and 
weighted edges, respectively; 
f  is the inter-document factor, 0 ? f ? 1;  
? is a vertex selection function ?(u, G) that takes 
in a vertex u and G, and chooses a vertex v?G; 
t  is the type of text units, t?{word, phrase,  
sentence, paragraph, document}; 
i  is the node increment factor, i?? +;  
s  is the skew degree, s ? -1 and s?? , where -1 
represent free skew and 0 no skew; 
?  is a document segmentation function ?(?). 
 
In the TSG model, the first set of parameters d, 
e, u, f deal with the properties of edges; ?, t, i, s 
deal with properties of nodes; finally, ? is a func-
tion that modifies input texts. We now discuss the 
first eight parameters; the relevance of ? will be 
expanded upon later in the paper. 
2.1 Edge Settings 
We can specify the direction of information flow 
by setting different d values. When a node v1 
chooses another node v2 to connect to, we set d to f 
to represent a forward (outgoing) edge. We say 
that v1 propagates some of its information into v2. 
When letting a node v1 choose another node v2 to 
connect to v1 itself, we set d to b to represent a 
backward (incoming) edge, and we say that v1 re-
ceives some information from v2. Similarly, d = u 
specifies undirected edges in which information 
propagates in both directions. The larger amount of 
information a node receives from other nodes, the 
higher the importance of this node. 
Our toy example in Figure 3 has small dimen-
sions: three sentences for each of three documents. 
Experimental document clusters often have much 
larger dimensions. In DUC, clusters routinely con-
tain over 25 documents, and the average length for 
documents can be as large as 50 sentences. In such 
cases, if we introduce one edge for each node at 
each timestep, the resulting graph is loosely con-
nected. We let e be the number of outgoing edges 
for each sentence in the graph at each timestep. To 
introduce more edges into the graph, we increase e. 
We can also incorporate unweighted or 
weighted edges into the graph by specifying the 
value of u. Unweighted edges are good when rank-
ing algorithms based on in-degree of nodes are 
used. However, unlike links between webpages, 
edges between text units often have weights to in-
dicate connection strength. In these cases, un-
weighted edges lose information and a weighted 
representation may be better, such as in cases 
where PageRank-like algorithms are used for rank-
ing.  
Edges can represent information flow from one 
node to another. We may prefer intra-document 
edges over inter-document edges, to model the in-
tuition that information flows within the same 
document more likely than across documents. Thus 
we introduce an inter-document factor f, where 0 ? 
f ? 1. When this feature is smaller than 1, we re-
place the weight w for inter-document edges by fw.  
27
2.2 Node Settings 
In Figure 1 Step 2, every existing node has a 
chance to choose another existing node to connect 
to. Which node to choose is decided by the selec-
tion strategy ?. One strategy is to choose the node 
with the highest similarity. There are many similar-
ity functions to use, including token-based Jaccard 
similarity, cosine similarity, or more complex 
models such as concept links (Ye et al, 2005).  
t controls the type of text unit that represents  
nodes. Depending on the application, text units can 
be words, phrases, sentences, paragraphs or even 
documents. In the task of automatic text summari-
zation, systems are conveniently assessed by let-
ting text units be sentences.  
i controls the number of sentences entering the 
graph at every iteration. Certain models, such as 
LexRank, introduce all of the input sentences in 
one time step (i.e., i = Lmax, where Lmax is the 
maximum length of the input documents), com-
pleting the construction of G in one step.  However, 
to model time evolution, i needs to be set to a value 
smaller than this. 
Most relevant to our study is the skew parame-
ter s. Up to now, the TSG models discussed all 
assume that authors start writing all documents in 
the input set at the same time. It is reflected by 
adding the first sentences of all documents simul-
taneously. However in reality, some documents are 
authored later than others, giving updates or report-
ing changes to events reported earlier. In DUC 
document clusters, news articles are typically taken 
from two or three different newswire sources. They 
report on a common event and thus follow a story-
line. A news article usually gives summary about 
what have been reported in early articles, and gives 
updates or changes on the same event.  
To model this, we arrange the documents in ac-
cordance with the publishing time of the docu-
ments. The earliest document is assigned to 
column 1, the second earliest document to column 
2, and so forth, until the latest document is as-
signed to the last column. The graph construction 
process is the same as before, except that we delay 
adding the first sentences of later documents until a 
proper iteration, governed by s. With s = 1, we de-
lay the addition of the first sentence of column 2 
until the second timestep, and delay the addition of 
the first sentence of column 3 until the third 
timestep. The resulting timestamped graph is 
skewed by 1 timestep (Figure 4 (a)). We can in-
crease the skew degree s if the time intervals be-
tween publishing time of documents are large. 
Figure 4 (b) shows a timestamped graph skewed by 
2 timesteps. We can also skew a graph freely by 
setting s to -1. When we start to add the first sen-
tence dis1 of a document di, we check whether there 
are existing sentences in the graph that want to 
connect to dis1 (i.e., that ? (?,G) = dis1). If there is, 
we add dis1 to the graph; else we delay the addition 
and reassess again in next timestep. The result is a 
freely skewed graph (Figure 4 (c)). In Figure 4 (c), 
we start adding the first sentences of documents d2 
to d4 at timesteps 2, 5 and 7, respectively. At 
timestep 1, d1s1 is added into the graph. At 
timestep 2, an existing node (d1s1 in this case) 
wants to connect to d2s1, so d2s1 is added. d3s1 is 
added at timestep 5 as no existing node wants to 
connect to d3s1 until timestep 5. Similarly, d4s1 is 
added until some nodes choose to connect to it at 
timestep 7. Notice that we hide edges in Figure 4 
for clarity. 
 
   (a) Skewed by 1         (b) Skewed by 2      (c) Freely skewed 
 
Figure 4: Skewing the graphs. Edges are hidden for clarity. 
For each graph, the leftmost column is the earliest document. 
Documents are then chronologically ordered, with the right-
most one being the latest. 
3 Comparison and Properties of TSG  
The TSG representation generalizes many pos-
sible specific algorithm configurations.  As such, it 
is natural that previous works can be cast as spe-
cific instances of a TSG.  For example, we can suc-
cinctly represent the algorithm used in the running 
example in Section 2 as the tuple (f, 1, 0, 1, max-
cosine-based, sentence, 1, 0, null). LexRank and 
TextRank can also be cast as TSGs: (u, N, 1, 1, 
cosine-based, sentence, Lmax, 0, null) and (u, L, 1, 1, 
modified-co-occurrence-based, sentence, L, 0, 
28
null). As LexRank is applied in multi-document 
summarizations, e is set to the total number of sen-
tences in the cluster, N, and i is set to the maxi-
mum document length in the cluster, Lmax. 
TextRank is applied in single-document summari-
zation, so both its e and i are set to the length of the 
input document, L. This compact notation empha-
sizes the salient differences between these two al-
gorithm variants: namely that, e, ? and i. 
Despite all of these possible variations, all 
timestamped graphs have two important features, 
regardless of their specific parameter settings. First, 
nodes that were added early have more chosen 
edges than nodes added later, as visible in Figure 3 
(c). If forward edges (d = f) represent information 
flow from one node to another, we can say that 
more information is flowing from these early 
nodes to the rest of the graph. The intuition for this 
is that, during the writing process of articles, early 
sentences have a greater influence to the develop-
ment of the articles? ideas; similarly, during the 
reading process, sentences that appear early con-
tribute more to the understanding of the articles. 
The fact that early nodes stay in the graph for a 
longer time leads to the second feature: early nodes 
may attract more edges from other nodes, as they 
have larger chance to be chosen and connected by 
other nodes. This is also intuitive for forward 
edges (d = f): during the writing process, later sen-
tences refer back to early sentences more often 
than vice versa; and during the reading process, 
readers tend to re-read early sentences when they 
are not able to understand the current sentence.  
4 Random Walk 
Once a timestamped graph is built, we want to 
compute an importance score for each node.  These 
scores are then used to determine which nodes 
(sentences) are the most important to extract sum-
maries from.  The graph G shows how information 
flows from node to node, but we have yet to let the 
information actually flow. One method to do this is 
to use the in-degree of each node as the score.  
However, most graph algorithms now use an itera-
tive method that allows the weights of the nodes 
redistribute until stability is reached.  One method 
for this is by applying a random walk, used in Pag-
eRank (Brin and Page, 1998). In PageRank the 
Web is treated as a graph of webpages connected 
by links. It assumes users start from a random 
webpage, moving from page to page by following 
the links. Each user follows the links at random 
until he gets ?bored? and jumps to a random web-
page. The probability of a user visiting a webpage 
is then proportional to its PageRank score.  PageR-
ank can be iteratively computed by: 
 
   ?
?
?+=
)(
)(
)(
1
)1()(
uInv
vPR
vOutN
uPR ??     (1) 
 
where N is the total number of nodes in the graph, 
In(u) is the set of nodes that point to u, and Out(u) 
is the set of nodes that node u points to. ? is a 
damping factor that can be set between 0 and 1, 
which has the role of integrating into the model the 
probability of jumping from a given node to an-
other random node in the graph. In the context of 
web surfing, a user either clicks on a link on the 
current page at random with probability 1 - ?, or 
opens a completely new random page with prob-
ability ?. 
Equation 1 does not take into consideration the 
weights of edges, as the original PageRank defini-
tion assumes hyperlinks are unweighted. Thus we 
can use Equation 1 to rank nodes for an un-
weighted timestamped graph. To integrate edge 
weights into the graph, we modify Eq. 1, yielding: 
 
  ? ??
?
?+=
)(
)(
)()1()(
uInv
vOutx
vx
vu vPR
w
w
N
uPR ??     (2) 
 
where Wvu represents the weight of the edge point-
ing from v to u.  
As we may have a query for each document 
cluster, we also wish to take queries into consid-
eration in ranking the nodes. Haveliwala (2003) 
introduces a topic-sensitive PageRank computation. 
Equations 1 and 2 assume a random walker jumps 
from the current node to a random node with prob-
ability ?. The key to creating topic-sensitive Pag-
eRank is that we can bias the computation by 
restricting the user to jump only to a random node 
which has non-zero similarity with the query. Ot-
terbacher et al (2005) gives an equation for topic-
sensitive and weighted PageRank as: 
 
? ?? ?
??
?+=
)(
)(
)()1(
),(
),(
)(
uInv
vOutx
vx
vu
Sy
vPR
w
w
Qysim
Qusim
uPR ??    (3) 
29
 
where S is the set of all nodes in the graph, and 
sim(u, Q) is the similarity score between node u 
and the query Q. 
5 Experiments and Results 
We have generalized and formalized evolutionary 
timestamped graph model. We want to apply it on 
automatic text summarization to confirm that these 
evolutionary models help in extracting important 
sentences. However, the parameter space is too 
large to test all possible TSG algorithms. We con-
duct experiments to focus on the following re-
search questions that relating to 3 TSG parameters 
- e, u and s, and the topic-sensitivity of PageRank.  
 
Q1: Do different e values affect the summariza-
tion process? 
Q2: How do topic-sensitivity and edge weight-
ing perform in running PageRank?  
Q3: How does skewing the graph affect infor-
mation flow in the graph? 
 
The datasets we use are DUC 2005 and 2006. 
These datasets both consist of 50 document clus-
ters. Each cluster consists of 25 news articles 
which are taken from two or three different news-
wire sources and are relating to a common event, 
and a query which contains a topic for the cluster 
and a sequence of statements or questions. The 
first three experiments are run on DUC 2006, and 
the last experiment is run on DUC 2005. 
In the first experiment, we analyze how e, the 
number of chosen edges for each node at each 
timestep, affects the performance, with other pa-
rameters fixed. Specifically the TSG algorithm we 
use is the tuple (f, e, 1, 1, max-cosine-based, sen-
tence, 1, 0, null), where e is being tested for differ-
ent values. The node selection function max-
cosine-based takes in a sentence  s and the current 
graph G, computes the TFIDF-based cosine simi-
larities between s and other sentences in G, and 
connects s to e sentence(s) that has(have) the high-
est cosine score(s) and is(are) not yet chosen by s 
in previous iterations. We run topic-sensitive Pag-
eRank with damping factor ? set to 0.5 on the 
graphs. Figures 5 (a)-(b) shows the ROUGE-1 and 
ROUGE-2 scores with e set to 1, 2, 3, 4, 5, 6, 7, 10, 
15, 20 and N, where N is the total number of sen-
tences in the cluster. We succinctly represent 
LexRank graphs by the tuple (u, N, 1, 1, cosine-
based, sentence, Lmax, 0, null) in Section 3; it can 
also be represented by a slightly different tuple (f, 
N, 1, 1, max-cosine-based, sentence, 1, 0, null). It 
differs from the first representation in that we itera-
tively add 1 sentence for each document in each 
timestep and let al nodes in the current graph con-
nect to every other node in the graph. In this ex-
periment, when e is set to N, the timestamped 
graph is equivalent to a LexRank graph. We do not 
use any reranker in this experiment. 
 
 
 
N
N
Figure 5: (a) ROUGE-1 and (b) ROUGE-2 scores for 
timestamped graphs with different e settings. N is the total 
number of sentences in the cluster. 
 
The results allow us to make several observa-
tions. First, when e = 2, the system gives the best 
performance, with ROUGE-1 score 0.37728 and 
ROUGE-2 score 0.07692. Some values of e give 
better scores than LexRank graph configuration, in 
which e = N. Second, the system gives very bad 
performance when e = 1. This is because when e is 
set to 1, the graph is too loosely connected and is 
not suitable to apply random walk on it. Third, the 
system gives similar performance when e is set 
30
greater than 10. The reason for this is that the 
higher values of e make the graph converge to a 
fully connected graph so that the performance 
starts to converge and display less variability.  
We run a second experiment to analyze how 
topic-sensitivity and edge weighting affect the sys-
tem performance. We use concept links (Ye et al, 
2005) as the similarity function and a MMR 
reranker to remove redundancy. Table 1 shows the 
results. We observe that both topic-sensitive Pag-
eRank and weighted edges perform better than ge-
neric PageRank on unweighted timestamped 
graphs. When topic-sensitivity and edge weighting 
are both set to true, the system gives the best per-
formance. 
 
Topic-
sensitive 
Weighted 
edges 
ROUGE-1 ROUGE-2 
No No 0.39358 0.07690 
Yes No 0.39443 0.07838 
No Yes 0.39823 0.08072 
Yes Yes 0.39845 0.08282 
Table 1: ROUGE-1 and ROUGE-2 scores for different com-
binations of topic-sensitivity and edge weighting(u) settings. 
 
To evaluate how skew degree s affects summa-
rization performance, we use the parameter setting 
from the first experiment, with e fixed to 1. Spe-
cifically, we use the tuple (f, 1, 1, 1, concept-link-
based, sentence, 1, s, null), with s set to 0, 1 and 2. 
Table 2 gives the evaluation results. We observe 
that s = 1 gives the best ROUGE-1 and ROUGE-2 
scores. Compared to the system without skewing (s 
= 0), s = 2 gives slightly better ROUGE-1 score 
but worse ROUGE-2 score. The reason for this is 
that s = 2 introduces a delay interval that is too 
large. We expect that a freely skewed graph (s =  
-1) will give more reasonable delay intervals.  
 
Skew degree ROUGE-1 ROUGE-2 
0 0.36982 0.07580 
1 0.37268 0.07682 
2 0.36998 0.07489 
Table 2: ROUGE-1 and ROUGE-2 scores for  
different skew degrees. 
 
We tune the system using different combina-
tions of parameters, and the TSG algorithm with 
tuple (f, 1, 1, 1, concept-link-based, sentence, 1, 0, 
null) gives the best scores. We run this TSG algo-
rithm with topic-sensitive PageRank and MMR 
reranker on DUC 2005 dataset. The results show 
that our system ranks third in both ROUGE-2 and 
ROUGE-SU4 scores. 
 
Rank System ROUGE-2 System ROUGE-SU4 
1 15 0.0725 15 0.1316 
2 17 0.0717 17 0.1297 
3 TSG 0.0712 TSG 0.1285 
4 10 0.0698 8 0.1279 
5 8 0.0696 4 0.1277 
Table 3: top ROUGE-2 and ROUGE-SU4 
scores in DUC 2005. TSG is our system. 
 
6 Discussion 
A closer inspection of the experimental clusters 
reveals one problem. Clusters that consist of 
documents that are of similar lengths tend to per-
form better than those that contain extremely long 
documents. The reason is that a very long docu-
ment introduces too many edges into the graph. 
Ideally we want to have documents with similar 
lengths in a cluster. One solution to this is that we 
split long documents into shorter documents with 
appropriate lengths. We introduce the last parame-
ter in the formal definition of timestamped graphs, 
?, which is a document segmentation function ?(?). 
?(M) takes in as input a set of documents M, ap-
plies segmentation on long documents to split them 
into shorter documents, and output a set of docu-
ments with similar lengths, M?. Slightly better re-
sults are achieved when a segmentation function is 
applied. One shortcoming of applying ?(?) is that 
when a document is split into two shorter ones, the 
early sentences of the second half now come be-
fore the later sentences of the first half, and this 
may introduce inconsistencies in our representation: 
early sentences of the second half contribute more 
into later sentences of the first half than the vice 
versa. 
7 Related Works 
Dorogovtsev and Mendes (2001) suggest schemes 
of the growth of citation networks and the Web, 
which are similar to the construction process of 
timestamped graphs.  
Erkan and Radev (2004) proposed LexRank to 
define sentence importance based on graph-based 
centrality ranking of sentences. They construct a 
similarity graph where the cosine similarity of each 
pair of sentences is computed. They introduce 
three different methods for computing centrality in 
31
similarity graphs. Degree centrality is defined as 
the in-degree of vertices after removing edges 
which have cosine similarity below a pre-defined 
threshold. LexRank with threshold is the second 
method that applies random walk on an un-
weighted similarity graph after removing edges 
below a pre-defined threshold. Continuous Le-
xRank is the last method that applies random walk 
on a fully connected, weighted similarity graph. 
LexRank has been applied on multi-document text 
summarization task in DUC 2004, and topic-
sensitive LexRank has been applied on the same 
task in DUC 2006.  
Mihalcea and Tarau (2004) independently pro-
posed another similar graph-based random walk 
model, TextRank. TextRank is applied on keyword 
extraction and single-document summarization. 
Mihalcea, Tarau and Figa (2004) later applied Pag-
eRank to word sense disambiguation. 
8 Conclusion 
We have proposed a timestamped graph model 
which is motivated by human writing and reading 
processes. We believe that a suitable evolutionary 
text graph which changes over timesteps captures 
how information propagates in the text graph. Ex-
perimental results on the multi-document text 
summarization task of DUC 2006 showed that 
when e is set to 2 with other parameters fixed, or 
when s is set to 1 with other parameters fixed, the 
graph gives the best performance. It also showed 
that topic-sensitive PageRank and weighted edges 
improve summarization process. This work also 
unifies representations of graph-based summariza-
tion, including LexRank and TextRank, modeling 
these prior works as specific instances of time-
stamped graphs.  
We are currently looking further on skewed 
timestamped graphs. Particularly we want to look 
at how a freely skewed graph propagates informa-
tion. We are also analyzing in-degree distribution 
of timestamped graphs. 
Acknowledgments 
The authors would like to thank Prof. Wee Sun Lee 
for his very helpful comments on random walk and 
the construction process of timestamped graphs, 
and thank Xinyi Yin (Yin, 2007) for his help in 
spearheading the development of this work. We 
also would like to thank the reviewers for their 
helpful suggestions in directing the future of this 
work. 
References 
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. In Proceedings of ACM-
SIAM Symposium on Discrete Algorithms, 1999. 
Sergey Brin and Lawrence Page. 1998. The anatomy of 
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1-7). 
G?nes Erkan and Dragomir R. Radev. 2004. LexRank: 
Graph-based centrality as salience in text summari-
zation. Journal of Artificial Intelligence Research, 
(22). 
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing order into texts. In Proceedings of EMNLP 2004. 
Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004. 
PageRank on semantic networks, with application to 
word sense disambiguation. In Proceedings of 
COLING 2004. 
S.N. Dorogovtsev and J.F.F. Mendes. 2001. Evolution 
of networks. Submitted to Advances in Physics on 
6th March 2001. 
Shiren Ye, Long Qiu, Tat-Seng Chua, and Min-Yen 
Kan. 2005. NUS at DUC 2005: Understanding docu-
ments via concepts links. In Proceedings of DUC 
2005. 
Xinyi Yin, 2007. Random walk and web information 
processing for mobile devices. PhD Thesis.  
Taher H. Haveliwala. 2003. Topic-sensitive pagerank: A 
context-sensitive ranking algorithm for web search. 
IEEE Transactions on Knowledge and Data Engi-
neering 
Jahna Otterbacher, G?nes Erkan and Dragomir R. 
Radev. 2005. Using Random Walks for Question-
focused Sentence Retrieval. In Proceedings of 
HLT/EMNLP 2005. 
Brigitte Endres-Niggemeyer. 1998. Summarizing infor-
mation. Springer New York. 
Elizabeth D. Liddy. 1991. The discourse-level structure 
of empirical abstracts: an exploratory study. Infor-
mation Processing and Management 27(1):55-81. 
William C. Mann and Sandra A. Thompson. 1988. Rhe-
torical structure theory: Towards a functional theory 
of text organization. Text 8(3): 243-281. 
32
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 940?949, Dublin, Ireland, August 23-29 2014.
The Impact of Deep Hierarchical Discourse Structures
in the Evaluation of Text Coherence
Vanessa Wei Feng
1
, Ziheng Lin
2
, and Graeme Hirst
1
1
Department of Computer Science
University of Toronto
{weifeng, gh}@cs.toronto.edu
2
Singapore Press Holdings
linziheng@gmail.com
Abstract
Previous work by Lin et al. (2011) demonstrated the effectiveness of using discourse relations
for evaluating text coherence. However, their work was based on discourse relations annotated
in accordance with the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which encodes
only very shallow discourse structures; therefore, they cannot capture long-distance discourse
dependencies. In this paper, we study the impact of deep discourse structures for the task of co-
herence evaluation, using two approaches: (1) We compare a model with features derived from
discourse relations in the style of Rhetorical Structure Theory (RST) (Mann and Thompson,
1988), which annotate the full hierarchical discourse structure, against our re-implementation of
Lin et al.?s model; (2) We compare a model encoded using only shallow RST-style discourse
relations, against the one encoded using the complete set of RST-style discourse relations. With
an evaluation on two tasks, we show that deep discourse structures are truly useful for better dif-
ferentiation of text coherence, and in general, RST-style encoding is more powerful than PDTB-
style encoding in these settings.
1 Introduction
In a well-written text, utterances are not simply presented in an arbitrary order; rather, they are presented
in a logical and coherent form, so that the readers can easily interpret the meaning that the writer wishes
to present. Therefore, coherence is one of the most essential aspects of text quality. Given its importance,
the automatic evaluation of text coherence is one of the crucial components of many NLP applications.
A particularly popular model for the evaluation of text coherence is the entity-based local coherence
model of Barzilay and Lapata (B&L) (2005; 2008), which extracts mentions of entities in the text, and
models local coherence by the transitions, from one sentence to the next, in the grammatical role of each
mention. Since the initial publication of this model, a number of extensions have been proposed, the
majority of which are focused on enriching the original feature set. However, these enriched feature
sets are usually application-specific, i.e., it requires a certain expertise and intuition to conceive good
features.
In contrast, we seek insights of better feature encoding from a more general problem: discourse parsing
(to be introduced in Section 2). Discourse parsing aims to identify the discourse relations held among
various discourse units in the text. Therefore, one can expect that discourse parsing provides useful
information to the evaluation of text coherence, because, essentially, the existence and the distribution of
discourse relations are the basis of the coherence in a text.
In fact, there is already evidence showing that discourse relations can help better capture text coher-
ence. Lin et al. (2011) use a PDTB-style discourse parser (to be introduced in Section 2.1) to identify
discourse relations in the text, and they represent a text by entities and their associated discourse roles
in each sentence. In their experiments, using discourse roles alone, their model performs very simi-
lar or even better than B&L?s model. Combining their discourse role features with B&L?s entity-based
transition features further improves the performance.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
940
S1
: The dollar finished lower yesterday, after tracking another rollercoaster session on Wall Street.
S
2
: [Concern about the volatile U.S. stock market had faded in recent sessions]C
2.1
, [and traders
appeared content to let the dollar languish in a narrow range until tomorrow, when the preliminary
report on third-quarter U.S. gross national product is released.]C
2.2
S
3
: But seesaw gyrations in the Dow Jones Industrial Average yesterday put Wall Street back in the
spotlight and inspired market participants to bid the U.S. unit lower.
Three discourse relations are presented in the text above:
1. Implicit EntRel between S
1
as Arg1, and S
2
as Arg2.
2. Explicit Conjunction within S
2
: C
2.1
as Arg1, C
2.2
as Arg2, with and as the connective.
3. Explicit Contrast between S
2
as Arg1 and S
3
as Arg2, with but as the connective.
Figure 1: An example text fragment composed of three sentences, and its PDTB-style discourse relations.
However, PDTB-style discourse relations encode only very shallow discourse structures, i.e., the re-
lations are mostly local, e.g., within a single sentence or between two adjacent sentences. Therefore,
in general, features derived from PDTB-style discourse relations cannot capture long discourse depen-
dency, and thus the resulting model is still limited to being a local model. Nonetheless, long-distance
discourse dependency could be quite useful for capturing text coherence from a global point of view.
Therefore, in this paper, we study the effect of deep hierarchical discourse structure in the evalua-
tion of text coherence, by adopting two approaches to perform a direct comparison between models that
incorporate deep hierarchical discourse structures and models with shallow structures. To evaluate our
models, we conduct experiments on two datasets, each of which resembles a real sub-task in the evalu-
ation of text coherence: sentence ordering and essay scoring. On both tasks, the model derived from
deep discourse structures is shown to be more powerful than the model derived from shallow discourse
structures. Moreover, for sentence ordering, combining our model with entity-based transition features
achieves the best performance. However, for essay scoring, the combination is detrimental.
2 Discourse parsing
Discourse parsing is the problem of identifying the discourse structure within a text, by recognizing the
specific type of its discourse relations, such as Contrast, Explanation, and Causal relations. Although
discourse parsing is still relatively less well-studied, a number of theories have been proposed to capture
different rhetorical characteristics or to serve different applications.
Currently, the two main directions in the study of discourse parsing are PDTB-style and RST-style
parsing. These two directions are based on distinct theoretical frameworks, and each can be potentially
useful for particular kinds of downstream applications. As will be discussed shortly, the major difference
between PDTB- and RST-style discourse parsing is the notion of deep hierarchical discourse structure,
which, according to our hypothesis, can be very useful for recognizing text coherence.
2.1 PDTB-style Discourse Parsing
The Penn Discourse Treebank (PDTB), developed by Prasad et al. (2008), is currently the largest
discourse-annotated corpus, consisting of 2159 Wall Street Journal articles. The annotation in PDTB
adopts the predicate-argument view of discourse relations, where a discourse connective (e.g., because)
is treated as a predicate that takes two text spans as its arguments. The argument that the discourse con-
nective structurally attaches to is called Arg2, and the other argument is called Arg1. In PDTB, relations
are further categorized into explicit and implicit relations: a relation is explicit if there is an explicit dis-
course connective presented in the text; otherwise, it is implicit. PDTB relations focus more on locality
and adjacency: explicit relations seldom connect text units beyond local context; for implicit relations,
941
S1
: [The dollar finished lower yesterday,]e
1
[after tracking another rollercoaster session on Wall
Street.]e
2
S
2
: [Concern about the volatile U.S. stock market had faded in recent sessions,]e
3
[and traders
appeared content to let the dollar languish in a narrow range until tomorrow,]e
4
[when the preliminary
report on third-quarter U.S. gross national product is released.]e
5
S
3
: [But seesaw gyrations in the Dow Jones Industrial Average yesterday put Wall Street back in the
spotlight]e
6
[and inspired market participants to bid the U.S. unit lower.]e
7
Condition
(e1-e7)
(e1) (e2)
(e1-e2) (e3-e7)
(e4-e5)
(e4) (e5)
Background
Temporal
List Cause
(e6) (e7)
(e6-e7)(e3-e5)
(e3)
Contrast
Figure 2: An example text fragment composed of seven EDUs, and its RST discourse tree representation.
only adjacent sentences within paragraphs are examined for the existence of implicit relations.
The PDTB-style discourse parsing is thus the type of framework in accordance with the PDTB, which
extracts the discourse relations in a text, by identifying the presence of discourse connectives, the asso-
ciated discourse arguments, and the specific types of the relations. An example text fragment is shown
in Figure 1, consisting of three sentences, S
1
, S
2
, and S
3
. A sentence may further contain clauses, e.g.,
C
2.1
and C
2.2
in S
2
. The three PDTB-style discourse relations in this text are explained below the text.
2.2 RST-style Discourse Parsing
RST-style discourse parsing follows the theoretical framework of Rhetorical Structure Theory (RST)
(Mann and Thompson, 1988). In the framework of RST, a coherent text can be represented as a discourse
tree whose leaves are non-overlapping text spans called elementary discourse units (EDUs); these are the
minimal text units of discourse trees. Adjacent nodes can be related through particular discourse relations
to form a discourse subtree, which can then be related to other adjacent nodes in the tree structure.
RST-style discourse relations can be categorized into two types: mononuclear and multi-nuclear. In
mononuclear relations, one of the text spans, the nucleus, is more salient than the other, the satellite,
while in multi-nuclear relations, all text spans are equally important for interpretation.
Consider Figure 2, in which the same example as in Figure 1 is chunked into seven EDUs (e
1
-e
7
),
segmented by square brackets. Its discourse tree representation is shown below in the figure, following
the notational convention of RST. The two EDUs e
1
and e
2
are related by a mononuclear relation Tem-
poral, where e
1
is the more salient span; e
4
and e
5
are related by Condition, with e
4
as the nucleus; and
e
6
and e
7
are related by Cause, with e
7
as the nucleus. Then, the spans (e
3
-e
5
) and (e
6
-e
7
) are related by
Contrast to form a higher-level discourse structure, and so on. Finally, a Background relation merges the
span (e
1
-e
2
) and (e
3
-e
7
) on the top level of the tree.
As can be seen, thanks to the tree-structured representation of RST, compared to PDTB-style repre-
sentation, we have a full hierarchy of discourse relations in the text: discourse relations exist not only in
a local context, but also on higher text levels, such as between S
1
and the concatenation of S
2
and S
3
.
3 Entity-based Local Coherence Model
The entity-based local coherence model was initially developed by Barzilay and Lapata (B&L) (2005;
2008). The fundamental assumption of this model is that a document makes repeated reference to ele-
ments of a set of entities that are central to its topic.
For a document d, an entity grid is constructed, in which the columns represent the entities referred
942
S1
: [The dollar]
S
finished lower [yesterday]
X
, after tracking [another rollercoaster session]
O
on
[Wall Street]
X
.
S
2
: [Concern]
S
about [the volatile U.S. stock market]
X
had faded in [recent sessions]
X
, and
[traders]
S
appeared content to let [the dollar]
S
languish in [a narrow range]
X
until [tomorrow]
X
,
when [the preliminary report]
S
on [third-quarter U.S. gross national product]
X
is released.
S
3
: But [seesaw gyrations]
S
in [the Dow Jones Industrial Average]
X
[yesterday]
X
put [Wall
Street]
O
back in [the spotlight]
X
and inspired [market participants]
O
to bid [the U.S. unit]
S
lower.
d
o
l
l
a
r
y
e
s
t
e
r
d
a
y
s
e
s
s
i
o
n
W
a
l
l
S
t
r
e
e
t
c
o
n
c
e
r
n
m
a
r
k
e
t
s
e
s
s
i
o
n
s
t
r
a
d
e
r
s
r
a
n
g
e
t
o
m
o
r
r
o
w
r
e
p
o
r
t
G
N
P
g
y
r
a
t
i
o
n
s
D
J
I
A
s
p
o
t
l
i
g
h
t
p
a
r
t
i
c
i
p
a
n
t
s
S
1
S X O X - - - - - - - - - - - -
S
2
S - - - S X S X X X S X - - - -
S
3
S X - O - - - - - - - - S X X O
Table 1: The entity grid for the example text with three sentences and eighteen entities. Grid cells
correspond to grammatical roles: subjects (S), objects (O), or neither (X).
to in d, and rows represent the sentences. Each cell corresponds to the grammatical role of an entity in
the corresponding sentence: subject (S), object (O), neither (X), or nothing (?), and an entity is defined
as a class of coreferent noun phrases. If the entity serves in multiple roles in a single sentence, then
we resolve its grammatical role following the priority order: S  O  X  ?. Consider the text in our
previous examples; its entity grid is shown in Table 1, and the entities are highlighted in boldface in the
text above
1
. A local transition is defined as a sequence {S,O,X,?}
n
, representing the occurrence and
grammatical roles of an entity in n adjacent sentences. Such transition sequences can be extracted from
the entity grid as continuous subsequences in each column. For example, the entity dollar in Table 1
has a bigram transition {S,S} from sentence 1 to 2. The entity grid is then encoded as a feature vector
?(d) = (p
1
(d), p
2
(d), . . . , p
m
(d)), where p
t
(d) is the normalized frequency of the transition t in the
entity grid, and m is the number of transitions with length no more than a predefined length k. p
t
(d) is
computed as the number of occurrences of t in the entity grid of document d, divided by the total number
of transitions of the same length. Moreover, entities are differentiated by their salience ? an entity is
deemed to be salient if it occurs at least l times in the text, and non-salient otherwise ? and transitions
are computed separately for salient and non-salient entities.
3.1 Extension: Lin et al.?s Discourse Role Matrix
As mentioned previously, most extensions to B&L?s entity-based local coherence model focus on enrich-
ing the feature set, including the work of Filippova and Strube (2007), Cheung and Penn (2010), Elsner
and Charniak (2011), and Lin et al. (2011). To the best of our knowledge, the only exception is Feng and
Hirst (2012a)?s extension from the perspective of improving the learning procedure.
Among various extensions to B&L?s entity-based local coherence model, the one most related to ours
is Lin et al. (2011)?s work on encoding a text as a set of entities with their associated discourse roles. Lin
et al. observed that coherent texts preferentially follow certain relation patterns. However, simply using
such patterns to measure the coherence of a text can result in feature sparseness. To solve this problem,
they expand the relation sequence into a discourse role matrix, as shown in Table 2. Columns correspond
to the entities in the text and rows represent the contiguous sentences. Each cell
?
E
i
,S
j
?
corresponds to
the set of discourse roles that the entity E
i
serves as in sentence S
j
. For example, the entity yesterday
from S
3
takes part in Arg2 of the last relation, so the cell ?yesterday,S
3
? contains the role Contrast.Arg2.
1
Text elements are considered to be a single entity with multiple mentions if they refer to the same object or concept in the
world, even if they have different textual realizations; e.g., dollar in S
1
and U.S. unit in S
3
refer to the same entity.
943
dollar yesterday session Wall Street concern market
S
1
EntRel.Arg1 EntRel.Arg1 EntRel.Arg1 EntRel.Arg1 nil nil
S
2
EntRel.Arg2
nil nil nil
EntRel.Arg2 EntRel.Arg2
Conj.Arg2 Conj.Arg1 Conj.Arg1
Contrast.Arg1 Contrast.Arg1 Contrast.Arg1
S
3
Contrast.Arg2 Contrast.Arg2 nil Contrast.Arg2 nil nil
Table 2: A fragment of Lin et al.?s PDTB-style discourse role matrix for the example text with the first
six entities across three sentences.
An entry may be empty (with a symbol nil, as in ?yesterday,S
2
?) or contain multiple discourse roles (as in
?dollar,S
2
?). Next, the frequencies of the discourse role transitions of lengths 2 and 3, e.g., EntRel.Arg1
? Conjunction.Arg2 and EntRel.Arg1? nil? Contrast.Arg2, are calculated with respect to the matrix.
For example, the frequency of EntRel.Arg1? Conjunction.Arg2 is 1/24 = 0.042 in Table 2.
4 Methodology
As discussed in Section 1, the main objective of our work is to study the impact of deep hierarchical dis-
course structures in the evaluation of text coherence. In order to conduct a direct comparison between a
model with features derived from deep hierarchical discourse relations and a model with features derived
from shallow discourse relations only, we adopt two separate approaches: (1) We implement a model
with features derived from RST-style discourse relations, and compare it against a model with features
derived from PDTB-style relations. (2) In the framework of RST-style discourse parsing, we deprive the
model of any information from higher-level discourse relations and compare its performance against the
model that uses the complete set of discourse relations. Moreover, as a baseline, we also re-implemented
B&L?s entity-based local coherence model, and we will study the effect of incorporating one of our dis-
course feature sets into this baseline model. Therefore, we have four ways to encode discourse relation
features, namely, entity-based, PDTB-style, full RST-style, and shallow RST-style.
4.1 Entity-based Feature Encoding
In entity-based feature encoding, our goal is to formulate a text into an entity grid, such as the one shown
in Table 1, from which we extract entity-based local transitions. In our re-implementation of B&L, we
use the same parameter settings as B&L?s original model, i.e., the optimal transition length k = 3 and the
salience threshold l = 2. However, when extracting entities in each sentence, e.g., dollar, yesterday, etc.,
we do not perform coreference resolution; rather, for better coverage, we follow the suggestion of Elsner
and Charniak (2011) and extract all nouns (including non-head nouns) as entities. We use the Stanford
dependency parser (de Marneffe et al., 2006) to extract nouns and their grammatical roles. This strategy
of entity extraction also applies to the other three feature encoding methods to be described below.
4.2 PDTB-style Feature Encoding
To encode PDTB-style discourse relations into the model, we parse the texts using an end-to-end PDTB-
style discourse parser
2
developed by Lin et al. (2014). The F
1
score of this parser is around 85% for rec-
ognizing explicit relations and around 40% for recognizing implicit relations. A text is thus represented
by a discourse role matrix in the same way as shown in Table 2. Most parameters in our PDTB-style fea-
ture encoding follow those of Lin et al. (2011): each entity is associated with the fully-fledged discourse
roles, i.e., with type and argument information included; the maximum length of discourse role transi-
tions is 3; and transitions are generated separately for salient and non-salient entities with a threshold set
at 2. However, compared to Lin et al.?s model, there are two differences in our re-implementation, and
evaluated on a held-out development set, these modifications are shown to be effective in improving the
performance.
2
http://wing.comp.nus.edu.sg/
?
linzihen/parser/
944
dollar yesterday session Wall Street concern market
S
1
Background.N Background.N Temporal.S Temporal.S nil nil
Temporal.N Temporal.N
List.N List.N List.N
S
2
Condition.N nil nil nil Contrast.S Contrast.S
Contrast.S
Contrast.N
S
3
Background.N Cause.S nil Cause.S nil nil
Cause.N
Table 3: A fragment of the full RST-style discourse role matrix for the example text with the first six
entities across three sentences.
First, we differentiate between intra- and multi-sentential discourse relations, which is motivated by a
finding in the field of RST-style discourse parsing ? distributions of various discourse relation types are
quite distinct between intra-sentential and multi-sentential instances (Feng and Hirst, 2012b; Joty et al.,
2012) ? and we assume that a similar phenomenon exists for PDTB-style discourse relations. Therefore,
we assign two sets of discourse roles to each entity: intra-sentential and multi-sentential roles, which are
the roles that the entity plays in the corresponding intra- and multi-sentential relations.
Second, instead of Level-1 PDTB discourse relations (6 in total), we use Level-2 relations (18 in total)
in feature encoding, so that richer information can be captured in the model, resulting in 18? 2 = 36
different discourse roles with argument attached. We then generate four separate set of features for the
combination of intra-/multi-sentential discourse relation roles, and salient/non-salient entities, among
which transitions consisting of only nil symbols are excluded. Therefore, the total number of features in
PDTB-style encoding is 4? (36
2
+36
3
?2)? 192K.
4.3 Full RST-style Feature Encoding
For RST-style feature encoding, we parse the texts using an end-to-end RST-style discourse parser de-
veloped by Feng and Hirst (2014), which produces a discourse tree representation for each text, such as
the one shown in Figure 2. For relation labeling, the overall accuracy of this discourse parser is 58%,
evaluated on the RST-DT.
We encode the RST-style discourse relations in a similar fashion to PDTB-style encoding. However,
since the definition of discourse roles depends on the particular discourse framework, here, we adapt Lin
et al.?s PDTB-style encoding by replacing the PDTB-style discourse relations with RST-style discourse
relations, and the argument information (Arg1 or Arg2) by the nuclearity information (nucleus or the
satellite) in an RST-style discourse relation. More importantly, in order to reflect the hierarchical struc-
ture in an RST-style discourse parse tree, when extracting the set of discourse relations that an entity
participates in, we find all those discourse relations that the entity appears in the main EDUs of each
relation
3
and represent the role of the entity in each of these discourse relations. In this way, we can
encode long-distance discourse relations for the most relevant entities. For example, considering the
RST-style discourse tree representation in Figure 2, we encode the Background relation for the entities
dollar and yesterday in S
1
, as well as the entity dollar in S
3
, but not for the remaining entities in the text,
even though the Background relation covers the whole text. The corresponding full RST-style discourse
role matrix for the example text is shown in Table 3.
As in PDTB-style feature encoding, we differentiate between intra- and multi-sentential discourse
relations; we use 17 coarse-grained classes of RST-style relations in feature encoding; the optimal transi-
3
The main EDUs of a discourse relation are the EDUs obtained by traversing the discourse subtree in which the relation of
interest constitutes the root node, following the nucleus branches down to the leaves. For instance, for the RST discourse tree
in Figure 2, the main EDUs of the Background relation on the top level are {e
1
,e
7
}, and the main EDUs of the List relation
among (e
3
-e
5
) are {e
3
,e
4
}.
945
tion length k is 3; and the salience threshold l is 2. The total number of features in RST-style encoding is
therefore 4?(34
2
+34
3
?2)? 162K, which is roughly the same as that in PDTB-style feature encoding.
4.4 Shallow RST-style Feature Encoding
Shallow RST-style encoding is almost identical to full RST-style encoding, as introduced in Section
4.3, except that, when we derive discourse roles, we consider shallow discourse relations only. To be
consistent with the majority of PDTB-style discourse relations, we define shallow discourse relations as
those relations which hold between text spans of the same sentence, or between two adjacent sentences.
For example, in Figure 2, the Background relation between (e
1
-e
2
) and (e
3
-e
7
) is not a shallow discourse
relation (it holds between a single sentence and the concatenation of two sentences), and thus will be
excluded from shallow RST-style feature encoding.
5 Experiments
To evaluate our proposed model with deep discourse structures encoded, we conduct two series of exper-
iments on two different datasets, each of which simulates a sub-task in the evaluation of text coherence,
i.e., sentence ordering and essay scoring. Since text coherence is a matter of degree rather than a bi-
nary classification, in both evaluation tasks we formulate the problem as a pairwise preference ranking
problem. Specifically, given a set of texts with different degrees of coherence, we train a ranker which
learns to prefer a more coherent text over a less coherent counterpart. Accuracy is therefore measured
as the fraction of correct pairwise rankings as recognized by the ranker. In our experiments, we use the
SVM
light
package
4
(Joachims, 1999) with the ranking configuration, and all parameters are set to their
default values.
5.1 Sentence Ordering
The task of sentence ordering, which has been extensively studied in previous work, attempts to simulate
the situation where, given a predefined set of information-bearing items, we need to determine the best
order in which the items should be presented. As argued by Barzilay and Lapata (2005), sentence order-
ing is an essential step in many content-generation components, such as multi-document summarization.
In this task, we use a dataset consisting of a subset of the Wall Street Journal (WSJ) corpus, in which
the minimum length of a text is 20 sentences, and the average length is 41 sentences. For each text, we
create 20 random permutations by shuffling the original order of the sentences. In total, we have 735
source documents and 735?20 = 14,700 permutations. Because the RST-style discourse parser we use
is trained on a fraction of the WSJ corpus, we remove the training texts from our dataset, to guarantee
that the discourse parser will not perform exceptionally well on some particular texts. However, since
the PDTB-style discourse parser we use is trained on almost the entire WSJ corpus, we cannot do the
same for the PDTB-style parser.
In this experiment, our learning instances are pairwise ranking preferences between a source text and
one of its permutations, where the source text is always considered more coherent than its permutations.
Therefore, we have 735?20 = 14,700 total pairwise rankings, and we conduct 5-fold cross-validation on
five disjoint subsets. In each fold, one-fifth of the rankings are used for testing, and the rest for training.
5.2 Essay Scoring
The second task is essay scoring, and we use a subset of International Corpus of Learner English (ICLE)
(Granger et al., 2009). The dataset consists of 1,003 essays about 34 distinct topics, written by university
undergraduates speaking 14 native languages who are learners of English as a Foreign Language. Each
essay has been annotated with an organization score from 1 to 4 at half-point increments by Persing et
al. (2010). We use these organization scores to approximate the degrees of coherence in the essays. The
average length of the essays is 32 sentences, and the average organization score is 3.05, with a standard
deviation of 0.59.
4
http://svmlight.joachis.org/
946
Model sentence ordering essay scoring
No discourse structure Entity 95.1 66.4
Shallow discourse structures
PDTB 97.2 82.2
PDTB&Entity 97.3 83.3
Shallow RST 98.5 87.2
Shallow RST&Entity 98.8 87.2
Deep discourse structures
Full RST 99.1 88.3
Full RST&Entity 99.3 87.7
Table 4: Accuracy (%) of various models on the two evaluation tasks: sentence ordering and essay
scoring. For sentence ordering, accuracy difference is significant with p < .01 for all pairs of models
except between PDTB and PDTB&Entity. For essay scoring, accuracy difference is significant with
p < .01 for all pairs of models except between shallow RST and shallow RST&Entity. Significance is
determined with the Wilcoxon signed-rank test.
In this experiment, our learning instances are pairwise ranking preferences between a pair of essays
on the same topic written by students speaking the same native language, excluding pairs with the same
organization score. In total, we have 22,362 pairwise rankings. Similarly, we conduct 5-fold cross-
validations on these rankings.
In fact, the two datasets used in the two evaluation tasks reflect different characteristics by themselves.
The WSJ dataset, although somewhat artificial due to the permuting procedure, is representative of texts
with well-formed syntax. By contrast, the ICLE dataset, although not artificial, contains occasional
syntactic errors, because the texts are written by non-native English speakers. Therefore, using these two
distinct datasets allows us to evaluate our models in tasks where different challenges may be expected.
6 Results
In this section, we demonstrate the performance of our models with discourse roles encoded in one of
the three ways: PDTB-style, full RST-style or shallow RST-style, and compare against their combination
with our re-implemented B&L?s entity-based local transition features. The evaluation is conducted on
the two tasks, sentence ordering and essay scoring, and the accuracy is reported as the fraction of correct
pairwise rankings averaged over 5-fold cross-validation.
The performance of various models is shown in Table 4. The first section of the table shows the
results of our re-implementation of B&L?s entity-based local coherence model, representing the effect
with no discourse structure encoded. The second section shows the results of four models with shallow
discourse structures encoded, including the two basic models, PDTB-style and shallow RST-style feature
encoding, and their combination with the entity-based feature encoding. The last section shows the
results of our models with deep discourse structures encoded, including the RST-style feature encoding
and its combination with the entity-base feature encoding. With respect to the performance, we observe
a number of consistent patterns across both evaluation tasks.
First, with no discourse structure encoded, the entity-based model (the first row) performs the worst
among all models, suggesting that discourse structures are truly important and can capture coherence in
a more sophisticated way than pure grammatical roles. Moreover, the performance gap is particularly
large for essay scoring, which is probably due to the fact that, as argued by Persing et al. (2010), the
organization score, which we use to approximate the degrees of coherence, is not equivalent to text
coherence. Organization relates more to the logical development in the texts, while coherence is about
lexical and semantic continuity; but discourse relations can capture the logical relations at least to some
extent.
Secondly, with deep discourse structures encoded, the RST-style model in the third section signif-
icantly outperforms (p < .01) the models with shallow discourse structures, i.e., the PDTB-style and
947
shallow RST-style models in the middle section, confirming our intuition that deep discourse structures
are more powerful than shallow structures. This is also the case when entity-based features are included.
Finally, considering the models in the middle section of the table, we can gain more insight into the
difference between PDTB-style and RST-style encoding. As can be seen, even without information from
the more powerful deep hierarchical discourse structures, shallow RST-style encoding still significantly
outperforms PDTB-style encoding on both tasks (p < .01). This is primarily due to the fact that the
discourse relations discovered by RST-style parsing have wider coverage of the text
5
, and thus induce
richer information about the text. Therefore, because of its ability to annotate deep discourse structures
and its better coverage of discourse relations, RST-style discourse parsing is generally more powerful
than PDTB-style parsing, as far as coherence evaluation is concerned.
However, with respect to combining full RST-style features with entity features, we have contradictory
results on the two tasks: for sentence ordering, the combination is significantly better than each single
model, while for essay scoring, the combination is worse than using RST-style features alone. This is
probably related to the previously discussed issue of using entity-based features for essay scoring, due to
the subtle difference between coherence and organization.
7 Conclusion and Future Work
In this paper, we have studied the impact of deep discourse structures in the evaluation of text coher-
ence by two approaches. In the first approach, we implemented a model with discourse role features
derived from RST-style discourse parsing, which represents deep discourse structures, and compared it
against our re-implemented Lin et al. (2011)?s model derived from PDTB-style parsing, with no deep
discourse structures annotated. In the second approach, we compared our complete RST-style model
against a model with shallow RST-style encoding. Evaluated on the two tasks, sentence ordering and
essay scoring, deep discourse structures are shown to be effective for better differentiation of text coher-
ence. Moreover, we showed that, even without deep discourse structures, shallow RST-style encoding is
more powerful than PDTB-style encoding, because it has better coverage of discourse relations in texts.
Finally, combining discourse relations with entity-based features is shown to have an inconsistent effect
on the two evaluation tasks, which is probably due to the different nature of the two tasks.
In our future work, we wish to explore the effect of automatic discourse parsers in our methodology.
As discussed previously, the PDTB- and RST-style discourse parsers used in our experiments are far from
perfect. Therefore, it is possible that using automatically extracted discourse relations creates some bias
to the training procedure; it is also possible that what our model actually learns is the distribution over
those discourse relations which automatic discourse parsers are mostly confident with, and thus errors (if
any) made on other relations do not matter. One potential way to verify these two possibilities is to study
the effect of each particular type of discourse relation to the resulting model, and we leave it for future
exploration.
Acknowledgements
We thank the reviewers for their valuable advice and comments. This work was financially supported by
the Natural Sciences and Engineering Research Council of Canada and by the University of Toronto.
References
Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proceedings
of the 42rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 141?148.
Jackie Chi Kit Cheung and Gerald Penn. 2010. Entity-based local coherence modelling using topological fields.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages
186?195.
5
The entire text is covered by the annotation produced by RST-style discourse parsing, while this is generally not true for
PDTB-style discourse parsing.
948
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources
and Evaluation (LREC 2006).
Micha Elsner and Eugene Charniak. 2011. Extending the entity grid with entity-specific features. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011), pages 125?129.
Vanessa Wei Feng and Graeme Hirst. 2012a. Extending the entity-based coherence model with multiple ranks. In
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics
(EACL 2012), pages 315?324, Avignon, France.
Vanessa Wei Feng and Graeme Hirst. 2012b. Text-level discourse parsing with rich linguistic features. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 60?68,
Jeju, Korea.
Vanessa Wei Feng and Graeme Hirst. 2014. A linear-time bottom-up discourse parser with constraints and post-
editing. In Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics (ACL
2014), Baltimore, USA, June.
Katja Filippova and Michael Strube. 2007. Extending the entity-grid coherence model to semantically related
entities. In Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG 2007),
pages 139?142.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot. 2009. International Corpus of Learner
English (Version 2). Presses universitaires de Louvain.
Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Sch?olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods ? Support Vector Learning, chapter 11, pages 169?184. MIT Press, Cam-
bridge, MA.
Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng. 2012. A novel discriminative framework for sentence-level
discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, EMNLP-CoNLL 2012, pages 904?915.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse
relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (ACL 2011), Portland, Oregon, USA, June.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 2:151?184.
William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Isaac Persing, Alan Davis, and Vincent Ng. 2010. Modeling organization in student essays. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229?239, Cambridge, MA,
October. Association for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation (LREC 2008).
949
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 12?23,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Discourse Analysis for Article-Wide Temporal Classification
Jun-Ping Ng1, Min-Yen Kan1,2, Ziheng Lin3, Wei Feng4, Bin Chen5, Jian Su5, Chew-Lim Tan1
1School of Computing, National University of Singapore, Singapore
2Interactive and Digital Media Institute, National University of Singapore, Singapore
3Research & Innovation, SAP Asia Pte Ltd, Singapore
4Department of Computer Science, University of Toronto, Canada
5Institute for Infocomm Research, Singapore
junping@comp.nus.edu.sg
Abstract
In this paper we classify the temporal relations
between pairs of events on an article-wide ba-
sis. This is in contrast to much of the exist-
ing literature which focuses on just event pairs
which are found within the same or adjacent
sentences. To achieve this, we leverage on dis-
course analysis as we believe that it provides
more useful semantic information than typical
lexico-syntactic features. We propose the use
of several discourse analysis frameworks, in-
cluding 1) Rhetorical Structure Theory (RST),
2) PDTB-styled discourse relations, and 3)
topical text segmentation. We explain how
features derived from these frameworks can be
effectively used with support vector machines
(SVM) paired with convolution kernels. Ex-
periments show that our proposal is effective
in improving on the state-of-the-art signifi-
cantly by as much as 16% in terms of F1, even
if we only adopt less-than-perfect automatic
discourse analyzers and parsers. Making use
of more accurate discourse analysis can fur-
ther boost gains to 35%.
1 Introduction
A good amount of research had been invested in un-
derstanding temporal relationships within text. Par-
ticular areas of interest include determining the re-
lationship between an event mention and a time ex-
pression (timex), as well as determining the relation-
ship between two event mentions. The latter, which
we refer to as event-event (E-E) temporal classifica-
tion is the focus of this work.
For a given event pair which consists of two
events e1 and e2 found anywhere within an article,
we want to be able to determine if e1 happens be-
fore e2 (BEFORE), after e2 (AFTER), or within the
same time span as e2 (OVERLAP).
Consider this sentence1:
At least 19 people were killed and 114 people were
wounded in Tuesday?s southern Philippines airport blast,
officials said, but reports said the death toll could climb
to 30.
(1)
Three event mentions found within the sentence are
bolded. We say that there is an OVERLAP rela-
tionship between the ?killed ? wounded? event pair
as these two events happened together after the air-
port blast. Similarly there is a BEFORE relationship
between both the ?killed ? said?, and ?wounded ?
said? event pairs, as the death and injuries happened
before reports from the officials.
Being able to infer these temporal relationships
allows us to build up a better understanding of the
text in question, and can aid several natural lan-
guage understanding tasks such as information ex-
traction and text summarization. For example, we
can build up a temporal characterization of an article
by constructing a temporal graph denoting the rela-
tionships between all events within an article (Ver-
hagen et al, 2009). This can then be used to help
construct an event timeline which layouts sequen-
tially event mentions in the order they take place (Do
et al, 2012). The temporal graph can also be used
in text summarization, where temporal order can be
used to improve sentence ordering and thereby the
eventual generated summary (Barzilay et al, 2002).
Given the importance and value of temporal re-
lations, the community has organized shared tasks
1From article AFP ENG 20030304.0250 of the ACE 2005
corpus (ACE, 2005).
12
to spur research efforts in this area, including the
TempEval-1, -2 and -3 evaluation workshops (Ver-
hagen et al, 2009; Verhagen et al, 2010; Uzzaman
et al, 2012). Most related work in this area have
focused primarily on the task defintitions of these
evaluation workshops. In the task definitions, E-
E temporal classification involves determining the
relationship between events found within the same
sentence, or in adjacent sentences. For brevity we
will refer to this loosely as intra-sentence E-E tem-
poral classification in the rest of this paper.
This definition however is limiting and insuffi-
cient. It was adopted as a trade-off between com-
pleteness, and the need to simplify the evaluation
process (Verhagen et al, 2009). In particular, one
deficiency is that it does not allow us to construct the
complete temporal graph we seek. As illustrated in
Figure 1, being able to perform only intra-sentence
E-E temporal classification may result in a forest of
disconnected temporal graphs. A sentence s3 sepa-
rates events C and D, as such an intra-sentence E-E
classification system will not be able to determine
the temporal relationship between them. While we
can determine the relationship between A and C in
the figure with the use of temporal transitivity rules
(Setzer et al, 2003; Verhagen, 2005), we cannot re-
liably determine the relationship between say A and
D.
A
B C
D E
s1
s2
s3
s4
Figure 1: A disconnected temporal graph of events within
an article. Horizontal lines depict sentences s1 to s4, and
the circles identify events of interest.
In this work, we seek to overcome this limitation,
and study what can enable effective article-wide E-E
temporal classification. That is, we want to be able
to determine the temporal relationship between two
events located anywhere within an article.
The main contribution of our work is going
beyond the surface lexical and syntactic features
commonly adopted by existing state-of-the-art ap-
proaches. We suggest making use of semantically
motivated features derived from discourse analysis
instead, and show that these discourse features are
superior.
While we are just focusing on E-E temporal
classification, our work can complement other ap-
proaches such as the joint inference approach pro-
posed by Do et al (2012) and Yoshikawa et al
(2009) which builds on top of event-timex (E-T) and
E-E temporal classification systems. We believe that
improvements to the underlying E-T and E-E classi-
fication systems will help with global inference.
2 Related Work
Many researchers have worked on the E-E temporal
classification problem, especially as part of the Tem-
pEval series of evaluation workshops. Bethard and
Martin (2007) presented one of the earliest super-
vised machine learning systems, making use of sup-
port vector machines (SVM) with a variety of lexical
and syntactic features. Kolya et al (2010) described
a conditional random field (CRF) based learner mak-
ing use of similar features. Other researchers includ-
ing Uzzaman and Allen (2010) and Ha et al (2010)
made use of Markov Logic Networks (MLN). By
leveraging on the transitivity properties of temporal
relationships (Setzer et al, 2003), they found that
MLNs are useful in inferring new temporal relation-
ships from known ones.
Recognizing that the temporal relationships be-
tween event pairs and time expressions are related,
Yoshikawa et al (2009) proposed the use of a joint
inference model and showed that improvements in
performance are obtained. However this gain is at-
tributed to the joint inference model they had devel-
oped, making use of similar surface features.
To the best of our knowledge, the only piece
of work to have gone beyond sentence boundaries
and tackle the problem of article-wide E-E temporal
classification is by Do et al (2012). Making use of
integer linear programming (ILP), they built a joint
inference model which is capable of classifying tem-
poral relationships between any event pair within
a given document. They also showed that event
co-reference information can be useful in determin-
ing these temporal relationships. However they did
not make use of features directed specifically at de-
termining the temporal relationships of event pairs
13
across different sentences. Other than event co-
reference information, they adopted the same mix
of lexico-syntactic features.
Underlying these disparate data-driven methods
for similar temporal processing tasks, the reviewed
works all adopted a similar set of surface fea-
tures including vocabulary features, part-of-speech
tags, constituent grammar parses, governing gram-
mar nodes and verb tenses, among others. We ar-
gue that these features are not sufficiently discrimi-
native of temporal relationships because they do not
explain how sentences are combined together, and
thus are unable to properly differentiate between the
different temporal classifications. Supporting our
argument is the work of Smith (2010), where she
argued that syntax cannot fully account for the un-
derlying semantics beneath surface text. D?Souza
and Ng (2013) found out as much, and showed that
adopting richer linguistic features such as lexical re-
lations from curated dictionaries (e.g. Webster and
WordNet) as well as discourse relations help tempo-
ral classification. They had shown that the Penn Dis-
course TreeBank (PDTB) style (Prasad et al, 2008)
discourse relations are useful. We expand on their
study to assess the utility of adopting additional dis-
course frameworks as alternative and complemen-
tary views.
3 Making Use of Discourse
To highlight the deficiencies of surface features, we
quote here an example from Lascarides and Asher
(1993):
[A] Max opened the door. The room was pitch dark.
[B] Max switched off the light. The room was pitch dark.
(2)
The two lines of text A and B in Example 2 have
similar syntactic structure. Given only syntactic fea-
tures, we may be drawn to conclude that they share
similar temporal relationships. However in the first
line of text, the events temporally OVERLAP, while
in the second line they do not. Clearly, syntax alone
is not going to be useful to help us arrive at the cor-
rect temporal relations.
If existing surface features are insufficient, what is
sufficient? Given a E-E pair which crosses sentence
boundaries, how can we determine the temporal re-
lationship between them? We take our cue from the
work of Lascarides and Asher (1993). They sug-
gested instead that discourse relations hold the key
to interpreting such temporal relationships.
Building on their observations, we believe that
discourse analysis is integral to any solution for the
problem of article-wide E-E temporal classification.
We thus seek to exploit a series of different discourse
analysis studies, including 1) the Rhetorical Struc-
ture Theory (RST) discourse framework, 2) Penn
Discourse Treebank (PDTB)-styled discourse rela-
tions based on the lexicalized Tree Adjoining Gram-
mar for Discourse (D-LTAG), and 3) topical text seg-
mentation, and validate their effectiveness for tem-
poral classification.
RST Discourse Framework. RST (Mann and
Thompson, 1988) is a well-studied discourse anal-
ysis framework. In RST, a piece of text is split into a
sequence of non-overlapping text fragments known
as elementary discourse units (EDUs). Neighboring
EDUs are related to each other by a typed relation.
Most RST relations are hypotactic, where one of the
two EDUs participating in the relationship is demar-
cated as a nucleus, and the other a satellite. The nu-
cleus holds more importance, from the point of view
of the writer, while the satellite?s purpose is to pro-
vide more information to help with the understand-
ing of the nucleus. Some RST relations are however
paratactic, where the two participating EDUs are
both marked as nuclei. A discourse tree can be com-
posed by viewing each EDU as a leaf node. Nodes
in the discourse tree are linked to one another via the
discourse relations that hold between the EDUs.
RST discourse relations capture the semantic re-
lation between two EDUs, and these often offer a
clue to the temporal relationship between events in
the two EDUs too. As an example, let us refer once
again to Example 2. Recall that in the second line of
text ?switched off? happens BEFORE ?dark?. The
RST discourse structure for the second line of text
is shown on the left of Figure 2. We see that the
two sentences are related via a ?Result? discourse
relation. This fits our intuition that when there is
causation, there should be a BEFORE/AFTER rela-
tionship. The RST discourse relation in this case is
very useful in helping us determine the relationship
between the two events.
PDTB-styled Discourse Relations. Another widely
adopted discourse relation annotation is the PDTB
framework (Prasad et al, 2008). Unlike the RST
14
Max switched off the light. The room was pitch dark.
RESULT
The room was pitch dark.
CONTINGENCY :: CAUSE
arg1 arg2
Max switched off the light.
Figure 2: RST and PDTB discourse structures for the second line of text in Example 2. The structure on the left is the
RST discourse structure, while the structure on the right is for PDTB.
framework, the discourse relations in PDTB build on
the work on D-LTAG by Webber (2004), a lexicon-
grounded approach to discourse analysis. Practi-
cally, this means that instead of starting from a pre-
identified set of discourse relations, PDTB-styled
annotations are more focused on detecting possible
connectives (can be either explicit or implicit) within
the text, before identifying the text fragments which
they connect and how they are related to one another.
Applied again to the second line of text we have in
Example 2, we get a structure as shown on the right
side of Figure 2. From the figure we can see that
the two sentences are related via a ?Cause? relation-
ship. Similar to what we have explained earlier for
the case of RST, the presence of a causal effect here
strongly hints to us that events in the two sentences
share a BEFORE/AFTER relationship.
At this point we want to note the differences be-
tween the use of the RST framework and PDTB-
styled discourse relations in the context of our work.
The theoretical underpinnings behind these two dis-
course analysis are very different, and we believe
that they can be complementary to each other. First,
the RST framework breaks up text within an article
linearly into non-overlapping EDUs. Relations can
only be defined between neighboring EDUs. How-
ever this constraint is not found in PDTB-styled re-
lations, where a text fragment can participate in one
discourse relation, and a subsequence of it partic-
ipate in another. PDTB relations are also not re-
stricted only to adjacent text fragments. In this as-
pect, the flexibility of the PDTB relations can com-
plement the seemingly more rigid RST framework.
Second, with PDTB-styled relations not every
sentence needs to be in a relation with another as
the PDTB framework does not aim to build a global
discourse tree that covers all sentence pairs. This is
a problem when we need to do an article-wide anal-
ysis. The RST framework does not suffer from this
limitation however as we can build up a discourse
tree connecting all the text within a given article.
Topical Text Segmentation. A third complemen-
tary type of inter-sentential analysis is topical text
segmentation. This form of segmentation separates
a piece of text into non-overlapping segments, each
of which can span several sentences. Each segment
represents passages or topics, and provides a coarse-
grained study of the linear structure of the text (Sko-
rochod?Ko, 1972; Hearst, 1994). The transition be-
tween segments can represent possible topic shifts
which can provide useful information about tempo-
ral relationships.
Referring to Example 32, we have delimited the
different lines of text into segments with parenthe-
ses along with a subscript. Segment (1) talks about
the casualty numbers seen at a medical centre, while
Segment (2) provides background information that
informs us a bomb explosion had taken place. The
segment boundary signals to us a possible temporal
shift and can help us to infer that the bombing event
took place BEFORE the deaths and injuries had oc-
curred.
(The Davao Medical Center, a regional government hos-
pital, recorded 19 deaths with 50 wounded. Medical
evacuation workers however said the injured list was
around 114, spread out at various hospitals.)1
(A powerful bomb tore through a waiting shed at the
Davao City international airport at about 5.15 pm (0915
GMT) while another explosion hit a bus terminal at the
city.)2
(3)
4 Methodology
Having motivated the use of discourse analysis for
our problem, we now proceed to explain how we can
make use of them for temporal classification. The
different facets of discourse analysis that we are ex-
ploring in this work are structural in nature. RST
2From article AFP ENG 20030304.0250 of the ACE 2005
corpus.
15
EDU2 EDU3
r2
r1
EDU1
A
B
Figure 3: A possible RST discourse tree. The two circles
denote two events A and B which we are interested in.
t1 t2
t3
t4
r1 r2
r3
B
A
Figure 4: A possible PDTB-styled discourse annotation
where the circles represent events we are interested in.
and PDTB discourse relations are commonly repre-
sented as graphs, and we can also view the output
of text segmentation as a graph with individual text
segments forming vertices, and the transitions be-
tween them forming edges.
Considering this, we propose the use of support
vector machines (SVM), adopting a convolution ker-
nel (Collins and Duffy, 2001) for its kernel function
(Vapnik, 1999; Moschitti, 2006). The use of convo-
lution kernels allows us to do away with the exten-
sive feature engineering typically required to gener-
ate flat vectorized representations of features. This
process is time consuming and demands specialized
knowledge to achieve representations that are dis-
criminative, yet are sufficiently generalized. Con-
volution kernels had also previously been shown to
work well for the related problem of E-T temporal
classification (Ng and Kan, 2012), where the fea-
tures adopted are similarly structural in nature.
We now describe our use of the discourse analysis
frameworks to generate appropriate representations
for input to the convolution kernel.
RST Discourse Framework. Recall that the RST
framework provides us with a discourse tree for an
entire input article. In recent years several automatic
RST discourse parsers have been made available. In
our work, we first make use of the parser by Feng
and Hirst (2012) to obtain a discourse tree represen-
tation of our input. To represent the meaningful por-
tion of the resultant tree, we encode path information
between the two sentences of interest.
We illustrate this procedure using the example
discourse tree illustrated in Figure 3. EDUs includ-
ing EDU1 to EDU3 form the vertices while dis-
course relations r1 and r2 between the EDUs form
the edges. For a E-E pair, {A,B}, we can obtain a
feature structure by first locating the EDUs within
which A and B are found. A is found inside EDU1
and B is found within EDU3. We trace the short-
est path between EDU1 and EDU3, and use this
path as the feature structure for the E-E pair, i.e.
{r1 ? r2}.
PDTB-styled Discourse Relations. We make use of
the automatic PDTB discourse parser from Lin et al
(2013) to obtain the discourse relations over an input
article. Similar to how we work with the RST dis-
course framework, for a given E-E pair, we retrieve
the relevant text fragments and use the shortest path
linking the two events as a feature structure for our
convolution kernel classifier.
An example of a possible PDTB-styled discourse
annotation is shown in Figure 4. The horizontal
lines represent different sentences in an article. The
parentheses delimit text fragments, t1 to t4, which
have been identified as arguments participating in
discourse relations, r1 to r3. For a given E-E pair
{A,B}, we use the trace of the shortest path be-
tween them i.e. {r1 ? r2} as a feature structure.
We take special care to regularize the input (as,
unlike EDUs in RST, arguments to different PDTB
relations may overlap, as in r2 and r3). We model
each PDTB discourse annotation as a graph and em-
ploy Dijkstra?s shortest path algorithm. The graph
resulting from the annotation in Figure 4 is given in
Figure 5. Each text fragment ti maps to a vertex
ni in the graph. PDTB relations between text frag-
ments form edges between corresponding vertices.
As r2 relates t2 to both t3 and t4, two edges link
up n2 to the corresponding vertices n3 and n4 re-
spectively. By doing this, Dijkstra?s algorithm will
always allow us to find the desired shortest path.
n1 n2 n3 n4
r1
r2 r3
r2
Figure 5: Graph derived from discourse annotation in
Figure 4.
16
Topical Text Segmentation. Taking as input a com-
plete text article, we make use of the state-of-the-art
text segmentation system from Kazantseva and Sz-
pakowicz (2011). The output of the system is a se-
ries of non-overlapping, linear text segments, which
we can number sequentially.
In Figure 6 the horizontal lines represent sen-
tences. Parentheses with subscripts mark out the
segment boundaries. We can see two segments s1
and s2 here. Given a target E-E pair {A,B} (repre-
sented as circles inside the figure), we identify the
segment number of the corresponding segment in
which each of A and B is found. We build a fea-
ture structure with the identified segment numbers,
i.e. {s1 ? s2} to capture the segmentation.
A
B
s1
s2
Figure 6: A possible segmentation of three sentences into
two segments.
5 Results
We conduct a series of experiments to validate the
utility of our proposed features.
Data Set. We make use of the same data set built
by Do et al (2012). The data set consists of 20
newswire articles which originate from the ACE
2005 corpus (ACE, 2005). Initially, the data set
consist of 324 event mentions, and a total of 375
annotated E-E pairs. We perform the same temporal
saturation step as described in Do et al (2012), and
obtained a total of 7,994 E-E pairs3.
A breakdown of the number of instances by each
temporal classes is shown in Table 1. Unlike earlier
data sets such as that for TempEval-2 where more
than half (about 55%) of test instances belong to the
3Though we have obtained the data set from the original au-
thors, there was a discrepancy in the number of E-E pairs. The
original paper reported a total of 376 annotated E-E pairs. Be-
sides this, we also repeated the saturation steps iteratively until
no new relationship pairs are generated. We believe this to be
an enhancement as it ensures that all inferred temporal relation-
ships are generated.
OVERLAP class, OVERLAP instances make up just
10% of the data set.
This difference is due mainly to the fact that our
data set consists not only of intra-sentence E-E pairs,
but also of article-wide E-E pairs. Figure 7 shows
the number of instances for each temporal class bro-
ken down by the number of sentences (i.e. sentence
gap) that separate the events within each E-E pair.
We see that as the sentence gap increases, the pro-
portion of OVERLAP instances decreases. The in-
tuitive explanation for this is that when event men-
tions are very far apart in an article, it becomes more
unlikely that they happen within the same time span.
Class AFTER BEFORE OVERLAP
# E-E pairs 3,588 (45%) 3,589 (45%) 815 (10%)
Table 1: Number of E-E pairs in data set attributable to
each temporal class. Percentages shown in parentheses.
Figure 7: Breakdown of number of E-E pairs for each
temporal class based on sentence gap.
Experiments. The work done in Do et al (2012) is
highly related to our experiments, and so we have
reported the relevant results for local E-E classifi-
cation in Row 1 of Table 2 as a reference. While
largely comparable, note that a direct comparison is
not possible because 1) the number of E-E instances
we have is slightly different from what was reported,
and 2) we do not have access to the exact partitions
they have created for 5-fold cross-validation.
As such, we have implemented a baseline adopt-
ing similar surface lexico-syntactic features used in
previous work (Mani et al, 2006; Bethard and Mar-
tin, 2007; Ng and Kan, 2012; Do et al, 2012), in-
cluding 1) part-of-speech tags, 2) tenses, 3) depen-
dency parses, 4) relative position of events in article,
17
System Precision Recall F1
(1) DO2012 43.86 52.65 47.46
(2) BASE 59.55 38.14 46.50
(3) BASE + RST + PDTB + TOPICSEG 71.89 41.99 53.01
(4) BASE + RST + PDTB + TOPICSEG + COREF 75.23 43.58 55.19
(5) BASE + O-RST + PDTB + O-TOPICSEG + O-COREF 78.35 54.24 64.10
Table 2: Macro-averaged results obtained from our experiments. The difference in F1 scores between each successive
row is statistically significant, but a comparison is not possible between rows (1) and (2).
5) the number of sentences between the target events
and 6) VerbOcean (Chklovski and Pantel, 2004) re-
lations between events. This baseline system, and
the subsequent systems we will describe, comprises
of three separate one-vs-all classifiers for each of the
temporal classes. The result obtained by our base-
line is shown in Row 2 (i.e. BASE) in Table 2. We
note that our baseline is competitive and performs
similarly to the results obtained by Do et al (2012).
However as we do not have the raw judgements from
Do?s system, we cannot test for statistical signifi-
cance.
We also implemented our proposed features and
show the results obtained in the remaining rows of
Table 2. In Row 3, RST denotes the RST discourse
feature, PDTB denotes the PDTB-styled discourse
features, and TOPICSEG denotes the text segmen-
tation feature. Compared to our own baseline, there
is a relative increase of 14% in F1, which is statis-
tically significant when verified with the one-tailed
Student?s paired t-test (p < 0.01).
In addition, Do et al (2012) have shown the value
of event co-reference. Therefore we have also in-
cluded this feature by making use of an automatic
event co-reference system by Chen et al (2011).
The result obtained after adding this feature (de-
noted by COREF) is shown in Row 4. The relative in-
crease in F1 of about 4% from Row 3 is statistically
significant (p < 0.01) and affirms that event co-
reference is a useful feature to have, together with
our proposed features. We note that our complete
system in Row 4 gives a 16% improvement in F1,
relative to the reference system DO2012 in Row 1.
To get a better idea of the performance we can ob-
tain if oracular versions of our features are available,
we also show the results obtained if hand-annotated
RST discourse structures, text segments, as well as
event co-reference information were used. Annota-
tions for the RST discourse structures and text seg-
ments were performed by the first author (RST an-
notations were made following the annotation guide-
lines given by Carlson and Marcu (2001)). Oracular
event co-reference information was included in the
dataset that we have used.
In Row 5 the prefix O denotes oracular versions
of the features we had proposed. From the results
we see that there is a marked increase of over 15%
in F1 relative to Row 4. Compared to Do?s state-of-
the-art system, there is also a relative gain of at least
35%. These oracular results further confirm the im-
portance of non-local discourse analysis for tempo-
ral processing.
6 Discussion
Ablation tests. We performed ablation tests to as-
sess the efficacy of the discourse features used in
our earlier experiments. Starting from the full sys-
tem, we dropped each discourse feature in turn to see
the effect this has on overall system performance.
Our test is performed over the same data set, again
with 5-fold cross-validation. The results in Table 3
show a statistically significant (based on the one-
tailed Student?s paired t-test) drop in F1 in each case,
which proves that each of our proposed features is
useful and required.
From the ablation tests, we also observe that the
RST discourse feature contributes the most to over-
all system performance while the PDTB discourse
feature contributes the least. However we should not
conclude prematurely that the former is more use-
ful than the latter; as the results are obtained using
parses from automatic systems, and are not reflec-
tive of the full utility of ground truth discourse an-
notations.
Useful Relations. The ablation test results showed
us that discourse relations (in particular RST dis-
18
Figure 8: Proportion of occurence in temporal classes for every RST and PDTB relation.
Ablated Feature Change in F1 Sig
?RST -9.03 **
?TOPICSEG -2.98 **
?COREF -2.18 **
?PDTB -1.42 *
Table 3: Ablation test results. ?**? and ?*? denote statis-
tically significance against the full system with p < 0.01
and p < 0.05, respectively.
course relations) are the most important in our sys-
tem. We have also motivated our work earlier with
the intuition that certain relations such as the RST
?Result? and the PDTB ?Cause? relations provide
very useful temporal cues. We now offer an intro-
spection into the use of these discourse relations.
Figure 8 illustrates the relative proportion of tem-
poral classes in which each RST and PDTB re-
lation appear. If the relations are randomly dis-
tributed, we should expect their distribution to fol-
low that of the temporal classes as shown in Table 1.
However we see that many of the relations do not
follow this distribution. For example, we observe
that several relations such as the RST ?Condition?
and PDTB ?Cause? relations are almost exclusively
found within AFTER and BEFORE event pairs only,
while the RST ?Manner-means? and PDTB ?Syn-
chrony? relations occur in a disproportionately large
number of OVERLAP event pairs. These relations
are likely useful in disambiguating between the dif-
ferent temporal classes.
To verify this, we examine the convolution tree
fragments that lie on the support vector of our SVM
classifier. The work of Pighin and Moschitti (2010)
in linearizing kernel functions allows us to take a
look at these tree fragments. Applying the lineariza-
tion process leads to a different classifier from the
one we have used. The identified tree fragments are
therefore just an approximation to those actually em-
ployed by our classifier. However, this analysis still
offers an introspection as to what relations are most
influential for classification.
BEFORE OVERLAP
B1 (Temporal ... O1 (Manner-means ...
B2 (Temporal (Elaboration ...
B3 (Condition (Explanation ...
B4 (Condition (Attribution ...
B5 (Elaboration (Bckgrnd ...
Table 4: Subset of top RST discourse fragments on sup-
port vectors identified by linearizing kernel function.
Table 4 shows a subset of the top RST discourse
fragments identified for the BEFORE and OVER-
LAP one-vs-all classifiers. The list is in line with
what we expect from Figure 8. The former consists
of fragments containing relations such as ?Tempo-
ral? and ?Condition?, while the latter has a sole frag-
ment containing ?Manner-Means?.
To illustrate what these fragments may mean, we
show several example sentences from our data set
in Example 4. Sentence A consists of the tree frag-
ment B1, i.e. ?(Temporal...?. Its corresponding dis-
course structure is illustrated in the top half of Fig-
ure 9. This fragment indicates to us (correctly) that
the event ?wielded? happened BEFORE Milosevic
was ?swept out? of power. Sentence B is made
up of tree fragment O1, i.e. ?(Manner-means...?,
19
and its discourse structure is shown in the bottom
half of Figure 9. As with the previous example, the
fragment suggests (correctly) that there should be a
OVERLAP relationship for the ?requested ? said?
event pair.
[A] Milosevic and his wife wielded enormous power in
Yugoslavia for more than a decade before he was swept
out of power after a popular revolt in October 2000.
[B] The court order was requested by Jack Welch?s at-
torney, Daniel K. Webb, who said Welch would likely be
asked about his business dealings, his health and entries
in his personal diary.
(4)
Milosevic ? wielded? 
a decade 
before.. swept out.. 
power
after a?  October
2000.
temporal
temporal
The court? requested
by Jack .. Webb,
elaboration
who said Welch would ?
diary.
attribution
manner-means
Figure 9: RST discourse structures for sentences A (top
half) and B (bottom half) in Example 4.
Segment Numbers. From the ablation test results,
text segmentation is the next most important feature
after the RST discourse feature. This is interesting
given that the defined feature structure for topical
text segmentation is not the most intuitive. By us-
ing actual segment numbers, the structure may not
generalize well for articles of different lengths for
example, as each article may have vastly different
number of segments. The transition across segments
may also not carry the same semantic significance
for different articles.
Our experiments have however shown that this
feature design is useful in improving performance.
This is likely because:
1. The default settings of the text segmentation
system we had used are such that precision is
favoured over recall (Kazantseva and Szpakow-
icz, 2011, p. 292). As such there is just an aver-
age of between two to three identified segments
per article. This makes the feature more gener-
alizable despite making use of actual segment
numbers.
2. The style of writing in newswire articles which
we are experimenting on generally follows
common journalistic guidelines. The semantics
behind the transitions across the coarse-grained
segments that were identified are thus likely to
be of a similar nature across many different ar-
ticles.
We leave for future work an investigation into
whether more fine-grained topic segments can lead
to further performance gains. In particular, it will be
interesting to study if work on argumentative zoning
(Teufel and Kan, 2011) can be applied to newswire
articles, and whether the subsequent learnt docu-
ment structures can be used to delineate topic seg-
ments more accurately.
Error Analysis. Besides examining the features we
had used, we also want to get a better idea of the er-
rors made by our classifier. Recall that we are using
separate one-vs-all classifiers for each of the tempo-
ral classes, so each of the three classifiers generates
a column in the aggregate confusion matrix shown
in Table 5. In cases where none of the SVM clas-
sifiers return a positive confidence value, we do not
assign a temporal class (captured as column N). The
high number of event pairs which are not assigned to
any temporal class explains the lower recall scores
obtained by our system, as observed in Table 2.
Predicted
O B A N
O 119 (14.7%) 114 (14.1%) 104 (12.8%) 474 (58.5%)
B 19 (0.5%) 2067 (57.9%) 554 (15.5%) 928 (26.0%)
A 16 (0.5%) 559 (15.7%) 2046 (57.3%) 947 (26.5%)
Table 5: Confusion matrix obtained for the full system,
classifying into (O)VERLAP, (B)EFORE, (A)FTER, and
(N)o result.
Additionally, an interesting observation is the low
percentage of OVERLAP instances that our classi-
fier managed to predict correctly. About 57% of
BEFORE and AFTER instances are classified cor-
20
rectly, however only about 15% of OVERLAP in-
stances are correct.
Figure 10 offers more evidence to suggest that
our classifier works better for the BEFORE and AF-
TER classes than the OVERLAP class. We see that
as sentence gap increases, we achieve a fairly con-
sistent performance for both BEFORE and AFTER
instances. OVERLAP instances are concentrated
where the sentence gap is less than 7, with the best
accuracy figure coming in below 30%.
Although not definitive, this may be because our
data set consists of much fewer OVERLAP in-
stances than the other two classes. This bias may
have led to insufficient training data for accurate
OVERLAP classification. It will be useful to inves-
tigate if using a more balanced data set for training
can help overcome this problem.
Figure 10: Accuracy of the classifer for each temporal
class, plotted against the sentence gap of each E-E pair.
7 Conclusion
We believe that discourse features play an important
role in the temporal ordering of events in text. We
have proposed the use of different discourse anal-
ysis frameworks and shown that they are effective
for classifying the temporal relationships of article-
wide E-E pairs. Our proposed discourse-based fea-
tures are robust and work well even though auto-
matic discourse analysis is noisy. Experiments fur-
ther show that improvements to these underlying
discourse analysis systems will benefit system per-
formance.
In future work, we will like to explore how to
better exploit the various discourse analysis frame-
works for temporal classification. For instance, RST
relations are either hypotactic or paratactic. Marcu
(1997) made use of this to generate automatic sum-
maries by considering EDUs which are nuclei to be
more salient. We believe it is interesting to examine
how such information can help. We are also inter-
ested to apply discourse features in the context of a
global inferencing system (Yoshikawa et al, 2009;
Do et al, 2012), as we think such analyses will also
benefit these systems as well.
Acknowledgments
We like to express our gratitude to Quang Xuan Do,
Wei Lu, and Dan Roth for generously making avail-
able the data set they have used for their work in
EMNLP 2012. We would also like to thank the
anonymous reviewers who reviewed this paper for
their valuable feedback.
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
ACE. 2005. The ACE 2005 (ACE05) Evaluation Plan.
October.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring Strategies for Sentence Order-
ing in Multidocument News Summarization. Journal
of Artificial Intelligence Research (JAIR), 17:35?55.
Steven Bethard and James H. Martin. 2007. CU-TMP:
Temporal Relation Classification Using Syntactic and
Semantic Features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval),
pages 129?132, June.
Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging manual. Technical Report ISI-TR-545, Informa-
tion Sciences Institute, University of Southern Califor-
nia, July.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.
2011. A Unified Event Coreference Resolution by In-
tegrating Multiple Resolvers. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP), pages 102?110, Novem-
ber.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bOcean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 33?40, July.
21
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
NIPS.
Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
Inference for Event Timeline Construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP), pages
677?689, July.
Jennifer D?Souza and Vincent Ng. 2013. Classifying
Temporal Relations with Rich Linguistic Knowledge.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-
HLT), pages 918?927, June.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistics Features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL), pages 60?68, July.
Eun Young Ha, Alok Baikadi, Carlyle Licata, and
James C. Lester. 2010. NCSU: Modeling Temporal
Relations with Markov Logic and Lexical Ontology.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (SemEval), pages 341?344, July.
Marti A. Hearst. 1994. Multi-Paragraph Segmentation
of Expository Text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 9?16, June.
Anna Kazantseva and Stan Szpakowicz. 2011. Lin-
ear Text Segmentation Using Affinity Propagation.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 284?293, July.
Anup Kumar Kolya, Asif Ekbal, and Sivaji Bandyopad-
hyay. 2010. JU CSE TEMP: A First Step Towards
Evaluating Events, Time Expressions and Temporal
Relations. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
345?350, July.
Alex Lascarides and Nicholas Asher. 1993. Temporal
Interpretation, Discourse Relations and Commonsense
Entailment. Linguistics and Philosophy, 16(5):437?
493.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2013. A
PDTB-styled End-to-End Discourse Parser. Natural
Language Engineering, FirstView:1?34, February.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learning
of Temporal Relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 753?760, July.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8(3):243?281.
Daniel Marcu. 1997. From Discourse Structures to Text
Summaries. In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization, volume 97,
pages 82?88, July.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML), September.
Jun-Ping Ng and Min-Yen Kan. 2012. Improved Tem-
poral Relation Classification using Dependency Parses
and Selective Crowdsourced Annotations. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING), pages 2109?2124, December.
Daniele Pighin and Alessandro Moschitti. 2010. On Re-
verse Feature Engineering of Syntactic Tree Kernels.
In Proceedings of the 14th Conference on Natural Lan-
guage Learning (CoNLL), August.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC), May.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple.
2003. Using Semantic Inferences for Temporal An-
notation Comparison. In Proceedings of the 4th In-
ternational Workshop on Inference in Computational
Semantics (ICoS), September.
Eduard F. Skorochod?Ko. 1972. Adaptive Method of
Automatic Abstracting and Indexing. In Proceedings
of the IFIP Congress, pages 1179?1182.
Carlota S. Smith. 2010. Temporal Structures in Dis-
course. Text, Time, and Context, 87:285?302.
Simone Teufel and Min-Yen Kan. 2011. Robust Argu-
mentative Zoning for Sensemaking in Scholarly Doc-
uments. In Advanced Language Technologies for Dig-
ital Libraries, pages 154?170. Springer.
Naushad Uzzaman and James F. Allen. 2010. TRIPS and
TRIOS System for TempEval-2: Extracting Temporal
Information. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
276?283, July.
Naushad Uzzaman, Hector Llorens, James F. Allen, Leon
Derczynski, Marc Verhagen, and James Pustejovsky.
2012. TempEval-3: Evaluating Events, Time Expres-
sions, and Temporal Relations. Computing Research
Repository (CoRR), abs/1206.5333.
Vladimir N. Vapnik, 1999. The Nature of Statistical
Learning Theory, chapter 5. Springer.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The TempEval Challenge: Identifying
22
Temporal Relations in Text. Language Resources and
Evaluation, 43(2):161?179.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
57?62, July.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2-3):211?241.
Bonnie Webber. 2004. D-LTAG: Extending Lexicalized
TAG to Discourse. Cognitive Science, 28(5):751?779.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly Identifying
Temporal Relations with Markov Logic. In Proceed-
ings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL) and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the Asian Federation of Natural Language Pro-
cessing (AFNLP), pages 405?413, August.
23
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 997?1006,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatically Evaluating Text Coherence Using Discourse Relations
Ziheng Lin, Hwee Tou Ng and Min-Yen Kan
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
{linzihen,nght,kanmy}@comp.nus.edu.sg
Abstract
We present a novel model to represent and
assess the discourse coherence of text. Our
model assumes that coherent text implicitly
favors certain types of discourse relation tran-
sitions. We implement this model and apply it
towards the text ordering ranking task, which
aims to discern an original text from a per-
muted ordering of its sentences. The experi-
mental results demonstrate that our model is
able to significantly outperform the state-of-
the-art coherence model by Barzilay and Lap-
ata (2005), reducing the error rate of the previ-
ous approach by an average of 29% over three
data sets against human upper bounds. We fur-
ther show that our model is synergistic with
the previous approach, demonstrating an error
reduction of 73% when the features from both
models are combined for the task.
1 Introduction
The coherence of a text is usually reflected by its dis-
course structure and relations. In Rhetorical Struc-
ture Theory (RST), Mann and Thompson (1988) ob-
served that certain RST relations tend to favor one
of two possible canonical orderings. Some rela-
tions (e.g., Concessive and Conditional) favor ar-
ranging their satellite span before the nucleus span.
In contrast, other relations (e.g., Elaboration and Ev-
idence) usually order their nucleus before the satel-
lite. If a text that uses non-canonical relation order-
ings is rewritten to use canonical orderings, it often
improves text quality and coherence.
This notion of preferential ordering of discourse
relations is observed in natural language in general,
and generalizes to other discourse frameworks aside
from RST. The following example shows a Contrast
relation between the two sentences.
(1) [ Everyone agrees that most of the nation?s old
bridges need to be repaired or replaced. ]S1 [ But
there?s disagreement over how to do it. ]S2
Here the second sentence provides contrasting infor-
mation to the first. If this order is violated without
rewording (i.e., if the two sentences are swapped), it
produces an incoherent text (Marcu, 1996).
In addition to the intra-relation ordering, such
preferences also extend to inter-relation ordering:
(2) [ The Constitution does not expressly give the
president such power. ]S1 [ However, the president
does have a duty not to violate the Constitution. ]S2
[ The question is whether his only means of
defense is the veto. ]S3
The second sentence above provides a contrast to the
previous sentence and an explanation for the next
one. This pattern of Contrast-followed-by-Cause is
rather common in text (Pitler et al, 2008). Ordering
the three sentences differently results in incoherent,
cryptic text.
Thus coherent text exhibits measurable prefer-
ences for specific intra- and inter-discourse relation
ordering. Our key idea is to use the converse of this
phenomenon to assess the coherence of a text. In
this paper, we detail our model to capture the coher-
ence of a text based on the statistical distribution of
the discourse structure and relations. Our method
specifically focuses on the discourse relation transi-
tions between adjacent sentences, modeling them in
a discourse role matrix.
997
Our study makes additional contributions. We im-
plement and validate our model on three data sets,
which show robust improvements over the current
state-of-the-art for coherence assessment. We also
provide the first assessment of the upper-bound of
human performance on the standard task of distin-
guishing coherent from incoherent orderings. To the
best our knowledge, this is also the first study in
which we show output from an automatic discourse
parser helps in coherence modeling.
2 Related Work
The study of coherence in discourse has led to many
linguistic theories, of which we only discuss algo-
rithms that have been reduced to practice.
Barzilay and Lapata (2005; 2008) proposed an
entity-based model to represent and assess local tex-
tual coherence. The model is motivated by Center-
ing Theory (Grosz et al, 1995), which states that
subsequent sentences in a locally coherent text are
likely to continue to focus on the same entities as
in previous sentences. Barzilay and Lapata op-
erationalized Centering Theory by creating an en-
tity grid model to capture discourse entity transi-
tions at the sentence-to-sentence level, and demon-
strated their model?s ability to discern coherent texts
from incoherent ones. Barzilay and Lee (2004) pro-
posed a domain-dependent HMM model to capture
topic shift in a text, where topics are represented by
hidden states and sentences are observations. The
global coherence of a text can then be summarized
by the overall probability of topic shift from the first
sentence to the last. Following these two directions,
Soricut and Marcu (2006) and Elsner et al (2007)
combined the entity-based and HMM-based models
and demonstrated that these two models are comple-
mentary to each other in coherence assessment.
Our approach differs from these models in that
it introduces and operationalizes another indicator
of discourse coherence, by modeling a text?s dis-
course relation transitions. Karamanis (2007) has
tried to integrate local discourse relations into the
Centering-based coherence metrics for the task of
information ordering, but was not able to obtain im-
provement over the baseline method, which is partly
due to the much smaller data set and the way the
discourse relation information is utilized in heuristic
constraints and rules.
To implement our proposal, we need to identify
the text?s discourse relations. This task, discourse
parsing, has been a recent focus of study in the nat-
ural language processing (NLP) community, largely
enabled by the availability of large-scale discourse
annotated corpora (Wellner and Pustejovsky, 2007;
Elwell and Baldridge, 2008; Lin et al, 2009; Pitler
et al, 2009; Pitler and Nenkova, 2009; Lin et al,
2010; Wang et al, 2010). The Penn Discourse Tree-
bank (PDTB) (Prasad et al, 2008) is such a cor-
pus which provides a discourse-level annotation on
top of the Penn Treebank, following a predicate-
argument approach (Webber, 2004). Crucially, the
PDTB provides annotations not only on explicit (i.e.,
signaled by discourse connectives such as because)
discourse relations, but also implicit (i.e., inferred
by readers) ones.
3 Using Discourse Relations
To utilize discourse relations of a text, we first ap-
ply automatic discourse parsing on the input text.
While any discourse framework, such as the Rhetor-
ical Structure Theory (RST), could be applied in our
work to encode discourse information, we have cho-
sen to work with the Discourse Lexicalized Tree Ad-
joining Grammar (D-LTAG) by Webber (2004) as
embodied in the PDTB, as a PDTB-styled discourse
parser1 developed by Lin et al (2010) has recently
become freely available.
This parser tags each explicit/implicit relation
with two levels of relation types. In this work,
we utilize the four PDTB Level-1 types: Temporal
(Temp), Contingency (Cont), Comparison (Comp),
and Expansion (Exp). This parser automatically
identifies the discourse relations, labels the argu-
ment spans, and classifies the relation types, includ-
ing identifying common entity and no relation (En-
tRel and NoRel) as types.
A simple approach to directly model the connec-
tions among discourse relations is to use the se-
quence of discourse relation transitions. Text (2) in
Section 1 can be represented by S1
Comp?? S2 Cont??
S3, for instance, when we use Level-1 types. In
such a basic approach, we can compile a distribu-
1http://wing.comp.nus.edu.sg/
?
linzihen/
parser/
998
tion of the n-gram discourse relation transition se-
quences in gold standard coherent text, and a similar
one for incoherent text. For example, the above text
would generate the transition bigram Comp?Cont.
We can build a classifier to distinguish one from the
other through learned examples or using a suitable
distribution distance measure (e.g., KL Divergence).
In our pilot work where we implemented such a
basic model with n-gram features for relation tran-
sitions, the performance was very poor. Our analy-
sis revealed a serious shortcoming: as the discourse
relation transitions in short texts are few in num-
ber, we have very little data to base the coherence
judgment on. However, when faced with even short
text excerpts, humans can distinguish coherent texts
from incoherent ones, as exemplified in our exam-
ple texts. The basic approach also does not model
the intra-relation preference. In Text (1), a Com-
parison (Comp) relation would be recorded between
the two sentences, irregardless of whether S1 or S2
comes first. However, it is clear that the ordering of
(S1 ? S2) is more coherent.
4 A Refined Approach
The central problem with the basic approach is in its
sparse modeling of discourse relations. In develop-
ing an improved model, we need to better exploit the
discourse parser?s output to provide more circum-
stantial evidence to support the system?s coherence
decision.
In this section, we introduce the concept of a dis-
course role matrix which aims to capture an ex-
panded set of discourse relation transition patterns.
We describe how to represent the coherence of a text
with its discourse relations and how to transform
such information into a matrix representation. We
then illustrate how we use the matrix to formulate a
preference ranking problem.
4.1 Discourse Role Matrix
Figure 1 shows a text and its gold standard PDTB
discourse relations. When a term appears in a dis-
course relation, the discourse role of this term is
defined as the discourse relation type plus the argu-
ment span in which the term is located (i.e., the argu-
ment tag). For instance, consider the term ?cananea?
in the first relation. Since the relation type is a
[ Japan normally depends heavily on the Highland
Valley and Cananea mines as well as the Bougainville
mine in Papua New Guinea. ]S1 [ Recently, Japan
has been buying copper elsewhere. ]S2 [ [ But as
Highland Valley and Cananea begin operating, ]C3.1
[ they are expected to resume their roles as Japan?s
suppliers. ]C3.2 ]S3 [ [ According to Fred Demler,
metals economist for Drexel Burnham Lambert, New
York, ]C4.1 [ ?Highland Valley has already started
operating ]C4.2 [ and Cananea is expected to do so
soon.? ]C4.3 ]S4
5 discourse relations are present in the above text:
1. Implicit Comparison between S1 as Arg1, and S2
as Arg2
2. Explicit Comparison using ?but? between S2 as
Arg1, and S3 as Arg2
3. Explicit Temporal using ?as? within S3 (Clause
C3.1 as Arg1, and C3.2 as Arg2)
4. Implicit Expansion between S3 as Arg1, and S4
as Arg2
5. Explicit Expansion using ?and? within S4
(Clause C4.2 as Arg1, and C4.3 as Arg2)
Figure 1: An excerpt with four contiguous sentences from
wsj 0437, showing five gold standard discourse relations.
?Cananea? is highlighted for illustration.
S# Terms
copper cananea operat depend . . .
S1 nil Comp.Arg1 nil Comp.Arg1
S2
Comp.Arg2
nil nil nilComp.Arg1
S3 nil
Comp.Arg2 Comp.Arg2
nilTemp.Arg1 Temp.Arg1
Exp.Arg1 Exp.Arg1
S4 nil Exp.Arg2
Exp.Arg1
nilExp.Arg2
Table 1: Discourse role matrix fragment for Figure 1.
Rows correspond to sentences, columns to stemmed
terms, and cells contain extracted discourse roles.
Comparison and ?cananea? is found in the Arg1
span, the discourse role of ?cananea? is defined as
Comp.Arg1. When terms appear in different rela-
tions and/or argument spans, they obtain different
discourse roles in the text. For instance, ?cananea?
plays a different discourse role of Temp.Arg1 in the
third relation in Figure 1. In the fourth relation,
since ?cananea? appears in both argument spans, it
has two additional discourse roles, Exp.Arg1 and
999
Exp.Arg2. The discourse role matrix thus represents
the different discourse roles of the terms across the
continuous text units. We use sentences as the text
units, and define terms to be the stemmed forms
of the open class words: nouns, verbs, adjectives,
and adverbs. We formulate the discourse role matrix
such that it encodes the discourse roles of the terms
across adjacent sentences.
Table 1 shows a fragment of the matrix represen-
tation of the text in Figure 1. Columns correspond to
the extracted terms; rows, the contiguous sentences.
A cell CTi,Sj then contains the set of the discourse
roles of the term Ti that appears in sentence Sj . For
example, the term ?cananea? from S1 takes part in
the first relation, so the cell Ccananea,S1 contains the
role Comp.Arg1. A cell may be empty (nil, as in
Ccananea,S2) or contain multiple discourse roles (as
in Ccananea,S3 , as ?cananea? in S3 participates in
the second, third, and fourth relations). Given these
discourse relations, building the matrix is straight-
forward: we note down the relations that a term Ti
from a sentence Sj participates in, and record its dis-
course roles in the respective cell.
We hypothesize that the sequence of discourse
role transitions in a coherent text provides clues that
distinguish it from an incoherent text. The discourse
role matrix thus provides the foundation for com-
puting such role transitions, on a per term basis. In
fact, each column of the matrix corresponds to a
lexical chain (Morris and Hirst, 1991) for a partic-
ular term across the whole text. The key differences
from the traditional lexical chains are that our chain
nodes? entities are simplified (they share the same
stemmed form, instead being connected by WordNet
relations), but are further enriched by being typed
with discourse relations.
We compile the set of sub-sequences of discourse
role transitions for every term in the matrix. These
transitions tell us how the discourse role of a term
varies through the progression of the text. For in-
stance, ?cananea? functions as Comp.Arg1 in S1 and
Comp.Arg2 in S3, and plays the role of Exp.Arg1
and Exp.Arg2 in S3 and S4, respectively. As we
have six relation types (Temp(oral), Cont(ingency),
Comp(arison), Exp(ansion), EntRel and NoRel) and
two argument tags (Arg1 and Arg2) for each type,
we have a total of 6 ? 2 = 12 possible dis-
course roles, plus a nil value. We define a dis-
course role transition as the sub-sequence of dis-
course roles for a term in multiple consecutive sen-
tences. For example, the discourse role transition of
?cananea? from S1 to S2 is Comp.Arg1?nil. As a
cell may contain multiple discourse roles, a transi-
tion may produce multiple sub-sequences. For ex-
ample, the length 2 sub-sequences for ?cananea?
from S3 to S4, are Comp.Arg2?Exp.Arg2,
Temp.Arg1?Exp.Arg2, and Exp.Arg1?Exp.Arg2.
Each sub-sequence has a probability that can be
computed from the matrix. To illustrate the calcu-
lation, suppose the matrix fragment in Table 1 is
the entire discourse role matrix. Then since there
are in total 25 length 2 sub-sequences and the sub-
sequence Comp.Arg2?Exp.Arg2 has a count of
two, its probability is 2/25 = 0.08. A key prop-
erty of our approach is that, while discourse tran-
sitions are captured locally on a per-term basis, the
probabilities of the discourse transitions are aggre-
gated globally, across all terms. We believe that the
overall distribution of discourse role transitions for
a coherent text is distinguishable from that for an in-
coherent text. Our model captures the distributional
differences of such sub-sequences in coherent and
incoherent text in training to determine an unseen
text?s coherence. To evaluate the coherence of a text,
we extract sub-sequences with various lengths from
the discourse role matrix as features2 and compute
the sub-sequence probabilities as the feature values.
To further refine the computation of the sub-
sequence distribution, we follow (Barzilay and La-
pata, 2005) and divide the matrix into a salient ma-
trix and a non-salient matrix. Terms (columns) with
a frequency greater than a threshold form the salient
matrix, while the rest form the non-salient matrix.
The sub-sequence distributions are then calculated
separately for these two matrices.
4.2 Preference Ranking
While some texts can be said to be simply coherent
or incoherent, often it is a matter of degree. A text
can be less coherent when compared to one text, but
more coherent when compared to another. As such,
since the notion of coherence is relative, we feel
that coherence assessment is better represented as
2Sub-sequences consisting of only nil values are not used as
features.
1000
a ranking problem rather than a classification prob-
lem. Given a pair of texts, the system ranks them
based on how coherent they are. Applications of
such a system include differentiating a text from its
permutation (i.e., the sentence ordering of the text
is shuffled) and identifying a more well-written es-
say from a pair. Such a system can easily generalize
from pairwise ranking into listwise, suitable for the
ordinal ranking of a set of texts. Coherence scoring
equations can also be deduced (Lapata and Barzilay,
2005) from such a model, yielding coherence scores.
To induce a model for preference ranking, we use
the SVMlight package3 by (Joachims, 1999) with
the preference ranking configuration for training and
testing. All parameters are set to their default values.
5 Experiments
We evaluate our coherence model on the task of text
ordering ranking, a standard coherence evaluation
task used in both (Barzilay and Lapata, 2005) and
(Elsner et al, 2007). In this task, the system is
asked to decide which of two texts is more coherent.
The pair of texts consists of a source text and one
of its permutations (i.e., the text?s sentence order is
randomized). Assuming that the original text is al-
ways more discourse-coherent than its permutation,
an ideal system will prefer the original to the per-
muted text. A system?s accuracy is thus the number
of times the system correctly chooses the original
divided by the total number of test pairs.
In order to acquire a large data set for training and
testing, we follow the approach in (Barzilay and La-
pata, 2005) to create a collection of synthetic data
from Wall Street Journal (WSJ) articles in the Penn
Treebank. All of the WSJ articles are randomly split
into a training and a testing set; 40 articles are held
out from the training set for development. For each
article, its sentences are permuted up to 20 times to
create a set of permutations4 . Each permutation is
paired with its source text to form a pair.
We also evaluate on two other data collections
(cf. Table 2), provided by (Barzilay and Lapata,
2005), for a direct comparison with their entity-
based model. These two data sets consist of Associ-
ated Press articles about earthquakes from the North
3http://svmlight.joachims.org/
4Short articles may produce less than 20 permutations.
WSJ Earthquakes Accidents
Train # Articles 1040 97 100# Pairs 19120 1862 1996
Avg. # Sents 22.0 10.4 11.5
Test # Articles 1079 99 100# Pairs 19896 1956 1986
Table 2: Details of the WSJ, Earthquakes, and Accidents
data sets, showing the number of training/testing articles,
number of pairs of articles, and average length of an arti-
cle (in sentences).
American News Corpus, and narratives from the Na-
tional Transportation Safety Board. These collec-
tions are much smaller than the WSJ data, as each
training/testing set contains only up to 100 source
articles. Similar to the WSJ data, we construct pairs
by permuting each source article up to 20 times.
Our model has two parameters: (1) the term fre-
quency (TF) that is used as a threshold to iden-
tify salient terms, and (2) the lengths of the sub-
sequences that are extracted as features. These pa-
rameters are tuned on the development set, and the
best ones that produce the optimal accuracy are
TF >= 2 and lengths of the sub-sequences <= 3.
We must also be careful in using the automatic
discourse parser. We note that the discourse parser
of Lin et al (2010) comes trained on the PDTB,
which provides annotations on top of the whole WSJ
data. As we also use the WSJ data for evaluation,
we must avoid parsing an article that has already
been used in training the parser to prevent training
on the test data. We re-train the parser with 24 WSJ
sections and use the trained parser to parse the sen-
tences in our WSJ collection from the remaining
section. We repeat this re-training/parsing process
for all 25 sections. Because the Earthquakes and
Accidents data do not overlap with the WSJ training
data, we use the parser as distributed to parse these
two data sets. Since the discourse parser utilizes
paragraph boundaries but a permuted text does not
have such boundaries, we ignore paragraph bound-
aries and treat the source text as if it has only one
paragraph. This is to make sure that we do not give
the system extra information because of this differ-
ence between the source and permuted text.
1001
5.1 Human Evaluation
While the text ordering ranking task has been used
in previous studies, two key questions about this task
have remained unaddressed in the previous work:
(1) to what extent is the assumption that the source
text is more coherent than its permutation correct?
and (2) how well do humans perform on this task?
The answer to the first is needed to validate the cor-
rectness of this synthetic task, while the second aims
to obtain the upper bound for evaluation. We con-
duct a human evaluation to answer these questions.
We randomly select 50 source text/permutation
pairs from each of the WSJ, Earthquakes, and Ac-
cidents training sets. We observe that some of the
source texts have formulaic structures in their ini-
tial sentences that give away the correct ordering.
Sources from the Earthquakes data always begin
with a headline sentence and a location-newswire
sentence, and many sources from the Accidents data
start with two sentences of ?This is preliminary
. . . errors. Any errors . . . completed.? We remove
these sentences from the source and permuted texts,
to avoid the subjects judging based on these clues in-
stead of textual coherence. For each set of 50 pairs,
we assigned two human subjects (who are not au-
thors of this paper) to perform the ranking. The sub-
jects are told to identify the source text from the pair.
When both subjects rank a source text higher than its
permutation, we interpret it as the subjects agreeing
that the source text is more coherent than the permu-
tation. Table 3 shows the inter-subject agreements.
WSJ Earthquakes Accidents Overall
90.0 90.0 94.0 91.3
Table 3: Inter-subject agreements on the three data sets.
While our study is limited and only indicative, we
conclude from these results that the task is tractable.
Also, since our subjects? judgments correlate highly
with the gold standard, the assumption that the orig-
inal text is always more coherent than the permuted
text is supported. Importantly though, human per-
formance is not perfect, suggesting fair upper bound
limits on system performance. We note that the Ac-
cidents data set is relatively easier to rank, as it has
a higher upper bound than the other two.
5.2 Baseline
Barzilay and Lapata (2005) showed that their entity-
based model is able to distinguish a source text from
its permutation accurately. Thus, it can serve as a
good comparison point for our discourse relation-
based model. We compare against their Syn-
tax+Salience setting. Since they did not automat-
ically determine the coreferential information of a
permuted text but obtained that from its correspond-
ing source text, we do not perform automatic coref-
erence resolution in our reimplementation of their
system. For fair comparison, we follow their experi-
ment settings as closely as possible. We re-use their
Earthquakes and Accidents dataset as is, using their
exact permutations and pre-processing. For the WSJ
data, we need to perform our own pre-processing,
thus we employed the Stanford parser5 to perform
sentence segmentation and constituent parsing, fol-
lowed by entity extraction.
5.3 Results
We perform a series of experiments to answer the
following four questions:
1. Does our model outperform the baseline?
2. How do the different features derived from us-
ing relation types, argument tags, and salience
information affect performance?
3. Can the combination of the baseline and our
model outperform the single models?
4. How does system performance of these models
compare with human performance on the task?
Baseline results are shown in the first row of Ta-
ble 4. The results on the Earthquakes and Accidents
data are quite similar to those published in (Barzilay
and Lapata, 2005) (they reported 83.4% on Earth-
quakes and 89.7% on Accidents), validating the cor-
rectness of our reimplementation of their method.
Row 2 in Table 4 shows the overall performance
of the proposed refined model, answering Question
1. The model setting of Type+Arg+Sal means that
the model makes use of the discourse roles consist-
ing of 1) relation types and 2) argument tags (e.g.,
5http://nlp.stanford.edu/software/
lex-parser.shtml
1002
WSJ Earthquakes Accidents
Baseline 85.71 83.59 89.93
Type+Arg+Sal 88.06** 86.50** 89.38
Arg+Sal 88.28** 85.89* 87.06
Type+Sal 87.06** 82.98 86.05
Type+Arg 85.98 82.67 87.87
Baseline & 89.25** 89.72** 91.64**
Type+Arg+Sal
Table 4: Test set ranking accuracy. The first row shows
the baseline performance, the next four show our model
with different settings, and the last row is a combined
model. Double (**) and single (*) asterisks indicate that
the respective model significantly outperforms the base-
line at p < 0.01 and p < 0.05, respectively. We follow
Barzilay and Lapata (2008) and use the Fisher Sign test.
the discourse role Comp.Arg2 consists of the type
Comp(arison) and the tag Arg2), and 3) two dis-
tinct feature sets from salient and non-salient terms.
Comparing these accuracies to the baseline, our
model significantly outperforms the baseline with
p < 0.01 in the WSJ and Earthquakes data sets
with accuracy increments of 2.35% and 2.91%, re-
spectively. In Accidents, our model?s performance
is slightly lower than the baseline, but the difference
is not statistically significant.
To answer Question 2, we perform feature abla-
tion testing. We eliminate each of the information
sources from the full model. In Row 3, we first
delete relation types from the discourse roles, which
causes discourse roles to only contain the argument
tags. A discourse role such as Comp.Arg2 becomes
Arg2 after deleting the relation type. Comparing
Row 3 to Row 2, we see performance reductions on
the Earthquakes and Accidents data after eliminat-
ing type information. Row 4 measures the effect of
omitting argument tags (Type+Sal). In this setting,
the discourse role Comp.Arg2 reduces to Comp. We
see a large reduction in performance across all three
data sets. This model is also most similar to the ba-
sic na??ve model in Section 3. These results suggest
that the argument tag information plays an impor-
tant role in our discourse role transition model. Row
5 omits the salience information (Type+Arg), which
also markedly reduces performance. This result sup-
ports the use of salience, in line with the conclusion
drawn in (Barzilay and Lapata, 2005).
To answer Question 3, we train and test a com-
bined model using features from both the baseline
and our model (shown as Row 6 in Table 4). The
entity-based model of Barzilay and Lapata (2005)
connects the local entity transition with textual co-
herence, while our model looks at the patterns of
discourse relation transitions. As these two models
focus on different aspects of coherence, we expect
that they are complementary to each other. The com-
bined model in all three data sets gives the highest
performance in comparison to all single models, and
it significantly outperforms the baseline model with
p < 0.01. This confirms that the combined model is
linguistically richer than the single models as it inte-
grates different information together, and the entity-
based model and our model are synergistic.
To answer Question 4, when compared to the hu-
man upper bound (Table 3), the performance gaps
for the baseline model are relatively large, while
those for our full model are more acceptable in
the WSJ and Earthquakes data. For the combined
model, the error rates are significantly reduced in
all three data sets. The average error rate reduc-
tions against 100% are 9.57% for the full model and
26.37% for the combined model. If we compute the
average error rate reductions against the human up-
per bounds (rather than an oracular 100%), the aver-
age error rate reduction for the full model is 29% and
that for the combined model is 73%. While these are
only indicative results, they do highlight the signifi-
cant gains that our model is making towards reach-
ing human performance levels.
We further note that some of the permuted texts
may read as coherently as the original text. This phe-
nomenon has been observed in several natural lan-
guage synthesis tasks such as generation and sum-
marization, in which a single gold standard is inade-
quate to fully assess performance. As such, both au-
tomated systems and humans may actually perform
better than our performance measures indicate. We
leave it to future work to measure the impact of this
phenomenon.
6 Analysis and Discussion
When we compare the accuracies of the full model
in the three data sets (Row 2), the accuracy in the
Accidents data is the highest (89.38%), followed by
1003
that in the WSJ (88.06%), with Earthquakes at the
lowest (86.50%). To explain the variation, we exam-
ine the ratio between the number of the relations in
the article and the article length (i.e., number of sen-
tences). This ratio is 1.22 for the Accidents source
articles, 1.2 for the WSJ, and 1.08 for Earthquakes.
The relation/length ratio gives us an idea of how of-
ten a sentence participates in discourse relations. A
high ratio means that the article is densely intercon-
nected by discourse relations, and may make dis-
tinguishing this article from its permutation easier
compared to that for a loosely connected article.
We expect that when a text contains more dis-
course relation types (i.e., Temporal, Contingency,
Comparison, Expansion) and less EntRel and NoRel
types, it is easier to compute how coherent this text
is. This is because compared to EntRel and NoRel,
these four discourse relations can combine to pro-
duce meaningful transitions, such as the example
Text (2). To examine how this affects performance,
we calculate the average ratio between the number
of the four discourse relations in the permuted text
and the length for the permuted text. The ratio is
0.58 for those that are correctly ranked by our sys-
tem, and 0.48 for those that are incorrectly ranked,
which supports our hypothesis.
We also examined the learning curves for our
Type+Arg+Sal model, the baseline model, and the
combined model on the data sets, as shown in Fig-
ure 2(a)?2(c). In the WSJ data, the accuracies for
all three models increase rapidly as more pairs are
added to the training set. After 2,000 pairs, the in-
crease slows until 8,000 pairs, after which the curve
is nearly flat. From the curves, our model consis-
tently performs better than the baseline with a signif-
icant gap, and the combined model also consistently
and significantly outperforms the other two. Only
about half of the total training data is needed to reach
optimal performance for all three models. The learn-
ing curves in the Earthquakes data show that the per-
formance for all models is always increasing as more
training pairs are utilized. The Type+Arg+Sal and
combined models start with lower accuracies than
the baseline, but catch up with it at 1,000 and 400
pairs, respectively, and consistently outperform the
baseline beyond this point. On the other hand, the
learning curves for the Type+Arg+Sal and baseline
models in Accidents do not show any one curve con-
 55
 60
 65
 70
 75
 80
 85
 90
 0  4000  8000  12000  16000  20000
A
cc
ur
ac
y 
(%
)
Number of pairs in training data
Combined
Type+Arg+Sal
Baseline
(a) WSJ
 55
 60
 65
 70
 75
 80
 85
 90
 0  400  800  1200  1600  2000
A
cc
ur
ac
y 
(%
)
Number of pairs in training data
Combined
Type+Arg+Sal
Baseline
(b) Earthquakes
 55
 60
 65
 70
 75
 80
 85
 90
 0  400  800  1200  1600  2000
A
cc
ur
ac
y 
(%
)
Number of pairs in training data
Combined
Type+Arg+Sal
Baseline
(c) Accidents
Figure 2: Learning curves for the Type+Arg+Sal, the
baseline, and the combined models on the three data sets.
sistently better than the other: our model outper-
forms in the middle segment but underperforms in
the first and last segments. The curve for the com-
bined model shows a consistently significant gap be-
tween it and the other two curves after the point at
400 pairs.
With the performance of the model as it is, how
can future work improve upon it? We point out one
weakness that we plan to explore. We use the full
Type+Arg+Sal model trained on the WSJ training
1004
data to test Text (2) from the introduction. As (2)
has 3 sentences, permuting it gives rise to 5 permu-
tations. The model is able to correctly rank four
of these 5 pairs. The only permutation it fails on
is (S3 ? S1 ? S2), when the last sentence is
moved to the beginning. A very good clue of co-
herence in Text (2) is the explicit Comp relation
between S1 and S2. Since this clue is retained in
(S3 ? S1 ? S2), it is difficult for the system to dis-
tinguish this ordering from the source. In contrast,
as this clue is not present in the other four permuta-
tions, it is easier to distinguish them as incoherent.
By modeling longer range discourse relation transi-
tions, we may be able to discern these two cases.
While performance on identifying explicit dis-
course relations in the PDTB is as high as
93% (Pitler et al, 2008), identifying implicit ones
has been shown to be a difficult task with accuracy
of 40% at Level-2 types (Lin et al, 2009). As the
overall performance of the PDTB parser is still less
accurate than we hope it to be, we expect that our
proposed model will give better performance than
it does now, when the current PDTB parser perfor-
mance is improved.
7 Conclusion
We have proposed a new model for discourse co-
herence that leverages the observation that coherent
texts preferentially follow certain discourse struc-
tures. We posit that these structures can be cap-
tured in and represented by the patterns of discourse
relation transitions. We first demonstrate that sim-
ply using the sequence of discourse relation tran-
sition leads to sparse features and is insufficient to
distinguish coherent from incoherent text. To ad-
dress this, our method transforms the discourse re-
lation transitions into a discourse role matrix. The
matrix schematically represents term occurrences in
text units and associates each occurrence with its
discourse roles in the text units. In our approach,
n-gram sub-sequences of transitions per term in the
discourse role matrix then constitute the more fine-
grained evidence used in our model to distinguish
coherence from incoherence.
When applied to distinguish a source text from
a sentence-reordered permutation, our model sig-
nificantly outperforms the previous state-of-the-art,
the entity-based local coherence model. While the
entity-based model captures repetitive mentions of
entities, our discourse relation-based model gleans
its evidence from the argumentative and discourse
structure of the text. Our model is complementary to
the entity-based model, as it tackles the same prob-
lem from a different perspective. Experiments vali-
date our claim, with a combined model outperform-
ing both single models.
The idea of modeling coherence with discourse
relations and formulating it in a discourse role ma-
trix can also be applied to other NLP tasks. We
plan to apply our methodology to other tasks, such
as summarization, text generation and essay scoring,
which also need to produce and assess discourse co-
herence.
References
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: an entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), pages
141?148, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34:1?34, March.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference / North
American Chapter of the Association for Computa-
tional Linguistics Annual Meeting 2004.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for dis-
course coherence. In Proceedings of the Conference
on Human Language Technology and North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL 2007), Rochester, New York, USA,
April.
Robert Elwell and Jason Baldridge. 2008. Discourse
connective argument identification with connective
specific rankers. In Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC
2010), Washington, DC, USA.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
1995. Centering: a framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225, June.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
1005
Schlkopf, Christopher J. C. Burges, and Alexander J.
Smola, editors, Advances in Kernel Methods ? Support
Vector Learning, pages 169?184. MIT Press, Cam-
bridge, MA, USA.
Nikiforos Karamanis. 2007. Supplementing entity co-
herence with local rhetorical relations for information
ordering. Journal of Logic, Language and Informa-
tion, 16:445?464, October.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Leslie Pack Kaelbling and Alessandro Saf-
fiotti, editors, Proceedings of the Nineteenth Interna-
tional Joint Conference on Artificial Intelligence, Ed-
inburgh, Scotland, UK.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2009), Singapore.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
PDTB-styled end-to-end discourse parser. Technical
Report TRB8/10, School of Computing, National Uni-
versity of Singapore, August.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Daniel Marcu. 1996. Distinguishing between coher-
ent and incoherent texts. In The Proceedings of the
Student Conference on Computational Linguistics in
Montreal, pages 136?143.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of the
structure of text. Computational Linguistics, 17:21?
48, March.
Emily Pitler and Ani Nenkova. 2009. Using syntax
to disambiguate explicit discourse connectives in text.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, Singapore.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Easily
identifiable discourse relations. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING 2008) Short Papers, Manchester,
UK.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Au-
tomatic sense prediction for implicit discourse rela-
tions in text. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP (ACL-IJCNLP 2009), Sin-
gapore.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the COLING/ACL Main Conference Poster
Sessions, pages 803?810, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
WenTing Wang, Jian Su, and Chew Lim Tan. 2010. Ker-
nel based discourse relation recognition with tempo-
ral ordering information. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL 2010), Uppsala, Sweden, July.
Bonnie Webber. 2004. D-LTAG: Extending lexicalized
TAG to discourse. Cognitive Science, 28(5):751?779.
Ben Wellner and James Pustejovsky. 2007. Automati-
cally identifying the arguments of discourse connec-
tives. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), Prague, Czech Republic.
1006
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1006?1014,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Combining Coherence Models and Machine Translation Evaluation Metrics
for Summarization Evaluation
Ziheng Lin?, Chang Liu?, Hwee Tou Ng? and Min-Yen Kan?
? SAP Research, SAP Asia Pte Ltd
30 Pasir Panjang Road, Singapore 117440
ziheng.lin@sap.com
? Department of Computer Science, National University of Singapore
13 Computing Drive, Singapore 117417
{liuchan1,nght,kanmy}@comp.nus.edu.sg
Abstract
An ideal summarization system should pro-
duce summaries that have high content cov-
erage and linguistic quality. Many state-of-
the-art summarization systems focus on con-
tent coverage by extracting content-dense sen-
tences from source articles. A current research
focus is to process these sentences so that they
read fluently as a whole. The current AE-
SOP task encourages research on evaluating
summaries on content, readability, and over-
all responsiveness. In this work, we adapt
a machine translation metric to measure con-
tent coverage, apply an enhanced discourse
coherence model to evaluate summary read-
ability, and combine both in a trained regres-
sion model to evaluate overall responsiveness.
The results show significantly improved per-
formance over AESOP 2011 submitted met-
rics.
1 Introduction
Research and development on automatic and man-
ual evaluation of summarization systems have been
mainly focused on content coverage (Lin and Hovy,
2003; Nenkova and Passonneau, 2004; Hovy et al,
2006; Zhou et al, 2006). However, users may still
find it difficult to read such high-content coverage
summaries as they lack fluency. To promote research
on automatic evaluation of summary readability, the
Text Analysis Conference (TAC) (Owczarzak and
Dang, 2011) introduced a new subtask on readability
to its Automatically Evaluating Summaries of Peers
(AESOP) task.
Most of the state-of-the-art summarization sys-
tems (Ng et al, 2011; Zhang et al, 2011; Conroy
et al, 2011) are extraction-based. They extract the
most content-dense sentences from source articles.
If no post-processing is performed to the generated
summaries, the presentation of the extracted sen-
tences may confuse readers. Knott (1996) argued
that when the sentences of a text are randomly or-
dered, the text becomes difficult to understand, as its
discourse structure is disturbed. Lin et al (2011)
validated this argument by using a trained model
to differentiate an original text from a randomly-
ordered permutation of its sentences by looking at
their discourse structures. This prior work leads us
to believe that we can apply such discourse mod-
els to evaluate the readability of extract-based sum-
maries. We will discuss the application of Lin et
al.?s discourse coherence model to evaluate read-
ability of machine generated summaries. We also
introduce two new feature sources to enhance the
model with hierarchical and Explicit/Non-Explicit
information, and demonstrate that they improve the
original model.
There are parallels between evaluations of ma-
chine translation (MT) and summarization with re-
spect to textual content. For instance, the widely
used ROUGE (Lin and Hovy, 2003) metrics are in-
fluenced by BLEU (Papineni et al, 2002): both
look at surface n-gram overlap for content cover-
age. Motivated by this, we will adapt a state-of-the-
art, linear programming-based MT evaluation met-
ric, TESLA (Liu et al, 2010), to evaluate the content
coverage of summaries.
TAC?s overall responsiveness metric evaluates the
1006
quality of a summary with regard to both its con-
tent and readability. Given this, we combine our
two component coherence and content models into
an SVM-trained regression model as our surrogate
to overall responsiveness. Our experiments show
that the coherence model significantly outperforms
all AESOP 2011 submissions on both initial and up-
date tasks, while the adapted MT evaluation metric
and the combined model significantly outperform all
submissions on the initial task. To the best of our
knowledge, this is the first work that applies a dis-
course coherence model to measure the readability
of summaries in the AESOP task.
2 Related Work
Nenkova and Passonneau (2004) proposed a manual
evaluation method that was based on the idea that
there is no single best model summary for a collec-
tion of documents. Human annotators construct a
pyramid to capture important Summarization Con-
tent Units (SCUs) and their weights, which is used
to evaluate machine generated summaries.
Lin and Hovy (2003) introduced an automatic
summarization evaluation metric, called ROUGE,
which was motivated by the MT evaluation met-
ric, BLEU (Papineni et al, 2002). It automati-
cally determines the content quality of a summary
by comparing it to the model summaries and count-
ing the overlapping n-gram units. Two configura-
tions ? ROUGE-2, which counts bigram overlaps,
and ROUGE-SU4, which counts unigram and bi-
gram overlaps in a word window of four ? have been
found to correlate well with human evaluations.
Hovy et al (2006) pointed out that automated
methods such as ROUGE, which match fixed length
n-grams, face two problems of tuning the appropri-
ate fragment lengths and matching them properly.
They introduced an evaluation method that makes
use of small units of content, called Basic Elements
(BEs). Their method automatically segments a text
into BEs, matches similar BEs, and finally scores
them.
Both ROUGE and BE have been implemented
and included in the ROUGE/BE evaluation toolkit1,
which has been used as the default evaluation tool
in the summarization track in the Document Un-
1http://berouge.com/default.aspx
derstanding Conference (DUC) and Text Analysis
Conference (TAC). DUC and TAC also manually
evaluated machine generated summaries by adopt-
ing the Pyramid method. Besides evaluating with
ROUGE/BE and Pyramid, DUC and TAC also asked
human judges to score every candidate summary
with regard to its content, readability, and overall re-
sponsiveness.
DUC and TAC defined linguistic quality to cover
several aspects: grammaticality, non-redundancy,
referential clarity, focus, and structure/coherence.
Recently, Pitler et al (2010) conducted experiments
on various metrics designed to capture these as-
pects. Their experimental results on DUC 2006 and
2007 show that grammaticality can be measured by
a set of syntactic features, while the last three as-
pects are best evaluated by local coherence. Con-
roy and Dang (2008) combined two manual linguis-
tic scores ? grammaticality and focus ? with various
ROUGE/BE metrics, and showed this helps better
predict the responsiveness of the summarizers.
Since 2009, TAC introduced the task of Auto-
matically Evaluating Summaries of Peers (AESOP).
AESOP 2009 and 2010 focused on two summary
qualities: content and overall responsiveness. Sum-
mary content is measured by comparing the output
of an automatic metric with the manual Pyramid
score. Overall responsiveness measures a combi-
nation of content and linguistic quality. In AESOP
2011 (Owczarzak and Dang, 2011), automatic met-
rics are also evaluated for their ability to assess sum-
mary readability, i.e., to measure how linguistically
readable a machine generated summary is. Sub-
mitted metrics that perform consistently well on the
three aspects include Giannakopoulos and Karkalet-
sis (2011), Conroy et al (2011), and de Oliveira
(2011). Giannakopoulos and Karkaletsis (2011) cre-
ated two character-based n-gram graph representa-
tions for both the model and candidate summaries,
and applied graph matching algorithm to assess their
similarity. Conroy et al (2011) extended the model
in (Conroy and Dang, 2008) to include shallow lin-
guistic features such as term overlap, redundancy,
and term and sentence entropy. de Oliveira (2011)
modeled the similarity between the model and can-
didate summaries as a maximum bipartite matching
problem, where the two summaries are represented
as two sets of nodes and precision and recall are cal-
1007
w=1.0 w=0.8 w=0.2 w=0.1
w=1.0 w=0.8 w=0.1
.2
s=0.5 s=1.0s=0.5 s=1.0
(a) The matching problem
w=1.0 w=0.8 w=0.2 w=0.1
w=1.0 w=0.8 w=0.1
.2
w=1.0 w=0.2w=0.s w=0.1
(b) The matching solution
Figure 1: A BNG matching problem. Top and
bottom rows of each figure represent BNG from
the model and candidate summaries, respectively.
Links are similarities. Both n-grams and links are
weighted.
culated from the matched edges. However, none of
the AESOP metrics currently apply deep linguistic
analysis, which includes discourse analysis.
Motivated by the parallels between summariza-
tion and MT evaluation, we will adapt a state-of-
the-art MT evaluation metric to measure summary
content quality. To apply deep linguistic analysis,
we also enhance an existing discourse coherence
model to evaluate summary readability. We focus
on metrics that measure the average quality of ma-
chine summarizers, i.e., metrics that can rank a set
of machine summarizers correctly (human summa-
rizers are not included in the list).
3 TESLA-S: Evaluating Summary
Content
TESLA (Liu et al, 2010) is an MT evaluation
metric which extends BLEU by introducing a lin-
ear programming-based framework for improved
matching. It also makes use of linguistic resources
and considers both precision and recall.
3.1 The Linear Programming Matching
Framework
Figure 1 shows the matching of bags of n-grams
(BNGs) that forms the core of the TESLA metric.
The top row in Figure 1a represents the bag of n-
grams (BNG) from the model summary, and the
bottom row represents the BNG from the candidate
summary. Each n-gram has a weight. The links
between the n-grams represent the similarity score,
which are constrained to be between 0 and 1. Math-
ematically, TESLA takes as input the following:
1. The BNG of the model summary, X , and the
BNG of the candidate summary, Y . The ith en-
try in X is xi and has weight xWi (analogously
for yi and yWi ).
2. A similarity score s(xi, yj) between all n-
grams xi and yj .
The goal of the matching process is to align the
two BNGs so as to maximize the overall similar-
ity. The variables of the problem are the allocated
weights for the edges,
w(xi, yj) ?i, j
TESLA maximizes
?
i,j
s(xi, yj)w(xi, yj)
subject to
w(xi, yj) ? 0 ?i, j
?
j
w(xi, yj) ? x
W
i ?i
?
i
w(xi, yj) ? y
W
j ?j
This real-valued linear programming problem can
be solved efficiently. The overall similarity S is the
value of the objective function. Thus,
Precision =
S
?
j y
W
j
Recall =
S
?
i x
W
i
The final TESLA score is given by the F-measure:
F =
Precision? Recall
?? Precision + (1? ?)? Recall
In this work, we set ? = 0.8, following (Liu et al,
2010). The score places more importance on recall
than precision. When multiple model summaries are
provided, TESLA matches the candidate BNG with
each of the model BNGs. The maximum score is
taken as the combined score.
1008
3.2 TESLA-S: TESLA for Summarization
We adapted TESLA for the nuances of summariza-
tion. Mimicking ROUGE-SU4, we construct one
matching problem between the unigrams and one
between skip bigrams with a window size of four.
The two F scores are averaged to give the final score.
The similarity score s(xi, yj) is 1 if the word sur-
face forms of xi and yj are identical, and 0 other-
wise. TESLA has a more sophisticated similarity
measure that focuses on awarding partial scores for
synonyms and parts of speech (POS) matches. How-
ever, the majority of current state-of-the-art sum-
marization systems are extraction-based systems,
which do not generate new words. Although our
simplistic similarity score may be problematic when
evaluating abstract-based systems, the experimen-
tal results support our choice of the similarity func-
tion. This reflects a major difference between MT
and summarization evaluation: while MT systems
always generate new sentences, most summarization
systems focus on locating existing salient sentences.
Like in TESLA, function words (words in closed
POS categories, such as prepositions and articles)
have their weights reduced by a factor of 0.1, thus
placing more emphasis on the content words. We
found this useful empirically.
3.3 Significance Test
Koehn (2004) introduced a bootstrap resampling
method to compute statistical significance of the dif-
ference between two machine translation systems
with regard to the BLEU score. We adapt this
method to compute the difference between two eval-
uation metrics in summarization:
1. Randomly choose n topics from the n given
topics with replacement.
2. Summarize the topics with the list of machine
summarizers.
3. Evaluate the list of summaries from Step 2 with
the two evaluation metrics under comparison.
4. Determine which metric gives a higher correla-
tion score.
5. Repeat Step 1 ? 4 for 1,000 times.
As we have 44 topics in TAC 2011 summarization
track, n = 44. The percentage of times metric a
gives higher correlation than metric b is said to be
the significance level at which a outperforms b.
Initial Update
P S K P S K
R-2 0.9606 0.8943 0.7450 0.9029 0.8024 0.6323
R-SU4 0.9806 0.8935 0.7371 0.8847 0.8382 0.6654
BE 0.9388 0.9030 0.7456 0.9057 0.8385 0.6843
4 0.9672 0.9017 0.7351 0.8249 0.8035 0.6070
6 0.9678 0.8816 0.7229 0.9107 0.8370 0.6606
8 0.9555 0.8686 0.7024 0.8981 0.8251 0.6606
10 0.9501 0.8973 0.7550 0.7680 0.7149 0.5504
11 0.9617 0.8937 0.7450 0.9037 0.8018 0.6291
12 0.9739 0.8972 0.7466 0.8559 0.8249 0.6402
13 0.9648 0.9033 0.7582 0.8842 0.7961 0.6276
24 0.9509 0.8997 0.7535 0.8115 0.8199 0.6386
TESLA-S 0.9807 0.9173 0.7734 0.9072 0.8457 0.6811
Table 1: Content correlation with human judgment
on summarizer level. Top three scores among AE-
SOP metrics are underlined. The TESLA-S score is
bolded when it outperforms all others. ROUGE-2 is
shortened to R-2 and ROUGE-SU4 to R-SU4.
3.4 Experiments
We test TESLA-S on the AESOP 2011 content eval-
uation task, judging the metric fitness by compar-
ing its correlations with human judgments for con-
tent. The results for the initial and update tasks are
reported in Table 1. We show the three baselines
(ROUGE-2, ROUGE-SU4, and BE) and submitted
metrics with correlations among the top three scores,
which are underlined. This setting remains the same
for the rest of the experiments. We use three cor-
relation measures: Pearson?s r, Spearman?s ?, and
Kendall?s ? , represented by P, S, and K, respectively.
The ROUGE scores are the recall scores, as per con-
vention. On the initial task, TESLA-S outperforms
all metrics on all three correlation measures. On the
update task, TESLA-S ranks second, first, and sec-
ond on Pearson?s r, Spearman?s ?, and Kendall?s ? ,
respectively.
To test how significant the differences are, we per-
form significance testing using Koehn?s resampling
method between TESLA-S and ROUGE-2/ROUGE-
SU4, on which TESLA-S is based. The findings are:
? Initial task: TESLA-S is better than ROUGE-2
at 99% significance level as measured by Pear-
son?s r.
? Update task: TESLA-S is better than ROUGE-
SU4 at 95% significance level as measured by
Pearson?s r.
? All other differences are statistically insignifi-
cant, including all correlations on Spearman?s
1009
? and Kendall?s ? .
The last point can be explained by the fact that
Spearman?s ? and Kendall?s ? are sensitive to only
the system rankings, whereas Pearson?s r is sensitive
to the magnitude of the differences as well, hence
Pearson?s r is in general a more sensitive measure.
4 DICOMER: Evaluating Summary
Readability
Intuitively, a readable text should also be coherent,
and an incoherent text will result in low readabil-
ity. Both readability and coherence indicate how
fluent a text is. We thus hypothesize that a model
that measures how coherent a text is can also mea-
sure its readability. Lin et al (2011) introduced dis-
course role matrix to represent discourse coherence
of a text. W first illustrate their model with an exam-
ple, and then introduce two new feature sources. We
then apply the models and evaluate summary read-
ability.
4.1 Lin et al?s Discourse Coherence Model
First, a free text in Figure 2 is parsed by a dis-
course parser to derive its discourse relations, which
are shown in Figure 3. Lin et al observed that
coherent texts preferentially follow certain relation
patterns. However, simply using such patterns to
measure the coherence of a text can result in fea-
ture sparseness. To solve this problem, they expand
the relation sequence into a discourse role matrix,
as shown in Table 2. The matrix essentially cap-
tures term occurrences in the sentence-to-sentence
relation sequences. This model is motivated by
the entity-based model (Barzilay and Lapata, 2008)
which captures sentence-to-sentence entity transi-
tions. Next, the discourse role transition probabili-
ties of lengths 2 and 3 (e.g., Temp.Arg1?Exp.Arg2
and Comp.Arg1?nil?Temp.Arg1) are calculated
with respect to the matrix. For example, the prob-
ability of Comp.Arg2?Exp.Arg2 is 2/25 = 0.08 in
Table 2.
Lin et al applied their model on the task of dis-
cerning an original text from a permuted ordering of
its sentences. They modeled it as a pairwise rank-
ing model (i.e., original vs. permuted), and trained a
SVM preference ranking model with discourse role
S1 Japan normally depends heavily on the High-
land Valley and Cananea mines as well as the
Bougainville mine in Papua New Guinea.
S2 Recently, Japan has been buying copper elsewhere.
S3.1 But as Highland Valley and Cananea begin operat-
ing,
S3.2 they are expected to resume their roles as Japan?s
suppliers.
S4.1 According to Fred Demler, metals economist for
Drexel Burnham Lambert, New York,
S4.2 ?Highland Valley has already started operating
S4.3 and Cananea is expected to do so soon.?
Figure 2: A text with four sentences. Si.j means the
jth clause in the ith sentence.
S1           S2          S3.1          S3.2          S4.1          S4.2          S4.3 
Implicit Comparison 
Explicit Comparison 
Explicit Temporal 
Implicit Expansion 
Explicit Expansion 
Figure 3: The discourse relations for Figure 2. Ar-
rows are pointing from Arg2 to Arg1.
S#
Terms
copper cananea operat depend . . .
S1 nil Comp.Arg1 nil Comp.Arg1
S2
Comp.Arg2
nil nil nil
Comp.Arg1
S3 nil
Comp.Arg2 Comp.Arg2
nilTemp.Arg1 Temp.Arg1
Exp.Arg1 Exp.Arg1
S4 nil Exp.Arg2
Exp.Arg1
nil
Exp.Arg2
Table 2: Discourse role matrix fragment extracted
from Figure 2 and 3. Rows correspond to sen-
tences, columns to stemmed terms, and cells contain
extracted discourse roles. Temporal, Contingency,
Comparison, and Expansion are shortened to Temp,
Cont, Comp, and Exp, respectively.
transitions as features and their probabilities as val-
ues.
4.2 Two New Feature Sources
We observe that there are two kinds of informa-
tion in Figure 3 that are not captured by Lin et al?s
1010
model. The first one is whether a relation is Ex-
plicit or Non-Explicit (Lin et al (2010) termed Non-
Explicit to include Implicit, AltLex, EntRel, and
NoRel). Explicit relation and Non-Explicit relation
have different distributions on each discourse rela-
tion (PDTB-Group, 2007). Thus, adding this in-
formation may further improve the model. In ad-
dition to the set of the discourse roles of ?Rela-
tion type . Argument tag?, we introduce another
set of ?Explicit/Non-Explicit . Relation type . Ar-
gument tag?. The cell Ccananea,S3 now contains
Comp.Arg2, Temp.Arg1, Exp.Arg1, E.Comp.Arg2,
E.Temp.Arg1, and N.Exp.Arg1 (E for Explicit and
N for Non-Explicit).
The other information that is not in the discourse
role matrix is the discourse hierarchy structure,
i.e., whether one relation is embedded within
another relation. In Figure 3, S3.1 is Arg1 of
Explicit Temporal, which is Arg2 of the higher
relation Explicit Comparison as well as Arg1 of
another higher relation Implicit Expansion. These
dependencies are important for us to know how
well-structured a summary is. It is represented
by the multiple discourse roles in each cell of the
matrix. For example, the multiple discourse roles in
the cell Ccananea,S3 capture the three dependencies
just mentioned. We introduce intra-cell bigrams
as a new set of features to the original model: for
a cell with multiple discourse roles, we sort them
by their surface strings and multiply to obtain
the bigrams. For instance, Ccananea,S3 will pro-
duce bigrams such as Comp.Arg2?Exp.Arg1
and Comp.Arg2?Temp.Arg1. When both
the Explicit/Non-Explicit feature source and
the intra-cell feature source are joined to-
gether, it also produces bigram features such
as E.Comp.Arg2?Temp.Arg1.
4.3 Predicting Readability Scores
Lin et al (2011) used the SVMlight (Joachims,
1999) package with the preference ranking config-
uration. To train the model, each source text and
one of its permutations form a training pair, where
the source text is given a rank of 1 and the permuta-
tion is given 0. In testing, the trained model predicts
a real number score for each instance, and the in-
stance with the higher score in a pair is said to be
the source text.
In the TAC summarization track, human judges
scored each model and candidate summary with a
readability score from 1 to 5 (5 means most read-
able). Thus in our setting, instead of a pair of texts,
the training input consists of a list of model and can-
didate summaries from each topic, with their anno-
tated scores as the rankings. Given an unseen test
summary, the trained model predicts a real number
score. This score essentially is the readability rank-
ing of the test summary. Such ranking can be eval-
uated by the ranking-based correlations of Spear-
man?s ? and Kendall?s ? . As Pearson?s r measures
linear correlation and we do not know whether the
real number score follows a linear function, we take
the logarithm of this score as the readability score
for this instance.
We use the data from AESOP 2009 and 2010 as
the training data, and test our metrics on AESOP
2011 data. To obtain the discourse relations of a
summary, we use the discourse parser2 developed in
Lin et al (2010).
4.4 Experiments
Table 3 shows the resulting readability correlations.
The last four rows show the correlation scores for
our coherence model: LIN is the default model
by (Lin et al, 2011), LIN+C is LIN with the
intra-cell feature class, LIN+E is enhanced with
the Explicit/Non-Explicit feature class. We name
the LIN model with both new feature sources (i.e.,
LIN+C+E) DICOMER ? a DIscourse COherence
Model for Evaluating Readability.
LIN outperforms all metrics on all correlations on
both tasks. On the initial task, it outperforms the
best scores by 3.62%, 16.20%, and 12.95% on Pear-
son, Spearman, and Kendall, respectively. Similar
gaps (4.27%, 18.52%, and 13.96%) are observed
on the update task. The results are much better
on Spearman and Kendall. This is because LIN is
trained with a ranking model, and both Spearman
and Kendall are ranking-based correlations.
Adding either intra-cell or Explicit/Non-Explicit
features improves all correlation scores, with
Explicit/Non-Explicit giving more pronounced im-
provements. When both new feature sources are in-
2http://wing.comp.nus.edu.sg/?linzihen/
parser/
1011
Initial Update
P S K P S K
R-2 0.7524 0.3975 0.2925 0.6580 0.3732 0.2635
R-SU4 0.7840 0.3953 0.2925 0.6716 0.3627 0.2540
BE 0.7171 0.4091 0.2911 0.5455 0.2445 0.1622
4 0.8194 0.4937 0.3658 0.7423 0.4819 0.3612
6 0.7840 0.4070 0.3036 0.6830 0.4263 0.3141
12 0.7944 0.4973 0.3589 0.6443 0.3991 0.3062
18 0.7914 0.4746 0.3510 0.6698 0.3941 0.2856
23 0.7677 0.4341 0.3162 0.7054 0.4223 0.3014
LIN 0.8556 0.6593 0.4953 0.7850 0.6671 0.5008
LIN+C 0.8612 0.6703 0.4984 0.7879 0.6828 0.5135
LIN+E 0.8619 0.6855 0.5079 0.7928 0.6990 0.5309
DICOMER 0.8666 0.7122 0.5348 0.8100 0.7145 0.5435
Table 3: Readability correlation with human judg-
ment on summarizer level. Top three scores among
AESOP metrics are underlined. Our score is bolded
when it outperforms all AESOP metrics.
Initial Update
vs. P S K P S K
LIN
4
? ?? ?? ?? ?? ??
LIN+C ?? ?? ?? ?? ?? ??
LIN+E ?? ?? ?? ? ?? ??
DICOMER ?? ?? ?? ?? ?? ??
DICOMER LIN ? ? ? ? ? ?
Table 4: Koehn?s significance test for readability.
??, ?, and ? indicate significance level >=99%,
>=95%, and <95%, respectively.
corporated into the metric, we obtain the best results
for all correlation scores: DICOMER outperforms
LIN by 1.10%, 5.29%, and 3.95% on the initial task,
and 2.50%, 4.74%, and 4.27% on the update task.
Table 3 shows that summarization evaluation
Metric 4 tops all other AESOP metrics, except in
the case of Spearman?s ? on the initial task. We
compare our four models to this metric. The results
of Koehn?s significance test are reported in Table 4,
which demonstrates that all four models outperform
Metric 4 significantly. In the last row, we see that
when comparing DICOMER to LIN, DICOMER is
significantly better on three correlation measures.
5 CREMER: Evaluating Overall
Responsiveness
With TESLA-S measuring content coverage and DI-
COMER measuring readability, it is feasible to com-
bine them to predict the overall responsiveness of a
summary. There exist many ways to combine two
variables mathematically: we can combine them in
a linear function or polynomial function, or in a way
Initial Update
P S K P S K
R-2 0.9416 0.7897 0.6096 0.9169 0.8401 0.6778
R-SU4 0.9545 0.7902 0.6017 0.9123 0.8758 0.7065
BE 0.9155 0.7683 0.5673 0.8755 0.7964 0.6254
4 0.9498 0.8372 0.6662 0.8706 0.8674 0.7033
6 0.9512 0.7955 0.6112 0.9271 0.8769 0.7160
11 0.9427 0.7873 0.6064 0.9194 0.8432 0.6794
12 0.9469 0.8450 0.6746 0.8728 0.8611 0.6858
18 0.9480 0.8447 0.6715 0.8912 0.8377 0.6683
23 0.9317 0.7952 0.6080 0.9192 0.8664 0.6953
25 0.9512 0.7899 0.6033 0.9033 0.8139 0.6349
CREMERLF 0.9381 0.8346 0.6635 0.8280 0.6860 0.5173
CREMERPF 0.9621 0.8567 0.6921 0.8852 0.7863 0.6159
CREMERRBF 0.9716 0.8836 0.7206 0.9018 0.8285 0.6588
Table 5: Responsiveness correlation with human
judgment on summarizer level. Top three scores
among AESOP metrics are underlined. CREMER
score is bolded when it outperforms all AESOP met-
rics.
similar to how precision and recall are combined
in F measure. We applied a machine learning ap-
proach to train a regression model for measuring
responsiveness. The scores predicted by TESLA-
S and DICOMER are used as two features. We
use SVMlight with the regression configuration, test-
ing three kernels: linear function, polynomial func-
tion, and radial basis function. We called this model
CREMER ? a Combined REgression Model for
Evaluating Responsiveness.
We train the regression model on AESOP 2009
and 2010 data sets, and test it on AESOP 2011. The
DICOMER model that is trained in Section 4 is used
to predict the readability scores on all AESOP 2009,
2010, and 2011 summaries. We apply TESLA-S to
predict content scores on all AESOP 2009, 2010,
and 2011 summaries.
5.1 Experiments
The last three rows in Table 5 show the correlation
scores of our regression model trained with SVM
linear function (LF), polynomial function (PF), and
radial basis function (RBF). PF performs better than
LF, suggesting that content and readability scores
should not be linearly combined. RBF gives bet-
ter performances than both LF and PF, suggesting
that RBF better models the way humans combine
content and readability. On the initial task, the
model trained with RBF outperforms all submitted
metrics. It outperforms the best correlation scores
1012
by 1.71%, 3.86%, and 4.60% on Pearson, Spear-
man, and Kendall, respectively. All three regression
models do not perform as well on the update task.
Koehn?s significance test shows that when trained
with RBF, CREMER outperforms ROUGE-2 and
ROUGE-SU4 on the initial task at a significance
level of 99% for all three correlation measures.
6 Discussion
The intuition behind the combined regression model
is that combining the readability and content scores
will give an overall good responsiveness score. The
function to combine them and their weights can be
obtained by training. While the results showed that
SVM radial basis kernel gave the best performances,
this function may not truly mimic how human evalu-
ates responsiveness. Human judges were told to rate
summaries by their overall qualities. They may take
into account other aspects besides content and read-
ability. Given CREMER did not perform well on the
update task, we hypothesize that human judgment
of update summaries may involve more complicated
rankings or factor in additional input that CREMER
currently does not model. We plan to devise a bet-
ter responsiveness metric in our future work, beyond
using a simple combination.
Figure 4 shows a complete picture of Pearson?s r
for all AESOP 2011 metrics and our three met-
rics on both initial and update tasks. We highlight
our metrics with a circle on these curves. On the
initial task, correlation scores for content are con-
sistently higher than those for responsiveness with
small gaps, whereas on the update task, they are al-
most overlapping. On the other hand, correlation
scores for readability are much lower than those for
content and responsiveness, with a gap of about 0.2.
Comparing Figure 4a and 4b, evaluation metrics al-
ways correlate better on the initial task than on the
update task. This suggests that there is much room
for improvement for readability metrics, and metrics
need to consider update information when evaluat-
ing update summarizers.
7 Conclusion
We proposed TESLA-S by adapting an MT eval-
uation metric to measure summary content cover-
age, and introduced DICOMER by applying a dis-
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
P
e
a
r
s
o
n
?
s
 
r
ContentResponsivenessReadability
(a) Evaluation metric values on the initial task.
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9 1
P
e
a
r
s
o
n
?
s
 
r
ContentResponsivenessReadability
(b) Evaluation metric values on the update
task.
Figure 4: Pearson?s r for all AESOP 2011 submitted
metrics and our proposed metrics. Our metrics are
circled. Higher r value is better.
course coherence model with newly introduced fea-
tures to evaluate summary readability. We com-
bined these two metrics in the CREMER metric
? an SVM-trained regression model ? for auto-
matic summarization overall responsiveness evalu-
ation. Experimental results on AESOP 2011 show
that DICOMER significantly outperforms all sub-
mitted metrics on both initial and update tasks with
large gaps, while TESLA-S and CREMER signifi-
cantly outperform all metrics on the initial task. 3
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative and
administered by the IDM Programme Office.
3Our metrics are publicly available at http://wing.
comp.nus.edu.sg/?linzihen/summeval/.
1013
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34:1?34, March.
John M. Conroy and Hoa Trang Dang. 2008. Mind
the gap: Dangers of divorcing evaluations of summary
content from linguistic quality. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (Coling 2008), Manchester, UK, August.
John M. Conroy, Judith D. Schlesinger, Jeff Kubina,
Peter A. Rankel, and Dianne P. O?Leary. 2011.
CLASSY 2011 at TAC: Guided and multi-lingual sum-
maries and evaluation metrics. In Proceedings of the
Text Analysis Conference 2011 (TAC 2011), Gaithers-
burg, Maryland, USA, November.
Paulo C. F. de Oliveira. 2011. CatolicaSC at TAC 2011.
In Proceedings of the Text Analysis Conference (TAC
2011), Gaithersburg, Maryland, USA, November.
George Giannakopoulos and Vangelis Karkaletsis. 2011.
AutoSummENG and MeMoG in evaluating guided
summaries. In Proceedings of the Text Analysis Con-
ference (TAC 2011), Gaithersburg, Maryland, USA,
November.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated summarization evalua-
tion with basic elements. In Proceedings of the Fifth
Conference on Language Resources and Evaluation
(LREC 2006).
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Schlkopf, Christopher J. C. Burges, and Alexander J.
Smola, editors, Advances in Kernel Methods ? Support
Vector Learning. MIT Press, Cambridge, MA, USA.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. the-
sis, Department of Artificial Intelligence, University
of Edinburgh.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2004).
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy (NAACL 2003), Morristown, NJ, USA.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
PDTB-styled end-to-end discourse parser. Technical
Report TRB8/10, School of Computing, National Uni-
versity of Singapore, August.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Au-
tomatically evaluating text coherence using discourse
relations. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), Port-
land, Oregon, USA, June.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. TESLA: Translation evaluation of sentences
with linear-programming-based analysis. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, Uppsala, Sweden. As-
sociation for Computational Linguistics.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of the 2004 Human Language
Technology Conference / North American Chapter of
the Association for Computational Linguistics Annual
Meeting (HLT-NAACL 2004), Boston, Massachusetts,
USA, May.
Jun Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen
Kan, and Chew Lim Tan. 2011. SWING: Exploit-
ing category-specific information for guided summa-
rization. In Proceedings of the Text Analysis Confer-
ence 2011 (TAC 2011), Gaithersburg, Maryland, USA,
November.
Karolina Owczarzak and Hoa Trang Dang. 2011.
Overview of the TAC 2011 summarization track:
Guided task and AESOP task. In Proceedings of the
Text Analysis Conference (TAC 2011), Gaithersburg,
Maryland, USA, November.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2002), Stroudsburg, PA, USA.
PDTB-Group, 2007. The Penn Discourse Treebank 2.0
Annotation Manual. The PDTB Research Group.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL 2010), Stroudsburg, PA, USA.
Renxian Zhang, You Ouyang, and Wenjie Li. 2011.
Guided summarization with aspect recognition. In
Proceedings of the Text Analysis Conference 2011
(TAC 2011), Gaithersburg, Maryland, USA, Novem-
ber.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL 2006), Strouds-
burg, PA, USA.
1014
