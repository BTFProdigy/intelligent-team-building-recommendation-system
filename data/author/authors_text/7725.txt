The TIGER 700 RMRS Bank:
RMRS Construction from Dependencies
Kathrin Spreyer
Computational Linguistics Department
Saarland University
66041 Saarbru?cken, Germany
kathrins@coli.uni-sb.de
Anette Frank
Language Technology Lab
DFKI GmbH
66123 Saarbru?cken, Germany
frank@dfki.de
Abstract
We present a treebank conversion
method by which we construct an
RMRS bank for HPSG parser evalu-
ation from the TIGER Dependency
Bank. Our method effectively per-
forms automatic RMRS semantics
construction from functional depen-
dencies, following the semantic alge-
bra of Copestake et al (2001). We
present the semantics construction
mechanism, and focus on some spe-
cial phenomena. Automatic conver-
sion is followed by manual valida-
tion. First evaluation results yield
high precision of the automatic se-
mantics construction rules.
1 Introduction
Treebanks are under development for many
languages. They are successfully exploited for
the induction of treebank grammars, train-
ing of stochastic parsers, and for evaluat-
ing and benchmarking competitive parsing
and grammar models. While parser evalu-
ation against treebanks is most natural for
treebank-derived grammars, it is extremely
difficult for hand-crafted grammars that rep-
resent higher-level functional or semantic in-
formation, such as LFG or HPSG grammars.
In a recent joint initiative, the TIGER
project provides dependency-based treebank
representations for German, on the basis of
the TIGER treebank (Brants et al, 2002).
Forst (2003) applied treebank conversion
methods to the TIGER treebank, to derive
an f-structure bank for stochastic training and
evaluation of a German LFG parser. A more
theory-neutral dependency representation is
currently derived from this TIGER-LFG tree-
bank for cross-framework parser evaluation
(Forst et al, 2004). However, while Penn-
treebank style grammars and LFG analyses
are relatively close to dependency represen-
tations, the output of HPSG parsing is diffi-
cult to match against such structures. HPSG
analyses do not come with an explicit repre-
sentation of functional structure, but directly
encode semantic structures, in terms of (Ro-
bust) Minimal Recursion Semantics (hence-
forth (R)MRS.1 This leaves a gap to be
bridged in terms of the encoding of argu-
ments vs. adjuncts, the representation of spe-
cial constructions like relative clauses, and
not least, the representation of quantifiers and
their (underspecified) scoping relations.
In order to bridge this gap, we construct
an RMRS ?treebank? from a subset of the
TIGER Dependency Bank (Forst et al, 2004),
which can serve as a gold standard for HPSG
parsing for evaluation, and for training of
stochastic HPSG grammar models. In con-
trast to treebanks constructed from analyses
of hand-crafted grammars, our treebank con-
1RMRS (Copestake, 2003) is a formalism for par-
tial semantic representation that is derived from MRS
(Copestake et al, 2005). It is designed for the in-
tegration of semantic representations produced by
NLP components of different degrees of partiality and
depth, ranging from chunk parsers and PCFGs to deep
HPSG grammars with (R)MRS output.
1
version approach yields a standard for com-
parative parser evaluation where the upper
bound for coverage is defined by the corpus
(here, German newspaper text), not by the
grammar.
Our method for treebank conversion effec-
tively performs priniciple-based (R)MRS se-
mantics construction from LFG-based depen-
dency representations, which can be extended
to a general parsing architecture for (R)MRS
construction from LFG f-structures.
The remainder of this paper is organised
as follows. Section 2 introduces the input
dependency representations provided by the
TIGER Dependency Bank, and describes the
main features of the term rewriting machinery
we use for treebank conversion, i.e., RMRS se-
mantics construction from dependency struc-
tures. Section 3 presents the core of the se-
mantics construction process. We show how
to adapt the construction principles of the se-
mantic algebra of Copestake et al (2001) to
RMRS construction from dependencies in a
rewrite scenario, and discuss the treatment of
some special phenomena, such as verbal com-
plementation, coordination and modification.
Section 4 reports on the treebank construc-
tion methodology, with first results of quality
control. Section 5 concludes.
2 From TIGER Dependency Bank
to TIGER RMRS Bank
2.1 The TIGER Dependency Bank
The input to the treebank conversion pro-
cess consists of dependency representations
of the TIGER Dependency Bank (TIGER-
DB). The TIGER-DB has been derived semi-
automatically from (a subset of) the TIGER-
LFG Bank of Forst (2003), which is in turn
derived from the TIGER treebank. The de-
pendency format is similar to the Parc 700
Dependency Bank (King et al, 2003). It ab-
stracts away from constituency in order to re-
main as theory-neutral as possible. So-called
dependency triples are sets of two-place pred-
icates that encode grammatical relations, the
arguments representing the head of the depen-
dency and the dependent, respectively. The
sb(mu?ssen~0, Museum~1)
oc inf(mu?ssen~0, weichen~3)
mood(mu?ssen~0, ind)
tense(mu?ssen~0, pres)
mod(Museum~1, privat~1001)
cmpd lemma(Museum~1, Privatmuseum)
case(Museum~1, nom)
gend(Museum~1, neut)
num(Museum~1, sg)
sb(weichen~3, Museum~1)
Figure 1: TIGER dependency representation
of sentence #8595: Privatmuseum muss wei-
chen ? Private museum deemed to vanish.
triples further retain a number of morpholog-
ical features from the LFG representations,
such as agreement features or tense informa-
tion. Figure 1 displays an example.
For the purpose of RMRS construction, the
triples format has advantages and disadvan-
tages. The LFG-derived dependencies offer
all the advantages of a functional as opposed
to a constituent-based representation. This
representation already filters out the seman-
tically inappropriate status of auxiliaries as
heads; their contribution is encoded by fea-
tures such as perf or fut, which can be
directly translated into features of semantic
event variables. Most importantly, the triples
localise dependencies which are not locally re-
alised in phrase structure (as in long-distance
constructions), so that there is no need for
additional mechanisms to identify the argu-
ments of a governing predicate. Moreover,
the dependency representation format is to a
large extent uniform across languages, in con-
trast to phrase-structural encoding. There-
fore, the dependency-based semantics con-
struction mechanism can be quickly ported to
other languages.
The challenges we face mainly concern a
lack of specific types of phrase structure in-
formation that are crucial for RMRS compo-
sition. Linear precedence, e.g., plays a crucial
role when it comes to multiple modification or
coordination. Yet, it is possible to reconstruct
the surface order from the indices attached to
the Pred values in the triples. Part-of-speech
information, which is useful to trigger differ-
ent types of semantics construction rules, can
be induced from the presence or absence of
2
certain morphological features, yet to a lim-
ited extent.
For our current purpose of treebank con-
version, we are dependent on the specific in-
put format of the TIGER-DB, while in a more
general parsing context, one could ensure that
missing information of this type is included in
the input to semantics construction.
2.2 Treebank Conversion
Similar to the TIGER to TIGER-DB conver-
sion (Forst, 2003; Forst et al, 2004), we are
using the term rewriting system of Crouch
(2005) for treebank conversion. Originally de-
signed for machine translation, the system is a
powerful rewriting tool that has been applied
to other tasks, such as frame semantics con-
struction (Frank and Erk, 2004), or induction
of knowledge representations (Crouch, 2005).
The input to the system consists of a
set of facts in a prolog-like term representa-
tion. The rewrite rules refer to these facts in
the left-hand side (LHS), either conjunctively
(marked by ?,?) or disjunctively (marked by
?|?). Expressions on the LHS may be negated
(by prefix ?-?), thereby encoding negative con-
straints for matching. A rule applies if and
only if all facts specified on the LHS are satis-
fied by the input set of facts. The right-hand
side (RHS) of a rewrite rule defines a conjunc-
tion of facts which are added to the input set
of facts if the rule applies. The system further
allows the user to specify whether a matched
fact will be consumed (i. e., removed from the
set of facts) or whether it will be retained in
the output set of facts (marked by prefix ?+?).2
The system offers powerful rule encoding
facilities in terms of macros and templates.
Macros are parameterized patterns of (possi-
bly disjunctive) facts; templates are parame-
terized abstractions over entire (disjunctive)
rule applications. These abstraction means
help the user to define rules in a perspicious
and modular way, and significantly enhance
2The system additionally features optional rules
(??=>?), as opposed to deterministic rewriting (?==>?).
However, given that the input structures for RMRS
construction are disambiguated, and since our target
structures are underspecified semantic structures, we
can define the semantics deterministically.
the maintainability of complex rule sets.
The processing of rules is strictly ordered.
The rules are applied in the order of textual
appearance. Each rule is tested against the
current input set of facts and, if it matches,
produces an output set of facts that provides
the input to the next rule in sequence. Each
rule applies concurrently to all distinct sets of
matching facts, i.e. it performs parallel appli-
cation in case of alternative matching facts.
3 RMRS Construction from
Dependencies
Within the framework of HPSG, every lex-
ical item defines a complete (R)MRS struc-
ture. The semantic representation of a phrase
is defined as the assembly and combination
of the RMRSs of its daughters, according to
semantic constraints, which apply in parallel
with syntactic constraints. In each compo-
sition step, the RMRSs of the daughters are
combined according to semantic composition
rules that define the semantic representation
of the phrase, cf. (Copestake et al, 2005). Fol-
lowing the scaffolding of the syntactic struc-
ture in this way finally yields the semantics of
the sentence.
For the present task, the input to semantics
construction is a dependency structure. As
established by work on Glue Semantics (Dal-
rymple, 1999), semantics construction from
dependency structures can in similar ways
proceed recursively, to deliver a semantic pro-
jection of the sentence. Note, however, that
the resource-based approach of Glue Seman-
tics leads to alternative derivations in case of
scope ambiguities, whereas RMRS targets an
underspecified semantic representation.
For (R)MRS construction from dependen-
cies we follow the algebra for semantics com-
position in Copestake et al (2001). In HPSG
implementations of this algebra, composition
is triggered by phrasal configurations. Yet,
the algebra is neutral with regard to the syn-
tactic representation, and can be transposed
to composition on the basis of dependency re-
lations, much alike the Glue framework.
However, the rewriting system we are using
is not suited for a typical recursive application
3
scheme: the rules are strictly ordered, and
each rule simultaneously applies to all facts
that satisfy the constraints in the LHS. That
is, the RMRS composition cannot recursively
follow the composition of dependents in the
input structure. In section 3.2 we present a
design of RMRS that is suited for this con-
current application scheme. Before, we briefly
sketch the semantic algebra.
3.1 An Algebra for Semantic
Construction
Copestake et al (2001) define a semantic en-
tity as a 5-tuple ?s1, s2, s3, s4, s5? such that
s1 is a hook, s2 is a (possibly empty) set of
holes, s3 and s4 are bags of Elementary Pred-
ications (EPs) and handle constraints, respec-
tively, and s5 is a set of equalities holding be-
tween variables. The hook is understood to
represent the externalised part of the seman-
tic entity as a pair of a handle and an index
(a variable). It is used for reference in compo-
sition: Hooks of semantic arguments fill holes
(or slots) of functors. Holes, in turn, record
gaps in a semantic representation which re-
main to be filled. They, too, are pairs of a
handle and an index; furthermore, holes are
labelled with the grammatical function they
bear syntactically. That is, the labels on holes
serve two purposes: They help determine the
appropriate operation of composition (see be-
low), and they link the semantics to syntax.3
EPs (predicate applications) represent the
binding of argument variables to their predi-
cators. An EP h : r(a1, . . . , an, sa1, . . . , sam)
consists of the EP?s handle (or label) h, a
relation r, and a list of zero or more vari-
able arguments a1, . . . , an, followed by zero or
more scopal arguments sa1, . . . , sam (denot-
ing handles) of the relation. Finally, the bag
3Copestake et al (2001) mention a third feature to
be included in the hook as an externally visible vari-
able, which they instantiate with the index of the con-
trolled subject in equi constructions and which is also
used to implement the semantics of predicative modifi-
cation. However, this feature is not crucial given that
the underlying syntactic structures represent depen-
dencies rather than immediate dominance relations,
and therefore make non-local information available lo-
cally. Likewise, the dependency scenario does not ne-
cessitate that modifiers externalise their ARG1 argu-
ment position (see section 3.3.3).
of handle constraints (Hcons) contains con-
ditions which (partially) specify the relations
between scopal arguments and their scope, i.e.
between the scopal argument and the handles
that may fill the hole.
The operators of semantic composition
opl1 , . . . , oplk are drawn from ? ? ? ? ?,
where ? is the set of all semantic entities, and
l1, . . . , lk correspond to the labels on holes:
An operator opli defines the composition of
a semantic head which has a hole labelled li
with the argument filling that hole as follows:
The result of opli(a1, a2) is undefined if a2 has
no hole labelled li, otherwise:
1. hook(opli(a1, a2)) = hook(a2);
2. holesl?(opli(a1, a2)) = holesl?(a1) ?
holesl?(a2) for all labels l? 6= li;
3. eps(opli(a1, a2)) = eps(a1)
? eps(a2);
4. eqs(opli(a1, a2)) = Tr(eqs(a1)? eqs(a2)?
{hook(a1) = holeli(a2)}); where Tr
stands for the transitive closure.
3.2 RMRS Design
As mentioned earlier, the concurrent nature of
rule application makes it impossible to pro-
ceed recursively in a scaffolding way, inher-
ent to tree-based analyses, since the rules ap-
ply simultaneously to all structures. RMRS
construction is therefore designed around one
designated ?global? RMRS. Instead of pro-
jecting and accumulating RMRS constraints
step-wise by recursive composition, we di-
rectly insert the meaning descriptions into a
single global RMRS. Otherwise, composition
strictly follows the semantic operations of the
algebra of Copestake et al (2001): the compo-
sition rules only refer to the hook and slots of
functors and arguments, to achieve the bind-
ing of argument variables and the encoding of
scope constraints.
Global and Lexical RMRSs. The global
RMRS features a top handle (Top, usually
the label of the matrix proposition), sets of
EPs (Rels) and handle constraints (Hcons),
respectively, as described in the algebra, and
a set of Ing constraints.4
4Whenever two labels are related via an Ing (in-
group) constraint, they can be understood to be con-
4
+pred(X,Pred),-mo( ,X), -spec( ,X),
+?s::?(X,SemX), +hook(SemX,Hook)
==> lb(Hook,Lb), var(Hook,Var)
&& add ep(Lb,ep rel,rel,Pred)
&& add ep(Lb,ep arg0,arg0,Var).
lexical RMRS: [
Hook
[
Lb Lb
Var Var
]]
global RMRS: ?
?
?
?
?
?
Rels
?
?
?
. . .,
[
Pred n
Lb Lb
Arg0 Var
]
, . . .
?
?
?
Hcons
{
. . .
}
Ing
{
. . .
}
?
?
?
?
?
?
Figure 2: A rule for nominals (top) with re-
sulting lexical and global RMRS (bottom).
In addition, every predicate in the depen-
dency structure projects a lexical RMRS. Lex-
ical RMRSs are semantic entities which con-
sist of only a hook (i.e. a label and a variable),
that makes the entity available for reference
by subsequent (composition) rules, whereas
the basic semantic content (which is deter-
mined on the basis of the predicate?s category,
and comprises, at least, EPs for the relation
and the ARG0)5 is uniformly maintained in
the bags of the global RMRS, yet still an-
chored to the lexical hook labels and variables.
Figure 2 shows an example of a lexical
RMRS with its links to the global RMRS, and
a simplified version of the corresponding rule:
The rule applies to predicates, i.e. pred fea-
tures, with a value Pred. It introduces the
lexical RMRS, i.e., the hook?s label and vari-
able, and adds the predicate?s basic semantic
content to the global RMRS, here the relation
represented by Pred and the ARG0 variable,
which is co-referent with the hook?s variable.
Composition. The semantic composition
of arguments and functors is driven by the
predicate arg(Fctor,N,Arg), where N en-
codes the argument position, Fctor and Arg
are indices of functor and argument, respec-
joined. This is relevant, e.g., for intersective modifi-
cation, since a quantifier that outscopes the modified
noun must also take scope over the modifier.
5The category information required to define the
concrete basic semantics is not explicit in the depen-
dencies, but is induced from the grammatical function
borne by the predicate, as well as the presence or ab-
sence of certain morphological features (section 2.1).
+arg(X,2,Arg), +g f(Arg,?oc fin?),
+comp form(Arg,dass)
get lb(X,LbX), get lb(Arg,LbA)
==> sort(Lb,h), sort(LbP,h)
&& add ep(LbX,ep arg2,argx,LbP)
&& add ep(LbP,ep rel,rel,?prpstn m rel?)
&& add ep(LbP,ep arg0,arg0,Lb)
&& add qeq(Lb,LbA).
lexical RMRSs:
X:
[
Hook
[
Lb LbX
]
]
Arg:
[
Hook
[
Lb LbA
]
]
global RMRS:
?
?
?
?
?
Rels
?
?
?
..,
?
?
X v
Lb LbX
Arg2 LbP
?
?,
?
?
prpstn m rel
Lb LbP
Arg0 Lb
?
?,
[
Arg v
Lb LbA
]
,..
?
?
?
Hcons
{
. . ., Lb qeq LbA , . . .
}
?
?
?
?
?
Figure 3: Sample argument binding rule trig-
gered by arg(X,2,Arg) (top), referred lexical
RMRSs and resulting global RMRS (bottom).
tively.6 We interpret the arg-predicate as a
slot/hole of the functor, such that the binding
of the argument to the functor comes down
to filling the hole, in the sense of the alge-
bra described above: This is steered by the
previously defined hooks of the two semantic
entities, in that the matching rule introduces
an EP with an attribute ARGN that is an-
chored to the externalised label in the func-
tor?s hook. The value of the attribute ARGN
is the hook variable or hook label of the argu-
ment, depending on the category. A slightly
more complicated example is shown in Figure
3, it involves the introduction of an additional
proposition and a scope constraint. This rule
performs the composition of a declarative fi-
nite clausal object (oc fin) with its verbal
head. It assigns a proposition relation as the
value of the verb?s ARG2, which in turn has
an ARG0 that takes scope over the hook label
of the matrix verb in the object clause.
In general, composition does not depend on
the order of rule applications. That is, the
fact that the system performs concurrent rule
6The arg predicates are introduced by a set of
preprocessing rules which reconstruct the argument
structure by referring to the local grammatical func-
tions of a predicate and testing for (morphological)
features typically borne by non-arguments. E.g.,
pron type( ,expl) identifies an expletive pronoun.
5
??
?
?
?
?
?
?
Rels
?
?
?
?
?
?
?
?
?
. . .,
?
?
?
?
wissen v
Lb h
Arg0 e
Arg1 x
Arg2 1
?
?
?
?
,
?
?
?
?
versammeln v
Lb 1
Arg0 e
Arg1 2
Arg2 x
?
?
?
?
,
?
?
?
?
pronoun q rel
Lb h
Arg0 2
Rstr 3
Body h
?
?
?
?
,
[
pron null u
Lb 4
Arg0 2
]
, . . .
?
?
?
?
?
?
?
?
?
Hcons
{
. . ., 3 qeq 4 , . . .
}
?
?
?
?
?
?
?
?
Figure 4: Control analysis with unspecified coreference in [. . .], als so gut wie er kaum
ein anderer die Studentenmassen [. . .] zu versammeln wu?te. ? [. . .] when hardly anybody
knew how to rally the crowd of students [. . .] as well as he did. (from corpus sentence # 8074).
applications in a cascaded rule set is not prob-
lematic for semantics construction. Though,
we have to ensure that every partial structure
is assigned a hook, prior to the application of
composition rules. This is ensured by stating
the rules for lexical RMRSs first.
Scope constraints. By introducing handle
constraints, we define restrictions on the pos-
sible scoped readings. This is achieved by
gradually adding qeq relations to the global
Hcons set. Typically, this constraint relates
a handle argument of a scopal element, e.g. a
quantifier, and the label of the outscoped el-
ement. However, we cannot always fully pre-
dict the interaction among several scoping el-
ements. This is the case, inter alia, for the
modification of verbs by more than one scopal
adverb. This ambiguity is modeled by means
of a UDRT-style underspecification, that is,
we leave the scope among the modifiers un-
specified, but restrict each to outscope the
verb handle.7
3.3 Selected Phenomena
3.3.1 Verbal complements.
The treebank distinguishes three kinds of ver-
bal complements: infinitival phrases govered
by a raising verb or by a control verb, and
finite clausal arguments.
Infinitival complements. Raising verbs
do not assign an ARG1, and the infinitival ar-
gument is bound via an additional proposition
which fills the ARG2 position of the governor.
A handle constraint requires the proposition
7This is in accordance with the German HPSG
grammar of Crysmann (2003), and will also be
adapted in the ERG (p.c. D. Flickinger).
to take scope over the label of the infinitive.
Modal verbs lend themselves most naturally
to the same analysis, by virtue of identical
annotation in the dependency triples.
The implementation of RMRS for equi
constructions relies on external lexicon re-
sources, since the underlying dependency
structures do not encode the coreference be-
tween the controlled subject and the exter-
nal controller. Instead, the controlee is anno-
tated as a null pronoun. In order to differ-
entiate subject from object control, we enrich
the transfer input with a list of static facts
s_control(Pred) and o_control(Pred), re-
spectively, which we extracted from the Ger-
man HPSG grammar (Crysmann, 2003). The
rules refer to these facts, and establish the ap-
propriate bindings. If no information about
coreference is available (due to sparse lexical
data), the controlled subject appears in the
RMRS as an unbound pronoun, as assumed
in the syntactic structure. This is shown in
Fig. 4. In the manual correction phase, these
cases are corrected in the output RMRS, by
introducing the missing control relation.
Finite complements. For finite clausal
complements we assume the basic analysis il-
lustrated in section 3.2. But finite clauses are
not necessarily declarative, they can also have
interrogative meaning. In RMRS, this dis-
tinction is typically drawn in a type hierarchy,
of which we assume a simplified version:
message m rel
prop ques m rel imp m rel
prpstn m rel int m rel
German embedded clauses are usually marked
by one of the complementizers dass (that)
6
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Rels
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
. . .,
?
?
?
?
?
def q rel
Lb 10
Arg0 4
Rstr 7
Body h
?
?
?
?
?
,
[
Achtung n
Lb 1
Arg0 4
]
,
?
?
?
?
?
?
?
?
und c
Lb h
Arg0 x
L hndl 13
R hndl 16
L index 4
R index 17
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
?
implicit conj rel
Lb 16
Arg0 17
L hndl 14
R hndl 15
L index 5
R index 6
?
?
?
?
?
?
?
?
,
?
?
?
?
?
def q rel
Lb 11
Arg0 5
Rstr 8
Body h
?
?
?
?
?
,
[
Zuneigung n
Lb 2
Arg0 5
]
,
?
?
?
?
?
def q rel
Lb 12
Arg0 6
Rstr 9
Body h
?
?
?
?
?
,
[
Liebe n
Lb 3
Arg0 6
]
, . . .
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Hcons
{
. . ., 7 qeq 1 , 8 qeq 2 , 9 qeq 3 , 13 qeq 10 , 14 qeq 11 , 15 qeq 12 , . . .
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 5: RMRS for the coordinate NP ihre Achtung, ihre Zuneigung und Liebe ? their esteem,
their affection and love (from corpus sentence # 8345).
or ob (whether), in initial position, but
may occur without it, though less fre-
quently. If a complementizer is present,
this is recorded as comp_form(_,dass) (resp.
comp_form(_,ob)), and we can fully deter-
mine the kind of message relation from its
lexical form, i.e., prpstn m rel for declar-
ative and int m rel for interrogative ones.
In the absence of an overt complementizer,
we could introduce the underspecified type
prop ques m rel, but rather chose to use a
default rule for the declarative reading prp-
stn m rel, which occurs far more often. This
reduces the manual correction effort.
3.3.2 Coordination
The HPSG analysis of coordinate structures
takes the form of a binary, right-branching
structure. Since semantics construction in
HPSG proceeds along this tree, an RMRS for
a coordinate phrase likewise mirrors the recur-
sive organisation of conjuncts in the syntax.
Each partial coordination introduces an im-
plicit conj rel, while the meaning contributed
by the lexical conjunction is conveyed in the
EP which spans the entire coordination.
By contrast, the dependency structures
preserve the flat LFG-analysis of coordina-
tion as a set of conjuncts. To overcome this
discrepancy between source and target struc-
tures, we define specialised rules that mimic
recursion in that they process the conjuncts
from right to left, two at a time, thereby build-
ing the desired, binary-structure semantics for
the coordination. Fig. 5 shows a sample out-
put RMRS for coordinated NPs.8 Note that
we posit the L/R hndl handle arguments to
outscope each label that takes scope over the
noun. This accounts for scope ambiguities
among quantifiers and scopal adjectives.
3.3.3 Recursive Modification
The algebra of Copestake et al (2001) de-
fines modifiers to externalise the variable of
the ARG1. This, however, runs into problems
when a construction needs to incorporate the
inherent event variable (ARG0) of a modifier
as an argument, as e.g. in recursive modifica-
tion. In these cases, the ARG0 variable is not
accessible as a hook for composition.
In contrast, we identify the hook vari-
able of modifiers with their ARG0 variable.
This enables a uniform account of recur-
sive intersective modification, since the in-
herent variable is legitimatly accessible via
the hook, whereas the ARG1?like any other
argument?is bound in a slot-filling opera-
tion.9 The corresponding rule and an example
output RMRS are displayed in Fig. 6: When-
ever the dependency relation mo is encoun-
tered, no matter what the exact pred value,
the semantics contributed by the head of the
8The semantic contribution of the possessive pro-
nouns has been neglected for ease of exposition.
9Similarly, this treatment of modification correctly
accounts for modification in coordination structures,
as in the NP ihrer munteren und farbenfreudigen In-
szenierung ? of her lively and colourful production
(from corpus sentence # 9821).
7
+mo(X,M), +pred(M,Pred),
-scopal(Pred),
+?s::?(M,SemM), +hook(SemM,Hook),
+lb(Hook,LbM),
get var(X,VarX), get lb(X,LbX)
==> var(Hook,VarM)
&& add ep(LbM,ep rel,rel,Pred)
&& add ep(LbM,ep arg0,arg0,VarM)
&& add ep(LbM,ep arg1,argx,VarX)
&& add ing(LbM,LbX).
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Rels
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
. . .,
?
?
?
liegen v
Lb 1
Arg0 2
Arg1 x
?
?
?
,
?
?
?
hoch r
Lb 3
Arg0 4
Arg1 2
?
?
?
,
?
?
?
sehr r
Lb 5
Arg0 e
Arg1 4
?
?
?
, . . .
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Ing
{
. . ., 3 ing 1 , 5 ing 3 , . . .
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 6: Rule defining the lexical RMRS for
modifiers (top), resulting global RMRS for
the recursive modification in liege [. . .] sehr
hoch ? [. . .] is at a very high level (from cor-
pus sentence # 8893).
dependency can be unambiguously identified
as the argument of the semantic head. In fact,
given that modifiers are in this way locally
annotated as mo dependents in the triples, we
can bind the ARG1 already when defining the
lexical RMRS of the modifier.
4 The TIGER 700 RMRS Bank
4.1 Design and methodology
Treebank Design. Our aim is to make avai-
lable manually validated RMRS structures for
700 sentences of the TIGER-DB. Since the
underlying data is contiguous newspaper text,
we chose to select a block of consecutive sen-
tences instead of a random sample. In this
way, the treebank can be further extended
by annotation of intersentential phenomena,
such as co-reference or discourse relations.
However, we have to accommodate for gaps,
due to sentences for which there are rea-
sonable functional-syntactic, but (currently)
no sound semantic analyses. This problem
arises for sentences involving, e.g., elliptical
constructions, or else ungrammatical or frag-
mented sentences. We will include, but ex-
plicitly mark such sentences for which we can
only obtain partial, but no fully sound seman-
tic analyses. We will correspondingly extend
the annotation set to yield a total of 700 cor-
rectly annotated sentences.
The composition rules are designed to
record their application by way of rule-specific
identifiers. These may serve as a filtering
means in case the analysis of certain phenom-
ena as assumed in the treebank is incompati-
ble with the grammar to be evaluated.
Quality Control. For compilation of a
manually controlled RMRS bank, we imple-
mented a cascaded approach for quality con-
trol, with a feedback loop between (i) and (ii):
(i) Manual sample-based error-detection.
We are using the application markers of
specific construction rules to select sample
RMRSs for phenomenon-based inspection, as
well as random sampling, in order to detect
problems that can be corrected by adjust-
ments of the automatic conversion procedure.
(ii) Adjustment of conversion rules. The
construction rules are modified to adjust er-
rors detected in the automatic conversion pro-
cess. Errors that cannot be covered by general
rules need to be manually corrected in (iii).
(iii) Manual control. Finally, we perform
manual control and correction of errors that
cannot be covered by automatic RMRS con-
struction. Here, we mark and separate the
phenomena that are not covered by the state-
of-the-art in RMRS-based semantic theory.
Viewing and editing support. The in-
spection of RMRSs is supported by convert-
ing the underlying XML format to HTML.
RMRSs can thus be comfortably viewed in
a browser, with highlighting of coreferences,
display of agreement features, and links of
EPs to the surface forms they originated from.
Correction is supported by an XSLT-based
interactive editing tool. It enables the user to
specify which EPs, arguments or constraints
are to be added/removed. With each change,
the HTML representation is updated, so that
the result is immediately visible for verifica-
tion. The tool features a simple mechanism
for version maintenance and retrieval, and
8
avg # of
abs # in % corrections/sent.
validated 700 100 2.24
odd 28 4
fully perfect 281 40.14 0
corrected 419 59.86 3.75
<5 corrections 601 85.86 0.96
avg # of
abs # in % corrections/sent.
validated 100 100 1.3
odd 5 5
fully perfect 68 68 0
corrected 32 32 4.2
<5 corrections 88 88 0.44
Table 1: Evaluation of current data set for
complete correction (top) and for correction
ignoring part-of-speech (bottom).
separate storage for fully validated structures.
4.2 First Results
The transfer grammar comprises 74 rewrite
rules for converting dependency structures to
RMRS, plus 34 macros and templates.
In a first validation experiment on the ba-
sis of 100 structures, we classified 20% of the
RMRSs as involving errors that can be cap-
tured by adjustments of the automatic con-
version rules (see step (ii) above), while 59%
were fully correct.10
After improvement of the rules we evalu-
ated the quality of the automatic construction
procedure by validating the 700 sentences of
the treebank. Average counts for this sam-
ple are 15.57 tokens/sentence, 15.92 depen-
dencies/sentence. Table 1 (top) summarises
the results. Of the 700 structures, 4% con-
tained phenomena which we do not analyse at
all. 40% required no correction at all. For the
59% that needed manual correction, the aver-
age count of units to be corrected per sentence
was 3.75. The number of RMRSs that needed
less than the average of corrections was 601,
i.e. 85.86%. The time needed for inspection
and correction was 5 mins 12 secs/sentence,
calculated on the entire data set.
Error analysis. A large portion of the er-
rors did not concern the RMRS as such, but
10This evaluation did not perform correction of part-
of-speech tags (cf. below, error analysis).
simply the part-of-speech tags, encoded in the
relation names. If part-of-speech errors are
ignored, the number of correct RMRSs in-
creases from 41% to 68%. The results of vali-
dation without part-of-speech correction, cal-
culated on a third sample of 100 sentences,
are given in Table 1 (bottom).
Significant structural errors arise primarily
in the context of modification. This is due
to the TIGER annotation scheme. For exam-
ple, certain adjunct clauses are embedded in
main clauses as mo dependents, yet the em-
bedding conjunction is, again, annotated as a
modifier of the embedded clause. This leads
to erroneous analyses. Refinement of the rules
could considerably improve accuracy, but dis-
tinguishing these cases from ordinary modifi-
cation is not always possible, due to missing
category information.
While modifiers turned out challenging in
the mapping from dependencies to semantics,
we did not observe many errors in the treat-
ment of arguments: the rules that map de-
pendents to semantic arg predicates yield a
very precise argument structure.
5 Conclusion
We presented a method for semantics con-
struction that converts dependency structures
to RMRSs as they are output by HPSG gram-
mars. By applying this method to the TIGER
Dependency Bank, we construct an RMRS
Bank that allows cross-framework parser eval-
uation for German. Our method for RMRS
construction can be transposed to dependency
banks for other languages, such as the PARC
700 Dependency Bank for English (King et
al., 2003). The choice of RMRS also ensures
that the semantic bank can be used for com-
parative evaluation of HPSG grammars with
low-level parsers that output partial seman-
tics in terms of RMRS, such as the RASP
parser of Carroll and Briscoe (2002).
While the formalism of (R)MRS has its ori-
gins in HPSG, we have shown that RMRS se-
mantics construction can be carried over to
dependency-based frameworks like LFG. In
future research, we will investigate how the
semantic algebra of Copestake et al (2001)
9
compares to Glue Semantics (Dalrymple,
1999). Our construction rules may in fact be
modified and extended to yield semantics con-
struction along the lines of Glue Semantics,
with hooks as resources and Rels, Hcons
and Ing sets as meaning language. In this sce-
nario, the composition rules would consume
the hook of the semantic argument, so that
resource-sensitivity is assured. Scope ambi-
guities would not result in alternative deriva-
tions, since RMRS makes use of scope under-
specification in the meaning language.
Related work in Dyvik et al (2005) presents
MRS construction from LFG grammars in
a correspondence architecture, where seman-
tics is defined as a projection in individ-
ual syntactic rules. Our architecture follows
a description-by-analysis (DBA) approach,
where semantics construction applies to fully
resolved syntactic structures. This architec-
ture is especially suited for the present task
of treebank creation, where grammars for a
given language may not have full coverage.
Also, in a DBA architecture, incomplete rule
sets can still yield partially annotated, i.e.,
unconnected semantic structures. Likewise,
this construction method can deal with par-
tially analysed syntactic input.
Finally, our method can be extended to a
full parsing architecture with deep semantic
output, where care should be taken to pre-
serve structural or categorial information that
we identified as crucial for the purpose of
principle-driven semantics construction.
Acknowledgements
The research reported here was conducted
in cooperation with the TIGER project and
has partially been supported by the project
QUETAL (DFKI), funded by the German
Ministry for Education and Research, grant
no. 01 IW C02. Special thanks go to Dan
Flickinger and Berthold Crysmann for advice
on theoretical and grammar-specific issues.
We also thank Martin Forst, who provided us
with the TIGER DB dependency structures,
Berthold Crysmann for providing HPSG lex-
ical resources, and Ulrich Scha?fer for XSLT-
scripts used for visualisation.
References
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER Treebank. In
Proceedings of the Workshop on Treebanks and
Linguistic Theories, Sozopol, Bulgaria.
C. Carroll and E. Briscoe. 2002. High precision
extraction of grammatical relations. In Proceed-
ings of COLING 2002, pages 134?140.
A. Copestake, A. Lascarides, and D. Flickinger.
2001. An Algebra for Semantic Construction in
Constraint-based Grammars. In Proceedings of
the ACL 2001, Toulouse, France.
A. Copestake, D. Flickinger, I. Sag, and C. Pol-
lard. 2005. Minimal Recursion Semantics. to
appear.
A. Copestake. 2003. Report on the Design of
RMRS. Technical Report D1.1a, University of
Cambridge, University of Cambridge, UK.
R. Crouch. 2005. Packed Rewriting for Mapping
Semantics to KR. In Proceedings of the Sixth
International Workshop on Computational Se-
mantics, IWCS-05, Tilburg, The Netherlands.
B. Crysmann. 2003. On the efficient implemen-
tation of German verb placement in HPSG. In
Proceedings of RANLP 2004, Bulgaria.
M. Dalrymple, editor. 1999. Semantics and Syn-
tax in Lexical Functional Grammar: The Re-
source Logic Approach. MIT Press.
H. Dyvik, V. Rose?n, and P. Meurer. 2005. LFG,
Minimal Recursion Semantics and Translation.
In Proceedings of the LFG 2005 Conference,
Bergen, Norway. to appear.
M. Forst, N. Bertomeu, B. Crysmann, F. Fouvry,
S. Hansen-Schirra, and V. Kordoni. 2004. To-
wards a Dependency-Based Gold Standard for
German Parsers: The Tiger Dependency Bank.
In S. Hansen-Schirra, S. Oepen, and H. Uszkor-
eit, editors, Proceedings of LINC 2004, Geneva,
Switzerland.
M. Forst. 2003. Treebank Conversion ? Estab-
lishing a testsuite for a broad-coverage LFG
from the TIGER treebank. In Proceedings of
LINC?03, Budapest, Hungary.
A. Frank and K. Erk. 2004. Towards an LFG
Syntax?Semantics Interface for Frame Seman-
tics Annotation. In A. Gelbukh, editor, Com-
putational Linguistics and Intelligent Text Pro-
cessing, LNCS, Vol. 2945. Springer, Heidelberg.
T.H. King, R. Crouch, S. Riezler, M. Dalrymple,
and R. Kaplan. 2003. The PARC 700 Depen-
dency Bank. In Proceedings of LINC 2003, Bu-
dapest, Hungary.
10
Projection-based Acquisition of a Temporal Labeller
Kathrin Spreyer?
Department of Linguistics
University of Potsdam
Germany
spreyer@uni-potsdam.de
Anette Frank
Dept. of Computational Linguistics
University of Heidelberg
Germany
frank@cl.uni-heidelberg.de
Abstract
We present a cross-lingual projection frame-
work for temporal annotations. Auto-
matically obtained TimeML annotations in
the English portion of a parallel corpus
are transferred to the German translation
along a word alignment. Direct projection
augmented with shallow heuristic knowl-
edge outperforms the uninformed baseline
by 6.64% F
1
-measure for events, and by
17.93% for time expressions. Subsequent
training of statistical classifiers on the (im-
perfect) projected annotations significantly
boosts precision by up to 31% to 83.95% and
89.52%, respectively.
1 Introduction
In recent years, supervised machine learning has be-
come the standard approach to obtain robust and
wide-coverage NLP tools. But manually annotated
training data is a scarce and expensive resource. An-
notation projection (Yarowsky and Ngai, 2001) aims
at overcoming this resource bottleneck by scaling
conceptually monolingual resources and tools to a
multilingual level: annotations in existing monolin-
gual corpora are transferred to a different language
along the word alignment to a parallel corpus.
In this paper, we present a projection framework
for temporal annotations. The TimeML specifica-
tion language (Pustejovsky et al, 2003a) defines an
annotation scheme for time expressions (timex for
? The first author was affiliated with Saarland University
(Saarbru?cken, Germany) at the time of writing.
John [met]
event
Mary [last night]
timex
.
John [traf]
event
Mary [gestern Abend]
timex
.
Figure 1: Annotation projection.
short) and events, and there are tools for the auto-
matic TimeML annotation of English text (Verha-
gen et al, 2005). Similar rule-based systems exist
for Spanish and Italian (Saquete et al, 2006). How-
ever, such resources are restricted to a handful of
languages.
We employ the existing TimeML labellers to an-
notate the English portion of a parallel corpus, and
automatically project the annotations to the word-
aligned German translation. Fig. 1 shows a simple
example. The English sentence contains an event
and a timex annotation. The event-denoting verb met
is aligned with the German traf, hence the latter also
receives the event tag. Likewise, the components of
the multi-word timex last night align with German
gestern and abend, respectively, and the timex tag is
transferred to the expression gestern abend.
Projection-based approaches to multilingual an-
notation have proven adequate in various domains,
including part-of-speech tagging (Yarowsky and
Ngai, 2001), NP-bracketing (Yarowsky et al, 2001),
dependency analysis (Hwa et al, 2005), and role se-
mantic analysis (Pado? and Lapata, 2006). To our
knowledge, the present proposal is the first to apply
projection algorithms to temporal annotations.
489
Cross-lingually projected information is typically
noisy, due to errors in the source annotations as
well as in the word alignment. Moreover, success-
ful projection relies on the direct correspondence
assumption (DCA, Hwa et al (2002)) which de-
mands that the annotations in the source text be
homomorphous with those in its (literal) transla-
tion. The DCA has been found to hold, to a sub-
stantial degree, for the above mentioned domains.
The results we report here show that it can also
be confirmed for temporal annotations in English
and German. Yet, we cannot preclude divergence
from translational correspondence; on the contrary,
it occurs routinely and to a certain extent systemat-
ically (Dorr, 1994). We employ two different tech-
niques to filter noise. Firstly, the projection process
is equipped with (partly language-specific) knowl-
edge for a principled account of typical alignment
errors and cross-language discrepancies in the reali-
sation of events and timexes (section 3.2). Secondly,
we apply aggressive data engineering techniques to
the noisy projections and use them to train statistical
classifiers which generalise beyond the noise (sec-
tion 5).
The paper is structured as follows. Section 2
gives an overview of the TimeML specification lan-
guage and compatible annotation tools. Section 3
presents our projection models for temporal annota-
tions, which are evaluated in section 4. Section 5
describes how we induce temporal labellers for Ger-
man from the projected annotations; section 6 con-
cludes.
2 Temporal Annotation
2.1 The TimeML Specification Language
The TimeML specification language (Pustejovsky
et al, 2003a)1 and annotation framework emerged
from the TERQAS workshop2 in the context of the
ARDA AQUAINT programme. The goal of the pro-
gramme is the development of question answering
(QA) systems which index content rather than plain
keywords. Semantic indexing based on the identifi-
cation of named entities in free text is an established
1A standardised version ISO-TimeML is in preparation, cf.
Schiffrin and Bunt (2006).
2See http://www.timeml.org/site/terqas/in
dex.html
method in QA and related applications. Recent years
have also seen advances in relation extraction, a vari-
ant of event identification, albeit restricted in terms
of coverage: the majority of systems addressing
the task use a pre-defined set of?typically domain-
specific?templates. In contrast, TimeML models
events in a domain-independent manner and pro-
vides principled definitions for various event classes.
Besides the identification of events, it addresses their
relative ordering and anchoring in time by integrat-
ing timexes in the annotation. The major contri-
bution of TimeML is the explicit representation of
dependencies (so-called links) between timexes and
events.
Unlike traditional accounts of events (e.g.,
Vendler (1967)), TimeML adopts a very broad
notion of eventualities as ?situations that happen
or occur? and ?states or circumstances in which
something obtains or holds true? (Pustejovsky et
al., 2003a); besides verbs, this definition includes
event nominals such as accident, and stative mod-
ifiers (prepared, on board). Events are annotated
with EVENT tags. TimeML postulates seven event
classes: REPORTING, PERCEPTION, ASPECTUAL, I-
ACTION, I-STATE, STATE, and OCCURRENCE. For
definitions of the individual classes, the reader is re-
ferred to Saur?? et al (2005b).
Explicit timexes are marked by the TIMEX3 tag.
It is modelled on the basis of Setzer?s (2001) TIMEX
tag and the TIDES TIMEX2 annotation (Ferro et al,
2005). Timexes are classified into four types: dates,
times, durations, and sets.
Events and timexes are interrelated by three kinds
of links: temporal, aspectual, and subordinating.
Here, we consider only subordinating links (slinks).
Slinks explicate event modalities, which are of cru-
cial importance when reasoning about the certainty
and factuality of propositions conveyed by event-
denoting expressions; they are thus directly rel-
evant to QA and information extraction applica-
tions. Slinks relate events in modal, factive, counter-
factive, evidential, negative evidential, or condi-
tional relationships, and can be triggered by lexical
or structural cues.
2.2 Automatic Labellers for English
The basis of any projection architecture are high-
quality annotations of the source (English) portion
490
e ? E temporal entity
l ? E ? E (subordination) link
ws ? Ws, wt ? Wt source/target words
al ? Al : Ws ? Wt word alignment
As ? as : E ? 2Ws source annotation
At ? at : projected target
(E ? As ? Al) ? 2Wt annotation
Table 1: Notational conventions.
of the parallel corpus. However, given that the pro-
jected annotations are to provide enough data for
training a target language labeller (section 5), man-
ual annotation is not an option. Instead, we use the
TARSQI tools for automatic TimeML annotation of
English text (Verhagen et al, 2005). They have been
modelled and evaluated on the basis of the Time-
Bank (Pustejovsky et al, 2003b), yet for the most
part rely on hand-crafted rules. To obtain a full tem-
poral annotation, the modules are combined in a cas-
cade. We are using the components for timex recog-
nition and normalisation (Mani and Wilson, 2000),
event extraction (Saur?? et al, 2005a), and identifica-
tion of modal contexts (Saur?? et al, 2006).3
3 Informed Projection
3.1 The Core Algorithm
Recall that TimeML represents temporal entities
with EVENT and TIMEX3 tags which are anchored
to words in the text. Slinks, on the other hand, are
not anchored in the text directly, but rather relate
temporal entities. The projection of links is there-
fore entirely determined by the projection of the en-
tities they are defined on (see Table 1 for the nota-
tion used throughout this paper): a link l = (e, e?)
in the source annotation as projects to the target an-
notation at iff both e and e? project to non-empty
sequences of words. The projection of the enti-
ties e, e? themselves, however, is a non-trivial task.
3TARSQI also comprises a component that introduces tem-
poral links (Mani et al, 2003); we are not using it here because
the output includes the entire tlink closure. Although Mani et al
(2006) use the links introduced by closure to boost the amount
of training data for a tlink classifier, this technique is not suit-
able for our learning task since the closure might easily propa-
gate errors in the automatic annotations.
a.. . . [ ws ]e . . . b. . . . [ ws ]e . . .
. . . [ wt ]e . . . . . . [ wtj wtj+1 ]e . . .
c. . . . [ wsi wsi+1 ]e . . .
. . . [ wtj wtj+1 wtj+2 ]e . . .
Figure 2: Projection scenarios: (a) single-word 1-to-
1, (b) single-word 1-to-many, (c) multi-word.
a. [ . . . ]e b. [ . . . ]e . . .[ . . . ]e?
wtj?2 wtj?1 wtj wtj+1 wt
Figure 3: Problematic projection scenarios: (a) non-
contiguous aligned span, (b) rivalling tags.
Given a temporal entity e covering a sequence as(e)
of tokens in the source annotation, the projection
model needs to determine the extent at(e, as, al) of
e in the target annotation, based on the word align-
ment al . Possible projection scenarios are depicted
in Fig. 2. In the simplest case (Fig. 2a), e spans a
single word ws which aligns with exactly one word
wt in the target sentence. In this case, the model
predicts e to project to wt. A single tagged word
with 1-to-many alignments (as in Fig. 2b) requires
a more thorough inspection of the aligned words. If
they form a contiguous sequence, e can be projected
onto the entire sequence as a multi-word unit. This
is problematic in a scenario such as the one shown in
Fig. 3a, where the aligned words do not form a con-
tiguous sequence. There are various strategies, de-
scribed in section 3.2, to deal with non-contiguous
cases. For the moment, we can adopt a conservative
approach which categorically blocks discontinuous
projections. Finally, Fig. 2c illustrates the projec-
tion of an entity spanning multiple words. Here, the
model composes the projection span of e from the
alignment contribution of each individual word ws
covered by e. Again, the final extent of the projected
entity is required to be contiguous.
With any of these scenarios, a problem arises
when two distinct entities e and e? in the source an-
491
1. project(as, al ):
2. at,C = ?
3. for each entity e defined by as:
4. at,C(e, as, al) =
SC
ws?as(e) proj(ws, e, as, al)
5. for each link l = (e, e?) defined over as:
6. if at,C(e, as, al) 6= ? and at,C(e?, as, al) 6= ?
7. then define l to hold for at,C
8. return at,C
where
proj(ws, e, as, al) = {wt ? Wt | (ws, wt) ? al ?
?e? ? as. e? 6= e ? wt 6? at,C(e?, as, al)}
and
[C
S =
?
S
S :
S
S is convex
? : otherwise
Figure 4: The projection algorithm.
notation have conflicting projection extents, that is,
when at(e, as, al) ? at(e?, as, al ) 6= ?. This is il-
lustrated in Fig. 3b. The easiest strategy to resolve
conflicts like these is to pick an arbitrary entity and
privilege it for projection to the target word(s) wt in
question. All other rivalling entities e? project onto
their remaining target words at(e?, as, al) \ {wt}.
Pseudocode for this word-based projection of
temporal annotations is provided in Fig. 4.
3.2 Incorporating Additional Knowledge
The projection model described so far is extremely
susceptible to errors in the word alignment. Re-
lated efforts (Hwa et al, 2005; Pado? and Lapata,
2006) have already suggested that additional lin-
guistic information can have considerable impact on
the quality of the projected annotations. We there-
fore augment the baseline model with several shal-
low heuristics encoding linguistic or else topologi-
cal constraints for the choice of words to project to.
Linguistically motivated filters refer to the part-of-
speech (POS) tags of words in the target language
sentence, whereas topological criteria investigate the
alignment topology.
Linguistic constraints. Following Pado? and La-
pata (2006), we implement a filter which discards
alignments to non-content words, for two reasons:
(i) alignment algorithms are known to perform
poorly on non-content words, and (ii) events as
well as timexes are necessarily content-bearing and
hence unlikely to be realised by non-content words.
This non-content (NC) filter is defined in terms of
POS tags and affects conjunctions, prepositions and
punctuation. In the context of temporal annotations,
we extend the scope of the filter such that it effec-
tively applies to all word classes that we deem un-
likely to occur as part of a temporal entity. There-
fore, the NC filter is actually defined stronger for
events than for timexes, in that it further blocks
projection of events to pronouns, whereas pronouns
may be part of a timex such as jeden Freitag ?ev-
ery Friday?. Moreover, events prohibit the projec-
tion to adverbs; this restriction is motivated by the
fact that events in English are frequently translated
in German as adverbials which lack an event read-
ing (cf. head switching translations like prefer to X
vs. German lieber X ?rather X?). We also devise an
unknown word filter: it applies to words for which
no lemma could be identified in the preprocessing
stage. Projection to unknown words is prohibited
unless the alignment is supported bidirectionally.
The strictness concerning unknown words is due to
the empirical observation that alignments which in-
volve such words are frequently incorrect.
In order to adhere to the TimeML specification, a
simple transformation ensures that articles and con-
tracted prepositions such as am ?on the? are included
in the extent of timexes. Another heuristics is de-
signed to remedy alignment errors involving auxil-
iary and modal verbs, which are not to be annotated
as events. If an event aligns to more than one word,
then this filter singles out the main verb or noun and
discards auxiliaries.
Topological constraints. In section 3.1, we de-
scribed a conservative projection principle which re-
jects the transfer of annotations to non-contiguous
sequences. That model sets an unnecessarily modest
upper bound on recall; but giving up the contiguity
requirement entirely is not sensible either, since it is
indeed highly unlikely for temporal entities to be re-
alised discontinuously in either source or target lan-
guage (noun phrase cohesion, Yarowsky and Ngai
(2001)). Based on these observations, we propose
two refined models which manipulate the projected
annotation span so as to ensure contiguity. One
492
model identifies and discards outlier alignments,
which actively violate contiguity; the other one adds
missing alignments, which form gaps. Technically,
both models establish convexity in non-convex sets.
Hence, we first have to come up with a backbone
model which is less restrictive than the baseline, so
that the convexation models will have a basis to op-
erate on. A possible backbone model at,0 is pro-
vided in (1).
(1) at,0(e, as, al) =
?
ws?as(e)
proj(ws, e, as, al )
This model simply gathers all words aligned with
any word covered by e in the source annotation, ir-
respective of contiguity in the resulting sequence of
words. Discarding outlier alignments is then for-
malised as a reduction of at,0?s output to (one of)
its greatest convex subset(s) (GCS). Let us call this
model at,GCS. In terms of a linear sequence of
words, at,GCS chooses the longest contiguous sub-
sequence. The GCS-model thus serves a filtering
purpose similar to the NC filter. However, whereas
the latter discards single alignment links on linguis-
tic grounds, the former is motivated by topological
properties of the alignment as a whole.
The second model, which fills gaps in the word
alignment, constructs the convex hull of at,0 (cf.
Pado? and Lapata (2005)). We will refer to this model
as at,CH. The example in (2) illustrates both models.
(2)
[ . . . ]e
?C : ?
GCS : {1, 2}
1 2 3 4 5 CH : {1, 2, 3, 4, 5}
Here, entity e aligns to the non-contiguous token
sequence [1, 2, 5], or equivalently, the non-convex
set {1, 2, 5}(= at,0(e)). The conservative base-
line at,C rejects the projection altogether, whereas
at,GCS projects to the tokens 1 and 2. The additional
padding introduced by the convex hull (at,CH) fur-
ther extends the projected extent to {1, 2, 3, 4, 5}.
Alignment selection. Although bi-alignments are
known to exhibit high precision (Koehn et al, 2003),
in the face of sparse annotations we use unidirec-
tional alignments as a fallback, as has been proposed
in the context of phrase-based machine translation
(Koehn et al, 2003; Tillmann, 2003). Furthermore,
we follow Hwa et al (2005) in imposing a limit on
the maximum number of words that a single word
may align to.
4 Experiments
Our evaluation setup consists of experiments con-
ducted on the English-German portion of the Eu-
roparl corpus (Koehn, 2005); specifically, we work
with the preprocessed and word-aligned version
used in Pado? and Lapata (2006): the source-target
and target-source word alignments were automati-
cally established by GIZA++ (Och and Ney, 2003),
and their intersection achieves a precision of 98.6%
and a recall of 52.9% (Pado?, 2007). The preprocess-
ing consisted of automatic POS tagging and lemma-
tisation.
To assess the quality of the TimeML projec-
tions, we put aside and manually annotated a de-
velopment set of 101 and a test set of 236 bi-
sentences.4 All remaining data (approx. 960K bi-
sentences) was used for training (section 5). We
report the weighted macro average over all possi-
ble subclasses of timexes/events, and consider only
exact matches. The TARSQI annotations exhibit
an F
1
-measure of 80.56% (timex), 84.64% (events),
and 43.32% (slinks) when evaluated against the En-
glish gold standard.
In order to assess the usefulness of the linguis-
tic and topological parameters presented in section
3.2, we determined the best performing combination
of parameters on the development set. Not surpris-
ingly, event and timex models benefit from the var-
ious heuristics to different degrees. While the pro-
jection of events can benefit from the NC filter, the
projection of timexes is rather hampered by it. In-
stead, it exploits the flexibility of the GCS convexa-
tion model together with a conservative limit of 2 on
per-word alignments. In the underlying data sample
of 101 sentences, the English-to-German alignment
direction appears to be most accurate for timexes.
Table 2 shows the results of evaluating the optimised
models on the test set, along with the baseline from
section 3.1 and a ?full? model which activates all
4The unconventional balance of test and development data is
due to the fact that a large portion of the annotated data became
available only after the parameter estimation phase.
493
events slinks time expressions
model prec recall F prec recall F prec recall F
timex-optimised 48.53 33.73 39.80 30.09 10.71 15.80 71.01 52.76 60.54
event-optimised 50.94 44.23 47.34 30.96 14.29 19.55 56.55 42.52 48.54
combined 50.98 44.36 47.44 30.96 14.29 19.55 71.75 52.76 60.80
baseline 52.26 33.46 40.80 26.98 10.71 15.34 49.53 37.80 42.87
full 51.10 40.42 45.14 29.95 13.57 18.68 73.74 54.33 62.56
Table 2: Performance of projection models over test data.
[. . .] must today decide [. . .]: [. . .] (108723)
[. . .] hat heute u?ber
1
[. . .] zu entscheiden, na?mlich u?ber
2
[. . .]
APPR VVINF APPR
Figure 5: Amending alignment errors.
heuristics. The results confirm our initial assump-
tion that linguistic and topological knowledge does
indeed improve the quality of the projected annota-
tions. The model which combines the optimal set-
tings for timexes and events outperforms the un-
informed baseline by 17.93% (timexes) and 6.64%
(events) F
1
-measure. However, exploration of the
model space on the basis of the (larger and thus pre-
sumably more representative) test set shows that the
optimised models do not generalise well. The test
set-optimised model activates all linguistic heuris-
tics, and employs at,CH convexation. For events,
projection considers bi-alignments with a fallback to
unidirectional alignments, preferably from English
to German; timex projection considers all alignment
links. This test set-optimised model, which we will
use to project the training instances for the maxi-
mum entropy classifier, achieves an F
1
-measure of
48.82% (53.15% precision) for events and 62.04%
(73.74% precision) for timexes.5
With these settings, our projection model is ca-
pable of repairing alignment errors, as shown in
Fig. 5, where the automatic word alignments are rep-
resented as arrows. The conservative baseline con-
sidering only bidirectional alignments discards all
5The model actually includes an additional strategy to ad-
just event and timex class labels on the basis of designated
FrameNet frames; the reader is referred to Spreyer (2007), ch.
4.5 for details.
event timex
data prec recall prec recall
all 53.15 45.14 73.74 53.54
best 75% 54.81 47.06 74.61 62.82
Table 3: Correlation between alignment probability
and projection quality.
alignments but the (incorrect) one to u?ber
1
. The op-
timised model, on the other hand, does not exclude
any alignments in the first place; the faulty align-
ments to u?ber
1
and u?ber
2
are discarded on linguistic
grounds by the NC filter, and only the correct align-
ment to entscheiden remains for projection.
5 Robust Induction
The projected annotations, although noisy, can be
exploited to train a temporal labeller for German.
As Yarowsky and Ngai (2001) demonstrate for POS
tagging, aggressive filtering techniques applied to
vast amounts of (potentially noisy) training data are
capable of distilling relatively high-quality data sets,
which may then serve as input to machine learn-
ing algorithms. Yarowsky and Ngai (2001) use the
Model-3 alignment score as an indicator for the
quality of (i) the alignment, and therefore (ii) the
projection. In the present study, discarding 25% of
the sentences based on this criterion leads to gains
in both recall and precision (Table 3). In accor-
dance with the TimeML definition, we further re-
strict training instances on the basis of POS tags by
basically re-applying the NC filter (section 3.2). But
even so, the proportion of positive and negative in-
stances remains heavily skewed?an issue which we
will address below by formulating a 2-phase classi-
494
prec recall F F
model event slink
1-step 83.48 32.58 46.87 17.01
1-step unk 83.88 32.19 46.53 16.87
2-step 83.95 34.44 48.84 19.06
2-step unk 84.21 34.30 48.75 19.06
timex
1-step 87.77 49.11 62.98
1-step unk 87.22 49.55 63.20
2-step 89.52 51.79 65.62
2-step unk 88.68 50.89 64.67
Table 4: Classifier performance over test data.
fication task.
The remaining instances6 are converted to feature
vectors encoding standard lexical and grammatical
features such as (lower case) lemma, POS, govern-
ing prepositions, verbal dependents, etc.7 For slink
instances, we further encode the syntactic subordi-
nation path (if any) between the two events.
We trained 4 classifiers,8 with and without
smoothing with artificial unknowns (Collins, 2003),
and as a 1-step versus a 2-step decision in which
instances are first discriminated by a binary classi-
fier, so that only positive instances are passed on to
be classified for a subclass. The performance of the
various classifiers is given in Table 4. Although the
overall F
1
-measure does not notably differ from that
achieved by direct projection, we observe a drastic
gain in precision, albeit at the cost of recall. With
almost 84% and 90% precision, this is an ideal start-
ing point for a bootstrapping procedure.
6 Discussion and Future Work
Clearly, the?essentially unsupervised?projection
framework presented here does not produce state-
of-the-art annotations. But it does provide an inex-
6Note that slink instances are constructed for event pairs, as
opposed to event and timex instances, which are constructed for
individual words.
7The grammatical features have been extracted from analy-
ses of the German ParGram LFG grammar (Rohrer and Forst,
2006).
8We used the opennlp.maxent package,
http://maxent.sourceforge.net/.
pensive and largely language-independent basis (a)
for manual correction, and (b) for bootstrapping al-
gorithms. In the future, we will investigate how
weakly supervised machine learning techniques like
co-training (Blum and Mitchell, 1998) could further
enhance projection, e.g. taking into account a third
language in a triangulation setting (Kay, 1997).
Acknowledgements
We would like to thank Sebastian Pado? for provid-
ing us with the aligned Europarl data, Inderjeet Mani
and Marc Verhagen for access to the TARSQI tools,
and James Pustejovsky for clarification of TimeML
issues. We would also like to thank the three anony-
mous reviewers for helpful comments.
References
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the 1998 Conference on Computational
Learning Theory, pages 92?100, July.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational Lin-
guistics, 29(4):589?637, December.
Bonnie J. Dorr. 1994. Machine Translation Divergences:
A Formal Description and Proposed Solution. Com-
putational Linguistics, 20(4):597?635.
Lisa Ferro, Laurie Gerber, Inderjeet Mani, Beth Sund-
heim, and George Wilson, 2005. TIDES 2005 Stan-
dard for the Annotation of Temporal Expressions,
September.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating Translational Correspon-
dence using Annotation Projection. In Proceedings of
ACL-2002, Philadelphia, PA.
R. Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas,
and Okan Kolak. 2005. Bootstrapping Parsers via
Syntactic Projection across Parallel Texts. Natural
Language Engineering, 11(3):311?325.
Martin Kay. 1997. The Proper Place of Men and Ma-
chines in Language Translation. Machine Translation,
12(1-2):3?23.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of HLT/NAACL 2003, pages 127?133.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
MT Summit 2005.
495
Inderjeet Mani and George Wilson. 2000. Robust Tem-
poral Processing of News. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL-2000), pages 69?76, Hong Kong.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring Temporal Ordering of Events in News.
In Proceedings of the Human Language Technology
Conference (HLT-NAACL-2003). Short paper.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine
Learning of Temporal Relations. In Proceedings of
ACL/COLING 2006, pages 753?760, Sydney, Aus-
tralia.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Sebastian Pado? and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of HLT/EMNLP 2005, Vancouver, BC.
Sebastian Pado? and Mirella. Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of ACL-COLING 2006, Syd-
ney, Australia.
Sebastian Pado?. 2007. Cross-Lingual Annotation Pro-
jection Models for Role-Semantic Information. Ph.D.
thesis, Saarland University, Saarbru?cken, Germany.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003a. TimeML: Robust Specification of Event
and Temporal Expressions in Text. In Proceedings
of the Fifth International Workshop on Computational
Semantics.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TimeBank Corpus. In Pro-
ceedings of Corpus Linguistics, pages 647?656.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG for
German. In Proceedings of LREC 2006, pages 2206?
2211, Genoa, Italy, May.
Estela Saquete, Patricio Mart??nez-Barco, Rafael Mun?oz,
Matteo Negri, Manuela Speranza, and Rachele Sprug-
noli. 2006. Multilingual Extension of a Temporal
Expression Normalizer using Annotated Corpora. In
Proceedings of the EACL 2006 Workshop on Cross-
Language Knowledge Induction, Trento, Italy, April.
Roser Saur??, Robert Knippen, Marc Verhagen, and
James Pustejovsky. 2005a. Evita: A Robust Event
Recognizer For QA Systems. In Proceedings of
HLT/EMNLP 2005, pages 700?707.
Roser Saur??, Jessica Littman, Bob Knippen, Robert
Gaizauskas, Andrea Setzer, and James Pustejovsky,
2005b. TimeML Annotation Guidelines Version 1.2.1,
October.
Roser Saur??, Marc Verhagen, and James Pustejovsky.
2006. SlinkET: A Partial Modal Parser for Events. In
Proceedings of LREC-2006, Genova, Italy, May. To
appear.
Amanda Schiffrin and Harry Bunt. 2006. Defining a
preliminary set of interoperable semantic descriptors.
Technical Report D4.2, INRIA-Loria, Nancy, France,
August.
Andrea Setzer. 2001. Temporal Information in Newswire
Articles: an Annotation Scheme and Corpus Study.
Ph.D. thesis, University of Sheffield, Sheffield, UK.
Kathrin Spreyer. 2007. Projecting Temporal Annotations
Across Languages. Diploma thesis, Saarland Univer-
sity, Saarbru?cken, Germany.
Christoph Tillmann. 2003. A Projection Extension Algo-
rithm for Statistical Machine Translation. In Michael
Collins and Mark Steedman, editors, Proceedings of
the 2003 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2003), pages 1?8.
Zeno Vendler, 1967. Linguistics in Philosophy, chapter
Verbs and Times, pages 97?121. Cornell University
Press, Ithaca, NY.
Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert
Knippen, Jessica Littman, and James Pustejovsky.
2005. Automating Temporal Annotation with
TARSQI. In Proceedings of the ACL-2005.
David Yarowsky and Grace Ngai. 2001. Inducing Mul-
tilingual POS Taggers and NP Bracketers via Robust
Projection across Aligned Corpora. In Proceedings of
NAACL-2001, pages 200?207.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing Multilingual Text Analysis Tools via
Robust Projection across Aligned Corpora. In Pro-
ceedings of HLT 2001, First International Conference
on Human Language Technology Research.
496
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 37?40,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Improving data-driven dependency parsing
using large-scale LFG grammars
Lilja ?vrelid, Jonas Kuhn and Kathrin Spreyer
Department of Linguistics
University of Potsdam
{lilja,kuhn,spreyer}@ling.uni-potsdam.de
Abstract
This paper presents experiments which
combine a grammar-driven and a data-
driven parser. We show how the con-
version of LFG output to dependency
representation allows for a technique of
parser stacking, whereby the output of the
grammar-driven parser supplies features
for a data-driven dependency parser. We
evaluate on English and German and show
significant improvements stemming from
the proposed dependency structure as well
as various other, deep linguistic features
derived from the respective grammars.
1 Introduction
The divide between grammar-driven and data-
driven approaches to parsing has become less pro-
nounced in recent years due to extensive work on
robustness and efficiency for the grammar-driven
approaches (Riezler et al, 2002; Cahill et al,
2008b). The linguistic generalizations captured in
such knowledge-based resources are thus increas-
ingly available for use in practical applications.
The NLP-community has in recent years wit-
nessed a surge of interest in dependency-based
approaches to syntactic parsing, spurred by the
CoNLL shared tasks of dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007).
Nivre and McDonald (2008) show how two differ-
ent approaches to dependency parsing, the graph-
based and transition-based approaches, may be
combined and subsequently learn to complement
each other to achieve improved parse results for a
range of different languages.
In this paper, we show how a data-driven depen-
dency parser may straightforwardly be modified to
learn directly from a grammar-driven parser. We
evaluate on English and German and show signifi-
cant improvements for both languages. Like Nivre
and McDonald (2008), we supply a data-driven
dependency parser with features from a different
parser to guide parsing. The additional parser em-
ployed in this work, is not however, a data-driven
parser trained on the same data set, but a grammar-
driven parser outputing a deep LFG analysis. We
furthermore show how a range of other features ?
morphological, structural and semantic ? from the
grammar-driven analysis may be employed dur-
ing data-driven parsing and lead to significant im-
provements.
2 Grammar-driven LFG-parsing
The XLE system (Crouch et al, 2007) performs
unification-based parsing using hand-crafted LFG
grammars. It processes raw text and assigns to it
both a phrase-structural (?c-structure?) and a fea-
ture structural, functional (?f-structure?).
In the work described in this paper, we employ
the XLE platform using the grammars available
for English and German from the ParGram project
(Butt et al, 2002). In order to increase the cover-
age of the grammars, we employ the robustness
techniques of fragment parsing and ?skimming?
available in XLE (Riezler et al, 2002).
3 Dependency conversion and feature
extraction
In extracting information from the output of the
deep grammars we wish to capture as much of the
precise, linguistic generalizations embodied in the
grammars as possible, whilst keeping with the re-
quirements posed by the dependency parser. The
process is illustrated in Figure 1.
3.1 Data
The English data set consists of the Wall Street
Journal sections 2-24 of the Penn treebank (Mar-
cus et al, 1993), converted to dependency format.
The treebank data used for German is the Tiger
37
f1
?
?
?
?
?
?
?
?
?
?
?
?
PRED ?halte?. . .??
VTYPE predicative
SUBJ ?pro?
OBJ
f2
?
?
PRED ?Verhalten?
CASE acc
SPEC f3?das?
ADJUNCT
{
f4?damalige?
}
?
?
XCOMP-PRED
?
?
PRED ?fu?r?. . .??
PTYPE nosem
OBJ
[
PRED ?richtig?
SUBJ
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
SUBJ
converted:
SPEC
XCOMP-PRED
ADJCT
SUBJ-OBJ
OBJ
Ich halte das damalige Verhalten fu?r richtig.
1sg pred. acc nosem
g
SB
old:
NK
OA
NK
MO
NK
Figure 1: Treebank enrichment with LFG output; German example: I consider the past behaviour cor-
rect.
treebank (Brants et al, 2004), where we employ
the version released with the CoNLL-X shared
task on dependency parsing (Buchholz and Marsi,
2006).
3.2 LFG to dependency structure
We start out by converting the XLE output to a
dependency representation. This is quite straight-
forward since the f-structures produced by LFG
parsers can be interpreted as dependency struc-
tures. The conversion is performed by a set of
rewrite rules which are executed by XLE?s built-
in extraction engine. We employ two strategies for
the extraction of dependency structures from out-
put containing multiple heads. We attach the de-
pendent to the closest head and, i) label it with the
corresponding label (Single), ii) label it with the
complex label corresponding to the concatenation
of the labels from the multiple head attachments
(Complex). The converted dependency analysis in
Figure 1 shows the f-structure and the correspond-
ing converted dependency output of a German ex-
ample sentence, where a raised object Verhalten
receives the complex SUBJ-OBJ label. Following
the XLE-parsing of the treebanks and the ensu-
ing dependency conversion, we have a grammar-
based analysis for 95.2% of the English sentence,
45238 sentences altogether, and 96.5% of the Ger-
man sentences, 38189 sentences altogether.
3.3 Deep linguistic features
The LFG grammars capture linguistic generaliza-
tions which may not be reduced to a dependency
representation. For instance, the grammars con-
tain information on morphosyntactic properties
such as case, gender and tense, as well as more se-
mantic properties detailing various types of adver-
bials, specifying semantic conceptual categories
such as human, time and location etc., see Fig-
ure 1. Table 1 presents the features extracted for
use during parsing from the German and English
XLE-parses.
4 Data-driven dependency parsing
MaltParser (Nivre et al, 2006a) is a language-
independent system for data-driven dependency
parsing which is freely available.1 MaltParser is
based on a deterministic parsing strategy in com-
bination with treebank-induced classifiers for pre-
dicting parse transitions. MaltParser constructs
parsing as a set of transitions between parse con-
figurations. A parse configuration is a triple
?S, I,G?, where S represents the parse stack, I is
the queue of remaining input tokens, and G repre-
sents the dependency graph defined thus far.
The feature model in MaltParser defines the rel-
evant attributes of tokens in a parse configuration.
Parse configurations are represented by a set of
features, which focus on attributes of the top of the
stack, the next input token and neighboring tokens
in the stack, input queue and dependency graph
under construction. Table 2 shows an example of
a feature model.2
For the training of baseline parsers we employ
feature models which make use of the word form
(FORM), part-of-speech (POS) and the dependency
relation (DEP) of a given token, exemplified in
Table 2. For the baseline parsers and all subse-
quent parsers we employ the arg-eager algorithm
in combination with SVM learners with a polyno-
mial kernel.3
1http://maltparser.org
2Note that the feature model in Table 2 is an example fea-
ture model and not the actual model employed in the parse
experiments. The details or references for the English and
German models are provided below.
3For training of the baseline parsers we also em-
ploy some language-specific settings. For English we
use learner and parser settings, as well as feature model
from the English pretrained MaltParser-model available from
http://maltparser.org. For German, we use the learner and
parser settings from the parser employed in the CoNLL-X
38
POS XFeats
Verb CLAUSETYPE, GOVPREP, MOOD, PASSIVE, PERF,
TENSE, VTYPE
Noun CASE, COMMON, GOVPREP, LOCATIONTYPE, NUM,
NTYPE, PERS, PROPERTYPE
Pronoun CASE, GOVPREP, NUM, NTYPE, PERS
Prep PSEM, PTYPE
Conj COORD, COORD-FORM, COORD-LEVEL
Adv ADJUNCTTYPE, ADVTYPE
Adj ATYPE, DEGREE
English DEVERBAL, PROG, SUBCAT, GENDSEM, HUMAN,
TIME
German AUXSELECT, AUXFLIP, COHERENT, FUT, DEF, GEND,
GENITIVE, COUNT
Table 1: Features from XLE output, common for
both languages and language-speciffic
FORM POS DEP XFEATS XDEP
S:top + + + + +
I:next + + + +
I:next?1 + +
G:head of top + +
G:leftmost dependent of top + +
InputArc(XHEAD)
Table 2: Example feature model; S: stack, I: input,
G: graph; ?n = n positions to the left(?) or right
(+).
5 Parser stacking
The procedure to enable the data-driven parser to
learn from the grammar-driven parser is quite sim-
ple. We parse a treebank with the XLE platform.
We then convert the LFG output to dependency
structures, so that we have two parallel versions
of the treebank ? one gold standard and one with
LFG-annotation. We extend the gold standard
treebank with additional information from the cor-
responding LFG analysis, as illustrated by Figure
1 and train the data-driven dependency parser on
the enhanced data set.
We extend the feature model of the baseline
parsers in the same way as Nivre and McDon-
ald (2008). The example feature model in Table
2 shows how we add the proposed dependency
relation (XDEP) top and next as features for the
parser. We furthermore add a feature which looks
at whether there is an arc between these two tokens
in the dependency structure (InputArc(XHEAD)),
with three possible values: Left, Right, None. In
order to incorporate further information supplied
by the LFG grammars we extend the feature mod-
els with an additional, static attribute, XFEATS.
This is employed for the range of deep linguistic
features, detailed in section 3.3 above.
5.1 Experimental setup
All parse experiments are performed using 10-fold
cross-validation for training and testing. Overall
parsing accuracy will be reported using the stan-
dard metrics of labeled attachment score (LAS)
and unlabeled attachment score (UAS).Statistical
significance is checked using Dan Bikel?s random-
ized parsing evaluation comparator.4
shared task (Nivre et al, 2006b). For both languages, we em-
ploy so-called ?relaxed? root handling.
4http://www.cis.upenn.edu/?dbikel/software.html
6 Results
We experiment with the addition of two types of
features: i) the dependency structure proposed by
XLE for a given sentence ii) other morphosyntac-
tic, structural or lexical semantic features provided
by the XLE grammar. The results are presented in
Table 3.
For English, we find that the addition of pro-
posed dependency structure from the grammar-
driven parser causes a small, but significant im-
provement of results (p<.0001). In terms of la-
beled accuracy the results improve with 0.15 per-
centage points, from 89.64 to 89.79. The introduc-
tion of complex dependency labels to account for
multiple heads in the LFG output causes a smaller
improvement of results than the single labeling
scheme. The corresponding results for German are
presented in Table 3. We find that the addition of
grammar-driven dependency structures with sin-
gle labels (Single) improves the parse results sig-
nificantly (p<.0001), both in terms of unlabeled
and labeled accuracy. For labeled accuracy we ob-
serve an improvement of 1.45 percentage points,
from 85.97 to 87.42. For the German data, we
find that the addition of dependency structure with
complex labels (Complex) gives a further small,
but significant (p<.03) improvement over the ex-
periment with single labels.
The results following the addition of the
grammar-extracted features in Table 1 (Feats) are
presented in Table 3.5 We observe significant im-
provements of overall parse results for both lan-
guages (p<.0001).
5We experimented with several feature models for the in-
clusion of the additional information, however, found no sig-
nificant differences when performing a forward feature selec-
tion. The simple feature model simply adds the XFEATS of
the top and next tokens of the parse configuration.
39
English German
UAS LAS UAS LAS
Baseline 92.48 89.64 88.68 85.97
Single 92.61 89.79 89.72 87.42
Complex 92.58 89.74 89.76 87.46
Feats 92.55 89.77 89.63 87.30
Single+Feats 92.52 89.69 90.01 87.77
Complex+Feats 92.53 89.70 90.02 87.78
Table 3: Overall results in experiments expressed as unlabeled and labeled attachment scores.
We also investigated combinations of the dif-
ferent sources of information ? dependency struc-
tures and deep features. These results are pre-
sented in the final lines of Table 3. We find
that for the English parser, the combination of
the features do not cause a further improve-
ment of results, compared to the individual ex-
periments. The combined experiments (Sin-
gle+Feats, Complex+Feats) for German, on the
other hand, differ significantly from the base-
line experiment, as well as the individual ex-
periments (Single,Complex,Feats) reported above
(p<.0001). By combination of the grammar-
derived features we improve on the baseline by
1.81 percentage points.
A comparison with the German results obtained
using MaltParser with graph-based dependency
structures supplied by MSTParser (Nivre and Mc-
Donald, 2008) shows that our results using a
grammar-driven parser largely corroborate the ten-
dencies observed there. Our best results for Ger-
man, combining dependency structures and addi-
tional features, slightly improve on those reported
for MaltParser (by 0.11 percentage points).6
7 Conclusions and future work
This paper has presented experiments in the com-
bination of a grammar-driven LFG-parser and a
data-driven dependency parser. We have shown
how the use of converted dependency structures
in the training of a data-driven dependency parser,
MaltParser, causes significant improvements in
overall parse results for English and German. We
have furthermore presented a set of additional,
deep features which may straightforwardly be ex-
tracted from the grammar-based output and cause
individual improvements for both languages and a
combined effect for German.
In terms of future work, a more extensive er-
ror analysis will be performed to locate the pre-
6English was not among the languages investigated in-
Nivre and McDonald (2008).
cise benefits of the parser combination. We will
also investigate the application of the method di-
rectly to raw text and application to a task which
may benefit specifically from the combined anal-
yses, such as semantic role labeling or semantic
verb classification.
It has recently been shown that automatically
acquired LFG grammars may actually outperform
hand-crafted grammars in parsing (Cahill et al,
2008a). These results add further to the relevance
of the results shown in this paper, bypassing the
bottleneck of grammar hand-crafting as a prereq-
uisite for the applicability of our results.
References
Sabine Brants, Stefanie Dipper, Peter Eisenberg, Silvia Hansen-Schirra, Esther
Knig, Wolfgang Lezius, Christian Rohrer, George Smith, and Hans Uszko-
reit. 2004. Tiger: Linguistic interpretation of a German corpus. Research
on Language and Computation, 2:597?620.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilin-
gual dependency parsing. In Proceedings of CoNLL-X).
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Masuichi, and
Christian Rohrer. 2002. The Parallel Grammar Project. In Proceedings
of COLING-2002 Workshop on Grammar Engineering and Evaluation.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan Riezler, Josef van Gen-
abith, and Andy Way. 2008a. Wide-coverage deep statistical parsing using
automatic dependency structure annotation. Computational Linguistics.
Aoife Cahill, John T. Maxwell, Paul Meurer, Christian Rohrer, and Victoria
Rosen. 2008b. Speeding up LFG parsing using c-structure pruning. In
Proceedings of the Workshop on Grammar Engineering Across Frame-
works.
D. Crouch, M. Dalrymple, R. Kaplan, T. King, J. Maxwell, and P. Newman,
2007. XLE Documentation. http://www2.parc.com/isl/.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large
annotated corpus for English: The Penn treebank. Computational Linguis-
tics, 19(2):313?330.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and
transition-based dependency parsers. In Proceedings of ACL-HLT 2008.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a. Maltparser: A data-driven
parser-generator for dependency parsing. In Proceedings of LREC.
Joakim Nivre, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, and Svetoslav Mari-
nov. 2006b. Labeled pseudo-projective dependency parsing with Support
Vector Machines. In Proceedings of CoNLL.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald, Jens Nilsson, Se-
bastian Riedel, and Deniz Yuret. 2007. CoNLL 2007 Shared Task on
Dependency Parsing. In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007, pages 915?932.
Stefan Riezler, Tracy King, Ronald Kaplan, Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing the Wall Street journal using a lexical-
functional grammar and discriminative estimation techniques. In Proceed-
ings of ACL.
40
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 12?20,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Data-Driven Dependency Parsing of New Languages
Using Incomplete and Noisy Training Data
Kathrin Spreyer and Jonas Kuhn
Department of Linguistics
University of Potsdam, Germany
{spreyer,kuhn}@ling.uni-potsdam.de
Abstract
We present a simple but very effective ap-
proach to identifying high-quality data in
noisy data sets for structured problems like
parsing, by greedily exploiting partial struc-
tures. We analyze our approach in an anno-
tation projection framework for dependency
trees, and show how dependency parsers from
two different paradigms (graph-based and
transition-based) can be trained on the result-
ing tree fragments. We train parsers for Dutch
to evaluate our method and to investigate
to which degree graph-based and transition-
based parsers can benefit from incomplete
training data. We find that partial correspon-
dence projection gives rise to parsers that out-
perform parsers trained on aggressively fil-
tered data sets, and achieve unlabeled attach-
ment scores that are only 5% behind the aver-
age UAS for Dutch in the CoNLL-X Shared
Task on supervised parsing (Buchholz and
Marsi, 2006).
1 Introduction
Many weakly supervised approaches to NLP rely on
heuristics or filtering techniques to deal with noise
in unlabeled or automatically labeled training data,
e.g., in the exploitation of parallel corpora for cross-
lingual projection of morphological, syntactic or se-
mantic information. While heuristic approaches can
implement (linguistic) knowledge that helps to de-
tect noisy data (e.g., Hwa et al (2005)), they are typ-
ically task- and language-specific and thus introduce
a component of indirect supervision. Non-heuristic
filtering techniques, on the other hand, employ re-
liability measures (often unrelated to the task) to
predict high-precision data points (e.g., Yarowsky
et al (2001)). In order to reach a sufficient level
of precision, filtering typically has to be aggressive,
especially for highly structured tasks like parsing.
Such aggressive filtering techniques incur massive
data loss and enforce trade-offs between the quality
and the amount of usable data.
Ideally, a general filtering strategy for weakly su-
pervised training of structured analysis tools should
eliminate noisy subparts in the automatic annota-
tion without discarding its high-precision aspects;
thereby data loss would be kept to a minimum.
In this paper, we propose an extremely simple ap-
proach to noise reduction which greedily exploits
partial correspondences in a parallel corpus, i.e.,
correspondences potentially covering only substruc-
tures of translated sentences. We implemented this
method in an annotation projection framework to
create training data for two dependency parsers rep-
resenting different parsing paradigms: The MST-
Parser (McDonald et al, 2005) as an instance of
graph-based dependency parsing, and the Malt-
Parser (Nivre et al, 2006) to represent transition-
based dependency parsing. In an empirical evalu-
ation, we investigate how they react differently to
incomplete and noisy training data.
Despite its simplicity, the partial correspondence
approach proves very effective and leads to parsers
that achieve unlabeled attachment scores that are
only 5% behind the average UAS for Dutch in the
CoNLL-X Shared Task (Buchholz and Marsi, 2006).
After a summary of related work in Sec. 2, we
discuss dependency tree projection (Sec. 3) and par-
tial correspondence (Sec. 4). In Sec. 5, we give an
overview of graph- and transition-based dependency
parsing and describe how each can be adapted for
training on partial training data in Sec. 6. Experi-
mental results are presented in Sec. 7, followed by
an analysis in Sec. 8. Sec. 9 concludes.
12
a. b. c.
English (L1): I have two questions You are absolutely right You are absolutely right
Dutch (L2): Ik heb twee vragen U heeft volkomen gelijk U heeft volkomen gelijk
1
2 3
Figure 1: Dependency tree projection from English to Dutch. (a) Ideal scenario with bidirectional alignments. (b)
Projection fails due to weak alignments. (c) Constrained fallback projection.
2 Related Work
Annotation projection has been applied to many dif-
ferent NLP tasks. On the word or phrase level, these
include morphological analysis, part-of-speech tag-
ging and NP-bracketing (Yarowsky et al, 2001),
temporal analysis (Spreyer and Frank, 2008), or se-
mantic role labeling (Pado? and Lapata, 2006). In
these tasks, word labels can technically be intro-
duced in isolation, without reference to the rest of
the annotation. This means that an aggressive filter
can be used to discard unreliable data points (words
in a sentence) without necessarily affecting high-
precision data points in the same sentence. By us-
ing only the bidirectional word alignment links, one
can implement a very robust such filter, as the bidi-
rectional links are generally reliable, even though
they have low recall for overall translational cor-
respondences (Koehn et al, 2003). The bidirec-
tional alignment filter is common practice (Pado? and
Lapata, 2006); a similar strategy is to discard en-
tire sentences with low aggregated alignment scores
(Yarowsky et al, 2001).
On the sentence level, Hwa et al (2005) were
the first to project dependency trees from English
to Spanish and Chinese. They identify unreliable
target parses (as a whole) on the basis of the num-
ber of unaligned or over-aligned words. In addition,
they manipulate the trees to accommodate for non-
isomorphic sentences. Systematic non-parallelisms
between source and target language are then ad-
dressed by hand-crafted rules in a post-projection
step. These rules account for an enormous increase
in the unlabeled f-score of the direct projections,
from 33.9 to 65.7 for Spanish and from 26.3 to 52.4
for Chinese. But they need to be designed anew for
every target language, which is time-consuming and
requires knowledge of that language.
Research in the field of unsupervised and weakly
supervised parsing ranges from various forms of EM
training (Pereira and Schabes, 1992; Klein and Man-
ning, 2004; Smith and Eisner, 2004; Smith and Eis-
ner, 2005) over bootstrapping approaches like self-
training (McClosky et al, 2006) to feature-based
enhancements of discriminative reranking models
(Koo et al, 2008) and the application of semi-
supervised SVMs (Wang et al, 2008). The partial
correspondence method we present in this paper is
compatible with such approaches and can be com-
bined with other weakly supervised machine learn-
ing schemes. Our approach is similar to that of
Clark and Curran (2006) who use partial training
data (CCG lexical categories) for domain adaptation;
however, they assume an existing CCG resource for
the language in question to provide this data.
3 Projection of Dependency Trees
Most state-of-the-art parsers for natural languages
are data-driven and depend on the availability of suf-
ficient amounts of labeled training data. However,
manual creation of treebanks is time-consuming and
labour-intensive. One way to avoid the expensive
annotation process is to automatically label the train-
ing data using annotation projection (Yarowsky et
al., 2001): Given a suitable resource (such as a
parser) in language L1, and a word-aligned paral-
lel corpus with languages L1 and L2, label the L1-
portion of the parallel text (with the parser) and copy
the annotations to the corresponding (i.e., aligned)
elements in language L2. This is illustrated in Fig.
1a. The arrows between English and Dutch words
indicate the word alignment. Assuming we have a
parser to produce the dependency tree for the En-
glish sentence, we build the tree for the Dutch sen-
tence by establishing arcs between words wD (e.g.,
Ik) and hD (heb) if there are aligned pairs (wD, wE)
13
#sents w/ avg. sent vocab
projected parse length (lemma)
unfiltered (100,000) 24.92 19,066
bidirectional 2,112 6.39 1,905
fallback 6,426 9.72 4,801
bi+frags?3 7,208 9.44 4,631
Table 1: Data reduction effect of noise filters.
(Ik and I) and (hD, hE) (heb and have) such that hE
is the head of wE in the English tree.
Annotation projection assumes direct correspon-
dence (Hwa et al, 2005) between languages (or
annotations), which?although it is valid in many
cases?does not hold in general: non-parallelism
between corresponding expressions in L1 and L2
causes errors in the target annotations. The word
alignment constitutes a further source for errors if it
is established automatically?which is typically the
case in large parallel corpora.
We have implemented a language-independent
framework for dependency projection and use the
Europarl corpus (Koehn, 2005) as the parallel text.
Europarl consists of the proceedings of the Euro-
pean Parliament, professionally translated in 11 lan-
guages (approx. 30mln words per language). The
data was aligned on the word level with GIZA++
(Och and Ney, 2003).1 In the experiments reported
here, we use the language pair English-Dutch, with
English as the source for projection (L1) and Dutch
as L2. The English portion of the Europarl cor-
pus was lemmatized and POS tagged with the Tree-
Tagger (Schmid, 1994) and then parsed with Malt-
Parser (which is described in Sec. 6), trained on a
dependency-converted version of the WSJ part from
the Penn Treebank (Marcus et al, 1994), but with
the automatic POS tags. The Dutch sentences were
only POS tagged (with TreeTagger).2
3.1 Data Loss Through Filtering
We quantitatively assess the impact of various fil-
tering techniques on a random sample of 100,000
English-Dutch sentence pairs from Europarl (avg.
1Following standard practice, we computed word align-
ments in both directions (L1 ? L2 and L2 ? L1); this gives
rise to two unidirectional alignments. The bidirectional align-
ment is the intersection of the two unidirectional ones.
2The Dutch POS tags are used to train the monolingual
parsers from the projected dependency trees (Sec. 7).
24.9 words/sentence). The English dependency
trees are projected to their Dutch counterparts as ex-
plained above for Fig. 1a.
The first filter we examine is the one that consid-
ers exclusively bidirectional alignments. It admits
dependency arcs to be projected only if the head hE
and the dependent wE are each aligned bidirection-
ally with some word in the Dutch sentence. This is
indicated in Fig. 1b, where the English verb are is
aligned with the Dutch translation heeft only in one
direction. This means that none of the dependencies
involving are are projected, and the projected struc-
ture is not connected. We will discuss in subsequent
sections how less restricted projection methods can
still incorporate such data.
Table 1 shows the quantitative effect of the bidi-
rectional filter in the row labeled ?bidirectional?. The
proportion of usable sentences is reduced to 2.11%.
Consequently, the vocabulary size diminishes by a
factor of 10, and the average sentence length drops
considerably from almost 25 to less than 7 words,
suggesting that most non-trivial examples are lost.
3.2 Constrained Fallback Projection
As an instance of a more relaxed projection of com-
plete structures, we also implemented a fallback to
unidirectional links which projects further depen-
dencies after a partial structure has been built based
on the more reliable bidirectional links. That is, the
dependencies established via unidirectional align-
ments are constrained by the existing subtrees, and
are subject to the wellformedness conditions for de-
pendency trees.3 Fig. 1c shows how the fallback
mechanism, initialized with the unconnected struc-
ture built with the bidirectional filter, recovers a
parse tree for the weakly aligned sentence pair in
Fig. 1b. Starting with the leftmost word in the Dutch
sentence and its English translation (U and You),
there is a unidirectional alignment for the head of
You: are is aligned to heeft, so U is established as
a dependent of heeft via fallback. Likewise, heeft
can now be identified as the root node. Note that the
(incorrect) alignment between heeft and You will not
be pursued because it would lead to heeft being a de-
pendent of itself and thus violating the wellformed-
3I.e., single headedness and acyclicity; we do not require the
trees to be projective, but instead train pseudo-projective models
(Nivre and Nilsson, 2005) on the projected data (cf. fn. 5).
14
#frags 1 2 3 4?15 >15
#words
<4 425 80 12 ? ?
4?9 1,331 1,375 1,567 4,793 ?
10?19 339 859 1,503 27,910 522
20?30 17 45 143 20,756 10,087
>30 0 5 5 4,813 23,362
Table 2: Fragmented parses projected with the alignment
filter. The sentences included in the data set ?bi+frags?3?
are in boldface.
ness conditions. Finally, the subtree rooted in gelijk
is incorporated as the second dependent of heeft.
As expected, the proportion of examples that pass
this filter rises, to 6.42% (Table 1, ?fallback?). How-
ever, we will see in Sec. 7 that parsers trained on
this data do not improve over parsers trained on the
bidirectionally aligned sentences alone. This is pre-
sumably due to the noise that inevitably enters the
training data through fallback.
4 Partial Correspondence Projection
So far, we have only considered complete trees,
i.e., projected structures with exactly one root node.
This is a rather strict requirement, given that even
state-of-the-art parsers sometimes fail to produce
plausible complete analyses for long sentences, and
that non-sentential phrases such as complex noun
phrases still contain valuable, non-trivial informa-
tion. We therefore propose partial correspondence
projection which, in addition to the complete anno-
tations produced by tree-oriented projection, yields
partial structures: It admits fragmented analyses in
case the tree-oriented projection cannot construct a
complete tree. Of course, the nature of those frag-
ments needs to be restricted so as to exclude data
with no (interesting) dependencies. E.g., a sentence
of five words with a parse consisting of five frag-
ments provides virtually no information about de-
pendency structure. Hence, we impose a limit (fixed
at 3 after quick preliminary tests on automatically
labeled development data) on the number of frag-
ments that can make up an analysis. Alternatively,
one could require a minimum fragment size.
As an example, consider again Fig. 1b. This ex-
ample would be discarded in strict tree projection,
but under partial correspondence it is included as a
partial analysis consisting of three fragments:
U heeft volkomen gelijk
Although the amount of information provided in
this analysis is limited, the arc between gelijk and
volkomen, which is strongly supported by the align-
ment, can be established without including poten-
tially noisy data points that are only weakly aligned.
We use partial correspondence in combination
with bidirectional projection.4 As can be seen in
Table 1 (?bi+frags?3?), this combination boosts the
amount of usable data to a range similar to that of
the fallback technique for trees; but unlike the latter,
partial correspondence continues to impose a high-
precision filter (bidirectionality) while improving re-
call through relaxed structural requirements (partial
correspondence). Table 2 shows how fragment size
varies with sentence length.
5 Data-driven Dependency Parsing
Models for data-driven dependency parsing can be
roughly divided into two paradigms: Graph-based
and transition-based models (McDonald and Nivre,
2007).
5.1 Graph-based Models
In the graph-based approach, global optimization
considers all possible arcs to find the tree T? s.t.
T? = argmax
T?D
s(T ) = argmax
T?D
?
(i,j,l)?AT
s(i, j, l)
where D is the set of all well-formed dependency
trees for the sentence, AT is the set of arcs in T , and
s(i, j, l) is the score of an arc between words wi and
wj with label l. The specific graph-based parser we
use in this paper is the MSTParser of McDonald et
al. (2005). The MSTParser learns the scoring func-
tion s using an online learning algorithm (Crammer
and Singer, 2003) which maximizes the margin be-
tween T? and D \ {T?}, based on a loss function that
counts the number of words with incorrect parents
relative to the correct tree.
5.2 Transition-based Models
In contrast to the global optimization employed in
graph-based models, transition-based models con-
struct a parse tree in a stepwise way: At each point,
4Fragments from fallback projection turned out not to be
helpful as training data for dependency parsers.
15
the locally optimal parser action (transition) t? is de-
termined greedily on the basis of the current config-
uration c (previous actions plus local features):
t? = argmax
t?T
s(c, t)
where T is the set of possible transitions. As a rep-
resentative of the transition-based paradigm, we use
the MaltParser (Nivre et al, 2006). It implements in-
cremental, deterministic parsing algorithms and em-
ploys SVMs to learn the transition scores s.
6 Parsing with Fragmented Trees
To make effective use of the fragmented trees pro-
duced by partial correspondence projection, both
parsing approaches need to be adapted for training
on sentences with unconnected substructures. Here
we briefly discuss how we represent these structures,
and then describe how we modified the parsers.
We use the CoNLL-X data format for dependency
trees (Buchholz and Marsi, 2006) to encode partial
structures. Specifically, every fragment root spec-
ifies as its head an artificial root token w0 (distin-
guished from a true root dependency by a special
relation FRAG). Thus, sentences with a fragmented
parse are still represented as a single sentence, in-
cluding all words; the difference from a fully parsed
sentence is that unconnected substructures are at-
tached directly under w0. For instance, the partial
parse in Fig. 1b would be represented as follows (de-
tails omitted):
(1) 1 U pron 0 FRAG
2 heeft verb 0 ROOT
3 volkomen adj 4 mod
4 gelijk noun 0 FRAG
6.1 Graph-based Model: fMST
In the training phase, the MSTParser tries to max-
imize the scoring margin between the correct parse
and all other valid dependency trees for the sentence.
However, in the case of fragmented trees, the train-
ing example is not strictly speaking correct, in the
sense that it does not coincide with the desired parse
tree. In fact, this desired tree is among the other
possible trees that MST assumes to be incorrect, or
at least suboptimal. In order to relax this assump-
tion, we have to ensure that the loss of the desired
tree is zero. While it is impossible to single out this
one tree (since we do not know which one it is), we
can steer the margin in the right direction with a loss
function that assigns zero loss to all trees that are
consistent with the training example, i.e., trees that
differ from the training example at most on those
words that are fragment roots (e.g., gelijk in Fig. 1).
To reflect this notion of loss during optimization, we
also adjust the definition of the score of a tree:
s(T ) = ?
(i,j,l)?AT : l 6=FRAG
s(i, j, l)
We refer to this modified model as f(iltering)MST.
6.2 Transition-based Model: fMalt
In the transition-based paradigm, it is particularly
important to preserve the original context (includ-
ing unattached words) of a partial analysis, because
the parser partly bases its decisions on neighboring
words in the sentence.
Emphasis of the role of isolated FRAG dependents
as context rather than proper nodes in the tree can
be achieved, as with the MSTParser, by eliminat-
ing their effect on the margin learned by the SVMs.
Since MaltParser scores local decisions, this simply
amounts to suppressing the creation of SVM train-
ing instances for such nodes (U and gelijk in (1)).
That is, where the feature model refers to context
information, unattached words provide this infor-
mation (e.g., the feature vector for volkomen in (1)
contains the form and POS of gelijk), but there are
no instances indicating how they should be attached
themselves. This technique of excluding fragment
roots during training will be referred to as fMalt.
7 Experiments
7.1 Setup
We train instances of the graph- and the transition-
based parser on projected dependencies, and occa-
sionally refer to these as ?projected parsers?.5
All results were obtained on the held-out
CoNLL-X test set of 386 sentences (avg. 12.9
5The MaltParsers use the projective Nivre arc-standard pars-
ing algorithm. For SVM training, data are split on the coarse
POS tag, with a threshold of 5,000 instances. MSTParser in-
stances use the projective Eisner parsing algorithm, and first-
order features. The input for both systems is projectivized using
the head+path schema (Nivre and Nilsson, 2005).
16
Malt MST
Alpino 80.05 82.43
EP 75.33 73.09
Alpino + EP 77.47 81.63
baseline 1 (previous) 23.65
baseline 2 (next) 27.63
Table 3: Upper and lower bounds (UAS).
words/sentence) from the Alpino treebank (van der
Beek et al, 2002). The Alpino treebank consists
mostly of newspaper text, which means that we are
evaluating the projected parsers, which are trained
on Europarl, in an out-of-domain setting, in the ab-
sence of manually annotated Europarl test data.
Parsing performance is measured in terms of un-
labeled attachment score (UAS), i.e., the proportion
of tokens that are assigned the correct head, irrespec-
tive of the label.6
To establish upper and lower bounds for our task
of weakly supervised dependency parsing, we pro-
ceed as follows. We train MaltParsers and MST-
Parsers on (i) the CoNLL-X training portion of the
Alpino treebank (195,000 words), (ii) 100,000 Eu-
roparl sentences parsed with the parser obtained
from (i), and (iii) the concatenation of the data
sets (i) and (ii). The first is a supervised upper
bound (80.05/82.43% UAS)7 trained on manually
labeled in-domain data, while the second constitutes
a weaker bound (75.33/73.09%) subject to the same
out-of-domain evaluation as the projected parsers,
and the third (77.47%) is a self-trained version of (i).
We note in passing that the supervised model does
not benefit from self-training. Two simple baselines
provide approximations to a lower bound: Baseline
1 attaches every word to the preceding word, achiev-
ing 23.65%. Analogously, baseline 2 attaches every
word to the following word (27.63%). These sys-
tems are summarized in Table 3.
6The labeled accuracy of our parsers lags behind the UAS,
because the Dutch dependency relations in the projected anno-
tations arise from a coarse heuristic mapping from the original
English labels. We therefore report only UAS.
7The upper bound models are trained with the same param-
eter settings as the projected parsers (see fn. 5), which were ad-
justed for noisy training data. Thus improvements are likely
with other settings: Nivre et al (2006) report 81.35% for a
Dutch MaltParser with optimized parameter settings. McDon-
ald et al (2006) report 83.57% with MST.
words Malt MST
a. trees (bidirectional) 13,500 65.94 67.76
trees (fallback) 62,500 59.28 65.08
bi+frags?3 68,000 55.09 57.14
bi+frags?3 (fMalt/fMST) 68,000 69.15 70.02
b. trees (bidirectional) 100,000 61.86 69.91
trees (fallback) 100,000 60.05 64.84
bi+frags?3 100,000 54.50 55.87
bi+frags?3 (fMalt/fMST) 100,000 68.65 69.86
c. trees (bidirectional) 102,300 63.32 69.85
trees (fallback) 465,500 53.45 64.88
bi+frags?3 523,000 51.48 57.20
bi+frags?3 (fMalt/fMST) 523,000 69.52 70.33
Table 4: UAS of parsers trained on projected dependency
structures for (a) a sample of 100,000 sentences, subject
to filtering, (b) 10 random samples, each with 100,000
words after filtering (average scores given), and (c) the
entire Europarl corpus, subject to filtering.
7.2 Results
Table 4a summarizes the results of training parsers
on the 100,000-sentence sample analyzed above.
Both the graph-based (MST) and the transition-
based (Malt) parsers react similarly to the more or
less aggressive filtering methods, but to different de-
grees. The first two rows of the table show the
parsers trained on complete trees (?trees (bidirec-
tional)? and ?trees (fallback)?). In spite of the ad-
ditional training data gained by the fallback method,
the resulting parsers do not achieve higher accuracy;
on the contrary, there is a drop in UAS, especially
in the transition-based model (?6.66%). The in-
creased level of noise in the fallback data has less
(but significant)8 impact on the graph-based coun-
terpart (?2.68%).
Turning to the parsers trained on partial cor-
respondence data (?bi+frags?3?), we observe even
greater deterioration in both parsing paradigms if the
data is used as is. However, in combination with the
fMalt/fMST systems (?bi+frags?3 (fMalt/fMST)?),
both parsers significantly outperform the tree-
8Significance testing (p<.01) was performed by means of
the t-test on the results of 10 training cycles (Table 4c ?trees
(fb.)? only 2 cycles due to time constraints). For the experiments
in Table 4a and 4c, the cycles differed in terms of the order in
which sentences where passed to the parser. In Table 4b we base
significance on 10 true random samples for training.
17
Recall Precision
dep. length 1 2 3?6 ?7 root 1 2 3?6 ?7 root
a. trees (bi.) 83.41 66.44 52.94 40.64 52.45 82.46 66.06 61.38 34.95 50.97
trees (fb.) 82.20 64.21 54.59 37.95 55.72 82.64 61.41 54.39 31.96 68.55
bi+frags?3 70.18 59.50 46.61 32.14 61.87 83.75 67.22 58.25 32.81 27.01
bi+frags?3 (fMalt) 89.23 75.34 59.18 41.65 59.06 83.46 69.05 65.85 48.21 75.79
Alpino-Malt 92.81 84.94 75.11 65.44 66.15 89.71 81.08 77.56 62.57 84.58
b. trees (bi.) 87.53 73.79 59.57 46.79 71.01 86.43 74.08 64.78 45.17 66.79
trees (fb.) 82.53 69.37 55.77 37.46 70.24 85.31 69.29 59.85 40.14 53.99
bi+frags?3 68.11 57.48 34.30 13.00 90.68 90.28 78.54 66.36 43.70 23.41
bi+frags?3 (fMST) 87.73 72.84 62.55 50.15 67.78 86.94 71.60 66.05 48.48 68.20
Alpino-MST 94.13 86.60 76.91 65.14 71.60 91.76 82.49 76.23 71.96 85.38
Table 5: Performance relative to dependency length. (a) Projected MaltParsers and (b) projected MSTParsers.
oriented models (?trees (bidirectional)?) by 3.21%
(Malt) and 2.26% (MST).
It would be natural to presume that the superior-
ity of the partial correspondence filter is merely due
to the amount of training data, which is larger by
a factor of 5.04. We address this issue by isolat-
ing the effect on the quality of the data, and hence
the success at noise reduction: In Table 4b, we con-
trol for the amount of data that is effectively used
in training, so that each filtered training set consists
of 100,000 words. Considering the Malt models, we
find that the trends suggested in Table 4a are con-
firmed: The pattern of relative performance emerges
even though any quantitative (dis-)advantages have
been eliminated.9 10 Interestingly, the MSTParser
does not appear to gain from the increased variety
(cf. Table 1) in the partial data: it does not differ
significantly from the ?trees (bi.)? model.
Finally, Table 4c provides the results of training
on the entire Europarl, or what remains of the corpus
after the respective filters have applied. The results
corroborate those obtained for the smaller samples.
In summary, the results support our initial hy-
pothesis that partial correspondence for sentences
containing a highly reliable part is preferable to
9The degree of skewedness in the filtered data is not con-
trolled, as it is an important characteristic of the filters.
10Some of the parsers trained on the larger data sets (Table
4b+c) achieve worse results than their smaller counterparts in
Table 4a. We conjecture that it is due to the thresholded POS-
based data split, performed prior to SVM training: Larger train-
ing sets induce decision models with more specialized SVMs,
which are more susceptible to tagging errors. This could be
avoided by increasing the threshold for splitting.
relaxing the reliability citerion, and?in the case
of the transition-based MaltParser?also to aggres-
sively filtering out all but the reliable complete trees.
With UASs around 70%, both systems are only 5%
behind the average 75.07% UAS achieved for Dutch
in the CoNLL-X Shared Task.
8 Analysis
We have seen that the graph- and the transition-
based parser react similarly to the various filtering
methods. However, there are interesting differences
in the magnitude of the performance changes. If
we compare the two tree-oriented filters ?trees (bi.)?
and ?trees (fb.)?, we observe that, although both Malt
and MST suffer from the additional noise that is in-
troduced via the unidirectional alignments, the drop
in accuracy is much less pronounced in the latter,
graph-based model. Recall that in this paradigm,
optimization is performed over the entire tree by
scoring edges independenly; this might explain why
noisy arcs in the training data have only a negligi-
ble impact. Conversely, the transition-based Malt-
Parser, which constructs parse trees in steps of lo-
cally optimal decisions, has an advantage when con-
fronted with partial structures: The individual frag-
ments provide exactly the local context, plus lexical
information about the (unconnected) wider context.
To give a more detailed picture of the differences
between predicted and actual annotations, we show
the performance (of the parsers from Table 4b) sep-
arately for binned arc length (Table 5) and sen-
tence length (Table 6). As expected, the perfor-
mance of both the supervised upper bounds (Alpino-
18
sent. length <4 4?9 10?19 20?30 > 30
a. trees (bi.) 73.87 62.13 65.67 60.81 55.18
trees (fb.) 69.91 57.84 62.29 60.04 55.47
bi+frags?3 74.14 54.40 56.62 54.07 48.95
bi+fr?3 (fMalt) 73.51 65.69 71.70 68.49 63.71
Alpino-Malt 81.98 69.81 81.11 82.82 76.02
b. trees (bi.) 76.67 70.16 73.09 69.56 63.57
trees (fb.) 73.24 64.93 67.79 64.98 57.70
bi+frags?3 77.48 59.65 55.96 55.27 52.74
bi+fr?3 (fMST) 73.24 67.84 73.46 70.04 62.92
Alpino-MST 81.98 72.24 85.10 83.86 78.51
Table 6: UAS relative to sentence length. (a) Projected
MaltParsers and (b) projected MSTParsers.
Malt/MST) and the projected parsers degrades as de-
pendencies get longer, and the difference between
the two grows. Performance across sentence length
remains relatively stable. But note that both tables
again reflect the pattern we saw in Table 4. Impor-
tantly, the relative ranking (in terms of f-score, not
shown, resp. UAS) is still in place even in long dis-
tance dependencies and long sentences. This indi-
cates that the effects we have described are not arti-
facts of a bias towards short dependencies.
In addition, Table 5 sheds some light on the im-
pact of fMalt/fMST in terms of the trade-off between
precision and recall. Without the specific adjust-
ments to handle fragments, partial structures in the
training data lead to an immense drop in recall. By
contrast, when the adapted parsers fMalt/fMST are
applied, they boosts recall back to a level compara-
ble to or even above that of the tree-oriented pro-
jection parsers, while maintaining precision. Again,
this effect can be observed across all arc lengths, ex-
cept arcs to root, which naturally the ?bi+frags? mod-
els are overly eager to predict.
Finally, the learning curves in Fig. 2 illus-
trate how much labeled data would be required to
achieve comparable performance in a supervised
setting. The graph-based upper bound (Alpino-
MST) reaches the performance of fMST (trained
on the entire Europarl) with approx. 25,000 words
of manually labeled treebank data; Alpino-Malt
achieves the performance of fMalt with approx.
35,000 words. The manual annotation of even these
moderate amounts of data involves considerable ef-
forts, including the creation of annotation guidelines
Figure 2: Learning curves for the supervised upper
bounds. They reach the performance of the projected
parsers with ?25,000 (MST) resp. 35,000 (Malt) words.
and tools, the training of annotators etc.
9 Conclusion
In the context of dependency parsing, we have pro-
posed partial correspondence projection as a greedy
method for noise reduction, and illustrated how it
can be integrated with data-driven parsing. Our ex-
perimental results show that partial tree structures
are well suited to train transition-based dependency
parsers. Graph-based models do not benefit as much
from additional partial structures, but instead are
more robust to noisy training data, even when the
training set is very small.
In future work, we will explore how well the tech-
niques presented here for English and Dutch work
for languages that are typologically further apart,
e.g., English-Greek or English-Finnish. Moreover,
we are going to investigate how our approach, which
essentially ignores unknown parts of the annotation,
compares to approaches that marginalize over hid-
den variables. We will also explore ways of combin-
ing graph-based and transition-based parsers along
the lines of Nivre and McDonald (2008).
Acknowledgments
The research reported in this paper has been sup-
ported by the German Research Foundation DFG as
part of SFB 632 ?Information structure? (project D4;
PI: Kuhn).
19
References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of CoNLL-X, pages 149?164, New York City,
June.
Stephen Clark and James R. Curran. 2006. Partial train-
ing for a lexicalized-grammar parser. In Proceed-
ings of HLT-NAACL 2006, pages 144?151, New York,
June.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Reseach, 3:951?991, Jan-
uary.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL
2004, pages 478?485, Barcelona, Spain.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 127?133.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
MT Summit 2005.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-HLT 2008), pages 595?603, Colum-
bus, Ohio, June.
Mitchell Marcus, Grace Kim, Mary Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT-NAACL 2006, pages 152?159, New York,
June.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL 2007, pages
122?131.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP 2005).
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of CoNLL-
X.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-HLT 2008, pages 950?958,
Columbus, Ohio, June.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of ACL 2005,
pages 99?106.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL-X, pages 221?225.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sebastian Pado? and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of COLING/ACL 2006, Syd-
ney, Australia.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of ACL 1992, pages 128?135.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, pages 44?
49, Manchester, England.
Noah A. Smith and Jason Eisner. 2004. Annealing
techniques for unsupervised statistical language learn-
ing. In Proceedings of ACL 2004, pages 487?494,
Barcelona, July.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of ACL 2005, pages 354?362, Ann Ar-
bor, MI, June.
Kathrin Spreyer and Anette Frank. 2008. Projection-
based acquisition of a temporal labeller. In Proceed-
ings of IJCNLP 2008, Hyderabad, India, January.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Computational Linguistics in the
Netherlands (CLIN).
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2008.
Semi-supervised convex training for dependency pars-
ing. In Proceedings of ACL-HLT 2008, pages 532?
540, Columbus, Ohio, June.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT 2001.
20
Coling 2010: Poster Volume, pages 1176?1184,
Beijing, August 2010
Notes on the Evaluation of Dependency Parsers
Obtained Through Cross-Lingual Projection
Kathrin Spreyer
Department of Linguistics
University of Potsdam
spreyer@uni-potsdam.de
Abstract
In this paper we address methodological
issues in the evaluation of a projection-
based framework for dependency parsing
in which annotations for a source lan-
guage are transfered to a target language
using word alignments in a parallel cor-
pus. The projected trees then constitute
the training data for a data-driven parser in
the target language. We discuss two prob-
lems that arise in the evaluation of such
cross-lingual approaches. First, the anno-
tation scheme underlying the source lan-
guage annotations ? and hence the pro-
jected target annotations and predictions
of the parser derived from them ? is likely
to differ from previously existing gold
standard test sets devised specifically for
the target language. Second, the stan-
dard procedure of cross-validation cannot
be performed in the absence of parallel
gold standard annotations, so an alterna-
tive method has to be used to assess the
generalization capabilities of the projected
parsers.
1 Introduction
The manual annotation of treebanks for natu-
ral language parsing is time-consuming and ex-
pensive, but the availability of such resources
is crucial for data-driven parsers, which require
large amounts of training examples. A technique
known as annotation projection (Yarowsky and
Ngai, 2001) provides a means to relax this re-
source bottleneck to some extent: In a word-
aligned parallel corpus, the text of one language
(source language, SL), say English, is annotated
with an existing parser, and the word alignments
are then used to transfer (or project) the result-
ing annotations to the other language (target lan-
guage, TL). The projected trees, albeit noisy, can
then constitute the training data for data-driven
TL parsers (Hwa et al, 2005; Spreyer and Kuhn,
2009). Finally, in order to assess the quality of the
projected parser, its output needs to be compared
to held-out TL test data.
Two problems arise in the evaluation of such
approaches. First, the annotations projected from
the SL usually differ stylistically from those found
in the TL test data, rendering any immediate com-
parison between the predictions of the projected
parser and the gold standard meaningless. We dis-
cuss the use of tree transformations for evaluation
purposes, namely to consolidate discrepancies be-
tween the annotation schemes. We then present
experiments that investigate the influence of the
annotation scheme used in training on the general-
ization capabilities of the resulting parser. We also
briefly address the interaction between annotation
style and parsing algorithm (transition-based vs.
graph-based).
The second problem addressed here is the as-
sessment of variance in the training data, and
hence in parser quality. The standard proce-
dure for this purpose would be cross-validation.
However, the popular data sets used for bench-
marking parsers, such as those that emerged
1176
from the CoNLL-X shared task on dependency
parsing (Buchholz and Marsi, 2006), are typi-
cally based on monolingual text. This means
that cross-validation is unavailable for projection-
based frameworks, because no projection can be
performed for the training splits in the absence of
a translation in the SL. We therefore propose a val-
idation scheme which accounts for training data
variance by training a parser multiple times, on
random samples drawn from the projected train-
ing data. Each of the obtained parsers can subse-
quently be evaluated against a fixed, held-out test
set independent of the projection step, and the ar-
ray of accuracy measurements thus obtained can
be further subjected to significance testing to ver-
ify that observed performance differences are not
merely random effects.
The paper is structured as follows. Section 2
describes the projection framework we are assum-
ing. Section 3 summarizes and contrasts the char-
acteristics of four different annotation schemes
underlying our SL parsers (English, German) and
TL test data (Dutch, Italian). Experiments with
different annotation schemes and parsing algo-
rithms are presented in Section 4. In Section 5 we
discuss variance assessment in more detail. Sec-
tion 6 concludes.
2 The Projection Framework
This section briefly describes how we obtain de-
pendency parsers for new languages via annota-
tion projection in a parallel corpus. A detailed dis-
cussion can be found in Spreyer and Kuhn (2009).
We use the Europarl corpus (Koehn, 2005) as
our parallel corpus. It comprises parallel data
from 11 languages; in this paper, we present ex-
periments with English and German as SLs, and
Dutch and Italian as TLs.
First, the bitexts for the language pairs un-
der consideration (English-Dutch, English-Italian,
German-Dutch, and German-Italian) are word-
aligned using Giza++ (Och and Ney, 2003), and
all texts are part-of-speech tagged with the Tree-
Tagger (Schmid, 1994) according to pre-trained
models.1
1Available from http://www.ims.
uni-stuttgart.de/projekte/corplex/
TreeTagger/DecisionTreeTagger.html.
the minutes of the sitting
de notulen van de vergadering
Figure 1: Dependency tree projection from En-
glish to Dutch.
Second, we annotate the SL portions, i.e., the
German and English texts, with MaltParser de-
pendency parsers (Nivre et al, 2006) trained on
standard data sets for the two languages; specifi-
cally, we are using the baseline parsers of ?vrelid
et al (2010). The English training data consists of
the Wall Street Journal sections 2?24 of the Penn
Treebank (Marcus et al, 1993), converted to de-
pendencies (Johansson and Nugues, 2007). The
treebank data used to train the German parser is
the Tiger Treebank (Brants et al, 2002), in the
version released with the CoNLL-X shared task
(Buchholz and Marsi, 2006).
Given the SL dependency trees, we project the
dependencies to the corresponding (i.e., aligned)
TL elements as shown in Figure 1. The links be-
tween the English and Dutch words indicate the
word alignment. We postulate edges between TL
words (e.g., de and notulen) if there is an edge
between their respective SL counterparts (the and
minutes).
The projected dependencies are then used as
training data for TL (Dutch and Italian) depen-
dency parsers. In order to account for the fact
that many of the projected dependency structures
are incomplete due to missing alignments or non-
parallelism of the translation, we employ fMalt
(Spreyer and Kuhn, 2009), a modified version of
the MaltParser which handles fragmented training
data. We restrict the admissible fragmentation to
three fragments per sentence, for sentences with
four or more words, based on early experiments
with automatically labeled Dutch data. Sentences
that receive more fragmented analyses are dis-
carded.
Finally, we evaluate the projected TL parsers
against gold standard test sets by parsing the
TL test data and comparing the parser output to
1177
PTB (en) Tiger (de) Alpino (nl) TUT (it)
NP/PP
Prep Det Noun Prep Det Noun Prep Det Noun Prep Det Noun
auxiliaries
Aux Verb Aux Verb Aux Verb Aux Verb
subord. clauses
Comp Verb Comp Verb Comp Verb Comp Verb
relative clauses
Rel Verb Rel Verb Rel Verb Rel Verb
coordination
X1 Conj X2 X1 Conj X2 X1 Conj X2 X1 Conj X2
Table 1: Different annotation schemes in dependency-converted treebanks.
the reference annotations. However, we discuss
below how differences in annotation style pro-
hibit a direct comparison, and how the annotation
schemes affect the learnability of the grammar and
therefore the accuracy of the derived parsers.
3 Annotation Schemes
In a projection setting like the one described
above, we deal with two sets of annotations: those
projected from the SL, and those marked up in the
TL gold standard. The four annotation schemes
we compare here are those used in the Penn Tree-
bank (PTB; WSJ sections) (Marcus et al, 1993)
for English, the Tiger Treebank (Brants et al,
2002) for German, the Alpino Treebank (van der
Beek et al, 2002) for Dutch, and the Turin Uni-
versity Treebank2 (TUT) for Italian.
Table 1 illustrates the most obvious differences
among the annotation schemes. Note that we
compare annotations in the dependency-converted
format. This restricts the comparison to attach-
ment decisions and eliminates the bracket bias in-
herent to constituent-based comparisons (Carroll
et al, 1998; Rehbein and van Genabith, 2007).
Again, we use the dependency-converted data sets
of the CoNLL-X shared task.
As shown in the table, both the English and the
2http://www.di.unito.it/
?
tutreeb
Dutch treebank annotate prepositional phrases hi-
erarchically, with an embedded NP. The flat an-
notation scheme of the German treebank, on the
other hand, makes every word in the PP a depen-
dent of the preposition (with some exceptions).
The Italian annotation scheme assumes a hierar-
chical structure like English and Dutch, but de-
clares the determiner rather than the noun as the
head of nominal phrases. Another idiosyncrasy
of the Italian annotation scheme is the treatment
of fused prepositions such as della which incor-
porate the determiner of the embedded NP: In the
dependency-converted TUT, these fused preposi-
tions are represented as two separate tokens, one
tagged as a preposition, the other as a determiner.
Next, auxiliaries take the lexical verb as their
dependent in all treebanks except the Italian TUT,
which inverts the dependency, resulting in a flat
structure with the lexical verb as its head. The
structure of subordinate clauses is hierarchical ac-
cording to the English, Dutch and Italian anno-
tation schemes, but flat in Tiger, with the com-
plementizer as a dependent of the embedded verb.
Relative clauses, on the other hand, are assigned
a flat structure in all but the Dutch scheme, where
the relativizer is the head of the embedded verb.
Finally, coordination is annotated in three differ-
ent ways: While the treebanks for English and
Italian implement a strictly right-branching strat-
1178
egy, the German annotation scheme attaches both
the conjunction and the second conjunct to the
first conjunct. The Dutch treebank annotates coor-
dinations as flat structures, with all conjuncts de-
pending on the conjunction.
In order to evaluate projected parsers, any dif-
ferences in the source and target annotations need
to be consolidated. A straightforward way of
doing so is by means of tree transformations.
Naturally, this begs the question of where such
transformations should take place: One could
transform the projected annotations to conform
to the reference annotations encountered in the
test set; alternatively, one can manipulate the test
set to reflect the annotation decisions adopted in
the source annotations. A variant of the former
approach has been implemented by Hwa et al
(2005). They apply post-projection transforma-
tions to Chinese training data projected from En-
glish in order to infuse TL-specific information
which has no counterpart in the source language.
We argue in favor of the alternative, since in a
practical application scenario, where rapid, inex-
pensive development plays a prominent role, it is
conceivable that the SL annotation scheme would
be adopted unaltered for the TL parser. Con-
sider, for instance, an architecture for multilingual
syntax-based information retrieval which is based
on parsers for various TLs, all to be derived from a
single SL. Devising a tailored annotation scheme
for each of the TLs would require linguistically
trained personnel with extensive knowledge of the
languages at hand. By contrast, adhering to the SL
annotation scheme results in homogeneous parser
output across the TLs and thus facilitates stream-
lined higher-level processing.
In Section 4 we present experiments that
involve the language pairs English?Dutch,
German?Dutch, English?Italian, and German?
Italian. For each of the TLs Dutch and Italian,
we therefore derive transformed test sets for each
SL: one version according to the English PTB
annotation style to evaluate the parsers projected
from English, and another version according to
the German Tiger-style annotations to evaluate
parsers projected from German. As an example,
Table 2 illustrates the transformations performed
on the Italian test set for the parser projected from
TUT (it) ? PTB (en)
NP/PP
Prep Det Noun ? Prep Det Noun
auxiliaries
Aux Verb ? Aux Verb
fused
prepositions
PrepDetp PrepDetd ? PrepDet
Table 2: Transformations performed on the Italian
test set for the parser projected from English.
a. lang orig PTB Tiger
nl ? 69.21 67.38
it ? 66.44 53.09
b. lang orig PTB Tiger
nl 79.23 80.79 79.19
it 88.52 86.88 84.02
Table 3: Unlabeled attachment scores obtained by
training MaltParsers on (a) projected and (b) gold
standard dependencies according to different an-
notation schemes.
English.
4 Annotation Scheme Experiments
4.1 Learnability
If the annotation style is carried over from the
source language as we suggest above, we may
ask: Is one annotation scheme more appropriate
than the other? When more than one source lan-
guage (annotation scheme) is available, will one
produce more ?learnable? TL annotations than the
other? We explore these questions experimentally.
Table 3a shows the performance of Dutch (?nl?)
and Italian (?it?) MaltParsers trained on annota-
tions projected from English (?PTB?) and German
(?Tiger?), as evaluated against the respective trans-
formed Dutch and Italian gold standards.
Looking at the results for Dutch, we find that
there is indeed a significant difference between
the parser projected from English and the one
projected from German. The former, generating
PTB-style dependencies, achieves 69.21% unla-
1179
lang. words/sent words/frag frags/sent
en?nl 27.83 1.95 14.25
de?nl 27.55 1.98 13.92
en?it 28.86 2.26 12.79
de?it 28.79 1.66 17.33
Table 4: Average fragmentation in the projected
dependencies.
beled attachment score (UAS). According to a t-
test (cf. Section 5), this is significantly (p<0.01)
better than the parser projected from German
Tiger-style annotations, which achieves 67.38%.
Turning to Italian, the parser projected from
the English PTB-style annotations again performs
better. However, the huge difference of 13.35%
UAS suggests a more fundamental underlying
problem with the word alignment between the
German and Italian sentences. And indeed, in-
spection of the degree of fragmentation in the Ital-
ian projected dependencies (Table 4) confirms that
considerably more edges are missing in the de-
pendencies projected from German than from En-
glish. Missing edges are an indication of missing
word alignment links.
In order to control such factors and focus
only on the learnability of the different anno-
tation schemes, we report in Table 3b the re-
sults of training on gold standard monolingual
treebank data (distinct from the test data), trans-
formed ? like the test sets ? to conform with the
English and German annotation scheme, respec-
tively.3 In addition, the column labeled ?orig?
shows the performance obtained when the origi-
nal (dependency-converted) Alpino/TUT annota-
tion scheme is used. For Italian, the results cor-
roborate those obtained with the projected parsers:
training on the PTB-transformed treebank is sig-
nificantly4 (p<0.01) more effective than training
on the Tiger-transformed treebank. The origi-
nal TUT scheme is even more effective (p<0.01),
which comes as no surprise given that the TUT
guidelines were tailored to the traits of the Italian
3We did not attempt parameter optimization, so the fig-
ures reported here do not represent the state-of-the-art in de-
pendency parsing for either language.
4According to Dan Bikel?s Randomized Parsing Eval-
uation Comparator: http://www.cis.upenn.edu/
?
dbikel/software.html#comparator
parser orig PTB Tiger
MST 81.41 83.01 83.87
Tiger ? PTB > orig
Malt 79.23 80.79 79.19
PTB > orig > Tiger
Table 5: UAS of the Dutch MST parsers trained
on gold standard dependencies. (MaltParser re-
sults repeated from Table 3b.)
language.
The Dutch parser, too, responds better to the
hierarchical PTB-based annotation scheme than
to the flat Tiger scheme (p<0.01). In fact, it
also outperforms the parser trained with the orig-
inal Alpino annotations (p<0.01). This demands
for further investigation, reported in the following
section.
4.2 Interaction with Parsing Algorithms
The results in Table 3 affirm that the performance
of a parser hinges on the annotation scheme that
it is trained on. However, the learnability of a
given scheme depends not only on the annotation
decisions, but also on the parsing algorithm im-
plemented by the parser. For instance, it has been
noted (Joakim Nivre, p.c. 2008) that flat coordina-
tion structures like those in the Alpino Treebank
generally pose a challenge to incremental, deter-
ministic parsers like MaltParser.
In order to see to what extent our results are
influenced by characteristics of the MaltParser,
we repeated the experiments with the MST parser
(McDonald et al, 2005), focusing on Dutch
parsers from gold standard training data.5
The MST parser is a graph-based dependency
parser which considers all possible edges to find
the globally optimal tree. The results of the MST
experiments are given in Table 5, together with
the corresponding Malt results repeated from Ta-
ble 3b. We observe that the relative learnability
ranking among the three annotation schemes is in-
deed different with MST. While in the transition-
based paradigm the original Alpino annotations
still appeared more adequate for training than the
5With projected training data for Dutch, and in all ex-
periments with Italian, MST produced the same pattern of
relative performance as Malt.
1180
trans Malt MST
none 79.23 81.41
coordinationen 80.91 83.01
relativeen 79.21 81.81
allen 80.79 83.01
coordinationde 79.39 82.19
relativede 79.21 81.81
subordde 79.47 82.67
np/ppde 80.73 83.83
allde 79.19 83.87
Table 6: Impact of individual transformations on
Dutch treebank parsers. Significant improvements
(p<0.01) over original Alpino annotation (?none?)
are in bold face.
Tiger trees, it is now outperformed by both the
PTB and the Tiger trees under the graph-based ap-
proach. There is no significant difference between
the Tiger-based and the PTB-based parser.
To shed some light on the unexpected rank-
ing of the Alpino annotation scheme, we look at
the impact of the individual transformations sep-
arately in Table 6. The upper part of the table
shows how the transformations of the Alpino data
towards PTB-style annotations affects learnabil-
ity. We find that both the MaltParser and the MST
parser benefit from the right-branching coordina-
tion markup of the PTB scheme. The attachment
of relativizers in relative clauses seems to play
only a minor role and makes no significant dif-
ference.
Turning to the Tiger-style transformations, first
note that the semi-flat coordination adopted in the
German treebank does not seem to be superior to
the flat annotations in Alpino: no significant im-
provement is achieved for either parser by using
the former (?coordinationde?). Surprisingly, both
parsers benefit from the flat annotation of prepo-
sitional phrases (?np/ppde?). The MST parser, but
not the MaltParser, further takes advantage of the
flat subordination structure annotated in Tiger. As
mentioned earlier, this is in line with the funda-
mentally different parsing paradigms represented
by Malt and MST.
We tentatively conclude that the MST parser
is in fact better at exploiting the flat aspects of
the Tiger annotations, while both parsers largely
benefit from the highly hierarchical coordination
structure of the PTB annotation scheme. A more
detailed exploration of these issues is clearly in
order, and subject to future research.
4.3 Discussion
Ku?bler et al (2008) present an extensive compar-
ison of two German treebanks: the Tiger treebank
with its rather flat annotation scheme, and the
Tu?Ba/DZ treebank with more hierarchical struc-
tures. They find that the flat Tiger annotation
scheme is more easily learned by constituent-
based (PCFG) parsers when evaluated on a depen-
dency level. Our results suggest the opposite, but
this may well be due to the differences in the ex-
perimental setup: Our training data represent de-
pendency trees directly, and we learn incremen-
tal, deterministic dependency parsers rather than
PCFGs.
5 Variance Assessment
The second question we address in this paper is
the assessment of variance in the training data,
and hence in parser quality. The standard proce-
dure for this purpose would be cross-validation.
To perform k-fold cross-validation, the data is par-
titioned into k splits of equal size, and one of the
splits is used as test data, while the remaining k-1
splits serve as training data. The train?test cycle is
repeated until each of the k subsamples has been
used as test data exactly once.
However, the popular data sets used for bench-
marking parsers, such as the CoNLL-X shared
task data used here, are typically based on mono-
lingual text. This means that cross-validation is
unavailable for projection-based frameworks, be-
cause no projection can be performed for the train-
ing splits in the absence of a translation in the SL.
Moreover, the expected noise level in the pro-
jected dependencies requires that there be a con-
siderable amount of training data for an evaluation
to be meaningful. So even if parallel test data is
available, the data partitioning performed in cross-
validation may compromise the results.
We therefore propose a validation scheme
which (i) does not reduce the amount of test data
by partitioning (this may be a problem when only
a small number of gold standard annotations is
1181
nlptb nltig itptb ittig
68.51 67.25 66.56 54.01
70.07 66.79 66.45 54.21
69.21 68.13 66.07 53.37
69.45 68.29 66.47 52.77
68.47 67.31 66.74 52.55
69.07 66.97 66.20 53.66
69.99 67.87 66.56 52.70
69.71 66.43 66.37 52.70
68.77 67.11 66.05 52.08
68.83 67.67 66.96 52.82
mean 69.21 67.38 66.44 53.09
sd 0.58 0.60 0.29 0.69
Table 7: Intra-system variance assessment.
available), (ii) does not require parallel test data
and is independent of the projection step, and (iii)
takes advantage of the fact that training data is
cheap and therefore abundant in projection-based
settings. Specifically, given that we have plenty
of training data, we can train a particular parser
multiple (say, k) times, each time sampling a
fixed number of training examples from the pool
of training data. The k parsers can then each
parse the unseen test set, and subsequent compar-
ison against the gold standard annotations yields
k values of the performance metric at hand (here,
UAS). As in conventional cross-validation, these
k values are then averaged to provide an aggre-
gated score, and they can be used to derive stan-
dard deviations etc. The arrays of measurements
for different systems can further be subjected to
significance tests such as the two-sample t-test to
verify that observed performance differences are
not merely random effects.
5.1 Experiments
We use the validation procedure just described
(with k=10) to investigate the variance in the pro-
jected parsers discussed in the previous section
(Table 3a). Table 7 lists the scores obtained by
the individual parsers, each trained on a different
random sample of 100,000 words, drawn from the
pool of all projected annotations. We also show
the standard deviation and repeat the mean UAS.
We observe that, for a given language, standard
deviation seems to correlate negatively with mean
UAS; in other words, the better parsers also seem
to be more robust towards variance in the training
data.
5.2 Discussion
Classical cross-validation and the validation
method described here do measure slightly dif-
ferent things. First, in cross-validation it is not
only the training data that is varied, but the test
data as well. Second, when two systems are com-
pared under the cross-validation regime, the k
rounds can usually be considered paired samples
because both systems are trained and evaluated
on identical partitionings of the data. In contrast,
projection-based settings typically involve some
form of filtering on the basis of the projected an-
notations; in our case, the filter restricts the de-
gree of fragmentation in the projected dependency
tree. This filtering makes it all but impossible
to pair the training samples without seriously di-
minishing the pool from which the samples are
drawn. For instance, when comparing the Italian
parser projected from English (itptb) and the one
projected from German (ittig), a training sentence
may receive a complete analysis from the English
translation, and hence be included in the training
pool for itptb; but the same (Italian) sentence may
receive a highly fragmented analysis under projec-
tion from German (e.g., due to missing alignment
links) and be discarded from the training pool for
ittig.
With samples that cannot be paired, it is also
not obvious how evaluation strategies like the
randomized comparison mentioned above (fn. 4)
could be employed in a sound way (by non-
statisticians).
6 Conclusions
We have discussed two issues that arise in the
evaluation of frameworks that involve cross-
lingual projection of annotations. We focused on
the projection of dependency trees from German
and English to Dutch and Italian, and presented
experiments that compare parsers trained on the
projected dependencies. The parsers differ in the
annotation scheme they follow: When they are
projected from German, they employ the flat Tiger
annotation scheme of the source language; pro-
1182
jected from English, they learn the more hierar-
chical PTB structures. In order to evaluate the
projected parsers against target language (Dutch,
Italian) gold standard annotations, we convert the
test sets to the annotation scheme employed in the
respective source language.
While our experiments with gold standard tree-
bank data affirm that the annotation scheme that
is being learned has some influence on the perfor-
mance of the parser, one should bear in mind that
in a projection scenario, the quality of the word
alignment plays at least an equally important role
when it comes to chosing a suitable source lan-
guage and annotation scheme.
We have further proposed a validation scheme
which unlike cross-validation does not require
parallel test data. Instead, it exploits the fact that
training data is usually available in abundance in
projection scenarios, so parsers can be trained on
multiple random samples and evaluated against a
single, independent test set which need not be fur-
ther partitioned.
Acknowledgments
The work reported in this paper was supported
by the Deutsche Forschungsgemeinschaft (DFG;
German Research Foundation) in the SFB 632 on
Information Structure, project D4 (Methods for
interactive linguistic corpus analysis).
References
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, pages 24?41.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of CoNLL-X, pages 149?164, New
York City, June.
Carroll, John, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proceedings of LREC 1998, pages 447?
454, Granada, Spain.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
Johansson, Richard and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Nivre, J., H.-J. Kaalep, and M. Koit, ed-
itors, Proceedings of NODALIDA 2007, pages 105?
112.
Koehn, Philipp. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the MT Summit 2005.
Ku?bler, Sandra, Wolfgang Maier, Ines Rehbein, and
Yannick Versley. 2008. How to Compare Tree-
banks. In Proceedings of LREC 2008, pages 2322?
2329.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP 2005.
Nivre, Joakim, Johan Hall, Jens Nilsson, Gu?ls?en
Eryig?it, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of CoNLL-X,
pages 221?225, New York City, June.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
?vrelid, Lilja, Jonas Kuhn, and Kathrin Spreyer. 2010.
Cross-framework parser stacking for data-driven de-
pendency parsing. To appear in TAL 2010 special
issue on Machine Learning for NLP 50(3), eds. Is-
abelle Tellier and Mark Steedman.
Rehbein, Ines and Josef van Genabith. 2007. Tree-
bank annotation schemes and parser evaluation for
German. In Proceedings of EMNLP-CoNLL 2007,
pages 630?639, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49, Manchester, England.
Spreyer, Kathrin and Jonas Kuhn. 2009. Data-driven
dependency parsing of new languages using incom-
plete and noisy training data. In Proceedings of
CoNLL 2009, pages 12?20, Boulder, CO, June.
van der Beek, Leonoor, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Computational Linguistics in the
Netherlands (CLIN).
1183
Yarowsky, David and Grace Ngai. 2001. Inducing
Multilingual POS Taggers and NP Bracketers via
Robust Projection across Aligned Corpora. In Pro-
ceedings of NAACL 2001, pages 200?207.
1184
