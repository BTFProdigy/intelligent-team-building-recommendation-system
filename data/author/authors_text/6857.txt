Wide-Coverage Deep Statistical Parsing
Using Automatic Dependency
Structure Annotation
Aoife Cahill?
Dublin City University
Michael Burke??,?
Dublin City University
IBM Center for Advanced Studies
Ruth O?Donovan??
Dublin City University
Stefan Riezler?
Palo Alto Research Center
Josef van Genabith??,?
Dublin City University
IBM Center for Advanced Studies
Andy Way??,?
Dublin City University
IBM Center for Advanced Studies
A number of researchers have recently conducted experiments comparing ?deep? hand-crafted
wide-coverage with ?shallow? treebank- and machine-learning-based parsers at the level of
dependencies, using simple and automatic methods to convert tree output generated by the
shallow parsers into dependencies. In this article, we revisit such experiments, this time using
sophisticated automatic LFG f-structure annotation methodologies with surprising results. We
compare various PCFG and history-based parsers to find a baseline parsing system that fits
best into our automatic dependency structure annotation technique. This combined system of
syntactic parser and dependency structure annotation is compared to two hand-crafted, deep
constraint-based parsers, RASP and XLE. We evaluate using dependency-based gold standards
? Now at the Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Germany. E-mail: aoife.
cahill@ims.uni-stuttgart.de.
?? National Centre for Language Technology, Dublin City University, Dublin 9, Ireland.
? IBM Dublin Center for Advanced Studies (CAS), Dublin 15, Ireland.
? Now at Google Inc., Mountain View, CA.
Submission received: 24 August 2005; revised submission received: 20 March 2007; accepted for publication:
2 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 1
and use the Approximate Randomization Test to test the statistical significance of the results.
Our experiments show that machine-learning-based shallow grammars augmented with so-
phisticated automatic dependency annotation technology outperform hand-crafted, deep, wide-
coverage constraint grammars. Currently our best system achieves an f-score of 82.73% against
the PARC 700 Dependency Bank, a statistically significant improvement of 2.18% over the
most recent results of 80.55% for the hand-crafted LFG grammar and XLE parsing system
and an f-score of 80.23% against the CBS 500 Dependency Bank, a statistically significant
3.66% improvement over the 76.57% achieved by the hand-crafted RASP grammar and parsing
system.
1. Introduction
Wide-coverage parsers are often evaluated against gold-standard CFG trees (e.g.,
Penn-II WSJ Section 23 trees) reporting traditional PARSEVALmetrics (Black et al 1991)
of labeled and unlabeled bracketing precision, recall and f-score measures, number of
crossing brackets, complete matches, and so forth. Although tree-based parser evalua-
tion provides valuable insights into the performance of grammars and parsing systems,
it is subject to a number of (related) drawbacks:
1. Bracketed trees do not always provide NLP applications with enough
information to carry out the required tasks: Many applications involve
a deeper analysis of the input in the form of semantically motivated
information such as deep dependency relations, predicate?argument
structures, or simple logical forms.
2. A number of alternative, but equally valid tree representations can
potentially be given for the same input. To give just a few examples: In
English, VPs containing modals and auxiliaries can be analyzed using
(predominantly) binary branching rules (Penn-II [Marcus et al 1994]), or
employ flatter analyses where modals and auxiliaries are sisters of the
main verb (AP treebank [Leech and Garside 1991]), or indeed do without
a designated VP constituent at all (SUSANNE [Sampson 1995]). Treebank
bracketing guidelines can use ?traditional? CFG categories such as S, NP,
and so on (Penn-II) or a maximal projection-inspired analysis with IPs
and DPs (Chinese Penn Treebank [Xue et al 2004]).
3. Because a tree-based gold standard for parser evaluation must adopt a
particular style of linguistic analysis (reflected in the geometry and
nomenclature of the nodes in the trees), evaluation of statistical parsers
and grammars that are derived from particular treebank resources (as
well as hand-crafted grammars/parsers) can suffer unduly if the gold
standard deviates systematically from the (possibly) equally valid style
of linguistic analysis provided by the parser.
Problems such as these have motivated research on more abstract, dependency-
based parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Carroll
et al 2002; Clark and Hockenmaier 2002; King et al 2003; Preiss 2003; Kaplan et al
2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are ap-
proximations of abstract predicate-argument-adjunct (or more basic head-dependent)
82
Cahill et al Statistical Parsing Using Automatic Dependency Structures
structures, providing a more normalized representation abstracting away from the
particulars of surface realization or CFG-tree representation, which enables meaningful
cross-parser evaluation.
A related contrast holds between shallow and deep grammars and parsers.1 In
addition to defining a language (as a set of strings), deep grammars relate strings to in-
formation/meaning, often in the form of predicate?argument structure, dependency re-
lations,2 or logical forms. By contrast, a shallow grammar simply defines a language and
may associate syntactic (e.g., CFG tree) representations with strings. Natural languages
do not always interpret linguistic material locally where the material is encountered
in the string (or tree). In order to obtain accurate and complete predicate?argument,
dependency, or logical form representations, a hallmark of deep grammars is that they
usually involve a long-distance dependency (LDD) resolution mechanism.
Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language
Tools [Briscoe et al 1987], the Core Language Engine [Alshawi and Pulman 1992], the
Alpino Dutch dependency parser [Bouma, van Noord, andMalouf 2000], the Xerox Lin-
guistic Environment [Butt et al 2002], the RASP dependency parser [Carroll and Briscoe
2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al 2004]).
Wide-coverage, deep-grammar development, particularly in rich formalisms such as
LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard
and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting
an instance of the (in-)famous ?knowledge acquisition bottleneck? familiar from other
areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars
(Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al 2002)
have, in fact, been successfully scaled to unrestricted input.
The last 15 years have seen extensive efforts on treebank-based automatic gram-
mar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995;
Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein
and Manning 2003). These grammars are wide-coverage and robust and in contrast
to manual grammar development, machine-learning-based grammar acquisition in-
curs relatively low development cost. With few notable exceptions,3 however, these
treebank-induced wide-coverage grammars are shallow: They usually do not attempt
to resolve LDDs nor do they associate strings with meaning representations.
Over the last few years, addressing the knowledge acquisition bottleneck in deep
constraint-based grammar development, a growing body of research has emerged to au-
tomatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia
1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii
2003], LFG [Cahill et al 2002b, 2004]). To a first approximation, these approaches can
be classified as ?conversion?- or ?annotation?-based. TAG-based approaches convert
1 Our use of the terms ?shallow? and ?deep? parsers/grammars follows Kaplan et al (2004) where
a ?shallow parser? does not relate strings to meaning representations. This deviates from a more
common use of the terms where, for example, a ?shallow parser? refers to (often finite-state-based)
parsers (or chunkers) that may produce partial bracketings of input strings.
2 By dependency relations we mean deep, fine-grained, labeled dependencies that encode long-distance
dependencies and passive information, for example. These differ from the types of unlabeled
dependency relations in other work such as (McDonald and Pereira 2006).
3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins
Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range
of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced
material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and
Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of
CFG-based parsers. None of them map strings into dependencies.
83
Computational Linguistics Volume 34, Number 1
treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches
convert trees into CCG derivations fromwhich CCG categories can be extracted. HPSG-
and LFG-based grammar induction methods automatically annotate treebank trees
with (typed) attribute-value structure information for the extraction of constraint-based
grammars and lexical resources.
Two recent papers (Preiss 2003; Kaplan et al 2004) have started tying together
the research strands just sketched: They use dependency-based parser evaluation to
compare wide-coverage parsing systems using hand-crafted, deep, constraint-based
grammars with systems based on a simple version of treebank-based deep grammar
acquisition technology in the conversion paradigm. In the experiments, tree output
generated by Collins?s Model 1, 2, and 3 (1999) and Charniak?s (2000) parsers, for
example, are automatically translated into dependency structures and evaluated against
gold-standard dependency banks.
Preiss (2003) uses the grammatical relations and the CBS 500 Dependency Bank
described in Carroll, Briscoe, and Sanfilippo (1998) to compare a number of parsing
systems (Briscoe and Carroll 1993; Collins?s 1997 models 1 and 2; and Charniak 2000)
using a simple version of the conversion-based deep grammar acquisition process (i.e.,
reading off grammatical relations fromCFG parse trees produced by the treebank-based
shallow parsers). The article also reports on a task-based evaluation experiment to rank
the parsers using the grammatical relations as input to an anaphora resolution system.
Preiss concluded that parser ranking using grammatical relations reflected the absolute
ranking (between treebank-induced parsers) using traditional tree-based metrics, but
that the difference between the performance of the parsing algorithms narrowed when
they carried out the anaphora resolution task. Her results show that the hand-crafted
deep unification parser (Briscoe and Carroll 1993) outperforms the machine-learned
parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision
and recall on grammatical relations.4 Kaplan et al (2004) compare their deep, hand-
crafted, LFG-based XLE parsing system (Riezler et al 2002) with Collins?s (1999) model
3 using a simple conversion-based approach, capturing dependencies from the tree
output of the machine-learned parser, and evaluating both parsers against the PARC
700 Dependency Bank (King et al 2003). They conclude that the hand-crafted, deep
grammar outperforms the state-of-the-art treebank-based shallow parser on the level of
dependency representation, at the price of a small decrease in parsing speed.
Both Preiss (2003) and Kaplan et al (2004) emphasize that they use rather basic
versions of the conversion-based deep grammar acquisition technology outlined herein.
In this article we revisit the experiments carried out by Preiss and Kaplan et al, this time
using the sophisticated and fine-grained treebank- and annotation-based, deep, probabilis-
tic LFG grammar acquisitionmethodology developed in Cahill et al (2002b), Cahill et al
(2004), O?Donovan et al (2004), and Burke (2006) with a number of surprising results:
1. Evaluating against the PARC 700 Dependency Bank (King et al 2003)
using a retrained version of Bikel?s (2002) parser, the best automatically
induced, deep LFG resources achieve an f-score of 82.73%. This is an
improvement of 3.13 percentage points over the previously best published
results established by Kaplan et al (2004) who use a hand-crafted,
wide-coverage, deep LFG and the XLE parsing system. This is also a
4 The numbers given are difficult to compare as the results for the Briscoe and Carroll (1993) parser were
captured for a richer set of grammatical relations than those for Collins (1997) and Charniak (2000).
84
Cahill et al Statistical Parsing Using Automatic Dependency Structures
statistically significant improvement of 2.18 percentage points over the
most recent improved results presented in this article for the XLE system.
2. Evaluating against the Carroll, Briscoe, and Sanfilippo (1998) CBS 500
gold-standard dependency bank using a retrained version of Bikel?s (2002)
parser, the best Penn-II treebank-based, automatically acquired, deep LFG
resources achieve an f-score of 80.23%. This is a statistically significant
improvement of 3.66 percentage points over Carroll and Briscoe (2002),
who use a hand-crafted, wide-coverage, deep, unification grammar and
the RASP parsing system.
Evaluation results on a reannotated version (Briscoe and Carroll 2006) of the PARC
700 Dependency Bank were recently published in Clark and Curran (2007), reporting
f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and Carroll
(2006) point out, these evaluations are not directly comparable with the Kaplan et al
(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotation schemes are different.
The article is structured as follows: In Section 2, we outline the automatic LFG
f-structure annotation algorithm and the pipeline parsing architecture of Cahill et al
(2002b), Cahill et al (2004), and Burke (2006). In Section 3, we present our experiment
design. In Section 4, using the DCU 105 Dependency Bank as our development set, we
evaluate a number of treebank-induced LFG parsing systems against the automatically
generated Penn-II WSJ Section 22 Dependency Bank test set. We use the Approximate
Randomization Test (Noreen 1989) to test for statistical significance and choose the best
parsing system for the evaluations against the wide-coverage, hand-crafted RASP and
LFG grammars of Carroll and Briscoe (2002) and Kaplan et al (2004) using the CBS
500 and PARC 700 Dependency Banks in Section 5. In Section 6, we discuss results and
issues raised by our methodology, outline related and future research and conclude in
Section 7.
2. Methodology
In this section, we briefly outline LFG and present our automatic f-structure annotation
algorithm and parsing architecture. The parsing architecture enables us to integrate
PCFG- and history-based parsers, which allows us to compare these parsers at the level
of dependency structures, rather than just trees.
2.1 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple
2001) is a constraint-based theory of grammar. It (minimally) posits two levels of
representation, c(onstituent)-structure and f(unctional)-structure. C-structure is rep-
resented by context-free phrase-structure trees, and captures surface grammatical
configurations such as word order. The nodes in the trees are annotated with functional
equations (attribute-value structure constraints, for example (?OBJ)=?) which are
resolved (in the case of well-formed strings) to produce an f-structure. F-structures
are recursive attribute-value matrices, representing abstract syntactic functions, which
85
Computational Linguistics Volume 34, Number 1
Figure 1
C- and f-structures for the sentence U.N. signs treaty.
approximate to basic predicate-argument-adjunct structures or dependency relations.5
Figure 1 shows the c- and f-structures for the string U.N. signs treaty. Each node in the
c-structure is annotated with f-structure equations, for example (? SUBJ)= ?. The
uparrows (?) point to the f-structure associated with the mother node, downarrows
(?) to that of the local node. In a complete parse tree, these ? and ? meta variables are
instantiated to unique tree node identifiers and a set of constraints (a set of terms in an
equality logic) is generated which (if satisfiable) generates an f-structure.
2.2 Automatic F-Structure Annotation Algorithm
Deep grammars can be induced from treebank resources if the treebank encodes
enough information to support the derivation of deep grammatical information, such
as predicate?argument structures, deep dependency relations, or logical forms. Many
second generation treebanks such as Penn-II provide information to support the compi-
lation of meaning representations, for example in the form of traces relating displaced
linguistic material to where it should be interpreted semantically. The f-structure anno-
tation algorithm exploits configurational and categorial information, as well as traces
and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II
CFG trees with LFG f-structure information.
Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traverse
the tree and deterministically add f-structure equations to the phrasal and leaf nodes
of the tree, resulting in an f-structure annotated version of the tree. The annotations are
then collected and passed on to a constraint solver which generates an f-structure (if the
constraints are satisfiable). We use a simple graph-unification-based constraint solver
(Eisele and Do?rre 1986), extended to handle path, set-valued, disjunctive, and existential
constraints. Given parser output without Penn-II style annotations and traces, the same
algorithm is used to assign annotations to each node in the tree, whereas a separate
module is applied at the level of f-structure to resolve any long-distance dependencies
(see Section 2.3).
5 van Genabith and Crouch (1996, 1997) provide translations between f-structures, Quasi-Logical Forms
(QLFs), and Underspecified Discourse Representation Structures (UDRSs).
86
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 1
A complete list of the Penn-II functional labels.
Tag Description
Form/function discrepancies
-ADV clausal and NP adverbials
-NOM non NPs that function as NPs
Grammatical role
-DTV dative
-LGS logical subjects in passives
-PRD non VP predicates
-PUT locative complement of put
-SBJ surface subject
-TPC topicalized and fronted constituents
-VOC vocatives
Adverbials
-BNF benefactive
-DIR direction and trajectory
-EXT extent
-LOC location
-MNR manner
-PRP purpose and reason
-TMP temporal phrases
Miscellaneous
-CLR closely related to verb
-CLF true clefts
-HLN headlines and datelines
-TTL titles
The f-structure annotation algorithm is described in detail in Cahill et al (2002a),
McCarthy (2003), Cahill et al (2004), and Burke (2006). In brief, the algorithm is modular
with four components (Figure 3), taking Penn-II trees as input and automatically adding
LFG f-structure equations to each node in the tree.
Lexical Information. Lexical information is generated automatically by macros for each
of the POS classes in Penn-II. To give a simple example, third-person plural noun
Penn-II POS-word sequences of the form NNS word are automatically associated with
the equations (?PRED) = word?, (?NUM) = pl and (?PERS) = 3rd, where word? is the
lemmatized word.
Left?Right Context Annotation. The Left?Right context annotation component identifies
the heads of Penn-II trees using a modified version of the head finding rules of
Magerman (1994). This partitions each local subtree (of depth one) into a local head, a
left context (left sisters), and a right context (right sisters). The contexts together with
information about the local mother and daughter categories and (if present) Penn-II
87
Computational Linguistics Volume 34, Number 1
Figure 2
Trees for the sentence U.N. signs treaty, the headline said before and after automatic f-structure
annotation, with the f-structure automatically produced.
Figure 3
F-structure annotation algorithm modules.
88
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 2
Sample from an NP Annotation matrix.
Left context Head Right context
DT: (?SPEC DET)=? NN, NNS, NNP, NNPS, NP: RRC, SBAR: (?RELMOD)=?
CD: (?SPEC QUANT)=? ?=? PP: ??(?ADJUNCT)
ADJP, JJ, NN, NNP: ??(?ADJUNCT) NP: ??(?APP)
functional tag labels (Table 1) are used by the f-structure annotation algorithm. For each
Penn-II mother (i.e., phrasal) category an Annotation matrix expresses generalizations
about how to annotate immediate daughters dominated by the mother category relative
to their location in relation to the local head. To give a (much simplified) example,
the head finding rules for NPs state that the rightmost nominal (NN, NNS, NNP, . . . )
not preceded by a comma or ?-?6 is likely to be the local head. The Annotation ma-
trix for NPs states (inter alia) that heads are annotated ?=?, that DTs (determiners)
to the left of the head are annotated (? SPEC DET) = ?, NPs to the right of the head as
??(? APP) (appositions). Table 2 provides a sample extract from the NP Annotation
matrix. Figure 4 provides an example of the application of the NP and PP Annotation
matrices to a simple tree.
For each phrasal category, Annotation matrices are constructed by inspecting the
most frequent Penn-II rule types expanding the category such that the token occurrences
of these rule types cover more than 85% of all occurrences of expansions of that category
in Penn-II. For NP rules, for example, this means that we analyze the most frequent
102 rule types expanding NP, rather than the complete set of more than 6,500 Penn-II
NP rule types, in order to populate the NP Annotation matrix. Annotation matrices
generalize to unseen rule types as, in the case of NPs, these may also feature DTs to
the left of the local head and NPs to the right and similarly for rule types expanding
other categories.
Coordination. In order to support the modularity, maintainability, and extendability of
the annotation algorithm, the Left?Right Annotation matrices apply only to local trees
of depth one, which do not feature coordination. This keeps the statement of Annotation
matrices perspicuous and compact. The Penn-II treatment of coordination is (inten-
tionally) flat. The annotation algorithm has modules for like- and unlike-constituent
coordination. Coordinated constituents are elements of a COORD set and annotated ??
(? COORD). The Coordination module reuses the Left?Right context Annotation ma-
trices to annotate any remaining nodes in a local subtree containing a coordinating
conjunction. Figure 5 provides a VP-coordination example (with right-node-raising).
Catch-All and Clean-Up. The Catch-All and Clean-Up module provides defaults to cap-
ture remaining unannotated nodes (Catch-All) and corrects (Clean-Up) overgeneraliza-
tions resulting from the application of the Left?Right context Annotation matrices. The
Left?Right Annotation matrices are allowed a certain amount of overgeneralization as
this facilitates the perspicuous statement of generalizations and a separate statement of
exceptions, supporting the modularity and maintainability of the annotation algorithm.
PPs under VPs are a case in point. The VP Annotation matrix analyses PPs to the right
of the local VP head as adjuncts: ? ? (?ADJUNCT). The Catch-All and Clean-Up module
6 If the rightmost nominal is preceded by a comma or ?-?, it is likely to be an apposition to the head.
89
Computational Linguistics Volume 34, Number 1
Figure 4
Automatically annotated Penn-II tree (fragment) and f-structure (simplified) for Gerry Purdy,
director of marketing.
uses Penn-II functional tag (Table 1) information (if present), for example -CLR (closely
related to local head), to replace the original adjunct analysis by an oblique argument
analysis: (?OBL)=?. An example of this is provided by the PP-CLR in the left VP-conjunct
in Figure 5. In other cases, argument?adjunct distinctions are encoded configurationally
in Penn-II (without the use of -CLR tags). To give a simple example, the NP Anno-
tation matrix indiscriminately associates SBARs to the right of the local head with
(? RELMOD) = ?. However, some of these SBARs are actually arguments of the local
NP head and, unlike SBAR relative clauses which are Chomsky-adjoined to NP (i.e.,
relative clauses are daughters of an NP mother and sisters of a phrasal NP head), SBAR
arguments are sisters of non-phrasal NP heads.7 In such cases, the Catch-All and Clean-
Up module rewrites the original relative clause analysis into the correct complement
argument analysis (?COMP)=?. Figure 6 shows the COMP f-structure analyses for an
example NP containing an internal SBAR argument (rather than relative clause) node.
Traces. The Traces module translates traces and coindexed material in Penn-II trees
representing long-distance dependencies into corresponding reentrancies at f-structure.
Penn-II provides a rich arsenal of trace types to relate ?displaced? material to where it
7 Structural information of this kind is not encoded in the Annotation matrices; compare Table 2.
90
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 5
Automatically annotated Penn-II tree (fragment) and resulting f-structure for asked for and
received refunds.
should be interpreted semantically. The f-structure annotation algorithm coverswh- and
wh-less relative clause constructions, interrogatives, control and raising constructions,
right-node-raising, and general ICH (interpret constituent here) traces. Figure 5 gives an
example that shows the interplay between coordination, right-node-raising traces and
the corresponding automatically generated reentrancies at f-structure.
2.3 Parsing Architecture
The pipeline parsing architecture of Cahill et al (2004) and Cahill (2004) for parsing raw
text into LFG f-structures is shown in Figure 7. In this model, PCFGs or history-based
lexicalized parsers are extracted from the unannotated treebank and used to parse raw
text into trees. The resulting parse trees are then passed to the automatic f-structure
annotation algorithm to generate f-structures.8
Compared to full Penn-II treebank trees, the output of standard probabilistic
parsers is impoverished: Parsers do not normally output Penn-II functional tag an-
notations (Table 1) nor do they indicate/resolve long-distance dependencies, recorded
8 In the integratedmodel (Cahill et al 2004; Cahill 2004), we extract f-structure annotated PCFGs
(A-PCFGs) from the f-structure annotated treebank, where each non-terminal symbol in the grammar
has been augmented with LFG functional equations, such as NP[?OBJ=?] ? DT[?SPEC=?] NN[?=?].
We treat a non-terminal symbol followed by annotations as a monadic category for grammar extraction
and parsing. Parsing with A-PCFGs results in annotated parse trees, from which an f-structure can be
generated. In this article we only use the pipeline parsing architecture.
91
Computational Linguistics Volume 34, Number 1
Figure 6
Automatically annotated Penn-II tree (fragment) and f-structure for signs that managers
expect declines.
in terms of a fine-grained system of empty productions (traces) and coindexation in
the full Penn-II treebank trees. The f-structure annotation algorithm, as described in
Section 2.2, makes use of Penn-II functional tag information (if present) and relies on
traces and coindexation to capture LDDs in terms of corresponding reentrancies at
f-structure.
Penn-II functional labels are used by the annotation algorithm to discriminate
between adjuncts and (oblique) arguments. PP-sisters to a head verb are analyzed as
arguments iff they are labeled -CLR, -PUT, -DTV or -BNF, for example. Conversely,
functional labels (e.g., -TMP) are also used to analyze certain NPs as adjuncts, and
-LGS labels help to identify logical subjects in passive constructions. In the absence of
functional labels, the annotation algorithm will default to decisions based on simple
structural, configurational, and CFG-category information (and, for example, conserva-
tively analyze a PP sister to a head verb as an adjunct, rather than as an argument).
In Sections 3 and 4 we present a number of treebank-based parsers (in particular the
PCFGs and a version of Bikel?s history-based, lexicalized generative parser) trained to
output CFG categories with Penn-II functional tags. We achieve this through a simple
92
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 7
Treebank-based LFG parsing architecture.
masking and un-masking operation where functional tags are joined with their local
CFG category label to form a new (larger) set of (monadic) CFG category labels (e.g.,
PP-CLR goes to PP CLR) for training and parsing (for Bikel, the parser head-finding rules
are also adjusted to the expanded set of categories). After parsing, the Penn-II functional
tags are unmasked and available to the f-structure annotation algorithm.
The Traces component in the f-structure annotation algorithm (Figure 3) translates
LDDs represented in terms of traces and coindexation in the original Penn-II treebank
trees into corresponding reentrancies at f-structure. Most probabilistic treebank-based
parsers, however, do not indicate/resolve LDDs, and the Traces component of the an-
notation algorithm does not apply. Initially, the f-structures produced for parser output
trees in the architecture in Figure 7 are therefore LDD-unresolved: They are incomplete
(or proto) f-structures, where displaced material (e.g., the values of FOCUS, TOPIC, and
TOPICREL attributes [wh- and wh-less relative clauses, topicalization, and interrogative
constructions] at f-structure) is not yet linked to the appropriate argument grammati-
cal functions (or elements of adjunct sets) for the governing local PRED. A dedicated
LDD Resolution component in the architecture in Figure 7 turns parser output proto-
f-structures into fully LDD-resolved proper f-structures, without traces and coindexa-
tion in parse trees.
Consider the following fragment of a proper Penn-II treebank tree (Figure 8), where
the LDD between the WHNP in the relative clause and the embedded direct object
position of the verb reward is indicated in terms of the trace *T*-3 and its coindexation
with the antecedent WHNP-3. Note further that the control relation between the subject
of the verbs wanted and reward is similarly expressed in terms of traces (*T*-2) and
coindexation (NP-SBJ-2). From the treebank tree, the f-structure annotation algorithm
is able to derive a fully resolved f-structure where the LDD and the control relation are
captured in terms of corresponding reentrancies (Figure 9).
93
Computational Linguistics Volume 34, Number 1
Figure 8
Penn-II treebank tree with LDD indicated in terms of traces (empty productions) and
coindexation and f-structure annotations generated by the annotation algorithm.
Now consider the corresponding ?impoverished? (but otherwise correct) parser
output tree (Figure 10) for the same string: The parser output does not explicitly record
the control relation nor the LDD.
Given this parser output tree, prior to the LDD resolution component in the parsing
architecture (Figure 7), the f-structure annotation algorithmwould initially construct the
partial (proto-) f-structure in Figure 11, where the LDD indicated by the TOPICREL func-
tion is unresolved (i.e., the value of TOPICREL is not coindexedwith the OBJ grammatical
function of the embedded verb reward). The control relation (shared subject between
the two verbs in the relative clause) is in fact captured by the annotation algorithm in
terms of a default annotation (? SUBJ) = (? SUBJ) on sole argument VPs to the right of
head verbs (as often, even in the full Penn-II treebank trees, control relations are not
consistently captured through explicit argument traces).
In LFG, LDD resolution operates at the level of f-structure, using functional un-
certainty equations (regular expressions over paths in f-structure [Kaplan and Zaenen
1989] relating f-structure components in different parts of an f-structure), obviating
traces and coindexation in c-structure trees. For the example in Figure 10, a functional
uncertainty equation of the form (?TOPICREL) = (?[COMP|XCOMP]? [SUBJ|OBJ]) would
be associated with the WHNP daughter node of the SBAR relative clause. The equation
states that the value of the TOPICREL attribute is token-identical (re-entrant) with the
value of a SUBJ or OBJ function, reached through a path along any number (including
94
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 9
Fully LDD-resolved f-structure.
Figure 10
Impoverished parser output tree: LDDs not captured.
zero) of COMP or XCOMP attributes. This equation, together with subcategorization
frames (LFG semantic forms) for the local PREDs and the usual LFG completeness and
coherence conditions, resolve the partial proto-f-structure in Figure 11 into the fully
LDD-resolved proper f-structure in Figure 9.
95
Computational Linguistics Volume 34, Number 1
Figure 11
Proto-f-structure: LDDs not captured.
Following Cahill et al (2004), in our parsing architecture (Figure 7) we model
LFG LDD resolution using automatically induced finite approximations of functional-
uncertainty equations and subcategorization frames from the f-structure-annotated
Penn-II treebank (O?Donovan et al 2004) in an LDD resolution component. From the
fully LDD-resolved f-structures from the Penn-II training section treebank trees we
learn probabilistic LDD resolution paths (reentrancies in f-structure), conditional on
LDD type (Table 3), and subcategorization frames, conditional on lemma (and voice)
(Table 4). Table 3 lists the eight most probable TOPICREL paths (out of a total of 37
TOPICREL paths acquired). The totality of these paths constitutes a finite subset of the
reference language definde by the full functional uncertainty equation (?TOPICREL) =
(?[COMP|XCOMP]? [SUBJ|OBJ]). Given an unresolved LDD type (such as TOPICREL in
the parser output for the relative clause example in Figure 11), admissible LDD res-
olutions assert a reentrancy between the value of the LDD trigger (here, TOPICREL)
and a grammatical function (or adjunct set element) of an embedded local predicate,
subject to the conditions that (i) the local predicate can be reached from the LDD trigger
using the LDD path; (ii) the grammatical function terminates the LDD path; (iii) the
grammatical function is not already present (at the relevant level of embedding in
the local f-structure); and (vi) the local predicate subcategorizes for the grammatical
function in question.9 Solutions satisfying (i)?(iv) are ranked using the product of
LDD path and subcategorization frame probabilities and the highest ranked solution
(possibly involving multiple interacting LDDs for a single f-structure) is returned by
the algorithm (for details and comparison against alternative LDD resolution methods,
see Cahill et al 2004).10
For our example (Figure 11), the highest ranked LDD resolution is for LDD path
(?TOPICREL) = (? XCOMP OBJ) and the local subcat frame REWARD?? SUBJ, ? OBJ?. This
9 Conditions (i)?(iv) are suitably adapted for LDD resolutions terminating in adjunct sets.
10 In our experiments we do not use the limited LDD resolution for wh-phrases provided by Collins?s Model
3 parser as better results are achieved using the purely f-structure-based LDD resolution as shown in
Cahill et al (2004).
96
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 3
Most frequent wh-TOPICREL paths.
wh-TOPICREL Probability wh-TOPICREL Probability
subj .7583 xcomp .0830
obj .0458 xcomp:obj .0338
xcomp:xcomp .0168 xcomp:subj .0109
comp .0097 comp:subj .0073
Table 4
Most frequent semantic forms for active and passive (p) occurrences of the verb want and
reward.
Semantic form Probability
want([subj,xcomp]) .6208
want([subj,obj]) .2496
want([subj,obj,xcomp]) .1008
want([subj]) .0096
want([subj,obj,obl]) .0048
want([subj,obj,part]),p) .5000
want([subj,obl]),p) .1667
want([subj,part]),p) .1667
want([subj]),p) .1667
reward([subj,obj]) .8000
reward([subj,obj,obl]) .2000
reward([subj]),p) 1.0000
(together with the subject control equation described previously) turns the parser-
output proto-f-structure (in Figure 11) into the fully LDD resolved f-structure in
(Figure 9).
The full pipeline parsing architecture with the LDD resolution (rather than the
Traces component for LDD resolved Penn-II treebank trees) component (and the LDD
path and subcategorization frame extraction) is given in Figure 7.
The pipeline architecture supports flexible integration of treebank-based PCFGs
or state-of-the-art, history-based, and lexicalized parsers (Collins 1999; Charniak 2000;
Bikel 2002) and enables dependency-based evaluation of such parsers.
3. Experiment Design
In our experiments we compare four history-based parsers for integration into the
pipeline parsing architecture described in Section 2.3:
 Collins?s 1999 Models 311
 Charniak?s 2000 maximum-entropy inspired parser12
11 Downloaded from ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz.
12 Downloaded from ftp://ftp.cs.brown.edu/pub/nlparser/.
97
Computational Linguistics Volume 34, Number 1
 Bikel?s 2002 emulation of Collins Model 213
 a retrained version of Bikel?s (2002) parser which retains Penn-II functional
tags
Input for Collins?s and Bikel?s parsers was pre-tagged using the MXPOST POS tag-
ger (Ratnaparkhi 1996). Charniak?s parser provides its own POS tagger. The combined
system of best history-based parser and automatic f-structure annotation is compared
to two probabilistic parsing systems based on hand-crafted, wide-coverage, constraint-
based, deep grammars:
 the RASP parsing system (Carroll and Briscoe 2002)
 the XLE parsing system (Riezler et al 2002; Kaplan et al 2004)
Both hand-crafted grammars perform their own POS tagging, resolve LDDs, and
associate strings with dependency relations (in the form of grammatical relations or
LFG f-structures).
We evaluate the parsers against a number of gold-standard dependency banks.
We use the DCU 105 Dependency Bank (Cahill et al 2002a) as our development set
for the treebank-based LFG parsers. We use the f-structure annotation algorithm to
automatically generate a gold-standard test set from the original Section 22 treebank
trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best
treebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments.
Following the experimental setup in Kaplan et al (2004), we use the Penn-II Section 23-
based PARC 700 Dependency Bank (King et al 2003) to evaluate the treebank-induced
LFG resources against the hand-crafted XLE grammar and parsing system of Riezler
et al (2002) and Kaplan et al Following Preiss (2003), we use the SUSANNE Based CBS
500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998) to evaluate the treebank-
induced LFG resources against the hand-crafted RASP grammar and parsing system
(Carroll and Briscoe 2002) as well as against the XLE system (Riezler et al 2002).
For each gold standard, our experiment design is as follows: We parse automati-
cally tagged input14 sentences with the treebank- and machine-learning-based parsers
trained on WSJ Sections 02?21 in the pipeline architecture, pass the resulting parse
trees to our automatic f-structure annotation algorithm, collect the f-structure equations,
pass them to a constraint-solver which generates an f-structure, resolve long-distance
dependencies at f-structure following Cahill et al (2004) and convert the resulting LDD-
resolved f-structures into dependency representations using the formats and software
of Crouch et al (2002) (for the DCU 105, PARC 700, and WSJ Section 22 evaluations)
and the formats and software of Carroll, Briscoe, and Sanfilippo (1998) (for the CBS
500 evaluation). In the experiments we did not use any additional annotations such as
-A (for argument) that can be generated by some of the history-based parsers (Collins
1999) as the f-structure annotation algorithm is designed for Penn-II trees (which do
not contain such annotations). We also did not use the limited LDD resolution for wh-
relative clauses provided by Collins?s Model 3 as better results are achieved by LDD
13 This was developed at the University of Pennsylvania by Dan Bikel and is freely available to download
from http://www.cis.upenn.edu/?dbikel/software.html.
14 Tags were automatically assigned either by the parsers themselves or by the MXPOST tagger
(Ratnaparkhi 1996).
98
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 5
Results of tree-based evaluation on all sentences WSJ section 23, Penn-II.
Parser Labeled
f-score (%)
PCFG 73.03
Parent-PCFG 78.05
Collins M3 88.33
Charniak 89.73
Bikel 88.32
Bikel+Tags 87.53
resolution on f-structure (Cahill et al 2004). A complete set of parameter settings for the
parsers is provided in the Appendix.
In order to evaluate the treebank-induced LFG resources against the PARC 700
and the CBS 500 dependency banks, a certain amount of automatic mapping is re-
quired to account for systematic differences in linguistic analysis, feature geometry,
and nomenclature at the level of dependencies. This is discussed in Sections 5.1 and
5.2. Throughout, we use the Approximate Randomization Test (Noreen 1989) to test the
statistical significance of the results.
4. Choosing a Treebank-Based LFG Parsing System
In this section, we choose the best treebank-based LFG parsing system for the compar-
isons with the hand-crafted XLE and RASP resources in Section 5. We use the DCU
105 Dependency Bank as our development set and carry out comparative evaluation
and statistical significance testing on the larger, automatically generated WSJ Section 22
Dependency Bank as a test set. The system based on Bikel?s (2002) parser retrained to
retain Penn-II functional tags (Table 1) achieves overall best results.
4.1 Tree-Based Evaluation against WSJ Section 23
For reference, we include the traditional CFG-tree-based comparison for treebank-
induced parsers. The parsers are trained on Sections 02 to 21 of the Penn-II Treebank and
tested on Section 23. The published results15 on these experiments for the history-based
parsers are given in Table 5. We also include figures for a PCFG and a Parent-PCFG (a
PCFG which has undergone the parent transformation [Johnson 1999]). These PCFGs
are induced following standard treebank preprocessing steps, including elimination of
empty nodes, but following Cahill et al (2004), they do include Penn-II functional tags
(Table 1), as these tags contain valuable information for the automatic f-structure anno-
tation algorithm (Section 2.2). These tags are removed for the tree-based evaluation.
The results show that the history-based parsers produce considerably better trees
than the more basic PCFGs (with and without parent transformations). Charniak?s
(2000) parser scores best with an f-score of 89.73% on all sentences in Section 23. The
15 Where there were no published results available for Section 23, we calculated them using the
downloadable versions of the parsers.
99
Computational Linguistics Volume 34, Number 1
vanilla PCFG achieves the lowest f-score of 73.03%, a difference of 16.7 percentage
points. The hand-crafted XLE and RASP grammars achieve around 80% coverage
(measured in terms of complete spanning parse) on Section 23 and use a variety of
(longest) fragments combining techniques to generate dependency representations for
the remaining 20% of Section 23 strings. By contrast, the treebank-induced PCFGs and
history-based parsers all achieve coverage of over 99.9%. Given that the history-based
parsers score considerably better than PCFGs on trees, we would also expect them to
produce dependency structures of substantially higher quality.
4.2 Using DCU 105 as a Development Set
The DCU 105 (Cahill et al 2002a) is a hand-crafted gold-standard dependency bank
for 105 sentences, randomly chosen from Section 23 of the Penn-II Treebank.16 This is a
relatively small gold standard, initially developed to evaluate the automatic f-structure
annotation algorithm. We parse the 105 tagged sentences into LFG f-structures with
each of the treebank-induced parsers in the pipeline parsing and f-structure annotation
architecture. The f-structures of the gold standard and the f-structures returned by the
parsing systems are converted into dependency triples following Crouch et al (2002)
and Riezler et al (2002) and we also use their software for evaluation. The following
dependency triples are produced by the f-structure in Figure 1:
subj(sign?0,U.N.?1)
obj(sign?0,treaty?2)
num(U.N.?1,sg)
pers(U.N.?1,3)
num(treaty?2,sg)
pers(treaty?3,3)
tense(sign?0,present)
We evaluate preds-only f-structures (i.e., where paths in f-structures end in a PRED
value: the predicate-argument-adjunct structure skeleton) and all grammatical func-
tions (GFs) including number, tense, person, and so on. The results are given in Table 6.
With one main exception, Tables 5 and 6 confirm the general expectation that
the better the trees produced by the parsers, the better the f-structures automatically
generated for those trees. The exception is Bikel+Tags. The automatic f-structure an-
notation algorithm will exploit Penn-II functional tag information if present to generate
appropriate f-structure equations (see Section 2.2). It will default to possibly less reliable
configurational and categorial information if Penn-II tags are not present in the trees.
In order to test whether the retention of Penn-II functional labels in the history-
based parser output will improve LFG f-structure-based dependency results, we use
Bikel?s (2002) training software,17 and retrain the parser on a version of the Penn-II
treebank (Sections 02 to 21) with the Penn-II functional tag labels (Table 1) annotated
in such a way that the resulting history-based parser will retain them (Section 2.3). The
retrained parser (Bikel+Tags) then produces CFG-trees with Penn-II functional labels
and these are used by the f-structure annotation algorithm. We evaluate the f-structure
dependencies against the DCU 105 (Table 6) and achieve an f-score of 82.92% preds-only
16 It is publicly available for download from: http://nclt.computing.dcu.ie/gold105.txt.
17 We use Bikel?s software rather than Charniak?s for this experiment as the former proved more stable
during the retraining phase.
100
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 6
Treebank-induced parsers: results of dependency-based evaluation against DCU 105.
Parser Preds only All GFs
f-score (%) f-score (%)
PCFG 70.24 79.90
Parent-PCFG 75.84 83.58
Collins M3 77.84 85.08
Charniak 79.61 85.66
Bikel 79.39 86.56
Bikel+Tags 82.92 88.30
Table 7
Treebank induced parsers: breakdown by dependency relation of preds-only evaluation against
DCU 105.
Dep. Percent of total F-score (%)
Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
ADJUNCT 33.73 71 72 76 73 79
APP 0.68 61 0 55 70 65
COMP 2.31 60 61 66 61 73
COORD 5.73 64 73 77 67 76
DET 9.58 91 93 96 96 96
FOCUS 0.04 100 100 0 100 100
OBJ 16.42 82 84 86 85 90
OBJ2 0.07 80 57 50 57 50
OBL 2.17 58 24 27 23 63
OBL2 0.07 50 0 0 0 67
OBL AG 0.43 40 96 96 92 92
POSS 2.88 80 83 82 82 79
QUANT 1.85 70 67 69 70 70
RELMOD 1.78 50 78 67 78 73
SUBJ 14.74 80 81 83 85 85
TOPIC 0.46 85 87 96 96 89
TOPICREL 1.85 61 80 80 79 74
XCOMP 5.20 90 92 79 93 93
and 88.3% all GFs. A detailed breakdown by dependency is given in Table 7. The system
based on the retrained parser is nowmuch better able to identify oblique arguments and
overall preds-only accuracy has improved by 3.53% over the original Bikel experiment
and 3.31% over Charniak?s parser, even though Charniak?s parser performs more than
2% better on the tree-based scores in Table 5 and even though the retrained parser drops
0.79% against the original Bikel parser on the tree-based scores.18
Inspection of the results broken down by grammatical function (Table 7) for the
preds-only evaluation against the DCU 105 shows that just over one third of all depen-
dency triples in the gold standard are adjuncts. SUBJ(ects) and OBJ(ects) together make
up a further 30%.
18 The figures suggest that retraining Charniak?s parser to retain Penn-II functional tags is likely to produce
even better dependency scores than those achieved by Bikel?s retrained parser.
101
Computational Linguistics Volume 34, Number 1
Table 7 shows that the treebank-based LFG system using Collins?s Models 3 is
unable to identify APP(osition). This is due to Collins?s treatment of punctuation and
the fact that punctuation is often required to reliably identify apposition.19 None of
the original history-based parsers produced trees which enabled the annotation algo-
rithm to identify second oblique dependencies (OBL2), and they generally performed
considerably worse than Parent-PCFG when identifying OBL(ique) dependencies. This
is because the automatic f-structure annotation algorithm is cautious to the point of
undergeneralization when identifying oblique arguments. In many cases, the algorithm
relies on the presence of, for example, a -CLR Penn-II functional label (indicating that the
phrase is closely related to the verb), and the history-based (Collins M3, Charniak, and
Bikel) parsers do not produce these labels, whereas Parent-PCFG (as well as PCFG) are
trained to retain Penn-II functional labels. Parent-PCFG, by contrast, performs poorly
for oblique agents (OBL AG, agentive by-phrases in passive constructions), whereas the
history-based parsers are able to identify these with considerable accuracy. This is be-
cause Parent-PCFG often erroneously finds oblique agents, even when the preposition
is not by, as it never has enough context in which to distinguish by prepositional phrases
from other PPs. The history-based parsers produce trees from which the automatic
f-structure annotation algorithm can better identify RELMOD and TOPICREL dependen-
cies than Parent-PCFG. This, in turn, leads to improved long distance dependency
resolution which improves overall accuracy.
The DCU 105 development set is too small to support reliable statistical significance
testing of the performance ranking of the six treebank-based LFG parsing systems. In
order to carry out significance testing to select the best treebank-based LFG parsing
system for comparative evaluation against the hand-crafted deep XLE and RASP re-
sources, we move to a larger dependency-based evaluation data set: the gold-standard
dependency bank automatically generated fromWSJ Section 22.
4.3 Evaluation against WSJ Section 22 Dependencies
In an experimental setup similar to that of Hockenmaier and Steedman (2002),20 we
evaluate each parser against a large automatically generated gold standard. The gold-
standard dependency bank is automatically generated by annotating the original 1,700
treebank trees from WSJ Section 22 of the Penn-II Treebank with our f-structure an-
notation algorithm. We then evaluate the f-structures generated from the tree output
of the six parsers trained on Sections 02 to 21 resulting from parsing the Section 22
strings against the automatically produced f-structures for the original Section 22 Penn-II
treebank trees. The results are given in Table 8.
Compared to Table 6 for the DCU 105 gold standard, most scores are up, particularly
so for the history-based parsers. This trend is possibly due to the fact that the WSJ
19 The annotation algorithm relies on Penn-II-style punctuation patterns where an NP apposition follows a
nominal head separated by a comma ([NP [NP Bush ] , [NP the president ] ]), all three sisters of the same
mother node, while the trees produced by Collins?s parser attach the comma low in the tree ([NP [NP
Bush,] [NP the president ] ]). Although it would be trivial to carry out a tree transformation on the Collins
output to raise the punctuation to the expected level, we have not done this here.
20 This corresponds to experiments where the original Penn-II Section 23 treebank trees are automatically
converted into CCG derivations, which are then used as a gold standard to evaluate the CCG parser
trained on Sections 02?21. A similar methodology is used for the evaluation of treebank-based HPSG
resources (Miyao, Ninomiya, and Tsujii 2003) where Penn-II treebank trees are automatically annotated
with HPSG typed-feature structure information.
102
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 8
Results of dependency-based evaluation against the automatically generated gold standard for
WSJ Section 22.
Parser Preds only All GFs
f-score (%) f-score (%)
PCFG 70.76 80.44
Parent-PCFG 74.92 83.04
Collins M3 79.30 86.00
Charniak 81.35 86.96
Bikel 81.40 87.00
Bikel+Tags 83.06 87.63
Section 22 gold standard is generated automatically from the original ?perfect? Penn-II
treebank trees using the automatic f-structure annotation algorithm, whereas the DCU
105 has been created manually without regard as to whether or not the f-structure
annotation algorithm could ever generate the f-structures, even given the ?perfect?
trees.
The LFG system based on Bikel?s retrained parser achieves the highest f-score of
83.06% preds-only and 87.63% all GFs. Parent-PCFG achieves an f-score of 74.92%
preds-only and 83.04% all GFs. Table 9 provides a breakdown by feature of the preds-
only evaluation.
Table 9 shows that, once again, the automatic f-structure annotation algorithm is
not able to identify any cases of apposition from the output of Collins?s Model 3 parser.
Apart from Bikel?s retrained parser, none of the history-based parsers are able to identify
Table 9
Breakdown by dependency of results of preds-only evaluation against the automatically
generated Section 22 gold standard.
Dep. Percent of total F-score (%)
Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
ADJUNCT 33.77 70 75 78 78 80
APP 0.74 61 0 77 77 71
COMP 1.35 60 72 70 70 80
COORD 5.11 74 78 82 82 81
DET 10.72 88 91 92 92 91
FOCUS 0.02 27 67 88 88 71
OBJ 16.17 80 85 87 87 88
OBJ2 0.07 15 30 32 32 71
OBL 1.92 50 19 21 21 73
OBL2 0.07 47 3 3 3 69
OBL AG 0.31 50 90 89 89 85
POSS 2.47 86 91 91 91 91
QUANT 2.12 89 89 93 93 92
RELMOD 1.84 51 71 72 72 69
SUBJ 15.45 75 80 81 81 82
TOPIC 0.44 81 85 84 84 76
TOPICREL 1.82 61 72 74 75 69
XCOMP 5.62 81 87 81 81 88
103
Computational Linguistics Volume 34, Number 1
Figure 12
Approximate Randomization Test for statistical significance testing.
OBJ2, OBL or OBL2 dependencies very well, although Parent-PCFG is able to produce
trees from which it is easier to identify obliques (OBL), because of the Penn-II functional
-CLR label. The automatic annotation algorithm is unable to identify RELMOD depen-
dencies satisfactorily from the trees produced by parsing with Parent-PCFG, although
the history-based parsers score reasonably well for this function. Whereas Charniak?s
parser is able to identify some dependencies better than Bikel?s retrained parser, overall
the system based on Bikel?s retrained parser performs better when evaluating against
the dependencies in WSJ Section 22.
In order to determine whether the results are statistically significant, we use the Ap-
proximate Randomization Test (Noreen 1989).21 This test is an example of a computer-
intensive statistical hypothesis test. Such tests are designed to assess result differences
with respect to a test statistic in cases where the sampling distribution of the test statistic
is unknown. Comparative evaluations of outputs of parsing systems according to test
statistics, such as differences in f-score, are examples of this situation. The test statistics
are computed by accumulating certain count variables over the sentences in the test
set. In the case of f-score, variable tuples consisting of the number of dependency-
relations in the parse for the system translation, the number of dependency-relations
in the parse for the reference translation, and the number of matching dependency-
relations between system and reference parse, are accumulated over the test set.
Under the null hypothesis, the compared systems are not different, thus any vari-
able tuple produced by one of the systems could just as likely have been produced by
the other system. So shuffling the variable tuples between the two systems with equal
probability, and recomputing the test statistic, creates an approximate distribution of
the test statistic under the null hypothesis. For a test set of S sentences there are 2S
different ways to shuffle the variable tuples between the two systems. Approximate
randomization produces shuffles by random assignments instead of evaluating all 2S
possible assignments. Significance levels are computed as the percentage of trials where
the pseudo statistic, that is the test statistic computed on the shuffled data, is greater
than or equal to the actual statistic, that is the test statistic computed on the test data. A
sketch of an algorithm for approximate randomization testing is given in Figure 12.
21 Applications of this test to natural language processing problems can be found in Chinchor et al (1993)
and Yeh (2000).
104
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 10
Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values for
approximate randomization test for 10,000,000 randomizations.
PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
PCFG - - - - - -
Parent-PCFG <.0001 - - - - -
Collins M3 <.0001 <.0001 - - - -
Charniak <.0001 <.0001 <.0001 - - -
Bikel <.0001 <.0001 <.0001 .0003 - -
Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 -
Table 10 gives the p-values (the smallest fixed level at which the null hypothesis can
be rejected) for comparing each parser against all of the other parsers. We test for sig-
nificance at the 95% level. Because we are doing a pairwise comparison of six systems,
giving 15 comparisons, the p-value needs to be below .0034 for there to be a significant
difference at the 95% level.22 For each parser, the values in the row corresponding to
that parser represent the p-values for those parsers that achieve a lower f-score than
that parser. This shows that the system based on Bikel?s retrained parser is significantly
better than those based on the other parsers with a statistical significance of >95%. For
the XLE and RASP comparisons, we will use the f-structure-annotation algorithm and
Bikel retrained-based LFG system.
5. Cross-Formalism Comparison of Treebank-Induced and Hand-Crafted Grammars
From the experiments in Section 4, we choose the treebank-based LFG system using
the retrained version of Bikel?s parser (which retains Penn-II functional tag labels) to
compare against parsing systems using deep, hand-crafted, constraint-based grammars
at the level of dependencies. We report on two experiments. In the first experiment
(Section 5.1), we evaluate the f-structure annotation algorithm and Bikel retrained
parser-based LFG system against the hand-crafted, wide-coverage LFG and XLE pars-
ing system (Riezler et al 2002; Kaplan et al 2004) on the PARC 700 Dependency Bank
(King et al 2003). In the second experiment (Section 5.2), we evaluate against the hand-
crafted, wide-coverage unification grammar and RASP parsing system of Carroll and
Briscoe (2002) on the CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998).
5.1 Evaluation against PARC 700
The PARC 700 Dependency Bank (King et al 2003) provides dependency relations
(including LDD relations) for 700 sentences randomly selected from WSJ Section 23 of
the Penn-II Treebank. In order to evaluate the parsers, we follow the experimental setup
of Kaplan et al (2004) with a split of 560 dependency structures for the test set and 140
for the development set. The set of features (Table 12, later in this article) evaluated
in the experiment form a proper superset of preds-only, but a proper subset of all
22 Based on Cohen (1995, p. 190): ?e ? 1 ? (1 ? ?c )m, where m is the number of pairwise comparisons, ?e is
the experiment-wise error, and ?c is the per-comparison error.
105
Computational Linguistics Volume 34, Number 1
Figure 13
PARC 700 conversion software.
grammatical functions (preds-only ? PARC ? all GFs). This feature set was selected in
Kaplan et al because the features carry important semantic information. There are sys-
tematic differences between the PARC 700 dependencies and the f-structures generated
in our approach as regards feature geometry, feature nomenclature, and the treatment
of named entities. In order to evaluate against the PARC 700 test set, we automatically
map the f-structures produced by our parsers to a format similar to that of the PARC
700 Dependency Bank. This is done with conversion software in a post-processing stage
on the f-structure annotated trees (Figure 13).
The conversion software is developed on the 140-sentence development set of the
PARC 700, except for the Multi-Word Expressions section. Following the experimental
setup of Kaplan et al (2004), we mark up multi-word expression predicates based on
the gold-standard PARC 700 Dependency Bank.
Multi-Word Expressions The f-structure annotation algorithm analyzes the internal
structure of all noun phrases fully. In Figure 14, for example, BT is analyzed as
an ADJUNCT modifier of the head securities, whereas PARC 700 analyzes this and
other (more complex) named entities as multi-word expression predicates. The
conversion software transforms the output of the f-structure annotation algorithm
into the multi-word expression predicate format.
Feature Geometry In constructions such as Figure 2, the f-structure annotation algo-
rithm analyzes say as the main PRED with what is said as the value of a COMP
argument. In the PARC 700, these constructions are analyzed in such a way that
what is said/reported provides the top level f-structure whereas other material
(who reported, etc.) is analyzed in terms of ADJUNCTs modifying the top level
f-structure. A further systematic structural divergence is provided by the analysis
Figure 14
Named entity and OBL AG feature geometry mapping.
106
Cahill et al Statistical Parsing Using Automatic Dependency Structures
of passive oblique agent constructions (Figure 14): The f-structure annotation
algorithm generates a complex internal analysis of the oblique agent PP, whereas
the PARC analysis encodes a flat representation. The conversion software adjusts
the output of the f-structure annotation algorithm to the PARC-style encoding of
linguistic information.
Feature Nomenclature There are a number of systematic differences between feature
names used by the automatic annotation algorithm and PARC 700: For example,
DET is DET FORM in the PARC 700, COORD is CONJ, FOCUS is FOCUS INT. Nomen-
clature differences are treated in terms of a simple relabeling by the conversion
software.
Additional Features A number of features in the PARC 700 are not produced by the au-
tomatic annotation algorithm. These include: AQUANT for adjectival quantifiers,
MOD for NP-internal modifiers, and STMT TYPE for statement type (declarative,
interrogative, etc.). Additional features (and their values) are automatically gen-
erated by the mapping software, using categorial, configurational, and already
produced f-structure annotation information, extending the original annotation
algorithm.
XCOMP Flattening The automatic annotation algorithm treats both auxiliary and
modal verb constructions in terms of hierarchically cascading XCOMPs, whereas
in PARC 700 the temporal and aspectual information expressed by auxiliary verbs
is represented in terms of a flat analysis and features (Figure 15). The conversion
software automatically flattens the f-structures produced by the automatic anno-
tation algorithm into the PARC-style encoding.
For full details of the mapping, see Burke et al (2004).
In our parsing experiments, we used the most up-to-date version of the hand-
crafted, wide-coverage, deep LFG resources and XLE parsing system with improved
results over those reported in Kaplan et al (2004): This latest version achieves 80.55%
f-score, a 0.95 percentage point improvement on the previous 79.6%. The XLE parsing
system combines a large-scale, hand-crafted LFG for English and a statistical disam-
biguation component to choose the most likely analysis among those returned by
the symbolic parser. The statistical component is a log-linear model trained on 10,000
partially labeled structures from the WSJ. The results of the parsing experiments are
presented in Table 11. We also include a figure for the upper bound of each system.23
Using Bikel?s retrained parser, the treebank-based LFG system achieves an f-score of
82.73%, and the hand-crafted grammar and XLE-based system achieves an f-score
of 80.55%. The approximate randomization test produced a p-value of .0054 for this
pairwise comparison, showing that this result difference is statistically significant at
the 95% level. Evaluation results on a reannotated version (Briscoe and Carroll 2006) of
the PARC 700 Dependency Bank were recently published in Clark and Curran (2007),
reporting f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and
23 The upper bound for the treebank-based LFG system is determined by taking the original Penn-II WSJ
Section 23 trees corresponding to the PARC 700 strings, automatically annotating them with the
f-structure annotation algorithm, and evaluating the f-structures against the PARC 700 dependencies. The
upper bound for the XLE system is determined by selecting the XLE parse that scores best against the
PARC 700 dependencies for each of the PARC 700 strings. It is interesting to note that the upper bound
for the treebank-based system is only 1.18 percentage points higher than that for the XLE system. Apart
from the two different methods for establishing the upper bounds, this is most likely due to the fact that
the mapping required for evaluating the treebank-based LFG system against PARC 700 is lossy (cf. the
discussion in Section 6).
107
Computational Linguistics Volume 34, Number 1
Figure 15
DCU 105 and PARC 700 analyses for the sentence Unlike 1987, interest rates have been falling
this year.
Carroll point out, these evaluations are not directly comparable with the Kaplan et al
(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotation schemes are different. Kaplan et al and our experiments use a fine-grained
feature set of 34 features (Table 12), while the Briscoe and Carroll scheme uses 17
features.
A breakdown by dependency relation for each system is given in Table 12. The
treebank-induced grammar system can better identify DET FORM, SUBORD FORM, and
Table 11
Results of evaluation against the PARC 700 Dependency Bank following the experimental setup
of Kaplan et al (2004).
Bikel+Tags XLE p-Value
F-score 82.73 80.55 .0054
Upper bound 86.83 85.65 -
108
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 12
Breakdown by dependency relation of results of evaluation against PARC 700.
Dep. Percent of deps. F-score (%)
Bikel+Tags XLE
ADEGREE 6.17 80 82
ADJUNCT 14.32 68 66
AQUANT 0.06 78 61
COMP 1.23 80 74
CONJ 2.64 73 69
COORD FORM 1.20 83 90
DET FORM 4.61 97 91
FOCUS 0.02 0 36
MOD 2.74 74 67
NUM 19.82 91 89
NUMBER 1.42 89 83
NUMBER TYPE 2.10 94 86
OBJ 8.92 87 78
OBJ THETA 0.05 43 31
OBL 0.83 55 69
OBL AG 0.22 82 76
OBL COMPAR 0.07 38 56
PASSIVE 1.14 80 88
PCASE 0.25 79 68
PERF 0.41 89 90
POSS 0.98 88 80
PRECOORD FORM 0.03 0 91
PROG 0.97 89 81
PRON FORM 2.54 92 94
PRON INT 0.03 0 33
PRON REL 0.57 74 72
PROPER 3.56 83 93
PRT FORM 0.22 80 41
QUANT 0.34 77 80
STMT TYPE 5.23 87 80
SUBJ 8.51 78 78
SUBORD FORM 0.93 47 42
TENSE 5.02 95 90
TOPIC REL 0.57 56 73
XCOMP 2.29 80 78
PRT FORM dependencies24 and achieves higher f-scores for OBJ and POSS. However, the
hand-crafted parsing system can better identify FOCUS, OBL(ique) arguments, PRECO-
ORD FORM and TOPICREL relations.
5.2 Evaluation against CBS 500
We also compare the hand-crafted, deep, probabilistic unification grammar-based RASP
parsing system of Carroll and Briscoe (2002) to our treebank- and retrained Bikel
24 DET FORM, SUBORD FORM, and PRT FORM (and in general X FORM) dependencies record (semantically
relevant) surface forms in f-structure for X-type closed class categories.
109
Computational Linguistics Volume 34, Number 1
Figure 16
CBS 500 conversion software.
parser-based LFG system. The RASP parsing system is a domain-independent, robust
statistical parsing system for English, based on a hand-written, feature-based unification
grammar. A probabilistic parse selection model conditioned on the structural parse
context, degree of support for a subanalysis in the parse forest, and lexical informa-
tion (when available) chooses the most likely parses. For this experiment, we evaluate
against the CBS 500,25 developed by Carroll, Briscoe, and Sanfilippo (1998) in order to
evaluate a precursor of the RASP parsing resources. The CBS 500 contains dependency
structures (including some long distance dependencies26) for 500 sentences chosen at
random from the SUSANNE corpus (Sampson 1995), but subject to the constraint that
they are parsable by the parser in Carroll, Briscoe, and Sanfilippo. Aswith the PARC 700,
there are systematic differences between the f-structures produced by our methodology
and the dependency structures of the CBS 500. In order to be able to evaluate against
the CBS 500, we automatically map our f-structures into a format similar to theirs. We
did not split the data into a heldout and a test set when developing the mapping, so
that a comparison could be made with other systems that report evaluations against
the CBS 500. The following CBS 500-style grammatical relations are produced from the
f-structure in Figure 1:
(ncsubj sign U.N.)
(dobj sign treaty)
Some mapping is carried out (as in the evaluation against the PARC 700) on the f-
structure annotated trees, and the remaining mapping is carried out on the f-structures
(Figure 16). As with the PARC 700 mapping, all mappings are carried out automatically.
The following phenomena were dealt with on the f-structure annotated trees:
Auxiliary verbs (xcomp flattening) XCOMPS were flattened to promote the main verb
to the top level, while maintaining a list of auxiliary and modal verbs and their
relation to one another.
Treatment of topicalized sentences The predicate of the topicalized sentence became
the main predicate and any other top level material became an adjunct.
Multi-word expressions Multi-word expressions (such as according to) were not
marked up in the parser input, but captured in the annotated trees and the
annotations adjusted accordingly.
Treatment of the verbs be and become Our automatic annotation algorithm does not
treat the verbs be and become differently from any other verbs when they are used
transitively. This analysis conflicted with the CBS 500 analysis, so was changed to
match theirs.
25 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
26 The long distance dependencies include passive, wh-less relative clauses, control verbs, and so forth.
110
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 13
Results of dependency evaluation against the CBS 500 (Carroll, Briscoe, and Sanfillipo 1998).
Bikel+Tags RASP p-Value
F-score 80.23 76.57 <.0001
The following are the main mappings carried out on the f-structures:
Encoding of Passive We treat passive as a feature in our automatic f-structure annota-
tion algorithm, whereas the CBS 500 triples encode this information indirectly.
Objects of Prepositional Phrases No dependency was generated for these objects, as
there was no corresponding dependency in the CBS 500 analyses.
Nomenclature Differences There were some trivial mappings to account for differ-
ences in nomenclature, for example OBL in our analyses became IOBJ in the
mapped dependencies.
Encoding of wh-less relative clauses These are encoded by means of reentrancies in
f-structure, but were encoded in a more indirect way in the mapped dependencies
to match the CBS 500 annotation format.
To carry out the experiments, we POS-tagged the tokenized CBS 500 sentences with
the MXPOST tagger (Ratnaparkhi 1996) and parsed the tag sequences with our Penn-II
and Bikel retrained-based LFG system. We use the evaluation software of Carroll,
Briscoe, and Sanfilippo (1998)27 to evaluate the grammatical relations produced by each
parser. The results are given in Table 13.
Our LFG system based on Bikel?s retrained parser achieves an f-score of 80.23%,
whereas the hand-crafted RASP grammar and parser achieves an f-score of 76.57%.
Crouch et al (2002) report that their XLE system achieves an f-score of 76.1% for the
same experiment. A detailed breakdown by dependency is given in Table 14. The
LFG system based on Bikel?s retrained parser is able to better identify MOD(ifier) de-
pendency relations, ARG MOD (the relation between a head and a semantic argument
which is syntactically realized as a modifier, for example by-phrases), IOBJ (indirect
object) and AUXiliary relations. RASP is able to better identify XSUBJ (clausal subjects
controlled from without), CSUBJ (clausal subjects), and COMP (clausal complement)
relations. Again we use the Approximate Randomization Test to test the parsing results
for statistical significance. The p-value for the test comparing our system using Bikel?s
retrained parser against RASP is <.0001. The treebank-based LFG system using Bikel?s
retrained parser is significantly better than the hand-crafted, deep, unification grammar-
based RASP parsing system with a statistical significance of >95%.
6. Discussion and Related Work
At the moment, we can only speculate as to why our treebank-based LFG resources
outperform the hand-crafted XLE and RASP grammars.
In Section 4, we observed that the treebank-induced LFG resources have consid-
erably wider coverage (>99.9% measured in terms of complete spanning parse) than
27 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
111
Computational Linguistics Volume 34, Number 1
Table 14
Breakdown by grammatical relation for results of evaluation against CBS 500.
Dep. Percent of deps. F-score (%)
Bikel+Tags RASP
DEPENDANT 100.00 80.23 76.57
MOD 48.96 80.30 75.29
NCMOD 30.42 85.37 72.98
XMOD 1.60 70.05 55.88
CMOD 2.61 75.60 53.08
DETMOD 14.06 95.85 91.97
ARG MOD 0.51 80.00 64.52
ARG 43.70 78.28 77.57
SUBJ 13.10 79.84 83.57
NCSUBJ 12.99 87.84 84.32
XSUBJ 0.06 0.00 88.89
CSUBJ 0.04 0.00 22.22
SUBJ OR DOBJ 18.22 81.21 83.84
COMP 12.38 76.73 71.87
OBJ 7.34 76.05 69.53
DOBJ 5.11 84.55 84.57
OBJ2 0.25 48.00 43.84
IOBJ 1.98 59.04 47.60
CLAUSAL 5.04 77.74 75.37
XCOMP 4.03 80.00 84.11
CCOMP 1.01 69.61 75.14
AUX 4.76 94.94 88.27
CONJ 2.06 68.84 69.09
the hand-crafted grammars (?80% for XLE and RASP grammars on unseen treebank
text). Both XLE and RASP use a number of (largest) fragment-combining techniques
to achieve full coverage. If coverage is a significant component in the performance
difference observed between the hand-crafted and treebank-induced resources, then
it is reasonable to expect that the performance difference is more pronounced with
increasing sentence length (with shorter sentences being simpler and more likely to
be within the coverage of the hand-crafted grammars). In other words, we expect the
hand-crafted, deep, precision grammars to do better on shorter sentences (more likely to
be within their coverage), whereas the treebank-induced grammars should show better
performance on longer strings (less likely to be within the coverage of the hand-crafted
grammars).
In order to test this hypothesis, we carried out a number of experiments:
First, we plotted the sentence length distribution for the 560 PARC 700 test sen-
tences and the 500 CBS 500 sentences (Figures 17 and 18). Both gold standards are
approximately normally distributed, with the CBS 500 distribution possibly showing
the effects of being chosen subject to the constraint that the strings are parsable by the
parser in Carroll, Briscoe, and Sanfilippo (1998). For each case we use the mean, ?, and
two standard deviations, 2?, to the left and right of themean to exclude sentence lengths
not supported by sufficient observations: For PARC 700, ? = 23.27, ?? 2? = 2.17, and
?+ 2? = 44.36; for CBS 500, ? = 17.27, ?? 2? = 1.59, and ?+ 2? = 32.96. Both the
PARC 700 and the CB 500 distributions are positively skewed. For the PARC 700, ?? 2?
is actually outside the observed data range, whereas for CB 500, ?? 2? almost coincides
112
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 17
Distribution of sentence frequency by sentence length in the PARC 700 test set with Bezier
interpolation. Vertical lines mark two standard deviations from the mean.
with the left border. It is therefore useful to further constrain the ?2? range by a
sentence count threshold of ? 5.28 This results in a sentence length range of 4?41 for
PARC 700 and 4?32 for CBS 500.
Second, in order to test whether fragment parses increase with sentence length, we
plotted the percentage of fragment parses over sentence length for the XLE parses of the
560-sentence test set of the PARC 700 (we did not do this for the CBS 500 as its strings
are selected to be fully parsable by RASP). Figure 19 shows that the number of fragment
parses tends to increase with sentence length.
Third, we plotted the average dependency f-score for the hand-crafted and the
treebank-induced resources against sentence lengths. Figure 20 shows the results for
PARC 700, Figure 21 for CBS 500.
In both cases, contrary to our (perhaps somewhat naive) assumption, the graphs
show that the treebank-induced resources outperform the hand-crafted resources
within (most of) the 4?41 and 4?32 sentence length bounds, with the results for the very
short and the very long strings outside those bounds not being supported by sufficient
data points.
In the parsing literature, results are often also provided for strings with lengths
?40. Below we give those results and statistical significance testing for the PARC 700
and CBS 500 (Tables 15 and 16). The results show that the Bikel retrained?based LFG
system achieves a higher dependency f-score on sentences of length ?40 than on all
sentences, whereas the XLE system achieves a slightly lower score on sentences of
length ?40. The Bikel-retrained system achieves an f-score of 83.18%, a statistically
28 Note that because the distributions in Figures 17 and 18 are Bezier interpolated, the constraint does not
guarantee that every sentence length within the range occurs five or more times.
113
Computational Linguistics Volume 34, Number 1
Figure 18
Distribution of sentence frequency by sentence length in the CB 500 test set with Bezier
interpolation. Vertical lines mark two standard deviations from the mean.
Figure 19
Percentage of fragment sentences for XLE parsing system per sentence length with Bezier
interpolation.
significant improvement of 2.67 percentage points over the XLE system on sentences of
length?40. Against the CBS 500, Bikel?s retrained system achieves a weighted f-score of
82.58%, a statistically significant improvement of 3.87 percentage points over the RASP
system which achieves a weighted f-score of 78.81% on sentences of length ?40.
114
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 20
Average f-score by sentence length for PARC 700 test set with Bezier interpolation.
Figure 21
Average f-score by sentence length for CB 500 test set with Bezier interpolation.
Finally, we test whether the strict preds-only dependency evaluation has an ef-
fect on the results for the PARC 700 experiments. Recall that following Kaplan et al
(2004) for the PARC 700 evaluation we used a set of semantically relevant grammatical
functions that is a superset of preds-only and a subset of all-GFs. A preds-only based
evaluation is ?stricter? and tends to produce lower scores as it directly reflects the effects
115
Computational Linguistics Volume 34, Number 1
Table 15
Evaluation and significance testing of sentences length ?40 against the PARC 700.
All sentence lengths Length ?40
f-score f-score
Bikel + Tags 82.73 83.18
XLE 80.55 80.51
p-value .0054 .0010
Table 16
Evaluation and significance testing of sentences length ?40 against the CBS 500.
All sentence lengths Length ?40
f-score f-score
Bikel + Tags 80.23 82.58
RASP 76.57 78.81
p-value <.0001 <.0001
Table 17
Preds-only evaluation against the PARC 700 Dependency Bank.
All GFs Preds only
f-score f-score
Bikel + Tags 82.73 77.40
XLE 80.55 74.31
of predicate?argument/adjunct misattachments in the resulting dependency represen-
tations (while local functions such as NUM(ber), for example, can score properly even
if the local predicate is misattached). Table 1729 below gives the results for preds-only
evaluation30 on the PARC 700 for all sentence lengths. The results show that the Bikel-
retrained treebank-based LFG resource achieves an f-score of 77.40%, 5.33 percentage
points lower than the score for all the PARC dependencies. The XLE system achieves
an f-score of 74.31%, 6.24 percentage points lower than the score for all the PARC
dependencies and a 3.09 percentage point drop against the results obtained by the
Bikel-retrained treebank-based LFG resources. The performance of the Bikel retrained?
based LFG system suffers less than the XLE system when preds-only dependencies are
evaluated.
It is important to note that in our f-structure annotation algorithm and treebank-
based LFG parsing architectures, we do not claim to provide fully adequate statistical
models. It is well known (Abney 1997) that PCFG- or history-based parser approxima-
tions to general constraint-based grammars can yield inconsistent probability models
29 We do not include a p-value here as the breakdown by function per sentence was not available to us for
the XLE data.
30 The dependency relations we include in preds-only evaluation are: ADJUNCT, AQUANT, COMP, CONJ,
COORD FORM, DET FORM, FOCUS INT, MOD, NUMBER, OBJ, OBJ THETA, OBL, OBL AG, OBL COMPAR,
POSS, PRON INT, PRON REL, PRT FORM, QUANT, SUBJ, SUBORD FORM, TOPIC REL, XCOMP.
116
Cahill et al Statistical Parsing Using Automatic Dependency Structures
due to loss of probability mass: The parser successfully returns the highest ranked
parse tree but the constraint solver cannot resolve the f-structure equations and the
probability mass associated with that tree is lost. Research on adequate probability
models for constraint-based grammars is important (Bouma, van Noord, and Malouf
2000; Miyao and Tsujii 2002; Riezler et al 2002; Clark and Curran 2004). In this context,
it is interesting to compare parser performance against upper bounds. For the PARC
700 evaluation, the upper bound for the XLE-based resources is 85.65%, against 86.83%
for the treebank-based LFG resources. XLE-based parsing currently achieves 94.05%
(f-score 80.55%) of its upper bound using a discriminative disambiguation method,
whereas the treebank-based LFG resource achieves 95.28% (f-score 82.73%) of its upper
bound.
Although this seems to indicate that the two disambiguationmodels achieve similar
results, the figures are actually very difficult to compare. In the case of the XLE, the
upper bound is established by unpacking all parses for a string and scoring the best
match against the gold standard (rather than letting the probability model select a
parse). By contrast, in the case of the treebank-based LFG resources, we use the original
?perfect? Penn-II treebank trees (rather than the trees produced by the parser), auto-
matically annotate those trees with the f-structure annotation algorithm, and score the
results against the PARC 700 (it is not feasible to generate all parses for a string, there
are simply too many for treebank-induced resources). The upper bound computed in
this fashion for the treebank-based LFG resource (86.83%) is relatively low. The main
reason is that the automatic mapping required to relate the f-structures generated by the
treebank-based LFG resources to the PARC 700 dependencies is lossy. This is indicated
by comparing the upper bound for the treebank-based LFG resources for the PARC 700
against the upper bound for the DCU 105 gold standard, where little or no mapping
(apart from the feature-structure to dependency-triple conversion) is required: Scoring
the f-structure annotations for the original treebank trees results in 86.83% against PARC
700 versus 96.80% against DCU 105.
Our discussion shows how delicate it can be to compare parsing systems and
their disambiguation models. Ultimately what is required is an evaluation strategy that
separates out and clearly distinguishes between the grammar, parsing algorithm, and
disambiguation model and is capable of assessing different combinations of these core
components. Of course, this will not always be possible andmoving towards it is part of
a much more extended research agenda, well beyond the scope of the research reported
in the present article. Our approach, and previous approaches, evaluate systems at
the highest level of granularity, that of the complete package: the combined grammar-
parser-disambiguation model. The results show that machine-learning-based resources
can outperform deep, state-of-the-art hand-crafted resources with respect to the quality
of dependencies generated.
Treebank-based, deep and wide-coverage constraint-based grammar acquisition
has become an important research topic: Starting with the seminal TAG-based work
of Xia (1999), there are now also HPSG-, CCG- and LFG-based approaches. Miyao,
Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present re-
search on inducing Penn-II treebank-based HPSGs with log-linear probability models.
Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced
CCG-basedmodels including LDD resolution. It would be interesting to conduct a com-
parative evaluation involving treebank-based HPSG, CCG, and LFG resources against
the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive compar-
ison of machine-learning-based with hand-crafted, deep, wide-coverage resources such
as those used in the XLE or RASP parsing systems.
117
Computational Linguistics Volume 34, Number 1
Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank
(Kingsbury, Palmer, andMarcus 2002)-based evaluations of their automatically induced
CCG and HPSG resources. PropBank-based evaluations provide valuable information
to rank parsing systems. Currently, however, PropBank-based evaluations are some-
what partial: They only represent (and hence score) verbal arguments and disregard a
raft of other semantically important dependencies (e.g., temporal and aspectual infor-
mation, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS
500 Dependency Banks.31
7. Conclusions
Parser comparison is a non-trivial and time-consuming exercise. We extensively eval-
uated four machine-learning-based shallow parsers and two hand-crafted, wide-
coverage deep probabilistic parsers involving four gold-standard dependency banks,
using the Approximate Randomization Test (Noreen 1989) to test for statistical signifi-
cance. We used a sophisticated method for automatically producing deep dependency
relations from Penn-II-style CFG-trees (Cahill et al 2002b, 2004) to compare shallow
parser output at the level of dependency relation and revisit experiments carried out by
Preiss (2003) and Kaplan et al (2004).
Our main findings are twofold:
1. Using our CFG-tree-to-dependency annotation technology, together with treebank- and
machine-learning-based parsers, we can outperform hand-crafted, wide-coverage, deep,
probabilistic, constraint-based grammars and parsers.
This result is surprising for two reasons. First, it is established against two externally-
provided dependency banks (the PARC 700 and the CBS 500 gold standards), originally
designed using the hand-crafted, wide-coverage, probabilistic grammars for the XLE
(Riezler et al 2002) and RASP (Carroll and Briscoe 2002) parsing systems to evaluate
those systems. What is more, the SUSANNE corpus-based CBS 500 constitutes an
instance of domain variation for the Penn-II-trained LFG resources, likely to adversely
affect scores. Second, the treebank- and machine-learning-based LFG resources require
automatic mapping to relate f-structure output of the treebank-based parsing systems
to the representation format in the PARC 700 and CBS 500 Dependency Banks. These
mappings are partial and lossy: That is, they do not cover all of the systematic dif-
ferences between f-structure and dependency bank representations and introduce a
certain amount of error in what they are designed to capture, that is they both over-
and undergeneralize, again adversely affecting scores. Improvements of the mappings
should lead to a further improvement in the dependency scores.
2. Parser evaluation at the level of dependency representation still requires non-trivial
mappings between different dependency representation formats.
31 In a sense, PropBank does not yet provide a single agreed upon gold standard: Role information is
provided indirectly and an evaluation gold-standard has to be computed from this. In doing so, choices
have to be made as regards the representation of shared arguments, the analysis of coordinate structures,
and so forth, and it is not clear that the same choices are currently made for evaluations carried out by
different research groups.
118
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Earlier we criticized tree-based parser evaluation on the grounds that equally valid
different tree-typologies can be associated with strings, and identified this as a major
obstacle to fair and unbiased parser evaluation. Dependency-based parser evaluation,
as it turns out, is not entirely free of this criticism either: There are significant systematic
differences between the PARC 700 dependency and the CBS 500 dependency represen-
tations; there are significant systematic differences between the LFG f-structures gener-
ated by the hand-crafted, wide-coverage grammars of Riezler et al (2002) and Kaplan
et al (2004) and those of the treebank-induced and f-structure annotation algorithm
based resources of Cahill et al (2004). These differences require careful implementation
of mappings if parsers are not to be unduly penalized for systematic and motivated
differences at the level of dependency representation. By and large, these differences
are, however, less pronounced than differences on CFG tree representations, making
dependency-based parser evaluation a worthwhile and rewarding exercise.
Summarizing our results, we find that against the DCU 105 development set, the
treebank- and f-structure annotation algorithm-based LFG system using Bikel?s parser
retrained to retain Penn-II functional tag labels performs best, achieving f-scores of
82.92% preds-only and 88.3% all grammatical functions. Against the automatically
generated WSJ Section 22 Dependency Bank, the system using Bikel?s retrained parser
achieves the highest results, achieving f-scores of 83.06% preds-only and 87.861% all
GFs. This is statistically significantly better than all other parsers. In order to evaluate
against the PARC 700 and CBS 500 gold standards, we automaticallymap the dependen-
cies produced by our treebank-based LFG system into a format compatible with the gold
standards. Against the PARC 700 Dependency Bank, the treebank-based LFG system
using Bikel?s retrained parser achieves an f-score of 82.73%, a statistically significant
improvement of 2.18% against the most up-to-date results of the hand-crafted XLE-
based parsing resources. Against the CBS 500, the treebank-based LFG system using
Bikel?s retrained parser achieved the highest f-score of 80.23%, a statistically significant
improvement of 3.66 percentage points on the highest previously published results
for the same experiment with the hand-crafted RASP resources in Carroll and Briscoe
(2002).
Appendix A. Parser Parameter Settings
This section provides a complete list of the parameter settings used for each of the
parsers described in this article.
Parser Parameters
Collins Model 3 We used the Collins parser with its default settings of a
beam size of 10,000 and where the values of the following
flags are set to 1: punctuation-flag, distaflag, distvflag,
and npbflag. Input was pre-tagged using the MXPOST
POS tagger (Ratnaparkhi 1996). We parse the input file
with all three models and use the scripts provided to
merge the outputs into the final parser output file. Note
that this file has been cleaned of all -A functional tags and
trace nodes.
Charniak We used the parser dated August 2005 and ran the
parser using the data provided in the download on pre-
tokenized sentences of length ?200. Input was automati-
cally tagged by the parser.
119
Computational Linguistics Volume 34, Number 1
Bikel Emulation of We used version 0.9.9b of the parser trained on the file
Collins Model 2 of observed events made available on the downloads
page. We used the collins.properties file and a maximum
heap size of 1,500 MB. Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996).
Bikel + Functional Tags We used version 0.9.9b of the parser trained on a version
of Sections 02?21 of the Penn-II treebank where functional
tags were not ignored by the parser. We updated the de-
fault head-finding rules to deal with the new categories.
We also trained on all sentences from the training set
(the default collins.properties file is set to ignore trees of
more than 500 tokens). Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996) and the maxi-
mum heap size was 1,500 MB.
RASP Weused a file of parser output provided through personal
communication with John Carroll. (Tagging is carried out
automatically by the parser.)
XLE We used a file of parser output provided by the Natural
Language Theory and Technology group at the Palo Alto
Research Center. (Tagging is carried out automatically by
the parser.)
Acknowledgments
We are grateful to our anonymous reviewers
whose insightful comments have improved
the article significantly. We would like to
thank John Carroll for discussion and help
with reproducing the RASP parsing results;
Michael Collins, Eugene Charniak, Dan
Bikel, and Helmut Schmid for making their
parsing engines available; and Ron Kaplan
and the team at PARC for discussion,
feedback, and support. Part of the research
presented here has been supported by
Science Foundation Ireland grants
04/BR/CS0370 and 04/IN/I527, Enterprise
Ireland Basic Research Grant SC/2001/0186,
an Irish Research Council for Science,
Engineering and Technology Ph.D.
studentship, an IBM Ph.D. studentship and
support from IBM?s Centre for Advanced
Studies (CAS) in Dublin.
References
Abney, Stephen. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597?618.
Alshawi, Hiyan and Stephen Pulman, 1992.
Ellipsis, Comparatives, and Generation,
chapter 13. The MIT Press, Cambridge,
MA.
Baldwin, Timothy, Emily Bender, Dan
Flickinger, Ara Kim, and Stephan Oepen.
2004. Road-testing the English Resource
Grammar over the British National
Corpus. In Proceedings of the Fourth
International Conference on Language
Resources and Evaluation (LREC 2004),
pages 2047?2050, Lisbon, Portugal.
Bikel, Dan. 2002. Design of a multi-lingual,
parallel-processing statistical parsing
engine. In Proceedings of HLT 2002,
pages 24?27, San Diego, CA.
Black, Ezra, Steven Abney, Dan Flickenger,
Claudia Gdaniec, Ralph Grishman, Philip
Harrison, Donald Hindle, Robert Ingria,
Fred Jelineck, Judith Klavans, Mark
Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic
coverage of english grammars. In
Proceedings of the Speech and Natural
Language Workshop, pages 306?311, Pacific
Grove, CA.
Bod, Rens. 2003. An efficient implementation
of a new DOP model. In Proceedings of the
Tenth Conference of the European Chapter of
the Association for Computational Linguistics
(EACL?03), pages 19?26, Budapest,
Hungary.
Bouma, Gosse, Gertjan van Noord, and
Robert Malouf. 2000. Alpino:
Wide-coverage computational analysis of
dutch. In Walter Daelemans, Khalil
Sima?an, Jorn Veenstra, and Jakub Zavrel,
editors, Computational Linguistics in The
Netherlands 2000. Rodopi, Amsterdam,
pages 45?59.
120
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford, England.
Briscoe, Edward and John Carroll. 1993.
Generalized probabilistic LR parsing of
natural language (corpora) with
unification-based grammars. Computational
Linguistics, 19(1):25?59.
Briscoe, Ted and John Carroll. 2006.
Evaluating the accuracy of an
unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the
COLING/ACL 2006 Main Conference Poster
Sessions, pages 41?48, Sydney, Australia.
Briscoe, Edward, Claire Grover, Bran
Boguraev, and John Carroll. 1987. A
formalism and environment for the
development of a large grammar of
English. In Proceedings of the 10th
International Joint Conference on AI,
pages 703?708, Milan, Italy.
Burke, Michael. 2006. Automatic Treebank
Annotation for the Acquisition of LFG
Resources. Ph.D. thesis, School of
Computing, Dublin City University,
Dublin, Ireland.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Evaluation of an automatic
annotation algorithm against the PARC
700 Dependency Bank. In Proceedings of the
Ninth International Conference on LFG,
pages 101?121, Christchurch, New Zealand.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The Parallel Grammar
Project. In Proceedings of COLING 2002,
Workshop on Grammar Engineering and
Evaluation, pages 1?7, Taipei, Taiwan.
Cahill, Aoife. 2004. Parsing with Automatically
Acquired, Wide-Coverage, Robust,
Probabilistic LFG Approximations. Ph.D.
thesis, School of Computing, Dublin City
University, Dublin, Ireland.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Long-distance dependency
resolution in automatically acquired
wide-coverage PCFG-based LFG
approximations. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 320?327,
Barcelona, Spain.
Cahill, Aoife, Maire?ad McCarthy, Josef van
Genabith, and Andy Way. 2002a.
Automatic annotation of the Penn
Treebank with LFG f-structure
information. In Proceedings of the LREC
Workshop on Linguistic Knowledge
Acquisition and Representation: Bootstrapping
Annotated Language Data, pages 8?15, Las
Palmas, Canary Islands, Spain.
Cahill, Aoife, Maire?ad McCarthy, Josef van
Genabith, and Andy Way. 2002b. Parsing
with PCFGs and automatic f-structure
annotation. In Proceedings of the Seventh
International Conference on LFG,
pages 76?95, Palo Alto, CA.
Carroll, John and Edward Briscoe. 2002.
High precision extraction of grammatical
relations. In Proceedings of the 19th
International Conference on Computational
Linguistics (COLING), pages 134?140,
Taipei, Taiwan.
Carroll, John, Edward Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation: A
survey and new proposal. In Proceedings of
the International Conference on Language
Resources and Evaluation, pages 447?454,
Granada, Spain.
Carroll, John, Anette Frank, Dekang Lin,
Detlef Prescher, and Hans Uszkoreit,
editors. 2002. HLT Workhop: ?Beyond
PARSEVAL ? Towards improved evaluation
measures for parsing systems?, Las Palmas,
Canary Islands, Spain.
Charniak, Eugene. 1996. Tree-bank
grammars. In Proceedings of the
Thirteenth National Conference on
Artificial Intelligence, pages 1031?1036,
Menlo Park, CA.
Charniak, Eugene. 2000. A maximum
entropy inspired parser. In Proceedings
of the First Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 2000),
pages 132?139, Seattle, WA.
Chinchor, Nancy, Lynette Hirschman, and
David D. Lewis. 1993. Evaluating message
understanding systems: An analysis of the
Third Message Understanding Conference
(MUC-3). Computational Linguistics,
19(3):409?449.
Clark, Stephen and James Curran. 2004.
Parsing the WSJ using CCG and log-linear
models. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL-04), pages 104?111,
Barcelona, Spain.
Clark, Stephen and James Curran. 2007.
Formalism-independent parser evaluation
with CCG and DepBank. In Proceedings of
the 45th Annual Meeting of the Association
for Computational Linguistics (ACL 2007),
pages 248?255, Prague, Czech Republic
http://www.aclweb-org/anthology/P/
P07/P07-1032.
Clark, Stephen and Julia Hockenmaier. 2002.
Evaluating a wide-coverage CCG parser.
121
Computational Linguistics Volume 34, Number 1
In Proceedings of the LREC 2002 Beyond
Parseval Workshop, pages 60?66, Las
Palmas, Canary Islands, Spain.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. The MIT Press,
Cambridge, MA.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid, Spain.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia, PA.
Crouch, Richard, Ron Kaplan, Tracy Holloway
King, and Stefan Riezler. 2002. A
comparison of evaluation metrics for a
broad coverage parser. In Proceedings of
the LREC Workshop: Beyond PARSEVAL?
Towards Improved Evaluation Measures for
Parsing Systems, pages 67?74, Las Palmas,
Canary Islands, Spain.
Dalrymple, Mary. 2001. Lexical-Functional
Grammar. London, Academic Press.
Dienes, Pe?ter and Amit Dubey. 2003.
Antecedent recovery: Experiments with a
trace tagger. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing, pages 33?40, Sapporo,
Japan.
Eisele, Andreas and Jochen Do?rre. 1986. A
lexical functional grammar system in
Prolog. In Proceedings of the 11th International
Conference on Computational Linguistics
(COLING 1986), pages 551?553, Bonn.
Flickinger, Dan. 2000. On building a more
efficient grammar by exploiting types.
Natural Language Engineering, 6(1):15?28.
Gaizauskas, Rob. 1995. Investigations into
the grammar underlying the Penn
Treebank II. Research Memorandum
CS-95-25, Department of Computer
Science, Univeristy of Sheffield, UK.
Gildea, Daniel and Julia Hockenmaier.
2003. Identifying semantic roles using
combinatory categorial grammar.
In Proceedings of the 2003 Conference
on Empirical Methods in Natural
Language Processing, pages 57?64,
Sapporo, Japan.
Hockenmaier, Julia. 2003. Parsing with
generative models of predicate?argument
structure. In Proceedings of the 41st Annual
Conference of the Association for
Computational Linguistics, pages 359?366,
Sapporo, Japan.
Hockenmaier, Julia and Mark Steedman.
2002. Generative models for statistical
parsing with combinatory categorial
grammar. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 335?342,
Philadelphia, PA.
Johnson, Mark. 1999. PCFG models of
linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for
recovering empty nodes and their
antecedents. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 136?143,
Philadelphia, PA.
Kaplan, Ron and Joan Bresnan. 1982.
Lexical functional grammar, a formal
system for grammatical representation.
In Joan Bresnan, editor, The Mental
Representation of Grammatical Relations.
MIT Press, Cambridge, MA,
pages 173?281.
Kaplan, Ron, Stefan Riezler, Tracy Holloway
King, John T. Maxwell, Alexander
Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of the
Human Language Technology Conference and
the 4th Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL?04),
pages 97?104, Boston, MA.
Kaplan, Ronald and Annie Zaenen. 1989.
Long-distance dependencies, constituent
structure and functional uncertainty. In
Mark Baltin and Anthony Kroch, editors,
Alternative Conceptions of Phrase Structure,
pages 17?42, University of Chicago Press,
Chicago.
King, Tracy Holloway, Richard Crouch,
Stefan Riezler, Mary Dalrymple, and Ron
Kaplan. 2003. The PARC 700 dependency
bank. In Proceedings of the EACL03: 4th
International Workshop on Linguistically
Interpreted Corpora (LINC-03), pages 1?8,
Budapest, Hungary.
Kingsbury, Paul, Martha Palmer, and
Mitch Marcus. 2002. Adding semantic
annotation to the Penn TreeBank. In
Proceedings of the Human Language
Technology Conference, pages 252?256,
San Diego, CA.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo, Japan.
Leech, Geoffrey and Roger Garside. 1991.
Running a grammar factory: On the
122
Cahill et al Statistical Parsing Using Automatic Dependency Structures
compilation of parsed corpora, or
?treebanks?. In Stig Johansson and
Anna-Brita Stenstro?m, editors, English
Computer Corpora: Selected Papers. Mouton
de Gruyter, Berlin, pages 15?32.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In Proceedings
of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL 2004),
pages 328?335, Barcelona, Spain.
Lin, Dekang. 1995. A dependency-based
method for evaluating broad-coverage
parsers. In Proceedings of the International
Joint Conference on AI, pages 1420?1427,
Montre?al, Canada.
Magerman, David. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Department of Computer
Science, Stanford University, CA.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The
Penn Treebank: Annotating predicate
argument structure. In Proceedings
of the ARPA Workshop on Human
Language Technology, pages 110?115,
Princeton, NJ.
McCarthy, Maire?ad. 2003. Design and
Evaluation of the Linguistic Basis of an
Automatic F-Structure Annotation Algorithm
for the Penn-II Treebank. Master?s thesis,
School of Computing, Dublin City
University, Dublin, Ireland.
McDonald, Ryan and Fernando Pereira. 2006.
Online learning of approximate
dependency parsing algorithms. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 81?88,
Trento, Italy.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2003. Probabilistic modeling
of argument structures including non-local
dependencies. In Proceedings of the
Conference on Recent Advances in Natural
Language Processing (RANLP),
pages 285?291, Borovets, Bulgaria.
Miyao, Yusuke and Jun?ichi Tsujii. 2002.
Maximum entropy estimation for feature
forests. In Proceedings of Human Language
Technology Conference (HLT 2002),
pages 292?297, San Diego, CA.
Miyao, Yusuke and Jun?ichi Tsujii.
2004. Deep linguistic analysis
for the accurate identification of
predicate?argument relations. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2004), pages 1392?1397,
Geneva, Switzerland.
Noreen, Eric W. 1989. Computer Intensive
Methods for Testing Hypotheses: An
Introduction. Wiley, New York.
O?Donovan, Ruth, Michael Burke, Aoife
Cahill, Josef van Genabith, and Andy
Way. 2004. Large-scale induction and
evaluation of lexical resources from the
Penn-II treebank. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 368?375,
Barcelona, Spain.
Pollard, Carl and Ivan Sag. 1994. Head-driven
Phrase Structure Grammar. CSLI
Publications, Stanford, CA.
Preiss, Judita. 2003. Using grammatical
relations to compare parsers. In Proceedings
of the Tenth Conference of the European
Chapter of the Association for Computational
Linguistics (EACL?03), pages 291?298,
Budapest, Hungary.
Ratnaparkhi, Adwait. 1996. A maximum
entropy part-of-speech tagger. In
Proceedings of the Empirical Methods in
Natural Language Processing Conference,
pages 133?142, Philadelphia, PA.
Riezler, Stefan, Tracy King, Ronald Kaplan,
Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing
theWall Street Journal using a
lexical-functional grammar and
discriminative estimation techniques.
In Proceedings of the 40th Annual
Conference of the Association for
Computational Linguistics (ACL-02),
pages 271?278, Philadelphia, PA.
Sampson, Geoffrey. 1995. English for the
Computer: The SUSANNE Corpus and
Analytic Scheme. Clarendon Press,
Oxford, England.
Tsuruoka, Yoshimasa, Yusuke Miyao,
and Jun?ichi Tsujii. 2004. Towards
efficient probabilistic HPSG parsing:
Integrating semantic and syntactic
preference to guide the parsing.
In Proceedings of IJCNLP-04 Workshop:
Beyond shallow analyses?Formalisms
and statistical modeling for deep
analyses, Hainan Island, China. [No
page numbers].
van Genabith, Josef and Richard Crouch.
1996. Direct and underspecified
interpretations of LFG f-structures. In 16th
International Conference on Computational
Linguistics (COLING 96), pages 262?267,
Copenhagen, Denmark.
123
Computational Linguistics Volume 34, Number 1
van Genabith, Josef and Richard
Crouch. 1997. On interpreting
f-structures as UDRSs. In Proceedings
of ACL-EACL-97, pages 402?409,
Madrid, Spain.
Xia, Fei. 1999. Extracting tree adjoining
grammars from bracketed corpora. In
Proceedings of the 5th Natural Language
Processing Pacific Rim Symposium
(NLPRS-99), pages 398?403, Beijing,
China.
Xue, Nianwen, Fei Xia, Fu-Dong Chiou, and
Martha Palmer. 2004. The Penn Chinese
treebank: Phrase structure annotation of a
large corpus. Natural Language Engineering,
10(4):1?30.
Yeh, Alexander. 2000. More accurate tests
for the statistical significance of result
differences. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING, 2000), pages 947?953,
Saarbru?cken, Germany.
124
Statistical Sentence Condensation using Ambiguity Packing and Stochastic
Disambiguation Methods for Lexical-Functional Grammar
Stefan Riezler and Tracy H. King and Richard Crouch and Annie Zaenen
Palo Alto Research Center, 3333 Coyote Hill Rd., Palo Alto, CA 94304
{riezler|thking|crouch|zaenen}@parc.com
Abstract
We present an application of ambiguity pack-
ing and stochastic disambiguation techniques
for Lexical-Functional Grammars (LFG) to the
domain of sentence condensation. Our system
incorporates a linguistic parser/generator for
LFG, a transfer component for parse reduc-
tion operating on packed parse forests, and a
maximum-entropy model for stochastic output
selection. Furthermore, we propose the use of
standard parser evaluation methods for auto-
matically evaluating the summarization qual-
ity of sentence condensation systems. An ex-
perimental evaluation of summarization qual-
ity shows a close correlation between the au-
tomatic parse-based evaluation and a manual
evaluation of generated strings. Overall sum-
marization quality of the proposed system is
state-of-the-art, with guaranteed grammatical-
ity of the system output due to the use of a
constraint-based parser/generator.
1 Introduction
Recent work in statistical text summarization has put for-
ward systems that do not merely extract and concate-
nate sentences, but learn how to generate new sentences
from ?Summary, Text? tuples. Depending on the cho-
sen task, such systems either generate single-sentence
?headlines? for multi-sentence text (Witbrock and Mittal,
1999), or they provide a sentence condensation module
designed for combination with sentence extraction sys-
tems (Knight and Marcu, 2000; Jing, 2000). The chal-
lenge for such systems is to guarantee the grammatical-
ity and summarization quality of the system output, i.e.
the generated sentences need to be syntactically well-
formed and need to retain the most salient information of
the original document. For example a sentence extraction
system might choose a sentence like:
The UNIX operating system, with implementations
from Apples to Crays, appears to have the advan-
tage.
from a document, which could be condensed as:
UNIX appears to have the advantage.
In the approach of Witbrock and Mittal (1999), selec-
tion and ordering of summary terms is based on bag-
of-words models and n-grams. Such models may well
produce summaries that are indicative of the original?s
content; however, n-gram models seem to be insufficient
to guarantee grammatical well-formedness of the system
output. To overcome this problem, linguistic parsing and
generation systems are used in the sentence condensation
approaches of Knight and Marcu (2000) and Jing (2000).
In these approaches, decisions about which material to in-
clude/delete in the sentence summaries do not rely on rel-
ative frequency information on words, but rather on prob-
ability models of subtree deletions that are learned from
a corpus of parses for sentences and their summaries.
A related area where linguistic parsing systems
have been applied successfully is sentence simplifica-
tion. Grefenstette (1998) presented a sentence reduction
method that is based on finite-state technology for lin-
guistic markup and selection, and Carroll et al (1998)
present a sentence simplification system based on linguis-
tic parsing. However, these approaches do not employ
statistical learning techniques to disambiguate simplifi-
cation decisions, but iteratively apply symbolic reduction
rules, producing a single output for each sentence.
The goal of our approach is to apply the fine-grained
tools for stochastic Lexical-Functional Grammar (LFG)
parsing to the task of sentence condensation. The system
presented in this paper is conceptualized as a tool that can
be used as a standalone system for sentence condensation
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 118-125
                                                         Proceedings of HLT-NAACL 2003
or simplification, or in combination with sentence extrac-
tion for text-summarization beyond the sentence-level. In
our system, to produce a condensed version of a sen-
tence, the sentence is first parsed using a broad-coverage
LFG grammar for English. The parser produces a set of
functional (f )-structures for an ambiguous sentence in a
packed format. It presents these to the transfer compo-
nent in a single packed data structure that represents in
one place the substructures shared by several different in-
terpretations. The transfer component operates on these
packed representations and modifies the parser output to
produce reduced f -structures. The reduced f -structures
are then filtered by the generator to determine syntac-
tic well-formedness. A stochastic disambiguator using a
maximum entropy model is trained on parsed and manu-
ally disambiguated f -structures for pairs of sentences and
their condensations. Using the disambiguator, the string
generated from the most probable reduced f -structure
produced by the transfer system is chosen. In contrast
to the approaches mentioned above, our system guaran-
tees the grammaticality of generated strings through the
use of a constraint-based generator for LFG which uses
a slightly tighter version of the grammar than is used by
the parser. As shown in an experimental evaluation, sum-
marization quality of our system is high, due to the com-
bination of linguistically fine-grained analysis tools and
expressive stochastic disambiguation models.
A second goal of our approach is to apply the standard
evaluation methods for parsing to an automatic evaluation
of summarization quality for sentence condensation sys-
tems. Instead of deploying costly and non-reusable hu-
man evaluation, or using automatic evaluation methods
based on word error rate or n-gram match, summariza-
tion quality can be evaluated directly and automatically
by matching the reduced f -structures that were produced
by the system against manually selected f -structures that
were produced by parsing a set of manually created con-
densations. Such an evaluation only requires human labor
for the construction and manual structural disambigua-
tion of a reusable gold standard test set. Matching against
the test set can be done automatically and rapidly, and
is repeatable for development purposes and system com-
parison. As shown in an experimental evaluation, a close
correspondence can be established for rankings produced
by the f -structure based automatic evaluation and a man-
ual evaluation of generated strings.
2 Statistical Sentence Condensation in the
LFG Framework
In this section, each of the system components will be
described in more detail.
2.1 Parsing and Transfer
In this project, a broad-coverage LFG gram-
mar and parser for English was employed (see
Riezler et al (2002)). The parser produces a set of
context-free constituent (c-)structures and associated
functional (f -)structures for each input sentence, repre-
sented in packed form (see Maxwell and Kaplan (1989)).
For sentence condensation we are only interested in the
predicate-argument structures encoded in f -structures.
For example, Fig. 1 shows an f -structure manually
selected out of the 40 f -structures for the sentence:
A prototype is ready for testing, and Leary hopes to
set requirements for a full system by the end of the
year.
The transfer component for the sentence condensation
system is based on a component previously used in a ma-
chine translation system (see Frank (1999)). It consists
of an ordered set of rules that rewrite one f -structure
into another. Structures are broken down into flat lists
of facts, and rules may add, delete, or change individ-
ual facts. Rules may be optional or obligatory. In the case
of optional rules, transfer of a single input structure may
lead to multiple alternate output structures. The transfer
component is designed to operate on packed input from
the parser and can also produce packed representations
of the condensation alternatives, using methods adapted
from parse packing.1
An example rule that (optionally) removes an adjunct
is shown below:
+adjunct(X,Y), in-set(Z,Y) ?=>
delete-node(Z,r1), rule-trace(r1,del(Z,X)).
This rule eliminates an adjunct, Z, by deleting the fact that
Z is contained within the set of adjuncts, Y, associated
with the expression X. The + before the adjunct(X,Y)
fact marks this fact as one that needs to be present for the
rule to be applied, but which is left unaltered by the rule
application. The in-set(Z,Y) fact is deleted. Two
new facts are added. delete-node(Z,r1) indicates
that the structure rooted at node Z is to be deleted, and
rule-trace(r1,del(Z,X)) adds a trace of this
rule to an accumulating history of rule applications. This
history records the relation of transferred f -structures to
the original f -structure and is available for stochastic dis-
ambiguation.
Rules used in the sentence condensation transfer sys-
tem include the optional deletion of all intersective ad-
juncts (e.g., He slept in the bed. can become He slept.,
but He did not sleep. cannot become He did sleep. or He
1The packing feature of the transfer component could not
be employed in these experiments since the current interface
to the generator and stochastic disambiguation component still
requires unpacked representations.
"A prototype is ready for testing , and Leary hopes to set requirements for a full system by the end of the year."
?be<[93:ready]>[30:prototype]?PRED
?prototype?PRED
countGRAINNTYPE
?a?PREDDET?FORM a, DET?TYPE indefDETSPEC
CASE nom, NUM sg, PERS 330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJADEGREE positive, ATYPE predicative93XCOMP
?for<[141:test]>?PRED
?test?PRED
gerundGRAINNTYPE
CASE acc, NUM sg, PERS 3, PFORM for, VTYPE main141
OBJ
ADV?TYPE vpadv, PSEM unspecified, PTYPE sem125
ADJUNCT
MOOD indicative, PERF ?_, PROG ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular[252:hope]>s73
?hope<[235:Leary], [280:set]>?PRED
?Leary?PRED
properGRAIN
namePROPERNSEMNTYPE
ANIM +, CASE nom, NUM sg, PERS 3235
SUBJ
?set<[235:Leary], [336:requirement], [355:for]>?PRED [235:Leary]SUBJ
?requirement?PRED
unspecifiedGRAINNTYPE
CASE acc, NUM pl, PERS 3336
OBJ
?for<[391:system]>?PRED
?system?PRED
?full?PREDADEGREE positive, ADJUNCT?TYPE nominal, ATYPE attributive398ADJUNCT
unspecifiedGRAINNTYPE
?a?PREDDET?FORM a, DET?TYPE indefDETSPEC
CASE acc, NUM sg, PERS 3, PFORM for391
OBJ
PSEM unspecified, PTYPE sem355
OBL
?by<[469:end]>?PRED
?end?PRED
?of<[519:year]>?PRED
?year?PRED
countGRAINNTYPE
?the?PREDDET?FORM the, DET?TYPE defDETSPEC
CASE acc, NUM sg, PERS 3, PFORM of519
OBJ
ADJUNCT?TYPE nominal, PSEM unspecified, PTYPE sem512
ADJUNCT
countGRAINNTYPE
?the?PREDDET?FORM the, DET?TYPE defDETSPEC
CASE acc, NUM sg, PERS 3, PFORM by469
OBJ
ADV?TYPE vpadv, PSEM unspecified, PTYPE sem451
ADJUNCT
PERF ?_, PROG ?_TNS?ASP
INF?FORM to, PASSIVE ?, VTYPE main280
XCOMP
MOOD indicative, PERF ?_, PROG ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE main252
COORD +_, COORD?FORM and, COORD?LEVEL ROOT197
Figure 1: F -structure for non-condensed sentence.
slept.), the optional deletion of parts of coordinate struc-
tures (e.g., They laughed and giggled. can become They
giggled.), and certain simplifications (e.g. It is clear that
the earth is round. can become The earth is round. but
It seems that he is asleep. cannot become He is asleep.).
For example, one possible post-transfer output of the sen-
tence in Fig. 1 is shown in Fig. 2.
2.2 Stochastic Selection and Generation
The transfer rules are independent of the grammar and are
not constrained to preserve the grammaticality or well-
formedness of the reduced f-structures. Some of the re-
duced structures therefore may not correspond to any En-
glish sentence, and these are eliminated from future con-
sideration by using the generator as a filter. The filter-
ing is done by running each transferred structure through
the generator to see whether it produces an output string.
If it does not, the structure is rejected. For example, for
the f -structure in Fig. 1, the transfer system proposed
32 possible reductions. After filtering these structures by
generation, 16 reduced f -structures comprising possible
"A prototype is ready for testing."
?be  <[93:ready]>[30:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PREDDET?FORM  a, DET?TYPE  indefDETSPEC
CASE nom, NUM sg, PERS 330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJADEGREE  positive , ATYPE  predicative93XCOMP
?for<[141:test]>?PRED
?test ?PRED
gerundGRAINNTYPE
CASE acc, NUM sg, PERS 3, PFORM for, VTYPE main141
OBJ
ADV?TYPE  vpadv , PSEM  unspecified , PTYPE  sem125
ADJUNCT
MOOD	  indicative, PERF  ?_, PROG  ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular73
Figure 2: Gold standard f -structure reduction.
condensations of the input sentence survive. The 16 well-
formed structures correspond to the following strings that
were outputted by the generator (note that a single struc-
ture may correspond to more than one string and a given
string may correspond to more than one structure):
A prototype is ready.
A prototype is ready for testing.
Leary hopes to set requirements for a full system.
A prototype is ready and Leary hopes to set require-
ments for a full system.
A prototype is ready for testing and Leary hopes to
set requirements for a full system.
Leary hopes to set requirements for a full system by
the end of the year.
A prototype is ready and Leary hopes to set require-
ments for a full system by the end of the year.
A prototype is ready for testing and Leary hopes to
set requirements for a full system by the end of the
year.
In order to guarantee non-empty output for the over-
all condensation system, the generation component has
to be fault-tolerant in cases where the transfer system op-
erates on a fragmentary parse, or produces non-valid f -
structures from valid input f -structures. Robustness tech-
niques currently applied to the generator include insertion
and deletion of features in order to match invalid transfer-
output to the grammar rules and lexicon. Furthermore,
repair mechanisms such as repairing subject-verb agree-
ment from the subject?s number value are employed. As
a last resort, a fall-back mechanism to the original un-
condensed f -structure is used. These techniques guaran-
tee that a non-empty set of reduced f -structures yielding
grammatical strings in generation is passed on to the next
system component. In case of fragmentary input to the
transfer component, grammaticaliy of the output is guar-
anteed for the separate fragments. In other words, strings
generated from a reduced fragmentary f -structure will be
as grammatical as the string that was fed into the parsing
component.
After filtering by the generator, the remaining f -
structures were weighted by the stochastic disambigua-
tion component. Similar to stochastic disambiguation for
constraint-based parsing (Johnson et al, 1999; Riezler et
al., 2002), an exponential (a.k.a. log-linear or maximum-
entropy) probability model on transferred structures is es-
timated from a set of training data. The data for estima-
tion consists of pairs of original sentences y and gold-
standard summarized f -structures s which were manu-
ally selected from the transfer output for each sentence.
For training data {(sj , yj)}mj=1 and a set of possible sum-
marized structures S(y) for each sentence y, the objective
was to maximize a discriminative criterion, namely the
conditional likelihood L(?) of a summarized f -structure
given the sentence. Optimization of the function shown
below was performed using a conjugate gradient opti-
mization routine:
L(?) = log
m?
j=1
e??f(sj)
?
s?S(yj)
e??f(s)
.
At the core of the exponential probability model is a vec-
tor of property-functions f to be weighted by parameters
?. For the application of sentence condensation, 13,000
property-functions of roughly three categories were used:
? Property-functions indicating attributes, attribute-
combinations, or attribute-value pairs for f -structure
attributes (? 1,000 properties)
? Property-functions indicating co-occurences of verb
stems and subcategorization frames (? 12,000 prop-
erties)
? Property-functions indicating transfer rules used to
arrive at the reduced f - structures (? 60 properties).
A trained probability model is applied to unseen data
by selecting the most probable transferred f -structure,
yielding the string generated from the selected struc-
ture as the target condensation. The transfered f -structure
chosen for our current example is shown in Fig. 3.
"A prototype is ready."
?be  <[93:ready]>[30:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PRED
DET?FORM a, DET?TYPE indefDETSPEC
CASE nom, NUM  sg, PERS  330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJ
ADEGREE positive , ATYPE predicative93XCOMP

MOOD indicative, PERF ?_, PROG ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular73
Figure 3: Transferred f -structure chosen by system.
This structure was produced by the following set of
transfer rules, where var refers to the indices in the rep-
resentation of the f -structure:
rtrace(r13,keep(var(98),of)),
rtrace(r161,keep(system,var(85))),
rtrace(r1,del(var(91),set,by)),
rtrace(r1,del(var(53),be,for)),
rtrace(r20,equal(var(1),and)),
rtrace(r20,equal(var(2),and)),
rtrace(r2,del(var(1),hope,and)),
rtrace(r22,delb(var(0),and)).
These rules delete the adjunct of the first conjunct (for
testing), the adjunct of the second conjunct (by the end
of the year), the rest of the second conjunct (Leary hopes
to set requirements for a full system), and the conjunction
itself (and).
3 A Method for Automatic Evaluation of
Sentence Summarization
Evaluation of quality of sentence condensation systems,
and of text summarization and simplification systems in
general, has mostly been conducted as intrinsic evalua-
tion by human experts. Recently, Papineni et al?s (2001)
proposal for an automatic evaluation of translation sys-
tems by measuring n-gram matches of the system out-
put against reference examples has become popular for
evaluation of summarization systems. In addition, an au-
tomatic evaluation method based on context-free deletion
decisions has been proposed by Jing (2000). However, for
summarization systems that employ a linguistic parser as
an integral system component, it is possible to employ
the standard evaluation techniques for parsing directly
to an evaluation of summarization quality. A parsing-
based evaluation allows us to measure the semantic as-
pects of summarization quality in terms of grammatical-
functional information provided by deep parsers. Further-
more, human expertise was necessary only for the cre-
ation of condensed versions of sentences, and for the
manual disambiguation of parses assigned to those sen-
tences. Given such a gold standard, summarization qual-
ity of a system can be evaluated automatically and re-
peatedly by matching the structures of the system out-
put against the gold standard structures. The standard
metrics of precision, recall, and F-score from statisti-
cal parsing can be used as evaluation metrics for mea-
suring matching quality: Precision measures the number
of matching structural items in the parses of the sys-
tem output and the gold standard, out of all structural
items in the system output?s parse; recall measures the
number of matches, out of all items in the gold stan-
dard?s parse. F-score balances precision and recall as
(2 ? precision ? recall)/(precision + recall).
For the sentence condensation system presented above,
the structural items to be matched consist of rela-
tion(predicate, argument) triples. For example, the gold-
standard f -structure of Fig. 2 corresponds to 23 depen-
dency relations, the first 14 of which are shared with the
reduced f -structure chosen by the stochastic disambigua-
tion system:
tense(be:0, pres),
mood(be:0, indicative),
subj(be:0, prototype:2),
xcomp(be:0, ready:1),
stmt_type(be:0, declarative),
vtype(be:0, copular),
subj(ready:1, prototype:2),
adegree(ready:1, positive),
atype(ready:1, predicative),
det(prototype:2, a:7),
num(prototype:2, sg),
pers(prototype:2, 3),
det_form(a:7, a),
det_type(a:7, indef),
adjunct(be:0, for:12),
obj(for:12, test:14),
adv_type(for:12, vpadv),
psem(for:12, unspecified),
ptype(for:12, semantic),
num(test:14, sg),
pers(test:14, 3),
pform(test:14, for),
vtype(test:14, main).
Matching these f -structures against each other corre-
sponds to a precision of 1, recall of .61, and F-score of
.76.
The fact that our method does not rely on a compar-
ison of the characteristics of surface strings is a clear
advantage. Such comparisons are bad at handling exam-
ples which are similar in meaning but differ in word or-
der or vary structurally, such as in passivization or nom-
inalization. Our method handles such examples straight-
forwardly. Fig. 4 shows two serialization variants of the
condensed sentence of Fig. 2. The f -structures for these
examples are similar to the f -structure assigned to the
gold standard condensation shown in Fig. 2 (except for
the relations ADJUNT-TYPE:parenthetical ver-
sus ADV-TYPE:vpadv versus ADV-TYPE:sadv). An
evaluation of summarization quality that is based on
matching f -structures will treat these examples equally,
whereas an evaluation based on string matching will yield
different quality scores for different serializations.
"A prototype, for testing, is ready."
?be  <[221:ready]>[30:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PREDDET?FORM  a, DET?TYPE  indefDETSPEC
CASE nom, NUM sg, PERS 330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJADEGREE  positive , ATYPE  predicative221XCOMP
?for<[117:test]>?PRED
?test ?PRED
gerundGRAINNTYPECASE acc, NUM sg, PERS 3, PFORM for, VTYPE main117OBJADJUNCT?TYPE  parenthetical , PSEM  unspecified , PTYPE  sem73
ADJUNCT
MOOD  indicative, PERF  ?_, PROG  ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular201
"For testing, a prototype is ready."
?be  <[177:ready]>[131:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PREDDET?FORM  a, DET?TYPE  indefDETSPEC
CASE nom, NUM sg, PERS 3131
SUBJ
?ready<[131:prototype]>?PRED [131:prototype]SUBJADEGREE  positive , ATYPE  predicative177XCOMP
?for<[27:test]>?PRED
?test ?PRED
gerundGRAINNTYPECASE acc, NUM sg, PERS 3, PFORM for, VTYPE main27OBJADV?TYPE  sadv, PSEM  unspecified , PTYPE  sem11
ADJUNCT
MOOD  indicative, PERF  ?_, PROG  ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular83
Figure 4: F -structure for word-order variants of gold
standard condensation.
In the next section, we present experimental results
of an automatic evaluation of the sentence condensation
system described above. These results show a close cor-
respondence between automatically produced evaluation
results and human judgments on the quality of generated
condensed strings.
4 Experimental Evaluation
The sentences and condensations we used are taken from
data for the experiments of Knight and Marcu (2000),
which were provided to us by Daniel Marcu. These data
consist of pairs of sentences and their condensed versions
that have been extracted from computer-news articles and
abstracts of the Ziff-Davis corpus. Out of these data, we
parsed and manually disambiguated 500 sentence pairs.
These included a set of 32 sentence pairs that were used
for testing purposes in Knight and Marcu (2000). In or-
der to control for the small corpus size of this test set, we
randomly extracted an additional 32 sentence pairs from
the 500 parsed and disambiguated examples as a second
test set. The rest of the 436 randomly selected sentence
pairs were used to create training data. For the purpose
of discriminative training, a gold-standard of transferred
f -structures was created from the transfer output and the
manually selected f -structures for the condensed strings.
This was done automatically by selecting for each exam-
ple the transferred f -structure that best matched the f -
structure annotated for the condensed string.
In the automatic evaluation of f -structure match, three
different system variants were compared. Firstly, ran-
domly chosen transferred f -structures were matched
against the manually selected f -structures for the man-
ually created condensations. This evaluation constitutes
a lower bound on the F-score against the given gold
standard. Secondly, matching results for transferred f -
structures yielding the maximal F-score against the gold
standard were recorded, giving an upper bound for the
system. Thirdly, the performance of the stochastic model
within the range of the lower bound and upper bound was
measured by recording the F-score for the f -structure that
received highest probability according to the learned dis-
tribution on transferred structures.
In order to make our results comparable to the re-
sults of Knight and Marcu (2000) and also to investigate
the correspondence between the automatic evaluation and
human judgments, a manual evaluation of the strings gen-
erated by these system variants was conducted. Two hu-
man judges were presented with the uncondensed sur-
face string and five condensed strings that were displayed
in random order for each test example. The five con-
densed strings presented to the human judges contained
(1) strings generated from three randomly selected f -
structures, (2) the strings generated from the f -structures
which were selected by the stochastic model, and (3) the
manually created gold-standard condensations extracted
from the Ziff-Davis abstracts. The judges were asked
to judge summarization quality on a scale of increasing
quality from 1 to 5 by assessing how well the generated
strings retained the most salient information of the orig-
inal uncondensed sentences. Grammaticality of the sys-
tem output is optimal and not reported separately. Results
for both evaluations are reported for two test corpora of
32 examples each. Testset I contains the sentences and
condensations used to evaluate the system described in
Knight and Marcu (2000). Testset II consists of another
randomly extracted 32 sentence pairs from the same do-
main, prepared in the same way.
Fig. 5 shows evaluation results for a sentence conden-
sation run that uses manually selected f -structures for
the original sentences as input to the transfer component.
These results demonstrate how the condenstation system
performs under the optimal circumstances when the parse
chosen as input is the best available. Fig. 6 applies the
same evaluation data and metrics to a sentence conden-
sation experiment that performs transfer from packed f -
structures, i.e. transfer is performed on all parses for an
ambiguous sentence instead of on a single manually se-
lected parse. Alternatively, a single input parse could be
selected by stochastic models such as the one described
in Riezler et al (2002). A separate phase of parse disam-
biguation, and perhaps the effects of any errors that this
might introduce, can be avoided by transferring from all
parses for an ambiguous sentence. This approach is com-
putationally feasible, however, only if condensation can
be carried all the way through without unpacking. Our
technology is not yet able to do this (in particular, as men-
tioned earlier, we have not yet implemented a method for
stochastic disambiguation on packed f -structures). How-
ever, we conducted a preliminary assessment of this pos-
sibility by unpacking and enumerating the transferred f -
structures. For many sentences this resulted in more can-
didates than we could operate on in the available time
and space, and in those cases we arbitrarily set a cut-off
on the number of transferred f -structures we considered.
Since transferred f -structures are produced according to
the number of rules applied to transfer them, in this setup
the transfer system produces smaller f -structures first,
and cuts off less condensed output. The result of this ex-
periment, shown in Fig. 6, thus provides a conservative
estimate on the quality of the condensations we might
achieve with a full-packing implementation.
In Figs. 5 and 6, the first row shows F-scores for a
random selection, the system selection, and the best pos-
sible selection from the transfer output against the gold
standard. The second rows show summarization quality
scores for generations from a random selection and the
system selection, and for the human-written condensa-
tion. The third rows report compression ratios. As can
testset I lowerbound
system
selection
upper
bound
F-score 58% 67.3% 77.2 %
sum-quality 2.0 3.5 4.4
compr. 50.2% 60.4% 54.9%
testset II lowerbound
system
selection
upper
bound
F-score 59% 65.4% 83.3%
sum-quality 2.1 3.4 4.6
compr. 52.7% 65.9% 56.8%
Figure 5: Sentence condensation from manually selected
f -structure for original uncondensed sentences.
be seen from these tables, the ranking of system variants
produced by the automatic and manual evaluation con-
firm a close correlation between the automatic evaluation
and human judgments. A comparison of evaluation re-
sults across colums, i.e. across selection variants, shows
that a stochastic selection of transferred f -structures is
indeed important. Even if all f -structures are transferred
from the same linguistically rich source, and all gener-
ated strings are grammatical, a reduction in error rate of
around 50% relative to the upper bound can be achieved
by stochastic selection. In contrast, a comparison be-
tween transfer runs with and without perfect disambigua-
tion of the original string shows a decrease of about 5% in
F-score, and of only .1 points for summarization quality
when transferring from packed parses instead of from the
manually selected parse. This shows that it is more im-
portant to learn what a good transferred f -structure looks
like than to have a perfect f -structure to transfer from.
The compression rates associated with the systems that
used stochastic selection is around 60%, which is accept-
able, but not as aggressive as human-written condensa-
tions. Note that in our current implementation, in some
cases the transfer component was unable to operate on
the packed representation. In those cases a parse was cho-
sen at random as a conservative estimate of transfer from
all parses. This fall-back mechanism explains the drop in
F-score for the upper bound in comparing Figs. 5 and 6.
5 Conclusion
We presented an approach to sentence condensation
that employs linguistically rich LFG grammars in a
parsing/generation-based stochastic sentence condensa-
tion system. Fine-grained dependency structures are out-
put by the parser, then modified by a highly expressive
transfer system, and filtered by a constraint-based gener-
ator. Stochastic selection of generation-filtered reduced
structures uses a powerful Maximum-Entropy model.
As shown in an experimental evaluation, summarization
testset I lowerbound
system
selection
upper
bound
F-score 55.2% 63.0% 72.0%
sum-quality 2.1 3.4 4.4
compres. 46.5% 61.6% 54.9%
testset II lowerbound
system
selection
upper
bound
F-score 54% 59.7% 76.0 %
sum-quality 1.9 3.3 4.6
compres. 50.9% 60.0% 56.8%
Figure 6: Sentence condensation from packed f -
structures for original uncondensed sentences.
quality of the system output is state-of-the-art, and gram-
maticality of condensed strings is guaranteed. Robustness
techniques for parsing and generation guarantee that the
system produces non-empty output for unseen input.
Overall, the summarization quality achieved by
our system is similar to the results reported in
Knight and Marcu (2000). This might seem disappoint-
ing considering the more complex machinery employed
in our approach. It has to be noted that these re-
sults are partially due to the somewhat artificial na-
ture of the data that were used in the experiments of
Knight and Marcu (2000) and therefore in our experi-
ments: The human-written condensations in the data set
extracted from the Ziff-Davis corpus show the same
word order as the original sentences and do not exhibit
any structural modification that are common in human-
written summaries. For example, humans tend to make
use of structural modifications such as nominalization
and verb alternations such as active/passive or transi-
tive/intransitive alternations in condensation. Such alter-
nations can easily be expressed in our transfer-based
approach, whereas they impose severe problems to ap-
proaches that operate only on phrase structure trees. In
the given test set, however, the condensation task re-
stricted to the operation of deletion. A creation of addi-
tional condensations for the original sentences other than
the condensed versions extracted from the human-written
abstracts would provide a more diverse test set, and fur-
thermore make it possible to match each system output
against any number of independent human-written con-
densations of the same original sentence. This idea of
computing matching scores to multiple reference exam-
ples was proposed by Alshawi et al (1998), and later by
Papineni et al (2001) for evaluation of machine transla-
tion systems. Similar to these proposals, an evaluation
of condensation quality could consider multiple reference
condensations and record the matching score against the
most similar example.
Another desideratum for future work is to carry
condensation all the way through without unpacking
at any stage. Work on employing packing techniques
not only for parsing and transfer, but also for genera-
tion and stochastic selection is currently underway (see
Geman and Johnson (2002)). This will eventually lead to
a system whose components work on packed represen-
tations of all or n-best solutions, but completely avoid
costly unpacking of representations.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
1998. Automatic acquisition of hierarchical trans-
duction models for machine translation. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics (ACL?98), Montreal, Que-
bec, Canada.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
Proceedings of the AAAI Workshop on Integrating Arti-
ficial Intelligence and Assistive Technology, Madison,
WI.
Anette Frank. 1999. From parallel grammar develop-
ment towards machine translation. In Proceedings of
the MT Summit VII. MT in the Great Translation Era,
pages 134?142. Kent Ridge Digital Labs, Singapore.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochas-
tic unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?02), Philadelphia, PA.
Gregory Grefenstette. 1998. Producing intelligent tele-
graphic text reduction to provide an audio scanning
service for the blind. In Proceedings of the AAAI
Spring Workshop on Intelligent Text Summarization,
Stanford, CA.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP?00),
Seattle, WA.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?99), College Park, MD.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization?step one: Sentence compression. In
Proceedings of the 17th National Conference on Arti-
ficial Intelligence (AAAI-2000), Austin, TX.
John Maxwell and Ronald M. Kaplan. 1989. An
overview of disjunctive constraint satisfaction. In Pro-
ceedings of the International Workshop on Parsing
Technologies, Pittsburgh, PA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. Technical Report IBM Re-
search Division Technical Report, RC22176 (W0190-
022), Yorktown Heights, N.Y.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization: A statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
Berkeley, CA.
Speed and Accuracy in Shallow and Deep Stochastic Parsing
Ronald M. Kaplan , Stefan Riezler , Tracy Holloway King
John T. Maxwell III, Alexander Vasserman and Richard Crouch
Palo Alto Research Center, 3333 Coyote Hill Rd., Palo Alto, CA 94304
{kaplan|riezler|king|maxwell|avasserm|crouch}@parc.com
Abstract
This paper reports some experiments that com-
pare the accuracy and performance of two
stochastic parsing systems. The currently pop-
ular Collins parser is a shallow parser whose
output contains more detailed semantically-
relevant information than other such parsers.
The XLE parser is a deep-parsing system that
couples a Lexical Functional Grammar to a log-
linear disambiguation component and provides
much richer representations theory. We mea-
sured the accuracy of both systems against a
gold standard of the PARC 700 dependency
bank, and also measured their processing times.
We found the deep-parsing system to be more
accurate than the Collins parser with only a
slight reduction in parsing speed.1
1 Introduction
In applications that are sensitive to the meanings ex-
pressed by natural language sentences, it has become
common in recent years simply to incorporate publicly
available statistical parsers. A state-of-the-art statistical
parsing system that enjoys great popularity in research
systems is the parser described in Collins (1999) (hence-
forth ?the Collins parser?). This system not only is fre-
quently used for off-line data preprocessing, but also
is included as a black-box component for applications
such as document summarization (Daume and Marcu,
2002), information extraction (Miller et al, 2000), ma-
chine translation (Yamada and Knight, 2001), and ques-
tion answering (Harabagiu et al, 2001). This is be-
1This research has been funded in part by contract #
MDA904-03-C-0404 awarded from the Advanced Research and
Development Activity, Novel Intelligence from Massive Data
program. We would like to thank Chris Culy whose original ex-
periments inspired this research.
cause the Collins parser shares the property of robustness
with other statistical parsers, but more than other such
parsers, the categories of its parse-trees make grammati-
cal distinctions that presumably are useful for meaning-
sensitive applications. For example, the categories of
the Model 3 Collins parser distinguish between heads,
arguments, and adjuncts and they mark some long-
distance dependency paths; these distinctions can guide
application-specific postprocessors in extracting impor-
tant semantic relations.
In contrast, state-of-the-art parsing systems based on
deep grammars mark explicitly and in much more de-
tail a wider variety of syntactic and semantic dependen-
cies and should therefore provide even better support for
meaning-sensitive applications. But common wisdom has
it that parsing systems based on deep linguistic grammars
are too difficult to produce, lack coverage and robustness,
and also have poor run-time performance. The Collins
parser is thought to be accurate and fast and thus to repre-
sent a reasonable trade-off between ?good-enough? out-
put, speed, and robustness.
This paper reports on some experiments that put this
conventional wisdom to an empirical test. We investi-
gated the accuracy of recovering semantically-relevant
grammatical dependencies from the tree-structures pro-
duced by the Collins parser, comparing these dependen-
cies to gold-standard dependencies which are available
for a subset of 700 sentences randomly drawn from sec-
tion 23 of the Wall Street Journal (see King et al (2003)).
We compared the output of the XLE system, a
deep-grammar-based parsing system using the English
Lexical-Functional Grammar previously constructed as
part of the Pargram project (Butt et al, 2002), to the
same gold standard. This system incorporates sophisti-
cated ambiguity-management technology so that all pos-
sible syntactic analyses of a sentence are computed in
an efficient, packed representation (Maxwell and Ka-
plan, 1993). In accordance with LFG theory, the output
includes not only standard context-free phrase-structure
trees but also attribute-value matrices (LFG?s f(unctional)
structures) that explicitly encode predicate-argument re-
lations and other meaningful properties. XLE selects the
most probable analysis from the potentially large candi-
date set by means of a stochastic disambiguation com-
ponent based on a log-linear (a.k.a. maximum-entropy)
probability model (Riezler et al, 2002). The stochas-
tic component is also ?ambiguity-enabled? in the sense
that the computations for statistical estimation and selec-
tion of the most probable analyses are done efficiently
by dynamic programming, avoiding the need to unpack
the parse forests and enumerate individual analyses. The
underlying parsing system also has built-in robustness
mechanisms that allow it to parse strings that are outside
the scope of the grammar as a shortest sequence of well-
formed ?fragments?. Furthermore, performance parame-
ters that bound parsing and disambiguation work can be
tuned for efficient but accurate operation.
As part of our assessment, we also measured the pars-
ing speed of the two systems, taking into account all
stages of processing that each system requires to produce
its output. For example, since the Collins parser depends
on a prior part-of-speech tagger (Ratnaparkhi, 1996), we
included the time for POS tagging in our Collins mea-
surements. XLE incorporates a sophisticated finite-state
morphology and dictionary lookup component, and its
time is part of the measure of XLE performance.
Performance parameters of both the Collins parser and
the XLE system were adjusted on a heldout set consist-
ing of a random selection of 1/5 of the PARC 700 depen-
dency bank; experimental results were then based on the
other 560 sentences. For Model 3 of the Collins parser, a
beam size of 1000, and not the recommended beam size
of 10000, was found to optimize parsing speed at little
loss in accuracy. On the same heldout set, parameters of
the stochastic disambiguation system and parameters for
parsing performance were adjusted for a Core and a Com-
plete version of the XLE system, differing in the size of
the constraint-set of the underlying grammar.
For both XLE and the Collins parser we wrote con-
version programs to transform the normal (tree or f-
structure) output into the corresponding relations of
the dependency bank. This conversion was relatively
straightforward for LFG structures (King et al, 2003).
However, a certain amount of skill and intuition was
required to provide a fair conversion of the Collins
trees: we did not want to penalize configurations in the
Collins trees that encoded alternative but equally legit-
imate representations of the same linguistic properties
(e.g. whether auxiliaries are encoded as main verbs or
aspect features), but we also did not want to build into
the conversion program transformations that compensate
for information that Collins cannot provide without ap-
pealing to additional linguistic resources (such as identi-
fying the subjects of infinitival complements). We did not
include the time for dependency conversion in our mea-
sures of performance.
The experimental results show that stochastic parsing
with the Core LFG grammar achieves a better F-score
than the Collins parser at a roughly comparable parsing
speed. The XLE system achieves 12% reduction in error
rate over the Collins parser, that is 77.6% F-score for the
XLE system versus 74.6% for the Collins parser, at a cost
in parsing time of a factor of 1.49.
2 Stochastic Parsing with LFG
2.1 Parsing with Lexical-Functional Grammar
The grammar used for this experiment was developed in
the ParGram project (Butt et al, 2002). It uses LFG as a
formalism, producing c(onstituent)-structures (trees) and
f(unctional)-structures (attribute value matrices) as out-
put. The c-structures encode constituency and linear or-
der. F-structures encode predicate-argument relations and
other grammatical information, e.g., number, tense, state-
ment type. The XLE parser was used to produce packed
representations, specifying all possible grammar analyses
of the input.
In our system, tokenization and morphological analy-
sis are performed by finite-state transductions arranged in
a compositional cascade. Both the tokenizer and the mor-
phological analyzer can produce multiple outputs. For ex-
ample, the tokenizer will optionaly lowercase sentence
initial words, and the morphological analyzer will pro-
duce walk +Verb +Pres +3sg and walk +Noun +Pl for
the input form walks. The resulting tokenized and mor-
phologically analyzed strings are presented to the sym-
bolic LFG grammar.
The grammar can parse input that has XML de-
limited named entity markup: <company>Columbia
Savings</company> is a major holder of so-called junk
bonds. To allow the grammar to parse this markup,
the tokenizer includes an additional tokenization of the
strings whereby the material between the XML markup
is treated as a single token with a special morphologi-
cal tag (+NamedEntity). As a fall back, the tokenization
that the string would have received without that markup
is also produced. The named entities have a single mul-
tiword predicate. This helps in parsing both because it
means that no internal structure has to be built for the
predicate and because predicates that would otherwise be
unrecognized by the grammar can be parsed (e.g., Cie.
Financiere de Paribas). As described in section 5, it was
also important to use named entity markup in these ex-
periments to more fairly match the analyses in the PARC
700 dependency bank.
To increase robustness, the standard grammar is aug-
mented with a FRAGMENT grammar. This allows sen-
tences to be parsed as well-formed chunks specified by
the grammar, in particular as Ss, NPs, PPs, and VPs, with
unparsable tokens possibly interspersed. These chunks
have both c-structures and f-structures corresponding to
them. The grammar has a fewest-chunk method for de-
termining the correct parse.
The grammar incorporates a version of Optimality
Theory that allows certain (sub)rules in the grammar to be
prefered or disprefered based on OT marks triggered by
the (sub)rule (Frank et al, 1998). The Complete version
of the grammar uses all of the (sub)rules in a multi-pass
system that depends on the ranking of the OT marks in
the rules. For example, topicalization is disprefered, but
the topicalization rule will be triggered if no other parse
can be built. A one-line rewrite of the Complete grammar
creates a Core version of the grammar that moves the ma-
jority of the OT marks into the NOGOOD space. This ef-
fectively removes the (sub)rules that they mark from the
grammar. So, for example, in the Core grammar there is
no topicalization rule, and sentences with topics will re-
ceive a FRAGMENT parse. This single-pass Core grammar
is smaller than the Complete grammar and hence is faster.
The XLE parser also allows the user to adjust per-
formance parameters bounding the amount of work that
is done in parsing for efficient but accurate operation.
XLE?s ambiguity management technology takes advan-
tage of the fact that relatively few f-structure constraints
apply to constituents that are far apart in the c-structure,
so that sentences are typically parsed in polynomial time
even though LFG parsing is known to be an NP-complete
problem. But the worst-case exponential behavior does
begin to appear for some constructions in some sentences,
and the computational effort is limited by a SKIMMING
mode whose onset is controlled by a user-specified pa-
rameter. When skimming, XLE will stop processing the
subtree of a constituent whenever the amount of work ex-
ceeds that user-specified limit. The subtree is discarded,
and the parser will move on to another subtree. This guar-
antees that parsing will be finished within reasonable lim-
its of time and memory but at a cost of possibly lower
accuracy if it causes the best analysis of a constituent
to be discarded. As a separate parameter, XLE also lets
the user limit the length of medial constituents, i.e., con-
stituents that do not appear at the beginning or the end
of a sentence (ignoring punctuation). The rationale be-
hind this heuristic is to limit the weight of constituents in
the middle of the sentence but still to allow sentence-final
heavy constituents. This discards constituents in a some-
what more principled way as it tries to capture the psy-
cholinguistic tendency to avoid deep center-embedding.
When limiting the length of medial constituents, cubic-
time parsing is possible for sentences up to that length,
even with a deep, non-context-free grammar, and linear
parsing time is possible for sentences beyond that length.
The Complete grammar achieved 100% coverage of
section 23 as unseen unlabeled data: 79% as full parses,
21% FRAGMENT and/or SKIMMED parses.
2.2 Dynamic Programming for Estimation and
Stochastic Disambiguation
The stochastic disambiguation model we employ defines
an exponential (a.k.a. log-linear or maximum-entropy)
probability model over the parses of the LFG grammar.
The advantage of this family of probability distributions
is that it allows the user to encode arbitrary properties
of the parse trees as feature-functions of the probability
model, without the feature-functions needing to be inde-
pendent and non-overlapping. The general form of con-
ditional exponential models is as follows:
p?(x|y) = Z?(y)
?1e??f(x)
where Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant over the set X(y) of parses for sentence y, ? is
a vector of log-parameters, f is a vector of feature-
values, and ? ? f(x) is a vector dot product denoting the
(log-)weight of parse x.
Dynamic-programming algorithms that allow the ef-
ficient estimation and searching of log-linear mod-
els from a packed parse representation without enu-
merating an exponential number of parses have
been recently presented by Miyao and Tsujii (2002)
and Geman and Johnson (2002). These algorithms can
be readily applied to the packed and/or-forests of
Maxwell and Kaplan (1993), provided that each conjunc-
tive node is annotated with feature-values of the log-
linear model. In the notation of Miyao and Tsujii (2002),
such a feature forest ? is defined as a tuple ?C,D, r, ?, ??
where C is a set of conjunctive nodes, D is a set of dis-
junctive nodes, r ? C is the root node, ? : D ? 2C is
a conjunctive daughter function, and ? : C ? 2D is a
disjunctive daughter function.
A dynamic-programming solution to the problem of
finding most probable parses is to compute the weight
?d of each disjunctive node as the maximum weight of
its conjunctive daugher nodes, i.e.,
?d = max
c??(d)
?c (1)
and to recursively define the weight ?c of a conjunctive
node as the product of the weights of all its descendant
disjunctive nodes and of its own weight:
?c =
?
d??(c)
?d e
??f(c) (2)
Keeping a trace of the maximally weighted choices in a
computaton of the weight ?r of the root conjunctive node
r allows us to efficiently recover the most probable parse
of a sentence from the packed representation of its parses.
The same formulae can be employed for an effi-
cient calculation of probabilistic expectations of feature-
functions for the statistical estimation of the parameters
?. Replacing the maximization in equation 1 by a sum-
mation defines the inside weight of disjunctive node. Cor-
respondingly, equation 2 denotes the inside weight of a
conjunctive node. The outside weight ?c of a conjunctive
node is defined as the outside weight of its disjunctive
mother node(s):
?c =
?
{d|c??(d)}
?d (3)
The outside weight of a disjunctive node is the sum of
the product of the outside weight(s) of its conjunctive
mother(s), the weight(s) of its mother(s), and the inside
weight(s) of its disjunctive sister(s):
?d =
?
{c|d??(c)}
{?c e
??f(c)
?
{d?|d???(c),d? 6=d}
?d?} (4)
From these formulae, the conditional expectation of a
feature-function fi can be computed from a chart with
root node r for a sentence y in the following way:
?
x?X(y)
e??f(x)fi(x)
Z?(y)
=
?
c?C
?c?cfi(c)
?r
(5)
Formula 5 is used in our system to compute expectations
for discriminative Bayesian estimation from partially la-
beled data using a first-order conjugate-gradient routine.
For a more detailed description of the optimization prob-
lem and the feature-functions we use for stochastic LFG
parsing see Riezler et al (2002). We also employed a
combined `1 regularization and feature selection tech-
nique described in Riezler and Vasserman (2004) that
considerably speeds up estimation and guarantees small
feature sets for stochastic disambiguation. In the experi-
ments reported in this paper, however, dynamic program-
ming is crucial for efficient stochastic disambiguation,
i.e. to efficiently find the most probable parse from a
packed parse forest that is annotated with feature-values.
There are two operations involved in stochastic disam-
biguation, namely calculating feature-values from a parse
forest and calculating node weights from a feature forest.
Clearly, the first one is more expensive, especially for
the extraction of values for non-local feature-functions
over large charts. To control the cost of this compu-
tation, our stochastic disambiguation system includes
a user-specified parameter for bounding the amount of
work that is done in calculating feature-values. When the
user-specified threshold for feature-value calculation is
reached, this computation is discontinued, and the dy-
namic programming calculation for most-probable-parse
search is computed from the current feature-value anno-
tation of the parse forest. Since feature-value computa-
tion proceeds incrementally over the feature forest, i.e.
for each node that is visited all feature-functions that ap-
ply to it are evaluated, a complete feature annotation can
be guaranteed for the part of the and/or-forest that is vis-
ited until discontinuation. As discussed below, these pa-
rameters were set on a held-out portion of the PARC700
which was also used to set the Collins parameters.
In the experiments reported in this paper, we used a
threshold on feature-extraction that allowed us to cut off
feature-extraction in 3% of the cases at no loss in accu-
racy. Overall, feature extraction and weight calculation
accounted for 5% of the computation time in combined
parsing and stochastic selection.
3 The Gold-Standard Dependency Bank
We used the PARC 700 Dependency Bank (DEPBANK)
as the gold standard in our experiments. The DEPBANK
consists of dependency annotations for 700 sentences that
were randomly extracted from section 23 of the UPenn
Wall Street Journal (WSJ) treebank. As described by
(King et al, 2003), the annotations were boot-strapped
by parsing the sentences with a LFG grammar and trans-
forming the resulting f-structures to a collection of depen-
dency triples in the DEPBANK format. To prepare a true
gold standard of dependencies, the tentative set of depen-
dencies produced by the robust parser was then corrected
and extended by human validators2. In this format each
triple specifies that a particular relation holds between a
head and either another head or a feature value, for ex-
ample, that the SUBJ relation holds between the heads
run and dog in the sentence The dog ran. Average sen-
tence length of sentences in DEPBANK is 19.8 words, and
the average number of dependencies per sentence is 65.4.
The corpus is freely available for research and evaluation,
as are documentation and tools for displaying and prun-
ing structures.3
In our experiments we used a Reduced version of the
DEPBANK, including just the minimum set of dependen-
cies necessary for reading out the central semantic rela-
tions and properties of a sentence. We tested against this
Reduced gold standard to establish accuracy on a lower
bound of the information that a meaning-sensitive appli-
cation would require. The Reduced version contained all
the argument and adjunct dependencies shown in Fig.
1, and a few selected semantically-relevant features, as
shown in Fig. 2. The features in Fig. 2 were chosen be-
2The resulting test set is thus unseen to the grammar and
stochastic disambiguation system used in our experiments. This
is indicated by the fact that the upperbound of F-score for the
best matching parses for the experiment grammar is in the range
of 85%, not 100%.
3http://www2.parc.com/istl/groups/nltt/fsbank/
Function Meaning
adjunct adjuncts
aquant adjectival quantifiers (many, etc.)
comp complement clauses (that, whether)
conj conjuncts in coordinate structures
focus int fronted element in interrogatives
mod noun-noun modifiers
number numbers modifying nouns
obj objects
obj theta secondary objects
obl oblique
obl ag demoted subject of a passive
obl compar comparative than/as clauses
poss possessives (John?s book)
pron int interrogative pronouns
pron rel relative pronouns
quant quantifiers (all, etc.)
subj subjects
topic rel fronted element in relative clauses
xcomp non-finite complements
verbal and small clauses
Figure 1: Grammatical functions in DEPBANK.
cause it was felt that they were fundamental to the mean-
ing of the sentences, and in fact they are required by the
semantic interpreter we have used in a knowledge-based
application (Crouch et al, 2002).
Feature Meaning
adegree degree of adjectives and adverbs
(positive, comparative, superlative)
coord form form of a coordinating
conjunction (e.g., and, or)
det form form of a determiner (e.g., the, a)
num number of nouns (sg, pl)
number type cardinals vs. ordinals
passive passive verb (e.g., It was eaten.)
perf perfective verb (e.g., have eaten)
precoord form either, neither
prog progressive verb (e.g., were eating)
pron form form of a pronoun (he, she, etc.)
prt form particle in a particle verb
(e.g., They threw it out.)
stmt type statement type (declarative,
interrogative, etc.)
subord form subordinating conjunction (e.g. that)
tense tense of the verb (past, present, etc.)
Figure 2: Selected features for Reduced DEPBANK
.
As a concrete example, the dependency list in Fig. 3 is
the Reduced set corresponding to the following sentence:
He reiterated his opposition to such funding,
but expressed hope of a compromise.
An additional feature of the DEPBANK that is relevant
to our comparisons is that dependency heads are rep-
resented by their standard citation forms (e.g. the verb
swam in a sentence appears as swim in its dependencies).
We believe that most applications will require a conver-
sion to canonical citation forms so that semantic relations
can be mapped into application-specific databases or on-
tologies. The predicates of LFG f-structures are already
represented as citation forms; for a fair comparison we
ran the leaves of the Collins tree through the same stem-
mer modules as part of the tree-to-dependency transla-
tion. We also note that proper names appear in the DEP-
BANK as single multi-word expressions without any in-
ternal structure. That is, there are no dependencies hold-
ing among the parts of people names (A. Boyd Simpson),
company names (Goldman, Sachs & Co), and organiza-
tion names (Federal Reserve). This multiword analysis
was chosen because many applications do not require
the internal structure of names, and the identification of
named entities is now typically carried out by a separate
non-syntactic pre-processing module. This was captured
for the LFG parser by using named entity markup and for
the Collins parser by creating complex word forms with
a single POS tag (section 5).
conj(coord?0, express?3)
conj(coord?0, reiterate?1)
coord form(coord?0, but)
stmt type(coord?0, declarative)
obj(reiterate?1, opposition?6)
subj(reiterate?1, pro?7)
tense(reiterate?1, past)
obj(express?3, hope?15)
subj(express?3, pro?7)
tense(express?3, past)
adjunct(opposition?6, to?11)
num(opposition?6, sg)
poss(opposition?6, pro?19)
num(pro?7, sg)
pron form(pro?7, he)
obj(to?11, funding?13)
adjunct(funding?13, such?45)
num(funding?13, sg)
adjunct(hope?15, of?46)
num(hope?15, sg)
num(pro?19, sg)
pron form(pro?19, he)
adegree(such?45, positive)
obj(of?46, compromise?54)
det form(compromise?54, a)
num(compromise?54, sg)
Figure 3: Reduced dependency relations for He reiterated
his opposition to such funding, but expressed hope of a
compromise.
4 Conversion to Dependency Bank Format
A conversion routine was required for each system to
transform its output so that it could be compared to the
DEPBANK dependencies. While it is relatively straightfor-
ward to convert LFG f-structures to the dependency bank
format because the f-structure is effectively a dependency
format, it is more difficult to transform the output trees of
the Model 3 Collins parser in a way that fairly allocates
both credits and penalties.
LFG Conversion We discarded the LFG tree structures
and used a general rewriting system previously developed
for machine translation to rewrite the relevant f-structure
attributes as dependencies (see King et al (2003)). The
rewritings involved some deletions of irrelevant features,
some systematic manipulations of the analyses, and some
trivial respellings. The deletions involved features pro-
duced by the grammar but not included in the PARC 700
such as negative values of PASS, PERF, and PROG and
the feature MEASURE used to mark measure phrases. The
manipulations are more interesting and are necessary to
map systematic differences between the analyses in the
grammar and those in the dependency bank. For example,
coordination is treated as a set by the LFG grammar but as
a single COORD dependency with several CONJ relations
in the dependency bank. Finally, the trivial rewritings
were used to, for example, change STMT-TYPE decl in
the grammar to STMT-TYPE declarative in the de-
pendency bank. For the Reduced version of the PARC
700 substantially more features were deleted.
Collins Model 3 Conversion An abbreviated represen-
tation of the Collins tree for the example above is shown
in Fig. 4. In this display we have eliminated the head lex-
ical items that appear redundantly at all the nonterminals
in a head chain, instead indicating by a single number
which daughter is the head. Thus, S?2 indicates that the
head of the main clause is its second daughter, the VP,
and its head is its first VP daughter. Indirectly, then, the
lexical head of the S is the first verb reiterated.
(TOP?1
(S?2 (NP-A?1 (NPB?1 He/PRP))
(VP?1 (VP?1 reiterated/VBD
(NP-A?1 (NPB?2 his/PRP$
opposition/NN)
(PP?1 to/TO
(NPB?2 such/JJ
funding/NN))))
but/CC
(VP?1 expressed/VBD
(NP-A?1 (NPB?1 hope/NN)
(PP?1 of/IN
(NP-A?1 (NPB?2 a/DT
compromise/NN))))))))
Figure 4: Collins Model 3 tree for He reiterated his op-
position to such funding, but expressed hope of a compro-
mise.
The Model 3 output in this example includes standard
phrase structure categories, indications of the heads, and
the additional -A marker to distinguish arguments from
adjuncts. The terminal nodes of this tree are inflected
forms, and the first phase of our conversion replaces them
with their citation forms (the verbs reiterate and express,
and the decapitalized and standardized he for He and his).
We also adjust for systematic differences in the choice of
heads. The first conjunct tends to be marked as the head
of a coordination in Model 3 output, whereas the depen-
dency bank has a more symmetric representation: it in-
troduces a new COORD head and connects that up to the
conjunction, and it uses a separate CONJ relation for each
of the coordinated items. Similarly, Model 3 identifies
the syntactic markers to and that as the heads of com-
plements, whereas the dependency bank treats these as
selectional features and marks the main predicate of the
complements as the head. These adjustments are carried
out without penalty. We also compensate for the differ-
ences in the representation of auxiliaries: Model 3 treats
these as main verbs with embedded complements instead
of the PERF, PROG, and PASSIVE features of the DEP-
BANK, and our conversion flattens the trees so that the
features can be read off.
The dependencies are read off after these and a few
other adjustments are made. NPs under VPs are read off
either as objects or adjuncts, depending on whether or
not the NP is annotated with the argument indicator (-A)
as in this example; the -A presumably would be miss-
ing in a sentence like John arrived Friday, and Friday
would be treated as an ADJUNCT. Similarly, NP-As un-
der S are read off as subject. In this example, however,
this principle of conversion does not lead to a match with
the dependency bank: in the DEPBANK grammatical rela-
tions that are factored out of conjoined structures are dis-
tributed back into those structures, to establish the correct
semantic dependencies (in this case, that he is the subject
of both reiterate and express and not of the introduced
coord). We avoided the temptation of building coordinate
distribution into the conversion routine because, first, it is
not always obvious from the Model 3 output when dis-
tribution should take place, and second, that would be
a first step towards building into the conversion routine
the deep lexical and syntactic knowledge (essentially the
functional component of our LFG grammar) that the shal-
low approach explicitly discounts4.
For the same reasons our conversion routine does not
identify the subjects of infinitival complements with par-
ticular arguments of matrix verbs. The Model 3 trees pro-
vide no indication of how this is to be done, and in many
cases the proper assignment depends on lexical informa-
tion about specific predicates (to capture, for example, the
well-known contrast between promise and persuade).
Model 3 trees also provide information about certain
4However, we did explore a few of these additional transfor-
mations and found only marginal F-score increases.
long-distance dependencies, by marking with -g annota-
tions the path between a filler and a gap and marking the
gap by an explicit TRACE in the terminal string. The filler
itself is not clearly identified, but our conversion treats
all WH categories under SBAR as potential fillers and
attempts to propagate them down the gap-chain to link
them up to appropriate traces.
In sum, it is not a trivial matter to convert a Model 3
tree to an appropriate set of dependency relations, and the
process requires a certain amount of intuition and skill.
For our experiments we tried to define a conversion that
gives appropriate credit to the dependencies that can be
read from the trees without relying on an undue amount
of sophisticated linguistic knowledge5.
5 Experiments
We conducted our experiments by preparing versions of
the test sentences in the form appropriate to each sys-
tem. We used a configuration of the XLE parser that ex-
pects sentences conforming to ordinary text conventions
to appear in a file separated by double line-feeds. A cer-
tain amount of effort was required to remove the part-of-
speech tags and labeled brackets of the WSJ corpus in a
way that restored the sentences to a standard English for-
mat (for example, to remove the space between wo and n?t
that remains when the POS tags are removed). Since the
PARC 700 treats proper names as multiword expressions,
we then augmented the input strings with XML markup
of the named entities. These are parsed by the grammar
as described in section 2. We used manual named entity
markup for this experiment because our intent is to mea-
sure parsing technology independent of either the time
or errors of an automatic named-entity extractor. How-
ever, in other experiments with an automatic finite-state
extractor, we have found that the time for named-entity
recognition is negligible (on the order of seconds across
the entire corpus) and makes relatively few errors, so that
the results reported here are good approximations of what
might be expected in more realistic situations.
As input to the Collins parser, we used the part-of-
speech tagged version of section 23 that was provided
with the parser. From this we extracted the 700 sentences
in the PARC 700. We then modified them to produce
named entity input so that the parses would match the
PARC 700. This was done by putting underscores be-
tween the parts of the named entity and changing the final
part of speech tag to the appropriate one (usually NNP)
if necessary. (The number of words indicated at the be-
ginning of the input string was also reduced accordingly.)
An example is shown in (1).
5The results of this conversion are available at
http://www2.parc.com/istl/groups/nltt/fsbank/
(1) Sen. NNP Christopher NNP Dodd NNP ??
Sen. Christopher Dodd NNP
After parsing, the underscores were converted to spaces
to match the PARC 700 predicates.
Before the final evaluation, 1/5 of the PARC 700 de-
pendency bank was randomly extracted as a heldout set.
This set was used to adjust the performance parameters of
the XLE system and the Collins parser so as to optimize
parsing speed without losing accuracy. For example, the
limit on the length of medial phrases was set to 20 words
for the XLE system (see Sec. 2), and a regularizer penalty
of 10 was found optimal for the `1 prior used in stochas-
tic disambiguation. For the Collins parser, a beam size
of 1000 was found to improve speed considerably at lit-
tle cost in accuracy. Furthermore, the np-bracketing flag
(npbflag) was set to 0 to produce an extended set of NP
levels for improved argument/adjunct distinction6. The fi-
nal evaluation was done on the remaining 560 examples.
Timing results are reported in seconds of CPU time7. POS
tagging of the input to the Collins parser took 6 seconds
and this was added to the timing result of the Collins
parser. Time spent for finite-state morphology and dictio-
nary lookup for XLE is part of the measure of its timing
performance. We did not include the time for dependency
extraction or stemming the Collins output.
Table 1 shows timing and accuracy results for the Re-
duced dependency set. The parser settings compared are
Model 3 of the Collins parser adjusted to beam size 1000,
and the Core and Complete versions of the XLE sys-
tem, differing in the size of the grammar?s constraint-
set. Clearly, both versions of the XLE system achieve a
significant reduction in error rate over the Collins parser
(12% for the core XLE system and 20% for the complete
system) at an increase in parsing time of a factor of only
1.49 for the core XLE system. The complete version gives
an overall improvement in F-score of 5% over the Collins
parser at a cost of a factor of 5 in parsing time.
Table 1: Timing and accuracy results for Collins parser
and Complete and Core versions of XLE system on Re-
duced version of PARC 700 dependency bank.
time prec. rec. F-score
LFG core 298.88 79.1 76.2 77.6
LFG complete 985.3 79.4 79.8 79.6
Collins 1000 199.6 78.3 71.2 74.6
6A beam size of 10000 as used in Collins (1999) improved
the F-score on the heldout set only by .1% at an increase of pars-
ing time by a factor of 3. Beam sizes lower than 1000 decreased
the heldout F-score significantly.
7All experiments were run on one CPU of a dual proces-
sor AMD Opteron 244 with 1.8 GHz and 4GB main memory.
Loading times are included in CPU times.
6 Conclusion
We presented some experiments that compare the accu-
racy and performance of two stochastic parsing systems,
the shallow Collins parser and the deep-grammar-based
XLE system. We measured the accuracy of both systems
against a gold standard derived from the PARC 700 de-
pendency bank, and also measured their processing times.
Contrary to conventional wisdom, we found that the shal-
low system was not substantially faster than the deep
parser operating on a core grammar, while the deep sys-
tem was significantly more accurate. Furthermore, ex-
tending the grammar base of the deep system results in
much better accuracy at a cost of a factor of 5 in speed.
Our experiment is comparable to recent work on read-
ing off Propbank-style (Kingsbury and Palmer, 2002)
predicate-argument relations from gold-standard tree-
bank trees and automatic parses of the Collins parser.
Gildea and Palmer (2002) report F-score results in the
55% range for argument and boundary recognition based
on automatic parses. From this perspective, the nearly
75% F-score that is achieved for our deterministic rewrit-
ing of Collins? trees into dependencies is remarkable,
even if the results are not directly comparable. Our scores
and Gildea and Palmer?s are both substantially lower than
the 90% typically cited for evaluations based on labeled
or unlabeled bracketing, suggesting that extracting se-
mantically relevant dependencies is a more difficult, but
we think more valuable, task.
References
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proceedings of COL-
ING2002, Workshop on Grammar Engineering and
Evaluation, pages 1?7.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
D. Crouch, C. Condoravdi, R. Stolle, T.H. King,
V. de Paiva, J. Everett, and D. Bobrow. 2002. Scal-
ability of redundancy detection in focused document
collections. In Proceedings of Scalable Natural Lan-
guage Understanding, Heidelberg.
Hal Daume and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), Philadelphia, PA.
Anette Frank, Tracy H. King, Jonas Kuhn, and John
Maxwell. 1998. Optimality theory style constraint
ranking in large-scale LFG grammars. In Proceedings
of the Third LFG Conference.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochatic
unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?02), Philadelphia, PA.
Dan Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL?02), Philadelphia.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Roxana
G??rju, Vasile Rus, and Paul Mora?rescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of the 39th
Annual Meeting and 10th Conference of the European
Chapter of the Asssociation for Computational Lin-
guistics (ACL?01), Toulouse, France.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the Work-
shop on ?Linguistically Interpreted Corpora? at the
10th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (LINC?03), Bu-
dapest, Hungary.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?02), Las Palmas, Spain.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
the 1st Conference of the North American Chapter of
the Association for Computational Linguistics (ANLP-
NAACL 2000), Seattle, WA.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the Human Language Technology Conference
(HLT?02), San Diego, CA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP-
1.
Stefan Riezler and Alexander Vasserman. 2004. Gradi-
ent feature testing and `1 regularization for maximum
entropy parsing. Submitted for publication.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting and 10th Conference of the Eu-
ropean Chapter of the Asssociation for Computational
Linguistics (ACL?01), Toulouse, France.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 248?255,
New York, June 2006. c?2006 Association for Computational Linguistics
Grammatical Machine Translation
Stefan Riezler and John T. Maxwell III
Palo Alto Research Center
3333 Coyote Hill Road, Palo Alto, CA 94304
Abstract
We present an approach to statistical
machine translation that combines ideas
from phrase-based SMT and traditional
grammar-based MT. Our system incor-
porates the concept of multi-word trans-
lation units into transfer of dependency
structure snippets, and models and trains
statistical components according to state-
of-the-art SMT systems. Compliant with
classical transfer-based MT, target depen-
dency structure snippets are input to a
grammar-based generator. An experimen-
tal evaluation shows that the incorpora-
tion of a grammar-based generator into an
SMT framework provides improved gram-
maticality while achieving state-of-the-art
quality on in-coverage examples, suggest-
ing a possible hybrid framework.
1 Introduction
Recent approaches to statistical machine translation
(SMT) piggyback on the central concepts of phrase-
based SMT (Och et al, 1999; Koehn et al, 2003)
and at the same time attempt to improve some of its
shortcomings by incorporating syntactic knowledge
in the translation process. Phrase-based translation
with multi-word units excels at modeling local or-
dering and short idiomatic expressions, however, it
lacks a mechanism to learn long-distance dependen-
cies and is unable to generalize to unseen phrases
that share non-overt linguistic information. Publicly
available statistical parsers can provide the syntactic
information that is necessary for linguistic general-
izations and for the resolution of non-local depen-
dencies. This information source is deployed in re-
cent work either for pre-ordering source sentences
before they are input to to a phrase-based system
(Xia and McCord, 2004; Collins et al, 2005), or
for re-ordering the output of translation models by
statistical ordering models that access linguistic in-
formation on dependencies and part-of-speech (Lin,
2004; Ding and Palmer, 2005; Quirk et al, 2005)1 .
While these approaches deploy dependency-style
grammars for parsing source and/or target text, a uti-
lization of grammar-based generation on the output
of translation models has not yet been attempted in
dependency-based SMT. Instead, simple target lan-
guage realization models that can easily be trained
to reflect the ordering of the reference translations in
the training corpus are preferred. The advantage of
such models over grammar-based generation seems
to be supported, for example, by Quirk et al?s (2005)
improvements over phrase-based SMT as well as
over an SMT system that deploys a grammar-based
generator (Menezes and Richardson, 2001) on n-
gram based automatic evaluation scores (Papineni et
al., 2001; Doddington, 2002). Another data point,
however, is given by Charniak et al (2003) who
show that parsing-based language modeling can im-
prove grammaticality of translations, even if these
improvements are not recorded under n-gram based
evaluation measures.
1A notable exception to this kind of approach is Chiang
(2005) who introduces syntactic information into phrase-based
SMT via hierarchical phrases rather than by external parsing.
248
In this paper we would like to step away from
n-gram based automatic evaluation scores for a
moment, and investigate the possible contributions
of incorporating a grammar-based generator into
a dependency-based SMT system. We present a
dependency-based SMT model that integrates the
idea of multi-word translation units from phrase-
based SMT into a transfer system for dependency
structure snippets. The statistical components of
our system are modeled on the phrase-based sys-
tem of Koehn et al (2003), and component weights
are adjusted by minimum error rate training (Och,
2003). In contrast to phrase-based SMT and to the
above cited dependency-based SMT approaches, our
system feeds dependency-structure snippets into a
grammar-based generator, and determines target lan-
guage ordering by applying n-gram and distortion
models after grammar-based generation. The goal of
this ordering model is thus not foremost to reflect the
ordering of the reference translations, but to improve
the grammaticality of translations.
Since our system uses standard SMT techniques
to learn about correct lexical choice and idiomatic
expressions, it allows us to investigate the contri-
bution of grammar-based generation to dependency-
based SMT2. In an experimental evaluation on the
test-set that was used in Koehn et al (2003) we
show that for examples that are in coverage of
the grammar-based system, we can achieve state-
of-the-art quality on n-gram based evaluation mea-
sures. To discern the factors of grammaticality
and translational adequacy, we conducted a man-
ual evaluation on 500 in-coverage and 500 out-of-
coverage examples. This showed that an incorpo-
ration of a grammar-based generator into an SMT
framework provides improved grammaticality over
phrase-based SMT on in-coverage examples. Since
in our system it is determinable whether an example
is in-coverage, this opens the possibility for a hy-
brid system that achieves improved grammaticality
at state-of-the-art translation quality.
2A comparison of the approaches of Quirk et al (2005) and
Menezes and Richardson (2001) with respect to ordering mod-
els is difficult because they differ from each other in their statis-
tical and dependency-tree alignment models.
2 Extracting F-Structure Snippets
Our method for extracting transfer rules for depen-
dency structure snippets operates on the paired sen-
tences of a sentence-aligned bilingual corpus. Sim-
ilar to phrase-based SMT, our approach starts with
an improved word-alignment that is created by in-
tersecting alignment matrices for both translation di-
rections, and refining the intersection alignment by
adding directly adjacent alignment points, and align-
ment points that align previously unaligned words
(see Och et al (1999)). Next, source and target sen-
tences are parsed using source and target LFG gram-
mars to produce a set of possible f(unctional) de-
pendency structures for each side (see Riezler et al
(2002) for the English grammar and parser; Butt et
al. (2002) for German). The two f-structures that
most preserve dependencies are selected for further
consideration. Selecting the most similar instead of
the most probable f-structures is advantageous for
rule induction since it provides for higher cover-
age with simpler rules. In the third step, the many-
to-many word alignment created in the first step is
used to define many-to-many correspondences be-
tween the substructures of the f-structures selected
in the second step. The parsing process maintains
an association between words in the string and par-
ticular predicate features in the f-structure, and thus
the predicates on the two sides are implicitly linked
by virtue of the original word alignment. The word
alignment is extended to f-structures by setting into
correspondence the f-structure units that immedi-
ately contain linked predicates. These f-structure
correspondences are the basis for hypothesizing can-
didate transfer rules.
To illustrate, suppose our corpus contains the fol-
lowing aligned sentences (this example is taken from
our experiments on German-to-English translation):
Dafu?r bin ich zutiefst dankbar.
I have a deep appreciation for that.
Suppose further that we have created the many-to-
many bi-directional word alignment
Dafu?r{6 7} bin{2} ich{1} zutiefst{3 4 5}
dankbar{5}
indicating for example that Dafu?r is aligned with
words 6 and 7 of the English sentence (for and that).
249
??
?
?
?
?
?
?
?
?
?
PRED sein
SUBJ
[
PRED ich
]
XCOMP
?
?
?
?
?
PRED dankbar
ADJ
?
?
?
?
?
[
PRED zutiefst
]
[
PRED dafu?r
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED have
SUBJ
[
PRED I
]
OBJ
?
?
?
?
?
?
?
?
?
?
?
PRED appreciation
SPEC
[
PRED a
]
ADJ
?
?
?
?
?
?
?
[
PRED deep
]
?
?
PRED for
OBJ
[
PRED that
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: F-structure alignment for induction of German-to-English transfer rules.
This results in the links between the predicates of the
source and target f-structures shown in Fig. 1.
From these source-target f-structure alignments
transfer rules are extracted in two steps. In the first
step, primitive transfer rules are extracted directly
from the alignment of f-structure units. These in-
clude simple rules for mapping lexical predicates
such as:
PRED(%X1, ich) ==> PRED(%X1, I)
and somewhat more complicated rules for mapping
local f-structure configurations. For example, the
rule shown below is derived from the alignment of
the outermost f-structures. It maps any f-structure
whose pred is sein to an f-structure with pred have,
and in addition interprets the subj-to-subj link as an
indication to map the subject of a source with this
predicate into the subject of the target and the xcomp
of the source into the object of the target. Features
denoting number, person, type, etc. are not shown;
variables %X denote f-structure values.
PRED(%X1,sein) PRED(%X1,have)
SUBJ(%X1,%X2) ==> SUBJ(%X1,%X2)
XCOMP(%X1,%X3) OBJ(%X1,%X3)
The following rule shows how a single source f-
structure can be mapped to a local configuration of
several units on the target side, in this case the sin-
gle f-structure headed by dafu?r into one that corre-
sponds to an English preposition+object f-structure.
PRED(%X1,for)
PRED(%X1, dafu?r) ==> OBJ(%X1,%X2)
PRED(%X2,that)
Transfer rules are required to only operate on con-
tiguous units of the f-structure that are consistent
with the word alignment. This transfer contiguity
constraint states that
1. source and target f-structures are each con-
nected.
2. f-structures in the transfer source can only be
aligned with f-structures in the transfer target,
and vice versa.
This constraint on f-structures is analogous to the
constraint on contiguous and alignment-consistent
phrases employed in phrase-based SMT. It prevents
the extraction of a transfer rule that would trans-
late dankbar directly into appreciation since appre-
ciation is aligned also to zutiefst and its f-structure
would also have to be included in the transfer. Thus,
the primitive transfer rule for these predicates must
be:
PRED(%X1,dankbar) PRED(%X1,appr.)
ADJ(%X1,%X2) ==> SPEC(%X1,%X2)
in set(%X3,%X2) PRED(%X2,a)
PRED(%X3,zutiefst) ADJ(%X1,%X3)
in set(%X4,%X3)
PRED(%X4,deep)
In the second step, rules for more complex map-
pings are created by combining primitive transfer
rules that are adjacent in the source and target f-
structures. For instance, we can combine the prim-
itive transfer rule that maps sein to have with the
primitive transfer rule that maps ich to I to produce
the complex transfer rule:
PRED(%X1,sein) PRED(%X1,have)
SUBJ(%X1,%X2) ==> SUBJ(%X1,%X2)
PRED(%X2,ich) PRED(%X2,I)
XCOMP(%X1,%X3) OBJ(%X1,%X3)
In the worst case, there can be an exponential
number of combinations of primitive transfer rules,
so we only allow at most three primitive transfer
rules to be combined. This produces O(n2) trans-
250
fer rules in the worst case, where n is the number of
f-structures in the source.
Other points where linguistic information comes
into play is in morphological stemming in f-
structures, and in the optional filtering of f-structure
phrases based on consistency of linguistic types. For
example, the extraction of a phrase-pair that trans-
lates zutiefst dankbar into a deep appreciation is
valid in the string-based world, but would be pre-
vented in the f-structure world because of the incom-
patibility of the types A and N for adjectival dankbar
and nominal appreciation. Similarly, a transfer rule
translating sein to have could be dispreferred be-
cause of a mismatch in the the verbal types V/A and
V/N. However, the transfer of sein zutiefst dankbar
to have a deep appreciation is licensed by compati-
ble head types V.
3 Parsing-Transfer-Generation
We use LFG grammars, producing c(onstituent)-
structures (trees) and f(unctional)-structures (at-
tribute value matrices) as output, for parsing source
and target text (Riezler et al, 2002; Butt et al, 2002).
To increase robustness, the standard grammar is aug-
mented with a FRAGMENT grammar. This allows
sentences that are outside the scope of the standard
grammar to be parsed as well-formed chunks speci-
fied by the grammar, with unparsable tokens possi-
bly interspersed. The correct parse is determined by
a fewest-chunk method.
Transfer converts source into a target f-structures
by non-deterministically applying all of the induced
transfer rules in parallel. Each fact in the German f-
structure must be transferred by exactly one trans-
fer rule. For robustness a default rule is included
that transfers any fact as itself. Similar to parsing,
transfer works on a chart. The chart has an edge for
each combination of facts that have been transferred.
When the chart is complete, the outputs of the trans-
fer rules are unified to make sure they are consistent
(for instance, that the transfer rules did not produce
two determiners for the same noun). Selection of
the most probable transfer output is done by beam-
decoding on the transfer chart.
LFG grammars can be used bidirectionally for
parsing and generation, thus the existing English
grammar used for parsing the training data can
also be used for generation of English translations.
For in-coverage examples, the grammar specifies c-
structures that differ in linear precedence of sub-
trees for a given f-structure, and realizes the termi-
nal yield according to morphological rules. In order
to guarantee non-empty output for the overall trans-
lation system, the generation component has to be
fault-tolerant in cases where the transfer system op-
erates on a fragmentary parse, or produces non-valid
f-structures from valid input f-structures. For gener-
ation from unknown predicates, a default morphol-
ogy is used to inflect the source stem correctly for
English. For generation from unknown structures, a
default grammar is used that allows any attribute to
be generated in any order as any category, with op-
timality marks set so as to prefer the standard gram-
mar over the default grammar.
4 Statistical Models and Training
The statistical components of our system are mod-
eled on the statistical components of the phrase-
based system Pharaoh, described in Koehn et al
(2003) and Koehn (2004). Pharaoh integrates the
following 8 statistical models: relative frequency of
phrase translations in source-to-target and target-
to-source direction, lexical weighting in source-to-
target and target-to-source direction, phrase count,
language model probability, word count, and distor-
tion probability.
Correspondingly, our system computes the fol-
lowing statistics for each translation:
1. log-probability of source-to-target transfer
rules, where the probability r(e|f) of a rule
that transfers source snippet f into target snip-
pet e is estimated by the relative frequency
r(e|f) = count(f ==> e)?
e? count(f ==> e?)
2. log-probability of target-to-source rules
3. log-probability of lexical translations from
source to target snippets, estimated from
Viterbi alignments a? between source word po-
sitions i = 1, . . . , n and target word positions
j = 1, . . . ,m for stems fi and ej in snippets
f and e with relative word translation frequen-
251
cies t(ej |fi):
l(e|f) =
?
j
1
|{i|(i, j) ? a?}|
?
(i,j)?a?
t(ej |fi)
4. log-probability of lexical translations from tar-
get to source snippets
5. number of transfer rules
6. number of transfer rules with frequency 1
7. number of default transfer rules (translating
source features into themselves)
8. log-probability of strings of predicates from
root to frontier of target f-structure, estimated
from predicate trigrams in English f-structures
9. number of predicates in target f-structure
10. number of constituent movements during gen-
eration based on the original order of the head
predicates of the constituents (for example,
AP[2] BP[3] CP[1] counts as two move-
ments since the head predicate of CP moved
from the first position to the third position)
11. number of generation repairs
12. log-probability of target string as computed by
trigram language model
13. number of words in target string
These statistics are combined into a log-linear model
whose parameters are adjusted by minimum error
rate training (Och, 2003).
5 Experimental Evaluation
The setup for our experimental comparison is
German-to-English translation on the Europarl par-
allel data set3. For quick experimental turnaround
we restricted our attention to sentences with 5 to
15 words, resulting in a training set of 163,141 sen-
tences and a development set of 1967 sentences. Fi-
nal results are reported on the test set of 1,755 sen-
tences of length 5-15 that was used in Koehn et al
(2003). To extract transfer rules, an improved bidi-
rectional word alignment was created for the train-
ing data from the word alignment of IBM model 4 as
3http://people.csail.mit.edu/koehn/publications/europarl/
implemented by GIZA++ (Och et al, 1999). Train-
ing sentences were parsed using German and En-
glish LFG grammars (Riezler et al, 2002; Butt et
al., 2002). The grammars obtain 100% coverage on
unseen data. 80% are parsed as full parses; 20% re-
ceive FRAGMENT parses. Around 700,000 transfer
rules were extracted from f-structures pairs chosen
according to a dependency similarity measure. For
language modeling, we used the trigram model of
Stolcke (2002).
When applied to translating unseen text, the sys-
tem operates on n-best lists of parses, transferred
f-structures, and generated strings. For minimum-
error-rate training on the development set, and for
translating the test set, we considered 1 German
parse for each source sentence, 10 transferred f-
structures for each source parse, and 1,000 gener-
ated strings for each transferred f-structure. Selec-
tion of most probable translations proceeds in two
steps: First, the most probable transferred f-structure
is computed by a beam search on the transfer chart
using the first 10 features described above. These
features include tests on source and target f-structure
snippets related via transfer rules (features 1-7) as
well as language model and distortion features on
the target c- and f-structures (features 8-10). In our
experiments, the beam size was set to 20 hypotheses.
The second step is based on features 11-13, which
are computed on the strings that were actually gen-
erated from the selected n-best f-structures.
We compared our system to IBM model 4 as pro-
duced by GIZA++ (Och et al, 1999) and a phrase-
based SMT model as provided by Pharaoh (2004).
The same improved word alignment matrix and the
same training data were used for phrase-extraction
for phrase-based SMT as well as for transfer-rule
extraction for LFG-based SMT. Minimum-error-rate
training was done using Koehn?s implementation of
Och?s (2003) minimum-error-rate model. To train
the weights for phrase-based SMT we used the first
500 sentences of the development set; the weights of
the LFG-based translator were adjusted on the 750
sentences that were in coverage of our grammars.
For automatic evaluation, we use the NIST metric
(Doddington, 2002) combined with the approximate
randomization test (Noreen, 1989), providing the de-
sired combination of a sensitive evaluation metric
and an accurate significance test (see Riezler and
252
Table 1: NIST scores on test set for IBM model 4 (M4),
phrase-based SMT (P), and the LFG-based SMT (LFG) on the
full test set and on in-coverage examples for LFG. Results in the
same row that are not statistically significant from each other are
marked with a ?.
M4 LFG P
in-coverage 5.13 *5.82 *5.99
full test set *5.57 *5.62 6.40
Table 2: Preference ratings of two human judges for transla-
tions of phrase-based SMT (P) or LFG-based SMT (LFG) under
criteria of fluency/grammaticality and translational/semantic
adequacy on 500 in-coverage examples. Ratings by judge 1 are
shown in rows, for judge 2 in columns. Agreed-on examples are
shown in boldface in the diagonals.
adequacy grammaticality
j1\j2 P LFG equal P LFG equal
P 48 8 7 36 2 9
LFG 10 105 18 6 113 17
equal 53 60 192 51 44 223
Maxwell (2005)). In order to avoid a random as-
sessment of statistical significance in our three-fold
pairwise comparison, we reduce the per-comparison
significance level to 0.01 so as to achieve a standard
experimentwise significance level of 0.05 (see Co-
hen (1995)). Table 1 shows results for IBM model
4, phrase-based SMT, and LFG-based SMT, where
examples that are in coverage of the LFG-based sys-
tems are evaluated separately. Out of the 1,755 sen-
tences of the test set, 44% were in coverage of the
LFG-grammars; for 51% the system had to resort to
the FRAGMENT technique for parsing and/or repair
techniques in generation; in 5% of the cases our sys-
tem timed out. Since our grammars are not set up
with punctuation in mind, punctuation is ignored in
all evaluations reported below.
For in-coverage examples, the difference between
NIST scores for the LFG system and the phrase-
based system is statistically not significant. On the
full set of test examples, the suboptimal quality on
out-of-coverage examples overwhelms the quality
achieved on in-coverage examples, resulting in a sta-
tistically not significant result difference in NIST
scores between the LFG system and IBM model 4.
In order to discern the factors of grammaticality
and translational adequacy, we conducted a manual
evaluation on randomly selected 500 examples that
were in coverage of the grammar-based generator.
Two independent human judges were presented with
the source sentence, and the output of the phrase-
based and LFG-based systems in a blind test. This
was achieved by displaying the system outputs in
random order. The judges were asked to indicate a
preference for one system translation over the other,
or whether they thought them to be of equal quality.
These questions had to be answered separately un-
der the criteria of grammaticality/fluency and trans-
lational/semantic adequacy. As shown in Table 2,
both judges express a preference for the LFG system
over the phrase-based system for both adequacy and
grammaticality. If we just look at sentences where
judges agree, we see a net improvement on trans-
lational adequacy of 57 sentences, which is an im-
provement of 11.4% over the 500 sentences. If this
were part of a hybrid system, this would amount to a
5% overall improvement in translational adequacy.
Similarly we see a net improvement on grammat-
icality of 77 sentences, which is an improvement
of 15.4% over the 500 sentences or 6.7% overall
in a hybrid system. Result differences on agreed-
on ratings are statistically significant, where sig-
nificance was assessed by approximate randomiza-
tion via stratified shuffling of the preferences be-
tween the systems (Noreen, 1989). Examples from
the manual evaluation are shown in Fig. 2.
Along the same lines, a further manual evaluation
was conducted on 500 randomly selected examples
that were out of coverage of the LFG-based gram-
mars. Across the combined set of 1,000 in-coverage
and out-of-coverage sentences, this resulted in an
agreed-on preference for the phrase-based system
in 204 cases and for the LFG-based system in 158
cases under the measure of translational adequacy.
Under the grammaticality measure the phrase-based
system was preferred by both judges in 157 cases
and the LFG-based system in 136 cases.
6 Discussion
The above presented evaluation of the LFG-based
translator shows promising results for examples that
are in coverage of the employed LFG grammars.
However, a back-off to robustness techniques in
parsing and/or generation results in a considerable
253
(1) src: in diesem fall werde ich meine verantwortung wahrnehmen
ref: then i will exercise my responsibility
LFG: in this case i accept my responsibility
P: in this case i shall my responsibilities
(2) src: die politische stabilita?t ha?ngt ab von der besserung der lebensbedingungen
ref: political stability depends upon the improvement of living conditions
LFG: the political stability hinges on the recovery the conditions
P: the political stability is rejects the recovery of the living conditions
(3) src: und schlie?lich mu? dieser agentur eine kritische haltung gegenu?ber der kommission selbst erlaubt sein
ref: moreover the agency must be able to criticise the commission itself
LFG: and even to the commission a critical stance must finally be allowed this agency
P: finally this is a critical attitude towards the commission itself to be agency
(4) src: nach der ratifizierung werden co2 emissionen ihren preis haben
ref: after ratification co2 emission will have a price tag
LFG: carbon dioxide emissions have its price following the ratification
P: after the ratification co2 emissions are a price
(5) src: die lebensmittel mu?ssen die sichere erna?hrung des menschen gewa?hrleisten
ref: man?s food must be safe to eat
LFG: food must guarantee the safe nutrition of the people
P: the people of the nutrition safe food must guarantee
(6) src: was wir morgen beschlie?en werden ist letztlich material fu?r das vermittlungsverfahren
ref: whatever we agree tomorrow will ultimately have to go into the conciliation procedure
LFG: one tomorrow we approved what is ultimately material for the conciliation procedure
P: what we decide tomorrow is ultimately material for the conciliation procedure
(7) src: die verwaltung mu? ku?nftig schneller reagieren ko?nnen
ref: in future the administration must be able to react more quickly
LFG: more in future the administration must be able to react
P: the administration must be able to react more quickly
(8) src: das ist jetzt u?ber 40 jahre her
ref: that was over 40 years ago
LFG: on 40 years ago it is now
P: that is now over 40 years ago
(9) src: das ist schon eine seltsame vorstellung von gleichheit
ref: a strange notion of equality
LFG: equality that is even a strange idea
P: this is already a strange idea of equality
(10) src: frau pra?sidentin ich beglu?ckwu?nsche herrn nicholson zu seinem ausgezeichneten bericht
ref: madam president i congratulate mr nicholson on his excellent report
LFG: madam president i congratulate mister nicholson on his report excellented
P: madam president i congratulate mr nicholson for his excellent report
Figure 2: Examples from manual evaluation: Preference for LFG-based system (LFG) over phrase-based system (P) under both
adequacy and grammaticality (ex 1-5), preference of phrased-based system over LFG (6-10) , together with source (src) sentences
and human reference (ref) translations. All ratings are agreed on by both judges.
loss in translation quality. The high percentage of
examples that fall out of coverage of the LFG-
based system can partially be explained by the ac-
cumulation of errors in parsing the training data
where source and target language parser each pro-
duce FRAGMENT parses in 20% of the cases. To-
gether with errors in rule extraction, this results in
a large number ill-formed transfer rules that force
the generator to back-off to robustness techniques.
In applying the parse-transfer-generation pipeline to
translating unseen text, parsing errors can cause er-
roneous transfer, which can result in generation er-
rors. Similar effects can be observed for errors in
translating in-coverage examples. Here disambigua-
tion errors in parsing and transfer propagate through
the system, producing suboptimal translations. An
error analysis on 100 suboptimal in-coverage exam-
ples from the development set showed that 69 sub-
optimal translations were due to transfer errors, 10
of which were due to errors in parsing.
The discrepancy between NIST scores and man-
ual preference rankings can be explained on the one
hand by the suboptimal integration of transfer and
generation in our system, making it infeasible to
work with large n-best lists in training and applica-
tion. Moreover, despite our use of minimum-error-
254
rate training and n-gram language models, our sys-
tem cannot be adjusted to maximize n-gram scores
on reference translation in the same way as phrase-
based systems since statistical ordering models are
employed in our framework after grammar-based
generation, thus giving preference to grammatical-
ity over similarity to reference translations.
7 Conclusion
We presented an SMT model that marries phrase-
based SMT with traditional grammar-based MT
by incorporating a grammar-based generator into a
dependency-based SMT system. Under the NIST
measure, we achieve results in the range of the
state-of-the-art phrase-based system of Koehn et
al. (2003) for in-coverage examples of the LFG-
based system. A manual evaluation of a large set
of such examples shows that on in-coverage ex-
amples our system achieves significant improve-
ments in grammaticality and also translational ad-
equacy over the phrase-based system. Fortunately,
it is determinable when our system is in-coverage,
which opens the possibility for a hybrid system that
achieves improved grammaticality at state-of-the-art
translation quality. Future work thus will concen-
trate on improvements of in-coverage translations
e.g., by stochastic generation. Furthermore, we in-
tend to apply our system to other language pairs and
larger data sets.
Acknowledgements
We would like to thank Sabine Blum for her invalu-
able help with the manual evaluation.
References
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Ma-
suichi, and Christian Rohrer. 2002. The parallel grammar
project. COLING?02, Workshop on Grammar Engineering
and Evaluation.
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003.
Syntax-based language models for statistical machine trans-
lation. MT Summit IX.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. ACL?05.
Paul R. Cohen. 1995. Empirical Methods for Artificial Intelli-
gence. The MIT Press.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005.
Clause restructuring for statistical machine translation.
ACL?05.
Yuan Ding and Martha Palmer. 2005. Machine translation
using probabilistic synchronous dependency insertion gram-
mars. ACL?05.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence statis-
tics. ARPA Workshop on Human Language Technology.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. HLT-NAACL?03.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. User
manual. Technical report, USC ISI.
Dekang Lin. 2004. A path-based transfer model for statistical
machine translation. COLING?04.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of transfer-
mappings from bilingual corpora. Workshop on Data-
Driven Machine Translation.
Eric W. Noreen. 1989. Computer Intensive Methods for Testing
Hypotheses. An Introduction. Wiley.
Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999.
Improved alignment models for statistical machine transla-
tion. EMNLP?99.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. HLT-NAACL?03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a method for automatic evaluation of ma-
chine translation. Technical Report IBM RC22176 (W0190-
022).
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed phrasal
SMT. ACL?05.
Stefan Riezler and John Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing for mt. ACL-
05 Workshop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing
the Wall Street Journal using a Lexical-Functional Grammar
and discriminative estimation techniques. ACL?02.
Stefan Riezler, Tracy H. King, Richard Crouch, and Annie Za-
enen. 2003. Statistical sentence condensation using am-
biguity packing and stochastic disambiguation methods for
lexical-functional grammar. HLT-NAACL?03.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. International Conference on Spoken Language
Processing.
Fei Xia and Michael McCord. 2004. Improving a statistical mt
system with automatically learned rewrite patterns. COL-
ING?04.
255
Parsing the Wall Street Journal using a Lexical-Functional Grammar and
Discriminative Estimation Techniques
Stefan Riezler Tracy H. King Ronald M. Kaplan
Palo Alto Research Center Palo Alto Research Center Palo Alto Research Center
Palo Alto, CA 94304 Palo Alto, CA 94304 Palo Alto, CA 94304
riezler@parc.com thking@parc.com kaplan@parc.com
Richard Crouch John T. Maxwell III Mark Johnson
Palo Alto Research Center Palo Alto Research Center Brown University
Palo Alto, CA 94304 Palo Alto, CA 94304 Providence, RI 02912
crouch@parc.com maxwell@parc.com mj@cs.brown.edu
Abstract
We present a stochastic parsing system
consisting of a Lexical-Functional Gram-
mar (LFG), a constraint-based parser and
a stochastic disambiguation model. We re-
port on the results of applying this sys-
tem to parsing the UPenn Wall Street
Journal (WSJ) treebank. The model com-
bines full and partial parsing techniques
to reach full grammar coverage on unseen
data. The treebank annotations are used
to provide partially labeled data for dis-
criminative statistical estimation using ex-
ponential models. Disambiguation perfor-
mance is evaluated by measuring matches
of predicate-argument relations on two
distinct test sets. On a gold standard of
manually annotated f-structures for a sub-
set of the WSJ treebank, this evaluation
reaches 79% F-score. An evaluation on a
gold standard of dependency relations for
Brown corpus data achieves 76% F-score.
1 Introduction
Statistical parsing using combined systems of hand-
coded linguistically fine-grained grammars and
stochastic disambiguation components has seen con-
siderable progress in recent years. However, such at-
tempts have so far been confined to a relatively small
scale for various reasons. Firstly, the rudimentary
character of functional annotations in standard tree-
banks has hindered the direct use of such data for
statistical estimation of linguistically fine-grained
statistical parsing systems. Rather, parameter esti-
mation for such models had to resort to unsupervised
techniques (Bouma et al, 2000; Riezler et al, 2000),
or training corpora tailored to the specific grammars
had to be created by parsing and manual disam-
biguation, resulting in relatively small training sets
of around 1,000 sentences (Johnson et al, 1999).
Furthermore, the effort involved in coding broad-
coverage grammars by hand has often led to the spe-
cialization of grammars to relatively small domains,
thus sacrificing grammar coverage (i.e. the percent-
age of sentences for which at least one analysis is
found) on free text. The approach presented in this
paper is a first attempt to scale up stochastic parsing
systems based on linguistically fine-grained hand-
coded grammars to the UPenn Wall Street Journal
(henceforth WSJ) treebank (Marcus et al, 1994).
The problem of grammar coverage, i.e. the fact
that not all sentences receive an analysis, is tack-
led in our approach by an extension of a full-
fledged Lexical-Functional Grammar (LFG) and a
constraint-based parser with partial parsing tech-
niques. In the absence of a complete parse, a so-
called ?FRAGMENT grammar? allows the input to be
analyzed as a sequence of well-formed chunks. The
set of fragment parses is then chosen on the basis
of a fewest-chunk method. With this combination of
full and partial parsing techniques we achieve 100%
grammar coverage on unseen data.
Another goal of this work is the best possible ex-
ploitation of the WSJ treebank for discriminative es-
timation of an exponential model on LFG parses. We
define discriminative or conditional criteria with re-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 271-278.
                         Proceedings of the 40th Annual Meeting of the Association for
CS 1: FRAGMENTS
Sadj[fin]
S[fin]
NP
D 
the
NPadj
AP[attr]
A
golden
NPzero
N
share
VPall[fin]
VP[pass,fin]
AUX[pass,fin]
was
VPv[pass]
V[pass]
scheduled
VPinf
VPinf?pos
PARTinf
to
VPall[base]
VPv[base]
V[base]
expire
PPcl
PP
P
at
NP
D
the
NPadj
NPzero
N
beginning
FRAGMENTS
TOKEN
of
"The golden share was scheduled to expire at the beginning of"
?schedule<NULL, [132:expire]>[11:share]?PRED
?share?PRED 
?golden<[11:share]>?PRED  [11:share]SUBJADEGREE positive , ADJUNCT?TYPE nominal, ATYPE attributive23ADJUNCT
unspecifiedGRAINNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE nom , NUM  sg, PERS   311
SUBJ
?expire<[11:share]>?PRED  [11:share]SUBJ
?at<[170:beginning]>?PRED
?beginning ?PRED 
GERUND +, GRAIN unspecifiedNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE acc, NUM  sg, PCASE   at, PERS   3170
OBJ
ADV?TYPE	  vpadv
 , PSEM   locative, PTYPE   sem164
ADJUNCT	
INF?FORM to , PASSIVE   ?, VTYPE  main132
XCOMP
MOOD indicative, TENSE pastTNS?ASP
PASSIVE +, STMT?TYPE decl, VTYPE main67
FIRST
ofTOKEN229FIRST3218REST3188
Figure 1: FRAGMENT c-/f-structure for The golden share was scheduled to expire at the beginning of
spect to the set of grammar parses consistent with
the treebank annotations. Such data can be gathered
by applying labels and brackets taken from the tree-
bank annotation to the parser input. The rudimen-
tary treebank annotations are thus used to provide
partially labeled data for discriminative estimation
of a probability model on linguistically fine-grained
parses.
Concerning empirical evaluation of disambigua-
tion performance, we feel that an evaluation measur-
ing matches of predicate-argument relations is more
appropriate for assessing the quality of our LFG-
based system than the standard measure of match-
ing labeled bracketing on section 23 of the WSJ
treebank. The first evaluation we present measures
matches of predicate-argument relations in LFG f-
structures (henceforth the LFG annotation scheme)
to a gold standard of manually annotated f-structures
for a representative subset of the WSJ treebank. The
evaluation measure counts the number of predicate-
argument relations in the f-structure of the parse
selected by the stochastic model that match those
in the gold standard annotation. Our parser plus
stochastic disambiguator achieves 79% F-score un-
der this evaluation regime.
Furthermore, we employ another metric which
maps predicate-argument relations in LFG f-
structures to the dependency relations (henceforth
the DR annotation scheme) proposed by Carroll et
al. (1999). Evaluation with this metric measures the
matches of dependency relations to Carroll et al?s
gold standard corpus. For a direct comparison of our
results with Carroll et al?s system, we computed an
F-score that does not distinguish different types of
dependency relations. Under this measure we obtain
76% F-score.
This paper is organized as follows. Section 2
describes the Lexical-Functional Grammar, the
constraint-based parser, and the robustness tech-
niques employed in this work. In section 3 we
present the details of the exponential model on LFG
parses and the discriminative statistical estimation
technique. Experimental results are reported in sec-
tion 4. A discussion of results is in section 5.
2 Robust Parsing using LFG
2.1 A Broad-Coverage LFG
The grammar used for this project was developed in
the ParGram project (Butt et al, 1999). It uses LFG
as a formalism, producing c(onstituent)-structures
(trees) and f(unctional)-structures (attribute value
matrices) as output. The c-structures encode con-
stituency. F-structures encode predicate-argument
relations and other grammatical information, e.g.,
number, tense. The XLE parser (Maxwell and Ka-
plan, 1993) was used to produce packed represen-
tations, specifying all possible grammar analyses of
the input.
The grammar has 314 rules with regular expres-
sion right-hand sides which compile into a collec-
tion of finite-state machines with a total of 8,759
states and 19,695 arcs. The grammar uses several
lexicons and two guessers: one guesser for words
recognized by the morphological analyzer but not
in the lexicons and one for those not recognized.
As such, most nouns, adjectives, and adverbs have
no explicit lexical entry. The main verb lexicon con-
tains 9,652 verb stems and 23,525 subcategorization
frame-verb stem entries; there are also lexicons for
adjectives and nouns with subcategorization frames
and for closed class items.
For estimation purposes using the WSJ treebank,
the grammar was modified to parse part of speech
tags and labeled bracketing. A stripped down ver-
sion of the WSJ treebank was created that used
only those POS tags and labeled brackets relevant
for determining grammatical relations. The WSJ la-
beled brackets are given LFG lexical entries which
constrain both the c-structure and the f-structure of
the parse. For example, the WSJ?s ADJP-PRD la-
bel must correspond to an AP in the c-structure and
an XCOMP in the f-structure. In this version of the
corpus, all WSJ labels with -SBJ are retained and
are restricted to phrases corresponding to SUBJ in
the LFG grammar; in addition, it contains NP under
VP (OBJ and OBJth in the LFG grammar), all -LGS
tags (OBL-AG), all -PRD tags (XCOMP), VP under
VP (XCOMP), SBAR- (COMP), and verb POS tags
under VP (V in the c-structure). For example, our
labeled bracketing of wsj 1305.mrg is [NP-SBJ His
credibility] is/VBZ also [PP-PRD on the line] in the
investment community.
Some mismatches between the WSJ labeled
bracketing and the LFG grammar remain. These
often arise when a given constituent fills a gram-
matical role in more than one clause. For exam-
ple, in wsj 1303.mrg Japan?s Daiwa Securities Co.
named Masahiro Dozen president., the noun phrase
Masahiro Dozen is labeled as an NP-SBJ. However,
the LFG grammar treats it as the OBJ of the ma-
trix clause. As a result, the labeled bracketed version
of this sentence does not receive a full parse, even
though its unlabeled, string-only counterpart is well-
formed. Some other bracketing mismatches remain,
usually the result of adjunct attachment. Such mis-
matches occur in part because, besides minor mod-
ifications to match the bracketing for special con-
structions, e.g., negated infinitives, the grammar was
not altered to mirror the idiosyncrasies of the WSJ
bracketing.
2.2 Robustness Techniques
To increase robustness, the standard grammar has
been augmented with a FRAGMENT grammar. This
grammar parses the sentence as well-formed chunks
specified by the grammar, in particular as Ss, NPs,
PPs, and VPs. These chunks have both c-structures
and f-structures corresponding to them. Any token
that cannot be parsed as one of these chunks is
parsed as a TOKEN chunk. The TOKENs are also
recorded in the c- and f-structures. The grammar has
a fewest-chunk method for determining the correct
parse. For example, if a string can be parsed as two
NPs and a VP or as one NP and an S, the NP-S
option is chosen. A sample FRAGMENT c-structure
and f-structure are shown in Fig. 1 for wsj 0231.mrg
(The golden share was scheduled to expire at the
beginning of), an incomplete sentence; the parser
builds one S chunk and then one TOKEN for the
stranded preposition.
A final capability of XLE that increases cov-
erage of the standard-plus-fragment grammar is a
SKIMMING technique. Skimming is used to avoid
timeouts and memory problems. When the amount
of time or memory spent on a sentence exceeds
a threshhold, XLE goes into skimming mode for
the constituents whose processing has not been
completed. When XLE skims these remaining con-
stituents, it does a bounded amount of work per sub-
tree. This guarantees that XLE finishes processing
a sentence in a polynomial amount of time. In pars-
ing section 23, 7.2% of the sentences were skimmed;
26.1% of these resulted in full parses, while 73.9%
were FRAGMENT parses.
The grammar coverage achieved 100% of section
23 as unseen unlabeled data: 74.7% as full parses,
25.3% FRAGMENT and/or SKIMMED parses.
3 Discriminative Statistical Estimation
from Partially Labeled Data
3.1 Exponential Models on LFG Parses
We employed the well-known family of exponential
models for stochastic disambiguation. In this paper
we are concerned with conditional exponential mod-
els of the form:
p?(x|y) = Z?(y)
?1e??f(x)
where X(y) is the set of parses for sentence y,
Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant, ? = (?1, . . . , ?n) ? IRn is a vector of
log-parameters, f = (f1, . . . , fn) is a vector of
property-functions fi : X ? IR for i = 1, . . . , n
on the set of parses X , and ? ? f(x) is the vector dot
product
?n
i=1 ?ifi(x).
In our experiments, we used around 1000
complex property-functions comprising information
about c-structure, f-structure, and lexical elements
in parses, similar to the properties used in Johnson
et al (1999). For example, there are property func-
tions for c-structure nodes and c-structure subtrees,
indicating attachment preferences. High versus low
attachment is indicated by property functions count-
ing the number of recursively embedded phrases.
Other property functions are designed to refer to
f-structure attributes, which correspond to gram-
matical functions in LFG, or to atomic attribute-
value pairs in f-structures. More complex property
functions are designed to indicate, for example, the
branching behaviour of c-structures and the (non)-
parallelism of coordinations on both c-structure and
f-structure levels. Furthermore, properties refering
to lexical elements based on an auxiliary distribution
approach as presented in Riezler et al (2000) are
included in the model. Here tuples of head words,
argument words, and grammatical relations are ex-
tracted from the training sections of the WSJ, and
fed into a finite mixture model for clustering gram-
matical relations. The clustering model itself is then
used to yield smoothed probabilities as values for
property functions on head-argument-relation tuples
of LFG parses.
3.2 Discriminative Estimation
Discriminative estimation techniques have recently
received great attention in the statistical machine
learning community and have already been applied
to statistical parsing (Johnson et al, 1999; Collins,
2000; Collins and Duffy, 2001). In discriminative es-
timation, only the conditional relation of an analysis
given an example is considered relevant, whereas in
maximum likelihood estimation the joint probability
of the training data to best describe observations is
maximized. Since the discriminative task is kept in
mind during estimation, discriminative methods can
yield improved performance. In our case, discrimi-
native criteria cannot be defined directly with respect
to ?correct labels? or ?gold standard? parses since
the WSJ annotations are not sufficient to disam-
biguate the more complex LFG parses. However, in-
stead of retreating to unsupervised estimation tech-
niques or creating small LFG treebanks by hand, we
use the labeled bracketing of the WSJ training sec-
tions to guide discriminative estimation. That is, dis-
criminative criteria are defined with respect to the set
of parses consistent with the WSJ annotations.1
The objective function in our approach, denoted
by P (?), is the joint of the negative log-likelihood
?L(?) and a Gaussian regularization term ?G(?)
on the parameters ?. Let {(yj , zj)}mj=1 be a set of
training data, consisting of pairs of sentences y and
partial annotations z, let X(y, z) be the set of parses
for sentence y consistent with annotation z, and let
X(y) be the set of all parses produced by the gram-
mar for sentence y. Furthermore, let p[f ] denote the
expectation of function f under distribution p. Then
P (?) can be defined for a conditional exponential
model p?(z|y) as:
P (?) = ?L(?)?G(?)
= ? log
m?
j=1
p?(zj |yj) +
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
?
X(yj)
e??f(x)
+
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
+
m?
j=1
log
?
X(yj)
e??f(x) +
n?
i=1
?2i
2?2i
.
Intuitively, the goal of estimation is to find model pa-
1An earlier approach using partially labeled data for estimat-
ing stochastics parsers is Pereira and Schabes?s (1992) work on
training PCFG from partially bracketed data. Their approach
differs from the one we use here in that Pereira and Schabes
take an EM-based approach maximizing the joint likelihood of
the parses and strings of their training data, while we maximize
the conditional likelihood of the sets of parses given the corre-
sponding strings in a discriminative estimation setting.
rameters which make the two expectations in the last
equation equal, i.e. which adjust the model param-
eters to put all the weight on the parses consistent
with the annotations, modulo a penalty term from
the Gaussian prior for too large or too small weights.
Since a closed form solution for such parame-
ters is not available, numerical optimization meth-
ods have to be used. In our experiments, we applied
a conjugate gradient routine, yielding a fast converg-
ing optimization algorithm where at each iteration
the negative log-likelihood P (?) and the gradient
vector have to be evaluated.2 For our task the gra-
dient takes the form:
?P (?) =
?
?P (?)
??1
,
?P (?)
??2
, . . . ,
?P (?)
??n
?
, and
?P (?)
??i
= ?
m?
j=1
(
?
x?X(yj ,zj)
e??f(x)fi(x)
?
x?X(yj ,zj)
e??f(x)
?
?
x?X(yj)
e??f(x)fi(x)
?
x?X(yj)
e??f(x)
) +
?i
?2i
.
The derivatives in the gradient vector intuitively are
again just a difference of two expectations
?
m?
j=1
p?[fi|yj , zj ] +
m?
j=1
p?[fi|yj ] +
?i
?2i
.
Note also that this expression shares many common
terms with the likelihood function, suggesting an ef-
ficient implementation of the optimization routine.
4 Experimental Evaluation
4.1 Training
The basic training data for our experiments are sec-
tions 02-21 of the WSJ treebank. As a first step, all
sections were parsed, and the packed parse forests
unpacked and stored. For discriminative estimation,
this data set was restricted to sentences which re-
ceive a full parse (in contrast to a FRAGMENT or
SKIMMED parse) for both its partially labeled and
its unlabeled variant. Furthermore, only sentences
2An alternative numerical method would be a combination
of iterative scaling techniques with a conditional EM algorithm
(Jebara and Pentland, 1998). However, it has been shown exper-
imentally that conjugate gradient techniques can outperform it-
erative scaling techniques by far in running time (Minka, 2001).
which received at most 1,000 parses were used.
From this set, sentences of which a discriminative
learner cannot possibly take advantage, i.e. sen-
tences where the set of parses assigned to the par-
tially labeled string was not a proper subset of the
parses assigned the unlabeled string, were removed.
These successive selection steps resulted in a fi-
nal training set consisting of 10,000 sentences, each
with parses for partially labeled and unlabeled ver-
sions. Altogether there were 150,000 parses for par-
tially labeled input and 500,000 for unlabeled input.
For estimation, a simple property selection pro-
cedure was applied to the full set of around 1000
properties. This procedure is based on a frequency
cutoff on instantiations of properties for the parses
in the labeled training set. The result of this proce-
dure is a reduction of the property vector to about
half its size. Furthermore, a held-out data set was
created from section 24 of the WSJ treebank for ex-
perimental selection of the variance parameter of the
prior distribution. This set consists of 120 sentences
which received only full parses, out of which the
most plausible one was selected manually.
4.2 Testing
Two different sets of test data were used: (i) 700 sen-
tences randomly extracted from section 23 of the
WSJ treebank and given gold-standard f-structure
annotations according to our LFG scheme, and (ii)
500 sentences from the Brown corpus given gold
standard annotations by Carroll et al (1999) accord-
ing to their dependency relations (DR) scheme.3
Annotating the WSJ test set was bootstrapped
by parsing the test sentences using the LFG gram-
mar and also checking for consistency with the
Penn Treebank annotation. Starting from the (some-
times fragmentary) parser analyses and the Tree-
bank annotations, gold standard parses were created
by manual corrections and extensions of the LFG
parses. Manual corrections were necessary in about
half of the cases. The average sentence length of
the WSJ f-structure bank is 19.8 words; the average
number of predicate-argument relations in the gold-
standard f-structures is 31.2.
Performance on the LFG-annotated WSJ test set
3Both corpora are available online. The WSJ f-structure
bank at www.parc.com/istl/groups/nltt/fsbank/, and Carroll et
al.?s corpus at www.cogs.susx.ac.uk/lab/nlp/carroll/greval.html.
was measured using both the LFG and DR metrics,
thanks to an f-structure-to-DR annotation mapping.
Performance on the DR-annotated Brown test set
was only measured using the DR metric.
The LFG evaluation metric is based on the com-
parison of full f-structures, represented as triples
relation(predicate, argument). The predicate-
argument relations of the f-structure for one parse of
the sentence Meridian will pay a premium of $30.5
million to assume $2 billion in deposits. are shown
in Fig. 2.
number($:9, billion:17) number($:24, million:4)
detform(premium:3, a) mood(pay:0, indicative)
tense(pay:0, fut) adjunct(million:4, ?30.5?:28)
adjunct(premium:3, of:23) adjunct(billion:17, ?2?:19)
adjunct($:9, in:11) adjunct(pay:0, assume:7)
obj(pay:0, premium:3) stmttype(pay:0, decl)
subj(pay:0, ?Meridian?:5) obj(assume:7, $:9)
obj(of:23, $:24) subj(assume:7, pro:8)
obj(in:11, deposit:12) prontype(pro:8, null)
stmttype(assume:7, purpose)
Figure 2: LFG predicate-argument relation represen-
tation
The DR annotation for our example sentence, ob-
tained via a mapping from f-structures to Carroll et
al?s annotation scheme, is shown in Fig. 3.
(aux pay will) (subj pay Meridian )
(detmod premium a) (mod million 30.5)
(mod $ million) (mod of premium $)
(dobj pay premium ) (mod billion 2)
(mod $ billion) (mod in $ deposit)
(dobj assume $ ) (mod to pay assume)
Figure 3: Mapping to Carroll et al?s dependency-
relation representation
Superficially, the LFG and DR representations are
very similar. One difference between the annotation
schemes is that the LFG representation in general
specifies more relation tuples than the DR represen-
tation. Also, multiple occurences of the same lex-
ical item are indicated explicitly in the LFG rep-
resentation but not in the DR representation. The
main conceptual difference between the two an-
notation schemes is the fact that the DR scheme
crucially refers to phrase-structure properties and
word order as well as to grammatical relations in
the definition of dependency relations, whereas the
LFG scheme abstracts away from serialization and
phrase-structure. Facts like this can make a correct
mapping of LFG f-structures to DR relations prob-
lematic. Indeed, we believe that we still underesti-
mate by a few points because of DR mapping diffi-
culties. 4
4.3 Results
In our evaluation, we report F-scores for both types
of annotation, LFG and DR, and for three types
of parse selection, (i) lower bound: random choice
of a parse from the set of analyses (averaged over
10 runs), (ii) upper bound: selection of the parse
with the best F-score according to the annotation
scheme used, and (iii) stochastic: the parse selected
by the stochastic disambiguator. The error reduc-
tion row lists the reduction in error rate relative to
the upper and lower bounds obtained by the stochas-
tic disambiguation model. F-score is defined as 2 ?
precision? recall/(precision+ recall).
Table 1 gives results for 700 examples randomly
selected from section 23 of the WSJ treebank, using
both LFG and DR measures.
Table 1: Disambiguation results for 700 randomly
selected examples from section 23 of the WSJ tree-
bank using LFG and DR measures.
LFG DR
upper bound 84.1 80.7
stochastic 78.6 73.0
lower bound 75.5 68.8
error reduction 36 35
The effect of the quality of the parses on disam-
biguation performance can be illustrated by break-
ing down the F-scores according to whether the
parser yields full parses, FRAGMENT, SKIMMED, or
SKIMMED+FRAGMENT parses for the test sentences.
The percentages of test examples which belong to
the respective classes of quality are listed in the first
row of Table 2. F-scores broken down according to
classes of parse quality are recorded in the follow-
4See Carroll et al (1999) for more detail on the DR an-
notation scheme, and see Crouch et al (2002) for more de-
tail on the differences between the DR and the LFG annotation
schemes, as well as on the difficulties of the mapping from LFG
f-structures to DR annotations.
ing rows. The first column shows F-scores for all
parses in the test set, as in Table 1. The second col-
umn shows the best F-scores when restricting atten-
tion to examples which receive only full parses. The
third column reports F-scores for examples which
receive only non-full parses, i.e. FRAGMENT or
SKIMMED parses or SKIMMED+FRAGMENT parses.
Columns 4-6 break down non-full parses according
to examples which receive only FRAGMENT, only
SKIMMED, or only SKIMMED+FRAGMENT parses.
Results of the evaluation on Carroll et al?s Brown
test set are given in Table 3. Evaluation results for
the DR measure applied to the Brown corpus test set
broken down according to parse-quality are shown
in Table 2.
In Table 3 we show the DR measure along with an
evaluation measure which facilitates a direct com-
parison of our results to those of Carroll et al
(1999). Following Carroll et al (1999), we count
a dependency relation as correct if the gold stan-
dard has a relation with the same governor and de-
pendent but perhaps with a different relation-type.
This dependency-only (DO) measure thus does not
reflect mismatches between arguments and modi-
fiers in a small number of cases. Note that since
for the evaluation on the Brown corpus, no heldout
data were available to adjust the variance parame-
ter of a Bayesian model, we used a plain maximum-
likelihood model for disambiguation on this test set.
Table 3: Disambiguation results on 500 Brown cor-
pus examples using DO measure and DR measures.
DO DR
Carroll et al (1999) 75.1 -
upper bound 82.0 80.0
stochastic 76.1 74.0
lower bound 73.3 71.7
error reduction 32 33
5 Discussion
We have presented a first attempt at scaling up a
stochastic parsing system combining a hand-coded
linguistically fine-grained grammar and a stochas-
tic disambiguation model to the WSJ treebank.
Full grammar coverage is achieved by combining
specialized constraint-based parsing techniques for
LFG grammars with partial parsing techniques. Fur-
thermore, a maximal exploitation of treebank anno-
tations for estimating a distribution on fine-grained
LFG parses is achieved by letting grammar analyses
which are consistent with the WSJ labeled bracket-
ing define a gold standard set for discriminative es-
timation. The combined system trained on WSJ data
achieves full grammar coverage and disambiguation
performance of 79% F-score on WSJ data, and 76%
F-score on the Brown corpus test set.
While disambiguation performance of around
79% F-score on WSJ data seems promising, from
one perspective it only offers a 3% absolute im-
provement over a lower bound random baseline.
We think that the high lower bound measure high-
lights an important aspect of symbolic constraint-
based grammars (in contrast to treebank gram-
mars): the symbolic grammar already significantly
restricts/disambiguates the range of possible analy-
ses, giving the disambiguator a much narrower win-
dow in which to operate. As such, it is more appro-
priate to assess the disambiguator in terms of reduc-
tion in error rate (36% relative to the upper bound)
than in terms of absolute F-score. Both the DR and
LFG annotations broadly agree in their measure of
error reduction.
The lower reduction in error rate relative to the
upper bound for DR evaluation on the Brown corpus
can be attributed to a corpus effect that has also been
observed by Gildea (2001) for training and testing
PCFGs on the WSJ and Brown corpora.5
Breaking down results according to parse quality
shows that irrespective of evaluation measure and
corpus, around 4% overall performance is lost due
to non-full parses, i.e. FRAGMENT, or SKIMMED, or
SKIMMED+FRAGMENT parses.
Due to the lack of standard evaluation measures
and gold standards for predicate-argument match-
ing, a comparison of our results to other stochastic
parsing systems is difficult. To our knowledge, so
far the only direct point of comparison is the parser
of Carroll et al (1999) which is also evaluated on
Carroll et al?s test corpus. They report an F-score
5Gildea reports a decrease from 86.1%/86.6% re-
call/precision on labeled bracketing to 80.3%/81% when
going from training and testing on the WSJ to training on the
WSJ and testing on the Brown corpus.
Table 2: LFG F-scores for the 700 WSJ test examples and DR F-scores for the 500 Brown test examples
broken down according to parse quality.
WSJ-LFG all full non-full fragments skimmed skimmed+fragments
% of test set 100 74.7 25.3 20.4 1.4 3.4
upper bound 84.1 88.5 73.4 76.7 70.3 61.3
stochastic 78.6 82.5 69.0 72.4 66.6 56.2
lower bound 75.5 78.4 67.7 71.0 63.0 55.9
Brown-DR all full non-full fragments skimmed skimmed+fragments
% of test set 100 79.6 20.4 20.0 2.0 1.6
upper bound 80.0 84.5 65.4 65.4 56.0 53.5
stochastic 74.0 77.9 61.5 61.5 52.8 50.0
lower bound 71.1 74.8 59.2 59.1 51.2 48.9
of 75.1% for a DO evaluation that ignores predicate
labels, counting only dependencies. Under this mea-
sure, our system achieves 76.1% F-score.
References
Gosse Bouma, Gertjan von Noord, and Robert Malouf.
2000. Alpino: Wide-coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands, Amsterdam, Netherlands.
Miriam Butt, Tracy King, Maria-Eugenia Nin?o, and
Fre?de?rique Segond. 1999. A Grammar Writer?s Cook-
book. Number 95 in CSLI Lecture Notes. CSLI Publi-
cations, Stanford, CA.
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Proceed-
ings of the EACL workshop on Linguistically Inter-
preted Corpora (LINC), Bergen, Norway.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14(NIPS?01), Van-
couver.
Michael Collins. 2000. Discriminative reranking for nat-
ural language processing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML?00), Stanford, CA.
Richard Crouch, Ronald M. Kaplan, Tracy H. King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad-coverage stochastic parser. In Pro-
ceedings of the ?Beyond PARSEVAL? Workshop at the
3rd International Conference on Language Resources
and Evaluation (LREC?02), Las Palmas, Spain.
Dan Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Pittsburgh, PA.
Tony Jebara and Alex Pentland. 1998. Maximum con-
ditional likelihood via bound maximization and the
CEM algorithm. In Advances in Neural Information
Processing Systems 11 (NIPS?98).
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?99), College Park, MD.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Thomas Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Department of Statis-
tics, Carnegie Mellon University.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics (ACL?92),
Newark, Delaware.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized Stochastic Modeling of
Constraint-Based Grammars using Log-Linear Mea-
sures and EM Training. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hong Kong.
Incremental Feature Selection and `1 Regularization
for Relaxed Maximum-Entropy Modeling
Stefan Riezler and Alexander Vasserman
Palo Alto Research Center
3333 Coyote Hill Road, Palo Alto, CA 94304
Abstract
We present an approach to bounded constraint-
relaxation for entropy maximization that corre-
sponds to using a double-exponential prior or `1 reg-
ularizer in likelihood maximization for log-linear
models. We show that a combined incremental fea-
ture selection and regularization method can be es-
tablished for maximum entropy modeling by a nat-
ural incorporation of the regularizer into gradient-
based feature selection, following Perkins et al
(2003). This provides an efficient alternative to stan-
dard `1 regularization on the full feature set, and
a mathematical justification for thresholding tech-
niques used in likelihood-based feature selection.
Also, we motivate an extension to n-best feature
selection for linguistic features sets with moderate
redundancy, and present experimental results show-
ing its advantage over `0, 1-best `1, `2 regularization
and over standard incremental feature selection for
the task of maximum-entropy parsing.1
1 Introduction
The maximum-entropy (ME) principle, which pre-
scribes choosing the model that maximizes the en-
tropy out of all models that satisfy given feature
constraints, can be seen as a built-in regularization
mechanism that avoids overfitting the training data.
However, it is only a weak regularizer that cannot
avoid overfitting in situations where the number of
training examples is significantly smaller than the
number of features. In such situations some fea-
tures will occur zero times on the training set and
receive negative infinity weights, causing the as-
signment of zero probabilities for inputs including
those features. Similar assignment of (negative) in-
finity weights happens to features that are pseudo-
minimal (or pseudo-maximal) on the training set
(see Johnson et al (1999)), that is, features whose
value on correct parses always is less (or greater)
1This research has been funded in part by contract
MDA904-03-C-0404 of the Advanced Research and Develop-
ment Activity, Novel Intelligence from Massive Data program.
than or equal to their value on all other parses. Also,
if large features sets are generated automatically
from conjunctions of simple feature tests, many fea-
tures will be redundant. Besides overfitting, large
feature sets also create the problem of increased
time and space complexity.
Common techniques to deal with these problems
are regularization and feature selection. For ME
models, the use of an `2 regularizer, corresponding
to imposing a Gaussian prior on the parameter val-
ues, has been proposed by Johnson et al (1999) and
Chen and Rosenfeld (1999). Feature selection for
ME models has commonly used simple frequency-
based cut-off, or likelihood-based feature induction
as introduced by Della Pietra et al (1997). Whereas
`2 regularization produces excellent generalization
performance and effectively avoids numerical prob-
lems, parameter values almost never decrease to
zero, leaving the problem of inefficient computa-
tion with the full feature set. In contrast, feature se-
lection methods effectively decrease computational
complexity by selecting a fraction of the feature
set for computation; however, generalization per-
formance suffers from the ad-hoc character of hard
thresholds on feature counts or likelihood gains.
Tibshirani (1996) proposed a technique based on
`1 regularization that embeds feature selection into
regularization such that both a precise assessment of
the reliability of features and the decision about in-
clusion or deletion of features can be done in the
same framework. Feature sparsity is produced by
the polyhedral structure of the `1 norm which ex-
hibits a gradient discontinuity at zero that tends to
force a subset of parameter values to be exactly
zero at the optimum. Since this discontinuity makes
optimization a hard numerical problem, standard
gradient-based techniques for estimation cannot be
applied directly. Tibshirani (1996) presents a spe-
cialized optimization algorithm for `1 regularization
for linear least-squares regression called the Lasso
algorithm. Goodman (2003) and Kazama and Tsujii
(2003) employ standard iterative scaling and con-
jugate gradient techniques, however, for regulariza-
tion a simplified one-sided exponential prior is em-
ployed which is non-zero only for non-negative pa-
rameter values. In these approaches the full fea-
ture space is considered in estimation, so savings
in computational complexity are gained only in ap-
plications of the resulting sparse models. Perkins
et al (2003) presented an approach that combines
`1 based regularization with incremental feature se-
lection. Their basic idea is to start with a model in
which almost all weights are zero, and iteratively
decide, by comparing regularized feature gradients,
which weight should be adjusted away from zero
in order to decrease the regularized objective func-
tion by the maximum amount. The `1 regularizer is
thus used directly for incremental feature selection,
which on the one hand makes feature selection fast,
and on the other hand avoids numerical problems
for zero-valued weights since only non-zero weights
are included in the model. Besides the experimental
evidence presented in these papers, recently a theo-
retical account on the superior sample complexity of
`1 over `2 regularization has been presented by Ng
(2004), showing logarithmic versus linear growth in
the number of irrelevant features for `1 versus `2
regularized logistic regression.
In this paper, we apply `1 regularization to log-
linear models, and motivate our approach in terms
of maximum entropy estimation subject to relaxed
constraints. We apply the gradient-based feature se-
lection technique of Perkins et al (2003) to our
framework, and improve its computational com-
plexity by an n-best feature inclusion technique.
This extension is tailored to linguistically motivated
feature sets where the number of irrelevant features
is moderate. In experiments on real-world data from
maximum-entropy parsing, we show the advantage
of n-best `1 regularization over `2, `1, `0 regulariza-
tion and standard incremental feature selection in
terms of better computational complexity and im-
proved generalization performance.
2 `p Regularizers for Log-Linear Models
Let p?(x|y) = e
?n
i=1 ?ifi(x,y)
?
x e
?n
i=1 ?ifi(x,y)
be a conditional
log-linear model defined by feature functions f and
log-parameters ?. For data {(xj , yj)}mj=1, the objec-
tive function to be minimized in `p regularization of
the negative log-likelihood L(?) is
C(?) = L(?) + ?p(?)
= ?
1
m
m?
j=1
ln p?(xj |yj) + ????
p
p
The regularizer family ?p(?) is defined by the
Minkowski `p norm of the parameter vector ?
raised to the pth power, i.e. ???pp =
?n
i=1 |?i|
p
.
The essence of this regularizer family is to penalize
overly large parameter values. If p = 2, the regu-
larizer corresponds to a zero-mean Gaussian prior
distribution on the parameters with ? corresponding
to the inverse variance of the Gaussian. If p = 0,
the regularizer is equivalent to setting a limit on the
maximum number of non-zero weights. In our ex-
periments we replace `0 regularization by the re-
lated technique of frequency-based feature cutoff.
`1 regularization is defined by the case where
p = 1. Here parameters are penalized in the sum
of their absolute values, which corresponds to ap-
plying a zero-mean Laplacian or double exponential
prior distribution of the form
p(?i) =
1
2?
e?
|?i|
?
with ? = 1? being proportional to the inverse stan-
dard deviation
?
2? . In contrast to the Gaussian, the
Laplacian prior puts more mass near zero (and in
the tails), thus tightening the prior by decreasing
the standard deviation ? provides stronger regular-
ization against overfitting and produces more zero-
valued parameter estimates. In terms of `1-norm
regularization, feature sparsity can be explained by
the following observation: Since every non-zero pa-
rameter weight incurs a regularizer penalty of ?|?i|,
its contribution to minimizing the negative log-
likelihood has to outweigh this penalty. Thus param-
eters values where the gradient at ? = 0 is
?
?
?
?
?L(?)
??i
?
?
?
? ? ? (1)
can be kept zero without changing the optimality of
the solution.
3 Bounded Constraint Relaxation for
Maximum Entropy Estimation
As shown by Lebanon and Lafferty (2001), in terms
of convex duality, a regularization term for the dual
problem corresponds to a ?potential? on the con-
straint values in the primal problem. For a dual
problem of regularized likelihood estimation for
log-linear models, the corresponding primal prob-
lem is a maximum entropy problem subject to re-
laxed constraints. Let H(p) denote the entropy with
respect to probability function p, and g : IRn ? IR
be a convex potential function, and p?[?] and p[?] be
expectations with respect to the empirical distribu-
tion p?(x, y) = 1m
?m
j=1 ?(xj , x)?(yj , y) and the
model distribution p(x|y)p?(y). The primal problem
can then be stated as
Maximize H(p)? g(c) subject to
p[fi]? p?[fi] = ci, i = 1, . . . , n
Constraint relaxation is achieved in that equality of
the feature expectations is not enforced, but a certain
amount of overshooting or undershooting is allowed
by a parameter vector c ? IRn whose potential is de-
termined by a convex function g(c) that is combined
with the entropy term H(p).
In the case of `2 regularization, the potential func-
tion for the primal problem is a quadratic penalty
of the form 12?
?
i c
2
i for ? = 1?2i , i = 1, . . . , n(Lebanon and Lafferty, 2001). In order to recover
the specific form of the primal problem for our case,
we have to start from the given dual problem. Fol-
lowing Lebanon and Lafferty (2001), the dual func-
tion for regularized estimation can be expressed in
terms of the dual function ?(p?,?) for the unregu-
larized case and the convex conjugate g?(?) of the
potential function g(c). In our case the negative of
?(p?,?) corresponds to the likelihood term L(?),
and the negative of the convex conjugate g?(?) is
the `1 regularizer. Thus our dual problem can be
stated as
?? = argmax
?
?(p?,?)? g
?(?)
= argmin
?
L(?) + ????11
Since for convex and closed functions, the con-
jugate of the conjugate is the original function, i.e.
g?? = g (Boyd and Vandenberghe, 2004), the poten-
tial function g(c) for the primal problem can be re-
covered by calculating the conjugate g?? of the con-
jugate g?(?) = ????11. In our case, we get
g??(c) = g(c) =
{
0 ?c?? ? ?
? otherwise (2)
where ?c?? = max{|c1|, . . . , |cn|}. A proof for
this proposition is given in the Appendix. The re-
sulting potential function g(c) is the indicator func-
tion on the interval [??, ?]. That is, it restricts the
allowable amount of constraint relaxation to at most
??. From this perspective, increasing ? means to al-
low for more slack in constraint satisfaction, which
in turn allows to fit a more uniform, less overfit-
ting distribution to the data. For features that are in-
cluded in the model, the parameter values have to be
adjusted away from zero to meet the constraints
|p[fi]? p?[fi]| ? ?, i = 1, . . . , n (3)
Initialization: Initialize selected features S to ?, and
zero-weighted features Z to the full feature set,
yielding the uniform distribution p?(0),S(0) .
n-best grafting: For steps t = 1, . . . , T ,
(1) for all features fi in Z(t?1), calculate
?
?
?
?
?
?L(?(t?1), S(t?1))
??i
?
?
?
?
?
> ?,
(2) S(t) := S(t?1) ?N (t) and Z(t) := Z(t?1) \
N (t) where N (t) is the set of n-best features
passing the test in (1),
(3) perform conjugate gradient optimization to
find the optimal model p??,S(t) where ? is
initialized at ?(t?1), and ?(t) := ?? =
argmax
?
C(?, S(t)).
Stopping condition: Stop if for all fi in Z(t?1):
?
?
?
?
?
?L(?(t?1), S(t?1))
??i
?
?
?
?
?
? ?
Figure 1: n-best gradient feature testing
For features that meet the constraints without pa-
rameter adjustment, parameter values can be kept at
zero, effectively discarding the features. Note that
equality of equations 3 and 1 connects the maxi-
mum entropy problem to likelihood regularization.
4 Standardization
Note that the ?p regularizer presented above penal-
izes the model parameters uniformly, correspond-
ing to imposing a uniform variance onto all model
parameters. This motivates a normalization of in-
put data to the same scale. A standard technique
to achieve this is to linearly rescale each feature
count to zero mean and standard deviation of one
over all training data. The same rescaling has to be
done for training and application of the model to un-
seen data. As we will see in the experimental evalua-
tion presented below, a standardization of input data
can also dramatically improve convergence behav-
ior in unregularized optimization . Furthermore, pa-
rameter values estimated from standardized feature
counts are directly interpretable to humans. Com-
bined with feature selection, interpretable parame-
ter weights are particularly useful for error analysis
of the model?s feature design.
5 Incremental n-best Feature Selection
The basic idea of the ?grafting? (for ?gradient fea-
ture testing?) algorithm presented by (Perkins et al,
2003) is to assume a tendency of `1 regularization
to produce a large number of zero-valued param-
eters at the function?s optimum, thus to start with
all-zero weights, and incrementally add features to
the model only if adjusting their parameter weights
away from zero sufficiently decreases the optimiza-
tion criterion. This idea allows for efficient, incre-
mental feature selection, and at the same time avoids
numerical problems caused by the discontinuity of
the gradient in `1 regularization. Furthermore, the
regularizer is incorporated directly into a criterion
for feature selection, based on the observation made
above: It only makes sense to add a feature to the
model if the regularizer penalty is outweighed by
the reduction in negative log-likelihood. Thus fea-
tures considered for selection have to pass the fol-
lowing test:
?
?
?
?
?L(?)
??i
?
?
?
? > ?
In the grafting procedure suggested by (Perkins
et al, 2003), this gradient test is applied to each fea-
ture, and at each step the feature passing the test
with maximum magnitude is added to the model.
Adding one feature at a time effectively discards
noisy and irrelevant features, however, the overhead
introduced by grafting can outweigh the gain in ef-
ficiency if there is a moderate number of noisy and
truly redundant features. In such cases, it is bene-
ficial to add a number of n > 1 features at each
step, where n is adjusted by cross-validation or on a
held-out data set. In the experiments on maximum-
entropy parsing presented below, a feature set of lin-
guistically motivated features is used that exhibits
only a moderate amount of redundancy. We will see
that for such cases, n-best feature selection consid-
erably improves computational complexity, and also
achieves slightly better generalization performance.
After adding n ? 1 features to the model in
a grafting step, the model is optimized with re-
spect to all parameters corresponding to currently
included features. This optimization is done by call-
ing a gradient-based general purpose optimization
routine for the regularized objective function. We
use a conjugate gradient routine for this purpose
(Minka, 2001; Malouf, 2002)2. The gradient of our
criterion with respect to a parameter ?i is:
?C(?)
??i
=
1
m
m?
k=1
?L(?)
??i
+ ? sign(?i)
2Note that despite gradient feature testing, the parameters
for some features can be driven to zero in conjugate gradient
optimization of the `1-regularized objective function. Care has
to be taken to catch those features and prune them explicitly to
avoid numerical instability.
The sign of ?i decides if ? is added or subtracted
from the gradient for feature fi. For a feature that
is newly added to the model and thus has weight
?i = 0, we use the feature gradient test to determine
the sign. If ?L(?)??i > ?, we know that
?C(?)
??i
> 0,
thus we let sign(?i) = ?1 in order to decrease C.
Following the same rationale, if ?L(?)??i < ?? we
set sign(?i) = +1. An outline of an n-best grafting
algorithm is given in Fig. 1.
6 Experiments
6.1 Train and Test Data
In the experiments presented in this paper, we eval-
uate `2, `1, and `0 regularization on the task of
stochastic parsing with maximum-entropy models
For our experiments, we used a stochastic parsing
system for LFG that we trained on section 02-21
of the UPenn Wall Street Journal treebank (Mar-
cus et al, 1993) by discriminative estimation of a
conditional maximum-entropy model from partially
labeled data (see Riezler et al (2002)). For esti-
mation and best-parse searching, efficient dynamic-
programming techniques over features forests are
employed (see Kaplan et al (2004)). For the setup
of discriminative estimation from partially labeled
data, we found that a restriction of the training data
to sentences with a relatively low ambiguity rate
was possible at no loss in accuracy compared to
training from all sentences. Furthermore, data were
restricted to sentences of which a discriminative
learner can possibly take advantage, i.e. sentences
where the set of parses assigned to the labeled string
is a proper subset of the parses assigned to the un-
labeled string. Together with a restriction to exam-
ples that could be parsed by the full grammar and
did not have to use a backoff mechanism of frag-
ment parses, this resulted in a training set of 10,000
examples with at most 100 parses. Evaluation was
done on the PARC 700 dependency bank3, which
is an LFG annotation of 700 examples randomly
extracted from section 23 of the UPenn WSJ tree-
bank. To tune regularization parameters, we split the
PARC 700 into a heldout and test set of equal size.
6.2 Feature Construction
Table 1 shows the 11 feature templates that were
used in our experiments to create 60, 109 features.
On the around 300,000 parses for 10,000 sentences
in our final training set, 10, 986 features were active,
resulting in a matrix of active features times parses
that has 66 million non-zero entries. The scale of
this experiment is comparable to experiments where
3http://www2.parc.com/istl/groups/nltt/fsbank/
Table 1: Feature templates
name parameters activation condition
Local Templates
cs label label constituent label is present in parse
cs adj label parent label, constituent child label is
child label child of constituent parent label
cs right branch constituent has right child
cs conj nonpar depth non-parallel conjuncts within depth levels
fs attrs attrs f-structure attribute is one of attrs
fs attr value attr, value attribute attr has value value
fs attr subsets attr sum of cardinalities of subsets of attr
lex subcat pred, args sets verb pred has one of args sets as arguments
Non-Local (Top-Down) Templates
cs embedded label, size chain of size constituents
labeled label embedded into one another
cs sub label ancestor label, constituent descendant label
descendant label is descendant of ancestor label
fs aunt subattr aunts, parents, one of descendants is descendant of one of
descendants parents which is a sister of one of aunts
much larger, but sparser feature sets are employed4.
The reason why the matrix of non-zeroes is less
sparse in our case is that most of our feature tem-
plates are instantiated to linguistically motivated
cases, and only a few feature templates encode all
possible conjunctions of simple feature tests. Re-
dundant features are introduced mostly by the lat-
ter templates, whereas the former features are gen-
eralizations over possible combinations of grammar
constants. We conjecture that feature sets like this
are typical for natural language applications.
Efficient feature detection is achieved by a com-
bination of hashing and dynamic programming on
the packed representation of c- and f-structures
(Maxwell and Kaplan, 1993). Features can be de-
scribed as local and non-local, depending on the size
of the graph that has to be traversed in their compu-
tation. For each local template one of the parame-
ters is selected as a key for hashing. Non-local fea-
tures are treated as two (or more) local sub-features.
Packed structures are traversed depth-first, visiting
each node only once. Only the features keyed on
the label of the current node are considered for
matching. For each non-local feature, the contexts
of matching subfeatures are stored at the respective
nodes, propagated upward in dynamic programing
fashion, and conjoined with contexts of other sub-
features of the feature. Fully matched features are
associated with the corresponding contexts resulting
in a feature-annotated and/or-forest. This annotated
4For example, Malouf (2002) reports a matrix of non-zeroes
that has 55 million entries for a shallow parsing experiment
where 260,000 features were employed.
and/or forest is exploited for dynamic programming
computation in estimation and best parse selection.
6.3 Experimental Results
Table 2 shows the results of an evaluation of five
different systems of the test split of the PARC 700
dependency bank. The presented systems are unreg-
ularized maximum-likelihood estimation of a log-
linear model including the full feature set (mle),
standardized maximum-likelihood estimation as de-
scribed in Sect. 4 (std), `0 regularization using
frequency-based cutoff, `1 regularization using n-
best grafting, and `2 regularization using a Gaus-
sian prior. All `p regularization runs use a standard-
ization of the feature space. Special regularization
parameters were adjusted on the heldout split, re-
sulting in a cutoff threshold of 16, and penaliza-
tion factors of 20 and 100 for `1 and `2 regular-
ization respectively, with an optimal choice of 100
features to be added in each n-best grafting step.
Performance of these systems is evaluated firstly
with respect to F-score on matching dependency re-
lations. Note that the F-score values on the PARC
700 dependency bank range between a lower bound
of 68.0% for averaging over all parses and an upper
bound of 83.6% for the parses producing the best
possible matches. Furthermore, compression of the
full feature set by feature selection, number of con-
jugate gradient iterations, and computation time (in
hours:minutes of elapsed time) are reported.5
5All experiments were run on one CPU of a dual processor
AMD Opteron 244 with 1.8GHz clock speed and 4GB of main
memory.
Table 2: F-score, compression, number of iterations,
and elapsed time for unregularized and standardized
maximum-likelihood estimation, and `0, `1, and `2
regularization on test split of PARC 700 dependency
bank.
mle std `0 `2 `1
F-score 77.9 78.1 78.1 78.9 79.3
compr. 0 0 18.4 0 82.7
cg its. 761 371 372 34 226
time 129:12 66:41 60:47 6:19 5:25
Unregularized maximum-likelihood estimation
using the full feature set exhibits severe overtraining
problems, as the relation of F-score to the number
of conjugate gradient iterations shows. Standard-
ization of input data can alleviate this problem by
improving convergence behavior to half the num-
ber of conjugate gradient iterations. `0 regulariza-
tion achieves its maximum on the heldout data for a
threshold of 16, which results in an estimation run
that is slightly faster than standardized estimation
using all features, due to a compression of the full
feature set by 18%. `2 regularization benefits from
a very tight prior (standard deviation of 0.1 corre-
sponding to penalty 100) that was chosen on the
heldout set. Despite the fact that no reduction of the
full feature set is achieved, this estimation run in-
creases the F-score to 78.9% and improves compu-
tation time by a factor of 20 compared to unregular-
ized estimation using all features. `1 regularization
for n-best grafting, however, even improves upon
this result by increasing the F-score to 79.3%, fur-
ther decreasing computation time to 5:25 hours, at a
compression of the full feature set of 83%.
77.5
78
78.5
79
79.5
1 10 100 1000 10000
10
100
1000
F-score Num CG Iterations
Features Added At Each Step
F-score
3
3
3
3
3
3
3
Num CG Iterations
+
+
+
+
+
+
+
Figure 2: n-best grafting with n of features added
at each step plotted against F-score on test set and
conjugate gradient iterations.
As shown in Fig. 2, for feature selection from lin-
guistically motivated feature sets with only a mod-
erate amount of truly redundant features, it is crucial
to choose the right number n of features to be added
in each grafting step. The number of conjugate gra-
dient iterations decreases rapidly in the number of
features added at each step, whereas F-score evalu-
ated on the test set does not decrease (or increases
slightly) until more than 100 features are added in
each step. 100-best grafting thus reduces estimation
time by a factor of 10 at no loss in F-score compared
to 1-best grafting. Further increasing n results in a
significant drop in F-score, while smaller n is com-
putationally expensive, and also shows slight over-
training effects.
Table 3: F-score, compression, number of itera-
tions, and elapsed time for gradient-based incre-
mental feature selection without regularization, and
with `2, and `1 regularization on test split of PARC
700 dependency bank.
mle-ifs `2-ifs `1
F-score 78.8 79.1 79.3
compr. 88.1 81.7 82.7
cg its. 310 274 226
time 6:04 6:56 5:25
In another experiment we tried to assess the rel-
ative contribution of regularization and incremental
feature selection to the `1-grafting technique. Re-
sults of this experiments are shown in Table 3. In
this experiment we applied incremental feature se-
lection using the gradient test described above to un-
regularized maximum-likelihood estimation (mle-
ifs) and `2-regularized maximum-likelihood estima-
tion (`2-ifs). Threshold parameters ? are adjusted
on the heldout set, in addition to and independent
of regularization parameters such as the variance
of the Gaussian prior. Results are compared to `1-
regularized grafting as presented above. For all runs
a number of 100 features to be added in each graft-
ing step is chosen. The best result for the mle-ifs run
is achieved at a threshold of 25, yielding an F-score
of 78.8%. This shows that incremental feature se-
lection is a powerful tool to avoid overfitting. A fur-
ther improvement in F-score to 79.1% is achieved
by combining incremental feature selection with the
`2 regularizer at a variance of 0.1 for the Gaussian
prior and a threshold of 15. Both runs provide ex-
cellent compression rates and convergence times.
However, they are still outperformed by the `1 run
that achieves a slight improvement in F-score to
79.3% and a slightly better runtime. Furthermore,
by integrating regularization naturally into thresh-
olding for feature selection, a separate thresholding
parameter is avoided in `1-based incremental fea-
ture selection.
A theoretical account of the savings in com-
putational complexity that can be achieved by n-
best grafting can be given as follows. Perkins et
al. (2003) assess the computational complexity for
standard gradient-based optimization with the full
feature set by ? cmp2? , for a multiple c of p line
minimizations for p derivatives over m data points,
each of which has cost ? . In contrast, for grafting,
the cost is assessed by adding up the costs for fea-
ture testing and optimization for s grafting steps as
? (msp+13cms
3)? . For n-best grafting as proposed
in this paper, the number of steps can be decom-
posed into s = n ? t for n features added at each
of t steps. This results in a cost of ? mtp for fea-
ture testing, and ? 13cmn
2t3? for optimization. If
we assume that t  n  s, this indicates consid-
erable savings compared to both 1-best grafting and
standard gradient-based optimization.
7 Discussion and Conclusion
A related approach to `1 regularization and
constraint-relaxation for maximum-entropy mod-
eling has been presented by Kazama and Tsujii
(2003). In this approach, constraint relaxation is
done by allowing two-sided inequality constraints
?Bi ? p?[fi]? p[fi] ? Ai, Ai, Bi > 0
in entropy maximization. The dual function is the
regularized likelihood function
1
m
m?
j=1
p???(xj |yj)?
n?
i=1
?iAi ?
n?
i=1
?iBi
where the two parameter vectors ? and ? replace
our parameter vector ?, and ?i, ?i ? 0. This reg-
ularizer corresponds to a simplification of double-
sided exponentials to a one-sided exponential dis-
tribution which is non-zero only for non-negative
parameters. The use of one-sided exponential pri-
ors for log-linear models has also been proposed
by Goodman (2003), however, without a motiva-
tion in a maximum entropy framework. The fact that
Kazama and Tsujii (2003) allow for lower and up-
per bounds of different size requires the parameter
space to be doubled in their approach. Furthermore,
similar to Goodman (2003), the requirement to work
with a one-sided strictly positive exponential dis-
tribution makes it necessary to double the feature
space to account for (dis)preferences in terms of
strictly positive parameter values. These are consid-
erable computational and implementational disad-
vantages of these approaches. More importantly, an
integration of `1 regularization into incremental fea-
ture selection was not considered.
Incremental feature selection has been proposed
firstly by Della Pietra et al (1997) in a likelihood-
based framework. In this approach, an approximate
gain in likelihood for adding a feature to the model
is used as feature selection criterion, and thresholds
on this gain are used as stopping criterion. Maxi-
mization of approximate likelihood gains and gra-
dient feature testing both are greedy approxima-
tions to the true gain in the objective function -
grafting can be seen as applying one iteration of
Newton?s method, where the weight of the newly
added feature is initialized at 0, to calculate the ap-
proximate likelihood gain. Efficiency and accuracy
of both approaches are comparable, however, the
grafting framework provides a well-defined mathe-
matical basis for feature selection and optimization
by incorporating selection thresholds naturally as
penalty factors of the regularizer. The idea of adding
n-best features in each selection step also has been
investigated earlier in the likelihood-based frame-
work (see for example McCallum (2003)). How-
ever, the possible improvements in computational
complexity and generalization performance due to
n-best selection were not addressed explicitly. Fur-
ther improvements of efficiency of grafting are pos-
sible by applying Zhou et al?s (2003) technique of
restricting feature selection in each step to the top-
ranked features from previous stages.
In sum, we presented an application of `1 regu-
larization to likelihood maximization for log-linear
models that has a simple interpretation as bounded
constraint relaxation in terms of maximum entropy
estimation. The presented n-best grafting method
does not require specialized algorithms or simplifi-
cations of the prior, but allows for an efficient, math-
ematically well-defined combination of feature se-
lection and regularization. In an experimental eval-
uation, we showed n-best grafting to outperform `0,
1-best `1, `2 regularization and standard incremen-
tal feature selection in terms of computational effi-
ciency and generalization performance.
References
Stephen Boyd and Lieven Vandenberghe. 2004.
Convex Optimization. Cambridge University
Press.
Stanley F. Chen and Ronald Rosenfeld. 1999.
A gaussian prior for smoothing maximum en-
tropy models. Technical Report CMU-CS-99-
108, Carnegie Mellon University, Pittsburgh, PA.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random
fields. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 19(4):380?393.
Joshua Goodman. 2003. Exponential priors
for maximum entropy models. Unpublished
Manuscript, Microsoft Research, Redmont, WA.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Pro-
ceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?99),
College Park, MD.
Ronald M. Kaplan, Stefan Riezler, Tracy H. King,
John T. Maxwell III, and Alexander Vasserman.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of the Human
Language Technology conference / North Ameri-
can chapter of the Association for Computational
Linguistics annual meeting (HLT/NAACL?04),
Boston, MA.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Eval-
uation and extension of maximum entropy mod-
els with inequality constraints. In Proceedings of
EMNLP?03, Sapporo, Japan.
Guy Lebanon and John Lafferty. 2001. Boosting
and maximum likelihood for exponential models.
In Advances in Neural Information Processing 14
(NIPS?01), Vancouver.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In
Proceedings of Computational Natural Language
Learning (CoNLL?02), Taipei, Taiwan.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of english: The Penn
treebank. Computational Linguistics, 19(2):313?
330.
John Maxwell and Ron Kaplan. 1993. The inter-
face between phrasal and functional constraints.
Computational Linguistics, 19(4):571?589.
Andrew McCallum. 2003. Efficiently inducing fea-
tures of conditional random fields. In Proceed-
ings of the 19th Conference on Uncertainty in Ar-
tificial Intelligence (UAI?03), Acapulco, Mexico.
Thomas Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Department of
Statistics, Carnegie Mellon University.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2
regularization, and rotational invariance. In Pro-
ceedings of the 21st International Conference on
Machine Learning, Banff, Canada.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: Fast, incremetal feature selection
by gradient descent in function space. Machine
Learning, 3:1333?1356.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Jour-
nal using a Lexical-Functional Grammar and dis-
criminative estimation techniques. In Proceed-
ings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?02),
Philadelphia, PA.
Robert Tibshirani. 1996. Regression shrinkage and
selection via the lasso. Journal of the Royal Sta-
tistical Society. Series B, 58(1):267?288.
Yaqian Zhou, Fuliang Weng, Lide Wu, and Hauke
Schmidt. 2003. A fast algorithm for feature se-
lection in conditional maximum entropy mod-
eling. In Proceedings of EMNLP?03, Sapporo,
Japan.
Appendix: Proof of Proposition 2
Following Boyd and Vandenberghe (2004), the con-
vex conjugate of function g : IRn ? IR is
g?(w) = sup
u
{
n?
i=1
wiui ? g(u)}
and the dual norm ? ? ?? of norm ? ? ? on IRn is
?w?? = sup
u
{
n?
i=1
wiui| ?u? ? 1} (4)
and the dual norm of the `1 norm is the `? norm
?w?? = ?w?? for ?u? = ?u?11 (5)
We show that the convex conjugate of
g(u) = ??u?11, for ? > 0
is g?(w) =
{
0 ?w?? ? ?
? otherwise
Proof. Let ?w?? ? ?, then
?
i wiui ?
?u?11?w?? (from 4 and 5)? ?u?11? (since ?w?? ?
?). Then ?i wiui ? ?u?11? ? 0 and u = 0 maxi-
mizes it with maximum value g?(w) = 0.
Let ?w?? > ?, then ?z s.t. ?z?11 ? 1 and?
i wizi > ? (from 4 and 5). For u = tz, let t ?
?, then
?
i wiui???u?
1
1 = t(
?
i wizi???z?
1
1) ?
? (since?i wizi? ??z?11 > 0), thus g?(w) = ?.
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 57?64, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
On Some Pitfalls in Automatic Evaluation and Significance Testing for MT
Stefan Riezler and John T. Maxwell III
Palo Alto Research Center
3333 Coyote Hill Road, Palo Alto, CA 94304
Abstract
We investigate some pitfalls regarding the
discriminatory power of MT evaluation
metrics and the accuracy of statistical sig-
nificance tests. In a discriminative rerank-
ing experiment for phrase-based SMT we
show that the NIST metric is more sensi-
tive than BLEU or F-score despite their in-
corporation of aspects of fluency or mean-
ing adequacy into MT evaluation. In an
experimental comparison of two statistical
significance tests we show that p-values
are estimated more conservatively by ap-
proximate randomization than by boot-
strap tests, thus increasing the likelihood
of type-I error for the latter. We point
out a pitfall of randomly assessing signif-
icance in multiple pairwise comparisons,
and conclude with a recommendation to
combine NIST with approximate random-
ization, at more stringent rejection levels
than is currently standard.
1 Introduction
Rapid and accurate detection of result differences is
crucial in system development and system bench-
marking. In both situations a multitude of systems
or system variants has to be evaluated, so it is highly
desirable to employ automatic evaluation measures
for detection of result differences, and statistical hy-
pothesis tests to assess the significance of the de-
tected differences. When evaluating subtle differ-
ences between system variants in development, or
when benchmarking multiple systems, result differ-
ences may be very small in magnitude. This imposes
strong requirements on both automatic evaluation
measures and statistical significance tests: Evalua-
tion measures are needed that have high discrimi-
native power and yet are sensitive to the interesting
aspects of the evaluation task. Significance tests are
required to be powerful and yet accurate, i.e., if there
are significant differences they should be able to as-
sess them, but not if there are none.
In the area of statistical machine translation
(SMT), recently a combination of the BLEU evalua-
tion metric (Papineni et al, 2001) and the bootstrap
method for statistical significance testing (Efron and
Tibshirani, 1993) has become popular (Och, 2003;
Kumar and Byrne, 2004; Koehn, 2004b; Zhang et
al., 2004). Given the current practice of reporting
result differences as small as .3% in BLEU score,
assessed at confidence levels as low as 70%, ques-
tions arise concerning the sensitivity of the em-
ployed evaluation metrics and the accuracy of the
employed significance tests, especially when result
differences are small. We believe that is important to
accurately detect such small-magnitude differences
in order to understand how to improve systems and
technologies, even though such differences may not
matter in current applications.
In this paper we will investigate some pitfalls that
arise in automatic evaluation and statistical signifi-
cance testing in MT research. The first pitfall con-
cerns the discriminatory power of automatic eval-
uation measures. In the following, we compare the
sensitivity of three intrinsic evaluation measures that
differ with respect to their focus on different aspects
57
of translation. We consider the well-known BLEU
score (Papineni et al, 2001) which emphasizes flu-
ency by incorporating matches of high n-grams. Fur-
thermore, we consider an F-score measure that is
adapted from dependency-based parsing (Crouch et
al., 2002) and sentence-condensation (Riezler et al,
2003). This measure matches grammatical depen-
dency relations of parses for system output and ref-
erence translations, and thus emphasizes semantic
aspects of translational adequacy. As a third mea-
sure we consider NIST (Doddington, 2002), which
favors lexical choice over word order and does not
take structural information into account. On an ex-
perimental evaluation on a reranking experiment we
found that only NIST was sensitive enough to de-
tect small result differences, whereas BLEU and F-
score produced result differences that were statisti-
cally not significant. A second pitfall addressed in
this paper concerns the relation of power and ac-
curacy of significance tests. In situations where the
employed evaluation measure produces small result
differences, the most powerful significance test is
demanded to assess statistical significance of the re-
sults. However, accuracy of the assessments of sig-
nificance is seldom questioned. In the following,
we will take a closer look at the bootstrap test and
compare it with the related technique of approxi-
mate randomization (Noreen (1989)). In an exper-
imental evaluation on our reranking data we found
that approximate randomization estimated p-values
more conservatively than the bootstrap, thus increas-
ing the likelihood of type-I error for the latter test.
Lastly, we point out a common mistake of randomly
assessing significance in multiple pairwise compar-
isons (Cohen, 1995). This is especially relevant in
k-fold pairwise comparisons of systems or system
variants where k is high. Taking this multiplicity
problem into account, we conclude with a recom-
mendation of a combination of NIST for evaluation
and the approximate randomization test for signifi-
cance testing, at more stringent rejection levels than
is currently standard in the MT literature. This is es-
pecially important in situations where multiple pair-
wise comparisons are conducted, and small result
differences are expected.
2 The Experimental Setup: Discriminative
Reranking for Phrase-Based SMT
The experimental setup we employed to compare
evaluation measures and significance tests is a dis-
criminative reranking experiment on 1000-best lists
of a phrase-based SMT system. Our system is a
re-implementation of the phrase-based system de-
scribed in Koehn (2003), and uses publicly avail-
able components for word alignment (Och and Ney,
2003)1, decoding (Koehn, 2004a)2, language mod-
eling (Stolcke, 2002)3 and finite-state processing
(Knight and Al-Onaizan, 1999)4. Training and test
data are taken from the Europarl parallel corpus
(Koehn, 2002)5.
Phrase-extraction follows Och et al (1999) and
was implemented by the authors: First, the word
aligner is applied in both translation directions, and
the intersection of the alignment matrices is built.
Then, the alignment is extended by adding immedi-
ately adjacent alignment points and alignment points
that align previously unaligned words. From this
many-to-many alignment matrix, phrases are ex-
tracted according to a contiguity requirement that
states that words in the source phrase are aligned
only with words in the target phrase, and vice versa.
Discriminative reranking on a 1000-best list of
translations of the SMT system uses an `1 regu-
larized log-linear model that combines a standard
maximum-entropy estimator with an efficient, in-
cremental feature selection technique for `1 regu-
larization (Riezler and Vasserman, 2004). Training
data are defined as pairs {(sj , tj)}mj=1 of source sen-
tences sj and gold-standard translations tj that are
determined as the translations in the 1000-best list
that best match a given reference translation. The
objective function to be minimized is the conditional
log-likelihood L(?) subject to a regularization term
R(?), where T (s) is the set of 1000-best translations
for sentence s, ? is a vector or log-parameters, and
1http://www.fjoch.com/GIZA++.html
2http://www.isi.edu/licensed-sw/pharaoh/
3http://www.speech.sri.com/projects/srilm/
4http://www.isi.edu/licensed-sw/carmel/
5http://people.csail.mit.edu/people/
koehn/publications/europarl/
58
Table 1: NIST, BLEU, F-scores for reranker and baseline on development set
NIST BLEU F
baseline 6.43 .301 .385
reranking 6.58 .298 .383
approxrand p-value < .0001 .158 .424
bootstrap p-value < .0001 .1 -
f is a vector of feature functions:
L(?) + R(?) = ? log
m?
j=1
p?(tj |sj) + R(?)
= ?
m?
j=1
log
e??f(tj)
?
t?T (sj)
e??f(t)
+ R(?)
The features employed in our experiments con-
sist of 8 features corresponding to system compo-
nents (distortion model, language model, phrase-
translations, lexical weights, phrase penalty, word
penalty) as provided by PHARAOH, together with a
multitude of overlapping phrase features. For exam-
ple, for a phrase-table of phrases consisting of max-
imally 3 words, we allow all 3-word phrases and 2-
word phrases as features. Since bigram features can
overlap, information about trigrams can be gathered
by composing bigram features even if the actual tri-
gram is not seen in the training data.
Feature selection makes it possible to employ and
evaluate a large number of features, without con-
cerns about redundant or irrelevant features hamper-
ing generalization performance. The `1 regularizer is
defined by the weighted `1-norm of the parameters
R(?) = ?||?||1 = ?
n?
i=1
|?i|
where ? is a regularization coefficient, and n is num-
ber of parameters. This regularizer penalizes overly
large parameter values in their absolute values, and
tends to force a subset of the parameters to be ex-
actly zero at the optimum. This fact leads to a natural
integration of regularization into incremental feature
selection as follows: Assuming a tendency of the `1
regularizer to produce a large number of zero-valued
parameters at the function?s optimum, we start with
all-zero weights, and incrementally add features to
the model only if adjusting their parameters away
from zero sufficiently decreases the optimization cri-
terion. Since every non-zero weight added to the
model incurs a regularizer penalty of ?|?i|, it only
makes sense to add a feature to the model if this
penalty is outweighed by the reduction in negative
log-likelihood. Thus features considered for selec-
tion have to pass the following test:
?
?
?
?
?L(?)
??i
?
?
?
? > ?
This gradient test is applied to each feature and at
each step the features that pass the test with maxi-
mum magnitude are added to the model. This pro-
vides both efficient and accurate estimation with
large feature sets.
Work on discriminative reranking has been re-
ported before by Och and Ney (2002), Och et al
(2004), and Shen et al (2004). The main purpose of
our reranking experiments is to have a system that
can easily be adjusted to yield system variants that
differ at controllable amounts. For quick experimen-
tal turnaround we selected the training and test data
from sentences with 5 to 15 words, resulting in a
training set of 160,000 sentences, and a development
set of 2,000 sentences. The phrase-table employed
was restricted to phrases of maximally 3 words, re-
sulting in 200,000 phrases.
3 Detecting Small Result Differences by
Intrinsic Evaluations Metrics
The intrinsic evaluation measures used in our ex-
periments are the well-known BLEU (Papineni et
al., 2001) and NIST (Doddington, 2002) metrics,
and an F-score measure that adapts evaluation tech-
niques from dependency-based parsing (Crouch et
al., 2002) and sentence-condensation (Riezler et al,
2003) to machine translation. All of these measures
59
Set c = 0
Compute actual statistic of score differences |SX ? SY| on test data
For random shuffles r = 0, . . . , R
For sentences in test set
Shuffle variable tuples between system X and Y with probability 0.5
Compute pseudo-statistic |SXr ? SYr | on shuffled data
If |SXr ? SYr | ? |SX ? SY|
c + +
p = (c + 1)/(R + 1)
Reject null hypothesis if p is less than or equal to specified rejection level.
Figure 1: Approximate Randomization Test for Statistical Significance Testing
evaluate document similarity of SMT output against
manually created reference translations. The mea-
sures differ in their focus on different entities in
matching, corresponding to a focus on different as-
pects of translation quality.
BLEU and NIST both consider n-grams in source
and reference strings as matching entities. BLEU
weighs all n-grams equally whereas NIST puts more
weight on n-grams that are more informative, i.e.,
occur less frequently. This results in BLEU favor-
ing matches in larger n-grams, corresponding to giv-
ing more credit to correct word order. NIST weighs
lower n-grams more highly, thus it gives more credit
to correct lexical choice than to word order.
F-score is computed by parsing reference sen-
tences and SMT outputs, and matching grammatical
dependency relations. The reported value is the har-
monic mean of precision and recall, which is defined
as (2? precision ? recall )/( precision + recall ).
Precision is the ratio of matching dependency re-
lations to the total number of dependency relations
in the parse for the system translation, and recall is
the ratio of matches to the total number of depen-
dency relations in the parse for the reference trans-
lation. The goal of this measure is to focus on as-
pects of meaning in measuring similarity of system
translations to reference translations, and to allow
for meaning-preserving word order variation.
Evaluation results for a comparison of rerank-
ing against a baseline model that only includes fea-
tures corresponding to the 8 system components are
shown in Table 1. Since the task is a comparison
of system variants for development, all results are
reported on the development set of 2,000 exam-
ples of length 5-15. The reranking model achieves
an increase in NIST score of .15 units, whereas
BLEU and F-score decrease by .3% and .2% respec-
tively. However, as measured by the statistical sig-
nificance tests described below, the differences in
BLEU and F-scores are not statistically significant
with p-values exceeding the standard rejection level
of .05. In contrast, the differences in NIST score
are highly significant. These findings correspond to
results reported in Zhang et al (2004) showing a
higher sensitivity of NIST versus BLEU to small re-
sult differences. Taking also the results from F-score
matching in account, we can conclude that similar-
ity measures that are based on matching more com-
plex entities (such as BLEU?s higher n-grams or F?s
grammatical relations) are not as sensitive to small
result differences as scoring techniques that are able
to distinguish models by matching simpler entities
(such as NIST?s focus on lexical choice). Further-
more, we get an indication that differences of .3%
in BLEU score or .2% in F-score might not be large
enough to conclude statistical significance of result
differences. This leads to questions of power and ac-
curacy of the employed statistical significance tests
which will be addressed in the next section.
4 Assessing Statistical Significance of
Small Result Differences
The bootstrap method is an example for a computer-
intensive statistical hypothesis test (see, e.g., Noreen
(1989)). Such tests are designed to assess result
differences with respect to a test statistic in cases
where the sampling distribution of the test statistic
60
Set c = 0
Compute actual statistic of score differences |SX ? SY| on test data
Calculate sample mean ?B = 1B
?B
b=0 |SXb ? SYb | over bootstrap samples b = 0, . . . , B
For bootstrap samples b = 0, . . . , B
Sample with replacement from variable tuples for systems X and Y for test sentences
Compute pseudo-statistic |SXb ? SYb | on bootstrap data
If |SXb ? SYb | ? ?B (+?) ? |SX ? SY|
c + +
p = (c + 1)/(B + 1)
Reject null hypothesis if p is less than or equal to specified rejection level.
Figure 2: Bootstrap Test for Statistical Significance Testing
is unknown. Comparative evaluations of outputs of
SMT systems according to test statistics such as dif-
ferences in BLEU, NIST, or F-score are examples
of this situation. The attractiveness of computer-
intensive significance tests such as the bootstrap
or the approximate randomization method lies in
their power and simplicity. As noted in standard
textbooks such as Cohen (1995) or Noreen (1989)
such tests are as powerful as parametric tests when
parametric assumptions are met and they outper-
form them when parametric assumptions are vio-
lated. Because of their generality and simplicity they
are also attractive alternatives to conventional non-
parametric tests (see, e.g., Siegel (1988)). The power
of these tests lies in the fact that they answer only a
very simple question without making too many as-
sumptions that may not be met in the experimen-
tal situation. In case of the approximate random-
ization test, only the question whether two sam-
ples are related to each other is answered, with-
out assuming that the samples are representative of
the populations from which they were drawn. The
bootstrap method makes exactly this one assump-
tion. This makes it formally possible to draw in-
ferences about population parameters for the boot-
strap, but not for approximate randomization. How-
ever, if the goal is to assess statistical significance
of a result difference between two systems the ap-
proximate randomization test provides the desired
power and accuracy whereas the bootstrap?s advan-
tage to draw inferences about population parameters
comes at the price of reduced accuracy. Noreen sum-
marizes this shortcoming of the bootstrap technique
as follows: ?The principal disadvantage of [the boot-
strap] method is that the null hypothesis may be re-
jected because the shape of the sampling distribution
is not well-approximated by the shape of the boot-
strap sampling distribution rather than because the
expected value of the test statistic differs from the
value that is hypothesized.?(Noreen (1989), p. 89).
Below we describe these two test procedures in more
detail, and compare them in our experimental setup.
4.1 Approximate Randomization
An excellent introduction to the approximate ran-
domization test is Noreen (1989). Applications of
this test to natural language processing problems can
be found in Chinchor et al (1993).
In our case of assessing statistical significance of
result differences between SMT systems, the test
statistic of interest is the absolute value of the differ-
ence in BLEU, NIST, or F-scores produced by two
systems on the same test set. These test statistics are
computed by accumulating certain count variables
over the sentences in the test set. For example, in
case of BLEU and NIST, variables for the length of
reference translations and system translations, and
for n-gram matches and n-gram counts are accumu-
lated over the test corpus. In case of F-score, vari-
able tuples consisting of the number of dependency-
relations in the parse for the system translation, the
number of dependency-relations in the parse for the
reference translation, and the number of matching
dependency-relations between system and reference
parse, are accumulated over the test set.
Under the null hypothesis, the compared systems
are not different, thus any variable tuple produced by
one of the systems could have been produced just as
61
Table 2: NIST scores for equivalent systems under bootstrap and approximate randomization tests.
compared systems 1:2 1:3 1:4 1:5 1:6
NIST difference .031 .032 .029 .028 .036
approxrand p-value .03 .025 .05 .079 .028
bootstrap p-value .014 .013 .028 .039 .013
likely by the other system. So shuffling the variable
tuples between the two systems with equal probabil-
ity, and recomputing the test statistic, creates an ap-
proximate distribution of the test statistic under the
null hypothesis. For a test set of S sentences there
are 2S different ways to shuffle the variable tuples
between the two systems. Approximate randomiza-
tion produce shuffles by random assignments instead
of evaluating all 2S possible assignments. Signifi-
cance levels are computed as the percentage of trials
where the pseudo statistic, i.e., the test statistic com-
puted on the shuffled data, is greater than or equal to
the actual statistic, i.e., the test statistic computed on
the test data. A sketch of an algorithm for approxi-
mate randomization testing is given in Fig. 1.
4.2 The Bootstrap
An excellent introduction to the technique is the
textbook by Efron and Tibshirani (1993). In contrast
to approximate randomization, the bootstrap method
makes the assumption that the sample is a repre-
sentative ?proxy? for the population. The shape of
the sampling distribution is estimated by repeatedly
sampling (with replacement) from the sample itself.
A sketch of a procedure for bootstrap testing is
given in Fig. 2. First, the test statistic is computed on
the test data. Then, the sample mean of the pseudo
statistic is computed on the bootstrapped data, i.e.,
the test statistic is computed on bootstrap samples
of equal size and averaged over bootstrap samples.
In order to compute significance levels based on
the bootstrap sampling distribution, we employ the
?shift? method described in Noreen (1989). Here it
is assumed that the sampling distribution of the null
hypothesis and the bootstrap sampling distribution
have the same shape but a different location. The
location of the bootstrap sampling distribution is
shifted so that it is centered over the location where
the null hypothesis sampling distribution should be
centered. This is achieved by subtracting from each
value of the pseudo-statistic its expected value ?B
and then adding back the expected value ? of the
test statistic under the null hypothesis. ?B can be es-
timated by the sample mean of the bootstrap sam-
ples; ? is 0 under the null hypothesis. Then, similar
to the approximate randomization test, significance
levels are computed as the percentage of trials where
the (shifted) pseudo statistic is greater than or equal
to the actual statistic.
4.3 Power vs. Type I Errors
In order to evaluate accuracy of the bootstrap and the
approximate randomization test, we conduct an ex-
perimental evaluation of type-I errors of both boot-
strap and approximate randomization on real data.
Type-I errors indicate failures to reject the null hy-
pothesis when it is true. We construct SMT system
variants that are essentially equal but produce su-
perficially different results. This can be achieved by
constructing reranking variants that differ in the re-
dundant features that are included in the models, but
are similar in the number and kind of selected fea-
tures. The results of this experiment are shown in Ta-
ble 2. System 1 does not include irrelevant features,
whereas systems 2-6 were constructed by adding a
slightly different number of features in each step,
but resulted in the same number of selected features.
Thus competing features bearing the same informa-
tion are exchanged in different models, yet overall
the same information is conveyed by slightly dif-
ferent feature sets. The results of Table 2 show that
the bootstrap method yields p-values < .015 in 3
out of 5 pairwise comparisons whereas the approx-
imate randomization test yields p-values ? .025 in
all cases. Even if the true p-value is unknown, we
can say that the approximate randomization test es-
timates p-values more conservatively than the boot-
strap, thus increasing the likelihood of type-I error
for the bootstrap test. For a restrictive significance
level of 0.15, which is motivated below for multiple
62
comparisons, the bootstrap would assess statistical
significance in 3 out of 5 cases whereas statistical
significance would not be assessed under approxi-
mate randomization. Assuming equivalence of the
compared system variants, these assessments would
count as type-I errors.
4.4 The Multiplicity Problem
In the experiment on type-I error described above, a
more stringent rejection level than the usual .05 was
assumed. This was necessary to circumvent a com-
mon pitfall in significance testing for k-fold pairwise
comparisons. Following the argumentation given in
Cohen (1995), the probability of randomly assess-
ing statistical significance for result differences in
k-fold pairwise comparisons grows exponentially in
k. Recall that for a pairwise comparison of systems,
specifying that p < .05 means that the probability of
incorrectly rejecting the null hypothesis that the sys-
tems are not different be less than .05. Caution has
to be exercised in k-fold pairwise comparisons: For
a probability pc of incorrectly rejecting the null hy-
pothesis in a specific pairwise comparison, the prob-
ability pe of at least once incorrectly rejecting this
null hypothesis in an experiment involving k pair-
wise comparisons is
pe ? 1? (1? pc)
k
For large values of k, the probability of concluding
result differences incorrectly at least once is unde-
sirably high. For example, in benchmark testing of
15 systems, 15(15 ? 1)/2 = 105 pairwise compar-
isons will have to be conducted. At a per-comparison
rejection level pc = .05 this results in an experi-
mentwise error pe = .9954, i.e., the probability of
at least one spurious assessment of significance is
1? (1? .05)105 = .9954. One possibility to reduce
the likelihood that one ore more of differences as-
sessed in pairwise comparisons is spurious is to run
the comparisons at a more stringent per-comparison
rejection level. Reducing the per-comparison rejec-
tion level pc until an experimentwise error rate pe
of a standard value, e.g., .05, is achieved, will favor
pe over pc. In the example of 5 pairwise compar-
isons described above, a per-comparison error rate
pc = .015 was sufficient to achieve an experimen-
twise error rate pe ? .07. In many cases this tech-
nique would require to reduce pc to the point where
a result difference has to be unrealistically large to
be significant. Here conventional tests for post-hoc
comparisons such as the Scheffe? or Tukey test have
to be employed (see Cohen (1995), p. 185ff.).
5 Conclusion
Situations where a researcher has to deal with subtle
differences between systems are common in system
development and large benchmark tests. We have
shown that it is useful in such situations to trade in
expressivity of evaluation measures for sensitivity.
For MT evaluation this means that recording differ-
ences in lexical choice by the NIST measure is more
useful than failing to record differences by employ-
ing measures such as BLEU or F-score that incorpo-
rate aspects of fluency and meaning adequacy into
MT evaluation. Similarly, in significance testing, it
is useful to trade in the possibility to draw inferences
about the sampling distribution for accuracy and
power of the test method. We found experimental
evidence confirming textbook knowledge about re-
duced accuracy of the bootstrap test compared to the
approximate randomization test. Lastly, we pointed
out a well-known problem of randomly assessing
significance in multiple pairwise comparisons. Tak-
ing these findings together, we recommend for mul-
tiple comparisons of subtle differences to combine
the NIST score for evaluation with the approximate
randomization test for significance testing, at more
stringent rejection levels than is currently standard
in the MT literature.
References
Nancy Chinchor, Lynette Hirschman, and David D.
Lewis. 1993. Evaluating message understanding sys-
tems: An analysis of the third message understand-
ing conference (MUC-3). Computational Linguistics,
19(3):409?449.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press, Cambridge, MA.
Richard Crouch, Ronald M. Kaplan, Tracy H. King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad-coverage stochastic parser. In Pro-
ceedings of the ?Beyond PARSEVAL? Workshop at the
3rd International Conference on Language Resources
and Evaluation (LREC?02), Las Palmas, Spain.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
63
statistics. In Proceedings of the ARPA Workshop on
Human Language Technology.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman and Hall, New
York.
Kevin Knight and Yaser Al-Onaizan. 1999. A primer on
finite-state software for natural language processing.
Technical report, USC Information Sciences Institute,
Marina del Rey, CA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
and the 3rd Meeting of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?03), Edmonton, Cananda.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation. Technical re-
port, USC Information Sciences Institute, Marina del
Rey, CA.
Philipp Koehn. 2004a. PHARAOH. a beam search de-
coder for phrase-based statistical machine translation
models. user manual. Technical report, USC Informa-
tion Sciences Institute, Marina del Rey, CA.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP?04), Barcelona, Spain.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy conference / North American chapter of the Asso-
ciation for Computational Linguistics annual meeting
(HLT/NAACL?04), Boston, MA.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL?02), Philadelphia, PA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 1999 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP?99).
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Ketherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy conference / North American chapter of the Asso-
ciation for Computational Linguistics annual meeting
(HLT/NAACL?04), Boston, MA.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceedings
of the Human Language Technology Conference and
the 3rd Meeting of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?03), Edmonton, Cananda.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
IBM Research Division Technical Report, RC22176
(W0190-022), Yorktown Heights, N.Y.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and `1 regularization for re-
laxed maximum-entropy modeling. In Proceedings of
the 2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP?04), Barcelona, Spain.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the Human Language Technology Confer-
ence and the 3rd Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL?03), Edmonton, Cananda.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proceedings of the Human Language Technology con-
ference / North American chapter of the Associa-
tion for Computational Linguistics annual meeting
(HLT/NAACL?04), Boston, MA.
Sidney Siegel. 1988. Nonparametric Statistics for the
Behavioral Sciences. Second Edition. MacGraw-Hill,
Boston, MA.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Denver,
CO.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (LREC?04), Lisbon, Portu-
gal.
64
Exploiting auxiliary distributions in stochastic unification-based 
grammars 
Mark  Johnson*  
Cognitive and Linguistic Sciences 
Brown University 
Mark_Johnson@Brown.edu 
Ste fan  R iez le r  
Inst i tut  fiir Maschinelle Sprachverarbeitung 
Universit?t Stut tgart  
riezler~ims.uni-stuttgart.de 
Abst rac t  
This paper describes a method for estimat- 
ing conditional probability distributions over 
the parses of "unification-based" grammars 
which can utilize auxiliary distributions that 
are estimated by other means. We show how 
this can be used to incorporate information 
about lexical selectional preferences gathered 
from other sources into Stochastic "Unification- 
based" Grammars (SUBGs). While we ap- 
ply this estimator to a Stochastic Lexical- 
Functional Grammar, the method is general, 
and should be applicable to stochastic versions 
of HPSGs, categorial grammars and transfor- 
mational grammars. 
1 In t roduct ion  
"Unification-based" Grammars (UBGs) can 
capture a wide variety of linguistically impor- 
tant syntactic and semantic onstraints. How- 
ever, because these constraints can be non-local 
or context-sensitive, developing stochastic ver- 
sions of UBGs and associated estimation pro- 
cedures is not as straight-forward as it is for, 
e.g., PCFGs. Recent work has shown how to 
define probability distributions over the parses 
of UBGs (Abney, 1997) and efficiently estimate 
and use conditional probabilities for parsing 
(Johnson et al, 1999). Like most other practical 
stochastic grammar estimation procedures, this 
latter estimation procedure requires a parsed 
training corpus. 
Unfortunately, large parsed UBG corpora are 
not yet available. This restricts the kinds of 
models one can realistically expect to be able 
to estimate. For example, a model incorporat- 
ing lexical selectional preferences of the kind 
* This research was supported by NSF awards 9720368, 
9870676 and 9812169. 
described below might have tens or hundreds 
of thousands of parameters, which one could 
not reasonably attempt o estimate from a cor- 
pus with on the order of a thousand clauses. 
However, statistical models of lexical selec- 
tional preferences can be estimated from very 
large corpora based on simpler syntactic struc- 
tures, e.g., those produced by a shallow parser. 
While there is undoubtedly disagreement be- 
tween these simple syntactic structures and the 
syntactic structures produced by the UBG, one 
might hope that they are close enough for lexical 
information gathered from the simpler syntactic 
structures to be of use in defining a probability 
distribution over the UBG's structures. 
In the estimation procedure described here, 
we call the probability distribution estimated 
from the larger, simpler corpus an auxiliary dis- 
tribution. Our treatment of auxiliary distribu- 
tions is inspired by the treatment of reference 
distributions in Jelinek's (1997) presentation of
Maximum Entropy estimation, but in our es- 
timation procedure we simply regard the loga- 
r ithm of each auxiliary distribution as another 
(real-valued) feature. Despite its simplicity, our 
approach seems to offer several advantages over 
the reference distribution approach. First, it 
is straight-forward to utilize several auxiliary 
distributions simultaneously: each is treated as 
a distinct feature. Second, each auxiliary dis- 
tribution is associated with a parameter which 
scales its contribution to the final distribution. 
In applications such as ours where the auxiliary 
distribution may be of questionable relevance 
to the distribution we are trying to estimate, it
seems reasonable to permit the estimation pro- 
cedure to discount or even ignore the auxiliary 
distribution. Finally, note that neither Jelinek's 
nor our estimation procedures require that an 
auxiliary or reference distribution Q be a prob- 
154 
ability distribution; i.e., it is not necessary that 
Q(i2) -- 1, where f~ is the set of well-formed 
linguistic structures. 
The rest of this paper is structured as fol- 
lows. Section 2 reviews how exponential mod- 
els can be defined over the parses of UBGs, 
gives a brief description of Stochastic Lexical- 
Functional Grammar, and reviews why maxi- 
mum pseudo-likelihood estimation is both feasi- 
ble and sufficient of parsing purposes. Section 3 
presents our new estimator, and shows how it 
is related to the minimization of the Kullback- 
Leibler divergence between the conditional es- 
t imated and auxiliary distributions. Section 4 
describes the auxiliary distribution used in our 
experiments, and section 5 presents the results 
of those experiments. 
2 S tochast i c  Un i f i ca t ion -based  
Grammars  
Most of the classes of probabilistic language 
models used in computational linguistic are ex- 
ponential families. That is, the probability P(w) 
of a well-formed syntactic structure w E ~ is de- 
fined by a function of the form 
PA(w) = Q(~v) eX.f(oj ) (1) 
where f (w) E R m is a vector of feature values, 
)~ E It m is a vector of adjustable feature param- 
eters, Q is a function of w (which Jelinek (1997) 
calls a reference distribution when it is not an in- 
dicator function), and ZA = fn Q(w) ex'f(~)dw is 
a normalization factor called the partition func- 
tion. (Note that a feature here is just a real- 
valued function of a syntactic structure w; to 
avoid confusion we use the term "attribute" to 
refer to a feature in a feature structure). If 
Q(w) = 1 then the class of exponential dis- 
tributions is precisely the class of distributions 
with maximum entropy satisfying the constraint 
that the expected values of the features is a cer- 
tain specified value (e.g., a value estimated from 
training data), so exponential models are some- 
times also called "Maximum Entropy" models. 
For example, the class of distributions ob- 
tained by varying the parameters of a PCFG 
is an exponential family. In a PCFG each rule 
or production is associated with a feature, so m 
is the number of rules and the j th  feature value 
f j  (o.,) is the number of times the j rule is used 
in the derivation of the tree w E ~. Simple ma- 
nipulations how that P,x (w) is equivalent to the 
PCFG distribution ifAj = logpj, where pj is the 
rule emission probability, and Q(w) = Z~ = 1. 
If the features atisfy suitable Markovian in- 
dependence constraints, estimation from fully 
observed training data is straight-forward. For 
example, because the rule features of a PCFG 
meet "context-free" Markovian independence 
conditions, the well-known "relative frequency" 
estimator for PCFGs both maximizes the likeli- 
hood of the training data (and hence is asymp- 
totically consistent and efficient) and minimizes 
the Kullback-Leibler divergence between train- 
ing and estimated istributions. 
However, the situation changes dramatically 
if we enforce non-local or context-sensitive con- 
straints on linguistic structures of the kind that 
can be expressed by a UBG. As Abney (1997) 
showed, under these circumstances the relative 
frequency estimator is in general inconsistent, 
even if one restricts attention to rule features. 
Consequently, maximum likelihood estimation 
is much more complicated, as discussed in sec- 
tion 2.2. Moreover, while rule features are natu- 
ral for PCFGs given their context-free indepen- 
dence properties, there is no particular eason 
to use only rule features in Stochastic UBGs 
(SUBGs). Thus an SUBG is a triple (G, f,  A), 
where G is a UBG which generates a set of well- 
formed linguistic structures i2, and f and A are 
vectors of feature functions and feature param- 
eters as above. The probability of a structure 
w E ~ is given by (1) with Q(w) = 1. Given a 
base UBG, there are usually infinitely many dif- 
ferent ways of selecting the features f to make 
a SUBG, and each of these makes an empirical 
claim about the class of possible distributions 
of structures. 
2.1 S tochast i c  Lexica l  Funct iona l  
Grammar  
Stochastic Lexical-Functional Grammar 
(SLFG) is a stochastic extension of Lexical- 
Functional Grammar (LFG), a UBG formalism 
developed by Kaplan and Bresnan (1982). 
Given a base LFG, an SLFG is constructed 
by defining features which identify salient 
constructions in a linguistic structure (in LFG 
this is a c-structure/f-structure pair and its 
associated mapping; see Kaplan (1995)). Apart 
from the auxiliary distributions, we based our 
155 
features on those used in Johnson et al (1999), 
which should be consulted for further details. 
Most of these feature values range over the 
natural numbers, counting the number of times 
that a particular construction appears in a 
linguistic structure. For example, adjunct and 
argument features count the number of adjunct 
and argument attachments, permitting SLFG 
to capture a general argument attachment pref- 
erence, while more specialized features count 
the number of attachments oeach grammatical 
function (e.g., SUB J, OBJ,  COMP, etc.). 
The flexibility of features in stochastic UBGs 
permits us to include features for relatively 
complex constructions, such as date expres- 
sions (it seems that date interpretations, if 
possible, are usually preferred), right-branching 
constituent structures (usually preferred) and 
non-parallel coordinate structures (usually 
dispreferred). Johnson et al remark that they 
would have liked to have included features for 
lexical selectional preferences. While such fea- 
tures are perfectly acceptable in a SLFG, they 
felt that their corpora were so small that the 
large number of lexical dependency parameters 
could not be accurately estimated. The present 
paper proposes a method to address this by 
using an auxiliary distribution estimated from 
a corpus large enough to (hopefully) provide 
reliable estimates for these parameters. 
2.2 Estimating stochastic 
unification-based grammars 
Suppose ~ = Wl,...,Wn is a corpus of n syn- 
tactic structures. Letting fj(fJ) = ~--~=1 fj(oJi) 
and assuming each wi E 12, the likelihood of the 
corpus L~(&) is: 
T~ 
L~(~) = 1-I Px(w,) 
i=1 
= e ~/(c~) Z-~ n (2) 
0 
logan(&) = fj(Co) - -  nEa(fj) (3) 0Aj 
where E~(fj) is the expected value of f~ un- 
der the distribution P~. The maximum likeli- 
hood estimates are the )~ which maximize (2), or 
equivalently, which make (3) zero, but as John- 
son et al (1999) explain, there seems to be no 
practical way of computing these for realistic 
SUBGs since evaluating (2) and its derivatives 
(3) involves integrating over all syntactic struc- 
tures ft. 
However, Johnson et al observe that parsing 
applications require only the conditional prob- 
ability distribution P~(wly), where y is the ter- 
minal string or yield being parsed, and that this 
can be estimated by maximizing the pseudo- 
likelihood of the corpus PL~(SJ): 
rz 
PLx(SJ) = I I  P~(wilyi) 
i=-I 
n 
= eA'f(w) ~I Z;  l(yi) 
i=1 
In (4), Yi is the yield of wi and 
Z~(yi) = f~(y,) e~I(~)dw, 
(4) 
where f~(Yi) is the set of all syntactic structures 
in f~ with yield yi (i.e., all parses of Yi gener- 
ated by the base UBG). It turns out that cal- 
culating the pseudo-likelihood f a corpus only 
involves integrations over the sets of parses of 
its yields f~(Yi), which is feasible for many inter- 
esting UBGs. Moreover, the maximum pseudo- 
likelihood estimator isasymptotically consistent 
for the conditional distribution P(w\]y). For the 
reasons explained in Johnson et al (1999) we ac- 
tually estimate )~ by maximizing a regularized 
version of the log pseudo-likelihood (5), where 
aj is 7 times the maximum value of fj found in 
the training corpus: 
m ~2 
logPL~(~) - ~ 2"~2 (5) 
j= l  v j  
See Johnson et al (1999) for details of the calcu- 
lation of this quantity and its derivatives, and 
the conjugate gradient routine used to calcu- 
late the )~ which maximize the regularized log 
pseudo-likelihood f the training corpus. 
3 Aux i l i a ry  d i s t r ibut ions  
We modify the estimation problem presented in 
section 2.2 by assuming that in addition to the 
corpus ~ and the m feature functions f we are 
given k auxiliary distributions Q1,. . . ,  Qk whose 
support includes f~ that we suspect may be re- 
lated to the joint distribution P(w) or condi- 
tional distribution P(wly ) that we wish to esti- 
156 
mate. We do not require that the Qj be proba- 
bility distributions, i.e., it is not necessary that 
f~ Qj(w)dw = 1, but we do require that they 
are strictly positive (i.e., Qj(w) > O, Vw E ~). 
We define k new features fro+l,..., fm+k where 
fm+j(w) = log Qj(w), which we call auxiliary 
features. The m + k parameters associated with 
the resulting m+k features can be estimated us- 
ing any method for estimating the parameters 
of an exponential family with real-valued fea- 
tures (in our experiments we used the pseudo- 
likelihood estimation procedure reviewed in sec- 
tion 2.2). Such a procedure stimates parame- 
ters )~m+l,.-., Am+k associated with the auxil- 
iary features, so the estimated istributions take 
the form (6) (for simplicity we only discuss joint 
distributions here, but the treatment of condi- 
tional distributions i parallel). 
P,(w) = I'Ik=l QJ(w)A~+J eZ_,~=lAjlj(~)(6 ) v - - ~  
Note that the auxiliary distributions Qj are 
treated as fixed distributions for the purposes 
of this estimation, even though each Qj may it- 
self be a complex model obtained via a previous 
estimation process. Comparing (6) with (1) on 
page 2, we see that the two equations become 
identical if the reference distribution Q in (1) is 
replaced by a geometric mixture of the auxiliary 
distributions Qj, i.e., if: 
k 
Q(w) = ~I Q~(w) xm+i- 
j= l  
The parameter associated with an auxiliary fea- 
ture represents he weight of that feature in the 
mixture. If a parameter ~m+j = 1 then the 
corresponding auxiliary feature Qj is equivalent 
to a reference distribution in Jelinek's sense, 
while if ~m+j = 0 then Qj is effectively ig- 
nored. Thus our approach can be regarded as 
a smoothed version Jelinek's reference distribu- 
tion approach, generalized to permit multiple 
auxiliary distributions. 
4 Lex ica l  se lec t iona l  p re ferences  
The auxiliary distribution we used here is based 
on the probabilistic model of lexical selectional 
preferences described in Rooth et al (1999). An 
existing broad-coverage parser was used to find 
shallow parses (compared to the LFG parses) 
for the 117 million word British National Cor- 
pus (Carroll and Rooth, 1998). We based our 
auxiliary distribution on 3.7 million (g, r, a) tu- 
ples (belonging to 600,000 types) we extracted 
these parses, where g is a lexical governor (for 
the shallow parses, g is either a verb or a prepo- 
sition), a is the head of one of its NP arguments 
and r is the the grammatical relationship be- 
tween the governor and argument (in the shal- 
low parses r is always OBJ for prepositional gov- 
ernors, and r is either SUBJ or OBJ for verbal 
governors). 
In order to avoid sparse data problems we 
smoothed this distribution over tuples as de- 
scribed in (Rooth et al, 1999). We assume that 
governor-relation pairs (g, r) and arguments a 
are independently generated from 25 hidden 
classes C, i.e.: 
P((g,r,a)) = ~'~ Pe((g,r)lc)~)e(alc)ee(c) 
cEC 
where the distributions Pe are estimated from 
the training tuples using the Expectation- 
Maximization algorithm. While the hidden 
classes axe not given any prior interpretation 
they often cluster semantically coherent pred- 
icates and arguments, as shown in Figure 1. 
The smoothing power of a clustering model such 
as this can be calculated explicitly as the per- 
centage of possible tuples which are assigned a
non-zero probability. For the 25-class model 
we get a smoothing power of 99%, compared 
to only 1.7% using the empirical distribution of 
the training data. 
5 Empi r i ca l  eva luat ion  
Hadar Shemtov and Ron Kaplan at Xerox PARC 
provided us with two LFG parsed corpora called 
the Verbmobil corpus and the Homecentre cor- 
pus. These contain parse forests for each sen- 
tence (packed according to scheme described in 
Maxwell and Kaplan (1995)), together with a 
manual annotation as to which parse is cor- 
rect. The Verbmobil corpus contains 540 sen- 
tences relating to appointment planning, while 
the Homecentre corpus contains 980 sentences 
from Xerox documentation their "homecen- 
tre" multifunction devices. Xerox did not pro- 
vide us with the base LFGs for intellectual prop- 
erty reasons, but from inspection of the parses 
157 
Class  16 
PROB 0.0340 o?5 d d ~ c ;d  ?~ d dd  ~ d d d?5?~ d ?5~ ~d~dd dd  ?~d ?~ 
I .  
0.3183 say :s  \] ? ? 
0 .0405 say :o  i ? ? 
0 .0345 ask:s  ? 
0.0276 te l l : s  ? ? 
0 .0214 be:s  ? 
0 .0193 know:s  ? 
0 .0147 h&ve:s  
0.0144 nod:s  ? 
0 .0137 th lnk :s  ? 
0 .0130 shake:s  ? 
0.0128 take :s  ? 
0 .0104 rep ly :s  ? 
0 .0096 smi le :s  ? 
10.0094 do:s  
0.0094 laugh:s  ? 
0.0089 te lho  
0.0084 saw:s  ? 
~ 0.0082 add:s  ? 
0.0078 feehs ? 
0.0071 make:s  ? 
0.0070 g ive:s  ? ? 
0 .0067 ask :o  ? 
0.0066 shrug:s  ? 
0 .0061 exp la in :s  ? ? 
0 .0051 l ike:s ? 
0 .0050 Iook:s 
0 .0050 s igh:s  ? 
0 .0049 watch :s  ? 
0 .0049 hear :s  
0.0047 answer :s  ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? ? ? ? ? ? ? ? ? ? 
: : : : : : : : : : : : : : : : : : : : : : : : : : : :  
? ? Q ? ? ? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? 
? ? ? ? ? ? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? ? ? ? ? ? ? 
? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? o ? o ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? ? ? ? o ? 
. . . .  . . . . . . . . . . . . . . .  : : : : : : : :  
? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 6 ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? o ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
: : : : : : : : : : : : : : : : : : : : : : : : : : : :  
? ? ? ? ? ? ? ? ? ? ? o o ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? Q t ? ? ? ? 
? ? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? Q ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? o ? ? ? ? ? ? ? ? ? ? ? ? ? ? 
Figure 1: A depiction of the highest probability predicates and arguments in Class 16. The class 
matrix shows at the top the 30 most probable nouns in the Pe (a116) distribution and their probabil- 
ities, and at the left the 30 most probable verbs and prepositions listed according to Pre((g, r)116) 
and their probabilities. Dots in the matrix indicate that the respective pair was seen in the training 
data. Predicates with suffix : s indicate the subject slot of an intransitive or transitive verb; the 
suffix : o specifies the nouns in the corresponding row as objects of verbs or prepositions. 
it seems that slightly different grammars were 
used with each corpus, so we did not merge the 
corpora. We chose the features of our SLFG 
based solely on the basis of the Verbmobil cor- 
pus, so the Homecentre corpus can be regarded 
as a held-out evaluation corpus. 
We discarded the unambiguous sentences in 
each corpus for both training and testing (as 
explained in Johnson et al (1999), pseudo- 
likelihood estimation ignores unambiguous en- 
tences), leaving us with a corpus of 324 am- 
biguous sentences in the Verbmobil corpus and 
481 sentences in the Homecentre corpus; these 
sentences had a total of 3,245 and 3,169 parses 
respectively. 
The (non-auxiliary) features used in were 
based on those described by Johnson et 
al. (1999). Different numbers of features 
were used with the two corpora because 
some of the features were generated semi- 
automatically (e.g., we introduced a feature for 
every attribute-value pair found in any feature 
structure), and "pseudo-constant" features (i.e., 
features whose values never differ on the parses 
of the same sentence) are discarded. We used 
172 features in the SLFG for the Verbmobil cor- 
pus and 186 features in the SLFG for the Home- 
centre corpus. 
We used three additional auxiliary features 
derived from the lexical selectional preference 
model described in section 4. These were de- 
fined in the following way. For each governing 
predicate g, grammatical relation r and argu- 
ment a, let n(g,r,a)(w) be the number of times 
that the f-structure: 
PRED ~ g \] 
r = \[PRED=a\] 
appears as a subgraph of the f-structure of 
w, i.e., the number of times that a fills the 
158 
grammatical role r of g. We used the lexical 
model described in the last section to estimate 
P(alg , r), and defined our first auxiliary feature 
as :  
ft(w) = logP(g0) + Z n(g,r,a)(w)l?gP(al g,r) 
(g,r,a) 
where g0 is the predicate of the root feature 
structure. The justification for this feature is 
that if f-structures were in fact a tree, ft(w) 
would be the (logarithm of) a probability dis- 
tribution over them. The auxiliary feature ft 
is defective in many ways. Because LFG f- 
structures are DAGs with reentrancies rather 
than trees we double count certain arguments, 
so ft is certainly not the logarithm of a prob- 
ability distribution (which is why we stressed 
that our approach does not require an auxiliary 
distribution to be a distribution). 
The number of governor-argument tuples 
found in different parses of the same sentence 
can vary markedly. Since the conditional prob- 
abilities P(alg, r) are usually very small, we 
found that ft(w) was strongly related to the 
number of tuples found in w, so the parse with 
the smaller number of tuples usually obtains the 
higher fl score. We tried to address this by 
adding two additional features. We set fc(w) to 
be the number of tuples in w, i.e.: 
fc(w) = Z n(g,r,a)(w) ? 
(g,r,a) 
Then we set .Q(w) = h(w)/L(w), i . e . , / , (w)  is 
the average log probability of a lexical depen- 
dency tuple under the auxiliary lexical distribu- 
tion. We performed our experiments with ft as 
the sole auxiliary distribution, and with ft, fe 
and fn as three auxiliary distributions. 
Because our corpora were so small, we trained 
and tested these models using a 10-fold cross- 
validation paradigm; the cumulative results are 
shown in Table 1. On each fold we evaluated 
each model in two ways. The correct parses 
measure simply counts the number of test sen- 
tences for which the estimated model assigns 
its maximum parse probability to the correct 
parse, with ties broken randomly. The pseudo- 
likelihood measure is the pseudo-likelihood f
test set parses; i.e., the conditional probability 
of the test parses given their yields. We actu- 
ally report he negative log of this measure, so a 
smaller score corresponds to better performance 
here. The correct parses measure is most closely 
related to parser performance, but the pseudo- 
likelihood measure is more closely related to the 
quantity we are optimizing and may be more 
relevant to applications where the parser has to 
return a certainty factor associated with each 
parse. 
Table 1 also provides the number of indistin- 
guishable sentences under each model. A sen- 
tence y is indistinguishable with respect o fea- 
tures f iff f(wc) : f(w'), where wc is the correct 
parse of y and wc ~ w I E ~(y), i.e., the feature 
values of correct parse of y are identical to the 
feature values of some other parse of y. If a 
sentence is indistinguishable it is not possible 
to assign its correct parse a (conditional) prob- 
ability higher than the (conditional) probability 
assigned to other parses, so all else being equal 
we would expect a SUBG with with fewer indis- 
tinguishable sentences to perform better than 
one with more. 
Adding auxiliary features reduced the already 
low number of indistinguishable sentences in the 
Verbmobil corpus by only 11%, while it reduced 
the number of indistinguishable sentences in the 
Homecentre corpus by 24%. This probably re- 
flects the fact that the feature set was designed 
by inspecting only the Verbmobil corpus. 
We must admit disappointment with these 
results. Adding auxiliary lexical features im- 
proves the correct parses measure only slightly, 
and degrades rather than improves performance 
on the pseudo-likelihood measure. Perhaps this 
is due to the fact that adding auxiliary features 
increases the dimensionality of the feature vec- 
tor f ,  so the pseudo-likelihood scores with dif- 
ferent numbers of features are not strictly com- 
parable. 
The small improvement in the correct parses 
measure is typical of the improvement we might 
expect to achieve by adding a "good" non- 
auxiliary feature, but given the importance usu- 
ally placed on lexical dependencies in statistical 
models one might have expected more improve- 
ment. Probably the poor performance is due 
in part to the fairly large differences between 
the parses from which the lexical dependencies 
were estimated and the parses produced by the 
LFG. LFG parses are very detailed, and many 
ambiguities depend on the precise grammatical 
159 
Verbmobi l  corpus  (324 sentences, 172 non-auxiliary features) 
Auxi l iary  features  used Ind is t ingu ishab le  Cor rect  - log PL  
(none) 9 180 401.3 
fl 8 183 401.6 
f,, fc, .f. 8 180.5 404.0 
Homecentre  corpus (481 sentences, 186 non-auxiliary features) 
Aux i l ia ry  features  used Ind is t ingu ishab le  Cor rect  - log PL  
(none) 45 283.25 580.6 
fl 34 284 580.6 
f l, f c, f n 34 285 582.2 
Table h The effect of adding auxiliary lexical dependency features to a SLFG. The auxiliary 
features are described in the text. The column labelled "indistinguishable" gives the number of 
indistinguishable s ntences with respect o each feature set, while "correct" and "- log PL" give 
the correct parses and pseudo-likelihood measures respectively. 
relationship holding between a predicate and its 
argument. It could also be that better perfor- 
mance could be achieved if the lexical dependen- 
cies were estimated from a corpus more closely 
related to the actual test corpus. For example, 
the verb feed in the Homecentre corpus is used in 
the sense of "insert (paper into printer)", which 
hardly seems to be a prototypical usage. 
Note that overall system performance is quite 
good; taking the unambiguous sentences into 
account he combined LFG parser and statisti- 
cal model finds the correct parse for 73% of the 
Verbmobil test sentences and 80% of the Home- 
centre test sentences. On just the ambiguous 
sentences, our system selects the correct parse 
for 56% of the Verbmobil test sentences and 59% 
of the Homecentre test sentences. 
6 Conc lus ion  
This paper has presented a method for incorpo- 
rating auxiliary distributional information gath- 
ered by other means possibly from other corpora 
into a Stochastic "Unification-based" Grammar 
(SUBG). This permits one to incorporate de- 
pendencies into a SUBG which probably can- 
not be estimated irectly from the small UBG 
parsed corpora vailable today. It has the virtue 
that it can incorporate several auxiliary dis- 
tributions imultaneously, and because it asso- 
ciates each auxiliary distribution with its own 
"weight" parameter, it can scale the contribu- 
tions of each auxiliary distribution toward the 
final estimated istribution, or even ignore it 
entirely. We have applied this to incorporate 
lexical selectional preference information into 
a Stochastic Lexical-Functional Grammar, but 
the technique generalizes to stochastic versions 
of HPSGs, categorial grammars and transfor- 
mational grammars. An obvious extension of 
this work, which we hope will be persued in the 
future, is to apply these techniques in broad- 
coverage feature-based TAG parsers. 
References  
Steven P. Abney. 1997. Stochastic Attribute- 
Value Grammars. Computational Linguis- 
tics, 23(4):597-617. 
Glenn Carroll and Mats Rooth. 1998. Valence 
induction with a head-lexicalized PCFG. In 
Proceedings of EMNLP-3, Granada. 
Frederick Jelinek. 1997. Statistical Methods for 
Speech Recognition. The MIT Press, Cam- 
bridge, Massachusetts. 
Mark Johnson, Stuart Geman, Stephen Canon, 
Zhiyi Chi, and Stefan Riezler. 1999. Estima- 
tors for stochastic "unification-based" gram- 
mars. In The Proceedings of the 37th Annual 
Conference of the Association for Computa- 
tional Linguistics, pages 535-541, San Fran- 
cisco. Morgan Kaufmann. 
Ronald M. Kaplan and Joan Bresnan. 1982. 
Lexical-Functional Grammar: A formal sys- 
tem for grammatical representation. In Joan 
Bresnan, editor, The Mental Representation 
of Grammatical Relations, chapter 4, pages 
173-281. The MIT Press. 
160 
Ronald M. Kaplan. 1995. The formal architec- 
ture of LFG. In Mary Dalrymple, Ronald M. 
Kaplan, John T. Maxwell III, and Annie 
Zaenen, editors, Formal Issues in Lexical- 
Functional Grammar, number 47 in CSLI 
Lecture Notes Series, chapter 1, pages 7-28. 
CSLI Publications. 
John T. Maxwell III and Ronald M. Kaplan. 
1995. A method for disjunctive constraint 
satisfaction. In Mary Dalrymple, Ronald M. 
Kaplan, John T. Maxwell III, and Annie 
Zaenen, editors, Formal Issues in Lexical- 
Functional Grammar, number 47 in CSLI 
Lecture Notes Series, chapter 14, pages 381- 
481. CSLI Publications. 
Mats Rooth, Stefan Riezler, Detlef Prescher, 
Glenn Carroll,, and Franz Beil. 1999. Induc- 
ing a semantically annotated lexicon via EM- 
based clustering. In Proceedings of the 37th 
Annual Meeting of the Association .for Com- 
putational Linguistics, San Francisco. Mor- 
gan Kaufmann. 
161 
Using a Probabil istic Class-Based Lexicon 
for Lexical Ambiguity Resolution 
Det le f  P rescher  and Ste fan  R iez le r  and Mats  Rooth  
Inst i tut  fiir Maschinelle S1)rachvcrarbeitung 
UniversitSt Stuttgart ,  Germany 
Abst ract  
This paper presents the use of prot)abilistie 
class-based lexica tbr dismnbiguati(m in target- 
woxd selection. Our method emlfloys nfinimal 
1)llt; precise contextual information for disam- 
biguation. That is, only information provided 
by the target-verb, enriched by the condensed 
information of a probabilistic class-based lexi- 
con, is used. Induction of classes and fine-tuning 
to verbal arguments i done in an unsupervised 
manner by EM-lmsed clustering techniques. The 
method shows pronlising results in an evaluation 
on real-world translations. 
1 I n t roduct ion  
Disambiguation of lexical ambiguities in nat- 
urally oceuring free text is considered a hard 
task for computational linguistics. For instance, 
word sense disa.inbiguatiol~ is concerned with the 
protflem of assigning sense labels to occurrences 
of an ambiguous word. Resolving such ambi- 
guil;ies is useful in constraining semantic inter- 
pretation. A related task is target-word isam- 
biguation in machine translation. Here a deci- 
sion has to be made which of a set of alterna- 
tive target-language words is the most appro- 
priate translation of a source-language word. A 
sohltion to this disambiguation problem is di- 
rectly applicable in a machine translation sys- 
tem which is able to propose the translation al- 
ternatives. A further problem is the resolution 
of attachment ambiguities in syntactic parsing. 
Here the decision of verb versus argunlent at- 
ta&ment of noun phrases, or the choice for verb 
phrase versus noun phrase attachment of prepo- 
sitional phrases Call build upon a resolution of 
the related lexical mnbiguities. 
Statistical approaches have been applied suc- 
cessfully to these 1)roblems. The great advantage 
of statistical methods over symbolic-linguistic 
methods has been deemed to be their effec- 
tive exploitation of minimal linguist;it knowl- 
edge. However, the best performing statisti- 
cal approaches to lexical ambiguity resolution 
l;lmmselves rely on complex infornmtion sources 
such as "lemmas, inflected forms, parts of speech 
and arbitrary word classes If-.. \] local and dis- 
tant collocations, trigram sequences, a.nd predi- 
cate m'gument association" (Yarowsky (1995), p. 
190) or large context-windows up to 1000 neigh- 
boring words (Sch/itze, 1992). Unfortmmtely, in
many applications uch information is not read- 
ily available. For instance, in incremental ma- 
chine translation, it may be desirable to decide 
for the most probable translation of the argu- 
ments of a verb with only the translation of the 
verb as information source lint no large window 
of sun'ounding translations available. In parsing, 
the attachment of a nolninal head nlay haa~e to 
be resolved with only information al)out the se- 
mmltic roles of the verb but no other predi('ate 
argument associations at; hand. 
The aim of this paper is to use only nfinimal, 
but yet precise information fbr lexical ambiguity 
resolution. We will show that good results are 
obtainable by employing a simple and natural 
look-up in a probabilistic lass-labeled lexicon 
for disambiguation. The lexicon provides a prob- 
ability distribution on semantic selection-classes 
labeling the slots of verbal subcategorization 
frames. Induction of distributions on frames and 
class-labels is accomplished in an unsupervised 
manner by applying the EM Mgorittnn. Disam- 
biguation then is done by a simple look-up in the 
probabilistie lexicon. We restrict our attention 
to a definition of senses as alternative transla- 
tions of source-words. Our approach provides a 
very natural solution for such a target-language 
disambiguation task--look for the most fl'equent 
target-noun whose semantics fits best with the 
649 
Class  19 
PROB 0 .0235 
0 .0629 
0 .0386 
0 .0321 
0 .0236 
0 .0226 
0 .0214 
0 .0173 
0 .0136 
0 .0132 
0 .0126 
0 .0124 
0 .0115 
0 .0113 
0 .0108 
0 .0009 
0 .0086 
0 .0085 
0 .0082 
0 .0082 
0 .0082 
enter .aso :o  
cover .aso :o  
ca l l .aso :o  
i nc lude .aso :o  
l 't l lh &SO ; O 
a t tend .aso :o  
cross .aso :o  
dominate .aso :o  
have .aso :s  
at t rac t .aso :s  
occupy .aso :o  
inc lude .aso :s  
COllt aJ n,  ~-'~s o :S 
beCOlne,g8:S 
fo rn l .aso :o  
co l lapse .as :s  
cre l~te.aso:o  
prov ide .aso :s  
o rga l l i ze .aso :o  
o f fe r .aso :s  
d c:; c5 d d d d d d d d d d d d ~ d c; d c:; d d d c5 d d d d c:; 6 
? ? ? 
? ? ? 
? ? 
? ? ? 
? ? ? 
o 
? ? ? 
? ? ? ? ? ? * ? ? 
? ? ? ? ? ? ? ? ? el ? ? ? 
? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
Figure 1: Class 19: "locative action". At the top are listed the 20 most t:)robable nouns in the 
pLc(n119 ) distribution and their probabilities, and at leg are tile 30 most probable verbs in the 
pLC(V 119) distribution. 19 is the class index. Those verb-noun pairs which were seen in the training 
data appear with a dot in the class matrix. Verbs with suffix .as : s indicate the subject slot of an 
active intransitive. Similarily .aso : s denotes the subject slot of an active transitive, and .aso : o 
denotes the object slot of an active transitive. 
semantics required by the target-verb. We eval- 
uated this simple method on a large number of 
real-world translations and got results compara- 
ble to related approaches such as that of Dagan 
and Itai (1994) where much more selectional in- 
t!ormation is used. 
2 Lex icon  Induct ion  v ia EM-Based  
C lus ter ing  
2.1 EM-Based Clustering 
For clustering, we used the method described 
in Rooth et al (1999). There classes are de- 
rived from distributional data a sample of 
pairs of verbs and nouns, gathered by parsing 
an unannotated corpus and extracting tile fillers 
of grammatical relations. The semantically 
smoothed probability of a pair (v,n) is calcu- 
lated in a latent class (LC) model as pLC(V, n) = 
~<cPLC(C, v,'n). The joint distribution is de- 
fined by PLC(C, v, n) = PLC(C)PLc(V\[C)PLC(nIC ). 
By construction, conditioning of v and n on 
each other is solely made through the classes 
c. The parameters PLC(C), PLC(V\[C), PLC(n\[c) 
are estilnated by a particularily silnple version 
of tile EM algorithm for context-free models. 
Input to our clustering algorithm was a train- 
ing corpus of 1,178,698 tokens (608,850 types) 
of verb-noun pairs participating in the gram- 
matical relations of intransitive and transitive 
verbs and their subject- and object-fillers. Fig. 
1 shows an induced class froln a model with 35 
classes. Induced classes often have a basis in lex- 
ical semantics; class 19 can be interpreted as 
locative, involving location nouns "room", "are?', 
and "world" and verbs as "enter" and "cross". 
2.2 Probabil ist ic Labeling with Latent 
Classes using EM-est imat ion 
To induce latent classes tbr the object slot; of a 
fixed transitive verb v, another statistical infer- 
ence step was performed. Given a latent class 
modal PLC(') Ibr verb-noun pairs, and a sam- 
ple n l , . . . ,  nM of objects for a fixed transitive 
verb, we calculate tile probability of ml arbitrary 
object noun ,I, I~ N by p(n) = ~<cP(C,  ~;,) = 
~<c P(c)pLc(n'Ic)" This fine-tuning of the class 
parameters p(c) to tile sample of objects for a 
fixed verb is formalized again as a simple in- 
stance of the EM algorithm. In an experiment 
with English data, we used a clustering model 
with 35 classes. From the maximum probabil- 
650 
ity pm:ses derived fl)r the British National Cor- 
pus with the head-lexicalized parser of Carroll 
and Rooth (1.998), we extracted frequency ta- 
bles tbr transitive verb-noun pairs. These tables 
were used to induce a small class-labeled lexicon 
(336 verbs). 
cross.aso:o 19 0.692 
mind 74.2 
road 30.3 
line 28.1 
1)ridge 27.5 
room 20.5 
t)order 17.8 
l)oundary 16.2 
river 14.6 
street 11.5 
atlantic 9.9 
mobilize.aso:o 6 0.386 
h)rce 2.00 
t)eoi)le 1.95 
army 1.46 
sector 0.90 
society 0.90 
worker 0.90 
meinber 0.88 
company 0.86 
majority 0.85 
party 0.80 
lID 160867) Es gibt einigc alte Passvorschriften, die. be- 
sagen, dass man cinch Pass habcn muss, wcnn man dic 
Grenze iiberschreitct. There are some old provisions re- 
ga.rding passports which state that people crossing the 
{border/ frontier/ boundary/ limit/ periphery/ 
edge} shoukI have their 1)assl)ort on them. 
lID 201946) Es 9ibt sehlie.sslich keinc L5sung ohne 
die Mobilisierung der bii~yerlichen Gesellschaft und 
die Solidaritiit dcr Dcmok,nten in der 9anzcn Welt. 
Ttmrc can be no solution, tinally, mflcss civilian {com- 
pany/  society/companionship/party/associate} 
is mobilized and solidarity demonstrated bydemocrats 
throughout the world. 
Figure 3: Exami)les for target-word ambiguities 
Figure 2: Estinmted fl:equencies of the objects 
of' the transitive verbs cross and mobi l ize 
Fig. 2 shows the topmost parts of the lexical 
entries for the transitive verbs cross and mo-  
bilize. Class 19 is the most prol)abh ~,class-label 
for the ol)jeet-slot of cross (prol)al)ility 0.692); 
tl~e objects of mobi l ize belong with prol)ability 
0.386 to class 16, which is the most probable 
(:lass for this slot. Fig. 2 shows for each verb the 
te l l  llOllllS 'It with highest estimated frequencies 
.l',,('n,) = f (n)p(cln), where . f lu) is the fre(\]ll(~.ll(:y 
of n in the sample v,l, ? ? ? , 'n,M. For example, the 
Dequency of seeing mind as object of c,ro.ss is 
estimated as 74.2 times, and the most fl'equent 
object of mobi l ize  is estimated to be force. 
3 Disambiguat ion  w i th  P robab i l i s t i c  
C lus ter -Based  Lex icons  
Ii:t the following, we will des(:ril)e the simt}le 
and natural lexicon look-up mechanism which 
is eml)loyed in our disambiguation at)t)roach. 
Consider Fig. 3 which shows two bilingual sen- 
tences taken from our evaluation corlms (see 
Sect. 4). The source-words and their correspond- 
ing target-words are highlighted in bo ld  thee. 
The correct translation of the source-noun (e.g. 
Gre.nzc) as deternfined by the actual trmlslators 
is replaced by the set of alterlmtive translations 
(e.g. { border, frontier, b(mndary, limit, peril)h- 
cry, edge }) as proposed by the word-to-word 
dictionary of Fig. 5 (see Sect. LI). 
The prol)lem to be solved is to lind a correct 
l;ranslation of the source-word using only min- 
imal contextual intbrmation. In our apt)roach , 
the decision between alternative target-nouns i
done by llSillg only int'ormal,ion provided by the 
governing target-verb. The key idea is to back 
up this nfinimal information with the condensed 
and precise information of a probabilistic lass- 
based lexicon. The criterion for choosing an al- 
terlmtive target-noun is thus the best fit of the 
lexical and semantic information of the target:- 
noun to the semantics of the argument-slot of 
the target-verb. This criterion is checked by a 
silnple lexicon look-up where the target-noun 
with highest estinmted class-based fl'equeney is 
determined. Fornmlly, choose l;11(; tm'get-nom~ gt,
(and a class ~?) such that 
j& , , )=   .ax 
'nC N~c~C 
where L-(-.) = f ( - , )v(d- . )  is the estimated fre- 
quency of 'n, in tile sample of objects of a 
fixed target-verb, p(cl,n ) is the class-melnbershi t)
probability of'n in c as determined by the proba- 
bilistic lexicon, and f (n)  is the frequency of n in 
the combined sample of objects and trmlslation 
alternatives1. 
Consider example ID 160867 fron, Fig. 3. The 
mnbiguity to be resolved concerns the direct ob- 
jects of the verb cross  whose lexical entry is 
partly shown in Fig. 2. Class 19 and the noun 
border is the pair yielding a higher estimated 
trequency than any other combination of a class 
and an alternative translation such as boundary. 
Similarly, for example ID 301946, the pair of the 
1Note that p(8) = max p(c) in most, but l lOt all cases. 
c E C  - - 
651 
target-noun society and class 6 gives highest es- 
timated frequency of the objects of mobilize. 
4 Evaluat ion 
We evaluated our resolution methods on a 
pseudo-disambiguation ask sinlilar to that used 
in Rooth et al (1999) for evaluating clustering 
models. We used a test set of 298 (v, n, n ~) triples 
where (v, n) is chosen randomly from a test cor- 
pus of pairs, and n ~ is chosen randomly accord- 
ing to the marginal noun distribution for the test 
corpus. Precision was calculated as the nmnber 
of times the disambiguation method decided for 
the non-random target noun (f~. = n). 
As shown in Fig. 4, we obtained 88 % pre- 
cision for the class-based lexicon (ProbLex), 
which is a gain of 9 % over the best cluster- 
ing model and a gain of 15 % over the hmnan 
baseline 2 .
human clustering ProbLex mnt)iguity baseline 
2 73.5 % 79.0 % 88.3 % I 
Figure 4: Evaluation on pseudo-disambiguation 
task for noun-ambiguity 
The results of the pseudo-disambiguation 
could be confirmed in a fllrther evaluation on a 
large number of randonfly selected examples of 
a real-world bilingual corpus. The corpus con- 
sists of sentence-aligned debates of the Euro- 
pean parliament (mlcc = multilingual corpus 
for cooperation) with ca. 9 million tokens for 
German and English. From this corpus we pre- 
pared a gold standard as follows. We gathered 
word-to-word translations from online-available 
dictionaries and eliminated German nouns fbr 
which we could not find at least two English 
translations in the mice-corpus. The resulting 
35 word dictionary is shown in Fig. 5. Based on 
this dictionary, we extracted all bilingual sen- 
tence pairs from the corpus which included both 
the source-noun and the target-noun. We re- 
stricted the resulting ca. 10,000 sentence pairs 
to those which included a source-noun from this 
2Similar esults for pseudo-dismnbiguation were ob- 
tained for a simpler approach which avoids an- 
other EM application for probabilistic lass labeling. 
Here ~ (and ~) was chosen such that f~(v,~) = 
max((fLc (v, n) + 1)pcc (el v, n)). However, the sensitivity 
to class-parmnetcrs was lost in this approach. 
dictionary in the object position of a verb. Fm'- 
therniore, the target-object was required to be 
included in our dictionary mid had to appear 
in a similar verb-object position as the source- 
object fbr an acceptable English translation of 
the German verb. We marked the German noun 
n q in the source-sentence, its English translation 
ne as appearing in the corpus, and the English 
lexical verb re. For the 35 word dictionary of 
Fig. 5 this senti-automatic procedure resulted 
ill a test corpus of 1,340 examples. The aver- 
age ambiguity in this test corpus is 8.63 trans- 
lations per source-word. Furthermore, we took 
the semantically most distant ranslations for 25 
words which occured with a certain fi'equency 
in the ew~luation corpus. This gave a corpus of 
814 examples with an average ambiguity of 2.83 
translations. The entries belonging to this dic- 
tionary are highlighted in bo ld  face in Fig. 5. 
The dictionaries and the related test corpora are 
available on the web 3. 
We believe that an evaluation on these test 
corpora is a realistic simulation of the hard task 
of target-language disambiguation i  real-word 
machine translation. The translation alterna- 
tives are selected fl'om online dictionaries, cor- 
rect translations are deternfined as the actual 
translations found in the bilingual corpus, no 
examples are omitted, the average ambiguity is 
high, and the translations are often very close 
to each other. In constrast o this, most other 
evaluations are based on frequent uses of only 
two clearly distant senses that were deternfined 
as interesting by the experimenters. 
Fig. 6 shows the results of lexical ambigu- 
ity resolution with probabilistic lcxica in com- 
parison to simpler methods. The rows show 
the results tbr evaluations on the two corpora 
with average ambiguity of 8.63 and 2.83 respec- 
tively. Colunm 2 shows the percentage of cor- 
rect translations found by disambiguation by 
random choice. Column 3 presents as another 
baseline disambiguation with the major sense, 
i.e., always choose the most frequent target- 
noun as translation of the source-noun. In col- 
unto 4, the empirical distribution of (v, n) pairs 
in the training corpus extracted from the BNC 
is used as disambiguator. Note that this method 
yields good results in terms of precision (P - 
#correct / $correct + $incorrect), but is much 
3http ://www. ims .uni-stuttgart. de/proj ekt e/gramot ron/ 
652 
Angrif \[  
A r t  
Au fgabe  
Auswahl 
Begriff 
Boden 
E in r i cht  ung  
Erwe i t  e rung  
Feh ler  
Gcnehmigung 
C leseh ichte  
Gesd lschaf t  
O| -e l |ze  
Grund  
Kar te  
Lage 
Mangel 
Menge 
Pr f i fung  
Schwler igke l t  
Se i te  
S icherhe i t  
S thnme 
' rer l l l in  
"Vet'blnd ung  
Verbot 
Verp f l i ch t  u ng  
"~'et't ra l len  
"Wahl 
"lgVeg 
Widers tand 
Zeiehen 
Ziel  
Z \[isaln 111 e llll alia 
Zust lmmung 
aggression, assault,  oll)2nce, onset, onsbmght,  attack , charge, raid, whammy, inroad 
form~ type ,  way ,  fashion, lit, kind, wise, lllallller, species, mode, sort, wtriety 
abandonment~ otIieo~ task ,  exercise, lesson, giveup, jot) , 1)roblcm, tax 
eligibility, selection, choice, wwlty, assortment,  extract,  range, sample 
concept, item, notion, idea 
ground,  land,  soi l ,  floor, bottom 
ar rangement ,  ins t i tu t ion ,  const itut ion,  cstablishlnellt,  feature, instal lation, construction, setup, adjustment,  composit ion, 
organizat ion 
ampl i f i ca t ion ,  extens ion ,  enhancement,  expansion, di latat ion,  upgr~ding, add-on, increment 
error~ shor tcoming ,  blemish, I)lunder, bug, defect, demeri t ,  failure, fault, flaw, mistake,  trouble, slip, blooper, lapsus 
pernlission, approval, COllSellt, acceptance, al)l)robation , author izat ion 
hlstory~ s tory ,  tale, saga, str ip 
company~ soc ie ty ,  COmlmnionshil), party, associate 
border ,  f ront ie r ,  boundary, Ihnl t ,  periphery, edge 
nlaster~ n lat ter~ reasoll~ base, catlse, grOlllld~ bottoli i  root 
card ,  map,  ticket, chart 
site, s i tuat ion,  position, bearing, layer, tier 
deficiency, lack, pr ivation, want, shortage, shortcoming,  absence, dearth,  demerit ,  des ideratum, insufticimlcy, paucity, scarceness 
alnol lnt~ deal ,  lot,  Illass I mtlltitttde, l)lenty, qtlalltity, quiverful~ vOhlllle 1 abull(latlce, aplellty 1 
assemblage , crowd, batch, crop, heal), lashings, scores, set, loads, I)ulk 
examinat lon ,  sc rut iny ,  ver i f i ca t ion ,  ordeal, test, trial,  inspection, tryout,  
assay, canvass, check, inquiry~ perusal, reconsideration, scrut ing 
difficttlty~ trol l l l le  1 problenl, severity, ar(lotlSlleSS 1heaviness 
page~ party~ s lde,  point, aspect 
cer ta in ty ,  guarantee ,  sa fe ty ,  immunity,  security , collateral , doubtlessness, ureness, deposit  
voice~ vote ,  tones 
elate, deadl ine~ meethtg ,  appointment,  t ime, term 
assoc la t ion ,  contact ,  link~ cha\[ll, ColIjtlnCtlOll~ COlll/ectioll~ fllSiOll, joint , conlpOtlll(l~ all iance, cl~tenation, tie, lllllOIl I t)Olld~ 
interface, liaison, touch, relation, incorporat ion 
ban, interdiction, I)rohibition, forbiddance 
eomin i tment :  ob l igat ion ,  under tak ing ,  duty, indebtedness , onus, debt, engagement,  liability, bond 
COllfidence~ re l lance ,  trl lst~ faith, asstlrance~ dependence,  pr ivate, secret 
e lec t ion ,  opt ion ,  choice , ballot, alternagive, poll , list 
path~ road ,  way ,  alley, route, lane 
resistance, opposit ion, drag 
character,  icon, Sigll I sigllal, Syllll)ol, lllark, tokell~ figure, olneil 
ahn ,  des t inat ion ,  end ,  designation, target,  goal, object, objective, sightings, intent imb prompt  
coherence, context~ COlltlgtllty, connectloli 
agree inent~ approvaI~ assont ,  accordance, approbat ion,  consent, af I innation, allowance, compliance, comi)Iiancy, acclamation 
Figure 5: Dictionaries extracted from online resources 
ambiguity random major emlfirical sense distrib, clusl;ering ProbLex 
P: 46.1% 
8.63 14.2 % 31.9 % E: 36.2 % 43.3 % 49.4 % 
P: 60.8 % 
2.83 35.9 % 45.5 % E: 49.4 % 61.5 % 68.2 % 
Figure 6: Disambig, mtion results for clustering versus probabilistic lexicon methods 
worse in terms of effectiveness (E //corre(-t 
/ \]/-correct q #:incorrect \]/:don't know). The 
reason for this is that even if the distribution 
(ff (v,n) pairs is estimated quite precisely for 
the pairs in the large training corpus, there are 
still many pairs which receive the same or no 
positive probability at all. These effects can'be 
overcome by a clustering approach to disam- 
biguation (column 5). Here the class-smoothed 
probability of a (v, n) pair is used to decide be- 
tween alternative target-nouns. Since the clus- 
tering model assigns a more fine-grained prob- 
ability to nearly every pair in its domain, there 
are no don't know cases for comparable preci- 
sion values. However, the senmntically smoothed 
probability of the clustering models is still too 
coarse-grained when compared to a disambigua- 
tion with a prot)abilistic lexicon. Here ~ fllrther 
gain in precision and equally effectiveness of ca. 
7 % is obtained on both corpora (column 6). 
We conjecture that this gain (:an be attrilmted 
to the combination of Dequency iilformation of 
the nouns and the fine-tuned istribution on the 
selection classes of the the nominal arguments 
of the verbs. We believe that including the set 
of translation alternatives in the ProbLex dis- 
tribution is important for increasing efficiency, 
because it gives the dismnbiguation model the 
opportunity to choose among unseen alterna- 
tives. Furthermore, it seems that the higher pre- 
cision of ProbLex can not be attributed to filling 
in zeroes in the empirical distribution. Rather, 
we speculate that ProbLex intelligently filters 
the empirical distribution by reducing maximal 
653 
counts for observations which do not fit into 
classes. This might help in cases where the em- 
pirical distribution has equal values for two al- 
ternatives. 
source target 
Seite 
Sicherheit 
Verbindung 
Verpflichtung 
Ziel 
overall precision 
page 
side 
guarantee 
safety 
commction 
link 
commitment 
obligation 
objective 
target 
Figure 7: Precision for finding correct and ac- 
ceptable translations by lexicon look-up 
Fig. 7 shows the results for disambiguation 
with probabilistic lexica for five sample words 
with two translations each. For this dictionary, 
a test corpus of 219 sentences was extracted, 200 
of which were additionally labeled with accept- 
able translations. Precision is 78 % for finding 
correct translations and 90 % for finding accept- 
able translations. 
Furthermore, in a subset of 100 test items 
with average ambiguity 8.6, a lmnlan judge hav- 
ing access only to the English verb and the set of 
candidates for the targel,-lloun, i.e. the informa- 
tion used by the model, selected anlong transla- 
tions. On this set;, human precision was 39 %. 
5 D iscuss ion  
Fig. 8 shows a comparison of our approadl 
to state-of-the-art unsupervised algorithlns for 
word sense disambiguation. Column 2 shows the 
number of test examples used to evaluate the 
various approaches. The range is from ca. 100 
examples to ca. 37,000 examples. Our method 
was evaluated on test corpora of sizes 219, 814, 
and 1,340. Column 3 gives the average number 
of senses/eranslations for the different disam- 
biguation methods. Here the range of the ambi- 
guity rate is from 2 to about 9 senses 4. Column 4 
4The mnbiguity factor 2.27 attributed to Dagan and 
Itai's (1994) experiment is calculated by dividing their 
average of 3.27 alternative translations by their average 
of 1.44 correct translations. Furthermore, we calculated 
the ambiguity factor 3.51 for Resnik's (1997) experiment 
shows the rmldom baselines cited for the respec- 
tive experiments, ranging t'rom ca. 11% to 50 %. 
Precision values are given in column 5. In order 
to compare these results which were computed 
for different ambiguity factors, we standardized 
the measures to an evaluation for binary ambi- 
guity. This is achieved by calculal;ing pl/log2 arab 
for precision p and ambiguity factor arab. The 
consistency of this "binarization" can be seen by 
a standardization of the different random base- 
lines which yields a value of ca. 50 % for all 
approaches 5. The standardized precision of our 
approach is ca. 79 % on all test corpora. The 
most direct point of comparison is the method 
of Dagan and Itai (1994) whirl1 gives 91.4 % pre- 
cision (92.7 % standardized) and 62.1% effec- 
tiveness (66.8 % standardized) on 103 test; exam- 
ples for target word selection in the transfer of 
Hebrew to English. However, colnpensating this 
high precision measure for the low effectiveness 
gives values comparable to our results. Dagan 
and Itai's (1994) method is based on a large vari- 
ety of gramnmtieal relations tbr verbal, nominal, 
and adjectival predicates, but no class-based in- 
fornmtion or slot-labeling is used. I{esnik (1997) 
presented a disambiguation method which yields 
44.3 % precision (63.8 % standardized) tbr a 
test set of 88 verb-object tokens. His approach is 
coral)arable to ours in terlns of infbrmedness of 
the (tisambiguator. Hc also uses a class-based se- 
lection measure, but based on WordNet classes. 
However, the task of his evaluation was to se- 
lect WordNet-senses tbr the objects rather than 
the objects themselves, so the results cannot 
be compared directly. The stone is true for the 
SENSEVAL evaluation exelcise (Kilgarriff and 
Rosenzweig, 2000)--there word senses from the 
HECTOl~-dictionary had to be disambiguated. 
The precision results for the ten unsupervised 
systems taking part in the comt)etitive valu- 
ation ranged Kern 20-65% at efficiency values 
from 3-54%. The SENSEVAL '~tan(lard is clearly 
beaten by the earlier results of Yarowsky (1995) 
(96.5 % precision) and Schiitze (1992) (92 % 
precision). However, a comparison to these re- 
from his random baseline 28.5 % by taking 100/28.5; re- 
versely, Dagan and Itai's (1994) random baseline can be 
calculated as 100/2.27 = 44.05. Tile ambiguity t;'~ctor for 
SENSEVAL is calculated for tile llOUll task in the English 
SENSEVAL test set. 
5Note that  we are guaranteed to get exactly 50 % 
standardized random 1)aseline if random, arab = 100 %. 
654 
disambiguation corlms random precision 
method size aml)iguity random 1)recision (standardized) (standardized) 
)robLex 1 340 8.63 14.2 % 49.4 % 53.4 % 79.7 % 
814 2.83 35.9 % 68.2 % 50.5 % 77.5 % 
219 2 50.0 % 78.0 % 50.0 % 78.0 % 
)agan, Itai 94 
{esnik 97 
;ENSEVAL 00 
(m'owsky 95 
',chiitze 92 
103 
88 
2 756 
37 000 
3 000 
2.27 
3.51 
9.17 
2 
2 
44.1% 
28.5 % 
10.9 % 
50.0 % 
50.0 % 
P: 91.4 % 
E: 62.1% 
44.3 % 
P: 20-65 % 
E: 3-54 % 
96.5 % 
92.0 % 
50.0 % 
50.0 % 
50.0 % 
50.0 % 
50.0 % 
P: 92.7 % 
E: 66.8 % 
63.8 % 
P: 60-87 % 
E: 33-83 % 
96.5 % 
92.0 % 
Figure 8: Comparison of unsupervised lexical disambiguation methods. 
sults is again somewhat difficult. Firstly, these 
at)proaches were ewfluated on words with two 
clearly (tistmlt senses which were de/;el'nfined by 
the experimenters. In contrast, our method was 
evalutated on randonfly selected actual transla- 
tions of a large t)ilingual cortms. Furthermore, 
these apl)roaches use large amounts of infbrma- 
tion in terms of linguistic ca.tegorizations, large 
context windows, or even 1111nual intervention 
such as initial sense seeding (hqtrowsky, 1995). 
Such information is easily obtainabh;, e.g., in I1\]. 
at)tflications , but often burdensome to gather or 
sim.i)ly uslavail~bh'~ in situations such as incre- 
mental parsing O1' translation. 
6 Conc lus ion  
The disanfl3iguation method presented in this 
pa.per delibera.tely is restricted to the limited 
mnomlt of information provided by a proba- 
bilistic class-based lexicon. This intbrmation yet 
proves itself accurate nough to yield good em- 
pirical results, e.g., in target-language disam- 
biguation. The t)rol)al)ilistic class-based lexica 
are induced in an unsupervised manner fl'om 
large mmnnotated corpora. Once the lexica are 
constructed, lexical mnbiguity resolution can be 
done by a simple lexicon look-up. I51 target- 
word selection, the nlOSt fl'equent target-noun 
whose semantics fits best to tit(; semantics of the 
argument-slot of the target-verb is chosen. We 
evaluated our method on randomly selected ex- 
amities Dora real-world bilingual corpora which 
constitutes a realistic hard task. Dismnbiguation 
based on probabilistie lexica perfornmd satisfim-' 
tory for this |;ask. The lesson lem'ned tYom our 
experimental results is that hybrid models con> 
bining fi:equency information and class-based 
t)robabilities outlmrtbnn both pure fl'equency- 
based models and pure clustering models. 1'511"- 
ther improvements are to be expected from 
extended lexica including, e.g., adjectival and 
prepositional predicates. 
References  
Gleml Carroll mid Mats F\[ooth. 1998. Valence 
induction with a head-lexicalized PCFG. In 
P'roceediugs o.f EMNLP-,7, Granada. 
Ido l)agan and Ahm Itai. 1994:. Word sense dis- 
ambiguation using a second language 1110510- 
linguaJ corlms. Computational Linguistics, 
20:563 596. 
Adam Kilgarriff and Joseph lq.osenzweig. 2000. 
English SENSEVAIA I-{.el)ol't and results. In 
Proceedings of LR\]';C 2000. 
Philip l{csnik. 1997. Selectional preference and 
sense dis~mfl)iguation, l\[ll Proceedings of the 
ANLP'97 Workshop: Tagging Tc:ct 'with Lezi- 
cal Semantics: Why, What, and How?, V~:ash- 
ington, D.C. 
Mats l\].ooth, Stefan I{iezler, Detlef Prescher, 
Glenn Carroll, and Franz Bell. 1999. Induc- 
ing a semantically annotated lexicon via EM- 
based clustering. In Proceedings of the 37th 
Annual Meeting of th, c Assoc.iation .for Com,- 
putational Linguistics (A CL '99), Maryland. 
Ilinrieh Schfitze. 1992. Dimensions of meaning. 
151 Proceedings of S'upercomlnd.ing '92. 
David Yarowsky. 1995. Unsupervised word 
sense dismnbiguation rivaling supervised 
methods. In Proceedings of the 33rd Annual 
Meeting of th, c Association for Compv, tational 
Linguistics (ACL'95), Cambridge, MA. 
655 
Lexicalized Stochastic Modeling of Constraint-Based Grammars
using Log-Linear Measures and EM Training
Stefan Riezler
IMS, Universit?t Stuttgart
riezler@ims.uni-stuttgart.de
Detlef Prescher
IMS, Universit?t Stuttgart
prescher@ims.uni-stuttgart.de
Jonas Kuhn
IMS, Universit?t Stuttgart
jonas@ims.uni-stuttgart.de
Mark Johnson
Cog. & Ling. Sciences, Brown University
Mark_Johnson@brown.edu
Abstract
We present a new approach to
stochastic modeling of constraint-
based grammars that is based on log-
linear models and uses EM for esti-
mation from unannotated data. The
techniques are applied to an LFG
grammar for German. Evaluation on
an exact match task yields 86% pre-
cision for an ambiguity rate of 5.4,
and 90% precision on a subcat frame
match for an ambiguity rate of 25.
Experimental comparison to train-
ing from a parsebank shows a 10%
gain from EM training. Also, a new
class-based grammar lexicalization is
presented, showing a 10% gain over
unlexicalized models.
1 Introduction
Stochastic parsing models capturing contex-
tual constraints beyond the dependencies of
probabilistic context-free grammars (PCFGs)
are currently the subject of intensive research.
An interesting feature common to most such
models is the incorporation of contextual de-
pendencies on individual head words into rule-
based probability models. Such word-based
lexicalizations of probability models are used
successfully in the statistical parsing mod-
els of, e.g., Collins (1997), Charniak (1997),
or Ratnaparkhi (1997). However, it is still
an open question which kind of lexicaliza-
tion, e.g., statistics on individual words or
statistics based upon word classes, is the best
choice. Secondly, these approaches have in
common the fact that the probability models
are trained on treebanks, i.e., corpora of man-
ually disambiguated sentences, and not from
corpora of unannotated sentences. In all of the
cited approaches, the Penn Wall Street Jour-
nal Treebank (Marcus et al, 1993) is used,
the availability of which obviates the standard
eort required for treebank traininghand-
annotating large corpora of specic domains
of specic languages with specic parse types.
Moreover, common wisdom is that training
from unannotated data via the expectation-
maximization (EM) algorithm (Dempster et
al., 1977) yields poor results unless at
least partial annotation is applied. Experi-
mental results conrming this wisdom have
been presented, e.g., by Elworthy (1994) and
Pereira and Schabes (1992) for EM training
of Hidden Markov Models and PCFGs.
In this paper, we present a new lexicalized
stochastic model for constraint-based gram-
mars that employs a combination of head-
word frequencies and EM-based clustering
for grammar lexicalization. Furthermore, we
make crucial use of EM for estimating the
parameters of the stochastic grammar from
unannotated data. Our usage of EM was ini-
tiated by the current lack of large unication-
based treebanks for German. However, our ex-
perimental results also show an exception to
the common wisdom of the insuciency of EM
for highly accurate statistical modeling.
Our approach to lexicalized stochastic mod-
eling is based on the parametric family of log-
linear probability models, which is used to de-
ne a probability distribution on the parses
of a Lexical-Functional Grammar (LFG) for
German. In previous work on log-linear mod-
els for LFG by Johnson et al (1999), pseudo-
likelihood estimation from annotated corpora
has been introduced and experimented with
on a small scale. However, to our knowledge,
to date no large LFG annotated corpora of
unrestricted German text are available. For-
tunately, algorithms exist for statistical infer-
ence of log-linear models from unannotated
data (Riezler, 1999). We apply this algorithm
to estimate log-linear LFG models from large
corpora of newspaper text. In our largest ex-
periment, we used 250,000 parses which were
produced by parsing 36,000 newspaper sen-
tences with the German LFG. Experimental
evaluation of our models on an exact-match
task (i.e. percentage of exact match of most
probable parse with correct parse) on 550
manually examined examples with on average
5.4 analyses gave 86% precision. Another eval-
uation on a verb frame recognition task (i.e.
percentage of agreement between subcatego-
rization frames of main verb of most proba-
ble parse and correct parse) gave 90% pre-
cision on 375 manually disambiguated exam-
ples with an average ambiguity of 25. Clearly,
a direct comparison of these results to state-
of-the-art statistical parsers cannot be made
because of dierent training and test data and
other evaluation measures. However, we would
like to draw the following conclusions from our
experiments:
 The problem of chaotic convergence be-
haviour of EM estimation can be solved
for log-linear models.
 EM does help constraint-based gram-
mars, e.g. using about 10 times more sen-
tences and about 100 times more parses
for EM training than for training from an
automatically constructed parsebank can
improve precision by about 10%.
 Class-based lexicalization can yield a gain
in precision of about 10%.
In the rest of this paper we intro-
duce incomplete-data estimation for log-linear
models (Sec. 2), and present the actual design
of our models (Sec. 3) and report our experi-
mental results (Sec. 4).
2 Incomplete-Data Estimation for
Log-Linear Models
2.1 Log-Linear Models
A log-linear distribution p

(x) on the set of
analyses X of a constraint-based grammar can
be dened as follows:
p

(x) = Z

 1
e
(x)
p
0
(x)
where Z

=
P
x2X
e
(x)
p
0
(x) is a normal-
izing constant,  = (
1
; : : : ; 
n
) 2 IR
n
is a
vector of log-parameters,  = (
1
; : : : ; 
n
) is
a vector of property-functions 
i
: X ! IR for
i = 1; : : : ; n,   (x) is the vector dot prod-
uct
P
n
i=1

i

i
(x), and p
0
is a xed reference
distribution.
The task of probabilistic modeling with log-
linear distributions is to build salient proper-
ties of the data as property-functions 
i
into
the probability model. For a given vector  of
property-functions, the task of statistical in-
ference is to tune the parameters  to best
reect the empirical distribution of the train-
ing data.
2.2 Incomplete-Data Estimation
Standard numerical methods for statis-
tical inference of log-linear models from
fully annotated dataso-called complete
dataare the iterative scaling meth-
ods of Darroch and Ratcli (1972) and
Della Pietra et al (1997). For data consisting
of unannotated sentencesso-called incom-
plete datathe iterative method of the EM
algorithm (Dempster et al, 1977) has to be
employed. However, since even complete-data
estimation for log-linear models requires
iterative methods, an application of EM to
log-linear models results in an algorithm
which is expensive since it is doubly-iterative.
A singly-iterative algorithm interleaving EM
and iterative scaling into a mathematically
well-dened estimation method for log-linear
models from incomplete data is the IM
algorithm of Riezler (1999). Applying this
algorithm to stochastic constraint-based
grammars, we assume the following to be
given: A training sample of unannotated sen-
tences y from a set Y, observed with empirical
Input Reference model p
0
, property-functions vector  with constant 
#
, parses
X(y) for each y in incomplete-data sample from Y.
Output MLE model p


on X .
Procedure
Until convergence do
Compute p

; k

, based on  = (
1
; : : : ; 
n
),
For i from 1 to n do

i
:=
1

#
ln
P
y2Y
~p(y)
P
x2X(y)
k

(xjy)
i
(x)
P
x2X
p

(x)
i
(x)
,

i
:= 
i
+ 
i
,
Return 

= (
1
; : : : ; 
n
).
Figure 1: Closed-form version of IM algorithm
probability ~p(y), a constraint-based grammar
yielding a set X(y) of parses for each sentence
y, and a log-linear model p

() on the parses
X =
P
y2Yj~p(y)>0
X(y) for the sentences in
the training corpus, with known values of
property-functions  and unknown values
of . The aim of incomplete-data maximum
likelihood estimation (MLE) is to nd a value


that maximizes the incomplete-data log-
likelihood L =
P
y2Y
~p(y) ln
P
x2X(y)
p

(x),
i.e.,


= argmax
2IR
n
L():
Closed-form parameter-updates for this prob-
lem can be computed by the algorithm of Fig.
1, where 
#
(x) =
P
n
i=1

i
(x), and k

(xjy) =
p

(x)=
P
x2X(y)
p

(x) is the conditional prob-
ability of a parse x given the sentence y and
the current parameter value .
The constancy requirement on 
#
can be
enforced by adding a correction property-
function 
l
:
Choose K = max
x2X

#
(x) and

l
(x) = K   
#
(x) for all x 2 X .
Then
P
l
i=1

i
(x) = K for all x 2 X .
Note that because of the restriction of X to
the parses obtainable by a grammar from the
training corpus, we have a log-linear probabil-
ity measure only on those parses and not on
all possible parses of the grammar. We shall
therefore speak of mere log-linear measures in
our application of disambiguation.
2.3 Searching for Order in Chaos
For incomplete-data estimation, a sequence
of likelihood values is guaranteed to converge
to a critical point of the likelihood function
L. This is shown for the IM algorithm in
Riezler (1999). The process of nding likeli-
hood maxima is chaotic in that the nal likeli-
hood value is extremely sensitive to the start-
ing values of , i.e. limit points can be lo-
cal maxima (or saddlepoints), which are not
necessarily also global maxima. A way to
search for order in this chaos is to search for
starting values which are hopefully attracted
by the global maximum of L. This problem
can best be explained in terms of the mini-
mum divergence paradigm (Kullback, 1959),
which is equivalent to the maximum likeli-
hood paradigm by the following theorem. Let
p[f ] =
P
x2X
p(x)f(x) be the expectation of
a function f with respect to a distribution p:
The probability distribution p

that
minimizes the divergence D(pjjp
0
) to
a reference model p
0
subject to the
constraints p[
i
] = q[
i
]; i = 1; : : : ; n
is the model in the parametric fam-
ily of log-linear distributions p

that
maximizes the likelihood L() =
q[ln p

] of the training data
1
.
1
If the training sample consists of complete data
Reasonable starting values for minimum di-
vergence estimation is to set 
i
= 0 for
i = 1; : : : ; n. This yields a distribution which
minimizes the divergence to p
0
, over the
set of models p to which the constraints
p[
i
] = q[
i
]; i = 1; : : : ; n have yet to be ap-
plied. Clearly, this argument applies to both
complete-data and incomplete-data estima-
tion. Note that for a uniformly distributed
reference model p
0
, the minimum divergence
model is a maximum entropy model (Jaynes,
1957). In Sec. 4, we will demonstrate that
a uniform initialization of the IM algorithm
shows a signicant improvement in likelihood
maximization as well as in linguistic perfor-
mance when compared to standard random
initialization.
3 Property Design and
Lexicalization
3.1 Basic Congurational Properties
The basic 190 properties employed in our
models are similar to the properties of
Johnson et al (1999) which incorporate gen-
eral linguistic principles into a log-linear
model. They refer to both the c(onstituent)-
structure and the f(eature)-structure of the
LFG parses. Examples are properties for
 c-structure nodes, corresponding to stan-
dard production properties,
 c-structure subtrees, indicating argument
versus adjunct attachment,
 f-structure attributes, corresponding to
grammatical functions used in LFG,
 atomic attribute-value pairs in f-
structures,
 complexity of the phrase being attached
to, thus indicating both high and low at-
tachment,
 non-right-branching behavior of nonter-
minal nodes,
 non-parallelism of coordinations.
x 2 X , the expectation q[] corresponds to the em-
pirical expectation ~p[]. If we observe incomplete data
y 2 Y, the expectation q[] is replaced by the condi-
tional expectation ~p[k

0
[]] given the observed data y
and the current parameter value 
0
.
3.2 Class-Based Lexicalization
Our approach to grammar lexicalization is
class-based in the sense that we use class-
based estimated frequencies f
c
(v; n) of head-
verbs v and argument head-nouns n in-
stead of pure frequency statistics or class-
based probabilities of head word dependen-
cies. Class-based estimated frequencies are in-
troduced in Prescher et al (2000) as the fre-
quency f(v; n) of a (v; n)-pair in the train-
ing corpus, weighted by the best estimate of
the class-membership probability p(cjv; n) of
an EM-based clustering model on (v; n)-pairs,
i.e., f
c
(v; n) = max
c2C
p(cjv; n)(f(v; n) + 1).
As is shown in Prescher et al (2000) in an
evaluation on lexical ambiguity resolution, a
gain of about 7% can be obtained by using
the class-based estimated frequency f
c
(v; n)
as disambiguation criterion instead of class-
based probabilities p(njv). In order to make
the most direct use possible of this fact, we
incorporated the decisions of the disambigua-
tor directly into 45 additional properties for
the grammatical relations of the subject, di-
rect object, indirect object, innitival object,
oblique and adjunctival dative and accusative
preposition, for active and passive forms of the
rst three verbs in each parse. Let v
r
(x) be the
verbal head of grammatical relation r in parse
x, and n
r
(x) the nominal head of grammatical
relation r in x. Then a lexicalized property 
r
for grammatical relation r is dened as

r
(x) =
8
<
:
1
if f
c
(v
r
(x); n
r
(x)) 
f
c
(v
r
(x
0
); n
r
(x
0
)) 8x
0
2 X(y);
0 otherwise:
The property-function 
r
thus pre-
disambiguates the parses x 2 X(y) of a
sentence y according to f
c
(v; n), and stores
the best parse directly instead of taking the
actual estimated frequencies as its value. In
Sec. 4, we will see that an incorporation of
this pre-disambiguation routine into the mod-
els improves performance in disambiguation
by about 10%.
exact match
evaluation
basic
model
lexicalized
model
selected
+ lexicalized
model
complete-data
estimation
P: 68
E: 59.6
P: 73.9
E: 71.6
P: 74.3
E: 71.8
incomplete-data
estimation
P: 73
E: 65.4
P: 86
E: 85.2
P: 86.1
E: 85.4
Figure 2: Evaluation on exact match task for 550 examples with average ambiguity 5.4
frame match
evaluation
basic
model
lexicalized
model
selected
+ lexicalized
model
complete-data
estimation
P: 80.6
E: 70.4
P: 82.7
E: 76.4
P: 83.4
E: 76
incomplete-data
estimation
P: 84.5
E: 73.1
P: 88.5
E: 84.9
P: 90
E: 86.3
Figure 3: Evaluation on frame match task for 375 examples with average ambiguity 25
4 Experiments
4.1 Incomplete Data and Parsebanks
In our experiments, we used an LFG grammar
for German
2
for parsing unrestricted text.
Since training was faster than parsing, we
parsed in advance and stored the resulting
packed c/f-structures. The low ambiguity rate
of the German LFG grammar allowed us to
restrict the training data to sentences with
at most 20 parses. The resulting training cor-
pus of unannotated, incomplete data consists
of approximately 36,000 sentences of online
available German newspaper text, comprising
approximately 250,000 parses.
In order to compare the contribution of un-
ambiguous and ambiguous sentences to the es-
timation results, we extracted a subcorpus of
4,000 sentences, for which the LFG grammar
produced a unique parse, from the full train-
2
The German LFG grammar is being imple-
mented in the Xerox Linguistic Environment (XLE,
see Maxwell and Kaplan (1996)) as part of the Paral-
lel Grammar (ParGram) project at the IMS Stuttgart.
The coverage of the grammar is about 50% for unre-
stricted newspaper text. For the experiments reported
here, the eective coverage was lower, since the cor-
pus preprocessing we applied was minimal. Note that
for the disambiguation task we were interested in,
the overall grammar coverage was of subordinate rel-
evance.
ing corpus. The average sentence length of
7.5 for this automatically constructed parse-
bank is only slightly smaller than that of
10.5 for the full set of 36,000 training sen-
tences and 250,000 parses. Thus, we conjec-
ture that the parsebank includes a representa-
tive variety of linguistic phenomena. Estima-
tion from this automatically disambiguated
parsebank enjoys the same complete-data es-
timation properties
3
as training from manu-
ally disambiguated treebanks. This makes a
comparison of complete-data estimation from
this parsebank to incomplete-data estimation
from the full set of training data interesting.
4.2 Test Data and Evaluation Tasks
To evaluate our models, we constructed
two dierent test corpora. We rst parsed
with the LFG grammar 550 sentences
which are used for illustrative purposes in
the foreign language learner's grammar of
Helbig and Buscha (1996). In a next step, the
correct parse was indicated by a human dis-
ambiguator, according to the reading intended
in Helbig and Buscha (1996). Thus a precise
3
For example, convergence to the global maximum
of the complete-data log-likelihood function is guar-
anteed, which is a good condition for highly precise
statistical disambiguation.
indication of correct c/f-structure pairs was
possible. However, the average ambiguity of
this corpus is only 5.4 parses per sentence, for
sentences with on average 7.5 words. In order
to evaluate on sentences with higher ambigu-
ity rate, we manually disambiguated further
375 sentences of LFG-parsed newspaper text.
The sentences of this corpus have on average
25 parses and 11.2 words.
We tested our models on two evalua-
tion tasks. The statistical disambiguator was
tested on an exact match task, where ex-
act correspondence of the full c/f-structure
pair of the hand-annotated correct parse and
the most probable parse is checked. Another
evaluation was done on a frame match task,
where exact correspondence only of the sub-
categorization frame of the main verb of the
most probable parse and the correct parse is
checked. Clearly, the latter task involves a
smaller eective ambiguity rate, and is thus
to be interpreted as an evaluation of the com-
bined system of highly-constrained symbolic
parsing and statistical disambiguation.
Performance on these two evaluation tasks
was assessed according to the following evalu-
ation measures:
Precision =
#correct
#correct+#incorrect
,
Eectiveness =
#correct
#correct+#incorrect+#don't know
.
Correct and incorrect species a suc-
cess/failure on the respective evaluation tasks;
don't know cases are cases where the system
is unable to make a decision, i.e. cases with
more than one most probable parse.
4.3 Experimental Results
For each task and each test corpus, we cal-
culated a random baseline by averaging over
several models with randomly chosen pa-
rameter values. This baseline measures the
disambiguation power of the pure symbolic
parser. The results of an exact-match evalu-
ation on the Helbig-Buscha corpus is shown
in Fig. 2. The random baseline was around
33% for this case. The columns list dierent
models according to their property-vectors.
Basic models consist of 190 congurational
properties as described in Sec. 3.1. Lexical-
ized models are extended by 45 lexical pre-
disambiguation properties as described in Sec.
3.2. Selected + lexicalized models result
from a simple property selection procedure
where a cuto on the number of parses with
non-negative value of the property-functions
was set. Estimation of basic models from com-
plete data gave 68% precision (P), whereas
training lexicalized and selected models from
incomplete data gave 86.1% precision, which
is an improvement of 18%. Comparing lex-
icalized models in the estimation method
shows that incomplete-data estimation gives
an improvement of 12% precision over train-
ing from the parsebank. A comparison of mod-
els trained from incomplete data shows that
lexicalization yields a gain of 13% in preci-
sion. Note also the gain in eectiveness (E)
due to the pre-disambigution routine included
in the lexicalized properties. The gain due to
property selection both in precision and eec-
tiveness is minimal. A similar pattern of per-
formance arises in an exact match evaluation
on the newspaper corpus with an ambiguity
rate of 25. The lexicalized and selected model
trained from incomplete data achieved here
60.1% precision and 57.9% eectiveness, for a
random baseline of around 17%.
As shown in Fig. 3, the improvement in per-
formance due to both lexicalization and EM
training is smaller for the easier task of frame
evaluation. Here the random baseline is 70%
for frame evaluation on the newspaper corpus
with an ambiguity rate of 25. An overall gain
of roughly 10% can be achieved by going from
unlexicalized parsebank models (80.6% preci-
sion) to lexicalized EM-trained models (90%
precision). Again, the contribution to this im-
provement is about the same for lexicalization
and incomplete-data training. Applying the
same evaluation to the Helbig-Buscha corpus
shows 97.6% precision and 96.7% eectiveness
for the lexicalized and selected incomplete-
data model, compared to around 80% for the
random baseline.
Optimal iteration numbers were decided by
repeated evaluation of the models at every
fth iteration. Fig. 4 shows the precision of
lexicalized and selected models on the exact
68
70
72
74
76
78
80
82
84
86
88
10 20 30 40 50 60 70 80 90
pre
cis
ion
number of iterations
complete-data estimation
incomplete-data estimation
Figure 4: Precision on exact match task in number of training iterations
match task plotted against the number of it-
erations of the training algorithm. For parse-
bank training, the maximal precision value
is obtained at 35 iterations. Iterating fur-
ther shows a clear overtraining eect. For
incomplete-data estimation more iterations
are necessary to reach a maximal precision
value. A comparison of models with random
or uniform starting values shows an increase
in precision of 10% to 40% for the latter.
In terms of maximization of likelihood, this
corresponds to the fact that uniform starting
values immediately push the likelihood up to
nearly its nal value, whereas random starting
values yield an initial likelihood which has to
be increased by factors of 2 to 20 to an often
lower nal value.
5 Discussion
The most direct points of compar-
ison of our method are the ap-
proaches of Johnson et al (1999) and
Johnson and Riezler (2000). In the rst ap-
proach, log-linear models on LFG grammars
using about 200 congurational properties
were trained on treebanks of about 400
sentences by maximum pseudo-likelihood
estimation. Precision was evaluated on an
exact match task in a 10-way cross valida-
tion paradigm for an ambiguity rate of 10,
and achieved 59% for the rst approach.
Johnson and Riezler (2000) achieved a gain
of 1% over this result by including a class-
based lexicalization. Our best models clearly
outperform these results, both in terms of
precision relative to ambiguity and in terms
of relative gain due to lexicalization. A
comparison of performance is more dicult
for the lexicalized PCFG of Beil et al (1999)
which was trained by EM on 450,000 sen-
tences of German newspaper text. There, a
70.4% precision is reported on a verb frame
recognition task on 584 examples. However,
the gain achieved by Beil et al (1999) due to
grammar lexicalizaton is only 2%, compared
to about 10% in our case. A comparison
is dicult also for most other state-of-the-
art PCFG-based statistical parsers, since
dierent training and test data, and most
importantly, dierent evaluation criteria were
used. A comparison of the performance gain
due to grammar lexicalization shows that our
results are on a par with that reported in
Charniak (1997).
6 Conclusion
We have presented a new approach to stochas-
tic modeling of constraint-based grammars.
Our experimental results show that EM train-
ing can in fact be very helpful for accurate
stochastic modeling in natural language pro-
cessing. We conjecture that this result is due
partly to the fact that the space of parses
produced by a constraint-based grammar is
only mildly incomplete, i.e. the ambiguity
rate can be kept relatively low. Another rea-
son may be that EM is especially useful for
log-linear models, where the search space in
maximization can be kept under control. Fur-
thermore, we have introduced a new class-
based grammar lexicalization, which again
uses EM training and incorporates a pre-
disambiguation routine into log-linear models.
An impressive gain in performance could also
be demonstrated for this method. Clearly, a
central task of future work is a further explo-
ration of the relation between complete-data
and incomplete-data estimation for larger,
manually disambiguated treebanks. An inter-
esting question is whether a systematic vari-
ation of training data size along the lines
of the EM-experiments of Nigam et al (2000)
for text classication will show similar results,
namely a systematic dependence of the rela-
tive gain due to EM training from the relative
sizes of unannotated and annotated data. Fur-
thermore, it is important to show that EM-
based methods can be applied successfully
also to other statistical parsing frameworks.
Acknowledgements
We thank Stefanie Dipper and Bettina
Schrader for help with disambiguation of the
test suites, and the anonymous ACL review-
ers for helpful suggestions. This research was
supported by the ParGram project and the
project B7 of the SFB 340 of the DFG.
References
Franz Beil, Glenn Carroll, Detlef Prescher, Stefan
Riezler, and Mats Rooth. 1999. Inside-outside
estimation of a lexicalized PCFG for German.
In Proceedings of the 37th ACL, College Park,
MD.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the 14th AAAI, Menlo Park, CA.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th ACL, Madrid.
J.N. Darroch and D. Ratcli. 1972. General-
ized iterative scaling for log-linear models. The
Annals of Mathematical Statistics, 43(5):1470
1480.
Stephen Della Pietra, Vincent Della Pietra, and
John Laerty. 1997. Inducing features of ran-
dom elds. IEEE PAMI, 19(4):380393.
A. P. Dempster, N. M. Laird, and D. B. Ru-
bin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of
the Royal Statistical Society, 39(B):138.
David Elworthy. 1994. Does Baum-Welch re-
estimation help taggers? In Proceedings of the
4th ANLP, Stuttgart.
Gerhard Helbig and Joachim Buscha. 1996.
Deutsche Grammatik. Ein Handbuch f?r den
Ausl?nderunterricht. Langenscheidt, Leipzig.
Edwin T. Jaynes. 1957. Information theory
and statistical mechanics. Physical Review,
106:620630.
Mark Johnson and Stefan Riezler. 2000. Ex-
ploiting auxiliary distributions in stochastic
unication-based grammars. In Proceedings of
the 1st NAACL, Seattle, WA.
Mark Johnson, Stuart Geman, Stephen Canon,
Zhiyi Chi, and Stefan Riezler. 1999. Estimators
for stochastic unication-based grammars. In
Proceedings of the 37th ACL, College Park, MD.
Solomon Kullback. 1959. Information Theory and
Statistics. Wiley, New York.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Build-
ing a large annotated corpus of english: The
Penn treebank. Computational Linguistics,
19(2):313330.
John Maxwell and R. Kaplan. 1996. Unication-
based parsers that automatically take ad-
vantage of context freeness. Unpublished
manuscript, Xerox Palo Alto Research Center.
Kamal Nigam, Andrew McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classi-
cation from labeled and unlabeled documents
using EM. Machine Learning, 39(2/4):103134.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed
corpora. In Proceedings of the 30th ACL,
Newark, Delaware.
Detlef Prescher, Stefan Riezler, and Mats Rooth.
2000. Using a probabilistic class-based lexicon
for lexical ambiguity resolution. In Proceedings
of the 18th COLING, Saarbr?cken.
Adwait Ratnaparkhi. 1997. A linear observed
time statistical parser based on maximum en-
tropy models. In Proceedings of EMNLP-2.
Stefan Riezler. 1999. Probabilistic Constraint
Logic Programming Ph.D. thesis, Seminar
f?r Sprachwissenschaft, Universit?t T?bingen.
AIMS Report, 5(1), IMS, Universit?t Stuttgart.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 737?744
Manchester, August 2008
Translating Queries into Snippets for Improved Query Expansion
Stefan Riezler and Yi Liu and Alexander Vasserman
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043
{riezler,yliu,avasserm}@google.com
Abstract
User logs of search engines have recently
been applied successfully to improve var-
ious aspects of web search quality. In this
paper, we will apply pairs of user queries
and snippets of clicked results to train a
machine translation model to bridge the
?lexical gap? between query and document
space. We show that the combination of
a query-to-snippet translation model with
a large n-gram language model trained
on queries achieves improved contextual
query expansion compared to a system
based on term correlations.
1 Introduction
In recent years, user logs of search engines have at-
tracted considerable attention in research on query
clustering, query suggestions, query expansion, or
general web search. Besides the sheer size of these
data sets, the main attraction of user logs lies in
the possibility to capitalize on users? input, either
in form of user-generated query reformulations, or
in form of user clicks on presented search results.
However noisy, sparse, incomplete, and volatile
these data may be, recent research has presented
impressive results that are based on simply taking
the majority vote of user clicks as a signal for the
relevance of results.
In this paper we will apply user logs to the prob-
lem of the ?word mismatch? or ?lexical chasm?
(Berger et al, 2000) between user queries and
documents. The standard solution to this prob-
lem, query expansion, attempts to overcome this
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
mismatch in query and document vocabularies by
adding terms with similar statistical properties to
those in the original query. This will increase the
chances of matching words in relevant documents
and also decrease the ambiguity of the overall
query that is inherent to natural language. A suc-
cessful approach to this problem is local feed-
back, or pseudo-relevance feedback (Xu and Croft,
1996), where expansion terms are extracted from
the top-most documents that were retrieved in an
initial retrieval round. Because of irrelevant results
in the initial retrieval, caused by ambiguous terms
or retrieval errors, this technique may cause expan-
sion by unrelated terms, leading to query drift. Fur-
thermore, the requirement of two retrieval steps is
computationally expensive.
Several approaches have been presented that de-
ploy user query logs to remedy these problems.
One set of approaches focuses on user reformu-
lations of queries that differ only in one segment
(Jones et al, 2006; Fonseca et al, 2005; Huang
et al, 2003). Such segments are then identified
as candidate expansion terms, and filtered by var-
ious signals such as cooccurrence in similar ses-
sions or log-likelihood ratio of original and ex-
pansion phrases. Other approaches focus on the
relation of queries and retrieval results, either by
deploying the graph induced by queries and user
clicks in calculating query similarity (Beeferman
and Berger, 2000; Wen et al, 2002; Baeza-Yates
and Tiberi, 2007), or by leveraging top results from
past queries to provide greater context in find-
ing related queries (Raghavan and Sever, 1995;
Fitzpatrick and Dent, 1997; Sahami and Heilman,
2006). Cui et al (2002) present an all together dif-
ferent way to deploy user clickthrough data by ex-
tracting expansion terms directly from clicked re-
sults. They claim significant improvements over
737
the local feedback technique of Xu and Croft
(1996).
Cui et al?s (2002) work is the closest to ours.
We follow their approach in extracting expansion
terms directly from clicked results, however, with a
focus on high precision of query expansion. While
expansion from the domain of document terms has
the advantage that expansion terms are guaranteed
to be in the search domain, expansion precision
may suffer from the noisy and indirect ?approval?
of retrieval results by user clicks. Thus expansion
terms from the document domain are more likely
to be generalizations, specifications, or otherwise
related terms, than terms extracted from query sub-
stitutions that resemble synonyms more closely.
Furthermore, if the model that learns to correlate
document terms to query terms is required to ig-
nore context in order to generalize, finding appro-
priate expansions for ambiguous query terms is
difficult.
Our approach is to look at the ?word mismatch?
problem as a problem of translating from a source
language of queries into a target language of docu-
ments, represented as snippets. Since both queries
and snippets are arguably natural language, sta-
tistical machine translation technology (SMT) is
readily applicable to this task. In previous work,
this has been done successfully for question an-
swering tasks (Riezler et al, 2007; Soricut and
Brill, 2006; Echihabi and Marcu, 2003; Berger et
al., 2000), but not for web search in general. Cui et
al.?s (2002) model is to our knowledge the first to
deploy query-document relations for direct extrac-
tion of expansion terms for general web retrieval.
Our SMT approach has two main advantages over
Cui et al?s model: Firstly, Cui et al?s model re-
lates document terms to query terms by using sim-
ple term frequency counts in session data, with-
out considering smoothing techniques. Our ap-
proach deploys a sophisticated machine learning
approach to word alignment, including smooth-
ing techniques, to map query phrases to snippet
phrases. Secondly, Cui et al?s model only indi-
rectly uses context information to disambiguate
expansion terms. This is done by calculating the
relationship of an expansion term to the whole
query by multiplying its contributions to all query
terms. In our SMT approach, contextual disam-
biguation is done by deploying an n-gram lan-
guage model trained on queries to decide about the
appropriateness of an expansion term in the con-
text of the rest of the query terms. As shown in
an experimental evaluation, together the orthogo-
nal information sources of a translation model and
a language model provide significantly better con-
textual query expansion than Cui et al?s (2002)
correlation-based approach.
In the following, we recapitulate the essentials
of Cui et al?s (2002) model, and contrast it with
our SMT-based query expansion system. Further-
more, we will present a detailed comparison of the
two systems on a real-world query expansion task.
2 Query-Document Term Correlations
The query expansion model of Cui et al (2002)
is based on the principle that if queries containing
one term often lead to the selection of documents
containing another term, then a strong relationship
between the two terms is assumed. Query terms
and document terms are linked via clicked docu-
ments in user sessions. Formally, Cui et al (2002)
compute the following probability distribution of
document words w
d
given query words w
q
from
counts over clicked documents D:
P (w
d
|w
q
) =
?
D
P (w
d
|D)P (D|w
q
) (1)
The first term in the righthandside of equation 1 is
a normalized tfidf weight of the the document term
in the clicked document, and the second term is the
relative cooccurrence of document and query term
in sessions.
Since equation 1 calculates expansion probabil-
ities for each term separately, Cui et al (2002)
introduce the following cohesion formula that re-
spects the whole query Q by aggregating the ex-
pansion probabilities for each query term:
CoWeight
Q
(w
d
) = ln(
?
w
q
?Q
P (w
d
|w
q
) + 1) (2)
In contrast to local feedback techniques (Xu
and Croft, 1996), Cui et al?s algorithm allows to
precompute term correlations offline by collecting
counts from query logs. This reliance on pure fre-
quency counting is both a blessing and a curse: On
the one hand it allows for efficient non-iterative es-
timation, on the other hand it makes the implicit
assumption that data sparsity will be overcome by
counting from huge datasets. The only attempt at
smoothing that is made in this approach is a recur-
rence to words in query context, using equation 2,
when equation 1 assigns zero probability to unseen
pairs.
738
3 Query-Snippet Translation
The SMT system deployed in our approach is
an implementation of the alignment template ap-
proach of Och and Ney (Och and Ney, 2004). The
basic features of the model consist of a translation
model and a language model which go back to the
noisy channel formulation of machine translation
in Brown et al (1993). Their ?fundamental equa-
tion of machine translation? defines the job of a
translation system as finding the English string
?
e
that is a translation of a foreign string f such that
?
e = argmax
e
P (e|f)
= argmax
e
P (f |e)P (e) (3)
Equation 3 allows for a separation of a language
model P (e), and a translation model P (f |e). Och
and Ney (2004) reformulate equation 3 as a lin-
ear combination of feature functions h
m
(e, f) and
weights ?
m
, including feature functions for trans-
lation models h
i
(e, f) = P (f |e) and language
models h
j
(e) = P (e):
?
e = argmax
e
M
?
m=1
?
m
h
m
(e, f) (4)
The translation model used in our approach is
based on the sequence of alignment models de-
scribed in Och and Ney (2003). The relationship of
translation model and alignment model for source
language string f = f
J
1
and target string e = e
I
1
is via a hidden variable describing an alignment
mapping from source position j to target position
a
j
:
P (f
J
1
|e
I
1
) =
?
a
J
1
P (f
J
1
, a
J
1
|e
I
1
) (5)
The alignment a
J
1
contains so-called null-word
alignments a
j
= 0 that align source words to the
empty word. The different alignment models de-
scribed in Och and Ney (2003) each parameter-
ize equation 5 differently so as to capture differ-
ent properties of source and target mappings. All
models are based on estimating parameters ? by
maximizing the likelihood of training data con-
sisting of sentence-aligned, but not word-aligned
strings {(f
s
, e
s
) : s = 1, . . . , S}. Since each sen-
tence pair is linked by a hidden alignment variable
a = a
J
1
, the optimal
?
? is found using unlabeled-
data log-likelihood estimation techniques such as
the EM algorithm (Dempster et al, 1977):
?
? = argmax
?
S
?
s=1
?
a
p
?
(f
s
,a|e
s
) (6)
The final translation model is calculated from rel-
ative frequencies of phrases, i.e. consecutive se-
quences of words occurring in text. Phrases are
extracted via various heuristics as larger blocks of
aligned words from best word alignments, as de-
scribed in Och and Ney (2004).
Language modeling in our approach deploys an
n-gram language model that assigns the following
probability to a string w
L
1
of words (see Brants et
al. (2007)):
P (w
L
1
) =
L
?
i=1
P (w
i
|w
i?1
1
) (7)
?
L
?
i=1
P (w
i
|w
i?1
i?n+1
) (8)
Estimation of n-gram probabilities is done by
counting relative frequencies of n-grams in a cor-
pus of user queries. Remedies against sparse data
problems are achieved by various smoothing tech-
niques, as described in Brants et al (2007).
For applications of the system to translate un-
seen queries, a standard dynamic-programming
beam-search decoder (Och and Ney, 2004) that
tightly integrates translation model and language
model is used. Expansion terms are taken from
those terms in the 5-best translations of the query
that have not been seen in the original query string.
In our opinion, the advantages of using an
alignment-based translation model to correlate
document terms with query terms, instead of rely-
ing on a term frequency counts as in equation 1, are
as follows. The formalization of translation mod-
els as involving a hidden alignment variable allows
us to induce a probability distribution that assigns
some probability of being translated into a target
word to every source word. This is a crucial step
towards solving the problem of the ?lexical gap?
described above. Furthermore, various additional
smoothing techniques are employed in alignment
to avoid overfitting and improved coping with rare
words (see Och and Ney (2003)). Lastly, estima-
tion of hidden-variable models can be based on
the well-defined framework of statistical estima-
tion via the EM algorithm.
Similar arguments hold for the language model:
N-gram language modeling is a well-understood
739
sentence source target
pairs words words
tokens 3 billion 8 billion 25 billion
avg. length - 2.6 8.3
Table 1: Statistics of query-snippet training data
for translation model.
problem, with a host of well-proven smoothing
techniques to avoid data sparsity problems (see
Brants et al (2007).)
In combination, translation model and language
model provide orthogonal sources of information
to the overall translation quality. While the trans-
lation model induces a smooth probability distri-
bution that relates source to target words, the lan-
guage model deploys probabilities of target lan-
guage strings to assess the adequacy of a target
word as a translation in context. Reliance on or-
dering information of the words in the context of a
source word is a huge advantage over the bag-of-
words aggregation of context information in Cui et
al?s (2002) model. Furthermore, in the SMT model
used in our approach, translation model and lan-
guage model are efficiently integrated in a beam-
search decoder.
In our application of SMT to query expansion,
queries are considered as source language sen-
tences and snippets of clicked result documents
as target sentences. A parallel corpus of sentence-
aligned data is created by pairing each query with
each snippet of its clicked results. Further adjust-
ments to system parameters were applied in or-
der to adapt the training procedure to this special
data set. For example, in order to account for the
difference in sentence length between queries and
snippets, we set the null-word probability to 0.9.
This allows us to improve precision of alignment
of noisy data by concentrating the alignment to a
small number of key words. Furthermore, extrac-
tion of phrases in our approach is restricted to the
intersection of alignments from both translation di-
rections, thus favoring precision over recall also in
phrase extraction. The only major adjustment of
the language model to the special case of query-
snippet translation is the fact that we train our n-
gram model on queries taken from user logs, in-
stead of on standard English text.
1-grams 2-grams 3-grams
9 million 1.5 billion 5 billion
Table 2: Statistics of unique query n-grams in lan-
guage model.
items disagreements
w/ agreement included
# items 102 125
mean item score 0.333 0.279
95% conf. int. [0.216, 0.451] [0.176, 0.381]
Table 3: Comparison of SMT-based expan-
sion with correlation-based expansion on 7-point
Likert-type scale.
4 Experimental Evaluation
4.1 Data
The training data for the translation model and
the correlation-based model consist of pairs of
queries and snippets for clicked results taken from
anonymized query logs. Using snippets instead of
full documents makes iterative training feasible
and also reduces noise considerably. This parallel
corpus of query-snippet pairs is fed into a standard
SMT training pipeline (modulo the adjustments to
word and phrase alignment discussed above). The
parallel corpus consists of 3 billion query-snippet
pairs that are input to training of word and phrase
alignment models. The resulting phrase translation
table that builds the basis of the translation model
consists 700 million query-snippet phrase transla-
tions. A collection of data statistics for the training
data is shown in table 1.
The language model used in our experiment is a
trigram language model trained on English queries
in user logs. N-grams were cut off at a minimum
frequency of 4. Data statistics for resulting unique
n-grams are shown in table 2.
4.2 Experimental Comparison
Our experimental setup for query expansion de-
ploys a real-world search engine, google.com, for
a comparison of expansions from the SMT-based
system and the correlation-based system. The ex-
perimental evaluation was done as direct compari-
son of search results for queries where both exper-
imental systems suggested expansion terms. Since
expansions from both experimental systems are
done on top of the same underlying search engine,
this allows us to abstract away from interactions
with the underlying system. The queries used for
evaluation were extracted randomly from 3+ word
740
query SMT-based expansions corr-based expansions score
applying U.S. passport passport - visa applying - home -1.0
configure debian to use dhcp debian - linux configure - configuring -1.0
configure - install
how many episodes of 30 rock? episodes - season how many episodes - tv -0.83
episodes - series many episodes - wikipedia
lampasas county sheriff department department - office department - home -0.83
sheriff - office
weakerthans cat virtue chords chords - guitar cat - tabs -0.83
chords - lyrics chords - tabs
chords - tab
Henry VIII Menu Portland, Maine menu - restaurant portland - six 1.3
menu - restaurants menu - england
ladybug birthday parties parties - ideas ladybug - kids 1.3
parties - party
political cartoon calvin coolidge cartoon - cartoons political cartoon - encyclopedia 1.3
top ten dining, vancouver dining - restaurants dining vancouver - 10 1.3
international communication communication - communications international communication - college 1.3
in veterinary medicine communication - skills
Table 4: SMT-based versus correlation-based expansions with mean item score.
queries in user logs in order to allow the systems
to deploy context information for expansion.
In order to evaluate Cui et al?s (2002)
correlation-based system in this setup, we required
the system to assign expansion terms to particu-
lar query terms. This could be achieved by using
a linear interpolation of scores in equation 2 and
equation 1. Equation 1 thus introduces a prefer-
ence for a particular query term to the whole-query
score calculated by equation 2. Our reimplementa-
tion uses unigram and bigram phrases in queries
and expansions. Furthermore, we use Okapi BM25
instead of tfidf in the calculation of equation 1 (see
Robertson et al (1998)).
Query expansion for the SMT-based system is
done by extracting terms introduced in the 5-best
list of query translations as expansion terms for the
respective query terms.
The evaluation was performed by three in-
dependent raters. The raters were given task-
specific rating guidelines, and were shown queries
and 10-best search results from both systems,
anonymized, and presented randomly on left or
right sides. The raters? task was to evaluate the re-
sults on a 7-point Likert-type
1
scale, defined as:
-1.5: much worse
-1.0: worse
-0.5: slightly worse
1
Likert?s (1932) original rating system is a 5-point scale
using integer scores 1 through 5. Our system uses average
scores over three raters for each item, and uses a 7-point in-
stead of a 5-point scale. See Dawes (2008) on the compara-
bility of 5-, 7-, or 10-point scales.
0: about the same
0.5: slightly better
1.0: better
1.5: much better
Results on 125 queries where both systems sug-
gested expansion terms are shown in table 3. For
each query, rating scores are averaged over the
scores assigned by three raters. The overall mean
item score for a comparison of SMT-based ex-
pansion against correlation-based expansion was
0.333 for 102 items with rater agreement, and
0.279 for 125 items including rater disagreements.
All result differences are statistically significant.
Examples for SMT-based and correlation-based
expansions are given in table 4. The first five ex-
amples are losses for the SMT-based system. In
the first example, passport is replaced by the re-
lated, but not synonymous term visa in the SMT-
based expansion. The second example is a loss for
SMT-based expansion because of a replacement of
the specific term debian by the more general term
linux. The correlation-based expansions tv 30 rock
in the third example, lampasas county sheriff home
in the fourth example, and weakerthans tabs in the
fifth example directly hit the title of relevant web
pages, while the SMT-based expansion terms do
not improve retrieval results. However, even from
these negative examples it becomes apparent that
the SMT-based expansion terms are clearly related
to the query terms, and for a majority cases this
has a positive effect. Such examples are shown in
741
(herbs , herbs) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , herb) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , remedies) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , medicine) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , supplements) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , herbs) ( for , for) ( mexican , mexican) ( cooking , cooking)
(herbs , herbs) ( for , for) ( cooking , cooking) ( mexican , mexican)
(herbs , herbs) ( for , for) ( mexican , mexican) ( cooking , food)
(mexican , mexican) ( herbs , herbs) ( for , for) ( cooking , cooking)
(herbs , spices) ( for , for) ( mexican , mexican) ( cooking , cooking)
Table 5: Unique 5-best phrase-level translations of queries herbs for chronic constipation and herbs for
mexican cooking.
query terms n-best expansions
herbs com treatment encyclopedia
chronic interpret treating com
constipation interpret treating com
herbs for medicinal support women
for chronic com gold encyclopedia
chronic constipation interpret treating
herbs cooks recipes com
mexican recipes com cooks
cooking cooks recipes com
herbs for medicinal women support
for mexican cooks com allrecipes
Table 6: Correlation-based expansions for queries herbs for chronic constipation and herbs for mexican
cooking.
the second set of expansions. SMT-based expan-
sions such as henry viii restaurant portland, maine,
or ladybug birthday ideas, or top ten restaurants,
vancouver achieve a change in retrieval results that
does not result in a query drift, but rather in im-
proved retrieval results. In contrast, the terms in-
troduced by the correlation-based system are either
only vaguely related or noise.
5 Discussion
We attribute the experimental result of a signif-
icant preference for SMT-based expansions over
correlation-based expansions to the fruitful com-
bination of translation model and language model
provided by the SMT system. The SMT approach
can be viewed as a combined system that proposes
candidate expansion via the translation model, and
filters them by the language model. Thus we may
find a certain amount of non-sensical expansion
candidates at the phrase translation level. This can
be seen from inspecting table 7 which shows the
most probable phrase translations that are applica-
ble to the queries herbs for chronic constipation
and herbs for mexican cooking. The phrase table
includes identity translations and closely related
terms as most probable translations for nearly ev-
ery phrase, however, it also clearly includes noisy
and non-related terms. More importantly, an ex-
traction of expansion terms from the phrase table
alone would not allow to choose the appropriate
term for the given query context. This can be at-
tained by combining the phrase translations with a
language model: As shown in table 5, the 5-best
translations of the full queries attain a proper dis-
ambiguation of the senses of herbs by replacing
the term by remedies, medicine, and supplements
for the first query, and with spices for the second
query. Expansion terms highlighted in bold face.
The fact that the most probable translation for
the whole query mostly is the identity translation
can be seen as a feature, not as a bug, of the SMT-
based approach: By the option to prefer identity
translations or word reorderings over translations
of source words, the SMT model effectively can
choose not to generate any expansion terms. This
will happen if none of the candidate phrase trans-
lations fit with high enough probability into the
context of the whole query, as assessed by the lan-
guage model.
In contrast to the SMT model, the correlation-
based model cannot fall back onto the ordering in-
formation of the language model, but aggregates
information for the whole query from a bag-of-
words of query terms. Table 6 shows the top three
742
correlation-based expansion terms assigned to uni-
grams and bigrams in the queries herbs for chronic
constipation and herbs for mexican cooking. Ex-
pansion terms are chosen by overall highest weight
and shown in bold face. Relevant expansion terms
such as treatment or recipes that would disam-
biguate the meaning of herbs are in fact proposed
by the correlation-based model, however, the cohe-
sion score also promotes terms such as interpret or
com as best whole-query expansions, thus leading
to query drift.
6 Conclusion
We presented an approach to contextual query ex-
pansion that deploys natural language technology
in form of statistical machine translation. The key
idea of our approach is to consider the problem
of the ?lexical gap? between queries and docu-
ments from a linguistic point of view, and at-
tempt to bridge this gap by translating from the
query language into the document language. Us-
ing search engine user logs, we could extract large
amounts of parallel data of queries and snippets
from clicked documents. These data were used to
train an alignment-based translation model, and
an n-gram based language model. The same data
were used to train a reimplementation of Cui
et al?s (2002) term-correlation based query ex-
pansion system. An experimental comparison of
the two systems showed a considerable prefer-
ence for SMT-based expansions over correlation-
based expansion. Our explanation for this result
is the fruitful combination of the orthogonal in-
formation sources from translation model and lan-
guage model. While in the SMT approach expan-
sion candidates proposed by the translation model
are effectively filtered by ordering information
on the query context from the language model,
the correlation-based approach resorts to an in-
ferior bag-of-word aggregation of scores for the
whole query. Furthermore, each component of the
SMT model takes great care to avoid sparse data
problems by various sophisticated smoothing tech-
niques. In contrast, the correlation-based model re-
lies on pure counts of term frequencies.
An interesting task for future work is to dis-
sect the contributions of translation model and
language model, for example, by combining a
correlation-based system with a language model
filter. The challenge here is a proper integration of
n-gram lookup into correlation-based expansion.
References
Baeza-Yates, Ricardo and Alessandro Tiberi. 2007.
Extracting semantic relations from query logs. In
Proceedings of the 13th ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining
(KDD?07), San Jose, CA.
Beeferman, Doug and Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. In
Proceedings of the 6th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD?00), Boston, MA.
Berger, Adam L., Rich Caruana, David Cohn, Dayne
Freitag, and Vibhu Mittal. 2000. Bridging the lexi-
cal chasm: Statistical approaches to answer-finding.
In Proceedings of SIGIR?00, Athens, Greece.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?07), Prague, Czech Re-
public.
Brown, Peter F., Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
Cui, Hang, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying
Ma. 2002. Probabilistic query expansion using
query logs. In Proceedings of WWW 2002, Hon-
olulu, Hawaii.
Dawes, John. 2008. Do data characteristics change ac-
cording to the number of scale points used? An ex-
periment using 5-point, 7-point and 10-point scales.
International Journal of Market Research, 50(1):61?
77.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical So-
ciety, 39(B):1?38.
Echihabi, Abdessamad and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL?03),
Sapporo, Japan.
Fitzpatrick, Larry and Mei Dent. 1997. Automatic
feedback using past queries: Social searching? In
Proceedings of SIGIR?97, Philadelphia, PA.
Fonseca, Bruno M., Paulo Golgher, Bruno Possas,
Berthier Ribeiro-Neto, and Nivio Ziviani. 2005.
Concept-based interactive query expansion. In Pro-
ceedings of the 14th Conference on Information and
Knowledge Management (CIKM?05), Bremen, Ger-
many.
743
Huang, Chien-Kang, Lee-Feng Chien, and Yen-Jen
Oyang. 2003. Relevant term suggestion in interac-
tive web search based on contextual information in
query session logs. Journal of the American Society
for Information Science and Technology, 54(7):638?
649.
Jones, Rosie, Benjamin Rey, Omid Madani, and Wi-
ley Greiner. 2006. Generating query substitutions.
In Proceedings of the 15th International World Wide
Web conference (WWW?06), Edinburgh, Scotland.
Likert, Rensis. 1932. A technique for the measurement
of attitudes. Archives of Psychology, 140:5?55.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Raghavan, Vijay V. and Hayri Sever. 1995. On the
reuse of past optimal queries. In Proceedings of SI-
GIR?95, Seattle, WA.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL?07), Prague, Czech Republic.
Robertson, Stephen E., Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Confer-
ence (TREC-7), Gaithersburg, MD.
Sahami, Mehran and Timothy D. Heilman. 2006. A
web-based kernel function for measuring the sim-
ilarity of short text snippets. In Proceedings of
the 15th International World Wide Web conference
(WWW?06), Edinburgh, Scotland.
Soricut, Radu and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Jour-
nal of Information Retrieval - Special Issue on Web
Information Retrieval, 9:191?206.
Wen, Ji-Rong, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query clustering using user logs. ACM Trans-
actions on Information Systems, 20(1):59?81.
Xu, Jinxi and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Pro-
ceedings of SIGIR?96, Zurich, Switzerland.
herbs herbs
herbal
medicinal
spices
supplements
remedies
herbs for herbs for
herbs
herbs and
with herbs
herbs for chronic herbs for chronic
and herbs for chronic
herbs for
for for
for chronic for chronic
chronic
of chronic
for chronic constipation for chronic constipation
chronic constipation
for constipation
chronic chronic
acute
patients
treatment
chronic constipation chronic constipation
of chronic constipation
with chronic constipation
constipation constipation
bowel
common
symptoms
for mexican for mexican
mexican
the mexican
of mexican
for mexican cooking mexican food
mexican food and
mexican glossary
mexican mexican
mexico
the mexican
mexican cooking mexican cooking
mexican food
mexican
cooking
cooking cooking
culinary
recipes
cook
food
recipe
Table 7: Phrase translations applicable to source
strings herbs for chronic constipation and herbs
for mexican cooking.
744
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464?471,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Statistical Machine Translation for Query Expansion in Answer Retrieval
Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal and Yi Liu
Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA 94043
{riezler|avasserm|ioannis|vibhu|yliu}@google.com
Abstract
We present an approach to query expan-
sion in answer retrieval that uses Statisti-
cal Machine Translation (SMT) techniques
to bridge the lexical gap between ques-
tions and answers. SMT-based query ex-
pansion is done by i) using a full-sentence
paraphraser to introduce synonyms in con-
text of the entire query, and ii) by trans-
lating query terms into answer terms us-
ing a full-sentence SMT model trained on
question-answer pairs. We evaluate these
global, context-aware query expansion tech-
niques on tfidf retrieval from 10 million
question-answer pairs extracted from FAQ
pages. Experimental results show that SMT-
based expansion improves retrieval perfor-
mance over local expansion and over re-
trieval without expansion.
1 Introduction
One of the fundamental problems in Question An-
swering (QA) has been recognized to be the ?lexi-
cal chasm? (Berger et al, 2000) between question
strings and answer strings. This problem is mani-
fested in a mismatch between question and answer
vocabularies, and is aggravated by the inherent am-
biguity of natural language. Several approaches have
been presented that apply natural language process-
ing technology to close this gap. For example, syn-
tactic information has been deployed to reformu-
late questions (Hermjakob et al, 2002) or to re-
place questions by syntactically similar ones (Lin
and Pantel, 2001); lexical ontologies such as Word-
net1 have been used to find synonyms for question
words (Burke et al, 1997; Hovy et al, 2000; Prager
et al, 2001; Harabagiu et al, 2001), and statisti-
cal machine translation (SMT) models trained on
question-answer pairs have been used to rank can-
didate answers according to their translation prob-
abilities (Berger et al, 2000; Echihabi and Marcu,
2003; Soricut and Brill, 2006). Information retrieval
(IR) is faced by a similar fundamental problem of
?term mismatch? between queries and documents.
A standard IR solution, query expansion, attempts to
increase the chances of matching words in relevant
documents by adding terms with similar statistical
properties to those in the original query (Voorhees,
1994; Qiu and Frei, 1993; Xu and Croft, 1996).
In this paper we will concentrate on the task of
answer retrieval from FAQ pages, i.e., an IR prob-
lem where user queries are matched against docu-
ments consisting of question-answer pairs found in
FAQ pages. Equivalently, this is a QA problem that
concentrates on finding answers given FAQ docu-
ments that are known to contain the answers. Our
approach to close the lexical gap in this setting at-
tempts to marry QA and IR technology by deploy-
ing SMT methods for query expansion in answer
retrieval. We present two approaches to SMT-based
query expansion, both of which are implemented in
the framework of phrase-based SMT (Och and Ney,
2004; Koehn et al, 2003).
Our first query expansion model trains an end-
to-end phrase-based SMT model on 10 million
question-answer pairs extracted from FAQ pages.
1http://wordnet.princeton.edu
464
The goal of this system is to learn lexical correla-
tions between words and phrases in questions and
answers, for example by allowing for multiple un-
aligned words in automatic word alignment, and dis-
regarding issues such as word order. The ability to
translate phrases instead of words and the use of a
large language model serve as rich context to make
precise decisions in the case of ambiguous transla-
tions. Query expansion is performed by adding con-
tent words that have not been seen in the original
query from the n-best translations of the query.
Our second query expansion model is based on
the use of SMT technology for full-sentence para-
phrasing. A phrase table of paraphrases is extracted
from bilingual phrase tables (Bannard and Callison-
Burch, 2005), and paraphrasing quality is improved
by additional discriminative training on manually
created paraphrases. This approach utilizes large
bilingual phrase tables as information source to ex-
tract a table of para-phrases. Synonyms for query
expansion are read off from the n-best paraphrases
of full queries instead of from paraphrases of sep-
arate words or phrases. This allows the model to
take advantage of the rich context of a large n-gram
language model when adding terms from the n-best
paraphrases to the original query.
In our experimental evaluation we deploy a
database of question-answer pairs extracted from
FAQ pages for both training a question-answer
translation model, and for a comparative evalua-
tion of different systems on the task of answer re-
trieval. Retrieval is based on the tfidf framework
of Jijkoun and de Rijke (2005), and query expan-
sion is done straightforwardly by adding expansion
terms to the query for a second retrieval cycle. We
compare our global, context-aware query expansion
techniques with Jijkoun and de Rijke?s (2005) tfidf
model for answer retrieval and a local query expan-
sion technique (Xu and Croft, 1996). Experimen-
tal results show a significant improvement of SMT-
based query expansion over both baselines.
2 Related Work
QA has approached the problem of the lexical gap
by various techniques for question reformulation,
including rule-based syntactic and semantic refor-
mulation patterns (Hermjakob et al, 2002), refor-
mulations based on shared dependency parses (Lin
and Pantel, 2001), or various uses of the Word-
Net ontology to close the lexical gap word-by-word
(Hovy et al, 2000; Prager et al, 2001; Harabagiu
et al, 2001). Another use of natural language pro-
cessing has been the deployment of SMT models on
question-answer pairs for (re)ranking candidate an-
swers which were either assumed to be contained
in FAQ pages (Berger et al, 2000) or retrieved by
baseline systems (Echihabi and Marcu, 2003; Sori-
cut and Brill, 2006).
IR has approached the term mismatch problem by
various approaches to query expansion (Voorhees,
1994; Qiu and Frei, 1993; Xu and Croft, 1996).
Inconclusive results have been reported for tech-
niques that expand query terms separately by adding
strongly related terms from an external thesaurus
such as WordNet (Voorhees, 1994). Significant
improvements in retrieval performance could be
achieved by global expansion techniques that com-
pute corpus-wide statistics and take the entire query,
or query concept (Qiu and Frei, 1993), into account,
or by local expansion techniques that select expan-
sion terms from the top ranked documents retrieved
by the original query (Xu and Croft, 1996).
A similar picture emerges for query expansion
in QA: Mixed results have been reported for word-
by-word expansion based on WordNet (Burke et
al., 1997; Hovy et al, 2000; Prager et al, 2001;
Harabagiu et al, 2001). Considerable improvements
have been reported for the use of the local context
analysis model of Xu and Croft (1996) in the QA
system of Ittycheriah et al (2001), or for the sys-
tems of Agichtein et al (2004) or Harabagiu and
Lacatusu (2004) that use FAQ data to learn how to
expand query terms by answer terms.
The SMT-based approaches presented in this pa-
per can be seen as global query expansion tech-
niques in that our question-answer translation model
uses the whole question-answer corpus as informa-
tion source, and our approach to paraphrasing de-
ploys large amounts of bilingual phrases as high-
coverage information source for synonym finding.
Furthermore, both approaches take the entire query
context into account when proposing to add new
terms to the original query. The approaches that
are closest to our models are the SMT approach of
Radev et al (2001) and the paraphrasing approach
465
web pages FAQ pages QA pairs
count 4 billion 795,483 10,568,160
Table 1: Corpus statistics of QA pair data
of Duboue and Chu-Carroll (2006). None of these
approaches defines the problem of the lexical gap
as a query expansion problem, and both approaches
use much simpler SMT models than our systems,
e.g., Radev et al (2001) neglect to use a language
model to aid disambiguation of translation choices,
and Duboue and Chu-Carroll (2006) use SMT as
black box altogether.
In sum, our approach differs from previous work
in QA and IR in the use SMT technology for query
expansion, and should be applicable in both areas
even though experimental results are only given for
the restricted domain of retrieval from FAQ pages.
3 Question-Answer Pairs from FAQ Pages
Large-scale collection of question-answer pairs has
been hampered in previous work by the small sizes
of publicly available FAQ collections or by restricted
access to retrieval results via public APIs of search
engines. Jijkoun and de Rijke (2005) nevertheless
managed to extract around 300,000 FAQ pages
and 2.8 million question-answer pairs by repeatedly
querying search engines with ?intitle:faq?
and ?inurl:faq?. Soricut and Brill (2006) could
deploy a proprietary URL collection of 1 billion
URLs to extract 2.3 million FAQ pages contain-
ing the uncased string ?faq? in the url string. The
extraction of question-answer pairs amounted to a
database of 1 million pairs in their experiment.
However, inspection of the publicly available Web-
FAQ collection provided by Jijkoun and de Rijke2
showed a great amount of noise in the retrieved
FAQ pages and question-answer pairs, and yet the
indexed question-answer pairs showed a serious re-
call problem in that no answer could be retrieved for
many well-formed queries. For our experiment, we
decided to prefer precision over recall and to attempt
a precision-oriented FAQ and question-answer pair
extraction that benefits the training of question-
answer translation models.
2http://ilps.science.uva.nl/Resources/WazDah/
As shown in Table 1, the FAQ pages used in our
experiment were extracted from a 4 billion page
subset of the web using the queries ?inurl:faq?
and ?inurl:faqs? to match the tokens ?faq? or
?faqs? in the urls. This extraction resulted in 2.6
million web pages (0.07% of the crawl). Since not
all those pages are actually FAQs, we manually la-
beled 1,000 of those pages to train an online passive-
aggressive classificier (Crammer et al, 2006) in a
10-fold cross validation setup. Training was done
using 20 feature functions on occurrences question
marks and key words in different fields of web
pages, and resulted in an F1 score of around 90%
for FAQ classification. Application of the classifier
to the extracted web pages resulted in a classification
of 795,483 pages as FAQ pages.
The extraction of question-answer pairs from this
database of FAQ pages was performed again in a
precision-oriented manner. The goal of this step
was to extract url, title, question, and answers fields
from the question-answer pairs in FAQ pages. This
was achieved by using feature functions on punc-
tuations, HTML tags (e.g., <p>, <BR>), listing
markers (e.g., Q:, (1)), and lexical cues (e.g.,
What, How), and an algorithm similar to Joachims
(2003) to propagate initial labels across similar text
pieces. The result of this extraction step is a database
of about 10 million question answer pairs (13.3
pairs per FAQ page). A manual evaluation of 100
documents, containing 1,303 question-answer pairs,
achieved a precision of 98% and a recall of 82% for
extracting question-answer pairs.
4 SMT-Based Query Expansion
Our SMT-based query expansion techniques are
based on a recent implementation of the phrase-
based SMT framework (Koehn et al, 2003; Och and
Ney, 2004). The probability of translating a foreign
sentence f into English e is defined in the noisy chan-
nel model as
argmax
e
p(e|f) = argmax
e
p(f|e)p(e) (1)
This allows for a separation of a language model
p(e), and a translation model p(f|e). Translation
probabilities are calculated from relative frequencies
of phrases, which are extracted via various heuris-
tics as larger blocks of aligned words from best word
466
alignments. Word alignments are estimated by mod-
els similar to Brown et al (1993). For a sequence of
I phrases, the translation probability in equation (1)
can be decomposed into
p(f Ii |e
I
i ) =
I?
i=1
p(fi|ei) (2)
Recent SMT models have shown significant im-
provements in translation quality by improved mod-
eling of local word order and idiomatic expressions
through the use of phrases, and by the deployment
of large n-gram language models to model fluency
and lexical choice.
4.1 Question-Answer Translation
Our first approach to query expansion treats the
questions and answers in the question-answer cor-
pus as two distinct languages. That is, the 10 million
question-answer pairs extracted from FAQ pages are
fed as parallel training data into an SMT training
pipeline. This training procedure includes various
standard procedures such as preprocessing, sentence
and chunk alignment, word alignment, and phrase
extraction. The goal of question-answer translation
is to learn associations between question words and
synonymous answer words, rather than the trans-
lation of questions into fluent answers. Thus we
did not conduct discriminative training of feature
weights for translation probabilities or language
model probabilities, but we held out 4,000 question-
answer pairs for manual development and testing of
the system. For example, the system was adjusted
to account for the difference in sentence length be-
tween questions and answers by setting the null-
word probability parameter in word alignment to
0.9. This allowed us to concentrate the word align-
ments to a small number of key words. Furthermore,
extraction of phrases was based on the intersection
of alignments from both translation directions, thus
favoring precision over recall also in phrase align-
ment.
Table 2 shows unique translations of the query
?how to live with cat allergies? on the phrase-level,
with corresponding source and target phrases shown
in brackets. Expansion terms are taken from phrase
terms that have not been seen in the original query,
and are highlighted in bold face.
4.2 SMT-Based Paraphrasing
Our SMT-based paraphrasing system is based on the
approach presented in Bannard and Callison-Burch
(2005). The central idea in this approach is to iden-
tify paraphrases or synonyms at the phrase level by
pivoting on another language. For example, given
a table of Chinese-to-English phrase translations,
phrasal synonyms in the target language are defined
as those English phrases that are aligned to the same
Chinese source phrases. Translation probabilities for
extracted para-phrases can be inferred from bilin-
gual translation probabilities as follows: Given an
English para-phrase pair (trg, syn), the probability
p(syn|trg) that trg translates into syn is defined
as the joint probability that the English phrase trg
translates into the foreign phrase src, and that the
foreign phrase src translates into the English phrase
syn. Under an independence assumption of those
two events, this probability and the reverse transla-
tion direction p(trg|syn) can be defined as follows:
p(syn|trg) = max
src
p(src|trg)p(syn|src) (3)
p(trg|syn) = max
src
p(src|syn)p(trg|src)
Since the same para-phrase pair can be obtained
by pivoting on multiple foreign language phrases, a
summation or maximization over foreign language
phrases is necessary. In order not to put too much
probability mass onto para-phrase translations that
can be obtained from multiple foreign language
phrases, we maximize instead of summing over src.
In our experiments, we employed equation (3)
to infer for each para-phrase pair translation model
probabilities p?(syn|trg) and p??(trg|syn) from
relative frequencies of phrases in bilingual tables.
In contrast to Bannard and Callison-Burch (2005),
we applied the same inference step to infer also
lexical translation probabilities pw(syn|trg) and
pw?(trg|syn) as defined in Koehn et al (2003) for
para-phrases. Furthermore, we deployed features for
the number of words lw, number of phrases c? , a
reordering score pd , and a score for a 6-gram lan-
guage model pLM trained on English web data. The
final model combines these features in a log-linear
model that defines the probability of paraphrasing a
full sentence, consisting of a sequence of I phrases
467
qa-translation (how, how) (to, to) (live, live) (with, with) (cat, pet) (allergies, allergies)
(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, allergy)
(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, food)
(how, how) (to, to) (live, live) (with, with) (cat, cats) (allergies, allergies)
paraphrasing (how, how) (to live, to live) (with cat, with cat) (allergies, allergy)
(how, ways) (to live, to live) (with cat, with cat) (allergies, allergies)
(how, how) (to live with, to live with) (cat, feline) (allergies, allergies)
(how to, how to) (live, living) (with cat, with cat) (allergies, allergies)
(how to, how to) (live, life) (with cat, with cat) (allergies, allergies)
(how, way) (to live, to live) (with cat, with cat) (allergies, allergies)
(how, how) (to live, to live) (with cat, with cat) (allergies, allergens)
(how, how) (to live, to live) (with cat, with cat) (allergies, allergen)
Table 2: Unique n-best phrase-level translations of query ?how to live with cat allergies?.
as follows:
p(synI1|trg
I
1) = (
I?
i=1
p?(syni|trgi)
?? (4)
? p??(trgi|syni)
???
? pw(syni|trgi)
?w
? pw?(trgi|syni)
?w?
? pd(syni, trgi)
?d)
? lw(syn
I
1)
?l
? c?(syn
I
1)
?c
? pLM (syn
I
1)
?LM
For estimation of the feature weights ~? defined
in equation (4) we employed minimum error rate
(MER) training under the BLEU measure (Och,
2003). Training data for MER training were taken
from multiple manual English translations of Chi-
nese sources from the NIST 2006 evaluation data.
The first of four reference translations for each Chi-
nese sentence was taken as source paraphrase, the
rest as reference paraphrases. Discriminative train-
ing was conducted on 1,820 sentences; final evalua-
tion on 2,390 sentences. A baseline paraphrase table
consisting of 33 million English para-phrase pairs
was extracted from 1 billion phrase pairs from three
different languages, at a cutoff of para-phrase prob-
abilities of 0.0025.
Query expansion is done by adding terms intro-
duced in n-best paraphrases of the query. Table 2
shows example paraphrases for the query ?how to
live with cat allergies? with newly introduced terms
highlighted in bold face.
5 Experimental Evaluation
Our baseline answer retrieval system is modeled af-
ter the tfidf retrieval model of Jijkoun and de Ri-
jke (2005). Their model calculates a linear com-
bination of vector similarity scores between the
user query and several fields in the question-answer
pair. We used the cosine similarity metric with
logarithmically weighted term and document fre-
quency weights in order to reproduce the Lucene3
model used in Jijkoun and de Rijke (2005). For
indexing of fields, we adopted the settings that
were reported to be optimal in Jijkoun and de
Rijke (2005). These settings comprise the use of
8 question-answer pair fields, and a weight vec-
tor ?0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3? for fields or-
dered as follows: (1) full FAQ document text, (2)
question text, (3) answer text, (4) title text, (5)-(8)
each of the above without stopwords. The second
field thus takes takes wh-words, which would typ-
ically be filtered out, into account. All other fields
are matched without stopwords, with higher weight
assigned to document and question than to answer
and title fields. We did not use phrase-matching or
stemming in our experiments, similar to Jijkoun and
de Rijke (2005), who could not find positive effects
for these features in their experiments.
Expansion terms are taken from those terms
in the n-best translations of the query that have
not been seen in the original query string. For
paraphrasing-based query expansion, a 50-best list
of paraphrases of the original query was used.
For the noisier question-answer translation, expan-
sion terms and phrases were extracted from a 10-
3http://lucene.apache.org
468
S2@10 S2@20 S1,2@10 S1,2@20
baseline tfidf 27 35 58 65
local expansion 30 (+ 11.1) 40 (+ 14.2) 57 (- 1) 63 (- 3)
SMT-based expansion 38 (+ 40.7) 43 (+ 22.8) 58 65
Table 3: Success rate at 10 or 20 results for retrieval of adequate (2) or material (1) answers; relative change
in brackets.
best list of query translations. Terms taken from
query paraphrases were matched with the same field
weight vector ?0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3? as
above. Terms taken from question-answer trans-
lation were matched with the weight vector
?0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3?, preferring an-
swer fields over question fields. After stopword
removal, the average number of expansion terms
produced was 7.8 for paraphrasing, and 3.1 for
question-answer translation.
The local expansion technique used in our exper-
iments follows Xu and Croft (1996) in taking ex-
pansion terms from the top n answers that were re-
trieved by the baseline tfidf system, and by incorpo-
rating cooccurrence information with query terms.
This is done by calculating term frequencies for ex-
pansion terms by summing up the tfidf weights of
the answers in which they occur, thus giving higher
weight to terms that occur in answers that receive
a higher similarity score to the original query. In
our experiments, expansion terms are ranked accord-
ing to this modified tfidf calculation over the top 20
answers retrieved by the baseline retrieval run, and
matched a second time with the field weight vector
?0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3? that prefers an-
swer fields over question fields. After stopword re-
moval, the average number of expansion terms pro-
duced by the local expansion technique was 9.25.
The test queries we used for retrieval are taken
from query logs of the MetaCrawler search en-
gine4 and were provided to us by Valentin Jijk-
oun. In order to maximize recall for the comparative
evaluation of systems, we selected 60 queries that
were well-formed natural language questions with-
out metacharacters and spelling errors. However, for
one third of these well-formed queries none of the
five compared systems could retrieve an answer. Ex-
amples are ?how do you make a cornhusk doll?,
4http://www.metacrawler.com
?what is the idea of materialization?, or ?what does
8x certified mean?, pointing to a severe recall prob-
lem of the question-answer database.
Evaluation was performed by manual labeling of
top 20 answers retrieved for each of 60 queries for
each system by two independent judges. For the sake
of consistency, we chose not to use the assessments
provided by Jijkoun and de Rijke. Instead, the judges
were asked to find agreement on the examples on
which they disagreed after each evaluation round.
The ratings together with the question-answer pair
id were stored and merged into the retrieval results
for the next system evaluation. In this way consis-
tency across system evaluations could be ensured,
and the effort of manual labeling could be substan-
tially reduced. The quality of retrieval results was
assessed according to Jijkoun and de Rijke?s (2005)
three point scale:
? adequate (2): answer is contained
? material (1): no exact answer, but important in-
formation given
? unsatisfactory (0): user?s information need is
not addressed
The evaluation measure used in Jijkoun and de
Rijke (2005) is the success rate at 10 or 20 an-
swers, i.e., S2@n is the percentage of queries with
at least one adequate answer in the top n retrieved
question-answer pairs, and S1,2@n is the percentage
of queries with at least one adequate or material an-
swer in the top n results. This evaluation measure ac-
counts for improvements in coverage, i.e., it rewards
cases where answers are found for queries that did
not have an adequate or material answer before. In
contrast, the mean reciprocal rank (MRR) measure
standardly used in QA can have the effect of prefer-
ring systems that find answers only for a small set
of queries, but rank them higher than systems with
469
(1) query: how to live with cat allergies
local expansion (-): allergens allergic infections filter plasmacluster rhinitis introduction effective replacement
qa-translation (+): allergy cats pet food
paraphrasing (+): way allergens life allergy feline ways living allergen
(2) query: how to design model rockets
local expansion (-): models represented orientation drawings analysis element environment different structure
qa-translation (+): models rocket
paraphrasing (+): missiles missile rocket grenades arrow designing prototype models ways paradigm
(3) query: what is dna hybridization
local expansion (-): instructions individual blueprint characteristics chromosomes deoxyribonucleic information biological
genetic molecule
qa-translation (+): slides clone cdna sitting sequences
paraphrasing (+): hibridization hybrids hybridation anything hibridacion hybridising adn hybridisation nothing
(4) query: how to enhance competitiveness of indian industries
local expansion (+): resources production quality processing established investment development facilities institutional
qa-translation (+): increase industry
paraphrasing (+): promote raise improve increase industry strengthen
(5) query: how to induce labour
local expansion (-): experience induction practice imagination concentration information consciousness different meditation
relaxation
qa-translation (-): birth industrial induced induces
paraphrasing (-): way workers inducing employment ways labor working child work job action unions
Table 4: Examples for queries and expansion terms yielding improved (+), decreased (-), or unchanged (0)
retrieval performance compared to retrieval without expansion.
higher coverage. This makes MRR less adequate for
the low-recall setup of FAQ retrieval.
Table 3 shows success rates at 10 and 20 retrieved
question-answer pairs for five different systems. The
results for the baseline tfidf system, following Jijk-
oun and de Rijke (2005), are shown in row 2. Row
3 presents results for our variant of local expansion
by pseudo-relevance feedback (Xu and Croft, 1996).
Results for SMT-based expansion are given in row 4.
A comparison of success rates for retrieving at least
one adequate answer in the top 10 results shows rel-
ative improvements over the baseline of 11.1% for
local query expansion, and of 40.7% for combined
SMT-based expansion. Success rates at top 20 re-
sults show similar relative improvements of 14.2%
for local query expansion, and of 22.8% for com-
bined SMT-based expansion. On the easier task of
retrieving a material or adequate answer, success
rates drop by a small amount for local expansion,
and stay unchanged for SMT-based expansion.
These results can be explained by inspecting a few
sample query expansions. Examples (1)-(3) in Ta-
ble 4 illustrate cases where SMT-based query expan-
sion improves results over baseline performance, but
local expansion decreases performance by introduc-
ing irrelevant terms. In (4) retrieval performance is
improved over the baseline for both expansion tech-
niques. In (5) both local and SMT-based expansion
introduce terms that decrease retrieval performance
compared to retrieval without expansion.
6 Conclusion
We presented two techniques for query expansion in
answer retrieval that are based on SMT technology.
Our method for question-answer translation uses a
large corpus of question-answer pairs extracted from
FAQ pages to learn a translation model from ques-
tions to answers. SMT-based paraphrasing utilizes
large amounts of bilingual data as a new informa-
tion source to extract phrase-level synonyms. Both
SMT-based techniques take the entire query context
into account when adding new terms to the orig-
inal query. In an experimental comparison with a
baseline tfidf approach and a local query expansion
technique on the task of answer retrieval from FAQ
pages, we showed a significant improvement of both
SMT-based query expansion over both baselines.
Despite the small-scale nature of our current ex-
perimental results, we hope to apply the presented
techniques to general web retrieval in future work.
Another task for future work is to scale up the ex-
traction of question-answer pair data in order to
provide an improved resource for question-answer
translation.
470
References
Eugene Agichtein, Steve Lawrence, and Luis Gravano.
2004. Learning to find answers to questions on
the web. ACM Transactions on Internet Technology,
4(2):129?162.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of (ACL?05), Ann Arbor, MI.
Adam L. Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the lexical
chasm: Statistical approaches to answer-finding. In
Proceedings of SIGIR?00, Athens, Greece.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Robin B. Burke, Kristian J. Hammond, and Vladimir A.
Kulyukin. 1997. Question answering from
frequently-asked question files: Experiences with the
FAQ finder system. AI Magazine, 18(2):57?66.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yo-ram Singer. 2006. Online passive-
agressive algorithms. Machine Learning, 7:551?585.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006. An-
swering the question you wish they had asked: The im-
pact of paraphrasing for question answering. In Pro-
ceedings of (HLT-NAACL?06), New York, NY.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of (ACL?03), Sapporo, Japan.
Sanda Harabagiu and Finley Lacatusu. 2004. Strategies
for advanced question answering. In Proceedings of
the HLT-NAACL?04 Workshop on Pragmatics of Ques-
tion Answering, Boston, MA.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Roxana
G??rju, Vasile Rus, and Paul Mora?rescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of (ACL?01),
Toulouse, France.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and web exploitation for question answering.
In Proceedings of TREC-11, Gaithersburg, MD.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2000. Question answering
in webclopedia. In Proceedings of TREC 9, Gaithers-
burg, MD.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2001. IBM?s statistical question answering system. In
Proceedings of TREC 10, Gaithersburg, MD.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management (CIKM?05),
Bremen, Germany.
Thorsten Joachims. 2003. Transductive learning
via spectral graph partitioning. In Proceedings of
ICML?03, Washington, DC.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of (HLT-NAACL?03), Edmonton, Cananda.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Journal of Natural
Language Engineering, 7(3):343?360.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
(HLT-NAACL?03), Edmonton, Cananda.
John Prager, Jennifer Chu-Carroll, and Krysztof Czuba.
2001. Use of wordnet hypernyms for answering what-
is questions. In Proceedings of TREC 10, Gaithers-
burg, MD.
Yonggang Qiu and H. P. Frei. 1993. Concept based query
expansion. In Proceedings of SIGIR?93, Pittsburgh,
PA.
Dragomir R. Radev, Hong Qi, Zhiping Zheng, Sasha
Blair-Goldensohn, Zhu Zhang, Weigo Fan, and John
Prager. 2001. Mining the web for answers to natu-
ral language questions. In Proceedings of (CIKM?01),
Atlanta, GA.
Radu Soricut and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Journal
of Information Retrieval - Special Issue on Web Infor-
mation Retrieval, 9:191?206.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SI-
GIR?94, Dublin, Ireland.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Proceed-
ings of SIGIR?96, Zurich, Switzerland.
471
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1688?1699,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Boosting Cross-Language Retrieval by Learning
Bilingual Phrase Associations from Relevance Rankings
Artem Sokolov and Laura Jehl and Felix Hieber and Stefan Riezler
Department of Computational Linguistics
Heidelberg University, 69120 Heidelberg, Germany
{sokolov,jehl,hieber,riezler}@cl.uni-heidelberg.de
Abstract
We present an approach to learning bilin-
gual n-gram correspondences from relevance
rankings of English documents for Japanese
queries. We show that directly optimizing
cross-lingual rankings rivals and complements
machine translation-based cross-language in-
formation retrieval (CLIR). We propose an ef-
ficient boosting algorithm that deals with very
large cross-product spaces of word correspon-
dences. We show in an experimental evalu-
ation on patent prior art search that our ap-
proach, and in particular a consensus-based
combination of boosting and translation-based
approaches, yields substantial improvements
in CLIR performance. Our training and test
data are made publicly available.
1 Introduction
The central problem addressed in Cross-Language
Information Retrieval (CLIR) is that of translating
or projecting a query into the language of the docu-
ment repository across which retrieval is performed.
There are two main approaches to tackle this prob-
lem: The first approach leverages the standard Sta-
tistical Machine Translation (SMT) machinery to
produce a single best translation that is used as
search query in the target language. We will hence-
forth call this the direct translation approach. This
technique is particularly useful if large amounts of
data are available in domain-specific form.
Alternative approaches avoid to solve the hard
problem of word reordering, and instead rely on
token-to-token translations that are used to project
the query terms into the target language with a
probabilistic weighting of the standard term tf-idf
scheme. Darwish and Oard (2003) termed this
method the probabilistic structured query approach.
The advantage of this technique is an implicit query
expansion effect due to the use of probability distri-
butions over term translations (Xu et al, 2001). Re-
cent research has shown that leveraging query con-
text by extracting term translation probabilities from
n-best direct translations of queries instead of using
context-free translation tables outperforms both di-
rect translation and context-free projection (Ture et
al., 2012b; Ture et al, 2012a).
While direct translation as well as probabilistic
structured query approaches use machine learning to
optimize the SMT module, retrieval is done by stan-
dard search algorithms in both approaches. For ex-
ample, Google?s CLIR approach uses their standard
proprietary search engine (Chin et al, 2008). Ture et
al. (2012b; 2012a) use standard retrieval algorithms
such as BM25 (Robertson et al, 1998). That means,
machine learning in SMT-based approaches concen-
trates on the cross-language aspect of CLIR and is
agnostic of the ultimate ranking task.
In this paper, we present a method to project
search queries into the target language that is com-
plementary to SMT-based CLIR approaches. Our
method learns a table of n-gram correspondences by
direct optimization of a ranking objective on rele-
vance rankings of English documents for Japanese
queries. Our model is similar to the approach of
Bai et al (2010) who characterize their technique as
?Learning to rank with (a Lot of) Word Features?.
Given a set of search queries q ? IRQ and docu-
1688
ments d ? IRD, where the jth dimension of a vector
indicates the occurrence of the jth word for dictio-
naries of size Q and D, we want to learn a score
f(q,d) between a query and a given document us-
ing the model1
f(q,d) = q>Wd =
Q?
i=1
D?
j=1
qiWijdj .
We take a pairwise ranking approach to optimiza-
tion. That is, given labeled data in the form of a
set R of tuples (q,d+,d?), where d+ is a relevant
(or higher ranked) document and d? an irrelevant
(or lower ranked) document for query q, the goal
is to find a weight matrix W ? IRQ?D such that
f(q,d+) > f(q,d?) for all data tuples from R.
The scoring model learns weights for all possible
correspondences of query terms and document terms
by directly optimizing the ranking objective at hand.
Such a phrase table contains domain-specific word
associations that are useful to discern relevant from
irrelevant documents, something that is orthogonal
and complementary to standard SMT models.
The challenge of our approach can be explained
by constructing a joint feature map ? from the outer
product of the vectors q and d where
?((i?1)D+j)(q,d) = (q? d)ij = (qd
>)ij . (1)
Using this feature map, we see that the score func-
tion f can be written in the standard form of a lin-
ear model that computes the inner product between
a weight vector w and a feature vector ? where
w, ? ? IRQ?D and
f(q,d) = ?w, ?(q,d)?. (2)
While various standard algorithms exist to optimize
linear models, the difficulty lies in the memory foot-
print and capacity of the word-based model. A full-
sized model includes Q ? D parameters which is
easily in the billions even for moderately sized dic-
tionaries. Clearly, an efficient implementation and
remedies against overfitting are essential.
The main contribution of our paper is the pre-
sentation of algorithms that make learning a phrase
1With bold letters we denote vectors for query q and docu-
ment d. Vector components are denoted with normal font letters
and indices (e.g., qi).
table by direct rank optimization feasible, and an
experimental verification of the benefits of this ap-
proach, especially with regard to a combination
of the orthogonal information sources of ranking-
based and SMT-based CLIR approaches. Our ap-
proach builds upon a boosting framework for pair-
wise ranking (Freund et al, 2003) that allows the
model to grow incrementally, thus avoiding having
to deal with the full matrix W . Furthermore, we
present an implementation of boosting that utilizes
parallel estimation on bootstrap samples from the
training set for increased efficiency and reduced er-
ror (Breiman, 1996). Our ?bagged boosting? ap-
proach allows to combine incremental feature selec-
tion, parallel training, and efficient management of
large data structures.
We show in an experimental evaluation on large-
scale retrieval on patent abstracts that our boosting
approach is comparable in MAP and improves sig-
nificantly by 13-15 PRES points over very competi-
tive translation-based CLIR systems that are trained
on 1.8 million parallel sentence pairs from Japanese-
English patent documents. Moreover, a combination
of the orthogonal information learned in ranking-
based and translation-based approaches improves
over 7 MAP points and over 15 PRES points over the
respective translation-based system in a consensus-
based voting approach following the Borda Count
technique (Aslam and Montague, 2001).
2 Related Work
Recent research in CLIR follows the two main
paradigms of direct translation and probabilistic
structured query approaches. An example for the
first approach is the work of Magdy and Jones
(2011) who presented an efficient technique to adapt
off-the-shelf SMT systems for CLIR by training
them on data pre-processed for retrieval (case fold-
ing, stopword removal, stemming). Nikoulina et al
(2012) presented an approach to direct translation-
based CLIR where the n-best list of an SMT system
is re-ranked according to the MAP performance of
the translated queries. The probabilistic structured
query approach has seen a lot of work on context-
aware query expansion across languages, based on
various similarity statistics (Ballesteros and Croft,
1998; Gao et al, 2001; Lavrenko et al, 2002; Gao
1689
et al, 2007). At the time of writing this paper, the
most recent extension to this paradigm is Ture et
al. (2012a). In addition to projecting terms from
n-best translations, they propose a projection ex-
tracted from the hierarchical phrase- based grammar
models, and a scoring method based on multi-token
terms. Since the latter techniques achieved only
marginal improvements over the context-sensitive
query translation from n-best lists, we did not pur-
sue them in our work.
CLIR in the context of patent prior art search
was done as extrinsic evaluation at the NTCIR
PatentMT2 workshops until 2010, and has been on-
going in the CLEF-IP3 benchmarking workshops
since 2009. However, most workshop participants
did either not make use of automatic translation at
all, or they used an off-the-shelf translation tool.
This is due to the CLEF-IP data collection where
parts of patent documents are provided as man-
ual translations into three languages. In order to
evaluate CLIR in a truly cross-lingual scenario, we
created a large patent CLIR dataset where queries
and documents are Japanese and English patent ab-
stracts, respectively.
Ranking approaches to CLIR have been presented
by Guo and Gomes (2009) who use pairwise rank-
ing for patent retrieval. Their method is a classical
learning-to-rank setup where retrieval scores such as
tf-idf or BM25 are combined with domain knowl-
edge on patent class, inventor, date, location, etc.
into a dense feature vector of a few hundred fea-
tures. Methods to learn word-based translation cor-
respondences from supervised ranking signals have
been presented by Bai et al (2010) and Chen et
al. (2010). These approaches tackle the problem of
complexity and capacity of the cross product matrix
of word correspondences from different directions.
The first proposes to learn a low rank representa-
tion of the matrix; the second deploys sparse online
learning under `1 regularization to keep the matrix
small. Both approaches are mainly evaluated in a
monolingual setting. The cross-lingual evaluation
presented in Bai et al (2010) uses weak translation-
based baselines and non-public data such that a di-
rect comparison is not possible.
2http://research.nii.ac.jp/ntcir/ntcir/
3http://www.ifs.tuwien.ac.at/?clef-ip/
A combination of bagging and boosting in the
context of retrieval has been presented by Pavlov et
al. (2010) and Ganjisaffar et al (2011). This work
is done in a standard learning-to-rank setup using a
few hundred dense features trained on hundreds of
thousands of pairs. Our setup deals with billions of
sparse features (from the cross-product of the un-
restricted dictionaries) trained on millions of pairs
(sampled from a much larger space). Parallel boost-
ing where all feature weights are updated simultane-
ously has been presented by Collins et al (2002) and
Canini et al (2010). The first method distributes the
gradient calculation for different features among dif-
ferent compute nodes. This is not possible in our ap-
proach because we construct the cross-product ma-
trix on-the-fly. The second approach requires sub-
stantial efforts in changing the data representation
to use the MapReduce framework. Overall, one of
the goals of our work is sequential updating for im-
plicit feature selection, something that runs contrary
to parallel boosting.
3 CLIR Approaches
3.1 Direct translation approach
For direct translation, we use the SCFG decoder
cdec (Dyer et al, 2010)4 and build grammars us-
ing its implementation of the suffix array extraction
method described in Lopez (2007). Word align-
ments are built from all parallel data using mgiza5
and the Moses scripts6. SCFG models use the same
settings as described in Chiang (2007). Training
and querying of a modified Kneser-Ney smoothed 5-
gram language model are done on the English side
of the training data using KenLM (Heafield, 2011)7.
Model parameters were optimized using cdec?s im-
plementation of MERT (Och (2003)).
At retrieval time, all queries are translated
sentence-wise and subsequently re-joined to form
one query per patent. Our baseline retrieval system
uses the Okapi BM25 scores for document ranking.
4https://github.com/redpony/cdec
5http://www.kyloo.net/software/doku.php/
mgiza:overview
6http://www.statmt.org/moses/?n=Moses.
SupportTools
7http://kheafield.com/code/kenlm/
estimation/
1690
3.2 Probabilistic structured query approach
Early Probabilistic Structured Query approaches
(Xu et al, 2001; Darwish and Oard, 2003) represent
translation options by lexical, i.e., token-to-token
translation tables that are estimated using standard
word alignment techniques (Och and Ney, 2000).
Later approaches (Ture et al, 2012b; Ture et al,
2012a) extract translation options from the decoder?s
n-best list for translating a particular query. The
central idea is to let the language model choose flu-
ent, context-aware translations for each query term
during decoding. This retains the desired query-
expansion effect of probabilistic structured models,
but it reduces query drift by filtering translations
with respect to the context of the full query.
A projection of source language query terms f ?
F into the target language is achieved by repre-
senting each source token f by its probabilistically
weighted translations. The score of target document
E, given source language query F , is computed by
calculating the BM25 rank over projected term fre-
quency and document frequency weights as follows:
score(E|F ) =
?
f?F
BM25(tf(f,E), df(f)) (3)
tf(f,E) =
?
e?Ef
tf(e, E)p(e|f)
df(f) =
?
e?Ef
df(e)p(e|f)
where Ef = {e ? E|p(e|f) > pL} is the set of
translation options for query term f with probability
greater than pL. We also use a cumulative threshold
pC so that only the most probable options are added
until pC is reached.
Ture et al (2012b; 2012a) achieved best retrieval
performance by interpolating between (context-free)
lexical translation probabilities plex estimated on
symmetrized word alignments, and (context-aware)
translation probabilities pnbest estimated on the n-
best list of an SMT decoder:
p(e|f) = ?pnbest(e|f) + (1? ?)plex(e|f) (4)
pnbest(e|f) is estimated by calculating expectations
of term translations from k-best translations:
pnbest(e|f) =
?n
k=1 ak(e, f)D(k, F )?n
k=1
?
e? ak(e
?, f)D(k, F )
where ak(e, f) is a function indicating an alignment
of target term e to source term f in the kth derivation
of queryF , andD(k, F ) is the model score of the kth
derivation in the n-best list for query F .
We use the same hierarchical phrase-based sys-
tem that was used for direct translation to calcu-
late n-best translations for the probabilistic struc-
tured query approach. This allows us to extract
word alignments between source and target text for
F from the SCFG rules used in the derivation. The
concept of self-translation is covered by the de-
coder?s ability to use pass-through rules if words or
phrases cannot be translated.
Probabilistic structured queries that include
context-aware estimates of translation probabilities
require a preservation of sentence-wise context-
sensitivity also in retrieval. Thus, unlike the direct
translation approach, we compute weighted term
and document frequencies for each sentence s in
query F separately. The scoring (3) of a target doc-
ument for a multiple sentence query then becomes:
score(E|F ) =
?
s in F
?
f?s
BM25(tf(f,E), df(f))
3.3 Direct Phrase Table Learning from
Relevance Rankings
Pairwise Ranking using Boosting The general
form of the RankBoost algorithm (Freund et al,
2003; Collins and Koo, 2005) defines a scoring
function f(q,d) on query q and document d as a
weighted linear combination of T weak learners ht
such that f(q,d) =
?T
t=1wtht(q,d). Weak learn-
ers can belong to an arbitrary family of functions,
but in our case they are restricted to the simplest
case of unparameterized indicator functions select-
ing components of the feature vector ?(q,d) in (1)
such that f is of the standard linear form (2). In our
experiments, these features indicate the presence of
pairs of uni- and bi-grams from the source-side vo-
cabulary of query terms and the target-side vocabu-
lary of document-terms, respectively. Furthermore,
in order to simulate the pass-through behavior of
SMT, we introduce additional features to the model
that indicate the identity of terms in source and tar-
get. All identity features have the same fixed weight
?, which is found on the development set.
For training, we are given labeled data in the form
1691
of a set R of tuples (q,d+,d?), where d+ is a rel-
evant (or higher ranked) document and d? an ir-
relevant (or lower ranked) document for query q.
RankBoost?s objective is to correctly rank query-
document pairs such that f(q,d+) > f(q,d?) for
all data tuples from R. RankBoost achieves this by
optimizing the following convex exponential loss:
Lexp =
?
(q,d+,d?)?R
D(q,d+,d?)ef(q,d
?)?f(q,d+),
where D(q,d+,d?) is a non-negative importance
function on pairs of documents for a given q.
We optimize Lexp in a greedy iterative fash-
ion, which closely follows an efficient algorithm of
Collins and Koo (2005) for the case of binary-valued
h. In each step, the single feature h is selected that
provides the largest decrease of Lexp, i.e., that has
the largest projection on the direction of the gradi-
ent ?hLexp. Because of the sequential nature of
the algorithm, RankBoost implicitly performs auto-
matic feature selection and regularization (Rosset et
al., 2004), which is crucial to reduce complexity and
capacity for our application.
Parallelization and Bagging To achieve paral-
lelization we use a variant of bagging (Breiman,
1996) on top of boosting, which has been observed
to improve performance, reduce variance and is
trivial to parallelize. The procedure is described
as part of Algorithm 1: From the set of prefer-
ence pairs R, draw S equal-sized samples with
replacement and distribute to nodes. Then, us-
ing each of the samples as a training set, sep-
arate boosting models {wst , h
s
t}, s = 1 . . . S are
trained that contain the same number of features
t = 1 . . . T . Finally the models are averaged:
f(q,d) = 1S
?
t
?
sw
s
th
s
t (q,d).
Algorithm The entire training procedure is out-
lined in Algorithm 1. For each possible feature h
we maintain auxiliary variables W+h and W
?
h :
W?h =
?
(q,d+,d?):h(q,d+)?h(q,d?)=?1
D(q,d+,d?),
which are the cumulative weights of correctly and
incorrectly ranked instances by a candidate feature
h. The absolute value of ?Lexp/?h can be ex-
pressed as
?
?
?
W+h ?
?
W?h
?
? which is used as fea-
ture selection criterion (Collins and Koo, 2005).
The optimum of minimizing Lexp over w (with
fixed h) can be shown to be w = 12 ln
W+h +Z
W?h +Z
,
where  is a smoothing parameter to avoid prob-
lems with small W?h (Schapire and Singer, 1999),
and Z =
?
(q,d+,d?)?RD(q,d
+,d?). Further-
more, for each step t of the learning process, values
of D are updated to concentrate on pairs that have
not been correctly ranked so far:
Dt+1 = Dt ? e
wt
(
ht(q,d?)?ht(q,d+)
)
. (5)
Finally, to speed up learning, on iteration t we
recalculate W?h only for those h that cooccur
with previously selected ht and keep the rest un-
changed (Collins and Koo, 2005).
Algorithm 1: Bagged Boosting
Input: training tuplesR, max number of features
T , initial D0, smoothing param.  ' 10?5
Initialize:
fromR draw S samples with replacement and
distribute to nodes
Learn:
for all samples s = 1 . . . S in parallel do
calculate W+h ,W
?
h , Z on sample?s data
for all t = 1 . . . T do
choose ht = argmaxh
?
?
?
W+h ?
?
W?h
?
?
and wt = 12 ln
W+h +Z
W?h +Z
update Dt according to (5)
update W?h for all h that cooccur with ht
end
return to master {hst , w
s
t }, t = 1 . . . T
end
Bagging:
return scoring function
f(q,d) = 1S
?
t
?
sw
s
th
s
t (q,d)
Implementation Because of the total number of
features (billions) there are several obstacles for the
straight-forward implementation of Algorithm 1.
First, we cannot directly access all pairs (q,d)
containing a particular feature h needed for calcu-
lating W?h . Building an inverted index is compli-
cated as it needs to fit into memory for fast fre-
1692
quent access8. We resort to the on-the-fly creation of
the cross-product space of features, following prior
work by Grangier and Bengio (2008) and Goel et al
(2008). That is, while processing a pair (q,d), we
update W?h for all h found for the pair.
Second, even if the explicit representation of all
features is avoided by on-the-fly feature construc-
tion, we still need to keep all W?h in addressable
RAM. To achieve that we use hash kernels (Shi et
al., 2009) and map original features into b-bit integer
hashes. The values W?h? for new, ?hashed?, features
h? become W?h? =
?
h:HASH(h)=h?W
?
h . We used
the MurmurHash3 function on the UTF-8 represen-
tations of features and b = 30 (resulting in more
than 1 billion distinct hashes).
4 Model Combination by Borda Counts
SMT-based approaches to CLIR and our boosting
approach have different strengths. The SMT-based
approaches produce fluent translations that are use-
ful for matching general passages written in natu-
ral language. Both baseline SMT-based approaches
presented above are agnostic of the ultimate retrieval
task and are not specifically adapted for it. The
boosting method, on the other hand, learns domain-
specific word associations that are useful to discern
relevant from irrelevant documents. In order to com-
bine these orthogonal sources of information in a
way that democratically respects each approach we
use Borda Counts, i.e., a consensus-based voting
procedure that has been successfully employed to
aggregate ranked lists of documents for metasearch
(Aslam and Montague, 2001).
We implemented a weighted version of the Borda
Count method where each voter has a fixed amount
of voting points which she is free to distribute among
the candidates to indicate the amount of preference
she is giving to each of them. In the case of retrieval,
for each q, the candidates are the scored documents
in the retrieved subset of the whole document set.
The aggregate score fagg for two rankings f1(q,d)
8It is possible to construct separate query and document in-
verted indices and intersect them on the fly to determine the
set of documents that contains some pair of words. In practice,
however, we found the overhead of set intersection during each
feature access prohibitive.
and f2(q,d) for all (q,d) in the test set is then:
fagg(q,d) = ?
f1(q,d)
?
d f1(q,d)
+(1??)
f2(q,d)
?
d f2(q,d)
.
In practice, the normalizations sum over the top K
retrieved documents. If a document is present only
in the top-K list of one system, its score is con-
sidered zero for the other system. The aggregated
scores fagg(q,d) are sorted in descending order and
top K scores are kept for evaluation.
Using the terminology proposed by Belkin et
al. (1995), combining several systems? scores with
Borda Counts can be viewed as the ?data fusion?
approach to IR, that merges outputs of the systems,
while the PSQ baseline is an example of the ?query
combination? approach that extends the query at the
input. Both techniques were earlier found to have
similar performance in CLIR tasks based on direct
translation, with a preference for the data fusion ap-
proach (Jones and Lam-Adesina, 2002).
5 Translation and Ranking Data
5.1 Parallel Translation Data
For Japanese-to-English patent translation we used
data provided by the organizers of the NTCIR9
workshop for the JP-EN PatentMT subtask. In par-
ticular, we used the data provided for NTCIR-7 (Fu-
jii et al, 2008), consisting of 1.8 million parallel
sentence pairs from the years 1993-2002 for train-
ing. For parameter tuning we used the develop-
ment set of the NTCIR-8 test collection, consisting
of 2,000 sentence pairs. The data were extracted
from the description section of patents published
by the Japanese Patent Office (JPO) and the United
States Patent and Trademark Office (USPTO) by the
method described in Utiyama and Isahara (2007).
Japanese text was segmented using the MeCab10
toolkit. Following Feng et al (2011), we applied
a modified version of the compound splitter de-
scribed in Koehn and Knight (2003) to katakana
terms, which are often transliterations of English
compound words. As these are usually not split by
MeCab, they can cause a large number of out-of-
vocabulary terms.
9http://research.nii.ac.jp/ntcir/ntcir/
10https://code.google.com/p/mecab/
1693
#queries #relevant #unique docs
train 107,061 1,422,253 888,127
dev 2,000 26,478 25,669
test 2,000 25,173 24,668
Table 1: Statistics of ranking data.
For the English side of the training data, we ap-
plied a modified version of the tokenizer included in
the Moses scripts. This tokenizer relies on a list of
non-breaking prefixes which mark expressions that
are usually followed by a ?.? (period). We cus-
tomized the list of prefixes by adding some abbrevi-
ations like ?Chem?, ?FIG? or ?Pat?, which are spe-
cific to patent documents.
5.2 Ranking Data from Patent Citations
Graf and Azzopardi (2008) describe a method to ex-
tract relevance judgements for patent retrieval from
patent citations. The key idea is to regard patent doc-
uments that are cited in a query patent, either by the
patent applicant, or by the patent examiner or in a
patent office?s search report, as relevant for the query
patent. Furthermore, patent documents that are re-
lated to the query patent via a patent family relation-
ship, i.e., patents granted by different patent author-
ities but related to the same invention, are regarded
as relevant. We assign three integer relevance levels
to these three categories of relationships, with high-
est relevance (3) for family patents, lower relevance
for patents cited in search reports by patent examin-
ers (2), and lowest relevance level (1) for applicants?
citations. We also include all patents which are in
the same patent family as an applicant or examiner
citation to avoid false negatives. This methodol-
ogy has been used to create patent retrieval data at
CLEF-IP11 and proved very useful to automatically
create a patent retrieval dataset for our experiments.
For the creation of our dataset, we used the
MAREC12 citation graph to extract patents in cita-
tion or family relation. Since the Japanese portion
of the MAREC corpus only contains English ab-
stracts, but not the Japanese full texts, we merged
the patent documents in the NTCIR-10 test collec-
tion described above with the Japanese (JP) section
11http://www.ifs.tuwien.ac.at/?clef-ip/
12http://www.ifs.tuwien.ac.at/imp/marec.
shtml
of MAREC. Title, abstract, description and claims
were added to the MAREC-JP data if the docu-
ment was available in NTCIR. In order to keep par-
allel data for SMT training separate from ranking
data, we used only data from the years 2003-2005
to extract training data for ranking, and two small
datasets of 2,000 queries each from the years 2006-
2007 for development and testing. Table 1 gives an
overview over the data used for ranking. For de-
velopment and test data, we randomly added irrele-
vant documents from the NTCIR-10 collection until
we obtained two pools of 100,000 documents. The
necessary information to reproduce the exact train,
development and test data samples is downloadable
from authors? webpage13.
The experiments reported here use only the ab-
stract of the Japanese and English patents in our
training, development and test collection.
6 Experiments
6.1 System Development
System development and evaluation in our exper-
iments was done on the ranking data described
in the previous section (see Table 1). We report
Mean Average Precision (MAP) scores, using the
trec eval (ver. 8.1) script from the TREC evalu-
ation campaign14, with a limit of top K = 1, 000 re-
trieved documents for each query. Furthermore, we
use the Patent Retrieval Evaluation Score (PRES)15
introduced by Magdy and Jones (2010). This met-
ric accounts for both precision and recall. In the
study by Magdy and Jones (2010), PRES agreed
with MAP in almost 80% of cases, and both agreed
on the ranks of the best and the worst IR system.
Both MAP and PRES scores are reported in the same
range [0, 1], and 0.01 stands for 1 MAP (PRES)
point. Statistical significance of pairwise system
comparisons was assessed using the paired random-
ization test (Noreen, 1989; Smucker et al, 2007).
For each system, optimal meta-parameter settings
were found by choosing the configuration with high-
est MAP score on the development set. These results
13http://www.cl.uni-heidelberg.de/
statnlpgroup/boostclir
14http://trec.nist.gov/trec_eval
15http://www.computing.dcu.ie/?wmagdy/
Scripts/PRESeval.htm
1694
method MAP PRESdev test dev test
1 DT 0.2636 0.2555 0.5669 0.5681
2 PSQ lexical table 0.2520 0.2444 0.5445 0.5498
3 PSQ n-best table 0.2698 0.2659 0.5789 0.5851
Boost-1g 0.2064 1230.1982 0.5850 120.6122
Boost-2g 0.2526 30.2474 0.6900 1230.7196
Table 2: MAP and PRES scores for CLIR methods (best
configurations) on the development and test sets. Prefixed
numbers denote statistical significance of a pairwise com-
parison with the baseline indicated by the superscript. For
example, the bottom right result shows that Boost-2g is
significantly better than DT (method 1), PSQ lexical ta-
ble (method 2) and PSQ n-best table (method 3).
(together with PRES results) are shown in the sec-
ond and fourth column of Table 2.
The direct translation approach (DT) was devel-
oped in three configurations: no stopword filtering,
small stopword list (52 words) and a large stopword
list (543 words). The last configuration achieved the
highest score (MAP 0.2636).
The probabilistic structured query (PSQ) ap-
proach was developed using the lexical translation
table and the translation table estimated on the de-
coder?s n-best list, both optionally pruned with a
variable lower pL and cumulative pC threshold on
the word pair probability in the table (Section 3.2).
A further meta-parameter of PSQ was whether to use
standard or unique n-best lists. Finally, all variants
were coupled with the same stopword filters as in
the DT approach. The configurations that achieved
the highest scores were: MAP 0.2520 for PSQ with
a lexical table (pL = 0.01, pC = 0.95, no stop-
word filtering), and MAP 0.2698 for PSQ with a
translation table estimated on the n-best list (pL =
0.005, pC = 0.95, large stopword list). Interpolat-
ing between lexical and n-best tables did not im-
prove results in our experiments, thus we set ? = 1
in equation (4).
Each SMT-based system was run with 4 different
MERT optimizations, leading to variations of less
than 1 MAP point for each system. The best con-
figurations for DT and PSQ on the development set
were fixed and used for evaluation on the test set.
Training of the boosting approach (Boost) was
done in parallel on bootstrap samples from the train-
ing data. First, a query q (i.e., a Japanese abstract)
was sampled uniformly from all training queries.
method MAP PRESdev test dev test
DT + PSQ n-best 0.2778 ?0.2726 0.5884 ?0.5942
DT + Boost-1g 0.2778 ?0.2728 0.6157 ?0.6225
DT + Boost-2g 0.3309 ?0.3300 0.7132 ?0.7279
PSQ lexical + Boost-1g 0.2695 ?0.2653 0.6068 ?0.6131
PSQ lexical + Boost-2g 0.3215 ?0.3187 0.7071 ?0.7240
PSQ n-best + Boost-1g 0.2863 ?0.2850 0.6309 ?0.6402
PSQ n-best + Boost-2g 0.3439 ?0.3416 0.7212 ?0.7376
Table 3: MAP and PRES scores of the aggregated mod-
els on the development and test sets. Development scores
correspond to peaks in Figures 1 and 3, respectively, for
MAP and PRES; test scores are given for the ??s deliv-
ering these peaks on the development set. Prefixed ? in-
dicates statistical significance of the result difference be-
tween aggregated system and the respective translation-
based system used in the aggregation.
Then we sampled independently and uniformly a
relevant document d+ (i.e., an English abstract)
from the English patents marked relevant for the
Japanese patent, and a random document d? from
the whole pool of English patent abstracts. If d?
had a relevance score greater or equal to the rele-
vance score of d+, it was resampled. The initial im-
portance weight D0 for a triplet (q,d+,d?) was set
to the positive difference in relevance scores for d+
and d?. Each bootstrap sample consisted of 10 pairs
of documents for each of 10, 000 queries, resulting
in 100, 000 training instances per sample.
The Boost approach was developed for uni-gram
and combined uni- and bi-gram versions. We ob-
served that the performance of the Boost method
continuously improved with the number of iterations
T and with the number of samples S, but saturated
at about 15-20 samples without visible over-fitting
in the tested range of T . Therefore we arbitrarily
stopped training after obtaining 5, 000 features per
sample, and used 35 samples for uni-gram version
and 65 samples for the combined bi-gram version,
resulting in models with 104K and 172K unique fea-
tures, respectively. The optimal values for the pass-
through weight ? were found to be 0.3 and 0.2 for
the uni-gram and bi-gram models on the develop-
ment set. The best configuration of uni-gram and
bi-gram model achieved MAP scores of 0.2064 and
0.2526 the development set. Using stopword filters
during training did not improve the results here.
1695
0.24
0.25
0.26
0.27
0.28
0.29
0.30
0.31
0.32
0.33
0.34
0.35
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
M
A
P
?
DT + Boost-2gPSQ lexical + Boost-2gPSQ n-best + Boost-2gDT, 0.2636PSQ lexical, 0.2520PSQ n-best, 0.2698Boost-2g, 0.2526PSQ n-best + DT
Figure 1: MAP rank aggregation for combinations of the
bi-gram boosting and the baselines on the dev set.
0.24
0.25
0.26
0.27
0.28
0.29
0.30
0.31
0.32
0.33
0.34
0.35
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
M
A
P
?
dev, PSQ n-best + Boost-2gtest, PSQ n-best + Boost-2gdev, PSQ n-best, 0.2698test, PSQ n-best, 0.2659dev, Boost-2g, 0.2526test, Boost-2g, 0.2474
Figure 2: MAP rank aggregation for the bi-gram boosting
and the ?PSQ n-best table? approach on dev and test sets.
0.54
0.56
0.58
0.60
0.62
0.64
0.66
0.68
0.70
0.72
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
P
R
E
S
?
DT + Boost-2gPSQ lexical + Boost-2gPSQ n-best + Boost-2gDT, 0.5669PSQ lexical, 0.5445PSQ n-best, 0.5789Boost-2g, 0.6900PSQ n-best + DT
Figure 3: PRES rank aggregation for combinations of the
bi-gram boosting and the baselines on the dev set.
6.2 Testing and Model Combination
The third and the fifth columns of Table 2 give a
comparison of the MAP scores of the baseline ap-
proaches and the Boost model evaluated individu-
ally on the test set. Each score corresponds to the
best configuration found on the development set. We
see that the PSQ approach using n-best lists for pro-
jection outperforms all other methods in terms of
MAP, but loses to both Boost approaches when eval-
uated with PRES. Direct translation is about 1 MAP
point lower than PSQ n-best; Boost with combined
uni- and bi-grams is another 0.8 MAP points worse,
but is better in terms of PRES, especially for the bi-
gram version. Given the fact that the complex SMT
system behind the direct translation and PSQ ap-
proach is trained and tuned on very large in-domain
datasets, the performance of the bare phrase table
induced by the Boost method is respectable.
Our best results are obtained by a combination
of the orthogonal information sources of the SMT
and the Boost approaches. We evaluated the Borda
Count aggregation scheme on the development data
in order to find the optimal value for ? ? [0, 1]. The
interpolation was done for the best combined uni-
and bi-gram boosting model with the best variants of
the DT and PSQ approaches. As can be seen from
Figures 1 and 3, rank aggregation by Borda Count
outperforms both individual approaches by a large
margin. Figure 2 verifies that the results are trans-
ferable from the development set to the test set. The
best performing system combination on the develop-
ment data is also optimal on the test data.
Table 3 shows the retrieval performance of the
best baseline model (PSQ n-best) combined with
the best Boost model (bi-gram), with an impres-
sive gain of over 7 MAP points (15 PRES points)
over the best individual baseline result from Table 2.
Even when, according to the PRES measure (Fig-
ure 3), the Boost-2g system is better on its own, in-
jecting complementary information from the PSQ or
DT approach still contributes several points. Simi-
lar gains are obtained by model combination of the
DT approach with the best Boost model. However,
a combination of the SMT-based CLIR approaches
DT and PSQ barely improved results over the best
input model. In summary, aggregating rankings is
helpful for orthogonal systems, but not for systems
including similar information.
1696
6.3 Analysis
Table 4 lists some of the top-200 selected features
for the boosting approach (the most common trans-
lation of the Japanese term is put in subscript).
We see that the direct ranking approach is able
to penalize uni- and bi-gram cooccurrences that are
harmful for retrieval by assigning them a negative
weight, e.g., the pairing of??resolution with image.
Pairs of uni- and bi-grams that are useful for re-
trieval are boosted by positive weights, e.g., the pair
??compression,?machine and compressor captures an
important compound. Further examples, not shown
in the table, are matches of the same source (tar-
get) n-gram with several different target (source) n-
grams, e.g., the Japanese term ??image is paired
not only with its main translation, but also with
dozens of related notions: video, picture, scanning,
printing, photosensitive, pixel, background etc. This
has a query expansion effect that is not possible in
systems that use one translation or a small list of n-
best translations. In addition, associations of source
n-grams with overlapping target n-grams help boost
the final score: e.g., the same term??image is pos-
itively paired with target bi-grams as {an,original},
{original,image} and {image,for}. This has the ef-
fect of compensating for the lack of handling phrase
overlaps in an SMT decoder.
7 Conclusion
We presented a boosting approach to induce a table
of bilingual n-gram correspondences by direct pref-
erence learning on relevance rankings. This table
can be seen as a phrase table that encodes word-
based information that is orthogonal and comple-
mentary to the information in standard translation-
based CLIR approaches. We compared our boosting
approach to very competitive CLIR baselines that
use a complex SMT system trained and tuned on
large in-domain datasets. Furthermore, our patent
retrieval setup gives SMT-based approaches an ad-
vantage in that queries consist of several normal-
length sentences, as opposed to the short queries
common to web search. Despite this and despite the
tiny size (about 170K parameters) of the boosting
phrase table, compared to standard SMT phrase ta-
bles, this approach reached performance similar to
direct translation using a full SMT model in terms
t ht (uni- & bi-grams) wt
1 ?layer - layer 1.29
2 ???data - data 1.13
3 ??circuit - circuit 1.13
76 ?in - voltage -0.39
77 ?guide,?power - conductive 1.25
81 ??resolution - image -0.25
99 ??speed - transmission 1.68
100 ??LCD - liquid,crystal 1.73
123 ?power - force 0.91
124 ??compression,?machine - compressor 2.83
132 ????cable - cable 1.81
133 ?hyper,??sound wave - ultrasonic 3.34
169 ??particle - particles 1.57
170 ??calculation - for,each 1.14
184 ???rotor - rotor 2.01
185 ??detection,?vessel - detector 1.43
Table 4: Examples of the features found by boosting.
of MAP, and was significantly better in terms of
PRES. Overall, we obtained the best results by a
model combination using consensus- based voting
where the best SMT-based approach was combined
with the boosting phrase table (gaining more than 7
MAP or 15 PRES points). We attribute this to the
fact that the boosting approach augments SMT ap-
proaches with valuable information that is hard to
get in approaches that are agnostic about the rank-
ing data and the ranking task at hand.
The experimental setup presented in this paper
uses relevance links between patent abstracts as
ranking data. While this technique is useful to de-
velop patent retrieval systems, it would be interest-
ing to see if our results transfer to patent retrieval
scenarios where full patent documents are used in-
stead of only abstracts, or to standard CLIR scenar-
ios that use short search queries in retrieval.
Acknowledgements
The research presented in this paper was supported
in part by DFG grant ?Cross-language Learning-to-
Rank for Patent Retrieval?. We would like to thank
Eugen Ruppert for his contribution to the ranking
data construction.
1697
References
Javed A. Aslam and Mark Montague. 2001. Models for
metasearch. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR?01), New Orleans, LA.
Bing Bai, Jason Weston, David Grangier, Ronan
Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2010. Learning to
rank with (a lot of) word features. Information Re-
trieval Journal, 13(3):291?314.
Lisa Ballesteros and W. Bruce Croft. 1998. Resolving
ambiguity for cross-language retrieval. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?98), Mel-
bourne, Australia.
Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and
Joseph A. Shaw. 1995. Combining the evidence
of multiple query representations for information re-
trieval. Inf. Process. Manage., 31(3):431?448.
Leo Breiman. 1996. Bagging predictors. Journal of Ma-
chine Learning Research, 24:123?140.
Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFad-
den, Ken Goldman, Mike Gunter, Jeremiah Harm-
sen, Kristen LeFevre, Dmitry Lepikhin, Tomas Lloret
Llinares, Indraneel Mukherjee, Fernando Pereira, Josh
Redstone, Tal Shaked, and Yoram Singer. 2010.
Sibyl: A system for large scale machine learning.
In LADIS: The 4th ACM SIGOPS/SIGACT Workshop
on Large Scale Distributed Systems and Middleware,
Zurich, Switzerland.
Xi Chen, Bing Bai, Yanjun Qi, Qihang Ling, and Jaime
Carbonell. 2010. Learning preferences with millions
of parameters by enforcing sparsity. In Proceedings
of the IEEE International Conference on Data Mining
(ICDM?10), Sydney, Australia.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jeffrey Chin, Maureen Heymans, Alexandre Kojoukhov,
Jocelyn Lin, and Hui Tan. 2008. Cross-language
information retrieval. Patent Application. US
2008/0288474 A1.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?69.
Michael Collins, Robert E. Schapire, and Yoram Singer.
2002. Logistic regression, AdaBoost and Bregman
distances. Journal of Machine Learning Research,
48(1-3):253?285.
Kareem Darwish and Douglas W. Oard. 2003. Proba-
bilistic structured query methods. In Proceedings. of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR?03), Toronto,
Canada.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, Upp-
sala, Sweden.
Minwei Feng, Christoph Schmidt, Joern Wuebker,
Stephan Peitz, Markus Freitag, and Hermann Ney.
2011. The RWTH Aachen system for NTCIR-9
PatentMT. In Proceedings of the NTCIR-9 Workshop,
Tokyo, Japan.
Yoav Freund, Ray Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. Journal of Machine Learning
Research, 4:933?969.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent trans-
lation task at the NTCIR-7 workshop. In Proceedings
of NTCIR-7 Workshop Meeting, Tokyo, Japan.
Yasser Ganjisaffar, Rich Caruana, and Cristina Videira
Lopes. 2011. Bagging gradient-boosted trees for
high precision, low variance ranking models. In Pro-
ceedings of the ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR?11),
Beijing, China.
Jianfeng Gao, Jian-Yun Nie, Endong Xun, Jian Zhang,
Ming Zhou, and Changning Huang. 2001. Improv-
ing query translation for cross-language information
retrieval using statistical models. In Proceedings of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR?01), New Or-
leans, LA.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,
Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Cross-
lingual query suggestion using query logs of different
languages. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR?07), Amsterdam, The Netherlands.
Sharad Goel, John Langford, and Alexander L. Strehl.
2008. Predictive indexing for fast search. In Advances
in Neural Information Processing Systems, Vancouver,
Canada.
Erik Graf and Leif Azzopardi. 2008. A methodology
for building a patent test collection for prior art search.
In Proceedings of the 2nd International Workshop on
Evaluating Information Access (EVIA), Tokyo, Japan.
David Grangier and Samy Bengio. 2008. A discrimi-
native kernel-based approach to rank images from text
queries. IEEE Transactions on Pattern Analysis and
Machine Intelligence (PAMI), 30(8):1371?1384.
Yunsong Guo and Carla Gomes. 2009. Ranking struc-
tured documents: A large margin based approach for
1698
patent prior art search. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI?09), Pasadena, CA.
Kenneth Heafield. 2011. KenLM: faster and smaller lan-
guage model queries. In Proceedings of the EMNLP
2011 Sixth Workshop on Statistical Machine Transla-
tion (WMT?11), Edinburgh, UK.
Gareth J.F. Jones and Adenike M. Lam-Adesina. 2002.
Combination methods for improving the reliability of
machine translation based cross-language information
retrieval. In Proceedings of the 13th Irish Interna-
tional Conference on Artificial Intelligence and Cog-
nitive Science (AICS?02), Limerick, Ireland.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
Conference on European Chapter of the Association
for Computational Linguistics (EACL?03), Budapest,
Hungary.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-lingual relevance models. In Proceed-
ings of the ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR?02), Tampere,
Finland.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007), Prague,
Czech Republic.
Walid Magdy and Gareth J.F. Jones. 2010. PRES: a
score metric for evaluating recall-oriented information
retrieval applications. In Proceedings of the ACM SI-
GIR conference on Research and development in in-
formation retrieval (SIGIR?10), New York, NY.
Walid Magdy and Gareth J. F. Jones. 2011. An efficient
method for using machine translation technologies in
cross-language patent search. In Proceedings of the
20th ACM Conference on Informationand Knowledge
Management (CIKM?11), Glasgow, Scotland, UK.
Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos,
and Christof Monz. 2012. Adaptation of statistical
machine translation model for cross-lingual informa-
tion retrieval in a service context. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics (EACL?12),
Avignon, France.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Meeting of the Association for Computational Linguis-
tics (ACL?00), Hongkong, China.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Meeting on Association for Computational Lin-
guistics (ACL?03), Sapporo, Japan.
Dmitry Pavlov, Alexey Gorodilov, and Cliff A. Brunk.
2010. Bagboo: a scalable hybrid bagging-the-
boosting model. In Proceedings of the 19th ACM
International Conference on Information and Knowl-
edge Management (CIKM?10), Toronto, Canada.
Stephen E. Robertson, Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Conference
(TREC-7), Gaithersburg, MD.
Saharon Rosset, Ji Zhu, and Trevor Hastie. 2004. Boost-
ing as a regularized path to a maximum margin clas-
sifier. Journal of Machine Learning Research, 5:941?
973.
Robert E. Schapire and Yoram Singer. 1999. Im-
proved boosting algorithms using confidence-rated
predictions. Journal of Machine Learning Research,
37(3):297?336.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alexander J. Smola, Alexander L. Strehl, and
Vishy Vishwanathan. 2009. Hash Kernels. In Pro-
ceedings of the 12th Int. Conference on Artificial In-
telligence and Statistics (AISTATS?09), Irvine, CA.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests for
information retrieval evaluation. In Proceedings of the
16th ACM conference on Conference on Information
and Knowledge Management (CIKM ?07), New York,
NY.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a.
Combining statistical translation techniques for cross-
language information retrieval. In Proceedings of the
International Conference on Computational Linguis-
tics (COLING 2012), Bombay, India.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012b.
Looking inside the box: Context-sensitive translation
for cross-language information retrieval. In Proceed-
ings of the ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR 2012),
Portland, OR.
Masao Utiyama and Hitoshi Isahara. 2007. A Japanese-
English patent parallel corpus. In Proceedings of MT
Summit XI, Copenhagen, Denmark.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual in-
formation retrieval. In Proceedings of the ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR?01), New York, NY.
1699
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 818?828,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Structural and Topical Dimensions in Multi-Task Patent Translation
Katharina Wa?schle and Stefan Riezler
Department of Computational Linguistics
Heidelberg University, Germany
{waeschle,riezler}@cl.uni-heidelberg.de
Abstract
Patent translation is a complex problem due
to the highly specialized technical vocab-
ulary and the peculiar textual structure of
patent documents. In this paper we analyze
patents along the orthogonal dimensions of
topic and textual structure. We view differ-
ent patent classes and different patent text
sections such as title, abstract, and claims,
as separate translation tasks, and investi-
gate the influence of such tasks on machine
translation performance. We study multi-
task learning techniques that exploit com-
monalities between tasks by mixtures of
translation models or by multi-task meta-
parameter tuning. We find small but sig-
nificant gains over task-specific training
by techniques that model commonalities
through shared parameters. A by-product
of our work is a parallel patent corpus of 23
million German-English sentence pairs.
1 Introduction
Patents are an important tool for the protection
of intellectual property and also play a significant
role in business strategies in modern economies.
Patent translation is an enabling technique for
patent prior art search which aims to detect a
patent?s novelty and thus needs to be cross-lingual
for a multitude of languages. Patent translation is
complicated by a highly specialized vocabulary,
consisting of technical terms specific to the field
of invention the patent relates to. Patents are writ-
ten in a sophisticated legal jargon (?patentese?)
that is not found in everyday language and ex-
hibits a complex textual structure. Also, patents
are often intentionally ambiguous or vague in or-
der to maximize the coverage of the claims.
In this paper, we analyze patents with respect
to the orthogonal dimensions of topic ? the tech-
nical field covered by the patent ? and structure
? a patent?s text sections ?, with respect to their
influence on machine translation performance.
The topical dimension of patents is charac-
terized by the International Patent Classification
(IPC)1 which categorizes patents hierarchically
into 8 sections, 120 classes, 600 subclasses, down
to 70,000 subgroups at the leaf level. Table 1
shows the 8 top level sections.
A Human Necessities
B Performing Operations, Transporting
C Chemistry, Metallurgy
D Textiles, Paper
E Fixed Constructions
F Mechanical Engineering, Lighting,
Heating, Weapons
G Physics
H Electricity
Table 1: IPC top level sections.
Orthogonal to the patent classification, patent
documents can be sub-categorized along the di-
mension of textual structure. Article 78.1 of the
European Patent Convention (EPC) lists all sec-
tions required in a patent document2:
?A European patent application shall
contain:
(a) a request for the grant of a Euro-
pean patent;
1http://www.wipo.int/classifications/
ipc/en/
2Highlights by the authors.
818
(b) a description of the invention;
(c) one or more claims;
(d) any drawings referred to in the de-
scription or the claims;
(e) an abstract,
and satisfy the requirements laid down
in the Implementing Regulations.?
The request for grant contains the patent title; thus
a patent document comprises the textual elements
of title, description, claim, and abstract.
We investigate whether it is worthwhile to treat
different values along the structural and topical
dimensions as different tasks that are not com-
pletely independent of each other but share some
commonalities, yet differ enough to counter a
simple pooling of data. For example, we con-
sider different tasks such as patents from different
IPC classes, or along an orthogonal dimension,
patent documents of all IPC classes but consisting
only of titles or only of claims. We ask whether
such tasks should be addressed as separate trans-
lation tasks, or whether translation performance
can be improved by learning several tasks simul-
taneously through shared models that are more so-
phisticated than simple data pooling. Our goal is
to learn a patent translation system that performs
well across several different tasks, thus benefits
from shared information, but is yet able to address
the specifics of each task.
One contribution of this paper is a thorough
analysis of the differences and similarities of mul-
tilingual patent data along the dimensions of tex-
tual structure and topic. The second contribution
is the experimental investigation of the influence
of various such tasks on patent translation perfor-
mance. Starting from baseline models that are
trained on individual tasks or on data pooled from
all tasks, we apply mixtures of translation mod-
els and multi-task minimum error rate training to
multiple patent translation tasks. A by-product of
our research is a parallel patent corpus of over 23
million sentence pairs.
2 Related work
Multi-task learning has mostly been discussed un-
der the name of multi-domain adaptation in the
area of statistical machine translation (SMT). If
we consider domains as tasks, domain adapta-
tion is a special two-task case of multi-task learn-
ing. Most previous work has concentrated on
adapting unsupervised generative modules such
as translation models or language models to new
tasks. For example, transductive approaches have
used automatic translations of monolingual cor-
pora for self-training modules of the generative
SMT pipeline (Ueffing et al 2007; Schwenk,
2008; Bertoldi and Federico, 2009). Other ap-
proaches have extracted parallel data from similar
or comparable corpora (Zhao et al 2004; Snover
et al 2008). Several approaches have been pre-
sented that train separate translation and language
models on task-specific subsets of the data and
combine them in different mixture models (Fos-
ter and Kuhn, 2007; Koehn and Schroeder, 2007;
Foster et al 2010). The latter kind of approach is
applied in our work to multiple patent tasks.
Multi-task learning efforts in patent transla-
tion have so far been restricted to experimental
combinations of translation and language mod-
els from different sets of IPC sections. For ex-
ample, Utiyama and Isahara (2007) and Tinsley
et al(2010) investigate translation and language
models trained on different sets of patent sections,
with larger pools of parallel data improving re-
sults. Ceaus?u et al(2011) find that language mod-
els always and translation model mostly benefit
from larger pools of data from different sections.
Models trained on pooled patent data are used as
baselines in our approach.
The machine learning community has devel-
oped several different formalizations of the cen-
tral idea of trading off optimality of parameter
vectors for each task-specific model and close-
ness of these model parameters to the average pa-
rameter vector across models. For example, start-
ing from a separate SVM for each task, Evgeniou
and Pontil (2004) present a regularization method
that trades off optimization of the task-specific pa-
rameter vectors and the distance of each SVM to
the average SVM. Equivalent formalizations re-
place parameter regularization by Bayesian prior
distributions on the parameters (Finkel and Man-
ning, 2009) or by augmentation of the feature
space with domain independent features (Daume?,
2007). Besides SVMs, several learning algo-
rithms have been extended to the multi-task sce-
nario in a parameter regularization setting, e.g.,
perceptron-type algorithms (Dredze et al 2010)
or boosting (Chapelle et al 2011). Further vari-
ants include different formalizations of norms for
parameter regularization, e.g., `1,2 regularization
819
(Obozinski et al 2010) or `1,? regularization
(Quattoni et al 2009), where only the features
that are most important across all tasks are kept in
the model. In our experiments, we apply parame-
ter regularization for multi-task learning to mini-
mum error rate training for patent translation.
3 Extraction of a parallel patent corpus
from comparable data
Our work on patent translation is based on the
MAREC3 patent data corpus. MAREC con-
tains over 19 million patent applications and
granted patents in a standardized format from
four patent organizations (European Patent Of-
fice (EP), World Intellectual Property Organiza-
tion (WO), United States Patent and Trademark
Office (US), Japan Patent Office (JP)), from 1976
to 2008. The data for our experiments are ex-
tracted from the EP and WO collections which
contain patent documents that include translations
of some of the patent text. To extract such parallel
patent sections, we first determine the longest in-
stance, if different kinds4 exist for a patent. We
assume titles to be sentence-aligned by default,
and define sections with a token ratio larger than
0.7 as parallel. For the language pair German-
English we extracted a total of 2,101,107 parallel
titles, 291,716 parallel abstracts, and 735,667 par-
allel claims sections.
The lack of directly translated descriptions
poses a serious limitation for patent translation,
since this section constitutes the largest part of the
document. It is possible to obtain comparable de-
scriptions from related patents that have been filed
in different countries and are connected through
the patent family id. We extracted 172,472 patents
that were both filed with the USPTO and the EPO
and contain an English and a German description,
respectively.
For sentence alignment, we used the Gargan-
tua5 tool (Braune and Fraser, 2010) that fil-
ters a sentence-length based alignment with IBM
Model-1 lexical word translation probabilities, es-
timated on parallel data obtained from the first-
3http://www.ir-facility.org/
prototypes/marec
4A patent kind code indicates the document stage in the
filing process, e.g., A for applications and B for granted
patents, with publication levels from 1-9. See http://
www.wipo.int/standards/en/part\_03.html.
5http://gargantua.sourceforge.net
pass alignment. This yields the parallel corpus
listed in table 2 with high input-output ratios for
claims, and much lower ratios for abstracts and
descriptions, showing that claims exhibit a nat-
ural parallelism due to their structure, while ab-
stracts and descriptions are considerably less par-
allel. Removing duplicates and adding parallel ti-
tles results in a corpus of over 23 million parallel
sentence pairs.
output de ratio en ratio
abstract 720,571 92.36% 76.81%
claims 8,346,863 97.82% 96.17%
descr. 14,082,381 86.23% 82.67%
Table 2: Number of parallel sentences in output with
input/output ratio of sentence aligner.
Differences between the text sections become
visible in an analysis of token to type ratios. Ta-
ble 3 gives the average number of tokens com-
pared to the average type frequencies for a win-
dow of 100,000 tokens from every subsection. It
shows that titles contain considerably fewer to-
kens than other sections, however, the disadvan-
tage is partially made up by a relatively large
amount of types, indicated by a lower average
type frequency.
tokens types
de en de en
title 6.5 8.0 2.9 4.8
abstract 37.4 43.2 4.3 9.0
claims 53.2 61.3 5.5 9.5
description 27.5 35.5 4.0 7.0
Table 3: Average number of tokens and average type
frequencies in text sections.
We reserved patent data published between
1979 and 2007 for training and documents pub-
lished in 2008 for tuning and testing in SMT.
For the dimension of text sections, we sampled
500,000 sentences ? distributed across all IPC
sections ? for training and 2,000 sentences for
each text section for development and testing. Be-
cause of a relatively high number of identical sen-
tences in test and training set for titles, we re-
moved the overlap for this section.
Table 4 shows the distribution of IPC sections
on claims, with the smallest class accounting for
820
around 300,000 parallel sentences. In order to ob-
tain similar amounts of training data for each task
along the topical dimension, we sampled 300,000
sentences from each IPC class for training, and
2,000 sentences for each IPC class for develop-
ment and testing.
A 1,947,542
B 2,522,995
C 2,263,375
D 299,742
E 353,910
F 1,012,808
G 2,066,132
H 1,754,573
Table 4: Distribution of IPC sections on claims.
4 Machine translation experiments
4.1 Individual task baselines
For our experiments we used the phrase-based,
open-source SMT toolkit Moses6 (Koehn et al
2007). For language modeling, we computed
5-gram models using IRSTLM7 (Federico et
al., 2008) and queried the model with KenLM
(Heafield, 2011). BLEU (Papineni et al 2001)
scores were computed up to 4-grams on lower-
cased data.
Europarl-v6 MAREC
BLEU OOV BLEU OOV
abstract 0.1726 14.40% 0.3721 3.00%
claim 0.2301 15.80% 0.4711 4.20%
title 0.0964 26.00% 0.3228 9.20%
Table 5: BLEU scores and OOV rate for Europarl base-
line and MAREC model.
Table 5 shows a first comparison of results of
Moses models trained on 500,000 parallel sen-
tences from patent text sections balanced over IPC
classes, against Moses trained on 1.7 Million sen-
tences of parliament proceedings from Europarl8
(Koehn, 2005). The best result on each section is
indicated in bold face. The Europarl model per-
forms very poorly on all three sections in compar-
6http://statmt.org/moses/
7http://sourceforge.net/projects/
irstlm/
8http://www.statmt.org/europarl/
ison to the task-specific MAREC model, although
the former has been learned on more than three
times the amount of data. An analysis of the out-
put of both system shows that the Europarl model
suffers from two problems: Firstly, there is an ob-
vious out of vocabulary (OOV) problem of the
Europarl model compared to the MAREC model.
Secondly, the Europarl model suffers from incor-
rect word sense disambiguation, as illustrated by
the samples in table 6.
source steuerbar leitet
Europarl taxable is in charge of
MAREC controllable guiding
reference controllable guides
Table 6: Output of Europarl model on MAREC data.
Table 7 shows the results of the evaluation
across text sections; we measured the perfor-
mance of separately trained and tuned individual
models on every section. The results allow some
conclusions about the textual characteristics of the
sections and indicate similarities. Naturally, ev-
ery task is best translated with a model trained
on the respective section, as the BLEU scores
on the diagonal are the highest in every column.
Accordingly, we are interested in the runner-up
on each section, which is indicated in bold font.
The results on abstracts suggest that this section
bears the strongest resemblance to claims, since
the model trained on claims achieves a respectable
score. The abstract model seems to be the most
robust and varied model, yielding the runner-up
score on all other sections. Claims are easiest to
translate, yielding the highest overall BLEU score
of 0.4879. In contrast to that, all models score
considerably lower on titles.
test
train abstract claim title desc.
abstract 0.3737 0.4076 0.2681 0.2812
claim 0.3416 0.4879 0.2420 0.2623
title 0.2839 0.3512 0.3196 0.1743
desc. 0.32189 0.403 0.2342 0.3347
Table 7: BLEU scores for 500k individual text section
models.
The cross-section evaluation on the IPC classes
(table 8) shows similar patterns. Each section
821
is best translated with a model trained on data
from the same section. Note that best section
scores vary considerably, ranging from 0.5719 on
C to 0.4714 on H, indicating that higher-scoring
classes, such as C and A, are more homogeneous
and therefore easier to translate. C, the Chem-
istry section, presumably benefits from the fact
that the data contain chemical formulae, which
are language-independent and do not have to be
translated. Again, for determining the relation-
ship between the classes, we examine the best
runner-up on each section, considering the BLEU
score, although asymmetrical, as a kind of mea-
sure of similarity between classes. We can es-
tablish symmetric relationships between sections
A and C, B and F as well as G and H, which
means that the models are mutual runner-up on
the other?s test section.
The similarities of translation tasks estab-
lished in the previous section can be confirmed
by information-theoretic similarity measures that
perform a pairwise comparison of the vocabulary
probability distribution of each task-specific cor-
pus. This distribution is calculated on the basis of
the 500 most frequent words in the union of two
corpora, normalized by vocabulary size. As met-
ric we use the A-distance measure of Kifer et al
(2004). IfA is the set of events on which the word
distributions of two corpora are defined, then the
A-distance is the supremum of the difference of
probabilities assigned to the same event. Low dis-
tance means higher similarity.
Table 9 shows the A-distance of corpora spe-
cific to IPC classes. The most similar section or
sections ? apart from the section itself on the di-
agonal ? is indicated in bold face. The pairwise
similarity of A and C, B and F, G and H obtained
by BLEU score is confirmed. Furthermore, a close
similarity between E and F is indicated. G and
H (electricity and physics, respectively) are very
similar to each other but not close to any other
section apart from B.
4.2 Task pooling and mixture
One straightforward technique to exploit com-
monalities between tasks is pooling data from
separate tasks into a single training set. Instead of
a trivial enlargement of training data by pooling,
we train the pooled models on the same amount
of sentences as the individual models. For in-
stance, the pooled model for the pairing of IPC
section B and C is trained on a data set composed
of 150,000 sentences from each IPC section. The
pooled model for pairing data from abstracts and
claims is trained on data composed of 250,000
sentences from each text section.
Another approach to exploit commonalities be-
tween tasks is to train separate language and trans-
lation models9 on the sentences from each task
and combine the models in the global log-linear
model of the SMT framework, following Fos-
ter and Kuhn (2007) and Koehn and Schroeder
(2007). Model combination is accomplished by
adding additional language model and translation
model features to the log-linear model and tuning
the additional meta-parameters by standard mini-
mum error rate training (Bertoldi et al 2009).
We try out mixture and pooling for all pairwise
combinations of the three structural sections, for
which we have high-quality data, i.e. abstract,
claims and title. Due to the large number of pos-
sible combinations of IPC sections, we limit the
experiments to pairs of similar sections, based on
the A-distance measure.
Table 10 lists the results for two combinations
of data from different sections: a log-linear mix-
ture of separately trained models and simple pool-
ing, i.e. concatenation, of the training data. Over-
all, the mixture models perform slightly better
than the pooled models on the text sections, al-
though the difference is significant only in two
cases. This is indicated by highlighting best re-
sults in bold face (with more than one result high-
lighted if the difference is not significant).10
We investigate the same mixture and pooling
techniques on the IPC sections we considered
pairwise similar (see table 11). Somehow contra-
dicting the former results, the mixture models per-
form significantly worse than the pooled model on
three sections. This might be the result of inade-
quate tuning, since most of the time the MERT
algorithm did not converge after the maximum
number of iterations, due to the larger number of
features when using several models.
9Following Duh et al(2010), we use the alignment
model trained on the pooled data set in the phrase extraction
phase of the separate models. Similarly, we use a globally
trained lexical reordering model.
10For assessing significance, we apply the approximate
randomization method described in Riezler and Maxwell
(2005). We consider pairwise differing results scoring a p-
value smaller than 0.05 as significant; the assessment is re-
peated three times and the average value is taken.
822
test
train A B C D E F G H
A 0.5349 0.4475 0.5472 0.4746 0.4438 0.4523 0.4318 0.4109
B 0.4846 0.4736 0.5161 0.4847 0.4578 0.4734 0.4396 0.4248
C 0.5047 0.4257 0.5719 0.462 0.4134 0.4249 0.409 0.3845
D 0.47 0.4387 0.5106 0.5167 0.4344 0.4435 0.407 0.3917
E 0.4486 0.4458 0.4681 0.4531 0.4771 0.4591 0.4073 0.4028
F 0.4595 0.4588 0.4761 0.4655 0.4517 0.4909 0.422 0.4188
G 0.4935 0.4489 0.5239 0.4629 0.4414 0.4565 0.4748 0.4532
H 0.4628 0.4484 0.4914 0.4621 0.4421 0.4616 0.4588 0.4714
Table 8: BLEU scores for 300k individual IPC section models.
A B C D E F G H
A 0 0.1303 0.1317 0.1311 0.188 0.186 0.164 0.1906
B 0.1302 0 0.2388 0.1242 0.0974 0.0875 0.1417 0.1514
C 0.1317 0.2388 0 0.1992 0.311 0.3068 0.2506 0.2825
D 0.1311 0.1242 0.1992 0 0.1811 0.1808 0.1876 0.201
E 0.188 0.0974 0.311 0.1811 0 0.0921 0.2058 0.2025
F 0.186 0.0875 0.3068 0.1808 0.0921 0 0.1824 0.1743
G 0.164 0.1417 0.2506 0.1876 0.2056 0.1824 0 0.064
H 0.1906 0.1514 0.2825 0.201 0.2025 0.1743 0.064 0
Table 9: Pairwise A-distance for 300k IPC training sets.
train test pooling mixture
abstract-claim abstract 0.3703 0.3704
claim 0.4809 0.4834
claim-title claim 0.4799 0.4789
title 0.3269 0.328
title-abstract title 0.3311 0.3275
abstract 0.3643 0.366
Table 10: Mixture and pooling on text sections.
A comparison of the results for pooling and
mixture with the respective results for individual
models (tables 7 and 8) shows that replacing data
from the same task by data from related tasks
decreases translation performance in almost all
cases. The exception is the title model that bene-
fits from pooling and mixing with both abstracts
and claims due to their richer data structure.
4.3 Multi-task minimum error rate training
In contrast to task pooling and task mixtures, the
specific setting addressed by multi-task minimum
error rate training is one in which the generative
train test pooling mixture
A-C A 0.5271 0.5274
C 0.5664 0.5632
B-F B 0.4696 0.4354
F 0.4859 0.4769
G-H G 0.4735 0.4754
H 0.4634 0.467
Table 11: Mixture and pooling on IPC sections.
SMT pipeline is not adaptable. Such situations
arise if there are not enough data to train transla-
tion models or language models on the new tasks.
However, we assume that there are enough paral-
lel data available to perform meta-parameter tun-
ing by minimum error rate training (MERT) (Och,
2003; Bertoldi et al 2009) for each task.
A generic algorithm for multi-task learning
can be motivated as follows: Multi-task learning
aims to take advantage of commonalities shared
among tasks by learning several independent but
related tasks together. Information is shared be-
tween tasks through a joint representation and in-
823
tuning
test individual pooled average MMERT MMERT-average
abstract 0.3721 0.362 0.3657?+ 0.3719+ 0.3685?+
claim 0.4711 0.4681 0.4749?+ 0.475?+ 0.4734?+
title 0.3228 0.3152 0.3326?+ 0.3268?+ 0.3325?+
Table 12: Multi-task tuning on text sections.
tuning
test individual pooled average MMERT MMERT-average
A 0.5187 0.5199 0.5213?+ 0.5195 0.5196
B 0.4877 0.4885 0.4908?+ 0.4911?+ 0.4921?+
C 0.5214 0.5175 0.5199?+ 0.5218+ 0.5162?+
D 0.4724 0.4730 0.4733 0.4736 0.4734
E 0.4666 0.4661 0.4679?+ 0.4669+ 0.4685?+
F 0.4794 0.4801 0.4811? 0.4821?+ 0.4830?+
G 0.4596 0.4576 0.4607+ 0.4606+ 0.4610?+
H 0.4573 0.4560 0.4578 0.4581+ 0.4581+
Table 13: Multi-task tuning on IPC sections.
troduces an inductive bias. Evgeniou and Pon-
til (2004) propose a regularization method that
balances task-specific parameter vectors and their
distance to the average. The learning objective is
to minimize task-specific loss functions ld across
all tasks d with weight vectors wd, while keep-
ing each parameter vector close to the average
1
D
?D
d=1wd = wavg. This is enforced by min-
imizing the norm (here the `1-norm) of the dif-
ference of each task-specific weight vector to the
avarage weight vector.
min
w1,...,wD
D?
d=1
ld(wd) + ?
D?
d=1
||wd ? wavg||1 (1)
The MMERT algorithm is given in figure 1.
The algorithm starts with initial weights w(0). At
each iteration step, the average of the parame-
ter vectors from the previous iteration is com-
puted. For each task d ? D, one iteration of stan-
dard MERT is called, continuing from weight vec-
tor w(t?1)d and minimizing translation loss func-
tion ld on the data from task d. The individu-
ally tuned weight vectors returned by MERT are
then moved towards the previously calculated av-
erage by adding or subtracting a penalty term ?
for each weight component w(t)d [k]. If a weight
moves beyond the average, it is clipped to the av-
erage value. The process is iterated until a stop-
ping criterion is met, e.g. a threshold on the max-
imum change in the average weight vector. The
parameter ? controls the influence of the regular-
ization. A larger ? pulls the weights closer to the
average, a smaller ? leaves more freedom to the
individual tasks.
MMERT(w(0), D, {ld}Dd=1):
for t = 1, . . . , T do
w(t)avg = 1D
?D
d=1w
(t?1)
d
for d = 1, . . . , D parallel do
w(t)d = MERT(w
(t?1)
d , ld)
for k = 1, . . . ,K do
if w[k](t)d ? w
(t)
avg[k] > 0 then
w(t)d [k] = max(w
(t)
avg[k], w
(t)
d [k]??)
else if w(t)d [k]? w
(t)
avg[k] < 0 then
w(t)d [k] = min(w
(t)
avg[k], w
(t)
d [k] + ?)
end if
end for
end for
end for
return w(T )1 , . . . , w
(T )
D , w
(T )
avg
Figure 1: Multi-task MERT.
824
The weight updates and the clipping strategy
can be motivated in a framework of gradient de-
scent optimization under `1-regularization (Tsu-
ruoka et al 2009). Assuming MERT as algorith-
mic minimizer11 of the loss function ld in equa-
tion 1, the weight update towards the average
follows from the subgradient of the `1 regular-
izer. Since w(t)avg is taken as average over weights
w(t?1)d from the step before, the term w
(t)
avg is con-
stant with respect to w(t)d , leading to the follow-
ing subgradient (where sgn(x) = 1 if x > 0,
sgn(x) = ?1 if x < 0, and sgn(x) = 0 if x = 0):
?
?w(t)r [k]
?
D?
d=1
?
?
?
?
?
w(t)d ?
1
D
D?
s=1
w(t?1)s
?
?
?
?
?
1
= ? sgn
(
w(t)r [k]?
1
D
D?
s=1
w(t?1)s [k]
)
.
Gradient descent minimization tells us to move in
the opposite direction of the subgradient, thus mo-
tivating the addition or subtraction of the regular-
ization penalty. Clipping is motivated by the de-
sire to avoid oscillating parameter weights and in
order to to enforce parameter sharing.
Experimental results for multi-task MERT
(MMERT) are reported for both dimensions of
patent tasks. For the IPC sections we trained
a pooled model on 1,000,000 sentences sampled
from abstracts and claims from all sections. We
did not balance the sections but kept their orig-
inal distribution, reflecting a real-life task where
the distribution of sections is unknown. We then
extend this experiment to the structural dimen-
sion. Since we do not have an intuitive notion of a
natural distribution for the text sections, we train
a balanced pooled model on a corpus composed
of 170,000 sentences each from abstracts, claims
and titles, i.e. 510,000 sentences in total. For
both dimensions, for each task, we sampled 2,000
parallel sentences for development, development-
testing, and testing from patents that were pub-
lished in different years than the training data.
We compare the multi-task experiments with
two baselines. The first baseline is individual
task learning, corresponding to standard separate
MERT tuning on each section (individual). This
results in three separately learned weight vectors
11MERT as presented in Och (2003) is not a gradient-
based optimization techniquem, thus MMERT is strictly
speaking only ?inspired? by gradient descent optimization.
for each task, where no information has been
shared between the tasks. The second baseline
simulates the setting where the sections are not
differentiated at all. We tune the model on a
pooled development set of 2,000 sentences that
combines the same amount of data from all sec-
tions (pooled). This yields a single joint weight
vector for all tasks optimized to perform well
across all sections. Furthermore, we compare
multi-task MERT tuning with two parameter av-
eraging methods. The first method computes the
arithmetic mean of the weight vectors returned by
the individual baseline for each weight compo-
nent, yielding a joint average vector for all tasks
(average). The second method takes the last av-
erage vector computed during multi-task MERT
tuning (MMERT-average).12
Tables 12 and 13 give the results for multi-task
learning on text and IPC sections. The latter re-
sults have been presented earlier in Simianer et al
(2011). The former table extends the technique
of multi-task MERT to the structural dimension
of patent SMT tasks. In all experiments, the pa-
rameter ? was adjusted to 0.001 after evaluating
different settings on a development set. The best
result on each section is indicated in bold face; *
indicates significance with respect to the individ-
ual baseline, + the same for the pooled baseline.
We observe statistically significant improvements
of 0.5 to 1% BLEU over the individual baseline for
claims and titles; for abstracts, the multi-task vari-
ant yields the same result as the baseline, while
the averaging methods perform worse. Multi-task
MERT yields the best result for claims; on titles,
the simple average and the last MMERT average
dominate. Pooled tuning always performs signifi-
cantly worse than any other method, confirming
that it is beneficial to differentiate between the
text section sections.
Similarly for IPC sections, small but statisti-
cally significant improvements over the individual
and pooled baselines are achieved by multi-task
tuning and averaging over IPC sections, except-
ing C and D. However, an advantage of multi-task
tuning over averaging is hard to establish.
Note that the averaging techniques implicitly
benefit from a larger tuning set. In order to ascer-
tain that the improvements by averaging are not
12The aspect of averaging found in all of our multi-task
learning techniques effectively controls for optimizer insta-
bility as mentioned in Clark et al(2011).
825
test pooled-6k significance
abstract 0.3628 <
claim 0.4696 <
title 0.3174 <
Table 14: Multi-task tuning on 6,000 sentences pooled
from text sections. ?<? denotes a statistically signifi-
cant difference to the best result.
simply due to increasing the size of the tuning set,
we ran a control experiment where we tuned the
model on a pooled development set of 3 ? 2, 000
sentences for text sections and on a development
set of 8 ? 2, 000 sentences for IPC sections. The
results given in table 14 show that tuning on a
pooled set of 6,000 text sections yields only min-
imal differences to tuning on 2,000 sentence pairs
such that the BLEU scores for the new pooled
models are still significantly lower than the best
results in table 12 (indicated by ?<?). However,
increasing the tuning set to 16,000 sentence pairs
for IPC sections makes the pooled baseline per-
form as well as the best results in table 13, except
for two cases (indicated by ?<?) (see table 15).
This is due to the smaller differences between best
and worst results for tuning on IPC sections com-
pared to tuning on text sections, indicating that
IPC sections are less well suited for multi-task
tuning than the textual domains.
test pooled-16k significance
A 0.5177 <
B 0.4920
C 0.5133 <
D 0.4737
E 0.4685
F 0.4832
G 0.4608
H 0.4579
Table 15: Multi-task tuning on 16,000 sentences
pooled from IPC sections. ?<? denotes a statistically
significant difference to the best result.
5 Conclusion
The most straightforward approach to improve
machine translation performance on patents is to
enlarge the training set to include all available
data. This question has been investigated by Tins-
ley et al(2010) and Utiyama and Isahara (2007).
A caveat in this situation is that data need to be
from the general patent domain, as shown by the
inferior performance of a large Europarl-trained
model compared to a small patent-trained model.
The goal of this paper is to analyze patent data
along the topical dimension of IPC classes and
along the structural dimension of textual sections.
Instead of trying to beat a pooling baseline that
simply increases the data size, our research goal
is to investigate whether different subtasks along
these dimensions share commonalities that can
fruitfully be exploited by multi-task learning in
machine translation. We thus aim to investigate
the benefits of multi-task learning in realistic sit-
uations where a simple enlargement of training
data is not possible.
Starting from baseline models that are trained
on individual tasks or on data pooled from all
tasks, we apply mixtures of translation models
and multi-task MERT tuning to multiple patent
translation tasks. We find small, but statistically
significant improvements for multi-task MERT
tuning and parameter averaging techniques. Im-
provements are more pronounced for multi-task
learning on textual domains than on IPC domains.
This might indicate that the IPC sections are less
well delimitated than the structural domains. Fur-
thermore, this is owing to the limited expressive-
ness of a standard linear model including 14-20
features in tuning. The available features are very
coarse and more likely to capture structural dif-
ferences, such as sentence length, than the lexi-
cal differences that differentiate the semantic do-
mains. We expect to see larger gains due to multi-
task learning for discriminatively trained SMT
models that involve very large numbers of fea-
tures, especially when multi-task learning is done
in a framework that combines parameter regular-
ization with feature selection (Obozinski et al
2010). In future work, we will explore a combina-
tion of large-scale discriminative training (Liang
et al 2006) with multi-task learning for SMT.
Acknowledgments
This work was supported in part by DFG grant
?Cross-language Learning-to-Rank for Patent Re-
trieval?.
826
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of the
4th EACL Workshop on Statistical Machine Trans-
lation, Athens, Greece.
Nicola Bertoldi, Barry Haddow, and Jean-Baptiste
Fouet. 2009. Improved minimum error rate train-
ing in Moses. The Prague Bulletin of Mathematical
Linguistics, 91:7?16.
Fabienne Braune and Alexander Fraser. 2010. Im-
proved unsupervised sentence alignment for sym-
metrical and asymmetrical parallel corpora. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (COLING?10), Beijing,
China.
Alexandru Ceaus?u, John Tinsley, Jian Zhang, and
Andy Way. 2011. Experiments on domain adap-
tation for patent machine translation in the PLuTO
project. In Proceedings of the 15th Conference of
the European Assocation for Machine Translation
(EAMT 2011), Leuven, Belgium.
Olivier Chapelle, Pannagadatta Shivaswamy, Srinivas
Vadrevu, Kilian Weinberger, Ya Zhang, and Belle
Tseng. 2011. Boosted multi-task learning. Ma-
chine Learning.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better hypothesis testing for statis-
tical machine translation: Controlling for optimizer
instability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?11), Portland, OR.
Hal Daume?. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?07), Prague, Czech Republic.
Mark Dredze, Alex Kulesza, and Koby Crammer.
2010. Multi-domain learning by confidence-
weighted parameter combination. Machine Learn-
ing, 79:123?149.
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation in
statistical machine translation. In Proceedings of
the International Workshop on Spoken Language
Translation (IWSLT?10), Paris, France.
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi-task learning. In Proceedings of
the 10th ACM SIGKDD conference on knowledge
discovery and data mining (KDD?04), Seattle, WA.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech, Brisbane, Australia.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics - Human Language Technologies (NAACL-
HLT?09), Boulder, CO.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, Prague, Czech Republic.
George Foster, Pierre Isabelle, and Roland Kuhn.
2010. Translating structured documents. In Pro-
ceedings of the 9th Conference of the Association
for Machine Translation in the Americas (AMTA
2010), Denver, CO.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation (WMT?11), Edinburgh, UK.
Daniel Kifer, Shain Ben-David, and Johannes Gehrke.
2004. Detecting change in data streams. In Pro-
ceedings of the 30th international conference on
Very large data bases, Toronta, Ontario, Canada.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, Prague, Czech
Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Birch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL 2007 Demo and Poster Ses-
sions, Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
Machine Translation Summit X, Phuket, Thailand.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end dis-
criminative approach to machine translation. In
Proceedings of the joint conference of the Inter-
national Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING-ACL?06), Sydney, Australia.
Guillaume Obozinski, Ben Taskar, and Michael I. Jor-
dan. 2010. Joint covariate selection and joint sub-
space selection for multiple classification problems.
Statistics and Computing, 20:231?252.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the Human Language Technology Confer-
ence and the 3rd Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL?03), Edmonton, Cananda.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for auto-
matic evaluation of machine translation. Technical
Report IBM Research Division Technical Report,
RC22176 (W0190-022), Yorktown Heights, N.Y.
827
Ariadna Quattoni, Xavier Carreras, Michael Collins,
and Trevor Darrell. 2009. An efficient projec-
tion for `1,? regularization. In Proceedings of the
26th International Conference on Machine Learn-
ing (ICML?09), Montreal, Canada.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT?08), Hawaii.
Patrick Simianer, Katharina Wa?schle, and Stefan Rie-
zler. 2011. Multi-task minimum error rate train-
ing for SMT. The Prague Bulletin of Mathematical
Linguistics, 96:99?108.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?08), Honolulu, Hawaii.
John Tinsley, Andy Way, and Paraic Sheridan. 2010.
PLuTO: MT for online patent translation. In Pro-
ceedings of the 9th Conference of the Association
for Machine Translation in the Americas (AMTA
2010), Denver, CO.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent train-
ing for `1-regularized log-linear models with cumu-
lative penalty. In Proceedings of the 47th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-IJCNLP?09), Singapore.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics (ACL?07), Prague, Czech Republic.
Masao Utiyama and Hitoshi Isahara. 2007. A
Japanese-English patent parallel corpus. In Pro-
ceedings of MT Summit XI, Copenhagen, Denmark.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics (COLING?04), Geneva,
Switzerland.
828
Query Rewriting Using Monolingual
Statistical Machine Translation
Stefan Riezler?
Google
Yi Liu??
Google
Long queries often suffer from low recall in Web search due to conjunctive term matching. The
chances of matching words in relevant documents can be increased by rewriting query terms into
new terms with similar statistical properties. We present a comparison of approaches that deploy
user query logs to learn rewrites of query terms into terms from the document space. We show
that the best results are achieved by adopting the perspective of bridging the ?lexical chasm?
between queries and documents by translating from a source language of user queries into a
target language of Web documents. We train a state-of-the-art statistical machine translation
model on query-snippet pairs from user query logs, and extract expansion terms from the query
rewrites produced by the monolingual translation system. We show in an extrinsic evaluation in
a real-world Web search task that the combination of a query-to-snippet translation model with
a query language model achieves improved contextual query expansion compared to a state-of-
the-art query expansion model that is trained on the same query log data.
1. Introduction
Information Retrieval (IR) applications have been notoriously resistant to improvement
attempts by Natural Language Processing (NLP). With a few exceptions for specialized
tasks,1 the contribution of part-of-speech taggers, syntactic parsers, or ontologies of
nouns or verbs has been inconclusive. In this article, instead of deploying NLP tools
or ontologies, we apply NLP ideas to IR problems. In particular, we take a viewpoint
that looks at the problem of the word mismatch between queries and documents in
Web search as a problem of translating from a source language of user queries into a
target language of Web documents. We concentrate on the task of query expansion by
query rewriting. This task consists of adding expansion terms with similar statistical
properties to the original query in order to increase the chances of matching words in
relevant documents, and also to decrease the ambiguity of the query that is inherent
in natural language. We focus on a comparison of models that learn to generate query
? Brandschenkestrasse 110, 8002 Zu?rich, Switzerland. E-mail: riezler@gmail.com.
?? 1600 Amphitheatre Parkway, Mountain View, CA. E-mail: yliu@google.com.
1 See for example Sable, McKeown, and Church (2002), who report improvements in text categorization by
using tagging and parsing for the task of categorizing captioned images.
Submission received: 19 June 2009; revised submission received: 4 March 2010; accepted for publication:
12 May 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
rewrites from large amounts of user query logs, and use query expansion in Web search
for an extrinsic evaluation of the produced rewrites. The experimental query expansion
setup used in this article is simple and direct: For a given set of randomly selected
queries, n-best rewrites are produced. From the changes introduced by the rewrites,
expansion terms are extracted and added as alternative terms to the query, leaving the
ranking function untouched.
Figure 1 shows expansions of the queries herbs for chronic constipation and herbs for
mexican cooking using AND and OR operators. Conjunctive matching of all query terms
is the default, and indicated by the AND operator. Expansion terms are added using
the OR operator. The example in Figure 1 illustrates the key requirements to successful
query expansion, namely, to find appropriate expansions in the context of the query.
While remedies, medicine, or supplement are appropriate expansions in the context of the
first query, they would cause a severe query drift if used in the second query. In the
context of the second query, spices is an appropriate expansion for herbs, whereas this
expansion would again not work for the first query.
The central idea behind our approach is to combine the orthogonal information
sources of the translation model and the language model to expand query terms in
context. The translation model proposes expansion candidates, and the query language
model performs a selection in the context of the surrounding query terms. Thus, in
combination, the incessant problems of term ambiguity and query drift can be solved.
One of the goals of this article is to show that existing SMT technology is readily
applicable to this task. We apply SMT to large parallel data of queries on the source
side, and snippets of clicked search results on the target side. Snippets are short text
fragments that represent the parts of the result pages that are most relevant to the
queries, for example, in terms of query term matches. Although the use of snippets
instead of the full documents makes our approach efficient, it introduces noise because
text fragments are used instead of full sentences. However, we show that state-of-the-
art statistical machine translation (SMT) technology is in fact robust and flexible enough
to capture the peculiarities of the language pair of user queries and result snippets.
We evaluate our system in a comparative, extrinsic evaluation in a real-world Web
search task. We compare our approach to the expansion system of Cui et al (2002)
that is trained on the same user logs data and has been shown to produce significant
improvements over the local feedback technique of Xu and Croft (1996) in a standard
evaluation on TREC data. Our extrinsic evaluation is done by embedding the expansion
systems into a real-world search engine, and comparing the two systems based on
the search results that are triggered by the respective query expansions. Our results
show that the combination of translation and language model of a state-of-the-art
SMT model produces high-quality rewrites and outperforms the expansion model of
Cui et al (2002).
In the following, we will discuss related work (Section 2) and quickly sketch
Cui et al (2002)?s approach (Section 3). Then we will recapitulate the essentials of
Figure 1
Search queries herbs for chronic constipation and herbs for mexican cooking integrating expansion
terms into OR-nodes in conjunctive matching.
570
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
state-of-the-art SMT and describe how to adapt an SMT system to the query expansion
task (Section 4). Results of the extrinsic experimental evaluation are presented in Sec-
tion 5. The presented results are based on earlier results presented in Riezler, Liu, and
Vasserman (2008), and extended by deeper analyses and further experiments.
2. Related Work
Standard query expansion techniques such as local feedback, or pseudo-relevance feed-
back, extract expansion terms from the topmost documents retrieved in an initial re-
trieval round (Xu and Croft 1996). The local feedback approach is costly and can lead to
query drift caused by irrelevant results in the initial retrieval round. Most importantly,
though, local feedback models do not learn from data, in contrast to the approaches
described in this article.
Recent research in the IR community has increasingly focused on deploying user
query logs for query reformulations (Huang, Chien, and Oyang 2003; Fonseca et al
2005; Jones et al 2006), query clustering (Beeferman and Berger 2000; Wen, Nie, and
Zhang 2002; Baeza-Yates and Tiberi 2007), or query similarity (Raghavan and Sever
1995; Fitzpatrick and Dent 1997; Sahami and Heilman 2006). The advantage of these ap-
proaches is that user feedback is readily available in user query logs and can efficiently
be precomputed. Similarly to this recent work, our approach uses data from user query
logs, but as input to a monolingual SMT model for learning query rewrites.
The SMT viewpoint has been introduced to the field of IR by Berger and Lafferty
(1999) and Berger et al (2000), who proposed to bridge the ?lexical chasm? by a retrieval
model based on IBM Model 1 (Brown et al 1993). Since then, ranking models based
on monolingual SMT have seen various applications, especially in areas like Question
Answering where a large lexical gap between questions and answers has to be bridged
(Berger et al 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al 2007;
Surdeanu, Ciaramita, and Zaragoza 2008; Xue, Jeon, and Croft 2008). Whereas most
applications of SMT ideas to IR problems used translation system scores for (re)ranking
purposes, only a few approaches use SMT to generate actual query rewrites (Riezler,
Liu, and Vasserman 2008). Similarly to Riezler, Liu, and Vasserman (2008), we use SMT
to produce actual rewrites rather than for (re)ranking, and evaluate the rewrites in a
query expansion task that leaves the ranking model of the search engine untouched.
Lastly, monolingual SMT has been established in the NLP community as a useful
expedient for paraphrasing, that is, the task of reformulating phrases or sentences into
semantically similar strings (Quirk, Brockett, and Dolan 2004; Bannard and Callison-
Burch 2005). Although the use of the SMT in paraphrasing goes beyond pure ranking to
actual rewriting, SMT-based paraphrasing has to our knowledge not yet been applied
to IR tasks.
3. Query Expansion by Query?Document Term Correlations
The query expansion model of Cui et al (2002) is based on the principle that if queries
containing one term often lead to the selection of documents containing another term,
then a strong relationship between the two terms can be assumed. Query terms and
document terms are linked via sessions in which users click on documents in the
retrieval result for the query. Cui et al define a session as follows:
session := <query text>[clicked document]*
571
Computational Linguistics Volume 36, Number 3
According to this definition, a link is established if at least one user clicks on a document
in the retrieval results for a query. Because query logs contain sessions from different
users, an aggregation of clicks over sessions will reflect the preferences of multiple users.
Cui et al (2002) compute the following probability distribution of document words wd
given query words wq from counts over clicked documents D aggregated over sessions:
P(wd|wq) =
?
D
P(wd|D)P(D|wq) (1)
The first term in Equation (1) is a normalized tfidf weight of the document term in
the clicked document, and the second term is the relative cooccurrence of the clicked
document and query term.
Because Equation (1) calculates expansion probabilities for each term separately,
Cui et al (2002) introduce the following cohesion formula that respects the whole query
Q by aggregating the expansion probabilities for each query term:
CoWeightQ(w
d) = ln(
?
wq?Q
P(wd|wq) + 1) (2)
In contrast to local feedback techniques (Xu and Croft 1996), Cui et al (2002)?s
algorithm allows us to precompute term correlations off-line by collecting counts from
query logs. This reliance on pure frequency counting is both a blessing and a curse:
On the one hand it allows for efficient non-iterative estimation, but on the other hand
it makes the implicit assumption that data sparsity will be overcome by counting
from huge data sets. The only attempt at smoothing that is made in this approach is
shifting the burden to words in the query context, using Equation (2), when Equation (1)
assigns zero probability to unseen pairs. Nonetheless, Cui et al (2002) show significant
improvements over the local feedback technique of Xu and Croft (1996) in an evaluation
on TREC data.
4. Query Expansion Using Monolingual SMT
4.1 Linear Models for SMT
The job of a translation system is defined in Och and Ney (2004) as finding the English
string e? that is a translation of a foreign string f using a linear combination of feature
functions hm(e, f) and weights ?m as follows:
e? = arg max
e
M
?
m=1
?mhm(e, f)
As is now standard in SMT, several complex features such as lexical translation models
and phrase translation models, trained in source-target and target-source directions, are
combined with language models and simple features such as phrase and word counts.
In the linear model formulation, SMT can be thought of as a general tool for computing
string similarities or for string rewriting.
572
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
4.2 Word Alignment
The relationship of translation model and alignment model for source language string
f = f J1 and target string e = e
I
1 is via a hidden variable describing an alignment mapping
from source position j to target position aj:
P( f J1|eI1) =
?
aJ1
P( f J1, a
J
1|eI1)
The alignment aJ1 contains so-called null-word alignments aj = 0 that align source words
to the empty word.
In our approach, ?sentence aligned? parallel training data are prepared by pairing
user queries with snippets of search results clicked for the respective queries. The
translation models used are based on a sequence of word alignment models, whereas in
our case three Model-1 iterations and three HMM iterations were performed. Another
important adjustment in our approach is the setting of the null-word alignment proba-
bility to 0.9 in order to account for the difference in sentence length between queries and
snippets. This setting improves alignment precision by filtering out noisy alignments
and instead concentrating on alignments with high support in the training data.
4.3 Phrase Extraction
Statistical estimation of alignment models is done by maximum-likelihood estimation
of sentence-aligned strings {(fs, es) : s = 1, . . . , S}. Because each sentence pair is linked
by a hidden alignment variable a = aJ1, the optimal ?? is found using unlabeled-data log-
likelihood estimation techniques such as the EM algorithm:
?? = arg max
?
S
?
s=1
?
a
p?(fs, a|es)
The (Viterbi-)alignment a?J1 that has the highest probability under a model is defined as
follows:
a?J1 = arg max
aJ1
p??( f
J
1, a
J
1|eI1)
Because a source?target algnment does not allow a source word to be aligned with two
or more target words, source?target and target?source alignments can be combined via
various heuristics to improve both recall and precision of alignments.
In our application, it is crucial to remove noise in the alignments of queries to
snippets. In order to achieve this, we symmetrize Viterbi alignments for source?target
and target?source directions by intersection only. That is, given two Viterbi alignments
A1 = {(aj, j)| aj > 0} and A2 = {(i, bi)| bi > 0}, the alignments in the intersection are de-
fined as A = A1 ? A2. Phrases are extracted as larger blocks of aligned words from the
alignments in the intersection, as described in Och and Ney (2004).
573
Computational Linguistics Volume 36, Number 3
4.4 Language Modeling
Language modeling in our approach deploys an n-gram language model that assigns
the following probability to a string wL1 of words:
P(wL1 ) =
L
?
i=1
P(wi|wi?11 )
?
L
?
i=1
P(wi|wi?1i?n+1)
Estimation of n-gram probabilities is done by counting relative frequencies of n-grams
in a corpus of user queries. Remedies for sparse data problems are achieved by various
smoothing techniques, as described in Brants et al (2007).
The most important departure of our approach from standard SMT is the use of a
language model trained on queries. Although this approach may seem counterintuitive
from the standpoint of the noisy-channel model for SMT (Brown et al 1993), it fits
perfectly into the linear model. Whereas in the first view a query language model
would be interpreted as a language model on the source language, in the linear model
directionality of translation is not essential. Furthermore, the ultimate task of a query
language model in our approach is to select appropriate phrase translations in the
context of the original query for query expansion. This is achieved perfectly by an
SMT model that assigns the identity translation as most probable translation to each
phrase. Descending the n-best list of translations, in effect the language model picks
alternative non-identity translations for a phrase in context of identity-translations of
the other phrases.
Another advantage of using identity translations and word reordering in our ap-
proach is the fact that, by preferring identity translations or word reorderings over
non-identity translations of source phrases, the SMT model can effectively abstain from
generating any expansion terms. This will happen if none of the candidate phrase
translations fits with high enough probability in the context of the whole query, as
assessed by the language model.
5. Evaluating Query Expansion in a Web Search Task
5.1 Data
The training data for the translation model and the correlation-based model consist of
pairs of queries and snippets for clicked results taken from query logs. Representing
documents by snippets makes it possible to create a parallel corpus that contains data of
roughly the same ?sentence? length. Furthermore, this makes iterative training feasible.
Queries and snippets are linked via clicks on result pages, where a parallel sentence pair
is introduced for each query and each snippet of its clicked results. This yields a data set
of 3 billion query?snippet pairs from which a phrase-table of 700 million query?snippet
phrase translations is extracted. A collection of data statistics for the training data is
shown in Table 1. The language model used in our experiment is a trigram language
model trained on English queries in user logs. n-grams were cut off at a minimum
frequency of 4. Data statistics for resulting unique n-grams are shown in Table 2.
574
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
Table 1
Statistics of query?snippet training data.
query?snippet query snippet
pairs words words
tokens 3 billion 8 billion 25 billion
avg. length - 2.6 8.3
Table 2
Statistics of unique n-grams in language model.
1-grams 2-grams 3-grams
9 million 1.5 billion 5 billion
5.2 Query Expansion Setup
The setup for our extrinsic evaluation deploys a real-world search engine, google.com,
for a comparison of expansions from the SMT-based system, the correlation-based sys-
tem, and the correlation-based system using the language model as additional filter. All
expansion systems are trained on the same set of parallel training data. SMT modules
such as the language model and the translation models in source?target and target?
source directions are combined in a uniform manner in order to give the SMT and
correlation-based models the same initial conditions.
The expansion terms used in our experiments were extracted as follows: Firstly,
a set of 150,000 randomly extracted 3+ word queries was rewritten by each of the
systems. For each system, expansion terms were extracted from the 5-best rewrites, and
stored in a table that maps source phrases to target phrases in the context of the full
queries. For example, Table 3 shows unique 5-best translations of the SMT system for the
queries herbs for chronic constipation and herbs for mexican cooking. Phrases that are newly
introduced in the translations are highlighted in boldface. These phrases are extracted
for expansion and stored in a table that maps source phrases to target phrases in the
context of the query from which they were extracted. When applying the expansion
Table 3
Unique 5-best phrase-level translations of queries herbs for chronic constipation and herbs for
mexican cooking. Terms extracted for expansion are highlighted in boldface.
(herbs , herbs) (for , for) (chronic , chronic) (constipation , constipation)
(herbs , herb) (for , for) (chronic , chronic) (constipation , constipation)
(herbs , remedies) (for , for) (chronic , chronic) (constipation , constipation)
(herbs , medicine) (for , for) (chronic , chronic) (constipation , constipation)
(herbs , supplements) (for , for) (chronic , chronic) (constipation , constipation)
(herbs , herbs) (for , for) (mexican , mexican) (cooking , cooking)
(herbs , herbs) (for , for) (cooking , cooking) (mexican , mexican)
(herbs , herbs) (for , for) (mexican , mexican) (cooking , food)
(mexican , mexican) (herbs , herbs) (for , for) (cooking , cooking)
(herbs , spices) (for , for) (mexican , mexican) (cooking , cooking)
575
Computational Linguistics Volume 36, Number 3
table to the same 150,000 queries that were input to the translation, expansion phrases
are included in the search query via an OR-operation. An example search query that
uses the SMT-based expansions from Table 3 is shown in Figure 1.
In order to evaluate Cui et al (2002)?s correlation-based system in this setup, we
required the system to assign expansion terms to particular query terms. The best results
were achieved by using a linear interpolation of scores in Equation (2) and Equation (1).
Equation (1) thus introduces a preference for a particular query term to the whole-
query score calculated by Equation (2). Our reimplementation uses unigram and bigram
phrases in queries and expansions. Furthermore, we use Okapi BM25 instead of tfidf in
the calculation of Equation (1) (see Robertson, Walker, and Hancock-Beaulieu 1998).
In addition to SMT and correlation-based expansion, we evaluate a system that uses
the query language model to rescore the rewrites produced by the correlation-based
model. The intended effect is to filter correlation-based expansions by a more effective
context model than the cohesion model proposed by Cui et al (2002).
Because expansions from all experimental systems are done on top of the same
underlying search engine, we can abstract away from interactions with the underlying
system. Rewrite scores or translation probabilities were only used to create n-best lists
for the respective systems; the ranking function of the underlying search engine was left
untouched.
5.3 Experimental Evaluation
The evaluation was performed by three independent raters. The raters were presented
with queries and 10-best search results from two systems, anonymized, and presented
randomly on left or right sides. The raters? task was to evaluate the results on a 7-point
Likert scale, defined as:
?1.5: much worse
?1.0: worse
?0.5: slightly worse
0: about the same
0.5: slightly better
1.0: better
1.5: much better
Table 4 shows evaluation results for all pairings of the three expansion systems.
For each pairwise comparison, a set of 200 queries that has non-empty, different re-
sult lists for both systems is randomly selected from the basic set of 150,000 queries.
The mean item score (averaged over queries and raters) for the experiment that com-
pares the correlation-based model with language model filtering (corr+lm) against
the correlation-based model (corr) shows a clear win for the experimental system.
Table 4
Comparison of query expansion systems on the Web search task with respect to a 7-point
Likert scale.
experiment corr+lm SMT SMT
baseline corr corr corr+lm
mean item score 0.264 ? 0.095 0.254 ? 0.09125 0.093 ? 0.0850
576
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
An experiment that compares SMT-based expansion (SMT) against correlation-based
expansions (corr) results in a clear preference for the SMT model. An experiment that
compares the SMT-based expansions (SMT) against the correlation-based expansions
filtered by the language model (corr+lm) shows a smaller, but still statistically signifi-
cant, preference for the SMT model. Statistical significance of result differences has been
computed with a paired t-test (Cohen 1995), yielding statistical significance at the 95%
level for the first two columns in Table 4, and statistical significance at the 90% level for
the last column in Table 4.
Examples for SMT-based and correlation-based expansions are given in Table 5.
The first five examples show the five biggest wins in terms of mean item score for the
SMT system over the correlation-based system. The second set of examples shows the
five biggest losses of the SMT system compared to the correlation-based system. On
inspection of the first set, we see that SMT-based expansions such as henry viii restaurant
portland, maine, or ladybug birthday ideas, or top ten restaurants, vancouver, achieve a
change in retrieval results that does not result in a query drift, but rather in improved
retrieval results. The first and fifth result are wins for the SMT system because of non-
sensical expansions by the baseline correlation-based system. A closer inspection of the
second set of examples shows that the SMT-based expansion terms are all clearly related
to the source terms, but not synonymous. In the first example, shutdown is replaced
by reboot or restart which causes a demotion of the top result that matches the query
exactly. In the second example, passport is replaced by the related term visa in the SMT-
based expansion. The third example is a loss for SMT-based expansion because of a
replacement of the specific term debian by the more general term linux. The correlation-
based expansions how many tv 30 rock in the fourth example, and lampasas county sheriff
Table 5
5-best and 5-worst expansions from SMT system and corr system with mean item score.
query SMT expansions corr expansions score
broyhill conference - broyhill - welcome; 1.5
center boone boone - welcome
Henry VIII Menu menu - restaurant, portland - six; 1.3
Portland, Maine restaurants menu - england
ladybug birthday parties - ideas, ladybug - kids 1.3
parties party
top ten dining, dining - restaurants dining - 10 1.3
vancouver
international communication - international 1.3
communication in communications, skills communication -
veterinary medicine college
SCRIPT TO SHUTDOWN SHUTDOWN - - ?1.0
NT 4.0 shutdown, reboot, restart
applying U.S. passport passport - visa applying - home ?1.0
configure debian debian - linux; configure - ?1.0
to use dhcp configure - install configuring
how many episodes episodes - season, episodes - tv; ?0.83
of 30 rock? series many episodes -
wikipedia
lampasas county department - office department - home ?0.83
sheriff department
577
Computational Linguistics Volume 36, Number 3
home in the fifth example directly hit the title of relevant Web pages, while the SMT-
based expansion terms do not improve retrieval results. However, even from these
negative examples it becomes apparent that the SMT-based expansion terms are clearly
related to the query terms, and for a majority of cases this has a positive effect. In
contrast, the terms introduced by the correlation-based system are either only vaguely
related or noise.
Similar results are shown in Table 6 where the five best and five worst examples
for the comparison of the SMT model with the corr+lm model are listed. The wins for
the SMT system are achieved by synonymous or closely related terms (make - build,
create; layouts - backgrounds; contractor - contractors) or terms that properly disambiguate
ambiguous query terms: For example, the term vet in the query dr. tim hammond, vet
is expanded by the appropriate term veterinarian in the SMT-based expansion, whereas
the correlation-based expansion to vets does not match the query context. The losses
of the SMT-based system are due to terms that are only marginally related. Furthermore,
the expansions of the correlation-based model are greatly improved by language model
filtering. This can be seen more clearly in Table 7, which shows the five best and worst
results from the comparison of correlation-based models with and without language
model filtering. Here the wins by the filtered model are due to filtering non-sensical
expansions or too general expansions by the unfiltered correlation-based model rather
than promoting new useful expansions.
We attribute the experimental result of a significant preference for SMT-based ex-
pansions over correlation-based expansions to the fruitful combination of translation
model and language model provided by the SMT system. The SMT approach can be
viewed as a combined system that proposes already reasonable candidate expansions
via the translation model, and filters them by the language model. We may find a
certain amount of non-sensical expansion candidates at the phrase translation level of
the SMT system. However, a comparison with unfiltered correlation-based expansions
shows that the candidate pool of phrase translations of the SMT model is of higher
quality, yielding overall better results after language model filtering. This can be seen
Table 6
5-best and 5-worst expansions from SMT system and corr+lm system with mean item score.
query SMT expansions corr+lm expansions score
how to make bombs make - build, create make - book 1.5
dominion power va - dominion - virginia 1.3
purple myspace layouts - backgrounds purple - free 1.167
layouts myspace - free
dr. tim hammond, vet vet - veterinarian, vet - vets 1.167
veterinary, hospital
tci general contractor contractor - contractors - 1.167
health effects of tea - coffee - ?1.5
drinking too much tea
tomahawk wis - wis - wisconsin ?1.0
bike rally
apprentice tv show - tv - com ?1.0
super nes roms roms - emulator nes - nintendo ?1.0
family guy family - genealogy clips - video ?1.0
clips hitler
578
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
Table 7
5-best and 5-worst expansions from corr system and corr+lm system with mean item score.
query corr+lm expansions corr expansions score
outer cape - cape - home; 1.5
health services health - home;
services - home
Henry VII Menu - menu - england; 1.5
Portland, Maine portland - six
easing to relieve gallbladder - gallstone gallbladder - disease, 1.333
gallbladder pain gallstones, gallstone
guardian angel picture - picture - lyrics 1.333
view full episodes episodes - watch naruto - tv 1.333
of naruto
iditarod 2007 schedule iditarod 2007 - race - ?1.5
40 inches plus inches plus - review inches - calculator ?1.333
Lovell sisters review lovell sisters - website - ?1.333
smartparts ion Review smartparts ion - reviews review - pbreview ?1.167
canon eos rebel epinion - com - ?1.167
xt slr + epinion
from inspecting Table 8 which shows the most probable phrase translations that are
applicable to the queries herbs for chronic constipation and herbs for mexican cooking. The
phrase tables include identity translations and closely related terms as most probable
translations for nearly every phrase. However, they also clearly include noisy and non-
related terms. Thus an extraction of expansion terms from the phrase table alone would
not allow the choice of the appropriate term for the given query context. This can be
attained by combining the phrase translations with a language model: As shown in
Table 3, the 5-best translations of the full queries attain a proper disambiguation of the
senses of herbs by replacing the term with remedies, medicine, and supplements for the first
Table 8
Phrase translations for source strings herbs for chronic constipation and herbs for mexican cooking.
herbs herbs, herbal, medicinal, spices, supplements, remedies
herbs for herbs for, herbs, herbs and, with herbs
herbs for chronic herbs for chronic, and herbs for chronic, herbs for
for chronic for chronic, chronic, of chronic
for chronic constipation for chronic constipation, chronic constipation, for constipation
chronic chronic, acute, patients, treatment
chronic constipation chronic constipation, of chronic constipation,
with chronic constipation
constipation constipation, bowel, common, symptoms
for mexican for mexican, mexican, the mexican, of mexican
for mexican cooking mexican food, mexican food and, mexican glossary
mexican mexican, mexico, the mexican
mexican cooking mexican cooking, mexican food, mexican, cooking
cooking cooking, culinary, recipes, cook, food, recipe
579
Computational Linguistics Volume 36, Number 3
Table 9
Correlation-based expansions for queries herbs for chronic constipation and herbs for mexican
cooking.
query terms n-best expansions
herbs com treatment encyclopedia
chronic interpret treating com
constipation interpret treating com
herbs for medicinal support women
for chronic com gold encyclopedia
chronic constipation interpret treating
herbs cooks recipes com
mexican recipes com cooks
cooking cooks recipes com
herbs for medicinal women support
for mexican cooks com allrecipes
query, and with spices for the second query. Table 9 shows the top three correlation-
based expansion terms assigned to unigrams and bigrams in the queries herbs for
chronic constipation and herbs for mexican cooking. Expansion terms are chosen by overall
highest weight and shown in boldface. Relevant expansion terms such as treatment
or recipes that would disambiguate the meaning of herbs are in fact in the candidate
list; however, the cohesion score promotes general terms such as interpret or com as
best whole-query expansions. Although language model filtering greatly improves the
quality of correlation-based expansions, overall the combination of phrase translations
and language model produces better results than the combination of correlation-based
expansions and language model. This is confirmed by the pairwise comparison of the
SMT and corr+lm systems shown in Table 4.
6. Conclusion
We presented a view of the term mismatch problem between queries and Web doc-
uments as a problem of translating from a source language of user queries to a tar-
get language of Web documents. We showed that a state-of-the-art SMT model can
be applied to parallel data of user queries and snippets for clicked Web documents,
and showed improvements over state-of-the-art probabilistic query expansion. Our
experimental evaluation showed firstly that state-of-the-art SMT is robust and flexible
enough to capture the peculiarities of query?snippet translation, thus questioning the
need for special-purpose models to control noisy translations as suggested by Lee et al
(2008). Furthermore, we showed that the combination of translation model and lan-
guage model significantly outperforms the combination of correlation-based model and
language model. We chose to take advantage of the access the google.com search engine
to evaluate the query rewrite systems by query expansion embedded in a real-word
search task. Although this conforms with recent appeals for more extrinsic evaluations
(Belz 2009), it decreases the reproducability of the evaluation experiment.
In future work, we hope to apply SMT-based rewriting to other rewriting tasks such
as query suggestions. Also, we hope that our successful application of SMT to query
580
Riezler and Liu Query Rewriting Using Monolingual Statistical Machine Translation
expansion might serve as an example and perhaps open the doors for new applications
and extrinsic evaluations of related NLP approaches such as paraphrasing.
References
Baeza-Yates, Ricardo and Alessandro Tiberi.
2007. Extracting semantic relations from
query logs. In Proceedings of the 13th
ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD?07),
San Jose, CA, pages 76?85.
Bannard, Colin and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel
corpora. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), Ann Arbor, MI,
pages 597?604.
Beeferman, Doug and Adam Berger. 2000.
Agglomerative clustering of a search
engine query log. In Proceedings of the 6th
ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining
(KDD?00), Boston, MA, pages 407-416.
Belz, Anja. 2009. That?s nice ... what can you
do with it? Computational Linguistics,
35(1):111?118.
Berger, Adam and John Lafferty. 1999.
Information retrieval as statistical
translation. In Proceedings of the 22nd
ACM SIGIR Conference on Research and
Development in Information Retrieval
(SIGIR?99), Berkeley, CA, pages 222-229.
Berger, Adam L., Rich Caruana, David Cohn,
Dayne Freitag, and Vibhu Mittal. 2000.
Bridging the lexical chasm: Statistical
approaches to answer-finding. In
Proceedings of the 23rd ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR?00),
Athens, Greece, 192?199.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007. Large
language models in machine translation.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP?07), Prague Czech Republic,
pages 858?867.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. The MIT Press,
Cambridge, MA.
Cui, Hang, Ji-Rong Wen, Jian-Yun Nie,
and Wei-Ying Ma. 2002. Probabilistic
query expansion using query logs.
In Proceedings of the 11th International
World Wide Web conference (WWW?02),
Honolulu, HI, pages 325?332.
Echihabi, Abdessamad and Daniel Marcu.
2003. A noisy-channel approach to
question answering. In Proceedings of the
41st Annual Meeting of the Association
for Computational Linguistics (ACL?03),
Sapporo, Japan, pages 16?23.
Fitzpatrick, Larry and Mei Dent. 1997.
Automatic feedback using past queries:
Social searching? In Proceedings of the 20th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR?97), Philadelphia, PA,
pages 306?313.
Fonseca, Bruno M., Paulo Golgher, Bruno
Possas, Berthier Ribeiro-Neto, and Nivio
Ziviani. 2005. Concept-based interactive
query expansion. In Proceedings of the 14th
Conference on Information and Knowledge
Management (CIKM?05), Bremen, Germany,
pages 696?703.
Huang, Chien-Kang, Lee-Feng Chien, and
Yen-Jen Oyang. 2003. Relevant term
suggestion in interactive Web search based
on contextual information in query session
logs. Journal of the American Society for
Information Science and Technology,
54(7):638?649.
Jones, Rosie, Benjamin Rey, Omid Madani,
and Wiley Greiner. 2006. Generating query
substitutions. In Proceedings of the 15th
International World Wide Web conference
(WWW?06), Edinburgh, Scotland,
pages 387?396.
Lee, Jung-Tae, Sang-Bum Kim, Young-In
Song, and Hae-Chang Rim. 2008. Bridging
lexical gaps between queries and questions
on large online QA collections with
compact translation models. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing (EMNLP?08),
Honolulu, HI, pages 410?418.
Och, Franz Josef and Hermann Ney. 2004.
The alignment template approach
to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics
(ACL?04), Barcelona, Spain, pages 142?149.
Raghavan, Vijay V. and Hayri Sever. 1995.
On the reuse of past optimal queries. In
Proceedings of the 18th Annual International
581
Computational Linguistics Volume 36, Number 3
ACM SIGIR Conference on Research and
Development in Information Retrieval
(SIGIR?95), Seattle, WA, pages 344?350.
Riezler, Stefan, Yi Liu, and Alexander
Vasserman. 2008. Translating queries into
snippets for improved query expansion.
In Proceedings of the 22nd International
Conference on Computational Linguistics
(COLING?08), Manchester, England,
pages 737?744.
Riezler, Stefan, Alexander Vasserman,
Ioannis Tsochantaridis, Vibhu Mittal, and
Yi Liu. 2007. Statistical machine translation
for query expansion in answer retrieval. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics
(ACL?07), Prague Czech Republic, Vol. 1,
pages 464?471.
Robertson, Stephen E., Steve Walker, and
Micheline Hancock-Beaulieu. 1998.
Okapi at TREC-7. In Proceedings of the
Seventh Text REtrieval Conference
(TREC-7), Gaithersburg, MD,
pages 253?264.
Sable, Carl, Kathleen McKeown, and
Kenneth W. Church. 2002. NLP found
helpful (at least for one text categorization
task). In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language
Processing (EMNLP?02), Philadelphia, PA,
pages 172?179.
Sahami, Mehran and Timothy D. Heilman.
2006. A Web-based kernel function for
measuring the similarity of short text
snippets. In Proceedings of the 15th
International World Wide Web conference
(WWW?06), Edinburgh, Scotland,
pages 377-386.
Soricut, Radu and Eric Brill. 2006. Automatic
question answering using the Web:
Beyond the factoid. Journal of Information
Retrieval - Special Issue on Web Information
Retrieval, 9:191?206.
Surdeanu, M., M. Ciaramita, and
H. Zaragoza. 2008. Learning to rank
answers on large online QA collections. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics
(ACL?08), Columbus, OH, pages 719?727.
Wen, Ji-Rong, Jian-Yun Nie, and Hong-Jiang
Zhang. 2002. Query clustering using user
logs. ACM Transactions on Information
Systems, 20(1):59?81.
Xu, Jinxi and W. Bruce Croft. 1996.
Query expansion using local and
global document analysis. In Proceedings
of the 30th Annual International ACM
SIGIR Conference on Research and
Development in Information Retrieval
(SIGIR?07), Zurich, Switzerland,
pages 4?11.
Xue, Xiaobing, Jiwoon Jeon, and Bruce Croft.
2008. Retrieval models for question and
answer archives. In Proceedings of the 31st
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR?08), Singapore,
pages 475?482.
582
On the Problem of Theoretical Terms
in Empirical Computational Linguistics
Stefan Riezler?
Computational Linguistics
Heidelberg University, Germany
Philosophy of science has pointed out a problem of theoretical terms in empirical sciences. This
problem arises if all known measuring procedures for a quantity of a theory presuppose the
validity of this very theory, because then statements containing theoretical terms are circular.
We argue that a similar circularity can happen in empirical computational linguistics, especially
in cases where data are manually annotated by experts. We define a criterion of T-non-theoretical
grounding as guidance to avoid such circularities, and exemplify how this criterion can be met
by crowdsourcing, by task-related data annotation, or by data in the wild. We argue that this
criterion should be considered as a necessary condition for an empirical science, in addition to
measures for reliability of data annotation.
1. Introduction
The recent history of computational linguistics (CL) shows a trend towards encoding
natural language processing (NLP) problems as machine learning tasks, with the goal
of applying task-specific learning machines to solve the encoded NLP problems. In the
following we will refer to such approaches as empirical CL approaches.
Machine learning tools and statistical learning theory play an important enabling
and guiding role for research in empirical CL. A recent discussion in the machine learn-
ing community claims an even stronger and more general role of machine learning. We
allude here to a discussion concerning the relation of machine learning and philosophy
of science. For example, Corfield, Scho?lkopf, and Vapnik (2009) compare Popper?s ideas
of falsifiability of a scientific theory with ?similar notions? from statistical learning the-
ory regarding Vapnik-Chervonenkis theory. A recent NIPS workshop on ?Philosophy
and Machine Learning?1 presented a collection of papers investigating similar problems
and concepts in the two fields. Korb (2004) sums up the essence of the discussion by
directly advertising ?Machine Learning as Philosophy of Science.?
In this article we argue that adopting machine learning theory as philosophy of
science for empirical CL has to be done with great care. A problem arises in the applica-
tion of machine learning methods to natural language data under the assumption that
input?output pairs are given and do not have to be questioned. In contrast to machine
learning, in empirical CL neither a representation of instances nor an association of
? Department of Computational Linguistics, Heidelberg University, Im Neuenheimer Feld 325, 69120
Heidelberg, Germany. E-mail: riezler@cl.uni-heidelberg.de.
1 http://www.dsi.unive.it/PhiMaLe2011/.
doi:10.1162/COLI a 00182
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
instances and labels is always ?given.? We show that especially in cases where data
are manually annotated by expert coders, a problem of circularity arises if one and the
same theory of measurement is used in data annotation and in feature construction. In
this article, we use insights from philosophy of science to understand this problem. We
particularly point to the ?problem of theoretical terms,? introduced by Sneed (1971),
that shows how circularities can make empirical statements in sciences such as physics
impossible.
In the following, we will explain the problem of theoretical terms with the help
of a miniature physical theory used in philosophy of science (Section 2). We will then
exemplify this concept on examples from empirical CL (Section 3). We also make an
attempt at proposing solutions to this problem by using crowdsourcing techniques,
task-related annotation, or data in the wild (Section 4).
2. The Problem of Theoretical Terms in Philosophy of Science
In order to characterize the logical structure of empirical science, philosophy of science
has extensively discussed the notions of ?theoretical? and ?observational? language.
Sneed (1971)2 was the first to suggest a distinction between ?theoretical? and ?non-
theoretical? terms of a given theory by means of the roles they play in that theory.
Balzer (1996, page 140) gives a general definition that states that a term is ?theoretical
in theory T iff every determination of (a realization of) that term presupposes that T
has been successfully applied beforehand.? Because there are no theory-independent
terms in this view, an explicit reference to a theory T is always carried along when
characterizing terms as theoretical with respect to T (T-theoretical) or non-theoretical
with respect to T (T-non-theoretical). Stegmu?ller (1979) makes the notions of ?determina-
tion? or ?realization? more concrete by referring to procedures for measuring values of
quantities or functions in empirical science:
What does it mean to say that a quantity (function) f of a physical theory T is
T-theoretical?... In order to perform an empirical test of an empirical claim containing
the T-theoretical quantity f , we have to measure values of the function f . But all known
measuring procedures (or, if you like, all known theories of measurement of f -values)
presuppose the validity of this very theory T. (page 17)
The ?problem of theoretical terms? can then be stated as follows (see Stegmu?ller
1979): Suppose a statement of the form
x is a P (1)
where x is an entity and P is a set-theoretic predicate by which a physical theory
is axiomatized. If this theory contains P-theoretic terms, then (1) is not an empirical
statement because another sentence of exactly the same form and with exactly the same
predicate is presupposed. An illustration of this concept can be given by Stegmu?ller
(1986)?s miniature theory of an Archimedian Statics. Let us assume that this miniature
theory is formalized by the set-theoretic predicate AS. The intended applications of the
theory AS are objects a1, . . . , an that are in balance around a pivot point. The theory uses
2 The following discussion of concepts of the ?structuralist? or ?non-statement view of theories? is based
on works by Stegmu?ller (1979, 1986) and Balzer and Moulines (1996) that are more accessible than the
original book by Sneed (1971). All translations from German are by the author.
236
Riezler On the Problem of Theoretical Terms in Empirical CL
two functions that measure the distance d of the objects from the pivot point, and the
weight g. The central axiom of the theory states that the sum of the products d(ai)g(ai)
is the same for the objects on either side of the pivot point. The theory AS can then be
defined as follows:
x is an AS iff there is an A, d, g such that:
1. x = ?A, d, g?,
2. A = {a1, . . . , an},
3. d : A ? IR,
4. g : A ? IR,
5. ?a ? A : g(a) > 0,
6.
?n
i=1 d(ai)g(ai) = 0.
Entities that satisfy conditions (1) to (5) are called potential models of the theory. Enti-
ties that also satisfy the central axiom (6) are called models of the theory. An empirical
statement is a statement that a certain entity is a model of the theory.
Stegmu?ller (1986) uses the miniature theory AS to explain the problem of theoretical
terms as follows: Suppose we observe children sitting on a seesaw board. Suppose
further that the board is in balance. Translating this observation into the set-theoretic
language, we could denote by y the balanced seesaw including the children, and we
would be tempted to make the empirical statement that
y is an AS (2)
In order to verify the central axiom, we need to measure distance and weight of the
children. Suppose that we have a measuring tape available to measure distance, and
suppose further that our only method to measure weight is the use of beam balance
scales. Let us denote by z the entity consisting of the balanced beam scale, the child,
and the counterbalancing measuring weight; then the validity of our measuring result
depends on a statement
z is an AS (3)
Thus, in order to check statement (2), we have to presuppose statement (3), which
is of the very same form and uses the very same predicate. That means, in order to
measure the weight of the children, we have to presuppose successful applications of
the theory AS. But in order to decide for successful applications of AS, we need to be
able to measure the weight of the objects in such application. This epistemological circle
prevents us from claiming that our original statement (2) is an empirical statement.
The crux of the problem of theoretical terms for the miniature theory AS is the
measuring procedure for the function g that presupposes the validity of the theory AS.
The term g is thus AS-theoretical. There are two solutions to this problem:
1. In order to avoid the use of AS-theoretic terms such as g, we could discard
the assumption that our weight-measuring procedure uses beam balance
scales. Instead we could use AS-non-theoretic measuring procedures such
as spring scales. The miniature theory AS would no longer contain
237
Computational Linguistics Volume 40, Number 1
AS-theoretic terms. Thus we would be able to make empirical statements
of the form (2), that is, statements about certain entities being models of
the theory AS.
2. In complex physical theories such as particle mechanics there are no
simplified assumptions on measuring procedures that can be dropped
easily. Sneed (1971) proposed the so-called Ramsey solution3 that in
essence avoids AS-theoretical terms by existentially quantifying over them.
Solution (1), where T-theoretical terms are measured by applications of a theory
T?, thus is the standard case in empirical sciences. Solution (2) is a special case where
we need theory T in order to measure some terms in theory T. Gadenne (1985) argues
that this case can be understood as a tentative assumption of theory T that still makes
empirical testing possible.4
The important point for our discussion is that in both solutions to the problem
of theoretical terms, whether we refer to another theory T? (solution (1)) or whether
we tentatively assume theory T (solution (2)), we require an explicit dichotomy between
T-theoretical and T-non-theoretical terms. This insight is crucial in the following analysis
of possible circularities in the methodology of empirical CL.
3. The Problem of Theoretical Terms in Empirical CL
Most machine-learning approaches can be characterized as identifying a learning
problem as a problem of estimating a prediction function f (x) for given identically
and independently distributed (i.i.d.) data {(xi, yi)}Ni=1 of instances and labels. For
most approaches in empirical CL, this prediction function can be characterized by a
discriminant form of a function f where
f (x;w,?) = argmax
y
F(x, y;w,?)
and where w ? IRD denotes a D-dimensional parameter vector, ?(x, y) ? IRD is a
D-dimensional vector of features (also called attributes or covariates) jointly represent-
ing input patterns x and outputs y (denoting categorical, scalar, or structured variables),
3 For the miniature theory AS, this is done by firstly stripping out statements (4)?(6) containing theoretical
terms, achieving a partial potential model. Secondly statements (4) and (5) are replaced by a so-called
theoretical extension that existentially quantifies over measuring procedures for terms like g. The
resulting Ramsey claim applies a theoretical extension to a partial potential model that also satisfies
condition (6). Because such a statement does not contain theoretical terms we can make empirical
statements about entities being models of the theory AS.
4 Critics of the structuralist theory of science have remarked that both of the solutions are instances of a
more general problem, the so-called Duhem-Quine problem, thus the focus of the structuralist program
on solution (2) seems to be an exaggeration of the actual problem (von Kutschera 1982; Gadenne 1985).
The Duhem-Quine thesis states that theoretical assumptions cannot be tested in isolation, but rather
whole systems of theoretical assumptions and auxiliary assumptions are subjected to empirical testing.
That is, if our predictions are not in accordance with our theory, we can only conclude that one of our
many theoretical assumptions must be wrong, but we cannot know which one, and we can always
modify our system of assumptions, leading to various ways of immunity of theories (Stegmu?ller 1986).
This problem arises in Solution (1) as well as in Solution (2)
238
Riezler On the Problem of Theoretical Terms in Empirical CL
and F measures the compatibility of pairs (x, y), for example, in the form of a linear
discriminant function (Taskar et al. 2004; Tsochantaridis et al. 2005).5
The problem of theoretical terms arises in empirical CL in cases where a single
theoretical tier is used both in manual data annotation (i.e., in the assignment of labels
y to patterns x via the encoding of data pairs (x, y)), and in feature construction (i.e., in
the association of labels y to patterns x via features ?(x, y)).
This problem can be illustrated by looking at automatic methods for data an-
notation. For example, information retrieval (IR) in the patent domain uses citations
of patents in other patents to automatically create relevance judgments for ranking
(Graf and Azzopardi 2008). Learning-to-rank models such as that of Guo and Gomes
(2009) define domain knowledge features on patent pairs (e.g., same patent class in the
International Patent Classification [IPC], same inventor, same assignee company) and IR
score features (e.g., tf-idf, cosine similarity) to represent data in a structured prediction
framework. Clearly, one could have just as well used IPC classes to create automatic
relevance judgments, and patent citations as features in the structured predictionmodel.
It should also be evident that using the same criterion to automatically create relevance
labels and as feature representation would be circular. In terms of the philosophical con-
cepts introduced earlier, the theory of measurement of relevance used in data labeling
cannot be the same as the theory expressed by the features of the structured prediction
model; otherwise we exhibit the problem of theoretical terms.
This problem can also arise in scenarios of manual data annotation. One example is
data annotation by expert coders: The expert coder?s decisions of which labels to assign
to which types of patterns may be guided by implicit or tacit knowledge that is shared
among the community of experts. These experts may apply the very same knowledge to
design features for their machine learning models. For example, in attempts to construct
semantic annotations for machine learning purposes, the same criteria such as negation
tests might be used to distinguish presupposition from entailment in the labeling of
data, and in the construction of feature functions for a classifier to be trained and tested
on these data. Similar to the example of automatic data annotation in patent retrieval,
we exhibit the problem of theoretical terms in manual data annotation by experts in
that the theory of measurement used in data annotation and feature construction is
the same. This problem is exacerbated in the situation where a single expert annotator
codes the data and later assumes the role of a feature designer using the ?given? data.
For example, in constructing a treebank for the purpose of learning a statistical disam-
biguation model for parsing with a hand-written grammar, the same person might act in
different roles as grammar writer, as manual annotator using the grammar?s analyses as
candidate annotations, and as feature designer for the statistical disambiguation model.
The sketched scenarios are inherently circular in the sense of the problem of the-
oretical terms described previously. Thus in all cases, we are prevented from making
empirical statements. High prediction accuracy of machine learning in such scenarios
indicates high consistency in the application of implicit knowledge in different roles of
a single expert or of groups of experts, but not more.
This problem of circularity in expert coding is related to the problem of reliability in
data annotation, a solution to which is sought by methods for measuring and enhancing
inter-annotator agreement. A seminal paper by Carletta (1996) and a follow-up survey
5 In this article, we concentrate on supervised machine learning. Semisupervised, transductive, active,
or unsupervised learning deal with machine learning from incomplete or missing labelings where the
general assumption of i.i.d. data is not questioned. See Dundar et al. (2007) for an approach of machine
learning from non-i.i.d. data.
239
Computational Linguistics Volume 40, Number 1
paper by Artstein and Poesio (2008) have discussed this issue at length. Both papers
refer to Krippendorff (2004, 1980a, page 428) who recommends that reliability data
?have to be generated by coders that are widely available, follow explicit and commu-
nicable instructions (a data language), and work independently of each other. . . . [T]he
more coders participate in the process and the more common they are, the more likely
they can ensure the reliability of data.? Ironically, it seems as if the best inter-annotator
agreement is achieved by techniques that are in conflict with these recommendations,
namely, by using experts (Kilgarriff 1999) or intensively trained coders (Hovy et al. 2006)
for data annotation. Artstein and Poesio (2008) state explicitly that
experts as coders, particularly long-term collaborators, [. . . ] may agree not because they
are carefully following written instructions, but because they know the purpose of the
research very well?which makes it virtually impossible for others to reproduce the
results on the basis of the same coding scheme . . . . Practices which violate the third
requirement (independence) include asking the coders to discuss their judgments with
each other and reach their decisions by majority vote, or to consult with each other
when problems not foreseen in the coding instructions arise. Any of these practices
make the resulting data unusable for measuring reproducibility. (page 575)
Reidsma and Carletta (2007) and Beigman Klebanov and Beigman (2009) reach the
conclusion that high inter-annotator agreement is neither sufficient nor necessary to
achieve high reliability in data annotation. The problem lies in the implicit or tacit
knowledge that is shared among the community of experts. This implicit knowledge
is responsible for the high inter-annotator agreement, but hinders reproducibility. In
a similar way, implicit knowledge of expert coders can lead to a circularity in data
annotation and feature modeling.
4. Breaking the Circularity
Finke (1979), in attempting to establish criteria for an empirical theory of linguistics,
demands that the use of a single theoretical strategy to identify and describe the entities
of interest shall be excluded from empirical analyses. He recommends that the possibility
of using T-non-theoretic strategies to identify observations be made the defining crite-
rion for empirical sciences. That is, in order tomake an empirical statement, the two tiers
of a T-theoretical and a T-non-theoretical level are necessary because the use of a single
theoretical tier prevents distinguishing empirical statements from those that are not.
Let us call Finke?s requirement the criterion of T-non-theoretical grounding.6
Moulines (see Balzer 1996, page 141) gives a pragmatic condition for T-non-theoreticity
that can be used as a guideline: ?Term t? is T-non-theoretical if there exists and acknowl-
edged method of determination of t? in some theory T? different from T plus some link
from T? to T which permits the transfer of realizations of t? from T? into T.?
Balzer (1996) discusses a variety of more formal characterizations of the notion of
T-(non-)theoretical terms. Although the pragmatic definition cited here is rather infor-
mal, it is sufficient as a guideline in discussing concrete examples and strategies to break
the circlularity in the methodology of empirical CL. In the following, we will exemplify
how this criterion can be met by manual data annotation by using naive coders, or by
6 Note that our criterion of T-non-theoretical grounding is related to the more specific concept of
operationalization in social sciences (Friedrichs 1973). Operationalization refers to the process of
developing indicators of the form ?X is an a if Y is a b (at time t)? to connect T-theoretical and
T-non-theoretical levels. We will stick with the more general criterion in the rest of this article.
240
Riezler On the Problem of Theoretical Terms in Empirical CL
embedding data annotation into a task extrinsic to the theory to be tested, or by using
independently created language data that are available in the wild.
4.1 T-non-theoretical Grounding by Naive Coders and Crowdsourcing
Now that we have defined the criterion of T-non-theoretical grounding, we see that
Krippendorff?s (2004) request for ?coders that are widely available, follow explicit
and communicable instructions (a data language), and work independently of each
other? can be regarded as a concrete strategy to satisfy our criterion. The key is the
requirement for coders to be ?widely available? and to work on the basis of ?explicit
and communicable instructions.? The need to communicate the annotation task to non-
experts serves two purposes: On the one hand, the goal of reproducibility is supported
by having to communicate the annotation task explicitly in written form. Furthermore,
the ?naive? nature of annotators requires a verbalization in words comprehensible to
non-experts, without the option of relying on implicit or tacit knowledge that is shared
among expert annotators. The researcher will thus be forced to describe the annotation
task without using technical terms that are common to experts, but are not known to
naive coders.
Annotation by naive coders can be achieved by using crowdsourcing services such
as Amazon?s Mechanical Turk,7 or alternatively, by creating games with a purpose (von
Ahn and Dabbish 2004; Poesio et al. 2013).8 Non-expert annotations created by crowd-
sourcing have been shown to provide expert-level quality if certain recommendations
on experiment design and quality control are met (Snow et al. 2008). Successful exam-
ples of the use of crowdsourcing techniques for data annotation and system evaluation
can be found throughout all areas of NLP (see Callison-Burch and Dredze [2010] for a
recent overview). The main advantage of these techniques lies in the ability to achieve
high-quality annotations at a fraction of the time and the expense of expert annotation.
However, a less apparent advantage is the need for researchers to provide succinct
and comprehensible descriptions of Human Intelligence Tasks, and the need to break
complex annotation tasks down to simpler basic units of work for annotators. Receiving
high-quality annotations with sufficient inter-worker agreement from crowdsourcing
can be seen as a possible litmus test for a successful T-non-theoretical grounding of
complex annotation tasks. Circularity issues will vanish because T-theoretical terms
cannot be communicated directly to naive coders.
4.2 Grounding by Extrinsic Evaluation and Task-Related Annotation
Another way to achieve T-non-theoretical grounding is extrinsic evaluation of NLP
systems. This type of evaluation assesses ?the effect of a system on something that
is external to it, for example, the effect on human performance at a given task or
the value added to an application? (Belz 2009) and has been demanded for at least
20 years (Spa?rck Jones 1994). Extrinsic evaluation is advertised as a remedy against
?closed problem? approaches (Spa?rck Jones 1994) or against ?closed circles? in intrinsic
evaluation where system rankings produced by automatic measures are compared with
human rankings which are themselves unfalsifiable (Belz 2009).
7 http://www.mturk.com.
8 See Fort, Adda, and Cohen (2011) for a discussion of the ethical dimensions of crowdsourcing services
and their alternatives.
241
Computational Linguistics Volume 40, Number 1
An example of an extrinsic evaluation in NLP is the evaluation of the effect of
syntactic parsers on retrieval quality in a biomedical IR task (Miyao et al. 2008). In-
terestingly, the extrinsic set-up revealed a different system ranking than the standard
intrinsic evaluation, according to F-scores on the Penn WSJ corpus. Another example
is the area of clustering. Deficiencies in current intrinsic clustering evaluation methods
have led von Luxburg, Williamson, and Guyon (2012) to pose the question ?Clustering:
Science or Art??. They recommend to measure the usefulness of a clustering method for
a particular task under consideration, that is, to always study clustering in the context
of its end use.
Extrinsic scenarios are not only useful for the purpose of evaluation. Rather, every
extrinsic evaluation creates data that can be used as training data for another learning
task (e.g., rankings of system outputs with respect to an extrinsic task can be used to
train discriminative (re)ranking models). For example, Kim and Mooney (2013) use
the successful completion of navigation tasks to create training data for reranking
in grounded language learning. Nikoulina et al. (2012) use retrieval performance of
translated queries to create data for reranking in statistical machine translation. Clarke
et al. (2010) use the correct response for a query to a database of geographical facts to
select data for structured learning of a semantic parser. Thus the extrinsic set-up can
be seen as a general technique for T-non-theoretical grounding in training as well as
in testing scenarios. Circularity issues will not arise in extrinsic set-ups because the
extrinsic task is by definition external to the system outputs to be tested or ranked.
4.3 Grounded Data in the Wild
Halevy, Norvig, and Pereira (2009, page 8) mention statistical speech recognition
and statistical machine translation as ?the biggest successes in natural-language-related
machine learning.? This success is due to the fact that ?a large training set of the input?
output behavior that we seek to automate is available to us in the wild.? While they em-
phasize the large size of the training set, we think that the aspect that the training data
are given as a ?natural task routinely done every day for a real human need? (Halevy,
Norvig, and Pereira 2009), is just as important as the size of the training set. This is
because a real-world task that is extrinsic and independent of any scientific theory
avoids any methodological circularity in data annotation and enforces an application-
based evaluation.
Speech and translation are not the only lucky areas where data are available in the
wild. Other data sets that have been ?found? by NLP researchers are IMDb movie
reviews (exploited for sentiment analysis by Pang, Lee, and Vaithyanathan [2002]),
Amazon product reviews (used for multi-domain sentiment analysis by Blitzer, Dredze,
and Pereira [2007]), Yahoo! Answers (used for answer ranking by Surdeanu, Ciaramita,
and Zaragoza [2008]), reading comprehension tests (used for automated reading com-
prehension by Hirschman et al. [1999]), or Wikipedia (with too many uses to cite). Most
of these data were created by community-based efforts. This means that the data sets
are freely available and naturally increasing.
The extrinsic and independent aspect of data in the wild can also be created in
crowdsourcing approaches that enforce a distinction between data annotation tasks
and scientific modeling. For example, Denkowski, Al-Haj, and Lavie (2010) used
Amazon?s Mechanical Turk to create reference translations for statistical machine trans-
lation by monolingual phrase substitutions on existing references. ?Translations? cre-
ated by workers that paraphrase given references without knowing the source can
never lead to the circularity that data annotation by experts is susceptible to. In a
242
Riezler On the Problem of Theoretical Terms in Empirical CL
scenario of monolingual paraphrasing for reference translations even inter-annotator
agreement is not an issue anymore. Data created by single annotators (e.g., monolingual
meaning equivalents created for bilingual purposes [Dreyer and Marcu 2012]), can be
treated as ?given? data for machine learning purposes, even if each network of meaning
equivalences is created by a single annotator.
5. Conclusion
In this article, we have argued that the problem of theoretical terms as identified for
theoretical physics can occur in empirical CL in cases where data are not ?given? as
commonly assumed in machine learning. We exemplified this problem on the example
of manual data annotation by experts, where the task of relating instances to labels in
manual data annotation and the task of relating instances to labels via modeling fea-
ture functions are intertwined. Inspired by the structuralist theory of science, we have
defined a criterion of T-non-theoretical grounding and exemplified how this criterion
can be met by manual data annotation by using naive coders, or by embedding data
annotation into a task extrinsic to the theory to be tested, or by using independently
created language data that are available in the wild.
Our suggestions for T-non-theoretical grounding are related to work on grounded
language learning that is based on weak supervision in the form of the use of sentences
in naturally occurring contexts. For example, the meaning of natural language express-
sions can be grounded in visual scenes (Roy 2002; Yu and Ballard 2004; Yu and Siskind
2013) or actions in games or navigation tasks (Chen and Mooney 2008, 2011). Because
of the ambiguous supervision, most such approaches work with latent representations
and use unsupervised techniques in learning. Our suggestions for T-non-theoretical
grounding can be used to avoid circularities in standard supervised learning. We think
that this criterion should be considered a necessary condition for an empirical science,
in addition to ensuring reliability of measurements. Our negligence of related issues
such as validity of measurements (see Krippendorff 1980b) shows that there is a vast
methodological area to be explored, perhaps with further opportunity for guidance by
philosophy of science.
Acknowledgments
We are grateful for feedback on earlier
versions of this work from Sebastian Pado?,
Artem Sokolov, and Katharina Wa?schle.
Furthermore, we would like to thank Paola
Merlo for her suggestions and
encouragement.
References
Artstein, Ron and Massimo Poesio. 2008.
Inter-coder agreement for computational
linguistics. Computational Linguistics,
34(4):555?596.
Balzer, Wolfgang. 1996. Theoretical terms:
Recent developments. In Wolfgang
Balzer and C. Ulises Moulines, editors,
Structuralist Theory of Science. Focal
Issues, New Results. de Gruyter,
pages 139?166.
Balzer, Wolfgang and C. Ulises Moulines,
editors. 1996. Structuralist Theory of Science.
Focal Issues, New Results. de Gruyter.
Beigman Klebanov, Beata and Eyal Beigman.
2009. From annotator agreement to noise
models. Computational Linguistics,
35(4):495?503.
Belz, Anja. 2009. That?s nice ... what can you
do with it? Computational Linguistics,
35(1):111?118.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics
(ACL?07), pages 440?447, Prague.
Callison-Burch, Chris and Mark Dredze.
2010. Creating speech and language
data with Amazon?s Mechanical Turk.
In Proceedings of the NAACL-HLT 2010
243
Computational Linguistics Volume 40, Number 1
Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk,
pages 1?12, Los Angeles, CA.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):1?6.
Chen, David L. and Raymond J. Mooney.
2008. Learning to sportscast: A test
of grounded language learning.
In Proceedings of the 25th International
Conference on Machine Learning (ICML?08),
pages 128?135, Helsinki.
Chen, David L. and Raymond J. Mooney.
2011. Learning to interpret natural
language navigation instructions from
observations. In Proceedings of the
25th AAAI Conference on Artificial
Intelligence (AAAI?11), pages 859?866,
San Francisco, CA.
Clarke, James, Dan Goldwasser, Wing-Wei
Chang, and Dan Roth. 2010. Driving
semantic parsing from the world?s
response. In Proceedings of the 14th
Conference on Natural Language Learning
(CoNLL?10), pages 18?27, Uppsala.
Corfield, David, Bernhard Scho?lkopf, and
Vladimir Vapnik. 2009. Falsificationism
and statistical learning theory: Comparing
the Popper and Vapnik-Chervonenkis
dimensions. Journal for General Philosophy
of Science, 40:51?58.
Denkowski, Michael, Hassan Al-Haj,
and Alon Lavie. 2010. Turker-assisted
paraphrasing for English-Arabic
machine translation. In Proceedings of
the NAACL-HLT 2010 Workshop on
Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 66?70,
Los Angeles, CA.
Dreyer, Markus and Daniel Marcu. 2012.
HyTER: Meaning-equivalent semantics
for translation evaluation. In Proceedings of
2012 Conference of the North American
Chapter of the Association for Computational
Linguistics: Human Language Technologies
(NAACL-HLT 2012), pages 162?171,
Montreal.
Dundar, Murat, Balaji Krishnapuram, Jinbo
Bi, and R. Bharat Rao. 2007. Learning
classifiers when the training data is not
IID. In Proceedings of the 20th International
Joint Conference on Artifical Intelligence
(IJCAI?07), pages 756?761, Hyderabad.
Finke, Peter. 1979. Grundlagen einer
linguistischen Theorie. Empirie und
Begru?ndung in der Sprachwissenschaft.
Vieweg.
Fort, Kare?n, Gilles Adda, and K. Bretonnel
Cohen. 2011. Amazon Mechanical Turk:
Gold mine or coal mine? Computational
Linguistics, 37(2):413?420.
Friedrichs, Ju?rgen. 1973. Methoden empirischer
Sozialforschung. Opladen, Westdeutscher
Verlag, 14th (1990) edition.
Gadenne, Volker. 1985. Theoretische Begriffe
und die Pru?fbarkeit von Theorien.
Zeitschrift fu?r allgemeine
Wissenschaftstheorie, XVI(1):19?24.
Graf, Erik and Leif Azzopardi. 2008.
A methodology for building a patent
test collection for prior art search. In
Proceedings of the 2nd International Workshop
on Evaluating Information Access (EVIA),
pages 60?71, Tokyo.
Guo, Yunsong and Carla Gomes. 2009.
Ranking structured documents: A large
margin based approach for patent
prior art search. In Proceedings of the
International Joint Conference on Artificial
Intelligence (IJCAI?09), pages 1,058?1,064,
Pasadena, CA.
Halevy, Alon, Peter Norvig, and Fernando
Pereira. 2009. The unreasonable
effectiveness of data. IEEE Intelligent
Systems, 24:8?12.
Hirschman, Lynette, Marc Light, Eric Breck,
and John D. Burger. 1999. Deep read:
A reading comprehension system.
In Proceedings of the 37th Annual Meeting
of the Association for Computational
Linguistics (ACL?99), pages 325?332,
College Park, MD.
Hovy, Eduard, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph
Weischedel. 2006. Ontonotes: The 90%
solution. In Proceedings of the Human
Language Technology Conference of the
North American Chapter of the ACL
(HLT-NAACL?06), pages 57?60,
New York, NY.
Kilgarriff, Adam. 1999. 95% replicability for
manual word sense tagging. In Proceedings
of the Ninth Conference of the European
Chapter of the Association for Computational
Linguistics (EACL?99), pages 277?278,
Bergen.
Kim, Joohyun and Raymond J. Mooney.
2013. Adapting discriminative reranking
to grounded language learning.
In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics
(ACL?13), pages 218?277, Sofia.
Korb, Kevin. 2004. Introduction: Machine
learning as philosophy of science. Minds
and Machines, 14(4):1?7.
Krippendorff, Klaus. 1980a. Content Analysis.
An Introduction to Its Methodology. Sage,
third (2013) edition.
244
Riezler On the Problem of Theoretical Terms in Empirical CL
Krippendorff, Klaus. 1980b. Validity
in content analysis. In Ekkehard
Mochmann, editor, Computerstrategien
fu?r die Kommunikationsanalyse. Campus,
pages 69?112.
Krippendorff, Klaus. 2004. Reliability
in content analysis: Some common
misconceptions and recommendations.
Human Communication Research,
30(3):411?433.
Miyao, Yusuke, Rune Saetre, Kenji Sagae,
Takuya Matsuzaki, and Jun?ichi Tsujii.
2008. Task-oriented evaluation of
syntactic parsers and their representations.
In Proceedings of the 46th Annual
Meeting of the Association for
Computational Linguistics: Human
Language Technologies (ACL-HLT?08),
pages 46?54, Columbus, OH.
Nikoulina, Vassilina, Bogomil Kovachev,
Nikolaos Lagos, and Christof Monz. 2012.
Adaptation of statistical machine
translation model for cross-lingual
information retrieval in a service context.
In Proceedings of the 13th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL?12),
pages 109?119, Avignon.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine
learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP?02),
pages 79?86, Philadelphia, PA.
Poesio, Massimo, Jon Chamberlain, Udo
Kruschwitz, Livio Robaldo, and Luca
Ducceschi. 2013. Phrase detectives:
Utilizing collective intelligence for
Internet-scale language resource creation.
ACM Transactions on Interactive Intelligent
Systems, 3(1):Article 3.
Reidsma, Dennis and Jean Carletta. 2007.
Reliability measurements without limits.
Computational Linguistics, 34(3):319?326.
Roy, Deb K. 2002. Learning visually
grounded words and syntax for a scene
description task. Computer Speech and
Language, 16:353?385.
Sneed, Joseph D. 1971. The Logical Structure
of Mathematical Physics. D. Reidel.
Snow, Rion, Brendan O?Connor, Daniel
Jurafsky, and Andrew Y. Ng. 2008. Cheap
and fast?but is it good? Evaluating
non-expert annotations for natural
language tasks. In Proceedings of the
Conference on Empirical Methods in
Natural Language Processing (EMNLP?08),
pages 254?263, Edinburgh.
Spa?rck Jones, Karen. 1994. Towards better
NLP system evaluation. In Proceedings of
the Workshop on Human Language Technology
(HLT?94), pages 102?107, Plainsboro, NJ.
Stegmu?ller, Wolfgang. 1979. The Structuralist
View of Theories. A Possible Analogue of the
Bourbaki Programme in Physical Science.
Springer.
Stegmu?ller, Wolfgang. 1986. Probleme und
Resultate der Wissenschaftstheorie und
Analytischen Philosophie. Band II: Theorie
und Erfahrung. Springer.
Surdeanu, Mihai, Massimiliano Ciaramita,
and Hugo Zaragoza. 2008. Learning to
rank answers on large online QA
collections. In Proceedings of the 46th Annual
Meeting of the Association for Computational
Linguistics (ACL?08), pages 719?727,
Columbus, OH.
Taskar, Ben, Dan Klein, Michael Collins,
Daphne Koller, and Christopher Manning.
2004. Max-margin parsing. In Proceedings
of the 2004 Conference on Empirical Methods
in Natural Language Processing (EMNLP?04),
pages 1?8, Barcelona.
Tsochantaridis, Ioannis, Thorsten Joachims,
Thomas Hofmann, and Yasemin Altun.
2005. Large margin methods for structured
and interdependent output variables.
Journal of Machine Learning Research,
5:1453?1484.
von Ahn, Luis and Laura Dabbish. 2004.
Labeling images with a computer game.
In Proceedings of the Conference on Human
Factors in Computing Systems (CHI?04),
pages 319?326, Vienna.
von Kutschera, Franz. 1982. Grundfragen der
Erkenntnistheorie. de Gruyter.
von Luxburg, Ulrike, Robert C. Williamson,
and Isabelle Guyon. 2012. Clustering:
Science or art? In Proceedings of the ICML
2011 Workshop on Unsupervised and Transfer
Learning, pages 1?12, Bellevue, WA.
Yu, Chen and Dana H. Ballard. 2004. On
the integration of grounding language
and learning objects. In Proceedings of the
19th National Conference on Artificial
Intelligence (AAAI?04), pages 488?493,
San Jose, CA.
Yu, Haonan and Jeffrey Mark Siskind. 2013.
Grounded language learning from video
described with sentences. In Proceedings of
the 51st Annual Meeting of the Association
for Computational Linguistics (ACL?13),
pages 53?63, Sofia.
245

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 474?482,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Dense Models of Query Similarity from User Click Logs
Fabio De Bona?
Friedrich Miescher Laboratory
of the Max Planck Society
Tu?bingen, Germany
fabio@tuebingen.mpg.de
Stefan Riezler
Google Research
Zu?rich, Switzerland
riezler@google.com
Keith Hall
Google Research
Zu?rich, Switzerland
kbhall@google.com
Massimiliano Ciaramita
Google Research
Zu?rich, Switzerland
massi@google.com
Amac? Herdag?delen?
University of Trento
Rovereto, Italy
amac@herdagdelen.com
Maria Holmqvist?
Linkopings University
Linkopings, Sweden
marho@ida.liu.se
Abstract
The goal of this work is to integrate query
similarity metrics as features into a dense
model that can be trained on large amounts
of query log data, in order to rank query
rewrites. We propose features that incorpo-
rate various notions of syntactic and semantic
similarity in a generalized edit distance frame-
work. We use the implicit feedback of user
clicks on search results as weak labels in train-
ing linear ranking models on large data sets.
We optimize different ranking objectives in a
stochastic gradient descent framework. Our
experiments show that a pairwise SVM ranker
trained on multipartite rank levels outperforms
other pairwise and listwise ranking methods
under a variety of evaluation metrics.
1 Introduction
Measures of query similarity are used for a wide
range of web search applications, including query
expansion, query suggestions, or listings of related
queries. Several recent approaches deploy user
query logs to learn query similarities. One set of ap-
proaches focuses on user reformulations of queries
that differ only in one phrase, e.g., Jones et al
(2006). Such phrases are then identified as candi-
date expansion terms, and filtered by various signals
such as co-occurrence in similar sessions, or log-
likelihood ratio of original and expansion phrase.
Other approaches focus on the relation of queries
and search results, either by clustering queries based
?The work presented in this paper was done while the au-
thors were visiting Google Research, Zu?rich.
on their search results, e.g., Beeferman and Berger
(2000), or by deploying the graph of queries and re-
sults to find related queries, e.g., Sahami and Heil-
man (2006).
The approach closest to ours is that of Jones et al
(2006). Similar to their approach, we create a train-
ing set of candidate query rewrites from user query
logs, and use it to train learners. While the dataset
used in Jones et al (2006) is in the order of a few
thousand query-rewrite pairs, our dataset comprises
around 1 billion query-rewrite pairs. Clearly, man-
ual labeling of rewrite quality is not feasible for our
dataset, and perhaps not even desirable. Instead, our
intent is to learn from large amounts of user query
log data. Such data permit to learn smooth mod-
els because of the effectiveness of large data sets to
capture even rare aspects of language, and they also
are available as in the wild, i.e., they reflect the ac-
tual input-output behaviour that we seek to automate
(Halevy et al, 2009). We propose a technique to au-
tomatically create weak labels from co-click infor-
mation in user query logs of search engines. The
central idea is that two queries are related if they
lead to user clicks on the same documents for a large
amount of documents. A manual evaluation of a
small subset showed that a determination of positive
versus negative rewrites by thresholding the number
of co-clicks correlates well with human judgements
of similarity, thus justifying our method of eliciting
labels from co-clicks.
Similar to Jones et al (2006), the features of our
models are not based on word identities, but instead
on general string similarity metrics. This leads to
dense rather than sparse feature spaces. The dif-
474
ference of our approach to Jones et al (2006) lies
in our particular choice of string similarity metrics.
While Jones et al (2006) deploy ?syntactic? fea-
tures such as Levenshtein distance, and ?semantic?
features such as log-likelihood ratio or mutual in-
formation, we combine syntactic and semantic as-
pects into generalized edit-distance features where
the cost of each edit operation is weighted by vari-
ous term probability models.
Lastly, the learners used in our approach are appli-
cable to very large datasets by an integration of lin-
ear ranking models into a stochastic gradient descent
framework for optimization. We compare several
linear ranking models, including a log-linear prob-
ability model for bipartite ranking, and pairwise and
listwise SVM rankers. We show in an experimen-
tal evaluation that a pairwise SVM ranker trained on
multipartite rank levels outperforms state-of-the-art
pairwise and listwise ranking methods under a vari-
ety of evaluation metrics.
2 Query Similarity Measures
2.1 Semantic measures
In several of the similarity measures we describe be-
low, we employ pointwise mutual information (PMI)
as a measure of the association between two terms or
queries. Let wi and wj be two strings that we want
to measure the amount of association between. Let
p(wi) and p(wj) be the probability of observing wi
and wj in a given model; e.g., relative frequencies
estimated from occurrence counts in a corpus. We
also define p(wi, wj) as the joint probability of wi
and wj ; i.e., the probability of the two strings occur-
ring together. We define PMI as follows:
PMI(wi, wj) = log
p(wi, wj)
p(wi)p(wj)
. (1)
PMI has been introduced by Church and Hanks
(1990) as word assosiatio ratio, and since then
been used extensively to model semantic similar-
ity. Among several desirable properties, it correlates
well with human judgments (Recchia and Jones,
2009).
2.2 Taxonomic normalizations
As pointed out in earlier work, query transitions tend
to correlate with taxonomic relations such as gener-
alization and specialization (Lau and Horvitz, 1999;
Rieh and Xie, 2006). Boldi et al (2009) show how
knowledge of transition types can positively impact
query reformulation. We would like to exploit this
information as well. However, rather than building a
dedicated supervised classifier for this task we try to
capture it directly at the source. First, we notice how
string features; e.g., length, and edit distance already
model this phenomenon to some extent, and in fact
are part of the features used in Boldi et al (2009).
However, these measures are not always accurate
and it is easy to find counterexamples both at the
term level (e.g., ?camping? to ?outdoor activities? is
a generalization) and character level (?animal pic-
tures? to ?cat pictures? is a specialization). Sec-
ondly, we propose that by manipulating PMI we can
directly model taxonomic relations to some extent.
Rather than using raw PMI values we re-
normalize them. Notice that it is not obvious in our
context how to interpret the relation between strings
co-occurring less frequently than random. Such
noisy events will yield negative PMI values since
p(wi, wj) < p(wi)p(wj). We enforce zero PMI val-
ues for such cases. If PMI is thus constrained to
non-negative values, normalization will bound PMI
to the range between 0 and 1.
The first type of normalization, called joint nor-
malization, uses the negative log joint probability
and is defined as
PMI(J)(wi, wj) = PMI(wi, wj)/?log(p(wi, wj)).
The jointly normalized PMI(J) is a symmetric
measure between wi and wj in the sense that
PMI(J)(wi, wj) = PMI(J)(wj , wi). Intuitively it
is a measure of the amount of shared information
between the two strings relative to the sum of indi-
vidual strings information. The advantages of the
joint normalization of PMI have been noticed be-
fore (Bouma, 2009).
To capture asymmetries in the relation between
two strings, we introduce two non-symmetric nor-
malizations which also bound the measure between
0 and 1. The second normalization is called special-
ization normalization and is defined as
PMI(S)(wi, wj) = PMI(wi, wj)/? log(p(wi)).
The reason we call it specialization is that PMI(S)
favors pairs where the second string is a specializa-
475
tion of the first one. For instance, PMI(S) is at its
maximum when p(wi, wj) = p(wj) and that means
the conditional probability p(wi|wj) is 1 which is an
indication of a specialization relation.
The last normalization is called the generalization
normalization and is defined in the reverse direction
as
PMI(G)(wi, wj) = PMI(wi, wj)/? log(p(wj)).
Again, PMI(G) is a measure between 0 and 1 and is
at its maximum value when p(wj |wi) is 1.
The three normalizations provide a richer rep-
resentation of the association between two strings.
Furthermore, jointly, they model in an information-
theoretic sense the generalization-specialization di-
mension directly. As an example, for the query
transition ?apple? to ?mac os? PMI(G)=0.2917 and
PMI(S)=0.3686; i.e., there is more evidence for a
specialization. Conversely for the query transition
?ferrari models? to ?ferrari? we get PMI(G)=1 and
PMI(S)=0.5558; i.e., the target is a ?perfect? gener-
alization of the source1.
2.3 Syntactic measures
Let V be a finite vocabulary and ? be the null
symbol. An edit operation: insertion, deletion or
substitution, is a pair (a, b) ? {V ? {?} ? V ?
{?}} \ {(?, ?)}. An alignment between two se-
quences wi and wj is a sequence of edit oper-
ations ? = (a1, b1), ..., (an, bn). Given a non-
negative cost function c, the cost of an alignment is
c(?) =
?n
i=1 c(?i). The Levenshtein distance, or
edit distance, defined over V , dV (wi, wj) between
two sequences is the cost of the least expensive se-
quence of edit operations which transforms wi into
wj (Levenshtein, 1966). The distance computation
can be performed via dynamic programming in time
O(|wi||wj |). Similarity at the string, i.e., character
or term, level is an indicator of semantic similar-
ity. Edit distance captures the amount of overlap be-
tween the queries as sequences of symbols and has
been previously used in information retrieval (Boldi
et al, 2009; Jones et al, 2006).
We use two basic Levenshtein distance models.
The first, called Edit1 (E1), employs a unit cost func-
tion for each of the three operations. That is, given
1The values are computed from Web counts.
a finite vocabulary T containing all terms occurring
in queries:
?a, b ? T, cE1(a, b) = 1 if(a 6= b), 0 else.
The second, called Edit2 (E2), uses unit costs for
insertion and deletion, but computes the character-
based edit distance between two terms to decide on
the substitution cost. If two terms are very similar
at the character level, then the cost of substitution is
lower. Given a finite vocabulary T of terms and a
finite vocabulary A of characters, the cost function
is defined as:
?a, b ? T, cE2(a, b) = dA(a, b) ifa ? b 6= ?, 1 else.
where dA(a, b) is linearly scaled between 0 and 1
dividing by max(|a|, |b|).
We also investigate a variant of the edit distance
algorithm in which the terms in the input sequences
are sorted, alphabetically, before the distance com-
putation. The motivation behind this variant is the
observation that linear order in queries is not always
meaningful. For example, it seems reasonable to as-
sume that ?brooklyn pizza? and ?pizza brooklyn?
denote roughly the same user intent. However, the
pair has an edit distance of two (delete-insert), while
the distance between ?brooklyn pizza? and the less
relevant ?brooklyn college? is only one (substitute).
The sorted variant relaxes the ordering constraint.
2.4 Generalized measures
In this section we extend the edit distance frame-
work introduced in Section 2.3 with the semantic
similarity measures described in Section 2.1, using
the taxonomic normalizations defined in Section 2.2.
Extending the Levenshtein distance framework
to take into account semantic similarities between
terms is conceptually simple. As in the Edit2 model
above we use a modified cost function. We introduce
a cost matrix encoding individual costs for term sub-
stitution operations; the cost is defined in terms of
the normalized PMI measures of Section 2.2, recall
that these measures range between 0 and 1. Given a
normalized similarity measure f , an entry in a cost
matrix S for a term pair (wi, wj) is defined as:
s(wi, wj) = 2? 2f(wi, wj) + 
476
We call these models SEdit (SE), where S specifies
the cost matrix used. Given a finite term vocabulary
T and cost matrix S, the cost function is defined as:
?a, b ? T, cSE(a, b) = s(a, b) ifa ? b 6= ?, 1 else.
The cost function has the following properties.
Since insertion and deletion have unit cost, a term
is substituted only if a substitution is ?cheaper? than
deleting and inserting another term, namely, if the
similarity between the terms is not zero. The 
correction, coupled with unit insertion and deletion
cost, guarantees that for an unrelated term pair a
combination of insertion and deletion will always be
less costly then a substitution. Thus in the compu-
tation of the optimal alignment, each operation cost
ranges between 0 and 2.
As a remark on efficiency, we notice that here the
semantic similarities are computed between terms,
rather than full queries. At the term level, caching
techniques can be applied more effectively to speed
up feature computation. The cost function is imple-
mented as a pre-calculated matrix, in the next sec-
tion we describe how the matrix is estimated.
2.5 Cost matrix estimation
In our experiments we evaluated two different
sources to obtain the PMI-based cost matrices. In
both cases, we assumed that the cost of the substitu-
tion of a term with itself (i.e. identity substitution)
is always 0. The first technique uses a probabilis-
tic clustering model trained on queries and clicked
documents from user query logs. The second model
estimates cost matrices directly from user session
logs, consisting of approximately 1.3 billion U.S.
English queries. A session is defined as a sequence
of queries from the same user within a controlled
time interval. Let qs and qt be a query pair observed
in the session data where qt is issued immediately
after qs in the same session. Let q?s = qs \ qt and
q?t = qt \ qs, where \ is the set difference opera-
tor. The co-occurrence count of two terms wi and
wj from a query pair qs, qt is denoted by ni,j(qs, qt)
and is defined as:
ni,j(qs, qt) =
?
?
?
1 if wi = wj ? wi ? qs ? wj ? qt
1/(|q?s| |q
?
t|) if wi ? q
?
s ? wj ? q
?
t
0 else.
In other words, if a term occurs in both queries,
it has a co-occurrence count of 1. For all other term
pairs, a normalized co-occurrence count is computed
in order to make sure the sum of co-occurrence
counts for a term wi ? qs sums to 1 for a given
query pair. The normalization is an attempt to avoid
the under representation of terms occurring in both
queries.
The final co-occurrence count of two arbitrary
terms wi and wj is denoted by Ni,j and it is defined
as the sum over all query pairs in the session logs,
Ni,j =
?
qs,qt ni,j(qs, qt). Let N =
?
wi,wj
Ni,j be
the sum of co-occurrence counts over all term pairs.
Then we define a joint probability for a term pair as
p(wi, wj) =
Ni,j
N . Similarly, we define the single-
occurrence counts and probabilities of the terms
by computing the marginalized sums over all term
pairs. Namely, the probability of a termwi occurring
in the source query is p(i, ?) =
?
wj
Ni,j/N and
similarly the probability of a term wj occurring in
the target query is p(?, j) =
?
wi
Ni,j/N . Plugging
in these values in Eq. (1), we get the PMI(wi, wj)
for term pair wi and wj , which are further normal-
ized as described in Section 2.2.
More explanation and evaluation of the features
described in this section can be found in Ciaramita
et al (2010).
3 Learning to Rank from Co-Click Data
3.1 Extracting Weak Labels from Co-Clicks
Several studies have shown that implicit feedback
from clickstream data is a weaker signal than human
relevance judgements. Joachims (2002) or Agrawal
et al (2009) presented techniques to convert clicks
into labels that can be used for machine learning.
Our goal is not to elicit relevance judgments from
user clicks, but rather to relate queries by pivoting on
commonly clicked search results. The hypothesis is
that two queries are related if they lead to user clicks
on the same documents for a large amount of docu-
ments. This approach is similar to the method pro-
posed by Fitzpatrick and Dent (1997) who attempt
to measure the relatedness between two queries by
using the normalized intersection of the top 200 re-
trieval results. We add click information to this
setup, thus strengthening the preference for preci-
sion over recall in the extraction of related queries.
477
Table 1: Statistics of co-click data sets.
train dev test
number of queries 250,000 2,500 100
average number of
rewrites per query 4,500 4,500 30
percentage of rewrites
with ? 10 coclicks 0.2 0.2 43
In our experiments we created two ground-truth
ranking scenarios from the co-click signals. In a first
scenario, called bipartite ranking, we extract a set
of positive and a set of negative query-rewrite pairs
from the user logs data. We define positive pairs as
queries that have been co-clicked with at least 10 dif-
ferent results, and negative pairs as query pairs with
fewer than 10 co-clicks. In a second scenario, called
multipartite ranking, we define a hierarchy of levels
of ?goodness?, by combining rewrites with the same
number of co-clicks at the same level, with increas-
ing ranks for higher number of co-clicks. Statistics
on the co-click data prepared for our experiments are
given in Table 1.
For training and development, we collected
query-rewrite pairs from user query logs that con-
tained at least one positive rewrite. The training set
consists of about 1 billion of query-rewrite pairs; the
development set contains 10 million query-rewrite
pairs. The average number of rewrites per query is
around 4,500 for the training and development set,
with a very small amount of 0.2% positive rewrites
per query. In order to confirm the validity of our co-
click hypothesis, and for final evaluation, we held
out another sample of query-rewrite pairs for man-
ual evaluation. This dataset contains 100 queries for
each of which we sampled 30 rewrites in descending
order of co-clicks, resulting in a high percentage of
43% positive rewrites per query. The query-rewrite
pairs were annotated by 3 raters as follows: First the
raters were asked to rank the rewrites in descend-
ing order of relevance using a graphical user inter-
face. Second the raters assigned rank labels and bi-
nary relevance scores to the ranked list of rewrites.
This labeling strategy is similar to the labeling strat-
egy for synonymy judgements proposed by Ruben-
stein and Goodenough (1965). Inter-rater agree-
ments on binary relevance judgements, and agree-
ment between rounded averaged human relevance
scores and assignments of positive/negative labels
by the co-click threshold of 10 produced a Kappa
value of 0.65 (Siegel and Castellan, 1988).
3.2 Learning-to-Rank Query Rewrites
3.2.1 Notation
Let S = {(xq, yq)}nq=1 be a training sample
of queries, each represented by a set of rewrites
xq = {xq1, . . . , xq,n(q)}, and set of rank labels
yq = {yq1, . . . , yq,n(q)}, where n(q) is the num-
ber of rewrites for query q. For full rankings of
all rewrites for a query, a total order on rewrites is
assumed, with rank labels taking on values yqi ?
{1, . . . , n(q)}. Rewrites of equivalent rank can be
specified by assuming a partial order on rewrites,
where a multipartite ranking involves r < n(q) rele-
vance levels such that yqi ? {1, . . . , r} , and a bipar-
tite ranking involves two rank values yqi ? {1, 2}
with relevant rewrites at rank 1 and non-relevant
rewrites at rank 2.
Let the rewrites in xq be identified by the integers
{1, 2, . . . , n(q)}, and let a permutation piq on xq be
defined as a bijection from {1, 2, . . . , n(q)} onto it-
self. Let ?q denote the set of all possible permuta-
tions on xq, and let piqi denote the rank position of
xqi. Furthermore, let (i, j) denote a pair of rewrites
in xq and let Pq be the set of all pairs in xq.
We associate a feature function ?(xqi) with each
rewrite i = 1, . . . , n(q) for each query q. Further-
more, a partial-order feature map as used in Yue et
al. (2007) is created for each rewrite set as follows:
?(xq, piq) =
1
|Pq|
?
(i,j)?Pq
?(xqi)??(xqj)sgn(
1
piqi
?
1
piqj
).
The goal of learning a ranking over the rewrites
xq for a query q can be achieved either by sorting the
rewrites according to the rewrite-level ranking func-
tion f(xqi) = ?w, ?(xqi)?, or by finding the permu-
tation that scores highest according to a query-level
ranking function f(xq, piq) = ?w, ?(xq, piq)?.
In the following, we will describe a variety
of well-known ranking objectives, and extensions
thereof, that are used in our experiments. Optimiza-
tion is done in a stochastic gradient descent (SGD)
framework. We minimize an empirical loss objec-
tive
min
w
?
xq ,yq
`(w)
478
by stochastic updating
wt+1 = wt ? ?tgt
where ?t is a learning rate, and gt is the gradient
gt = ?`(w)
where
?`(w) =
?
?
?w1
`(w),
?
?w2
`(w), . . . ,
?
?wn
`(w)
?
.
3.2.2 Listwise Hinge Loss
Standard ranking evaluation metrics such as
(Mean) Average Precision (Manning et al, 2008)
are defined on permutations of whole lists and are
not decomposable over instances. Joachims (2005),
Yue et al (2007), or Chakrabarti et al (2008) have
proposed multivariate SVM models to optimize such
listwise evaluation metrics. The central idea is to
formalize the evaluation metric as a prediction loss
function L, and incorporate L via margin rescal-
ing into the hinge loss function, such that an up-
per bound on the prediction loss is achieved (see
Tsochantaridis et al (2004), Proposition 2).
The loss function is given by the following list-
wise hinge loss:
`lh(w) = (L(yq, pi
?
q )?
?
w, ?(xq, yq)? ?(xq, pi
?
q )
?
)+
where pi?q is the maximizer of the
maxpiq??q\yq L(yq, pi
?
q ) +
?
w, ?(xq, pi?q )
?
ex-
pression, (z)+ = max{0, z} and L(yq, piq) ? [0, 1]
denotes a prediction loss of a predicted ranking piq
compared to the ground-truth ranking yq.2
In this paper, we use Average Precision (AP) as
prediction loss function s.t.
LAP (yq, piq) = 1?AP (yq, piq)
where AP is defined as follows:
AP (yq, piq) =
?n(q)
j=1 Prec(j) ? (|yqj ? 2|)
?n(q)
j=1 (|yqj ? 2|)
,
P rec(j) =
?
k:piqk?piqj
(|yqk ? 2|)
piqj
.
2We slightly abuse the notation yq to denote the permutation
on xq that is induced by the rank labels. In case of full rankings,
the permutation piq corresponding to ranking yq is unique. For
multipartite and bipartite rankings, there is more than one pos-
sible permutation for a given ranking, so that we let piq denote
a permutation that is consistent with ranking yq .
Note that the ranking scenario is in this case bipartite
with yqi ? {1, 2}.
The derivatives for `lh are as follows:
?
?wk
`lh =
?
?
?
0 if
(?
w, ?(xq, yq)? ?(xq, pi?q )
?)
> L(yq, pi?q ),
?(?k(xq, yq)? ?k(xq, pi?q )) else.
SGD optimization involves computing pi?q for each
feature and each query, which can be done effi-
ciently using the greedy algorithm proposed by Yue
et al (2007). We will refer to this method as the
SVM-MAP model.
3.2.3 Pairwise Hinge Loss for Bipartite and
Multipartite Ranking
Joachims (2002) proposed an SVM method that
defines the ranking problem as a pairwise classifi-
cation problem. Cortes et al (2007) extended this
method to a magnitude-preserving version by penal-
izing a pairwise misranking by the magnitude of the
difference in preference labels. A position-sensitive
penalty for pairwise ranking SVMs was proposed
by Riezler and De Bona (2009) and Chapelle and
Keerthi (2010), and earlier for perceptrons by Shen
and Joshi (2005). In the latter approaches, the mag-
nitude of the difference in inverted ranks is accrued
for each misranked pair. The idea is to impose an
increased penalty for misrankings at the top of the
list, and for misrankings that involve a difference of
several rank levels.
Similar to the listwise case, we can view the
penalty as a prediction loss function, and incor-
porate it into the hinge loss function by rescaling
the margin by a pairwise prediction loss function
L(yqi, yqj). In our experiments we used a position-
sensitive prediction loss function
L(yqi, yqj) = |
1
yqi
?
1
yqj
|
defined on the difference of inverted ranks. The
margin-rescaled pairwise hinge loss is then defined
as follows:
`ph(w) =
?
(i,j)?Pq
(L(yqi, yqj)?
?w, ?(xqi)? ?(xqj)? sgn(
1
yqi
?
1
yqj
))+
479
Table 2: Experimental evaluation of random and best feature baselines, and log-linear, SVM-MAP, SVM-bipartite,
SVM-multipartite, and SVM-multipartite-margin-rescaled learning-to-rank models on manually labeled test set.
MAP NDCG@10 AUC Prec@1 Prec@3 Prec@5
Random 51.8 48.7 50.4 45.6 45.6 46.6
Best-feature 71.9 70.2 74.5 70.2 68.1 68.7
SVM-bipart. 73.7 73.7 74.7 79.4 70.1 70.1
SVM-MAP 74.3 75.2 75.3 76.3 71.8 72.0
Log-linear 74.7 75.1 75.7 75.3 72.2 71.3
SVM-pos.-sens. 75.7 76.0 76.6 82.5 72.9 73.0
SVM-multipart. 76.5 77.3 77.2 83.5 74.2 73.6
The derivative of `ph is calculated as follows:
?
?wk
`lp =
?
????
????
0 if (?w, ?(xqi)? ?(xqj)?
sgn( 1yqi ?
1
yqj
)) > L(yqi, yqj),
?(?k(xqi)? ?k(xqj))sgn( 1yqi ?
1
yqj
)
else.
Note that the effect of inducing a position-
sensitive penalty on pairwise misrankings applies
only in case of full rankings on n(q) rank levels,
or in case of multipartite rankings involving 2 <
r < n(q) rank levels. Henceforth we will refer to
margin-rescaled pairwise hinge loss for multipartite
rankings as the SVM-pos.-sens. method.
Bipartite ranking is a special case where
L(yqi, yqj) is constant so that margin rescaling does
not have the effect of inducing position-sensitivity.
This method will be referred to as the SVM-bipartite
model.
Also note that for full ranking or multipartite
ranking, predicting a low ranking for an instance
that is ranked high in the ground truth has a domino
effect of accruing an additional penalty at each
rank level. This effect is independent of margin-
rescaling. The method of pairwise hinge loss
for multipartite ranking with constant margin will
henceforth be referred to as the SVM-multipartite
model.
Computation in SGD optimization is dominated
by the number of pairwise comparisons |Pq| for
each query. For full ranking, a comparison of
|Pq| =
(n(q)
2
)
pairs has to be done. In the case
of multipartite ranking at r rank levels, each in-
cluding |li| rewrites, pairwise comparisons between
rewrites at the same rank level can be ignored.
This reduces the number of comparisons to |Pq| =
?r?1
i=1
?r
j=i+1 |li||lj |. For bipartite ranking of p
positive and n negative instances, |Pq| = p ? n com-
parisons are necessary.
3.2.4 Log-linear Models for Bipartite Ranking
A probabilistic model for bipartite ranking can be
defined as the conditional probability of the set of
relevant rewrites, i.e., rewrites at rank level 1, given
all rewrites at rank levels 1 and 2. A formalization in
the family of log-linear models yields the following
logistic loss function `llm that was used for discrim-
inative estimation from sets of partially labeled data
in Riezler et al (2002):
`llm(w) = ? log
?
xqi?xq |yqi=1
e?w,?(xqi)?
?
xqi?xq
e?w,?(xqi)?
.
The gradient of `llm is calculated as a difference be-
tween two expectations:
?
?wk
`llm = ?pw [?k|xq; yqi = 1] + pw [?k|xq] .
The SGD computation for the log-linear model is
dominated by the computation of expectations for
each query. The logistic loss for bipartite ranking is
henceforth referred to as the log-linear model.
4 Experimental Results
In the experiments reported in this paper, we trained
linear ranking models on 1 billion query-rewrite
pairs using 60 dense features, combined of the build-
ing blocks of syntactic and semantic similarity met-
rics under different estimations of cost matrices. De-
velopment testing was done on a data set that was
held-out from the training set. Final testing was car-
ried out on the manually labeled dataset. Data statis-
tics for all sets are given in Table 1.
480
Table 3: P-values computed by approximate randomization test for 15 pairwise comparisons of result differences.
Best-feature SVM-bipart. SVM-MAP Log-linear SVM-pos.-sens. SVM-multipart.
Best-feature - < 0.005 < 0.005 < 0.005 < 0.005 < 0.005
SVM-bipart. - - 0.324 < 0.005 < 0.005 < 0.005
SVM-MAP - - - 0.374 < 0.005 < 0.005
Log-linear - - - - 0.053 < 0.005
SVM-pos.-sens. - - - - - < 0.005
SVM-multipart. - - - - - -
Model selection was performed by adjusting
meta-parameters on the development set. We
trained each model at constant learning rates ? ?
{1, 0.5, 0.1, 0.01, 0.001}, and evaluated each variant
after every fifth out of 100 passes over the training
set. The variant with the highest MAP score on the
development set was chosen and evaluated on the
test set. This early stopping routine also served for
regularization.
Evaluation results for the systems are reported in
Table 2. We evaluate all models according to the fol-
lowing evaluation metrics: Mean Average Precision
(MAP), Normalized Discounted Cumulative Gain
with a cutoff at rank 10 (NDCG@10), Area-under-
the-ROC-curve (AUC), Precision@n3. As baselines
we report a random permutation of rewrites (ran-
dom), and the single dense feature that performed
best on the development set (best-feature). The latter
is the log-probability assigned to the query-rewrite
pair by the probabilistic clustering model used for
cost matrix estimation (see Section 2.5). P-values
are reported in Table 3 for all pairwise compar-
isons of systems (except the random baseline) us-
ing an Approximate Randomization test where strat-
ified shuffling is applied to results on the query level
(see Noreen (1989)). The rows in Tables 2 and 3
are ranked according to MAP values of the systems.
SVM-multipartite outperforms all other ranking sys-
tems under all evaluation metrics at a significance
level ? 0.995. For all other pairwise comparisons
of result differences, we find result differences of
systems ranked next to each other to be not statis-
tically significant. All systems outperform the ran-
dom and best-feature baselines with statistically sig-
nificant result differences. The distinctive advantage
of the SVM-multipartite models lies in the possibil-
3For a definition of these metrics see Manning et al (2008)
ity to rank rewrites with very high co-click num-
bers even higher than rewrites with reasonable num-
bers of co-clicks. This preference for ranking the
top co-clicked rewrites high seems the best avenue
for transferring co-click information to the human
judgements encoded in the manually labeled test set.
Position-sensitive margin rescaling does not seem to
help, but rather seems to hurt.
5 Discussion
We presented an approach to learn rankings of query
rewrites from large amounts of user query log data.
We showed how to use the implicit co-click feed-
back about rewrite quality in user log data to train
ranking models that perform well on ranking query
rewrites according to human quality standards. We
presented large-scale experiments using SGD opti-
mization for linear ranking models. Our experimen-
tal results show that an SVM model for multipartite
ranking outperforms other linear ranking models un-
der several evaluation metrics. In future work, we
would like to extend our approach to other models,
e.g., sparse combinations of lexicalized features.
References
R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra,
and P. Tsaparas. 2009. Generating labels from clicks.
In Proceedings of the 2nd ACM International Con-
ference on Web Search and Data Mining, Barcelona,
Spain.
Doug Beeferman and Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. In
Proceedings of the 6th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD?00), Boston, MA.
P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. 2009.
From ?Dango? to ?Japanese cakes?: Query reformula-
481
tion models and patterns. In Proceedings of Web Intel-
ligence. IEEE Cs Press.
G. Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Proceedings of
GSCL.
Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and
Chiru Bhattacharayya. 2008. Structured learning for
non-smooth ranking losses. In Proceedings of the 14th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD?08), Las Vegas, NV.
Olivier Chapelle and S. Sathiya Keerthi. 2010. Efficient
algorithms for ranking with SVMs. Information Re-
trieval Journal.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Massimiliano Ciaramita, Amac? Herdag?delen, Daniel
Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler,
and Enrique Alfonseca. 2010. Generalized syntactic
and semantic models of query reformulation. In Pro-
ceedings of the 33rd ACM SIGIR Conference, Geneva,
Switzerland.
Corinna Cortes, Mehryar Mohri, and Asish Rastogi.
2007. Magnitude-preserving ranking algorithms. In
Proceedings of the 24th International Conference on
Machine Learning (ICML?07), Corvallis, OR.
Larry Fitzpatrick and Mei Dent. 1997. Automatic feed-
back using past queries: Social searching? In Pro-
ceedings of the 20th Annual International ACM SIGIR
Conference, Philadelphia, PA.
Alon Halevy, Peter Norvig, and Fernando Pereira. 2009.
The unreasonable effectiveness of data. IEEE Intelli-
gent Systems, 24:8?12.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the 8th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD?08), New York, NY.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
the 22nd International Conference on Machine Learn-
ing (ICML?05), Bonn, Germany.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th International World Wide Web
conference (WWW?06), Edinburgh, Scotland.
T. Lau and E. Horvitz. 1999. Patterns of search: analyz-
ing and modeling web query refinement. In Proceed-
ings of the seventh international conference on User
modeling, pages 119?128. Springer-Verlag New York,
Inc.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
G. Recchia and M.N. Jones. 2009. More data trumps
smarter algorithms: comparing pointwise mutual in-
formation with latent semantic analysis. Behavioral
Research Methods, 41(3):647?656.
S.Y. Rieh and H. Xie. 2006. Analysis of multiple query
reformulations on the web: the interactive information
retrieval context. Inf. Process. Manage., 42(3):751?
768.
Stefan Riezler and Fabio De Bona. 2009. Simple risk
bounds for position-sensitive max-margin ranking al-
gorithms. In Proceedings of the Workshop on Ad-
vances in Ranking at the 23rd Annual Conference
on Neural Information Processing Systems (NIPS?09),
Whistler, Canada.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 10(3):627?633.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th Inter-
national World Wide Web conference (WWW?06), Ed-
inburgh, Scotland.
Libin Shen and Aravind K. Joshi. 2005. Ranking and
reranking with perceptron. Journal of Machine Learn-
ing Research, 60(1-3):73?96.
Sidney Siegel and John Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. Second Edition.
MacGraw-Hill, Boston, MA.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the 21st International
Conference on Machine Learning (ICML?04), Banff,
Canada.
Yisong Yue, Thomas Finley, Filip Radlinski, and
Thorsten Joachims. 2007. A support vector method
for optimizing average precision. In Proceedings of
the 30th Annual International ACM SIGIR Confer-
ence, Amsterdam, The Netherlands.
482
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 11?21,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Joint Feature Selection in Distributed Stochastic Learning
for Large-Scale Discriminative Training in SMT
Patrick Simianer and Stefan Riezler
Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany
{simianer,riezler}@cl.uni-heidelberg.de
Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
cdyer@cs.cmu.edu
Abstract
With a few exceptions, discriminative train-
ing in statistical machine translation (SMT)
has been content with tuning weights for large
feature sets on small development data. Ev-
idence from machine learning indicates that
increasing the training sample size results in
better prediction. The goal of this paper is to
show that this common wisdom can also be
brought to bear upon SMT. We deploy local
features for SCFG-based SMT that can be read
off from rules at runtime, and present a learn-
ing algorithm that applies `1/`2 regulariza-
tion for joint feature selection over distributed
stochastic learning processes. We present ex-
periments on learning on 1.5 million training
sentences, and show significant improvements
over tuning discriminative models on small
development sets.
1 Introduction
The standard SMT training pipeline combines
scores from large count-based translation models
and language models with a few other features and
tunes these using the well-understood line-search
technique for error minimization of Och (2003). If
only a handful of dense features need to be tuned,
minimum error rate training can be done on small
tuning sets and is hard to beat in terms of accuracy
and efficiency. In contrast, the promise of large-
scale discriminative training for SMT is to scale to
arbitrary types and numbers of features and to pro-
vide sufficient statistical support by parameter esti-
mation on large sample sizes. Features may be lex-
icalized and sparse, non-local and overlapping, or
be designed to generalize beyond surface statistics
by incorporating part-of-speech or syntactic labels.
The modeler?s goals might be to identify complex
properties of translations, or to counter errors of pre-
trained translation models and language models by
explicitly down-weighting translations that exhibit
certain undesired properties. Various approaches to
feature engineering for discriminative models have
been presented (see Section 2), however, with a few
exceptions, discriminative learning in SMT has been
confined to training on small tuning sets of a few
thousand examples. This contradicts theoretical and
practical evidence from machine learning that sug-
gests that larger training samples should be benefi-
cial to improve prediction also in SMT. Why is this?
One possible reason why discriminative SMT has
mostly been content with small tuning sets lies in
the particular design of the features themselves. For
example, the features introduced by Chiang et al
(2008) and Chiang et al (2009) for an SCFG model
for Chinese/English translation are of two types:
The first type explicitly counters overestimates of
rule counts, or rules with bad overlap points, bad
rewrites, or with undesired insertions of target-side
terminals. These features are specified in hand-
crafted lists based on a thorough analysis of a tuning
set. Such finely hand-crafted features will find suf-
ficient statistical support on a few thousand exam-
ples and thus do not benefit from larger training sets.
The second type of features deploys external infor-
mation such as syntactic parses or word alignments
to penalize bad reorderings or undesired translations
of phrases that cross syntactic constraints. At large
scale, extraction of such features quickly becomes
11
(1) X ? X1 hat X2 versprochen, X1 promised X2
(2) X ? X1 hat mir X2 versprochen,
X1 promised me X2
(3) X ? X1 versprach X2, X1 promised X2
Figure 1: SCFG rules for translation.
infeasible because of costly generation and storage
of linguistic annotations. Another possible reason
why large training data did not yet show the ex-
pected improvements in discriminative SMT is a
special overfitting problem of current popular online
learning techniques. This is due to stochastic learn-
ing on a per-example basis where a weight update on
a misclassified example may apply only to a small
fraction of data that have been seen before. Thus
many features will not generalize well beyond the
training examples on which they were introduced.
The goal of this paper is to investigate if and
how it is possible to benefit from scaling discrimi-
native training for SMT to large training sets. We
deploy generic features for SCFG-based SMT that
can efficiently be read off from rules at runtime.
Such features include rule ids, rule-local n-grams,
or types of rule shapes. Another crucial ingredi-
ent of our approach is a combination of parallelized
stochastic learning with feature selection inspired
by multi-task learning. The simple but effective
idea is to randomly divide training data into evenly
sized shards, use stochastic learning on each shard
in parallel, while performing `1/`2 regularization
for joint feature selection on the shards after each
epoch, before starting a new epoch with a reduced
feature vector averaged across shards. Iterative fea-
ture selection procedure is the key to both efficiency
and improved prediction: Without interleaving par-
allelized stochastic learning with feature selection
our largest experiments would not be feasible. Se-
lecting features jointly across shards and averaging
does counter the overfitting effect that is inherent
to stochastic updating. Our resulting models are
learned on large data sets, but they are small and
outperform models that tune feature sets of various
sizes on small development sets. Our software is
freely available as a part of the cdec1 framework.
1https://github.com/redpony/cdec
2 Related Work
The great promise of discriminative training for
SMT is the possibility to design arbitrarily expres-
sive, complex, or overlapping features in great num-
bers. The focus of many approaches thus has been
on feature engineering and on adaptations of ma-
chine learning algorithms to the special case of SMT
(where gold standard rankings have to be created
automatically). Examples for adapted algorithms
include Maximum-Entropy Models (Och and Ney,
2002; Blunsom et al, 2008), Pairwise Ranking Per-
ceptrons (Shen et al, 2004; Watanabe et al, 2006;
Hopkins and May, 2011), Structured Perceptrons
(Liang et al, 2006a), Boosting (Duh and Kirchhoff,
2008; Wellington et al, 2009), Structured SVMs
(Tillmann and Zhang, 2006; Hayashi et al, 2009),
MIRA (Watanabe et al, 2007; Chiang et al, 2008;
Chiang et al, 2009), and others. Adaptations of the
loss functions underlying such algorithms to SMT
have recently been described as particular forms
of ramp loss optimization (McAllester and Keshet,
2011; Gimpel and Smith, 2012).
All approaches have been shown to scale to large
feature sets and all include some kind of regulariza-
tion method. However, most approaches have been
confined to training on small tuning sets. Exceptions
where discriminative SMT has been used on large
training data are Liang et al (2006a) who trained 1.5
million features on 67,000 sentences, Blunsom et
al. (2008) who trained 7.8 million rules on 100,000
sentences, or Tillmann and Zhang (2006) who used
230,000 sentences for training.
Our approach is inspired by Duh et al (2010)
who applied multi-task learning for improved gen-
eralization in n-best reranking. In contrast to our
work, Duh et al (2010) did not incorporate multi-
task learning into distributed learning, but defined
tasks as n-best lists, nor did they develop new algo-
rithms, but used off-the-shelf multi-task tools.
3 Local Features for Synchronous CFGs
The work described in this paper is based on the
SMT framework of hierarchical phrase-based trans-
lation (Chiang, 2005; Chiang, 2007). Transla-
tion rules are extracted from word-aligned paral-
lel sentences and can be seen as productions of a
synchronous CFG. Examples are rules like (1)-(3)
12
shown in Figure 1. Local features are designed to be
readable directly off the rule at decoding time. We
use three rule templates in our work:
Rule identifiers: These features identify each rule
by a unique identifier. Such features corre-
spond to the relative frequencies of rewrites
rules used in standard models.
Rule n-grams: These features identify n-grams of
consecutive items in a rule. We use bigrams
on source-sides of rules. Such features identify
possible source side phrases and thus can give
preference to rules including them.2
Rule shape: These features are indicators that ab-
stract away from lexical items to templates that
identify the location of sequences of terminal
symbols in relation to non-terminal symbols,
on both the source- and target-sides of each
rule used. For example, both rules (1) and (2)
map to the same indicator, namely that a rule
is being used that consists of a (NT, term*, NT,
term*) pattern on its source side, and an (NT,
term*, NT) pattern on its target side. Rule (3)
maps to a different template, that of (NT, term*,
NT) on source and target sides.
4 Joint Feature Selection in Distributed
Stochastic Learning
The following discussion of learning methods is
based on pairwise ranking in a Stochastic Gradi-
ent Descent (SGD) framework. The resulting al-
gorithms can be seen as variants of the perceptron
algorithm. Let each translation candidate be repre-
sented by a feature vector x ? IRD where preference
pairs for training are prepared by sorting translations
according to smoothed sentence-wise BLEU score
(Liang et al, 2006a) against the reference. For a
preference pair xj = (x(1)j ,x
(2)
j ) where x
(1)
j is pre-
ferred over x(2)j , and x?j = x
(1)
j ? x
(2)
j , we consider
the following hinge loss-type objective function:
lj(w) = (??w, x?j ?)+
where (a)+ = max(0, a) , w ? IRD is a weight vec-
tor, and ??, ?? denotes the standard vector dot prod-
uct. Instantiating SGD to the following stochastic
2Similar ?monolingual parse features? have been used in
Dyer et al (2011).
subgradient leads to the perceptron algorithm for
pairwise ranking3 (Shen and Joshi, 2005):
?lj(w) =
{
?x?j if ?w, x?j? ? 0,
0 else.
Our baseline algorithm 1 (SDG) scales pairwise
ranking to large scale scenarios. The algorithm takes
an average over the final weight updates of each
epoch instead of keeping a record of all weight up-
dates for final averaging (Collins, 2002) or for voting
(Freund and Schapire, 1999).
Algorithm 1 SGD: int I, T , float ?
Initialize w0,0,0 ? 0.
for epochs t? 0 . . . T ? 1: do
for all i ? {0 . . . I ? 1}: do
Decode ith input with wt,i,0.
for all pairs xj , j ? {0 . . . P ? 1}: do
wt,i,j+1 ? wt,i,j ? ??lj(wt,i,j)
end for
wt,i+1,0 ? wt,i,P
end for
wt+1,0,0 ? wt,I,0
end for
return 1T
T?
t=1
wt,0,0
While stochastic learning exhibits a runtime be-
havior that is linear in sample size (Bottou, 2004),
very large datasets can make sequential process-
ing infeasible. Algorithm 2 (MixSGD) addresses
this problem by parallelization in the framework of
MapReduce (Dean and Ghemawat, 2004).
Algorithm 2 MixSGD: int I, T, Z, float ?
Partition data into Z shards, each of size S ? I/Z;
distribute to machines.
for all shards z ? {1 . . . Z}: parallel do
Initialize wz,0,0,0 ? 0.
for epochs t? 0 . . . T ? 1: do
for all i ? {0 . . . S ? 1}: do
Decode ith input with wz,t,i,0.
for all pairs xj , j ? {0 . . . P ? 1}: do
wz,t,i,j+1 ? wz,t,i,j ? ??lj(wz,t,i,j)
end for
wz,t,i+1,0 ? wz,t,i,P
end for
wz,t+1,0,0 ? wz,t,S,0
end for
end for
Collect final weights from each machine,
return 1Z
Z?
z=1
(
1
T
T?
t=1
wz,t,0,0
)
.
3Other loss functions lead to stochastic versions of SVMs
(Collobert and Bengio, 2004; Shalev-Shwartz et al, 2007;
Chapelle and Keerthi, 2010).
13
Algorithm 2 is a variant of the SimuParallelSGD
algorithm of Zinkevich et al (2010) or equivalently
of the parameter mixing algorithm of McDonald et
al. (2010). The key idea of algorithm 2 is to parti-
tion the data into disjoint shards, then train SGD on
each shard in parallel, and after training mix the final
parameters from each shard by averaging. The algo-
rithm requires no communication between machines
until the end.
McDonald et al (2010) also present an iterative
mixing algorithm where weights are mixed from
each shard after training a single epoch of the per-
ceptron in parallel on each shard. The mixed weight
vector is re-sent to each shard to start another epoch
of training in parallel on each shard. This algorithm
corresponds to our algorithm 3 (IterMixSGD).
Algorithm 3 IterMixSGD: int I, T, Z, float ?
Partition data into Z shards, each of size S ? I/Z;
distribute to machines.
Initialize v? 0.
for epochs t? 0 . . . T ? 1: do
for all shards z ? {1 . . . Z}: parallel do
wz,t,0,0 ? v
for all i ? {0 . . . S ? 1}: do
Decode ith input with wz,t,i,0.
for all pairs xj , j ? {0 . . . P ? 1}: do
wz,t,i,j+1 ? wz,t,i,j ? ??lj(wz,t,i,j)
end for
wz,t,i+1,0 ? wz,t,i,P
end for
end for
Collect weights v? 1Z
Z?
z=1
wz,t,S,0.
end for
return v
Parameter mixing by averaging will help to ease
the feature sparsity problem, however, keeping fea-
ture vectors on the scale of several million features
in memory can be prohibitive. If network latency
is a bottleneck, the increased amount of information
sent across the network after each epoch may be a
further problem.
Our algorithm 4 (IterSelSGD) introduces feature
selection into distributed learning for increased effi-
ciency and as a more radical measure against over-
fitting. The key idea is to view shards as tasks, and
to apply methods for joint feature selection from
multi-task learning to achieve small sets of features
that are useful across all tasks or shards. Our algo-
rithm represents weights in a Z-by-D matrix W =
[wz1 | . . . |wzZ ]T of stacked D-dimensional weight
vectors across Z shards. We compute the `2 norm of
the weights in each feature column, sort features by
this value, and keep K features in the model. This
feature selection procedure is done after each epoch.
Reduced weight vectors are mixed and the result is
re-sent to each shard to start another epoch of paral-
lel training on each shard.
Algorithm 4 IterSelSGD: int I, T, Z,K, float ?
Partition data into Z shards, each of size S = I/Z;
distribute to machines.
Initialize v? 0.
for epochs t? 0 . . . T ? 1: do
for all shards z ? {1 . . . Z}: parallel do
wz,t,0,0 ? v
for all i ? {0 . . . S ? 1}: do
Decode ith input with wz,t,i,0.
for all pairs xj , j ? {0 . . . P ? 1}: do
wz,t,i,j+1 ? wz,t,i,j ? ??lj(wz,t,i,j)
end for
wz,t,i+1,0 ? wz,t,i,P
end for
end for
Collect/stack weights W? [w1,t,S,0| . . . |wZ,t,S,0]T
Select top K feature columns of W by `2 norm and
for k ? 1 . . .K do
v[k] = 1Z
Z?
z=1
W[z][k].
end for
end for
return v
This algorithm can be seen as an instance of `1/`2
regularization as follows: Let wd be the dth column
vector of W, representing the weights for the dth
feature across tasks/shards. `1/`2 regularization pe-
nalizes weights W by the weighted `1/`2 norm
?||W||1,2 = ?
D?
d=1
||wd||2.
Each `2 norm of a weight column represents
the relevance of the corresponding feature across
tasks/shards. The `1 sum of the `2 norms en-
forces a selection among features based on these
norms. Consider for example the two 5-feature, 3-
task weight matrices in Figure 2. Assuming the
same loss for both matrices, the right-hand side ma-
trix is preferred because of a smaller `1/`2 norm
(12 instead of 18). This matrix shares features
across tasks which leads to larger `2 norms for some
columns (here ||w1||2 and ||w2||2) and forces other
columns to zero. This results in shrinking the ma-
trix to those features that are useful across all tasks.
14
w1 w2 w3 w4 w5 w1 w2 w3 w4 w5
wz1 [ 6 4 0 0 0 ] [ 6 4 0 0 0 ]
wz2 [ 0 0 3 0 0 ] [ 3 0 0 0 0 ]
wz3 [ 0 0 0 2 3 ] [ 2 3 0 0 0 ]
column `2 norm: 6 4 3 2 3 7 5 0 0 0
`1 sum: ? 18 ? 12
Figure 2: `1/`2 regularization enforcing feature selection.
Our algorithm is related to Obozinski et al
(2010)?s approach to `1/`2 regularization where fea-
ture columns are incrementally selected based on the
`2 norms of the gradient vectors corresponding to
feature columns. Their algorithm is itself an exten-
sion of gradient-based feature selection based on the
`1 norm, e.g., Perkins et al (2003).4 In contrast to
these approaches we approximate the gradient by us-
ing the weights given by the ranking algorithm itself.
This relates our work to weight-based recursive fea-
ture elimination (RFE) (Lal et al, 2006). Further-
more, algorithm 4 performs feature selection based
on a choice of meta-parameter of K features instead
of by thresholding a regularization meta-parameter
?, however, these techniques are equivalent and can
be transformed into each other.
5 Experiments
5.1 Data, Systems, Experiment Settings
The datasets used in our experiments are versions
of the News Commentary (nc), News Crawl (crawl)
and Europarl (ep) corpora described in Table 1. The
translation direction is German-to-English.
The SMT framework used in our experiments
is hierarchical phrase-based translation (Chiang,
2007). We use the cdec decoder5 (Dyer et al,
2010) and induce SCFG grammars from two sets of
symmetrized alignments using the method described
by Chiang (2007). All data was tokenized and
lowercased; German compounds were split (Dyer,
2009). For word alignment of the news-commentary
data, we used GIZA++ (Och and Ney, 2000); for
aligning the Europarl data, we used the Berke-
ley aligner (Liang et al, 2006b). Before train-
ing, we collect all the grammar rules necessary to
4Note that by definition of ||W||1,2, standard `1 regulariza-
tion is a special case of `1/`2 regularization for a single task.
5cdec metaparameters were set to a non-terminal span limit
of 15 and standard cube pruning with a pop limit of 200.
translate each individual sentence into separate files
(so-called per-sentence grammars) (Lopez, 2007).
When decoding, cdec loads the appropriate file im-
mediately prior to translation of the sentence. The
computational overhead is minimal compared to the
expense of decoding. Also, deploying disk space
instead of memory fits perfectly into the MapRe-
duce framework we are working in. Furthermore,
the extraction of grammars for training is done in
a leave-one-out fashion (Zollmann and Sima?an,
2005) where rules are extracted for a parallel sen-
tence pair only if the same rules are found in other
sentences of the corpus as well.
3-gram (news-commentary) and 5-gram (Eu-
roparl) language models are trained on the data de-
scribed in Table 1, using the SRILM toolkit (Stol-
cke, 2002) and binarized for efficient querying using
kenlm (Heafield, 2011). For the 5-gram language
models, we replaced every word in the lm training
data with <unk> that did not appear in the English
part of the parallel training data to build an open vo-
cabulary language model.
HI
MID
LOW
Figure 3: Multipartite pairwise ranking.
Training data for discriminative learning are pre-
pared by comparing a 100-best list of transla-
tions against a single reference using smoothed per-
sentence BLEU (Liang et al, 2006a). From the
BLEU-reordered n-best list, translations were put
into sets for the top 10% level (HI), the middle
80% level (MID), and the bottom 10% level (LOW).
These level sets are used for multipartite ranking
15
News Commentary(nc)
train-nc lm-train-nc dev-nc devtest-nc test-nc
Sentences 132,753 180,657 1057 1064 2007
Tokens de 3,530,907 ? 27,782 28,415 53,989
Tokens en 3,293,363 4,394,428 26,098 26,219 50,443
Rule Count 14,350,552 (1G) ? 2,322,912 2,320,264 3,274,771
Europarl(ep)
train-ep lm-train-ep dev-ep devtest-ep test-ep
Sentences 1,655,238 2,015,440 2000 2000 2000
Tokens de 45,293,925 ? 57,723 56,783 59,297
Tokens en 45,374,649 54,728,786 58,825 58,100 60,240
Rule Count 203,552,525 (31.5G) ? 17,738,763 17,682,176 18,273,078
News Crawl(crawl)
dev-crawl test-crawl10 test-crawl11
Sentences 2051 2489 3003
Tokens de 49,848 64,301 76,193
Tokens en 49,767 61,925 74,753
Rule Count 9,404,339 11,307,304 12,561,636
Table 1: Overview of data used for train/dev/test. News Commentary (nc) and Europarl (ep) training data and
also News Crawl (crawl) dev/test data were taken from the WMT11 translation task (http://statmt.org/
wmt11/translation-task.html). The dev/test data of nc are the sets provided with the WMT07 shared
task (http://statmt.org/wmt07/shared-task.html). Ep dev/test data is from WMT08 shared task
(http://statmt.org/wmt08/shared-task.html). The numbers in brackets for the rule counts of ep/nc
training data are total counts of rules in the per-sentence grammars.
where translation pairs are built between the ele-
ments in HI-MID, HI-LOW, and MID-LOW, but not
between translations inside sets on the same level.
This idea is depicted graphically in Figure 3. The
intuition is to ensure that good translations are pre-
ferred over bad translations without teasing apart
small differences.
For evaluation, we used the mteval-v11b.pl
script to compute lowercased BLEU-4 scores (Pa-
pineni et al, 2001). Statistical significance was
measured using an Approximate Randomization test
(Noreen, 1989; Riezler and Maxwell, 2005).
All experiments for training on dev sets were car-
ried out on a single computer. For grammar extrac-
tion and training of the full data set we used a 30
node hadoop Map/Reduce cluster that can handle
300 jobs at once. We split the data into 2290 shards
for the ep runs and 141 shards for the nc runs, each
shard holding about 1,000 sentences, which corre-
sponds to the dev set size of the nc data set.
5.2 Experimental Results
The baseline learner in our experiments is a pairwise
ranking perceptron that is used on various features
and training data and plugged into various meta-
M
x?
BLEU[%] 23.0 25.0 27.0 29.0
Figure 4: Boxplot of BLEU-4 results for 100 runs of
MIRA on news commentary data, depicting median (M),
mean (x?), interquartile range (box), standard deviation
(whiskers), outliers (end points).
algorithms for distributed processing. The percep-
tron algorithm itself compares favorably to related
learning techniques such as the MIRA adaptation of
Chiang et al (2008). Figure 4 gives a boxplot depict-
ing BLEU-4 results for 100 runs of the MIRA imple-
mentation of the cdec package, tuned on dev-nc,
and evaluated on the respective test set test-nc.6 We
see a high variance (whiskers denote standard devi-
ations) around a median of 27.2 BLEU and a mean
of 27.1 BLEU. The fluctuation of results is due to
sampling training examples from the translation hy-
6MIRA was used with default meta parameters: 250 hypoth-
esis list to search for oracles, regularization strength C = 0.01
and using 15 passes over the input. It optimized IBM BLEU-4.
The initial weight vector was 0.
16
Algorithm Tuning set Features #Features devtest-nc test-nc
MIRA dev-nc default 12 ? 27.10
1
dev-nc default 12 25.88 28.0
dev-nc +id 137k 25.53 27.6?23
dev-nc +ng 29k 25.82 27.42?234
dev-nc +shape 51 25.91 28.1
dev-nc +id,ng,shape 180k 25.71 28.1534
2
train-nc default 12 25.73 27.86
train-nc +id 4.1M 25.13 27.19?134
train-nc +ng 354k 26.09 28.03134
train-nc +shape 51 26.07 27.913
train-nc +id,ng,shape 4.7M 26.08 27.8634
3
train-nc default 12 26.09 @2 27.94?
train-nc +id 3.4M 26.1 @4 27.97?12
train-nc +ng 330k 26.33 @4 28.3412
train-nc +shape 51 26.39 @9 28.312
train-nc +id,ng,shape 4.7M 26.42 @9 28.55124
4
train-nc +id 100k 25.91 @7 27.82?2
train-nc +ng 100k 26.42 @4 28.37?12
train-nc +id,ng,shape 100k 26.8 @8 28.81123
Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news-
commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and
rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo-
rithm applied to the same feature group is indicated by raised algorithm number. ? indicates statistically significant
differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal
number of epochs chosen on the devtest set.
pergraph as is done in the cdec implementation of
MIRA. We found similar fluctuations for the cdec
implementations of PRO (Hopkins and May, 2011)
or hypergraph-MERT (Kumar et al, 2009) both of
which depend on hypergraph sampling. In contrast,
the perceptron is deterministic when started from a
zero-vector of weights and achieves favorable 28.0
BLEU on the news-commentary test set. Since we
are interested in relative improvements over a stable
baseline, we restrict our attention in all following ex-
periments to the perceptron.7
Table 2 shows the results of the experimental
comparison of the 4 algorithms of Section 4. The
7Absolute improvements would be possible, e.g., by using
larger language models or by adding news data to the ep train-
ing set when evaluating on crawl test sets (see, e.g., Dyer et al
(2011)), however, this is not the focus of this paper.
default features include 12 dense models defined on
SCFG rules;8 The sparse features are the 3 templates
described in Section 3. All feature weights were
tuned together using algorithms 1-4. If not indicated
otherwise, the perceptron was run for 10 epochs with
learning rate ? = 0.0001, started at zero weight vec-
tor, using deduplicated 100-best lists.
The results on the news-commentary (nc) data
show that training on the development set does not
benefit from adding large feature sets ? BLEU re-
sult differences between tuning 12 default features
8negative log relative frequency p(e|f); log count(f ); log
count(e, f ); lexical translation probability p(f |e) and p(e|f)
(Koehn et al, 2003); indicator variable on singleton phrase e;
indicator variable on singleton phrase pair f, e; word penalty;
language model weight; OOV count of language model; num-
ber of untranslated words; Hiero glue rules (Chiang, 2007).
17
Alg. Tuning set Features #Feats devtest-ep test-ep Tuning set test-crawl10 test-crawl11
1
dev-ep default 12 25.62 26.42? dev-crawl 15.39? 14.43?
dev-ep +id,ng,shape 300k 27.84 28.37 dev-crawl 17.84 16.834
4 train-ep +id,ng,shape 100k 28.0 @9 28.62 train-ep 19.121 17.331
Table 3: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl) test
data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape).
Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the
same feature group is indicated by raised algorithm number. ? indicates statistically significant differences to best
result across features groups for same algorithm, indicated in bold face. @ indicates the optimal number of epochs
chosen on the devtest set.
and tuning the full set of 180,000 features are not
significant. However, scaling all features to the full
training set shows significant improvements for al-
gorithm 3, and especially for algorithm 4, which
gains 0.8 BLEU points over tuning 12 features on
the development set. The number of features rises
to 4.7 million without feature selection, which iter-
atively selects 100,000 features with best `2 norm
values across shards. Feature templates such as rule
n-grams and rule shapes only work if iterative mix-
ing (algorithm 3) or feature selection (algorithm 4)
are used. Adding rule id features works in combina-
tion with other sparse features.
Table 3 shows results for algorithms 1 and 4 on
the Europarl data (ep) for different devtest and test
sets. Europarl data were used in all runs for train-
ing and for setting the meta-parameter of number
of epochs. Testing was done on the Europarl test
set and news crawl test data from the years 2010
and 2011. Here tuning large feature sets on the
respective dev sets yields significant improvements
of around 2 BLEU points over tuning the 12 de-
fault features on the dev sets. Another 0.5 BLEU
points (test-crawl11) or even 1.3 BLEU points (test-
crawl10) are gained when scaling to the full training
set using iterative features selection. Result differ-
ences on the Europarl test set were not significant
for moving from dev to full train set. Algorithms 2
and 3 were infeasible to run on Europarl data beyond
one epoch because features vectors grew too large to
be kept in memory.
6 Discussion
We presented an approach to scaling discrimina-
tive learning for SMT not only to large feature
sets but also to large sets of parallel training data.
Since inference for SMT (unlike many other learn-
ing problems) is very expensive, especially on large
training sets, good parallelization is key. Our ap-
proach is made feasible and effective by applying
joint feature selection across distributed stochastic
learning processes. Furthermore, our local features
are efficiently computable at runtime. Our algo-
rithms and features are generic and can easily be re-
implemented and make our results relevant across
datasets and language pairs.
In future work, we would like to investigate more
sophisticated features, better learners, and in gen-
eral improve the components of our system that have
been neglected in the current investigation of rela-
tive improvements by scaling the size of data and
feature sets. Ultimately, since our algorithms are in-
spired by multi-task learning, we would like to apply
them to scenarios where a natural definition of tasks
is given. For example, patent data can be charac-
terized along the dimensions of patent classes and
patent text fields (Wa?schle and Riezler, 2012) and
thus are well suited for multi-task translation.
Acknowledgments
Stefan Riezler and Patrick Simianer were supported
in part by DFG grant ?Cross-language Learning-to-
Rank for Patent Retrieval?. Chris Dyer was sup-
ported in part by a MURI grant ?The linguistic-
core approach to structured translation and analysis
of low-resource languages? from the US Army Re-
search Office and a grant ?Unsupervised Induction
of Multi-Nonterminal Grammars for SMT? from
Google, Inc.
18
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable models for statistical
machine translation. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT?08), Columbus, OH.
Le?on Bottou. 2004. Stochastic learning. In Olivier
Bousquet, Ulrike von Luxburg, and Gunnar Ra?tsch,
editors, Advanced Lectures on Machine Learning,
pages 146?168. Springer, Berlin.
Olivier Chapelle and S. Sathiya Keerthi. 2010. Efficient
algorithms for ranking with SVMs. Information Re-
trieval Journal.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?08), Waikiki, Honolulu,
Hawaii.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of the 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL-HLT?09),
Boulder, CO.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In Proceedings of the con-
ference on Empirical Methods in Natural Language
Processing (EMNLP?02), Philadelphia, PA.
Ronan Collobert and Samy Bengio. 2004. Links be-
tween perceptrons, MLPs, and SVMs. In Proceed-
ings of the 21st International Conference on Machine
Learning (ICML?04), Banff, Canada.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapre-
duce: Simplified data processing on large clusters. In
Proceedings of the 6th Symposium on Operating Sys-
tem Design and Implementation (OSDI?04), San Fran-
cisco, CA.
Kevin Duh and Katrin Kirchhoff. 2008. Beyond log-
linear models: Boosted minimum error rate training
for n-best ranking. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?08), Short Paper Track, Columbus, OH.
Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. N-best reranking
by multitask learning. In Proceedings of the 5th Joint
Workshop on Statistical Machine Translation and Met-
ricsMATR, Uppsala, Sweden.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, Upp-
sala, Sweden.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK german-
english translation system. In Proceedings of the 6th
Workshop on Machine Translation (WMT11), Edin-
burgh, UK.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics - Hu-
man Language Technologies (NAACL-HLT?09), Boul-
der, CO.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Journal of Machine Learning Research, 37:277?296.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-
HLT 2012), Montreal, Canada.
Katsuhiko Hayashi, Taro Watanabe, Hajime Tsukada,
and Hideki Isozaki. 2009. Structural support vector
machines for log-linear approach in statistical machine
translation. In Proceedings of IWSLT, Tokyo, Japan.
Kenneth Heafield. 2011. KenLM: faster and smaller lan-
guage model queries. In Proceedings of the EMNLP
2011 Sixth Workshop on Statistical Machine Transla-
tion (WMT?11), Edinburgh, UK.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP?11), Edinburgh, Scotland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
and the 3rd Meeting of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?03), Edmonton, Cananda.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum Bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the 47th
Annual Meeting of the Association for Computational
19
Linguistics and the 4th IJCNLP of the AFNLP (ACL-
IJCNLP?09, Suntec, Singapore.
Thomas Navin Lal, Olivier Chapelle, Jason Weston, and
Andre? Elisseeff. 2006. Embedded methods. In I.M.
Guyon, S.R. Gunn, M. Nikravesh, and L. Zadeh, ed-
itors, Feature Extraction: Foundations and Applica-
tions. Springer.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006a. An end-to-end discriminative
approach to machine translation. In Proceedings of
the joint conference of the International Committee
on Computational Linguistics and the Association for
Computational Linguistics (COLING-ACL?06), Syd-
ney, Australia.
Percy Liang, Ben Taskar, and Dan Klein. 2006b. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference - North American
Chapter of the Association for Computational Linguis-
tics annual meeting (HLT-NAACL?06), New York, NY.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In Proceedings of EMNLP-
CoNLL, Prague, Czech Republic.
David McAllester and Joseph Keshet. 2011. Generaliza-
tion bounds and consistency for latent structural pro-
bit and ramp loss. In Proceedings of the 25th Annual
Conference on Neural Information Processing Sytems
(NIPS 2011), Granada, Spain.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proceedings of Human Language Tech-
nologies: The 11th Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL-HLT?10), Los Angeles,
CA.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Guillaume Obozinski, Ben Taskar, and Michael I. Jordan.
2010. Joint covariate selection and joint subspace se-
lection for multiple classification problems. Statistics
and Computing, 20:231?252.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hongkong, China.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL?02), Philadelphia, PA.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceedings
of the Human Language Technology Conference and
the 3rd Meeting of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?03), Edmonton, Cananda.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
IBM Research Division Technical Report, RC22176
(W0190-022), Yorktown Heights, N.Y.
Simon Perkins, Kevin Lacker, and James Theiler. 2003.
Grafting: Fast, incremental feature selection by gra-
dient descent in function space. Journal of Machine
Learning Research, 3:1333?1356.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2007. Pegasos: Primal Estimated sub-GrAdient
SOlver for SVM. In Proceedings of the 24th Inter-
national Conference on Machine Learning (ICML?07),
Corvallis, OR.
Libin Shen and Aravind K. Joshi. 2005. Ranking and
reranking with perceptron. Journal of Machine Learn-
ing Research, 60(1-3):73?96.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proceedings of the Human Language Technology con-
ference / North American chapter of the Associa-
tion for Computational Linguistics annual meeting
(HLT/NAACL?04), Boston, MA.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Denver,
CO.
Christoph Tillmann and Tong Zhang. 2006. A dis-
criminatie global training algorithm for statistical MT.
In Proceedings of the joint conference of the In-
ternational Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING-ACL?06), Sydney, Australia.
Katharina Wa?schle and Stefan Riezler. 2012. Structural
and topical dimensions in multi-task patent translation.
In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, Avignon, France.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2006. NTT statistical machine translation
for IWSLT 2006. In Proceedings of the International
Workshop on Spoken Language Translation (IWSLT),
Kyoto, Japan.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proceedings of the 2007
20
Joint Conference on Empirical Mehtods in Natural
Language Processing and Computational Language
Learning (EMNLP?07), Prague, Czech Republic.
Benjamin Wellington, Joseph Turian, and Dan Melamed.
2009. Toward purely discriminative training for tree-
structured translation models. In Cyril Goutte, Nicola
Cancedda, and Marc Dymetman, editors, Learning
Machine Translation, pages 132?149, Cambridge,
MA. The MIT Press.
Martin A. Zinkevich, Markus Weimer, Alex Smola, and
Lihong Li. 2010. Parallelized stochastic gradient de-
scent. In Proceedings of the 24th Annual Conference
on Neural Information Processing Sytems (NIPS?10),
Vancouver, Canada.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.
Journal of Automata, Languages and Combinatorics,
10(2/3):367?388.
21
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323?327,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Task Alternation in Parallel Sentence Retrieval for Twitter Translation
Felix Hieber and Laura Jehl and Stefan Riezler
Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany
{jehl,hieber,riezler}@cl.uni-heidelberg.de
Abstract
We present an approach to mine com-
parable data for parallel sentences us-
ing translation-based cross-lingual infor-
mation retrieval (CLIR). By iteratively al-
ternating between the tasks of retrieval
and translation, an initial general-domain
model is allowed to adapt to in-domain
data. Adaptation is done by training the
translation system on a few thousand sen-
tences retrieved in the step before. Our
setup is time- and memory-efficient and of
similar quality as CLIR-based adaptation
on millions of parallel sentences.
1 Introduction
Statistical Machine Translation (SMT) crucially
relies on large amounts of bilingual data (Brown et
al., 1993). Unfortunately sentence-parallel bilin-
gual data are not always available. Various ap-
proaches have been presented to remedy this prob-
lem by mining parallel sentences from comparable
data, for example by using cross-lingual informa-
tion retrieval (CLIR) techniques to retrieve a target
language sentence for a source language sentence
treated as a query. Most such approaches try to
overcome the noise inherent in automatically ex-
tracted parallel data by sheer size. However, find-
ing good quality parallel data from noisy resources
like Twitter requires sophisticated retrieval meth-
ods. Running these methods on millions of queries
and documents can take weeks.
Our method aims to achieve improvements sim-
ilar to large-scale parallel sentence extraction ap-
proaches, while requiring only a fraction of the ex-
tracted data and considerably less computing re-
sources. Our key idea is to extend a straightfor-
ward application of translation-based CLIR to an
iterative method: Instead of attempting to retrieve
in one step as many parallel sentences as possible,
we allow the retrieval model to gradually adapt to
new data by using an SMT model trained on the
freshly retrieved sentence pairs in the translation-
based retrieval step. We alternate between the
tasks of translation-based retrieval of target sen-
tences, and the task of SMT, by re-training the
SMT model on the data that were retrieved in the
previous step. This task alternation is done itera-
tively until the number of newly added pairs stabi-
lizes at a relatively small value.
In our experiments on Arabic-English Twitter
translation, we achieved improvements of over 1
BLEU point over a strong baseline that uses in-
domain data for language modeling and parameter
tuning. Compared to a CLIR-approach which ex-
tracts more than 3 million parallel sentences from
a noisy comparable corpus, our system produces
similar results in terms of BLEU using only about
40 thousand sentences for training in each of a
few iterations, thus being much more time- and
resource-efficient.
2 Related Work
In the terminology of semi-supervised learning
(Abney, 2008), our method resembles self-training
and co-training by training a learning method on
its own predictions. It is different in the aspect of
task alternation: The SMT model trained on re-
trieved sentence pairs is not used for generating
training data, but for scoring noisy parallel data
in a translation-based retrieval setup. Our method
also incorporates aspects of transductive learning
in that candidate sentences used as queries are fil-
tered for out-of-vocabulary (OOV) words and sim-
ilarity to sentences in the development set in or-
der to maximize the impact of translation-based
retrieval.
Our work most closely resembles approaches
that make use of variants of SMT to mine com-
parable corpora for parallel sentences. Recent
work uses word-based translation (Munteanu and
323
Marcu, 2005; Munteanu and Marcu, 2006), full-
sentence translation (Abdul-Rauf and Schwenk,
2009; Uszkoreit et al, 2010), or a sophisticated
interpolation of word-based and contextual trans-
lation of full sentences (Snover et al, 2008; Jehl
et al, 2012; Ture and Lin, 2012) to project source
language sentences into the target language for re-
trieval. The novel aspect of task alternation in-
troduced in this paper can be applied to all ap-
proaches incorporating SMT for sentence retrieval
from comparable data.
For our baseline system we use in-domain lan-
guage models (Bertoldi and Federico, 2009) and
meta-parameter tuning on in-domain development
sets (Koehn and Schroeder, 2007).
3 CLIR for Parallel Sentence Retrieval
3.1 Context-Sensitive Translation for CLIR
Our CLIR model extends the translation-based re-
trieval model of Xu et al (2001). While transla-
tion options in this approach are given by a lexical
translation table, we also select translation options
estimated from the decoder?s n-best list for trans-
lating a particular query. The central idea is to let
the language model choose fluent, context-aware
translations for each query term during decoding.
For mapping source language query terms to
target language query terms, we follow Ture et
al. (2012a; 2012). Given a source language query
Q with query terms qj , we project it into the tar-
get language by representing each source token qj
by its probabilistically weighted translations. The
score of target documentD, given source language
query Q, is computed by calculating the Okapi
BM25 rank (Robertson et al, 1998) over projected
term frequency and document frequency weights
as follows:
score(D|Q) =
|Q|?
j=1
bm25(tf(qj , D), df(qj))
tf(q,D) =
|Tq|?
i=1
tf(ti, D)P (ti|q)
df(q) =
|Tq|?
i=1
df(ti)P (ti|q)
where Tq = {t|P (t|q) > L} is the set of trans-
lation options for query term q with probability
greater than L. Following Ture et al (2012a;
2012) we impose a cumulative thresholdC, so that
only the most probable options are added until C
is reached.
Like Ture et al (2012a; 2012) we achieved best
retrieval performance when translation probabil-
ities are calculated as an interpolation between
(context-free) lexical translation probabilities Plex
estimated on symmetrized word alignments, and
(context-aware) translation probabilities Pnbest es-
timated on the n-best list of an SMT decoder:
P (t|q) = ?Pnbest(t|q) + (1? ?)Plex(t|q) (1)
Pnbest(t|q) is the decoder?s confidence to trans-
late q into t within the context of query Q. Let
ak(t, q) be a function indicating an alignment of
target term t to source term q in the k-th deriva-
tion of query Q. Then we can estimate Pnbest(t|q)
as follows:
Pnbest(t|q) =
?n
k=1 ak(t, q)D(k,Q)?n
k=1 ak(?, q)D(k,Q)
(2)
D(k,Q) is the model score of the k-th derivation
in the n-best list for query Q.
In our work, we use hierarchical phrase-based
translation (Chiang, 2007), as implemented in the
cdec framework (Dyer et al, 2010). This allows
us to extract word alignments between source and
target text for Q from the SCFG rules used in the
derivation. The concept of self-translation is cov-
ered by the decoder?s ability to use pass-through
rules if words or phrases cannot be translated.
3.2 Task Alternation in CLIR
The key idea of our approach is to iteratively al-
ternate between the tasks of retrieval and trans-
lation for efficient mining of parallel sentences.
We allow the initial general-domain CLIR model
to adapt to in-domain data over multiple itera-
tions. Since our set of in-domain queries was
small (see 4.2), we trained an adapted SMT model
on the concatenation of general-domain sentences
and in-domain sentences retrieved in the step be-
fore, rather than working with separate models.
Algorithm 1 shows the iterative task alternation
procedure. In terms of semi-supervised learning,
we can view algorithm 1 as non-persistent as we
do not keep labels/pairs from previous iterations.
We have tried different variations of label persis-
tency but did not find any improvements. A sim-
ilar effect of preventing the SMT model to ?for-
get? general-domain knowledge across iterations
is achieved by mixing models from current and
previous iterations. This is accomplished in two
ways: First, by linearly interpolating the transla-
tion option weights P (t|q) from the current and
324
Algorithm 1 Task Alternation
Require: source language TweetsQsrc, target language TweetsDtrg , general-domain parallel sentences Sgen, general-domain
SMT model Mgen, interpolation parameter ?
procedure TASK-ALTERNATION(Qsrc, Dtrg, Sgen,Mgen, ?)
t? 1
while true do
Sin ? ? . Start with empty parallel in-domain sentences
if t == 1 then
M (t)clir ?Mgen . Start with general-domain SMT model for CLIRelse
M (t)clir ? ?M
(t?1)
smt + (1? ?)M (t)smt . Use mixture of previous and current SMT model for CLIRend if
Sin ? CLIR(Qsrc, Dtrg,M (t)clir) . Retrieve top 1 target language Tweets for each source language query
M (t+1)smt ? TRAIN(Sgen + Sin) . Train SMT model on general-domain and retrieved in-domain data
t? t+ 1
end while
end procedure
BLEU (test) # of in-domain sents
Standard DA 14.05 -
Full-scale CLIR 14.97 3,198,913
Task alternation 15.31 ?40k
Table 1: Standard Domain Adaptation with in-domain LM
and tuning; Full-scale CLIR yielding over 3M in-domain par-
allel sentences; Task alternation (? = 0.1, iteration 7) using
?40k parallel sentences per iteration.
previous model with interpolation parameter ?.
Second, by always using Plex(t|q) weights esti-
mated from word alignments on Sgen.
We experimented with different ways of using
the ranked retrieval results for each query and
found that taking just the highest ranked docu-
ment yielded the best results. This returns one pair
of parallel Twitter messages per query, which are
then used as additional training data for the SMT
model in each iteration.
4 Experiments
4.1 Data
We trained the general domain model Mgen on
data from the NIST evaluation campaign, includ-
ing UN reports, newswire, broadcast news and
blogs. Since we were interested in relative im-
provements rather than absolute performance, we
sampled 1 million parallel sentences Sgen from the
originally over 5.8 million parallel sentences.
We used a large corpus of Twitter messages,
originally created by Jehl et al (2012), as com-
parable in-domain data. Language identification
was carried out with an off-the-shelf tool (Lui and
Baldwin, 2012). We kept only Tweets classified
as Arabic or English with over 95% confidence.
After removing duplicates, we obtained 5.5 mil-
lion Arabic Tweets and 3.7 million English Tweets
(Dtrg). Jehl et al (2012) also supply a set of 1,022
Arabic Tweets with 3 English translations each for
evaluation purposes, which was created by crowd-
sourcing translation on Amazon Mechanical Turk.
We randomly split the parallel sentences into 511
sentences for development and 511 sentences for
testing. All URLs and user names in Tweets were
replaced by common placeholders. Hashtags were
kept, since they might be helpful in the retrieval
step. Since the evaluation data do not contain any
hashtags, URLs or user names, we apply a post-
processing step after decoding in which we re-
move those tokens.
4.2 Transductive Setup
Our method can be considered transductive in two
ways. First, all Twitter data were collected by
keyword-based crawling. Therefore, we can ex-
pect a topical similarity between development, test
and training data. Second, since our setup aims
for speed, we created a small set of queries Qsrc,
consisting of the source side of the evaluation data
and similar Tweets. Similarity was defined by
two criteria: First, we ranked all Arabic Tweets
with respect to their term overlap with the devel-
opment and test Tweets. Smoothed per-sentence
BLEU (Lin and Och, 2004) was used as a similar-
ity metric. OOV-coverage served as a second cri-
terion to remedy the problem of unknown words
in Twitter translation. We first created a general
list of all OOVs in the evaluation data under Mgen
(3,069 out of 7,641 types). For each of the top 100
BLEU-ranked Tweets, we counted OOV-coverage
with respect to the corresponding source Tweet
and the general OOV list. We only kept Tweets
325
0 1 2 3 4 5 6 7 8iteration
14.05
14.97
15.31
16.00
BLE
U (t
est)
(a)
?=0.0?=0.1?=0.5?=0.9
1 2 3 4 5 6 7 8iteration 0
10000
20000
30000
40000
50000
60000
70000
# n
ew p
airs
(b)
?=0.0?=0.1?=0.5?=0.9
Figure 1: Learning curves for varying ? parameters. (a) BLEU scores and (b) number of new pairs added per iteration.
containing at least one OOV term from the corre-
sponding source Tweet and two OOV terms from
the general list, resulting in 65,643 Arabic queries
covering 86% of all OOVs. Our query set Qsrc
performed better (14.76 BLEU) after one iteration
than a similar-sized set of random queries (13.39).
4.3 Experimental Results
We simulated the full-scale retrieval approach by
Jehl et al (2012) with the CLIR model described
in section 3. It took 14 days to run 5.5M Arabic
queries on 3.7M English documents. In contrast,
our iterative approach completed a single iteration
in less than 24 hours.1
In the absence of a Twitter data set for re-
trieval, we selected the parameters ? = 0.6 (eq.1),
L = 0.005 and C = 0.95 in a mate-finding
task on Wikipedia data. The n-best list size for
Pnbest(t|q) was 1000. All SMT models included
a 5-gram language model built from the English
side of the NIST data plus the English side of the
Twitter corpus Dtrg. Word alignments were cre-
ated using GIZA++ (Och and Ney, 2003). Rule
extraction and parameter tuning (MERT) was car-
ried out with cdec, using standard features. We
ran MERT 5 times per iteration, carrying over the
weights which achieved median performance on
the development set to the next iteration.
Table 1 reports median BLEU scores on test of
our standard adaptation baseline, the full-scale re-
trieval approach and the best result from our task
alternation systems. Approximate randomization
tests (Noreen, 1989; Riezler and Maxwell, 2005)
showed that improvements of full-scale retrieval
and task alternation over the baseline were statis-
1Retrieval was done in 4 batches on a Hadoop cluster us-
ing 190 mappers at once.
tically significant. Differences between full-scale
retrieval and task alternation were not significant.2
Figure 1 illustrates the impact of ?, which con-
trols the importance of the previous model com-
pared to the current one, on median BLEU (a) and
change of Sin (b) over iterations. For all ?, few
iterations suffice to reach or surpass full-scale re-
trieval performance. Yet, no run achieved good
performance after one iteration, showing that the
transductive setup must be combined with task al-
ternation to be effective. While we see fluctuations
in BLEU for all ?-values, ? = 0.1 achieves high
scores faster and more consistently, pointing to-
wards selecting a bolder updating strategy. This
is also supported by plot (b), which indicates that
choosing ? = 0.1 leads to faster stabilization in
the pairs added per iteration (Sin). We used this
stabilization as a stopping criterion.
5 Conclusion
We presented a method that makes translation-
based CLIR feasible for mining parallel sentences
from large amounts of comparable data. The key
of our approach is a translation-based high-quality
retrieval model which gradually adapts to the tar-
get domain by iteratively re-training the underly-
ing SMT model on a few thousand parallel sen-
tences retrieved in the step before. The number
of new pairs added per iteration stabilizes to a
few thousand after 7 iterations, yielding an SMT
model that improves 0.35 BLEU points over a
model trained on millions of retrieved pairs.
2Note that our full-scale results are not directly compara-
ble to those of Jehl et al (2012) since our setup uses less than
one fifth of the NIST data, a different decoder, a new CLIR
approach, and a different development and test split.
326
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL?09), Athens, Greece.
Steven Abney. 2008. Semisupervised Learning for
Computational Linguistics. Chapman and Hall.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of the
4th EACL Workshop on Statistical Machine Transla-
tion (WMT?09), Athens, Greece.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2).
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions (ACL?10), Uppsala, Sweden.
Laura Jehl, Felix Hieber, and Stefan Riezler. 2012.
Twitter translation using translation-based cross-
lingual retrieval. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), Montreal, Quebec, Canada.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine trans-
lation. In Proceedings of the Second Workshop on
Statistical Machine Translation, Prague, Czech Re-
public.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
for machine translation. In Proceedings the 20th In-
ternational Conference on Computational Linguis-
tics (COLING?04).
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics, Demo Session
(ACL?12), Jeju, Republic of Korea.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4).
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics (COLING-ACL?06), Sydney,
Australia.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley,
New York.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1).
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Stephen E. Robertson, Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Confer-
ence (TREC-7), Gaithersburg, MD.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?08), Honolulu, Hawaii.
Ferhan Ture and Jimmy Lin. 2012. Why not grab a
free lunch? mining large corpora for parallel sen-
tences to improve translation modeling. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT?12),
Montreal, Canada.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012. Combining statistical translation techniques
for cross-language information retrieval. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING?12), Mumbai, India.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a.
Looking inside the box: Context-sensitive transla-
tion for cross-language information retrieval. In
Proceedings of the ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR?12), Portland, OR.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel doc-
ument mining for machine translation. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (COLING?10), Beijing,
China.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual
information retrieval. In Proceedings of the 24th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR?01), New York, NY.
327
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 881?891,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Response-based Learning for Grounded Machine Translation
Stefan Riezler and Patrick Simianer and Carolin Haas
Department of Computational Linguistics
Heidelberg University, 69120 Heidelberg, Germany
{riezler,simianer,haas1}@cl.uni-heidelberg.de
Abstract
We propose a novel learning approach for
statistical machine translation (SMT) that
allows to extract supervision signals for
structured learning from an extrinsic re-
sponse to a translation input. We show
how to generate responses by grounding
SMT in the task of executing a seman-
tic parse of a translated query against
a database. Experiments on the GEO-
QUERY database show an improvement of
about 6 points in F1-score for response-
based learning over learning from refer-
ences only on returning the correct an-
swer from a semantic parse of a translated
query. In general, our approach alleviates
the dependency on human reference trans-
lations and solves the reachability problem
in structured learning for SMT.
1 Introduction
In this paper, we propose a novel approach
for learning and evaluation in statistical ma-
chine translation (SMT) that borrows ideas from
response-based learning for grounded semantic
parsing. In this framework, the meaning of a sen-
tence is defined in the context of an extrinsic task.
Successful communication of meaning is mea-
sured by a successful interaction in this task, and
feedback from this interaction is used for learning.
We suggest that in a similar way the preser-
vation of meaning in machine translation should
be defined in the context of an interaction in an
extrinsic task. For example, in the context of a
game, a description of a game rule is translated
successfully if correct game moves can be per-
formed based only on the translation. In the con-
text of a question-answering scenario, a question
is translated successfully if the correct answer is
returned based only on the translation of the query.
We propose a framework of response-based
learning that allows to extract supervision signals
for structured learning from the response of an
extrinsic task to a translation input. Here, learn-
ing proceeds by ?trying out? translation hypothe-
ses, receiving a response from interacting in the
task, and converting this response into a supervi-
sion signal for updating model parameters. In case
of positive feedback, the predicted translation can
be treated as reference translation for a structured
learning update. In case of negative feedback, a
structural update can be performed against transla-
tions that have been approved previously by pos-
itive task feedback. This framework has several
advantages:
? The supervision signal in response-based
learning has a different quality than super-
vision by human-generated reference transla-
tions. While a human reference translation
is generated independently of the SMT task,
conversion of predicted translations into ref-
erences is always done with respect to a spe-
cific task. In this sense we speak of ground-
ing meaning transfer in an extrinsic task.
? Response-based learning can repeatedly try
out system predictions by interacting in the
extrinsic task. Instead of and in addition
to learning from human reference transla-
tions, response-based learning allows to con-
vert multiple system translations into refer-
ences. This alleviates the supervision prob-
lem in cases where parallel data are scarce.
? Task-specific response acts upon system
translations. This avoids the problem of un-
reachability of independently generated ref-
erence translations by the SMT system.
The proposed approach of response-based
learning opens the doors for various extrinsic tasks
881
in which SMT systems can be trained and evalu-
ated. In this paper, we present a proof-of-concept
experiment that uses feedback from a simulated
world environment. Building on prior work in
grounded semantic parsing, we generate transla-
tions of queries, and receive feedback by execut-
ing semantic parses of translated queries against
the database. Successful response is defined as re-
ceiving the same answer from the semantic parses
for the translation and the original query. Our ex-
perimental results show an improvement of about
6 points in F1-score for response-based learning
over standard structured learning from reference
translations. We show in an error analysis that
this improvement can be attributed to using struc-
tural and lexical variants of reference translations
as positive examples in response-based learning.
Furthermore, translations produced by response-
based learning are found to be grammatical. This
is due to the possibility to boost similarity to hu-
man reference translations by the additional use of
a cost function in our approach.
2 Related Work
The key idea of grounded language learning
is to study natural language in the context of
a non-linguistic environment, in which meaning
is grounded in perception and/or action. This
presents an analogy to human learning, where a
learner tests her understanding in an actionable
setting. Such a setting can be a simulated world
environment in which the linguistic representa-
tion can be directly executed by a computer sys-
tem. For example, in semantic parsing, the learn-
ing goal is to produce and successfully execute
a meaning representation. Executable system ac-
tions include access to databases such as the GEO-
QUERY database on U.S. geography (Wong and
Mooney (2006), inter alia), the ATIS travel plan-
ning database (Zettlemoyer and Collins (2009),
inter alia), robotic control in simulated naviga-
tion tasks (Chen and Mooney (2011), inter alia),
databases of simulated card games (Goldwasser
and Roth (2013), inter alia), or the user-generated
contents of FREEBASE (Cai and Yates (2013), in-
ter alia). Since there are many possible correct
parses, matching against a single gold standard
falls short of grounding in a non-linguistic envi-
ronment. Rather, the semantic context for inter-
pretation, as well as the success criterion in evalua-
tion is defined by successful execution of an action
in the extrinsic environment, e.g., by receiving the
correct answer from the database or by successful
navigation to the destination. Recent attempts to
learn semantic parsing from question-answer pairs
without recurring to annotated logical forms have
been presented by Kwiatowski et al (2013), Be-
rant et al (2013), or Goldwasser and Roth (2013).
The algorithms presented in these works are vari-
ants of structured prediction that take executability
of semantic parses into account. Our work builds
upon these ideas, however, to our knowledge the
presented work is the first to embed translations
into grounded scenarios in order to use feedback
from interactions in these scenarios for structured
learning in SMT.
A recent important research direction in SMT
has focused on employing automated translation
as an aid to human translators. Computer as-
sisted translation (CAT) subsumes several modes
of interaction, ranging from binary feedback on
the quality of the system prediction (Saluja et
al., 2012), to human post-editing operations on a
system prediction resulting in a reference transla-
tion (Cesa-Bianchi et al, 2008), to human accep-
tance or overriding of sentence completion pre-
dictions (Langlais et al, 2000; Barrachina et al,
2008; Koehn and Haddow, 2009). In all inter-
action scenarios, it is important that the system
learns dynamically from its errors in order to of-
fer the user the experience of a system that adapts
to the provided feedback. Since retraining the
SMT model after each interaction is too costly,
online adaptation after each interaction has be-
come the learning protocol of choice for CAT. On-
line learning has been applied in generative SMT,
e.g., using incremental versions of the EM algo-
rithm (Ortiz-Mart??nez et al, 2010; Hardt and Elm-
ing, 2010), or in discriminative SMT, e.g., using
perceptron-type algorithms (Cesa-Bianchi et al,
2008; Mart??nez-G?omez et al, 2012; W?aschle et
al., 2013; Denkowski et al, 2014). In a simi-
lar way to deploying human feedback, extrinsic
loss functions have been used to provide learn-
ing signals for SMT. For example, Nikoulina et
al. (2012) propose a setup where an SMT system
feeds into cross-language information retrieval,
and receives feedback from the performance of
translated queries with respect to cross-language
retrieval performance. This feedback is used to
train a reranker on an n-best list of translations or-
der with respect to retrieval performance. In con-
882
Figure 1: Response-based learning cycle for grounding SMT in virtual trivia gameplay.
trast to our work, all mentioned approaches to in-
teractive or adaptive learning in SMT rely on hu-
man post-edits or human reference translations.
Our work differs from these approaches in that
exactly this dependency is alleviated by learning
from responses in an extrinsic task.
Interactive scenarios have been used for eval-
uation purposes of translation systems for nearly
50 years, especially using human reading compre-
hension testing (Pfafflin, 1965; Fuji, 1999; Jones
et al, 2005), and more recently, using face-to-
face conversation mediated via machine transla-
tion (Sakamoto et al, 2013). However, despite of-
fering direct and reliable prediction of translation
quality, the cost and lack of reusability has con-
fined task-based evaluations involving humans to
testing scenarios, but prevented a use for interac-
tive training of SMT systems as in our work.
Lastly, our work is related to cross-lingual nat-
ural language processing such as cross-lingual
question answering or cross-lingual information
retrieval as conducted at recent evaluation cam-
paigns of the CLEF initiative.
1
While these ap-
proaches focus on improvements of the respective
natural language processing task, our goal is to im-
prove SMT by gathering feedback from the task.
1
http://www.clef-initiative.eu
3 Grounding SMT in Semantic Parsing
In this paper, we present a proof-of-concept of our
ideas of embedding SMT into simulated world en-
vironments as used in semantic parsing. We use
the well-known GEOQUERY database on U.S. ge-
ography for this purpose. Embedding SMT in a
semantic parsing scenario means to define transla-
tion quality by the ability of a semantic parser to
construct a meaning representation from the trans-
lated query, which returns the correct answer when
executed against the database. If viewed as simu-
lated gameplay, a valid game move in this scenario
returns the correct answer to a translated query.
The diagram in Figure 1 gives a sketch of
response-based learning from semantic parsing in
the geographical domain. Given a manual Ger-
man translation of the English query as source sen-
tence, the SMT system produces an English target
translation. This sentence is fed into a semantic
parser that produces an executable parse represen-
tation p
h
. Feedback is generated by executing the
parse against the database of geographical facts.
Positive feedback means that the correct answer is
received, i.e., exec(p
g
)
?
= exec(p
h
) indicates that
the same answer is received from the gold standard
parse p
g
and the parse for the hypothesis transla-
tion p
h
; negative feedback results in case a differ-
ent or no answer is received.
The key advantage of response-based learning
883
is the possibility to receive positive feedback even
from predictions that differ from gold standard
reference translations, but yet receive the cor-
rect answer when parsed and matched against the
database. Such structural and lexical variation
broadens the learning capabilities in contrast to
learning from fixed labeled data. For example,
assume the following English query in the geo-
graphical domain, and assume positive feedback
from executing the corresponding semantic parse
against the geographical database:
Name prominent elevations in the
USA
The manual translation of the English original
reads
Nenne prominente Erhebungen in
den USA
An automatic translation
2
of the German string
produces the result
Give prominent surveys in the US
This translation will trigger negative task-based
feedback: A comparison with the original allows
the error to be traced back to the ambiguity of
the German word Erhebung. Choosing a gen-
eral domain translation instead of a translation ap-
propriate for the geographical domain hinders the
construction of a semantic parse that returns the
correct answer from the database. An alternative
translation might look as follows:
Give prominent heights in the US
Despite a large difference to the original En-
glish string, key terms such as elevations and
heights, or USA and US, can be mapped into the
same predicate in the semantic parse, thus allow-
ing to receive positive feedback from parse execu-
tion against the geographical database.
4 Response-based Online Learning
Recent approaches to machine learning for SMT
formalize the task of discriminating good from
bad translations as a structured prediction prob-
lem. Assume a joint feature representation ?(x, y)
of input sentences x and output translations y ?
Y (x), and a linear scoring function s(x, y;w) for
predicting a translation y? (where ??, ?? denotes the
standard vector dot product) s.t.
y? = argmax
y?Y (x)
s(x, y;w) = argmax
y?Y (x)
?w, ?(x, y)? .
2
http://translate.google.com
The structured perceptron algorithm (Collins,
2002) learns an optimal weight vector w by updat-
ing w on input x
(i)
by the following rule, in case
the predicted translation y? is different from and
scored higher than the reference translation y
(i)
:
w = w + ?(x
(i)
, y
(i)
)? ?(x
(i)
, y?).
This stochastic structural update aims to demote
weights of features corresponding to incorrect de-
cisions, and to promote weights of features for cor-
rect decisions.
An application of structured prediction to SMT
involves more than a straightforward replacement
of labeled output structures by reference transla-
tions. Firstly, update rules that require to com-
pute a feature representation for the reference
translation are suboptimal in SMT, because of-
ten human-generated reference translations can-
not be generated by the SMT system. Such ?un-
reachable? gold-standard translations need to be
replaced by ?surrogate? gold-standard translations
that are close to the human-generated translations
and still lie within the reach of the SMT sys-
tem. Computation of distance to the reference
translation usually involves cost functions based
on sentence-level BLEU (Nakov et al (2012), in-
ter alia) and incorporates the current model score,
leading to various ramp loss objectives described
in Gimpel and Smith (2012).
An alternative approach to alleviate the depen-
dency on labeled training data is response-based
learning. Clarke et al (2010) or Goldwasser and
Roth (2013) describe a response-driven learning
framework for the area of semantic parsing: Here
a meaning representation is ?tried out? by itera-
tively generating system outputs, receiving feed-
back from world interaction, and updating the
model parameters. Applied to SMT, this means
that we predict translations and use positive re-
sponse from acting in the world to create ?surro-
gate? gold-standard translations. This decreases
the dependency on a few (mostly only one) refer-
ence translations and guides the learner to promote
translations that perform well with respect to the
extrinsic task.
In the following, we will present a framework
that combines standard structured learning from
given reference translations with response-based
learning from task-approved references. We need
to ensure that gold-standard translations lead to
positive task-based feedback, that means they can
884
be parsed and executed successfully against the
database. In addition, we can use translation-
specific cost functions based on sentence-level
BLEU in order to boost similarity of translations
to human reference translations.
We denote feedback by a binary execution func-
tion e(y) ? {1, 0} that tests whether executing
the semantic parse for the prediction against the
database receives the same answer as the parse
for the gold standard reference. Our cost function
c(y
(i)
, y) = (1?BLEU(y
(i)
, y)) is based on a ver-
sion of sentence-level BLEU Nakov et al (2012).
Define y
+
as a surrogate gold-standard translation
that receives positive feedback, has a high model
score, and a low cost of predicting y instead of
y
(i)
:
y
+
= argmax
y?Y (x
(i)
):e(y)=1
(
s(x
(i)
, y;w)? c(y
(i)
, y)
)
.
The opposite of y
+
is the translation y
?
that leads
to negative feedback, has a high model score, and
a high cost. It is defined as follows:
y
?
= argmax
y?Y (x
(i)
):e(y)=0
(
s(x
(i)
, y;w) + c(y
(i)
, y)
)
.
Update rules can be derived by minimization of
the following ramp loss objective:
min
w
(
? max
y?Y (x
(i)
):e(y)=1
(
s(x
(i)
, y;w)? c(y
(i)
, y)
)
+ max
y?Y (x
(i)
):e(y)=0
(
s(x
(i)
, y;w) + c(y
(i)
, y)
)
)
.
Minimization of this objective using stochastic
(sub)gradient descent (McAllester and Keshet,
2011) yields the following update rule:
w = w + ?(x
(i)
, y
+
)? ?(x
(i)
, y
?
).
The intuition behind this update rule is to discrim-
inate the translation y
+
that leads to positive feed-
back and best approximates (or is identical to) the
reference within the means of the model from a
translation y
?
which is favored by the model but
does not execute and has high cost. This is done
by putting all the weight on the former.
Algorithm 1 presents pseudo-code for our
response-driven learning scenario. Upon predict-
ing translation y?, in case of positive feedback from
the task, we treat the prediction as surrogate refer-
ence by setting y
+
? y?, and by adding it to the
set of reference translations for future use. Then
we need to compute y
?
, and update by the differ-
ence in feature representations of y
+
and y
?
, at
a learning rate ?. If the feedback is negative, we
want to move the weights away from the predic-
tion, thus we treat it as y
?
. To perform an update,
we need to compute y
+
. If either y
+
or y
?
cannot
be computed, the example is skipped.
Algorithm 1 Response-based Online Learning
repeat
for i = 1, . . . , n do
Receive input string x
(i)
Predict translation y?
Receive task feedback e(y?) ? {1, 0}
if e(y?) = 1 then
y
+
? y?
Store y? as reference y
(i)
for x
(i)
Compute y
?
else
y
?
? y?
Receive reference y
(i)
Compute y
+
end if
w ? w + ?(?(x
(i)
, y
+
)? ?(x
(i)
, y
?
))
end for
until Convergence
The sketched algorithm allows several varia-
tions. In the form depicted above, it allows
to use human reference translations in addition
to task-approved surrogate references. The cost
function can be implemented by different ver-
sions of sentence-wise BLEU, or it can be omitted
completely so that learning relies on task-based
feedback alone, similar to algorithms recently
suggested for semantic parsing (Goldwasser and
Roth, 2013; Kwiatowski et al, 2013; Berant et
al., 2013). Lastly, regularization can be intro-
duced by using update rules corresponding to pri-
mal form optimization variants of support vector
machines (Collobert and Bengio, 2004; Chapelle,
2007; Shalev-Shwartz et al, 2007).
5 Experiments
5.1 Experimental Setup
In our experiments, we use the GEOQUERY
database on U.S. geography as provided by Jones
885
method precision recall F1 BLEU
1
CDEC 63.67 58.21 60.82 46.53
2
EXEC 70.36 63.57 66.79
1
48.00
1
3
RAMPION 75.58 69.64 72.49
12
56.64
12
4
REBOL 81.15 75.36 78.15
123
55.66
12
Table 1: Experimental results using extended parser for returning answers from GEOQUERY (precision,
recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples. Best
results for each column are highlighted in bold face. Superscripts
1234
denote a significant improvement
over the respective method.
method precision recall F1 BLEU
1
CDEC 65.59 57.86 61.48 46.53
2
EXEC 66.54 61.79 64.07 46.00
3
RAMPION 67.68 63.57 65.56 55.67
12
4
REBOL 70.68 67.14 68.86
12
55.67
12
Table 2: Experimental results using the original parser for returning answers from GEOQUERY (preci-
sion, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples.
et al (2012).
3
The dataset includes 880 English
questions and their logical forms. The English
strings were manually translated into German by
the authors of Jones et al (2012)), and corrected
for typos by the authors of this paper. We follow
the provided split into 600 training examples and
280 test examples.
For response-based learning, we retrained the
semantic parser of Andreas et al (2013)
4
on the
full 880 GEOQUERY examples in order to reach
full parse coverage. This parser is itself based on
SMT, trained on parallel data consisting of English
queries and linearized logical forms, and on a lan-
guage model trained on linearized logical forms.
We used the hierarchical phrase-based variant of
the parser. Note that we do not use GEOQUERY
test data in SMT training. Parser training includes
GEOQUERY test data in order to be less depen-
dent on parse and execution failures in the eval-
uation: If a translation system, response-based or
reference-based, translates the German input into
the gold standard English query it should be re-
warded by positive task feedback. To double-
check whether including the 280 test examples
in parser training gives an unfair advantage to
response-based learning, we also present experi-
mental results using the original parser of Andreas
3
http://homepages.inf.ed.ac.uk/
s1051107/geoquery-2012-08-27.zip
4
https://github.com/jacobandreas/
smt-semparse
et al (2013) that is trained only on the 600 GEO-
QUERY training examples.
The bilingual SMT system used in our experi-
ments is the state-of-the-art SCFG decoder CDEC
(Dyer et al, 2010)
5
. We built grammars us-
ing its implementation of the suffix array extrac-
tion method described in Lopez (2007). For lan-
guage modeling, we built a modified Kneser-Ney
smoothed 5-gram language model using the En-
glish side of the training data. We trained the SMT
system on the English-German parallel web data
provided in the COMMON CRAWL
6
(Smith et al,
2013) dataset.
5.2 Compared Systems
Method 1 is the baseline system, consisting of
the CDEC SMT system trained on the COMMON
CRAWL data as described above. This system does
not use any GEOQUERY data for training. Meth-
ods 2-4 use the 600 training examples from GEO-
QUERY for discriminative training only.
Variants of the response-based learning algo-
rithm described above are implemented as a stand-
alone tool that operates on CDEC n-best lists of
10,000 translations of the GEOQUERY training
data. All variants use sparse features of CDEC as
described in Simianer et al (2012) that extract rule
5
https://github.com/redpony/cdec
6
http://www.statmt.org/wmt13/
training-parallel-commoncrawl.tgz
886
prediction: how many inhabitants has new york
reference: how many people live in new york
prediction: how big is the population of texas
reference: how many people live in texas
prediction: which are the cities of the state with the highest elevation
reference: what are the cities of the state with the highest point
prediction: how big is the population of states , through which the mississippi runs
reference: what are the populations of the states through which the mississippi river runs
prediction: what state borders california
reference: what is the adjacent state of california
prediction: what are the capitals of the states which have cities with the name durham
reference: what is the capital of states that have cities named durham
prediction: what rivers go through states with the least cities
reference: which rivers run through states with fewest cities
Table 3: Predicted translations by response-based learning (REBOL) leading to positive feedback versus
gold standard references.
shapes, rule identifiers, and bigrams in rule source
and target directly from grammar rules. Method
4, named REBOL, implements REsponse-Based
Online Learning by instantiating y
+
and y
?
to
the form described in Section 4: In addition to
the model score s, it uses a cost function c based
on sentence-level BLEU (Nakov et al, 2012) and
tests translation hypotheses for task-based feed-
back using a binary execution function e. This
algorithm can convert predicted translations into
references by task-feedback, and additionally use
the given original English queries as references.
Method 2, named EXEC, relies on task-execution
by function e and searches for executable or non-
executable translations with highest score s to dis-
tinguish positive from negative training examples.
It does not use a cost function and thus cannot
make use of the original English queries.
We compare response-based learning with a
standard structured prediction setup that omits the
use of the execution function e in the definition
of y
+
and y
?
. This algorithm can be seen as a
stochastic (sub)gradient descent variant of RAM-
PION (Gimpel and Smith, 2012). It does not make
use of the semantic parser, but defines positive and
negative examples based on score s and cost cwith
respect to human reference translations.
We report BLEU (Papineni et al, 2001) of
translation system output measured against the
original English queries. Furthermore, we report
precision, recall, and F1-score for executing se-
mantic parses built from translation system out-
puts against the GEOQUERY database. Precision
is defined as the percentage of correctly answered
examples out of those for which a parse could be
produced; recall is defined as the percentage of to-
tal examples answered correctly; F1-score is the
harmonic mean of both. Statistical significance
is measured using Approximate Randomization
(Noreen, 1989) where result differences with a p-
value smaller than 0.05 are considered statistically
significant.
Methods 2-4 perform structured learning for
SMT on the 600 GEOQUERY training examples
and re-translate the 280 unseen GEOQUERY test
data, following the data split of Jones et al (2012).
Training for RAMPION, REBOL and EXEC was re-
peated for 10 epochs. The learning rate ? is set to
a constant that is adjusted by cross-validation on
the 600 training examples.
5.3 Empirical Results
We present an experimental comparison of the
four different systems according to BLEU and
887
reference RAMPION REBOL
how many colorado rivers are
there
how many rivers with the name
colorado gives it
how many rivers named col-
orado are there
what are the populations of
states which border texas
how big are the populations of
the states , which in texas bor-
ders
how big are the populations of
the states which on texas border
what is the biggest capital city in
the us
what is the largest city in the usa what is the largest capital in the
usa
what state borders new york what states limits of new york what states border new york
which states border the state
with the smallest area
what states boundaries of the
state with the smallest surface
area
what states border the state with
the smallest surface area
Table 4: Predicted translations by response-based learning (REBOL) leading to positive feedback versus
translations by supervised structured learning (RAMPION) leading to negative feedback.
F1, using an extended semantic parser (trained
on 880 GEOQUERY examples) and the original
parser (trained on 600 GEOQUERY training exam-
ples). The extended parser reaches and F1-score
of 99.64% on the 280 GEOQUERY test examples;
the original parser yields an F1-score of 82.76%.
Table 1 reports results for the extended seman-
tic parser. A system ranking according to F1-
score shows about 6 points difference between the
respective methods, ranking REBOL over RAM-
PION, EXEC and CDEC. The exploitation of task-
feedback allows both EXEC and REBOL to im-
prove task-performance over the baseline. RE-
BOL?s combination of task feedback with a cost
function achieves the best results since positively
executable hypotheses and reference translations
can both be exploited to guide the learning pro-
cess. Since all English reference queries lead to
positively executable parses in the setup that uses
the extended semantic parser, RAMPION implic-
itly also has access to task feedback. This allows
RAMPION to improve F1 over the baseline. All
result differences are statistically significant.
In terms of BLEU score measured against the
original English GEOQUERY queries, the best
nominal result is obtained by RAMPION which
uses them as reference translations. REBOL per-
forms worse since BLEU performance is opti-
mized only implicitly in cases where original En-
glish queries function as positive examples. How-
ever, the result differences between these two
systems do not score as statistically significant.
Despite not optimizing for BLEU performance
against references, the fact that positively exe-
cutable translations include the references allows
even EXEC to improve BLEU over CDEC which
does not use GEOQUERY data at all in training.
This result difference is statistically significant.
Table 2 compares the same systems using the
original parser trained on 600 training examples.
The system ranking according to F1-score shows
the same ordering that is obtained when using an
extended semantic parser. However, the respec-
tive methods are separated only by 3 or less points
in F1 score such that only the result difference of
REBOL over the baseline CDEC and over EXEC is
statistically significant. We conjecture that this is
due to a higher number of empty parses on the test
set which makes this comparison unstable.
In terms of BLEU measured against the original
queries, the result differences between REBOL and
RAMPION are not statistically significant, and nei-
ther are the result differences between EXEC and
CDEC. The result differences between systems of
the former group and the systems of latter group
are statistically significant.
5.4 Error Analysis
For a better understanding of the differences be-
tween the results produced by supervised and
response-based learning, we conducted an er-
888
reference RAMPION REBOL
how many states have a higher
point than the highest point of
the state with the largest capital
city in the us
how many states have a higher
nearby point as the highest point
of the state with the largest capi-
tal in the usa
how many states have a high
point than the highest point of
the state with the largest capital
in the usa
how tall is mount mckinley how high is mount mckinley what is mount mckinley
what is the longest river that
flows through a state that borders
indiana
how is the longest river , which
runs through a state , borders the
of indiana
what is the longest river which
runs through a state of indiana
borders
what states does the mississippi
river run through
through which states runs the
mississippi
through which states is the mis-
sissippi
which is the highest peak not in
alaska
how is the highest peaks of not
in alaska is
what is the highest peak in
alaska is
Table 5: Predicted translations where supervised structured learning (RAMPION) leads to positive feed-
back versus translations by response-based learning (REBOL) leading to negative feedback.
ror analysis on the test examples. Table 3
shows examples where the translation predicted by
response-based learning (REBOL) differs from the
gold standard reference translation, but yet leads
to positive feedback via a parse that returns the
correct answer from the database. The examples
show structural and lexical variation that leads to
differences on the string level at equivalent posi-
tive feedback from the extrinsic task. This can ex-
plain the success of response-based learning: Lex-
ical and structural variants of reference transla-
tions can be used to boost model parameters to-
wards translations with positive feedback, while
the same translations might be considered as neg-
ative examples in standard structured learning.
Table 4 shows examples where translations
from REBOL and RAMPION differ from the gold
standard reference, and predictions by REBOL
lead to positive feedback, while predictions by
RAMPION lead to negative feedback. Table 5
shows examples where translations from RAM-
PION outperform translations from REBOL in
terms of task feedback. We see that predictions
from both systems are in general grammatical.
This can be attributed to the use of sentence-
level BLEU as cost function in RAMPION and
REBOL. Translation errors of RAMPION can be
traced back to mistranslations of key terms (city
versus capital, limits or boundaries versus
border). Translation errors of REBOL more fre-
quently show missing translations of terms.
6 Conclusion
We presented a proposal for a new learning and
evaluation framework for SMT. The central idea
is to ground meaning transfer in successful in-
teraction in an extrinsic task, and use task-based
feedback for structured learning. We presented a
proof-of-concept experiment that defines the ex-
trinsic task as executing semantic parses of trans-
lated queries against the GEOQUERY database.
Our experiments show an improvement of about
6 points in F1-score for response-based learning
over structured learning from reference transla-
tions. Our error analysis shows that response-
based learning generates grammatical translations
which is due to the additional use of a cost func-
tion that boosts similarity of translations to human
reference translations.
In future work, we would like to extend our
work on embedding SMT in virtual gameplay to
larger and more diverse datasets, and involve hu-
man feedback in the response-based learning loop.
References
Jacob Andreas, Andreas Vlachos, and Stephen Clark.
2013. Semantic parsing as machine translation. In
889
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (ACL?13),
Sofia, Bulgaria.
Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,
Antonio Lagarda, Hermann Ney, Jes?us Tom?as, En-
rique Vidal, and Juan-Miguel Vilar. 2008. Sta-
tistical approaches to computer-assisted translation.
Computational Linguistics, 35(1):3?28.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?13), Seattle, WA.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extenstion. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL?13), Sofia, Bulgaria.
Nicol`o Cesa-Bianchi, Gabriele Reverberi, and San-
dor Szedmak. 2008. Online learning algorithms
for computer-assisted translation. Technical report,
SMART (www.smart-project.eu).
Olivier Chapelle. 2007. Training a support vec-
tor machine in the primal. Neural Computation,
19(5):1155?1178.
David L. Chen and Raymond J. Mooney. 2011.
Learning to interpret natural language navigation
instructions from observations. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence
(AAAI?11), pages 859?866, San Francisco, CA.
James Clarke, Dan Goldwasser, Wing-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of the 14th Con-
ference on Natural Language Learning (CoNLL?10),
pages 18?27, Uppsala, Sweden.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the conference on Empirical Methods in Nat-
ural Language Processing (EMNLP?02), Philadel-
phia, PA.
Ronan Collobert and Samy Bengio. 2004. Links be-
tween perceptrons, MLPs, and SVMs. In Proceed-
ings of the 21st International Conference on Ma-
chine Learning (ICML?04), Banff, Canada.
Michael Denkowski, Chris Dyer, and Alon Lavie.
2014. Learning from post-editing: Online model
adaptation for statistical machine translation. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL?14), Gothenburg, Sweden.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, Uppsala, Sweden.
Masaru Fuji. 1999. Evaluation experiment for reading
comprehension of machine translation outputs. In
Proceedings of the Machine Translation Summit VII,
Singapore.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation.
In Proceedings of 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2012), Montreal, Canada.
Dan Goldwasser and Dan Roth. 2013. Learning from
natural instructions. Machine Learning, 94(2):205?
232.
Daniel Hardt and Jakob Elming. 2010. Incremental
re-training for post-editing SMT. In Proceedings of
the 9th Conference of the Association for Machine
Tranlation in the Americas (AMTA?10), Denver, CO.
Douglas Jones, Wade Shen, Neil Granoien, Martha
Herzog, and Clifford Weinstein. 2005. Measuring
translation quality by testing english speakers with
a new defense language proficiency test for arabic.
In Proceedings of 2005 International Conference on
Intelligence Analysis, McLean, VA.
Bevan K. Jones, Mark Johnson, and Sharon Goldwater.
2012. Semantic parsing with bayesion tree trans-
ducers. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?12), Jeju Island, Korea.
Philipp Koehn and Barry Haddow. 2009. Interactive
assistance to human translators using statistical ma-
chine translation methods. In Proceedings of MT
Summit XII, Ottawa, Ontario, Canada.
Tom Kwiatowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP?13), Seattle, WA.
Philippe Langlais, George Foster, and Guy Lapalme.
2000. Transtype: a computer-aided translation typ-
ing system. In Proceedings of the ANLP-NAACL
2000 Workshop on Embedded Machine Translation
Systems, Seattle, WA.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2007), Prague,
Czech Republic.
Pascual Mart??nez-G?omez, Germ?an Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
890
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45(9):3193?
3202.
David McAllester and Joseph Keshet. 2011. General-
ization bounds and consistency for latent structural
probit and ramp loss. In Proceedings of the 25th An-
nual Conference on Neural Information Processing
Sytems (NIPS 2011), Granada, Spain.
Preslav Nakov, Francisco Guzm?an, and Stephan Vogel.
2012. Optimizing for sentence-level bleu+1 yields
short translations. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics
(COLING 2012), Bombay, India.
Vassilina Nikoulina, Bogomil Kovachev, Nikolaos La-
gos, and Christof Monz. 2012. Adaptation of statis-
tical machine translation model for cross-lingual in-
formation retrieval in a service context. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL?12), Avignon, France.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley,
New York.
Daniel Ortiz-Mart??nez, Ismal Garc??a-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for in-
teractive statistical machine translation. In Proceed-
ings of the Human Language Technologies confer-
ence and the 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL?10), Los Angeles,
CA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
IBM Research Division Technical Report, RC22176
(W0190-022), Yorktown Heights, N.Y.
Sheila M. Pfafflin. 1965. Evaluation of machine trans-
lations by reading comprehension tests and subjec-
tive judgements. Mechanical Translation and Com-
putational Linguistics, 8(2):2?8.
Akiko Sakamoto, Nayuko Watanabe, Satoshi Ka-
matani, and Kazuo Sumita. 2013. Development of a
simultaneous interpretation system for face-to-face
services and its evaluation experiment in real situ-
ation. In Proceedings of the Machine Translation
Summit XIV, Nice, France.
Avneesh Saluja, Ian Lane, and Ying Zhang. 2012.
Machine translation with binary feedback: A large-
margin approach. In Proceedings of the 10th Bi-
ennial Conference of the Association for Machine
Translation in the Americas (AMTA?12), San Diego,
CA.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2007. Pegasos: Primal Estimated sub-
GrAdient SOlver for SVM. In Proceedings of the
24th International Conference on Machine Learning
(ICML?07), Corvallis, OR.
Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012. Joint feature selection in distributed stochas-
tic learning for large-scale discriminative training in
SMT. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (ACL
2012), Jeju, Korea.
Jason R. Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale paral-
lel text from the common crawl. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL?13), Sofia, Bulgaria.
Katharina W?aschle, Patrick Simianer, Nicola Bertoldi,
Stefan Riezler, and Marcello Federico. 2013. Gen-
erative and discriminative methods for online adap-
tation in SMT. In Proceedings of the Machine
Translation Summit XIV, Nice, France.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT/NAACL?06), New York City, NY.
Luke S. Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-IJCNLP?09), Singapore.
891
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 488?494,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Translational and Knowledge-based Similarities
from Relevance Rankings for Cross-Language Retrieval
Shigehiko Schamoni and Felix Hieber and Artem Sokolov and Stefan Riezler
Department of Computational Linguistics
Heidelberg University, 69120 Heidelberg, Germany
{schamoni,hieber,sokolov,riezler}@cl.uni-heidelberg.de
Abstract
We present an approach to cross-language
retrieval that combines dense knowledge-
based features and sparse word transla-
tions. Both feature types are learned di-
rectly from relevance rankings of bilin-
gual documents in a pairwise ranking
framework. In large-scale experiments for
patent prior art search and cross-lingual re-
trieval in Wikipedia, our approach yields
considerable improvements over learning-
to-rank with either only dense or only
sparse features, and over very competitive
baselines that combine state-of-the-art ma-
chine translation and retrieval.
1 Introduction
Cross-Language Information Retrieval (CLIR) for
the domain of web search successfully lever-
ages state-of-the-art Statistical Machine Transla-
tion (SMT) to either produce a single most prob-
able translation, or a weighted list of alternatives,
that is used as search query to a standard search
engine (Chin et al, 2008; Ture et al, 2012). This
approach is advantageous if large amounts of in-
domain sentence-parallel data are available to train
SMT systems, but relevance rankings to train re-
trieval models are not.
The situation is different for CLIR in special
domains such as patents or Wikipedia. Paral-
lel data for translation have to be extracted with
some effort from comparable or noisy parallel data
(Utiyama and Isahara, 2007; Smith et al, 2010),
however, relevance judgments are often straight-
forwardly encoded in special domains. For ex-
ample, in patent prior art search, patents granted
at any patent office worldwide are considered rel-
evant if they constitute prior art with respect to
the invention claimed in the query patent. Since
patent applicants and lawyers are required to list
relevant prior work explicitly in the patent appli-
cation, patent citations can be used to automati-
cally extract large amounts of relevance judgments
across languages (Graf and Azzopardi, 2008). In
Wikipedia search, one can imagine a Wikipedia
author trying to investigate whether a Wikipedia
article covering the subject the author intends to
write about already exists in another language.
Since authors are encouraged to avoid orphan arti-
cles and to cite their sources, Wikipedia has a rich
linking structure between related articles, which
can be exploited to create relevance links between
articles across languages (Bai et al, 2010).
Besides a rich citation structure, patent docu-
ments and Wikipedia articles contain a number
of further cues on relatedness that can be ex-
ploited as features in learning-to-rank approaches.
For monolingual patent retrieval, Guo and Gomes
(2009) and Oh et al (2013) advocate the use of
dense features encoding domain knowledge on
inventors, assignees, location and date, together
with dense similarity scores based on bag-of-word
representations of patents. Bai et al (2010) show
that for the domain of Wikipedia, learning a sparse
matrix of word associations between the query and
document vocabularies from relevance rankings is
useful in monolingual and cross-lingual retrieval.
Sokolov et al (2013) apply the idea of learning
a sparse matrix of bilingual phrase associations
from relevance rankings to cross-lingual retrieval
in the patent domain. Both show improvements
of learning-to-rank on relevance data over SMT-
based approaches on their respective domains.
The main contribution of this paper is a thor-
ough evaluation of dense and sparse features
for learning-to-rank that have so far been used
only monolingually or only on either patents or
Wikipedia. We show that for both domains,
patents and Wikipedia, jointly learning bilingual
sparse word associations and dense knowledge-
based similarities directly on relevance ranked
488
data improves significantly over approaches that
use either only sparse or only dense features, and
over approaches that combine query translation
by SMT with standard retrieval in the target lan-
guage. Furthermore, we show that our approach
can be seen as supervised model combination
that allows to combine SMT-based and ranking-
based approaches for further substantial improve-
ments. We conjecture that the gains are due to
orthogonal information contributed by domain-
knowledge, ranking-based word associations, and
translation-based information.
2 Related Work
CLIR addresses the problem of translating or pro-
jecting a query into the language of the document
repository across which retrieval is performed. In
a direct translation approach (DT), a state-of-the-
art SMT system is used to produce a single best
translation that is used as search query in the target
language. For example, Google?s CLIR approach
combines their state-of-the-art SMT system with
their proprietary search engine (Chin et al, 2008).
Alternative approaches avoid to solve the hard
problem of word reordering, and instead rely on
token-to-token translations that are used to project
the query terms into the target language with a
probabilistic weighting of the standard term tf-
idf scheme. Darwish and Oard (2003) termed
this method the probabilistic structured query ap-
proach (PSQ). The advantage of this technique
is an implicit query expansion effect due to the
use of probability distributions over term trans-
lations (Xu et al, 2001). Ture et al (2012)
brought SMT back into this paradigm by pro-
jecting terms from n-best translations from syn-
chronous context-free grammars.
Ranking approaches have been presented by
Guo and Gomes (2009) and Oh et al (2013).
Their method is a classical learning-to-rank setup
where pairwise ranking is applied to a few hun-
dred dense features. Methods to learn sparse
word-based translation correspondences from su-
pervised ranking signals have been presented by
Bai et al (2010) and Sokolov et al (2013). Both
approaches work in a cross-lingual setting, the for-
mer on Wikipedia data, the latter on patents.
Our approach extends the work of Sokolov et
al. (2013) by presenting an alternative learning-
to-rank approach that can be used for supervised
model combination to integrate dense and sparse
features, and by evaluating both approaches on
cross-lingual retrieval for patents and Wikipedia.
This relates our work to supervised model merg-
ing approaches (Sheldon et al, 2011).
3 Translation and Ranking for CLIR
SMT-based Models. We will refer to DT and
PSQ as SMT-based models that translate a query,
and then perform monolingual retrieval using
BM25. Translation is agnostic of the retrieval task.
Linear Ranking for Word-Based Models. Let
q ? {0, 1}
Q
be a query and d ? {0, 1}
D
be a doc-
ument where the j
th
vector dimension indicates the
occurrence of the j
th
word for dictionaries of size
Q and D. A linear ranking model is defined as
f(q,d) = q
>
Wd =
Q
?
i=1
D
?
j=1
q
i
W
ij
d
j
,
where W ? IR
Q?D
encodes a matrix of ranking-
specific word associations (Bai et al, 2010) . We
optimize this model by pairwise ranking, which
assumes labeled data in the form of a set R of tu-
ples (q,d
+
,d
?
), where d
+
is a relevant (or higher
ranked) document and d
?
an irrelevant (or lower
ranked) document for query q. The goal is to
find a weight matrix W such that an inequality
f(q,d
+
) > f(q,d
?
) is violated for the fewest
number of tuples from R. We present two meth-
ods for optimizing W in the following.
Pairwise Ranking using Boosting (BM). The
Boosting-based Ranking baseline (Freund et al,
2003) optimizes an exponential loss:
L
exp
=
?
(q,d
+
,d
?
)?R
D(q,d
+
,d
?
)e
f(q,d
?
)?f(q,d
+
)
,
whereD(q,d
+
,d
?
) is a non-negative importance
function on tuples. The algorithm of Sokolov et
al. (2013) combines batch boosting with bagging
over a number of independently drawn bootstrap
data samples fromR. In each step, the single word
pair feature is selected that provides the largest de-
crease of L
exp
. The found corresponding models
are averaged. To reduce memory requirements we
used random feature hashing with the size of the
hash of 30 bits (Shi et al, 2009). For regulariza-
tion we rely on early stopping.
Pairwise Ranking with SGD (VW). The sec-
ond objective is an `
1
-regularized hinge loss:
L
hng
=
?
(q,d
+
,d
?
)?R
(
f(q,d
+
)? f(q,d
?
)
)
+
+ ?||W ||
1
,
489
where (x)
+
= max(0, 1 ? x) and ? is the regu-
larization parameter. This newly added model uti-
lizes the standard implementation of online SGD
from the Vowpal Wabbit (VW) toolkit (Goel et al,
2008) and was run on a data sample of 5M to 10M
tuples from R. On each step, W is updated with
a scaled gradient vector ?
W
L
hng
and clipped to
account for `
1
-regularization. Memory usage was
reduced using the same hashing technique as for
boosting.
Domain Knowledge Models. Domain knowl-
edge features for patents were inspired by Guo
and Gomes (2009): a feature fires if two patents
share similar aspects, e.g. a common inventor. As
we do not have access to address data, we omit
geolocation features and instead add features that
evaluate similarity w.r.t. patent classes extracted
from IPC codes. Documents within a patent sec-
tion, i.e. the topmost hierarchy, are too diverse
to provide useful information but more detailed
classes and the count of matching classes do.
For Wikipedia, we implemented features that
compare the relative length of documents, num-
ber of links and images, the number of common
links and common images, and Wikipedia cat-
egories: Given the categories associated with a
foreign query, we use the language links on the
Wikipedia category pages to generate a set of
?translated? English categories S. The English-
side category graph is used to construct sets of
super- and subcategories related to the candidate
document?s categories. This expansion is done in
both directions for two levels resulting in 5 cat-
egory sets. The intersection between target set
T
n
and the source category set S reflects the cat-
egory level similarity between query and docu-
ment, which we calculate as a mutual containment
score s
n
=
1
2
(|S ? T
n
|/|S| + |S ? T
n
|/|T
n
|) for
n ? {?2,?1, 0,+1,+2} (Broder, 1997).
Optimization for these additional models in-
cluding domain knowledge features was done by
overloading the vector representation of queries q
and documents d in the VW linear learner: Instead
of sparse word-based features, q and d are rep-
resented by real-valued vectors of dense domain-
knowledge features. Optimization for the over-
loaded vectors is done as described above for VW.
4 Model Combination
Combination by Borda Counts. The baseline
consensus-based voting Borda Count procedure
endows each voter with a fixed amount of voting
points which he is free to distribute among the
scored documents (Aslam and Montague, 2001;
Sokolov et al, 2013). The aggregate score for
two rankings f
1
(q,d) and f
2
(q,d) for all (q,d)
in the test set is then a simple linear interpolation:
f
agg
(q,d) = ?
f
1
(q,d)?
d
f
1
(q,d)
+(1??)
f
2
(q,d)?
d
f
2
(q,d)
. Pa-
rameter ? was adjusted on the dev set.
Combination by Linear Learning. In order to
acquire the best combination of more than two
models, we created vectors of model scores along
with domain knowledge features and reused the
VW pairwise ranking approach. This means
that the vector representation of queries q and
documents d in the VW linear learner is over-
loaded once more: In addition to dense domain-
knowledge features, we incorporate arbitrary
ranking models as dense features whose value is
the score of the ranking model. Training data was
sampled from the dev set and processed with VW.
5 Data
Patent Prior Art Search (JP-EN). We use
BoostCLIR
1
, a Japanese-English (JP-EN) corpus
of patent abstracts from the MAREC and NTCIR
data (Sokolov et al, 2013). It contains automati-
cally induced relevance judgments for patent ab-
stracts (Graf and Azzopardi, 2008): EN patents
are regarded as relevant with level (3) to a JP query
patent, if they are in a family relationship (e.g.,
same invention), cited by the patent examiner (2),
or cited by the applicant (1). Statistics on the rank-
ing data are given in Table 1. On average, queries
and documents contain about 5 sentences.
Wikipedia Article Retrieval (DE-EN). The in-
tuition behind our Wikipedia retrieval setup is as
follows: Consider the situation where the German
(DE) Wikipedia article on geological sea stacks
does not yet exist. A native speaker of Ger-
man with profound knowledge in geology intends
to write it, naming it ?Brandungspfeiler?, while
seeking to align its structure with the EN counter-
part. The task of a CLIR engine is to return rele-
vant EN Wikipedia articles that may describe the
very same concept (Stack (geology)), or relevant
instances of it (Bako National Park, Lange Anna).
The information need may be paraphrased as a
high-level definition of the topic. Since typically
the first sentence of any Wikipedia article is such
1
www.cl.uni-heidelberg.de/boostclir
490
#q #d #d
+
/q #words/q
Patents (JP-EN)
train 107,061 888,127 13.28 178.74
dev 2,000 100,000 13.24 181.70
test 2,000 100,000 12.59 182.39
Wikipedia (DE-EN)
train 225,294 1,226,741 13.04 25.80
dev 10,000 113,553 12.97 25.75
test 10,000 115,131 13.22 25.73
Table 1: Ranking data statistics: number of queries and doc-
uments, avg. number of relevant documents per query, avg.
number of words per query.
a well-formed definition, this allows us to extract
a large set of one sentence queries from Wikipedia
articles. For example: ?Brandungspfeiler sind vor
einer Kliffk?uste aufragende Felsent?urme und ver-
gleichbare Formationen, die durch Brandungsero-
sion gebildet werden.?
2
Similar to Bai et al (2010)
we induce relevance judgments by aligning DE
queries with their EN counterparts (?mates?) via
the graph of inter-language links available in arti-
cles and Wikidata
3
. We assign relevance level (3)
to the EN mate and level (2) to all other EN ar-
ticles that link to the mate, and are linked by the
mate. Instead of using all outgoing links from the
mate, we only use articles with bidirectional links.
To create this data
4
we downloaded XML and
SQL dumps of the DE and EN Wikipedia from,
resp., 22
nd
and 4
th
of November 2013. Wikipedia
markup removal and link extraction was carried
out using the Cloud9 toolkit
5
. Sentence extrac-
tion was done with NLTK
6
. Since Wikipedia arti-
cles vary greatly in length, we restricted EN doc-
uments to the first 200 words after extracting the
link graph to reduce the number of features for BM
and VW models. To avoid rendering the task too
easy for literal keyword matching of queries about
named entities, we removed title words from the
German queries. Statistics are given in Table 1.
Preprocessing Ranking Data. In addition to
lowercasing and punctuation removal, we applied
Correlated Feature Hashing (CFH), that makes
collisions more likely for words with close mean-
ing (Bai et al, 2010). For patents, vocabularies
contained 60k and 365k words for JP and EN.
Filtering special symbols and stopwords reduced
the JP vocabulary size to 50k (small enough not
to resort to CFH). To reduce the EN vocabulary
2
de.wikipedia.org/wiki/Brandungspfeiler
3
www.wikidata.org/
4
www.cl.uni-heidelberg.de/wikiclir
5
lintool.github.io/Cloud9/index.html
6
www.nltk.org/
to a comparable size, we applied similar prepro-
cessing and CFH with F=30k and k=5. Since for
Wikipedia data, the DE and EN vocabularies were
both large (6.7M and 6M), we used the same filter-
ing and preprocessing as for the patent data before
applying CFH with F=40k and k=5 on both sides.
Parallel Data for SMT-based CLIR. For both
tasks, DT and PSQ require an SMT baseline
system trained on parallel corpora that are dis-
junct from the ranking data. A JP-EN sys-
tem was trained on data described and prepro-
cessed by Sokolov et al (2013), consisting of
1.8M parallel sentences from the NTCIR-7 JP-EN
PatentMT subtask (Fujii et al, 2008) and 2k par-
allel sentences for parameter development from
the NTCIR-8 test collection. For Wikipedia, we
trained a DE-EN system on 4.1M parallel sen-
tences from Europarl, Common Crawl, and News-
Commentary. Parameter tuning was done on 3k
parallel sentences from the WMT?11 test set.
6 Experiments
Experiment Settings. The SMT-based models
use cdec (Dyer et al, 2010). Word align-
ments were created with mgiza (JP-EN) and
fast align (Dyer et al, 2013) (DE-EN). Lan-
guage models were trained with the KenLM
toolkit (Heafield, 2011). The JP-EN system uses
a 5-gram language model from the EN side of the
training data. For the DE-EN system, a 4-gram
model was built on the EN side of the training
data and the EN Wikipedia documents. Weights
for the standard feature set were optimized using
cdec?s MERT (JP-EN) and MIRA (DE-EN) im-
plementations (Och, 2003; Chiang et al, 2008).
PSQ on patents reuses settings found by Sokolov
et al (2013); settings for Wikipedia were adjusted
on its dev set (n=1000, ?=0.4, L=0, C=1).
Patent retrieval for DT was done by sentence-
wise translation and subsequent re-joining to form
one query per patent, which was ranked against the
documents using BM25. For PSQ, BM25 is com-
puted on expected term and document frequencies.
For ranking-based retrieval, we compare several
combinations of learners and features (Table 2).
VW denotes a sparse model using word-based fea-
tures trained with SGD. BM denotes a similar
model trained using Boosting. DK denotes VW
training of a model that represents queries q and
documents d by dense domain-knowledge fea-
tures instead of by sparse word-based vectors. In
491
order to simulate pass-through behavior of out-of-
vocabulary terms in SMT systems, additional fea-
tures accounting for source and target term iden-
tity were added to DK and BM models. The pa-
rameter ? for VW was found on dev set. Statis-
tical significance testing was performed using the
paired randomization test (Smucker et al, 2007).
Borda denotes model combination by Borda
Count voting where the linear interpolation pa-
rameter is adjusted for MAP on the respective de-
velopment sets with grid search. This type of
model combination only allows to combine pairs
of rankings. We present a combination of SMT-
based CLIR, DT+PSQ, a combination of dense
and sparse features, DK+VW, and a combination
of both combinations, (DT+PSQ)+(DK+VW).
LinLearn denotes model combination by over-
loading the vector representation of queries q and
documents d in the VW linear learner by incor-
porating arbitrary ranking models as dense fea-
tures. In difference to grid search for Borda, opti-
mal weights for the linear combination of incorpo-
rated ranking models can be learned automatically.
We investigate the same combinations of rank-
ing models as described for Borda above. We do
not report combination results including the sparse
BM model since they were consistently lower than
the ones with the sparse VW model.
Test Results. Experimental results on test data
are given in Table 2. Results are reported
with respect to MAP (Manning et al, 2008),
NDCG (J?arvelin and Kek?al?ainen, 2002), and
PRES (Magdy and Jones, 2010). Scores were
computed on the top 1,000 retrieved documents.
As can be seen from inspecting the two blocks
of results, one for patents, one for Wikipedia, we
find the same system rankings on both datasets. In
both cases, as standalone systems, DT and PSQ
are very close and far better than any ranking ap-
proach, irrespective of the objective function or the
choice of sparse or dense features. Model combi-
nation of similar models, e.g., DT and PSQ, gives
minimal gains, compared to combining orthogo-
nal models, e.g. DK and VW. The best result is
achieved by combining DT and PSQ with DK and
VW. This is due to the already high scores of the
combined models, but also to the combination of
yet other types of orthogonal information. Borda
voting gives the best result under MAP which is
probably due to the adjustment of the interpola-
tion parameter for MAP on the development set.
combination models MAP NDCG PRES
P
a
t
e
n
t
s
(
J
P
-
E
N
)
s
t
a
n
d
a
l
o
n
e
DT 0.2554 0.5397 0.5680
PSQ 0.2659 0.5508 0.5851
DK 0.2203 0.4874 0.5171
VW 0.2205 0.4989 0.4911
BM 0.1669 0.4167 0.4665
B
o
r
d
a
DT+PSQ
?
0.2747
?
0.5618
?
0.5988
DK+VW
?
0.3023
?
0.5980
?
0.6137
(DT+PSQ)+(DK+VW)
?
0.3465
?
0.6420
?
0.6858
L
i
n
L
e
a
r
n
DT+PSQ
??
0.2707
??
0.5578
??
0.5941
DK+VW
??
0.3283
??
0.6366
??
0.7104
DT+PSQ+DK+VW
??
0.3739
??
0.6755
??
0.7599
W
i
k
i
p
e
d
i
a
(
D
E
-
E
N
)
s
t
a
n
d
a
l
o
n
e
DT 0.3678 0.5691 0.7219
PSQ 0.3642 0.5671 0.7165
DK 0.2661 0.4584 0.6717
VW 0.1249 0.3389 0.6466
BM 0.1386 0.3418 0.6145
B
o
r
d
a
DT+PSQ
?
0.3742
?
0.5777
?
0.7306
DK+VW
?
0.3238
?
0.5484
?
0.7736
(DT+PSQ)+(DK+VW)
?
0.4173
?
0.6333
?
0.8031
L
i
n
L
e
a
r
n
DT+PSQ
??
0.3718
??
0.5751
??
0.7251
DK+VW
??
0.3436
??
0.5686
??
0.7914
DT+PSQ+DK+VW
?
0.4137
??
0.6435
??
0.8233
Table 2: Test results for standalone CLIR models using di-
rect translation (DT), probabilistic structured queries (PSQ),
sparse model with CFH (VW), sparse boosting model (BM),
dense domain knowledge features (DK), and model combi-
nations using Borda Count voting (Borda) or linear super-
vised model combination (LinLearn). Significant differences
(at p=0.01) between aggregated systems and all its compo-
nents are indicated by ?, between LinLearn and the respective
Borda system by ?.
Under NDCG and PRES, LinLearn achieves the
best results, showing the advantage of automati-
cally learning combination weights that leads to
stable results across various metrics.
7 Conclusion
Special domains such as patents or Wikipedia of-
fer the possibility to extract cross-lingual rele-
vance data from citation and link graphs. These
data can be used to directly optimizing cross-
lingual ranking models. We showed on two differ-
ent large-scale ranking scenarios that a supervised
combination of orthogonal information sources
such as domain-knowledge, translation knowl-
edge, and ranking-specific word associations by
far outperforms a pipeline of query translation and
retrieval. We conjecture that if these types of in-
formation sources are available, a supervised rank-
ing approach will yield superior results in other re-
trieval scenarios as well.
Acknowledgments
This research was supported in part by DFG
grant RI-2221/1-1 ?Cross-language Learning-to-
Rank for Patent Retrieval?.
492
References
Javed A. Aslam and Mark Montague. 2001. Models
for metasearch. In Proceedings of the ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval (SIGIR?01), New Orleans, LA.
Bing Bai, Jason Weston, David Grangier, Ronan Col-
lobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2010. Learning
to rank with (a lot of) word features. Information
Retrieval Journal, 13(3):291?314.
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. In Compression and Com-
plexity of Sequences (SEQUENCES?97), pages 21?
29. IEEE Computer Society.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP?08), Waikiki, Hawaii.
Jeffrey Chin, Maureen Heymans, Alexandre Ko-
joukhov, Jocelyn Lin, and Hui Tan. 2008. Cross-
language information retrieval. Patent Application.
US 2008/0288474 A1.
Kareem Darwish and Douglas W. Oard. 2003. Proba-
bilistic structured query methods. In Proceedings.
of the ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR?03),
Toronto, Canada.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, Uppsala, Sweden.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Atlanta, GA.
Yoav Freund, Ray Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. Journal of Machine Learn-
ing Research, 4:933?969.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Pro-
ceedings of NTCIR-7 Workshop Meeting, Tokyo,
Japan.
Sharad Goel, John Langford, and Alexander L. Strehl.
2008. Predictive indexing for fast search. In Ad-
vances in Neural Information Processing Systems,
Vancouver, Canada.
Erik Graf and Leif Azzopardi. 2008. A methodol-
ogy for building a patent test collection for prior
art search. In Proceedings of the 2nd Interna-
tional Workshop on Evaluating Information Access
(EVIA?08), Tokyo, Japan.
Yunsong Guo and Carla Gomes. 2009. Ranking struc-
tured documents: A large margin based approach for
patent prior art search. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence
(IJCAI?09), Pasadena, CA.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation (WMT?11), Edinburgh, UK.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions in Information Systems, 20(4):422?
446.
Walid Magdy and Gareth J.F. Jones. 2010. PRES:
a score metric for evaluating recall-oriented infor-
mation retrieval applications. In Proceedings of the
ACM SIGIR conference on Research and develop-
ment in information retrieval (SIGIR?10), New York,
NY.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Meeting on Association for Computational
Linguistics (ACL?03), Sapporo, Japan.
Sooyoung Oh, Zhen Lei, Wang-Chien Lee, Prasenjit
Mitra, and John Yen. 2013. CV-PCR: A context-
guided value-driven framework for patent citation
recommendation. In Proceedings of the Interna-
tional Conference on Information and Knowledge
Management (CIKM?13), San Francisco, CA.
Daniel Sheldon, Milad Shokouhi, Martin Szummer,
and Nick Craswell. 2011. Lambdamerge: Merging
the results of query reformulations. In Proceedings
of WSDM?11, Hong Kong, China.
Qinfeng Shi, James Petterson, Gideon Dror, John
Langford, Alexander J. Smola, Alexander L. Strehl,
and Vishy Vishwanathan. 2009. Hash Kernels. In
Proceedings of the 12th Int. Conference on Artifi-
cial Intelligence and Statistics (AISTATS?09), Irvine,
CA.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL-HLT?10), Los Angeles, CA.
493
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In Proceedings
of the 16th ACM conference on Conference on Infor-
mation and Knowledge Management (CIKM ?07),
New York, NY.
Artem Sokolov, Laura Jehl, Felix Hieber, and Stefan
Riezler. 2013. Boosting cross-language retrieval
by learning bilingual phrase associations from rele-
vance rankings. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP?13).
Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012. Combining statistical translation techniques
for cross-language information retrieval. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING?12), Bombay, India.
Masao Utiyama and Hitoshi Isahara. 2007. A
Japanese-English patent parallel corpus. In Pro-
ceedings of MT Summit XI, Copenhagen, Denmark.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual
information retrieval. In Proceedings of the ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR?01), New York, NY.
494
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 410?421,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Twitter Translation using Translation-Based Cross-Lingual Retrieval
Laura Jehl and Felix Hieber and Stefan Riezler
Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany
{jehl,hieber,riezler}@cl.uni-heidelberg.de
Abstract
Microblogging services such as Twitter have
become popular media for real-time user-
created news reporting. Such communica-
tion often happens in parallel in different lan-
guages, e.g., microblog posts related to the
same events of the Arab spring were written
in Arabic and in English. The goal of this
paper is to exploit this parallelism in order
to eliminate the main bottleneck in automatic
Twitter translation, namely the lack of bilin-
gual sentence pairs for training SMT systems.
We show that translation-based cross-lingual
information retrieval can retrieve microblog
messages across languages that are similar
enough to be used to train a standard phrase-
based SMT pipeline. Our method outper-
forms other approaches to domain adaptation
for SMT such as language model adaptation,
meta-parameter tuning, or self-translation.
1 Introduction
Among the various social media platforms, mi-
croblogging services such as Twitter1 have become
popular communication tools. This is due to the easy
accessibility of microblogging platforms via inter-
net or mobile phones, and due to the need for a fast
mode of communication that microblogging satis-
fies: Twitter messages are short (limited to 140 char-
acters) and simultaneous (due to frequent updates by
prolific microbloggers). Twitter users form a social
network by ?following? the updates of other users,
either reciprocal or one-way. The topics discussed
in Twitter messages range from private chatter to im-
portant real-time witness reports.
1http://twitter.com/
Events such as the Arab spring have shown the
power and also the shortcomings of this new mode
of communication. Microblogging services played a
crucial role in quickly spreading the news about im-
portant events, furthermore they were useful in help-
ing organizers plan their protest. The fact that news
on microblogging platforms is sometimes ahead of
newswire is one of the most interesting facets of
this new medium. However, while Twitter messag-
ing is happening in multiple languages, most net-
works of ?friends? and ?followers? are monolingual
and only about 40% of all messages are in English2.
One solution to sharing news quickly and interna-
tionally was crowdsourcing manual translations, for
example at Meedan3, a nonprofit organization built
to share news and opinion between the Arabic and
English speaking world, by translating articles and
blogs, using machine translation and human expert
corrections.
The goal of our research is to automate this trans-
lation process, with a further aim of providing rapid
crosslingual data access for downstream applica-
tions. The automated translation of microblogging
messages is facing two main problems. First, there
are no bilingual sentence pair data from microblog-
ging domains available. Second, the colloquial, non-
standard language of many microblogging messages
makes it very difficult to adapt a machine translation
system trained on any of the available bilingual re-
sources such as transcriptions from political organi-
zations or news text.
The approach presented in this paper aims to ex-
ploit the fact that microblogging often happens in
2http://semiocast.com/publications/2011_
11_24_Arabic_highest_growth_on_Twitter
3http://news.meedan.net
410
parallel in different languages, e.g., microblog posts
related to the same events of the Arab spring were
published in parallel in Arabic and in English. The
central idea is to crawl a large set of topically related
Arabic and English microblogging messages, and
use Arabic microblog messages as search queries in
a cross-lingual information retrieval (CLIR) setup.
We use the probabilistic translation-based retrieval
technique of Xu et al (2001) that naturally inte-
grates translation tables for cross-lingual retrieval.
The retrieval results are then used as input to a stan-
dard SMT pipeline to train translation models, start-
ing from unsupervised induction of word alignments
(Och and Ney, 2000) to phrase-extraction (Och and
Ney, 2004) and phrase-based decoding (Koehn et al,
2007). We investigate several filtering techniques
for retrieval and phrase extraction (Munteanu and
Marcu, 2006; Snover et al, 2008) and find a straight-
forward application of phrase extraction from sym-
metrized alignments to be optimal. Furthermore, we
compare our approach to related domain adaptation
techniques for SMT and find our approach to yield
large improvements over all related techniques.
Finally, a side-product of our research is a cor-
pus of around 1,000 Arabic Twitter messages with
3 manual English translations each, which were cre-
ated using crowdsourcing techniques. This corpus
is used for development and testing in our experi-
ments.
2 Related Work
SMT for user-generated noisy data has been pio-
neered at the 2011 Workshop on Statistical Ma-
chine Translation that featured a translation task of
Haitian Creole emergency SMS messages4. This
task is very similar to the problem of Twitter transla-
tion since SMS contain noisy, abbreviated language.
The research papers related to the featured transla-
tion task deploy several approaches to domain adap-
tation, including crowdsourcing (Hu et al, 2011)
or extraction of parallel sentences from comparable
data (Hewavitharana et al, 2011).
The use of crowdsourcing to evaluate machine
translation and to build development sets was pi-
oneered by Callison-Burch (2009) and Zaidan and
4http://www.statmt.org/wmt11/
featured-translation-task.html
Callison-Burch (2009). Crowdsourcing has its lim-
its when it comes to generating parallel training data
on the scale of millions of parallel sentences. In
our work, we use crowdsourcing via Amazon Me-
chanical Turk5 to create a development and test cor-
pus that includes 3 English translations for each of
around 1,000 Arabic microblog messages.
There is a substantial amount of previous work on
extracting parallel sentences from comparable data
such as newswire text (Fung and Cheung, 2004;
Munteanu and Marcu, 2005; Tillmann and ming Xu,
2009) and on finding parallel phrases in non-parallel
sentences (Munteanu and Marcu, 2006; Quirk et al,
2007; Cettolo et al, 2010; Vogel and Hewavitha-
rana, 2011). The approach that is closest to our
work is that of Munteanu and Marcu (2006): They
use standard information retrieval together with sim-
ple word-based translation for CLIR, and extract
phrases from the retrieval results using a clean bilin-
gual lexicon and an averaging filter. In this ap-
proach, filtering and cleaning techniques in align-
ment and phrase extraction have to compensate for
low-quality retrieval results. In our approach, the fo-
cus is on high-quality retrieval.
As our experimental results show, the main im-
provement of our technique is a decrease in out-of-
vocabulary (OOV) rate at an increase of the per-
centage of correctly translated unigrams and bi-
grams. Similar work on solving domain adaptation
for SMT by mining unseen words has been pre-
sented by Snover et al (2008) and Daum? and Ja-
garlamudi (2011). Both approaches show improve-
ments by adding new phrase tables; however, both
approaches rely on techniques that require larger
comparable texts for mining unseen words. Since
in our case documents are very short (they consist
of 140 character sequences), these techniques are
not applicable. However, the advantage of the fact
that microblog messages resemble sentences is that
we can apply standard word- and phrase-alignment
techniques directly to the retrieval results.
Further approaches to domain adaptation for SMT
include adaptation using in-domain language mod-
els (Bertoldi and Federico, 2009), meta-parameter
tuning on in-domain development sets (Koehn and
Schroeder, 2007), or translation model adaptation
5http://www.turk.com
411
using self-translations of in-domain source language
texts (Ueffing et al, 2007). In our experiments we
compare our approach to these domain adaptation
techniques.
3 Cross-Lingual Retrieval via Statistical
Translation
3.1 Retrieval Model
In our approach, comparable candidates for domain
adaptation are selected via cross-lingual retrieval.
In a probabilistic retrieval framework, we estimate
the probability of a relevant document microblog
message D given a query microblog message Q,
P (D|Q). Following Bayes rule, this can be sim-
plified to ranking documents according to the like-
lihood P (Q|D) if we assume a uniform prior over
documents.
score(Q,D) = P (D|Q) = P (D)P (Q|D)P (Q) (1)
Our model is defined as follows:
score(Q,D) = P (Q|D) =
?
q?Q
P (q|D) (2)
P (q|D) = ?Pmix(q|D)
? ?? ?
mixture model
+(1? ?) PML(q|C)
? ?? ?
query collection backoff
(3)
Pmix(q|D) = ?
?
d?D
T (q|d)PML(d|D)
? ?? ?
translation model
(4)
+(1? ?)PML(q|D)
? ?? ?
self-translation
Our retrieval model is related to monolingual re-
trieval models such as the language-modeling ap-
proach of Ponte and Croft (1998) and the monolin-
gual statistical translation approach of Berger and
Lafferty (1999). Xu et al (2001) extend the former
approaches to the cross-lingual setting by adding a
term translation table. They describe their model in
terms of a Hidden Markov Model with two states
that generate query terms: First, a document state
generates terms d in the document language and then
translates them into a query term q. Second, a back-
off state generates query terms q directly in the query
language. In the document state the probability of
emitting q depends on all d that translate to q, ac-
cording to a translation distribution T . This is esti-
mated by marginalizing out d as
?
d T (q|d)P (d|D).
In the backoff state the probability PML(q|C) of
emitting a query term is estimated as the relative
frequency of this term within a corpus in the query
language. The probability of transitioning into the
document state or the backoff state is given by ? and
1? ?.
We view this model from a smoothing perspective
where the backoff state is linearly interpolated with
the translation probability using a mixture weight
? to control the weighting between both terms.
Furthermore, we expand Xu et al (2001)?s gen-
erative model to incorporate the concept of ?self-
translation?, introduced by Xue et al (2008) in a
monolingual question-answering context: Twitter
messages across languages usually share relevant
terms such as hashtags, named entities or user men-
tions. Therefore, we model the event of a query
term literally occurring in the document in a sepa-
rate model that is itself linearly interpolated with a
parameter ? with the translation model.
We implemented the model based on a Lucene6
index, which allows efficient storage of term-
document and document-term vectors. To mini-
mize retrieval time, we consider only those doc-
uments as retrieval candidates where at least one
term translates to a query term, according to the
translation table T . Stopwords were removed for
both queries and documents. Compared to com-
mon inverted index retrieval implementations, our
model is quite slow since the document-term vectors
have to be loaded. However, multi-threading sup-
port and batch retrieval on a Hadoop cluster made
the model tractable. On the upside, the translation-
based model allows greater precision in finding
the candidates for comparable microblog messages
than simpler approaches that use a combination of
tfidf matching and n-best query term expansion:
The translation-based retrieval exploits all possi-
ble alignments between query and document terms
which is particularly important for short documents
such as microblog messages.
3.2 In-Domain Phrase Extraction
To prepare the extraction of phrases from retrieval
results, we conducted cross-lingual retrieval in both
directions: retrieving Arabic documents using En-
glish microblog messages as queries and vice versa.
6http://lucene.apache.org/core/
412
For each run we kept the top N retrieved documents.
Each document was then paired with its query to
generate pseudo-parallel data.
We tried two approaches for using this data to
improve our translations. The first, more restric-
tive method makes use of the word alignments we
obtained from 5.8 million clean parallel training
data from the NIST evaluation campaign. The re-
trieval step generates word-alignments in the direc-
tion D ? Q. After retrieval, the reverse alignment
for each query-document pair is also generated by
using a translation table in the direction Q ? D. An
alignment point between a query term q and a docu-
ment term d is created, iff T (q|d) or T (d|q) exist in
the translation tables D ? Q or Q ? D. Based on
these word-alignments, we extract phrases by apply-
ing the grow-diag-final-and heuristic and using Och
and Ney (2004)?s phrase extraction algorithm as im-
plemented in Moses7 (Koehn et al, 2007). We con-
ducted experiments using different constraints on
the number of alignment points required for a pair
to be considered as well as the value of N . Our first
technique resembles the technique of Munteanu and
Marcu (2006) who also perform phrase extraction
by combining clean alignment lexica for initial sig-
nals with heuristics to smooth alignments for final
fragment extraction.
While we obtained some gains using our heuris-
tics, we are aware that our method is severely re-
stricted in that it only learns new words which are
in the vicinity of known words. We therefore also
tried the bolder approach of treating our data as
parallel and running unsupervised word alignment8
(Och and Ney, 2000) directly on the query-document
pairs to obtain new world alignments and build a
phrase table. In contrast to previous work (Snover
et al, 2008; Daum? and Jagarlamudi, 2011), we can
take advantage of the sentence-like character of mi-
croblog messages and treat queries and retrieval re-
sults similar to sentence aligned data.
For both extraction methods, the standard five
translation features from the new phrase table
(phrase translation probability and lexical weight-
ing in both directions, phrase penalty) were added to
the translation features in Moses. We tried different
7http://statmt.org/moses/
8http://code.google.com/p/giza-pp/
al-Gaddafi, al-Qaddhafi, assad, babrain, bahrain,
egypt, gadaffi, gaddaffi, gaddafi, Gheddafi, homs,
human rights, human-rights, humanrights, libia, li-
bian, libya, libyan, lybia, lybian, lybya, lybyan,
manama, Misrata, nabeelrajab, nato, oman, Pos-
itiveLibyaTweets, Qaddhafi, sirte, syria, tripoli,
tripolis, yemen;
Table 1: Keywords used for Twitter crawl.
modes of combining new and original phrase table,
namely using either one or using the new phrase ta-
ble as backoff in case no phrase translation is found
in the original phrase table.
4 Data
4.1 Twitter Crawl
We crawled Twitter messages from September 20,
2011 until January 23, 2012 via the Streaming API9
in keyword-tracking mode, obtaining 25.5M Twit-
ter messages (tweets) in various languages. Table 1
shows the list of keywords that were chosen to re-
trieve microblog messages related to the events of
the Arab spring.10
In order to separate the microblog message cor-
pus by languages, we applied a Naive Bayes lan-
guage identifier11. This yielded a distribution with
the six most common languages (of 52) being Ara-
bic (57%), English (33%), Somali (2%), Spanish
(2%), Indonesian (1.5%), German (0.7%). We kept
only microblog messages classified as English or
Arabic with confidence greater 0.9. Keyword-based
crawling creates a strong bias towards the domain
of the keywords and it does not guarantee that all
microblog messages regarding a certain topic or re-
gion are retrieved or that all retrieved messages are
related to the Arab Spring and human righs in the
middle east. Additionally, retweets artificially in-
9https://dev.twitter.com/docs/
streaming-api/
10The Twitter Streaming API allows up to 400 tracking key-
words that are matched to uppercase, lowercase and quoted
variations of the keywords. Partial matching such as ?tripolis?
matching ?tripoli? as well as Arabic Unicode characters are not
supported. We extended our keywords over time by analyzing
the crawl, e.g., by introducing spelling variants and hashtags.
11Language Detection Library for Java, by
Shuyo Nakatani (http://code.google.com/p/
language-detection/).
413
Arabic English
tweets + retweets 14,565,513 8,501,788
tweets 6,614,126 5,129,829
avg. retweet/tweet 11.62 7.27
unique users 180,271 865,202
avg. tweets/user 36.6 5.9
Table 2: Twitter corpus statistics
flate the size of the data, although there are no new
terms added. Therefore, we removed all duplicate
retweets that did not introduce additional terms to
the original tweet. Table 2 explains the shrinkage
of the dataset after removing retweets - compared
to English users, a smaller number of Arabic users
produced a much larger number of retweets. Inter-
estingly, 56,087 users tweet a substantial amount in
both languages. This suggests that users spread mes-
sages simultaneously in Arabic and English.
4.2 Creating a Small Parallel Twitter Corpus
using Crowdsourcing
For the evaluation of our method, a small amount
of parallel in-domain data was required. Since there
are no corpora of translated microblog messages, we
decided to use Amazon Mechanical Turk12 to cre-
ate our own evaluation set, following the exploratory
work of Zaidan and Callison-Burch (2011b). We
randomly selected 2,000 Arabic microblog mes-
sages. Hashtags, user mentions and URLs were re-
moved from each microblog message beforehand,
because they do not need to be translated and would
just artificially inflate scores at test time. The mi-
croblog messages were then manually cleaned and
pruned. We discarded messages which contained
almost no text or large portions of other languages
and removed remaining Twitter markup. In the end,
1,022 microblog messages were used in the Me-
chanical Turk task. We split the data into batches
of ten sentences which comprised one HIT (human
intelligence task). Each HIT had to be completed by
three workers. In order to have some control over
translation quality, we inserted one control sentence
per HIT, taken from the LDC-GALE Phase 1 Arabic
Blog Parallel Text. Turkers were rewarded 10 cents
per translation. Following Zaidan and Callison-
Burch (2011b), all Arabic sentences were converted
12http://www.turk.com
into images in order to prevent turkers from past-
ing them into online machine translation engines.
Our final corpus consists of 1,022 translated mi-
croblog messages with three translations each. An
example containing translations for one of the sen-
tences which we inserted for quality checking pur-
poses, along with the reference translation, is given
in table 3. It can be seen that translators sometimes
made grammar mistakes or odd word choices. They
also tended to omit punctuation marks. However,
translations also contained reasonable translation al-
ternatives (such as ?gathered? or ?collected?). We
also asked translators to insert an ?unknown? token
whenever they were unable to translate a word. Our
HIT setup did not allow workers to skip a sentence,
forcing them to complete an entire batch. In order to
account for translation variants we decided to use all
three translations obtained via Mechanical Turk as
multiple references instead of just keeping the top
translation. We randomly split our small parallel
corpus, using half of the microblog messages for de-
velopment and half for testing.
4.3 Preprocessing
Besides removal of Twitter markup, several addi-
tional preprocessing steps such as digit normaliza-
tion were applied to the data. We also decided to ap-
ply the Buckwalter Arabic transliteration scheme13
to avoid encoding difficulties. Habash and Sadat
(2006) have shown that tokenization is helpful for
translating Arabic. We therefore decided to ap-
ply a more involved tokenization scheme than sim-
ple whitespace splitting to our data. As the re-
trieval relies on translation tables, all data need
to be tokenized the same way. We are aware
of the MADA+TOKAN Arabic morphological an-
alyzer and tokenizer (Habash and Rambow, 2005),
however, this toolkit produces very in-depth analy-
ses of the data and thus led to difficulties when we
tried to scale it to millions of sentences/microblog
messages. That is why we only used MADA for
transliteration and chose to implement the simpler
approach by Lee et al (2003) for tokenization. This
approach only requires a small set of annotated data
to obtain a list of prefixes and suffixes and uses n-
13http://www.qamus.org/transliteration.
htm
414
REFERENCE breaking the silence, a campaign group made up of israeli soldiers, gathered anonymous accounts from 26 soldiers.
TRANSLATION1 and breaking silence is a group of israeli soldiers that had unknown statistics from 26 soldiers israeli
TRANSLATION2 breaking the silence by a group of israeli soldiers who gathered unidentified statistics from 26 israeli soldier.
TRANSLATION3 breaking the silence is a group of israeli soldiers that collected unknown statistics of 26 israeli soldiers
Table 3: Example turker translations.
gram-models to determine the most likely prefix?-
stem-suffix? split of a word.14
5 Twitter Translation Experiments
We conducted a series of experiments to evaluate
our strategy of using CLIR and phrase-extraction to
extract comparable data in the Twitter domain. We
also explored more standard ways of domain adap-
tation such as using English microblog messages to
build an in-domain language model, or generating
synthetic bilingual corpora from monolingual data.
All experiments were conducted using the Moses
machine translation system15 (Koehn et al, 2007)
with standard settings. Language models were
built using the SRILM toolkit16 (Stolcke, 2002).
For all experiments, we report lowercased BLEU-
4 scores (Papineni et al, 2001) as calculated by
Moses? multi-bleu script. For assessing signifi-
cance, we apply the approximate randomization test
(Noreen, 1989; Riezler and Maxwell, 2005). We
consider pairwise differing results scoring a p-value
< 0.05 as significant.
Our baseline model was trained using 5,823,363
million parallel sentences in Modern Standard
Arabic (MSA) (198,500,436 tokens) and English
(193,671,201 tokens) from the NIST evaluation
campaign. This data contains parallel text from dif-
ferent domains, including UN reports, newsgroups,
newswire, broadcast news and weblogs.
5.1 Domain Adaption using Monolingual
Resources
As a first step, we used the available in-domain
data for a combination of domain adaptation tech-
14The n-gram-model required for tokenization was trained on
5.8 million Modern Standard Arabic sentences from the NIST
evaluation campaign. This data had previously been tokenized
with the same method, trained to match the Penn Arabic Tree-
bank, v3.
15http://statmt.org/moses/
16http://www.speech.sri.com/projects/
srilm/
niques similar to Bertoldi and Federico (2009).
There were three different adaptation measures:
First, the turker-generated development set was used
for optimizing the weights of the decoding meta-
parameters, as introduced by Koehn and Schroeder
(2007). Second, the English microblog messages in
our crawl were used to build an in-domain language
model. This adaptation technique was first proposed
by Zhao et al (2004). Third, the Arabic portion of
our crawl was used to synthetically generate addi-
tional parallel training data. This was accomplished
by machine-translating the Arabic microblog mes-
sages with the best system after performing the first
two adaptation steps. Since decoding is very time-
intensive, only 1 million randomly selected Ara-
bic microblog messages were used to generate syn-
thetic parallel data. This new data was then used
to train another phrase table. Such self-translation
techniques have been introduced by Ueffing et al
(2007). All results were evaluated against a base-
line of using only NIST data for translation model,
language model and weight optimization.
Our results are shown in table 4. Using an in-
domain development set while leaving everything
else untouched led to an improvement of approxi-
mately 1 BLEU point. Three experiments involv-
ing the Twitter language model confirm Bertoldi
and Federico (2009)?s findings that the language
model was most helpful. The BLEU-score could
be improved by 1.5 to 2 points in all experiments.
When using an in-domain language model, there
was no significant difference between deploying an
in-domain or out-of-domain development set. We
also compared the effect of using only the in-domain
language model to that of adding the in-domain
language model as an extra feature while keeping
the NIST language model.17 There was no signif-
17The weights for both language models were optimized
along with all other translation feature weights, rather than run-
ning an extra optimization step to interpolate between both lan-
guage models, since Koehn and Schroeder (2007) showed that
415
Run Translation Model Language Model Dev Set BLEU %
1 NIST NIST NIST 13.90
2 NIST NIST Twitter 14.83?
3 NIST Twitter NIST 15.98?
4 NIST Twitter Twitter 15.68?
5 NIST Twitter & NIST Twitter 16.04?
6 self-train Twitter & NIST Twitter 15.79?
7 self-train & NIST Twitter & NIST Twitter 15.94?
Table 4: Domain adaptation experiments. Asterisks indicate significant improvements over baseline (1).
Run Twitter Phrases extraction method # sentence pairs # extracted phrases BLEU %
8 top 3 retrieval results heuristics 14,855,985 6,508,141 17.04?
9 top 1 retrieval results GIZA++ 5,141,065 54,260,537 18.73??
10 retrieval intersection GIZA++ 3,452,566 29,091,009 18.85??
11 retrieval intersection as backoff GIZA++ 3,452,566 29,091,009 18.93??
Table 5: CLIR domain adaptation experiments. All weights were optimized on the Twitter dev set and used
the Twitter and NIST language models. One Asterisk indicates a significant improvement over baseline run
(5) from table 4. Two Asterisks indicate a significant improvement over run (8).
icant difference between both runs. However, for
further adaptation experiments we used the system
with the highest absolute BLEU score. In our case,
using synthetically generated data was not help-
ful, yielding similar results as the language model
experiments above. As has been observed before
by Bertoldi and Federico (2009), it did not matter
whether the synthetic data were used on their own
or in addition to the original training data.
5.2 Domain Adaptation using
Translation-based CLIR
Meta-parameters ?, ? ? [0, 1] of the retrieval model
were tuned in a mate-finding experiment: Mate-
finding refers to the task of retrieving the single rel-
evant document for a query. In our case, each source
tweet in the crowdsourced development set had ex-
actly one ?mate?, namely the crowdsourced transla-
tion that was ranked best in a further crowdsourced
ranking task. Using the retrieval model described
in section 3 we achieved precision@1 scores above
95% in finding the translations of a tweet when ?
and ? were set to 0.9. We fixed these parameter set-
tings for all following experiments. The translation
table was taken from the baseline experiments in ta-
ble 4. During retrieval, we kept up to 10 highest
scoring documents per query.
both strategies yielded the same results.
We first employed heuristic phrase extraction
based on the word alignments generated from the
NIST data as described above. To avoid learning
too much noise, maximum phrase length was re-
stricted to 3 (the default is 7). To evaluate the effects
of choosing more restrictive or more lax settings,
we ran experiments varying the following configu-
rations:
1. Constraints on alignment points:
? no constraints,
? 3+ alignment points in each direction,
? 3+ alignment points in both directions,
? 5+ alignment points in both directions.
2. Constraints on retrieval ranking:
? top 10 results,
? top 3 results,
? top 1 results,
? retrieval intersection (results found in both
retrieval directions)
We obtained improvements for all combinations
of these configurations. However, we observed that
requiring 5 common alignment points was too strict,
since few pairs met this constraint. We also noticed
that using only the top 3 retrieval results was benefi-
cial to performance, suggesting that more compara-
ble microblog messages were indeed ranked higher.
416
Using extraction heuristics we gained maximally 1.0
BLEU using the top 3 retrieval results and requiring
at least 3 alignment points in both alignment direc-
tions (see first line in table 5). However, other con-
figurations produced very similar results.
While heuristics led to small incremental im-
provements, we achieved a much larger improve-
ment by training a new phrase table from scratch us-
ing GIZA++. Again, we restricted maximum phrase
length to 3 words. In order to keep phrase table
size manageable, we had to restrict retrieval to top-
1 results or only use retrieval results in the inter-
section of retrieval directions. Best results are ob-
tained when combining phrase tables extracted from
GIZA++ alignments in the intersection of retrieval
results with NIST phrase tables in backoff mode (see
last line in table 5).
6 Error Analysis
Our cross-lingual retrieval approach succeeded in
finding nearly parallel tweets, confirming our hy-
pothesis that such data actually exists. Examples are
given in table 6.
Table 7 shows a more detailed breakdown of our
translation scores. First, standard adaptation meth-
ods increased n-gram precision, suggesting that us-
ing in-domain adaptation data caused the system to
choose more suitable words. As expected, there was
no reduction in OOVs, since using an in-domain
language model and development set does not in-
troduce new vocabulary. Heuristic phrase extrac-
tion again produced small improvements in n-gram
precision while reducing the number of unknown
words. Learning a new phrase table with GIZA++
produced substantial improvements both in OOV-
rate and in n-gram precision.
Nevertheless, even the scores of the adapted sys-
tem are still fairly low and translation quality as
judged by inspection of the output can be very poor.
This suggests that the language used on Twitter still
poses a great challenge, due to its variety of styles
as well as the users? tendency to use non-standard
spelling and colloquial or dialectal expressions. Our
development set contained many different genres,
from Qu?ran verses over news headlines to personal
chatter. Another difficulty was posed by dialectal
Arabic content. To gain an impression of the amount
of dialectal content in our data, we used the Arabic
Online Commentary Dataset created by Zaidan and
Callison-Burch (2011a) to classify our test set. Ta-
ble 8 shows the distribution of dialects in our test
data according to language model probability. This
distribution should be viewed with a grain of salt,
since the shortness of tweets might cause unreliable
results when using a model based on word frequen-
cies for classification. Still, the results suggest that
there is a high proportion of dialectal content and
spelling variation in our data, causing a large num-
ber of OOVs. For example, the preposition ??,
meaning ?in? is often written as Y?. Our phrase
table trained only on standard Arabic data as well as
our extraction heuristic failed to translate this fre-
quently occurring word. Only when retraining a
phrase table with GIZA++ did we translate it cor-
rectly.
Dialect # Sentences
Egyptian 141
Levantine 147
Gulf 78
Modern Standard Arabic 145
Table 8: Dialectal content in our test set as classified
by the AOC dataset.
Table 9 gives examples of translations generated
using different adaptation methods in comparison to
the references and the Google translation service to
illustrate strengths and weaknesses of our approach.
Example 1 shows a case where unknown words were
learned through translation model adaptation. Note
that even the Google translator did not recognize
the word ?ys? which was transliterated as
?Msellat?. Zaidan and Callison-Burch (2011a) point
out that dialectal variants are often transliterated
by Google. Note also, that the unadapted transla-
tion erroneously translated the place name ?sitra? as
?jacket?, a mistake which was also made in two of
the references and by Google. The same happened
to the place name ?wadyan?, which could also be
taken as meaning ?and religions?. This error was
enforced by our preprocessing step incorrectly split-
ting off the prefix ?w? which often carries the mean-
ing ?and?. In addition to that, the two runs which
used translation model adaptation each dropped a
part of the input sentence (?in sitra?, ?firing?). We
417
ARABIC TWEET fO?  Y?  ?yybyl?   w?d?? ??AyF ?? @q?  ?  d??? ?s?rf?  Hy?r?  
 ?  
GOOGLE TRANSLATION AFP confirms that the French President Gaddafi Libyans tried to call and forgiveness
ENGLISH TWEET french president assures that will be taken to court and tells the libyans to forgive each other
ARABIC TWEET Hym?  ??   ? rO? Y? ?wmm?  A?rJ ?ym ??C ? A?E Crq? ?AO?  ?y\n EAh
GOOGLE TRANSLATION NTRA decide to increase the number of all mobile operators in Egypt a commencement from Thursday
ENGLISH TWEET ntra decide to increase the number of all mobile operators in starting from thursday
ARABIC TWEET ?CA? ?lV ??rV ?? r?An? ?w? dm  Yl? ?y?  dyhK? 
GOOGLE TRANSLATION Shahid Amin AA Day January through gunshot
ENGLISH TWEET martyr amin ali ahmed on jan by gunshot
Table 6: Examples of nearly parallel tweets found by our retrieval method.
Adaptation method OOV-rate %/absolute unigram precision %/absolute bigram precision %/absolute output length (words)
None 22.56/2216 51.1/5020 20.2/1882 9832
LM and Dev 20.05/2220 51.4/5442 22.1/2227 10595
Retrieval (heuristic) 17.47/1790 53.5/5484 23.6/2299 10246
Retrieval (GIZA++) 4.22/439 56.1/5834 26.1/2575 10395
Table 7: OOV-rate and precision for different adaptation methods.
attribute this to that fact that the phrase table extrac-
tion often produced one-to-many alignments when
only one alignment point was known. In Example 2
GIZA++ extraction clearly outperformed heuristic
phrase extraction. This example also shows that our
method is good at learning proper names. While
the first two examples resemble news text, Exam-
ple 3 is a more informal message. It is particularly
interesting to note that with GIZA++ extraction the
term ?shabiha? is learned, which is commonly used
in Syria to mean ?thugs? and specifically refers to
armed civilians who assault protesters against Bashir
Al-Assad?s regime. Example 4 also shows substan-
tial OOV reduction. However, the term ? rtns
 r??  (?in Opera Central?, the location of Telecom
Egypt) is incorrectly translated as ?really opera?.
7 Conclusion
We presented an approach to translation of mi-
croblog messages from the Twitter domain. The
main obstacle to state-of-the-art SMT of such data
is the complete lack of sentence-parallel training
data. We presented a technique that uses translation-
based CLIR to find relevant Arabic Twitter messages
given English Twitter queries, and applies a standard
pipeline for unsupervised training of phrase-based
SMT to retrieval results. We found this straight-
forward technique to outperform more conservative
techniques to extract phrases from comparable data
and also to outperform techniques using monolin-
gual resources for language model adaptation, meta-
parameter tuning, or self-translation.
The greatest benefit of our approach is a signifi-
cant reduction of OOV terms at a simultaneous im-
provement of correct unigram and bigram transla-
tions. Despite this positive net effect, we still find
a considerable amount of noise in the automati-
cally extracted phrase tables. Noise reduction by
improved pre-processing and by more sophisticated
training will be subject to future work. Furthermore,
we would like to investigate a tighter integration of
CLIR and SMT training by using forced decoding
techniques for CLIR and by a integrating a feedback
loop into retrieval and training.
Acknowledgments
We would like to thank Julia Ostertag for several it-
erations of manual error analysis of Arabic transla-
tion output.
418
EXAMPLE 1
SRC ?w?d?  ?ys? ?lW? Tlrt? ?A?  ? ?tq 	?K?   w? ?rtF
GOOGLE Riot troops stormed the jacket and religions foot and launches Msellat tears
NO ADAPTATION jacket riot forces storm and religions foot ?ys? ?lW? tears
LM AND DEV sitra and religions of the foot of the riot forces storm ?ys? ?lW? tears
RETRIEVAL (HEURISTIC) in sitra riot police storming and religions of tear gas on foot
RETRIEVAL (GIZA++) the riot police stormed and religions of the foot firing tear gas
REF0 vest riot forces break into wadyan by foot and trough gas tear
REF1 sotra the riot forces enter on foot and shoot tear bombs
REF2 the cover for riot police enters wadian walking and shoot tear bombs
EXAMPLE 2
SRC Yq?w`?  ?tq? ?? ?wy?  dtyF A?A?
GOOGLE Obama will speak today the death of al-Awlaki
NO ADAPTATION dtyF A?A? today killed Yq?w`? 
LM AND DEV dtyF A?A? friday for the killing of Yq?w`? 
RETRIEVAL (HEURISTIC) A?A? today on the killing of
RETRIEVAL (GIZA++) obama today on the al awlaki killing
REF0 obama will talk today about the killing of al - awlaki
REF1 obama is talking today about el awlaqi death
REF2 obama will speak today about the killing of al - awlaqi
EXAMPLE 3
SRC (: ?wy?ts? ?Am ?? TybK? 
GOOGLE Cbihh in Hama are crying :)
NO ADAPTATION TybK?  mired in calling for help : )
LM AND DEV TybK?  in hama calling for help : )
RETRIEVAL (HEURISTIC) inside the protectors of the calling for help : )
RETRIEVAL (GIZA++) shabiha in hama calling for help : )
REF0 the gangsters in hama are asking for help
REF1 the gangs in hamah are peading :)
REF2 the thugs in hama are calling for help :)
EXAMPLE 4
SRC  r??  ? rtns T?r? Y? T?rK?  Hy?C ??zt? ?AO?? T?rOm?A ?wl?A? :: ???r?
GOOGLE Freedom :: Telecom Egypt workers holding company?s president in a room Psontral Opera
NO ADAPTATION : : free workers ?AO?? T?rOm?A holding company chairman  r??  ? rtns Y? chamber
LM AND DEV : : workers free ?AO?? T?rOm?A holding company chairman Y? r??  ? rtns room
RETRIEVAL (HEURISTIC) free : : afcd T?rOm?A hold ceo hostage ppl is the president of the chamber of  r??  ? rtns
RETRIEVAL (GIZA++) egypt : : workers telecom workers are holding the head of the company in the chamber of really opera
REF0 freedom :: workers in the egyptian for communication are holding the company president in a room in the opera central
REF1 freedom , workers in egypt for calls detain the head of the company in a room in opera central
REF2 hurriya :: workers in telecom egypt detaining the president of the company in a room in the opera central
Table 9: Example output using different adaptation methods.
References
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proceedings of the
22nd ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR?99), Berkeley,
CA.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the 4th
EACL Workshop on Statistical Machine Translation,
Athens, Greece.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?09), Singapore.
M. Cettolo, M. Federico, and N. Bertoldi. 2010. Min-
ing parallel fragments from comparable texts. In Pro-
ceedings of the 7th International Workshop on Spoken
419
Language Translation, Paris, France.
Hal Daum? and Jagadeesh Jagarlamudi. 2011. Domain
adaptation for machine translation by mining unseen
words. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT?11), Portland,
OR.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Proceedings of
the 2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP?04), Barcelona, Spain.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics (ACL?05), Ann Arbor, MI.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference - North American Chapter of the Association
for Computational Linguistics annual meeting (HLT-
NAACL?06), New York, NY.
Sanjika Hewavitharana, Nguyen Bach, Qin Gao, Vamshi
Ambati, and Stephan Vogel. 2011. CMU haitian
creole-english translation system for WMT 2011. In
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, Edinburgh, Scotland, UK.
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
haitian creole emergency SMS messages. In Proceed-
ings of the 6th Workshop on Statistical Machine Trans-
lation, Edinburgh, Scotland, UK.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Birch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the ACL 2007 Demo and Poster Sessions,
Prague, Czech Republic.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based arabic word segmentation. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics (ACL?03), Sapporo, Japan.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics (COLING-ACL?06), Sydney, Aus-
tralia.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hongkong, China.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
IBM Research Division Technical Report, RC22176
(W0190-022), Yorktown Heights, N.Y.
Jay M. Ponte and Bruce W. Croft. 1998. A language
modeling approach to information retrieval. Proceed-
ings of the 21st annual international ACM SIGIR con-
ference on Research and development in information
retrieval (SIGIR?98).
Chris Quirk, Raghavendra Udupa U, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of MT Summit XI, Copenhagen , Denmark.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP?08), Honolulu, Hawaii.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Denver,
CO.
Christoph Tillmann and Jian ming Xu. 2009. A sim-
ple sentence-level extraction algorithm for comparable
data. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
420
can Chapter of the Association for Computational Lin-
guistic (NAACL-HLT?09), Boulder, CO.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(ACL?07), Prague, Czech Republic.
Stephan Vogel and Sanjika Hewavitharana. 2011. Ex-
tracting parallel phrases from comparable data. In
Proceedings of the 4th Workshop on Building and Us-
ing Comparable Corpora: Comparable Corpora and
the Web, Portland, OR.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual in-
formation retrieval. In Proceedings of the 24th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR?01),
New York, NY.
Xiaobing Xue, Jiwoon Jeon, and Bruce Croft. 2008. Re-
trieval models for question and answer archives. In
Proceedings of the 31st Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval (SIGIR?08), Singapore.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP?09), Singapore.
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The arabic online commentary dataset: an annotated
dataset of informal arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?11), Port-
land, OR.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?11), Portland, OR.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics (COLING?04), Geneva, Switzer-
land.
421
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 292?300,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Multi-Task Learning for Improved Discriminative Training in SMT
Patrick Simianer and Stefan Riezler
Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany
{simianer,riezler}@cl.uni-heidelberg.de
Abstract
Multi-task learning has been shown to be
effective in various applications, including
discriminative SMT. We present an exper-
imental evaluation of the question whether
multi-task learning depends on a ?natu-
ral? division of data into tasks that bal-
ance shared and individual knowledge, or
whether its inherent regularization makes
multi-task learning a broadly applicable
remedy against overfitting. To investi-
gate this question, we compare ?natural?
tasks defined as sections of the Interna-
tional Patent Classification versus ?ran-
dom? tasks defined as random shards in
the context of patent SMT. We find that
both versions of multi-task learning im-
prove equally well over independent and
pooled baselines, and gain nearly 2 BLEU
points over standard MERT tuning.
1 Introduction
Multi-task learning is motivated by situations
where a number of statistical models need to be es-
timated from data belonging to different tasks. It is
assumed that the data are not completely indepen-
dent of one another as they share some common-
alities, yet they differ enough to counter a simple
pooling of data. The goal of multi-task learning is
to take advantage of commonalities among tasks
by learning a shared model without neglecting in-
dividual knowledge. For example, Obozinski et
al. (2010) present an optical character recognition
scenario where data consist of samples of hand-
written characters from several writers. While the
styles of different writers vary, it is expected that
there are also commonalities on a pixel- or stroke-
level that are shared across writers. Chapelle et al
(2011) present a scenario where data from search
engine query logs are available for different coun-
tries. While the rankings for some queries will
have to be country-specific (they cite ?football?
as a query requiring different rankings in the US
and the UK), a large fraction of queries will be
country-insensitive. Wa?schle and Riezler (2012b)
present multi-task learning for statistical machine
translation (SMT) of patents from different classes
(so-called sections) according to the International
Patent Classification (IPC)1. While the vocabulary
may differ between the different IPC sections, spe-
cific legal jargon and a typical textual structure
will be shared across IPC sections. As shown in
the cited works, treating data from different writ-
ers, countries, or IPC classes as data from differ-
ent tasks, and applying generic multi-task learning
to the specific scenario, improves learning results
over learning independent or pooled models.
The research question we ask in this paper is
as follows: Is multi-task learning dependent on a
?natural? task structure in the data, where shared
and individual knowledge is properly balanced?
Or can multi-task learning be seen as a general
regularization technique that prevents overfitting
irrespective of the task structure in the data?
We investigate this research question on the ex-
ample of discriminative training for patent trans-
lation, using the algorithm for multi-task learn-
ing with `1/`2 regularization presented by Simi-
aner et al (2012). We compare multi-task learning
on ?natural? tasks given by IPC sections to multi-
task learning on ?random? tasks given by random
shards and to baseline models trained on indepen-
dent tasks and pooled tasks. We find that both
versions of multi-task learning improve over inde-
pendent or pooled training. However, differences
between multi-task learning on IPC tasks and ran-
dom tasks are small. This points to a more general
regularization effect of multi-task learning and in-
dicates a broad applicability of multi-task learning
techniques. Another advantage of the `1/`2 reg-
1http://wipo.int/classifications/ipc/
en/
292
ularization technique of Simianer et al (2012) is
a considerable efficiency gain due to paralleliza-
tion and iterative feature selection that makes the
algorithm suitable for big data applications and
for large-scale training with millions of sparse fea-
tures. Last but not least, our best result for multi-
task learning improves by nearly 2 BLEU points
over the standard MERT baseline.
2 Related Work
Multi-task learning is an active area in machine
learning, dating back at least to Caruana (1997). A
regularization perspective was introduced by Ev-
geniou and Pontil (2004), who formalize the cen-
tral idea of trading off optimality of parameter vec-
tors for each task-specific model and closeness of
these model parameters to the average parame-
ter vector across models in an SVM framework.
Equivalent formalizations replace parameter reg-
ularization by Bayesian prior distributions on the
parameters (Finkel and Manning, 2009) or by aug-
mentation of the feature space with domain inde-
pendent features (Daume?, 2007). Besides SVMs,
several learning algorithms have been extended
to the multi-task scenario in a parameter regu-
larization setting, e.g., perceptron-type algorithms
(Dredze et al, 2010) or boosting (Chapelle et
al., 2011). Further variants include different for-
malizations of norms for parameter regularization,
e.g., `1/`2 regularization (Obozinski et al, 2010)
or `1/`? regularization (Quattoni et al, 2009),
where only the features that are most important
across all tasks are kept in the model.
Early research on multi-task learning for SMT
has investigated pooling of IPC sections, with
larger pools improving results (Utiyama and Isa-
hara, 2007; Tinsley et al, 2010; Ceaus?u et al,
2011). Wa?schle and Riezler (2012b) apply multi-
task learning to tasks defined as IPC sections and
compare patent translation on independent tasks,
pooled tasks, and multi-task learning, using same-
sized training data. They show small but sta-
tistically significant improvements for multi-task
learning over independent and pooled training.
Duh et al (2010) introduce random tasks as n-best
lists of translations and showed significant im-
provements by applying various multi-task learn-
ing techniques to discriminative reranking. Song
et al (2011) define tasks as bootstrap samples
from the development set and show significant im-
provements for a bagging-based system combina-
tion over individual MERT training.
In this paper we apply the multi-task learning
technique of Simianer et al (2012) to tasks de-
fined as IPC sections and to random tasks. Their
algorithm can be seen as a weight-based back-
ward feature elimination variant of Obozinski et
al. (2010)?s gradient-based forward feature selec-
tion algorithm for `1/`2 regularization. The lat-
ter approach is related to the general methodol-
ogy of using block norms to select entire groups
of features jointly. For example, such groups can
be defined as non-overlapping subsets of features
(Yuan and Lin, 2006), or as hierarchical groups
of features (Zhao et al, 2009), or they can be
grouped by the general structure of the prediction
problem (Martins et al, 2011). However, these
approaches are concerned with grouping features
within a single prediction problem whereas multi-
task learning adds an orthogonal layer of multiple
task-specific prediction problems. By virtue of av-
eraging selected weights after each epoch, the al-
gorithm of Simianer et al (2012) is related to Mc-
Donald et al (2010)?s iterative mixing procedure.
This algorithm is itself related to the bagging pro-
cedure of Breiman (1996), if random shards are
considered from the perspective of random sam-
ples. In both cases averaging helps to reduce the
variance of the per-sample classifiers.
3 Multi-task Learning for Discriminative
Training in SMT
In multi-task learning, we have data points
{(xiz, yiz), i = 1, . . . , Nz, z = 1, . . . , Z}, sampled
from a distribution Pz on X ? Y . The subscript
z indexes tasks and the superscript i indexes i.i.d.
data for each task. For the application of discrimi-
native ranking in SMT, the spaceX can be thought
of as feature representations of n-best translations,
and the space Y denotes corresponding sentence-
level BLEU scores.2 We assume that Pz is differ-
ent for each task but that the Pz?s are related as,
for example, considered in Evgeniou and Pontil
(2004). The standard approach is to fit an inde-
pendent model involving a D-dimensional param-
eter vector wz for each task z. In multi-task learn-
ing, we consider a Z-by-D matrix W = (wdz)z,d
of stacked D-dimensional row vectors wz , and Z-
dimensional column vectors wd of weights asso-
ciated with feature d across tasks. The central al-
2See Duh et al (2010) for a similar formalization for the
case of n-best reranking via multi-task learning.
293
gorithms in most multi-task learning techniques
can be characterized as a form of regularization
that enforces closeness of task-specific parameter
vectors to shared parameter vectors, or promotes
sparse models that only contain features that are
shared across tasks. In this paper, we will fol-
low the approach of Simianer et al (2012), who
formalize multi-task learning as a distributed fea-
ture selection algorithm using `1/`2 regulariza-
tion. `1/`2 regularization can be described as pe-
nalizing weights W by the weighted `1/`2 norm,
which is defined following Obozinski et al (2010),
as
?||W||1,2 = ?
D?
d=1
||wd||2.
Each `2 norm of a weight column wd represents
the relevance of the corresponding feature across
tasks. The `1 sum of the `2 norms enforces a
selection of features by encouraging several fea-
ture columns wd to be 0 and others to have high
weights across all tasks. This results in shrinking
the matrix to the features that are useful across all
tasks.
Simianer et al (2012) achieve this behavior by
the following weight-based iterative feature elimi-
nation algorithm that is wrapped around a stochas-
tic gradient descent (SGD) algorithm for pairwise
ranking (Shen and Joshi, 2005):
Algorithm 1 Multi-task SGD
Get data for Z tasks, each including S sentences;
distribute to machines.
Initialize v? 0.
for epochs t? 0 . . . T ? 1: do
for all tasks z ? {1 . . . Z}: parallel do
wz,t,0,0 ? vfor all sentences i ? {0 . . . S ? 1}: do
Decode ith input with wz,t,i,0.for all pairs j ? {0 . . . P ? 1}: do
wz,t,i,j+1 ? wz,t,i,j ? ??lj(wz,t,i,j)end for
wz,t,i+1,0 ? wz,t,i,Pend for
end for
Stack weights W? [w1,t,S,0| . . . |wZ,t,S,0]TSelect topK feature columns of W by `2 normfor k ? 1 . . .K do
v[k] = 1Z
Z?
z=1
W[z][k].
end for
end for
return v
The innermost loop of the algorithm computes
an SGD update based on the subgradient ?lj of a
pairwise loss function. `1/`2-based feature selec-
tion is done after each epoch of SGD training for
each task in parallel. The `2 norm of the weights
is computed for each feature column across tasks;
features are sorted by this value; K top features
are kept in the model; reduced weight vectors are
mixed and the result is re-sent to each task-specific
model to start another epoch of parallel training
for each task.
We compare two different loss functions for
pairwise ranking, one corresponding to the orig-
inal perceptron algorithm (Rosenblatt, 1958), and
an improved version called the margin perceptron
(Collobert and Bengio, 2004). To create train-
ing data for a pairwise ranking setup, we gener-
ate preference pairs by ordering translations ac-
cording to smoothed sentence-wise BLEU score
(Nakov et al, 2012). Let each translation candi-
date in the n-best list be represented by a feature
vector x ? IRD: For notational convenience, we
denote by xj a preference pair xj = (x(1)j ,x(2)j )
where x(1)j is ordered above x(2)j w.r.t. BLEU. Fur-
thermore, we use the shorthand x?j = x(1)j ? x(2)j
to denote aD-dimensional difference vector repre-
senting an input pattern. For completeness, a label
y = +1 can be assigned to patterns x?j where x(1)j
is ordered above x(2)j (y = ?1 otherwise), how-
ever, since the ordering relation is antisymmetric,
we can consider an ordering in one direction and
omit the label entirely.
The original perceptron algorithm is based on
the following hinge loss-type objective function:
lj(w) = (??w, x?j ?)+
where (a)+ = max(0, a) , w ? IRD is a weight
vector, and ??, ?? denotes the standard vector dot
product. Instantiating SGD to the stochastic sub-
gradient
?lj(w) =
{
?x?j if ?w, x?j? ? 0,
0 else.
leads to the perceptron algorithm for pairwise
ranking (Shen and Joshi, 2005).
Collobert and Bengio (2004) presented a ver-
sion of perceptron learning that includes a margin
term in order to control the capacity and thus the
generalization performance. Their margin percep-
tron algorithm follows from applying SGD to the
loss function
lj(w) = (1? ?w, x?j ?)+
294
with the following stochastic subgradient
?lj(w) =
{
?x?j if ?w, x?j? < 1,
0 else.
Collobert and Bengio (2004) argue that the use of
a margin term justifies not using an explicit regu-
larization, thus making the margin perceptron an
efficient and effective learning machine.
4 Experiments
4.1 Data & System Setup
For training, development and testing, we use data
extracted from the PatTR3 corpus for the experi-
ments in Wa?schle and Riezler (2012b). Training
data consists of about 1.2 million German-English
parallel sentences. We translate from German into
English. German compound words were split us-
ing the technique of Koehn and Knight (2003). We
use the SCFG decoder cdec (Dyer et al, 2010)4
and build grammars using its implementation of
the suffix array extraction method described in
Lopez (2007). Word alignments are built from all
parallel data using mgiza5 and the Moses scripts6.
SCFG models use the same settings as described
in Chiang (2007). We built a modified Kneser-
Ney smoothed 5-gram language model using the
English side of the training data and performed
querying with KenLM (Heafield, 2011)7.
The International Patent Classification (IPC)
categorizes patents hierarchically into 8 sections,
120 classes, 600 subclasses, down to 70,000 sub-
groups at the leaf level. The eight top classes
(called sections) are listed in Table 1.
Typically, a patent belongs to more than one
section, with one section chosen as main classi-
fication. Our development and test sets for each
of the classes, A to H, comprise 2,000 sentences
each, originating from a patent with the respec-
tive class. These sets were built so that there is no
overlap of development sets and test sets, and no
overlap between sets of different classes. These
eight test sets are referred to as independent test
sets. Furthermore, we test on a combined set,
3http://www.cl.uni-heidelberg.de/
statnlpgroup/pattr
4https://github.com/redpony/cdec
5http://www.kyloo.net/software/doku.
php/mgiza:overview
6http://www.statmt.org/moses/?n=Moses.
SupportTools
7http://kheafield.com/code/kenlm/
estimation/
A Human Necessities
B Performing Operations, Transporting
C Chemistry, Metallurgy
D Textiles, Paper
E Fixed Constructions
F Mechanical Engineering, Lighting,
Heating, Weapons
G Physics
H Electricity
Table 1: IPC top level sections.
called pooled-cat, that is constructed by concate-
nating the independent sets. Additionally we use
two pooled sets for development and testing, each
containing 2,000 sentences with all classes evenly
represented.
Our tuning baseline is an implementation of hy-
pergraph MERT (Kumar et al, 2009), directly op-
timizing IBM BLEU4 (Papineni et al, 2002). Fur-
thermore, we present a regularization baseline by
applying `1 regularization with clipping (Carpen-
ter, 2008; Tsuruoka et al, 2009) to the standard
pairwise ranking perceptron. All pairwise ranking
methods use a smoothed sentence-wise BLEU+1
score (Nakov et al, 2012) to create gold standard
rankings. Our multi-task learning experiments are
based on pairwise ranking perceptrons that differ
in their objective, corresponding either to the orig-
inal perceptron or to the margin-perceptron. Both
versions of the perceptron are used for single-task
tuning and multi-task tuning. In the multi-task
setting, we compare three different methods for
defining a task: ?natural? tasks given by IPC sec-
tions where each independent data set is consid-
ered as task; ?random? tasks, defined by sharding
where data is shuffled and split once, tasks are kept
fixed throughout, and by resharding where after an
epoch data is shuffled and new random tasks are
constructed. In all cases a task/shard is defined to
contain 2,000 sentences8, resulting in eight shards
for each setting. The number of features selected
after each epoch was set to K = 100, 000.
For all perceptron runs, the following meta pa-
rameters were fixed: A cube pruning pop limit of
200 and non-terminal span limit of 15; 100-best
lists with unique entries; constant learning rate;
multipartite pair selection. Single-task perceptron
runs on independent and pooled tasks were done
8This number is determined by the size of the original de-
velopment sets; variations of this size did not change results.
295
single-task tuning
indep. 0 pooled 1 pooled-cat 2
pooled test ? 51.18 51.22
A 54.92 0255.27 055.17
B 51.53 51.48 0151.69
C 1256.31 255.90 55.74
D 49.94 050.33 050.26
E 149.19 48.97 149.13
F 1251.26 51.02 51.12
G 149.61 49.44 49.55
H 49.38 49.50 0149.67
average test 51.52 51.49 51.54
Table 2: BLEU4 results of MERT baseline using dense
features for three different tuning sets: independent (sepa-
rate tuning sets for each IPC class), pooled and pooled-cat
(concatenated independent sets). Significant superior per-
formance over other systems in the same row is denoted by
prefixed numbers. The first row shows, e.g., that the result
of pooled 1 is significantly better than independent 0, and
pooled-cat 2.
for 15 epochs; multi-task perceptron runs used
10 epochs. Single-task tuning on pooled-cat data
increases computation time by a factor of eight
which makes this setup infeasible in practice. For
the sake of comparison we performed 10 epochs
in this setup.
MERT (with default parameters) is used to op-
timize the weights of 12 dense default features;
eight translation model features, a word penalty,
the passthrough weight, the language model (LM)
score, and an LM out-of-vocabulary penalty. Per-
ceptron training allows to add millions of sparse
features which are directly derived from grammar
rules: rule shape, rule identifier, bigrams in rule
source and target. For a further explanation of
these features see Simianer et al (2012).
For testing we measured IBM BLEU4 on tok-
enized and lowercased data. Significance results
were obtained by approximate randomization tests
using the approach of Clark et al (2011)9 to ac-
count for optimizer instability. Tuning methods
with a random component (MERT, randomized
experiments) were repeated three times, scores re-
ported in the tables are averaged over optimizer
runs.
4.2 Experimental Results
In single-task tuning mode, systems are tuned
on the eight independent data sets separately, the
pooled data set, and the independent data sets con-
9https://github.com/jhclark/multeval
single-task tuning
indep. 0 pooled 1 pooled-cat 2
pooled test ? 50.75 1 52.08
A 1 55.11 54.32 01 55.94
B 1 52.61 50.84 1 52.57
C 56.18 56.11 01 56.75
D 1 50.68 49.48 01 51.22
E 1 50.27 48.69 1 50.01
F 1 51.68 50.71 1 51.95
G 1 49.90 49.06 01 50.51
H 1 50.48 49.16 1 50.53
average test 52.11 51.05 52.44
model size 430,092.5 457,428 1,574,259
Table 3: BLEU4 results for standard perceptron with `1 reg-
ularization baseline using sparse rule features, tuned on in-
dependent, pooled and pooled-cat sets. Prefixed superscripts
denote a significant improvement over the result in the same
row indicated by the superscript.
catenated (pooled-cat). Testing is done on each of
the eight IPC sections separately, and on a pooled
test set of 2,000 sentences where all sections are
equally represented. Furthermore, we report aver-
age test results over runs for all independent data
sets.
Results for the MERT baseline are shown in
Table 2: Neither pooling nor concatenating inde-
pendent sets leads to significant performance im-
provements on all sets with averaged scores being
nearly identical.
Evaluation results obtained with the standard
perceptron algorithm (Table 4) show improve-
ments over MERT in single-task tuning mode. The
gain on pooled-cat data shows that in contrast to
MERT training on 12 dense features, discrimi-
native training using large feature sets is able to
benefit from large data sets. However, since the
pooled-cat scenario increases computation time
by a factor of 8, it is quite infeasible when used
with large sets of sparse features. Single-task tun-
ing on a small set of pooled data seems to show
overfitting behavior.
Table 3 shows evaluation results for a regular-
ization baseline that applies `1 regularization with
clipping to the the single-task tuned standard per-
ceptron in Table 4. We see gains in BLEU on in-
dependent and pooled-cat tuning data, but not on
the small pooled data set.
Multi-task tuning for the standard perceptron
is shown in the right half of Table 4. Because
of parallelization, this scenario is as efficient as
296
single-task tuning multi-task tuning
indep. 0 pooled 1 pooled-cat 2 IPC 3 sharding 4 resharding 5
pooled test ? 51.33 1 51.77 12 52.56 12 52.54 12 52.60
A 54.79 54.76 01 55.31 012 56.35 012 56.22 012 56.21
B 12 52.45 51.30 1 52.19 012 52.78 0123 52.98 012 52.96
C 2 56.62 56.65 1 56.12 01245 57.76 012 57.30 012 57.44
D 1 50.75 49.88 1 50.63 01245 51.54 012 51.33 012 51.20
E 1 49.70 49.23 01 49.92 012 50.51 012 50.52 012 50.38
F 1 51.60 51.09 1 51.71 012 52.28 012 52.43 012 52.32
G 1 49.50 49.06 01 49.97 012 50.84 012 50.88 012 50.74
H 1 49.77 49.50 01 50.64 012 51.16 012 51.07 012 51.10
average test 51.90 51.42 52.06 52.90 52.84 52.79
model size 366,869.4 448,359 1,478,049 100,000 100,000 100,000
Table 4: BLEU4 results for standard perceptron algorithm using sparse rule features, tuned in single-task mode on independent,
pooled, and pooled-cat sets, and in multi-task mode on eight tasks taken from IPC sections or by random (re)sharding. Prefixed
superscripts denote a significant improvement over the result in the same row indicated by the superscript.
single-task tuning multi-task tuning
indep. 0 pooled 1 pooled-cat 2 IPC 3 sharding 4 resharding 5
pooled test ? 51.33 1 52.58 12 52.98 12 52.95 12 52.99
A 1 56.09 55.33 1 55.92 0124556.78 012 56.62 012 56.53
B 1 52.45 51.59 1 52.44 01253.31 012 53.35 012 53.21
C 1 57.20 56.85 01 57.54 0157.46 1 57.42 1 57.43
D 1 50.51 50.18 01 51.38 0124552.14 0125 51.82 012 51.66
E 1 50.27 49.36 01 50.72 012451.13 012 50.89 012 51.02
F 1 52.06 51.20 01 52.61 0124553.07 012 52.80 012 52.87
G 1 50.00 49.58 01 50.90 0124551.36 012 51.19 012 51.11
H 1 50.57 49.80 01 51.32 01251.57 012 51.62 01 51.47
average test 52.39 51.74 52.85 53.35 53.21 53.16
model size 423,731.5 484,483 1,697,398 100,000 100,000 100,000
Table 5: BLEU4 results for margin-perceptron algorithm using sparse rule features, tuned in single-task mode on independent
tasks, and in multi-task mode on eight tasks taken from IPC sections or by random (re)sharding. Prefixed superscripts denote
a significant improvement over the result in the same row indicated by the superscript.
single-task tuning on small data. We see improve-
ments in BLEU over single-task tuning on small
and large tuning data sets. Concerning our initial
research questions, we see that the performance
difference between ?natural? tasks (IPC) and ?ran-
dom? tasks is not conclusive. However, multi-
task learning using `1/`2 regularization consis-
tently outperforms the standard perceptron under
`1 regularization as shown in Table 3 and MERT
tuning as shown in Table 2.
Table 5 shows the evaluation results of the
margin-perceptron algorithm. Evaluation results
on single-task tuning show that this algorithm im-
proves over the standard perceptron (Table 4),
even in its `1-regularized version (Table 3), on
all tuning sets. Results for multi-task tuning
show improvements over the same scenario for the
standard perceptron (Table 4). This means that
the improvements due to the orthogonal regular-
ization techniques in example space and feature
space, namely large-margin learning and multi-
task learning, add up. A comparison between
single-task and multi-task tuning modes of the
margin-perceptron shows a gain for the latter sce-
narios. Differences between multi-task learning
on IPC classes versus random sharding or re-
sharding are again small, with the best overall re-
sult obtained by multi-task learning of the margin-
perceptron on IPC classes.
Overall, our best multi-task learing result is
nearly 2 BLEU points better than MERT training.
The algorithm to achieve this result is efficient due
297
to parallelization and due to iterative feature se-
lection. As shown in the last rows of Tables 3-5 ,
the average size is around 400K features for inde-
pendently tuned models and around 1.6M features
for models tuned on pooled-cat data. In multi-task
learning, models can be iteratively cut to 100K
shared features whose weights are tuned in par-
allel.
5 Conclusion
We presented an experimental investigation of the
question whether the power of multi-task learning
depends on data structured along tasks that exhibit
a proper balance of shared and individual knowl-
edge, or whether its inherent feature selection and
regularization makes multi-task learning a widely
applicable remedy against overfitting. We com-
pared multi-task patent SMT for ?natural? tasks
of IPC sections and ?random? tasks of shards in
distributed learning. Both versions of multi-task
learning yield significant improvements over in-
dependent and pooled training, however, the dif-
ference between ?natural? and ?random? tasks is
marginal. This is an indication for the useful-
ness of multi-task learning as a generic regulariza-
tion tool. Considering also the efficiency gained
by iterative feature selection, the `1/`2 regulariza-
tion algorithm presented in Simianer et al (2012)
presents itself as an efficient and effective learning
algorithm for general big data and sparse feature
applications. Furthermore, the improvements by
multi-task feature selection add up with improve-
ments by large-margin learning, delivering overall
improvements of nearly 2 BLEU points over the
standard MERT baseline.
Our research question regarding the superiority
of ?natural? or ?random? tasks was shown to be
undetermined for the application of patent trans-
lation. The obvious question for future work is if
and how a task division can be found that improves
multi-task learning over our current results. Such
an investigation will have to explore various sim-
ilarity metrics and clustering techniques for IPC
sub-classes (Wa?schle and Riezler, 2012a), e.g., for
the goal of optimizing clustering with respect to
the ratio of between-cluster to within-cluster sim-
ilarity for a given metric. However, the final crite-
rion for the usefulness of a clustering is necessar-
ily application specific (von Luxburg et al, 2012),
in our case specific to patent translation perfor-
mance. Nevertheless, we hope that the presented
and future work will prove useful and generaliz-
able for related multi-task learning scenarios.
Acknowledgments
The research presented in this paper was supported
in part by DFG grant ?Cross-language Learning-
to-Rank for Patent Retrieval?.
References
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24:123?140.
Bob Carpenter. 2008. Lazy sparse stochastic gradient
descent for regularized multinomial logistic regres-
sion. Technical report, Alias-i.
Rich Caruana. 1997. Multitask learning. Journal of
Machine Learning Research, 28.
Alexandru Ceaus?u, John Tinsley, Jian Zhang, and Andy
Way. 2011. Experiments on domain adaptation for
patent machine translation in the PLuTO project. In
Proceedings of the 15th Conference of the European
Association for Machine Translation (EAMT 2011),
Leuven, Belgium.
Olivier Chapelle, Pannagadatta Shivaswamy, Srinivas
Vadrevu, Kilian Weinberger, Ya Zhang, and Belle
Tseng. 2011. Boosted multi-task learning. Machine
Learning.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better hypothesis testing for statis-
tical machine translation: Controlling for optimizer
instability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?11), Portland, OR.
Ronan Collobert and Samy Bengio. 2004. Links be-
tween perceptrons, MLPs, and SVMs. In Proceed-
ings of the 21st International Conference on Ma-
chine Learning (ICML?04), Banff, Canada.
Hal Daume?. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?07), Prague, Czech Republic.
Mark Dredze, Alex Kulesza, and Koby Crammer.
2010. Multi-domain learning by confidence-
weighted parameter combination. Machine Learn-
ing, 79:123?149.
Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. N-best rerank-
ing by multitask learning. In Proceedings of the 5th
Joint Workshop on Statistical Machine Translation
and MetricsMATR, Uppsala, Sweden.
298
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?10).
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi-task learning. In Proceedings of
the 10th ACM SIGKDD conference on knowledge
discovery and data mining (KDD?04), Seattle, WA.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian domain adaptation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics - Human Language Technologies (NAACL-
HLT?09), Boulder, CO.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation
(WMT?11), Edinburgh, UK.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the 10th conference on European chapter of the As-
sociation for Computational Linguistics (EACL?03),
Budapest, Hungary.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In Proceedings
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th IJCNLP of
the AFNLP (ACL-IJCNLP?09, Suntec, Singapore.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoNLL, Prague, Czech Republic.
Andre? F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and Ma?rio A. T. Figueiredo. 2011. Struc-
tured sparsity in structured prediction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, Edinburgh, Scot-
land.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proceedings of Human Language Tech-
nologies: The 11th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT?10), Los Angeles,
CA.
Preslav Nakov, Francisco Guzma?n, and Stephan Vogel.
2012. Optimizing for sentence-level bleu+1 yields
short translations. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics
(COLING 2012), Bombay, India.
Guillaume Obozinski, Ben Taskar, and Michael I. Jor-
dan. 2010. Joint covariate selection and joint sub-
space selection for multiple classification problems.
Statistics and Computing, 20:231?252.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Ariadna Quattoni, Xavier Carreras, Michael Collins,
and Trevor Darrell. 2009. An efficient projec-
tion for `1,? regularization. In Proceedings of the
26th International Conference on Machine Learning
(ICML?09), Montreal, Canada.
Frank Rosenblatt. 1958. The perceptron: A probabilis-
tic model for information storage and organization in
the brain. Psychological Review, 65(6).
Libin Shen and Aravind K. Joshi. 2005. Ranking
and reranking with perceptron. Journal of Machine
Learning Research, 60(1-3):73?96.
Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012. Joint feature selection in distributed stochas-
tic learning for large-scale discriminative training in
SMT. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (ACL
2012), Jeju, Korea.
Linfeng Song, Haitao Mi, Yajuan Lu?, and Qun Liu.
2011. Bagging-based system combination for do-
main adaptation. In Proceedings of MT Summit XIII,
Xiamen, China.
John Tinsley, Andy Way, and Paraic Sheridan. 2010.
PLuTO: MT for online patent translation. In Pro-
ceedings of the 9th Conference of the Association for
Machine Translation in the Americas (AMTA 2010),
Denver, CO.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for `1-regularized log-linear models with cumulative
penalty. In Proceedings of the 47th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-IJCNLP?09), Singapore.
Masao Utiyama and Hitoshi Isahara. 2007. A
Japanese-English patent parallel corpus. In Pro-
ceedings of MT Summit XI, Copenhagen, Denmark.
Ulrike von Luxburg, Robert C. Williamson, and Is-
abelle Guyon. 2012. Clustering: Science or art?
In Proceedings of the ICML 2011 Workshop on Un-
supervised and Transfer Learning, Bellevue, WA.
Katharina Wa?schle and Stefan Riezler. 2012a. An-
alyzing parallelism and domain similarities in the
MAREC patent corpus. In Proceedings of the 5th
Information Retrieval Facility Conference (IRFC
2012), Vienna, Austria.
299
Katharina Wa?schle and Stefan Riezler. 2012b. Struc-
tural and topical dimensions in multi-task patent
translation. In Proceedings of the 13th Conference
of the European Chapter of the Association for Com-
putational Linguistics, Avignon, France.
Ming Yuan and Yi Lin. 2006. Model selection
and estimation in regression with grouped variables.
J.R.Statist.Soc.B, 68(1):49?67.
Peng Zhao, Guilherme Rocha, and Bin Yu. 2009. The
composite absolute penalties family for grouped and
hierarchical variable selection. The Annals of Statis-
tics, 37(6A):3468?3497.
300
