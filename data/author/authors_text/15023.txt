Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 905?914,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Graph Approach to Spelling Correction in Domain-Centric Search
Zhuowei Bao
University of Pennsylvania
Philadelphia, PA 19104, USA
zhuowei@cis.upenn.edu
Benny Kimelfeld
IBM Research?Almaden
San Jose, CA 95120, USA
kimelfeld@us.ibm.com
Yunyao Li
IBM Research?Almaden
San Jose, CA 95120, USA
yunyaoli@us.ibm.com
Abstract
Spelling correction for keyword-search
queries is challenging in restricted domains
such as personal email (or desktop) search,
due to the scarcity of query logs, and due to
the specialized nature of the domain. For that
task, this paper presents an algorithm that is
based on statistics from the corpus data (rather
than the query log). This algorithm, which
employs a simple graph-based approach, can
incorporate different types of data sources
with different levels of reliability (e.g., email
subject vs. email body), and can handle
complex spelling errors like splitting and
merging of words. An experimental study
shows the superiority of the algorithm over
existing alternatives in the email domain.
1 Introduction
An abundance of applications require spelling cor-
rection, which (at the high level) is the following
task. The user intends to type a chunk q of text,
but types instead the chunk s that contains spelling
errors (which we discuss in detail later), due to un-
careful typing or lack of knowledge of the exact
spelling of q. The goal is to restore q, when given
s. Spelling correction has been extensively studied
in the literature, and we refer the reader to compre-
hensive summaries of prior work (Peterson, 1980;
Kukich, 1992; Jurafsky and Martin, 2000; Mitton,
2010). The focus of this paper is on the special case
where q is a search query, and where s instead of q
is submitted to a search engine (with the goal of re-
trieving documents that match the search query q).
Spelling correction for search queries is important,
because a significant portion of posed queries may
be misspelled (Cucerzan and Brill, 2004). Effective
spelling correction has a major effect on the expe-
rience and effort of the user, who is otherwise re-
quired to ensure the exact spellings of her queries.
Furthermore, it is critical when the exact spelling is
unknown (e.g., person names like Schwarzenegger).
1.1 Spelling Errors
The more common and studied type of spelling error
is word-to-word error: a single word w is misspelled
into another single word w?. The specific spelling er-
rors involved include omission of a character (e.g.,
atachment), inclusion of a redundant character
(e.g., attachement), and replacement of charac-
ters (e.g., attachemnt). The fact that w? is a mis-
spelling of (and should be corrected to) w is denoted
by w? ? w (e.g., atachment ? attachment).
Additional common spelling errors are splitting of
a word, and merging two (or more) words:
? attach ment ? attachment
? emailattachment? email attachment
Part of our experiments, as well as most of our
examples, are from the domain of (personal) email
search. An email from the Enron email collec-
tion (Klimt and Yang, 2004) is shown in Figure 1.
Our running example is the following misspelling of
a search query, involving multiple types of errors.
sadeep kohli excellatach ment ?
sandeep kohli excel attachment (1)
In this example, correction entails fixing sadeep,
splitting excellatach, fixing excell, merging
atach ment, and fixing atachment. Beyond the
complexity of errors, this example also illustrates
other challenges in spelling correction for search.
We need to identify not only that sadeep is mis-
spelled, but also that kohli is correctly spelled.
Just having kohli in a dictionary is not enough.
905
Subject: Follow-Up on Captive Generation
From: sandeep.kohli@enron.com
X-From: Sandeep Kohli
X-To: Stinson Gibner@ECT, Vince J Kaminski@ECT
Vince/Stinson,
Please find below two attachemnts. The Excell spreadsheet
shows some calculations. . . The seond attachement (Word) has
the wordings that I think we can send in to the press. . .
I am availabel on mobile if you have questions o clarifications. . .
Regards,
Sandeep.
Figure 1: Enron email (misspelled words are underlined)
For example, in kohli coupons the user may very
well mean kohls coupons if Sandeep Kohli has
nothing to do with coupons (in contrast to the store
chain Kohl?s). A similar example is the word nail,
which is a legitimate English word, but in the con-
text of email the query nail box is likely to be
a misspelling of mail box (unless nail boxes are
indeed relevant to the user?s email collection). Fi-
nally, while the word kohli is relevant to some
email users (e.g., Kohli?s colleagues), it may have
no meaning at all to other users.
1.2 Domain Knowledge
The common approach to spelling correction uti-
lizes statistical information (Kernighan et al, 1990;
Schierle et al, 2007; Mitton, 2010). As a sim-
ple example, if we want to avoid maintaining a
manually-crafted dictionary to accommodate the
wealth of new terms introduced every day (e.g.,
ipod and ipad), we may decide that atachment
is a misspelling of attachment due to both the
(relative) proximity between the words, and the
fact that attachment is significantly more pop-
ular than atachment. As another example, the
fact that the expression sandeep kohli is fre-
quent in the domain increases our confidence in
sadeep kohli ? sandeep kohli (rather than,
e.g., sadeep kohli ? sudeep kohli). One
can further note that, in email search, the fact that
Sandeep Kohli sent multiple excel attachments in-
creases our confidence in excell ? excel.
A source of statistics widely used in prior work
is the query log (Cucerzan and Brill, 2004; Ahmad
and Kondrak, 2005; Li et al, 2006a; Chen et al,
2007; Sun et al, 2010). However, while query logs
are abundant in the context of Web search, in many
other search applications (e.g. email search, desktop
search, and even small-enterprise search) query logs
are too scarce to provide statistical information that
is sufficient for effective spelling correction. Even
an email provider of a massive scale (such as GMail)
may need to rely on the (possibly tiny) query log of
the single user at hand, due to privacy or security
concerns; moreover, as noted earlier about kohli,
the statistics of one user may be relevant to one user,
while irrelevant to another.
The focus of this paper is on spelling correction
for search applications like the above, where query-
log analysis is impossible or undesirable (with email
search being a prominent example). Our approach
relies mainly on the corpus data (e.g., the collection
of emails of the user at hand) and external, generic
dictionaries (e.g., English). As shown in Figure 1,
the corpus data may very well contain misspelled
words (like query logs do), and such noise is a part of
the challenge. Relying on the corpus has been shown
to be successful in spelling correction for text clean-
ing (Schierle et al, 2007). Nevertheless, as we later
explain, our approach can still incorporate query-log
data as features involved in the correction, as well as
means to refine the parameters.
1.3 Contribution and Outline
As said above, our goal is to devise spelling cor-
rection that relies on the corpus. The corpus often
contains various types of information, with different
levels of reliability (e.g., n-grams from email sub-
jects and sender information, vs. those from email
bodies). The major question is how to effectively
exploit that information while addressing the vari-
ous types of spelling errors such as those discussed
in Section 1.1. The key contribution of this work is
a novel graph-based algorithm, MaxPaths, that han-
dles the different types of errors and incorporates the
corpus data in a uniform (and simple) fashion. We
describe MaxPaths in Section 2. We evaluate the
effectiveness of our algorithm via an experimental
study in Section 3. Finally, we make concluding re-
marks and discuss future directions in Section 4.
2 Spelling-Correction Algorithm
In this section, we describe our algorithm for
spelling correction. Recall that given a search query
906
s of a user who intends to phrase q, the goal is to
find q. Our corpus is essentially a collection D of
unstructured or semistructured documents. For ex-
ample, in email search such a document is an email
with a title, a body, one or more recipients, and so
on. As conventional in spelling correction, we de-
vise a scoring function scoreD(r | s) that estimates
our confidence in r being the correction of s (i.e.,
that r is equal to q). Eventually, we suggest a se-
quence r from a set CD(s) of candidates, such that
scoreD(r | s) is maximal among all the candidates
in CD(s). In this section, we describe our graph-
based approach to finding CD(s) and to determining
scoreD(r | s).
We first give some basic notation. We fix an al-
phabet ? of characters that does not include any
of the conventional whitespace characters. By ??
we denote the set of all the words, namely, fi-
nite sequences over ?. A search query s is a
sequence w1, . . . , wn, where each wi is a word.
For convenience, in our examples we use whites-
pace instead of comma (e.g., sandeep kohli in-
stead of sandeep, kohli). We use the Damerau-
Levenshtein edit distance (as implemented by the
Jazzy tool) as our primary edit distance between two
words r1, r2 ? ??, and we denote this distance by
ed(r1, r2).
2.1 Word-Level Correction
We first handle a restriction of our problem, where
the search query is a single word w (rather than
a general sequence s of words). Moreover, we
consider only candidate suggestions that are words
(rather than sequences of words that account for the
case where w is obtained by merging keywords).
Later, we will use the solution for this restricted
problem as a basic component in our algorithm for
the general problem.
Let UD ? ?? be a finite universal lexicon, which
(conceptually) consists of all the words in the corpus
D. (In practice, one may want add to D words of
auxiliary sources, like English dictionary, and to fil-
ter out noisy words; we did so in the site-search do-
main that is discussed in Section 3.) The set CD(w)
of candidates is defined by
CD(w) def= {w} ? {w? ? UD | ed(w,w?) ? ?} .
for some fixed number ?. Note that CD(w) contains
Table 1: Feature set WFD in email search
Basic Features
ed(w,w?): weighted Damerau-Levenshtein edit distance
ph(w,w?): 1 if w and w? are phonetically equal, 0 otherwise
english(w?): 1 is w? is in English, 0 otherwise
Corpus-Based Features
logfreq(w?)): logarithm of #occurrences of w? in the corpus
Domain-Specific Features
subject(w?): 1 if w? is in some ?Subject? field, 0 otherwise
from(w?): 1 if w? is in some ?From? field, 0 otherwise
xfrom(w?): 1 if w? is in some ?X-From? field, 0 otherwise
w even if w is misspelled; furthermore, CD(w) may
contain other misspelled words (with a small edit
distance to w) that appear in D.
We now define scoreD(w? | w). Here, our cor-
pus D is translated into a set WFD of word features,
where each feature f ? WFD gives a scoring func-
tion scoref (w? | w). The function scoreD(w? | w) is
simply a linear combination of the scoref (w? | w):
scoreD(w? | w) def=
?
f?WFD
af ? scoref (w? | w)
As a concrete example, the features of WFD we used
in the email domain are listed in Table 1; the result-
ing scoref (w? |w) is in the spirit of the noisy channel
model (Kernighan et al, 1990). Note that additional
features could be used, like ones involving the stems
of w and w?, and even query-log statistics (when
available). Rather than manually tuning the param-
eters af , we learned them using the well known
Support Vector Machine, abbreviated SVM (Cortes
and Vapnik, 1995), as also done by Schaback and
Li (2007) for spelling correction. We further discuss
this learning step in Section 3.
We fix a natural number k, and in the sequel we
denote by topD(w) a set of k words w? ? CD(w)
with the highest scoreD(w? | w). If |CD(w)| < k,
then topD(w) is simply CD(w).
2.2 Query-Level Correction: MaxPaths
We now describe our algorithm, MaxPaths, for
spelling correction. The input is a (possibly mis-
spelled) search query s = s1, . . . , sn. As done in
the word-level correction, the algorithm produces a
set CD(s) of suggestions and determines the values
907
Algorithm 1 MaxPaths
Input: a search query s
Output: a set CD(s) of candidate suggestions r,
ranked by scoreD(r | s)
1: Find the strongly plausible tokens
2: Construct the correction graph
3: Find top-k full paths (with the largest weights)
4: Re-rank the paths by word correlation
scoreD(r | s), for all r ? CD(s), in order to rank
CD(s). A high-level overview of MaxPaths is given
in the pseudo-code of Algorithm 1. In the rest of this
section, we will detail each of the four steps in Al-
gorithm 1. The name MaxPaths will become clear
towards the end of this section.
We use the following notation. For a word w =
c1 ? ? ? cm of m characters ci and integers i < j
in {1, . . . ,m + 1}, we denote by w[i,j) the word
ci ? ? ? cj?1. For two words w1, w2 ? ??, the word
w1w2 ? ?? is obtained by concatenating w1 and
w2. Note that for the search query s = s1, . . . , sn
it holds that s1 ? ? ? sn is a single word (in ??). We
denote the word s1 ? ? ? sn by bsc. For example, if
s1 = sadeep and s2 = kohli, then s corresponds
to the query sadeep kohli while bsc is the word
sadeepkohli; furthermore, bsc[1,7) = sadeep.
2.2.1 Plausible Tokens
To support merging and splitting, we first iden-
tify the possible tokens of the given query s. For
example, in excellatach ment we would like to
identify excell and atach ment as tokens, since
those are indeed the tokens that the user has in mind.
Formally, suppose that bsc = c1 ? ? ? cm. A token is
a word bsc[i,j) where 1 ? i < j ? m + 1. To
simplify the presentation, we make the (often false)
assumption that a token bsc[i,j) uniquely identifies
i and j (that is, bsc[i,j) 6= bsc[i?,j?) if i 6= i? or
j 6= j?); in reality, we should define a token as a
triple (bsc[i,j), i, j). In principle, every token bsc[i,j)
could be viewed as a possible word that user meant
to phrase. However, such liberty would require our
algorithm to process a search space that is too large
to manage in reasonable time. Instead, we restrict to
strongly plausible tokens, which we define next.
A token w = bsc[i,j) is plausible if w is a word
of s, or there is a word w? ? CD(w) (as defined in
Section 2.1) such that scoreD(w? | w) > ? for some
fixed number ?. Intuitively, w is plausible if it is an
original token of s, or we have a high confidence in
our word-level suggestion to correct w (note that the
suggested correction for w can be w itself). Recall
that bsc = c1 ? ? ? cm. A tokenization of s is a se-
quence j1, . . . , jl, such that j1 = 1, jl = m+1, and
ji < ji+1 for 1 ? i < l. The tokenization j1, . . . , jl
induces the tokens bsc[j1,j2),. . . ,bsc[jl?1,jl). A tok-
enization is plausible if each of its induced tokens
is plausible. Observe that a plausible token is not
necessarily induced by any plausible tokenization;
in that case, the plausible token is useless to us.
Thus, we define a strongly plausible token, abbre-
viated sp-token, which is a token that is induced by
some plausible tokenization. As a concrete example,
for the query excellatach ment, the sp-tokens in
our implementation include excellatach, ment,
excell, and atachment.
As the first step (line 1 in Algorithm 1), we find
the sp-tokens by employing an efficient (and fairly
straightforward) dynamic-programming algorithm.
2.2.2 Correction Graph
In the next step (line 2 in Algorithm 1), we con-
struct the correction graph, which we denote by
GD(s). The construction is as follows.
We first find the set topD(w) (defined in Sec-
tion 2.1) for each sp-token w. Table 2 shows the sp-
tokens and suggestions thereon in our running exam-
ple. This example shows the actual execution of our
implementation within email search, where s is the
query sadeep kohli excellatach ment; for
clarity of presentation, we omitted a few sp-tokens
and suggested corrections. Observe that some of
the corrections in the table are actually misspelled
words (as those naturally occur in the corpus).
A node of the graph GD(s) is a pair ?w,w??, where
w is an sp-token and w? ? topD(w). Recall our
simplifying assumption that a token bsc[i,j) uniquely
identifies the indices i and j. The graph GD(s) con-
tains a (directed) edge from a node ?w1, w?1? to a
node ?w2, w?2? if w2 immediately follows w1 in bqc;
in other words, GD(s) has an edge from ?w1, w?1?
to ?w2, w?2? whenever there exist indices i, j and k,
such that w1 = bsc[i,j) and w2 = bsc[j,k). Observe
that GD(s) is a directed acyclic graph (DAG).
908
except
excell
excel
excellence
excellent
sandeep
jaideep
kohli
attachement
attachment
attached
sandeep kohli
sent
meet
ment
Figure 2: The graph GD(s)
For example, Figure 2 shows GD(s) for the
query sadeep kohli excellatach ment, with
the sp-tokens w and the sets topD(w) being those of
Table 2. For now, the reader should ignore the node
in the grey box (containing sandeep kohli) and
its incident edges. For simplicity, in this figure we
depict each node ?w,w?? by just mentioning w?; the
word w is in the first row of Table 2, above w?.
2.2.3 Top-k Paths
Let P = ?w1, w?1? ? ? ? ? ? ?wk, w?k? be a path
in GD(s). We say that P is full if ?w1, w?1? has no
incoming edges in GD(s), and ?wk, w?k? has no out-
going edges in GD(s). An easy observation is that,
since we consider only strongly plausible tokens, if
P is full then w1 ? ? ?wk = bsc; in that case, the se-
quence w?1, . . . , w?k is a suggestion for spelling cor-
rection, and we denote it by crc(P ). As an example,
Figure 3 shows two full paths P1 and P2 in the graph
GD(s) of Figure 2. The corrections crc(Pi), for
i = 1, 2, are jaideep kohli excellent ment
and sandeep kohli excel attachement, re-
spectively.
To obtain corrections crc(P ) with high quality,
we produce a set of k full paths with the largest
weights, for some fixed k; we denote this set by
topPathsD(s). The weight of a path P , denoted
weight(P ), is the sum of the weights of all the nodes
and edges in P , and we define the weights of nodes
and edges next. To find these paths, we use a well
known efficient algorithm (Eppstein, 1994).
kohli
kohli
excellent ment
excel attachment
jaideep
sandeep
P1
P2
Figure 3: Full paths in the graph GD(s) of Figure 2
Consider a node u = ?w,w?? of GD(s). In the
construction of GD(s), zero or more merges of (part
of) original tokens have been applied to obtain the
token w; let #merges(w) be that number. Consider
an edge e of GD(s) from a node u1 = ?w1, w?1? to
u2 = ?w2, w?2?. In s, either w1 and w2 belong to
different words (i.e., there is a whitespace between
them) or not; in the former case define #splits(e) =
0, and in the latter #splits(e) = 1. We define:
weight(u) def= scoreD(w? | w) + am ?#merges(w)
weight(e) def= as ?#splits(e)
Note that am and as are negative, as they penalize
for merges and splits, respectively. Again, in our
implementations, we learned am and as by means
of SVM.
Recall that topPathsD(s) is the set of k full paths
(in the graph GD(s)) with the largest weights. From
topPathsD(s) we get the set CD(s) of candidate
suggestions:
CD(s) def= {crc(P ) | P ? topPathsD(s)} .
2.2.4 Word Correlation
To compute scoreD(r|s) for r ? CD(s), we incor-
porate correlation among the words of r. Intuitively,
we would like to reward a candidate with pairs of
words that are likely to co-exist in a query. For
that, we assume a (symmetric) numerical function
crl(w?1, w?2) that estimates the extent to which the
words w?1 and w?2 are correlated. As an example, in
the email domain we would like crl(kohli, excel)
to be high if Kohli sent many emails with excel at-
tachments. Our implementation of crl(w?1, w?2) es-
sentially employs pointwise mutual information that
has also been used in (Schierle et al, 2007), and that
909
Table 2: topD(w) for sp-tokens w
sadeep kohli excellatach ment excell atachment
sandeep kohli excellent ment excel attachmentjaideep excellence sent excell attachedmeet except attachement
compares the number of documents (emails) con-
taining w?1 and w?2 separately and jointly.
Let P ? topPathsD(s) be a path. We de-
note by crl(P ) a function that aggregates the num-
bers crl(w?1, w?2) for nodes ?w1, w?1? and ?w2, w?2?
of P (where ?w1, w?1? and ?w2, w?2? are not nec-
essarily neighbors in P ). Over the email domain,
our crl(P ) is the minimum of the crl(w?1, w?2). We
define scoreD(P ) = weight(P ) + crl(P ). To
improve the performance, in our implementation
we learned again (re-trained) all the parameters in-
volved in scoreD(P ).
Finally, as the top suggestions we take crc(P )
for full paths P with highest scoreD(P ). Note that
crc(P ) is not necessarily injective; that is, there can
be two full paths P1 6= P2 satisfying crc(P1) =
crc(P2). Thus, in effect, scoreD(r | s) is determined
by the best evidence of r; that is,
scoreD(r | s) def= max{scoreD(P ) | crc(P ) = r?
P ? topPathsD(s)} .
Note that our final scoring function essentially views
P as a clique rather than a path. In principle,
we could define GD(s) in a way that we would
extract the maximal cliques directly without find-
ing topPathsD(s) first. However, we chose our
method (finding top paths first, and then re-ranking)
to avoid the inherent computational hardness in-
volved in finding maximal cliques.
2.3 Handling Expressions
We now briefly discuss our handling of frequent n-
grams (expressions). We handle n-grams by intro-
ducing new nodes to the graph GD(s); such a new
node u is a pair ?t, t??, where t is a sequence of
n consecutive sp-tokens and t? is a n-gram. The
weight of such a node u is rewarded for consti-
tuting a frequent or important n-gram. An exam-
ple of such a node is in the grey box of Figure 2,
where sandeep kohli is a bigram. Observe that
sandeep kohli may be deemed an important bi-
gram because it occurs as a sender of an email, and
not necessarily because it is frequent.
An advantage of our approach is avoidance
of over-scoring due to conflicting n-grams. For
example, consider the query textile import
expert, and assume that both textile import
and import export (with an ?o? rather than an
?e?) are frequent bigrams. If the user referred to the
bigram textile import, then expert is likely to
be correct. But if she meant for import export,
then expert is misspelled. However, only one of
these two options can hold true, and we would like
textile import export to be rewarded only
once?for the bigram import export. This is
achieved in our approach, since a full path in GD(s)
may contain either a node for textile import or
a node for import export, but it cannot contain
nodes for both of these bigrams.
Finally, we note that our algorithm is in the spirit
of that of Cucerzan and Brill (2004), with a few in-
herent differences. In essence, a node in the graph
they construct corresponds to what we denote here
as ?w,w?? in the special case where w is an actual
word of the query; that is, no re-tokenization is ap-
plied. They can split a word by comparing it to a bi-
gram. However, it is not clear how they can split into
non-bigrams (without a huge index) and to handle si-
multaneous merging and splitting as in our running
example (1). Furthermore, they translate bigram in-
formation into edge weights, which implies that the
above problem of over-rewarding due to conflicting
bigrams occurs.
3 Experimental Study
Our experimental study aims to investigate the ef-
fectiveness of our approach in various settings, as
we explain next.
3.1 Experimental Setup
We first describe our experimental setup, and specif-
ically the datasets and general methodology.
Datasets. The focus of our experimental study is
on personal email search; later on (Section 3.6),
we will consider (and give experimental results for)
a totally different setting?site search over www.
ibm.com, which is a massive and open domain.
Our dataset (for the email domain) is obtained from
910
the Enron email collection (Bekkerman et al, 2004;
Klimt and Yang, 2004). Specifically, we chose the
three users with the largest number of emails. We re-
fer to the three email collections by the last names of
their owners: Farmer, Kaminski and Kitchen. Each
user mailbox is a separate domain, with a separate
corpus D, that one can search upon. Due to the ab-
sence of real user queries, we constructed our dataset
by conducting a user study, as described next.
For each user, we randomly sampled 50 emails
and divided them into 5 disjoint sets of 10 emails
each. We gave each 10-email set to a unique hu-
man subject that was asked to phrase two search
queries for each email: one for the entire email con-
tent (general query), and the other for the From and
X-From fields (sender query). (Figure 1 shows ex-
amples of the From and X-From fields.) The latter
represents queries posed against a specific field (e.g.,
using ?advanced search?). The participants were not
told about the goal of this study (i.e., spelling correc-
tion), and the collected queries have no spelling er-
rors. For generating spelling errors, we implemented
a typo generator.1 This generator extends an online
typo generator (Seobook, 2010) that produces a vari-
ety of spelling errors, including skipped letter, dou-
bled letter, reversed letter, skipped space (merge),
missed key and inserted key; in addition, our gener-
ator produces inserted space (split). When applied
to a search query, our generator adds random typos
to each word, independently, with a specified prob-
ability p that is 50% by default. For each collected
query (and for each considered value of p) we gener-
ated 5 misspelled queries, and thereby obtained 250
instances of misspelled general queries and 250 in-
stances of misspelled sender queries.
Methodology. We compared the accuracy of
MaxPaths (Section 2) with three alternatives. The
first alternative is the open-source Jazzy, which
is a widely used spelling-correction tool based on
(weighted) edit distance. The second alternative is
the spelling correction provided by Google. We
provided Jazzy with our unigram index (as a dic-
tionary). However, we were not able to do so
with Google, as we used remote access via its Java
API (Google, 2010); hence, the Google tool is un-
1The queries and our typo generator are publicly available
at https://dbappserv.cis.upenn.edu/spell/.
aware of our domain, but is rather based on its
own statistics (from the World Wide Web). The
third alternative is what we call WordWise, which
applies word-level correction (Section 2.1) to each
input query term, independently. More precisely,
WordWise is a simplified version of MaxPaths,
where we forbid splitting and merging of words (i.e.,
only the original tokens are considered), and where
we do not take correlation into account.
Our emphasis is on correcting misspelled queries,
rather than recognizing correctly spelled queries,
due to the role of spelling in a search engine: we
wish to provide the user with the correct query upon
misspelling, but there is no harm in making a sug-
gestion for correctly spelled queries, except for vi-
sual discomfort. Hence, by default accuracy means
the number of properly corrected queries (within
the top-k suggestions) divided by the number of the
misspelled queries. An exception is in Section 3.5,
where we study the accuracy on correct queries.
Since MaxPaths and WordWise involve parame-
ter learning (SVM), the results for them are consis-
tently obtained by performing 5-folder cross valida-
tion over each collection of misspelled queries.
3.2 Fixed Error Probability
Here, we compare MaxPaths to the alternatives
when the error probability p is fixed (0.5). We con-
sider only the Kaminski dataset; the results for the
other two datasets are similar. Figure 4(a) shows the
accuracy, for general queries, of top-k suggestions
for k = 1, k = 3 and k = 10. Note that we can get
only one (top-1) suggestion from Google. As can
be seen, MaxPaths has the highest accuracy in all
cases. Moreover, the advantage of MaxPaths over
the alternatives increases as k increases, which indi-
cates potential for further improving MaxPaths.
Figure 4(b) shows the accuracy of top-k sugges-
tions for sender queries. Overall, the results are sim-
ilar to those of Figure 4(a), except that top-1 of both
WordWise and MaxPaths has a higher accuracy in
sender queries than in general queries. This is due
to the fact that the dictionaries of person names and
email addresses extracted from the X-From and
From fields, respectively, provide strong features
for the scoring function, since a sender query refers
to these two fields. In addition, the accuracy of
MaxPaths is further enhanced by exploiting the cor-
911
0%
20%
40%
60%
80%
100%
Top 1 Top 3 Top 10
Google Jazzy WordWise MaxPaths
(a) General queries (Kaminski)
0%
20%
40%
60%
80%
100%
Top 1 Top 3 Top 10
Google Jazzy WordWise MaxPaths
(b) Sender queries (Kaminski)
0%
25%
50%
75%
100%
0% 20% 40% 60% 80% 100%
Google Jazzy WordWise MaxPaths
Spelling Error Probability
(c) Varying error probability (Kaminski)
Figure 4: Accuracy for Kaminski (misspelled queries)
relation between the first and last name of a person.
3.3 Impact of Error Probability
We now study the impact of the complexity of
spelling errors on our algorithm. For that, we mea-
sure the accuracy while the error probability p varies
from 10% to 90% (with gaps of 20%). The re-
sults are in Figure 4(c). Again, we show the results
only for Kaminski, since we get similar results for
the other two datasets. As expected, in all exam-
ined methods the accuracy decreases as p increases.
Now, not only does MaxPaths outperform the alter-
natives, its decrease (as well as that of WordWise) is
the mildest?13% as p increases from 10% to 90%
(while Google and Jazzy decrease by 23% or more).
We got similar results for the sender queries (and for
each of the three users).
3.4 Adaptiveness of Parameters
Obtaining the labeled data needed for parameter
learning entails a nontrivial manual effort. Ideally,
we would like to learn the parameters of MaxPaths
in one domain, and use them in similar domains.
0%
25%
50%
75%
100%
0% 20% 40% 60% 80% 100%
Google Jazzy MaxPaths* MaxPaths
Spelling Error Probability
(a) General queries (Farmer)
0%
25%
50%
75%
100%
0% 20% 40% 60% 80% 100%
Google Jazzy MaxPaths* MaxPaths
Spelling Error Probability
(b) Sender queries (Farmer)
Figure 5: Accuracy for Farmer (misspelled queries)
More specifically, our desire is to use the parame-
ters learned over one corpus (e.g., the email collec-
tion of one user) on a second corpus (e.g., the email
collection of another user), rather than learning the
parameters again over the second corpus. In this set
of experiments, we examine the feasibility of that
approach. Specifically, we consider the user Farmer
and observe the accuracy of our algorithm with two
sets of parameters: the first, denoted by MaxPaths in
Figures 5(a) and 5(b), is learned within the Farmer
dataset, and the second, denoted by MaxPaths?, is
learned within the Kaminski dataset. Figures 5(a)
and 5(b) show the accuracy of the top-1 suggestion
for general queries and sender queries, respectively,
with varying error probabilities. As can be seen,
these results mean good news?the accuracies of
MaxPaths? and MaxPaths are extremely close (their
curves are barely distinguishable, as in most cases
the difference is smaller than 1%). We repeated this
experiment for Kitchen and Kaminski, and got sim-
ilar results.
3.5 Accuracy for Correct Queries
Next, we study the accuracy on correct queries,
where the task is to recognize the given query as cor-
rect by returning it as the top suggestion. For each
of the three users, we considered the 50 + 50 (gen-
eral + sender) collected queries (having no spelling
errors), and measured the accuracy, which is the
percentage of queries that are equal to the top sug-
912
Table 3: Accuracy for Correct Queries
Dataset Google Jazzy MaxPaths
Kaminski (general) 90% 98% 94%
Kaminski (sender) 94% 98% 94%
Farmer (general) 96% 98% 96%
Farmer (sender) 96% 96% 92%
Kitchen (general) 86% 100% 92%
Kitchen (sender) 94% 100% 98%
gestion. Table 3 shows the results. Since Jazzy is
based on edit distance, it almost always gives the in-
put query as the top suggestion; the misses of Jazzy
are for queries that contain a word that is not the cor-
pus. MaxPaths is fairly close to the upper bound set
by Jazzy. Google (having no access to the domain)
also performs well, partly because it returns the in-
put query if no reasonable suggestion is found.
3.6 Applicability to Large-Scale Site Search
Up to now, our focus has been on email search,
which represents a restricted (closed) domain with
specialized knowledge (e.g., sender names). In this
part, we examine the effectiveness of our algorithm
in a totally different setting?large-scale site search
within www.ibm.com, a domain that is popular on
a world scale. There, the accuracy of Google is very
high, due to this domain?s popularity, scale, and full
accessibility on the Web. We crawled 10 million
documents in that domain to obtain the corpus. We
manually collected 1348 misspelled queries from
the log of search issued against developerWorks
(www.ibm.com/developerworks/) during a
week. To facilitate the manual collection of these
queries, we inspected each query with two or fewer
search results, after applying a random permutation
to those queries. Figure 6 shows the accuracy of
top-k suggestions. Note that the performance of
MaxPaths is very close to that of Google?only 2%
lower for top-1. For k = 3 and k = 10, MaxPaths
outperforms Jazzy and the top-1 of Google (from
which we cannot obtain top-k for k > 1).
3.7 Summary
To conclude, our experiments demonstrate various
important qualities of MaxPaths. First, it outper-
forms its alternatives, in both accuracy (Section 3.2)
and robustness to varying error complexities (Sec-
tion 3.3). Second, the parameters learned in one
domain (e.g., an email user) can be applied to sim-
0%
20%
40%
60%
80%
100%
Top 1 Top 3 Top 10
Google Jazzy WordWise MaxPaths
Figure 6: Accuracy for site search
ilar domains (e.g., other email users) with essen-
tially no loss in performance (Section 3.4). Third,
it is highly accurate in recognition of correct queries
(Section 3.5). Fourth, even when applied to large
(open) domains, it achieves a comparable perfor-
mance to the state-of-the-art Google spelling correc-
tion (Section 3.6). Finally, the higher performance
of MaxPaths on top-3 and top-10 corrections sug-
gests a potential for further improvement of top-1
(which is important since search engines often re-
strict their interfaces to only one suggestion).
4 Conclusions
We presented the algorithm MaxPaths for spelling
correction in domain-centric search. This algo-
rithm relies primarily on corpus statistics and do-
main knowledge (rather than on query logs). It can
handle a variety of spelling errors, and can incor-
porate different levels of spelling reliability among
different parts of the corpus. Our experimental study
demonstrates the superiority of MaxPaths over ex-
isting alternatives in the domain of email search, and
indicates its effectiveness beyond that domain.
In future work, we plan to explore how to utilize
additional domain knowledge to better estimate the
correlation between words. Particularly, from avail-
able auxiliary data (Fagin et al, 2010) and tools like
information extraction (Chiticariu et al, 2010), we
can infer and utilize type information from the cor-
pus (Li et al, 2006b; Zhu et al, 2007). For instance,
if kohli is of type person, and phone is highly cor-
related with person instances, then phone is highly
correlated with kohli even if the two words do not
frequently co-occur. We also plan to explore as-
pects of corpus maintenance in dynamic (constantly
changing) domains.
913
References
F. Ahmad and G. Kondrak. 2005. Learning a spelling
error model from search query logs. In HLT/EMNLP.
R. Bekkerman, A. Mccallum, and G. Huang. 2004. Au-
tomatic categorization of email into folders: Bench-
mark experiments on Enron and Sri Corpora. Techni-
cal report, University of Massachusetts - Amherst.
Q. Chen, M. Li, and M. Zhou. 2007. Improving
query spelling correction using Web search results. In
EMNLP-CoNLL, pages 181?189.
L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,
F. Reiss, and S. Vaithyanathan. 2010. SystemT: An
algebraic approach to declarative information extrac-
tion. In ACL, pages 128?137.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine Learning, 20(3):273?297.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of Web users. In EMNLP, pages 293?300.
D. Eppstein. 1994. Finding the k shortest paths. In
FOCS, pages 154?165.
R. Fagin, B. Kimelfeld, Y. Li, S. Raghavan, and
S. Vaithyanathan. 2010. Understanding queries in a
search database system. In PODS, pages 273?284.
Google. 2010. A Java API for Google spelling check ser-
vice. http://code.google.com/p/google-api-spelling-
java/.
D. Jurafsky and J. H. Martin. 2000. Speech and
Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and
Speech Recognition. Prentice Hall PTR.
M. D. Kernighan, K. W. Church, and W. A. Gale. 1990.
A spelling correction program based on a noisy chan-
nel model. In COLING, pages 205?210.
B. Klimt and Y. Yang. 2004. Introducing the Enron cor-
pus. In CEAS.
K. Kukich. 1992. Techniques for automatically correct-
ing words in text. ACM Comput. Surv., 24(4):377?
439.
M. Li, M. Zhu, Y. Zhang, and M. Zhou. 2006a. Explor-
ing distributional similarity based models for query
spelling correction. In ACL.
Y. Li, R. Krishnamurthy, S. Vaithyanathan, and H. V. Ja-
gadish. 2006b. Getting work done on the web: sup-
porting transactional queries. In SIGIR, pages 557?
564.
R. Mitton. 2010. Fifty years of spellchecking. Wring
Systems Research, 2:1?7.
J. L. Peterson. 1980. Computer Programs for Spelling
Correction: An Experiment in Program Design, vol-
ume 96 of Lecture Notes in Computer Science.
Springer.
J. Schaback and F. Li. 2007. Multi-level feature extrac-
tion for spelling correction. In AND, pages 79?86.
M. Schierle, S. Schulz, and M. Ackermann. 2007. From
spelling correction to text cleaning - using context in-
formation. In GfKl, Studies in Classification, Data
Analysis, and Knowledge Organization, pages 397?
404.
Seobook. 2010. Keyword typo generator.
http://tools.seobook.com/spelling/keywords-
typos.cgi.
X. Sun, J. Gao, D. Micol, and C. Quirk. 2010. Learning
phrase-based spelling error models from clickthrough
data. In ACL, pages 266?274.
H. Zhu, S. Raghavan, S. Vaithyanathan, and A. Lo?ser.
2007. Navigating the intranet with high precision. In
WWW, pages 491?500.
914
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adaptive Parser-Centric Text Normalization
Congle Zhang?
Dept of Computer Science and Engineering
University of Washington, Seattle, WA 98195, USA
clzhang@cs.washington.edu
Tyler Baldwin Howard Ho Benny Kimelfeld Yunyao Li
IBM Research - Almaden
650 Harry Road, San Jose, CA 95120, USA
{tbaldwi,ctho,kimelfeld,yunyaoli}@us.ibm.com
Abstract
Text normalization is an important first
step towards enabling many Natural Lan-
guage Processing (NLP) tasks over infor-
mal text. While many of these tasks, such
as parsing, perform the best over fully
grammatically correct text, most existing
text normalization approaches narrowly
define the task in the word-to-word sense;
that is, the task is seen as that of mapping
all out-of-vocabulary non-standard words
to their in-vocabulary standard forms. In
this paper, we take a parser-centric view
of normalization that aims to convert raw
informal text into grammatically correct
text. To understand the real effect of nor-
malization on the parser, we tie normal-
ization performance directly to parser per-
formance. Additionally, we design a cus-
tomizable framework to address the often
overlooked concept of domain adaptabil-
ity, and illustrate that the system allows for
transfer to new domains with a minimal
amount of data and effort. Our experimen-
tal study over datasets from three domains
demonstrates that our approach outper-
forms not only the state-of-the-art word-
to-word normalization techniques, but also
manual word-to-word annotations.
1 Introduction
Text normalization is the task of transforming in-
formal writing into its standard form in the lan-
guage. It is an important processing step for a
wide range of Natural Language Processing (NLP)
tasks such as text-to-speech synthesis, speech
recognition, information extraction, parsing, and
machine translation (Sproat et al, 2001).
?This work was conducted at IBM.
The use of normalization in these applications
poses multiple challenges. First, as it is most often
conceptualized, normalization is seen as the task
of mapping all out-of-vocabulary non-standard
word tokens to their in-vocabulary standard forms.
However, the scope of the task can also be seen as
much wider, encompassing whatever actions are
required to convert the raw text into a fully gram-
matical sentence. This broader definition of the
normalization task may include modifying punc-
tuation and capitalization, and adding, removing,
or reordering words. Second, as with other NLP
techniques, normalization approaches are often fo-
cused on one primary domain of interest (e.g.,
Twitter data). Because the style of informal writ-
ing may be different in different data sources,
tailoring an approach towards a particular data
source can improve performance in the desired do-
main. However, this is often done at the cost of
adaptability.
This work introduces a customizable normal-
ization approach designed with domain transfer in
mind. In short, customization is done by provid-
ing the normalizer with replacement generators,
which we define in Section 3. We show that the
introduction of a small set of domain-specific gen-
erators and training data allows our model to out-
perform a set of competitive baselines, including
state-of-the-art word-to-word normalization. Ad-
ditionally, the flexibility of the model also allows it
to attempt to produce fully grammatical sentences,
something not typically handled by word-to-word
normalization approaches.
Another potential problem with state-of-the-art
normalization is the lack of appropriate evaluation
metrics. The normalization task is most frequently
motivated by pointing to the need for clean text
for downstream processing applications, such as
syntactic parsing. However, most studies of nor-
malization give little insight into whether and to
what degree the normalization process improves
1159
the performance of the downstream application.
For instance, it is unclear how performance mea-
sured by the typical normalization evaluation met-
rics of word error rate and BLEU score (Pap-
ineni et al, 2002) translates into performance on
a parsing task, where a well placed punctuation
mark may provide more substantial improvements
than changing a non-standard word form. To ad-
dress this problem, this work introduces an eval-
uation metric that ties normalization performance
directly to the performance of a downstream de-
pendency parser.
The rest of this paper is organized as follows.
In Section 2 we discuss previous approaches to
the normalization problem. Section 3 presents
our normalization framework, including the actual
normalization and learning procedures. Our in-
stantiation of this model is presented in Section 4.
In Section 5 we introduce the parser driven eval-
uation metric, and present experimental results of
our model with respect to several baselines in three
different domains. Finally, we discuss our exper-
imental study in Section 6 and conclude in Sec-
tion 7.
2 Related Work
Sproat et al (2001) took the first major look at
the normalization problem, citing the need for nor-
malized text for downstream applications. Unlike
later works that would primarily focus on specific
noisy data sets, their work is notable for attempt-
ing to develop normalization as a general process
that could be applied to different domains. The re-
cent rise of heavily informal writing styles such as
Twitter and SMS messages set off a new round of
interest in the normalization problem.
Research on SMS and Twitter normalization has
been roughly categorized as drawing inspiration
from three other areas of NLP (Kobus et al, 2008):
machine translation, spell checking, and automatic
speech recognition. The statistical machine trans-
lation (SMT) metaphor was the first proposed to
handle the text normalization problem (Aw et al,
2006). In this mindset, normalizing SMS can be
seen as a translation task from a source language
(informal) to a target language (formal), which can
be undertaken with typical noisy channel based
models. Work by Choudhury et al (2007) adopted
the spell checking metaphor, casting the problem
in terms of character-level, rather than word-level,
edits. They proposed an HMM based model that
takes into account both grapheme and phoneme
information. Kobus et al (2008) undertook a
hybrid approach that pulls inspiration from both
the machine translation and speech recognition
metaphors.
Many other approaches have been examined,
most of which are at least partially reliant on
the above three metaphors. Cook and Steven-
son (2009) perform an unsupervised method,
again based on the noisy channel model. Pen-
nell and Liu (2011) developed a CRF tagger for
deletion-based abbreviation on tweets. Xue et
al. (2011) incorporated orthographic, phonetic,
contextual, and acronym expansion factors to nor-
malize words in both Twitter and SMS. Liu et
al. (2011) modeled the generation process from
dictionary words to non-standard tokens under an
unsupervised sequence labeling framework. Han
and Baldwin (2011) use a classifier to detect ill-
formed words, and then generate correction can-
didates based on morphophonemic similarity. Re-
cent work has looked at the construction of nor-
malization dictionaries (Han et al, 2012) and on
improving coverage by integrating different hu-
man perspectives (Liu et al, 2012).
Although it is almost universally used as a mo-
tivating factor, most normalization work does not
directly focus on improving downstream appli-
cations. While a few notable exceptions high-
light the need for normalization as part of text-
to-speech systems (Beaufort et al, 2010; Pennell
and Liu, 2010), these works do not give any di-
rect insight into how much the normalization pro-
cess actually improves the performance of these
systems. To our knowledge, the work presented
here is the first to clearly link the output of a nor-
malization system to the output of the downstream
application. Similarly, our work is the first to pri-
oritize domain adaptation during the new wave of
text message normalization.
3 Model
In this section we introduce our normalization
framework, which draws inspiration from our pre-
vious work on spelling correction for search (Bao
et al, 2011).
3.1 Replacement Generators
Our input the original, unnormalized text, repre-
sented as a sequence x = x1, x2, . . . , xn of tokens
xi. In this section we will use the following se-
1160
quence as our running example:
x = Ay1 woudent2 of3 see4 ?em5
where space replaces comma for readability, and
each token is subscripted by its position. Given the
input x, we apply a series of replacement genera-
tors, where a replacement generator is a function
that takes x as input and produces a collection of
replacements. Here, a replacement is a statement
of the form ?replace tokens xi, . . . , xj?1 with s.?
More precisely, a replacement is a triple ?i, j, s?,
where 1 ? i ? j ? n + 1 and s is a sequence of
tokens. Note that in the case where i = j, the se-
quence s should be inserted right before xi; and in
the special case where s is empty, we simply delete
xi, . . . , xj?1. For instance, in our running exam-
ple the replacement ?2, 3,would not? replaces
x2 = woudent with would not; ?1, 2,Ay? re-
places x1 with itself (hence, does not change x);
?1, 2, ? (where  is the empty sequence) deletes
x1; ?6, 6,.? inserts a period at the end of the se-
quence.
The provided replacement generators can be ei-
ther generic (cross domain) or domain-specific, al-
lowing for domain customization. In Section 4,
we discuss the replacement generators used in our
empirical study.
3.2 Normalization Graph
Given the input x and the set of replacements pro-
duced by our generators, we associate a unique
Boolean variable Xr with each replacement r. As
expected, Xr being true means that the replace-
ment r takes place in producing the output se-
quence.
Next, we introduce dependencies among vari-
ables. We first discuss the syntactic consistency
of truth assignments. Let r1 = ?i1, j1, s1? and
r2 = ?i2, j2, s2? be two replacements. We say
that r1 and r2 are locally consistent if the inter-
vals [i1, j1) and [i2, j2) are disjoint. Moreover,
we do not allow two insertions to take place at
the same position; therefore, we exclude [i1, j1)
and [i2, j2) from the definition of local consistency
when i1 = j1 = i2 = j2. If r1 and r2 are locally
consistent and j1 = i2, then we say that r2 is a
consistent follower of r1.
A truth assignment ? to our variables Xr is
sound if every two replacements r and r? with
?(Xr) = ?(Xr?) = true are locally consis-
tent. We say that ? is complete if every token
of x is captured by at least one replacement r
with ?(Xr) = true. Finally, we say that ?
is legal if it is sound and complete. The out-
put (normalized sequence) defined by a legal as-
signment ? is, naturally, the concatenation (from
left to right) of the strings s in the replacements
r = ?i, j, s? with ?(Xr) = true. In Fig-
ure 1, for example, if the nodes with a grey
shade are the ones associated with true vari-
ables under ?, then the output defined by ? is
I would not have seen them.
Our variables carry two types of interdependen-
cies. The first is that of syntactic consistency: the
entire assignment is required to be legal. The sec-
ond captures correlation among replacements. For
instance, if we replace of with have in our run-
ning example, then the next see token is more
likely to be replaced with seen. In this work,
dependencies of the second type are restricted to
pairs of variables, where each pair corresponds to
a replacement and a consistent follower thereof.
The above dependencies can be modeled over a
standard undirected graph using Conditional Ran-
dom Fields (Lafferty et al, 2001). However, the
graph would be complex: in order to model lo-
cal consistency, there should be edges between ev-
ery two nodes that violate local consistency. Such
a model renders inference and learning infeasi-
ble. Therefore, we propose a clearer model by a
directed graph, as illustrated in Figure 1 (where
nodes are represented by replacements r instead
of the variables Xr, for readability). To incorpo-
rate correlation among replacements, we introduce
an edge from Xr to Xr? whenever r? is a consis-
tent follower of r. Moreover, we introduce two
dummy nodes, start and end, with an edge from
start to each variable that corresponds to a prefix
of the input sequence x, and an edge from each
variable that corresponds to a suffix of x to end.
The principal advantage of modeling the depen-
dencies in such a directed graph is that now, the le-
gal assignments are in one-to-one correspondence
with the paths from start to end; this is a straight-
forward observation that we do not prove here.
We appeal to the log-linear model formulation
to define the probability of an assignment. The
conditional probability of an assignment ?, given
an input sequence x and the weight vector ? =
??1, . . . , ?k? for our features, is defined as p(? |
1161
?1, 2,I?
end
?2, 4,would not have?
?1, 2,Ay?
?5, 6,them?
?4, 5,seen?
?2, 3,would?
?4, 6,see him?
?3, 4,of?
start
?6, 6, .?
Figure 1: Example of a normalization graph; the
nodes are replacements generated by the replace-
ment generators, and every path from start to end
implies a legal assignment
x,?) = 0 if ? is not legal, and otherwise,
p(? | x,?) = 1Z(x)
?
X?Y ??
exp(
?
j
?j?j(X,Y,x)) .
Here, Z(x) is the partition function, X ? Y ? ?
refers to an edge X ? Y with ?(X) = true and
?(Y ) = true, and ?1(X,Y,x), . . . , ?k(X,Y,x)
are real valued feature functions that are weighted
by ?1, . . . , ?k (the model?s parameters), respec-
tively.
3.3 Inference
When performing inference, we wish to select
the output sequence with the highest probability,
given the input sequence x and the weight vector
? (i.e., MAP inference). Specifically, we want an
assignment ?? = arg max? p(? | x,?).
While exact inference is computationally hard
on general graph models, in our model it boils
down to finding the longest path in a weighted
and acyclic directed graph. Indeed, our directed
graph (illustrated in Figure 1) is acyclic. We as-
sign the real value ?j ?j?j(X,Y,x) to the edge
X ? Y , as the weight. As stated in Section 3.2,
a legal assignment ? corresponds to a path from
start to end; moreover, the sum of the weights on
that path is equal to log p(? | x,?) + logZ(x).
In particular, a longer path corresponds to an as-
signment with greater probability. Therefore, we
can solve the MAP inference within our model by
finding the weighted longest path in the directed
acyclic graph. The algorithm in Figure 2 summa-
rizes the inference procedure to normalize the in-
put sequence x.
Input:
1. A sequence x to normalize;
2. A weight vector ? = ??1, . . . , ?k?.
Generate replacements: Apply all replace-
ment generators to get a set of replacements r,
each r is a triple ?i, j, s?.
Build a normalization graph:
1. For each replacement r, create a node Xr.
2. For each r? and r, create an edge Xr to
Xr? if r? is a consistent follower of r.
3. Create two dummy nodes start and end,
and create edges from start to all prefix
nodes and end to all suffix nodes.
4. For each edge X ? Y , compute the fea-
tures ?j(X,Y,x), and weight the edge by?
j ?j?j(X,Y,x).
MAP Inference: Find a weighted longest path
P from start to end, and return ??, where
??(Xr) = true iff Xr ? P .
Figure 2: Normalization algorithm
3.4 Learning
Our labeled data consists of pairs (xi,ygoldi ),
where xi is an input sequence (to normalize) and
ygoldi is a (manually) normalized sequence. We
obtain a truth assignment ?goldi from each ygoldi
by selecting an assignment ? that minimizes the
edit distance between ygoldi and the normalized
text implied by ?:
?goldi = arg min? DIST(y(?),y
gold
i ) (1)
Here, y(?) denotes the normalized text implied by
?, and DIST is a token-level edit distance. We
apply a simple dynamic-programming algorithm
to compute ?goldi . Finally, the items in our training
data are the pairs (xi, ?goldi ).
Learning over similar models is commonly
done via maximum likelihood estimation:
L(?) = log
?
i
p(?i = ?goldi | xi,?)
Taking the partial derivative gives the following:
?
i
(
?j(?goldi ,xi)? Ep(?i|xi,?)?j(?i,xi)
)
where ?j(?,x) = ?X?Y ?j(X,Y,x), that is,
the sum of values for the jth feature along the
1162
Input:
1. A set {(xi,ygoldi )}
n
i=1 of sequences andtheir gold normalization;
2. Number T of iterations.
Initialization: Initialize each ?j as zero, and
obtain each ?goldi according to (1).
Repeat T times:
1. Infer each ??i from xi using the current ?;
2. ?j ? ?j+?i(?j(?goldi ,xi)??j(??i ,xi))
for all j = 1, . . . , k.
Output: ? = ??1, . . . , ?k?
Figure 3: Learning algorithm
path defined by ?, andEp(?i|xi,?)?j(?i,xi) is the
expected value of that sum (over all legal assign-
ments ?i), assuming the current weight vector.
How to efficiently compute
Ep(?i|xi,?)?j(?i,xi) in our model is un-
clear; naively, it requires enumerating all legal
assignments. We instead opt to use a more
tractable perceptron-style algorithm (Collins,
2002). Instead of computing the expectation,
we simply compute ?j(??i ,xi), where ??i is the
assignment with the highest probability, generated
using the current weight vector. The result is then:
?
i
(
?j(?goldi ,xi)? ?j(??i ,xi)
)
Our learning applies the following two steps it-
eratively. (1) Generate the most probable sequence
within the current weights. (2) Update the weights
by comparing the path generated in the previous
step to the gold standard path. The algorithm in
Figure 3 summarizes the procedure.
4 Instantiation
In this section, we discuss our instantiation of the
model presented in the previous section. In partic-
ular, we describe our replacement generators and
features.
4.1 Replacement Generators
One advantage of our proposed model is that
the reliance on replacement generators allows for
strong flexibility. Each generator can be seen as a
black box, allowing replacements that are created
heuristically, statistically, or by external tools to be
incorporated within the same framework.
Generator From To
leave intact good good
edit distance bac back
lowercase NEED need
capitalize it It
Google spell disspaear disappear
contraction wouldn?t would not
slang language ima I am going to
insert punctuation  .
duplicated punctuation !? !
delete filler lmao 
Table 1: Example replacement generators
To build a set of generic replacement generators
suitable for normalizing a variety of data types, we
collected a set of about 400 Twitter posts as devel-
opment data. Using that data, a series of gener-
ators were created; a sample of them are shown
in Table 1. As shown in the table, these gener-
ators cover a variety of normalization behavior,
from changing non-standard word forms to insert-
ing and deleting tokens.
4.2 Features
Although the proposed framework supports real
valued features, all features in our system are bi-
nary. In total, we used 70 features. Our feature set
pulls information from several different sources:
N-gram: Our n-gram features indicate the fre-
quency of the phrases induced by an edge. These
features are turned into binary ones by bucketing
their log values. For example, on the edge from
?1, 2,I? to ?2, 3,would? such a feature will indi-
cate whether the frequency of I would is over
a threshold. We use the Corpus of Contemporary
English (Davies, 2008 ) to produce our n-gram in-
formation.
Part-of-speech: Part-of-speech information
can be used to produce features that encourage
certain behavior, such as avoiding the deletion of
noun phrases. We generate part-of-speech infor-
mation over the original raw text using a Twit-
ter part-of-speech tagger (Ritter et al, 2011). Of
course, the part-of-speech information obtained
this way is likely to be noisy, and we expect our
learning algorithm to take that into account.
Positional: Information from positions is used
primarily to handle capitalization and punctuation
insertion, for example, by incorporating features
for capitalized words after stop punctuation or the
insertion of stop punctuation at the end of the sen-
tence.
Lineage: Finally, we include binary features
1163
that indicate which generator spawned the replace-
ment.
5 Evaluation
In this section, we present an empirical study of
our framework. The study is done over datasets
from three different domains. The goal is to eval-
uate the framework in two aspects: (1) usefulness
for downstream applications (specifically depen-
dency parsing), and (2) domain adaptability.
5.1 Evaluation Metrics
A few different metrics have been used to evaluate
normalizer performance, including word error rate
and BLEU score. While each metric has its pros
and cons, they all rely on word-to-word matching
and treat each word equally. In this work, we aim
to evaluate the performance of a normalizer based
on how it affects the performance of downstream
applications. We find that the conventional metrics
are not directly applicable, for several reasons. To
begin with, the assumption that words have equal
weights is unlikely to hold. Additionally, these
metrics tend to ignore other important non-word
information such as punctuation or capitalization.
They also cannot take into account other aspects
that may have an impact on downstream perfor-
mance, such as the word reordering as seen in the
example in Figure 4. Therefore, we propose a new
evaluation metric that directly equates normaliza-
tion performance with the performance of a com-
mon downstream application?dependency pars-
ing.
To realize our desired metric, we apply the fol-
lowing procedure. First, we produce gold standard
normalized data by manually normalizing sen-
tences to their full grammatically correct form. In
addition to the word-to-word mapping performed
in typical normalization gold standard generation,
this annotation procedure includes all actions nec-
essary to make the sentence grammatical, such as
word reordering, modifying capitalization, and re-
moving emoticons. We then run an off-the-shelf
dependency parser on the gold standard normal-
ized data to produce our gold standard parses. Al-
though the parser could still produce mistakes on
the grammatical sentences, we feel that this pro-
vides a realistic benchmark for comparison, as it
represents an upper bound on the possible perfor-
mance of the parser, and avoids an expensive sec-
ond round of manual annotation.
Test Gold SVO
I kinda wanna get
ipad NEW
I kind of want to
get a new iPad.
verb(get) verb(want)verb(get)
precisionv = 11
recallv = 12
subj(get,I)
subj(get,wanna)
obj(get,NEW)
subj(want,I)
subj(get,I)
obj(get,iPad)
precisionso = 13
recallso = 13
Figure 4: The subjects, verbs, and objects identi-
fied on example test/gold text, and corresponding
metric scores
To compare the parses produced over automati-
cally normalized data to the gold standard, we look
at the subjects, verbs, and objects (SVO) identi-
fied in each parse. The metric shown in Equa-
tions (2) and (3) below is based on the identified
subjects and objects in those parses. Note that SO
denotes the set of identified subjects and objects
whereas SOgold denotes the set of subjects and
objects identified when parsing the gold-standard
normalization.
precisionso =
|SO ? SOgold|
|SO | (2)
recallso = |SO ? SO
gold|
|SOgold|
(3)
We similarly define precisionv and recallv, where
we compare the set V of identified verbs to V gold
of those found in the gold-standard normalization.
An example is shown in Figure 4.
5.2 Results
To establish the extensibility of our normaliza-
tion system, we present results in three different
domains: Twitter posts, Short Message Service
(SMS) messages, and call-center logs. For Twitter
and SMS messages, we used established datasets
to compare with previous work. As no estab-
lished call-center log dataset exists, we collected
our own. In each case, we ran the proposed system
with two different configurations: one using only
the generic replacement generators presented in
Section 4 (denoted as generic), and one that adds
additional domain-specific generators for the cor-
responding domain (denoted as domain-specific).
All runs use ten-fold cross validation for training
and evaluation. The Stanford parser1 (Marneffe
et al, 2006) was used to produce all dependency
1Version 2.0.4, http://nlp.stanford.edu/
software/lex-parser.shtml
1164
parses. We compare our system to the following
baseline solutions:
w/oN: No normalization is performed.
Google: Output of the Google spell checker.
w2wN: The output of the word-to-word normal-
ization of Han and Baldwin (2011). Not available
for call-center data.
Gw2wN: The manual gold standard word-to-
word normalizations of previous work (Choud-
hury et al, 2007; Han and Baldwin, 2011). Not
available for call-center data.
Our results use the metrics of Section 5.1.
5.2.1 Twitter
To evaluate the performance on Twitter data, we
use the dataset of randomly sampled tweets pro-
duced by (Han and Baldwin, 2011). Because the
gold standard used in this work only provided
word mappings for out-of-vocabulary words and
did not enforce grammaticality, we reannotated the
gold standard data2. Their original gold standard
annotations were kept as a baseline.
To produce Twitter-specific generators, we ex-
amined the Twitter development data collected for
generic generator production (Section 4). These
generators focused on the Twitter-specific notions
of hashtags (#), ats (@), and retweets (RT). For
each case, we implemented generators that al-
lowed for either the initial symbol or the entire to-
ken to be deleted (e.g., @Hertz to Hertz, @Hertz
to ).
The results are given in Table 2. As shown,
the domain-specific generators yielded perfor-
mance significantly above the generic ones and all
baselines. Even without domain-specific genera-
tors, our system outperformed the word-to-word
normalization approaches. Most notably, both
the generic and domain-specific systems outper-
formed the gold standard word-to-word normal-
izations. These results validate the hypothesis that
simple word-to-word normalization is insufficient
if the goal of normalization is to improve depen-
dency parsing; even if a system could produce
perfect word-to-word normalization, it would pro-
duce lower quality parses than those produced by
our approach.
2Our results and the reannotations of the Twitter and SMS
data are available at https://www.cs.washington.
edu/node/9091/
System Verb Subject-ObjectPre Rec F1 Pre Rec F1
w/oN 83.7 68.1 75.1 31.7 38.6 34.8
Google 88.9 78.8 83.5 36.1 46.3 40.6
w2wN 87.5 81.5 84.4 44.5 58.9 50.7
Gw2w 89.8 83.8 86.7 46.9 61.0 53.0
generic 91.7 88.9 90.3 53.6 70.2 60.8
domain specific 95.3 88.7 91.9 72.5 76.3 74.4
Table 2: Performance on Twitter dataset
5.2.2 SMS
To evaluate the performance on SMS data, we use
the Treasure My Text data collected by Choud-
hury et al (2007). As with the Twitter data, the
word-to-word normalizations were reannotated to
enforce grammaticality. As a replacement genera-
tor for SMS-specific substitutions, we used a map-
ping dictionary of SMS abbreviations.3 No further
SMS-specific development data was needed.
Table 3 gives the results on the SMS data. The
SMS dataset proved to be more difficult than the
Twitter dataset, with the overall performance of
every system being lower. While this drop of per-
formance may be a reflection of the difference in
data styles between SMS and Twitter, it is also
likely a product of the collection methodology.
The collection methodology of the Treasure My
Text dataset dictated that every message must have
at least one mistake, which may have resulted in a
dataset that was noisier than average.
Nonetheless, the trends on SMS data mirror
those on Twitter data, with the domain-specific
generators achieving the greatest overall perfor-
mance. However, while the generic setting still
manages to outperform most baselines, it did not
outperform the gold word-to-word normalization.
In fact, the gold word-to-word normalization was
much more competitive on this data, outperform-
ing even the domain-specific system on verbs
alone. This should not be seen as surprising, as
word-to-word normalization is most likely to be
beneficial for cases like this where the proportion
of non-standard tokens is high.
It should be noted that the SMS dataset as avail-
able has had all punctuation removed. While this
may be appropriate for word-to-word normaliza-
tion, this preprocessing may have an effect on the
parse of the sentence. As our system has the abil-
ity to add punctuation but our baseline systems do
not, this has the potential to artificially inflate our
results. To ensure a fair comparison, we manually
3http://www.netlingo.com/acronyms.php
1165
System Verb Subject-ObjectRec Pre F1 Rec Pre F1
w/oN 76.4 48.1 59.0 19.5 21.5 20.4
Google 85.1 61.6 71.5 22.4 26.2 24.1
w2wN 78.5 61.5 68.9 29.9 36.0 32.6
Gw2wN 87.6 76.6 81.8 38.0 50.6 43.4
generic 86.5 77.4 81.7 35.5 47.7 40.7
domain specific 88.1 75.0 81.0 41.0 49.5 44.8
Table 3: Performance on SMS dataset
System Verb Subject-ObjectPre Rec F1 Pre Rec F1
w/oN 98.5 97.1 97.8 69.2 66.1 67.6
Google 99.2 97.9 98.5 70.5 67.3 68.8
generic 98.9 97.4 98.1 71.3 67.9 69.6
domain specific 99.2 97.4 98.3 87.9 83.1 85.4
Table 4: Performance on call-center dataset
added punctuation to a randomly selected small
subset of the SMS data and reran each system.
This experiment suggested that, in contrast to the
hypothesis, adding punctuation actually improved
the results of the proposed system more substan-
tially than that of the baseline systems.
5.2.3 Call-Center
Although Twitter and SMS data are unmistakably
different, there are many similarities between the
two, such as the frequent use of shorthand word
forms that omit letters. The examination of call-
center logs allows us to examine the ability of our
system to perform normalization in more disparate
domains. Our call-center data consists of text-
based responses to questions about a user?s expe-
rience with a call-center (e.g., their overall satis-
faction with the service). We use call-center logs
from a major company, and collect about 150 re-
sponses for use in our evaluation. We collected
an additional small set of data to develop our call-
center-specific generators.
Results on the call-center dataset are in Table 4.
As shown, the raw call-center data was compar-
atively clean, resulting in higher baseline perfor-
mance than in other domains. Unlike on previ-
ous datasets, the use of generic mappings only
provided a small improvement over the baseline.
However, the use of domain-specific generators
once again led to significantly increased perfor-
mance on subjects and objects.
6 Discussion
The results presented in the previous section sug-
gest that domain transfer using the proposed nor-
malization framework is possible with only a
small amount of effort. The relatively modest
set of additional replacement generators included
in each data set alowed the domain-specific ap-
proaches to significantly outperform the generic
approach. In the call-center case, performance im-
provements could be seen by referencing a very
small amount of development data. In the SMS
case, the presence of a domain-specific dictionary
allowed for performance improvements without
the need for any development data at all. It is
likely, though not established, that employing fur-
ther development data would result in further per-
formance improvements. We leave further investi-
gation to future work.
The results in Section 5.2 establish a point that
has often been assumed but, to the best of our
knowledge, has never been explicitly shown: per-
forming normalization is indeed beneficial to de-
pendency parsing on informal text. The parse of
the normalized text was substantially better than
the parse of the original raw text in all domains,
with absolute performance increases ranging from
about 18-25% on subjects and objects. Further-
more, the results suggest that, as hypothesized,
preparing an informal text for a parsing task re-
quires more than simple word-to-word normaliza-
tion. The proposed approach significantly outper-
forms the state-of-the-art word-to-word normal-
ization approach. Perhaps most interestingly, the
proposed approach performs on par with, and in
several cases superior to, gold standard word-to-
word annotations. This result gives strong evi-
dence for the conclusion that parser-targeted nor-
malization requires a broader understanding of the
scope of the normalization task.
While the work presented here gives promis-
ing results, there are still many behaviors found
in informal text that prove challenging. One
such example is the word reordering seen in Fig-
ure 4. Although word reordering could be incor-
porated into the model as a combination of a dele-
tion and an insertion, the model as currently de-
vised cannot easily link these two replacements
to one another. Additionally, instances of re-
ordering proved hard to detect in practice. As
such, no reordering-based replacement generators
were implemented in the presented system. An-
other case that proved difficult was the insertion
of missing tokens. For instance, the informal
sentence ?Day 3 still don?t freaking
1166
feel good!:(? could be formally rendered
as ?It is day 3 and I still do not
feel good!?. Attempts to address missing to-
kens in the model resulted in frequent false pos-
itives. Similarly, punctuation insertion proved to
be challenging, often requiring a deep analysis
of the sentence. For example, contrast the sen-
tence ?I?m watching a movie I don?t
know its name.? which would benefit from
inserted punctuation, with ?I?m watching a
movie I don?t know.?, which would not.
We feel that the work presented here provides a
foundation for future work to more closely exam-
ine these challenges.
7 Conclusions
This work presents a framework for normalization
with an eye towards domain adaptation. The pro-
posed framework builds a statistical model over a
series of replacement generators. By doing so, it
allows a designer to quickly adapt a generic model
to a new domain with the inclusion of a small set of
domain-specific generators. Tests over three dif-
ferent domains suggest that, using this model, only
a small amount of domain-specific data is neces-
sary to tailor an approach towards a new domain.
Additionally, this work introduces a parser-
centric view of normalization, in which the per-
formance of the normalizer is directly tied to the
performance of a downstream dependency parser.
This evaluation metric allows for a deeper under-
standing of how certain normalization actions im-
pact the output of the parser. Using this met-
ric, this work established that, when dependency
parsing is the goal, typical word-to-word normal-
ization approaches are insufficient. By taking a
broader look at the normalization task, the ap-
proach presented here is able to outperform not
only state-of-the-art word-to-word normalization
approaches but also manual word-to-word annota-
tions.
Although the work presented here established
that more than word-to-word normalization was
necessary to produce parser-ready normalizations,
it remains unclear which specific normalization
tasks are most critical to parser performance. We
leave this interesting area of examination to future
work.
Acknowledgments
We thank the anonymous reviewers of ACL for
helpful comments and suggestions. We also thank
Ioana R. Stanoi for her comments on a prelim-
inary version of this work, Daniel S. Weld for
his support, and Alan Ritter, Monojit Choudhury,
Bo Han, and Fei Liu for sharing their tools and
data. The first author is partially supported by the
DARPA Machine Reading Program under AFRL
prime contract numbers FA8750-09-C-0181 and
FA8750-09-C-0179. Any opinions, findings, con-
clusions, or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA, AFRL, or the
US government. This work is a part of IBM?s Sys-
temT project (Chiticariu et al, 2010).
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normal-
ization. In ACL, pages 33?40.
Zhuowei Bao, Benny Kimelfeld, and Yunyao Li. 2011.
A graph approach to spelling correction in domain-
centric search. In ACL, pages 905?914.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normal-
izing sms messages. In ACL, pages 770?779.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Sriram Raghavan, Frederick Reiss, and Shivaku-
mar Vaithyanathan. 2010. SystemT: An algebraic
approach to declarative information extraction. In
ACL, pages 128?137.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR, 10(3-4):157?174.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1?8.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC, pages 71?78.
Mark Davies. 2008-. The corpus of contempo-
rary american english: 450 million words, 1990-
present. Avialable online at: http://corpus.
byu.edu/coca/.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In ACL, pages 368?378.
1167
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In EMNLP-CoNLL, pages 421?432.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing SMS: are two
metaphors better than one? In COLING, pages 441?
448.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML, pages 282?289.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor
supervision. In ACL, pages 71?76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broad-coverage normalization system for social me-
dia language. In ACL, pages 1035?1044.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Deana Pennell and Yang Liu. 2010. Normalization of
text messages for text-to-speech. In ICASSP, pages
4842?4845.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
SMS abbreviations. In IJCNLP, pages 974?982.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in Tweets: An ex-
perimental study. In EMNLP, pages 1524?1534.
Richard Sproat, Alan W. Black, Stanley F. Chen,
Shankar Kumar, Mari Ostendorf, and Christopher
Richards. 2001. Normalization of non-standard
words. Computer Speech & Language, 15(3):287?
333.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison.
2011. Normalizing microtext. In Analyzing Micro-
text, volume WS-11-05 of AAAI Workshops.
1168
