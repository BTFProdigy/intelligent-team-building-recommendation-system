Evaluation of Automatically Identified Index Terms 
for Browsing Electronic Documents I 
Nina Wacholder, Judith L. Klavans and David K. Evans 
Columbia University 
Department of Computer Science and 
Center for Research on Information Access 
1. Abstract 
We present an evaluation of domain- 
independent atural anguage tools for use in 
the identification of significant concepts in 
documents. Using qualitative evaluation, we 
compare three shallow processing methods for 
extracting index terms, i.e., terms that can be 
used to model the content of documents. We 
focus on two criteria: quality and coverage. In 
terms of quality alone, our results show that 
technical term (TT) extraction \[Justeson and 
Katz 1995\] receives the highest rating. How- 
ever, in terms of a combined quality and cover- 
age metric, the Head Sorting (HS) method, 
described in \[Wacholder 1998\], outperforms 
both other methods, keyword (KW) and TT. 
2. Introduction 
In this paper, we consider the problem of how 
to evaluate the automatic identification of index 
terms that have been derived without recourse 
to lexicons or to other kinds of domain-specific 
information. By index terms, we mean natural 
language xpressions that constitute a meaning- 
ful representation f a document for humans. 
The premise of this research is that if signifi- 
cant topics coherently represent information in 
a document, hese topics can be used as index 
terms that approximate the content of individ- 
ual documents in large collections of electronic 
documents. 
We compare three shallow processing 
methods for identifying index terms: 
? Keywords (KW) are terms identified by 
counting frequency of stemmed words in a 
document; 
Technical terms (TT) are noun phrases 
(NPs) or subparts of NPs repeated more 
than twice in a document \[Justeson and 
Katz 1995\]; 
Head sorted terms (HS) are identified by 
a method in which simplex noun phrases 
(as defined below) are sorted by head and 
then ranked in decreasing order of fre- 
quency \[Wacholder 1998\]. 
The three methods that we evaluated are do- 
main-independent i  that they use statistical 
and/or linguistic properties that apply to any 
natural anguage document in any field. These 
methods are also corpus-independent, i  that 
the ranking of terms for an individual document 
is not dependent on properties of the corpus. 
2.1 Overview of methods and results 
Subjects were drawn from two groups: 
professionals and students. Professionals in- 
cluded librarians and publishing professionals 
familiar with both manual and automatic text 
indexing. Students included undergraduate and 
graduate students with a variety of academic 
interests. 
To assess terms, we used a standard 
qualitative ranking technique. We presented 
subjects with an article and a list of terms 
identified by one of the three methods. Subjects 
were asked to answer the following general 
question: "Would this term be useful in an 
electronic index for this article?" Terms were 
rated on a scale of 1 to 5, where 1 indicates a
high quality term that should definitely be in- 
cluded in the index and 5 indicates a junk term 
that definitely should not be included. For ex- 
1 This research was partly funded by NSF IRI 97-12069, "Automatic identification ofsignificant topics in do- 
main independent full text documents" and NSF IRI 97-53054, "Computationally tractable methods for docu- 
ment analysis". _'tO~ 
ample, the phrase court-approved affirmative 
action plans received an average rating of 1 
from the professionals, meaning that it was 
ranked as useful for the article; the KW af- 
firmative received an average rating of 3.75, 
meaning that it was less useful; and the KW 
action received an average ranking of 4.5, 
meaning that it was not useful. 
The goal of our research is to determine 
which method, or combination of methods, 
provides the best results. We measure results 
in terms of two criteria: quality and coverage. 
By quality, we mean that evaluators ranked 
terms high on the 1 to 5 scale from highest o 
lowest. By coverage, we mean the thoroughness 
with which the terms cover the significant op- 
ics in the document. Our methodology permits 
us to measure both criteria, as shown in Figure 
4. 
Our results from both the professionals and 
students how that TTs are superior with re- 
spect to quality; however, there are only a 
small number of TTs per document, so they do 
not provide adequate coverage in that they are 
not fully representative of the document as a 
whole. In contrast, KWs provide good cover- 
age but relatively poor quality in that KWs are 
vague, and not well filtered. SNPs, which have 
been sorted using HS and filtered, provide a 
better balance of quality and coverage. 
From our study, we draw the following 
conclusions: 
? The KW approach identifies some useful 
index terms, but they are mixed in with a 
large number of low-ranked terms. 
? The TT approach identifies high quality 
terms, but with low coverage, i.e., rela- 
tively few indexing terms. 
? The HS approach achieves a balance be- 
tween quality and coverage. 
3. Domain-independent metrics for identi- 
fying significant opics 
In order to identify significant opics in 
a document, a significance measure is needed, 
i.e., a method for determining which concepts 
in the document are relatively important for a 
given task. The need to determine the impor- 
tance of a particular concept within a document 
is motivated by a range of applications, in- 
cluding information retrieval \[Salton 1989\], 
303 
automatic determination of authorship 
\[Mosteller and Wallace 1963\], similarity met- 
rics for cross-document clustering \[Hatzivas- 
siloglou et al 1999\], automatic indexing 
\[Hodges et al 1996\] and input to summariza- 
tion \[Paice 1990\]. 
For example, one of the earlier appli- 
cations using frequency for identifying signifi- 
cant topics in a document was proposed by 
\[Luhn 1958\] for use in creating automatic ab- 
stracts. For each document, a list of stop- 
listed stems was created, and ranked by fre- 
quency; the most frequent keywords were used 
to identify significant sentences in the original 
document. Luhn's premise was that emphasis, 
as indicated by repetition of words and collo- 
cation is an indicator of significance. Namely, 
"the more often certain words are found in each 
other's company within a sentence, the more 
significance may be attributed to each of these 
words." This basic observation, although re- 
fined extensively by later summarization tech- 
niques (as reviewed in \[Paice 1990\]), relies on 
the capability of identifying significant con- 
cepts. 
The standard IR technique known as 
tf*idf \[Salton 1989\] seeks to identify docu- 
ments relevant o a particular query by relativ- 
izing keyword frequency in a document as 
compared to frequency in a corpus. This 
method can be used to locate at least some im- 
portant concepts in full text. Although it has 
been effective for information retrieval, for 
other applications, such as human-oriented in- 
dexing, this technique is impractical. Ambigu- 
ity of stems (trad might refer to trader or 
tradition) and of isolated words (state might be 
a political entity or a mode of being) means 
that lists of keywords have not usually been 
used to represent the content of a document to 
human beings. Furthermore, humans have a 
difficult time processing stems and parts of 
words out of phrasal context. 
The technical term (TT) method, an- 
other technique for identification of significant 
terms in text that can be used as index terms 
was introduced by \[Justeson and Katz 1995\], 
who developed an algorithm for identifying 
repeated multi-word phrases such as central 
processing unit in the computer domain or 
word sense in the lexical semantic domain. 
This algorithm identifies candidate TTs in a 
corpus by locating NPs consisting of nouns, 
adjectives, and sometimes prepositional 
phrases. TTs are defined as those NPs, or their 
subparts, which occur above some frequency 
threshold in a corpus. However, as \[Boguraev 
and Kennedy 1998\] observe, the TT technique 
may not characterize the full content of docu- 
ments. Indeed, even in a technical document, 
TTs do not provide adequate coverage of the 
NPs in a document that contribute to its con- 
tent, especially since TTs are by definition 
multi-word. A truly domain-general method 
should apply to both technical and non- 
technical documents. The relevant difference 
between technical and non-technical documents 
is that in technical documents, many of the 
topics which are significant to the document as 
a whole may be also TTs. 
\[Wacholder 1998\] proposed the 
method of Head Sorting for identifying signifi- 
cant topics that can be used to represent a
source document. HS also uses a frequency 
measure to provide an approximation of topic 
significance. However, instead of counting fre- 
quency of stems or repetition of word se- 
quences, this method counts frequency of a 
relatively easily identified grammatical e ement, 
heads of simplex noun phrases (SNPs). For 
common NPs (NPs whose head is a common 
noun), an SNP is a maximal NP that includes 
premodifiers uch as determiners and posses- 
sives but not post-nominal constituents such as 
prepositions or relativizers. For example, the 
well-known book is an SNP but the well-known 
book on asteroids includes two SNPs, well- 
known book and asteroids. For proper names, 
an SNP is a name that refers to a single entity. 
For example, Museum of the City of New York, 
the name of an organization, is an SNP even 
though the organizational name incorporates a 
city name. Others, such as \[Church 1988\], 
have discussed a similar concept, sometimes 
called simple or base NPs. 
The HS approach is based on the as- 
sumption that nominal elements can be used to 
convey the gist of a document. SNPs, which 
are semantically and syntactically coherent, 
appear to be at a good level of detail for con- 
tent representation f the document. ' 
304 
SNPs are identified by a system \[Evans 
1998; Evans et al 2000\] which sequentially 
parses text that has been tagged with part of 
speech using a finite state machine. Next, the 
complete list of SNPs identified in a document 
is sorted by the head of the phrase, which, at 
least for English-language common SNPs, is 
almost always the last word. The intuitive justi- 
fication for sorting SNPs by head is based on 
the fundamental linguistic distinction between 
head and modifier: in general, a head makes a 
greater contribution to the syntax and seman- 
tics of a phrase than does a modifier. This lin- 
guistic insight can be extended to the document 
level. If, as a practical matter, it is necessary to 
rank the contribution to a whole document 
made by the sequence of words constituting an 
NP, the head should be ranked more highly 
than other words in the phrase. This distinction 
is important in linguistic theory; for example, 
\[Jackendoff 1977\] discusses the relationship of 
heads and modifiers in phrase structure. It is 
also important in NLP, where, for example, 
\[Strzalkowski 1997\] and \[Evans and Zhai 
1996\] have used the distinction between heads 
and modifiers to add query terms to informa- 
tion retrieval systems. 
Powerful corpus processing techniques 
have been developed to measure deviance from 
an average occurrence or co-occurrence in the 
corpus. In this paper we chose to evaluate 
methods that depend only on document-internal 
data, independent of corpus, domain or genre. 
We therefore did not use, for example, tf*idf, 
the purely statistical technique that is the used 
by most information retrieval systems, or 
\[Smadja 1993\], a hybrid statistical and sym- 
bolic technique for identifying collocations. 
4. Experimental Method 
To evaluate techniques, we performed a quali- 
tative user evaluation i  which the terms identi- 
fied by each method were compared for 
usefulness as index terms. 
4.1 Subjects 
We performed our study with librari- 
ans, publishing professionals and undergradu- 
ate and graduate students at our university. 29 
subjects participated in the study: 7 librarians 
and publishing professionals and 22 students. 
4.2 Data 
For this experiment, we selected three 
articles from the 1990 Wall Street Journal 
contained in the Tipster collection of docu- 
ments. The articles were about 500 words in 
length. 
To compare methods, each article was 
processed three times: 1) with SMART to 
identify stemmed keywords \[Salton 1989\]; 2) 
with an implementation of the TT algorithm 
based on \[Justeson and Katz 1995\]; and 3) with 
our implementation of the HS method. Output 
for one article is shown in Appendix A. Figure 
1 shows the articles selected, their length in 
words and the number of index terms from 
each method for each article presented to the 
subjects. 
DOC words KW TT HS 
415-0109 509 63 4 49 
516-0043 594 51 9 54 
517-0062 514 52 8 57 
Figure 1: Word and term count, by type, per 
article 
The number of TTs is much lower than the 
number of KWs or HSs. This presented us with 
a problem: on the one hand, we were concerned 
about preserving the integrity of the three 
methods, each of which has their own logic, 
and at the same time, we were concerned to 
present lists that were balanced relative to each 
other. Toward this end, we made several deci- 
sions about presentation of the data: 
1. Threshold: So that no bias would be un- 
intentionally introduced, we presented 
subjects with all terms output by each 
method, up to a specified cut-off poin- 
However, using lists of equal length for 
each method would have necessitated ither 
omitting HSs and KWs or changing the 
definition of TTs. Therefore we made the 
following decisions: 
? For TTs, we included all identified 
terms; 
? For HSs, we included all terms whose 
head occurred more than once in the 
document; 
305 
. 
. 
? For KWs, we included all terms in or- 
der of decreasing frequency, up to the 
point where we observed diminishing 
quality and where the number of KWs 
approximated the number of HSs. 
Order: For the KW and TT approach, 
order is not significant. However, for the 
HS approach, the grouping together of 
phrases with common heads is, we claim, 
one of the advantages of the method. We 
therefore alphabetized the KWs and TTs in 
standard left to right order and alphabet- 
ized the HSs by head, e.g., trust account 
precedes money market fund. 
Morphological expansion: The KW ap- 
proach identifies stems which represent a
set of one or more morphological variants 
of the stem. Since in some cases the stem 
is not an English word, we expanded each 
stem to include the morphological variants 
that actually occurred in the article. For 
example, for the stem reject, we listed re- 
jected and rejecting but did not list rejects, 
which did not occur in the article. 
4.3 Presentation to subjects 
Each subject was presented with three articles. 
For one article, the subject received a head 
sorted list of HSs; for another article, the sub- 
ject received a list of technical terms, and for 
the third article, the subject saw a list of key- 
words. No time limit was placed on the task. 
5. Results 
Our results for the three types of terms, by 
document, are shown in Figure 2. Although we 
asked subjects to rate three articles, some vol- 
unteers rated only two. All results were in- 
cluded. 
Doc 
900405-0109 
900516-0043 3.73 
900517-0062 2.98 
3.27 Avg of Avgs 
Figure 2: Average 
index terms 
Avg 
KW 
rating 
3.08 
Avg Avg 
TT HS 
rating rating 
1.45 2.71 
2.19 2.71 
1.7 3.25 
1.79 2.89 
ratings of 3 types of 
5.1 Quality 
For the three lists of index terms, TTs received 
the highest ratings for all three documents--an 
average of 1.79 on the scale of 1 to 5, with 1 
being the best rating. HS came in second, with 
an average of 2.89, and KW came in last with 
an average of 3.27. It should be noted that av- 
eraging the average conceals the fact that the 
number of TTs is much lower than the other 
two types of terms, as shown in Figure 1. 
Figure 3 (included before Appendix A) 
shows cumulative rankings of terms by method. 
The X axis represents ratings awarded by sub- 
jects. The Y axis reflects the percentage of 
terms receiving a given rank or better. All data 
series must reach 100% since every term has 
been assigned a rating by the evaluators. At 
any given data point, a larger value indicates 
that a larger percentage of that series' data has 
that particular ating or better. For example, 
100% of the TTs have a rating of 3 or better; 
while only about 30% of the terms of the low- 
est-scoring KW document received a score of 3 
or better. In two out of the three documents, 
HS terms fall between TTs and KWs. 
5.2 Coverage 
The graph in Figure 3 shows results 
for quality, not coverage. In contrast, Figure 4, 
which shows the total number of terms rated at 
or below specified rankings, allows us to meas- 
ure quality and coverage. (1 is the highest rat- 
ing; 5 is the lowest.) This figure shows that the 
HS method identifies more high quality terms 
than the TT method oes. 
~ od HS 
Number of terms ranked 
at or better than 
2 3 4 5 
27 75 124 166 
41 96 132 160 
15 21 21 21 
Figure 4: Running total of terms identified at 
or below a specified rank 
TT clearly identifies the highest quality terms: 
100% of TTs receive a rating of 2 or better. 
However, only 8 TTs received a rating of 2 or 
better (38% of the total), while 41 HSs re- 
306 
ceived a rating of 2 or better (26% of the total). 
This indicates that the TT method misses many 
high quality terms. KW, the least discriminat- 
ing method in terms of quality, also provides 
better coverage than does TT. 
This result is consistent with our observa- 
tion that TT identifies the highest quality terms, 
but there are very few of them: an average of 7 
per 500 words compared to over 50 for HS and 
KW. Therefore there is a need for additional 
high quality terms. The list of HSs received a
higher average rating than did the list of KWs, 
as shown in Figure 2. This is consistent with 
our expectation that phrases containing more 
content-bearing modifiers would be perceived 
as more useful index terms than would single 
word phrases consisting only of heads. 
5.3 Ranking variability 
The difference in the average ratings for 
the list of KWs and the list of head-sorted 
SNPs was less than expected. The small differ- 
ence in average ratings for the HS list and the 
KW list can be explained, at least in part, by 
two factors: 1) Differences among profession- 
als and students in inter-subject agreement and 
reliability; 2) A discrepancy in the rating of 
single word terms across term types. 
22 students and 7 professionals par- 
ticipated in the study. Figure 5 shows differ- 
ences in the ratings of professionals and of 
students. 
KW 
HS 
TT 
Professionals Students 
2.64 3.30 
2.3 3.03 
1.49 2.1 
Figure 5: Average ratings, by term type, of 
professionals and students 
When variation in the scores for terms was cal- 
culated using standard eviation, the standard 
deviation for the professionals was 0.78, while 
for the students it was 1.02. Because of the 
relatively low number of professionals, the 
standard deviation was calculated only over 
terms that were rated by more than one profes- 
sional. A review of the students' results showed 
that they appeared not to be as careful as the 
professionals. For example, the phrase 'Wall 
Street Journal' was included on the HS list only 
because it is specified as the document source. 
However, four of the eight students assigned 
this term a high rating (1 or 2); this is puzzling 
because the document is about asbestos-related 
disease. The other four students assigned a 4 
or 5 to 'Wall Street Journal', as we expected. 
But the average score for this term was 3, due 
to the anomalous ratings. We therefore have 
more confidence in the reliability of the profes- 
sional ratings, even though there are relatively 
few of them. 
We examined some of the differences in 
rating for term types. Single word index terms 
are rated more highly by professionals when 
they appear in the context of other single word 
index terms, but are downrated in the context 
of phrasal expansions that make the meaning of 
the one-word term more specific. The KW list 
and HS list overlap when the SNP consists only 
of a single word (the head) or only of a head 
modified by determiners. When the same word 
appears in both lists in identical form, the token 
in the KW list tends to receive a better ating 
than the token does when it appears in the HS 
list, where it is often followed by expansions of 
the head. For example, the word exposure re- 
ceived an average rating of 2.2 when it ap- 
peared on the KW list, but a rating of only 2.75 
on the HS list. However, the more specific 
phrase racial quotas, which immediately fol- 
lowed quota on the HS list received a rating of 
1. 
To better understand these differences, we 
selected 40 multi-word phrases and examined 
the average score that the phrase received in the 
TT and HS lists, and compared it to the aver- 
age ratings that individual words received in 
the KW list. We found that in about half of the 
cases (21 of 40), the phrase as a whole and the 
individual words in the phrase received similar 
scores, as in Example 1 in Figure 6. In just 
over one-fourth of the cases (12 of 40), the 
phrase scored well, but scores from the indi- 
vidual words were rated from good to poor, as 
in Example 2. In about one-eighth of the cases 
(6 of 40), the phrase scored well, but the indi- 
vidual words scored poorly, as in Example 3. 
Finally, in only one case, shown in Example 4 
of Figure 6, the phrase scored poorly but the 
individual words scored well. 
307 
Phrase 
Supreme Court 
(1.5) 
reverse discrimi- 
I nation 
(1) 
lymph system 
employment 
decisions 
(2.75) 
Word 1 
Supreme 
(1) 
reverse 
(3.25) 
lymph 
(1) 
employ- 
ment 
(1.25) 
Word 2 
Court 
(1.25) 
discrimi- 
nation 
(3.25) 
system 
(5) 
decisions 
(1.25) 
Figure 6: Comparison of scores of phrases 
and single words 
This shows that single words in isolation are 
judged differently than the same word when 
presented in the context of a larger phrase. 
These results have important implications in 
the design of indexing tools. 
6. Conclusion 
Our results show that the head sorting 
technique outperforms two other indexing 
methods, technical terms and keywords, as 
measured by balance of quality and coverage. 
We have performed a qualitative valuation of 
three techniques for identifying significant 
terms in a document, driven by an indexing 
task. Such an applicati;on can be used to create 
a profile or thumbnail of a document by pre- 
senting to users a set of terms which can be 
considered to be a representation f the content 
of the document. We have used human judges 
to evaluate the effectiveness of each method. 
This research is a contribution to the overall 
evaluation of computational linguistic tools in 
terms of their usefulness for human-oriented 
computational applications. 
8. References 
Boguraev, Branimir and Kennedy, Christopher 
(1998) "Applications of term identification 
terminology: domain description and content 
characterisation", Natural Language Engi- 
neering 1(1): 1-28. 
Church, Kenneth Ward (1988) "A stochastic parts 
program and noun phrase parser for unre- 
stricted text", in Proceedings of the Second 
Conference on Applied Natural Language 
Processing, pp. 136-143. 
Evans, David A. and Chengxiang Zhai (1996) 
"Noun-phrase analysis in unrestricted text for 
information retrieval", Proceedings of the 
34th Annual Meeting of the Association for 
Computational Linguistics, pp. 17-24.24-27 
June 1996, University of California, Santa 
Cruz, California, Morgan Kaufmann Pub- 
lishers. 
Evans, David K. (1998) LinklT Documentation, 
Columbia University Department ofCom- 
puter Science Report. 
Evans, David K., Klavans, Judith, and Wacholder, 
Nina (2000) "Document processing with 
LinklT", RIAO Conference, Paris, France, to 
appear. 
Hatzivassiloglou, Vasileios, Judith L. Klavans and 
Eleazar Eskin (1999) "Detecting text simi- 
larity over short passages: exploring linguis- 
tic feature combinations via machine 
learning", Proceedings of the EMNLP/VLC- 
99 Joint SIGDAT Conference on Empirical 
Methods in NLP and Very Large Corpora, 
June 21-22, 1999, University of Maryland, 
College Park, MD. 
Hedges, Julia, Shiyun Yie, Ray Reighart and Lois 
Boggess (1996) "An automated system that 
assists in the generation ofdocument in- 
dexes", Natural Language Engineering 
2(2): 137-160. 
Jackendoff, Ray (1977) X-bar Syntax: A Study of 
Phrase Structure, MIT Press, Cambridge, 
MA. 
Justeson, John S. and Slava M. Katz (1995) 
"Technical terminology: some linguistic 
properties and an algorithm for identification 
in text", Natural Language Engineering 
1(1):9-27. 
Luhn, Hans P. (1958) "The automatic creation of 
literature abstracts", IBM Journal, 159-165. 
Mosteller, Frederick and David L. Wallace (1963) 
"Inference in an authorship problem", Jour- 
nal of the American Statistical Association 
58(302):275-309. Available at 
http://www.jstor.org/. 
Paice, Chris D. (1990) "Constructing literature 
abstracts by computer: techniques and pros- 
pects". Information Processing & Manage- 
ment 26(1): 171-186. 
Salton, Gerald (1989) Automatic Text Processing: 
The Transformation, Analysis and Retrieval 
of lnformation by Computer. Addison- 
Wesley, Reading, MA. 
Smadja, Frank (1993) "Retrieving collocations 
from text", Computational Linguistics 
19(1):143-177. 
Strzalkowski, Thomas (1997) "Building effective 
queries in natural language information re- 
trieval", Proceedings of the ANLP, ACL, 
Washington, DC., pp.299-306. 
Wacholder, Nina (1998) "Simplex NPS sorted by 
head: a method for identifying significant 
topics within a document", Proceedings of 
the Workshop on the Computational Treat- 
ment of Nominals, pp.70-79. COLING-ACL 
'98, Montreal, Canada, August 16, 1998. 
Figure 3: Cumulative ranking of terms, by method 
0.9 
0.8 
~ 0.7 
i. 0.6 0.5 
0.4 
~ o.a 
0.2 
?~ 0.1 
0 
0.5 1 1.5 2 2.5 3 
Rating 
3.5 4 4.5 5 
308 
Appendix A: Terms identified in WSJ900405-0109 
HSs 
amendments 
Hatch amendment 
other amendments 
attempts 
bias 
job bias 
intentional bias 
bill 
committee 
Senate labor Committee 
court 
Supreme Court 
co-workers 
decisions 
Supreme Court decisions 
employment decisions 
Democrats 
discrimination 
reverse discrimination 
employees 
women employees 
employers 
groups 
civil-rights groups 
conservative policy 
groups 
Orrin Hatch 
health 
discriminatory impact 
Job-Bias Measure 
basic employment anti- 
discrimination law 
1866 civil-rights law 
lawsuits 
lawmakers 
legislation 
comprehensive legislation 
more modest measure 
minority/minorities 
panel 
plans 
court-approved affirmative 
action plans 
discriminatory seniority plans 
practices 
employment practices 
quotas 
racial quotas 
fight/rights 
equal rights 
year 
Keywords 
action 
address/addressing 
adopt/adopted 
affirmative 
agree 
aimed 
alleged/alleging 
amend 
approved 
attempt/attempts 
bias 
bill 
Bush 
challenge 
circumstances 
civil 
clears 
committee 
court/Court 
decision 
Democrats 
discrimination 
employment/employers/employees 
force/Force 
give/giving 
GOP 
groups 
Hatch 
health 
~gh 
impact 
job 
justify 
labor/Labor 
law 
lawmakers 
lawsuits 
legislative/legislation 
make 
measure 
minority/minorities 
Mr. 
overturning 
panel 
plans 
policy 
practices 
quotas 
racial 
rejected/rejecting 
reverse 
rights 
rules/ruling 
safety 
Sen./Sens. 
Senate 
shown 
street 
Supreme; vote/voted 
309 
women 
workers 
year 
Technical terms 
discriminatory impact 
employment practice 
Senator Hatch 
Supreme Court 
HITIQA: Towards Analytical Question Answering 
Sharon Small1, Tomek Strzalkowski1, Ting Liu1, Sean Ryan1, Robert Salkin1,  
Nobuyuki Shimizu1, Paul Kantor2, Diane Kelly2, Robert Rittman2, Nina Wacholder2 
 
1The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tomek,tl7612,seanryan, 
rs6021,ns3202}@albany.edu 
2Rutgers University 
4 Huntington Street 
New Brunswick, NJ 08904 
{kantor,diane,hitiqa, 
wacholder}@scils.rutgers.edu
 
Abstract 
In this paper we describe the analytic 
question answering system HITIQA (High-
Quality Interactive Question Answering) 
which has been developed over the last 2 years 
as an advanced research tool for information 
analysts. HITIQA is an interactive open-
domain question answering technology 
designed to allow analysts to pose complex 
exploratory questions in natural language and 
obtain relevant information units to prepare 
their briefing reports. The system uses novel 
data-driven semantics to conduct a 
clarification dialogue with the user that 
explores the scope and the context of the 
desired answer space. The system has 
undergone extensive hands-on evaluations by 
a group of intelligence analysts. This 
evaluation validated the overall approach in 
HITIQA but also exposed limitations of the 
current prototype.  
1 Introduction 
Our objective in HITIQA is to allow the user to 
submit exploratory, analytical questions, such as 
?What has been Russia?s reaction to the U.S. 
bombing of Kosovo?? The distinguishing property 
of such questions is that one cannot generally 
anticipate what might constitute the answer. While 
certain types of things may be expected (e.g., 
diplomatic statements), the answer is heavily 
conditioned by what information is in fact 
available on the topic. From a practical viewpoint, 
analytical questions are often underspecified, thus 
casting a broad net on a space of possible answers. 
Questions posed by professional analysts are 
aimed to probe the available data along certain 
dimensions. The results of these probes determine 
follow up questions, if necessary. Furthermore, at 
any stage clarifications may be needed to adjust 
the scope and intent of each question. Figure 1 
shows a fragment of an analytical session with 
HITIQA; note that these questions are not aimed at 
factoids, despite their simple form. 
User: What is the history of the nuclear arms 
program linking Iraq and other countries in the 
region? 
HITIQA: [responses and clarifications] 
User: Who financed the nuclear arms program 
in Iraq? 
HITIQA:? 
User: Has Iraq been able to import uranium? 
HITIQA:? 
User: What type of debt does exist between Iraq 
and her trading partners in the region? 
FIGURE 1: A fragment of an analyst?s session 
with HITIQA 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in 
the state of the art of automated question 
answering.  In this paper we focus on three aspects 
of our work: 
1. Question Semantics: how the system 
?understands? user requests 
2. Human-Computer Dialogue: how the user and 
the system negotiate this understanding 
3. User Evaluations and Results 
2 Factoid vs. Analytical QA 
There are significant differences between 
factoid, or fact-finding, and analytical question 
answering.  A factoid question is normally 
understood to seek a piece of information that 
would make a corresponding statement true (i.e., it 
becomes a fact): ?How many states are in the 
U.S.?? / ?There are X states in the U.S.? In this 
sense, a factoid question usually has just one 
correct answer that can generally be judged for its 
truthfulness with respect to some information 
source.  
As noted by Harabagiu et al (1999), factoid 
questions display a distinctive ?answer type?, 
which is the type of the information item needed 
for the answer, e.g., ?person? or ?country?, etc. 
Most existing factoid QA systems deduct this 
expected answer type from the form of the 
question using a finite list of possible answer 
types. For example, ?Who was the first man in 
space? expects a ?person? as the answer type. This 
is generally a very good strategy that has been 
exploited successfully in a number of automated 
QA systems, especially in the context of TREC 
QA1 evaluations. Given the excellent results posted 
by the best systems and an adequate performance 
attained even by some entry-level system, we 
believe that the process of factoid question 
answering is now fairly well understood 
(Harabagiu et al, 2002; Hovy et al, 2000; Prager 
at al., 2001, Wu et al, 2003). 
   In contrast to a factoid question, an analytical 
question has a virtually unlimited variety of 
syntactic forms with only a loose connection 
between their syntax and the expected answer. 
Given the many possible forms of analytical 
questions, it would be counter-productive to 
restrict them to a predefined number of 
question/answer types. Therefore, the formation of 
an answer in analytical QA should instead be 
guided by the user?s intended interest expressed in 
the question, as well as through any follow up 
dialogue with the system. This clearly involves 
user's intentions (the speech acts) and how they 
evolve with respect to the overall information 
strategy they are pursuing. 
In this paper we argue that the semantics 
(though not necessarily the intent) of an analytical 
question is more likely to be deduced from the 
information that is considered relevant to the 
question than through a detailed analysis of its 
particular form. We noted that the questions 
analysts ask, while clearly part of a strategy, are 
generally quite flexible and ?forgiving?, in the 
sense that there is always a strong possibility that 
the answer may not arrive in the expected form, 
and thus a change of strategy, and even the initial 
expectations, may be warranted. This suggests 
strongly that a solution to analytic QA must 
involve a dialogue that combines information 
seeking and problem solving strategies. 
3 Document Retrieval 
HITIQA works with unstructured text data, 
which means that a document retrieval step is 
required to detect any information that may be 
relevant to the user question. It has to be noted that 
determining ?relevant? information is not the same 
as finding an answer; indeed we can use relatively 
simple information retrieval methods (keyword 
matching, etc.) to obtain perhaps 200 ?relevant? 
                                                     
1 TREC QA is the annual Question Answering evaluation 
sponsored by the U.S. National Institute of Standards and Technology 
www.trec.nist.gov 
documents from a database. This gives us an initial 
information space to work on in order to determine 
the scope and complexity of the answer, but we are 
nowhere near the answer yet. The current version 
of HITIQA uses the INQUERY system (Callan et 
al., 1992), although we have also used SMART 
(Buckley, 1985) and other IR systems (such as 
Google).   
4 Text Framing 
In HITIQA we use a text framing technique to 
delineate the gap between the possible meaning of 
the user?s question and the system ?understanding? 
of this question. We can approximate the meaning 
of the question by extracting references to known 
concepts in it, including named entities. The 
information retrieved from the database may well 
lead to other interpretations of the question, and we 
need to determine which of these are ?correct?.  
The framing process imposes a partial structure 
on the text passages that allows the system to 
systematically compare different passages against 
each other and against the question. Framing is not 
attempting to capture the entire meaning of the 
passage; it needs to be just sufficient enough to 
communicate with the user about the differences in 
their question and the returned text. In particular, 
the framing process may uncover topics or aspects 
within the answer space which the user has not 
explicitly asked for, and thus may be unaware of 
their existence. If these topics or aspects align 
closely with the user?s question, (i.e., matching 
many of the salient attributes) we may want to 
make the user aware of them and let him/her 
decide if they should be included in the answer.   
Frames are built from the retrieved data, after 
clustering it into several topical groups. Passages 
are clustered using a combination of hierarchical 
clustering and n-bin classification (Hardy et al, 
2002a). Each cluster represents a topic theme 
within the retrieved set: usually an alternative or 
complimentary interpretation of the user?s 
question. Since clusters are built out of small text 
passages, we initially associate a frame with each 
passage that serves as a seed of a cluster. We 
subsequently merge passages and their associated 
frames to arrive at one or more combined frames 
for the cluster. 
HITIQA starts text framing by building a 
general frame on the seed passages of the clusters 
and any of the top N (currently N=10) scored 
passages that are not already in a cluster. The 
general frame represents an event or a relation 
involving any number of entities, which make up 
the frame?s attributes, such as LOCATION, PERSON, 
ORGANIZATION, DATE, etc. Attributes are extracted 
from text passages by BBN?s Identifinder, which 
tags 24 types of named entities. The event/relation 
itself could be pretty much anything, e.g., accident, 
pollution, trade, etc. and it is captured into the 
TOPIC attribute from the central verb or noun 
phrase of the passage. In the general frame, 
attributes have no assigned roles; they are loosely 
grouped around the TOPIC (Figure 2).  
We have also defined three slightly more 
specialized typed frames by assigning roles to 
selected attributes in the general frame. These 
three ?specialized? frames are: (1) a Transfer 
frame with three roles including FROM, TO and 
OBJECT; (2) a two-role Relation frame with AGENT 
and OBJECT roles; and (3) an one-role Property 
frame. These typed frames represent certain 
generic events/relationships, which then map into 
more specific event types in each domain. Other 
frame types may be defined if needed, but we do 
not anticipate there will be more than a handful all 
together.2 For example, another 3-role frame may 
be State-Change frame with AGENT, OBJECT and 
INSTRUMENT roles, etc.3  
FRAME TYPE: General 
TOPIC: imported 
LOCATION: Iraq, France, Israel 
ORGANIZATION: IAEA [missed: Nukem] 
PERSON: Leonard Spector 
WEAPON: uranium, nuclear bomb 
DATES: 1981, 30 November 1990, .. 
FIGURE 2: A general frame obtained from the 
text passage in Figure 3 (not all attributes shown). 
 
Where the general frame is little more than just 
a ?bag of attributes?, the typed frames capture 
some internal structure of an event, but only to the 
extent required to enable an efficient dialogue with 
the user. Typed frames are ?triggered? by 
appearance of specific words in text, for example 
the word export may trigger a Transfer frame. A 
single text passage may invoke one or more typed 
frames, or none at all. When no typed frame is 
invoked, the general frame is used as default. If a 
typed frame is invoked, HITIQA will attempt to 
identify the roles, e.g. FROM, TO, OBJECT, etc. This 
is done by mapping general frame attributes 
selected from text onto the typed attributes in the 
frames. In any given domain, e.g., weapon non-
proliferation, both the trigger words and the role 
identification rules can be specialized from a 
                                                     
2 Scalability is certainly an outstanding issue here, and we are 
working on effective frame acquisition methods, which is outside of 
the scope of this paper. While classifications such as (Levin, 1993) or 
FrameNet (Fillmore, 2001) are relevant, we are currently aiming at a 
less detailed system. 
3 A more detailed discussion of possible frame types is beyond the 
scope of the current paper. 
training corpus of typical documents and 
questions. For example, the role-id rules rely both 
on syntactic cues and the expected entity types, 
which are domain adaptable.  
Domain adaptation is desirable for obtaining 
more focused dialogue, but it is not necessary for 
HITIQA to work. We used both setups under 
different conditions: the generic frames were used 
with TREC document collection to measure impact 
of IR precision on QA accuracy (Small et al, 
2004). The domain-adapted frames were used for 
sessions with intelligence analysts working with 
the WMD Domain (see below). Currently, the 
adaptation process includes manual tuning 
followed by corpus bootstrapping using an 
unsupervised learning method (Strzalkowski & 
Wang, 1996). We generally rely on BBN?s 
Identifinder for extraction of basic entities, and use 
bootstrapping to define additional entity types as 
well as to assign roles to attributes. 
The version of HITIQA reported here and used 
by analysts during the evaluation has been adapted 
to the Weapons of Mass Destruction Non-
Proliferation domain (WMD domain, henceforth).  
Figure 3 contains an example passage from this 
data set. In the WMD domain, the typed frames 
were mapped onto WMDTransfer 3-role frame, 
and two 2-role frames WMDTreaty  and 
WMDDevelop. Adapting the frames to the WMD 
domain required very minimal modification, such 
as adding the WEAPON entity to augment the 
Identifinder entity set, generating a list of 
international weapon control treaties, etc. 
The Bush Administration claimed that Iraq was 
within one year of producing a nuclear bomb. On 
30 November 1990... Leonard Spector said that 
Iraq possesses 200 tons of natural uranium 
imported and smuggled from several countries. 
Iraq possesses a few working centrifuges and the 
blueprints to build them. Iraq imported centrifuge 
materials from Nukem of the FRG and from other 
sources. One decade ago, Iraq imported 27 pounds 
of weapons-grade uranium from France, for Osirak 
nuclear research center. In 1981, Israel destroyed 
the Osirak nuclear reactor. In November 1990, the 
IAEA inspected Iraq and found all material 
accounted for....  
FIGURE 3: A text passage from the WMD 
domain data    
 
HITIQA frames define top-down constraints on 
how to interpret a given text passage, which is 
quite different from MUC4 template filling task 
                                                     
4 MUC, the Message Understanding Conference, funded by 
DARPA, involved the evaluation of information extraction systems 
applied to a common task. 
(Humphreys et al, 1998). What we?re trying to do 
here is to ?fit? a frame over a text passage. This 
also means that multiple frames can be associated 
with a text passage, or to be exact, with a cluster of 
passages. Since most of the passages that undergo 
the framing process are part of some cluster of 
very similar passages, the added redundancy helps 
to reinforce the most salient features for extraction. 
This makes the framing process potentially less 
error-prone than MUC-style template filling. 
A very similar framing process is applied to the 
user?s question, resulting in one or more Goal 
frames, which are subsequently compared to the 
data frames obtained from retrieved text passages. 
A Goal frame can be a general frame or any of the 
typed frames. Goal frames generated from the 
question, ?Has Iraq been able to import 
uranium?? are shown in Figures 4 and 5. 
FRAME TYPE: General 
TOPIC: import 
WEAPON:  uranium 
LOCATION: Iraq 
FIGURE 4: A general goal frame from the Iraq 
question 
The frame in Figure 4 is simply a General 
frame which is invoked first. HITIQA then 
discovers that TOPIC=import denotes a Transfer-
event in the WMD domain, so it creates a 
WMDTransfer frame that replaces the general 
frame. This new frame, shown in Figure 5, has 
three role attributes TRF_TO, TRF_FROM and 
TRF_OBJECT, plus the relation type (TRF_TYPE). 
Each role attribute is defined over an underlying 
general frame attribute (given in parentheses), 
which are used to compare frames of different 
types.  The role-id rules rely both on syntactic cues 
and the expected entity types, which are domain 
adaptable. 
FRAME TYPE: WMDTransfer 
TRF_TYPE (TOPIC): import 
TRF_TO (LOCATION): Iraq 
TRF_FROM (LOCATION, ORGANIZATION): ? 
TRF_OBJECT (WEAPON): uranium 
FIGURE 5: A typed goal frame from the Iraq 
question 
HITIQA automatically judges a particular data 
frame as relevant, and subsequently the 
corresponding segment of text as relevant, by 
comparison to the Goal frame. The data frames are 
scored based on the number of conflicts found with 
the Goal frame. The conflicts are mismatches on 
values of corresponding attributes, specifically 
when the data frame attribute list does not contain 
any of the entities in the corresponding Goal 
Frame attribute list.  If a data frame is found to 
have no conflicts, it is given the highest relevance 
rank, and a conflict score of zero.   
All other data frames are scored with an 
increasing value based on the number of conflicts, 
score 1 for frames with one conflict with the Goal 
frame, score 2 for two conflicts etc. Frames that 
conflict with all information found in the query are 
given the score 99 indicating the lowest rank. 
Currently, frames with a conflict score 99 are 
excluded from further processing as outliers. The 
frame in Figure 6 is scored as relevant to the user?s 
query and included in the answer space. 
FRAME TYPE: WMDTransfer 
TRF_TYPE (TOPIC): imported 
TRF_TO (LOCATION): Iraq 
TRF_FROM (LOCATION): France 
TRF_OBJECT (WEAPON): uranium 
CONFLICT SCORE: 0 
FIGURE 6: A typed frame obtained from the 
text passage in Figure 3, in response to the Iraq 
question 
5 Enabling Dialogue with the User 
Framed information allows HITIQA to 
automatically judge text passages as fully or 
partially relevant and to conduct a meaningful 
dialogue with the user about their content. The 
purpose of the dialogue is to help the user navigate 
the answer space and to negotiate more precisely 
what information he or she is seeking. The main 
principle here is that the dialogue is primarily 
content oriented. Thus, it is okay to ask the user 
whether information about the AIDS conference in 
Cape Town should be included in the answer to a 
question about combating AIDS in Africa. 
However, the user should never be asked if a 
particular keyword is useful or not, or if a 
document is relevant or not.  
Our approach to dialogue in HITIQA is 
modeled to some degree upon the mixed-initiative 
dialogue management adopted in the AMITIES 
project (Hardy et al, 2002b). The main advantage 
of the AMITIES model is its reliance on data-
driven semantics which allows for spontaneous 
and mixed initiative dialogue to occur. By contrast, 
the major approaches to implementation of 
dialogue systems to date rely on systems of 
functional transitions that make the resulting 
system much less flexible. In the grammar-based 
approach, which is prevalent in commercial 
systems, such as in various telephony products, as 
well as in practically oriented research prototypes 
(e.g., DARPA Communicator; Seneff and Polifoni, 
2000; Ferguson and Allen, 1998), a complete 
dialogue transition graph is designed to guide the 
conversation and predict user responses, which is 
suitable for closed domains only. In the statistical 
variation of this approach, a transition graph is 
derived from a large body of annotated 
conversations (e.g., Walker, 2000; Litman and Pan, 
2002). This latter approach is facilitated through a 
dialogue annotation process, e.g., using Dialogue 
Act Markup in Several Layers (DAMSL) (Allen 
and Core, 1997), which is a system of functional 
dialogue acts.  
Nonetheless, an efficient, spontaneous dialogue 
cannot be designed on a purely functional layer. 
Therefore, here we are primarily interested in the 
semantic layer, that is, the information exchange 
and information building effects of a conversation. 
In order to properly understand a dialogue, both 
semantic and functional layers need to be 
considered. In this paper we are concentrating 
exclusively on the semantic layer. 
6 Clarification Dialogue 
The clarification dialogue is when the user and 
the system negotiate the information task that 
needs to be performed. Data frames with a conflict 
score of 0 form the initial kernel answer space and 
HITIQA proceeds by generating an answer from 
this space. Depending upon the presence of other 
frames outside of this set, the system may initiate a 
dialogue with the user. When the Goal frame is a 
general frame HITIQA first initiates a clarification 
dialogue on existing general data frames that have 
one conflict. All of these 1-conflict general frames 
are first grouped on their common conflict 
attribute. HITIQA begins asking the user questions 
on these near-miss frame groups, with the largest 
group first. The groups must be at least groups of 
size N, where N is a user controlled setting.  This 
setting restricts of all HITIQA?s generated 
dialogue. HITIQA then check for the existence of 
any data frames that are one of the three typed 
frames. Clarification dialogue will be initiated on 
these, when all of their general attributes agree 
with the general attributes of the Goal frame 
respectively. Alternatively, if the Goal frame is one 
of the three type specific frames, a clarification 
dialogue is first initiated on groups of one conflict 
data frames that are the same type as the Goal 
frame. The clarification dialogue will then 
continue to the remaining two type specific frames 
if any exist, and finally on to any General data 
frames. 
A 1-conflict frame has only a single attribute 
mismatch with the Goal frame. This could be a 
mismatch on any of the general frame attributes, 
for example, LOCATION, ORGANIZATION, TIME, 
etc., or in one of the role-assigned attributes, TO, 
FROM, OBJECT, etc.  A special case arises when the 
conflict occurs on the TOPIC attribute, which 
indicated the event type. Since all other attributes 
match, we may be looking at potentially different 
events (though similar type events) involving the 
same entities, occurring at the same location or 
time. The purpose of the clarification dialogue in 
this case is to probe which of these additional 
events may be of interest to the user.  
Another special case arises when the Goal 
frame has a different value of the TYPE attribute 
than a data frame. The purpose of the clarification 
dialogue here is to see if the user wishes to expand 
the answer space to include events of a different 
type. This situation is illustrated in the exchange 
shown in Figure 7.  
The clarification question asked by HITIQA in 
Figure 7 was obtained by comparing the Goal 
frame (Figure 5) to a partly matching frame 
(Figure 8) generated from another text passage. 
We note first that the Goal frame for this example 
is WMDTransfer type, while the data frame in 
Figure 8 is WMDDevelop type. Nonetheless, both 
frames match on their general-frame attributes 
WEAPON and LOCATION. Therefore, HITIQA asks 
the user if it should expand the answer space to 
include development of uranium in Iraq as well. 
User: ?Has Iraq been able to import uranium?? 
HITIQA: IRAQ REPORTED TO HAVE 
IMPORTED MISSILES AND URANIUM  
Supporting passage:  
...that Iraq possesses 200 tons of natural 
uranium imported and smuggled from several 
countries. Iraq possesses a few working 
centrifuges and the blueprints to build them. Iraq 
imported centrifuge materials from Nukem of the 
FRG and from other sources. One decade ago, 
Iraq imported 27 pounds of weapons-grade 
uranium from France, for Osirak nuclear research 
center... 
HITIQA: ?Are you also interested in 
background information on the uranium 
development program in Iraq?? 
User: ? 
 
FIGURE 7:  The clarification dialogue detail 
During the dialogue, as new information is 
obtained from the user, the Goal frame is updated 
and the scores of all the data frames are 
reevaluated.  If the user responds the equivalent of 
?yes? to the system clarification question in the 
dialogue in Figure 7, a corresponding 
WMDDevelop frame will be added to the set of 
active Goal frames and all WMDDevelop frames 
obtained from text passages will be re-scored for 
possible inclusion in the answer. 
FRAME TYPE: WMDDevelop    
DEV_TYPE (TOPIC): development, produced 
DEV_OBJ (WEAPON): nuc. weapons, uranium 
DEV_AGENT (LOCATION): Iraq, Tuwaitha 
CONFLICT SCORE: 2 
Conflicts with FRAME_TYPE and TOPIC  
FIGURE 8: A 2-conflict frame against the 
Iraq/uranium question that generated the dialogue 
in Figure 7. 
The user may end the dialogue at any point using 
the generated answer given the current state of the 
frames. Currently, the answer is simply composed 
of text passages from the zero conflict frames. In 
addition, HITIQA will generate a ?headline? for 
the text passages in the answer space.  This is done 
using a combination of text templates and simple 
grammar rules applied to the attributes of the 
passage frame. Figure 7 shows a portion of the 
answer generated by HITIQA for the Iraq query. 
7 HITIQA Preliminary Evaluations 
We have evaluated HITIQA in a series of 
workshops with professional analysts in order to 
obtain an in-depth and comprehensive assessment 
of the system usability and performance. In 
addition to evaluating our research progress, the 
purpose of these workshops was to test several 
evaluation instruments to see if they can be 
meaningfully applied to a complex information 
system such as HITIQA. 
     For the participating analysts, the primary 
activity at these workshops involved preparation of 
reports in response to ?scenarios? ? complex 
questions that often encompass multiple sub-
questions, aspects and hypotheses. For example, in 
one scenario, analysts were asked ti locate 
information about the al Qaeda terorist group: its 
membership, sources of funding and activities. In 
another scenario, the analysts were requested to 
find information on the chemical weapon Sarin. 
Figure 9 shows one of the analytical scenarios used 
in these workshops. We prepared a database of 
over 1GByte of text documents; it included articles 
from the Center for Non-proliferation (CNS) data 
collected for the AQUAINT program and similar 
data retrieved from the web using Google. The 
analysts? task was to prepare a report ?as much like 
what you would do in your normal work 
environment as possible.? Over the six days of the 
workshops, each analyst prepared five such reports 
in sessions of one to three hours. Each session 
involved multiple questions posed to the system, as 
well as clarification dialogue, visual browsing and 
report construction. Figure 10 shows an abridged 
transcript from another analytical session with 
HITIQA.  
 Figure 9: A scenario level analytic task  
One of our primary concerns was to design 
tasks that were similar in scope and difficulty to 
those that the analysts are used to performing at 
work and to ensure that they felt comfortable using 
the system. 5 questions in the scenario evaluation 
dealt with this issue; for example, one question 
asked how the scenarios compared in difficulty 
with the tasks the analysts normally perform at 
work. The mean score for these five questions was 
3.75 on a 5 point scale (five is the best score). The 
lowest score (M=2.88) was received on the 
question ?How did the scenario compare in 
difficulty to tasks that you normally perform at 
work??; this slightly above average rating of 
difficulty of the tasks was quite satisfactory for our 
purposes.  
    In the final evaluation, analysts were asked to 
rate their agreement with statements such as 
?Having HITIQA helps me find important 
information? (score 4.50), ?Having Hitiqa at work 
would help me find information faster than I can 
currently find it? (score 4.33), and ?Hitiqa would 
be a useful addition to the tools that I already have 
at work? (score 4.25). The mean normalized score 
for the combined final evaluation of Workshop I 
was 3.75 on the 5 point scale; this means that the 
system received many more ratings of 4 and 5 than 
of 1 and 2. Comments made by the analysts in the 
group discussion and in the individual interviews 
confirmed that analysts liked the interactive 
dialogue and were very pleased with the results. 
For example, one analyst said ?I learned more 
about Sarin gas in 30 minutes than I probably 
would have at work in a half a day.? As desired, 
the analysts also made many suggestions for 
improving the interface and the interoperation of 
The department chief has requested a report by the 
close of business today on the nuclear arms program in 
Iraq and how it was influenced by the neighboring 
countries. List the extent of the nuclear program in each 
involved country including funding, capabilities, quantity, 
etc. Your report should also include key figures in Iraq 
nuclear program as well as in other countries in the region, 
and,any travels that these key figures have made to other 
countries in regards to a nuclear program, any weapons 
that have been used in the past by either country, any 
purchases or trades that have been made relevant to 
weapons of mass destruction (possibly oil trade, etc.), any 
ingredients and chemicals that have been used, any 
potential weapons that could be under development, 
countries that are involved or have close ties to Iraq or her 
trade partners, possible locations of development sites, and 
possible companies or organizations that these countries 
work with for their nuclear arms program. Add any other 
information relating to the Iraqi Nuclear Arms Programs.  
the visual and text display. For a research system 
undergoing its first rigorous evaluation, these 
results are very satisfactory ? they support the 
value of the design of the HITIQA system, 
including the interactive mode and the visual 
display and encourage us to move forward with 
this approach. 
 FIGURE 10: Fragment of an analytical session 
8 Future work 
The AQUAINT Program has entered its second 
phase in May 2004. Over the next 2 years our 
focus will be on augmenting HITIQA to provide 
more advanced dialogue capabilities, including 
problem solving dialogue related to hypothesis 
formation and verification. This implies building 
up system?s knowledge acquisition capabilities by 
exploiting diverse data sources, including 
structured databases and the internet. 
9 Acknowledgements 
This paper is based on work supported in part by 
the Advanced Research and Development Activity 
(ARDA)?s Advanced Question Answering for 
Intelligence (AQUAINT) Program. Special thanks 
to Heather McCallum-Bayliss and John Rogers for 
helping to arrange the analyst workshops. 
Additional thanks for Google for extending their 
license for this experiment, to Ralph Weischedel of 
BBN/Verizon for the use of IdentiFinder, to Chuck 
Messenger and Peter LaMonica for assistance in 
development of the analytical scenarios, and to 
Bruce Croft at University of Massachusetts for the 
use of INQUERY system. 
References  
Allen, J. and M. Core. 1997. Draft of DAMSL:  
Dialog Act Markup in Several Layers. 
www.cs.rochester.edu/research/cisd.  
Buckley, C. 1985. Implementation of the Smart 
information retrieval system. TR85-686, 
Computer Science, Cornell University. 
Ferguson, G. and J. Allen. 1998. TRIPS: An 
Intelligent Integrated Problem-Solving Assistant. 
AAAI-98 Conf., pp. 567-573. 
Fillmore, C. & C. F. Baker. 2001. Frame semantics 
for text understanding. WordNet Workshop at 
NAACL. 
Hardy, H., et al 2002a. Cross-Document 
Summarization by Concept Classification. 
Proceedings of SIGIR, Tampere, Finland. 
Hardy, H., et al 2002b.  Multi-layer Dialogue 
Annotation for Automated Multilingual 
Customer Service. ISLE Workshop, UK. 
Harabagiu, S., et. al. 2002. Answering Complex, 
List and Context questions with LCC?s Question 
Answering Server.   TREC-10. 
Hovy, E., et al 2000. Question Answering in 
Webclopedia. Notebook. Proceedings of Text 
Retrieval Conference TREC-9. 
Humphreys, R. et al 1998. Description of the 
LaSIE-II System as Used for MUC-7. Proc. of  
7th Message Under. Conf. (MUC-7.). 
Levin, B. 1993. English Verb Class and 
Alternations: A Preliminary Investigation. 
Chicago: University of Chicago Press. 
Litman, Diane J. and Shimei Pan. 2002. Designing 
and Evaluating an Adaptive Spoken Dialogue 
System. User Modeling and User-Adapted 
Interaction. Vol. 12, No. 2/3, pp. 111-137. 
Prager, J. et al 2003. In Question-Answering Two 
Heads are Better Than One. Proceedings of 
HLT-NAACL 2003, pp 24-31.  
Seneff, S. and J. Polifroni. 2000. Dialogue 
Management in the MERCURY Flight 
Reservation System. ANLP-NAACL 2000. 
Small et al 2004. A Data Driven Approach to 
Interactive Question Answering. In M. Maybury 
(ed). Future Directions in Automated Question 
Answering. MIT Press (to appear). 
Strzalkowski, T and J. Wang. 1996. A self-learning 
Universal Concept Spotter. Proceedings of 
COLING-96, pp. 931-936. 
Walker, M. A. 2002. An Application of 
Reinforcement Learning to Dialogue Strategy 
Selection in a Spoken Dialogue System for 
Email. Journal of AI Research, vol 12., pp. 387-
416. 
Wu, M. et al 2003. Question Answering by 
Pattern Matching, Web-Proofing, Semantic 
Form Proofing. TREC-12.Notebook. 
 
User: What is the status of South Africa's chemical, 
biological, and nuclear programs?  
 Clarification Dialogue: 1 minute 
 Studying Answer Panel: 60 minutes  
Copying 24 passages to report 
 Visual Panel Browsing: 5 minutes 
User: Has South Africa provided CBW material or 
assistance to any other countries?  
 Clarification Dialogue: 1 minute 
 Studying Answer Panel: 26 minutes 
 Copying 6 passages to report 
 Visual Panel browsing: 1 minute 
 Adding 1 passage to report 
User: How was South Africa's CBW program 
financed?  
 Clarification Dialogue: 40 seconds 
 Studying Answer Panel: 11 minutes 
 Copying 3 passages to report 
Toward a Task-based Gold Standard for Evaluation  
of NP Chunks and Technical Terms  
 
Nina Wacholder 
Rutgers University 
nina@scils.rutgers.edu 
Peng Song 
Rutgers University 
psong@paul.rutgers.edu 
 
 
Abstract 
We propose a gold standard for evaluating two 
types of information extraction output -- noun 
phrase (NP) chunks (Abney 1991; Ramshaw and 
Marcus 1995) and technical terms (Justeson and 
Katz 1995; Daille 2000; Jacquemin 2002). The 
gold standard is built around the notion that since 
different semantic and syntactic variants of terms 
are arguably correct, a fully satisfactory assess-
ment of the quality of the output must include 
task-based evaluation. We conducted an experi-
ment that assessed subjects? choice of index terms 
in an information access task. Subjects showed 
significant preference for index terms that are 
longer, as measured by number of words, and 
more complex, as measured by number of prepo-
sitions. These terms, which were identified by a 
human indexer, serve as the gold standard. The 
experimental protocol is a reliable and rigorous 
method for evaluating the quality of a set of terms. 
An important advantage of this task-based evalua-
tion is that a set of index terms which is different 
than the gold standard can ?win? by providing 
better information access than the gold standard 
itself does. And although the individual human 
subject experiments are time consuming, the ex-
perimental interface, test materials and data 
analysis programs are completely re-usable.  
  
1 Introduction 
The standard metrics for evaluation of the output of 
NLP systems are precision and recall. Given an ar-
guably correct list of the units that a system would 
identify if it performed perfectly, there should in 
principle be no discrepancy between the units identi-
fied by a system and the units that are either useful in 
a particular application or are preferred by human 
beings for use in a particular task. But when the satis-
factory output can take many different forms, as in 
summarization and generation, evaluation by preci-
sion and recall is not sufficient. In these cases, the 
challenge for system designers and users is to effec-
tively distinguish between systems that provide gen-
erally satisfactory output and systems that do not.  
NP chunks (Abney 1991; Ramshaw and Marcus 
1995; Evans and Zhai 1996; Frantzi and Ananiadou 
1996) and technical terms (Dagan and Church 1994; 
Justeson and Katz 1995; Daille 1996; Jacquemin 
2001; Bourigault et al 2002) fall into this difficult-to-
assess category. NPs are recursive structures. For the 
maximal NP large number of recent newspaper articles 
on biomedical science and clinical practice, a full-
fledged parser would legitimately identify (at least) 
seven NPs in addition to the maximal one: large 
number; recent newspaper articles; large number of 
recent newspaper articles; biomedical science; clini-
cal practice; biomedical science and clinical prac-
tice; and recent newspaper articles on biomedical 
science and clinical practice. To evaluate the per-
formance of a parser, NP chunks can usefully be 
evaluated by a gold standard; many systems (e.g., 
Ramshaw and Marcus 1995 and Cardie and Pierce 
1988) use the Penn Treebank for this type of evalua-
tion. But for most applications, output that lists a 
maximal NP and each of its component NPs is bulky 
and redundant. Even a system that achieves 100% 
precision and recall in identifying all of the NPs in a 
document needs criteria for determining which units 
to use in different contexts or applications.  
Technical terms are a subset of NP chunks. Jac-
quemin (2001:3) defines terms as multi-word ?vehi-
cles of scientific and technical information?. 1  The 
operational difficulty, of course, is to decide whether 
a specific term is a vehicle of scientific and technical 
information (e.g., birth date or light truck). Evalua-
tion of mechanisms that filter out some terms while 
retaining others is subject to this difficulty. This is 
exactly the kind of case where context plays a sig-
nificant role in deciding whether a term conforms to a 
definition and where experts disagree.  
In this paper, we turn to an information access 
task in order to assess terms identified by different 
techniques. There are two basic types of information 
access mechanisms, searching and browsing. In 
searching, the user generates the search terms; in 
                                                          
1 Jacquemin does not use the modifier technical. 
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 189-196
                                                         Proceedings of HLT-NAACL 2003
browsing, the user recognizes potentially useful terms 
from a list of terms presented by the system. When an 
information seeker can readily think up a suitable 
term or linguistic expression to represent the informa-
tion need, direct searching of text by user-generated 
terms is faster and more effective than browsing. 
However, when users do not know (or can?t remem-
ber) the exact expression used in relevant documents, 
they necessarily struggle to find relevant information 
in full-text search systems. Experimental studies have 
repeatedly shown that information seekers use many 
different terms to describe the same concept and few 
of these terms are used frequently (Furnas et al 1987; 
Saracevic et al 1988; Bates et al 1998). When in-
formation seekers are unable to figure out the term 
used to describe a concept in a relevant document, 
electronic indexes are required for successful infor-
mation access.  
NP chunks and technical terms have been pro-
posed for use in this task (Boguraev and Kennedy 
1997; Wacholder 1998). NP chunks and technical 
terms have also been used in phrase browsing and 
phrase hierarchies (Jones and Staveley  1999;  Nevill-
Manning et al 1999; Witten et al 1999; Lawrie and 
Croft 2000) and summarization (e.g., McKeown et al 
1999; Oakes and Paice 2001). In fact, the distinction 
between task-based evaluation of a system and preci-
sion/recall evaluation of the quality of system output 
is similar to the extrinsic/intrinsic evaluation of 
summarization (Gallier and Jones 1993). 
In order to focus on the subjects? choice of index 
terms rather than on other aspects of the information 
access process, we asked subject to find answers to 
questions in a college level text book. Subjects used 
the Experimental Searching and Browsing Interface 
(ESBI) to browse a list of terms that were identified 
by different techniques and then merged. Subjects 
select an index term by clicking on it in order to hy-
perlink to the text itself. By design, ESBI forces the 
subjects to access the text indirectly, by searching 
and browsing the list of index terms, rather than by 
direct searching of the text.  
Three sets of terms were used in the experiment: 
one set (HS) was identified using the head-sorting 
method of Wacholder (1998); the second set (TT) 
was identified by an implementation of the technical 
term algorithm of Justeson and Katz (1995); a third 
set (HUM) was created by a human indexer. The 
methods for identifying these terms will be discussed 
in greater detail below.  
Somewhat to our surprise, subjects displayed a 
very strong preference for the index terms that were 
identified by the human indexer. Table 1 shows that 
when measured by percentage terms selected, sub-
jects chose over 13% of the available human terms, 
but only 1.73% and 1.43% of the automatically se-
lected terms; by this measure the subjects? preference 
for the human terms was more than 7 times greater 
than the preference for either of the automatic tech-
niques. (In Table 1 and in the rest of this paper, all 
index term counts are by type rather than by token, 
unless otherwise indicated.)  
 
 HUM HS TT 
Total number of 
terms 673 7980 1788 
Number of terms 
selected  89 114 31 
Percentage of 
terms selected 13.22% 1.43% 1.73% 
Table 1: Percentage of terms selected by human 
subjects relative to number of terms in the entire 
index. 
 
This initial experiment strongly indicates that 1) peo-
ple have a demonstrable preference for different 
types of index terms; 2) these human terms are a very 
good gold standard. If subjects use a greater propor-
tion of the terms identified by a particular technique, 
the terms can be judged better than the terms identi-
fied by another technique, even if the terms are dif-
ferent. Any automatic technique capable of 
identifying terms that are preferred over these human 
terms would be a very strong system indeed. Fur-
thermore, the properties of the terms preferred by the 
experimental subjects can be used to guide design of 
systems for identifying and selecting NP chunks and 
technical terms. 
In the next section, we describe the design of the 
experiment and in Section 3, we report on what the 
experimental data shows about human preferences 
for different kinds of index terms.   
2 Experimental design 
Our experiment assesses the index terms vis a vis 
their usefulness in a strictly controlled information 
access task. Subjects responded to a set of questions 
whose answers were contained in a 350 page college-
level text (Rice, Ronald E., McCreadie, Maureen and 
Chang, Shan-ju L. (2001) Accessing and Browsing 
Information and Communication. Cambridge, MA: 
MIT Press.) Subjects used the Experimental Search-
ing and Browsing Interface (ESBI) which forces 
them to access text via the index terms; direct text 
searching was prohibited. 25 subjects participated in 
the experiment; they were undergraduate and gradu-
ate students at Rutgers University. The experiments 
were conducted by graduate students at the Rutgers 
University School of Communication, Information 
and Library Studies (SCILS). 
2.1 ESBI (Experimental Searching and Brows-
ing Interface) 
Subjects used the Experimental Searching and 
Browsing Interface (ESBI) to find the answers to the 
questions. After an initial training session, ESBI pre-
sents the user with a Search/Browse screen (not 
shown); the question appears at the top of the screen.  
The subject may enter a string to search for in the 
index, or click on the "Browse" button for access to 
the whole index. At this point, "search" and "browse" 
apply only to the list of index terms, not to the text.  
The user may either browse the entire list of index 
terms or may enter a search term and specify criteria 
to select the subset of terms that will be returned. 
Most people begin with the latter option because the 
complete list of index terms is too long to be easily 
browsed.  The user may select (click on) an index 
term to view a list of the contexts in which the term 
appears. If the context appears useful, the user may 
choose to view the term in its full context; if not, the 
user may either do additional browsing or start the 
process over again. 
Figure 1 shows a screen shot of ESBI after the 
searcher has entered the string democracy in the 
search box. This view shows the demo question and 
the workspace for entering answers. The string was 
(previously) entered in the search box and all index 
terms that include the word democracy are displayed. 
Although it is not illustrated here, ESBI also permits 
substring searching and the option to specify case 
sensitivity.  
Regardless of the technique by which the term 
was identified, terms are organized by grammatical 
head of the phrase. Preliminary analysis of our results 
has shown that most subjects like this analysis, which 
resembles standard organization of back-of-the-book 
indexes.  
Readers may notice that the word participation 
appears at the left-most margin, where it represents 
the set of terms whose head is participation. The in-
dented occurrence represents the individual term. 
Selecting the left-most occurrence brings up contexts 
for all phrases for which participation is a head. Se-
lecting on the indented occurrence brings up contexts 
for the noun participation only when it is not part of 
a larger phrase. This is explained to subjects during 
the pre-experimental training and an experimenter is 
present to remind subjects of this distinction if a 
question arises during the experiment. 
Readers may also notice that in Figure 1, one of 
the terms, participation require, is ungrammatical. 
This particular error was caused by a faulty part-of-
speech tag. But since automatically identified index 
terms typically include some nonsensical terms, we 
have left these terms in ? these terms are one of the 
problems that information seekers have to cope with 
in a realistic task-based evaluation. 
 
 
 
Figure 1: ESBI Screen shot 
 
2.2 Questions 
After conducting initial testing to find out what types 
of questions subjects founder hard or easy, we spent 
considerable effort to design a set of 26 questions of 
varying degrees of difficulty. To obtain an initial 
assessment of difficulty, one of the experimenters 
used ESBI to answer all of the questions and rate 
each question with regard to how difficult it was to 
answer using the ESBI system. For example, the 
question What are the characteristics of 
Marchionini's model of browsing? was rated very 
easy because searching on the string marchionini 
reveals an index term Marchionini's which is linked 
to the text sentence: Marchionini's model of browsing 
considers five interactions among the information-
seeking factors of "task, domain, setting, user charac-
teristics and experience, and system content and in-
terface" (p.107). The question What factors 
determine when users decide to stop browsing? was 
rated very difficult because searching on stop (or 
synonyms such as halt, cease, end, terminate, finish, 
etc.) reveals no helpful index terms, while searching 
on factors or browsing yields an avalanche of over 
500 terms, none with any obvious relevance.   
After subjects finished answering each question, 
they were asked to rate the question in terms of its 
difficulty. A positive correlation between judgments 
of the experimenters and the experimental subjects 
(Sharp et al, under submission) confirmed that we 
had successfully devised questions with a range of 
difficulty.  In general, questions that included terms 
actually used in the index were judged easier; ques-
tions where the user had to devise the index terms 
were judged harder. 
To avoid effects of user learning, questions were 
presented to subjects in random order; in the one hour 
experiment, subjects answered an average of about 9 
questions.  
 
2.3 Terms 
Although the primary goal of this research is to point 
the way to improved techniques for automatic crea-
tion of index terms, we used human created terms to 
create a baseline. For the human index terms, we 
used the pre-existing back-of-the-book index, which 
we believe to be of high quality.2 
The two techniques for automatic identification 
were the technical terms algorithm of Justeson and 
Katz (1995) and the head sorting method (Dagan and 
Church (1994); Wacholder (1998). In the implemen-
tation of the Justeson and Katz? algorithm, technical 
terms are multi-word NPs repeated above some 
threshold in a corpus; in the head sorting method, 
technical terms are identified by grouping noun 
phrases with a common head (e.g., health-care work-
ers  and asbestos workers), and selecting as terms 
those NPs whose heads appear in two or more 
phrases. Definitionally, technical terms are a proper 
subset of terms identified by Head Sorting. Differ-
ences in the implementations, especially the pre-
processing module, result in there being some terms 
identified by Termer that were not identified by Head 
Sorting.   
Table 2 shows the number of terms identified by 
each method. (*Because some terms are identified by 
more than one technique, the percentage adds up to 
more than 100%.) The fewest terms (673) were iden-
tified by the human method; in part this reflects the 
judgment of the indexer and in part it is a result of 
restrictions on index length in a printed text. The 
largest number of terms (7980) was identified by the 
head sorting method. This is because it applies  
looser criteria for determining a term than does the 
Justeson and Katz algorithm which imposes a very 
strict standard--no single word can be considered a 
term, and an NP must be repeated in full to be con-
sidered a term.   
 
                                                          
2 Jim Snow  prepared the index under the supervision of  
SCILS Professor James D. Anderson. 
 HUM HS TT Total 
Total 
number 
of terms 
673 7980 1788 9992 
Per-
centage 
of total 
number 
of terms 
6.73% 79.86% 17.89% * 
Table 2: Number of terms in index by method of 
identification 
 
Wacholder et al (2000) showed that when experi-
mental subjects were asked to assess the usefulness 
of terms for an information access task without actu-
ally using the terms for information access showed 
that the terms identified by the technical term algo-
rithm, which are considerably fewer than the terms 
identified by head sorting, were overall of higher 
quality than the terms identified by the head sorting 
method. However, the fact that subjects assigned a 
high rank to many of the terms identified by Head 
Sorting suggested that the technical term algorithm 
was failing to pick up many potentially useful index 
terms.  
In preparation for the experiment, all index terms 
were merged into a single list and duplicates were 
removed, resulting in a list of nearly 10,000 index 
terms. 
 
2.4 Tracking results 
In the experiment, we logged the terms that sub-
jects searched for (i.e., entered in a search box) and 
selected. In this paper, we report only on the terms 
that the subjects selected (i.e., clicked on). This is 
because if a subject entered a single word, or a sub-
part of a word in the search box, ESBI returned to 
them a list of index terms; the subject then selected a 
term to view the context in which it appears in the 
text. This term might have been the same term origi-
nally searched for or it might have been a super-
string. The terms that subjects selected for searching 
are interesting in their own right, but are not analyzed 
here.  
3 Results 
At the outset of this experiment, we did not know 
whether it would be possible to discover differences 
in human preferences for terms in the information 
access task reported on in this paper. We therefore 
started our research with the null hypothesis that all 
index terms are created equal. If users selected index 
terms in roughly the same proportion as the terms 
occur in the text, the null hypothesis would be 
proven. 
 The results strongly discredit the null hypothesis. 
Table 3 shows that when measured by percentage of 
terms selected, subjects selected on over 13.2% of the 
available human terms, but only 1.73% and 1.43% 
respectively of the automatically selected terms. Ta-
ble 3 also shows that although the human index terms 
formed only 6% of the total number of index terms, 
40% of the terms which were selected by subjects in 
order to view the context were identified by human 
indexing. Although 80% of the index terms were 
identified by head sorting, only 51% of the terms 
subjects chose to select had been identified by this 
method. (*Because of overlap of terms selected by 
different techniques, total is greater than 100%) 
 
 HM HS  TT Total 
All terms  673 7980 1788 9992 
Percentage 
of  all 
terms 
6.73% 79.9% 17.9% * 
     
Total 
number of 
terms se-
lected  
89 114 31 223 
Percentage 
of terms 
selected 
39.9% 51.1% 13.9 * 
     
Percentage 
of avail-
able terms 
selected 
13.2% 1.43% 1.73% 
 
Table 3: Subject selection of index terms, by 
method. 
 
To determine whether the numbers represent statisti-
cally significant evidence that the null hypothesis is 
wrong, we represent the null hypothesis (HT)) as (1) 
and the falsification of the null hypothesis (HA) as 
(2). 
         HT: P1/?1 = P2/?2                          (1) 
         HA:  P1/?1 ? P2/?2                          (2)     
Pi is the expected percentage of the selected terms 
that are type i in all the selected terms; ?i is the ex-
pected percentage if there is no user preference, i.e. 
the proportion of this term type i in all the terms. We 
rewrite the above as (3). 
HT: X = 0    HA: X ? 0    X = P1/?1 ? P2/?2    (3) 
Assuming that X is normally distributed, we can use 
a one-sample t test on X to decide whether to accept 
the hypothesis (1). The two-tailed t test (df =222) 
produces a p-value of less than .01% for the compari-
son of the expected and selected proportions of a) 
human terms and head sorted terms and b) human 
terms and technical terms. In contrast, the p-value for 
the comparison of head-sorted and technical terms 
was 33.7%, so we draw no conclusions about relative 
preferences for head sorted and technical terms.  
We also considered the possibility that our formu-
lation of questions biased the terms that the subjects 
selected, perhaps because the words of the questions 
overlapped more with the terms selected by one of 
the methods. 3 We took the following steps:  
1) For each search word, calculate the number of 
terms overlapping with it from each source. 
2) Based on these numbers, determine the proportion 
of terms provided by each method. 
3) Sum the proportions of all the search words. 
As measured by the terms the subjects saw during 
browsing, 22% were human terms, 62% were head 
sorted terms and 16% were technical terms. Using the 
same reasoning about the null hypothesis as above, 
the p-value for the comparison of the ratios of human 
and head sorted terms was less than 0.01%, as was 
the comparison of the ratios of the human and techni-
cal terms. This supports the validity of the results of 
the initial test. In contrast, the p-value for the com-
parison of the two automatic techniques was 77.3%.  
Why did the subjects demonstrate such a strong 
preference for the human terms? Table 4 illustrates 
some important differences between the human terms 
and the automatically identified terms. The terms 
selected on are longer, as measured in number of 
words, and more complex, as measured by number of 
prepositions per index terms and by number of con-
tent-bearing words. As shown in Table 5, the differ-
ence of these complexity measures between human 
terms and automatically identified terms are statisti-
cally significant. 
Since longer terms are more specific than shorter 
terms (for example, participation in a democracy is 
longer and more specific than democracy), the results 
suggest that subjects prefer the more specific terms. 
If this result is upheld in future research, it has practi-
cal implications for the design of automatic term 
identification systems.  
 
                                                          
 
 Num-
ber of 
terms 
selected  
Average 
length of 
term in 
words 
Preposi-
tions per 
index 
term 
Content-
bearing 
words 
per in-
dex term 
HM 89 6.22 1.4 4.54 
HS 114 2.59 0.026 2.23 
TT 31 2.26 0 2.26 
Table 4: Measures of index term complexity  
 
 Average 
length of 
term in 
number of 
words 
Number 
of prepo-
sitions 
per index 
term 
Number of 
content-
bearing 
words per 
index term 
HM vs HS  <0.01% <0.01% <0.01% 
HM vs TT <0.01% <0.01% <0.01% 
HS vs TT 0.57% 8.33% 77.8% 
Table 5: Result of two-independent-sample two-
tailed t-test on index term complexity. The num-
bers in the cells are p-value of the test. 
4.3    Relationship between Term Source and 
Search Effectiveness 
In this paper, our primary focus is on the question of 
what makes index terms 'better', as measured by user 
preferences in a question-answering task. Also of 
interest, of course, is what makes index terms 'better' 
in terms of how accurate the resulting users' answers 
are.   The problem is that any facile judgment of free-
text answer accuracy is bound to be arbitrary and 
potentially unreliable; we discuss this in detail in 
[26].  Nevertheless, we address the issue in a prelimi-
nary way in the current paper.  We used an ad hoc set 
of canonical answers to score subjects' answers on a 
scale of 1 to 3, where 1 stands for 'very accurate', 2 
stands for 'partly accurate' and 3 represents 'not at all 
accurate'. Using general loglinear regression (Poisson 
model) under the hypothesis that these two variables 
are independent of each other, our analysis showed 
that there is a systematic relationship (significance 
probability is 0.0504) between source of selected 
terms and answer accuracy. Specifically, in cases 
where subjects used more index terms identified by 
the human indexer, the answers were more accurate. 
On the basis of our initial accuracy judgments, we 
can therefore draw the preliminary conclusion that 
terms that were better in that they were preferred by 
the experimental subjects were also better in that they 
were associated with better answers. We plan to con-
duct a more in-depth analysis of answer accuracy and 
will report on it in future work.   
But the primary question addressed in this paper 
is how to reliably assess NP chunks and technical 
terms. These results constitute experimental evidence 
that the index terms identified by the human indexer 
constitute a gold standard, at least for the text used in 
the experiment. Any set of index terms, regardless of 
the technique by which they were created or the crite-
ria by they were selected, can be compared vis a vis 
their usefulness in the information access task. 
4 Discussion 
The contribution of this paper is the description of a 
task-based gold-standard method for evaluating the 
usefulness and therefore the quality of NP chunks 
and technical terms. In this section, we address a 
number of questions about this method.  
1) What properties of terms can this technique 
be used to study?  
? One word or many. There are two parts to 
the process of identifying NP terms: NP 
chunks that are candidate terms must be 
identified and candidate terms must be fil-
tered in order to select a subset appropriate 
for use in the intended application. Justeson 
and Katz (1995) is an example of an algo-
rithm where the process used for identifying 
NP chunks is also the filtering process. A 
byproduct of this technique is that single-
word terms are excluded. In part, this is be-
cause it is much harder to determine in con-
text which single words actually qualify as 
terms. But dictionaries of technical termi-
nology have many one-word terms.   
? Simplex or complex NPs (e.g., Church 
1988; Hindle and Rooth 1991; Wacholder 
1998) identify simplex or base NPs ? NPs 
which do not have any component NPs -- at 
least in part because this bypasses the need 
to solve the quite difficult attachment prob-
lem, i.e., to determine which simpler NPs 
should be combined to output a more com-
plex NP.  But if people find complex NPs 
more useful than simpler ones, it is impor-
tant to focus on improvement of techniques 
to reliably identify more complex terms. 
? Semantic and syntactic terms variants. 
Daille et al (1996), Jacquemin (2001) and 
others address the question of how to iden-
tify semantic (synonymous) and syntactic 
variants. But independent of the question of 
how to recognize variants is the question of 
which variants are to be preferred for differ-
ent kinds of uses. 
? Impact of errors. Real-world NLP systems 
have a measurable error rate. By conducting 
experiments in which terms with errors are 
include in the set of test terms, the impact of 
these errors can be measured. The useful-
ness of a set of terms presumably is at least 
in part a function of the impact of the errors, 
whether the errors are a by-product of the 
algorithm or the implementation of the algo-
rithm. 
 
2) Could the set of human index terms be used 
as a gold standard without conducting the 
human subject experiments? This of course 
could be done, but then the terms are being 
evaluated by a fixed standard ? by definition, no 
set of terms can do better than the gold standard. 
This experimental method leaves open the possi-
bility that there is a set of terms that is better 
than the gold standard. In this case, of course, the 
gold standard would no longer be a gold standard 
-- perhaps we would have to call it a platinum 
standard. 
 
3) How reproducible is the experiment? The ex-
periment can be re-run with any set of terms 
deemed to be representative of the content of the 
Rice text. The preparation of the materials for 
additional texts is admittedly time-consuming. 
But over time a sizable corpus of experimental 
materials in different domains could be built up. 
These materials could be used for training as 
well as for testing. 
          
4) How extensible is the gold standard? The ex-
perimental protocol will be validated only if 
equally useful index terms can be created for 
other texts. We anticipate that they can. 
  
5) How can this research help in the design of 
real world NLP systems? This technique can 
help in assessing the relative usefulness of exist-
ing techniques for identifying terms. It is possi-
ble, for example, there already exist techniques 
for identifying terms that are superior to the two 
tested here. If we can find such systems, their al-
gorithms should be preferred. If not, there re-
mains a need for development of algorithms to 
identify single word terms and complex phrases. 
 
6) Do the benefits of this evaluation technique 
outweigh the costs?  Given the fundamental dif-
ficulty of evaluating NP chunks and technical 
terms, task-based evaluation is a promising sup-
plement to evaluation by precision and recall. 
These relatively time-consuming human subject 
experiments surely will not be undertaken by 
most system developers; ideally, they should be 
performed by neutral parties who do not have a 
stake in the outcome.  
 
7) Should automated indexes try to imitate hu-
man indexers? Automated indexes should  con-
tain terms that are most easily processed by 
users. If the properties of such terms can be re-
liably discovered, developers of systems that 
identify terms intended to be processed by peo-
ple surely should pay attention.  
 
5 Conclusion 
In this paper we have reported on a rigorous experi-
mental technique for black-box evaluation of the use-
fulness of NP chunks and technical terms in an 
information access task. Our experiment shows that it 
is possible to reliably identify human preferences for 
sets of terms.  
The set of human terms created for use in a back-
of-the-book index serves as a gold standard. An ad-
vantage of the task-based evaluation is that a set of 
terms could outperform the gold standard; any system 
that could do this would be a good system indeed.  
The two automatic methods that we evaluated 
performed much less well than the terms created by 
the human indexer; we plan to evaluate additional 
techniques for term identification in the hope of iden-
tifying automatic methods that identify index terms 
that people prefer over the human terms. We also 
plan to prepare test materials in different domains, 
and assess in greater depth the properties of the terms 
that our experimental subjects preferred; our goal is 
to develop practical guidelines for the identification 
and selection of technical terms that are optimal for 
human users. We will also study the impact of se-
mantic differences between terms on user preferences 
and investigate whether terms which are preferred for 
information access are equally suitable for other NLP 
tasks. 
 
6 Acknowledgements 
We are grateful to the other members of the Rutgers 
NLP-I research group, Lu Liu, Mark Sharp, and 
Xiaojun Yuan, for their valuable contribution to this 
project. We also thank Paul Kantor, Judith L. Kla-
vans, Evelyne Tzoukermann , Min Yen Kan, and 
three anonymous reviewers for their helpful sugges-
tions. Funding for this research has been provided by 
the Rutgers University Information Science and 
Technology Council. 
 
  
References
Abney, Steven (1991) Parsing by chunks. Principle-
Based Parsing, edited by Steven Abney, Robert 
Berwick and Carol Tenny. Kluwer: Dordrecht. 
Bates, Marcia J. (1998) Indexing and access for digital 
libraries and the Internet: human, database and do-
main factors. Journal of the American Society for 
Information Science, 49(13), 1185-1205. 
Boguraev, Branimir and Kennedy, Christopher (1997) 
Salience-based content characterization of text. ACL 
EACL Workshop on Intelligent Scalable Text 
Summarization, 2-9.  
Bourigault, Didier, Jacquemin, Christian and L?Homme, 
Marie Claude (2001) Recent Advances in Computa-
tional Terminology. John Benjamins: Philadelphia, 
PA. 
Church, Kenneth Ward (1988) A Stochastic Parts Pro-
gram and Noun Phrase Parser for Unrestricted Text. 
Proceedings of Second Applied Natural Language 
Processing Conference, pp.136-143. 
Dagan, Ido and Church, Kenneth (1994) TERMIGHT: 
Identifying and translating technical terminology. 
Proceedings of the Fourth ACL Conference on Ap-
plied Natural Language Processing, pp.34-40. 
Daille Beatrice (1996) Study and implementation of 
combined techniques for automatic extraction of ter-
minology. The Balancing Act, pp.49-66. Edited by 
Judith L. Klavans and Philip Resnik. MIT Press, 
Cambridge, MA. 
Daille, Beatrice, Habert, Benoit., Jacquemin, Christian, 
& Royaute, Jean (2000) Empirical observation of 
term variations and principles for their description. 
Terminology, 3(2):197-258. 
Furnas, George, Landauer, Thomas, Gomez, Louis & 
Dumais, Susan T. (1987) The vocabulary problem 
in human-system communication. Communications 
of the ACM, 30(11), 964-971. 
Galliers, Julia Rose & Jones, Karen Sparck (1995) 
Evaluating natural language processing systems. Lec-
ture Notes in Artificial Intelligence. Springer, New 
York, 1995.  
Jacquemin, Christian (2001). Spotting and Discovering 
Terms through Natural Language Processing. Cam-
bridge, MA: MIT Press. 
Jones, Steve and Staveley, Mark S. (1999) Phrasier: a 
system for interactive document retrieval using key-
phrases. Proceedings of the 22nd annual interna-
tional ACM SIGIR conference, pp.160-167.  
Justeson, John S. & Slava M. Katz (1995) ?Technical 
terminology: some linguistic properties and an algo-
rithm for identification in text?, Natural Language 
Engineering 1(1):9-27.  
Hindle, Donald and Rooth, Matt (1993) Structural am-
biguity and lexical relations. Computational Linguis-
tics 19(1):103-120. 
Lawrie, Dawn and Croft, W. Bruce (2000) Discovering 
and comparing topic hierarchies. Proceedings  of 
RIAO 2000 Conference, 314-330.  
McKeown, Kathy, Klavans, Judith, Hatzivassiloglou, 
Vasileios, Barzilay, Regina and Eskin, Eleazar 
(1999) Towards multidocument summarization by 
reformulation: Progress and prospects. Proceedings 
of AAAI-99, pp.453-460.   
Nevill-Manning, Craig, Witten, Ian and Paynter, 
Gordon W. (1999). Lexically-generated subject hi-
erarchies for browsing large collections. Int?l Jour-
nal on Digital Libraries, 2(2-3):111-123. 
Oakes, Michael P. and Paice, Chris D. (2001) Term ex-
traction for automatic abstracting. In Bourigault et 
al., eds. 
Ramshaw, Lance A., and Marcus, Mitchell P. (1995) 
Text chunking using transformation-based learning. 
Proceedings of the Third ACL Workshop on Very 
Large Corpora, pp. 82-94. 
Rice, Ronald E., Maureen McCreadie & Shan-ju L. 
Chang (2001). Accessing and Browsing Information 
and Communication. Cambridge, MA: MIT Press. 
Saracevic, Tefko, Paul Kantor, Alice Y. Chamis & 
Donna Trivison (1988) A study of information 
seeking and retrieving: I. Background and method-
ology. Journal of the American Society for Infor-
mation Science, 39(3), 161-176. 
Sharp, M., Liu, L., Yuan, X., Song, P., & Wacholder, N. 
(2003). Question difficulty effects on question an-
swering involving mandatory use of a term index. 
Under submission. 
Wacholder, N., Sharp, M., Liu, L., Yuan, X., & Song, P. 
(2003). Experimental study of index terms and in-
formation access. Under submission. 
Wacholder, Nina (1998) "Simplex noun phrases clus-
tered by head: a method for identifying significant 
topics in a document", Proceedings of Workshop on 
the Computational Treatment of Nominals, pp.70-
79. COLING-ACL, October 16, 1998. 
Wacholder, Nina, Judith L. Klavans and David Kirk 
Evans (2000) "Evaluation of automatically identified 
index terms for browsing electronic documents", 
Proceedings of the NAACL/ANLP2000, Seattle, 
Washington. 
Witten, Ian H., Paynter, Gordon W., Eibe, Frank, Gut-
win, and Nevill-Manning Craig G. KEA: practical 
automatic keyphrase extraction. Proceedings of the 
fourth ACM Conference on Digital Libraries, 
pp.254-255. 
 
HITIQA: Scenario Based Question Answering 
Sharon Small, Tomek Strzalkowski, Tracy Janack, Ting Liu,  
Sean Ryan, Robert Salkin, Nobuyuki Shimizu 
The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tomek,tj5550,tl7612,seanryan,rs6021,ns3203}@albany.edu 
 
Paul Kantor, Diane Kelly, Robert Rittman, Nina Wacholder 
Rutgers University 
New Brunswick, New Jersey 08903 
{kantor, nina, diane, rritt}@scils.rutgers.edu 
 
Boris Yamrom 
Lehman College of the City University of New York 
Bronx, New York 10468 
byamrom@lehman.cuny.edy 
 
 
 
Abstract 
In this paper we describe some preliminary 
results of qualitative evaluation of the answer-
ing system HITIQA (High-Quality Interactive 
Question Answering) which has been devel-
oped over the last 2 years as an advanced re-
search tool for information analysts. HITIQA 
is an interactive open-domain question an-
swering technology designed to allow analysts 
to pose complex exploratory questions in natu-
ral language and obtain relevant information 
units to prepare their briefing reports in order 
to satisfy a given scenario. The system uses 
novel data-driven semantics to conduct a clari-
fication dialogue with the user that explores 
the scope and the context of the desired answer 
space. The system has undergone extensive 
hands-on evaluations by a group of intelli-
gence analysts representing various foreign in-
telligence services. This evaluation validated 
the overall approach in HITIQA but also ex-
posed limitations of the current prototype.  
1   Introduction 
Our objective in HITIQA is to allow the user to 
submit exploratory, analytical questions, such as ?What 
has been Russia?s reaction to U.S. bombing of Kos-
ovo?? The distinguishing property of such questions is 
that one cannot generally anticipate what might consti-
tute the answer. While certain types of things may be 
expected (e.g., diplomatic statements), the answer is 
heavily conditioned by what information is in fact avail-
able on the topic, background knowledge of the user, 
context in the scenario, intended audience, etc. From a 
practical viewpoint, analytical questions are often un-
derspecified, thus casting a broad net on a space of pos-
sible answers. Therefore, clarification dialogue is often 
needed to negotiate with the user the exact scope and 
intent of the question, and clarify whether similar topics 
found might also be of interest to the user in order to 
complete their scenario report. This paper will present 
results from a series of evaluations conducted in a series 
of workshops with the intended end users of HITIQA 
(professional intelligence analysts) using the system to 
solve realistic analytic problems. 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in the 
state of the art of automated question answering.  In this 
paper we focus on our approach to analytical question 
answering in order to produce a report in response to a 
given scenario.  We also report on the user evaluations 
we conducted and their results with respect to our 
unique approach. 
2   Analytical QA Scenarios 
Analytical scenarios are information task directives 
assigned to analysts to support a larger foreign policy 
process. Scenarios thus contain the information need 
specifications at various levels of detail,  the type, for-
mat and timing of the response required (an intelligence 
report) as well as the primary recipient of the report 
(e.g., the Secretary of State). A hypothetical, but realis-
tic scenario is shown in Figure 1 below. This scenario, 
along with several others like it, was used in evaluating 
 HITIQA performance and fitness for supporting the 
analytical process.  
As can be readily assessed from the directives in 
Figure 1, scenarios are not merely tough questions; they 
are far too complex to be considered as a single question 
at all. It is equally clear that no simple answer can be 
expected and that preparing a report would mean find-
ing answers to a series of interlocking questions or vari-
ous granularities.  
 
Scenario: The al-Qaida Terrorist Group 
 
As an employee of the Central Intelligence Agency, your pro-
fession entails knowledge of the al-Qaida terrorist group.  
Your division chief has ordered a detailed report on the al-
Qaida Terrorist Group due in three weeks. Provide as much 
information as possible on this militant organization. Eventu-
ally, this report should present information regarding the most 
essential concerns, including who are the key figures involved 
with al-Qaida along with other organizations, countries, and 
members that are affiliated, any trades that al-Qaida has made 
with organizations or countries, what facilities they possess, 
where they receive their financial support, what capabilities 
they have (CBW program, other weapons, etc.) and how have 
they acquired them, what is their possible future activity, how 
their training program operates, who their new members are. 
Also, include any other relevant information to your report as 
you see fit.  
 FIGURE 1: Scenario used during user evaluations 
  
We have organized a series of usability evaluations 
with active duty intelligence analysts to find out how 
they approach the problem of solving a scenario. The 
prerequisites for this were are follows: 
1. A robust, broadly functional analytical QA sys-
tem capable of sustaining realistic analytic 
tasks. 
2. A realistic corpus of ?raw intelligence? in form 
of varying quality and verity new-like reports. 
3. A set of realistic, average complexity analytic 
tasks or scenarios to be used. 
HITIQA has been developed over the past two years as 
an open-ended highly flexible interactive QA system to 
allow just this type of evaluation. The system supports a 
variety of information gathering functions without 
straight jacketing the user into any particular mode or 
interaction style. The system does not produce cut and 
dry ?answers?; instead it allows the analysts to build the 
answers the way they want them. While this open-
endedness may seem like unfinished business, we be-
lieve that further development must take into account 
the needs of analysts if they were ever to adopt this 
technology in their work. 
Our main hypothesis is that analysts employ a range 
of strategies to find the required information and that 
these strategies depend significantly upon the nature of 
the task and the progress the analyst is making on the 
task, in addition to individual differences between ana-
lysts. Our experience with interactive systems also indi-
cated that real users are unlikely to follow any single 
information exploration strategy, but instead would use 
multiple, parallel, even overlapping approaches in order 
to maximize the returns and their confidence in the re-
sults. As a corollary we may expect that the scenario 
tasks are unlikely to be systematically decomposed into 
a series of smaller tasks ahead of actual search. In other 
words, the analytical process is a dialogue, not a se-
quence of commands. Moreover, questions actually 
submitted to the system during the analytical process 
seldom seek just the exact answer, instead they are often 
considered as ?light beams? through the data: focusing 
on the answer but also illuminating adjacent, related 
information which may prove just as valuable.  
AFRL, NIST, CNS and ARDA collaborated in the 
development of scenarios used in our evaluation ses-
sions.  
3   Data Driven Semantics of Questions 
When the user poses a question to a system having 
access to a huge database of unstructured data (text 
files), we need to first reduce the big pile to perhaps a 
handful of documents where the answer is likely to be 
found. The easiest way to do it is to convert the question 
into a search query (by removing stopwords and stem-
ming and tokenizing other words) and submitting this 
query to a fast but non-exact document retrieval system, 
e.g.,   Smart (Buckley, 1985) or InQuery (Callan et al, 
1992), or if you are on the web, Google, etc.   
In the current prototype of HITIQA, we use a com-
bination of Google and InQuery to retrieve the top 50 to 
200 documents from a large document database, con-
sisting of several smaller collections such as newspaper 
stories, documents from the Center of Nonproliferation 
Studies, as well as web mined files.  The retrieved 
documents are then broken down into passages, mostly 
exploiting the naturally occurring paragraph structure of 
the original sources. 
The set of text passages returned from the initial 
search is the first (very crude) approximation of the An-
swer Space for the user?s first question. In order to de-
termine what this answer space consists of we perform 
automatic analysis (a combination of hierarchical clus-
tering and classification) to uncover if what we got is a 
fairly homogenous collection (i.e., all texts have very 
similar content), or whether there are a number of di-
verse topics or aspects represented in there, somehow 
tied together by a common thread. In the former case, 
we may be reasonably confident that we have the an-
swer, modulo the retrievable information. In the latter 
case, we know that the question is more complex than 
the user may have intended, and a negotiation process is 
needed to clarify topics of interest for the scenario re-
port. 
 The next step is to measure how well each of the as-
pects within the answer space is ?matching up? against 
the original question. This is accomplished through the 
framing process described later in this paper. The out-
come of the framing process is twofold: first, the alter-
native interpretations of the question are ranked within 3 
broad categories: on-target, near-misses and outliers. 
Second, salient concepts and attributes for each topi-
cal/aspectual group are extracted into topic frames. This 
enables the system to conduct a meaningful dialogue 
with the user, a dialogue which is wholly content ori-
ented, and entirely data driven.  
4   Partial structuring of text data 
In HITIQA we use a text framing technique to de-
lineate the gap between the meaning of the user?s ques-
tion and the system ?understanding? of this question. 
The framing is an attempt to impose a partial structure 
on the text that would allow the system to systemati-
cally compare different text pieces against each other 
and against the question, and also to communicate with 
the user about this. In particular, the framing process 
may uncover topics or aspects within the answer space 
which the user has not explicitly asked for, and thus 
may be unaware of their existence.  This approach is 
particularly beneficial to the needs of the scenario prob-
lem, where these similar aspects frequently are needed 
in completely ?answering? the scenario, with the sce-
nario report.   
In the current version of HITIQA, frames are pre-
defined structures representing various event types. We 
started with the General frame, which can represent any 
event or relation involving any number of entities such 
as people, locations, organizations, time, and so forth.  
In a specialized domain, or if the user interests are 
known to be limited to a particular set of topics, we de-
fine domain-specific frames. Current HITIQA prototype 
has three broad domain-specific frames, related to the 
Weapon of Mass Destruction proliferation domain 
(which was one of the domains of interest to our users). 
These frames are: WMDTransfer, WMDDevelop, 
WMDTreaty, and of course we keep the General frame.  
Obviously, these three frames do not cover the domain 
represented by our data set; they merely capture the 
most commonly occurring types of events. All frames 
contain a small number of core attributes, such as LO-
CATION, PERSON, COUNTRY, ORGANIZATION, ETC., which 
are extracted using BBN?s Identifinder software, which 
extracts 24 types of entities.  Domain-specific frames 
add event specific attributes, which may require extract-
ing additional items from text, or assigning roles to ex-
isting attributes, or both.  For example, WMDTransfer?s 
attributes TRANSFER_TO and TRANSFER_FROM define 
roles of some COUNTRY or ORGANIZATION, while the 
TRANSFER_TYPE attribute scans the text for keywords 
that may indicate the type of transfer, e.g., export, sale, 
etc.  
HITIQA creates a Goal frame for the user?s ques-
tion, which can be subsequently compared to the data 
frames obtained from retrieved data. A Goal frame can 
be a General frame or any of the domain specific frames 
available in HITIQA.  For example, the Goal frame 
generated from the question, ?Where does al-Qaida 
have training facilities?? is a General frame as shown in 
Figure 2.  This was the first question generated by one 
of our analysts during the first evaluation while working 
on the al-Qaida scenario shown in Figure 1. 
 
FRAME TYPE: General 
TOPIC: training facilities 
ORGANIZATION: al-Qaida 
FIGURE 2: HITIQA generated General-type Goal frame from 
the al-Qaida training facilities question 
 
FRAME TYPE: General 
CONFLICT SCORE: 1 
TRANSFER TYPE: provided 
TRANSFER TO: al-Qaida 
TRANSFER FROM: Iraq 
TOPIC: provided 
SUB-TOPIC: imported 
LOCATION: Iraq 
PEOPLE: Abu Musab al-Zarqawi, Bush, George 
Tenet, Saddam Hussein 
ORGANIZATION:CIA, Administration, al-Qaida 
DOCUMENT: web_283330  
PARAGRAPHS:  ["CIA chief George Tenet seems to 
have gone a long way to back the Bush Administrations dec-
larations that the long split between Islamic fundamentalist 
terrorist organizations like Al-Qaida and secular Iraqi ruler 
Saddam Hussein is healed.   
He has testified that the CIA has evidence of Iraqi provid-
ing Al Qaida with training in forgery and bomb making and of 
providing two, Al Qaida associates with training in gas and 
poisons. He said also that Iraq is harboring senior members 
of a terrorist network led by Abu Musab al-Zarqawi, a close 
Al Qaida associate. "]  
RELEVANCE:  Conflict: [Topic] 
FIGURE 3: A HITIQA generated data frame and the un-
derlying text passage. Words in bold were used to fill the 
Frame.   
 
HTIQA automatically judges a particular data frame 
as relevant, and subsequently the corresponding seg-
ment of text as relevant, by comparison to the Goal 
frame. The data frames are scored based on the number 
of conflicts found between them and the Goal frame. 
The conflicts are mismatches on values of correspond-
ing attributes. If a data frame is found to have no con-
flicts, it is given the highest relevance rank, and a con-
flict score of zero.  All other data frames are scored with 
 a decreasing value based on the number of conflicts, 
negative one for frames with one conflict with the Goal 
frame, negative two for two conflicts etc.  Frames that 
conflict with all information found in the question are 
given a score of -99 indicating the lowest relevancy 
rank.  Currently, frames with a conflict score of -99 are 
excluded from further processing as outliers. The frame 
in Figure 2 is scored as a near miss and will generate 
dialogue, where the user will decide whether or not it 
should be included in the answer space. 
5   Clarification Dialogue 
Data frames with a conflict score of zero form the 
initial kernel answer space. Depending upon the pres-
ence of other frames outside of this set, the system ei-
ther proceeds to generate the answer or initiates a dia-
logue with the user.  HITIQA begins asking the user 
questions on these near-miss frame groups, with the 
largest group first.  The groups must be at least groups 
of size N, where N is a user controlled setting.  This 
setting restricts all of HITIQA?s generated dialogue.   
A one conflict frame has only a single attribute 
mismatch with the Goal frame. This could be a mis-
match on any of the General attributes, for example, 
LOCATION, or ORGANIZATION, or TIME, etc., or in one of 
the domain specific attributes, TRANSFER_TO, or TRANS-
FER_TYPE, etc.  A special case arises when the conflict 
occurs on the TOPIC attribute.  Since all other attributes 
match, we may be looking at potentially different events 
or situations involving the same entities, or occurring at 
the same location or time. The purpose of the clarifica-
tion dialogue in this case is to probe which of these top-
ics may be of interest to the user.  Another special case 
arises when the Goal frame is of a different type than a 
data frame.  The purpose of the clarification dialogue in 
this case is to expand the user?s answer space into a 
different but possibly related event.  A combination of 
both of these cases is illustrated in the exchange in Fig-
ure 4 below.   
User: ?Where does al-Qaida have training facili-
ties?? 
HITIQA: ?Do you want to see material on the trans-
fer of weapons and intelligence to al-Qaida?? 
FIGURE 4: Dialogue generated by HITIQA for the al-Qaida 
training facilities question 
 
In order to understand what happened here, we need 
to note first that the Goal frame for this example is a 
General Frame, from Figure 2.  One of the data frames 
that caused this dialogue to be generated is shown in 
Figure 3 above.  While this frame is of a different frame 
type than the Goal frame, namely WMD Transfer, it 
matches on all of the General attributes except TOPIC, so 
HITIQA asks the user if they would like to expand their 
answer space to this other domain, namely to include 
the transfer of weapons involving this organization as 
well.   
 
ANSWER REPORT:  
 
The New York Times said the Mindanao had become the 
training center for the Jemaah Islamiah network, believed by 
many Western governments to be affiliated to the al-Qaida 
movement of Osama bin Laden 
DocName: A-web_283305 ParaId: 2  
 
? 
IRAQ REPORTED TO HAVE PROVIDED MATERIALS 
TO AL QAIDA  
2003  
[CIA chief George Tenet seems to have gone a long way to 
back the Bush Administrations declarations that the long split 
between Islamic fundamentalist terrorist organizations like Al 
Qiada and secular Iraqi ruler Saddam Hussein is healed. 
DocName: A-web_283330 ParaId: 6  
He has testified that the CIA has evidence of Iraqi providing 
Al Qaida with training in forgery and bomb making and of 
providing two, Al Qaida associates with training in gas and 
poisons. He said also that Iraq is harboring senior members of 
a terrorist network led by Abu Musab al-Zarqawi, a close Al 
Qaida associate. The Bush Administration and the press has 
carelessly shorthanded this to mean, a senior Al Qaida mem-
ber, ignoring the real ambiguities that surround the true nature 
of that association, and whether Zarqawi shares Al Qaidas 
ends, or is receiving anything more than lodging inside Iraq. ] 
DocName: A-web_283330 ParaId: 7  
FIGURE 5: Partial answer generated by HITIQA to the al-
Qaida training facilities question 
 
During the dialogue, as new information is obtained 
from the user, the Goal frame is updated and the scores 
of all the data frames are reevaluated. The system may 
interpret the new information as a positive or negative. 
Positives are added to the Goal frame. Negatives are 
stored in a Negative-Goal frame and will also be used in 
the re-scoring of the data frames, possibly causing con-
flict scores to increase. If the user responds the equiva-
lent of ?yes? to the system clarification question in Fig-
ure 4, a corresponding WMD Transfer frame would be 
added to the Goal frame and all WMD Transfer frames 
will be re-scored.  If the user responds ?no?, the Nega-
tive-Goal frame will be generated and all WMD Trans-
fer frames will be rescored to 99 in order to remove 
them from further processing.  The user may end the 
dialogue, at any point and have an answer generated 
given the current state of the frames.   
Currently, the answer is simply composed of text 
passages from the zero conflict frames. In addition, 
HITIQA will generate a ?headline? for the text passages 
in all the Frames in the answer space.  This is done us-
ing grammar rules and the attributes of a frame.  Figure 
 5 shows a portion of the answer generated by HITIQA 
for the al-Qaida training facilities question. 
 
6   HITIQA Interface 
There are two distinct ways for the user to interact 
with HITIQA to explore their answer space.  The An-
swer Panel displays the user?s current answer at any 
given time during the interaction for a single question.  
Through this panel the user can read the paragraphs that 
are currently in their answer.  There are links on this 
panel so the user is able to view the full original source 
document from which the passage(s) were extracted. 
 The Visual panel offers the user an alternative to 
reading text by providing a tool for visually browsing 
the entire answer space.  Figure 6 shows a typical view 
of the visualization panel. The spheres are representa-
tive of single frames and groups of frames.  The user?s 
attention may be drawn to particular frames by the color 
coding or the attribute spikes.  The colors represent the 
frame?s score, so the user can quickly see what is in 
their answer, blue, and what is not, all other colors.  The 
attribute spikes may also be used as a navigation tool.  
The active attribute is chosen by the user through radio 
buttons. The current active attribute in Figure 6, is Lo-
cation.  This displays all instances of locations men-
tioned in the corresponding text. 
 
 
        Figure 6: Frame Level Display 
 
The underlying text that was used to build the frame 
may be displayed in the lower right hand window.  In 
this text display window there is a hyperlink that takes 
the user directly to the full source document. The user is 
able to interact with this panel by adding and removing 
information from their generated answer. Moving from 
the visualization to the textual dialogue, the generated 
answer, and back is seamless in a sense that any 
changes to the frame scores in one modality are imme-
diately accessible to the user in another modality. Users 
can add and remove frames from the answer space and 
HITIQA will always seamlessly pickup a new dialogue 
or generate a new answer.  
 
7   HITIQA Qualitative Evaluations 
In order to assess our progress thus far, and to also 
develop metrics to guide future evaluation, we invited a 
group of analysts employed by the US government to 
participate in two three-day workshops held in Septem-
ber and October 2003.  
The two basic objectives of the workshops were: 
1. To perform a realistic assessment of the useful-
ness and usability of HITIQA as an end-to-end system, 
from the information seeker's initial questions to com-
pletion of a draft report.  
2. To develop metrics to compare the answers ob-
tained by different analysts and evaluate the quality of 
the support that HITIQA provides.     
Each of these objectives entails a particular chal-
lenge. Performing a realistic assessment of HITIQA is 
difficult because many of the resources that the analysts 
use, as well as the reports they produce, are classified 
and therefore inaccessible to researchers.  
Assessing the quality of the support that the system 
provides is not easy because analytical questions rarely 
have a single right answer. It is not obvious how to de-
fine, for example, the precision of the system. We there-
fore conducted an 'information unit' exercise, whose 
purpose was to determine whether the analysts could 
identify information building blocks in their reports, so 
that we could compare and contrast different reports.  
To obtain an adequate supply of appropriate text 
data to support extensive question answering sessions 
(1, 2, 3 and 4 hours long), we prepared a new corpus of 
approximately 1.2 Gbytes. This new corpus consists of 
the reports from the Center for Non-Proliferation Stud-
ies (CNS) collected for the AQUAINT Program, aug-
mented with a much larger collection of texts on similar 
subject matter mined from the web using Google1. The 
final corpus proved to be sufficient to support about 
three hours of use of HITIQA to ?solve? each of the 
scenarios. 
The first day of the first workshop was devoted to 
training, including a two-part proficiency test. HITIQA 
is a fairly complex system, that includes multiple layers 
of data processing and user interaction, and it was criti-
cal that the users are sufficiently ?fluent? if we were to 
measure their productivity. The analysts' primary task 
on the second day was preparation of reports in re-
sponse to the scenarios. 
                                                 
1 Google has kindly agreed to temporarily extend our 
usage license so we could collect the data over a short 
time. 
  The third day was devoted to quantitative and quali-
tative evaluation, discussed later. In addition, we asked 
the analysts to score each others reports, as well as to 
identify key information units in them. These informa-
tion units could be later compared across different re-
ports in order to determine their completeness.  
8   Workshop Results 
The results of the quantitative evaluations strongly vali-
date the approach that we have taken. These conclusions 
are confirmed by analysts comments gleaned both from 
the formal qualitative assessment and from informal 
discussion. As one analyst said, ?the system as it stands 
now, in my mind, gave me enough information to try to 
put together a 80% solution but ?I don't think you're 
ever gonna reach that 100% state.? At the same time, we 
learned a great deal about how analysts work. 
It is important to determine the realism of the sce-
narios used during the workshop relative to the analysts? 
current work tasks in order for any results to be mean-
ingful. Each analyst was asked a series of five questions 
such as, ?How realistic was the scenario?  In other 
words did it resemble tasks you could imagine perform-
ing at work?? These 5 questions were all relative to the 
realism and difficulty of the scenario tasks.  Analysts 
used a scale of 1 to 5 based on their agreement with the 
statements, where 5 was complete agreement.  Our 
mean score was 3.84, indicating our scenarios were real-
istic and of about average difficulty when compared to 
the work they normally perform.   
We have classified the type of passages that an ana-
lyst copied to their report into two categories, answer 
passages and additional information passages, see Fig-
ure 7 below.  The answer passages either exactly an-
swered the user?s initial question or supplied supporting 
information.  The additional passages do not answer the 
original question posed, but may have been added to the 
answer through dialogue, or through the user?s explora-
tion of document links offered.  This could be a piece of 
information needed to satisfy some other aspect of the 
scenario that they had not asked about yet, or possibly a 
topic the user had not even considered but found rele-
vant when it was presented to them. As can be seen 
there was a very large amount of ?additional? informa-
tion that the user copied to their report.  The amounts 
reported here are the averages for all of the analysts for 
both workshops.  This supports our hypothesis that ana-
lysts seldom seek just the exact answer, but they are 
also looking at adjacent, related information, much of 
which they retain for their report.  Note that there were a 
small number of passages that contained a combination 
of answer and additional information; these were added 
to answer.   
 
Average Number of Passages Copied to Report
2.83
13.63
1.54
5.06
0.00
2.00
4.00
6.00
8.00
10.00
12.00
14.00
16.00
answ er additional
Passage Type
N
um
be
r o
f P
as
sa
ge
s
copied f rom link
copied f rom answ er
 Figure 7: Average Number of Passages Copied 
 
Total Passages Copied and Viewed: Analyst 2
37
8 16
28 27
230
50
242
352
152
16 11 4 7 4
49
27 26 34
44
0
50
100
150
200
250
300
350
400
1 2 3 4 5
Scenario
N
um
be
r o
f P
as
sa
ge
s
passages copied from links
passages viewed from links
passages copied from answer
passages viewed on answer
 
 
Figure 8: Number of Passages Copied Vs. Those Viewed 
 
We should now establish the number of passages 
copied versus those viewed, relative to links and the 
answer.  Figure 8 above shows the total number of pas-
sages copied versus the total number of passages 
viewed.  It is seen that many more passages need to be 
viewed through full document links before a useful pas-
sage is found.  In comparison a much smaller number of 
answer passages need to be viewed from the Answer 
panel in order to find useful passages.   
All of the analysts? sessions were recorded using 
Camtasia.  Figure 9 shows an annotation created for a 
typical session.  Analysts were observed to utilize a 
range of varying strategies as they worked different 
scenarios and even while working different queries of 
the same scenario.  Figure 10 shows the statistics for 
each Analyst?s use of HITIQA while working on the 
scenarios during the two workshops (note that Analyst-4 
was only able to attend the first workshop and Analyst-1 
did not create a report for Scenario 2).  Some of the 
variations in strategies among the analysts while work-
ing the same scenario are quite striking.  For example, 
Scenario 4 was  worked quite  differently  by  Analyst-1  
 versus Analyst-2.  While Analyst-1 spent almost all of 
his/her  time in  the Visual Panel, Analyst-2 spent virtu-
ally all of his/her time in the Answer panel.  Analyst-1 
produced his/her report copying 52 paragraphs while 
Analyst 2 copied only 35.  There are also large varia-
tions in the number of questions asked for the same sce-
nario.  Examine scenario 5, where Analyst-3 asked a 
total of 11 questions and Analyst-2 only needed to ask 2 
questions.  Relative to this, Analyst-3, who asked a 
much larger number of questions, copied only 28 pas-
sages, whereas Analyst-2 copied 31.  These variations, 
as stated earlier in the paper, could be due to the nature 
of the task, the progress the analyst is making on the 
task, in addition to individual differences between ana-
lysts. For example, the difference in the number of 
questions asked between Analyst-2 and Analyst-3 for 
scenario 5 may be due to difference in search strategies 
employed, but may also reflect the amount of back-
ground knowledge of the topic.   
 
      
        FIGURE 9: Fragment of an analytical session 
 
Variation of Strategies: Analyst 1
8
0
5 5 5
18
0
20
52
4248
0
41.5
86
60.5
12
0 1
6.67
26
1
10
100
1 2 3 4 5
Scenario
Variation of Strategies: Analyst 2
5
4
3 3
2
53
19 20
35 3135 29
47
5
17
61
20
33
100
72
1
10
100
1 2 3 4 5
Scenario
 
101 115
Variation of Strategies: Analyst 3
12
3
4
6
11
58
17
25 24
28
21
7
19
70
54
65
1
26
1
10
100
1 2 3 4 5
Scenario
 
Variation of Strategies: Analyst 4
2
3
0 0 0
35
0 0 0
37
0 0 0 0
36 40
0 0 0
34
1
10
100
1 2 3 4 5
Scenario
# questions asked
# passages copied
time in visual
time in answer
 
Figure 10: Varying Strategies Employed 
User: What is the status of South Africa's chemical, 
biological, and nuclear programs?  
          Clarification Dialogue: 1 minute 
? 6 questions generated by HITIQA 
? replied ?Yes? to 5 and ?No? to 1 
? 5+ passages added to answer 
           Studying Answer Panel: 60 minutes  
? Copying 24 passages to report 
? 10 from Answer 
? 14 from Links to Full Document 
? Visual Panel Browsing: 5 minutes 
? Nothing copied 
User: Has South Africa provided CBW material or 
assistance to any other countries?  
          Clarification Dialogue: 1 minute 
? 5 questions generated by HITIQA 
? replied ?Yes? to 2 and ?No? to 3 
? 2+ passages added to answer 
           Studying Answer Panel: 26 minutes 
? Copying 6 passages to report 
? 6 from Links to Full Document 
            Visual Panel browsing: 1 minute 
? Copying 1 passage to report 
? 1 from Links to Full Document 
User: How was South Africa's CBW program fi-
nanced?  
         Clarification Dialogue: 40 seconds 
? 7 questions generated by HITIQA 
? replied ?Yes? to 3 and ?No? to 4 
? 3+ passages added to answer 
            Studying Answer Panel: 11 minutes 
? Copying 3 passages to report 
? 1 from Answer 
      2 from Links to full Document 
  
There is, however, some consistency across the ana-
lysts in the amount of information retained per scenario. 
The charts are drawn in logarithmic scale, but it should 
be visible that scenarios 2 and 3 produced less interac-
tion and required less information to fulfill than scenar-
ios 4 and 5. It is also visible that scenario 1 required 
more questions to be asked and more exploration to be 
done in visual panel than other scenarios. 
Finally, it is important to provide some metric re-
garding the user?s overall satisfaction with their use of 
HITIQA.  At the end of each workshop Analysts were 
given a series of 17 questions, such as ?HITIQA helps 
me find important information?, shown in Figure 11, to 
assess their overall experience with the system.  Many 
of these questions were designed for the user to com-
pare HITIQA to the current tools they are using for this 
type of task.   Analysts again used a scale of 1 to 5 
based on their agreement with the statements.  The re-
sults were then converted, where 5 would always denote 
the best, and are shown in Figure 11 below.  It is impor-
tant to note that we scored highly overall, but addition-
ally we scored highly in the majority of questions rela-
tive to comparison of their current tools.  For example, 
for Question 14: ?Having HITIQA at work would help 
me find information faster than I can currently find it?, 
our mean score was 3.83.  
 
3.7215702092Total
3.00311117
4.141616
2.8614215
3.8314114
3.1632113
4.1424112
4.0024111
4.00710
3.71529
3.29258
3.143317
3.711516
4.43345
4.293314
4.14163
3.7114112
3.71611
ScoreScore5Score4Score3Score2Score1Question
MeanFrequency of Analyst's Scores of Overall Workshop I & II
1                  2                 3                 4    5 score
frequency
 
      FIGURE 11: Final Evaluation Results, Workshop 1 & 2 
 
In summary, the results from these two evaluations 
indicate that HITIQA, in its current state, is already 
competitive with the tools that the analysts are currently 
using in their work, supporting our overall approach to 
Analytical Question Answering.  HTIQA provides the 
user with a tool to find the passages needed to complete 
a report for a given scenario.  While working on a sce-
nario HITIQA has been shown to provide information 
which exactly answers the user?s question, and addi-
tionally HITIQA?s method brings to light other related 
information that the analyst retains in order to complete 
their report. 
 
Acknowledgements 
This paper is based on work supported by the Advanced 
Research and Development Activity (ARDA)?s Advanced 
Question Answering for Intelligence (AQUAINT) Program 
under contract number 2002-H790400-000. 
References  
Allen, J. and M. Core. 1997. Draft of DAMSL:  Dialog Act Markup in 
Several Layers. www.cs.rochester.edu/research/cisd/   
Baeza-Yates and Ribeiro-Neto. 1999. Modern Information Retrieval. 
Addison Wesley. 
Chris Buckley. 1985. Implementation of the Smart information re-
trieval system. Technical Report TR85-686, Department of Com-
puter Science, Cornell University, Ithaca, NY. 
Ferguson, George and James Allen. 1998. TRIPS: An Intelligent Inte-
grated Problem-Solving Assistant, in Proceedings of the 15th 
AAAI Conference (AAAI-98), Madison, WI, pp. 567-573. 
Hardy, H., N. Shimizu, T. Strzalkowski, L. Ting, B. Wise and X. 
Zhang. 2002a. Cross-Document Summarization by Concept Clas-
sification. Proceedings of SIGIR, Tampere, Finland. 
Hardy, H., K. Baker, L. Devillers, L. Lamel, S. Rosset, T. 
Strzalkowski, C. Ursu and N. Webb. 2002b.  Multi-layer Dialogue 
Annotation for Automated Multilingual Customer Service. ISLE 
Workshop, Edinburgh, Scotland. 
Harabagiu, S., et. al. 2002. Answering Complex, List and Context 
questions with LCC?s Question Answering Server.   In Proceedings 
of Text Retrieval Conference (TREC-10). 
Hovy, E., L. Gerber, U. Hermjakob, M. Junk, C-Y. Lin. 2000. Ques-
tion Answering in Webclopedia. Notebook. Proceedings of Text 
Retrieval Conference (TREC-9). 
Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, H. 
Cunningham, Y. Wilks. 1998. Description of the LaSIE-II System 
as Used for MUC-7. In Proceedings of the Seventh Message Un-
derstanding Conference (MUC-7.) 
Litman, Diane J. and Shimei Pan. 2002. Designing and Evaluating an 
Adaptive Spoken Dialogue System. User Modeling and User-
Adapted Interaction. Vol. 12, No. 2/3, pp. 111-137. 
Seneff, S. and J. Polifroni. 2000. Dialogue Management in the MER-
CURY Flight Reservation System. Proc. ANLP-NAACL 2000, 
Satellite Workshop, pp. 1-6, Seattle, WA. 
Small, Sharon, Nobuyuki Shimizu, Tomek Strzalkowski and Liu Ting 
(2003). HITIQA: A Data Driven Approach to Interactive Question 
Answering: A Preliminary Report. AAAI Spring Symposium on 
New Directions in Question Answering, Stanford University, 
March 24-26, 2003. pp. 94?104. 
Tang, Rong, K.B. Ng, Tomek Strzalkowski and Paul Kantor (2003). 
Automatic Prediction of Information Quality in News Documents. 
Proceedings of HLT-NAACL 2003, Edmonton, May 27-June 1 
Walker, Marilyn A. 2002. An Application of Reinforcement Learning 
to Dialogue Strategy Selection in a Spoken Dialogue System for 
Email . Journal of AI Research, vol 12., pp. 387-416. 
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 581?586,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identifying Sarcasm in Twitter: A Closer Look 
 
 
Roberto Gonz?lez-Ib??ez Smaranda Muresan Nina Wacholder 
 
School of Communication & Information 
Rutgers, The State University of New Jersey 
4 Huntington St, New Brunswick, NJ 08901 
{rgonzal, smuresan, ninwac}@rutgers.edu 
 
 
 
 
 
 
 
Abstract 
Sarcasm transforms the polarity of an ap-
parently positive or negative utterance into 
its opposite. We report on a method for 
constructing a corpus of sarcastic Twitter 
messages in which determination of the 
sarcasm of each message has been made by 
its author. We use this reliable corpus to 
compare sarcastic utterances in Twitter to 
utterances that express positive or negative 
attitudes without sarcasm. We investigate 
the impact of lexical and pragmatic factors 
on machine learning effectiveness for iden-
tifying sarcastic utterances and we compare 
the performance of machine learning tech-
niques and human judges on this task. Per-
haps unsurprisingly, neither the human 
judges nor the machine learning techniques 
perform very well. 
1 Introduction 
Automatic detection of sarcasm is still in its infan-
cy. One reason for the lack of computational mod-
els has been the absence of accurately-labeled 
naturally occurring utterances that can be used to 
train machine learning systems. Microblogging 
platforms such as Twitter, which allow users to 
communicate feelings, opinions and ideas in short 
messages and to assign labels to their own messag-
es, have been recently exploited in sentiment and 
opinion analysis (Pak and Paroubek, 2010; Davi-
dov et al, 2010). In Twitter, messages can be an-
notated with hashtags such as #bicycling, #happy 
and #sarcasm. We use these hashtags to build a 
labeled corpus of naturally occurring sarcastic, 
positive and negative tweets.  
    In this paper, we report on an empirical study on 
the use of lexical and pragmatic factors to distin-
guish sarcasm from positive and negative senti-
ments expressed in Twitter messages. The 
contributions of this paper include i) creation of a 
corpus that includes only sarcastic utterances that 
have been explicitly identified as such by the com-
poser of the message; ii) a report on the difficulty 
of distinguishing sarcastic tweets from tweets that 
are straight-forwardly positive or negative. Our 
results suggest that lexical features alone are not 
sufficient for identifying sarcasm and that pragmat-
ic and contextual features merit further study. 
2 Related Work 
Sarcasm and irony are well-studied phenomena in  
linguistics, psychology and cognitive science 
(Gibbs, 1986; Gibbs and Colston 2007; Kreuz and 
Glucksberg, 1989; Utsumi, 2002). But in the text 
mining literature, automatic detection of sarcasm is 
considered a difficult problem (Nigam & Hurst, 
2006 and Pang & Lee, 2008 for an overview) and 
has been addressed in only a few studies. In the 
context of spoken dialogues, automatic detection 
of sarcasm has relied primarily on speech-related 
cues such as laughter and prosody (Tepperman et 
al., 2006). The work most closely related to ours is 
that of Davidov et al (2010), whose objective was 
to identify sarcastic and non-sarcastic utterances in 
Twitter and in Amazon product reviews. In this 
paper, we consider the somewhat harder problem 
581
of distinguishing sarcastic tweets from non-
sarcastic tweets that directly convey positive and 
negative attitudes (we do not consider neutral ut-
terances at all).  
 Our approach of looking at lexical features for 
identification of sarcasm was inspired by the work 
of Kreuz and Caucci (2007). In addition, we also 
look at pragmatic features, such as establishing 
common ground between speaker and hearer 
(Clark and Gerring, 1984), and emoticons. 
3 Data 
In Twitter, people (tweeters) post messages of up 
to 140 characters (tweets). Apart from plain text, a 
tweet can contain references to other users 
(@<user>), URLs, and hashtags (#hashtag) which 
are tags assigned by the user to identify topic 
(#teaparty, #worldcup) or sentiment (#angry, 
#happy, #sarcasm). An example of a tweet is:  
?@UserName1 check out the twitter feed on 
@UserName2 for a few ideas :) http://xxxxxx.com 
#happy #hour?.  
   To build our corpus of sarcastic (S), positive (P) 
and negative (N) tweets, we relied on the annota-
tions that tweeters assign to their own tweets using 
hashtags. Our assumption is that the best judge of 
whether a tweet is intended to be sarcastic is the 
author of the tweet. As shown in the following sec-
tions, human judges other than the tweets? authors, 
achieve low levels of accuracy when trying to clas-
sify sarcastic tweets; we therefore argue that using 
the tweets labeled by their authors using hashtag 
produces a better quality gold standard. We used a 
Twitter API to collect tweets that include hashtags 
that express sarcasm (#sarcasm, #sarcastic), direct 
positive sentiment (e.g., #happy, #joy, #lucky), and 
direct negative sentiment (e.g., #sadness, #angry, 
#frustrated), respectively. We applied automatic 
filtering to remove retweets, duplicates, quotes, 
spam, tweets written in languages other than Eng-
lish, and tweets with URLs.  
To address the concern of Davidov et al 
(2010) that tweets with #hashtags are noisy, we 
automatically filtered all tweets where the hashtags 
of interest were not located at the very end of the 
message. We then performed a manual review of 
the filtered tweets to double check that the remain-
ing end hashtags were not part of the message. We 
thus eliminated messages about sarcasm such as ?I 
really love #sarcasm? and kept only messages that 
express sarcasm, such as ?lol thanks. I can always 
count on you for comfort :) #sarcasm?.  
Our final corpus consists of 900 tweets in each 
of the three categories, sarcastic, positive and 
negative. Examples of tweets in our corpus that are 
labeled with the #sarcasm hashtag include the fol-
lowing: 
 
1) @UserName That must suck.   
2) I can't express how much I love shopping 
on black Friday.                 
3) @UserName that's what I love about Mi-
ami. Attention to detail in preserving his-
toric landmarks of the past. 
4) @UserName im just loving the positive 
vibes out of that! 
 
The sarcastic tweets are primarily negative (i.e., 
messages that sound positive but are intended to 
convey a negative attitude) as in Examples 2-4, but 
there are also some positive messages (messages 
that sound negative but are apparently intended to 
be understood as positive), as in Example 1. 
4 Lexical and Pragmatic Features 
In this section we address the question of whether 
it is possible to empirically identify lexical and 
pragmatic factors that distinguish sarcastic, posi-
tive and negative utterances. 
Lexical Factors. We used two kinds of lexical fea-
tures ? unigrams and dictionary-based. The dictio-
nary-based features were derived from i) 
Pennebaker et al?s LIWC (2007) dictionary, which 
consists of a set of 64 word categories grouped into 
four general classes: Linguistic Processes (LP) 
(e.g., adverbs, pronouns), Psychological Processes 
(PP) (e.g., positive and negative emotions), Per-
sonal Concerns (PC) (e.g, work, achievement), and 
Spoken Categories (SC) (e.g., assent, non-
fluencies); ii) WordNet Affect (WNA) (Strappara-
va and Valitutti, 2004); and iii) list of interjections 
(e.g., ah, oh, yeah)1, and punctuations (e.g., !, ?). 
The latter are inspired by results from Kreuz and 
Caucci (2007). We merged all of the lists into a 
single dictionary. The token overlap between the 
words in combined dictionary and the words in the 
tweets was 85%. This demonstrates that lexical 
coverage is good, even though tweets are well 
                                                 
1
 http://www.vidarholen.net/contents/interjections/ 
582
known to contain many words that do not appear in 
standard dictionaries.  
Pragmatic Factors. We used three pragmatic fea-
tures: i) positive emoticons such as smileys; ii) 
negative emoticons such as frowning faces; and iii) 
ToUser, which marks if a tweets is a reply to 
another tweet (signaled by <@user> ).  
Feature Ranking.  To measure the impact of fea-
tures on discriminating among the three categories, 
we used two standard measures: presence and fre-
quency of the factors in each tweet. We did a 3-
way comparison of Sarcastic (S), Positive (P), and 
Negative (N) messages (S-P-N); as well as 2-way 
comparisons of i) Sarcastic and Non-Sarcastic (S-
NS);  ii) Sarcastic and Positive (S-P) and Sarcastic 
and Negative (S-N). The NS tweets were obtained 
by merging 450 randomly selected positive and 
450 negative tweets from our corpus.  
We ran a ?2 test to identify the features that were 
most useful in discriminating categories. Table 1 
shows the top 10 features based on presence of all 
dictionary-based lexical factors plus the pragmatic 
factors. We refer to this set of features as LIWC+. 
S-P-N S-NS S-N S-P 
Negemo(PP) 
Posemo(PP) 
Smiley(Pr) 
Question 
Negate(LP) 
Anger(PP) 
Present(LP) 
Joy(WNA) 
Swear(PP) 
AuxVb(LP)  
Posemo(PP) 
Present(LP) 
Question 
ToUser(Pr) 
Affect(PP)  
Verbs(LP) 
AuxVb(LP) 
Quotation 
Social(PP) 
Ingest(PP)  
Posemo(PP) 
Negemo(PP) 
Joy(WNA) 
Affect(PP) 
Anger(PP) 
Sad(PP) 
Swear(PP) 
Smiley(Pr) 
Body(PP) 
Frown(Pr)  
Question    
Present(LP) 
ToUser(Pr) 
Smiley(Pr) 
AuxVb(LP) 
Ipron(LP)   
Negate(LP) 
Verbs(LP) 
Time(PP) 
Negemo(PP)  
 
Table 1: 10 most discriminating features in LIWC+ 
for each task 
In all of the tasks, negative emotion (Negemo), 
positive emotion (Posemo), negation (Negate), 
emoticons (Smiley, Frown), auxiliary verbs 
(AuxVb), and punctuation marks are in the top 10 
features. We also observe indications of a possible 
dependence among factors that could differentiate 
sarcasm from both positive and negative tweets: 
sarcastic tweets tend to have positive emotion 
words like positive tweets do (Posemo is a signifi-
cant feature in S-N but not in S-P), while they use 
more negation words like  negative tweets do (Ne-
gate is an important feature for S-P). Table 1 also 
shows that the pragmatic factor ToUser is impor-
tant in sarcasm detection. This is an indication of 
the possible importance of features that indicate 
common ground in sarcasm identification.  
5 Classification Experiments 
In this section we investigate the usefulness of lex-
ical and pragmatic features in machine learning to 
classify sarcastic, positive and negative Tweets. 
    We used two standard classifiers often employed 
in sentiment classification: support vector machine 
with sequential minimal optimization (SMO) and 
logistic regression (LogR). For features we used: 
1) unigrams; 2) presence of dictionary-based lexi-
cal and pragmatic factors (LIWC+_P); and 3) fre-
quency of dictionary-based lexical and pragmatic 
factors (LIWC+_F). We also trained our models 
with bigrams and trigrams; however, results using 
these features did not report better results than uni-
grams and LICW+. The classifiers were trained on 
balanced datasets (900 instances per class) and 
tested through five-fold cross-validation. 
In Table 2, shaded cells indicate the best accura-
cies for each class, while bolded values indicate 
the best accuracies per row. In the three-way clas-
sification (S-P-N), SMO with unigrams as features 
outperformed SMO with LIWC+_P and LIWC+_F 
as features. Overall SMO outperformed LogR. The 
best accuracy of 57% is an indication of the diffi-
culty of the task.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We also performed several two-way classifica-
tion experiments. For the S-NS classification the 
best results were again obtained using SMO with 
Class Features SMO LogR 
S-
P-
N
 
Unigrams 57.22 49.00 
LIWC+_F 55.59 55.56 
LIWC+_P 55.67 55.59 
S-
N
S 
Unigrams 65.44 60.72 
LIWC+_F 61.22 59.83 
LIWC+_P 62.78 63.17 
S-
P 
Unigrams 70.94 64.83 
LIWC+_F 66.39 67.44 
LIWC+_P 67.22 67.83 
S-
N
 
Unigrams 69.17 64.61 
LIWC+_F 68.56 67.83 
LIWC+_P 68.33 68.67 
P-
N
 
Unigrams 74.67 72.39 
LIWC+_F 74.94 75.89 
LIWC+_P 75.78 75.78 
 
Table 2: Classifiers accuracies using 5-fold cross-
validation, in percent. 
583
unigrams as features (65.44%). For S-P and S-N 
the best accuracies were close to 70%. Overall, our 
best result (75.89%) was achieved in the polarity-
based classification P-N. It is intriguing that the 
machine learning systems have roughly equal dif-
ficulty in separating sarcastic tweets from positive 
tweets and from negative tweets.  
These results indicate that the lexical and prag-
matic features considered in this paper do not pro-
vide sufficient information to accurately 
differentiate sarcastic from positive and negative 
tweets. This may be due to the inherent difficulty 
of distinguishing short utterances in isolation, 
without use of contextual evidence.  
In the next section we explore the inherent diffi-
culty of identifying sarcastic utterances by compar-
ing human performance and classifier 
performance.  
6 Comparison against Human Perfor-
mance 
To get a better sense of how difficult the task of 
sarcasm identification really is, we conducted three 
studies with human judges (not the authors of this 
paper). In the first study, we asked three judges to 
classify 10% of our S-P-N dataset (90 randomly 
selected tweets per category) into sarcastic, posi-
tive and negative. In addition, they were able to 
indicate if they were unsure to which category 
tweets belonged and to add comments about the 
difficulty of the task. 
In this study, overall agreement of 50% was 
achieved among the three judges, with a Fleiss? 
Kappa value of 0.4788 (p<.05). The mean accuracy 
was 62.59% (7.7) with 13.58% (13.44) uncertainty. 
When we considered only the 135 of 270 tweets on 
which all three judges agreed, the accuracy, com-
puted over to the entire gold standard test set, fell 
to 43.33%2. We used the accuracy when the judges 
                                                 
2
 The accuracy on the set they agreed on (135  out of 270 
tweets) was 86.67%. 
agree (43.33%) and the average accuracy (62.59%) 
as a human baseline interval (HBI).  
We trained our SMO and LogR classifiers on 
the other 90% of the S-P-N. The models were then 
evaluated on 10% of the S-P-N dataset that was 
also labeled by humans. Classification accuracy 
was similar to results obtained in the previous sec-
tion. Our best result -- an accuracy of 57.41%-- 
was achieved using SMO and LIWC+_P (Table 3: 
S-P-N). The highest value in the established HBI 
achieved a slightly higher accuracy; however, 
when compared to the bottom value of the same 
interval, our best result significantly outperformed 
it.  It is intriguing that the difficulty of distinguish-
ing sarcastic utterances from positive ones and 
from negative ones was quite similar.  
In the second study, we investigated how well 
human judges performed on the two-way classifi-
cation task of labeling sarcastic and non-sarcastic 
tweets. We asked three other judges to classify 
10% of our S-NS dataset (i.e, 180 tweets) into sar-
castic and non-sarcastic. Results showed an 
agreement of 71.67% among the three judges with 
a Fleiss? Kappa value of 0.5861 (p<.05). The aver-
age accuracy rate was 66.85% (3.9) with 0.37% 
uncertainty (0.64). When we considered only cases 
where all three judges agreed, the accuracy, again 
computed over the entire gold standard test set, fell 
to 59.44% 3 . As shown in Table 3 (S-NS: 10% 
tweets), the HBI was outperformed by the automat-
ic classification using unigrams (68.33%) and 
LIWC+_P (67.78%) as features.  
Based on recent results which show that non-
linguistic cues such as emoticons are helpful in 
interpreting non-literal meaning such as sarcasm 
and irony in user generated content (Derks et al, 
2008; Carvalho et al, 2009), we explored how 
much emoticons help humans to distinguish sarcas-
tic from positive and negative tweets. For this test, 
we created a new dataset using only tweets with 
emoticons. This dataset consisted of 50 sarcastic 
                                                 
3
 The accuracy  on the set they agreed on (129 out of 180 
tweets) was 82.95%. 
 
 
Ta sk S ? N ? P    (10% data set) S ? NS (10% dataset) S ? NS (100 tweets + emoticons) 
HBI [43.33%-62.59%] [59.44% - 66.85%] [70% - 73%] 
Test Features SMO LogR SMO LogR SMO Log R 
1 Unigrams 55.92 46.66 68.33 57.78 71.00 66.00 
2 LIWC+_F 54.07 54.81 62.78 61.11 60.00 58.00 
3 LIWC+_P 57.41 57.04 67.78 67.22 51.00 53.00 
 
Table 3: Classifiers accuracies against humans? accuracies in three classification tasks. 
584
tweets and 50 non-sarcastic tweets (25 P and 25 
N). Two human judges classified the tweets using 
the same procedure as above. For this task judges 
achieved an overall agreement of 89% with Co-
hen?s Kappa value of 0.74 (p<.001). The results 
show that emoticons play an important role in 
helping people distinguish sarcastic from non-
sarcastic tweets. The overall accuracy for both 
judges was 73% (1.41) with uncertainty of 10% 
(1.4). When all judges agreed, the accuracy was 
70% when computed relative the entire gold stan-
dard set4  
Using our trained model for S-NS from the pre-
vious section, we also tested our classifiers on this 
new dataset. Table 3 (S-NS: 100 tweets) shows 
that our best result (71%) was achieved by SMO 
using unigrams as features. This value is located 
between the extreme values of the established HBI. 
These three studies show that humans do not 
perform significantly better than the simple auto-
matic classification methods discussed in this pa-
per. Some judges reported that the classification 
task was hard. The main issues judges identified 
were the lack of context and the brevity of the 
messages. As one judge explained, sometimes it 
was necessary to call on world knowledge such as 
recent events in order to make judgments about 
sarcasm. This suggests that accurate automatic 
identification of sarcasm on Twitter requires in-
formation about interaction between the tweeters 
such as common ground and world knowledge.  
7 Conclusion  
In this paper we have taken a closer look at the 
problem of automatically detecting sarcasm in 
Twitter messages. We used a corpus annotated by 
the tweeters themselves as our gold standard; we 
relied on the judgments of tweeters because of the 
relatively poor performance of human coders at 
this task.  We semi-automatically cleaned the cor-
pus to address concerns about corpus noisiness 
raised in previous work. We explored the contribu-
tion of linguistic and pragmatic features of tweets 
to the automatic separation of sarcastic messages 
from positive and negative ones; we found that the 
three pragmatic features ? ToUser, smiley and 
frown ? were among the ten most discriminating 
features in the classification tasks (Table 1).  
                                                 
4
 The accuracy on the set they agreed on (83 out of 100 
tweets) was 83.13%. 
We also compared the performance of automatic 
and human classification in three different studies. 
We found that automatic classification can be as 
good as human classification; however, the accura-
cy is still low. Our results demonstrate the difficul-
ty of sarcasm classification for both humans and 
machine learning methods. 
The length of tweets as well as the lack of expli-
cit context makes this classification task quite dif-
ficult. In future work, we plan to investigate the 
impact of contextual features such as common 
ground. 
Finally, the low performance of human coders in 
the classification task of sarcastic tweets suggests 
that gold standards built by using labels given by 
human coders other than tweets? authors may not 
be reliable. In this sense we believe that our ap-
proach to create the gold standard of sarcastic 
tweets is more suitable in the context of Twitter 
messages. 
Acknowledgments  
We thank all those who participated as coders in 
our human classification task. We also thank the 
anonymous reviewers for their insightful com-
ments. 
References  
Carvalho, P., Sarmento, S., Silva, M. J., and de Oliveira, 
E. 2009. Clues for detecting irony in user-generated 
contents: oh...!! it's "so easy" ;-). In Proceeding of 
the 1st international CIKM workshop on Topic-
sentiment analysis for mass opinion (TSA '09). 
ACM, New York, NY, USA, 53-56. 
Clark, H. and Gerrig, R. 1984. On the pretence theory of 
irony. Journal of Experimental Psychology: Gener-
al, 113:121?126. D.C.  
Davidov, D., Tsur, O., and Rappoport, A. 2010. Semi-
Supervised Recognition of Sarcastic Sentences in 
Twitter and Amazon, Dmitry Proceeding of Compu-
tational Natural Language Learning (ACL-CoNLL). 
Derks, D., Bos, A. E. R., and Grumbkow, J. V. 2008. 
Emoticons and Online Message Interpretation. Soc. 
Sci. Comput. Rev., 26(3), 379-388. 
Gibbs, R. 1986. On the psycholinguistics of sarcasm. 
Journal of Experimental Psychology: General, 
105:3?15. 
Gibbs, R. W. and Colston H. L. eds. 2007. Irony in 
Language and Thought. Routledge (Taylor and 
Francis), New York. 
585
Kreuz, R. J. and Glucksberg, S. 1989. How to be sarcas-
tic: The echoic reminder theory of verbal irony. 
Journal of Experimental Psychology: General, 
118:374-386. 
Kreuz, R. J. and Caucci, G. M. 2007. Lexical influences 
on the perception of sarcasm. In Proceedings of the 
Workshop on Computational Approaches to Figura-
tive Language (pp. 1-4). Rochester, New York: As-
sociation for Computational. 
LIWC Inc. 2007. The LIWC application. Retrieved May 
10, 2010, from 
http://www.liwc.net/liwcdescription.php. 
Nigam, K. and Hurst, M. 2006. Towards a Robust Me-
tric of Polarity. In Computing Attitude and Affect in 
Text: Theory and Applications (pp. 265-279). Re-
trieved February 22, 2010, from 
http://dx.doi.org/10.1007/1-4020-4102-0_20.   
Pak, A. and Paroubek, P. 2010. Twitter as a Corpus for 
Sentiment Analysis and Opinion Mining, in 
'Proceedings of the Seventh conference on Interna-
tional Language Resources and Evaluation 
(LREC'10)' , European Language Resources Associ-
ation (ELRA), Valletta, Malta 
Pang, B. and Lee, L. 2008. Opinion Mining and Senti-
ment Analysis. Now Publishers Inc, July. 
Pennebaker, J.W., Francis, M.E., & Booth, R.J. (2001). 
Linguistic Inquiry and Word Count (LIWC): 
LIWC2001 (this includes the manual only). Mah-
wah, NJ: Erlbaum Publishers 
Strapparava, C. and Valitutti, A. 2004. Wordnet-affect: 
an affective extension of wordnet. In Proceedings of 
the 4th International Conference on Language Re-
sources and Evaluation, Lisbon. 
Tepperman, J., Traum, D., and Narayanan, S. 2006. 
Yeah right: Sarcasm recognition for spoken dialogue 
systems. In InterSpeech ICSLP, Pittsburgh, PA. 
Utsumi, A. 2000. Verbal irony as implicit display of 
ironic environment: Distinguishing ironic utterances 
from nonirony. Journal of Pragmatics, 32(12):1777?
1806. 
 
586
Proceedings of the First Workshop on Argumentation Mining, pages 39?48,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Analyzing Argumentative Discourse Units in Online Interactions
Debanjan Ghosh* Smaranda Muresan? Nina Wacholder* Mark Aakhus* Matthew Mitsui**
*School of Communication and Information, Rutgers University
?Center of Computational Learning Systems, Columbia University
**Department of Computer Science, Rutgers University
debanjan.ghosh|ninwac|aakhus|mmitsui@rutgers.edu, smara@ccls.columbia.edu
Abstract
Argument mining of online interactions is
in its infancy. One reason is the lack of
annotated corpora in this genre. To make
progress, we need to develop a principled
and scalable way of determining which
portions of texts are argumentative and
what is the nature of argumentation. We
propose a two-tiered approach to achieve
this goal and report on several initial stud-
ies to assess its potential.
1 Introduction
An increasing portion of information and opin-
ion exchange occurs in online interactions such
as discussion forums, blogs, and webpage com-
ments. This type of user-generated conversation-
al data provides a wealth of naturally occurring
arguments. Argument mining of online interac-
tions, however, is still in its infancy (Abbott et al.,
2011; Biran and Rambow, 2011; Yin et al., 2012;
Andreas et al., 2012; Misra and Walker, 2013).
One reason is the lack of annotated corpora in this
genre. To make progress, we need to develop a
principled and scalable way of determining which
portions of texts are argumentative and what is the
nature of argumentation.
We propose a multi-step coding approach
grounded in findings from argumentation re-
search on managing the difficulties of coding ar-
guments (Meyers and Brashers, 2010). In the first
step, trained expert annotators identify basic ar-
gumentative features (coarse-grained analysis) in
full-length threads. In the second step, we explore
the feasibility of using crowdsourcing and novice
annotators to identify finer details and nuances of
the basic argumentative units focusing on limited
thread context. Our coarse-grained scheme for ar-
gumentation is based on Pragmatic Argumentation
Theory (PAT) (Van Eemeren et al., 1993; Hutchby,
Figure 1: Argumentative annotation of an Online
Thread
2013; Maynard, 1985). PAT states that an argu-
ment can arise at any point when two or more
actors engage in calling out and making prob-
lematic some aspect of another actor?s prior con-
tribution for what it (could have) said or meant
(Van Eemeren et al., 1993). The argumentative
relationships among contributions to a discussion
are indicated through links between what is tar-
geted and how it is called-out. Figure 1 shows
an example of two Callouts that refer back to the
same Target.
The annotation task performed by the trained
annotators includes three subtasks that Peldszus
and Stede (2013a) identify as part of the argu-
ment mining problem: 1) Segmentation, 2) Seg-
ment classification, and 3) Relationship identifi-
cation. In the language of Peldszus and Stede
(2013a), Callouts and Targets are the basic Argu-
ment Discourse Units (ADUs) that are segmented,
classified, and linked. There are two key advan-
tages of our coarse-grained annotation scheme:
1) It does not initially prescribe what constitutes
an argumentative text; 2) It makes it possible for
Expert Annotators (EAs) to find ADUs in long
39
threads. Assigning finer grained (more complex)
labels would have unduly increased the already
heavy cognitive load for the EAs. In Section
2 we present the corpus, describe the annotation
scheme and task, calculate Inter Annotator Agree-
ment (IAA), and propose a hierarchical clustering
approach to identify text segments that the EAs
found easier or harder to annotate.
In Section 3, we report on two Amazon
Mechanical Turk (MTurk) experiments, which
demonstrate that crowdsourcing is a feasible way
to obtain finer grained annotations of basic ADUs,
especially on the text segments that were easier
for the EAs to code. In the first crowd sourc-
ing study, the Turkers (the workers at MTurk,
who we consider novice annotators) assigned la-
bels (Agree/Disagree/Other) to the relations be-
tween Callout and Target identified by the EAs.
In the second study, Turkers labeled segments of
Callouts as Stance or Rationale. Turkers saw only
a limited context of the threaded discussion, i.e.
a particular Callout-Target pair identified by the
EA(s) who had analyzed the entire thread. In addi-
tion we report on initial classification experiments
to detect agreement/disagreement, with the best
F1 of 66.9% for the Agree class and 62.6% for the
Disagree class.
2 Expert Annotation for Coarse-Grained
Argumentation
Within Pragmatic Argumentation Theory, argu-
mentation refers to the ways in which people (seek
to) make some prior action or antecedent event
disputable by performing challenges, contradic-
tions, negations, accusations, resistance, and other
behaviors that call out a ?Target?, a prior action
or event. In this section, we present the corpus,
the annotation scheme based on PAT and the an-
notation task, the inter-annotator agreement, and a
method to identify which pieces of text are easier
or harder to annotate using a hierarchical cluster-
ing approach.
2.1 Corpus
Our corpus consists of blog comments posted as
responses to four blog postings selected from a
dataset crawled from Technorati between 2008-
2010
1
. We selected blog postings in the general
topic of technology and considered only postings
1
http://technorati.com/blogs/directory/
that had more than 200 comments. For the an-
notation we selected the first one hundred com-
ments on each blog together with the original post-
ing. Each blog together with its comments con-
stitutes a thread. The topics of each thread are:
Android (comparison of features of iPhone and
Android phones), iPad (the usefulness of iPads),
Twitter (the usefulness of Twitter as a microblog-
ging platform), and Layoffs (downsizing and out-
sourcing efforts of technology companies). We re-
fer to these threads as the argumentative corpus.
We plan to make the corpus available to the re-
search community.
2.2 Annotation Scheme and Expert
Annotation Task
The coarse-grained annotation scheme for argu-
mentation is based on the concept of Callout and
Target of Pragmatic Argumentation Theory. The
experts? annotation task was to identify expres-
sions of Callout and their Targets while also indi-
cating the links between them. We prepared a set
of guidelines with careful definitions of all techni-
cal terms. The following is an abbreviated excerpt
from the guidelines:
? Callout: A Callout is a subsequent action
that selects (i.e., refers back to) all or some
part of a prior action (i.e., Target) and com-
ments on it in some way. In addition to re-
ferring back to the Target, a Callout explic-
itly includes either one or both of the fol-
lowing: Stance (indication of attitude or posi-
tion relative to the Target) and Rationale (ar-
gument/justification/explanation of the Stance
taken).
? Target: A Target is a part of a prior action that
has been called out by a subsequent action.
Fig. 1 shows two examples of Callouts from
two comments referring back to the same Target.
Annotators were instructed to mark any text seg-
ment (from words to entire comments) that sat-
isfied the definitions above. A single text seg-
ment could be a Target and a Callout. To per-form
the expert annotation, we hired five graduate stu-
dents who had a strong background in humanities
and who received extensive training for the task.
The EAs performed three annotation subtasks
mentioned by Peldszus and Stede (2013a): Seg-
mentation (identify the Argumentative Dis-course
40
Thread A1 A2 A3 A4 A5
Android 73 99 97 118 110
iPad 68 86 85 109 118
Layoffs 71 83 74 109 117
Twitter 76 102 70 113 119
Avg. 72 92.5 81.5 112.3 116
Table 1: Number of Callouts by threads and EA
Thread F1 EM F1 OM ?
Android 54.4 87.8 0.64
iPad 51.2 86.0 0.73
Layoffs 51.9 87.5 0.87
Twitter 53.8 88.5 0.82
Table 2: IAA for 5 EA: F1 and alpha values per
thread
Units (ADUs) including their boundaries), Seg-
ment classification (label the roles of the ADUs,
in this case Callout and Target) and relation iden-
tification (indicate the link between a Callout and
the most recent Target to which is a response).
The segmentation task, which Artstein and Poe-
sio (2008) refer to as the unitization problem, is
particularly challenging. Table 1 shows extensive
variation in the number of ADUs (Callout in this
case) identified by the EAs for each of the four
threads. Annotator A1 identified the fewest Call-
outs (72) while A4 and A5 identified the most
(112.3 and 116, respectively). Although these dif-
ferences could be due to the issues with training,
we interpret the consistent variation among coders
as an indication that judges can be characterized
as ?lumpers? or ?splitters?. What lumpers con-
sidered a single long unit was treated as two (or
more) shorter units by splitters. This is an example
of the problem of annotator variability discussed
in (Peldszus and Stede, 2013b). Similar behavior
was noticed for Targets.
2
2.3 Inter Annotator Agreement
Since the annotation task includes the segmen-
tation step, to measure the IAA we have to ac-
count for fuzzy boundaries. Thus, we con-sider
two IAA metrics usually used in literature for
such cases: the information retrieval (IR) in-spired
precision-recall (P/R/F1) measure (Wiebe et al.,
2005) and Krippendorff?s ? (Krippendorff, 2004).
We present here the main results; a detailed dis-
cussion of the IAA is left for a different paper. Fol-
lowing Wiebe et al. (2005), to calculate P/R/F1 for
two annotators, one annotator?s ADUs are selected
2
Due to space limitations, here and in the rest of this paper
we report only on Callouts.
as the gold standard. If more than two annotators
are employed, the IAA is the average of the pair-
wise P/R/F1. To determine if two annotators have
selected the same text span to represent an ADU,
we use the two methods of Somasundaran et al.
(2008): exact match (EM) - text spans that vary
at the start or end point by five characters or less,
and overlap match (OM) - text spans that have at
least 10% of same overlapping characters. Table 2
shows the F1 measure for EM and OM for the five
EAs on each of the four threads. As expected, the
F1 measures are much lower for EM than for OM.
For the second IAA metric, we implement
Krippendorff?s ? (Krippendorff, 2004), where the
character overlap between any two annotations
and the gap between them are utilized to mea-
sure the expected disagreement and the observed
disagreement. Table 2 shows ? values for each
thread, which means significant agreement.
While the above metrics show reasonable agree-
ment across annotators, they do not tell us what
pieces of text are easier or harder to annotate. In
the next section we report on a hierarchical cluster-
ing technique that makes it possible to assess how
difficult it is to identify individual text segments as
Callouts.
2.4 Clustering of Callout ADUs
We use a hierarchical clustering technique (Hastie
et al., 2009) to cluster ADUs that are variants of
the same Callout. Each ADU starts in its own clus-
ter. The start and end points of each ADU are uti-
lized to identify overlapping characters in pairs of
ADUs. Then, using a ?bottom up? clustering ap-
proach, two ADUs (in this case, pairs of Callouts)
that share overlapping characters are merged into
a cluster. This process continues until no more
text segments can be merged. Clusters with five
overlapping ADUs include a text segment that all
five annotators have labeled as a Callout, while
clusters with one ADU indicates that only one an-
notator classified the text segment as a Callout
(see Table 3). These numbers provide information
about what segments of text are easier or harder to
code. For instance, when a cluster contains only
two ADUs, it means that three of the five anno-
tators did not label the text segment as a Callout.
Our MTurk study of Stance/Rationale (Sec. 3.2)
could highlight one reason for the variation ? some
coders consider a segment of text as Callout when
an implicit Stance is present, while others do not.
41
# Of EAs Callout Target
5 I disagree too. some things they get right, some
things they do not.
the iPhone is a truly great design.
I disagree too . . . they do not. That happened because the iPhone is a truly
great design.
I disagree too. But when we first tried the iPhone it felt natural
immediately . . . iPhone is a truly great design.
Hi there, I disagree too . . . they do not. Same as
OSX.
?Same as above-
I disagree too. . . Same as OSX . . . no problem. ?Same as above-
2 Like the reviewer said . . . (Apple) the industry
leader.. . . Good luck with that (iPhone clones).
Many of these iPhone . . . griping about issues
that will only affect them once in a blue moon
Like the reviewer said. . . (Apple) the industry
leader.
Many of these iPhone. . .
1 Do you know why the Pre . . . various hand-
set/builds/resolution issues?
Except for games?? iPhone is clearly dominant
there.
Table 3: Examples of Callouts lusters and their corresponding Targets
Thread # of Clusters # of EA ADUs per cluster
5 4 3 2 1
Android 91 52 16 11 7 5
Ipad 88 41 17 7 13 10
Layoffs 86 41 18 11 6 10
Twitter 84 44 17 14 4 5
Table 4: Number of clusters for each cluster type
Table 4 shows the number of Callout clusters in
each thread. The number of clusters with five and
four annotators shows that in each thread there are
Callouts that are plausibly easier to identify. On
the other hand, the clusters selected by only one
or two annotators are harder to identify.
3 Crowdsourcing for Fine-grained
Argumentation
To understand better the nature of the ADUs, we
conducted two studies asking Turkers to perform
finer grained analysis of Callouts and Targets. Our
first study asked five Turkers to label the relation
between a Callout and its corresponding Target
as Agree, Disagree, or Other. The Other relation
may be selected in a situation where the Callout
has no relationship with the Target (e.g., a pos-
sible digression) or is in a type of argumentative
relationship that is difficult to classify as either
Agreement or Disagreement. The second study
asked five Turkers to identify Stance and Ratio-
nale in Callouts identified by EAs. As discussed
in Section 2, by definition, a Callout contains an
explicit instance of Stance, Rationale or both. In
both of these crowdsourcing studies the Turkers
were shown only a limited portion of the threaded
discussion, i.e. the Callout-Target pairs that the
EAs had linked.
Crowdsourcing is becoming a popular mecha-
nism to collect annotations and other type of data
for natural language processing research (Wang
and Callison-Burch, 2010; Snow et al., 2008;
Chen and Dolan, 2011; Post et al., 2012). Crowd-
sourcing platforms such as Amazon Mechanical
Turk (MTurk) provide a flexible framework to sub-
mit various types of NLP tasks where novice anno-
tators (Turkers) can generate content (e.g., transla-
tions, paraphrases) or annotations (labeling) in an
inexpensive way and with limited training. MTurk
also provides researchers with the ability to con-
trol the quality of the Turkers, based on their past
performances. Section 3.1 and 3.2 describe our
two crowdsourcing studies for fine grain argumen-
tation annotation.
3.1 Crowdsourcing Study 1: Labeling the
Relation between Callout and Target
In this study, the Turkers? task was to assign a rela-
tion type between a Callout and its associated Tar-
get. The choices were Agree, Disagree, or Other.
Turkers were provided with detailed instructions,
including multiple examples of Callout and Target
pairs and their relation type. Each HIT (Human
Intelligence Task, in the language of MTurk) con-
tained one Callout-Target pair and Turkers were
paid 2 cents per HIT. To assure a level of qual-
ity control, only qualified Turkers were allowed
to perform the task (i.e., Master level with more
than 95% approval rate and at least 500 approved
HITs).
For this experiment, we randomly selected a
Callout from each cluster, along with its corre-
sponding Target. Our assumption is that all Call-
out ADUs in a given cluster have the same relation
type to their Targets (see Table 3). While this as-
sumption is logical, we plan to fully investigate it
42
in future work by running an MTurk experiment
on all the Callout ADUs and their corresponding
Targets.
We utilized Fleiss? kappa (Fleiss, 1971) to
compute IAA between the Turkers (every HIT
was completed by five Turkers). Kappa is be-
tween 0.45-0.55 for each thread showing moder-
ate agreement between the Turkers (Landis et al.,
1977). These agreement results are in line with the
agreement noticed in previous studies on agree-
ment/disagreement annotations in online interac-
tions (Bender et al., 2011; Abbott et al., 2011).
To select a gold standard for the relation type, we
used majority voting. That is, if three or more
Turkers agreed on a label, we selected that label
as the gold standard. In cases where there was
no majority, we assigned the label Other. The to-
tal number of Callouts that are in agreement and
in disagreement with Targets are 143 and 153, re-
spectively.
Table 5 shows the percentage of each
type of relation identified by Turkers
(Agree/Disagree/Other) for clusters annotated by
different number of EAs. The results suggest
that there is a correlation between text segments
that are easier or harder to annotate by EAs with
the ability of novice annotators to identify an
Agree/Disagree relation type between Callout and
Target. For example, Turkers generally discovered
Agree/Disagree relations between Callouts and
their Targets when the Callouts are part of those
clusters that are annotated by a higher number
of EAs. Turkers identified 57% as showing
a disagreement relation between Callout and
Target, and 39% as showing an agreement relation
(clusters with 5 EAs). For those clusters, only
4% of the Callouts are labeled as having an Other
relation with the Target. For clusters selected
by fewer EAs, however, the number of Callouts
having a relation with the Target labeled as Other
is much higher (39% for clusters with two EAs
and 32% for clusters with one EA). These results
show that those Callouts that are easier to discover
(i.e., identified by all five EAs) mostly have a
relation with the Target (Agree or Disagree) that
is clearly expressed and thus recognizable to the
Turkers. Table 5 also shows that in some cases
even if some EAs agreed on a piece of text to be
considered as a Callout, the novice annotators
assigned the Other relation to the Callout and Tar-
get ADUs. There are two possible explanations:
Relation label # of EA ADUs per cluster
5 4 3 2 1
Agree 39.36 43.33 42.50 35.48 48.39
Disagree 56.91 31.67 32.50 25.81 19.35
Other 3.72 25.00 25.00 38.71 32.26
Table 5: Percentage of Relation labels per EA
cluster type
either the novice annotators could not detect an
implicit agreement or disagreement and thus they
selected Other, or there are other types of relations
besides Agreement and Disagreement between
Callouts and their corresponding Targets. We
plan to extend this study to other fine grained
relation types in future work. In the next section
we discuss the results of building a supervised
classifier to predict the Agree or Disagree relation
type between Callout/Target pairs.
3.1.1 Predicting the Agree/Disagree Relation
Label
We propose a supervised learning setup to clas-
sify the relation types of Callout-Target pairs. The
classification categories are the labels collected
from the MTurk experiment. We only consider
the Agree and Disagree categories since the Other
category has a very small number of instances
(53). Based on the annotations from the Turkers,
we have 143 Agree and 153 Disagree training in-
stances.
We first conducted a simple baseline exper-
iment to check whether participants use words
or phrases to express explicit agreement or dis-
agreement such as ?I agree?, ?I disagree?. We
collected two small lists (twenty words each)
of words from Merriam-Webster dictionary that
explicitly represent agreement and disagreement
Stances. The agreement list contains the word
?agree? and its synonyms such as ?accept?, ?con-
cur?, and ?accede?. The disagreement list con-
tains the word ?disagree? and synonyms such as
?differ? and ?dissent?. We then checked whether
the text of the Callouts contains these explicit
agreement/disagreement markers. Note, that these
markers are utilized as rules and no statistical
learning is involved in this stage of experiment.
The first row of the Table 6 represents the base-
line results. Though the precision is high for
agreement category, the recall is quite low and that
results in a poor overall F1 measure. This shows
that even though markers like ?agree? or ?disagree?
43
Features Category P R F1
Baseline Agree 83.3 6.9 12.9
Disagree 50.0 5.2 9.5
Unigrams Agree 57.9 61.5 59.7
Disagree 61.8 58.2 59.9
MI-based unigram Agree 60.1 66.4 63.1
Disagree 65.2 58.8 61.9
LexF Agree 61.4 73.4 66.9
Disagree 69.6 56.9 62.63
Table 6: Classification of Agree/Disagree
are very precise, they occur in less than 15% of
all the Callouts expressing agreement or disagree-
ment.
For the next set of experiments we used a super-
vised machine learning approach for the two-way
classification (Agree/Disagree). We use Support
Vector Machines (SVM) as our machine-learning
algorithm for classification as implemented in
Weka (Hall et al., 2009) and ran 10-fold cross val-
idation. As a SVM baseline, we first use all un-
igrams in Callout and Target as features (Table
6, Row 2). We notice that the recall improves
significantly when compared with the rule-based
method. To further improve the classification ac-
curacy, we use Mutual Information (MI) to se-
lect the words in the Callouts and Targets that are
likely to be associated with the categories Agree
and Disagree, respectively. Specifically, we sort
each word based on its MI value and then se-
lect the first 180 words in each of the two cate-
gories to represent our new vocabulary set of 360
words. The feature vector includes only words
present in the MI list. Compared to the all uni-
grams baseline, the MI-based unigrams improve
the F1 by 4% (Agree) and 2% (Disagree) (Table
6). The MI approach discovers the words that
are highly associated with Agree/Disagree cate-
gories and these words turn to be useful features
for classification. In addition, we consider several
types of lexical features (LexF) inspired by previ-
ous work on agreement and disagreement (Galley
et al., 2004; Misra and Walker, 2013).
? Sentiment Lexicon (SL): Two features are de-
signed using a sentiment lexicon (Hu and Liu,
2004) where the first feature represents the num-
ber of times the Callout and the Target contain a
positive emotional word and the second feature
represents the number of the negative emotional
words.
? Initial unigrams in Callout (IU): Instead of
using all unigrams in the Callout and Target,
Features Category P R F1
LexF Agree 61.4 73.4 66.9
Disagree 69.6 56.9 62.6
LexF-SL Agree 60.6 74.1 66.7
Disagree 69.4 54.9 61.3
LexF-IU Agree 58.1 69.9 63.5
Disagree 65.3 52.9 58.5
LexF-LO Agree 57.2 74.8 64.8
Disagree 67.0 47.7 55.7
Table 7: Importance of Lexical Features
we only select the first words from the Call-
out (maximum ten). The assumption is that the
stance is generally expressed at the beginning
of a Callout. We used the same MI-based tech-
nique to filter any sparse words.
? Lexical Overlap and Length (LO): This set of
features represents the lexical overlap between
the Callout and the Target and the length of each
ADU.
Table 6 shows that using all these types of
lexical features improves the F1 score for both
categories as compared to the MI-based unigram
features. Table 7 shows the impact of remov-
ing each type of lexical features. From these re-
sults it seems that initial unigrams of Callout (IU)
and lexical overlap (LO) are useful features: re-
moving each of them lowers the results for both
Agree/Disagree categories. In future work, we
plan to explore context-based features such as the
thread structure, and semantic features such as
WordNet-based semantic similarity. We also hy-
pothesize that with additional training instances
the ML approaches will achieve better results.
3.2 Crowdsourcing Study 2: Analysis of
Stance and Rationale
In the second study aimed at identifying the ar-
gumentative nature of the Callouts identified by
the expert annotators, we focus on identifying the
Stance and Rationale segments of a Callout. Since
the presence of at least an explicit Stance or Ra-
tionale was part of the definition of a Callout, we
selected these two argumentation categories as our
finer-grained scheme for this experiment.
Given a pair of Callout and Target ADUs, five
Turkers were asked to identify the Stance and Ra-
tionale segments in the Callout, including the ex-
act boundaries of the text segments. Identifying
Stance and Rationale is a difficult task and thus,
we also asked Turkers to mark the level of diffi-
culty in the identification task. We provided the
44
Diff Number of EAs per cluster
5 4 3 2 1
VE 22.11 22.38 20.25 16.67 10.71
E 28.55 24.00 24.02 28.23 20.00
M 19.69 17.87 20.72 19.39 23.57
D 11.50 10.34 11.46 9.52 12.86
VD 7.02 5.61 4.55 4.42 6.43
TD 11.13 19.79 19.00 21.77 26.33
Table 8: Difficulty judgments by Turkers com-
pared to number of EAs who selected a cluster
Turkers with a scale of difficulty (similar to a Lik-
ert scale), where the Turkers have to choose one
of the following: very easy (VE), easy (E), moder-
ate (M), difficult (D), very difficult (VD), too diffi-
cult to code (TD). Turkers were instructed to select
the too difficult to code choice only in cases where
they felt it was impossible to detect a Stance or
Rationale in the Callout.
The Turkers were provided with detailed in-
structions including examples of Stance and Ra-
tionale annotations for multiple Callouts and only
highly qualified Turkers were allowed to perform
the task. Unlike the previous study, we also ran a
pre-screening testing phase and only Turkers that
passed the screening were allowed to complete the
tasks. Because of the difficult nature of the anno-
tation task, we paid ten cents per HIT.
For the Stance/Rationale study, we considered
all the Callouts in each cluster along with the asso-
ciated Targets. We selected all the Callouts from
each cluster because of variability in the bound-
aries of ADUs, i.e., in the segmentation process.
One benefit of this crowdsourcing experiment is
that it helps us understand better what the variabil-
ity means in terms of argumentative structure. For
example, one EA might mark a text segment as a
Callout only when it expresses a Stance, while an-
other EA might mark as Callout a larger piece of
text expressing both the Stance and Rationale (See
examples of Clusters in Table 3). We leave this
deeper analysis as future work.
Table 8 shows there is a correlation between
the number of EAs who selected a cluster and the
difficulty level Turkers assigned to identifying the
Stance and Rationale elements of a Callout. This
table shows that for more than 50% of the Callouts
that are identified by 5 EAs, the Stance and Ra-
tionale can be easily identified (refer to the ?VE?
and ?E? rows), where as in the case of Callouts
that are identified by only 1 EA, the number is
just 31%. Similarly, more than 26% of the Call-
Diff Number of EAs per cluster
5 4 3 2 1
E 81.04 70.76 60.98 63.64 25.00
M 7.65 7.02 17.07 6.06 25.00
D 5.91 5.85 7.32 9.09 12.50
TD 5.39 16.37 14.63 21.21 37.50
Table 9: Difficulty judgment (majority voting)
outs in that same category (1 EA) were labeled as
?Too difficult to code?, indicating that the Turk-
ers could not identify either a Stance or Rationale
in the Callout. These numbers are comparable to
what our first crowdsourcing study showed for the
Agree/Disagree/Other relation identification (Ta-
ble 5). Table 9 shows results where we selected
overall difficulty level by majority voting. We
combined the easy and very easy categories to the
category easy (E) and the difficult and very diffi-
cult categories to the category difficult (D) for a
simpler presentation.
Table 9 also shows that more than 80% of the
time, Turkers could easily identify Stance and/or
Rationale in Callouts identified by 5 EAs, while
they could perform the finer grained analysis eas-
ily only 25% of time for Callouts identified by a
single EA. Only 5% of Callouts identified by all
5 EAs were considered too difficult to code by the
Turkers (i.e., the novice annotators could not iden-
tify a Stance or a Rationale). In contrast, more
than 37% of Callouts annotated only by 1 EA were
considered too difficult to code by the novice an-
notators. Table 10 presents some of the examples
of Stance and Rationale pairs as selected by the
Turkers along with the difficulty labels.
4 Related Work
Primary tasks for argument analysis are to seg-
ment the text to identify ADUs, detect the roles
of each ADUs, and to establish the relationship
between the ADUs (Peldszus and Stede, 2013a).
Similarly, Cohen (1987) presented a computa-
tional model of argument analysis where the struc-
ture of each argument is restricted to the claim and
evidence relation. Teufel et al. (2009) introduce
the argumentative zoning (AZ) idea that identifies
important sections of scientific articles and later
Hachey and Grover (2005) applied similar idea of
AZ to summarize legal documents. Wyner et al.
(2012) propose a rule-based tool that can high-
light potential argumentative sections of text ac-
cording to discourse cues like ?suppose? or ?there-
fore?. They tested their system on product reviews
45
Target Callout Stance Rationale Difficulty
the iPhone is a truly
great design.
I disagree too. some
things they get right,
some things they do
not.
I. . . too Some things . . . do not Easy
the dedicated ?Back?
button
that back button is key.
navigation is actually
much easier on the an-
droid.
That back button is key Navigation
is. . . android
Moderate
It?s more about the fea-
tures and apps and An-
droid seriously lacks on
latter.
Just because the iPhone
has a huge amount of
apps, doesn?t mean
they?re all worth
having.
? Just because the iPhone
has a huge amount of
apps, doesn?t mean
they?re all worth
having.
Difficult
I feel like your com-
ments about Nexus One
is too positive . . .
I feel like your poor
grammar are to obvious
to be self thought...
? ? Too difficult to
code
Table 10: Examples of Callout/Target pairs with difficulty level (majority voting)
(Canon Camera) from Amazon e-commerce site.
Relatively little attention has so far been de-
voted to the issue of building argumentative cor-
pora from naturally occurring texts (Peldszus and
Stede, 2013a; Feng and Hirst, 2011). However,
(Reed et al., 2008; Reed and Rowe, 2004) have
developed the Araucaria project that maintains
an online repository of arguments (AraucariaDB),
which recently has been used as research cor-
pus for several automatic argumentation analyses
(Palau and Moens, 2009; Wyner et al., 2010; Feng
and Hirst, 2011). Our work contributes a new prin-
cipled method for building annotated corpora for
online interactions. The corpus and guidelines will
also be shared with the research community.
Another line of research that is correlated with
ours is recognition of agreement/disagreement
(Misra and Walker, 2013; Yin et al., 2012; Ab-
bott et al., 2011; Andreas et al., 2012; Galley et
al., 2004; Hillard et al., 2003) and classification of
stances (Walker et al., 2012; Somasundaran and
Wiebe, 2010) in online forums. For future work,
we can utilize textual features (contextual, depen-
dency, discourse markers), relevant multiword ex-
pressions and topic modeling (Mukherjee and Liu,
2013), and thread structure (Murakami and Ray-
mond, 2010; Agrawal et al., 2003) to improve the
Agree/Disagree classification accuracy.
Recently, Cabrio and Villata (2013) proposed
a new direction of argumentative analysis where
the authors show how arguments are associated
with Recognizing Textual Entailment (RTE) re-
search. They utilized RTE approach to detect the
relation of support/attack among arguments (en-
tailment expresses a ?support? and contradiction
expresses an ?attack?) on a dataset of arguments
collected from online debates (e.g., Debatepedia).
5 Conclusion and Future Work
To make progress in argument mining for online
interactions, we need to develop a principled and
scalable way to determine which portions of texts
are argumentative and what is the nature of argu-
mentation. We have proposed a two-tiered ap-
proach to achieve this goal. As a first step we
adopted a coarse-grained annotation scheme based
on Pragmatic Argumentation Theory and asked
expert annotators to label entire threads using this
scheme. Using a clustering technique we iden-
tified which pieces of text were easier or harder
for the Expert Annotators to annotate. Then we
showed that crowdsourcing is a feasible approach
to obtain annotations based on a finer grained ar-
gumentation scheme, especially on text segments
that were easier for the Expert Annotators to la-
bel as being argumentative. While more qualita-
tive analysis of these results is still needed, these
results are an example of the potential benefits of
our multi-step coding approach.
Avenues for future research include but are not
limited to: 1) analyzing the differences between
the stance and rationale annotations among the
novice annotators; 2) improving the classification
accuracies of the Agree/Disagree classifier using
more training data; 3) using syntax and seman-
tics inspired textual features and thread structure;
and 4) developing computational models to detect
Stance and Rationale.
46
Acknowledgements
Part of this paper is based on work supported by
the DARPA-DEFT program for the first two au-
thors. The views expressed are those of the au-
thors and do not reflect the official policy or po-
sition of the Department of Defense or the U.S.
Government.
References
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E
Fox Tree, Robeson Bowmani, and Joseph King.
2011. How can you say such things?!?: Recogniz-
ing disagreement in informal political argument. In
Proceedings of the Workshop on Languages in So-
cial Media, pages 2?11. Association for Computa-
tional Linguistics.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In
Proceedings of the 12th international conference on
World Wide Web, pages 529?535. ACM.
Jacob Andreas, Sara Rosenthal, and Kathleen McKe-
own. 2012. Annotating agreement and disagree-
ment in threaded discussion. In LREC, pages 818?
822.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Emily M Bender, Jonathan T Morgan, Meghan Oxley,
Mark Zachry, Brian Hutchinson, Alex Marin, Bin
Zhang, and Mari Ostendorf. 2011. Annotating so-
cial acts: Authority claims and alignment moves in
wikipedia talk pages. In Proceedings of the Work-
shop on Languages in Social Media, pages 48?57.
Association for Computational Linguistics.
Or Biran and Owen Rambow. 2011. Identifying jus-
tifications in written dialogs by classifying text as
argumentative. International Journal of Semantic
Computing, 5(04):363?381.
Elena Cabrio and Serena Villata. 2013. A natural
language bipolar argumentation approach to support
users in online debate interactions. Argument &
Computation, 4(3):209?230.
David L Chen and William B Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 190?200.
Association for Computational Linguistics.
Robin Cohen. 1987. Analyzing the structure of ar-
gumentative discourse. Computational linguistics,
13(1-2):11?24.
Vanessa Wei Feng and Graeme Hirst. 2011. Clas-
sifying arguments by scheme. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 987?996. Association
for Computational Linguistics.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, page 669. Association for Computational Lin-
guistics.
Ben Hachey and Claire Grover. 2005. Automatic le-
gal text summarisation: experiments with summary
structuring. In Proceedings of the 10th international
conference on Artificial intelligence and law, pages
75?84. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10?
18.
Trevor Hastie, Robert Tibshirani, Jerome Friedman,
T Hastie, J Friedman, and R Tibshirani. 2009. The
elements of statistical learning, volume 2. Springer.
Dustin Hillard, Mari Ostendorf, and Elizabeth
Shriberg. 2003. Detection of agreement vs. dis-
agreement in meetings: Training with unlabeled
data. In Proceedings of the 2003 Conference of
the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology: companion volume of the Proceedings
of HLT-NAACL 2003?short papers-Volume 2, pages
34?36. Association for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177.
ACM.
Ian Hutchby. 2013. Confrontation talk: Arguments,
asymmetries, and power on talk radio. Routledge.
Klaus Krippendorff. 2004. Measuring the reliability
of qualitative text analysis data. Quality & quantity,
38:787?800.
J Richard Landis, Gary G Koch, et al. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, 33(1):159?174.
Douglas W Maynard. 1985. How children start argu-
ments. Language in society, 14(01):1?29.
47
Renee A Meyers and Dale Brashers. 2010. Extend-
ing the conversational argument coding scheme: Ar-
gument categories, units, and coding procedures.
Communication Methods and Measures, 4(1-2):27?
45.
Amita Misra and Marilyn A Walker. 2013. Topic in-
dependent identification of agreement and disagree-
ment in social media dialogue. In Proceedings of
the SIGDIAL 2013 Conference, pages 41?50. Asso-
ciation for Computational Linguistics.
Arjun Mukherjee and Bing Liu. 2013. Discovering
user interactions in ideological discussions. In Pro-
ceedings of the 51st Annual Meeting on Association
for Computational Linguistics, pages 671?681. Cite-
seer.
Akiko Murakami and Rudy Raymond. 2010. Support
or oppose?: classifying positions in online debates
from reply activities and opinion expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: the detection, clas-
sification and structure of arguments in text. In Pro-
ceedings of the 12th international conference on ar-
tificial intelligence and law, pages 98?107. ACM.
Andreas Peldszus and Manfred Stede. 2013a. From ar-
gument diagrams to argumentation mining in texts:
A survey. International Journal of Cognitive Infor-
matics and Natural Intelligence (IJCINI), 7(1):1?31.
Andreas Peldszus and Manfred Stede. 2013b. Ranking
the annotators: An agreement study on argumenta-
tion structure. In Proceedings of the 7th linguistic
annotation workshop and interoperability with dis-
course, pages 196?204.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 401?409. Association for Computational
Linguistics.
Chris Reed and Glenn Rowe. 2004. Araucaria: Soft-
ware for argument analysis, diagramming and repre-
sentation. International Journal on Artificial Intelli-
gence Tools, 13(04):961?979.
Chris Reed, Raquel Mochales Palau, Glenn Rowe, and
Marie-Francine Moens. 2008. Language resources
for studying argument. In Proceedings of the 6th
conference on language resources and evaluation-
LREC 2008, pages 91?100.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254?263. Association for Computational
Linguistics.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124. Association
for Computational Linguistics.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2008. Discourse level opinion relations: An
annotation study. In Proceedings of the 9th SIGdial
Workshop on Discourse and Dialogue, pages 129?
137. Association for Computational Linguistics.
Simone Teufel, Advaith Siddharthan, and Colin Batch-
elor. 2009. Towards discipline-independent ar-
gumentative zoning: evidence from chemistry and
computational linguistics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 3-Volume 3, pages
1493?1502. Association for Computational Linguis-
tics.
Frans H Van Eemeren, Rob Grootendorst, Sally Jack-
son, and Scott Jacobs. 1993. Reconstructing argu-
mentative discourse. University of Alabama Press.
Marilyn A Walker, Pranav Anand, Rob Abbott, Jean
E Fox Tree, Craig Martell, and Joseph King. 2012.
That is your evidence?: Classifying stance in on-
line political debate. Decision Support Systems,
53(4):719?729.
Rui Wang and Chris Callison-Burch. 2010. Cheap
facts and counter-facts. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical
Turk, pages 163?167. Association for Computa-
tional Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165?210.
Adam Wyner, Raquel Mochales-Palau, Marie-Francine
Moens, and David Milward. 2010. Approaches to
text mining arguments from legal cases. In Semantic
processing of legal texts, pages 60?79. Springer.
Adam Wyner, Jodi Schneider, Katie Atkinson, and
Trevor JM Bench-Capon. 2012. Semi-automated
argumentative analysis of online product reviews. In
COMMA, pages 43?50.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.
2012. Unifying local and global agreement and
disagreement classification in online debates. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
pages 61?69. Association for Computational Lin-
guistics.
48
LAW VIII - The 8th Linguistic Annotation Workshop, pages 120?128,
Dublin, Ireland, August 23-24 2014.
Annotating Multiparty Discourse: Challenges for Agreement Metrics
Nina Wacholder* Smaranda Muresan? Debanjan Ghosh* Mark Aakhus*
*School of Communication and Information, Rutgers University
?Center for Computational Learning Systems, Columbia University
ninwac|debanjan.ghosh|aakhus@rutgers.edu, smara@ccls.columbia.edu
Abstract
To computationally model discourse phenomena such as argumentation we need corpora with
reliable annotation of the phenomena under study. Annotating complex discourse phenomena
poses two challenges: fuzziness of unit boundaries and the need for multiple annotators. We show
that current metrics for inter-annotator agreement (IAA) such as P/R/F1 and Krippendorff?s ?
provide inconsistent results for the same text. In addition, IAA metrics do not tell us what parts of
a text are easier or harder for human judges to annotate and so do not provide sufficiently specific
information for evaluating systems that automatically identify discourse units. We propose a
hierarchical clustering approach that aggregates overlapping text segments of text identified by
multiple annotators; the more annotators who identify a text segment, the easier we assume that
the text segment is to annotate. The clusters make it possible to quantify the extent of agreement
judges show about text segments; this information can be used to assess the output of systems
that automatically identify discourse units.
1 Introduction
Annotation of discourse typically involves three subtasks: segmentation (identification of discourse units,
including their boundaries), segment classification (labeling the role of discourse units) and relation iden-
tification (indicating the link between the discourse units) (Peldszus and Stede, 2013a). The difficulty
of achieving an Inter-Annotator Agreement (IAA) of .80, which is generally accepted as good agree-
ment, is compounded in studies of discourse annotations since annotators must unitize, i.e. identify the
boundaries of discourse units (Artstein and Poesio, 2008). The inconsistent assignment of boundaries in
annotation of discourse has been noted at least since Grosz and Sidner (1986) who observed that although
annotators tended to identify essentially the same units, the boundaries differed slightly. The need for
annotators to identify the boundaries of text segments makes measurement of IAA more difficult because
standard coefficients such as ? assume that the units to be coded have been identified before the coding
begins (Artstein and Poesio, 2008). A second challenge for measuring IAA for discourse annotation is
associated with larger numbers of annotators. Because of the many ways that ideas are expressed in hu-
man language, using multiple annotators to study discourse phenomena is important. Such an approach
capitalizes on the aggregated intuitions of multiple coders to overcome the potential biases of any one
coder and helps identify limitations in the coding scheme, thus adding to the reliability and validity of
the annotation study. The more annotators, however, the harder it is to achieve an IAA of .80 (Bayerl and
Paul, 2011). What to annotate also depends, among other characteristics, on the phenomenon of interest,
the text being annotated, the quality of the annotation scheme and the effectiveness of training. But even
if these are excellent, there is natural variability in human judgment for a task that involves subtle dis-
tinctions about which competent coders disagree. An accurate computational model should reflect this
variability (Aakhus et al., 2013).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
120
# Type Statement
Target I?m going to quit the iphone and switch to an android phone because I
can no long (sic) put up with the AT&T service contract
Callout I am going to switch too
Callout There is no point quitting the iphone because of the service package,
just jail break it and use the provider you want
Table 1: Examples of Callouts and Targets
Figure 1: Cluster where 3 judges identify a core
We propose an approach for overcoming these challenges based on evidence from an annotation study
of arguments in online interactions. Our scheme for argumentation is based on Pragmatic Argumentation
Theory (PAT) (Van Eemeren et al., 1993; Hutchby, 2013; Maynard, 1985). PAT states that argument can
arise at any point when two or more actors engage in calling out and making problematic some aspect
of another actor?s prior contribution for what it (could have) said or meant (Van Eemeren et al., 1993).
The argumentative relationships among contributions to a discussion are indicated through links between
what is targeted and how it is called out. Table 1 shows two Callouts that refer back to the same Target.
Callouts and Targets are Argumentative Discourse Units (ADUs) in the sense of Peldszus and Stede
(2013a), ?minimal units of analysis . . . inspired . . . by a . . . relation-based discourse theory? (p.20). In our
case the theory is PAT. Callouts are related to Targets by a relationship that we may refer to as Response,
though we do not discuss the Response relationship in this paper.
The hierarchical clustering technique that we propose systematically identifies clusters of ADUs; each
cluster contains a core of overlapping text that two or more judges have identified. Figure 1 shows
a schematic example of a cluster with a core identified by three judges. The variation in boundaries
represents the individual judges? differing intuitions; these differences reflect natural variation of human
judgments about discourse units. We interpret differences in the number (or percentage) of judges that
identify a core as evidence of how hard or easy a discourse unit is to recognize.
The contributions of this paper are two-fold. First, we show that methods for assessing IAA, such as
the information retrieval inspired (P/R/F1) approach (Wiebe et al., 2005) and Krippendorff?s ? (Krip-
pendorff, 1995; Krippendorff, 2004b), which was developed for content analysis in the social sciences,
provide inconsistent results when applied to segmentations involving fuzzy boundaries and multiple
coders.
In addition, these metrics do not tell us which parts of a text are easier or harder to annotate, or help
choose a reliable gold standard. Our second contribution is a new method for assessing IAA using hier-
archical clustering to find parts of text that are easier or harder to annotate. These clusters could serve as
the basis for assessing the performance of systems that automatically identify ADUs - the system would
be rewarded for identifying ADUs that are easier for people to recognize and penalized for identifying
ADUs that are relatively hard for people to recognize.
2 Annotation Study of Argumentative Discourse Units: Callouts and Targets
In this section, we describe the annotation study we conducted to determine whether trained human
judges can reliably identify Callouts and Targets. The main annotation task was to find Callouts and the
Targets to which they are linked and unitize them, i.e., assign boundaries to each ADU. As mentioned
above, these are the steps for argument mining delineated in Peldszus and Stede (2013a). The design of
121
the study was consistent with the conditions for generating reliable annotations set forth in Krippendorff
(2004a, p. 217).
We selected five blog postings from a corpus crawled from Technorati (technorati.com) between 2008-
2010; the comments contain many disputes. We used the first 100 comments on each blog as our corpus,
along with the original posting. We refer to each blog and the associated comments as a thread.
The complexity of the phenomenon required the perspective of multiple independent annotators, de-
spite the known difficulty in achieving reliable IAA with more than two annotators. For our initial
study, in which our goal was to obtain naturally occurring examples of Callouts and Targets and assess
the challenges of reliably identifying them, we engaged five graduate students with a strong humanities
background. The coding was performed with the open-source Knowtator software (Ogren, 2006). All
five judges annotated all 100 comments in all five threads. While the annotation process was under way,
annotators were instructed not to communicate with each other about the study.
The annotators? task was to find each instance of a Callout, determine the boundaries, link the Callout
to the most recent Target and determine the boundaries of the Target. We prepared and tested a set of
guidelines with definitions and examples of key concepts. The following is an adapted excerpt from the
guidelines:
? Callout: A Callout is (a part of) a subsequent action that selects (a part of) a prior action and marks
and comments on it in some way. In Table 1, Statements 2 and 3 are both Callouts, i.e., they perform
the action of calling out on Statement 1. Statement 2 calls out the first part of Statement 1 dealing
with switching phones. Statement 3 calls out all of Statement 1 ? both what?s proposed and the
rationale for the disagreement.
? Target: A Target is a part of a prior action that has been called out by a subsequent action. Statement
1 is a Target of Statements 2 and 3. But Statements 2 and 3 link to different parts of Statement 1, as
described above.
? Response: A link between Callout and Target that occurs when a subsequent action refers back to
(is a response to) a prior action.
Annotators were instructed to mark any text segment (from words to entire comments) that satisfied
the definitions above. A single text segment could be a Target and a Callout. To save effort on a difficult
task, judges were asked only to annotate the most recent plausible Target. We plan to study chains of
responses in future work.
Prior to the formal study, each annotator spent approximately eight hours in training, spread over about
two weeks, under the supervision of a PhD student who had helped to develop the guidelines. Training
materials included the guidelines and postings and comments from Technorati that were not used in the
formal study. Judges were reminded that our research goal was to find naturally occurring examples of
Callouts and Targets and that the research team did not know in advance what were the right answers
? the subjects? job was to identify Callouts and Targets that satisfied the definitions in the guidelines.
In response to the judges? questions, the guidelines were iteratively updated: definitions were reviewed,
additional examples were added, and a list of FAQs was developed
1
.
Table 2 shows the wide range of results among the annotators for Callouts that illustrates a problem to
be addressed when assessing reliability for multiple annotators.
Averaged over all five threads, A1 identified the fewest Callouts (66.8) while A4 and A5 identified
the most (107 and 109, respectively). Furthermore, the number of annotations assigned by A4 and A5
to each corpus is consistently higher than those of the other annotators, while the number of annotations
A1 assigned to each thread is consistently lower than that of all of the other annotators. Although these
differences could be due to issues with training, we interpret the consistent variation among coders as
potential evidence of two distinct types of behavior: some judges are ?lumpers? who consider a text
string as a single unit; others are ?splitters? who treat the same text string as two (or more) distinct units.
The high degree of variability among coders is consistent with the observations of Peldszus and Stede
1
The corpus, annotations and guidelines are available at <http://wp.comminfo.rutgers.edu/salts/projects/opposition/>.
122
Thread A1 A2 A3 A4 A5
Android 73 99 97 118 110
Ban 46 73 66 86 83
iPad 68 86 85 109 118
Layoffs 71 83 74 109 117
Twitter 76 102 70 113 119
Avg. 66.8 88.6 78.4 107 109.4
Table 2: Callouts per annotator per thread
(2013b). These differences could be due to issues with training and individual differences among coders,
but even so, the variability highlights an important challenge for calculating IAA with multiple coders
and fuzzy unit boundaries.
3 Some Problems of Unitization Reliability with Existing IAA Metrics
In this section we discuss two state-of-the-art metrics frequently used for measuring IAA for discourse
annotation and we show that these methods offer limited informativeness when text boundaries are fuzzy
and there are multiple judges. These methods are the information retrieval inspired precision-recall
(P/R/F1) metrics used in Wiebe and her collaborators? important work on sentiment analysis (Wiebe
et al., 2005; Somasundaran et al., 2008) and Krippendorff?s ?, a variant of the ? family of IAA coef-
ficients specifically designed to handle fuzzy boundaries and multiple annotators (Krippendorff, 1995;
Krippendorff, 2004b). Krippendorff?s ? determines IAA based on observed disagreement relative to
expected agreement and calculates differences in annotators? judgments. Although it is possible to use
number of words or even clauses to measure IAA, we use length in characters both for consistency with
Wiebe?s approach and because Krippendorff (2004b, pp.790-791) recommends using ?. . . the smallest
distinguishable length, for example the characters in text. . .? to measure IAA. We next show the results
of using P/R/F and Krippendorff?s ? to measure IAA for our annotation study and provide examples of
some challenges that need to be addressed.
3.1 Precision, Recall and F measures
Implementing P/R/F1 requires a gold standard annotation against which the other annotations can be
compared. P/R/F1 is calculated here, following (Wiebe et al., 2005), as follows: the units selected by
one annotator are taken as the gold standard and the remaining annotators are calculated against the
selected gold standard. To determine whether annotators selected the same text span, two different types
of matches were considered, as in Somasundaran et al. (2008): exact matches and overlap matches
(variation of their lenient match):
? Exact Matches (EM): Text spans that vary at the start or end point by five characters or less are
considered an exact match. This minor relaxation of exact matching (Somasundaran et al., 2008)
compensates for minor inconsistencies such as whether a judge included a sentence ending punctu-
ation mark in the unit.
? Overlap Matches (OM): Any overlap between text spans of more than 10% of the total number of
characters is considered a match. OM is weaker than EM but still an indicator of shared judgments
by annotators.
Tables 3 and 5 and Tables 4 and 6 show the P/R/F1-based IAA using EM and OM respectively. The
results are averaged across all five threads. Besides average P/R/F1 we also show Max F1 and Min F1,
which represent the maximum and minimum F1 relative to a particular annotator used as gold standard.
These tables show that the results vary greatly. Among the reasons for the variation are the following:
? Results are sensitive to which annotator is selected as the gold standard. In Table 4, pairing A4 with
the judge who agrees maximally produces an F measure of 90.2 while pairing A4 with the annotator
who agrees minimally produces an F measure of 73.3. In Tables 3 and 4, if we select A4 as the gold
standard we get the most variation; selecting A3 produces the least.
123
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 40.7 57.7 47.8 60 36.7
A2 51.7 51.2 51.4 58.3 43
A3 54.2 57.8 55.9 61.4 47.9
A4 59.7 49.1 53.9 61.4 47.3
A5 55 45.6 49.9 58.3 36.7
Table 3: Callouts: EM P/R/F1 over 5 threads
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 67.4 95.7 79.1 86.8 73.3
A2 85 83.7 84.3 88.7 76.1
A3 82.7 88 85.2 88.7 80.9
A4 92.7 76.8 84 90.2 73.3
A5 91.4 75.1 82.4 89.6 74
Table 4: Callouts: OM P/R/F1 over 5 threads
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 24.1 34.6 28.4 34.5 18.7
A2 26.9 24.7 25.7 37.6 18.7
A3 35.2 35.1 35.1 48.4 19.4
A4 37.3 34.5 35.8 50.4 22.1
A5 36.9 31.4 33.9 50.4 19.9
Table 5: Targets: EM P/R/F1 over 5 threads
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 60.1 86.5 70.9 76.1 64.2
A2 74.5 69.4 71.9 79.6 62.9
A3 75.9 74.5 75.1 80.1 67.7
A4 78.1 71.5 74.6 84.2 64
A5 83.8 70.3 76.4 83.8 67.2
Table 6: Targets: OM P/R/F1 over 5 threads
? The type of matching matters. As expected, OM, which is less strict than EM, produces substantially
higher F1 scores both for Callouts (Tables 3 and 4 ) and Targets (Tables 5 and 6).
? Different phenomena are associated with different levels of difficulty of annotation. The F1 scores
for Targets are considerably lower than the F1 scores for Callouts. We suspect that Callouts are
easier to recognize since they are often introduced with standard expressions that signal agreement
or disagreement such as ?yes?, ?no?, ?I agree?, or ?I disagree?. Targets, on the other hand, generally
lack such distinguishing lexical features.
We also observe differences across threads. For example, the Ban thread seems harder to annotate
than the other threads. Figure 2 and 3 show IAA results for OM for Callout and Target annotations for
annotators A1 and A5 respectively, across the five threads. We chose A1 and A5 because in general
A1 annotated the fewest Callouts and A5 annotated the most Callouts in the corpus. These figures show
different annotator behavior. For instance, for both Callout and Target annotations, A1 has higher average
R than P, while A5 has higher P but lower R. Figures 2 and 3 hint that the Ban thread is harder to annotate
than the others.
The examples in this section show two downsides to the P/R/F1 metric. First, the scores do not reflect
the extent to which two annotations match. This is crucial information for fuzzy boundary matching, be-
cause the agreement between two annotations can be over only a few characters or over the full length of
the selected text. Second, the variation across multiple judges demonstrates the disadvantage of arbitrary
selection of a gold standard set of annotations against which to measure IAA.
3.2 Krippendorff?s ?
Krippendorff?s ? calculates IAA based on the observed and expected disagreement between annotators.
We use the version of Kripendorff?s ? discussed in Krippendorff (2004b) which takes into account mul-
tiple annotators and fuzzy boundaries. Detailed proof and an explanation of the calculation can be found
in (Krippendorff, 2004b; Krippendorff, 1995).
Thread F1 Krippendorff?s ?
Android 87.8 0.64
Ban 85.3 0.75
iPad 86.0 0.73
Layoffs 87.5 0.87
Twitter 88.5 0.82
Table 7: F1 and ? for all 5 threads
Thread Rank by IAA (Descending)
F1 K?s ?
Twitter Layoffs
Android Twitter
Layoffs Ban
iPad iPad
Ban Android
Table 8: Threads ranked by IAA in descending order
Comparison of ? and P/R/F1 metrics shows that they generate inconsistent results that are difficult to
interpret. For example, in Table 7, the F1 measure for Callouts indicates lower agreement on the Ban
thread in comparison to Android while ? suggests higher agreement on the Ban subcorpus relative to the
124
Figure 2: IAA metrics per thread when A1 is gold standard (Left: Callout. Right: Target.)
Figure 3: IAA metrics per thread when A5 is gold standard ( Left: Callout. Right: Target.)
Android subcorpus. The inconsistencies are also apparent in Table 8, which ranks threads in descending
order of IAA. For example, the Android corpus receives the highest IAA using F1 but the lowest using
?.
We do not show the results for Krippendorff?s ? for Targets for the following reason. Relevant units
from a continuous text string are assigned to categories by individual annotators. But identification of
Targets is dependent on (temporally secondary to) identification of Callouts. In multiple instances we
observe that an annotator links multiple Callouts to two or more overlapping Targets. Depending on
the Callout, the same unit (i.e., text segment) can represent an annotation (a Target) or a gap between
two Targets. Computation of ? is based on the overlapping characters of the annotations and the gaps
between the annotations. Naturally, if a single text string is assigned different labels (i.e. annotation
or a gap between annotations) in different annotations, ? does not produce meaningful results. The
inapplicability of Krippendorff?s ? to Targets is a significant limitation for its use in discourse annotation
(To save space we only show results for Callouts in subsequent tables.)
The examples in Section 3 show a fundamental limitation of both P/R/F1 and Krippendorff?s ?: They
do not pinpoint the location in a document where the extent of variation can be observed. This limits the
usefulness of these measures for studying the discourse phenomenon of interest and for analyzing the
impact of factors such as text difficulty, corpus and judges on IAA. The impact of these factors on IAA
also makes it hard to pick gold standard examples on a principled basis.
4 Hierarchical Clustering of Discourse Units
In this section we introduce a clustering approach that aggregates overlapping annotations, thereby mak-
ing it possible to quantify agreement among annotators within a cluster. Then we show examples of
clusters from our annotation study in which the extent of annotator support for a core reflects how hard
or easy an ADU is for human judges to identify. The hierarchical clustering technique (Hastie et al.,
2009) assumes that overlapping annotations by two or more judges constitutes evidence of the approxi-
mate location of an instance of the phenomenon of interest. In our case, this is the annotation of ADUs
that contain overlapping text. Each ADU starts in its own cluster. The start and end points of each ADU
are utilized to identify overlapping characters in pairs of ADUs. Then, using a bottom-up clustering
125
# Annots Text selected
A1, A2, A3,
A4, A5
I remember Apple telling people give the UI and the keyboard a month
and you?ll get used to it. Plus all the commercials showing the interface.
So, no, you didn?t just pick up the iPhone and know how to use it. It
was pounded into to you.
Table 9: A cluster in which all five judges agreement on the boundaries of the ADU
# Annots Text selected
A1 I?m going to agree that my experience required a bit of getting used to
. . .
A2, A3, A4 I?m going to agree that my experience required a bit of getting used to
. . . I had arrived to the newly minted 2G Gmail and browsing
A5 I?m going to agree that my experience required a bit of getting used to
. . . I had arrived to the newly minted 2G Gmail and browsing. Great
browser on the iPhone but . . . Opera Mini can work wonders
Table 10: A cluster in which all 5 annotators agree on the core but disagree on the closing boundary of
the ADU
technique, pairs of clusters (e.g. pairs of Callout ADUs) with overlapping text strings are merged as they
move up in the hierarchy. An ADU that does not overlap with ADUs identified by any other judge will
remain in its own cluster.
Aggregating overlapping annotations makes it possible to quantify agreement among the annotators
within a cluster. Table 9 shows an example of a cluster that contains five annotations; all five annotators
assign identical unit boundaries, which means that there is a single core, with no variation in the extent of
the ADU. Table 9 thus shows an optimal case ? there is complete agreement among the five annotators.
We take this as strong evidence that the text string in Table 9 is an instance of a Callout that is relatively
easy to identify.
But of course, natural language does not make optimal annotation easy (even if coders were perfect).
Table 10 shows a cluster in which all five annotators agree on the core (shown in italics) but do not
agree about the boundaries of the ADU. A1 picked the shortest text segment. A2, A3 and A4 picked the
same text segment as A1 but they also included the rest of the sentence, up to the word ?browsing?. In
A5?s judgment, the ADU is still longer - it also includes the sentence ?Great browser . . . work wonders.?
Although not as clear-cut as the examples in Table 9, the fact that in Table 10 all annotators chose
overlapping text is evidence that the core has special status in the context of in an annotation task where it
is known that even expert annotators disagree about borders. Examples like those in Table 10 can be used
to study the reasons for variation in the judges? assignment of boundaries. Besides ease of recognition
of an ADU and differing human intuitions, the instructions in the guidelines or characteristics of the
Callouts may be also having an effect.
Table 11 shows a more complex annotation pattern in a cluster. Annotators A1 and A2 agree on the
boundaries of the ADU, but their annotation does not overlap with A4 at all. A3?s boundaries subsume
all other annotations. But because A4?s boundaries do not overlap with those of A1 and A2, technically
this cluster has no core (a text segment included in all ADUs in a cluster). 5% or less of the clusters
have this problem. To handle the absence of a core in this type of cluster, we split the clusters that fit this
pattern into multiple ?overlapping? clusters, that is, we put A1, A2, and A3 into one cluster and we put
A3 and A4 into another cluster. Using this splitting technique, we get two cores, each selected by two
judges: i) ?actually the only . . . app?s developer? from the cluster containing A1, A2, and A3 (shown in
italics) and ii) ?I think it hilarious . . . another device? from the cluster containing A3 and A4 (shown in
bold). The disagreement of the judges in identifying the Callout suggests that judges have quite different
judgments about boundaries of the Callouts.
Table 12 and 13 respectively show the number of clusters with overlapping annotations for Callouts
for each thread before and after splitting. The splitting process has only a small impact on results. The
number of clusters with five and four annotators shows that in each corpus there are Callouts that are
evidently easier to identify. On the other hand, clusters selected by only two or three judges are harder to
126
# Annots Text selected
A1, A2 Actually the only one responsible for the YouTube and Twitter multitask-
ing is the app?s developer
A3 Actually the . . . app?s developer. The Facebook app allows you to watch
videos posted by . . . I think it hilarious that people complain about
features that arent even available on another device
A4 I think it hilarious that people complain about features that arent
even available on another device
Table 11: A cluster with 2 cores, each selected by 2 judges
identify. The clusters containing a text string picked by only one annotator are hardest to identify. This
may be an indication that this text string is not a good example of a Callout, though it also could be an
indication that the judge is particularly good at recognizing subtly expressed Callouts. The clustering
technique thus scaffolds deeper examination of annotation behavior and annotation/concept refinement.
Table 13 also shows that overall, the number of clusters with five or four annotators is well over 50% for
each thread except Ban, even when we exclude the clusters with an ADU identified by only one judge.
This is another hint that the IAA in this thread should be much lower than in the other threads. (See also
Figures 2 and 3).
Thread # of Clusters Annots in each cluster
5 4 3 2 1
Android 91 52 16 11 7 5
Ban 89 25 18 12 20 14
Ipad 88 41 17 7 13 10
Layoffs 86 41 18 11 6 10
Twitter 84 44 17 14 4 5
Table 12: Callouts: Clusters before splitting process
Thread # of Clusters Annots in each cluster
5 4 3 2 1
Android 93 51 15 14 8 5
Ban 91 25 19 12 21 14
iPad 89 41 16 9 13 10
Layoffs 89 40 17 14 8 10
Twitter 87 43 15 20 4 5
Table 13: Callouts: Clusters after splitting process
The clusters with cores supported by four or five annotators show strong annotator agreement and are
very strong candidates for a gold standard, regardless of the IAA for the entire thread. Clusters with
an ADU selected by only one annotator are presumably harder to annotate and are more likely than
other clusters not to be actual instances of the ADU. This information can be used to assess the output
of systems that automatically identify discourse units. For example a system could be penalized more
for missing to identifying ADUs on which all five annotators agree on the boundaries, as in Table 9;
the penalty would be decreased for not identifying ADUs on which fewer annotators agree. Qualitative
analysis may help discover the reason for the variation in strength of clusters, thereby supporting our
ability to interpret IAA and to create accurate computational models of human judgments about dis-
course units. As a related research, PAT and the clustering technique discussed in this paper allow the
development of a finer-grained annotation scheme to analyze the type of links between Target-Callout
(e.g., Agree/Disagree/Other), and the nature of Callouts (e.g., Stance/Rationale) (Ghosh et al., 2014).
5 Conclusion and Future Work
Reliability of annotation studies is important both as part of the demonstration of the validity of the
phenomena being studied and also to support accurate computational modeling of discourse phenomena.
The nature of ADUs, with their fuzzy boundaries, makes it hard to achieve IAA of .80 or higher. Fur-
thermore, the use of a single figure for IAA is a little like relying on an average to convey the range of
variation of a set of numbers. The contributions of this paper are i) to provide concrete examples of the
difficulties of using state of the art metrics like P/R/F1 and Krippendorff?s ? to assess IAA for ADUs
and ii) to open up a new approach to studying IAA that can help us understand how factors like coder
variability and text difficulty affect IAA. Our approach supports reliable identification of discourse units
independent of the overall IAA of the document.
127
References
Mark Aakhus, Smaranda Muresan, and Nina Wacholder. 2013. Integrating natural language processing and
pragmatic argumentation theories for argumentation support. pages 1?12.
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4):555?596.
Petra Saskia Bayerl and Karsten Ingmar Paul. 2011. What determines inter-coder agreement in manual annota-
tions? a meta-analytic investigation. Computational Linguistics, 37(4):699?725.
Debanjan Ghosh, Smaranda Muresan, Nina Wacholder, Mark Aakhus, and Matthew Mitsui. 2014. Analyzing
argumentative discourse units in online interactions. In Proceedings of the First Workshop on Argumentation
Mining, pages 39?48, Baltimore, Maryland, June. Association for Computational Linguistics.
Barbara J Grosz and Candace L Sidner. 1986. Attention, intentions, and the structure of discourse. Computational
linguistics, 12(3):175?204.
Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and R Tibshirani. 2009. The elements
of statistical learning, volume 2. Springer.
Ian Hutchby. 2013. Confrontation talk: Arguments, asymmetries, and power on talk radio. Routledge.
Klaus Krippendorff. 1995. On the reliability of unitizing continuous data. Sociological Methodology, pages
47?76.
Klaus Krippendorff. 2004a. Content analysis: An introduction to its methodology. Sage.
Klaus Krippendorff. 2004b. Measuring the reliability of qualitative text analysis data. Quality & quantity, 38:787?
800.
Douglas W Maynard. 1985. How children start arguments. Language in society, 14(01):1?29.
Philip V Ogren. 2006. Knowtator: a prot?eg?e plug-in for annotated corpus construction. In Proceedings of the
2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human
Language Technology: companion volume: demonstrations, pages 273?275. Association for Computational
Linguistics.
Andreas Peldszus and Manfred Stede. 2013a. From argument diagrams to argumentation mining in texts: A
survey. International Journal of Cognitive Informatics and Natural Intelligence (IJCINI), 7(1):1?31.
Andreas Peldszus and Manfred Stede. 2013b. Ranking the annotators: An agreement study on argumentation
structure. In Proceedings of the 7th linguistic annotation workshop and interoperability with discourse, pages
196?204.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce Wiebe. 2008. Discourse level opinion relations: An
annotation study. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 129?137.
Association for Computational Linguistics.
Frans H Van Eemeren, Rob Grootendorst, Sally Jackson, and Scott Jacobs. 1993. Reconstructing argumentative
discourse. University of Alabama Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in
language. Language resources and evaluation, 39(2-3):165?210.
128
