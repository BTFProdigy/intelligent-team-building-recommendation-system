Proceedings of the Third Workshop on Statistical Machine Translation, pages 179?182,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Moses to Integrate Multiple Rule-Based Machine Translation Engines
into a Hybrid System
Andreas Eisele1,2, Christian Federmann2, Herve? Saint-Amand1,
Michael Jellinghaus1, Teresa Herrmann1, Yu Chen1
1: Saarland University, Saarbru?cken, Germany
2: DFKI GmbH, Saarbru?cken, Germany
Abstract
Based on an architecture that allows to com-
bine statistical machine translation (SMT)
with rule-based machine translation (RBMT)
in a multi-engine setup, we present new results
that show that this type of system combination
can actually increase the lexical coverage of
the resulting hybrid system, at least as far as
this can be measured via BLEU score.
1 Introduction
(Chen et al, 2007) describes an architecture that
allows to combine statistical machine translation
(SMT) with one or multiple rule-based machine
translation (RBMT) systems in a multi-engine setup.
It uses a variant of standard SMT technology to align
translations from one or more RBMT systems with
the source text and incorporated phrases extracted
from these alignments into the phrase table of the
SMT system. Using this approach it is possible to
employ a vanilla installation of the open-source de-
coder Moses1 (Koehn et al, 2007) to find good com-
binations of phrases from SMT training data with
the phrases derived from RBMT. A similar method
was presented in (Rosti et al, 2007).
This setup provides an elegant solution to the
fairly complex task of integrating multiple MT re-
sults that may differ in word order using only stan-
dard software modules, in particular GIZA++ (Och
and Ney, 2003) for the identification of building
blocks and Moses for the recombination, but the
authors were not able to observe improvements in
1see http://www.statmt.org/moses/
terms of BLEU score. A closer investigation re-
vealed that the experiments had suffered from a cou-
ple of technical difficulties, such as mismatches in
character encodings generated by different MT en-
gines and similar problems. This motivated us to
re-do these experiments in a somewhat more sys-
tematic way for this year?s shared translation task,
paying the required attention to all the technical de-
tails and also to try it out on more language pairs.
2 System Architecture
For conducting the translations, we use a multi-
engine MT approach based on a ?vanilla? Moses
SMT system with a modified phrase table as a cen-
tral element. This modification is performed by aug-
menting the standard phrase table with entries ob-
tained from translating the data with several rule-
based MT systems. The resulting phrase table thus
combines statistically gathered phrase pairs with
phrase pairs generated by linguistic rules.
Basing its decision about the final translation on
the obtained ?combined? phrase table, the SMT de-
coder searches for the best translation by recombin-
ing the building blocks that have been contributed by
the different RBMT systems and the original SMT
system trained on Europarl data.
A sketch of the overall architecture is given in
Fig. 1, where the lighter parts represent the mod-
ules and data sets used in purely statistical MT,
and the darker parts are the additional modules and
data sets derived from the rule-based engines. The
last word in the proposed setup is thus given to the
SMT decoder, which can recombine (and potentially
also tear apart) linguistically well-formed constructs
179
ModelLanguagePhrasetable
Combined
Alignment,PhraseExtraction
DecoderSMT
Rule?basedMT engines
ParallelCorpus
SourceText
TargetText
MonolingualCorpus
Hypotheses
CountingSmoothing
Figure 1: Hybrid architecture of the system
from the rule-based engines? output.
2.1 The Combined Phrase Table
The combined phrase table is built from the orig-
inal Moses phrase table and separate phrase tables
for each of the RBMT systems that are used in our
setup. Since the original phrase table is created
during the training process of the Moses decoder
with the Europarl bilingual corpus as training ma-
terial, it comprises general knowledge about typical
constructions and vocabulary from the Europarl do-
main. Therefore, a standard Moses SMT system is,
in principle, well adapted for input from this do-
main. However, it will have problems in dealing
with vocabulary and structures that did not occur in
the training data. The additional phrase tables are
generated separately for each RBMT system from
the source text and its translation by the respective
system. By using a combined phrase table that in-
cludes the original Moses phrase table as well as the
phrase tables from the RBMT systems, the hybrid
system can both handle a wider range of syntactic
constructions and exploit knowledge that the RBMT
systems possess about the particular vocabulary of
the source text.
3 Implementation
3.1 MT Systems and Knowledge Sources
Apart from the Moses SMT system, we used a
set of six rule-based MT engines that are partly
available via web interfaces and partly installed lo-
cally. The web interfaces are provided by Al-
tavista Babelfish (based on Systran), SDL, ProMT
and Lucy (a recent offspring of METAL). All of
them deliver significantly different output trans-
lations. Locally installed systems are OpenLo-
gos (for German?English, English?Spanish and
English?French) and translatePro by lingenio (for
German?English). The language model for our pri-
mary setup is based on the Europarl corpus whereas
the English Gigaword corpus served as training data
for a contrastive setup that was created for the trans-
lation direction German?English only.
3.2 Alignment of RBMT output
As already mentioned above, the construction of the
RBMT system specific phrase tables is a major part
of the overall system architecture. Such an RBMT
phrase table is generated from a bilingual corpus
consisting of the input text and its translation by
the respective RBMT system. Because this corpus
has the mere size of the text to be translated, it usu-
ally is not big enough to ensure the statistical meth-
ods for phrase table building of the Moses system to
work. Therefore, we create the alignments between
the RBMT input and output with help of another tool
(Theison, 2007) that is based on knowledge learned
in a previously conducted training phase with an ap-
propriately bigger corpus. On the basis of the align-
ments created in this manner, the Moses training
script provides a phrase table that consists of the
source text vocabulary. These steps are carried out
for each one of the six RBMT systems leading to
six source text specific phrase tables which are then
combined with the original Moses phrase table.
3.3 Combination of Phrase Tables
The combination process basically consists of the
concatenation of the Moses phrase table and the pre-
viously created RBMT phrase tables with one mi-
nor adjustment: The phrase table resulting from this
combination now also features additional columns
indicating which system each phrase table entry
originated from. For each new source text, the
RBMT phrase tables have to be created from scratch
and incorporated into a new combined phrase table.
3.4 Tuning
The typical process for creating an SMT system with
the Moses toolkit includes a tuning step in which
180
Europarl NewsCommentary
de-en en-de fr-en en-fr es-en en-es de-en en-de fr-en en-fr es-en en-es
SMT 22.81 19.78 24.18 21.62 31.68 24.46 14.24 9.75 11.60 12.24 17.27 14.48
Hybrid 27.85 20.75 28.12 28.82 33.15 32.31 17.36 13.57 17.66 20.71 22.16 22.55
RBMT1? 13.34 11.09 ?? 17.19 ?? 18.63 14.90 12.34 ?? 15.11 ?? 17.13
RBMT2 16.19 12.06 ?? ?? ?? ?? 16.66 13.64 ?? ?? ?? ??
RBMT3 16.32 10.88 18.18 20.38 19.32 20.89 16.88 12.53 17.20 18.82 19.00 19.98
RBMT4 15.58 12.09 19.00 22.20 18.99 21.69 17.41 13.93 17.73 20.85 19.14 21.70
RBMT5 15.58 9.54 21.36 12.98 18.47 20.59 15.99 11.05 18.65 19.49 20.50 20.02
RBMT6 13.96 9.44 17.16 18.91 18.01 19.18 15.08 10.41 16.86 17.82 18.70 19.97
Table 1: Performance of baseline SMT system, our system and RBMT systems (BLEU scores)
the system searches for the best weight configura-
tion for the columns in the phrase table while given
a development set to be translated, and correspond-
ing reference translations. In our hybrid setup, it is
equally essential to conduct tuning since the com-
bined phrase table we use contains 7 more columns
than the original Moses phrase table. All these
columns are given the same default weight initially
and thus still need be to be tuned to more meaning-
ful values. From this year?s Europarl development
data the first 200 sentences of each of the data sets
dev2006, test2006, test2007 and devtest2006 were
concatenated to build our development set. This set
of 800 sentences was used for Minimum Error Rate
Training (Och, 2003) to tune the weights of our sys-
tem with respect to BLEU score.
4 Results
In order to be able to evaluate our hybrid approaches
in contrast to stand-alone rule-based approaches, we
also calculated BLEU scores for the translations
conducted by the RBMT systems used in the hy-
brid setup. Our hybrid system is compared to a SMT
baseline and all the 6 RBMT systems that we used.
Table 1 shows the evaluation of all the systems in
terms of BLEU score (Papineni et al, 2002) with the
best score highlighted. The empty cells in the table
indicate the language pairs which are not available
in the corresponding systems2. The SMT system is
the one upon which we build the hybrid system. Ac-
cording to the scores, the hybrid system produces
better results than the baseline SMT system in all
2The identities of respective RBMT systems are not revealed
in this paper. RBMT1 is evaluated on the partial results pro-
duced due to some technical problems.
cases. The difference between our system and the
baseline is more significant for out-of-domain tests,
where gaps in the lexicon tend to be more severe.
Figure 2 illustrates an example of how the hy-
brid system differs from the baseline SMT system
and how it benefits from the RBMT systems. The
example lists the English translations of the same
German sentence (from News Commentary test set)
from different systems involved in our experiment.
Neither the word ?Pentecost? nor its German trans-
lation ?Pfingsten? has appeared in the training cor-
pus. Therefore, the SMT baseline system cannot
translate the word and chooses to leave the word
as it is whereas all the RBMT systems translate the
word correctly. The hybrid system appears to have
the corresponding lexicon gap covered by the ex-
tra entries produced by the RBMT systems. On the
other side, these additional entries may not always
be helpful. The errors in RBMT outputs can be sig-
nificant noise that destroys the correct information
in the SMT system. In the example translation pro-
duced by the hybrid system, there is a comma miss-
ing after ?in addition?, which appears to be frequent
in the RBMT outputs.
5 Outlook
The results reported in this paper are still somewhat
preliminary in the sense that many possible (includ-
ing some desirable) variants of the setup could not
be tried out due to lack of time. In particular, we
think that the full power of our approach on out-
of-domain test data can only be exploited with the
help of large language models trained on out-of-
domain text, but could not yet try this systematically.
Furthermore, the presence of multiple instances of
181
Source Daru?ber hinaus gibt es je zwei Feiertage zu Ostern, Pfingsten, und Weihnachten.
Reference In addition, Easter, Pentecost, and Christmas are each two-day holidays.
Moses In addition, there are two holidays, pfingsten to Easter, and Christmas.
Hybrid In addition there are the two holidays to Easter, Pentecost and Christmas.
RBMT1 Furthermore there are two holidays to Easter, Pentecost and Christmas .
RBMT2 Furthermore there are two holidays each at Easter, Pentecost and Christmas.
RBMT3 In addition there are each two holidays to Easters, Whitsun, and Christmas.
RBMT4 In addition, there is two holidays to Easter, Pentecost, and Christmas.
RBMT5 Beyond that there are ever two holidays to Easter, Whitsuntide, and Christmas.
RBMT6 In addition it gives two holidays apiece to easter, Pentecost, and Christmas.
Figure 2: German-English translation examples
the same phrase pair (with different weight) in the
combined phrase table causes the decoder to gen-
erate many instances of identical results in differ-
ent ways, which increases computational effort and
significantly decreases the number of distinct cases
that are considered during MERT. We suspect that a
modification of our scheme that avoids this problem
will be able to achieve better results, but experiments
in this direction are still ongoing.
The approach presented here combines the
strengths of multiple systems and is different from
recent work on post-correction of RBMT output as
presented in (Simard et al, 2007; Dugast et al,
2007), which focuses on the improvement of a sin-
gle RBMT system by correcting typical errors via
SMT techniques. These ideas are independent and a
suitable combination of them could give rise to even
better results.
Acknowledgments
This work was supported by the EuroMatrix project
funded by the European Commission (6th Frame-
work Programme). We thank Martin Kay, Hans
Uszkoreit, and Silke Theison for interesting discus-
sions and practical help, and two anonymous re-
viewers for hints to improve the paper.
References
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison. 2007.
Multi-engine machine translation with an open-source
SMT decoder. In Proceedings of WMT07, pages 193?
196, Prague, Czech Republic, June. Association for
Computational Linguistics.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of WMT07, pages
220?223, Prague, Czech Republic, June. Association
for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL Demo and Poster Sessions, pages 177?180, Jun.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, Mar.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL,
Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining translations from multiple machine
translation systems. In Proceedings of the Conference
on Human Language Technology and North American
chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL?2007), pages 228?
235, Rochester, NY, April 22-27.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with
statistical phrase-based post-editing. In Proceedings
of WMT07, pages 203?206, Prague, Czech Republic,
June. Association for Computational Linguistics.
Silke Theison. 2007. Optimizing rule-based machine
translation output with the help of statistical methods.
Diploma thesis, Saarland University.
182
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 80?84,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The Universita?t Karlsruhe Translation System for the EACL-WMT 2009
Jan Niehues, Teresa Herrmann, Muntsin Kolss and Alex Waibel
Universita?t Karlsruhe (TH)
Karlsruhe, Germany
{jniehues,therrman,kolss,waibel}@ira.uka.de
Abstract
In this paper we describe the statistical
machine translation system of the Univer-
sita?t Karlsruhe developed for the transla-
tion task of the Fourth Workshop on Sta-
tistical Machine Translation. The state-of-
the-art phrase-based SMT system is aug-
mented with alternative word reordering
and alignment mechanisms as well as op-
tional phrase table modifications. We par-
ticipate in the constrained condition of
German-English and English-German as
well as in the constrained condition of
French-English and English-French.
1 Introduction
This paper describes the statistical MT system
used for our participation in the WMT?09 Shared
Translation Task and the particular language-pair-
dependent variations of the system. We use stan-
dard alignment and training tools and a phrase-
based SMT decoder for creating state-of-the-art
MT systems for our contribution in the transla-
tion directions English-German, German-English,
English-French and French-English.
Depending on the language pair, the baseline
system is augmented with part-of-speech (POS)-
based short-range and long-range word reordering
models, discriminative word alignment (DWA)
and several modifications of the phrase table. Ex-
periments with different system variants were con-
ducted including some of those additional system
components. Significantly better translation re-
sults could be achieved compared to the baseline
results.
An overview of the system will follow in Sec-
tion 2, which describes the baseline architecture,
followed by descriptions of the additional system
components. Translation results for the different
languages and system variants are presented in
Section 5.
2 Baseline System
The core of our system is the STTK decoder (Vo-
gel, 2003), a phrase-based SMT decoder with a
local reordering window of 2 words. The de-
coder generates a translation for the input text
or word lattice by searching translation model
and language model for the hypothesis that max-
imizes phrase translation probabilities and target
language probabilities. The translation model, i.e.
the SMT phrase table is created during the training
phase by a modified version of the Moses Toolkit
(Koehn et al, 2007) applying GIZA++ for word
alignment. Language models are built using the
SRILM Toolkit. The POS-tags for the reorder-
ing models were generated with the TreeTagger
(Schmid, 1994) for all languages.
2.1 Training, Development and Test Data
We submitted translations for the English-
German, German-English, English-French and
French-English tasks. All systems were trained
on the Europarl and News Commentary corpora
using the Moses Toolkit and apply 4-gram lan-
guage models created from the respective mono-
lingual News corpora. All feature weights are au-
tomatically determined and optimized with respect
to BLEU via MERT (Venugopal et al, 2005).
For development and testing we used data pro-
vided by the WMT?09, news-dev2009a and news-
dev2009b, consisting of 1026 sentences each.
3 Word Reordering Model
One part of our system that differs from the base-
line system is the reordering model. To account
for the different word orders in the languages, we
used the POS-based reordering model presented in
Rottmann and Vogel (2007). This model learns
rules from a parallel text to reorder the source side.
The aim is to generate a reordered source side that
can be translated in a more monotone way.
80
In this framework, first, reordering rules are
extracted from an aligned parallel corpus and
POS information is added to the source side.
These rules are of the form VVIMP VMFIN PPER
? PPER VMFIN VVIMP and describe how the
source side has to be reordered to match the tar-
get side. Then the rules are scored according to
their relative frequencies.
In a preprocessing step to the actual decoding
different reorderings of the source sentences are
encoded in a word lattice. Therefore, for all re-
ordering rules that can be applied to a sentence the
resulting reorderings are added to the lattice if the
score is better than a given threshold. The decod-
ing is then performed on the resulting word lattice.
This approach does model the reordering well
if only short-range reorderings occur. But espe-
cially when translating from and to German, there
are also long-range reorderings that require the
verb to be shifted nearly across the whole sen-
tence. During this shift of the verb, the rest of
the sentence remains mainly unchanged. It does
not matter which words are in between, since they
are moved as a whole. Furthermore, rules in-
cluding an explicit sequence of POS-tags spanning
the whole sentence would be too specific. A lot
more rules would be needed to cover long-range
reorderings with each rule being applicable only
very sparsely. Therefore, we model long-range re-
ordering by generalizing over the unaffected se-
quences and introduce rules with gaps. (For more
details see Niehues and Kolss (2009)). These are
learned in a way similar to the other type of re-
ordering rules described above, but contain a gap
representing one or several arbitrary words. It is,
for example, possible to have the following rule
VAFIN * VVPP ? VAFIN VVPP *, which puts
both parts of the German verb next to each other.
4 Translation Model
The translation models of all systems we submit-
ted differ in some parts from the baseline system.
The main changes done will be described in this
section.
4.1 Word Alignment
The baseline method for creating the word align-
ment is to create the GIZA++ alignments in both
directions and then to combine both alignments
using a heuristic, e.g. grow-diag-final-and heuris-
tic, as provided by the Moses Toolkit. In some
of the submitted systems we used a discrimina-
tive word alignment model (DWA) to generate
the alignments as described in Niehues and Vogel
(2008) instead. This model is trained on a small
amount of hand-aligned data and uses the lexical
probability as well as the fertilities generated by
the GIZA++ Toolkit and POS information. We
used all local features, the GIZA and indicator fer-
tility features as well as first order features for 6
directions. The model was trained in three steps,
first using the maximum likelihood optimization
and afterwards it was optimized towards the align-
ment error rate. For more details see Niehues and
Vogel (2008).
4.2 Phrase Table Smoothing
The relative frequencies of the phrase pairs are a
very important feature of the translation model,
but they often overestimate rare phrase pairs.
Therefore, the raw relative frequency estimates
found in the phrase translation tables are smoothed
by applying modified Kneser-Ney discounting as
described in Foster et al (2006).
4.3 Lattice Phrase Extraction
For the test sentences the POS-based reordering
allows us to change the word order in the source
sentence, so that the sentence can be translated
more easily. But this approach does not reorder
the training sentences. This may cause problems
for phrase extraction, especially for long-range re-
orderings. For example, if the English verb is
aligned to both parts of the German verb, this
phrase can not be extracted, since it is not contin-
uous on the German side. In the case of German
as source language, the phrase could be extracted
if we also reorder the training corpus.
Therefore, we build lattices that encode the
different reorderings for every training sentence.
Then we can not only extract phrase pairs from the
monotone source path, but also from the reordered
paths. So it would be possible to extract the ex-
ample mentioned before, if both parts of the verb
were put together by a reordering rule. To limit
the number of extracted phrase pairs, we extract
a source phrase only once per sentence even if it
may be found on different paths. Furthermore, we
do not use the weights in the lattice.
If we use the same rules as for the test sets,
the lattice would be so big that the number of ex-
tracted phrase pairs would be still too high. As
mentioned before, the word reordering is mainly
81
a problem at the phrase extraction stage if one
word is aligned to two words which are far away
from each other in the sentence. Therefore, the
short-range reordering rules do not help much in
this case. So, only the long-range reordering rules
were used to generate the lattice for the training
corpus. This already leads to an increase of the
number of source phrases in the filtered phrase ta-
ble from 724K to 971K. The number of phrase
pairs grows from 5.1M to 6.7M.
4.4 Phrase Table Adaption
For most of the different tasks there was a huge
amount of parallel out-of-domain training data
available, but only a much smaller amount of in-
domain training data. Therefore, we tried to adapt
our system to the in-domain data. We want to
make use of the big out-of-domain data, but do
not want to lose the information encoded in the in-
domain data.
To achieve this, we built an additional phrase
table trained only on the in-domain data. Since
the word alignment does not depend heavily on the
domain we used the same word alignment. Then
we combined both phrase tables in the following
way. A phrase pair with features ? from the first
phrase table is added to the combined one with
features < ?, 1 >, where 1 is a vector of ones with
length equal to the number of features in the other
phrase table. The phrase pairs of the other phrase
table were added with the features < 1, ? >.
5 Results
We submitted system translations for the English-
German, German-English, English-French and
French-English task. Their performance is mea-
sured applying the BLEU metric. All BLEU
scores are computed on the lower-cased transla-
tions.
5.1 English-German
The system translating from English to German
was trained on the data described in Section 2.1.
The first system already uses the POS-based re-
ordering model for short-range reorderings. The
results of the different systems are shown in Ta-
ble 1.
We could improve the translation quality on the
test set by using the smoothed relative frequen-
cies in the phrase table as described before and
by adapting the phrase table. Then we used the
discriminative word alignment to generate a new
word alignment. For the training of the model
we used 500 hand-aligned sentences from the Eu-
roparl corpus. By training a translation model
based on this word alignment we could improve
the translation quality further. At last we added
the model for long-range reorderings, which per-
forms best on the test set.
The improvement achieved by smoothing is sig-
nificant at a level of 5%, the remaining changes are
not significant on their own. In all language pairs,
the problem occurs that some features do not lead
to an improvement on the development set, but on
the test set. One reason for this may be that the
development set is quite small.
Table 1: Translation results for English-German
(BLEU Score)
System Dev Test
Short-range 13.96 14.99
+ Smoothing 14.36 15.38
+ Adaptation 13.96 15.44
+ Discrim. WA 14.45 15.61
+ Long-range reordering 14.58 15.70
5.2 German-English
The German-English system was trained on the
same data as the English-German except that we
perform compound splitting as an additional pre-
processing step. The compound splitting was
done with the frequency-based method described
in Koehn et al (2003). For this language di-
rection, the initial system already uses phrase ta-
ble smoothing, adaptation and discriminative word
alignment, in addition to the techniques of the
English-German baseline system. The results are
shown in Table 2.
For this language pair, we could improve the
translation quality, first, by adding the long-range
reordering model. Further improvements could be
achieved by using lattice phrase extraction as de-
scribed before.
5.3 English-French
For creating the English-French translations, first,
the baseline system as described in Section 2
was used. This baseline was then augmented
with phrase table smoothing, short-range word re-
ordering and phrase table adaptation as described
above. In addition, the adapted phrase table was
82
Table 2: Translation results for German-English
(BLEU Score)
System Dev Test
Initial System 20.52 22.01
+ Long-range reordering 21.04 22.36
+ Lattice phrase extraction 20.69 22.64
postprocessed such that phrase table entries in-
clude the same amount of punctuation marks, es-
pecially quotation marks, in both source and tar-
get phrase. In contrast to the English?German
language pairs, the word reordering required
in English?French translations are restricted to
rather local word shifts which can be covered by
the short-range reordering feature. Applying addi-
tional long-range reordering is scarcely expected
to yield further improvements for these language
pairs and was not applied specifically in this task.
Table 3 shows the results of the system variants.
Table 3: Translation results for English-French
(BLEU Score)
System Dev Test
Baseline 20.97 20.87
+ Smoothing 21.42 21.32
+ Short-range reordering 20.79 22.26
+ Adaptation 21.05 21.97
+ cleanPT 21.50 21.98
Both on development and test set, smoothing
the probabilities in the phrase table resulted in an
increase of nearly 0.5 BLEU points. Applying
short-range word reordering did not lead to an im-
provement on the development set. However, the
increase in BLEU on the test set is substantial. The
opposite is the case when adapting the phrase ta-
ble: While phrase table adaptation improves the
translation quality on the development set, adapta-
tion leads to lower scores on the test set.
Thus, the system configuration that performed
best on the test set applies phrase table smoothing
and short-range word reordering. For creating the
translations for our submission, this configuration
was used.
5.4 French-English
For the French-English task, similar experiments
have been conducted. With respect to the base-
line system, improvements in translation quality
could be measured when applying phrase table
smoothing. An increase of 0.43 BLEU points was
achieved using short-range word reordering. Ad-
ditional experiments with adapting the phrase ta-
ble to the domain of the test set led to further im-
provement. Submissions for the shared task were
created using the system including all mentioned
features.
Table 4: Translation results for French-English
(BLEU Score)
System Dev Test
Baseline 21.29 22.41
+ Smoothing 21.55 22.59
+ Short-range reordering 22.55 23.02
+ Adaptation 21.72 23.20
+ cleanPT 22.60 23.21
6 Conclusions
We have presented our system for the WMT?09
Shared Translation Task. The submissions for the
language pairs English-German, German-English,
English-French and French-English have been
created by the STTK decoder applying different
additional methods for each individual language
pair to enhance translation quality.
Word reordering models covering short-
range reordering for the English?French and
English?German and long-range reordering for
English?German respectively proved to result in
better translations.
Smoothing the phrase probabilities in the phrase
table also increased the scores in all cases, while
adapting the phrase table to the test domain only
showed a positive influence on translation quality
in some of our experiments. Further tuning of the
adaptation procedure could help to clarify the ben-
efit of this method.
Using discriminative word alignment as an
alternative to performing word alignment with
GIZA++ did also improve the systems translating
between English and German. Future experiments
will be conducted applying discriminative word
alignment also in the English?French systems.
Acknowledgments
This work was partly supported by Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
83
References
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proc. of Empirical Methods in
Natural Language Processing. Sydney, Australia.
Philipp Koehn, Franz Josef Och and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
HLT/NAACL 2003. Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of Second ACL Workshop on Statistical Ma-
chine Translation. Prague, Czech Republic.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation. Columbus, OH, USA.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proc. of Forth ACL Workshop on Statistical Machine
Translation. Athens, Greece.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI. Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing. Manchester, UK.
Ashish Venugopal, Andreas Zollman and Alex Waibel.
2005. Training and Evaluation Error Minimiza-
tion Rules for Statistical Machine Translation. In
Proc. of ACL 2005, Workshop on Data-drive Ma-
chine Translation and Beyond (WPT-05). Ann Ar-
bor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In NLP-KE?03. Beijing, China.
84
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 138?142,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Karlsruhe Institute for Technology Translation System for the
ACL-WMT 2010
Jan Niehues, Teresa Herrmann, Mohammed Mediani and Alex Waibel
Karlsruhe Instiute of Technolgy
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes our phrase-based Sta-
tistical Machine Translation (SMT) sys-
tem for the WMT10 Translation Task. We
submitted translations for the German to
English and English to German transla-
tion tasks. Compared to state-of-the-art
phrase-based systems we preformed addi-
tional preprocessing and used a discrim-
inative word alignment approach. The
word reordering was modeled using POS
information and we extended the transla-
tion model with additional features.
1 Introduction
In this paper we describe the systems that we
built for our participation in the Shared Trans-
lation Task of the ACL 2010 Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR. Our translations are generated using
a state-of-the-art phrase-based translation system
and applying different extensions and modifica-
tions including Discriminative Word Alignment,
a POS-based reordering model and bilingual lan-
guage models using POS and stem information.
Depending on the source and target languages,
the proposed models differ in their benefit for the
translation task and also expose different correl-
ative effects. The Sections 2 to 4 introduce the
characteristics of the baseline system and the sup-
plementary models. In Section 5 we present the
performance of the system variants applying the
different models and chose the systems used for
creating the submissions for the English-German
and German-English translation task. Section 6
draws conclusions and suggests directions for fu-
ture work.
2 Baseline System
The baseline systems for the translation directions
German-English and English-German are both de-
veloped using Discriminative Word Alignment
(Niehues and Vogel, 2008) and the Moses Toolkit
(Koehn et al, 2007) for extracting phrase pairs
and generating the phrase table from the discrimi-
native word alignments. The difficult reordering
between German and English was modeled us-
ing POS-based reordering rules. These rules were
learned using a word-aligned parallel corpus. The
POS tags for the reordering models are generated
using the TreeTagger (Schmid, 1994) for all lan-
guages.
Translation is performed by the STTK Decoder
(Vogel, 2003) and all systems are optimized to-
wards BLEU using Minimum Error Rate Training
as proposed in Venugopal et al (2005).
2.1 Training, Development and Test Data
We used the data provided for the WMT for train-
ing, optimizing and testing our systems: Our
training corpus consists of Europarl and News
Commentary data, for optimization we use new-
stest2008 as development set and newstest2009 as
test set.
The baseline language models are trained on
the target language part of the Europarl and News
Commentary corpora. Additional, bigger lan-
guage models were trained on monolingual cor-
pora. For both systems the News corpus was used
while an English language model was also trained
on the even bigger Gigaword corpus.
2.2 Preprocessing
The training data was preprocessed before used for
training. In this step different normalizations were
done like mapping different types of quotes. In
the end the first word of every sentence was smart-
cased.
138
For the German text, additional preprocessing
steps were applied. First, the older German data
uses the old German orthography whereas the
newer parts of the corpus use the new German
orthography. We tried to normalize the text by
converting the whole text to the new German or-
thography. In a first step, we search for words that
are only correct according to the old writing rules.
Therefore, we selected all words in the corpus, that
are correct according to the hunspell lexicon1 us-
ing the old rules, but not correct according to the
hunspell lexicon using the new rules. In a second
step we tried to find the correct spelling according
to the new rules. We first applied rules describing
how words changed from one spelling system to
the other, for example replacing ??? by ?ss?. If the
new word is a correct word according to the hun-
spell lexicon using the new spelling rules, we map
the words.
When translating from German to English, we
apply compound splitting as described in Koehn
and Knight (2003) to the German corpus.
As a last preprocessing step we remove sen-
tences that are too long and empty lines to obtain
the final training corpus.
3 Word Reordering Model
Reordering was applied on the source side prior
to decoding through the generation of lattices en-
coding possible reorderings of each source sen-
tence that better match the word sequence in the
target language. These possible reorderings were
learned based on the POS of the source language
words in the training corpus and the information
about alignments between source and target lan-
guage words in the corpus. For short-range re-
orderings, continuous reordering rules were ap-
plied to the test sentences (Rottmann and Vogel,
2007). To model the long-range reorderings be-
tween German and English, different types of non-
continuous reordering rules were applied depend-
ing on the translation direction. (Niehues and
Kolss, 2009). When translating from English to
German, most of the changes in word order con-
sist in a shift to the right while typical word shifts
in German to English translations take place in the
reverse direction.
1http://hunspell.sourceforge.net/
4 Translation Model
The translation model was trained on the parallel
corpus and the word alignment was generated by
a discriminative word alignment model, which is
described below. The phrase table was trained us-
ing the Moses training scripts, but for the German
to English system we used a different phrase ex-
traction method described in detail in Section 4.2.
In addition, we applied phrase table smoothing as
described in Foster et al (2006). Furthermore, we
extended the translation model by additional fea-
tures for unaligned words and introduced bilingual
language models.
4.1 Word Alignment
In most phrase-based SMT systems the heuristic
grow-diag-final-and is used to combine the align-
ments generated by GIZA++ from both direc-
tions. Then these alignments are used to extract
the phrase pairs.
We used a discriminative word alignment model
(DWA) to generate the alignments as described in
Niehues and Vogel (2008) instead. This model is
trained on a small amount of hand-aligned data
and uses the lexical probability as well as the fer-
tilities generated by the PGIZA++2 Toolkit and
POS information. We used all local features, the
GIZA and indicator fertility features as well as
first order features for 6 directions. The model was
trained in three steps, first using maximum likeli-
hood optimization and afterwards it was optimized
towards the alignment error rate. For more details
see Niehues and Vogel (2008).
4.2 Lattice Phrase Extraction
In translations from German to English, we often
have the case that the English verb is aligned to
both parts of the German verb. Since this phrase
pair is not continuous on the German side, it can-
not be extracted. The phrase could be extracted, if
we also reorder the training corpus.
For the test sentences the POS-based reordering
allows us to change the word order in the source
sentence so that the sentence can be translated
more easily. If we apply this also to the train-
ing sentences, we would be able to extract the
phrase pairs for originally discontinuous phrases
and could apply them during translation of the re-
ordered test sentences.
2http://www.cs.cmu.edu/?qing/
139
Therefore, we build lattices that encode the dif-
ferent reorderings for every training sentence, as
described in Niehues et al (2009). Then we can
not only extract phrase pairs from the monotone
source path, but also from the reordered paths. So
it would be possible to extract the example men-
tioned before, if both parts of the verb were put
together by a reordering rule. To limit the num-
ber of extracted phrase pairs, we extract a source
phrase only once per sentence even if it may be
found on different paths. Furthermore, we do not
use the weights in the lattice.
If we used the same rules as for reordering the
test sets, the lattice would be so big that the num-
ber of extracted phrase pairs would be still too
high. As mentioned before, the word reordering
is mainly a problem at the phrase extraction stage
if one word is aligned to two words which are
far away from each other in the sentence. There-
fore, the short-range reordering rules do not help
much in this case. So, only the long-range reorder-
ing rules were used to generate the lattices for the
training corpus.
4.3 Unaligned Word Feature
Guzman et al (2009) analyzed the role of the word
alignment in the phrase extraction process. To bet-
ter model the relation between word alignment and
the phrase extraction process, they introduced two
new features into the log-linear model. One fea-
ture counts the number of unaligned words on the
source side and the other one does the same for the
target side. Using these additional features they
showed improvements on the Chinese to English
translation task. In order to investigate the impact
on closer related languages like English and Ger-
man, we incorporated those two features into our
systems.
4.4 Bilingual Word language model
Motivated by the improvements in translation
quality that could be achieved by using the n-gram
based approach to statistical machine translation,
for example by Allauzen et al (2009), we tried
to integrate a bilingual language model into our
phrase-based translation system.
To be able to integrate the approach easily into a
standard phrase-based SMT system, a token in the
bilingual language model is defined to consist of
a target word and all source words it is aligned to.
The tokens are ordered according to the target lan-
guage word order. Then the additional tokens can
be introduced into the decoder as an additional tar-
get factor. Consequently, no additional implemen-
tation work is needed to integrate this feature.
If we have the German sentence Ich bin nach
Hause gegangen with the English translation I
went home, the resulting bilingual text would look
like this: I Ich went bin gegangen home Hause.
As shown in the example, one problem with this
approach is that unaligned source words are ig-
nored in the model. One solution could be to have
a second bilingual text ordered according to the
source side. But since the target sentence and not
the source sentence is generated from left to right
during decoding, the integration of a source side
language model is more complex. Therefore, as
a first approach we only used a language model
based on the target word order.
4.5 Bilingual POS language model
The main advantage of POS-based information
is that there are less data sparsity problems and
therefore a longer context can be considered. Con-
sequently, if we want to use this information in the
translation model of a phrase-based SMT system,
the POS-based phrase pairs should be longer than
the word-based ones. But this is not possible in
many decoders or it leads to additional computa-
tion overhead.
If we instead use a bilingual POS-based lan-
guage model, the context length of the language
model is independent from the other models. Con-
sequently, a longer context can be considered for
the POS-based language model than for the word-
based bilingual language model or the phrase
pairs.
Instead of using POS-based information, this
approach can also be applied with other additional
linguistic word-level information like word stems.
5 Results
We submitted translations for English-German
and German-English for the Shared Translation
Task. In the following we present the experiments
we conducted for both translation directions ap-
plying the aforementioned models and extensions
to the baseline systems. The performance of each
individual system configuration was measured ap-
plying the BLEU metric. All BLEU scores are cal-
culated on the lower-cased translation hypotheses.
The individual systems that were used to create the
submission are indicated in bold.
140
5.1 English-German
The baseline system for English-German applies
short-range reordering rules and discriminative
word alignment. The language model is trained
on the News corpus. By expanding the coverage
of the rules to enable long-range reordering, the
score on the test set could be slightly improved.
We then combined the target language part of the
Europarl and News Commentary corpora with the
News corpus to build a bigger language model
which resulted in an increase of 0.11 BLEU points
on the development set and an increase of 0.25
points on the test set. Applying the bilingual lan-
guage model as described above led to 0.04 points
improvement on the test set.
Table 1: Translation results for English-German
(BLEU Score)
System Dev Test
Baseline 15.30 15.40
+ Long-range Reordering 15.25 15.44
+ EPNC LM 15.36 15.69
+ bilingual Word LM 15.37 15.73
+ bilingual POS LM 15.42 15.67
+ unaligned Word Feature 15.65 15.66
+ bilingual Stem LM 15.57 15.74
This system was used to create the submis-
sion to the Shared Translation Task of the WMT
2010. After submission we performed additional
experiments which only led to inconclusive re-
sults. Adding the bilingual POS language model
and introducing the unaligned word feature to the
phrase table only improved on the development
set, while the scores on the test set decreased. A
third bilingual language model based on stem in-
formation again only showed noteworthy effects
on the development set.
5.2 German-English
For the German to English translation system,
the baseline system already uses short-range re-
ordering rules and the discriminative word align-
ment. This system applies only the language
model trained on the News corpus. By adding the
possibility to model long-range reorderings with
POS-based rules, we could improve the system by
0.6 BLEU points. Adding the big language model
using also the English Gigaword corpus we could
improve by 0.3 BLEU points. We got an addi-
tional improvement by 0.1 BLEU points by adding
lattice phrase extraction.
Both the word-based and POS-based bilingual
language model could improve the translation
quality measured in BLEU. Together they im-
proved the system performance by 0.2 BLEU
points.
The best results could be achieved by using also
the unaligned word feature for source and target
words leading to the best performance on the test
set (22.09).
Table 2: Translation results for German-English
(BLEU Score)
System Dev Test
Baseline 20.94 20.83
+ Long-range Reordering 21.52 21.43
+ Gigaword LM 21.90 21.71
+ Lattice Phrase Extraction 21.94 21.81
+ bilingual Word LM 21.94 21.87
+ bilingual POS LM 22.02 22.05
+ unaligned Word Feature 22.09 22.09
6 Conclusions
For our participation in the WMT 2010 we built
translation systems for German to English and En-
glish to German. We addressed to the difficult
word reordering when translating from or to Ger-
man by using POS-based reordering rules during
decoding and by using lattice-based phrase extrac-
tion during training. By applying those methods
we achieved substantially better results for both
translation directions.
Furthermore, we tried to improve the translation
quality by introducing additional features to the
translation model. On the one hand we included
bilingual language models based on different word
factors into the log-linear model. This led to very
slight improvements which differed also with re-
spect to language and data set. We will investigate
in the future whether further improvements are
achievable with this approach. On the other hand
we included the unaligned word feature which has
been applied successfully for other language pairs.
The improvements we could gain with this method
are not as big as the ones reported for other lan-
guages, but still the performance of our systems
could be improved using this feature.
141
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep Crego, Aure?lien Max, and
Franc?ois Yvon. 2009. LIMSI?s statistical trans-
lation system for WMT?09. In Fourth Workshop
on Statistical Machine Translation (WMT 2009),
Athens, Greece.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Conference on Empirical
Methods in Natural Language Processing (EMNLP
2006), Sydney, Australia.
Francisco Guzman, Qin Gao, and Stephan Vogel.
2009. Reassessment of the Role of Phrase Extrac-
tion in PBSMT. In MT Summit XII, Ottawa, Ontario,
Canada.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In ACL 2007, Demonstration Session,
Prague, Czech Republic, June 23.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
142
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 198?206,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Wider Context by Using Bilingual Language Models in Machine Translation
Jan Niehues1, Teresa Herrmann1, Stephan Vogel2 and Alex Waibel1,2
1Institute for Anthropomatics, KIT - Karlsruhe Institute of Technology, Germany
2 Language Techonolgies Institute, Carnegie Mellon University, USA
1firstname.lastname@kit.edu 2lastname@cs.cmu.edu
Abstract
In past Evaluations for Machine Translation of
European Languages, it could be shown that
the translation performance of SMT systems
can be increased by integrating a bilingual lan-
guage model into a phrase-based SMT system.
In the bilingual language model, target words
with their aligned source words build the to-
kens of an n-gram based language model. We
analyzed the effect of bilingual language mod-
els and show where they could help to bet-
ter model the translation process. We could
show improvements of translation quality on
German-to-English and Arabic-to-English. In
addition, for the Arabic-to-English task, train-
ing an extra bilingual language model on the
POS tags instead of the surface word forms
led to further improvements.
1 Introduction
In many state-of-the art SMT systems, the phrase-
based (Koehn et al, 2003) approach is used. In
this approach, instead of building the translation by
translating word by word, sequences of source and
target words, so-called phrase pairs, are used as the
basic translation unit. A table of correspondences
between source and target phrases forms the transla-
tion model in this approach. Target language fluency
is modeled by a language model storing monolin-
gual n-gram occurrences. A log-linear combination
of these main models as well as additional features
is used to score the different translation hypotheses.
Then the decoder searches for the translation with
the highest score.
A different approach to SMT is to use a stochas-
tic finite state transducer based on bilingual n-
grams (Casacuberta and Vidal, 2004). This ap-
proach was for example successfully applied by Al-
lauzen et al (2010) on the French-English trans-
lation task. In this so-called n-gram approach the
translation model is trained by using an n-gram lan-
guage model of pairs of source and target words,
called tuples. While the phrase-based approach cap-
tures only bilingual context within the phrase pairs,
in the n-gram approach the n-gram model trained on
the tuples is used to capture bilingual context be-
tween the tuples. As in the phrase-based approach,
the translation model can also be combined with ad-
ditional models like, for example, language models
using log-linear combination.
Inspired by the n-gram-based approach, we in-
troduce a bilingual language model that extends
the translation model of the phrase-based SMT ap-
proach by providing bilingual word context. In ad-
dition to the bilingual word context, this approach
enables us also to integrate a bilingual context based
on part of speech (POS) into the translation model.
When using phrase pairs it is complicated to use
different kinds of bilingual contexts, since the con-
text of the POS-based phrase pairs should be bigger
than the word-based ones to make the most use of
them. But there is no straightforward way to inte-
grate phrase pairs of different lengths into the trans-
lation model in the phrase-based approach, while it
is quite easy to use n-gram models with different
context lengths on the tuples. We show how we can
use bilingual POS-based language models to capture
longer bilingual context in phrase-based translation
198
systems.
This paper is structured in the following way: In
the next section, we will present some related work.
Afterwards, in Section 3, a motivation for using the
bilingual language model will be given. In the fol-
lowing section the bilingual language model is de-
scribed in detail. In Section 5, the results and an
analysis of the translation results is given, followed
by a conclusion.
2 Related Work
The n-gram approach presented in Mari?o et al
(2006) has been derived from the work of Casacu-
berta and Vidal (2004), which used finite state trans-
ducers for statistical machine translation. In this ap-
proach, units of source and target words are used as
basic translation units. Then the translation model is
implemented as an n-gram model over the tuples. As
it is also done in phrase-based translations, the dif-
ferent translations are scored by a log-linear combi-
nation of the translation model and additional mod-
els.
Crego and Yvon (2010) extended the approach to
be able to handle different word factors. They used
factored language models introduced by Bilmes and
Kirchhoff (2003) to integrate different word factors
into the translation process. In contrast, we use a
log-linear combination of language models on dif-
ferent factors in our approach.
A first approach of integrating the idea presented
in the n-gram approach into phrase-based machine
translation was described in Matusov et al (2006).
In contrast to our work, they used the bilingual units
as defined in the original approach and they did not
use additional word factors.
Hasan et al (2008) used lexicalized triplets to in-
troduce bilingual context into the translation pro-
cess. These triplets include source words from out-
side the phrase and form and additional probability
p(f |e, e?) that modifies the conventional word prob-
ability of f given e depending on trigger words e? in
the sentence enabling a context-based translation of
ambiguous phrases.
Other approaches address this problem by inte-
grating word sense disambiguation engines into a
phrase-based SMT system. In Chan and Ng (2007)
a classifier exploits information such as local col-
locations, parts-of-speech or surrounding words to
determine the lexical choice of target words, while
Carpuat and Wu (2007) use rich context features
based on position, syntax and local collocations to
dynamically adapt the lexicons for each sentence
and facilitate the choice of longer phrases.
In this work we present a method to extend the
locally limited context of phrase pairs and n-grams
by using bilingual language models. We keep the
phrase-based approach as the main SMT framework
and introduce an n-gram language model trained in a
similar way as the one used in the finite state trans-
ducer approach as an additional feature in the log-
linear model.
3 Motivation
To motivate the introduction of the bilingual lan-
guage model, we will analyze the bilingual context
that is used when selecting the target words. In a
phrase-based system, this context is limited by the
phrase boundaries. No bilingual information outside
the phrase pair is used for selecting the target word.
The effect can be shown in the following example
sentence:
Ein gemeinsames Merkmal aller extremen
Rechten in Europa ist ihr Rassismus
und die Tatsache, dass sie das Einwan-
derungsproblem als politischen Hebel be-
nutzen.
Using our phrase-based SMT system, we get the
following segmentation into phrases on the source
side: ein gemeinsames, Merkmal, aller, extremen
Rechten. That means, that the translation of Merk-
mal is not influenced by the source words gemein-
sames or aller.
However, apart from this segmentation, other
phrases could have been conceivable for building a
translation:
ein, ein gemeinsames, ein gemeinsames Merk-
mal, gemeinsames, gemeinsames Merkmal, Merk-
mal aller, aller, extremen, extremen Rechten and
Rechten.
As shown in Figure 1 the translation for the
first three words ein gemeinsames Merkmal into a
common feature can be created by segmenting it
into ein gemeinsames and Merkmal as done by the
199
Figure 1: Alternative Segmentations
phrase-based system or by segmenting it into ein and
gemeinsames Merkmal. In the phrase-based system,
the decoder cannot make use of the fact that both
segmentation variants lead to the same translation,
but has to select one and use only this information
for scoring the hypothesis.
Consequently, if the first segmentation is cho-
sen, the fact that gemeinsames is translated to com-
mon does effect the translation of Merkmal only by
means of the language model, but no bilingual con-
text can be carried over the segmentation bound-
aries.
To overcome this drawback of the phrase-based
approach, we introduce a bilingual language model
into the phrase-based SMT system. Table 1 shows
the source and target words and demonstrates how
the bilingual phrases are constructed and how the
source context stays available over segment bound-
aries in the calculation of the language model score
for the sentence. For example, when calculating the
language model score for the word feature P ( fea-
ture_Merkmal | common_gemeinsames) we can see
that through the bilingual tokens not only the previ-
ous target word but also the previous source word is
known and can influence the translation even though
it is in a different segment.
4 Bilingual Language Model
The bilingual language model is a standard n-gram-
based language model trained on bilingual tokens in-
stead of simple words. These bilingual tokens are
motivated by the tuples used in n-gram approaches
to machine translation. We use different basic units
for the n-gram model compared to the n-gram ap-
proach, in order to be able to integrate them into a
phrase-based translation system.
In this context, a bilingual token consists of a tar-
get word and all source words that it is aligned to.
More formally, given a sentence pair eI1 = e1...eI
and fJ1 = f1...fJ and the corresponding word align-
ment A = {(i, j)} the following tokens are created:
tj = {fj} ? {ei|(i, j) ? A} (1)
Therefore, the number of bilingual tokens in a
sentence equals the number of target words. If a
source word is aligned to two target words like the
word aller in the example sentence, two bilingual to-
kens are created: all_aller and the_aller. If, in con-
trast, a target word is aligned to two source words,
only one bilingual token is created consisting of the
target word and both source words.
The existence of unaligned words is handled in
the following way. If a target word is not aligned
to any source word, the corresponding bilingual to-
ken consists only of the target word. In contrast, if a
source word is not aligned to any word in the target
language sentence, this word is ignored in the bilin-
gual language model.
Using this definition of bilingual tokens the trans-
lation probability of source and target sentence and
the word alignment is then defined by:
p(eI1, f
J
1 , A) =
J?
j=1
P (tj |tj?1...tj?n) (2)
This probability is then used in the log-linear com-
bination of a phrase-based translation system as an
additional feature. It is worth mentioning that al-
though it is modeled like a conventional language
model, the bilingual language model is an extension
to the translation model, since the translation for the
source words is modeled and not the fluency of the
target text.
To train the model a corpus of bilingual tokens can
be created in a straightforward way. In the genera-
tion of this corpus the order of the target words de-
fines the order of the bilingual tokens. Then we can
use the common language modeling tools to train
the bilingual language model. As it was done for
the normal language model, we used Kneser-Ney
smoothing.
4.1 Comparison to Tuples
While the bilingual tokens are motivated by the tu-
ples in the n-gram approach, there are quite some
differences. They are mainly due to the fact that the
200
Source Target Bi-word LM Prob
ein a a_ein P(a_ein | <s>)
gemeinsames common common_gemeinsames P(common_gemeinsames | a_ein, <s>)
Merkmal feature feature_Merkmal P(feature_Merkmal | common_gemeinsames)
of of_ P(of_ | feature_Merkmal)
aller all all_aller P(all_aller | of_)
aller the the_aller P(the_aller | all_aller, of_)
extremen extreme extreme_extremen P(extreme_extremen)
Rechten right right_Rechten P(right_Rechten | extreme_extremen)
Table 1: Example Sentence: Segmentation and Bilingual Tokens
tuples are also used to guide the search in the n-gram
approach, while the search in the phrase-based ap-
proach is guided by the phrase pairs and the bilin-
gual tokens are only used as an additional feature in
scoring.
While no word inside a tuple can be aligned to
a word outside the tuple, the bilingual tokens are
created based on the target words. Consequently,
source words of one bilingual token can also be
aligned to target words inside another bilingual to-
ken. Therefore, we do not have the problems of em-
bedded words, where there is no independent trans-
lation probability.
Since we do not create a a monotonic segmenta-
tion of the bilingual sentence, but only use the seg-
mentation according to the target word order, it is
not clear where to put source words, which have no
correspondence on the target side. As mentioned be-
fore, they are ignored in the model.
But an advantage of this approach is that we have
no problem handling unaligned target words. We
just create bilingual tokens with an empty source
side. Here, the placing order of the unaligned tar-
get words is guided by the segmentation into phrase
pairs.
Furthermore, we need no additional pruning of
the vocabulary due to computation cost, since this is
already done by the pruning of the phrase pairs. In
our phrase-based system, we allow only for twenty
translations of one source phrase.
4.2 Comparison to Phrase Pairs
Using the definition of the bilingual language model,
we can again have a look at the introductory example
sentence. We saw that when translating the phrase
ein gemeinsames Merkmal using a phrase-based sys-
tem, the translation of gemeinsames into common
can only be influenced by either the preceeding ein
# a or by the succeeding Merkmal # feature, but
not by both of them at the same time, since either
the phrase ein gemeinsames or the phrase gemein-
sames Merkmal has to be chosen when segmenting
the source sentence for translation. If we now look
at the context that can be used when translating this
segment applying the bilingual language model, we
see that the translation of gemeinsames into com-
mon is on the one hand influenced by the translation
of the token ein # a within the bilingual language
model probability P (common_gemeinsames | a_ein,
<s>).
On the other hand, it is also influenced by the
translation of the word Merkmal into feature en-
coded into the probability P (feature_Merkmal |
common_gemeinsames). In contrast to the phrase-
based translation model, this additional model is ca-
pable of using context information from both sides
to score the translation hypothesis. In this way,
when building the target sentence, the information
of aligned source words can be considered even be-
yond phrase boundaries.
4.3 POS-based Bilingual Language Models
When translating with the phrase-based approach,
the decoder evaluates different hypotheses with dif-
ferent segmentations of the source sentence into
phrases. The segmentation depends on available
phrase pair combinations but for one hypothesis
translation the segmentation into phrases is fixed.
This leads to problems, when integrating parallel
POS-based information. Since the amount of differ-
201
ent POS tags in a language is very small compared to
the number of words in a language, we could man-
age much longer phrase pairs based on POS tags
compared to the possible length of phrase pairs on
the word level.
In a phrase-based translation system the average
phrase length is often around two words. For POS
sequences, in contrast, sequences of 4 tokens can
often be matched. Consequently, this information
can only help, if a different segmentation could be
chosen for POS-based phrases and for word-based
phrases. Unfortunately, there is no straightforward
way to integrate this into the decoder.
If we now look at how the bilingual language
model is applied, it is much easier to integrate the
POS-based information. In addition to the bilin-
gual token for every target word we can generate a
bilingual token based on the POS information of the
source and target words. Using this bilingual POS
token, we can train an additional bilingual POS-
based language model and apply it during transla-
tion. In this case it is no longer problematic if the
context of the POS-based bilingual language model
is longer than the one based on the word informa-
tion, because word and POS sequences are scored
separately by two different language models which
cover different n-gram lengths.
The training of the bilingual POS language model
is straightforward. We can build the corpus of bilin-
gual POS tokens based on the parallel corpus of
POS tags generated by running a POS tagger over
both source and target side of the initial parallel cor-
pus and the alignment information for the respective
words in the text corpora.
During decoding, we then also need to know the
POS tag for every source and target word. Since
we build the sentence incrementally, we cannot use
the tagger directly. Instead, we store also the POS
source and target sequences during the phrase ex-
traction. When creating the bilingual phrase pair
with POS information, there might be different pos-
sibilities of POS sequences for the source and target
phrases. But we keep only the most probable one for
each phrase pair. For the Arabic-to-English trans-
lation task, we compared the generated target tags
with the tags created by the tagger on the automatic
translations. They are different on less than 5% of
the words.
Using the alignment information as well as the
source and target POS sequences we can then create
the POS-based bilingual tokens for every phrase pair
and store it in addition to the normal phrase pairs.
At decoding time, the most frequent POS tags in the
bilingual phrases are used as tags for the input sen-
tence and the translation is done based on the bilin-
gual POS tokens built from these tags together with
their alignment information.
5 Results
We evaluated and analyzed the influence of the bilin-
gual language model on different languages. On
the one hand, we measured the performance of the
bilingual language model on German-to-English on
the News translation task. On the other hand, we
evaluated the approach on the Arabic-to-English di-
rection on News and Web data. Additionally, we
present the impact of the bilingual language model
on the English-to-German, German-to-English and
French-to-English systems with which we partici-
pated in the WMT 2011.
5.1 System Description
The German-to-English translation system was
trained on the European Parliament corpus, News
Commentary corpus and small amounts of addi-
tional Web data. The data was preprocessed and
compound splitting was applied. Afterwards the dis-
criminative word alignment approach as described
in (Niehues and Vogel, 2008) was applied to gener-
ate the alignments between source and target words.
The phrase table was built using the scripts from the
Moses package (Koehn et al, 2007). The language
model was trained on the target side of the paral-
lel data as well as on additional monolingual News
data. The translation model as well as the language
model was adapted towards the target domain in a
log-linear way.
The Arabic-to-English system was trained us-
ing GALE Arabic data, which contains 6.1M sen-
tences. The word alignment is generated using
EMDC, which is a combination of a discriminative
approach and the IBM Models as described in Gao
et al (2010). The phrase table is generated using
Chaski as described in Gao and Vogel (2010). The
language model data we trained on the GIGAWord
202
V3 data plus BBN English data. After splitting the
corpus according to sources, individual models were
trained. Then the individual models were interpo-
lated to minimize the perplexity on the MT03/MT04
data.
For both tasks the reordering was performed as a
preprocessing step using POS information from the
TreeTagger (Schmid, 1994) for German and using
the Amira Tagger (Diab, 2009) for Arabic. For Ara-
bic the approach described in Rottmann and Vogel
(2007) was used covering short-range reorderings.
For the German-to-English translation task the ex-
tended approach described in Niehues et al (2009)
was used to cover also the long-range reorderings
typical when translating between German and En-
glish.
For both directions an in-house phrase-based de-
coder (Vogel, 2003) was used to generate the transla-
tion hypotheses and the optimization was performed
using MER training. The performance on the test-
sets were measured in case-insensitive BLEU and
TER scores.
5.2 German to English
We evaluated the approach on two different test sets
from the News Commentary domain. The first con-
sists of 2000 sentences with one reference. It will
be referred to as Test 1. The second test set consists
of 1000 sentences with two references and will be
called Test 2.
5.2.1 Translation Quality
In Tables 2 and 3 the results for translation per-
formance on the German-to-English translation task
are summarized.
As it can been seen, the improvements of transla-
tion quality vary considerably between the two dif-
ferent test sets. While using the bilingual language
model improves the translation by only 0.15 BLEU
and 0.21 TER points on Test 1, the improvement on
Test 2 is nearly 1 BLEU point and 0.5 TER points.
5.2.2 Context Length
One intention of using the bilingual language
model is its capability to capture the bilingual con-
texts in a different way. To see, whether additional
bilingual context is used during decoding, we ana-
lyzed the context used by the phrase pairs and by
the n-gram bilingual language model.
However, a comparison of the different context
lengths is not straightforward. The context of an n-
gram language model is normally described by the
average length of applied n-grams. For phrase pairs,
normally the average target phrase pair length (avg.
Target PL) is used as an indicator for the size of the
context. And these two numbers cannot be com-
pared directly.
To be able to compare the context used by the
phrase pairs to the context used in the n-gram lan-
guage model, we calculated the average left context
that is used for every target word where the word
itself is included, i.e. the context of a single word
is 1. In case of the bilingual language model the
score for the average left context is exactly the aver-
age length of applied n-grams in a given translation.
For phrase pairs the average left context can be cal-
culated in the following way: A phrase pair of length
1 gets a left context score of 1. In a phrase pair of
length 2, the first word has a left context score of 1,
since it is not influenced by any target word to the
left. The second word in that phrase pair gets a left
context count of 2, because it is influenced by the
first word in the phrase. Correspondingly, the left
context score of a phrase pair of length 3 is 6 (com-
posed of the score 1 for the first word, score 2 for
the second word and score 3 for the third word). To
get the average left context for the whole translation,
the context scores of all phrases are summed up and
divided by the number of words in the translation.
The scores for the average left contexts for the two
test sets are shown in Tables 2 and 3. They are called
avg. PP Left Context. As it can be seen, the con-
text used by the bilingual n-gram language model is
longer than the one by the phrase pairs. The average
n-gram length increases from 1.58 and 1.57, respec-
tively to 2.21 and 2.18 for the two given test sets.
If we compare the average n-gram length of the
bilingual language model to the one of the target
language model, the n-gram length of the first is of
course smaller, since the number of possible bilin-
gual tokens is higher than the number of possible
monolingual words. This can also be seen when
looking at the perplexities of the two language mod-
els on the generated translations. While the perplex-
ity of the target language model is 99 and 101 on
Test 1 and 2, respectively, the perplexity of the bilin-
203
gual language model is 512 and 538.
Metric No BiLM BiLM
BLEU 30.37 30.52
TER 50.27 50.06
avg. Target PL 1.66 1.66
avg. PP Left Context 1.57 1.58
avg. Target LM N-Gram 3.28 3.27
avg. BiLM N-Gram 2.21
Table 2: German-to-English results (Test 1)
Metric No BiLM BiLM
BLEU 44.16 45.09
TER 41.02 40.52
avg. Target PL 1.65 1.65
avg. PP Left Context 1.56 1.57
avg. Target LM N-Gram 3.25 3.23
avg. BiLM N-Gram 2.18
Table 3: German-to-English results (Test 2)
5.2.3 Overlapping Context
An additional advantage of the n-gram-based ap-
proach is the possibility to have overlapping con-
text. If we would always use phrase pairs of length
2 only half of the adjacent words would influence
each other in the translation. The others are only
influenced by the other target words through the lan-
guage model. If we in contrast would have a bilin-
gual language model which uses an n-gram length
of 2, this means that every choice of word influences
the previous and the following word.
To analyze this influence, we counted how many
borders of phrase pairs are covered by a bilingual
n-gram. For Test 1, 16783 of the 27785 borders
between phrase pairs are covered by a bilingual n-
gram. For Test 2, 9995 of 16735 borders are cov-
ered. Consequently, in both cases at around 60 per-
cent of the borders additional information can be
used by the bilingual n-gram language model.
5.2.4 Bilingual N-Gram Length
For the German-to-English translation task we
performed an additional experiment comparing dif-
ferent n-gram lengths of the bilingual language
BiLM Length aNGL BLEU TER
No 30.37 50.27
1 1 29.67 49.73
2 1.78 30.36 50.05
3 2.11 30.47 50.08
4 2.21 30.52 50.06
5 2.23 30.52 50.07
6 2.24 30.52 50.07
Table 4: Different N-Gram Lengths (Test 1)
BiLM Length aNGL BLEU TER
No 44.16 41.02
1 1 44.22 40.53
2 1.78 45.11 40.38
3 2.09 45.18 40.51
4 2.18 45.09 40.52
5 2.21 45.10 40.52
6 2.21 45.10 40.52
Table 5: Different N-Gram Lengths (Test 2)
model. To ensure comparability between the exper-
iments and avoid additional noise due to different
optimization results, we did not perform separate
optimization runs for for each of the system vari-
ants with different n-gram length, but used the same
scaling factors for all of them. Of course, the sys-
tem using no bilingual language model was trained
independently. In Tables 4 and 5 we can see that the
length of the actually applied n-grams as well as the
BLEU score increased until the bilingual language
model reaches an order of 4. For higher order bilin-
gual language models, nearly no additional n-grams
can be found in the language models. Also the trans-
lation quality does not increase further when using
longer n-grams.
5.3 Arabic to English
The Arabic-to-English system was optimized on the
MT06 data. As test set the Rosetta in-house test set
DEV07-nw (News) and wb (Web Data) was used.
The results for the Arabic-to-English translation
task are summarized in Tables 6 and 7. The perfor-
mance was tested on two different domains, transla-
tion of News and Web documents. On both tasks,
the translation could be improved by more than 1
204
BLEU point. Measuring the performance in TER
also shows an improvement by 0.7 and 0.5 points.
By adding a POS-based bilingual language
model, the performance could be improved further.
An additional gain of 0.2 BLEU points and decrease
of 0.3 points in TER could be reached. Conse-
quently, an overall improvement of up to 1.7 BLEU
points could be achieved by integrating two bilin-
gual language models, one based on surface word
forms and one based on parts-of-speech.
System
Dev Test
BLEU TER BLEU
NoBiLM 48.42 40.77 52.05
+ BiLM 49.29 40.04 53.51
+ POS BiLM 49.56 39.85 53.71
Table 6: Results on Arabic to English: Translation of
News
System
Dev Test
BLEU TER BLEU
NoBiLM 48.42 47.14 41.90
+ BiLM 49.29 46.66 43.12
+ POS BiLM 49.56 46.40 43.28
Table 7: Results on Arabic to English: Translation of
Web documents
As it was done for the German-to-English system,
we also compared the context used by the different
models for this translation direction. The results are
summarized in Table 8 for the News test set and in
Table 9 for the translation of Web data. It can be seen
like it was for the other language pair that the context
used in the bilingual language model is bigger than
the one used by the phrase-based translation model.
Furthermore, it is worth mentioning that shorter
phrase pairs are used, when using the POS-based
bilingual language model. Both bilingual language
models seem to model the context quite good, so that
less long phrase pairs are needed to build the trans-
lation. Instead, the more frequent short phrases can
be used to generate the translation.
5.4 Shared Translation Task @ WMT2011
The bilingual language model was included in 3
systems built for the WMT2011 Shared Translation
Metric No BiLM POS BiLM
BLEU 52.05 53.51 53.71
avg. Target PL 2.12 2.03 1.79
avg. PP Left Context 1.92 1.85 1.69
avg. BiLM N-Gram 2.66 2.65
avg. POS BiLM 4.91
Table 8: Bilingual Context in Arabic-to-English results
(News)
Metric No BiLM POS BiLM
BLEU 41.90 43.12 43.28
avg. Target PL 1.82 1.80 1.57
avg. PP Left Context 1.72 1.69 1.53
avg. BiLM N-Gram 2.33 2.31
avg. POS BiLM 4.49
Table 9: Bilingual Context in Arabic-to-English results
(Web data)
Task evaluation. A phrase-based system similar to
the one described before for the German-to-English
results was used. A detailed system description can
be found in Herrmann et al (2011). The results are
summarized in Table 10. The performance of com-
petitive systems could be improved in all three lan-
guages by up to 0.4 BLEU points.
Language Pair No BiLM BiLM
German-English 24.12 24.52
English-German 16.89 17.01
French-English 28.17 28.34
Table 10: Preformance of Bilingual language model at
WMT2011
6 Conclusion
In this work we showed how a feature of the n-gram-
based approach can be integrated into a phrase-
based statistical translation system. We performed
a detailed analysis on how this influences the scor-
ing of the translation system. We could show im-
provements on a variety of translation tasks cover-
ing different languages and domains. Furthermore,
we could show that additional bilingual context in-
formation is used.
Furthermore, the additional feature can easily be
205
extended to additional word factors such as part-of-
speech, which showed improvements for the Arabic-
to-English translation task.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Fran?ois Yvon. 2010. LIMSI?s Statisti-
cal Translation Systems for WMT?10. In Fifth Work-
shop on Statistical Machine Translation (WMT 2010),
Uppsala, Sweden.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Fac-
tored language models and generalized parallel back-
off. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 4?6, Stroudsburg, PA, USA.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation using Word Sense Disam-
biguation. In In The 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.
Francisco Casacuberta and Enrique Vidal. 2004. Ma-
chine Translation with Inferred Stochastic Finite-State
Transducers. Comput. Linguist., 30:205?225, June.
Yee Seng Chan and Hwee Tou Ng. 2007. Word Sense
Disambiguation improves Statistical Machine Trans-
lation. In In 45th Annual Meeting of the Association
for Computational Linguistics (ACL-07, pages 33?40.
Josep M. Crego and Fran?ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, 24, June.
Mona Diab. 2009. Second Generation Tools (AMIRA
2.0): Fast and Robust Tokenization, POS tagging, and
Base Phrase Chunking. In Proc. of the Second Interna-
tional Conference on Arabic Language Resources and
Tools, Cairo, Egypt, April.
Qin Gao and Stephan Vogel. 2010. Training Phrase-
Based Machine Translation Models on the Cloud:
Open Source Machine Translation Toolkit Chaski. In
The Prague Bulletin of Mathematical Linguistics No.
93.
Qin Gao, Francisco Guzman, and Stephan Vogel.
2010. EMDC: A Semi-supervised Approach for Word
Alignment. In Proc. of the 23rd International Confer-
ence on Computational Linguistics, Beijing, China.
Sa?a Hasan, Juri Ganitkevitch, Hermann Ney, and Jes?s
Andr?s-Ferrer. 2008. Triplet Lexicon Models for Sta-
tistical Machine Translation. In Proc. of Conference
on Empirical Methods in NLP, Honolulu, USA.
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The Karlsruhe Institute of
Technology Translation Systems for the WMT 2011.
In Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinbugh, U.K.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48?
54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Demonstration Session, Prague, Czech Repub-
lic, June 23.
Jos? B. Mari?o, Rafael E. Banchs, Josep M. Crego, Adri?
de Gispert, Patrik Lambert, Jos? A. R. Fonollosa, and
Marta R. Costa-juss?. 2006. N-gram-based machine
translation. Comput. Linguist., 32, December.
Evgeny Matusov, Richard Zens, David Vilar, Arne
Mauser, Maja Popovic?, Sa?a Hasan, and Hermann
Ney. 2006. The rwth machine translation system. In
TC-STAR Workshop on Speech-to-Speech Translation,
pages 31?36, Barcelona, Spain, June.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and Alex
Waibel. 2009. The Universit?t Karlsruhe Translation
System for the EACL-WMT 2009. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sk?vde, Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
206
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358?364,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joint WMT Submission of the QUAERO Project
?Markus Freitag, ?Gregor Leusch, ?Joern Wuebker, ?Stephan Peitz, ?Hermann Ney,
?Teresa Herrmann, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Gilles Adda,?Josep Maria Crego,
?Bianka Buschbeck, ?Tonio Wandmacher, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2011 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems. Then RWTH system combination
combines these translations to a better one. In
this paper, we describe the single systems of
each group. Before we present the results of
the system combination, we give a short de-
scription of the RWTH Aachen system com-
bination approach.
1 Overview
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
1.1 Data Sets
For WMT 2011 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned on
the newstest2009 dev set. The newstest2008 dev set
was used to train the system combination parame-
ters. Finally the newstest2010 dev set was used to
compare the results of the different system combi-
nation approaches and settings.
2 Translation Systems
2.1 RWTH Aachen Single Systems
For the WMT 2011 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
bilingual corpus, the translation probabilities are es-
timated by relative frequencies. The standard feature
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. Parameters are optimized with the Downhill-
Simplex algorithm (Nelder and Mead, 1965) on the
word graph.
358
2.1.2 Hierarchical System
For the hierarchical setups described in this pa-
per, the open source Jane toolkit (Vilar et al, 2010)
is employed. Jane has been developed at RWTH
and implements the hierarchical approach as intro-
duced by Chiang (2007) with some state-of-the-art
extensions. In hierarchical phrase-based translation,
a weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The model weights are optimized
with standard MERT (Och, 2003) on 100-best lists.
2.1.3 Phrase Model Training
For some PBT systems a forced alignment pro-
cedure was applied to train the phrase translation
model as described in Wuebker et al (2010). A
modified version of the translation decoder is used
to produce a phrase alignment on the bilingual train-
ing data. The phrase translation probabilities are es-
timated from their relative frequencies in the phrase-
aligned training data. In addition to providing a sta-
tistically well-founded phrase model, this has the
benefit of producing smaller phrase tables and thus
allowing more rapid and less memory consuming
experiments with a better translation quality.
2.1.4 Final Systems
For the German?English task, RWTH conducted
experiments comparing the standard phrase extrac-
tion with the phrase training technique described in
Section 2.1.3. Further experiments included the use
of additional language model training data, rerank-
ing of n-best lists generated by the phrase-based sys-
tem, and different optimization criteria.
A considerable increase in translation quality can
be achieved by application of German compound
splitting (Koehn and Knight, 2003). In comparison
to standard heuristic phrase extraction techniques,
performing force alignment phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006),
sentence length model, a 6-gram LM and IBM-1 lex-
icon models in both normal and inverse direction.
These models are combined in a log-linear fashion
and the scaling factors are tuned in the same man-
ner as the baseline system (using TER?4BLEU on
newstest2009).
The final table includes two identical Jane sys-
tems which are optimized on different criteria. The
one optimized on TER?BLEU yields a much lower
TER.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogeneous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation. Op-
timization with regard to the BLEU score is done
using Minimum Error Rate Training as described
by Venugopal et al (2005). The translation model
is trained on the Europarl and News Commentary
Corpus and the phrase table is based on a GIZA++
Word Alignment. We use two 4-gram SRI language
models, one trained on the News Shuffle corpus and
one trained on the Gigaword corpus. Reordering is
performed based on continuous and non-continuous
POS rules to cover short and long-range reorder-
ings. The long-range reordering rules were also ap-
plied to the training corpus and phrase extraction
was performed on the resulting reordering lattices.
Part-of-speech tags are obtained using the TreeTag-
1http://hunspell.sourceforge.net/
359
ger (Schmid, 1994). In addition, the system applies
a bilingual language model to extend the context of
source language words available for translation. The
individual models are described briefly in the fol-
lowing.
2.2.3 POS-based Reordering Model
We use a reordering model that is based on parts-
of-speech (POS) and learn probabilistic rules from
the POS tags of the words in the training corpus and
the alignment information. In addition to continu-
ous reordering rules that model short-range reorder-
ing (Rottmann and Vogel, 2007), we apply non-
continuous rules to address long-range reorderings
as typical for German-English translation (Niehues
and Kolss, 2009). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are encoded
in a word lattice which is used as input to the de-
coder.
2.2.4 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract also phrase pairs for origi-
nally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths. To limit the number of extracted
phrase pairs, we extract a source phrase only once
per sentence, even if it is found in different paths and
we only use long-range reordering rules to generate
the lattices for the training corpus.
2.2.5 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder during the search pro-
cess. This segmentation into phrases leads to the
loss of context information at the phrase boundaries.
The language model can make use of more target
side context. To make also source language context
available we use a bilingual language model, an ad-
ditional language model in the phrase-based system
in which each token consist of a target word and all
source words it is aligned to. The bilingual tokens
enter the translation process as an additional target
factor.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
The LIMSI system is built with n-code2, an open
source statistical machine translation system based
on bilingual n-grams.
2.3.2 n-code Overview
In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training this
model requires to reorder source sentences so as to
match the target word order. This is performed by a
stochastic finite-state reordering model, which uses
part-of-speech information3 to generalize reordering
patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a weak
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
use in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 data as development
set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2http://www.limsi.fr/Individu/jmcrego/n-code
3Part-of-speech information for English and German is com-
puted using the TreeTagger.
360
2.3.3 Data Preprocessing
Based on previous experiments which have
demonstrated that better normalization tools provide
better BLEU scores (K. Papineni and Zhu, 2002),
all the English texts are tokenized and detokenized
with in-house text processing tools (De?chelotte et
al., 2008). For German, the standard tokenizer sup-
plied by evaluation organizers is used.
2.3.4 Target n-gram Language Models
The English language model is trained assuming
that the test set consists in a selection of news texts
dating from the end of 2010 to the beginning of
2011. This assumption is based on what was done
for the 2010 evaluation. Thus, a development cor-
pus is built in order to create a vocabulary and to
optimize the target language model.
Development Set and Vocabulary In order to
cover different period, two development sets are
used. The first one is newstest2008. However, this
corpus is two years older than the targeted time pe-
riod. Thus a second development corpus is gath-
ered by randomly sampling bunches of 5 consecu-
tive sentences from the provided news data of 2010
and 2011.
To estimate a LM, the English vocabulary is first
defined by including all tokens observed in the Eu-
roparl and news-commentary corpora. This vocabu-
lary is then expanded with all words that occur more
that 5 times in the French-English giga-corpus, and
with the most frequent proper names taken from the
monolingual news data of 2010 and 2011. This pro-
cedure results in a vocabulary around 500k words.
Language Model Training All the training data
allowed in the constrained task are divided into 9
sets based on dates on genres. On each set, a
standard 4-gram LM is estimated from the 500k
word vocabulary with in-house tools using abso-
lute discounting interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 are first linearly interpolated.
The associated coefficients are estimated so as to
minimize the perplexity evaluated on the dev2010-
2011. The resulting LM and the 2010-2011 LM are
finally interpolated with newstest2008 as develop-
ment data. This two steps interpolation aims to avoid
an overestimate of the weight associated to the 2010-
2011 LM.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
the SYSTRAN baseline system in combination with
a statistical post editing (SPE) component.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k ?
800k entries per language pair).
The basic setup of the SPE component is identi-
cal to the one described in (L. Dugast and Koehn,
2007). A statistical translation model is trained on
the rule-based translation of the source and the tar-
get side of the parallel corpus. This is done sepa-
rately for each parallel corpus. Language models are
trained on each target half of the parallel corpora and
also on additional in-domain corpora. Moreover, the
following measures ? limiting unwanted statistical
effects ? were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is signif-
icantly reduced. In addition, entity translation
is handled more reliably by the rule-based en-
gine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus (whose target is identical
to the source). This was added to the parallel
text in order to improve word alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
361
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained 15M
phrases from the news/europarl corpora, provided
as training data for WMT 2011. Weights for these
separate models were tuned by the MERT algorithm
provided in the Moses toolkit (P. Koehn et al, 2007),
using the provided news development set.
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical mod-
els is selected as consensus translation. A deeper
description will be also given in the WMT11 sys-
tem combination paper of RWTH Aachen Univer-
sity. For this task only the A2L framework has been
used.
4 Experiments
We tried different system combinations with differ-
ent sets of single systems and different optimiza-
tion criteria. As RWTH has two different transla-
tion systems, we put the output of both systems into
system combination. Although both systems have
the same preprocessing, their hypotheses differ. Fi-
nally, we added for both RWTH systems two addi-
tional hypotheses to the system combination. The
two hypotheses of Jane were optimized on differ-
ent criteria. The first hypothesis was optimized on
BLEU and the second one on TER?BLEU. The first
RWTH phrase-based hypothesis was generated with
force alignment, the second RWTH phrase-based
hypothesis is a reranked version of the first one as
described in 2.1.4. Compared to the other systems,
the system by SYSTRAN has a completely different
approach (see section 2.4). It is mainly based on a
rule-based system. For the German?English pair,
SYSTRAN achieves a lower BLEU score in each
test set compared to the other groups. But since the
SYSTRAN system is very different to the others, we
still obtain an improvement when we add it also to
system combination.
We obtain the best result from system combina-
tion of all seven systems, optimizing the parameters
on BLEU. This system was the system we submitted
to the WMT 2011 evaluation.
For each dev set we obtain an improvement com-
pared to the best single systems. For newstest2008
and newstest2009 we get an improvement of 0.5
points in BLEU and 1.8 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. For newstest2010 we get an improvement
of 1.8 points in BLEU and 2.7 points in TER com-
pared to the best single system of RWTH. The sys-
tem combination weights optimized for the best run
are listed in Table 2. We see that although the single
system of SYSTRAN has the lowest BLEU scores,
it gets the second highest system weight. This high
value shows the influence of a completely different
system. On the other hand, all RWTH systems are
very similar, because of their same preprocessing
and their small variations. Therefor the system com-
bination parameter of all four systems by themselves
are relatively small. The summarized ?RWTH ap-
proach? system weight, though, is again on par with
the other systems.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU
compared to each single system. If the system
combination implementation can handle enough sin-
gle systems we would recommend to add all single
systems to the system combination. Although the
single system of SYSTRAN has the lowest BLEU
scores and the RWTH single systems are similar we
achieved the best result in using all single systems.
362
newstest2008 newstest2009 newstest2010 description
BLEU TER BLEU TER BLEU TER
22.73 60.73 22.50 59.82 25.26 57.37 sc (all systems) BLEU opt
22.61 60.60 22.28 59.39 25.07 56.95 sc (all systems - (1)) TER?BLEU opt
22.50 60.41 22.52 59.61 25.23 57.40 sc (all systems) TER?BLEU opt
22.19 60.09 22.05 59.31 24.74 56.89 sc (all systems - (4)) TER?BLEU opt
22.21 60.71 21.89 59.95 24.72 57.58 sc (all systems - (4,7)) TER?BLEU opt
22.22 60.45 21.79 59.72 24.32 57.59 sc (all systems - (3,4)) TER?BLEU opt
22.27 60.60 21.75 59.92 24.35 57.64 sc (all systems - (3,4)) BLEU opt
22.10 62.59 22.01 61.64 23.34 60.35 (1) Karlsruhe Institute of Technology
21.41 62.77 21.12 61.91 23.44 60.06 (2) RWTH PBT (FA) rerank +GW
21.11 62.96 21.06 62.16 23.29 60.26 (3) RWTH PBT (FA)
21.47 63.89 21.00 63.33 22.93 61.71 (4) RWTH jane + GW BLEU opt
20.89 61.05 20.36 60.47 23.42 58.31 (5) RWTH jane + GW TER?BLEU opt
20.33 64.50 19.79 64.91 21.97 61.44 (6) Limsi-CNRS
17.06 69.48 17.52 67.34 18.68 66.37 (7) SYSTRAN Software
Table 1: All systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results are in
percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model. sc denotes
system combination.
system weight
Karlsruhe Institute of Technology 0.350
RWTH PBT (FA) rerank +GW 0.001
RWTH PBT (FA) 0.046
RWTH jane + GW BLEU opt 0.023
RWTH jane + GW TER?BLEU opt 0.034
Limsi-CNRS 0.219
SYSTRAN Software 0.328
Table 2: Optimized systems weights for each system of the best system combination result.
Acknowledgments
This work was achieved as part of the QUAERO
Programme, funded by OSEO, French State agency
for innovation.
References
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
S.F. Chen and J.T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
T. Ward K. Papineni, S. Roukos and W. Zhu. 2002. Bleu:
363
a method for automatic evaluation of machine transla-
tion. In ACL ?02: Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, pages
311?318. Association for Computational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
364
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 379?385,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2011
Teresa Herrmann, Mohammed Mediani, Jan Niehues and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based SMT
systems developed for our participation
in the WMT11 Shared Translation Task.
Translations for English?German and
English?French were generated using a
phrase-based translation system which is
extended by additional models such as
bilingual and fine-grained POS language
models, POS-based reordering, lattice phrase
extraction and discriminative word alignment.
Furthermore, we present a special filtering
method for the English-French Giga corpus
and the phrase scoring step in the training is
parallelized.
1 Introduction
In this paper we describe our systems for the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French. We use a
phrase-based decoder that can use lattices as input
and developed several models that extend the stan-
dard log-linear model combination of phrase-based
MT. These include advanced reordering models and
corresponding adaptations to the phrase extraction
process as well as extension to the translation and
language model in form of discriminative word
alignment and a bilingual language model to ex-
tend source word context. For English-German, lan-
guage models based on fine-grained part-of-speech
tags were used to address the difficult target lan-
guage generation due to the rich morphology of Ger-
man.
We also present a filtering method directly ad-
dressing the problems of web-crawled corpora,
which enabled us to make use of the French-English
Giga corpus. Another novelty in our systems this
year is the parallel phrase scoring method that re-
duces the time needed for training which is espe-
cially convenient for such big corpora as the Giga
corpus.
2 System Description
The baseline systems for all languages use a trans-
lation model that is trained on EPPS and the News
Commentary corpus and the phrase table is based
on a GIZA++ word alignment. The language model
was trained on the monolingual parts of the same
corpora by the SRILM Toolkit (Stolcke, 2002). It
is a 4-gram SRI language model using Kneser-Ney
smoothing.
The problem of word reordering is addressed us-
ing the POS-based reordering model as described
in Section 2.4. The part-of-speech tags for the re-
ordering model are obtained using the TreeTagger
(Schmid, 1994).
An in-house phrase-based decoder (Vogel, 2003)
is used to perform translation and optimization with
regard to the BLEU score is done using Minimum
Error Rate Training as described in Venugopal et al
(2005). During decoding only the top 20 translation
options for every source phrase were considered.
2.1 Data
We trained all systems using the parallel EPPS and
News Commentary corpora. In addition, the UN
corpus and the Giga corpus were used for training
379
the French-English systems.
Optimization was done for most languages using
the news-test2008 data set and news-test2010 was
used as test set. The only exception is German-
English, where news-test2009 was used for opti-
mization due to system combination arrangements.
The language models for the baseline systems were
trained on the monolingual versions of the training
corpora. Later on, we used the News Shuffle and the
Gigaword corpus to train bigger language models.
For training a discriminative word alignment model,
a small amount of hand-aligned data was used.
2.2 Preprocessing
The training data is preprocessed prior to training
the system. This includes normalizing special sym-
bols, smart-casing the first words of each sentence
and removing long sentences and sentences with
length mismatch.
For the German parts of the training corpus
we use the hunspell1 lexicon to map words writ-
ten according to old German spelling to new Ger-
man spelling, to obtain a corpus with homogenous
spelling.
Compound splitting as described in Koehn and
Knight (2003) is applied to the German part of the
corpus for the German-to-English system to reduce
the out-of-vocabulary problem for German com-
pound words.
2.3 Special filtering of the Giga parallel Corpus
The Giga corpus incorporates non-neglegible
amounts of noise even after our usual preprocess-
ing. This noise may be due to different causes.
For instance: non-standard HTML characters,
meaningless parts composed of only hypertext
codes, sentences which are only partial translation
of the source, or eventually not a correct translation
at all.
Such noisy pairs potentially degrade the transla-
tion model quality, therefore it seemed more conve-
nient to eliminate them.
Given the size of the corpus, this task could not be
performed manually. Consequently, we used an au-
tomatic classifier inspired by the work of Munteanu
and Marcu (2005) on comparable corpora. This clas-
1http://hunspell.sourceforge.net/
sifier should be able to filter out the pairs which
likely are not beneficial for the translation model.
In order to reliably decide about the classifier to
use, we evaluated several techniques. The training
and test sets for this evaluation were built respec-
tively from nc-dev2007 and nc-devtest2007. In each
set, about 30% randomly selected source sentences
switch positions with the immediate following so
that they form negative examples. We also used lex-
ical dictionaries in both directions based on EPPS
and UN corpora.
We relied on seven features in our classifiers:
IBM1 score in both directions, number of unaligned
source words, the difference in number of words be-
tween source and target, the maximum source word
fertility, number of unaligned target words, and the
maximum target word fertility. It is noteworthy
that all the features requiring alignment information
(such as the unaligned source words) were computed
on the basis of the Viterbi path of the IBM1 align-
ment. The following classifiers were used:
Regression Choose either class based on a
weighted linear combination of the features
and a fixed threshold of 0.5.
Logistic regression The probability of the class is
expressed as a sigmoid of a linear combination
of the different features. Then the class with
the highest probability is picked.
Maximum entropy classifier We used the same set
of features to train a maximum entropy classi-
fier using the Megam package2.
Support vector machines classifier An SVM clas-
sifier was trained using the SVM-light pack-
age3.
Results of these experiments are summarized in
Table 1.
The regression weights were estimated so that to
minimize the squared error. This gave us a pretty
poor F-measure score of 90.42%. Given that the lo-
gistic regression is more suited for binary classifica-
tion in our case than the normal regression, it led to
significant increase in the performance. The training
2http://www.cs.utah.edu/?hal/megam/
3http://svmlight.joachims.org/
380
Approach Precision Recall F-measure
Regression 93.81 87.27 90.42
LogReg 93.43 94.84 94.13
MaxEnt 93.69 94.54 94.11
SVM 98.20 96.87 97.53
Table 1: Results of the filtering experiments
was held by maximizing the likelihood to the data
with L2 regularization (with ? = 0.1). This gave an
F-measure score of 94.78%.
The maximum entropy classifier performed better
than the logistic regression in terms of precision but
however it had worse F-measure.
Significant improvements could be noticed us-
ing the SVM classifier in both precision and recall:
98.20% precision, 96.87% recall, and thus 97.53%
F-measure.
As a result, we used the SVM classifier to filter
the Giga parallel corpus. The corpus contained orig-
inally around 22.52 million pairs. After preprocess-
ing and filtering it was reduced to 16.7 million pairs.
Thus throwing around 6 million pairs.
2.4 Word Reordering
In contrast to modeling the reordering by a distance-
based reordering model and/or a lexicalized distor-
tion model, we use a different approach that relies
on part-of-speech (POS) sequences. By abstracting
from surface words to parts-of-speech, we expect to
model the reordering more accurately.
2.4.1 POS-based Reordering Model
To model reordering we first learn probabilistic
rules from the POS tags of the words in the train-
ing corpus and the alignment information. Contin-
uous reordering rules are extracted as described in
Rottmann and Vogel (2007) to model short-range re-
orderings. When translating between German and
English, we apply a modified reordering model with
non-continuous rules to cover also long-range re-
orderings (Niehues and Kolss, 2009). The reorder-
ing rules are applied to the source text and the orig-
inal order of words and the reordered sentence vari-
ants generated by the rules are encoded in a word
lattice which is used as input to the decoder.
2.4.2 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract the phrase pairs for orig-
inally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths.
To limit the number of extracted phrase pairs, we
extract a source phrase only once per sentence even
if it is found in different paths.
2.5 Translation and Language Models
In addition to the models used in the baseline sys-
tem described above we conducted experiments in-
cluding additional models that enhance translation
quality by introducing alternative or additional in-
formation into the translation or language modelling
process.
2.5.1 Discriminative Word Alignment
In most of our systems we use the PGIZA++
Toolkit4 to generate alignments between words in
the training corpora. The word alignments are gen-
erated in both directions and the grow-diag-final-and
heuristic is used to combine them. The phrase ex-
traction is then done based on this word alignment.
In the English-German system we applied the
Discriminative Word Alignment approach as de-
scribed in Niehues and Vogel (2008) instead. This
alignment model is trained on a small corpus of
hand-aligned data and uses the lexical probability
as well as the fertilities generated by the PGIZA++
Toolkit and POS information.
2.5.2 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder according to the best com-
bination of phrases that maximize the translation
and language model scores. This segmentation into
phrases leads to the loss of context information at
the phrase boundaries. Although more target side
context is available to the language model, source
4http://www.cs.cmu.edu/?qing/
381
side context would also be valuable for the decoder
when searching for the best translation hypothesis.
To make also source language context available we
use a bilingual language model, an additional lan-
guage model in the phrase-based system in which
each token consist of a target word and all source
words it is aligned to. The bilingual tokens enter
the translation process as an additional target factor
and the bilingual language model is applied to the
additional factor like a normal language model. For
more details see (Niehues et al, 2011).
2.5.3 Parallel phrase scoring
The process of phrase scoring is held in two runs.
The objective of the first run is to compute the nec-
essary counts and to estimate the scores, all based
on the source phrases; while the second run is sim-
ilarly held based on the target phrases. Thus, the
extracted phrases have to be sorted twice: once by
source phrase and once by target phrase. These two
sorting operations are almost always done on an ex-
ternal storage device and hence consume most of the
time spent in this step.
The phrase scoring step was reimplemented in or-
der to exploit the available computation resources
more efficiently and therefore reduce the process-
ing time. It uses optimized sorting algorithms for
large data volumes which cannot fit into memory
(Vitter, 2008). In its core, our implementation re-
lies on STXXL: an extension of the STL library for
external memory (Kettner, 2005) and on OpenMP
for shared memory parallelization (Chapman et al,
2007).
Table 2 shows a comparison between Moses and
our phrase scoring tools. The comparison was held
using sixteen-core 64-bit machines with 128 Gb
RAM, where the files are accessed through NFS on
a RAID disk. The experiments show that the gain
grows linearly with the size of input with an average
of 40% of speed up.
2.5.4 POS Language Models
In addition to surface word language models, we
did experiments with language models based on
part-of-speech for English-German. We expect that
having additional information in form of probabil-
ities of part-of-speech sequences should help espe-
cially in case of the rich morphology of German and
#pairs(G) Moses ?103(s) KIT ?103(s)
0.203 25.99 17.58
1.444 184.19 103.41
1.693 230.97 132.79
Table 2: Comparison of Moses and KIT phrase extraction
systems
therefore the more difficult target language genera-
tion.
The part-of-speeches were generated using the
TreeTagger and the RFTagger (Schmid and Laws,
2008), which produces more fine-grained tags that
include also person, gender and case information.
While the TreeTagger assigns 54 different POS tags
to the 357K German words in the corpus, the RF-
Tagger produces 756 different fine-grained tags on
the same corpus.
We tried n-gram lengths of 4 and 7. While no im-
provement in translation quality could be achieved
using the POS language models based on the normal
POS tags, the 4-gram POS language model based
on fine-grained tags could improve the translation
system by 0.2 BLEU points as shown in Table 3.
Surprisingly, increasing the n-gram length to 7 de-
creased the translation quality again.
To investigate the impact of context length, we
performed an analysis on the outputs of two different
systems, one without a POS language model and one
with the 4-gram fine-grained POS language model.
For each of the translations we calculated the aver-
age length of the n-grams in the translation when
applying one of the two language models using 4-
grams of surface words or parts-of-speech. The re-
sults are also shown in Table 3.
The average n-gram length of surface words on
the translation generated by the system without POS
language model and the one using the 4-gram POS
language model stays practically the same. When
measuring the n-gram length using the 4-gram POS
language model, the context increases to 3.4. This
increase of context is not surprising, since with
the more general POS tags longer contexts can be
matched. Comparing the POS context length for
the two translations, we can see that the context in-
creases from 3.18 to 3.40 due to longer matching
POS sequences. This means that the system using
382
the POS language model actually generates trans-
lations with more probable POS sequences so that
longer matches are possible. Also the perplexity
drops by half since the POS language model helps
constructing sentences that have a better structure.
System BLEU avg. ngram length PPL
Word POS POS
no POS LM 16.64 2.77 3.18 66.78
POS LM 16.88 2.81 3.40 33.36
Table 3: Analysis of context length
3 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to the
workshop. The following sections describe the ex-
periments for the individual language pairs and show
the translation results. The results are reported as
case-sensitive BLEU scores (Papineni et al, 2002)
on one reference translation.
3.1 German-English
The German-to-English baseline system applies
short-range reordering rules and uses a language
model trained on the EPPS and News Commen-
tary. By exchanging the baseline language model
by one trained on the News Shuffle corpus we im-
prove the translation quality considerably, by more
than 3 BLEU points. When we expand the cov-
erage of the reordering rules to enable long-range
reordering we can improve even further by 0.4 and
adding a second language model trained on the En-
glish Gigaword corpus we gain another 0.3 BLEU
points. To ensure that the phrase table also includes
reordered phrases, we use lattice phrase extraction
and can achieve a small improvement. Finally, a
bilingual language model is added to extend the con-
text of source language words available for transla-
tion, reaching the best score of 23.35 BLEU points.
This system was used for generating the translation
submitted to the German-English Translation Task.
3.2 English-German
The English-to-German baseline system also in-
cludes short-range reordering and uses translation
System Dev Test
Baseline 18.49 19.10
+ NewsShuffle LM 20.63 22.24
+ LongRange Reordering 21.00 22.68
+ Additional Giga LM 21.80 22.92
+ Lattice Phrase Extraction 21.87 22.96
+ Bilingual LM 22.05 23.35
Table 4: Translation results for German-English
and language model based on EPPS and News Com-
mentary. Exchanging the language model by the
News Shuffle language model again yields a big im-
provement by 2.3 BLEU points. Adding long-range
reordering improves a lot on the development set
while the score on the test set remains practically
the same. Replacing the GIZA++ alignments by
alignments generated using the Discriminative Word
Alignment Model again only leads to a small im-
provement. By using the bilingual language model
to increase context we can gain 0.1 BLEU points
and by adding the part-of-speech language model
with rich parts-of-speech including case, number
and gender information for German we achieve the
best score of 16.88. This system was used to gener-
ate the translation used for submission.
System Dev Test
Baseline 13.55 14.19
+ NewsShuffle LM 15.10 16.46
+ LongRange Reordering 15.79 16.46
+ DWA 15.81 16.52
+ Bilingual LM 15.85 16.64
+ POS LM 15.88 16.88
Table 5: Translation results for English-German
3.3 English-French
Table 6 summarizes how our system for English-
French evolved. The baseline system for this direc-
tion was trained on the EPPS and News Commen-
tary corpora, while the language model was trained
on the French part of the EPPS, News Commen-
tary and UN parallel corpora. Some improvement
could be already seen by introducing the short-range
reorderings trained on the baseline parallel corpus.
383
Apparently, the UN data brought only slight im-
provement to the overall performance. On the other
hand, adding bigger language models trained on the
monolingual French version of EPPS, News Com-
mentary and the News Shuffle together with the
French Gigaword corpus introduces an improvement
of 3.7 on test. Using a system trained only on the
Giga corpus data with the same last configuration
shows a significant gain. It showed an improvement
of around 1.0. We were able to obtain some further
improvements by merging the translation models of
the last two systems. i.e. the one system based on
EPPS, UN, and News Commentary and the other on
the Giga corpus. This merging increased our score
by 0.2. Finally, our submitted system for this direc-
tion was obtained by using a single language model
trained on the union of all the French corpora in-
stead of using multiple models. This resulted in an
improvement of 0.1 leading to our best score: 28.28.
System Dev Test
Baseline 20.62 22.36
+ Reordering 21.29 23.11
+ UN 21.27 23.24
+ Big LMs 23.77 26.90
Giga data 24.53 27.94
Merge 24.74 28.14
+ Merged LMs 25.07 28.28
Table 6: Translation results for English-French
3.4 French-English
The development of our system for the French-
English direction is summarized in Table 7. Our sys-
tem for this direction evolved quite similarly to the
opposite direction. The largest improvement accom-
panied the integration of the bigger language mod-
els (trained on the English version of EPPS, News
Commentary, News Shuffle and the Gigaword cor-
pus): 3.3 BLEU points, whereas smaller improve-
ments could be gained by applying the short reorder-
ing rules and almost no change by including the UN
data. Further gains were obtained by training the
system on the Giga corpus added to the previous
parallel data. This increased our performance by
0.6. The submitted system was obtained by aug-
menting the last system with a bilingual language
model adding around 0.2 to the previous score and
thus giving 28.34 as final score.
System Dev Test
Baseline 20.76 23.78
+ Reordering 21.42 24.28
+ UN 21.55 24.21
+ Big LMs 24.16 27.55
+ Giga data 24.86 28.17
+ BiLM 25.01 28.34
Table 7: Translation results for French-English
4 Conclusions
We have presented the systems for our participation
in the WMT 2011 Evaluation for English?German
and English?French. For English?French, a spe-
cial filtering method for web-crawled data was de-
veloped. In addition, a parallel phrase scoring tech-
nique was implemented that could speed up the MT
training process tremendously. Using these two fea-
tures, we were able to integrate the huge amounts of
data available in the Giga corpus into our systems
translating between English and French.
We applied POS-based reordering to improve our
translations in all directions, using short-range re-
ordering for English?French and long-range re-
ordering for English?German. For German-
English, reordering also the training corpus lead to
further improvements of the translation quality.
A Discriminative Word Alignment Model led to
an increase in BLEU for English-German. For this
direction we also tried fine-grained POS language
models of different n-gram lengths. The best trans-
lations could be obtained by using 4-grams.
For nearly all experiments, a bilingual language
model was applied that expands the context of
source words that can be considered during decod-
ing. The improvements range from 0.1 to 0.4 in
BLEU score.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
384
References
Barbara Chapman, Gabriele Jost, and Ruud van der Pas.
2007. Using OpenMP: Portable Shared Memory Par-
allel Programming (Scientific and Engineering Com-
putation). The MIT Press.
Roman Dementiev Lutz Kettner. 2005. Stxxl: Stan-
dard template library for xxl data sets. In Proceedings
of ESA 2005. Volume 3669 of LNCS, pages 640?651.
Springer.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In EACL, Budapest,
Hungary.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division, T. J.
Watson Research Center.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sko?vde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation of
Conditional Probabilities with Decision Trees and an
Application to Fine-Grained POS Tagging. In COL-
ING 2008, Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Beyond
(WPT-05), Ann Arbor, MI.
Jeffrey Scott Vitter. 2008. Algorithms and Data Struc-
tures for External Memory. now Publishers Inc.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
385
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322?329,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joint WMT 2012 Submission of the QUAERO Project
?Markus Freitag, ?Stephan Peitz, ?Matthias Huck, ?Hermann Ney,
?Jan Niehues, ?Teresa Herrmann, ?Alex Waibel,
?Le Hai-son, ?Thomas Lavergne, ?Alexandre Allauzen,
?Bianka Buschbeck, ?Josep Maria Crego, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2012 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems and finally the RWTH system combi-
nation combined these translations in our final
submission. Experimental results show im-
provements of up to 1.7 points in BLEU and
3.4 points in TER compared to the best single
system.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
This paper is structured as follows. In Section
2, the different engines of all four groups are in-
troduced. In Section 3, the RWTH Aachen system
combination approach is presented. Experiments
with different system selections for system combi-
nation are described in Section 4. Finally in Section
5, we discuss the results.
2 Translation Systems
For WMT 2012 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned
on the newstest2009 or newstest2010 development
set. The newstest2011 dev set was used to train
the system combination parameters. Finally, the
newstest2008-newstest2010 dev sets were used to
compare the results of the different system combina-
tion settings. In this Section all four different system
engines are presented.
2.1 RWTH Aachen Single Systems
For the WMT 2012 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
parallel corpus, the translation probabilities are esti-
mated by relative frequencies. The standard feature
322
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. The model weights are optimized with standard
Mert (Och, 2003) on 200-best lists. The optimiza-
tion criterium is BLEU.
2.1.2 Hierarchical System
For the hierarchical setups (HPBT) described in
this paper, the open source Jane toolkit (Vilar et
al., 2010) is employed. Jane has been developed at
RWTH and implements the hierarchical approach as
introduced by Chiang (2007) with some state-of-the-
art extensions. In hierarchical phrase-based transla-
tion, a weighted synchronous context-free grammar
is induced from parallel text. In addition to contigu-
ous lexical phrases, hierarchical phrases with up to
two gaps are extracted. The search is typically car-
ried out using the cube pruning algorithm (Huang
and Chiang, 2007). The model weights are opti-
mized with standard Mert (Och, 2003) on 100-best
lists. The optimization criterium is 4BLEU ?TER.
2.1.3 Preprocessing
In order to reduce the source vocabulary size
translation, the German text was preprocessed
by splitting German compound words with the
frequency-based method described in (Koehn and
Knight, 2003a). To further reduce translation com-
plexity for the phrase-based approach, we performed
the long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.4 Language Model
For both decoders a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl, the
109 French-English, UN and LDC Gigaword Fourth
Edition corpora. For the 109 French-English, UN
and LDC Gigaword corpora RWTH applied the data
selection technique described in (Moore and Lewis,
2010).
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogenous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003b). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation and op-
timization with regard to the BLEU score is done us-
ing Minimum Error Rate Training as described in
Venugopal et al (2005).
2.2.3 Translation Models
The translation model is trained on the Europarl
and News Commentary Corpus and the phrase ta-
ble is based on a discriminative word alignment
(Niehues and Vogel, 2008).
In addition, the system applies a bilingual lan-
guage model (Niehues et al, 2011) to extend the
context of source language words available for trans-
lation.
Furthermore, we use a discriminative word lexi-
con as introduced in (Mauser et al, 2009). The lex-
icon was trained and integrated into our system as
described in (Mediani et al, 2011).
At last, we tried to find translations for
out-of-vocabulary (OOV) words by using quasi-
morphological operations as described in Niehues
and Waibel (2011). For each OOV word, we try to
find a related word that we can translate. We modify
the ending letters of the OOV word and learn quasi-
morphological operations to be performed on the
known translation of the related word to synthesize
a translation for the OOV word. By this approach
we were for example able to translate Kaminen into
chimneys using the known translation Kamin # chim-
ney.
2.2.4 Language Models
We use two 4-gram SRI language models, one
trained on the News Shuffle corpus and one trained
1http://hunspell.sourceforge.net/
323
on the Gigaword corpus. Furthermore, we use a 5-
gram cluster-based language model trained on the
News Shuffle corpus. The word clusters were cre-
ated using the MKCLS algorithm. We used 100
word clusters.
2.2.5 Reordering Model
Reordering is performed based on part-of-speech
tags obtained using the TreeTagger (Schmid, 1994).
Based on these tags we learn probabilistic continu-
ous (Rottmann and Vogel, 2007) and discontinuous
(Niehues and Kolss, 2009) rules to cover short and
long-range reorderings. The rules are learned from
the training corpus and the alignment. In addition,
we learned tree-based reordering rules. Therefore,
the training corpus was parsed by the Stanford parser
(Rafferty and Manning, 2008). The tree-based rules
consist of the head node of a subtree and all its
children as well as the new order and a probability.
These rules were applied recursively. The reordering
rules are applied to the source sentences and the re-
ordered sentence variants as well as the original se-
quence are encoded in a word lattice which is used
as input to the decoder. For the test sentences, the
reordering based on parts-of-speech and trees allows
us to change the word order in the source sentence
so that the sentence can be translated more easily.
In addition, we build reordering lattices for all train-
ing sentences and then extract phrase pairs from the
monotone source path as well as from the reordered
paths.
2.3 LIMSI-CNRS Single System
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine translation
system based on bilingual n-gram2. In this approach,
the translation model relies on a specific decomposi-
tion of the joint probability of a sentence pair P(s, t)
using the n-gram assumption: a sentence pair is de-
composed into a sequence of bilingual units called
tuples, defining a joint segmentation of the source
and target. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing which ultimately derives from initial word and
phrase alignments.
2http://ncode.limsi.fr/
2.3.1 An Overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using a
n-gram model of (source,target) pairs (Casacuberta
and Vidal, 2004). Training this model requires to
reorder source sentences so as to match the target
word order. This is performed by a stochastic finite-
state reordering model, which uses part-of-speech
information3 to generalize reordering patterns be-
yond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a ?weak?
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
used in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 development set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2.3.2 Continuous Space Translation Models
One critical issue with standard n-gram transla-
tion models is that the elementary units are bilingual
pairs, which means that the underlying vocabulary
can be quite large. Unfortunately, the parallel data
available to train these models are typically smaller
than the corresponding monolingual corpora used to
train target language models. It is very likely then,
that such models should face severe estimation prob-
lems. In such setting, using neural network language
3Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
324
model techniques seem all the more appropriate. For
this study, we follow the recommendations of Le et
al. (2012), who propose to factor the joint proba-
bility of a sentence pair by decomposing tuples in
two (source and target) parts, and further each part
in words. This yields a word factored translation
model that can be estimated in a continuous space
using the SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the computa-
tional cost of computing n-gram probabilities. The
solution used here was to resort to a two pass ap-
proach: the first pass uses a conventional back-off
n-gram model to produce a k-best list; in the second
pass, the k-best list is reordered using the probabil-
ities of m-gram SOUL translation models. In the
following experiments, we used a fixed context size
for SOUL of m = 10, and used k = 300.
2.3.3 Corpora and Data Preprocessing
The parallel data is word-aligned using
MGIZA++4 with default settings. For the En-
glish monolingual training data, we used the same
setup as last year5 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we took advantage of our in-house
text processing tools for tokenization and detok-
enization steps (De?chelotte et al, 2008) and our sys-
tem was built in ?true-case?. As German is mor-
phologically more complex than English, the default
policy which consists in treating each word form
independently is plagued with data sparsity, which
is detrimental both at training and decoding time.
Thus, the German side was normalized using a spe-
cific pre-processing scheme (Allauzen et al, 2010;
Durgar El-Kahlout and Yvon, 2010), which notably
aims at reducing the lexical redundancy by (i) nor-
malizing the orthography, (ii) neutralizing most in-
flections and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
a system composed of the standard SYSTRAN MT
engine in combination with a statistical post editing
(SPE) component.
4http://geek.kyloo.net/software
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k -
800k entries per language pair).
The SYSTRAN phrase-based SPE component
views the output of the rule-based system as the
source language, and the (human) reference trans-
lation as the target language, see (L. Dugast and
Koehn, 2007). It performs corrections and adaptions
learned from the 5-gram language model trained on
the parallel target-to-target corpus. Moreover, the
following measures - limiting unwanted statistical
effects - were applied:
? Named entities, time and numeric expressions
are replaced by special tokens on both sides.
This usually improves word alignment, since
the vocabulary size is significantly reduced. In
addition, entity translation is handled more re-
liably by the rule-based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus to help to improve word
alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
The SPE language model was trained on 2M bilin-
gual phrases from the news/Europarl corpora, pro-
vided as training data for WMT 2012. An addi-
tional language model built from 15M phrases of
the English LDC Gigaword corpus using Kneser-
Ney (Kneser and Ney, 1995) smoothing was added.
Weights for these separate models were tuned by
the Mert algorithm provided in the Moses toolkit
(P. Koehn et al, 2007), using the provided news de-
velopment set.
325
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical models
is selected as consensus translation.
4 Experiments
This year, we tried different sets of single systems
for system combination. As RWTH has two dif-
ferent translation systems, we put the output of
both systems into system combination. Although
both systems have the same preprocessing and lan-
guage model, their hypotheses differ because of
their different decoding approach. Compared to
the other systems, the system by SYSTRAN has a
completely different approach (see section 2.4). It
is mainly based on a rule-based system. For the
German?English pair, SYSTRAN achieves a lower
BLEU score in each test set compared to the other
groups. However, since the SYSTRAN system is
very different to the others, we still obtain an im-
provement when we add it also to system combina-
tion.
We did experiments with different optimization
criteria for the system combination optimization.
All results are listed in Table 1 (unoptimized), Table
2 (optimized on BLEU) and Table 3 (optimized on
TER-BLEU). Further, we investigated, whether we
will loose performance, if a single system is dropped
from the system combination. The results show that
for each optimization criteria we need all systems to
achieve the best results.
For the BLEU optimized system combination, we
obtain an improvement compared to the best sin-
gle systems for all dev sets. For newstest2008, we
get an improvement of 1.5 points in BLEU and 1.5
points in TER compared to the best single system of
Karlsruhe Institute of Technology. For newstest2009
we get an improvement of 1.9 points in BLEU and
1.5 points in TER compared to the best single sys-
tem. The system combination of all systems outper-
forms the best single system with 1.9 points in BLEU
and 1.9 points in TER for newstest2010. For new-
stest2011 the improvement is 1.3 points in BLEU
and 2.9 points in TER.
For the TER-BLEU optimized system combina-
tion, we achieved more improvement in TER com-
pared to the BLEU optimized system combination.
For newstest2008, we get an improvement of 0.8
points in BLEU and 3.0 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. The system combinations performs better
on newstest2009 with 1.3 points in BLEU and 2.7
points in TER. For newstest2010, we get an im-
provement of 1.7 points in BLEU and 3.4 points in
TER and for newstest2011 we get an improvement
of 0.7 points in BLEU and 2.5 points in TER.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally, the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU and
a lower TER score compared to each single sys-
tem. For each optimization criteria the system com-
binations using all single systems outperforms the
system combinations using one less single system.
Although the single system of SYSTRAN has the
worst error scores and the RWTH single systems are
similar, we achieved the best result in using all single
systems. For the WMT 12 evaluation, we submitted
the system combination of all systems optimized on
BLEU.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
326
Table 1: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are unoptimized.
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
KIT 22.2 61.8 21.3 61.0 24.1 59.0 22.4 60.2 37.9
RWTH.PBT 21.4 62.0 21.3 61.1 23.9 59.1 21.4 61.2 39.7
Limsi 22.2 63.0 22.0 61.8 23.9 59.9 21.8 62.0 40.2
RWTH.HPBT 21.5 62.6 21.5 61.6 23.6 60.2 21.5 61.8 40.4
SYSTRAN 18.3 64.6 17.9 63.4 21.1 60.5 18.3 63.1 44.8
sc-withAllSystems 23.4 59.7 22.9 59.0 26.2 56.5 23.3 58.8 35.5
sc-without-RWTH.PBT 23.2 59.8 22.8 59.0 25.9 56.6 23.1 58.7 35.6
sc-without-RWTH.HPBT 23.2 59.6 22.7 58.9 26.1 56.2 23.1 58.7 35.6
sc-without-Limsi 22.7 60.1 22.4 59.2 25.5 56.7 22.8 58.8 36.0
sc-without-SYSTRAN 23.0 60.3 22.5 59.5 25.7 57.2 23.1 59.2 36.1
sc-without-KIT 23.0 59.9 22.5 59.1 25.9 56.6 22.9 59.1 36.3
Table 2: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.7 60.3 23.2 59.5 26.0 57.1 23.7 59.2 35.6
sc-without-RWTH.PBT 23.4 61.1 23.1 59.8 25.5 57.6 23.5 59.5 36.1
sc-without-SYSTRAN 23.3 61.1 22.6 60.5 25.3 58.1 23.5 60.0 36.5
sc-without-Limsi 23.1 60.7 22.6 59.7 25.4 57.5 23.3 59.4 36.2
sc-without-KIT 23.4 60.7 23.0 59.7 25.6 57.7 23.3 59.8 36.5
sc-without-RWTH.HPBT 23.3 59.4 22.8 58.6 26.1 56.0 23.1 58.4 35.2
Table 3: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on TER-BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.0 58.8 22.4 58.3 25.8 55.6 23.1 57.7 34.6
sc-without-RWTH.PBT 23.0 59.3 22.5 58.5 25.6 56.0 23.1 58.0 34.9
sc-without-RWTH.HPBT 23.1 59.0 22.6 58.3 25.8 55.6 23.0 58.0 35.0
sc-without-SYSTRAN 22.9 59.7 22.4 59.1 25.6 56.7 23.2 58.5 35.3
sc-without-Limsi 22.7 59.4 22.2 58.7 25.3 56.1 22.7 58.1 35.5
sc-without-KIT 22.9 59.3 22.4 58.5 25.7 55.8 22.7 58.1 35.4
327
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003a. Empirical Methods for
Compound Splitting. In EACL, Budapest, Hungary.
P. Koehn and K. Knight. 2003b. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with neu-
ral networks. In NAACL ?12: Proceedings of the
2012 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology.
Jose? B. Marin?o, R. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram-based machine translation. Computa-
tional Linguistics, 32(4).
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT English-
French Translation Systems for IWSLT 2011. In Pro-
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
J. Niehues and S. Vogel. 2008. Discriminative Word
Alignment via Alignment Matrix Modeling. In Proc.
of Third ACL Workshop on Statistical Machine Trans-
lation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In Pro-
328
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT), San Francisco,
CA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Evelyne
Tzoukermann and SusanEditors Armstrong, editors,
Proceedings of the ACL SIGDATWorkshop, pages 47?
50. Kluwer Academic Publishers.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
329
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 349?355,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2012
Jan Niehues, Yuqi Zhang, Mohammed Mediani, Teresa Herrmann, Eunah Cho and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based SMT
systems developed for our participation
in the WMT12 Shared Translation Task.
Translations for English?German and
English?French were generated using a
phrase-based translation system which is
extended by additional models such as
bilingual, fine-grained part-of-speech (POS)
and automatic cluster language models and
discriminative word lexica. In addition, we
explicitly handle out-of-vocabulary (OOV)
words in German, if we have translations for
other morphological forms of the same stem.
Furthermore, we extended the POS-based
reordering approach to also use information
from syntactic trees.
1 Introduction
In this paper, we describe our systems for the
NAACL 2012 Seventh Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French. We use a
phrase-based decoder that can use lattices as input
and developed several models that extend the stan-
dard log-linear model combination of phrase-based
MT. In addition to the POS-based reordering model
used in past years, for German-English we extended
it to also use rules learned using syntax trees.
The translation model was extended by the bilin-
gual language model and a discriminative word lex-
icon using a maximum entropy classifier. For the
French-English and English-French translation sys-
tems, we also used phrase table adaptation to avoid
overestimation of the probabilities of the huge, but
noisy Giga corpus. In the German-English system,
we tried to learn translations for OOV words by ex-
ploring different morphological forms of the OOVs
with the same lemma.
Furthermore, we combined different language
models in the log-linear model. We used word-
based language models trained on different parts of
the training corpus as well as POS-based language
models using fine-grained POS information and lan-
guage models trained on automatic word clusters.
The paper is organized as follows: The next sec-
tion gives a detailed description of our systems in-
cluding all the models. The translation results for
all directions are presented afterwards and we close
with a conclusion.
2 System Description
For the French?English systems the phrase table
is based on a GIZA++ word alignment, while the
systems for German?English use a discriminative
word alignment as described in Niehues and Vogel
(2008). The language models are 4-gram SRI lan-
guage models using Kneser-Ney smoothing trained
by the SRILM Toolkit (Stolcke, 2002).
The problem of word reordering is addressed with
POS-based and tree-based reordering models as de-
scribed in Section 2.3. The POS tags used in the
reordering model are obtained using the TreeTagger
(Schmid, 1994). The syntactic parse trees are gen-
erated using the Stanford Parser (Rafferty and Man-
ning, 2008).
An in-house phrase-based decoder (Vogel, 2003)
is used to perform translation. Optimization with
349
regard to the BLEU score is done using Minimum
Error Rate Training as described in Venugopal et al
(2005). During decoding only the top 10 translation
options for every source phrase are considered.
2.1 Data
Our translation models were trained on the EPPS
and News Commentary (NC) corpora. Furthermore,
the additional available data for French and English
(i.e. UN and Giga corpora) were exploited in the
corresponding systems.
The systems were tuned with the news-test2011
data, while news-test2011 was used for testing in all
our systems. We trained language models for each
language on the monolingual part of the training cor-
pora as well as the News Shuffle and the Gigaword
(version 4) corpora. The discriminative word align-
ment model was trained on 500 hand-aligned sen-
tences selected from the EPPS corpus.
2.2 Preprocessing
The training data is preprocessed prior to training
the system. This includes normalizing special sym-
bols, smart-casing the first word of each sentence
and removing long sentences and sentences with
length mismatch.
For the German parts of the training corpus, in
order to obtain a homogenous spelling, we use the
hunspell1 lexicon to map words written according to
old German spelling rules to new German spelling
rules.
In order to reduce the OOV problem of German
compound words, Compound splitting as described
in Koehn and Knight (2003) is applied to the Ger-
man part of the corpus for the German-to-English
system.
The Giga corpus received a special preprocessing
by removing noisy pairs using an SVM classifier as
described in Mediani et al (2011). The SVM clas-
sifier training and test sets consist of randomly se-
lected sentence pairs from the corpora of EPPS, NC,
tuning, and test sets. Giving at the end around 16
million sentence pairs.
2.3 Word Reordering
In contrast to modeling the reordering by a distance-
based reordering model and/or a lexicalized distor-
1http://hunspell.sourceforge.net/
tion model, we use a different approach that relies on
POS sequences. By abstracting from surface words
to POS, we expect to model the reordering more ac-
curately. For German-to-English, we additionally
apply reordering rules learned from syntactic parse
trees.
2.3.1 POS-based Reordering Model
In order to build the POS-based reordering model,
we first learn probabilistic rules from the POS tags
of the training corpus and the alignment. Contin-
uous reordering rules are extracted as described in
Rottmann and Vogel (2007) to model short-range re-
orderings. When translating between German and
English, we apply a modified reordering model with
non-continuous rules to cover also long-range re-
orderings (Niehues and Kolss, 2009).
2.3.2 Tree-based Reordering Model
Word order is quite different between German and
English. And during translation especially verbs or
verb particles need to be shifted over a long dis-
tance in a sentence. Using discontinuous POS rules
already improves the translation tremendously. In
addition, we apply a tree-based reordering model
for the German-English translation. Syntactic parse
trees provide information about the words in a sen-
tence that form constituents and should therefore be
treated as inseparable units by the reordering model.
For the tree-based reordering model, syntactic parse
trees are generated for the whole training corpus.
Then the word alignment between the source and
target language part of the corpus is used to learn
rules on how to reorder the constituents in a Ger-
man source sentence to make it matches the English
target sentence word order better. In order to apply
the rules to the source text, POS tags and a parse
tree are generated for each sentence. Then the POS-
based and tree-based reordering rules are applied.
The original order of words as well as the reordered
sentence variants generated by the rules are encoded
in a word lattice. The lattice is then used as input to
the decoder.
For the test sentences, the reordering based on
POS and trees allows us to change the word order
in the source sentence so that the sentence can be
translated more easily. In addition, we build reorder-
ing lattices for all training sentences and then extract
350
phrase pairs from the monotone source path as well
as from the reordered paths.
2.4 Translation Models
In addition to the models used in the baseline system
described above, we conducted experiments includ-
ing additional models that enhance translation qual-
ity by introducing alternative or additional informa-
tion into the translation modeling process.
2.4.1 Phrase table adaptation
Since the Giga corpus is huge, but noisy, it is
advantageous to also use the translation probabil-
ities of the phrase pair extracted only from the
more reliable EPPS and News commentary cor-
pus. Therefore, we build two phrase tables for the
French?English system. One trained on all data
and the other only trained on the EPPS and News
commentary corpus. The two models are then com-
bined using a log-linear combination to achieve the
adaptation towards the cleaner corpora as described
in (Niehues et al, 2010). The newly created trans-
lation model uses the four scores from the general
model as well as the two smoothed relative frequen-
cies of both directions from the smaller, but cleaner
model. If a phrase pair does not occur in the in-
domain part, a default score is used instead of a rela-
tive frequency. In our case, we used the lowest prob-
ability.
2.4.2 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder according to the best com-
bination of phrases that maximize the translation
and language model scores. This segmentation into
phrases leads to the loss of context information at
the phrase boundaries. Although more target side
context is available to the language model, source
side context would also be valuable for the decoder
when searching for the best translation hypothesis.
To make also source language context available we
use a bilingual language model, in which each token
consists of a target word and all source words it is
aligned to. The bilingual tokens enter the translation
process as an additional target factor and the bilin-
gual language model is applied to the additional fac-
tor like a normal language model. For more details
see Niehues et al (2011).
2.4.3 Discriminative Word Lexica
Mauser et al (2009) have shown that the use
of discriminative word lexica (DWL) can improve
the translation quality. For every target word, they
trained a maximum entropy model to determine
whether this target word should be in the translated
sentence or not using one feature per one source
word.
When applying DWL in our experiments, we
would like to have the same conditions for the train-
ing and test case. For this we would need to change
the score of the feature only if a new word is added
to the hypothesis. If a word is added the second time,
we do not want to change the feature value. In order
to keep track of this, additional bookkeeping would
be required. Also the other models in our translation
system will prevent us from using a word too often.
Therefore, we ignore this problem and can calcu-
late the score for every phrase pair before starting
with the translation. This leads to the following def-
inition of the model:
p(e|f) =
J?
j=1
p(ej |f) (1)
In this definition, p(ej |f) is calculated using a max-
imum likelihood classifier.
Each classifier is trained independently on the
parallel training data. All sentences pairs where the
target word e occurs in the target sentence are used
as positive examples. We could now use all other
sentences as negative examples. But in many of
these sentences, we would anyway not generate the
target word, since there is no phrase pair that trans-
lates any of the source words into the target word.
Therefore, we build a target vocabulary for every
training sentence. This vocabulary consists of all
target side words of phrase pairs matching a source
phrase in the source part of the training sentence.
Then we use all sentence pairs where e is in the tar-
get vocabulary but not in the target sentences as neg-
ative examples. This has shown to have a postive
influence on the translation quality (Mediani et al,
2011) and also reduces training time.
2.4.4 Quasi-Morphological Operations for
OOV words
Since German is a highly inflected language, there
will be always some word forms of a given Ger-
351
Figure 1: Quasi-morphological operations
man lemma that did not occur in the training data.
In order to be able to also translate unseen word
forms, we try to learn quasi-morphological opera-
tions that change the lexical entry of a known word
form to the unknown word form. These have shown
to be beneficial in Niehues and Waibel (2011) using
Wikipedia2 titles. The idea is illustrated in Figure 1.
If we look at the data, our system is able to trans-
late a German word Kamin (engl. chimney), but not
the dative plural form Kaminen. To address this
problem, we try to automatically learn rules how
words can be modified. If we look at the example,
we would like the system to learn the following rule.
If an ?en? is appended to a German word, as it is
done when creating the dative plural form of Kami-
nen, we need to add an ?s? to the end of the English
word in order to perform the same morphological
word transformation. We use only rules where the
ending of the word has at most 3 letters.
Depending on the POS, number, gender or case of
the involved words, the same operation on the source
side does not necessarily correspond to the same op-
eration on the target side.
To account for this ambiguity, we rank the differ-
ent target operation using the following four features
and use the best ranked one. Firstly, we should not
generate target words that do not exist. Here, we
have an advantage that we can use monolingual data
to determine whether the word exists. In addition,
a target operation that often coincides with a given
source operation should be better than one that is
rarely used together with the source operation. We
therefore look at pairs of entries in the lexicon and
count in how many of them the source operation can
be applied to the source side and the target operation
can be applied to the target side. We then use only
operations that occur at least ten times. Furthermore,
2http://www.wikipedia.org/
we use the ending of the source and target word to
determine which pair of operations should be used.
Integration We only use the proposed method for
OOVs and do not try to improve translations of
words that the baseline system already covers. We
look for phrase pairs, for which a source operation
ops exists that changes one of the source words f1
into the OOV word f2. Since we need to apply a
target operation to one word on the target side of the
phrase pair, we only consider phrase pairs where f1
is aligned to one of the target words of the phrase
containing e1. If a target operation exists given f1
and ops, we select the one with the highest rank.
Then we generate a new phrase pair by applying
ops to f1 and opt to e1 keeping the original scores
from the phrase pairs, since the original and syn-
thesized phrase pair are not directly competing any-
way. We do not add several phrase pairs generated
by different operations, since we would then need to
add the features used for ranking the operations into
the MERT. This is problematic, since the operations
were only used for very few words and therefore a
good estimation of the weights is not possible.
2.5 Language Models
The 4-gram language models generated by the
SRILM toolkit are used as the main language mod-
els for all of our systems. For English-French and
French-English systems, we use a good quality cor-
pus as in-domain data to train in-domain language
models. Additionally, we apply the POS and clus-
ter language models in different systems. All lan-
guage models are integrated into the translation sys-
tem by a log-linear combination and received opti-
mal weights during tuning by the MERT.
2.5.1 POS Language Models
The POS language model is trained on the POS
sequences of the target language. In this evalua-
tion, the POS language model is applied for the
English-German system. We expect that having ad-
ditional information in form of probabilities of POS
sequences should help especially in case of the rich
morphology of German. The POS tags are gener-
ated with the RFTagger (Schmid and Laws, 2008)
for German, which produces fine-grained tags that
include person, gender and case information. We
352
use a 9-gram language model on the News Shuf-
fle corpus and the German side of all parallel cor-
pora. More details and discussions about the POS
language model can be found in Herrmann et al
(2011).
2.5.2 Cluster Language Models
The cluster language model follows a similar idea
as the POS language model. Since there is a data
sparsity problem when we substitute words with the
word classes, it is possible to make use of larger
context information. In the POS language model,
POS tags are the word classes. Here, we generated
word classes in a different way. First, we cluster
the words in the corpus using the MKCLS algorithm
(Och, 1999) given a number of classes. Second, we
replace the words in the corpus by their cluster IDs.
Finally, we train an n-gram language model on this
corpus consisting of cluster IDs. Generally, all clus-
ter language models used in our systems are 5-gram.
3 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to the
workshop. The following sections describe the ex-
periments for the individual language pairs and show
the translation results. The results are reported as
case-sensitive BLEU scores (Papineni et al, 2002)
on one reference translation.
3.1 German-English
The experiments for the German-English translation
system are summarized in Table 1. The Baseline
system uses POS-based reordering, discriminative
word alignment and a language model trained on the
News Shuffle corpus. By adding lattice phrase ex-
traction small improvements of the translation qual-
ity could be gained.
Further improvements could be gained by adding
a language model trained on the Gigaword corpus
and adding a bilingual and cluster-based language
model. We used 50 word classes and trained a 5-
gram language model. Afterwards, the translation
quality was improved by also using a discriminative
word lexicon. Finally, the best system was achieved
by using Tree-based reordering and using special
treatment for the OOVs. This system generates a
BLEU score of 22.31 on the test data. For the last
two systems, we did not perform new optimization
runs.
System Dev Test
Baseline 23.64 21.32
+ Lattice Phrase Extraction 23.76 21.36
+ Gigaward Language Model 24.01 21.73
+ Bilingual LM 24.19 21.91
+ Cluster LM 24.16 22.09
+ DWL 24.19 22.19
+ Tree-based Reordering - 22.26
+ OOV - 22.31
Table 1: Translation results for German-English
3.2 English-German
The English-German baseline system uses also
POS-based reordering, discriminative word align-
ment and a language model based on EPPS, NC and
News Shuffle. A small gain could be achieved by the
POS-based language model and the bilingual lan-
guage model. Further gain was achieved by using
also a cluster-based language model. For this lan-
guage model, we use 100 word classes and trained
a 5-gram language model. Finally, the best system
uses the discriminative word lexicon.
System Dev Test
Baseline 17.06 15.57
+ POSLM 17.27 15.63
+ Bilingual LM 17.40 15.78
+ Cluster LM 17.77 16.06
+ DWL 17.75 16.28
Table 2: Translation results for English-German
3.3 English-French
Table 3 summarizes how our English-French sys-
tem evolved. The baseline system here was trained
on the EPPS, NC, and UN corpora, while the lan-
guage model was trained on all the French part of
the parallel corpora (including the Giga corpus). It
also uses short-range reordering trained on EPPS
and NC. This system had a BLEU score of around
26.7. The Giga parallel data turned out to be quite
353
beneficial for this task. It improves the scores by
more than 1 BLEU point. More importantly, addi-
tional language models boosted the system quality:
around 1.8 points. In fact, three language models
were log-linearly combined: In addition to the afore-
mentioned, two additional language models were
trained on the monolingual sets (one for News and
one for Gigaword). We could get an improvement
of around 0.2 by retraining the reordering rules on
EPPS and NC only, but using Giza alignment from
the whole data. Adapting the translation model by
using EPPS and NC as in-domain data improves the
BLEU score by only 0.1. This small improvement
might be due to the fact that the news domain is
very broad and that the Giga corpus has already been
carefully cleaned and filtered. Furthermore, using a
bilingual language model enhances the BLEU score
by almost 0.3. Finally, incorporating a cluster lan-
guage model adds an additional 0.1 to the score.
This leads to a system with 30.58.
System Dev Test
Baseline 24.96 26.67
+ GigParData 26.12 28.16
+ Big LMs 29.22 29.92
+ All Reo 29.14 30.10
+ PT Adaptation 29.15 30.22
+ Bilingual LM 29.17 30.49
+ Cluster LM 29.08 30.58
Table 3: Translation results for English-French
3.4 French-English
The development of our system for the French-
English direction is summarized in Table 4. The
baseline system for this direction was trained on the
EPPS, NC, UN and Giga parallel corpora, while the
language model was trained on the French part of the
parallel training corpora. The baseline system in-
cludes the POS-based reordering model with short-
range rules. The largest improvement of 1.7 BLEU
score was achieved by the integration of the bigger
language models which are trained on the English
version of News Shuffle and the Gigaword corpus
(v4). We did not add the language models from the
monolingual English version of EPPS and NC data,
since the experiments have shown that they did not
provide improvement in our system. The second
largest improvement came from the domain adap-
tation that includes an in-domain language model
and adaptations to the phrase extraction. The BLEU
score has improved about 1 BLEU in total. The in-
domain data we used here are parallel EPPS and NC
corpus. Further gains were obtained by augmenting
the system with a bilingual language model adding
around 0.2 BLEU to the previous score. The sub-
mitted system was obtained by adding the cluster
5-gram language model trained on the News Shuf-
fle corpus with 100 clusters and thus giving 30.25 as
the final score.
System Dev Test
Baseline 25.81 27.15
+ Indomain LM 26.17 27.91
+ PT Adaptation 26.33 28.11
+ Big LMs 28.90 29.82
+ Bilingual LM 29.14 30.09
+ Cluster LM 29.31 30.25
Table 4: Translation results for French-English
4 Conclusions
We have presented the systems for our participation
in the WMT 2012 Evaluation for English?German
and English?French. In all systems we could im-
prove by using a class-based language model. Fur-
thermore, the translation quality could be improved
by using a discriminative word lexicon. Therefore,
we trained a maximum entropy classifier for ev-
ery target word. For English?French, adapting the
phrase table helps to avoid using wrong parts of the
noisy Giga corpus. For the German-to-English sys-
tem, we could improve the translation quality addi-
tionally by using a tree-based reordering model and
by special handling of OOV words. For the inverse
direction we could improve the translation quality
by using a 9-gram language model trained on the
fine-grained POS tags.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
354
References
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The karlsruhe institute of
technology translation systems for the wmt 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 379?385, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In EACL, Budapest,
Hungary.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The kit english-
french translation systems for iwslt 2011. In Proceed-
ings of the eight International Workshop on Spoken
Language Translation (IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using wikipedia to
translate domain-specific terms in smt. In Proceedings
of the eight International Workshop on Spoken Lan-
guage Translation (IWSLT).
Jan Niehues, Mohammed Mediani, Teresa Herrmann,
Michael Heck, Christian Herff, and Alex Waibel.
2010. The KIT Translation system for IWSLT 2010.
In Marcello Federico, Ian Lane, Michael Paul, and
Franc?ois Yvon, editors, Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 93?98.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Associa-
tion for Computational Linguistics, EACL ?99, pages
71?76, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division, T. J.
Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three german treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sko?vde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation of
Conditional Probabilities with Decision Trees and an
Application to Fine-Grained POS Tagging. In COL-
ING 2008, Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Beyond
(WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
355
Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39?47,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Combining Word Reordering Methods on different Linguistic Abstraction
Levels for Statistical Machine Translation
Teresa Herrmann, Jan Niehues, Alex Waibel
Institute for Anthropomatics
Karlsruhe Institute of Technology
Karlsruhe, Germany
{teresa.herrmann,jan.niehues,alexander.waibel}@kit.edu
Abstract
We describe a novel approach to combin-
ing lexicalized, POS-based and syntactic tree-
based word reordering in a phrase-based ma-
chine translation system. Our results show
that each of the presented reordering meth-
ods leads to improved translation quality on its
own. The strengths however can be combined
to achieve further improvements. We present
experiments on German-English and German-
French translation. We report improvements
of 0.7 BLEU points by adding tree-based and
lexicalized reordering. Up to 1.1 BLEU points
can be gained by POS and tree-based reorder-
ing over a baseline with lexicalized reorder-
ing. A human analysis, comparing subjec-
tive translation quality as well as a detailed er-
ror analysis show the impact of our presented
tree-based rules in terms of improved sentence
quality and reduction of errors related to miss-
ing verbs and verb positions.
1 Introduction
One of the main difficulties in statistical machine
translation (SMT) is presented by the different word
orders between languages. Most state-of-the-art
phrase-based SMT systems handle it within phrase
pairs or during decoding by allowing words to be
swapped while translation hypotheses are generated.
An additional reordering model might be included in
the log-linear model of translation. However, these
methods can cover reorderings only over a very lim-
ited distance. Recently, reordering as preprocessing
has drawn much attention. The idea is to detach the
reordering problem from the decoding process and
to apply a reordering model prior to translation in
order to facilitate a monotone translation.
Encouraged by the improvements that can be
achieved with part-of-speech (POS) reordering rules
(Niehues and Kolss, 2009; Rottmann and Vogel,
2007), we apply such rules on a different linguis-
tic level. We abstract from the words in the sentence
and learn reordering rules based on syntactic con-
stituents in the source language sentence. Syntac-
tic parse trees represent the sentence structure and
show the relations between constituents in the sen-
tence. Relying on syntactic constituents instead of
POS tags should help to model the reordering task
more reliably, since sentence constituents are moved
as whole blocks of words, thus keeping the sentence
structure intact.
In addition, we combine the POS-based and syn-
tactic tree-based reordering models and also add a
lexicalized reordering model, which is used in many
state-of-the-art phrase-based SMT systems nowa-
days.
2 Related Work
The problem of word reordering has been addressed
by several approaches over the last years.
In a phrase-based SMT system reordering can
be achieved during decoding by allowing swaps of
words within a defined window. Lexicalized re-
ordering models (Koehn et al, 2005; Tillmann,
2004) include information about the orientation of
adjacent phrases that is learned during phrase extrac-
tion. This reordering method, which affects the scor-
ing of translation hypotheses but does not generate
new reorderings, is used e.g. in the open source ma-
39
chine translation system Moses (Koehn et al, 2007).
Syntax-based (Yamada and Knight, 2001) or
syntax-augmented (Zollmann and Venugopal, 2006)
MT systems address the reordering problem by em-
bedding syntactic analysis in the decoding process.
Hierarchical MT systems (Chiang, 2005) construct
a syntactic hierarchy during decoding, which is in-
dependent of linguistic categories.
To our best knowledge Xia and McCord (2004)
were the first to model the word reordering problem
as a preprocessing step. They automatically learn
reordering rules for English-French translation from
source and target language dependency trees. After-
wards, many followed these footsteps. Earlier ap-
proaches craft reordering rules manually based on
syntactic or dependency parse trees or POS tags de-
signed for particular languages (Collins et al, 2005;
Popovic? and Ney, 2006; Habash, 2007; Wang et al,
2007). Later there were more and more approaches
using data-driven methods. Costa-jussa` and Fonol-
losa (2006) frame the word reordering problem as
a translation task and use word class information
to translate the original source sentence into a re-
ordered source sentence that can be translated more
easily. A very popular approach is to automatically
learn reordering rules based on POS tags or syn-
tactic chunks (Popovic? and Ney, 2006; Rottmann
and Vogel, 2007; Zhang et al, 2007; Crego and
Habash, 2008). Khalilov et al (2009) present re-
ordering rules learned from source and target side
syntax trees. More recently, Genzel (2010) proposed
to automatically learn reordering rules from IBM1
alignments and source side dependency trees. In
DeNero and Uszkoreit (2011) no parser is needed,
but the sentence structure used for learning the re-
ordering model is induced automatically from a par-
allel corpus. Among these approaches most are able
to cover short-range reorderings and some store re-
ordering variants in a word lattice leaving the selec-
tion of the path to the decoder. Long-range reorder-
ings are addressed by manual rules (Collins et al,
2005) or using automatically learned rules (Niehues
and Kolss, 2009).
Motivated by the POS-based reordering models
in Niehues and Kolss (2009) and Rottmann and Vo-
gel (2007), we present a reordering model based on
the syntactic structure of the source sentence. We
intend to cover both short-range and long-range re-
ordering more reliably by abstracting to constituents
extracted from syntactic parse trees instead of work-
ing only with morphosyntactic information on the
word level. Furthermore, we combine POS-based
and tree-based models and additionally include a
lexicalized reordering model. Altogether we apply
word reordering on three different levels: lexical-
ized reordering model on the word level, POS-based
reordering on the morphosyntactic level and syntax
tree-based reordering on the constituent level. In
contrast to previous work we use original syntactic
parse trees instead of binarized parse trees or depen-
dency trees. Furthermore, our goal is to address es-
pecially long-range reorderings involving verb con-
structions.
3 Motivation
When translating from German to English different
word order is the most prominent problem. Espe-
cially the verb needs to be shifted over long dis-
tances in the sentence, since the position of the verb
differs in German and English sentences. The finite
verbs in the English language are generally located
at the second position in the sentence. In German
this is only the case in a main clause. In German
subordinate clauses the verb is at the final position
as shown in Example 1.
Example 1:
Source: ..., nachdem ich eine Weile im Inter-
net gesucht habe.
Gloss: ... after I a while in-the internet
searched have.
POS Reord.: ..., nachdem ich habe eine Weile im
Internet gesucht.
POS Transl.: ... as I have for some time on the
Internet.
The example shows first the source sentence and
an English gloss. POS Reord presents the reordered
source sentence as produced by POS rules. This
should be the source sentence according to target
language word order. POS Transl shows the trans-
lation of the reordered sequence. We can see that
some cases remain unresolved. The POS rules suc-
ceed in putting the auxiliary habe/have to the right
position in the sentence. But the participle, carry-
ing the main meaning of the sentence, is not shifted
together with the auxiliary. During translation it is
40
dropped from the sentence, rendering it unintelligi-
ble.
A reason why the POS rules do not shift both
parts of the verb might be that the rules operate on
the word level only and treat every POS tag inde-
pendently of the others. A reordering model based
on syntactic constituents can help with this. Addi-
tional information about the syntactic structure of
the sentence allows to identify which words belong
together and should not be separated, but shifted as
a whole block. Abstracting from the word level to
the constituent level also provides the advantage that
even though reorderings are performed over long
sentence spans, the rules consist of less reordering
units (constituents which themselves consist of con-
stituents or words) and can be learned more reliably.
4 Tree-based Reordering
In order to encourage linguistically meaningful re-
orderings we learn rules based on syntactic tree con-
stituents. While the POS-based rules are flat and
perform the reordering on a sequence of words, the
tree-based rules operate on subtrees in the parse tree
as shown in Figure 1.
VP
VVPPNPPTNEG
?
VP
NPVVPPPTNEG
Figure 1: Example reordering rule based on subtrees
A syntactic parse tree contains both the word-
level categories, i.e. parts-of-speech and higher or-
der categories, i.e. constituents. In this way it pro-
vides information about the building blocks of a sen-
tence that belong together and should not be taken
apart by reordering. Consequently, the tree-based
reordering operates both on the word level and on
the constituent level to make use of all available in-
formation in the parse tree. It is able to handle long-
range reorderings as well as short-range reorder-
ings, depending on how many words the reordered
constituents cover. The tree-based reordering rules
should also be more stable and introduce less ran-
dom word shuffling than the POS-based rules.
The reordering model consists of two stages. First
the rule extraction, where the rules are learned by
searching the training corpus for crossing align-
ments which indicate a reordering between source
and target language. The second is the application
of the learned reordering rules to the input text prior
to translation.
4.1 Rule Extraction
As shown in Figure 4 we learn rules like this:
VP PTNEG NP VVPP? VP PTNEG VVPP NP
where the first item in the rule is the head node of
the subtree and the rest represent the children. In
the second part of the rule the children are indexed
so that children of the same category cannot be con-
fused. Figure 2 shows an example for rule extrac-
tion: a sentence in its syntactic parse tree representa-
tion, the sentence in the target language and an auto-
matically generated alignment. A reordering occurs
between the constituents VVPP and NP.
S
1-n
CS
...
VP
2-5
VVPP
3-3
gewa?hlt
NP
4-5
NN
5-5
Szenarien
ADJA
4-4
ku?nstliche
PTNEG
2-2
nicht
VAFIN
2-2
haben
PPER
1-1
Wir
1
We
2
didn?t
3
choose
4
artificial
5
scenarios
Figure 2: Example training sentence used to extract re-
ordering rules
In a first step the reordering rule has to be found.
We extract the rules from a word aligned corpus
where a syntactic parse tree is provided for each
source side sentence. We traverse the tree top down
and scan each subtree for reorderings, i.e. cross-
ings of alignment links between source and target
sentence. If there is a reordering, we extract a
rule that rearranges the source side constituents ac-
cording to the order of the corresponding words on
41
the target side. Each constituent in a subtree com-
prises one or more words. We determine the lowest
(min) and highest (max) alignment point for each
constituent ck and thus determine the range of the
constituent on the target side. This can be formal-
ized as min(ck) = min{j|fi ? ck; ai = j} and
max(ck) = max{j|fi ? ck; ai = j}. To illustrate
the process, we have annotated the parse tree in Fig-
ure 2 with the alignment points (min-max) for each
constituent.
After defining the range, we check for the follow-
ing conditions in order to determine whether to ex-
tract a reordering rule.
1. all constituents have a non-empty range
2. source and target word order differ
First, for each subtree at least one word in each con-
stituent needs to be aligned. Otherwise it is not pos-
sible to determine a conclusive order. Second, we
check whether there is actually a reordering, i.e. the
target language words are not in the same order as
the constituents in the source language: min(ck) >
min(ck+1) and max(ck) > max(ck+1).
Once we find a reordering rule to extract, we cal-
culate the probability of this rule as the relative fre-
quency with which such a reordering occurred in all
subtrees of the training corpus divided by the num-
ber of total occurrences of this subtree in the corpus.
We only store rules for reorderings that occur more
than 5 times in the corpus.
4.1.1 Partial Rules
The syntactic parse trees of German sentences are
quite flat, i.e. a subtree usually has many children.
When a rule is extracted, it always consists of the
head of the subtree and all its children. The ap-
plication requires that the applicable rule matches
the complete subtree: the head and all its children.
However, most of the time only some of the chil-
dren are actually involved in a reordering. There
are also many different subtree variants that are quite
similar. In verb phrases or noun phrases, for exam-
ple, modifiers such as prepositional phrases or ad-
verbial phrases can be added nearly arbitrarily. In
order to generalize the tree-based reordering rules,
we extend the rule extraction. We do not only extract
the rules from the complete child sequence, but also
from any continuous child sequence in a constituent.
This way, we extract generalized rules which can
be applied more often. Formally, for each subtree
h ? cn1 = c1c2...cn that matches the constraints
presented in Section 4.1, we modify the basic rule
extraction to: ?i, j1 ? i < j ? n : h ? cji . It
could be argued that the partial rules might be not
as reliable as the specific rules. In Section 6 we will
show that such generalizations are meaningful and
can have a positive effect on the translation quality.
4.2 Rule Application
During the training of the system all reordering rules
are extracted from the parallel corpus. Prior to trans-
lation the rules are applied to the original source text.
Each rule is applied independently producing a re-
ordering variant of that sentence. The original sen-
tence and all reordering variants are stored in a word
lattice which is later used as input to the decoder.
The rules may be applied recursively to already re-
ordered paths. If more than one rule can be applied,
all paths are added to the lattice unless the rules gen-
erate the same output. In this case only the rule with
the highest probability is applied.
The edges in a word lattice for one sentence are
assigned transition probabilities as follows. In the
monotone path with original word order all transi-
tion probabilities are initially set to 1. In a reordered
path the first branching transition is assigned the
probability of the rule that generated the path. All
other transition probabilities in this path are set to 1.
Whenever a reordered path branches from the mono-
tone path, the probability of the branching edge is
substracted from the probability of the monotone
edge. However, a minimum probability of 0.05 is
reserved for the monotone edge. The score of the
complete path is computed as the product of the tran-
sition probabilities. During decoding the best path
is searched for by including the score for the cur-
rent path weighted by the weight for the reordering
model in the log-linear model. In order to enable
efficient decoding we limit the lattice size by only
applying rules with a probability higher than a pre-
defined threshold.
4.2.1 Recursive Rule Application
As mentioned above, the tree-based rules may be
applied recursively. That means, after one rule is
applied to the source sentence, a reordered path may
42
SS
aus anderen Bundesla?ndern
PP
VAFIN
habe
VP
VVPP
bekommen
viele Anfragen
NP
ADV
schon
PPER
ich
KOUS
dass
Ich kann Ihnen nur sagen,
...
I may just tell you that I got already lots of requests from other federal states
Figure 3: Example parse tree with separated verb particles
be reordered again. The reason is the structure of
the syntactic parse trees. Verbs and their particles
are typically not located within the same subtree.
Hence, they cannot be covered by one reordering
rule. A separate rule is extracted for each subtree.
Figure 3 demonstrates this in an example. The two
parts that belong to the verb in this German sentence,
namely bekommen and habe, are not located within
the same constituent. The finite verb habe forms a
constituent of its own and the participle bekommen
forms part of the VP constituent. In English the fi-
nite verb and the participle need to be placed next to
each other. In order to rearrange the source language
words according to the target language word order,
the following two reordering movements need to be
performed: the finite verb habe needs to be placed
before the VP constituent and the participle bekom-
men needs to be moved within the VP constituent to
the first position. Only if both movements are per-
formed, the right word order can be generated.
However, the reordering model only considers
one subtree at a time when extracting reordering
rules. In this case two rules are learned, but if they
are applied to the source sentence separately, they
will end up in separate paths in the word lattice. The
decoder then has to choose which path to translate:
the one where the finite verb is placed before the VP
constituent or the path where the participle is at the
first position in the VP constituent.
To counter this drawback the rules may be applied
recursively to the new paths created by our reorder-
ing rules. We use the same rules, but newly created
paths are fed back into the queue of sentences to be
reordered. However, we only apply the rules to parts
of the reordered sentence that are still in the original
word order and restrict the recursion depth.
5 Combining reordering methods
In order to get a deeper insight into their individ-
ual strengths we compare the reordering methods on
different linguistic levels and also combine them to
investigate whether gains can be increased. We ad-
dress the word level using the lexicalized reordering,
the morphosyntactic level by POS-based reordering
and the constituent level by tree-based reordering.
5.1 POS-based and tree-based rules
The training of the POS-based reordering is per-
formed as described in (Rottmann and Vogel,
2007) for short-range reordering rules, such as
VVIMP VMFIN PPER ? PPER VMFIN VVIMP.
Long-range reordering rules trained according
to (Niehues and Kolss, 2009) include gaps match-
ing longer spans of arbitrary POS sequences
(VAFIN * VVPP ? VAFIN VVPP *). The POS-
based reordering used in our experiments always in-
cludes both short and long-range rules.
The tree-based rules are trained separately as de-
scribed above. First the POS-based rules are applied
to the monotone path of the source sentence and then
43
the tree-based rules are applied independently, pro-
ducing separate paths.
5.2 Rule-based and lexicalized reordering
As described in Section 4.2 we create word lattices
that encode the reordering variants. The lexical-
ized reordering model stores for each phrase pair
the probabilities for possible reordering orientations
at the incoming and outgoing phrase boundaries:
monotone, swap and discontinuous. In order to ap-
ply the lexicalized reordering model on lattices the
original position of each word is stored in the lat-
tice. While the translation hypothesis is generated,
the reordering orientation with respect to the origi-
nal position of the words is checked at each phrase
boundary. The probability for the respective orien-
tation is included as an additional score.
6 Results
The tree-based models are applied for German-
English and German-French translation. Results are
measured in case-sensitive BLEU (Papineni et al,
2002).
6.1 General System Description
First we describe the general system architecture
which underlies all the systems used later on. We
use a phrase-based decoder (Vogel, 2003) that takes
word lattices as input. Optimization is performed
using MERT with respect to BLEU. All POS-based
or tree-based systems apply monotone translation
only. Baseline systems without reordering rules use
a distance-based reordering model. In addition, a
lexicalized reordering model as described in (Koehn
et al, 2005) is applied where indicated. POS tags
and parse trees are generated using the Tree Tag-
ger (Schmid, 1994) and the Stanford Parser (Raf-
ferty and Manning, 2008).
6.1.1 Data
The German-English system is trained on the pro-
vided data of the WMT 2012. news-test2010 and
news-test2011 are used for development and test-
ing. The type of data used for training, development
and testing the German-French system is similar to
WMT data, except that 2 references are available.
The training corpus for the reordering models con-
sist of the word-aligned Europarl and News Com-
mentary corpora where POS tags and parse trees are
generated for the source side.
6.2 German-English
We built systems using POS-based and tree-based
reordering and show the impact of the individual
models as well as their combination on the transla-
tion quality. The results are presented in Table 1.
For each system, two different setups were evalu-
ated. First, with a distance-based reordering model
only (noLexRM) and with an additional lexicalized
reordering model (LexRM). The baseline system
which uses no reordering rules at all allows a re-
ordering window of 5 in the decoder for both setups.
For all systems where reordering rules are applied,
monotone translation is performed. Since the rules
take over the main reordering job, only monotone
translation is necessary from the reordered word lat-
tice input. In this experiment, we compare the tree-
based rules with and without recursion, and the par-
tial rules.
Rule Type
System noLexRM LexRM
Dev Test Dev Test
Baseline (no Rules) 22.82 21.06 23.54 21.61
POS 24.33 21.98 24.42 22.15
Tree 24.01 21.92 24.24 22.01
Tree rec. 24.37 21.97 24.53 22.19
Tree rec.+ par. 24.31 22.21 24.65 22.27
POS + Tree 24.57 22.21 24.91 22.47
POS + Tree rec. 24.61 22.39 24.81 22.45
POS + Tree rec.+ par. 24.80 22.45 24.78 22.70
Table 1: German-English
Compared to the baseline system using distance-
based reordering only, 1.4 BLEU points can be
gained by applying combined POS and tree-based
reordering. The tree rules including partial rules and
recursive application alone achieve already a bet-
ter performance than the POS rules, but using them
all in combination leads to an improvement of 0.4
BLEU points over the POS-based reordering alone.
When lexicalized reordering is added, the relative
improvements are similar: 1.1 BLEU points com-
pared to the Baseline and 0.55 BLEU points over the
POS-based reordering. We can therefore argue that
the individual rule types as well as the lexicalized re-
ordering model seem to address complementary re-
ordering issues and can be combined successfully to
44
obtain an even better translation quality.
We applied only tree rules with a probability of
0.1 and higher. Partial rules require a threshold of
0.4 to be applied, since they are less reliable. In or-
der to prevent the lattices from growing too large,
the recursive rule application is restricted to a max-
imum recursion depth of 3. These values were set
according to the results of preliminary experiments
investigating the impact of the rule probabilities on
the translation quality. Normal rules and partial rules
are not mixed during recursive application.
With the best system we performed a final exper-
iment on the official testset of the WMT 2012 and
achieved a score of 23.73 which is 0.4 BLEU points
better than the best constrained submission.
6.3 Translation Examples
Example 2 shows how the translation of the sen-
tence presented above is improved by adding the
tree-based rules. We can see that using tree con-
stituents in the reordering model indeed addresses
the problem of verb particles and especially missing
verb parts in German.
Example 2:
Src: ..., nachdem ich eine Weile im Internet
gesucht habe.
Gloss: ..., after I a while in-the Internet search-
ed have.
POS: ... as I have for some time on the Inter-
net.
+Tree: ... after I have looked for a while on the
Internet.
Example 3 shows another aspect of how the tree-
based rules work. With the help of the tree-based re-
ordering rules, it is possible to relocate the separated
prefix of German verbs and find the correct transla-
tion. The verb vorschlagen consist of the main verb
(MV) schlagen (here conjugated as schla?gt) and the
prefix (PX) vor. Depending on the verb form and
sentence type, the prefix must be separated from the
main verb and is located in a different part of the
sentence. The two parts of the verb can also have
individual meanings. Although the translation of
the verb stem were correct if it were the full verb,
not recognizing the separated prefix and ignoring it
in translation, corrupts the meaning of the sentence.
With the help of the tree-based rules, the dependency
between the main verb and its prefix is resolved and
the correct translation can be chosen.
6.4 German-French
The same experiments were tested on German-
French translation. For this language pair, similar
improvements could be achieved by combining POS
and tree-based reordering rules and applying a lexi-
calized reordering model in addition. Table 2 shows
the results. Up to 0.7 BLEU points could be gained
by adding tree rules and another 0.1 by lexicalized
reordering.
Rule Type
System noLexRM LexRM
Dev Test Dev Test
POS 41.29 38.07 42.04 38.55
POS + Tree 41.94 38.47 42.44 38.57
POS + Tree rec. 42.35 38.66 42.80 38.71
POS + Tree rec.+ par. 42.48 38.79 42.87 38.88
Table 2: German-French
6.5 Binarized Syntactic Trees
Even though related work using syntactic parse trees
in SMT for reordering purposes (Jiang et al, 2010)
have reported an advantage of binarized parse trees
over standard parse trees, our model did not bene-
fit from binarized parse trees. It seems that the flat
hierarchical structure of standard parse trees enables
our reordering model to learn the order of the con-
stituents most efficiently.
7 Human Evaluation
7.1 Sentence-based comparison
In order to have an additional perspective of the
impact of our tree-based reordering, we also pro-
vide a human evaluation of the translation output
of the German-English system without the lexical-
ized reordering model. 250 translation hypotheses
were selected to be annotated. For each input sen-
tence two translations generated by different sys-
tems were presented, one applying POS-based re-
ordering only and the other one applying both POS-
based and tree-based reordering rules. The hypothe-
ses were anonymized and presented in random order.
Table 3 shows the BLEU scores of the analyzed
systems and the manual judgement of comparative,
subjective translation quality. In 50.8% of the sen-
45
Example 3:
Src: Die RPG Byty schla?gt ihnen in den Schreiben eine Mieterho?hung von ca. 15 bis 38 Prozent vor.
Gloss: The RPG Byty proposes-MV them in the letters a rent increase of ca. 15 to 38 percent proposes-PX
POS: The RPG Byty beats them in the letter, a rental increase of around 15 to 38 percent.
+Tree: The RPG Byty proposes them in the letters a rental increase of around 15 to 38 percent.
System BLEU wins %
POS Rules 21.98 58 23.2
POS + Tree Rules rec. par. 22.45 127 50.8
Table 3: Human Evaluation of Translation quality
tences, the translation generated by the system us-
ing tree-based rules was judged to be better, whereas
in 23.2% of the cases the system without tree-based
rules was rated better. For 26% of the sentences the
translation quality was very similar. Consequently,
in 76.8% of the cases the tree-based system pro-
duced a translation that is either better or of the same
quality as the system using POS rules only.
7.2 Analysis of verbs
Since the verbs in German-to-English translation
were one of the issues that the tree-based reorder-
ing model should address, a more detailed analysis
was performed on the first 165 sentences. We espe-
cially investigated the changes regarding the verbs
between the translations stemming from the system
using the POS-based reordering only and the one us-
ing both the POS and the tree-based model. We ex-
amined three aspects of the verbs in the two trans-
lations. Each change introduced by the tree-based
reordering model was first classified either as an im-
provement or a degradation of the translation qual-
ity. Secondly, it was assigned to one of the following
categories: exist, position or form. In case of an im-
provement, exist means a verb existed in the trans-
lation due to the tree-based model, which did not
exist before. A degradation in this category means
that a verb was removed from the translation when
including the tree-based reordering model. An im-
provement or degradation in the category position
or form means that the verb position or verb form
was improved or degraded, respectively.
Table 4 shows the results of this analysis. In total,
48 of the verb changes were identified as improve-
ments, while only 16 were regarded as degradations
of translation quality. Improvements mainly concern
Type all exist position form
Improvements 48 22 21 5
Degradations 16 2 11 3
Table 4: Manual Analysis of verbs
improved verb position in the sentence and verbs
that could be translated with the help of the tree-
based rules that were not there before. Even though
also degradations were introduced by the tree-based
reordering model, the improvements outweigh them.
8 Conclusion
We have presented a reordering method based on
syntactic tree constituents to model long-range re-
orderings in SMT more reliably. Furthermore, we
combined the reordering methods addressing dif-
ferent linguistic abstraction levels. Experiments
on German-English and German-French translation
showed that the best translation quality could be
achieved by combining POS-based and tree-based
rules. Adding a lexicalized reordering model in-
creased the translation quality even further. In total
we could reach up to 0.7 BLEU points of improve-
ment by adding tree-based and lexicalized reorder-
ing compared to only POS-based rules. Up to 1.1
BLEU points were gained over to a baseline system
using a lexicalized reordering model.
A human evaluation showed a preference of the
POS+Tree-based reordering method in most cases.
A detailed analysis of the verbs in the transla-
tion outputs revealed that the tree-based reordering
model indeed addresses verb constructions and im-
proves the translation of German verbs.
Acknowledgments
This work was partly achieved as part of the Quaero
Programme, funded by OSEO, French State agency
for innovation. The research leading to these results
has received funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n? 287658.
46
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 263?270, Strouds-
burg, PA, USA.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL 2005, Ann Arbor,
Michigan.
Marta R. Costa-jussa` and Jose? A. R. Fonollosa. 2006.
Statistical Machine Reordering. In Conference on
Empirical Methods on Natural Language Processing
(EMNLP 2006), Sydney, Australia.
Josep M. Crego and Nizar Habash. 2008. Using Shallow
Syntax Information to Improve Word Alignment and
Reordering for SMT. In ACL-HLT 2008, Columbus,
Ohio, USA.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering. In
Proceedings of EMNLP 2011, pages 193?203, Edin-
burgh, Scotland, UK.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of COLING 2010, Beijing, China.
Nizar Habash. 2007. Syntactic preprocessing for statis-
tical machine translation. Proceedings of the 11th MT
Summit.
Jie Jiang, Jinhua Du, and Andy Way. 2010. Improved
phrase-based smt with syntactic reordering patterns
learned from lattice scoring. In Proceedings of AMTA
2010, Denver, CO, USA.
M. Khalilov, J.A.R. Fonollosa, and M. Dras. 2009.
A new subtree-transfer approach to syntax-based re-
ordering for statistical machine translation. In Proc.
of the 13th Annual Conference of the European As-
sociation for Machine Translation (EAMT?09), pages
198?204, Barcelona, Spain.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description for
the 2005 iwslt speech translation evaluation. In Inter-
national Workshop on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for
statistical machine translation. In Annual meeting-
association for computational linguistics, volume 45,
page 2.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division, T. J.
Watson Research Center.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Translation.
In International Conference on Language Resources
and Evaluation (LREC 2006), Genoa, Italy.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German, Columbus, Ohio.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sko?vde, Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004: Short Papers, pages 101?104.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 737?745.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of COLING 2004, Geneva,
Switzerland.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings of
the 39th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?01, pages 523?530, Strouds-
burg, PA, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-Level Reordering of Source Language Sen-
tences with Automatically Learned Rules for Statis-
tical Machine Translation. In HLT-NAACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, Rochester, NY, USA.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, StatMT ?06, pages 138?141, Stroudsburg,
PA, USA.
47
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 104?108,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2013
Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan Niehues,
Teresa Herrmann, Isabel Slawik and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based
SMT systems developed for our partici-
pation in the WMT13 Shared Translation
Task. Translations for English?German
and English?French were generated us-
ing a phrase-based translation system
which is extended by additional models
such as bilingual, fine-grained part-of-
speech (POS) and automatic cluster lan-
guage models and discriminative word
lexica (DWL). In addition, we combined
reordering models on different sentence
abstraction levels.
1 Introduction
In this paper, we describe our systems for the
ACL 2013 Eighth Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French using a
phrase-based decoder with lattice input.
The paper is organized as follows: the next sec-
tion gives a detailed description of our systems
including all the models. The translation results
for all directions are presented afterwards and we
close with a conclusion.
2 System Description
The phrase table is based on a GIZA++ word
alignment for the French?English systems. For
the German?English systems we use a Discrim-
inative Word Alignment (DWA) as described in
Niehues and Vogel (2008). For every source
phrase only the top 10 translation options are con-
sidered during decoding. The SRILM Toolkit
(Stolcke, 2002) is used for training SRI language
models using Kneser-Ney smoothing.
For the word reordering between languages, we
used POS-based reordering models as described in
Section 4. In addition to it, tree-based reordering
model and lexicalized reordering were added for
German?English systems.
An in-house phrase-based decoder (Vogel,
2003) is used to perform translation. The trans-
lation was optimized using Minimum Error Rate
Training (MERT) as described in Venugopal et
al. (2005) towards better BLEU (Papineni et al,
2002) scores.
2.1 Data
The Europarl corpus (EPPS) and News Commen-
tary (NC) corpus were used for training our trans-
lation models. We trained language models for
each language on the monolingual part of the
training corpora as well as the News Shuffle and
the Gigaword corpora. The additional data such as
web-crawled corpus, UN and Giga corpora were
used after filtering. The filtering work for this data
is discussed in Section 3.
For the German?English systems we use the
news-test2010 set for tuning, while the news-
test2011 set is used for the French?English sys-
tems. For testing, news-test2012 set was used for
all systems.
2.2 Preprocessing
The training data is preprocessed prior to train-
ing the system. This includes normalizing special
symbols, smart-casing the first word of each sen-
tence and removing long sentences and sentence
pairs with length mismatch.
Compound splitting is applied to the German
part of the corpus of the German?English system
as described in Koehn and Knight (2003).
3 Filtering of Noisy Pairs
The filtering was applied on the corpora which
are found to be noisy. Namely, the Giga English-
French parallel corpus and the all the new web-
crawled data . The operation was performed using
104
an SVM classifier as in our past systems (Medi-
ani et al, 2011). For each pair, the required lexica
were extracted from Giza alignment of the corre-
sponding EPPS and NC corpora. Furthermore, for
the web-crawled data, higher precision classifiers
were trained by providing a larger number of neg-
ative examples to the classifier.
After filtering, we could still find English sen-
tences in the other part of the corpus. Therefore,
we performed a language identification (LID)-
based filtering afterwards (performed only on the
French-English corpora, in this participation).
4 Word Reordering
Word reordering was modeled based on POS se-
quences. For the German?English system, re-
ordering rules learned from syntactic parse trees
were used in addition.
4.1 POS-based Reordering Model
In order to train the POS-based reordering model,
probabilistic rules were learned based on the POS
tags from the TreeTagger (Schmid and Laws,
2008) of the training corpus and the alignment. As
described in Rottmann and Vogel (2007), continu-
ous reordering rules are extracted. This modeling
of short-range reorderings was extended so that it
can cover also long-range reorderings with non-
continuous rules (Niehues and Kolss, 2009), for
German?English systems.
4.2 Tree-based Reordering Model
In addition to the POS-based reordering, we
apply a tree-based reordering model for the
German?English translation to better address the
differences in word order between German and
English. We use the Stanford Parser (Rafferty and
Manning, 2008) to generate syntactic parse trees
for the source side of the training corpus. Then
we use the word alignment between source and
target language to learn rules on how to reorder
the constituents in a German source sentence to
make it match the English target sentence word or-
der better (Herrmann et al, 2013). The POS-based
and tree-based reordering rules are applied to each
input sentence. The resulting reordered sentence
variants as well as the original sentence order are
encoded in a word lattice. The lattice is then used
as input to the decoder.
4.3 Lexicalized Reordering
The lexicalized reordering model stores the re-
ordering probabilities for each phrase pair. Pos-
sible reordering orientations at the incoming and
outgoing phrase boundaries are monotone, swap
or discontinuous. With the POS- and tree-based
reordering word lattices encode different reorder-
ing variants. In order to apply the lexicalized re-
ordering model, we store the original position of
each word in the lattice. At each phrase boundary
at the end, the reordering orientation with respect
to the original position of the words is checked.
The probability for the respective orientation is in-
cluded as an additional score.
5 Translation Models
In addition to the models used in the baseline sys-
tem described above, we conducted experiments
including additional models that enhance trans-
lation quality by introducing alternative or addi-
tional information into the translation modeling
process.
5.1 Bilingual Language Model
During the decoding the source sentence is seg-
mented so that the best combination of phrases
which maximizes the scores is available. How-
ever, this causes some loss of context information
at the phrase boundaries. In order to make bilin-
gual context available, we use a bilingual language
model (Niehues et al, 2011). In the bilingual lan-
guage model, each token consists of a target word
and all source words it is aligned to.
5.2 Discriminative Word Lexicon
Mauser et al (2009) introduced the Discriminative
Word Lexicon (DWL) into phrase-based machine
translation. In this approach, a maximum entropy
model is used to determine the probability of using
a target word in the translation.
In this evaluation, we used two extensions to
this work as shown in (Niehues and Waibel, 2013).
First, we added additional features to model the
order of the source words better. Instead of rep-
resenting the source sentence as a bag-of-words,
we used a bag-of-n-grams. We used n-grams up to
the order of three and applied count filtering to the
features for higher order n-grams.
Furthermore, we created the training examples
differently in order to focus on addressing errors
of the other models of the phrase-based translation
105
system. We first translated the whole corpus with a
baseline system. Then we only used the words that
occur in the N-Best List and not in the reference as
negative examples instead of using all words that
do not occur in the reference.
5.3 Quasi-Morphological Operations
Because of the inflected characteristic of the
German language, we try to learn quasi-
morphological operations that change the lexi-
cal entry of a known word form to the out-of-
vocabulary (OOV) word form as described in
Niehues and Waibel (2012).
5.4 Phrase Table Adaptation
For the French?English systems, we built two
phrase tables; one trained with all data and the
other trained only with the EPPS and NC cor-
pora. This is due to the fact that Giga corpus is big
but noisy and EPPS and NC corpus are more reli-
able. The two models are combined log-linearly to
achieve the adaptation towards the cleaner corpora
as described in Niehues et al (2010).
6 Language Models
The 4-gram language models generated by the
SRILM toolkit are used as the main language
models for all of our systems. For the
English?French systems, we use a good quality
corpus as in-domain data to train in-domain lan-
guage models. Additionally, we apply the POS
and cluster language models in different systems.
For the German?English system, we build sepa-
rate language models using each corpus and com-
bine them linearly before the decoding by mini-
mizing the perplexity. Language models are inte-
grated into the translation system by a log-linear
combination and receive optimal weights during
tuning by the MERT.
6.1 POS Language Models
For the English?German system, we use the POS
language model, which is trained on the POS se-
quence of the target language. The POS tags are
generated using the RFTagger (Schmid and Laws,
2008) for German. The RFTagger generates fine-
grained tags which include person, gender, and
case information. The language model is trained
with up to 9-gram information, using the German
side of the parallel EPPS and NC corpus, as well
as the News Shuffle corpus.
6.2 Cluster Language Models
In order to use larger context information, we use
a cluster language model for all our systems. The
cluster language model is based on the idea shown
in Och (1999). Using the MKCLS algorithm, we
cluster the words in the corpus, given a number
of classes. Then words in the corpus are replaced
with their cluster IDs. Using these cluster IDs,
we train n-gram language models as well as a
phrase table with this additional factor of cluster
ID. Our submitted systems have diversed range of
the number of clusters as well as n-gram.
7 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to
the workshop. The results are reported as case-
sensitive BLEU scores on one reference transla-
tion.
7.1 German?English
The experiments for the German to English trans-
lation system are summarized in Table 1. The
baseline system uses POS-based reordering, DWA
with lattice phrase extraction and language models
trained on the News Shuffle corpus and Giga cor-
pus separately. Then we added a 5-gram cluster
LM trained with 1,000 word classes. By adding a
language model using the filtered crawled data we
gained 0.3 BLEU on the test set. For this we com-
bined all language models linearly. The filtered
crawled data was also used to generate a phrase
table, which brought another improvement of 0.85
BLEU. Applying tree-based reordering improved
the BLEU score, and the performance had more
gain by adding the extended DWL, namely us-
ing both bag-of-ngrams and n-best lists. While
lexicalized reordering gave us a slight gain, we
added morphological operation and gained more
improvements.
7.2 English?German
The English to German baseline system uses POS-
based reordering and language models using par-
allel data (EPPS and NC) as shown in Table 2.
Gradual gains were achieved by changing align-
ment from GIZA++ to DWA, adding a bilingual
language model as well as a language model based
on the POS tokens. A 9-gram cluster-based lan-
guage model with 100 word classes gave us a
106
System Dev Test
Baseline 24.15 22.79
+ Cluster LM 24.18 22.84
+ Crawled Data LM (Comb.) 24.53 23.14
+ Crawled Data PT 25.38 23.99
+ Tree Rules 25.80 24.16
+ Extended DWL 25.59 24.54
+ Lexicalized Reordering 26.04 24.55
+ Morphological Operation - 24.62
Table 1: Translation results for German?English
small gain. Improving the reordering using lexi-
alized reordering gave us gain on the optimization
set. Using DWL let us have more improvements
on our test set. By using the filtered crawled data,
we gained a big improvement of 0.46 BLEU on
the test set. Then we extended the DWL with bag
of n-grams and n-best lists to achieve additional
improvements. Finally, the best system includes
lattices generated using tree rules.
System Dev Test
Baseline 17.00 16.24
+ DWA 17.27 16.53
+ Bilingual LM 17.27 16.59
+ POS LM 17.46 16.66
+ Cluster LM 17.49 16.68
+ Lexicalized Reordering 17.57 16.68
+ DWL 17.58 16.77
+ Crawled Data 18.43 17.23
+ Extended DWL 18.66 17.57
+ Tree Rules 18.63 17.70
Table 2: Translation results for English?German
7.3 French?English
Table 3 reports some remarkable improvements
as we combined several techniques on the
French?English direction. The baseline system
was trained on parallel corpora such as EPPS, NC
and Giga, while the language model was trained
on the English part of those corpora plus News
Shuffle. The newly presented web-crawled data
helps to achieve almost 0.6 BLEU points more
on test set. Adding bilingual language model and
cluster language model does not show a significant
impact. Further gains were achieved by the adap-
tation of in-domain data into general-theme phrase
table, bringing 0.15 BLEU better on the test set.
When we added the DWL feature, it notably im-
proves the system by 0.25 BLEU points, resulting
in our best system.
System Dev Test
Baseline 30.33 29.35
+ Crawled Data 30.59 29.93
+ Bilingual and Cluster LMs 30.67 30.01
+ In-Domain PT Adaptation 31.17 30.16
+ DWL 31.07 30.40
Table 3: Translation results for French?English
7.4 English?French
In the baseline system, EPPS, NC, Giga and News
Shuffle corpora are used for language modeling.
The big phrase tables tailored EPPC, NC and Giga
data. The system also uses short-range reordering
trained on EPPS and NC. Adding parallel and fil-
tered crawl data improves the system. It was fur-
ther enhanced by the integration of a 4-gram bilin-
gual language model. Moreover, the best config-
uration of 9-gram language model trained on 500
clusters of French texts gains 0.25 BLEU points
improvement. We also conducted phrase-table
adaptation from the general one into the domain
covered by EPPS and NC data and it helps as well.
The initial try-out with lexicalized reordering fea-
ture showed an improvement of 0.23 points on the
development set, but a surprising reduction on the
test set, thus we decided to take the system after
adaptation as our best English?French system.
System Dev Test
Baseline 30.50 27.77
+ Crawled Data 31.05 27.87
+ Bilingual LM 31.23 28.50
+ Cluster LM 31.58 28.75
+ In-Domain PT Adaptation 31.88 29.12
+ Lexicalized Reordering 32.11 28.98
Table 4: Translation results for English?French
8 Conclusions
We have presented the systems for our par-
ticipation in the WMT 2013 Evaluation for
English?German and English?French. All sys-
tems use a class-based language model as well
as a bilingual language model. Using a DWL
with source context improved the translation qual-
ity of English?German systems. Also for these
systems, we could improve even more with a
tree-based reordering model. Special handling
107
of OOV words improved German?English sys-
tem, while for the inverse direction the language
model with fine-grained POS tags was helpful. For
English?French, phrase table adaptation helps to
avoid using wrong parts of the noisy Giga corpus.
9 Acknowledgements
This work was partly achieved as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. The research lead-
ing to these results has received funding from
the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
n? 287658.
References
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The kit english-
french translation systems for iwslt 2011. In Pro-
ceedings of the eight International Workshop on
Spoken Language Translation (IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues and Alex Waibel. 2012. Detailed Analysis
of different Strategies for Phrase Table Adaptation
in SMT. In Proceedings of the American Machine
Translation Association (AMTA), San Diego, Cali-
fornia, October.
Jan Niehues and Alex Waibel. 2013. An MT Error-
driven Discriminative Word Lexicon using Sentence
Structure Features. In Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013), Sofia, Bul-
garia.
Jan Niehues, Mohammed Mediani, Teresa Herrmann,
Michael Heck, Christian Herff, and Alex Waibel.
2010. The KIT Translation system for IWSLT 2010.
In Marcello Federico, Ian Lane, Michael Paul, and
Franc?ois Yvon, editors, Proceedings of the seventh
International Workshop on Spoken Language Trans-
lation (IWSLT), pages 93?98.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
the ninth conference on European chapter of the As-
sociation for Computational Linguistics, EACL ?99,
pages 71?76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German, Columbus, Ohio.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, Great Britain.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
108
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185?192,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joint WMT 2013 Submission of the QUAERO Project
?Stephan Peitz, ?Saab Mansour, ?Matthias Huck, ?Markus Freitag, ?Hermann Ney,
?Eunah Cho, ?Teresa Herrmann, ?Mohammed Mediani, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Quoc Khanh Do,
?Bianka Buschbeck, ?Tonio Wandmacher
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint submis-
sion of the QUAERO project for the
German?English translation task of the
ACL 2013 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013).
The submission was a system combina-
tion of the output of four different transla-
tion systems provided by RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy (KIT), LIMSI-CNRS and SYSTRAN
Software, Inc. The translations were
joined using the RWTH?s system com-
bination approach. Experimental results
show improvements of up to 1.2 points in
BLEU and 1.2 points in TER compared to
the best single translation.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in ma-
chine translation is mainly assigned to the four
groups participating in this joint submission. The
aim of this submission was to show the quality of
a joint translation by combining the knowledge of
the four project partners. Each group develop and
maintain their own different machine translation
system. These single systems differ not only in
their general approach, but also in the preprocess-
ing of training and test data. To take advantage
of these differences of each translation system, we
combined all hypotheses of the different systems,
using the RWTH system combination approach.
This paper is structured as follows. First, the
different engines of all four groups are introduced.
In Section 3, the RWTH Aachen system combina-
tion approach is presented. Experiments with dif-
ferent system selections for system combination
are described in Section 4. This paper is concluded
in Section 5.
2 Translation Systems
For WMT 2013, each QUAERO partner trained
their systems on the parallel Europarl (EPPS),
News Commentary (NC) corpora and the web-
crawled corpus. All single systems were tuned on
the newstest2009 and newstest2010 development
set. The newstest2011 development set was used
to tune the system combination parameters. Fi-
nally, on newstest2012 the results of the different
system combination settings are compared. In this
Section, all four different translation engines are
presented.
2.1 RWTH Aachen Single System
For the WMT 2013 evaluation, RWTH utilized a
phrase-based decoder based on (Wuebker et al,
2012) which is part of RWTH?s open-source SMT
toolkit Jane 2.1 1. GIZA++ (Och and Ney, 2003)
was employed to train a word alignment, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
After phrase pair extraction from the word-
aligned parallel corpus, the translation probabil-
ities are estimated by relative frequencies. The
standard feature set alo includes an n-gram lan-
guage model, phrase-level IBM-1 and word-,
phrase- and distortion-penalties, which are com-
bined in log-linear fashion. Furthermore, we used
an additional reordering model as described in
(Galley and Manning, 2008). By this model six
1http://www-i6.informatik.rwth-aachen.
de/jane/
185
additional feature are added to the log-linear com-
bination. The model weights are optimized with
standard Mert (Och, 2003a) on 200-best lists. The
optimization criterion is BLEU.
2.1.1 Preprocessing
In order to reduce the source vocabulary size trans-
lation, the German text was preprocessed by split-
ting German compound words with the frequency-
based method described in (Koehn and Knight,
2003). To further reduce translation complexity
for the phrase-based approach, we performed the
long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.2 Translation Model
We applied filtering and weighting for domain-
adaptation similarly to (Mansour et al, 2011) and
(Mansour and Ney, 2012). For filtering the bilin-
gual data, a combination of LM and IBM Model
1 scores was used. In addition, we performed
weighted phrase extraction by using a combined
LM and IBM Model 1 weight.
2.1.3 Language Model
During decoding a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl,
the 109 French-English, UN and LDC Gigaword
Fourth Edition corpora.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
The training data was preprocessed prior to the
training. Symbols such as quotes, dashes and
apostrophes are normalized. Then the first words
of each sentence are smart-cased. For the Ger-
man part of the training corpus, the hunspell2 lex-
icon was used, in order to learn a mapping from
old German spelling to new German writing rules.
Compound-splitting was also performed as de-
scribed in Koehn and Knight (2003). We also re-
moved very long sentences, empty lines, and sen-
tences which show big mismatch on the length.
2.2.2 Filtering
The web-crawled corpus was filtered using an
SVM classifier as described in (Mediani et al,
2011). The lexica used in this filtering task were
obtained from Giza alignments trained on the
2http://hunspell.sourceforge.net/
cleaner corpora, EPPS and NC. Assuming that this
corpus is very noisy, we biased our classifier more
towards precision than recall. This was realized
by giving higher number of false examples (80%
of the training data).
This filtering technique ruled out more than
38% of the corpus (the unfiltered corpus contains
around 2.4M pairs, 0.9M of which were rejected
in the filtering task).
2.2.3 System Overview
The in-house phrase-based decoder (Vogel, 2003)
is used to perform decoding. Optimization with
regard to the BLEU score is done using Minimum
Error Rate Training (MERT) as described in Venu-
gopal et al (2005).
2.2.4 Reordering Model
We applied part-of-speech (POS) based reordering
using probabilistic continuous (Rottmann and Vo-
gel, 2007) and discontinuous (Niehues and Kolss,
2009) rules. This was learned using POS tags gen-
erated by the TreeTagger (Schmid, 1994) for short
and long range reorderings respectively.
In addition to this POS-based reordering, we
also used tree-based reordering rules. Syntactic
parse trees of the whole training corpus and the
word alignment between source and target lan-
guage are used to learn rules on how to reorder the
constituents in a German source sentence to make
it match the English target sentence word order
better (Herrmann et al, 2013). The training corpus
was parsed by the Stanford parser (Rafferty and
Manning, 2008). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are en-
coded in a word lattice which is used as input to
the decoder.
Moreover, our reordering model was extended
so that it could include the features of lexicalized
reordering model. The reordering probabilities for
each phrase pair are stored as well as the origi-
nal position of each word in the lattice. During
the decoding, the reordering origin of the words
is checked along with its probability added as an
additional score.
2.2.5 Translation Models
The translation model uses the parallel data of
EPPS, NC, and the filtered web-crawled data. As
word alignment, we used the Discriminative Word
Alignment (DWA) as shown in (Niehues and Vo-
186
gel, 2008). The phrase pairs were extracted using
different source word order suggested by the POS-
based reordering models presented previously as
described in (Niehues et al, 2009).
In order to extend the context of source lan-
guage words, we applied a bilingual language
model (Niehues et al, 2011). A Discriminative
Word Lexicon (DWL) introduced in (Mauser et
al., 2009) was extended so that it could take the
source context also into the account. For this,
we used a bag-of-ngrams instead of representing
the source sentence as a bag-of-words. Filtering
based on counts was then applied to the features
for higher order n-grams. In addition to this, the
training examples were created differently so that
we only used the words that occur in the n-best list
but not in the reference as negative example.
2.2.6 Language Models
We build separate language models and combined
them prior to decoding. As word-token based
language models, one language model is built on
EPPS, NC, and giga corpus, while another one is
built using crawled data. We combined the LMs
linearly by minimizing the perplexity on the de-
velopment data. As a bilingual language model we
used the EPPS, NC, and the web-crawled data and
combined them. Furthermore, we use a 5-gram
cluster-based language model with 1,000 word
clusters, which was trained on the EPPS and NC
corpus. The word clusters were created using the
MKCLS algorithm.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine transla-
tion system based on bilingual n-gram3. In this
approach, the translation model relies on a spe-
cific decomposition of the joint probability of a
sentence pair using the n-gram assumption: a sen-
tence pair is decomposed into a sequence of bilin-
gual units called tuples, defining a joint segmen-
tation of the source and target. In the approach of
(Marin?o et al, 2006), this segmentation is a by-
product of source reordering which ultimately de-
rives from initial word and phrase alignments.
2.3.2 An overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using
3http://ncode.limsi.fr/
a n-gram model of (source,target) pairs (Casacu-
berta and Vidal, 2004). Training this model re-
quires to reorder source sentences so as to match
the target word order. This is performed by
a stochastic finite-state reordering model, which
uses part-of-speech information4 to generalize re-
ordering patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized re-
ordering models (Tillmann, 2004) aiming at pre-
dicting the orientation of the next translation unit;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which compensate for the system prefer-
ence for short translations. The four lexicon mod-
els are similar to the ones use in a standard phrase
based system: two scores correspond to the rel-
ative frequencies of the tuples and two lexical
weights estimated from the automatically gener-
ated word alignments. The weights associated to
feature functions are optimally combined using a
discriminative training framework (Och, 2003b).
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the
tuple extraction process. The resulting reordering
hypotheses are passed to the decoder in the form
of word lattices (Crego and Mario, 2006).
2.3.3 Continuous space translation models
One critical issue with standard n-gram translation
models is that the elementary units are bilingual
pairs, which means that the underlying vocabu-
lary can be quite large, even for small translation
tasks. Unfortunately, the parallel data available to
train these models are typically order of magni-
tudes smaller than the corresponding monolingual
corpora used to train target language models. It is
very likely then, that such models should face se-
vere estimation problems. In such setting, using
neural network language model techniques seem
all the more appropriate. For this study, we fol-
low the recommendations of Le et al (2012), who
propose to factor the joint probability of a sen-
tence pair by decomposing tuples in two (source
and target) parts, and further each part in words.
This yields a word factored translation model that
4Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
187
can be estimated in a continuous space using the
SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the com-
putational cost of computing n-gram probabilities.
The solution used here was to resort to a two pass
approach: the first pass uses a conventional back-
off n-gram model to produce a k-best list; in the
second pass, the k-best list is reordered using the
probabilities of m-gram SOUL translation models.
In the following experiments, we used a fixed con-
text size for SOUL of m= 10, and used k = 300.
2.3.4 Corpora and data pre-processing
All the parallel data allowed in the constrained
task are pooled together to create a single par-
allel corpus. This corpus is word-aligned using
MGIZA++5 with default settings. For the English
monolingual training data, we used the same setup
as last year6 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we also took advantage of our in-
house text processing tools for the tokenization
and detokenization steps (Dchelotte et al, 2008)
and our system is built in ?true-case?. As Ger-
man is morphologically more complex than En-
glish, the default policy which consists in treat-
ing each word form independently is plagued with
data sparsity, which is detrimental both at training
and decoding time. Thus, the German side was
normalized using a specific pre-processing scheme
(described in (Allauzen et al, 2010; Durgar El-
Kahlout and Yvon, 2010)), which notably aims at
reducing the lexical redundancy by (i) normalizing
the orthography, (ii) neutralizing most inflections
and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
In the past few years, SYSTRAN has been focus-
ing on the introduction of statistical approaches
to its rule-based backbone, leading to Hybrid Ma-
chine Translation.
The technique of Statistical Post-Editing
(Dugast et al, 2007) is used to automatically edit
the output of the rule-based system. A Statistical
Post-Editing (SPE) module is generated from a
bilingual corpus. It is basically a translation mod-
ule by itself, however it is trained on rule-based
5http://geek.kyloo.net/software
6The fifth edition of the English Gigaword
(LDC2011T07) was not used.
translations and reference data. It applies correc-
tions and adaptations learned from a phrase-based
5-gram language model. Using this two-step
process will implicitly keep long distance re-
lations and other constraints determined by the
rule-based system while significantly improving
phrasal fluency. It has the advantage that quality
improvements can be achieved with very little
but targeted bilingual data, thus significantly
reducing training time and increasing translation
performance.
The basic setup of the SPE component is identi-
cal to the one described in (Dugast et al, 2007).
A statistical translation model is trained on the
rule-based translation of the source and the target
side of the parallel corpus. Language models are
trained on each target half of the parallel corpora
and also on additional in-domain corpora. More-
over, the following measures - limiting unwanted
statistical effects - were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is sig-
nificantly reduced. In addition, entity trans-
lation is handled more reliably by the rule-
based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the
reference translation) is used to produce an
additional parallel corpus (whose target is
identical to the source). This was added to the
parallel text in order to improve word align-
ment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side
are also discarded.
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained on 2M
phrases from the news/europarl and Common-
Crawl corpora, provided as training data for WMT
2013. Weights for these separate models were
tuned by the Mert algorithm provided in the Moses
toolkit (Koehn et al, 2007), using the provided
news development set.
188
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
3 RWTH Aachen System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses gener-
ated with different translation engines. First, a
word to word alignment for the given single sys-
tem hypotheses is produced. In a second step a
confusion network is constructed. Then, the hy-
pothesis with the highest probability is extracted
from this confusion network. For the alignment
procedure, each of the given single systems gen-
erates one confusion network with its own as pri-
mary system. To this primary system all other hy-
potheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1. The final network for one source sen-
tence is the union of all confusion networks gen-
erated from the different primary systems. That
allows the system combination to select the word
order from different system outputs.
Before performing system combination, each
translation output was normalized by tokenization
and lowercasing. The output of the combination
was then truecased based on the original truecased
output.
The model weights of the system combination
are optimized with standard Mert (Och, 2003a)
on 100-best lists. We add one voting feature for
each single system to the log-linear framework of
the system combination. The voting feature fires
for each word the single system agrees on. More-
over, a word penalty, a language model trained on
the input hypotheses, a binary feature which pe-
nalizes word deletions in the confusion network
and a primary feature which marks the system
which provides the word order are combined in
this log-linear model. The optimization criterion
is 4BLEU-TER.
4 Experimental Results
In this year?s experiments, we tried to improve the
result of the system combination further by com-
bining single systems tuned on different develop-
Table 1: Comparison of single systems tuned on
newstest2009 and newstest2010. The results are
reported on newstest2012.
single systems tuned on newstest2012
newstest BLEU TER
KIT 2009 24.6 58.4
2010 24.6 58.6
LIMSI 2009 22.5 61.5
2010 22.6 59.8
SYSTRAN 2009 20.9 63.3
2010 21.2 62.2
RWTH 2009 23.7 60.8
2010 24.4 58.8
ment sets. The idea is to achieve a more stable
performance in terms of translation quality, if the
single systems are not optimized on the same data
set. In Table 1, the results of each provided single
system tuned on newstest2009 and newstest2010
are shown. For RWTH, LIMSI and SYSTRAN,
it seems that the performance of the single system
depends on the chosen tuning set. However, the
translation quality of the single systems provided
by KIT is stable.
As initial approach and for the final submis-
sion, we grouped single systems with dissimilar
approaches. Thus, KIT (phrase-based SMT) and
SYSTRAN (rule-based MT) tuned their system on
newstest2010, while RWTH (phrase-based SMT)
and LIMSI (n-gram) optimized on newstest2009.
To compare the impact of this approach, all pos-
sible combinations were checked (Table 2). How-
ever, it seems that the translation quality can not be
improved by this approach. For the test set (new-
stest2012), BLEU is steady around 25.6 points.
Even if the single system with lowest BLEU are
combined (KIT 2010, LIMSI 2009, SYSTRAN
2010, RWTH 2009), the translation quality in
terms of BLEU is comparable with the combina-
tion of the best single systems (KIT 2009, LIMSI
2010, SYSTRAN 2010, RWTH 2010). However,
we could gain 1.0 point in TER.
Due to the fact, that for the final submission the
initial grouping was available only, we kept this
189
Table 2: Comparison of different system combination settings. For each possible combination of systems
tuned on different tuning sets, a system combination was set up, re-tuned on newstest2011 and evaluated
on newstest2012. The setting used for further experiments is set in boldface.
single systems system combinations
KIT LIMSI SYSTRAN RWTH newstest2011 newstest2012
tuned on newstest BLEU TER BLEU TER
2009 2009 2009 2009 24.6 58.0 25.6 56.8
2010 2010 2010 2010 24.2 58.1 25.6 57.7
2010 2009 2009 2009 24.5 57.9 25.7 57.4
2009 2010 2009 2009 24.4 58.3 25.7 57.0
2009 2009 2010 2009 24.5 57.9 25.6 57.0
2009 2009 2009 2010 24.5 58.0 25.6 56.8
2009 2010 2010 2010 24.1 57.5 25.4 56.4
2010 2009 2010 2010 24.3 57.6 25.6 56.9
2010 2010 2009 2010 24.2 58.0 25.6 57.3
2010 2010 2010 2009 24.3 57.9 25.5 57.6
2010 2010 2009 2009 24.4 58.1 25.6 57.5
2009 2009 2010 2010 24.4 57.8 25.5 56.6
2009 2010 2010 2009 24.4 58.2 25.5 57.0
2009 2010 2009 2010 24.2 57.8 25.5 56.8
2010 2009 2009 2010 24.4 57.9 25.6 57.4
2010 2009 2010 2009 24.4 57.7 25.6 57.4
Table 3: Results of the final submission (bold-
face) compared with best single system on new-
stest2012.
newstest2011 newstest2012
BLEU TER BLEU TER
best single 23.2 60.9 24.6 58.4
system comb. 24.4 57.7 25.6 57.4
+ IBM-1 24.6 58.1 25.6 57.6
+ bigLM 24.6 57.9 25.8 57.2
combination. To improve this baseline further, two
additional models were added. We applied lexi-
cal smoothing (IBM-1) and an additional language
model (bigLM) trained on the English side of the
parallel data and the News shuffle corpus. The re-
sults are presented in Table 3.
The baseline was slightly improved by 0.2
points in BLEU and TER. Note, this system com-
bination was the final submission.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, the partners of the QUAERO
project (Karlsruhe Institute of Technology, RWTH
Aachen University, LIMSI-CNRS and SYSTRAN
Software, Inc.) provided a joint submission. By
joining the output of four different translation sys-
tems with RWTH?s system combination, we re-
ported an improvement of up to 1.2 points in
BLEU and TER.
Combining systems optimized on different tun-
ing sets does not seem to improve the translation
quality. However, by adding additional model, the
baseline was slightly improved.
All in all, we conclude that the variability in
terms of BLEU does not influence the final result.
It seems that using different approaches of MT in
a system combination is more important (Freitag
et al, 2012).
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
190
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Josep M. Crego and Jose? B. Mario. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on systran?s rule-based trans-
lation system. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 220?223, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Daniel Dchelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Markus Freitag, Stephan Peitz, Matthias Huck, Her-
mann Ney, Teresa Herrmann, Jan Niehues, Alex
Waibel, Alexandre Allauzen, Gilles Adda, Bianka
Buschbeck, Josep Maria Crego, and Jean Senellart.
2012. Joint wmt 2012 submission of the quaero
project. In NAACL 2012 Seventh Workshop on Sta-
tistical Machine Translation, pages 322?329, Mon-
treal, Canada, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 847?855, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantine, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
pages 177?180, Prague, Czech Republic, June.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An Automatic Metric for MT Evaluation
with High Levels of Correlation with Human Judg-
ments. pages 228?231, Prague, Czech Republic,
June.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Saab Mansour and Hermann Ney. 2012. A sim-
ple and effective weighted phrase extraction for ma-
chine translation adaptation. In International Work-
shop on Spoken Language Translation, pages 193?
200, Hong Kong, December.
Sab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA,
December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
191
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation Systems for IWSLT
2011. In Proceedings of the Eighth Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003a. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sap-
poro, Japan, July.
Franz Josef Och. 2003b. Minimum error rate training
in statistical machine translation. In ACL ?03: Proc.
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Evelyne Tzoukermann and SusanEditors Arm-
strong, editors, Proceedings of the ACL SIGDAT-
Workshop, pages 47?50. Kluwer Academic Publish-
ers.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Int. Conf. on Spo-
ken Language Processing, volume 2, pages 901?
904, Denver, Colorado, USA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
192
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84?89,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The KIT-LIMSI Translation System for WMT 2014
?
Quoc Khanh Do,
?
Teresa Herrmann,
??
Jan Niehues,
?
Alexandre Allauzen,
?
Franc?ois Yvon and
?
Alex Waibel
?
LIMSI-CNRS, Orsay, France
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
surname@limsi.fr
?
firstname.surname@kit.edu
Abstract
This paper describes the joined submis-
sion of LIMSI and KIT to the Shared
Translation Task for the German-to-
English direction. The system consists
of a phrase-based translation system us-
ing a pre-reordering approach. The base-
line system already includes several mod-
els like conventional language models on
different word factors and a discriminative
word lexicon. This system is used to gen-
erate a k-best list. In a second step, the
list is reranked using SOUL language and
translation models (Le et al., 2011).
Originally, SOUL translation models were
applied to n-gram-based translation sys-
tems that use tuples as translation units
instead of phrase pairs. In this article,
we describe their integration into the KIT
phrase-based system. Experimental re-
sults show that their use can yield sig-
nificant improvements in terms of BLEU
score.
1 Introduction
This paper describes the KIT-LIMSI system for
the Shared Task of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation. The sys-
tem participates in the German-to-English trans-
lation task. It consists of two main components.
First, a k-best list is generated using a phrase-
based machine translation system. This system
will be described in Section 2. Afterwards, the k-
best list is reranked using SOUL (Structured OUt-
put Layer) models. Thereby, a neural network lan-
guage model (Le et al., 2011), as well as several
translation models (Le et al., 2012a) are used. A
detailed description of these models can be found
in Section 3. While the translation system uses
phrase pairs, the SOUL translation model uses tu-
ples as described in the n-gram approach (Mari?no
et al., 2006). We describe the integration of the
SOUL models into the translation system in Sec-
tion 3.2. Section 4 summarizes the experimen-
tal results and compares two different tuning al-
gorithms: Minimum Error Rate Training (Och,
2003) and k-best Batch Margin Infused Relaxed
Algorithm (Cherry and Foster, 2012).
2 Baseline system
The KIT translation system is an in-house imple-
mentation of the phrase-based approach and in-
cludes a pre-ordering step. This system is fully
described in Vogel (2003).
To train translation models, the provided Eu-
roparl, NC and Common Crawl parallel corpora
are used. The target side of those parallel corpora,
the News Shuffle corpus and the GigaWord cor-
pus are used as monolingual training data for the
different language models. Optimization is done
with Minimum Error Rate Training as described
in Venugopal et al. (2005), using newstest2012
and newstest2013 as development and test data,
respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side (German) of the
corpus before training. Since the web-crawled
Common Crawl corpus is noisy, this corpus is
first filtered using an SVM classifier as described
in Mediani et al. (2011).
The word alignment is generated using the
GIZA++ Toolkit (Och and Ney, 2003). Phrase
extraction and scoring is done using the Moses
toolkit (Koehn et al., 2007). Phrase pair proba-
bilities are computed using modified Kneser-Ney
smoothing (Foster et al., 2006).
We apply short-range reorderings (Rottmann
and Vogel, 2007) and long-range reorder-
ings (Niehues and Kolss, 2009) based on part-of-
speech tags. The POS tags are generated using
the TreeTagger (Schmid, 1994). Rewriting rules
84
based on POS sequences are learnt automatically
to perform source sentence reordering according
to the target language word order. The long-range
reordering rules are further applied to the training
corpus to create reordering lattices to extract the
phrases for the translation model. In addition,
a tree-based reordering model (Herrmann et al.,
2013) trained on syntactic parse trees (Rafferty
and Manning, 2008; Klein and Manning, 2003)
is applied to the source sentence. In addition
to these pre-reordering models, a lexicalized
reordering model (Koehn et al., 2005) is applied
during decoding.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) using modified Kneser-Ney
smoothing (Chen and Goodman, 1996). The sys-
tem uses a 4-gram word-based language model
trained on all monolingual data and an additional
language model trained on automatically selected
data (Moore and Lewis, 2010). The system fur-
ther applies a language model based on 1000 auto-
matically learned word classes using the MKCLS
algorithm (Och, 1999). In addition, a bilingual
language model (Niehues et al., 2011) is used as
well as a discriminative word lexicon (DWL) us-
ing source context to guide the word choices in the
target sentence.
3 SOUL models for statistical machine
translation
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al., 2003; Schwenk,
2007) as a potential means to improve discrete
language models. The SOUL model (Le et al.,
2011) is a specific neural network architecture that
allows us to estimate n-gram models using large
vocabularies, thereby making the training of large
neural network models feasible both for target lan-
guage models and translation models (Le et al.,
2012a).
3.1 SOUL translation models
While the integration of SOUL target language
models is straightforward, SOUL translation mod-
els rely on a specific decomposition of the joint
probability P (s, t) of a sentence pair, where s is a
sequence of I reordered source words (s
1
, ..., s
I
)
1
1
In the context of the n-gram translation model, (s, t) thus
denotes an aligned sentence pair, where the source words are
reordered.
and t contains J target words (t
1
, ..., t
J
). In the
n-gram approach (Mari?no et al., 2006; Crego et
al., 2011), this segmentation is a by-product of
source reordering, and ultimately derives from ini-
tial word and phrase alignments. In this frame-
work, the basic translation units are tuples, which
are analogous to phrase pairs, and represent a
matching u = (s, t) between a source phrase s
and a target phrase t.
Using the n-gram assumption, the joint proba-
bility of a segmented sentence pair using L tupels
decomposes as:
P (s, t) =
L
?
i=1
P (u
i
|u
i?1
, ..., u
i?n+1
) (1)
A first issue with this decomposition is that the
elementary units are bilingual pairs. Therefore,
the underlying vocabulary and hence the number
of parameters can be quite large, even for small
translation tasks. Due to data sparsity issues, such
models are bound to face severe estimation prob-
lems. Another problem with Equation (1) is that
the source and target sides play symmetric roles,
whereas the source side is known, and the tar-
get side must be predicted. To overcome some
of these issues, the n-gram probability in Equa-
tion (1) can be factored by first decomposing tu-
ples in two (source and target) parts, and then de-
composing the source and target parts at the word
level.
Let s
k
i
denote the k
th
word of source part of the
tuple s
i
. Let us consider the example of Figure 1,
s
1
11
corresponds to the source word nobel, s
4
11
to
the source word paix, and similarly t
2
11
is the tar-
get word peace. We finally define h
n?1
(t
k
i
) as the
sequence of the n?1 words preceding t
k
i
in the tar-
get sentence, and h
n?1
(s
k
i
) as the n?1 words pre-
ceding s
k
i
in the reordered source sentence: in Fig-
ure 1, h
3
(t
2
11
) thus refers to the three word context
receive the nobel associated with the target word
peace. Using these notations, Equation 1 can be
rewritten as:
P (s, t) =
L
?
i=1
[
|t
i
|
?
k=1
P
(
t
k
i
|h
n?1
(t
k
i
), h
n?1
(s
1
i+1
)
)
?
|s
i
|
?
k=1
P
(
s
k
i
|h
n?1
(t
1
i
), h
n?1
(s
k
i
)
)
]
(2)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
85
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
s :   .... 
t :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org)
French sentence appears at the top of the figure, just above the reordered source s and the target t. The
pair (s, t) decomposes into a sequence of L bilingual units (tuples) u
1
, ..., u
L
. Each tuple u
i
contains a
source and a target phrase: s
i
and t
i
.
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one
for each language; however, the moves of these
windows remain synchronized by the tuple seg-
mentation. Moreover, the context is not limited
to the current phrase, and continues to include
words in adjacent phrases. Equation (2) involves
two terms that will be further denoted as TrgSrc
and Src, respectively P
(
t
k
i
|h
n?1
(t
k
i
), h
n?1
(s
1
i+1
)
)
and P
(
s
k
i
|h
n?1
(t
1
i
), h
n?1
(s
k
i
)
)
. It is worth notic-
ing that the joint probability of a sentence pair
can also be decomposed by considering the fol-
lowing two terms: P
(
s
k
i
|h
n?1
(s
k
i
), h
n?1
(t
1
i+1
)
)
and P
(
t
k
i
|h
n?1
(s
1
i
), h
n?1
(t
k
i
)
)
. These two terms
will be further denoted by SrcTrg and Trg. There-
fore, adding SOUL translation models means that
4 scores are added to the phrase-based systems.
3.2 Integration
During the training step, the SOUL translation
models are trained as described in (Le et al.,
2012a). The main changes concern the inference
step. Given the computational cost of computing
n-gram probabilities with neural network models,
a solution is to resort to a two-pass approach: the
first pass uses a conventional system to produce
a k-best list (the k most likely hypotheses); in
the second pass, probabilities are computed by the
SOUL models for each hypothesis and added as
new features. Then the k-best list is reordered ac-
cording to a combination of all features including
these new features. In the following experiments,
we use 10-gram SOUL models to rescore 300-
best lists. Since the phrase-based system described
in Section 2 uses source reordering, the decoder
was modified in order to generate k-best lists that
contain necessary word alignment information be-
tween the reordered source sentence and its asso-
ciated target hypothesis. The goal is to recover
the information that is illustrated in Figure 1 and
to apply the n-gram decomposition of a sentence
pair.
These (target and bilingual) neural network
models produce scores for each hypothesis in the
k-best list; these new features, along with the fea-
tures from the baseline system, are then provided
to a new phase which runs the traditional Mini-
mum Error Rate Training (MERT ) (Och, 2003), or
a recently proposed k-best Batch Margin Infused
Relaxed Algorithm (KBMIRA ) (Cherry and Fos-
ter, 2012) for tuning purpose. The SOUL mod-
els used for this year?s evaluation are similar to
those described in Allauzen et al. (2013) and Le
et al. (2012b). However, since compared to these
evaluations less parallel data is available for the
German-to-English task, we use smaller vocabu-
laries of about 100K words.
4 Results
We evaluated the SOUL models on the German-
to-English translation task using two systems to
generate the k-best lists. The first system used
all models of the baseline system except the DWL
model and the other one used all models.
Table 1 summarizes experimental results in
terms of BLEU scores when the tuning is per-
formed using KBMIRA. As described in Section
3, the probability of a phrase pair can be decom-
posed into products of words? probabilities in 2
different ways: we can first estimate the probabil-
ity of words in the source phrase given the context,
and then the probability of the target phrase given
its associated source phrase and context words
(see Equation (2)); or inversely we can generate
the target side before the source side. The for-
mer proceeds by adding Src and TrgSrc scores as
86
No DWL DWL
Soul models Dev Test Dev Test
No 26.02 27.02 26.27 27.46
Target 26.30 27.42 26.43 27.85
Translation st 26.46 27.70 26.66 28.04
Translation ts 26.48 27.41 26.61 28.00
All Translation 26.50 27.86 26.70 28.08
All SOUL models 26.62 27.84 26.75 28.10
Table 1: Results using KBMIRA
No DWL DWL
Soul models Dev Test Dev Test
No 26.02 27.02 26.27 27.46
Target 26.18 27.09 26.44 27.54
Translation st 26.36 27.59 26.66 27.80
Translation ts 26.44 27.69 26.63 27.94
All Translation 26.53 27.65 26.69 27.99
All SOUL models 26.47 27.68 26.66 28.01
Table 2: Results using MERT. Results in bold correpond to the submitted system.
2 new features into the k-best list, and the latter by
adding Trg and SrcTrg scores. These 2 methods
correspond respectively to the Translation ts and
Translation st lines in the Table 1. The 4 trans-
lation models may also be added simultaneously
(All Translations). The first line gives baseline
results without SOUL models, while the Target
line shows results in adding only SOUL language
model. The last line (All SOUL models) shows
the results for adding all neural network models
into the baseline systems.
As evident in Table 1, using the SOUL trans-
lation models yields generally better results than
using the SOUL target language model, yielding
about 0.2 BLEU point differences on dev and test
sets. We can therefore assume that the SOUL
translation models provide richer information that,
to some extent, covers that contained in the neural
network language model. Indeed, these 4 trans-
lation models take into account not only lexi-
cal probabilities of translating target words given
source words (or in the inverse order), but also the
probabilities of generating words in the target side
(Trg model) as does a language model, with the
same context length over both source and target
sides. It is therefore not surprising that adding the
SOUL language model along with all translation
models (the last line in the table) does not give sig-
nificant improvement compared to the other con-
figurations. The different ways of using the SOUL
translation models perform very similarly.
Table 2 summarizes the results using MERT in-
stead of KBMIRA. We can observe that using KB-
MIRA results in 0.1 to 0.2 BLEU point improve-
ments compared to MERT. Moreover, this impact
becomes more important when more features are
considered (the last line when all 5 neural net-
work models are added into the baseline systems).
In short, the use of neural network models yields
up to 0.6 BLEU improvement on the DWL sys-
tem, and a 0.8 BLEU gain on the system without
DWL. Unfortunately, the experiments with KB-
MIRA were carried out after the the submission
date. Therefore the submitted system corresponds
to the last line of table 2 indicated in bold.
5 Conclusion
We presented a system with two main features: a
phrase-based translation system which uses pre-
reordering and the integration of SOUL target lan-
guage and translation models. Although the trans-
lation performance of the baseline system is al-
ready very competitive, the rescoring by SOUL
models improve the performance significantly. In
the rescoring step, we used a continuous language
model as well as four continuous translation mod-
87
els. When combining the different SOUL models,
the translation models are observed to be more im-
portant in increasing the translation performance
than the language model. Moreover, we observe a
slight benefit to use KBMIRA instead of the stan-
dard MERT tuning algorithm. It is worth noticing
that using KBMIRA improves the performance
but also reduces the variance of the final results.
As future work, the integration of the SOUL
translation models could be improved in differ-
ent ways. For SOUL translation models, there
is a mismatch between translation units used dur-
ing the training step and those used by the de-
coder. The former are derived using the n-gram-
based approach, while the latter use the conven-
tional phrase extraction heuristic. We assume that
reducing this mismatch could improve the overall
performance. This can be achieved for instance
using forced decoding to infer a segmentation of
the training data into translation units. Then the
SOUL translation models can be trained using
this segmentation. For the SOUL target language
model, in these experiments we only used the En-
glish part of the parallel data for training. Results
may be improved by including all the monolingual
data.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658 as well as the French Ar-
maments Procurement Agency (DGA) under the
RAPID Rapmat project.
References
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-Son Le, and Franc?ois Yvon. 2013.
Limsi@ wmt13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 60?
67.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
S.F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proceedings of the 34th Annual Meeting on Associa-
tion for Computational Linguistics (ACL ?96), pages
310?318, Santa Cruz, California, USA.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Josep M. Crego, Franois Yvon, and Jos B. Mari?no.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In EMNLP, pages 53?61.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of ACL 2007, Demonstration Ses-
sion, Prague, Czech Republic.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. pages 39?48, Montr?eal, Canada,
June. Association for Computational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur?elien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012b. Limsi@ wmt?12. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 330?337. Association for
Computational Linguistics.
88
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos?e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the ACL 2010 Conference Short Papers, pages 220?
224, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492?518, July.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluating Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
89
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
EU-BRIDGE MT: Combined Machine Translation
?
Markus Freitag,
?
Stephan Peitz,
?
Joern Wuebker,
?
Hermann Ney,
?
Matthias Huck,
?
Rico Sennrich,
?
Nadir Durrani,
?
Maria Nadejde,
?
Philip Williams,
?
Philipp Koehn,
?
Teresa Herrmann,
?
Eunah Cho,
?
Alex Waibel
?
RWTH Aachen University, Aachen, Germany
?
University of Edinburgh, Edinburgh, Scotland
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
?
{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
?
v1rsennr@staffmail.ed.ac.uk
?
maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk
?
{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
Abstract
This paper describes one of the col-
laborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two Euro-
pean language pairs, German?English
and English?German. Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation (WMT 2014).
We combined up to nine different machine
translation engines via system combina-
tion. RWTH Aachen University, the Uni-
versity of Edinburgh, and Karlsruhe In-
stitute of Technology developed several
individual systems which serve as sys-
tem combination input. We devoted spe-
cial attention to building syntax-based sys-
tems and combining them with the phrase-
based ones. The joint setups yield em-
pirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT news-
test2013 test set compared to the best sin-
gle systems.
1 Introduction
EU-BRIDGE
1
is a European research project
which is aimed at developing innovative speech
translation technology. This paper describes a
1
http://www.eu-bridge.eu
joint WMT submission of three EU-BRIDGE
project partners. RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach (Freitag et al., 2014). As
distinguished from our EU-BRIDGE joint submis-
sion to the IWSLT 2013 evaluation campaign (Fre-
itag et al., 2013), we particularly focused on trans-
lation of news text (instead of talks) for WMT. Be-
sides, we put an emphasis on engineering syntax-
based systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from Ger-
man to English as well as from English to Ger-
man. This paper gives some insight into the tech-
nology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as fol-
lows: We first describe the individual systems by
RWTH Aachen University (Section 2), the Uni-
versity of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4). We then
present the techniques for machine translation sys-
tem combination in Section 5. Experimental re-
sults are given in Section 6. We finally conclude
the paper with Section 7.
2 RWTH Aachen University
RWTH (Peitz et al., 2014) employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH?s
publicly available translation toolkit Jane (Vilar
105
et al., 2010; Wuebker et al., 2012). The model
weights of all systems have been tuned with stan-
dard Minimum Error Rate Training (Och, 2003)
on a concatenation of the newstest2011 and news-
test2012 sets. RWTH used BLEU as optimiza-
tion objective. Both for language model estima-
tion and querying at decoding, the KenLM toolkit
(Heafield et al., 2013) is used. All RWTH sys-
tems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model (Galley and Man-
ning, 2008; Huck et al., 2013) and a cluster lan-
guage model (Wuebker et al., 2013). The phrase-
based system (RWTH scss) has been further im-
proved by maximum expected BLEU training sim-
ilar to (He and Deng, 2012). The latter has been
performed on a selection from the News Commen-
tary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
(Mansour et al., 2011).
3 University of Edinburgh
UEDIN contributed phrase-based and syntax-
based systems to both the German?English and
the English?German joint submission.
3.1 Phrase-based Systems
UEDIN?s phrase-based systems (Durrani et al.,
2014) have been trained using the Moses toolkit
(Koehn et al., 2007), replicating the settings de-
scribed in (Durrani et al., 2013b). The features
include: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA
++
align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) (Durrani et al.,
2013a), msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), a distortion limit of 6, a maxi-
mum phrase length of 5, 100-best translation op-
tions, Minimum Bayes Risk decoding (Kumar and
Byrne, 2004), cube pruning (Huang and Chiang,
2007), with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-over-
punctuation heuristic. UEDIN uses POS and mor-
phological target sequence models built on the in-
domain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models (Koehn and Hoang,
2007). UEDIN has furthermore built OSM mod-
els over POS and morph sequences following
Durrani et al. (2013c). The English?German
system additionally comprises a target-side LM
over automatically built word classes (Birch et
al., 2013). UEDIN has applied syntactic pre-
reordering (Collins et al., 2005) and compound
splitting (Koehn and Knight, 2003) of the source
side for the German?English system. The sys-
tems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used news-
test2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
3.2 Syntax-based Systems
UEDIN?s syntax-based systems (Williams et al.,
2014) follow the GHKM syntax approach as pro-
posed by Galley, Hopkins, Knight, and Marcu
(Galley et al., 2004). The open source Moses
implementation has been employed to extract
GHKM rules (Williams and Koehn, 2012). Com-
posed rules (Galley et al., 2006) are extracted in
addition to minimal rules, but only up to the fol-
lowing limits: at most twenty tree nodes per rule,
a maximum depth of five, and a maximum size of
five. Singleton hierarchical rules are dropped.
The features for the syntax-based systems com-
prise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit (Stolcke, 2002) to train the lan-
guage model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been se-
lected as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntax-
based systems is carried out with cube pruning
using Moses? hierarchical decoder (Hoang et al.,
2009).
UEDIN?s German?English syntax-based setup
is a string-to-tree system with compound splitting
106
on the German source-language side and syntactic
annotation from the Berkeley Parser (Petrov et al.,
2006) on the English target-language side.
For English?German, UEDIN has trained var-
ious string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English?German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu): A string-to-tree
system trained with target-side syntactic an-
notation obtained with ParZu (Sennrich et
al., 2013). It uses a modified syntactic label
set, target-side compound splitting, and addi-
tional syntactic constraints.
UEDIN GHKM S2T (BitPar): A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
2004).
UEDIN GHKM S2T (Stanford): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Stan-
ford Parser (Rafferty and Manning, 2008a).
UEDIN GHKM S2T (Berkeley): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Berke-
ley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
UEDIN GHKM T2S (Berkeley): A tree-to-
string system trained with source-side syn-
tactic annotation obtained with the English
Berkeley Parser (Petrov et al., 2006).
UEDIN GHKM S2S (Berkeley): A string-to-
string system. The extraction is GHKM-
based with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The final gram-
mar contains rules with a single generic non-
terminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction (Huck et al., 2014).
4 Karlsruhe Institute of Technology
The KIT translations (Herrmann et al., 2014) are
generated by an in-house phrase-based transla-
tions system (Vogel, 2003). The provided News
Commentary, Europarl, and Common Crawl par-
allel corpora are used for training the translation
model. The monolingual part of those parallel
corpora, the News Shuffle corpus for both direc-
tions and additionally the Gigaword corpus for
German?English are used as monolingual train-
ing data for the different language models. Opti-
mization is done with Minimum Error Rate Train-
ing as described in (Venugopal et al., 2005), using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side of the corpus for
German?English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
filtered out using an SVM classifier as described
by Mediani et al. (2011).
The word alignment for German?English is
generated using the GIZA
++
toolkit (Och and Ney,
2003). For English?German, KIT uses discrimi-
native word alignment (Niehues and Vogel, 2008).
Phrase extraction and scoring is done using the
Moses toolkit (Koehn et al., 2007). Phrase pair
probabilities are computed using modified Kneser-
Ney smoothing as in (Foster et al., 2006).
In both systems KIT applies short-range re-
orderings (Rottmann and Vogel, 2007) and long-
range reorderings (Niehues and Kolss, 2009)
based on POS tags (Schmid, 1994) to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model. In addition, a tree-based re-
ordering model (Herrmann et al., 2013) trained
on syntactic parse trees (Rafferty and Manning,
2008b; Klein and Manning, 2003) as well as a lex-
icalized reordering model (Koehn et al., 2005) are
applied.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) and use modified Kneser-
Ney smoothing. Both systems utilize a lan-
guage model based on automatically learned
word classes using the MKCLS algorithm (Och,
1999). The English?German system comprises
language models based on fine-grained part-of-
speech tags (Schmid and Laws, 2008). In addi-
tion, a bilingual language model (Niehues et al.,
2011) is used as well as a discriminative word lex-
icon (Mauser et al., 2009) using source context to
guide the word choices in the target sentence.
107
In total, the English?German system uses the
following language models: two 4-gram word-
based language models trained on the parallel data
and the filtered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language mod-
els, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German?English system uses a 4-gram
word-based language model trained on all mono-
lingual data and an additional language model
trained on automatically selected data (Moore and
Lewis, 2010). Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
5 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hy-
potheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we ap-
ply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH?s approach to
machine translation system combination is de-
scribed in (Freitag et al., 2014). This approach
includes an enhanced alignment and reordering
framework. Alignments between the system out-
puts are learned using METEOR (Banerjee and
Lavie, 2005). A confusion network is then built
using one of the hypotheses as ?primary? hypoth-
esis. We do not make a hard decision on which
of the hypotheses to use for that, but instead com-
bine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as con-
sensus translation.
6 Results
In this section, we present our experimental results
on the two translation tasks, German?English
and English?German. The weights of the in-
dividual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combina-
tion weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an un-
seen test set which has not been used for tuning
the system combination or any of the individual
systems.
6.1 German?English
The automatic scores of all individual systems
as well as of our final system combination sub-
mission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrase-
based system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a string-
to-tree syntax model. The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER. For
German?English, our system combination pa-
rameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all in-
dividual systems as well as for the system combi-
nation output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the low-
est pairwise BLEU score. Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is com-
pared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evalu-
ation, all bi-, tri-, and four-gram counts are ini-
tialized with 1 instead of 0. Many sentences have
been improved by system combination. Neverthe-
less, some sentences fall off in quality compared
to the individual system output of KIT.
6.2 English?German
The results of all English?German system setups
are given in Table 3. For the English?German
translation task, only UEDIN and KIT are con-
108
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
KIT 25.0 57.6 25.2 57.4 27.5 54.4
UEDIN 23.9 59.2 24.7 58.3 27.4 55.0
RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0
RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9
UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9
syscom 25.6 57.1 26.4 56.5 29.1 53.4
Table 1: Results for the German?English translation task. The system combination is tuned on news-
test2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly better than the best single system
with p < 0.05.
KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom
KIT 59.07 57.60 57.91 55.62 77.68
UEDIN 59.17 56.96 57.84 59.89 72.89
RWTH scss 57.64 56.90 64.94 53.10 71.16
RWTH hiero 57.98 57.80 64.97 55.73 70.87
UEDIN S2T 55.75 59.95 53.19 55.82 65.35
syscom 77.76 72.83 71.17 70.85 65.24
Table 2: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7
UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3
UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8
UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9
UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2
UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8
UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8
UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3
KIT 17.1 67.0 17.8 64.8 20.2 62.2
syscom 18.4 65.0 18.7 63.4 21.3 60.6
Table 3: Results for the English?German translation task. The system combination is tuned on news-
test2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than
the best single system with p< 0.05. Italic font indicates system combination results that are significantly
better than the best single system with p < 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntax-
based ones (GHKM). For English?German, our
system combination parameters are optimized on
newstest2011. Combining all nine different sys-
tem outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best sin-
gle system performance.
In Table 4 the cross BLEU scores for all
English?German systems are given. The individ-
ual system of KIT and the syntax-based ParZu sys-
tem of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from differ-
ent institutes. In contrast, both phrase-based sys-
tems pbt 1 and pbt 2 from UEDIN are very sim-
ilar and hence have a high pairwise BLEU score.
109
pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom
pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12
pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75
ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39
BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08
Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51
S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81
T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13
S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46
KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33
syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27
Table 4: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 1: Sentence distribution for the
German?English newstest2013 test set compar-
ing system combination output against the best
individual system.
As for the German?English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evalu-
ated against the final system combination output.
In Figure 2 system combination output is com-
pared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been im-
proved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the indi-
vidual best system pbt 2.
7 Conclusion
We achieved significantly better translation perfor-
mance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three dif-
ferent research institutes (RWTH Aachen Univer-
sity, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation en-
gines based on different approaches like phrase-
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 2: Sentence distribution for the
English?German newstest2013 test set compar-
ing system combination output against the best
individual system.
based, hierarchical phrase-based, and syntax-
based. For English?German, we included six
different syntax-based systems, which were com-
bined to our final combined translation. The au-
tomatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.
2
Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German?English and
English?German without adding any new data.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658.
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2
http://matrix.statmt.org/
110
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 65?72, Ann Arbor, MI, USA, June.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg, Ger-
many, December.
Maximilian Bisani and Hermann Ney. 2004. Bootstrap
Estimates for Confidence Intervals in ASR Perfor-
mance Evaluation. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
volume 1, pages 409?412, Montr?eal, Canada, May.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Ma-
chine Translation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richard Farkas. 2013c. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh?s Phrase-based
Machine Translation Systems for WMT-14. In Pro-
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In EMNLP, pages 53?61.
M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-
rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,
M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi,
M. Cettolo, and M. Federico. 2013. EU-BRIDGE
MT: Text Translation of Talks in the EU-BRIDGE
Project. In International Workshop on Spoken Lan-
guage Translation, Heidelberg, Germany, Decem-
ber.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open Source Machine Translation Sys-
tem Combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, HI, USA, Octo-
ber.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
/ North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273?280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 292?301, Jeju, Republic of Korea,
July.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, UK, July.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Atlanta, GA, USA, June.
111
Teresa Herrmann, Mohammed Mediani, Eunah Cho,
Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi
Zhang, and Alex Waibel. 2014. The Karlsruhe In-
stitute of Technology Translation Systems for the
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. pages 152?159, Tokyo, Japan, December.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A Phrase Orientation Model
for Hierarchical Machine Translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, Baltimore, MD,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In EMNLP-CoNLL, pages 868?876,
Prague, Czech Republic, June.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177?180,
Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proc. Human Language Technol-
ogy Conf. / North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 169?176, Boston, MA, USA,
May.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, CA, USA, December.
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 210?217, Singapore, Au-
gust.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA, USA.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of Third ACL Workshop on Statisti-
cal Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English Machine Translation System for WMT
2014. In Proceedings of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation, Baltimore,
MD, USA, June.
112
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433?440, Sydney,
Australia, July.
Anna N. Rafferty and Christopher D. Manning. 2008a.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Anna N. Rafferty and Christopher D. Manning. 2008b.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, UK.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, USA, Septem-
ber.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation (WMT), pages 388?394,
Montr?eal, Canada, June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh?s Syntax-Based Systems at
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statisti-
cal Machine Translation. In COLING ?12: The 24th
Int. Conf. on Computational Linguistics, pages 483?
491, Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, WA, USA, Oc-
tober.
113
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130?135,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2014
Teresa Herrmann, Mohammed Mediani, Eunah Cho, Thanh-Le Ha,
Jan Niehues, Isabel Slawik, Yuqi Zhang and Alex Waibel
Institute for Anthropomatics and Robotics
KIT - Karlsruhe Institute of Technology
firstname.lastname@kit.edu
Abstract
In this paper, we present the KIT
systems participating in the Shared
Translation Task translating between
English?German and English?French.
All translations are generated using
phrase-based translation systems, using
different kinds of word-based, part-of-
speech-based and cluster-based language
models trained on the provided data.
Additional models include bilingual lan-
guage models, reordering models based
on part-of-speech tags and syntactic parse
trees, as well as a lexicalized reordering
model. In order to make use of noisy
web-crawled data, we apply filtering
and data selection methods for language
modeling. A discriminative word lexicon
using source context information proved
beneficial for all translation directions.
1 Introduction
We describe the KIT systems for the Shared Trans-
lation Task of the ACL 2014 Ninth Workshop on
Statistical Machine Translation. We participated
in the English?German and English?French
translation directions, using a phrase-based de-
coder with lattice input.
The paper is organized as follows: the next sec-
tion describes the data used for each translation
direction. Section 3 gives a detailed description of
our systems including all the models. The trans-
lation results for all directions are presented after-
wards and we close with a conclusion.
2 Data
We utilize the provided EPPS, NC and Common
Crawl parallel corpora for English?German and
German?English, plus Giga for English?French
and French?English. The monolingual part
of those parallel corpora, the News Shuffle
corpus for all four directions and additionally
the Gigaword corpus for English?French and
German?English are used as monolingual train-
ing data for the different language models. For
optimizing the system parameters, newstest2012
and newstest2013 are used as development and
test data respectively.
3 System Description
Before training we perform a common preprocess-
ing of the raw data, which includes removing long
sentences and sentences with a length mismatch
exceeding a certain threshold. Afterwards, we nor-
malize special symbols, dates, and numbers. Then
we perform smart-casing of the first letter of every
sentence. Compound splitting (Koehn and Knight,
2003) is performed on the source side of the cor-
pus for German?English translation. In order to
improve the quality of the web-crawled Common
Crawl corpus, we filter out noisy sentence pairs us-
ing an SVM classifier for all four translation tasks
as described in Mediani et al. (2011).
Unless stated otherwise, we use 4-gram lan-
guage models (LM) with modified Kneser-Ney
smoothing, trained with the SRILM toolkit (Stol-
cke, 2002). All translations are generated by
an in-house phrase-based translation system (Vo-
gel, 2003), and we use Minimum Error Rate
Training (MERT) as described in Venugopal et
al. (2005) for optimization. The word align-
ment of the parallel corpora is generated using
the GIZA++ Toolkit (Och and Ney, 2003) for
both directions. Afterwards, the alignments are
combined using the grow-diag-final-and heuris-
tic. For English?German, we use discrimi-
native word alignment trained on hand-aligned
data as described in Niehues and Vogel (2008).
The phrase table (PT) is built using the Moses
toolkit (Koehn et al., 2007). The phrase scoring
for the small data sets (German?English) is also
130
done by the Moses toolkit, whereas the bigger sets
(French?English) are scored by our in-house par-
allel phrase scorer (Mediani et al., 2012a). The
phrase pair probabilities are computed using mod-
ified Kneser-Ney smoothing as described in Foster
et al. (2006).
Since German is a highly inflected language,
we try to alleviate the out-of-vocabulary prob-
lem through quasi-morphological operations that
change the lexical entry of a known word form to
an unknown word form as described in Niehues
and Waibel (2011).
3.1 Word Reordering Models
We apply automatically learned reordering rules
based on part-of-speech (POS) sequences and syn-
tactic parse tree constituents to perform source
sentence reordering according to the target lan-
guage word order. The rules are learned
from a parallel corpus with POS tags (Schmid,
1994) for the source side and a word align-
ment to learn reordering rules that cover short
range (Rottmann and Vogel, 2007) and long
range reorderings (Niehues and Kolss, 2009).
In addition, we apply a tree-based reordering
model (Herrmann et al., 2013) to better address
the differences in word order between German and
English. Here, a word alignment and syntactic
parse trees (Rafferty and Manning, 2008; Klein
and Manning, 2003) for the source side of the
training corpus are required to learn rules on how
to reorder the constituents in the source sentence.
The POS-based and tree-based reordering rules
are applied to each input sentence before transla-
tion. The resulting reordered sentence variants as
well as the original sentence are encoded in a re-
ordering lattice. The lattice, which also includes
the original position of each word, is used as input
to the decoder.
In order to acquire phrase pairs matching the
reordered sentence variants, we perform lattice
phrase extraction (LPE) on the training corpus
where phrase are extracted from the reordered
word lattices instead of the original sentences.
In addition, we use a lexicalized reordering
model (Koehn et al., 2005) which stores reorder-
ing probabilities for each phrase pair. During
decoding the lexicalized reordering model deter-
mines the reordering orientation of each phrase
pair at the phrase boundaries. The probability for
the respective orientation with respect to the orig-
inal position of the words is included as an addi-
tional score in the log-linear model of the transla-
tion system.
3.2 Adaptation
In the French?English and English?French sys-
tems, we perform adaptation for translation mod-
els as well as for language models. The EPPS and
NC corpora are used as in-domain data for the di-
rection English?French, while NC corpus is the
in-domain data for French?English.
Two phrase tables are built: one is the out-
of-domain phrase table, which is trained on all
corpora; the other is the in-domain phrase table,
which is trained on in-domain data. We adapt the
translation model by using the scores from the two
phrase tables with the backoff approach described
in Niehues and Waibel (2012). This results in a
phrase table with six scores, the four scores from
the general phrase table as well as the two condi-
tional probabilities from the in-domain phrase ta-
ble. In addition, we take the union of the candidate
phrase pairs collected from both phrase tables A
detailed description of the union method can be
found in Mediani et al. (2012b).
The language model is adapted by log-linearly
combining the general language model and an in-
domain language model. We train a separate lan-
guage model using only the in-domain data. Then
it is used as an additional language model during
decoding. Optimal weights are set during tuning
by MERT.
3.3 Special Language Models
In addition to word-based language models, we
use different types of non-word language models
for each of the systems. With the help of a bilin-
gual language model (Niehues et al., 2011) we
are able to increase the bilingual context between
source and target words beyond phrase bound-
aries. This language model is trained on bilin-
gual tokens created from a target word and all its
aligned source words. The tokens are ordered ac-
cording to the target language word order.
Furthermore, we use language models based
on fine-grained part-of-speech tags (Schmid and
Laws, 2008) as well as word classes to allevi-
ate the sparsity problem for surface words. The
word classes are automatically learned by clus-
tering the words of the corpus using the MKCLS
algorithm (Och, 1999). These n-gram language
models are trained on the target language corpus,
131
where the words have been replaced either by their
corresponding POS tag or cluster ID. During de-
coding, these language models are used as addi-
tional models in the log-linear combination.
The data selection language model is trained
on data automatically selected using cross-entropy
differences between development sets from pre-
vious WMT workshops and the noisy crawled
data (Moore and Lewis, 2010). We selected the
top 10M sentences to train this language model.
3.4 Discriminative Word Lexicon
A discriminative word lexicon (DWL) models the
probability of a target word appearing in the trans-
lation given the words of the source sentence.
DWLs were first introduced by Mauser et al.
(2009). For every target word, they train a maxi-
mum entropy model to determine whether this tar-
get word should be in the translated sentence or
not using one feature per source word.
We use two simplifications of this model that
have shown beneficial to translation quality and
training time in the past (Mediani et al., 2011).
Firstly, we calculate the score for every phrase pair
before translating. Secondly, we restrict the nega-
tive training examples to words that occur within
matching phrase pairs.
In this evaluation, we extended the DWL
with n-gram source context features proposed
by Niehues and Waibel (2013). Instead of rep-
resenting the source sentence as a bag-of-words,
we model it as a bag-of-n-grams. This allows us
to include information about source word order in
the model. We used one feature per n-gram up to
the order of three and applied count filtering for
bigrams and trigrams.
4 Results
This section presents the participating systems
used for the submissions in the four translation
directions of the evaluation. We describe the in-
dividual components that form part of each of
the systems and report the translation qualities
achieved during system development. The scores
are reported in case-sensitive BLEU (Papineni et
al., 2002).
4.1 English-French
The development of our English?French system
is shown in Table 1.
It is noteworthy that, for this direction, we chose
to tune on a subset of 1,000 pairs from news-
test2012, due to the long time the whole set takes
to be decoded. In a preliminary set of experiments
(not reported here), we found no significant differ-
ences between tuning on the small or the big devel-
opment sets. The translation model of the baseline
system is trained on the whole parallel data after
filtering (EPPS, NC, Common Crawl, Giga). The
same data was also used for language modeling.
We also use POS-based reordering.
The biggest improvement was due to using two
additional language models. One consists of a log-
linear interpolation of individual language models
trained on the target side of the parallel data, the
News shuffle, Gigaword and NC corpora. In ad-
dition, an in-domain language model trained only
on NC data is used. This improves the score by
more than 1.4 points. Adaptation of the translation
model towards a smaller model trained on EPPS
and NC brings an additional 0.3 points.
Another 0.3 BLEU points could be gained by
using other special language models: a bilingual
language model together with a 4-gram cluster
language model (trained on all monolingual data
using the MKCLS tool and 500 clusters). Incor-
porating a lexicalized reordering model into the
system had a very noticeable effect on test namely
more than half a BLEU point.
Finally, using a discriminative word lexicon
with source context has a very small positive ef-
fect on the test score, however more than 0.3 on
dev. This final configuration was the basis of our
submitted official translation.
System Dev Test
Baseline 15.63 27.61
+ Big LMs 16.56 29.02
+ PT Adaptation 16.77 29.32
+ Bilingual + Cluster LM 16.87 29.64
+ Lexicalized Reordering 16.92 30.17
+ Source DWL 17.28 30.19
Table 1: Experiments for English?French
4.2 French-English
Several experiments were conducted for the
French?English translation system. They are
summarized in Table 2.
The baseline system is essentially a phrase-
based translation system with some preprocess-
132
ing steps on the source side and utilizing the
short-range POS-based reordering on all parallel
data and fine-grained monolingual corpora such as
EPPS and NC.
Adapting the translation model using a small in-
domain phrase table trained on NC data only helps
us gain more than 0.4 BLEU points.
Using non-word language models including a
bilingual language model and a 4-gram 50-cluster
language model trained on the whole parallel data
attains 0.24 BLEU points on the test set.
Lexicalized reordering improves our system on
the development set by 0.3 BLEU points but has
less effect on the test set with a minor improve-
ment of around 0.1 BLEU points.
We achieve our best system, which is used for
the evaluation, by adding a DWL with source con-
text yielding 31.54 BLEU points on the test set.
System Dev Test
Baseline 30.16 30.70
+ LM Adaptation 30.58 30.94
+ PT Adaptation 30.69 31.14
+ Bilingual + Cluster LM 30.85 31.38
+ Lexicalized Reordering 31.14 31.46
+ Source DWL 31.19 31.54
Table 2: Experiments for French?English
4.3 English-German
Table 3 presents how the English-German transla-
tion system is improved step by step.
In the baseline system, we used parallel data
which consists of the EPPS and NC corpora. The
phrase table is built using discriminative word
alignment. For word reordering, we use word lat-
tices with long range reordering rules. Five lan-
guage models are used in the baseline system; two
word-based language models, a bilingual language
model, and two 9-gram POS-based language mod-
els. The two word-based language models use 4-
gram context and are trained on the parallel data
and the filtered Common Crawl data separately,
while the bilingual language model is built only
on the Common Crawl corpus. The two POS-
based language models are also based on the paral-
lel data and the filtered crawled data, respectively.
When using a 9-gram cluster language model,
we get a slight improvement. The cluster is trained
with 1,000 classes using EPPS, NC, and Common
Crawl data.
We use the filtered crawled data in addition to
the parallel data in order to build the phrase table;
this gave us 1 BLEU point of improvement.
The system is improved by 0.1 BLEU points
when we use lattice phrase extraction along with
lexicalized reordering rules.
Tree-based reordering rules improved the sys-
tem performance further by another 0.1 BLEU
points.
By reducing the context of the two POS-based
language models from 9-grams to 5-grams and
shortening the context of the language model
trained on word classes to 4-grams, the score on
the development set hardly changes but we can see
a slightly improvement for the test case.
Finally, we use the DWL with source context
and build a big bilingual language model using
both the crawled and parallel data. By doing so,
we improved the translation performance by an-
other 0.3 BLEU points. This system was used for
the translation of the official test set.
System Dev Test
Baseline 16.64 18.60
+ Cluster LM 16.76 18.66
+ Common Crawl Data 17.27 19.66
+ LPE + Lexicalized Reordering 17.45 19.75
+ Tree Rules 17.53 19.85
+ Shorter n-grams 17.55 19.92
+ Source DWL + Big BiLM 17.82 20.21
Table 3: Experiments for English?German
4.4 German-English
Table 4 shows the development steps of the
German-English translation system.
For the baseline system, the training data of the
translation model consists of EPPS, NC and the
filtered parallel crawled data. The phrase table
is built using GIZA++ word alignment and lattice
phrase extraction. All language models are trained
with SRILM and scored in the decoding process
with KenLM (Heafield, 2011). We use word lat-
tices generated by short and long range reordering
rules as input to the decoder. In addition, a bilin-
gual language model and a target language model
trained on word clusters with 1,000 classes are in-
cluded in the system.
Enhancing the word reordering with tree-based
reordering rules and a lexicalized reordering
133
model improved the system performance by 0.6
BLEU points.
Adding a language model trained on selected
data from the monolingual corpora gave another
small improvement.
The DWL with source context increased the
score on the test set by another 0.5 BLEU points
and applying morphological operations to un-
known words reduced the out-of-vocabulary rate,
even though no improvement in BLEU can be ob-
served. This system was used to generate the
translation submitted to the evaluation.
System Dev Test
Baseline 24.40 26.34
+ Tree Rules 24.71 26.86
+ Lexicalized Reordering 24.89 26.93
+ LM Data Selection 24.96 27.03
+ Source DWL 25.32 27.53
+ Morphological Operations - 27.53
Table 4: Experiments for German?English
5 Conclusion
In this paper, we have described the systems
developed for our participation in the Shared
Translation Task of the WMT 2014 evaluation
for English?German and English?French. All
translations were generated using a phrase-based
translation system which was extended by addi-
tional models such as bilingual and fine-grained
part-of-speech language models. Discriminative
word lexica with source context proved beneficial
in all four language directions.
For English-French translation using a smaller
development set performed reasonably well and
reduced development time. The most noticeable
gain comes from log-linear interpolation of multi-
ple language models.
Due to the large amounts and diversity of
the data available for French-English, adapta-
tion methods and non-word language models con-
tribute the major improvements to the system.
For English-German translation, the crawled
data and a DWL using source context to guide
word choice brought most of the improvements.
Enhanced word reordering models, namely
tree-based reordering rules and a lexicalized re-
ordering model as well as the source-side fea-
tures for the discriminative word lexicon helped
improve the system performance for German-
English translation.
In average we achieved an improvement of over
1.5 BLEU over the respective baselines for all our
systems.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658.
References
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proceedings of the 2006 Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP), Sydney, Australia.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, Scotland, United Kingdom.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL 2003), Sapporo, Japan.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of the Eleventh Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2003), Budapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the Second International Workshop
on Spoken Language Translation (IWSLT 2005),
Pittsburgh, PA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL
2007), Demonstration Session, Prague, Czech Re-
public.
134
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models.
In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Suntec, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight International
Workshop on Spoken Language Translation (IWSLT
2011), San Francisco, CA, USA.
Mohammed Mediani, Jan Niehues, and Alex Waibel.
2012a. Parallel Phrase Scoring for Extra-large Cor-
pora. In The Prague Bulletin of Mathematical Lin-
guistics, number 98.
Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan
Niehues, Eunach Cho, Teresa Herrmann, Rainer
K?argel, and Alexander Waibel. 2012b. The KIT
Translation Systems for IWSLT 2012. In Proceed-
ings of the Ninth International Workshop on Spoken
Language Translation (IWSLT 2012), Hong Kong,
HK.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In Proceedings
of the ACL 2010 Conference Short Papers, Uppsala,
Sweden.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation (WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation (WMT 2008), Columbus, OH,
USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In
Proceedings of the Eight International Workshop on
Spoken Language Translation (IWSLT 2008), San
Francisco, CA, USA.
J. Niehues and A. Waibel. 2012. Detailed Analysis of
Different Strategies for Phrase Table Adaptation in
SMT. In Proceedings of the Tenth Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA 2012), San Diego, CA, USA.
J. Niehues and A. Waibel. 2013. An MT Error-Driven
Discriminative Word Lexicon using Sentence Struc-
ture Features. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, Scotland, United King-
dom.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for Deter-
mining Bilingual Word Classes. In Proceedings of
the Ninth Conference of the European Chapter of the
Association for Computational Linguistics (EACL
1999), Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German, Columbus, OH,
USA.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of the
11th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI
2007), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In In-
ternational Conference on Computational Linguis-
tics (COLING 2008), Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
135
