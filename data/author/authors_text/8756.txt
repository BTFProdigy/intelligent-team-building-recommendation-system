A Discourse Resource for Turkish: 
Annotating Discourse Connectives in the METU Corpus 
 
Deniz Zeyrek 
Department of Foreign Language  
Education 
Middle East Technical University 
Ankara, Turkey  
dezeyrek@metu.edu.tr 
Bonnie Webber 
School of Informatics 
University of Edinburgh 
Edinburgh, Scotland 
 
bonnie@inf.ed.ac.uk 
 
 
Abstract 
This paper describes first steps towards 
extending the METU Turkish Corpus 
from a sentence-level language resource to 
a discourse-level resource by annotating 
its discourse connectives and their 
arguments. The project is based on the 
same principles as the Penn Discourse 
TreeBank (http://www.seas.upenn.edu/~pdtb) 
and is supported by TUBITAK, The 
Scientific and Technological Research 
Council of Turkey. We first present the 
goals of the project and the METU 
Turkish corpus. We then describe how we 
decided what to take as explicit discourse 
connectives and the range of syntactic 
classes they come from. With 
representative examples of each class, we 
examine explicit connectives, their linear 
ordering, and types of syntactic units that 
can serve as their arguments. We then 
touch upon connectives with respect to 
free word order in Turkish and 
punctuation, as well as the important issue 
of how much material is needed to specify 
an argument. We close with a brief 
discussion of current plans. 
1 Introduction 
The goal of the project is to extend the METU 
Turkish Corpus (Say et al 2002) from a sentence-
level language resource to a discourse-level 
resource by annotating its discourse connectives, 
and their arguments. The 2-million word METU 
Turkish Corpus (MTC) is an electronic resource of 
520 samples of continuous text from 291 different 
sources written between 1990-2000. It includes 
multiple genres, such as novels, short stories, 
newspaper columns, biographies, memoirs, etc. 
annotated topographically, i.e., for paragraph 
boundaries, author, publication date, and the 
source of the text. A small part of the MTC, called 
the METU-Sabanc? TreeBank (5600 sentences) has 
been annotated with morphological features and 
dependency relationships (e.g., modifier-of, 
subject-of, object-of, etc.). The result is a set of 
dependency trees. The MTC as a whole provides a 
large-scale resource on Turkish discourse and is 
being used in research on Turkish. To date, there 
have been 81 requests for permission to use the 
MTC and 31 requests to use the TreeBank sub-
corpus. Most of the users are linguists, computer or 
cognitive scientists working on Turkish, or 
graduate students of similar disciplines. Some 
users have expressed a desire for the MTC to be 
extended by annotations at the discourse level, 
which provides further impetus for the present 
project.  
 The result of annotating discourse connectives 
will be a clearly defined level of discourse 
structure on the MTC. Annotation of text from the 
multiple genres present in the MTC will allow us 
to compare the distribution of connectives and 
their arguments across genres. The annotation will 
help researchers understand Turkish discourse by 
enabling them to give concise, clear descriptions of 
the issues concerning discourse structure and 
semantics, and support a rigorous empirical 
The 6th Workshop on Asian Languae Resources, 2008
65
characterization of where and how the free word-
order in a language like Turkish is sensitive to 
features of the surrounding discourse. It can thus 
serve as a major resource for natural language 
processing, language technology and pedagogy. 
2 Overview of Turkish Discourse 
Connectives 
From a semantic perspective, a discourse 
connective is a predicate that takes as its 
arguments, abstract objects (propositions, facts, 
events, descriptions, situations, and eventualities). 
The primary linguistic unit in which abstract 
objects (AOs) are realized in Turkish is the clause, 
either tensed or untensed. Discourse connectives 
themselves may be realized explicitly or implicitly. 
An explicit connective is realized in the form of a 
lexical item or a group of lexical items, while an 
implicit connective can be inferred from adjacent 
text spans that realise AOs and whose AOs are 
taken to be related. To constrain the amount of text 
selected for arguments, a minimality principle can 
be imposed, limiting arguments to the minimum 
amount of information needed to complete the 
interpretation of the discourse relation. The project 
will initially focus on annotating explicit 
connectives, integrating implicit ones at a later 
stage.  
One of the most challenging issues so far has 
been determining the set of explicit discourse 
connectives in Turkish (i.e., the various linguistic 
elements that can be interpreted as predicates on 
AO arguments) and the syntactic classes they are 
identified with. In the Penn Discourse TreeBank 
(PDTB), the explicit discourse connectives were 
taken to comprise (1) coordinating conjunctions, 
(2) subordinating conjunctions, and (3) discourse 
adverbials (Forbes-Riley et al 2006). But 
coordinating and subordinating conjunctions are 
not classes in Turkish per se. Moreover, most of 
the existing grammars of Turkish describe clausal 
adjuncts and adverbs in semantic (e.g., temporal, 
additive, resultative, etc.) rather than syntactic 
terms. We therefore made a rough classification 
first and determined the broad syntactic classes by 
considering the morpho-syntactic properties shared 
by elements of the initial classification.  
As a result of this process, we have come to 
identify explicit discourse connectives in Turkish 
with three grammatical types, forming five classes: 
(a) Coordinating conjunctions such as single 
lexical items ??nk? ?because?, ama ?but,? 
ve ?and?, and the particle dA.  (N.B., dA 
can also function as a subordinator.) 
(b) Paired coordinating conjunctions such as 
hem .. hem ?both and,? ne .. ne ?neither 
nor? which link two clauses, with one 
element of the pair associated with each 
clause in the discourse relation. 
(c) Simplex subordinators (also termed as 
converbs), i.e., suffixes forming non-finite 
adverbial clauses, e.g. ?(y)kAn, ?while?, -
(y)ArAk ?by means of?. 
(d) Complex subordinators, i.e., connectives 
which have two parts, usually a 
postposition (ra?men ?despite?, i?in ?for?, 
gibi ?as well as?) and an accompanying 
suffix on the (non-finite) verb of the 
subordinate clause.1  
(e)  Anaphoric connectives such as ne var ki 
?however?, ?stelik ?what is more?, ayr?ca 
?apart from this?, ilk olarak ?firstly?, etc. 
In the PDTB, non-finite clauses have not been 
annotated as arguments. However, since all non-
finite clauses are marked with a suffix in Turkish 
(see sections 4.1 and 4.2 below) and encode a 
relation between AOs, we would have missed an 
important property of the language if we had not 
identified them as discourse connectives (cf. 
Prasad et al, 2008).  
All the discourse connectives above have 
exactly two arguments. So as in English, while 
verbs in Turkish can vary in the number of 
arguments they take, Turkish discourse 
connectives take two and only two arguments. 
These can conveniently be called ARG1 and 
ARG2. It remains an open question whether there 
is any language in which discourse connectives 
take more than two arguments. 
In the following, we give representative 
examples of each of the above five classes of 
discourse connectives and discuss the assignment 
of the argument labels, linear order of arguments 
and types of arguments. By convention, we label 
                                                 
1
 Postpositions correspond to prepositions in English, though 
there are many fewer of them. They form a subordinate clause 
by nominalizing their complements and marking them with 
the dative, ablative, or the possessive case. In the examples 
given in this paper, suffixes are shown in upper-case letters. 
Case suffixes are underlined in addition to being presented in 
upper-case letters.   
The 6th Workshop on Asian Languae Resources, 2008
66
the argument containing (or with an affinity for) 
the connective as ARG2 (presented in boldface) 
and the other argument as ARG1 (presented in 
italics). Discourse connectives are underlined. This 
annotation convention is used in the English 
translations as well. Except for examples (12), 
(13), (19), (20), all examples have been taken from 
the MTC.  
3 Coordinating conjunctions 
3.1 Simple coordinating conjunctions 
Coordinating conjunctions are like English and 
combine two clauses of the same syntactic type, 
e.g., two main clauses. They are typically 
sentence-medial and show an affinity with the 
second clause (evidenced in part through 
punctuation and their ability to move to the end of 
the second clause). Whether a coordinating 
conjunction links clauses within a single sentence 
or clauses across adjacent sentences (cf. Section 6), 
it shows an affinity with the second clause. Thus 
ARG2 of these conjunctions is the second clause 
and ARG1 is the first clause.  
 
(1) Yap?lar?n? kerpi?ten yap?yorlar, ama sonra ta?? 
kullanmay? ??reniyorlar. Mimarl?k a??s?ndan ?ok 
?nemli, ??nk? bu yap? malzemesini ba?ka bir 
malzemeyle beraber kullanmay?, ilk defa 
burada g?r?yoruz. 
 
?They constructed their buildings first from mud-
bricks but then they learnt to use the stone. 
Architecturally, this is very important because we 
see the use of this construction material with 
another one at this site for the first time.?  
 
    The particle dA can serve a discourse 
connective function with an additive (Example 2) 
or adversative sense (Example 3). In contrast with 
coordinating conjunctions, the order of arguments 
to dA is normally ARG2-ARG1, thus exhibiting a 
similarity with subordinators (see below). 
However, since dA combines two clauses of the 
same syntactic type, we take it to be a simple 
coordinating conjunction.  
 
(2) Konu?may? unuttum diyorum da g?l?yorlar 
bana.   
 ?I said I?ve forgotton to talk and they laughed at 
me.? 
(3) Belki bir ?ocu?umuz olsa onunla oyalan?rd?m 
da  Allah k?smet etmedi. 
 ?If we had a child I would keep myself busy 
with her/him but God did not predestine it.?  
3.2 Paired coordinating conjunctions 
Paired coordinating conjunctions are composed of 
two lexical items, with the second often a duplicate 
of the first element. These lexical items express a 
single discourse relation, such as disjunction as in 
example (4). The order of arguments is ARG1-
ARG2 and the position of the conjunctions is 
clause-initial. 
    
(4) Birilerinin ya i?i vard?r, aceleyle y?r?rler, ya 
ko?arlar. 
 ?Some people are either busy and walk hurriedly, 
or they run.?  
4 Subordinators 
4.1 Simplex subordinators 
When a subordinate clause is reduced in Turkish, it 
loses its tense, aspect and mood properties. In this 
way, it becomes a nominal or adverbial clause 
associated with the matrix verb. The relationship of 
an adverbial clause with the AO expressed by the 
matrix verb and its arguments is conveyed by a 
small set of suffixes corresponding to English 
?while?, ?when?, ?by means of?, ?as if?, or  temporal 
?since?, added to the non-finite verb of the reduced 
clause. This pair of non-finite verb and suffix, we 
call a ?converb?. The normal order of the 
arguments of a converb is ARG2-ARG1, where the 
converb appears as the last element of ARG2. The 
following example illustrates ?(y)ArAk ?by means 
of? and its arguments: 
 
(5) Kafiye Han?m beni kucaklad?, yana??n? yana??ma 
s?rterek iyi yolculuklar diledi. 
 ?Kafiye hugged me and by rubbing her cheek 
against mine, she wished me a good trip.?  
4.2 Complex subordinators 
Complex subordinators constitute a larger set than 
the set of simplex subordinators. Here, a lexical 
item, usually a postposition, must appear with a 
nominalizing suffix and, if required, a case suffix 
as well. If the verb of the clause does not have a 
subject, it is nominalized with ?mAk (the infinitive 
suffix). If  it has a subject, it is nominalized with -
DIK (past) or ?mA (non-past) and carries the 
possessive marker agreeing with the subject of the 
The 6th Workshop on Asian Languae Resources, 2008
67
verb. The normal order of the arguments of a 
complex subordinator is the same as with 
converbs, i.e., ARG2-ARG1. The nominalizer, the 
possessive and the case suffix (if any) appear 
attached to the non-finite verb of ARG2 in that 
order. The connective appears as the last element 
of ARG2.  
Some postpositions have multiple senses, 
depending on the type of nominalizer attached to 
the non-finite verb. For example, the postposition 
i?in means causal ?since? with ?DIK (Example 6), 
and ?so as to? with ?mA or ?mAk (Example 7). In 
these examples, the lexical part of the complex 
subordinator is underlined, and the suffixes on the 
non-finite verb of ARG2 rendered in small caps.  
 
(6) Herkes ?oktan pazara ??kTI?I i?in kentin o dar,       
e?ri b??r? arka sokaklar?n? bo?alm?? ve sessiz 
bulurduk. 
 ?Since everyone has gone to the bazaar long 
time ago, we would find the narrow and curved 
back streets of the town empty and quiet.?  
(7) [Turhan Baytop] Paris Eczac?l?k Fak?ltesi 
Farmakognozi k?rs?s?nde g?rg? ve bilgisini 
artt?rMAK i?in ?al??m??t?r. 
 ?Turhan Baytop worked at Paris Pharmacology 
Faculty so as to increase his experience and 
knowledge,?  
 
Since postpositions also have a non-discourse 
role in which they signal a verb?s arguments and/or 
adjuncts, we will only annotate postpositions as 
discourse connectives when they have clausal 
elements as arguments. Given that a clausal 
element always has a nominalizing suffix, the 
distinction will be straightforward. For example, in 
(8) i?in takes an NP complement (marked with the 
possessive case) and will not be annotated, while 
in (9) ra?men  ?despite? comes with a nominalizer 
and the dative suffix, and it will be annotated: 
 
(8) Bunun i?in paraya ihtiyac?m?z var. 
 ?We need money for this.? 
(9) ?ok iyi bir bi?imde yay?lm?? olMASINA                 
ra?men Celtis (?itlenbik) poleninin yoklu?u 
dikkate de?erdir. 
 ?Despite not dispersing well, the absence of the 
Celtis [tree] polen is worthy of attention.? 
 
In general, both parts of a complex subordinator 
must be realized in the discourse. An exception is 
?if? e?er and its accompanying suffix ?sE (and the 
marker agreeing with the subject of the subordinate 
clause where necessary). The suffix suffices to 
introduce a discourse relation on its own, even 
without the postposition e?er:  
 
(10) Salman Rushdi ?ld?r?l?rSE ?slam dini bundan 
bir onur mu kazanacak? 
 ?If Salman Rushdi was to be killed, would the 
Islam religion be honoured??  
(11) E?er sigaray? b?rakmak i?in m?kemmel 
zaman? bekliyorSAn?z asla sigaray? 
b?rakamazs?n?z. 
 ?If you are waiting for the best time to stop 
smoking, you can never stop smoking?  
5 Anaphoric connectives 
The fifth type of explicit discourse connectives are 
anaphoric connectives. Anaphoric connectives are 
distinguished from clausal adverbs like ?o?unlukla 
?usually?, mutlaka ?definitely, maalesef 
?regrettably?, which are interpreted only with 
respect to their matrix sentence. In contrast, 
anaphoric connectives also require an AO from a 
sentence or group of sentences adjacent (Example 
12) or non-adjacent (Example 13) to the sentence 
containing the connective. Another important 
property of anaphoric connectives is that they can 
access the inferences in the prior discourse 
(Webber et al2003). This material is neither 
accessible by other types of discourse connectives 
nor clausal connectives.  For example, in example 
(14), the anaphoric connective yoksa ?or else, 
otherwise? accesses the inference that the 
organizations have not united and hence did not 
introduce political strategies unique to Turkey.  
 
(12) Ali hi? spor yapmaz. Sonu? olarak ?ok istedi?i   
halde kilo veremiyor. 
 ?Ali never exercises. Consequently, he can?t lose 
weight although he wants to very much.? 
(13) Zeynep ?nceleri Bodrum?da oturdu. Krediyle 
deniz kenar?nda bir ev ald?. Evi dayad?, d??edi, 
bah?eye yasemin ekti. Ne var ki banka kredisini 
?deyemedi?inden evi satmak zorunda kald?.  
 ?Zeynep first lived in Mersin. She bought a house 
by the sea on credit. She furnished it fully and 
planted jasmine in the garden. However, she had 
to sell the house because she couldn?t pay back 
the credit.? 
  
 
 
The 6th Workshop on Asian Languae Resources, 2008
68
(14) Bu ?rg?tlerin birle?erek T?rkiye?yi etkilemesi ve 
T?rkiye?ye ?zg? politikalar? g?ndeme getirmesi 
laz?m.  Yoksa Tony Blair ??yle yapt? ?imdi biz 
de ?imdi b?yle yapaca??zla olmaz. 
 ?These organizations must unite, have an impact 
on Turkey and introduce political strategies 
unique to Turkey. Or else talking about what 
Tony Blair did and hoping to do what he did is 
outright wrong.? 
6 Ordering flexibility of explicit discourse 
connectives and their arguments 
In Turkish, the linear ordering of coordinating 
conjunctions and subordinators and the clauses in 
which they occur shows some flexibility as to 
where in the clause they appear or as to the 
ordering of the clauses. For example, coordinating 
conjunctions may appear at the beginning of their 
ARG2, i.e. S-initially. This was shown earlier in 
Example (1). The sentences below illustrate ama 
?but? and ??nk? ?because? used at this position.  
 
(15) Hatem A?a?n?n mal?na kimse yana?amaz, 
dokunamazd?. Ama Osman gitmi?, Hatem 
A?a?n?n ?iftli?ini yakm??t?. 
 ?No one could approach and touch Agha 
Hatem?s property. But Osman had burnt Agha 
Hatem?s ranch.? 
(16) S?z ?zg?rl???n?n belli yasalar, belli ilkeler 
?er?evesinde kalmak zorunda oldu?unu 
biliyoruz. ??nk?, b?t?n ?zg?rl?kler gibi, belli 
s?n?rlar a??l?nca, ba?kalar?na zarar vermek, 
ba?kalar?n?n ?zg?rl?klerini zedelemek s?z 
konusu oluyor.  
 ?We know that freedom of speech should remain 
within the limits of certain laws and principles. 
Because, like all the other freedoms, when 
certain constraints are violated, one may 
harm others? freedom.? 
  
But coordinating conjunctions may also appear at 
the end of their ARG2 and so will appear S-finally 
in sentences with ARG1-ARG2 order. Below, we 
illustrate two cases of ama ?but? and ??nk? 
?because?.    
 
(17) Kaz?yabildi?ini sildi, biriktirdi mendilinin i?ine. 
?aba isteyen zor bir i?ti bu yapt??? ama. 
 ?He wiped the area he had scraped and saved all 
he could scrape in his rag. But what he was 
doing was a difficult job, requiring effort.? 
(18) Kimi m??teriler dore rengi kuma?larla, sar? 
taftalarla gelirdi de, elim dolu yapamam, diye 
geri ?evirirdi, pek anlam veremezdim. Paray? 
severdi ??nk?. 
 ?Some customers would come with gold coloured 
fabrics and yellow taffeta weaves but he would 
reject them saying his hands were full, which I 
could not give any meaning to. Because he loved 
money.?  
 
In contrast, the position of a subordinator (both 
simplex and complex) in its ARG2 clause is fixed: 
it must appear at the end of the clause, as shown in 
example (19). However, the clause is free in the 
sentence and may be moved to the right of the 
sentence, as in example (20). It is a matter of 
empirical research to find out whether different 
genres vary more in how clauses are ordered and 
what motivates preposing of ARG1. 
 
(19) Ay?e konu?urken ben dinlemiyordum. 
 ?I was not listening while Ay?e was talking.? 
(20) Ben dinlemiyordum Ay?e konu?urken. 
 ?I was not listening while Ay?e was talking.? 
7 Issues and plans 
As mentioned above, we also plan to annotate 
implicit connectives between adjacent sentences or 
clauses whose relation is not explicitly marked 
with a discourse connective. This we will do at a 
later stage, after explicit connectives have been 
annotated, following the procedure used in 
annotating implicit connectives in the PDTB 
(PDTB-Group, 2006). Preliminary analysis has 
shown that punctuation serves as a useful hint in 
inserting a coordinating conjunctions such as ?and? 
or an anaphoric connective such as ?then? or 
?consequently? between the multiple adjacent main 
clauses that can occur in a Turkish sentence 
separated by a comma. Example (21) illustrates 
these cases. 
 
(21) Y?r?yor, Imp = THEN oturuyor, resim yapmaya 
?al???yor ama yapam?yor, tabela yazmaya 
?al???yor ama yazam?yor, Imp= CONSEQUENTLY 
s?k?l?p soka?a ??k?yor, Imp=AND bisikletine 
atlad??? gibi pedallara bas?yor. 
 ?He walks around, then sits down and tries to 
draw, but he can?t. He tries to inscribe words on 
the wooden plaque, but again he can?t. 
Consequently he gets bored, goes out, and hops 
on his bike and pedals.? 
 
The 6th Workshop on Asian Languae Resources, 2008
69
A second important issue that will have to be 
tackled in the project is determining how much 
material is needed to specify the argument of a 
discourse connective. Annotation will be on text 
spans, rather than on syntactic structure. This 
reflects two facts: First, there is only a small 
amount of syntactically treebanked data in the 
MTC, and secondly, as has been discovered for 
English, one can not assume that discourse units 
map directly to syntactic units (Dinesh et al 2005). 
Preliminary analysis also shows that discourse 
units may not coincide with a clause in its entirety. 
For example, in examples (9) and (16), one can 
take ARG1 to cover only the nominal complement 
of the matrix verb: The rest of the clause is not 
necessary to the discourse relation. The ways in 
which the arguments of a discourse connective 
may diverge from syntactic units must be 
characterized for Turkish as is being done for 
English (Dinesh et al 2005). 
A third issue we will investigate is whether 
different senses of a subordinator may be identified 
simply from the type of nominalizing suffix 
required on the subordinate verb. For example, we 
have noted in examples (6) and (7) that the two 
senses of the postposition i?in (namely, ?since 
(causal)? and ?in order to?) are disambiguated by 
the nominalizing suffixes. The extent to which 
morphology aids sense disambiguation is an 
empirical issue that will be further addressed in the 
project. 
Acknowledgement 
We would like to thank Sumru ?zsoy, Asl? G?ksel 
and Cem Boz?ahin for their comments on an 
earlier version of this paper. The first author also 
thanks the Caledonian Research Foundation and 
the Royal Society of Edinburgh for awarding her 
with the European Visiting Research Fellowship, 
which made this research possible. All remaining 
errors are ours. 
References 
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi 
Prasad, Aravind Joshi and Bonnie Webber (2005). 
Attribution and the (Non-)Alignment of Syntactic 
and Discourse  Arguments of Connectives. 
Proceedings of the ACL Workshop on Frontiers in 
Corpus Annotation II: Pie in the Sky. Ann Arbor, 
Michigan. June 2005. 
Katherine Forbes-Riley, Bonnie Webber and Aravind 
Joshi (2006). Computing Discourse Semantics: The 
Predicate-Argument  Semantics of Discourse 
Connectives in D-LTAG. Journal of Semantics 23, 
pp. 55?106.  
Asl? G?ksel and Celia Kerslake (2005). Turkish: A 
Comprehensive Grammar.  London and New York: 
Routledge. 
Kornfilt, Jacklin (1997). Turkish. London and New 
York: Routledge. 
 PDTB-Group (2006). The Penn Discourse TreeBank 
1.0 Annotation Manual. Technical Report IRCS 06-
01, University of Pennsylvania. 
Rashmi Prasad, Samar Husain, Dipti Sharma and 
Aravind Joshi (2008). Towards an Annotated Corpus 
of Discourse Relations in Hindi. The Third 
International Joint Conference on Natural Language 
Processing, January 7-12, 2008.  
Bilge Say, Deniz Zeyrek, Kemal Oflazer and Umut 
?zge (2002). Development of a Corpus and a 
TreeBank for Present-day Written Turkish. 
Proceedings of the Eleventh International 
Conference of Turkish Linguistics, Eastern 
Mediterranean University, Cyprus, August 2002.  
Bonnie Webber, Aravind Joshi, Matthew Stone and 
Alistair Knott (2003). Anaphora and Discourse 
Structure. Computational Linguistics 29 (4) 547-588.  
Appendix: A preliminary list of explicit 
discourse connectives found in the MTC belonging 
to five syntactic classes and their English 
equivalents 
 
Simple coordinating  
conjunctions 
English equivalent 
ama but 
fakat but 
??nk? because 
dA and, but 
halbuki despite  
oysa despite 
?nce before 
sonra after 
ve and 
veya or 
ya da or 
veyahut or  
 
 
 
 
The 6th Workshop on Asian Languae Resources, 2008
70
Paired coordinating 
conjunctions 
English equivalent 
hem .. hem both and 
ya .. ya either or 
gerek .. gerek(se) either or 
 
Simplex subordinators 
(Converbs)  
English equivalent 
-ArAk by means of 
-Ip and 
-(y)kEn while, whereas 
-(y)AlI since 
-(I)ncA when 
 
Complex subordinators English equivalent 
-Ir gibi as if, as though  
-e?er (y)sE if 
-dI?I zaman when 
-dI?I kadar as much as 
-dI?I gibi as well as 
-dAn sonra after 
-dAn ?nce before 
-dAn dolay? due to 
-(y)sE dA even though 
-(y)Incaya kadar/dek  until 
-(y)AlI beri since (temporal) 
-(n)A ra?men/kar??l?k despite, although 
-(n)A g?re since (causal) 
  
Anaphoric connectives English equivalent 
aksi halde if not, otherwise 
aksine on the contrary 
bu nedenle for this reason 
buna ra?men/kar??l?k despite this 
bundan ba?ka besides this 
bunun yerine instead of this 
dahas? moreover, in addition 
ilk olarak firstly, first of all 
?rne?in for example 
mesela for example 
sonu? olarak consequently 
?stelik what is more 
yoksa otherwise 
ard?ndan afterwards 
 
 
 
 
 
 
 
 
 
 
The 6th Workshop on Asian Languae Resources, 2008
71
The 6th Workshop on Asian Languae Resources, 2008
72
c? 2003 Association for Computational Linguistics
Anaphora and Discourse Structure
Bonnie Webber? Matthew Stone?
Edinburgh University Rutgers University
Aravind Joshi? Alistair Knott?
University of Pennsylvania University of Otago
We argue in this article that many common adverbial phrases generally taken to signal a discourse
relation between syntactically connected units within discourse structure instead work anaphor-
ically to contribute relational meaning, with only indirect dependence on discourse structure.
This allows a simpler discourse structure to provide scaffolding for compositional semantics and
reveals multiple ways in which the relational meaning conveyed by adverbial connectives can
interact with that associated with discourse structure. We conclude by sketching out a lexicalized
grammar for discourse that facilitates discourse interpretation as a product of compositional rules,
anaphor resolution, and inference.
1. Introduction
It is a truism that a text means more than the sum of its component sentences. One
source of additional meaning are relations taken to hold between adjacent sentences
?syntactically? connected within a larger discourse structure. It has been very difficult,
however, to say what discourse relations there are, either theoretically (Mann and
Thompson 1988; Kehler 2002; Asher and Lascarides 2003) or empirically (Knott 1996).
Knott?s empirical attempt to identify and characterize cue phrases as evidence
for discourse relations illustrates some of the difficulties. Knott used the following
theory-neutral test to identify cue phrases: For a potential cue phrase ? in naturally
occurring text, consider in isolation the clause in which it appears. If the clause ap-
pears incomplete without an adjacent left context, whereas it appears complete if ? is
removed, then ? is a cue phrase. Knott?s test produced a nonexhaustive list of about
two hundred different phrases from 226 pages of text. He then attempted to charac-
terize the discourse relation(s) conveyed by each phrase by identifying when (always,
sometimes, never) one phrase could substitute for another in a way that preserved
meaning. He showed how these substitution patterns could be a consequence of a set
of semantic features and their values. Roughly speaking, one cue phrase could always
substitute for another if it had the same set of features and values, sometimes do so if
it was less specific than the other in terms of its feature values, and never do so if their
values conflicted for one or more features.
? School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail:
bonnie@inf.ed.ac.uk.
? Department of Computer Science, Rutgers Universtiy, 110 Frelinghuysen Road, Piscataway, NJ
08854-8019. E-mail: mdstone@cs.rutgers.edu.
? Department of Computer & Information Science, University of Pennsylvania, 200 South 33rd Street,
Philadelphia, PA 19104-6389. E-mail: joshi@linc.cis.upenn.edu.
? Department of Computer Science, University of Otago, P.O. Box 56, DUNEDIN 9015, New Zealand.
E-mail: alik@cs.otago.ac.nz.
546
Computational Linguistics Volume 29, Number 4
By assuming that cue phrases contribute meaning in a uniform way, Knott was
led to a set of surprisingly complex directed acyclic graphs relating cue phrases in
terms of features and their values, each graph loosely corresponding to some family of
discourse relations. But what if the relational meaning conveyed by cue phrases could
in fact interact with discourse meaning in multiple ways? Then Knott?s substitution
patterns among cue phrases may have reflected these complex interactions, as well as
the meanings of individual cue phrases themselves.
This article argues that cue phrases do depend on another mechanism for convey-
ing extrasentential meaning?specifically, anaphora. One early hint that adverbial cue
phrases (called here discourse connectives) might be anaphoric can be found in an
ACL workshop paper in which Janyce Wiebe (1993) used the following example to
question the adequacy of tree structures for discourse:
(1) a. The car was finally coming toward him.
b. He [Chee] finished his diagnostic tests,
c. feeling relief.
d. But then the car started to turn right.
The problem Wiebe noted was that the discourse connectives but and then appear to
link clause (1d) to two different things: then to clause (1b) in a sequence relation (i.e.,
the car?s starting to turn right being the next relevant event after Chee?s finishing his
tests) and but to a grouping of clauses (1a) and (1c) (i.e., reporting a contrast between,
on the one hand, Chee?s attitude toward the car coming toward him and his feeling
of relief and, on the other hand, his seeing the car turning right). (Wiebe doesn?t give
a name to the relation she posits between (1d) and the grouping of (1a) and (1c), but
it appears to be some form of contrast.)
If these relations are taken to be the basis for discourse structure, some possible
discourse structures for this example are given in Figure 1. Such structures might seem
advantageous in allowing the semantics of the example to be computed directly by
compositional rules and defeasible inference. However, both structures are directed
acyclic graphs (DAGs), with acyclicity the only constraint on what nodes can be con-
nected. Viewed syntactically, arbitrary DAGs are completely unconstrained systems.
They substantially complicate interpretive rules for discourse, in order for those rules
to account for the relative scope of unrelated operators and the contribution of syn-
tactic nodes with arbitrarily many parents.1
We are not committed to trees as the limiting case of discourse structure. For
example, we agree, by and large, with the analysis that Bateman (1999) gives of
(2) (vi) The first to do that were the German jewellers, (vii) in particular Klaus
Burie. (viii) And Morris followed very quickly after, (ix) using a lacquetry
technique to make the brooch, (x) and using acrylics, (xi) and exploring
the use of colour, (xii) and colour is another thing that was new at that
time.
1 A reviewer has suggested an alternative analysis of (1) in which clause (1a) is elaborated by clause
(1b), which is in turn elaborated by (1c), and clause (1d) stands in both a sequence relation and a
contrast relation to the segment as a whole. Although this might address Wiebe?s problem, the result is
still a DAG, and such a fix will not address the additional examples we present in section 2, in which a
purely structural account still requires DAGs with crossing arcs.
547
Webber et al Anaphora and Discourse Structure
b
seq
d
contrast
c
cb
elaboration
elaboration
a seq
a d
seq contrast
(i)
(ii)
Figure 1
Possible discourse structure for example (1). Each root and internal node is labeled by the type
of relation that Wiebe takes to hold between the daughters of that node. (i) uses an n-ary
branching sequence relation, whereas in (ii), sequence is binary branching.
(ix)(vi)
succession manner
(viii)
Figure 2
Simple multiparent structure.
in which clause (ix) stands in a manner relation with clause (viii), which in turn stands
in a succession (i.e., sequence) relation with clause (vi). This is illustrated in Figure 2,
which shows a DAG (rather than a tree), but without crossing dependencies.
So it is the cost of moving to arbitrary DAGs for discourse structure that we feel is
too great to be taken lightly. This is what has led us to look for another explanation for
these and other examples of apparent complex and crossing dependencies in discourse.
The position we argue for in this article, is that whereas adjacency and explicit
conjunction (coordinating conjunctions such as and, or, so, and but; subordinating con-
junctions such as although, whereas, and when) imply discourse relations between (the
interpretation of) adjacent or conjoined discourse units, discourse adverbials such as
then, otherwise, nevertheless, and instead are anaphors, signaling a relation between the
interpretation of their matrix clause and an entity in or derived from the discourse
context. This position has four advantages:
1. Understanding discourse adverbials as anaphors recognizes their
behavioral similarity to the pronouns and definite noun phrases (NPs)
that are the bread and butter of previous work on anaphora. This is
discussed in section 2.
2. By understanding and exploring the full range of phenomena for which
an anaphoric account is appropriate, we can better characterize anaphors
and devise more accurate algorithms for resolving them. This is explored
in section 3.
3. Any theory of discourse must still provide an account of how a sequence
of adjacent discourse units (clauses, sentences, and the larger units that
they can comprise) means more than just the sum of its component
548
Computational Linguistics Volume 29, Number 4
units. This is a goal that researchers have been pursuing for some time,
using both compositional rules and defeasible inference to determine
these additional aspects of meaning (Asher and Lascarides 1999; Gardent
1997; Hobbs et al 1993; Kehler 2002; Polanyi and van den Berg 1996;
Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996) If that
portion of discourse semantics that can be handled by mechanisms
already needed for resolving other forms of anaphora and deixis is
factored out, there is less need to stretch and possibly distort
compositional rules and defeasible inference to handle everything.2
Moreover, recognizing the possibility of two separate relations (one
derived anaphorically and one associated with adjacency and/or a
structural connective) admits additional richness to discourse semantics.
Both points are discussed further in section 4.
4. Understanding discourse adverbials as anaphors allows us to see more
clearly how a lexicalized approach to the computation of clausal syntax
and semantics extends naturally to the computation of discourse syntax
and semantics, providing a single syntactic and semantic matrix with
which to associate speaker intentions and other aspects of pragmatics
(section 5.)
The account we provide here is meant to be compatible with current approaches
to discourse semantics such as DRT (Kamp and Reyle 1993; van Eijck and Kamp
1997), dynamic semantics (Stokhof and Groenendijk 1999), and even SDRT (Asher
1993; Asher and Lascarides 2003), understood as a representational scheme rather
than an interpretive mechanism. It is also meant to be compatible with more detailed
analyses of the meaning and use of individual discourse adverbials, such as Jayes
and Rossari (1998a, 1998b) and Traugott, (1995, 1997). It provides what we believe to
be a more coherent account of how discourse meaning is computed, rather than an
alternative account of what that meaning is or what speaker intentions it is being used
to achieve.
2. Discourse Adverbials as Anaphors
2.1 Discourse Adverbials Do Not Behave like Structural Connectives
We take the building blocks of the most basic level of discourse structure to be explicit
structural connectives between adjacent discourse units (i.e., coordinating and subor-
dinating conjunctions and ?paired? conjunctions such as not only . . . but also, and on the
one hand . . . on the other (hand) and inferred relations between adjacent discourse units
(in the absense of an explicit structural connective). Here, adjacency is what triggers
the inference. Consider the following example:
(3) You shouldn?t trust John. He never returns what he borrows.
Adjacency leads the hearer to hypothesize that a discourse relation of something like
explanation holds between the two clauses. Placing the subordinate conjunction (struc-
tural connective) because between the two clauses provides more evidence for this rela-
2 There is an analogous situation at the sentence level, where the relationship between syntactic structure
and compositional semantics is simplified by factoring away intersentential anaphoric relations. Here
the factorization is so obvious that one does not even think about any other possibility.
549
Webber et al Anaphora and Discourse Structure
tion. Our goal in this section is to convince the reader that many discourse adverbials,
including then, also, otherwise, nevertheless, and instead, do not behave in this way.
Structural connectives and discourse adverbials do have one thing in common:
Like verbs, they can both be seen as heading a predicate-argument construction; unlike
verbs, their arguments are independent clauses. For example, both the subordinate
conjunction after and the adverbial then (in its temporal sense) can be seen as binary
predicates (e.g., sequence) whose arguments are clausally derived events, with the
earlier event in first position and the succeeding event in second.
But that is the only thing that discourse adverbials and structural connectives have
in common. As we have pointed out in earlier papers (Webber, Knott, and Joshi 2001;
Webber et al, 1999a, 1999b), structural connectives have two relevant properties: (1)
they admit stretching of predicate-argument dependencies; and (2) they do not admit
crossing of those dependencies. This is most obvious in the case of preposed subor-
dinate conjunctions (example (4)) or ?paired? coordinate conjunctions (example (5)).
With such connectives, the initial predicate signals that its two arguments will follow.
(4) Although John is generous, he is hard to find.
(5) On the one hand, Fred likes beans. On the other hand, he?s allergic to them.
Like verbs, structural connectives allow the distance between the predicate and its
arguments to be ?stretched? over embedded material, without loss of the dependency
between them. For the verb like and an object argument apples, such stretching without
loss of dependency is illustrated in example (6b).
(6) a. Apples John likes.
b. Apples Bill thinks he heard Fred say John likes.
That this also happens with structural connectives and their arguments is illustrated
in example (7) (in which the first clause of example (4) is elaborated by another pre-
posed subordinate-main clause construction embedded within it) and in example (8)
(in which the first conjunct of example (5) is elaborated by another paired-conjunction
construction embedded within it). Possible discourse structures for these examples are
given in Figure 3.
(7) a. Although John is very generous?
b. if you need some money,
c. you only have to ask him for it?
d. he?s very hard to find.
(8) a. On the one hand, Fred likes beans.
b. Not only does he eat them for dinner.
c. But he also eats them for breakfast and snacks.
d. On the other hand, he?s allergic to them.
But, as already noted, structural connectives do not admit crossing of predicate-
argument dependencies. If we admit crossing dependencies in examples (7) and (8),
we get
(9) a. Although John is very generous?
b. if you need some money?
550
Computational Linguistics Volume 29, Number 4
(i) (ii)
a
b
d
contrast[one/other]
elaboration
comparison[not only/but also]
a
b
d
elaboration
concession[although]
cc
condition[if]
Figure 3
Discourse structures associated with (i) example (7) and (ii) Example (8).
a cb
concession[although] condition[if]
elaboration
a b
elaboration
contrast[one/other] comparison[not only...]
(i) (ii)
dcd
Figure 4
(Impossible) discourse structures that would have to be associated with (i) Example (9) and
(ii) example (10).
c. he?s very hard to find?
d. you only have to ask him for it.
(10) a. On the one hand, Fred likes beans.
b. Not only does he eat them for dinner.
c. On the other hand, he?s allergic to them.
d. But he also eats them for breakfast and snacks.
Possible discourse structures for these (impossible) discourses are given in Figure 4.
Even if the reader finds no problem with these crossed versions, they clearly do not
mean the same thing as their uncrossed counterparts: In (10), but now appears to link
(10d) with (10c), conveying that despite being allergic to beans, Fred eats them for
breakfast and snacks. And although this might be inferred from (8), it is certainly not
conveyed directly. As a consequence, we stipulate that structural connectives do not
admit crossing of their predicate-argument dependencies.3
That is not all. Since we take the basic level of discourse structure to be a conse-
quence of (1) relations associated with explicit structural connectives and (2) relations
3 A reviewer has asked how much stretching is possible in discourse without losing its thread or having
to rephrase later material in light of the intervening material. One could ask a similar question about
the apparently unbounded dependencies of sentence-level syntax, which inattentive speakers are prone
to lose track of and ?fracture.? Neither question seems answerable on theoretical grounds alone, with
both demanding substantial amounts of empirical data from both written and spoken discourse. The
point we are trying to make is simply that there is a difference in discourse between any amount of
stretching and even the smallest amount of crossing.
551
Webber et al Anaphora and Discourse Structure
whose defeasible inference is triggered by adjacency, we stipulate that discourse struc-
ture itself does not admit crossing structural dependencies. (In this sense, discourse structure
may be truly simpler than sentence structure. To verify this, one might examine the
discourse structure of languages such as Dutch that allow crossing dependencies in
sentence-level syntax. Initial cursory examination does not give any evidence of cross-
ing dependencies in Dutch discourse.)
If we now consider the corresponding properties of discourse adverbials, we see
that they do admit crossing of predicate-argument dependencies, as shown in exam-
ples (11)?(13).
(11) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because then he discovered he was broke.
(12) a. High heels are fine for going to the theater.
b. But wear comfortable shoes
c. if instead you plan to go to the zoo.
(13) a. Because Fred is ill
b. you will have to stay home
c. whereas otherwise the two of you could have gone to the zoo.
Consider first the discourse adverbial then in clause (11d). For it to get its first
argument from (11b) (i.e., the event that the discovery in (d) is ?after?), it must cross
the structural connection between clauses (c) and (d) associated with because). This
crossing dependency is illustrated in Figure 5(i). Now consider the discourse adverbial
instead) in clause (12c). For it to get its first argument from (12a) (i.e., going to the zoo is
an alternative to going to the theater), it must cross the structural connection between
clauses (12b) and (12c) associated with if. This crossing dependency is illustrated in
Figure 5(ii). Example (13) is its mirror image: For the discourse adverbial otherwise in
(13c) to get its first argument from (13a) (i.e., alternatives to the state/condition of
Fred being ill), it must cross the structural connection associated with because. This is
illustrated in Figure 5(iii).
Crossing dependencies are not unusual in discourse when one considers anaphora
(e.g., pronouns and definite NPs), as for example in
b
conseq[so]
contrast[but]
dc
seq[then]
b
explanation[because]
conditional[if]
a
contrast[but]
alt[instead]
a
c ba
contrast[whereas]
c
explanation?[because]
alt[otherwise]
(i) (ii) (iii)
Figure 5
Discourse structures for examples (11)?(13). Structural dependencies are indicated by solid
lines and dependencies associate with discourse adverbials are indicated by broken lines.
(explanation? is the inverse of explanation?i.e., with its arguments in reverse order. Such
relations are used to maintain the given linear order of clauses.)
552
Computational Linguistics Volume 29, Number 4
(14) Every mani tells every womanj hei meets that shej reminds himi of hisi
mother.
(15) Suei drives an Alfa Romeo. Shei drives too fast. Maryj races heri on week-
ends. Shej often beats heri. (Strube 1998)
This suggests that in examples (11)?(13), the relationship between the discourse ad-
verbial and its (initial) argument from the previous discourse might usefully be taken
to be anaphoric as well.4
2.2 Discourse Adverbials Do Behave like Anaphors
There is additional evidence to suggest that otherwise, then, and other discourse adver-
bials are anaphors. First, anaphors in the form of definite and demonstrative NPs can
take implicit material as their referents. For example, in
(16) Stack five blocks on top of one another. Now close your eyes and try
knocking {the tower, this tower} over with your nose.
both NPs refer to the structure which is the implicit result of the block stacking.
(Further discussion of such examples can be found in Isard [1975]; Dale [1992]; and
Webber and Baldwin [1992].) The same is true of discourse adverbials. In
(17) Do you want an apple? Otherwise you can have a pear.
the situation in which you can have a pear is one in which you don?t want an apple?
that is, one in which your answer to the question is ?no.? But this answer isn?t there
structurally: It is only inferred. Although it appears natural to resolve an anaphor to an
inferred entity, it would be much more difficult to establish such links through purely
structural connections: To do so would require complex transformations that introduce
invisible elements into discourse syntax with no deeper motivation. For example, in
(17), we would need a rule that takes a discourse unit consisting solely of a yes/no
question P? and replaces it with a complex segment consisting of P? and the clause
it is possible that P, with the two related by something like elaboration. Then and only
then could we account for the interpretation of the subsequent otherwise structurally,
by a syntactic link to the covert material (i.e., to the possibility that P holds, which
otherwise introduces an alterative to).
Second, discourse adverbials have a wider range of options with respect to their
initial argument than do structural connectives (i.e., coordinating and subordinating
conjunctions). The latter are constrained to linking a discourse unit on the right frontier
of the evolving discourse (i.e., the clause, sentence and larger discourse units to its
immediate left). Discourse adverbials are not so constrained. To see this, consider the
following example:
4 We are aware that crossing examples such as (11)?(13) are rare in naturally occurring discourse. We
believe that this is because they are only possible when, as here, strong constraints from the discourse
adverbial and from context prevent the adverbial from relating to the closest (leftmost) eventuality or
an eventuality coerced from that one. But rarity doesn?t necessarily mean ill-formedness or marginality,
as readers can see for themselves if they use Google to search the Web for strings such as because then, if
instead, and whereas otherwise, and consider (1) whether the hundreds, even thousands, of texts in which
these strings occur are ill-formed, and (2) what then, instead, and otherwise are relating in these texts.
One must look at rare events if one is studying complex linguistic phenomena in detail. Thus it is not
the case that only common things in language are real or worth further study.
553
Webber et al Anaphora and Discourse Structure
(18) If the light is red, stop. Otherwise you?ll get a ticket.
(If you do something other than stop, you?ll get a ticket.)
This can be paraphrased using the conjunction or:
If the light is red, stop, or you?ll get a ticket.
Here or links its right argument to a unit on the right frontier of the evolving discourse?
in this case, the clause stop on its immediate left. Now consider the related example
(19) If the light is red, stop. Otherwise go straight on.
(If the light is not red, go straight on.)
This cannot be paraphrased with or, as in
(20) If the light is red, stop, or go straight on.
even though both stop and If the light is red, stop are on the right frontier of the evolving
discourse structure. This is because otherwise is accessing something else, so that (20)
means something quite different from either (18) or (19). What otherwise is accessing,
which or cannot, is the interpretation of the condition alone.5 Thus discourse adver-
bials, like other anaphors, have access to material that is not available to structural
connectives.
Finally, discourse adverbials, like other anaphors, may require semantic represen-
tations in which their arguments are bound variables ranging over discourse entities.
That is, whereas it might be possible to represent Although P, Q using a binary modal
operator
(21) although(p, q)
where formulas p and q translate the sentences P and Q that although combines, we
cannot represent P . . .Nevertheless, Q this way. We need something more like
(22) p ? nevertheless(e, q)
The motivation for the variable e in this representation is that discourse adverbials,
like pronouns, can appear intrasententially in an analog of donkey sentences. Donkey
sentences such as example (23) are a special kind of bound-variable reading:
(23) Every farmer who owns a donkey feeds it rutabagas.
In donkey sentences, anaphors are interpreted as covarying with their antecedents:
The it that is being fed in (23) varies with the farmer who feeds it. These anaphors,
however, appear in a structural and interpretive environment in which a direct syn-
tactic relationship between anaphor and antecedent is normally impossible, so they
cannot be a reflex of true binding in the syntax-semantics interface. Rather, donkey
sentences show that discourse semantics has to provide variables to translate pronouns,
5 This was independently pointed out by several people when this work was presented at ESSLLI?01 in
Helsinki in August 2001. The authors would like to thank Natalia Modjeska, Lauri Karttunen, Mark
Steedman, Robin Cooper, and David Traum for bringing it to their attention.
554
Computational Linguistics Volume 29, Number 4
and that discourse mechanisms must interpret these variables as bound?even though
the pronouns appear ?free? by syntactic criteria.
Thus, it is significant that discourse adverbials can appear in their own version of
donkey sentences, as in
(24) a. Anyone who has developed innovative new software has then had to
hire a lawyer to protect his/her interests. (i.e., after developing innovative
new software)
b. Several people who have developed innovative new software have
nevertheless failed to profit from it. (i.e., despite having developed innovative
new software)
c. Every person selling ?The Big Issue? might otherwise be asking for
spare change. (i.e., if he/she weren?t selling ?The Big Issue?)
The examples in (24) involve binding in the interpretation of discourse adverbials.
In (24a), the temporal use of then locates each hiring event after the corresponding
software development. Likewise in (24b), the adversative use of nevertheless signals
each developer?s unexpected failure to turn the corresponding profit. And in (24c),
otherwise envisions each person?s begging if that person weren?t selling ?The Big Issue?.
Such bound interpretations require variables in the semantic representations and
alternative values for them in some model?hence the representation given in (22).
Indeed, it is clear that the binding here has to be the discourse kind, not the syntac-
tic kind, for the same reason as in (23), although we cannot imagine anyone arguing
otherwise, since discourse adverbials have always been treated as elements of dis-
course interpretation. So the variables must be the discourse variables usually used to
translate other kinds of discourse anaphors.6
These arguments have been directed at the behavioral similarity between discourse
adverbials and what we normally take to be discourse anaphors. But this isn?t the only
reason to recognize discourse adverbials as anaphors: In the next section, we suggest
a framework for anaphora that is broad enough to include discourse adverbials as
well as definite and demonstrative pronouns and NPs, along with other discourse
phenomena that have been argued to be anaphoric, such as VP ellipsis (Hardt 1992,
1999; Kehler 2002), tense (Partee 1984; Webber 1988) and modality (Kibble 1995; Frank
and Kamp 1997; Stone and Hardt 1999).
3. A Framework for Anaphora
Here we show how only a single extension to a general framework for discourse
anaphora is needed to cover discourse adverbials. The general framework is presented
in Section 3.1, and the extension in Section 3.2.
3.1 Discourse Referents and Anaphor Interpretation
The simplest discourse anaphors are coreferential: definite pronouns and definite NPs
that denote one (or more) discourse referents in focus within the current discourse
6 Although rhetorical structure theory (RST) (Mann and Thompson 1998) was developed as an account
of the relation between adjacent units within a text, Marcu?s guide to RST annotation (Marcu 1999) has
added an ?embedded? version of each RST relation in order to handle examples such as (24). Although
this importantly recognizes that material in an embedded clause can bear a semantic relation to its
matrix clause, it does not contribute to understanding the nature of the phenomenon.
555
Webber et al Anaphora and Discourse Structure
context. (Under coreference we include split reference, in which a plural anaphor such
as the companies denotes all the separately mentioned companies in focus within the
discourse context.) Much has been written about the factors affecting what discourse
referents are taken to be in focus. For a recent review by Andrew Kehler, see chap-
ter 18 of Jurafsky and Martin (2000). For the effect of different types of quantifiers on
discourse referents and focus, see Kibble (1995).
Somewhat more complex than coreference is indirect anaphora (Hellman and
Fraurud 1996) (also called partial anaphora [Luperfoy 1992], textual ellipsis [Hahn,
Markert, and Strube 1996], associative anaphora [Cosse 1996] bridging anaphora
[Clark 1975; Clark and Marshall 1981; Not, Tovena, and Zancanaro 1999], and in-
ferrables [Prince 1992]), in which the anaphor (usually a definite NP) denotes a dis-
course referent associated with one (or more) discourse referents in the current discourse
context; for example,
(25) Myra darted to a phone and picked up the receiver.
Here the receiver denotes the receiver associated with (by virtue of being part of) the
already-mentioned phone Myra darted to.
Coreference and indirect anaphora can be uniformly modeled by saying that the
discourse referent e? denoted by an anaphoric expression ? is either equal to or asso-
ciated with an existing discourse referent er, that is, e?=er or e? ?assoc(er). But coref-
erence and associative anaphora do not exhaust the space of constructs that derive
all or part of their sense from the discourse context and are thus anaphoric. Consider
?other NPs? (Bierner 2001a; Bierner and Webber 2000; Modjeska 2001, 2002), as in:
(26) Sue grabbed one phone, as Tom darted to the other phone.
Although ?other NPs? are clearly anaphoric, should the referent of the other phone
(e?)?the phone other than the one Sue grabbed (er)?simply be considered a case of
e? ? assoc(er)? Here are two reasons why they should not.
First, in all cases of associative anaphora discussed in the literature, possible as-
sociations have depended only on the antecedent er and not on the anaphor. For
example, only antecedents that have parts participate in whole-part associations (e.g.,
phone ? receiver). Only antecedents with functional schemata participate in schema-
based associations (e.g., lock ? key). In (26), the relationship between e?, the referent
of the other phone, and its antecedent, er, depends in part on the anaphor, and not just
on the antecedent?in particular, on the presence of the word other.
Second, we also have examples such as
(27) Sue lifted the receiver as Tom darted to the other phone.7
in which the referent of the other phone (e?) is the phone other than the phone associated
with the receiver that Sue lifted. Together, these two points argue for a third possibility,
in which an anaphoric element can convey a specific function f? that is idiosyncratic
to the anaphor, which may be applied to either er or an associate of er. The result of
that application is e?. For want of a better name, we will call these lexically specified
anaphors.
Other lexically specified anaphors include noun phrases headed by other (exam-
ple (28)), NPs with such but no postmodifying as phrase (example (29)), comparative
7 Modjeska (2001) discovered such examples in the British National Corpus.
556
Computational Linguistics Volume 29, Number 4
NPs with no postmodifying than phrase (example (30)), and the pronoun elsewhere
(example (31)) (Bierner 2001b)
(28) Some dogs are constantly on the move. Others lie around until you call
them.
(29) I saw a 2kg lobster in the fish store yesterday. The fishmonger said it takes
about five years to grow to such a size.
(30) Terriers are very nervous. Larger dogs tend to have calmer dispositions.
(31) I don?t like sitting in this room. Can we move elsewhere?
To summarize the situation with anaphors so far, we have coreference when e?=er,
indirect anaphora when e? ?assoc(er), and lexically specified anaphora when e?=f?(ei)
where ei = er or ei ?assoc(er).
3.2 Discourse Adverbials as Lexical Anaphors
There is nothing in this generalized approach to discourse anaphora that requires that
the source of er be an NP, or that the anaphor be a pronoun or NP. For example, the
antecedent er of a singular demonstrative pronoun (in English, this or that) is often
an eventuality that derives from a clause, a sentence, or a larger unit in the recent
discourse (Asher 1993; Byron 2002; Eckert and Strube 2000; Webber 1991). We will
show that this is the case with discourse adverbials as well.
The extension we make to the general framework presented above in order to
include discourse adverbials as discourse anaphors is to allow more general functions
f? to be associated with lexically specified anaphors. In particular, for the discourse
adverbials considered in this article, the function associated with an adverbial maps its
anaphoric argument?an eventuality derived from the current discourse context?to
a function that applies to the interpretation of the adverbial?s matrix clause (itself an
eventuality). The result is a binary relation that holds between the two eventualities
and is added to the discourse context. For example, in
(32) John loves Barolo. So he ordered three cases of the ?97. But he had to
cancel the order because he then discovered he was broke.
then, roughly speaking, contributes the fact that its matrix clause event (John?s find-
ing he was broke) is after the anaphorically derived event of his ordering the wine.8
Similarly, in
(33) John didn?t have enough money to buy a mango. Instead, he bought a
guava.
instead contributes the fact that its matrix clause event (buying a guava) is an alternative
to the anaphorically derived event of buying a mango. The relation between the two
sentences is something like result, as in So instead, he bought a guava.
8 Words and phrases that function as discourse adverbials usually have other roles as well: For example,
otherwise also serves as an adjectival modifier, as in I was otherwise occupied with grading exams. This
overloading of closed-class lexico-syntactic items is not unusual in English, and any ambiguities that
arise must be handled as part of the normal ambiguity resolution process.
557
Webber et al Anaphora and Discourse Structure
Note that our only concern here is with the compositional and anaphoric mech-
anisms by which adverbials contribute meaning. For detailed analysis of the lexical
semantics of adverbials (but no attention to mechanism), the reader is referred to Jayes
and Rossari (1998a, 1998b, Lagerwerf (1998), Traugott (1995, 1997), and others.
Formally, we represent the function that a discourse adverbial ? contributes as a
?-expression involving a binary relation R? that is idiosyncratic to ?, one of whose
arguments (represented here by the variable EV) is resolved anaphorically:
?x . R?(x, EV)
R? gets its other argument compositionally, when this ?-expression is applied to ??s
matrix clause S interpreted as an eventuality ?, that is,
[?x . R?(x, EV)]? ? R?(?, EV)
The result of both function application and resolving EV to some eventuality ei derived
from the discourse context either directly or by association is the proposition R?(?, ei),
one of whose arguments (ei) has been supplied by the discourse context and the other
(?) compositionally from syntax.
Note that this is a formal model, meant to have no implications for how pro-
cessing takes place. We have not tried at this stage to instantiate our view of how
discourse adverbials are resolved in the context of (simultaneous) sentence-level and
discourse-level processing. Our basic view is that resolution is initiated when the dis-
course adverbial (?) is encountered. As ??s matrix clause S is incrementally parsed and
interpreted, producing eventuality ?, the resolution process polls the discourse context
and either finds an appropriate eventuality ei (or creates one by a bridging inference,
as illustrated in the next section) such that R?(?, ei) makes sense with respect to the
discourse so far. As is the case with resolving a discourse deictic (Asher 1993; Byron
2002; Eckert and Strube 2000; Webber 1991) this resolution process would use syn-
tactic and semantic constraints that it accumulates as the incremental sentence-level
parser/interpreter processes S. As with discourse deixis, this is best seen as a con-
straint satisfaction problem that involves finding or deriving an eventuality from the
current discourse context that meets the constraints of the adverbial with respect to the
eventuality interpretation of the matrix clause. (Examples of this are given throughout
the rest of the article.)
3.3 A Logical Form for Eventualities
Before using this generalized view of anaphora to show what discourse adverbials
contribute to discourse and how they interact with discourse relations that arise from
adjacency or explicit discourse connectives, we briefly describe how we represent
clausal interpretations in logical form (LF).
Essentially, we follow Hobbs (1985) in using a rich ontology and a representation
scheme that makes explicit all the individuals and abstract objects (i.e., propositions,
facts/beliefs, and eventualities) (Asher 1993) involved in the LF interpretation of an
utterance. We do so because we want to make intuitions about individuals, eventual-
ities, lexical meaning, and anaphora as clear as possible. But certainly, other forms of
representation are possible.
In this LF representation scheme, each clause and each relation between clauses
is indexed by the label of its associated abstract object. So, for example, the LF inter-
pretation of the sentence
(34) John left because Mary left.
558
Computational Linguistics Volume 29, Number 4
would be written
e1:left(j) ? john(j) ? e2:left(m) ? mary(m) ? e3:because(e1,e2)
where the first argument of the asymmetric binary predicate because is the consequent
and the second is the eventuality leading to this consequent. Thus when because occurs
sentence-medially, as in the above example, the eventuality arguments are in the same
order as their corresponding clauses occur in the text. When because occurs sentence-
initially (as in Because Mary left, John did), the interpretation of the second clause (John
[left]) will appear as the first argument and the interpretation of the first clause (Mary
left) will appear as the second.9
The set of available discourse referents includes both individuals like j and m, and
also abstract objects like e1 and e2. We then represent resolved anaphors by reusing
these discourse referents. So, for example, the LF interpretation of the follow-on sen-
tence
(35) This upset Sue.
would be written
e4:upset(DPRO, s) ? sue(s)
where DPRO is the anaphoric variable contributed by the demonstrative pronoun this.
Since the subject of upset could be either the eventuality of John?s leaving or the fact
that he left because Mary left, DPRO could be resolved to either e1 or e3, that is,
a. e4:upset(e1, s) ? sue(s)
b. e4:upset(e3, s) ? sue(s)
depending on whether one took Sue to have been upset by (1) John?s leaving or (2)
that he left because Mary left.
3.4 The Contribution of Discourse Adverbials to Discourse Semantics
Here we step through some examples of discourse adverbials and demonstrate how
they make their semantic contribution to the discourse context. We start with exam-
ple (32), repeated here as (36):
(36) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because he then discovered he was broke.
9 We are not claiming to give a detailed semantics of discourse connectives except insofar as they may
affect how discourse adverbials are resolved. Thus, for example, we are not bothering to distinguish
among different senses of because (epistemic vs. nonepistemic), while (temporal vs. concessive), since
(temporal vs. causal), etc. Of course, these distinctions are important to discourse interpretation, but
they are independent of and orthogonal to the points made in this article. Similarly, Asher (1993)
argues that a simple ontology of eventualities is too coarse-grained, and that discourse representations
need to distinguish different kinds of abstract objects, including actions, propositions, and facts as well
as eventualities. Different discourse connectives will require different kinds of abstract objects as
arguments. This distinction is also orthogonal to the points made in this article, because we can
understand these abstract referents to be associates of the corresponding Hobbsian eventualities and
leave the appropriate choice to the lexical semantics of discourse connectives. Byron (2002) advocates a
similar approach to resolving discourse anaphora.
559
Webber et al Anaphora and Discourse Structure
Using the above LF representation scheme and our notation from Section 3.2, namely,
? ? = the anaphoric expression (here, the discourse adverbial)
? R? = the relation name linked with ?
? S = the matrix clause/sentence containing ?
? ? = the interpretation of S as an abstract object
and ignoring, for now, the conjunction because (discussed in section 4), the relevant
elements of (36d) can be represented as:
? = then
R? = after
S = he [John] discovered he was broke
? = e4:find(j,e5), where e5:broke(j)
This means that the unresolved interpretation of (36d) is
[?x . R?(x,EV)]? ? [?x . after(x,EV)]e4 ? after(e4, EV)
The anaphoric argument EV is resolved to the eventuality e2, derived from (36b)?
e2:order(j, c1).
after(e4,EV) ? after(e4,e2)
That is, the eventuality of John?s finding he was broke is after that of John?s ordering
three cases of the ?97 Barolo. The resulting proposition after(e4,e2) would be given its
own index, e6, and added to the discourse context.
When then is understood temporally, as it is above, as opposed to logically, it
requires a culminated eventuality from the discourse context as its first argument
(which Vendler (1967) calls an achievement or an accomplishment). The ordering
event in (36b) is such a Vendlerian accomplishment. In example (37), though, there
is no culminated eventuality in the discourse context for then), to take as its first
argument.
(37) a. Go west on Lancaster Avenue.
b. Then turn right on County Line.
How does (37b) get its interpretation?
As with (36d), the relevant elements of (37b) can be represented as
? = then
R? = after
S = turn right on County Line
? = e3:turn-right(you, county line)
and the unresolved interpretation of (37b) is thus
[? x . after(x, EV)]e3 ? after(e3, EV)
560
Computational Linguistics Volume 29, Number 4
As for resolving EV, in a well-known article, Moens and Steedman (1988) discuss
several ways in which an eventuality of one type (e.g., a process) can be coerced into
an eventuality of another type (e.g., an accomplishment, which Moens and Steedman
call a culminated process). In this case, the matrix argument of then (the eventuality of
turning right on County Line) can be used to coerce the process eventuality in (37b) into
a culminated process of going west on Lancaster Avenue until County Line. We treat this
coercion as a type of associative or bridging inference, as in the examples discussed
in section 3.1. That is,
e2 = culmination(e1)?assoc(e1), where e1:go-west(you, lancaster ave)
Taking this e2 as the anaphoric argument EV of then yields the proposition
after(e3, e2)
That is, the eventuality of turning right onto County Line is after that of going west
on Lancaster Avenue to County Line. This proposition would be indexed and added
to the discourse context.
It is important to stress here that the level of representation we are concerned
with is essentially an LF for discourse. Any reasoning that might then have to be done
on the content of LFs might then require making explicit the different modal and
temporal contexts involved, their accessibility relations, the status of abstract objects
as facts, propositions or eventualities, and so on. But as our goal here is primarily
to capture the mechanism by means of which discourse adverbials are involved in
discourse structure and discourse semantics, we will continue to assume for as long
as possible that an LF representation will suffice.
Now it may appear as if there is no difference between treating adverbials as
anaphors and treating them as structural connectives, especially in cases like (37) in
which the antecedent comes from the immediately left-adjacent context, and in which
the only obvious semantic relation between the adjacent sentences appears to be the
one expressed by the discourse adverbial. (Of course, there may also be a separate
intentional relation between the two sentences [Moore and Pollack 1992], independent
of the relation conveyed by the discourse adverbial.)
One must distinguish, however, between whether a theory allows a distinction
to be made and whether that distinction needs to be made in a particular case. It
is clear that there are many examples in which the two approaches (i.e., a purely
structural treatment of all connectives, versus one that treats adverbials as linking into
the discourse context anaphorically) appear to make the same prediction. We have
already, however, demonstrated cases in which a purely structural account makes the
wrong prediction, and in the next section, we will demonstrate the additional power
of an account that allows for two relations between an adverbial?s matrix clause or
sentence and the previous discourse: one arising from the anaphoric connection and
the other inferred from adjacency or conveyed explicitly by a structural connective.
Before closing this section, we want to step through examples (19)?(20), repeated
here as examples (38)?(39).
(38) If the light is red, stop. Otherwise you?ll get a ticket.
(39) If the light is red, stop. Otherwise go straight on.
561
Webber et al Anaphora and Discourse Structure
Roughly speaking, otherwise conveys that the complement of its anaphorically derived
argument serves as the condition under which the interpretation of its structural ar-
gument holds. (This complement must be with respect to some contextually relevant
set.)10
If we represent a conditional relation between two eventualities with the asym-
metric relation if(e1,e2), where e1 is derived from the antecedent and e2 from the conse-
quent, and we approximate a single contextually relevant alternative e2 to an eventu-
ality e1 using a symmetric complement relation, complement(e1, e2), then we can represent
the interpretation of otherwise as
? x . if(VE, x), where complement(VE, EV)
where variable EV is resolved anaphorically to an eventuality in the current discourse
context that admits a complement. That is, otherwise requires a contextually relevant
complement to its antecedent and asserts that if that complement holds, the argument
to the ?-expression will as well. The resulting ?-expression applies to the interpretation
of the matrix clause of otherwise, resulting in the conditional?s being added to the
discourse context:
[?x . if(VE,x)] ? ? if(VE,?), where complement(VE,EV)
Here the relevant elements of (38b) and (39b) can be represented as
? = otherwise
R? = if
S38 = you get a ticket
?38 = e3, where e3:get ticket(you)
S39 = go straight on
?39 = e3? , where e3? :go straight(you)
The unresolved interpretations of (38b) and (39b) are thus:
[?x . if(VE38,x)] e3 ? if(VE38,e3), where complement(VE38,EV38)
[?x . if(VE39,x)] e3? ? if(VE39,e3? ), where complement(VE39,EV39)
As we showed in section 2.2, different ways of resolving the anaphoric argument lead
to different interpretations. In (38), the anaphoric argument is resolved to e2:stop(you),
10 Kruijff-Korbayova? and Webber (2001a) demonstrate that the information structure of sentences in the
previous discourse (theme-rheme partitioning, as well as focus within theme and within rheme
[Steedman 2000a]) can influence what eventualities er are available for resolving the anaphorically
derived argument of otherwise. This then correctly predicts different interpretations for ?otherwise? in
(i) and (ii):
(i) Q. How should I transport the dog?
A. You should carry the dog. Otherwise you might get hurt.
(ii) Q. What should I carry?
A. You should carry the dog. Otherwise you might get hurt.
In both (i) and (ii), the questions constrain the theme/rheme partition of the answer. Small capitals
represent focus within the rheme. In (i), the otherwise clause will be interpreted as warning the hearer
(H) that H might get hurt if he/she transports the dog in some way other than carrying it (e.g., H might
get tangled up in its lead). In (ii), the otherwise clause warns H that he/she might get hurt if what she
is carrying is not the dog (e.g., H might be walking past fanatical members of the Royal Kennel Club).
562
Computational Linguistics Volume 29, Number 4
whereas in (39), it is resolved to e1:red(light1). Thus the resulting interpretations of
(38b) and (39b) are, respectively,
if(e4,e3), where complement(e2,e4) and e2:stop(you)
(If you do something other than stop, you?ll get a ticket.)
if(e4? , e3? ), where complement(e1,e4? ) and e1:red(light)
(If the light is not red, go straight on.)
We have not been specific about how the anaphoric argument of otherwise (or
of any other discourse adverbial) is resolved, other than having it treated as a con-
straint satisfaction problem. This is the subject of current and future work, exploring
the empirical properties of resolution algorithms with data drawn from appropriately
annotated corpora and from psycholinguistic studies of human discourse interpreta-
tion. To this end, Creswell et al (2002) report on a preliminary annotation study of
discourse adverbials and the location and type of their antecedents. This initial ef-
fort involves nine discourse adverbials?three each from the classes of concessive,
result, and reinforcing (additive) conjuncts given in Quirk et al (1972). Meanwhile,
Venditti et al (2002) present a preliminary report on the use of a constraint satisfac-
tion model of interpretation, crucially combining anaphoric and structural reasoning
about discourse relations, to predict subjects? on-line interpretation of discourses in-
volving stressed pronouns. In addition, two proposals have recently been submitted to
construct a larger and more extensively annotated corpus, covering more adverbials,
based on what we have learned from this initial effort. This more extensive study
would be an adequate basis for developing resolution algorithms.11
3.5 Summary
In this section, we have presented a general framework for anaphora with the follow-
ing features:
? Anaphors can access one or more discourse referents or entities
associated with them through bridging inferences. These are sufficient
for interpreting anaphoric pronouns, definite NPs and demonstrative
NPs, allowing entities to be evoked by NPs or by clauses. In the case of
clauses, this may be on an as-needed basis, as in Eckert and Strube
(2000).
? A type of anaphor ? that we call lexically specified can also contribute
additional meaning through a function f? that is idiosyncratic to ?, that
can be applied to either an existing discourse referent or an entity
associated with it through a bridging inference. In the case of the
premodifier other, f? applied to its argument produces contextually
11 With respect to how many discourse adverbials there are, Quirk et al (1972) discuss 60 conjunctions
and discourse adverbials under the overall heading time relations and 123 under the overall heading
conjuncts. Some entries appear under several headings, so that the total number of conjunctions and
discourse adverbials they present is closer to 160. In another enumeration of discourse adverbials,
Forbes and Webber (2002) start with all annotations of sentence-level adverbials in the Penn Treebank,
then filter them systematically to determine which draw part of their meaning from the preceding
discourse and how they do so. What we understand from both of these studies is that there are fewer
than 200 adverbials to be considered, many of which are minor variations of one another (in contrast, by
contrast, by way of contrast, in comparison, by comparison, by way of comparison that are unlikely to differ in
their anaphoric properties, and some of which, such as contrariwise, hitherto, and to cap it all, will occur
only rarely in a corpus of modern English.
563
Webber et al Anaphora and Discourse Structure
relevant alternatives to that argument. In the case of the premodifier
such, it yields a set of entities that are similar to its argument in a
contextually relevant way.
? Discourse adverbials are lexically specified anaphors whose meaning
function f? is a ?-expression involving a binary relation R? that is
idiosyncratic to ?, one of whose arguments is resolved anaphorically and
the other is provided compositionally, when the ?-expression is applied
to ??s matrix clause interpreted as an eventuality ?.
In the next section, we move on to consider how the presence of both a semantic rela-
tion associated with a discourse adverbial and a semantic relation associated with the
adjacency of two clauses or a structural connective between them allows for interesting
interactions between the two.
4. Patterns of Anaphoric Relations and Structural/Inferred Relations
Prior to the current work, researchers have treated both explicit structural connec-
tives (coordinating and subordinating conjunctions, and ?paired? conjunctions) and
discourse adverbials simply as evidence for a particular structural relation holding
between adjacent units. For example, Kehler (2002) takes but as evidence of a contrast
relation between adjacent units, in general as evidence of a generalization relation,
in other words as evidence of an elaboration relation, therefore as evidence of a result
relation, because as evidence of an explanation relation, and even though as evidence
of a denial of preventer relation (Kehler 2002, Section 2.1). Here Kehler has probably
correctly identified the type of relation that holds between elements, but not which
elements it holds between.
In one respect, we follow previous researchers, in that we accept that when clauses,
sentences, or larger discourse units are placed adjacent to one another, listeners infer a
relation between the two, and that the structural connective (coordinate or subordinate
conjunction) gives evidence for the relation that is intended to hold between them.
Because we take discourse adverbials to contribute meaning through an anaphoric
connection with the previous discourse, however, this means that there may be two
relations on offer and opens up the possibility that the relation contributed by the
discourse adverbial can interact in more than one way with the relation conveyed
by a structural connective or inferred through adjacency. Below we show that this
prediction is correct.
We start from the idea that, in the absence of an explicit structural connective, de-
feasible inference correlates with structural attachment of adjacent discourse segments
in discourse structure, relating their interpretations. The most basic relation is that the
following segment in some way describes the same object or eventuality as the one it
abuts (elaboration). But evidence in the segments can lead (via defeasible inference) to
a more specific relation, such as one of the resemblance relations (e.g., parallel, contrast,
exemplification, generalisation), or cause-effect relations (result, explanation, violated expecta-
tion), or contiguity relations (narration) described in Hobbs (1990) and Kehler (2002). If
nothing more specific can be inferred, the relation will remain simply elaboration. What
explicit structural connectives can do is convey relations that are not easy to convey
by defeasible inference (e.g., if, conveying condition, and or, conveying disjunction) or
provide nondefeasible evidence for an inferrable relation (e.g., yet, so, and because).
Discourse adverbials can interact with structural connectives, with adjacency-
triggered defeasible inference, and with each other. To describe the ways in which we
564
Computational Linguistics Volume 29, Number 4
have so far observed discourse adverbials to interact with relations conveyed struc-
turally, we extend the notation used in the previous section:
? ? = discourse adverbial
? R? = the name of the relation associated with ?
? S = the matrix clause/sentence of ?
? ? = the logical form (LF) interpretation of S
adding the following:
? D = the discourse unit that is left-adjacent to S, to which a relationship
holds by either inference or a structural connective
? ? = the LF interpretation of D
? R = the name of the relation that holds with ?
Although ? is one argument of R, we show below that the other argument of R may
be one of at least two different abstract objects.
Case 1: ? separately serves as an argument to both R? and R. This is the case that
holds in example (36) (repeated here):
(36) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because he then discovered he was broke.
We have already seen that the interpretation of the clause in (36d) following because
involves
R? = after
? = e4:discover(j,e5), where e5:broke(j)
[?x . after(x,EV)]e4 ? after(e4, EV)
where EV is resolved to e2:order(j, c1), and the proposition after(e4, e2) is added to the
discourse context?that is, John?s discovering he was broke is after his ordering the
wine.
Now consider the explanation relation R associated with because in (36d). It relates
e4, John?s finding he was broke, to the intepretation of (36c), e3:cancel(j,o1)?that is,
explanation(e4,e3). Clause 36d thus adds both explanation(e4,e3) and after(e4, e2) to the
discourse. Although these two propositions share an argument (e4), they are neverthe-
less distinct.12
12 Because eventuality e4, John?s finding he was broke, both explains the canceling and follows the ordering, it
follows that the canceling is after the ordering.
565
Webber et al Anaphora and Discourse Structure
Case 2: R?(?, ei) is an argument of R. In case 1, it is the interpretation of the ad-
verbial?s matrix clause ? that serves as one argument to the discourse relation R. In
contrast, in case 2, that argument is filled by the relation contributed by the discourse
adverbial (itself an abstract object available for subsequent reference). In both cases,
the other argument to R is ?.
One configuration in which case 2 holds is with the discourse adverbial otherwise.
Recall from section 3.4 that the interpretation of otherwise involves a conditional relation
between the complement of its anaphoric argument and the interpretation ? of its
matrix clause:
[?x . if(VE,x)] ? ? if(VE,?), where complement(VE,EV)
With variable EV resolved to an eventuality in the discourse context, it is the resulting
relation (viewed as an abstract object) that serves as one argument to R, with ? serving
as the other. We can see this most clearly by considering variants of examples (38) and
(39) that contain an explicit connective between the clauses. In (38), the conjunction
because is made explicit (example (40)), and in (39), the connective is simply and or but
(example (41)).
(40) If the light is red, stop, because otherwise you?ll get a ticket.
R? = if
?38 = e3:get ticket(you)
(41) If the light is red, stop, and/but otherwise go straight on.
R? = if
?39 = e3? :go straight(you)
In the case of (40), resolving otherwise contributes the relation
e6: if(e4,e3), where complement(e4,e2) and e2:stop(you)
(If you do something other than stop, you?ll get a ticket.)
At the level of LF, the abstract object e6 that is associated with the conditional relation
serves as one argument to the explanation relation contributed by because, with e2 being
the other. That is, because and otherwise together end up contributing explanation(e2,e6)
(i.e., your needing to stop is explained by the fact that if you do something other than
stop, you?ll get a ticket).
In the case of (41), resolving otherwise contributes the relation
e6? :if(e4? , e3? ), where complement(e4? ,e1) and e1:red(light)
(If the light is not red, go straight on.)
What is the discourse relation to which otherwise contributes this abstract object e6??
Whether the connective is and or but, both its conjuncts describe (elaborate) alternative
specializations of the same situation e0 introduced earlier in the discourse (e.g., e0 could
be associated with the first sentence of Go another mile and you?ll get to a bridge. If the
light is red, stop. Otherwise go straight on.) If the connective is and, what is added to
context might simply be elaboration(e6? ,e0). (Note that without otherwise, the relation
elaboration(e5,e0) would have been added to context, where e5 is the abstract object
associated with the interpretation of If the light is red, stop.) If the connective is but, then
one might also possibly add contrast(e6? ,e5)?that is, the situation that (if the light is
566
Computational Linguistics Volume 29, Number 4
red) you should stop is in contrast to the situation that if the light is not red, you
should go straight on.13
As is clear from the original pair of examples (38) and (39), interpretations can
arise through adjacency-triggered inference that are similar to those that arise with an
explicit connective. In either case, the above treatment demonstrates that there is no
need for a separate otherwise relation, as proposed in rhetorical structure theory (Mann
and Thompson 1988). We are not, however, entirely clear at this point when case 1
holds and when case 2 does. A more careful analysis is clearly required.
Case 3: R? is parasitic on R. Case 3 appears to apply with discourse adverbials such
as for example and for instance. The interpretation of such adverbials appears to be
parasitic on the relation associated with a structural connective or discourse adverbial
to their left, or on an inferred relation triggered by adjacency. The way to understand
this is to first consider intraclausal for example, where it follows the verb, as in
(42) Q. What does this box contain?
A. It contains, for example, some hematite.
The interpretation of for example here involves abstracting the meaning of its matrix
structure with respect to the material to its right, then making an assertion with respect
to this abstraction. That is, if the LF contributed by the matrix clause of (42A) is,
roughly,
i. contain(box1,hematite1)
then the LF resulting from the addition of for example can be written either with set
notation (as in (ii)), taking an entity to exemplify a set, or with ?-notation (as in (iii)),
taking an entity to exemplify a property:
ii. exemplify(hematite1, {X | contain(box1,X)})
iii. exemplify(hematite1, ?X . contain(box1,X))
Both express the fact that hematite is an example of what is contained in the box.14 Since
one can derive (i) logically from either (ii) or (iii), one might choose to retain only (ii) or
(iii) and derive (i) if and when it is needed. In the remainder of the article, we use the
? notation given in (iii). Note that from the perspective of compositional semantics, for
example resembles a quantifier, in that the scope of its interpretation is not isomorphic
to its syntactic position. Thus producing an interpretation for for example will require
techniques similar to those that have long been used in interpreting quantifiers (Woods,
1978; Barwise and Cooper 1981). We take this up again in section 5.
If we look at the comparable situation in discourse, such as (43)?(44), where for
example occurs to the right of a discourse connective, it can also be seen as abstracting
13 A much finer-grained treatment of the semantics of otherwise in terms of context-update potential is
given in Kruijff-Korbayova? and Webber (2001b). Here we are just concerned with its interaction with
structural connectives and adjacency-triggered relations.
14 The material to the right of for example can be any kind of constituent, including such strange ones as
John gave, for example, a flower to a nurse.
Here, a flower to a nurse would be an example of the set of object-recipient pairs within John?s givings.
Such nonstandard constituents are also found with coordination, which was one motivation for
combinatory categorial grammar (Steedman 1996). This just illustrates another case in which such
nonstandard constituents are needed.
567
Webber et al Anaphora and Discourse Structure
the interpretation of its discourse-level matrix structure, with respect to the material
to its right:
(43) John just broke his arm. So, for example, he can?t cycle to work now.
(44) You shouldn?t trust John because, for example, he never returns what he
borrows.
In (43), the connective so leads to
result(?,?)
being added to the discourse, where ? is the interpretation of John can?t cycle to work
now, and ? is the interpretation of John just broke his arm. For example then abstracts this
relation with respect to the material to its right (i.e., ?), thereby contributing
exemplify(?, ?X . result(X, ?))
That is, John can?t cycle to work is an example of what results from John?s breaking his
arm. Similarly, because in (44) leads to
explanation(?,?)
being added to the discourse, where ? is the interpretation of he never returns what he
borrows, ? is the interpretation of you shouldn?t trust John, and for example adds
exemplify(?, ?X . explanation(X,?))
that is, that ? is an example of the reasons for not trusting John.
For example interacts with discourse adverbials in the same way:
(45) Shall we go to the Lincoln Memorial? Then, for example, we can go to the
White House.
(46) As a money manager and a grass-roots environmentalist, I was very dis-
appointed to read in the premiere issue of Garbage that The Wall Street
Journal uses 220,000 metric tons of newsprint each year, but that only 1.4%
of it comes from recycled paper. By contrast, the Los Angeles Times, for ex-
ample, uses 83% recycled paper. [WSJ, from Penn Treebank /02/wsj-0269]
In example (45), the resolved discourse adverbial then leads to after(?,?) being added
to the discourse context, where ? is the interpretation of we can go to the White House, ?
is the interpretation of we shall go to the Lincoln Memorial, and for example adds
exemplify(?, ?X . after(X,?))
that is, that ? is an example of the events that [can] follow going to the Lincoln
Memorial. (As already noted, we are being fairly fast and loose regarding tense and
modality, in the interests of focusing on the types of interactions.)
568
Computational Linguistics Volume 29, Number 4
In example (46), the resolved discourse anaphor by contrast contributes contrast(?,?),
where ? is the interpretation of the Los Angeles Times?s using 83% recycled paper and ? is
the intepretation of only 1.4% of it [newsprint used by the WSJ] comes from recycled paper.
For example then contributes
exemplify(?, ?X . contrast(X,?))
that is, that ? is one example of contrasts with the WSJ?s minimal use of recycled
paper.
What occurs with discourse connectives and adverbials can also occur with rela-
tions added through adjacency-triggered defeasible inference, as in
(47) You shouldn?t trust John. For example, he never returns what he borrows.
explanation(?,?)
exemplify(?, ?X . explanation(?,X))
Here, as in (44), the relation provided by adjacency-triggered inference is R = explana-
tion, which is then used by for example.
But what about the many cases in which only exemplify seems present, as in
(48) In some respects they [hypertext books] are clearly superior to normal
books, for example they have database cross-referencing facilities ordinary
volumes lack. (British National Corpus, CBX 1087)
(49) He [James Bellows] and his successor, Mary Anne Dolan, restored respect
for the editorial product, and though in recent years the paper had been
limping along on limited resources, its accomplishments were notable. For
example, the Herald consistently beat its much-larger rival on disclosures
about Los Angeles Mayor Tom Bradley?s financial dealings.
There are at least two explanations: One is that for example simply provides direct
nondefeasible evidence for exemplify, which is the only relation that holds. The other
explanation follows the same pattern as the examples given above, but with no further
relation than elaboration(?,?). That is, we understand in (48) that having database cross-
referencing facilities elaborates the respects in which hypertext books are superior to
normal books, whereas in (49), we understand that the Herald?s [newspaper] consistently
beating its much-larger rival elaborates the claim that its accomplishments were notable. This
elaboration relation is then abstracted (in response to for example) to produce:
exemplify(?, ?X . elaboration(X, ?))
that is, that this is one example of many possible elaborations. Because this is more
specific than elaboration and seems to mean the same as exemplify(?,?), one might
simply take it to be the only relation that holds. Given that so many naturally occuring
instances of for example occur with elaboration, it is probably useful to persist with the
above shorthand. But it shouldn?t obscure the regular pattern that appears to hold.
Before going on to case 4, we should comment on an ambiguity associated with for
example. When for example occurs after an NP, a PP, or a clause that can be interpreted
as a general concept or a set, it can contribute a relation between the general concept
or set and an instance, rather than being parasitic on another relation. For example,
in:
569
Webber et al Anaphora and Discourse Structure
(50) In the case of the managed funds they will be denominated in a leading
currency, for example US dollar, . . . (BNC CBX 1590)
for example relates the general concept denoted by a leading currency to a specific in-
stance, U.S. dollars. (In British English, the BNC shows that most such examples occur
with such as?i.e., in the construction such as for example. This paraphrase does not work
with the predicate-abstracting for example that is of primary concern here, as in exam-
ple (42).)
But for example occurring after an NP, a PP, or a clause can, alternatively, contribute
a more subtle parasitic relationship to the previous clause, as in
(51) All the children are ill, so Andrew, for example, can?t help out in the shop.
This differs from both (43) and (50). That is, one cannot paraphrase (51) as (52) as in
(43) where for example follows so:
(52) All the children are ill, so for example Andrew can?t help out in the shop.
Example (52) simply specifies an example consequence of all the children being ill, as
does
(53) All the children are ill, so for example one of us has to be at home at all
times.
In contrast, (51) specifies an example consequence for Andrew, as one of the children.
Support for this comes from the fact that in (52), Andrew doesn?t have to be one of
the children: He could be their nanny or child minder, now stuck with dealing with a
lot of sick kids. But (51) is not felicitous if Andrew is not one of the children.
We suspect here the involvement of information structure (Steedman 2000a):
Whereas the interpretation conveyed by for example is parasitic on the adjacency rela-
tion (result in example (51)), its position after the NP Andrew in (51) may indicate a
contrastive theme with respect to the previous clause, according to which Andrew in
contrast to the other children suffers this particular consequence. But more work needs
to be done on this to gain a full understanding of what is going on.
Case 4: R? is a defeasible rule that incorporates R. Case 4 occurs with discourse
adverbials that carry the same presupposition as the discourse connectives although
and the concessive sense of while (Lagerwerf 1998). Case 4 shares one feature with
case 1, in that the discourse relation R conveyed by a structural connective or inferred
from adjacency holds between ? (the interpretation of the adverbial?s matrix clause)
and ? (the interpretation of the left-adjacent discourse unit). Where it differs is that
the result is then incorporated into the presupposition of the discourse adverbial. This
presupposition, according to Lagerwerf (1998), has the nature of a presupposed (or
conventionally implicated) defeasible rule that fails to hold in the current situation.
He gives as an example
(54) Although Greta Garbo was called the yardstick of beauty, she never mar-
ried.
This asserts both that Greta Garbo was called the yardstick of beauty and that she
never married. The first implies that Greta Garbo was beautiful. The example also
570
Computational Linguistics Volume 29, Number 4
presupposes that, in general, if a woman is beautiful, she will marry. If such a pre-
supposition can be accommodated, it will simply be added to the discourse context.
If not, the hearer will find the utterance confusing or possibly even insulting.
We argue here that the same thing happens with the discourse adverbials never-
theless and though. The difference is that, with discourse adverbials, the antecedent to
the rule derives anaphorically from the previous discourse, whereas the consequent
derives from the adverbial?s matrix clause. (With the conjunctions although and con-
cessive while, both arguments are provided structurally.)
We first illustrate case 4 with two examples in which nevertheless occurs in the
main clause of a sentence containing a preposed subordinate clause. The subordinate
conjunction helps clarify the relation between the clauses that forms the basis for the
presupposed defeasible rule. After these, we give a further example in which the
relation between the adjacent clauses comes through inference.
(55) While John is discussing politics, he is nevertheless thinking about his fish.
In (55), the conjunction while conveys a temporal relation R between the two clauses
it connects:
during(e2, e1), where e1:discuss(john,politics) and e2:think about(john,fish)
What nevertheless contributes to (55) is a defeasible rule based on this relation, which
we will write informally as
during(X,E) ? E:discuss(Y,politics)) > ?X:think about(Y,fish))
Normally, whatever one does during the time one is discussing politics, it is
not thinking about one?s fish.
This rule uses Asher and Morreau?s (1991) defeasible implication operator (>) and
abstracts over the individual (John), which seems appropriate for the general statement
conveyed by the present tense of the utterance.
Similarly, in
(56) Even after John has had three glasses of wine, he is nevertheless able to
solve difficult math problems.
the conjunction after contributes a relation between the two clauses it connects:
after(e2, e1), where e1:drink(john,wine) and e2:solve(john,hard problems)
What nevertheless contributes to this example is a defeasible rule that we will again
write informally as
after(X,E) ? E:drink(Y,wine)) > ?X:solve(Y,hard problems))
Normally, whatever one is able to do after one has had three glasses of wine, it
is not solving difficult algebra problems.
571
Webber et al Anaphora and Discourse Structure
Again, we have abstracted over the individual, as the presupposed defeasible rule
associated with the present-tense sentence appears to be more general than a statement
about a particular individual.15
On the other hand, in the following example illustrating a presupposed defeasi-
ble rule and a discourse relation associated with adjacency, it seems possible for the
presupposed defeasible rule to be about John himself:
(57) John is discussing politics. Nevertheless, he is thinking about his fish.
Here the discourse relation between the two clauses, each of which denotes a specific
event, is
during(e2, e1), where e1:discuss(john,politics) and e2:think about(john,fish)
(Note that our LF representation isn?t sufficiently rich to express the difference between
(55) and (57).) What nevertheless contributes here is the presupposed defeasible rule
during(X,e1) > ?X = e2
Normally what occurs during John?s discussing politics is not John?s thinking
about his fish.
Lagerwerf (1998) does not discuss how specific or general will be the presup-
posed defeasible rule that is accommodated or what factors affect the choice. Kruijff-
Korbayova? and Webber (2001a) also punt on the question, when considering the effect
of information structure on what presupposed defeasible rule is associated with al-
though. Again, this seems to be a topic for future work.
Summary
We have indicated four ways in which we have found the relation associated with a
discourse adverbial to interact with a relation R triggered by adjacency or conveyed
by structural connectives or, in some cases, by another relational anaphor:
1. ? separately serves as an argument to both R? and R.
2. R?(?, ei) is an argument of R.
3. R? is parasitic on R.
4. R? is a defeasible rule that incorporates R.
We do not know whether this list is exhaustive or whether a discourse adverbial
always behaves the same way vis-a`-vis other relations. Moreover, in the process of
setting down the four cases we discuss, we have identified several problems that we
have not addressed, on which further work is needed. Still, we hope that we have
convinced the reader of our main thesis: that by recognizing discourse adverbials
as doing something different from simply signaling the discourse relation between
adjacent discourse units and by considering their contribution as relations in their own
right, one can begin to characterize different ways in which anaphoric and structural
relations may themselves interact.
15 We speculate that the reason examples such as (55) and (56) sound more natural with the focus particle
even applied to the subordinate clause is that even conveys an even greater likelihood that the
defeasible rule holds, so nevertheless emphasizes its failure to do so.
572
Computational Linguistics Volume 29, Number 4
5. Lexicalized Grammar for Discourse Syntax and Semantics
The question we consider in this section is how the treatment we have presented of
discourse adverbials and structural connectives can be incorporated into a general
approach to discourse interpretation. There are three possible ways.
The first possibility is simply to incorporate our treatment of adverbials and con-
nectives into a sentence-level grammar, since such grammars already cover the syntax
of sentence-level conjunction (both coordinate and subordinate) and the syntax of
adverbials of all types. The problem with this approach is that sentence-level gram-
mars, whether phrasal or lexicalized, stop at explicit sentence-level conjunction and do
not provide any mechanism for forming the meaning of multiclausal units that cross
sentence-level punctuation. Moreover, as we have already shown in section 3, the
interpretation of discourse adverbials can interact with the implicit relation between
adjacent sentences, as well as with an explicitly signaled relation, so that a syntax and
compositional semantics that stops at the sentence will not provide all the structures
and associated semantics needed to build the structures and interpretations of interest.
The second possibility is to have a completely different approach to discourse-
level syntax and semantics than to sentence-level syntax and semantics, combining
(for example) a definite clause grammar with rhetorical structure theory. But as we
and others have already noted, this requires discourse semantics reaching further and
further into sentence-level syntax and semantics to handle relations between main and
embedded clauses, and between embedded clauses themselves, as in example (58).
(58) If they?re drunk and they?re meant to be on parade and you go to their
room and they?re lying in a pool of piss, then you lock them up for a day.
(The Independent, June 17, 1997)
Thus it becomes harder and harder to distinguish the scope of discourse-level syntax
and semantics from that at the sentence-level.
The third possibility is to recognize the overlapping scope and similar mechanisms
and simply extend a sentence-level grammar and its associated semantic mechanisms
to discourse. The additional responsibilities of the grammer would be to account for
the formation of larger units of discourse from smaller units; the projection of the
interpretation of smaller discourse units onto the interpretation of the larger discourse
units they participate in; and the effect of discourse unit interpretation on the evolv-
ing discourse model. There are two styles of grammar one could use for this: (1) a
phrase structure grammar (PSG) extended to discourse, as in Figure 6, or (2) a lexi-
calized grammar that extends to discourse, a sentence-level lexicalized grammar such
as tree-adjoining grammar (Joshi, 1987; XTAG-Group 2001) or combinatory categorial
grammar (CCG) (Steedman 1996, 2000b).
Whereas Polanyi and van den Berg (1996) extend a PSG to discourse, we argue
for extending a lexicalized grammar, even though TAG and CCG are weakly context-
sensitive (CS) and the power needed for a discourse grammar with no crossing de-
pendencies is only context-free (section 2.1). Our argument is based on our desire to
use a discourse grammar in natural language generation (NLG). It is well-known that
context-free PSGs (CF PSGs) set up a complex search space for NLG. A discourse
grammar specified in terms of phrase structure rules such as those shown in Figure 6
doesn?t provide sufficient guidance when reversed for use in generating discourse.
For example, one might end up having to guess randomly how many sentences and
connectives one had, in what order, before being able to fill in the sentences and con-
nectives with any content. More generally, trying to generate exactly a given semantics
573
Webber et al Anaphora and Discourse Structure
Seg := SPunct Seg | Seg SPunct | SPunct |
on the one hand Seg on the other hand Seg |
not only Seg but also Seg
SPunct := S Punctuation
Punctuation := . | ; | : | ? | !
S := S Coord S | S Subord S | Subord S S | Sadv S |
NP Sadv VP | S Sadv | . . .
Coord := and | or | but | so
Subord := although | after | because | before | ...
Sadv := DAdv | SimpleAdv
DAdv := instead | otherwise | for example | meanwhile | ...
SimpleAdv := yesterday | today | surprisingly | hopefully | ...
Figure 6
PS rules for a discourse grammar.
when semantics underspecifies syntactic dependency (as discourse semantics must, on
our account) is known to be intractable (Koller and Striegnitz 2002). An effective so-
lution is to generate semantics and syntax simultaneously, which is straightforward
with a lexicalized grammar (Stone et al 2001).
Given the importance of various types of inference in discourse understanding,
there is a second argument for using a lexicalized discourse grammar that derives from
the role of implicature in discourse. Gricean reasoning about implicatures requires a
hearer be able to infer the meaningful alternatives that a speaker had in composing a
sentence. With lexicalization, these alternatives can be given by a grammar, allowing
the hearer, for example, to ask sensible questions like ?Why did the speaker say ?in-
stead? here instead of nothing at all?? and draw implicatures from this. A CF PSG, on
the other hand, might suggest questions like ?Why did the speaker say two sentences
rather than one here?? which seem empirically not to lead to any real implicatures.
(On the contrast between choices, which seem to lead to implicatures, and mere alter-
native linguistic formulations, which do not seem to, see, for example, Dale and Reiter
[1995] and Levison [2000].)
In several previous papers (Webber, Knott, and Joshi, 2001; Webber et al, 1999a,
1999b), we described how our approach fits into the framework of tree-adjoining gram-
mar. This led to the initial version of a discourse parser (Forbes et al 2001) in which
the same parser that builds trees for individual clauses using clause-level LTAG trees
then combines them using discourse-level LTAG trees. Here we simply outline the
grammar, called DLTAG (section 5.1), then show how it supports the approach to
structural and anaphoric discourse connectives presented earlier (section 5.2).
(Of course, one still needs to account for how speakers realize their intentions
through text and how what is achieved through a single unit of text contributes to
what a speaker hopes to achieve through any larger unit in which it is embedded.
Preliminary accounts are given in Grosz and Sidner [1990] and Moser and Moore
[1996]. Given the complex relation between individual sentences and speaker inten-
tions, however, it is unlikely that the relation between multisentence discourse and
speaker intentions can be modeled in a straightforward way similar to the basically
monotonic compositional process that we have discussed in this article for discourse
semantics.)
574
Computational Linguistics Volume 29, Number 4
Dc
DcDc
subconj
(a)
Dc
Dc Dc
subconj
(b)
?:subconj_mid ?: subconj_pre
Figure 7
Initial trees for a subordinate conjunction: (a) postposed; (b) preposed. Dc stands for discourse
clause, ? indicates a substitution site, and subconj stands for the particular subordinate
conjunction that anchors the tree.
5.1 DLTAG and Discourse Syntax
A lexicalized TAG begins with the notion of a lexical anchor, which can have one
or more associated tree structures. For example, the verb likes anchors one tree corre-
sponding to John likes apples, another corresponding to the topicalized Apples John likes,
a third corresponding to the passive Apples are liked by John, and others as well. That
is, there is a tree for each minimal syntactic construction in which likes can appear, all
sharing the same predicate-argument structure. This syntactic/semantic encapsulation
is possible because of the extended domain of locality of LTAG.
A lexicalized TAG contains two kinds of elementary trees: initial trees that reflect
basic functor-argument dependencies and auxiliary trees that introduce recursion and
allow elementary trees to be modified and/or elaborated. Unlike the wide variety of
trees needed at the clause level, we have found that extending a lexicalized TAG to
discourse requires only a few elementary tree structures, possibly because clause-level
syntax exploits structural variation in ways that discourse doesn?t.
5.1.1 Initial Trees. DLTAG has initial trees associated with subordinate conjunctions,
with parallel constructions, and with some coordinate conjuctions. We describe each
in turn.
In the large LTAG developed by the XTAG project (XTAG-Group 2001) subordi-
nate clauses are seen as adjuncts to sentences or verb phrases (i.e., as auxiliary trees)
because they are outside the domain of locality of the verb. In DLTAG, however, it
is predicates on clausal arguments (such as coordinate and subordinate conjunctions)
that define the domain of locality. Thus, at this level, these predicates anchor initial
trees into which clauses substitute as arguments. Figure 7 shows the initial trees for (a)
postposed subordinate clauses and (b) preposed subordinate clauses.16 At both leaves
and root is a discourse clause (Dc): a clause or a structure composed of discourse
clauses.
One reason for taking something to be an initial tree is that its local dependencies
can be stretched long distance. At the sentence level, the dependency between apples
and likes in Apples John likes is localized in all the trees for likes. This dependency can
be stretched long distance, as in Apples, Bill thinks John may like. In discourse, as we
noted in section 2, local dependencies can be stretched long distance as well, as in
(59) a. Although John is generous, he?s hard to find.
16 Although in an earlier paper (Webber and Joshi 1998), we discuss reasons for taking the lexical anchors
of the initial trees in Figures 7 and 8 to be feature structures, following the analysis in Knott (1996) and
Knott and Mellish (1996), here we just take them to be specific lexical items.
575
Webber et al Anaphora and Discourse Structure
Dc
On the
one hand
On the
other
Dc Dc
?:contrast
Figure 8
An initial tree for parallel constructions. This particular tree is for a contrastive construction
anchored by on the one hand and on the other hand.
b. Although John is generous?for example, he gives money to anyone
who asks him for it?he?s hard to find.
(60) a. On the one hand, John is generous. On the other hand, he?s hard to
find.
b. On the one hand, John is generous. For example, suppose you needed
some money: You?d only have to ask him for it. On the other hand,
he?s hard to find.
Thus DLTAG also contains initial trees for parallel constructions as in (60). Such an
initial tree is shown in Figure 8. Like some initial trees in XTAG (XTAG-Group 2001),
such trees can have a pair of anchors. Since there are different ways in which dis-
course units can be parallel, we assume a different initial tree for contrast (on the one
hand. . . on the other (hand). . . ), disjunction (either. . . or. . . ), addition (not only. . . but also. . . ),
and concession (admittedly. . . but. . . ).
Finally, there are initial trees for structural connectives between adjacent sentences
or clauses that convey a particular relation between the connected units. One clear
example is so, conveying result. Its initial tree is shown in Figure 9. We will have a
better sense of what other connectives to treat as structural as a result of annotation
efforts of the sort described in Creswell et al (2002).17
5.1.2 Auxiliary Trees. DLTAG uses auxiliary trees in two ways: (1) for discourse units
that continue a description in some way, and (2) for discourse adverbials. Again we
describe each in turn.
17 For example, one might also have initial trees for marked uses of and and or that have a specific
meaning beyond simple conjunction or disjunction, as in
(61) a. Throw another spit ball and you?ll regret it.
b. Eat your spinach or you won?t get dessert.
These differ from the more frequent, simple coordinate uses of and and or in that the second conjunct
in these marked cases bears a discourse relation to the first conjunct (result in both (61a) and (61b)).
With simple coordinate uses of and and or, all conjuncts (disjuncts) bear the same relation to the same
immediately left-adjacent discourse unit. For example, in (62), each conjunct is a separate explanation
for not trusting John, wheras in (63), each disjunct conveys an alternative result of John?s good fortune:
(62) You shouldn?t trust John. He never returns what he borrows, and he bad-mouths his associates
behind their backs.
(63) John just won the lottery. So he will quit his job, or he will at least stop working overtime.
For simple coordinate uses of and and or, we have auxiliary trees (section 5.1.2).
576
Computational Linguistics Volume 29, Number 4
Dc
DcDc
?:so
so
Figure 9
Initial tree for coordinate conjunction so.
Dc
Dc Dc
?
.
Dc
Dc Dc
? and ?
S
S
then
(a) (b) (c)
?: punct1 ?: and ?: then
Figure 10
Auxiliary trees for basic elaboration. These particular trees are anchored by (a) the
punctuation mark ?period? and (b) and. The symbol ? indicates the foot node of the auxiliary
tree, which has the same label as its root. (c) Auxiliary tree for the discourse adverbial then.
?: punct1
?: punct1
3
?2
*
.
T1
T2
T1 T2
.
?1
0
Figure 11
TAG derivation of example (64).
First, auxiliary trees anchored by punctuation (e.g., period, comma, semicolon.) (Fig-
ure 10a) or by simple coordination (Figure 10b) are used to provide further description
of a situation or of one or more entities (objects, events, situations, states, etc.) within
the situation.18 The additional information is conveyed by the discourse clause that fills
its substitution site. Such auxiliary trees are used in the derivation of simple discourses
such as
(64) a. John went to the zoo.
b. He took his cell phone with him.
Figure 11 shows the DLTAG derivation of example (64), starting from LTAG deriva-
tions of the individual sentences.19 To the left of the horizontal arrow are the elemen-
tary trees to be combined: T1 stands for the LTAG tree for clause (64a), T2 for clause
18 The latter use of an auxiliary tree is related to dominant topic chaining in Scha and Polanyi (1988) and
entity chains in Knott et al (2001).
19 We comment on left-to-right incremental construction of DLTAG structures in parallel with
sentence-level LTAG structures at the end of Section 5.2.
577
Webber et al Anaphora and Discourse Structure
(64b), and ?:punct1 for the auxiliary tree assocated with the period after (64a). In the
derivation, the foot node of ?:punct1 is adjoined to the root of T1 and its substitution
site filled by T2, resulting in the tree to the right of the horizontal arrow. (A standard
way of indicating TAG derivations is shown under the horizontal arrow, where bro-
ken lines indicate adjunction and solid lines, substitution. Each line is labeled with the
address of the argument at which the operation occurs. ?1 is the derivation tree for
T1 and ?2, the derivation tree for T2.)
The other auxiliary trees used in the lexicalized discourse grammar are those for
discourse adverbials, which are simply auxiliary trees in a sentence-level LTAG (XTAG-
Group 2001), but with an interpretation that projects up to the discourse level. An
example is shown in Figure 10c. Adjoining such an adverbial to a clausal/sentential
structure contributes to how information conveyed by that structure relates to the
previous discourse.
There is some lexical ambiguity in this grammar, but no more than serious con-
sideration of adverbials and conjunctions demands. First, as already noted, discourse
adverbials have other uses that may not be anaphoric (65a?b) and may not be clausal
(65a?c):
(65) a. John ate an apple instead of a pear.
b. In contrast with Sue, Fred was tired.
c. Mary was otherwise occupied.
Second, many of the adverbials found in second position in parallel constructions
(e.g., on the other hand, at the same time, nevertheless) can also serve as simple adverbial
discourse connectives on their own. In the first case, they will be one of the two anchors
of an initial tree (Figure 8), and in the second, they will anchor a simple auxiliary tree
(Figure 10c). These lexical ambiguities correlate with structural ambiguity.
5.2 Example Derivations
It should be clear by now that our approach aims to explain discourse semantics in
terms of a product of the same three interpretive mechanisms that operate within
clause-level semantics:
? compositional rules on syntactic structure (here, discourse structure)
? anaphor resolution
? inference triggered by adjacency and structural connection
For the compositional part of semantics in DLTAG (in particular, computing interpre-
tations on derivation trees), we follow Joshi and Vijay-Shanker (2001). Roughly, they
compute interpretations on the derivation tree using a bottom-up procedure. At each
level, function application is used to assemble the interpretation of the tree from the
interpretation of its root node and its subtrees. Where multiple subtrees have function
types, the interpretation procedure is potentially nondeterministic: The resulting am-
biguities in interpretation may be admitted as genuine, or they may be eliminated by a
lexical specification. Multicomponent TAG tree sets are used to provide an appropriate
compositional treatment for quantifiers, which we borrow for interpreting for example
(examples (66c?d)).
In showing how DLTAG and an interpretative process on its derivations operate,
we must, of necessity, gloss over how inference triggered by adjacency or associated
with a structural connective provides the intended relation between adjacent discourse
578
Computational Linguistics Volume 29, Number 4
units: It may be a matter simply of statistical inference, as in Marcu and Echihabi
(2002), or of more complex inference, as in Hobbs et al (1993). As we noted, our view
is that there are three mechanisms at work in discourse semantics, just as there are in
clause-level semantics: Inference isn?t the only process involved. Thus the focus of our
presentation here is on how compositional rules and anaphor resolution (which itself
often appears to require inference) operate together with inference to yield discourse
semantics.
We start with previous examples (44) (here (66c)) and (47) (here (66d)) and two
somewhat simpler variants (66a?b):
(66) a. You shouldn?t trust John because he never returns what he borrows.
b. You shouldn?t trust John. He never returns what he borrows.
c. You shouldn?t trust John because, for example, he never returns what
he borrows.
d. You shouldn?t trust John. For example, he never returns what he bor-
rows.
This allows us to show how (66a?b) and (66c?d) receive similar interpretations, despite
having somewhat different derivations, and how the discourse adverbial for example
contributes both syntactically and semantically to those interpretations.
We let T1 stand for the LTAG parse tree for you shouldn?t trust John, ?1, for its
derivation tree, and interp(T1), for the eventuality associated with its interpretation.
Similarly, we let T2 stand for the LTAG parse tree for he never returns what he bor-
rows, ?2, for its derivation tree, and interp(T2), for the eventuality associated with its
interpretation.
Example (66a) involves an initial tree (?:because-mid) anchored by because (Fig-
ure 12). Its derived tree comes from T1 substituting at the left-hand substitution
site of ?:because-mid (index 1) and T2 at its right-hand substitution site (index 3).
Compositional interpretation of the resulting derivation tree yields explanation(interp
(T2),interp(T1)). (A more precise interpretation would distinguish between the direct
and epistemic causality senses of because, but the derivation would proceed in the
same way.)
In contrast with (66a), example (66b) employs an auxiliary tree (?:punct1) anchored
by a period (Figure 13). Its derived tree comes from T2 substituting at the right-hand
substitution site (index 3) of ?:punct1, and ?:punct1 adjoining at the root of T1 (index 0).
Compositional interpretation of the derivation tree yields merely that T2 continues the
description of the situation associated with T1, that is, elaboration(interp(T2),interp(T1)).
Further inference triggered by adjacency and structural connection leads to a con-
?:because_mid
?:because_mid
31
T2
T1
?1 ?2because
because
T1 T2
Figure 12
Derivation of example (66a). The derivation tree is shown below the arrow, and the derived
tree, to its right. (Node labels Dc have been omitted for simplicity.)
579
Webber et al Anaphora and Discourse Structure
?: punct1
?: punct1
*
T2 . .
T1
T1 T2
3
?2
?1
0
Figure 13
Derivation of example (66b).
D
D  *
cD{ c
c
}
for-ex2
?: for-ex1
0
3
?:?1
0
because_mid
?2
?:
1
T2
T1
?: for-ex1
because_mid?:T1
T2 because
?: for-ex2
because
for example
?
for example
Figure 14
Derivation of example (66c).
clusion of causality between them, that is, explanation(interp(T2),interp(T1)), but this
conclusion is defeasible because it can be denied without a contradiction: for example,
(67) You shouldn?t trust John. He never returns what he borrows. But that?s
not why you shouldn?t trust him.
Example (66c) differs from (66a) in containing for example in its second clause. As
noted earlier, for example resembles a quantifier with respect to its semantics, as its
interpretation takes wider scope than would be explained by its syntactic position. We
handle this in the same way that quantifiers are handled in Joshi and Vijay-Shanker
(2001) by associating with for example a two-element TAG tree set (Figure 14). Both
trees in the tree set participate in the derivation: The auxiliary tree ?:for ex1 adjoins
at the root of T2, whereas the auxiliary tree ?:for ex2 adjoins at the root of the higher
discourse unit. Since we saw from example (66a) that the interpretation of this higher
discourse unit is explanation(interp(T2),interp(T1)), the interpretation associated with
the adjoined ?:for ex2 node both embeds and abstracts this interpretation, yielding
exemplification(interp(T2), ?X . explanation(X,interp(T1))
That is, John?s never returning what he borrows is one instance of a set of explanations.
Similarly, example (66d) differs from (66b) in containing for example in its second
sentence (Figure 15). As in example (66b), an inferred relation is triggered between
the interpretations of T2 and T1, namely, explanation(interp(T2),interp(T1)). Then, as
a result of ?:for ex1 adjoining at T2 and ?:for ex2 adjoining at the root of the higher
580
Computational Linguistics Volume 29, Number 4
D
D
* .
.
D* }{ for-ex1?: for-ex2
0
punct1
?23
0
?:
?1
?:
0
T1
T2
?: for-ex2
?:
punct1
for example
T2
T1
for example ?
for-ex1?:
Figure 15
Derivation of example (66d).
discourse unit, for example again contributes the interpretation
exemplification(interp(T2), ?X . explanation(X,interp(T1))
Thus (66c) and (66d) differ only in the derivation of the interpretation that for example
then abstracts over.
The next example we will walk through is example (11) (repeated here as exam-
ple (68)):
(68) John loves Barolo. So he ordered three cases of the ?97. But he had to
cancel the order because then he discovered he was broke.
As shown in Figure 16, this example involves two initial trees (?:so, ?:because mid) for
the structural connectives so and because; an auxiliary tree for the structural connective
but (?:but), since but functions as a simple conjunction to continue the description of
the situation under discussion; an auxiliary tree (?:then) for the discourse adverbial
then; and initial trees for the four individual clauses T1?T4. As can be seen from the
derivation tree, T1 and T2 substitute into ?:so as its first and third arguments, and ?:but
root-adjoins to the result. The substitution argument of ?:but is filled by ?:because mid,
with T3 and T4 substituted in as its first and third arguments, and ?:then is root-
adjoined to T4. The interpretation contributed by then, after its anaphoric argument is
resolved to interp(T2), is
?4: after(interp(T4), interp(T2))
The interpretations derived compositionally from the structural connectives so, because,
and but are
?1: result(interp(T2), interp(T1))
?2: explanation(interp(T4), interp(S3))
?3: elaboration(?2,?1)
Further inference may then refine elaboration to contrast, based on how but is being
used.
Finally, we want to point out one more way in which texts that seem to be close
paraphrases get their interpretations in different ways. Consider the two texts in ex-
ample (69):
581
Webber et al Anaphora and Discourse Structure
*
so
?:then
*then
because
?:because_mid
?3
T1 ?1
?4T3
T2
T4
?: but
?: so
?: but
?:because_mid
?:
?2
3
1 3
0
?3 ?4
then
?1 ?2
31 0
because
thenT3
T4
T2
so
T1
but
?:
but
so
Figure 16
Derivation of example (68).
(69) a. You should eliminate part 2 before part 3 because part 2 is more sus-
ceptible to damage.
b. You should eliminate part 2 before part 3. This is because part 2 is
more susceptible to damage.
Example (69b) is a simpler version of an example in Moser and Moore (1995), in which
This is because is treated as an unanalyzed cue phrase, no different from because in (69a).
We show here that this isn?t necessary: One can analyze (69b) using compositional
semantics and anaphor resolution and achieve the same results.
First consider (69a). Given the interpretations of its two component clauses, its
overall interpretation follows in the same way as (66a), shown in Figure 12. Now
consider (69b) and the derivation shown in Figure 17. Here the initial tree ?:because mid
T1
T2
TB
?:because_mid
because
?: punct1
*
.
.
because
T2 TB
?: punct1
?:because_mid
31
?2 ??
?1
0
3
T1
Figure 17
Derivation of example (69b).
582
Computational Linguistics Volume 29, Number 4
has its two arguments filled by T2, the TAG analysis of this is and TB, the TAG analysis
of part 2 is more susceptible to damage. The overall derived tree for (69b) comes from
?:punct1 root-adjoining to T1 (the TAG analysis of You should eliminate part 2 before
part 3), with the subsitution site of ?:punct1 filled by the ?:because mid derivation.
The compositional interpretation of the derivation tree yields the interpretation of the
?:because mid tree (i1) as an elaboration of the interpretation of T1:
i1: explanation(interp(TB),interp(T2))
i2: elaboration(i1,interp(T1))
But this is not all. The pronoun this in T2 is resolved anaphorically to the nearest con-
sistent eventuality (Eckert and Strube 2000; Byron 2002) which in this case is interp(T1).
Taking this as the interpretation of T2 and substituting, we get
i1: explanation(interp(TB),interp(T1))
i2: elaboration(i1,interp(T1))
Notice that i1 is also the interpretation of (69a). To this, i2 adds the somewhat redun-
dant information that i1 serves to elaborate the advice in T1. Thus (69a) and (69b)
receive similar interpretations but by different means. This treatment has the added
advantage that one does not have to treat This is not because as a separate cue phrase.
Rather, negation simply produces
i1: ?explanation(interp(TB),interp(T1))
i2: elaboration(i1,interp(T1))
That is, T1 is elaborated by a denial of a (possible) explanation. Presumably, the text
would go on to provide the actual explanation.
Finally, we want to comment on the Holy Grail of discourse parsing: a realistic
model that is computed in parallel with incremental sentence-level parsing. Neither
the analyses given in this section nor the discourse parsing described in Forbes et
al. (2001) is done in a left-to-right incremental fashion, in parallel with incremental
left-to-right sentence-level parsing.
What would an integrated incremental method of sentence-discourse processing
require? At minimum, we believe it would involve:
? A left-to-right parser that would simultaneously compute increments to
sentence-level syntactic structure, sentence-level semantics,
discourse-level syntactic structure, and discourse-level semantics.
Increments to the latter two would occur only at clause boundaries and
with discourse adverbials and structural connectives.
? An incremental anaphor resolution mechanism, similar to that in Strube
(1998), but extended both to deictic pronouns, as in Eckert and Strube
(2000) and Byron (2002) and to the anaphoric argument of discourse
adverbials.
? Incremental computation of discourse structure in terms of elaboration
relations and further nondefeasible reasoning to more specific relations,
where possible.
A left-to-right parser that simultaneously produces sentence-level syntactic and
semantic analyses already exists for combinatory categorial grammar (Steedman 1996,
583
Webber et al Anaphora and Discourse Structure
2000b; Hockenmaier, Bierner, and Baldridge, forthcoming), and it would seem straight-
forward to extend such a parser to computing discourse-level syntax and semantics
as well. Similarly, it seems straightforward to produce an incremental version of any
of the current generation of anaphor resolution mechanisms, extended to deictic pro-
nouns, although current approaches attempt to resolve this and that only with the
interpretation of a single clause, not with that of any larger discourse unit. As these
approaches are also not very accurate as yet, incremental anaphor resolution awaits
improvements to anaphor resolution in general. Moreover, as we better understand
the specific anaphoric properties of discourse adverbials through empirical analysis
such as Creswell et al (2000), such anaphor resolution mechanisms can be extended
to include them as well.
As for building discourse structure incrementally in parallel with syntactic struc-
ture, there is no working prototype yet that will do what is needed. But we have no
doubt that as psycholinguistics and computation together develop a better understand-
ing of incremental semantic processing, researchers? desire for a working prototype
will eventually result in the development of one.
6. Conclusion
In this article, we have argued that discourse adverbials make an anaphoric, rather than
a structural, connection with the previous discourse (section 2), and we have provided
a general view of anaphora in which it makes sense to talk of discourse adverbials as
being anaphoric (section 3). We have then shown that this view of discourse adverbials
allows us to characterize a range of ways in which the relation contributed by a
discourse adverbial can interact with the relation conveyed by a structural connective
or inferred through adjacency (section 4), and then illustrated how discourse syntax
and semantics can be treated as an extension of sentence-level syntax and semantics,
using a lexicalized discourse grammar (section 5).
We are clearly not the first to have proposed a grammatical treatment of low-level
aspects of discourse semantics (Asher and Lascarides 1999; Gardent 1997; Polanyi and
van den Berg 1996; Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996),
but we are the first to have recognized that the key to avoiding problems of maintain-
ing a compositional semantics for discourse lies in recognizing discourse adverbials as
anaphors and not trying to shoehorn everything into a single class of discourse connec-
tives. Although we are not yet able to propose a solution to the problem of correctly
resolving discourse adverbials or a way of achieving the Holy Grail of computing
discourse syntax and semantics in parallel with incremental sentence processing, the
proposed approach does simplify issues of discourse structure and discourse semantics
in ways that have not before been possible.
Acknowledgments
The authors would like to thank Kate
Forbes, Katja Markert, Natalia Modjeska,
Rashmi Prasad, Eleni Miltsakaki, Cassandra
Creswell, Mark Steedman, members of the
University of Edinburgh Dialogue Systems
Group, and participants at ESSLLI?01 for
helpful criticism as the ideas in the article
were being developed. We would also like
to thank our three anonymous reviewers.
We believe that in addressing their
criticisms and suggestions, both the article?s
arguments and its presentation have
become clearer. This work has been funded
in part by EPSRC grant GR/M75129
(Webber), NSF grant CISE CDA 9818322
(Stone), and NSF grants NSF-STC SBR
8920230 and NSF-EIA02-24417 (Joshi).
References
Asher, Nicholas. 1993. Reference to Abstract
Objects in Discourse. Kluwer, Boston.
Asher, Nicholas and Alex Lascarides. 1999.
The semantics and pragmatics of
584
Computational Linguistics Volume 29, Number 4
presupposition. Journal of Semantics,
15(3):239?300.
Asher, Nicholas and Alex Lascarides. 2003.
Logics of Conversation. Cambridge
University Press, Cambridge, England.
Asher, Nicholas and Michael Morreau. 1991.
Commonsense entailment. In Proceedings
of the Ninth International Joint Conference on
Artificial Intelligence IJCAI?91, pages
387?392, Sydney, Australia.
Barwise, Jon and Robin Cooper. 1981.
Generalized quantifiers and natural
language. Linguistics and Philosophy,
4:159?219.
Bateman, John. 1999. The dynamics of
?surfacing?: An initial exploration. In
Proceedings of International Workshop on
Levels of Representation in Discourse
(LORID?99), pages 127?133, Edinburgh.
Bierner, Gann. 2001a. Alternative phrases
and natural language information
retrieval. In Proceedings of the 39th Annual
Conference of the Association for
Computational Linguistics, Toulouse,
France, July.
Bierner, Gann. 2001b. Alternative Phrases:
Theoretical Analysis and Practical Application.
Ph.D. thesis, University of Edinburgh.
Bierner, Gann and Bonnie Webber. 2000.
Inference through alternative set
semantics. Journal of Language and
Computation, 1(2):259?274.
Byron, Donna. 2002. Resolving pronominal
reference to abstract entities. In
Proceedings of the 40th Annual Meeting,
Association for Computational Linguistics,
pages 80?87, University of Pennsylvania.
Clark, Herbert. 1975. Bridging. In
Proceedings of Theoretical Issues in Natural
Language Processing (TINLAP-1), pages
169?174, Cambridge, MA.
Clark, Herbert and Catherine Marshall.
1981. Definite reference and mutual
knowledge. In Aravind Joshi, Bonnie
Webber, and Ivan Sag, editors, Elements of
Discourse Understanding. Cambridge
University Press, Cambridge, England,
pages 10?63.
Cosse, Michel. 1996. Indefinite associative
anaphora in French. In Proceedings of the
IndiAna Workshop on Indirect Anaphora,
University of Lancaster, Lancaster,
England.
Creswell, Cassandre, Kate Forbes, Eleni
Miltsakaki, Rashmi Prasad, Aravind Joshi,
and Bonnie Webber. 2002. The discourse
anaphoric properties of connectives. In
Proceedings of the Discourse Anaphora and
Anaphor Resolution Colloquium, Lisbon,
Portugal.
Dale, Robert. 1992. Generating Referring
Expressions. MIT Press, Cambridge, MA.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
18:233?263.
Eckert, Miriam and Michael Strube. 2000.
Synchronising units and anaphora
resolution. Journal of Semantics, 17:51?89.
Forbes, Katherine, Eleni Miltsakaki, Rashmi
Prasad, Anoop Sarkar, Aravind Joshi, and
Bonnie Webber. 2001. D-LTAG system:
Discourse parsing with a lexicalized
tree-adjoining grammar. In ESSLLI?2001
Workshop on Information Structure, Discourse
Structure and Discourse Semantics, Helsinki,
Finland.
Forbes, Kate and Bonnie Webber. 2002. A
semantic account of adverbials as
discourse connectives. In Proceedings of
Third SIGDial Workshop, pages 27?36,
Philadelphia, PA.
Frank, Anette and Hans Kamp. 1997. On
context dependence in modal
constructions. In SALT-97, Stanford, CA.
Gardent, Claire. 1997. Discourse tree
adjoining grammars. Claus Report no. 89,
University of the Saarland, Saarbru?cken,
Germany.
Grosz, Barbara and Candace Sidner. 1990.
Plans for discourse. In Philip Cohen, Jerry
Morgan, and Martha Pollack, editors,
Intentions in Communication. MIT Press,
Cambridge, MA, pages 417?444.
Hahn, Udo, Katja Markert, and Michael
Strube. 1996. A conceptual reasoning
approach to textual ellipsis. In Proceedings
of the 12th European Conference on Artificial
Intelligence, pages 572?576, Budapest,
Hungary.
Hardt, Dan. 1992. VP ellipsis and contextual
interpretation. In Proceedings of
International Conference on Computational
Linguistics(COLING-92), pages 303?309,
Nantes.
Hardt, Dan. 1999. Dynamic interpretation of
verb phrase ellipsis. Linguistics and
Philosophy, 22:187?221.
Hellman, Christina and Kari Fraurud. 1996.
Proceedings of the IndiAna Workshop on
Indirect Anaphora. University of Lancaster,
Lancaster, England.
Hobbs, Jerry. 1985. Ontological promiscuity.
In Proceedings of the 23rd Annual Meeting of
the Association for Computational Linguistics,
pages 61?69, Palo Alto, CA. Morgan
Kaufmann.
Hobbs, Jerry. 1990. Literature and Cognition.
Volume 21 of CSLI Lecture Notes. Center
for the Study of Language and
Information, Stanford, CA.
585
Webber et al Anaphora and Discourse Structure
Hobbs, Jerry, Mark Stickel, Paul Martin, and
Douglas Edwards. 1993. Interpretation as
abduction. Artificial Intelligence,
63(1?2):69?142.
Hockenmaier, Julia, Gann Bierner, and Jason
Baldridge. Forthcoming. Providing
robustness for a CCG system. Journal of
Language and Computation.
Isard, Stephen. 1975. Changing the context.
In Edward Keenan, editor, Formal
Semantics of Natural Language. Cambridge
University Press, Cambridge, England,
pages 287?296.
Jayez, Jacques and Corinne Rossari. 1998a.
Pragmatic connectives as predicates. In
Patrick Saint-Dizier, editor, Predicative
Structures in Natural Language and Lexical
Knowledge Bases. Kluwer Academic,
Dordrecht, the Netherlands, pages
306?340.
Jayez, Jacques and Corinne Rossari. 1998b.
The semantics of pragmatic connectives
in TAG: The French donc example. In
Anne Abeille? and Owen Rambow, editors,
Proceedings of the TAG+4 Conference. CSLI
Publications, Stanford, CA.
Joshi, Aravind. 1987. An introduction to tree
adjoining grammar. In Alexis
Manaster-Ramer, editor, Mathematics of
Language. John Benjamins, Amsterdam,
pages 87?114.
Joshi, Aravind and K. Vijay-Shanker. 2001.
Compositional semantics with lexicalized
tree-adjoining grammar (LTAG): How
much underspecification is necessary? In
Harry Bunt, Reinhard Muskens, and Elias
Thijsse, editors, Computing Meaning,
Volume 2, Kluwer, Dordrecht, the
Netherlands, pages 147?163.
Jurafsky, Dan and James Martin. 2000.
Speech and Language Processing.
Prentice-Hall, Englewood Cliffs, NJ.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic. Kluwer, Dordrecht, the
Netherlands.
Kehler, Andrew. 2002. Coherence, Reference
and the Theory of Grammar. CSLI
Publications, Stanford, CA.
Kibble, Rodger. 1995. Modal
insubordination. In Empirical Issues in
Formal Syntax and Semantics, Selected Papers
from the Colloque de Syntaxe et de Se?mantique
de Paris, pages 317?332.
Knott, Alistair. 1996. A Data-Driven
Methodology for Motivating a Set of Coherence
Relations. Ph.D. thesis, Department of
Artificial Intelligence, University of
Edinburgh.
Knott, Alistair and Chris Mellish. 1996. A
feature-based account of the relations
signalled by sentence and clause
connectives. Language and Speech,
39(2?3):143?183.
Knott, Alistair, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001.
Beyond elaboration: The interaction of
relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and
W. Spooren, editors, Text Representation:
Linguistic and Psycholinguistic Aspects. John
Benjamins, Amsterdam, pages 181?196.
Koller, Alexander and Kristina Striegnitz.
2002. Generation as dependency parsing.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
pages 17?24, Philadelphia, PA.
Kruijff-Korbayova?, Ivana and Bonnie
Webber. 2001a. Concession, implicature
and alternative sets. In Fourth International
Workshop on Computational Semantics,
Tilburg, the Netherlands.
Kruijff-Korbayova?, Ivana and Bonnie
Webber. 2001b. Information structure and
the semantics of ?otherwise.? In
ESSLLI?2001 Workshop on Information
Structure, Discourse Structure and Discourse
Semantics, pages 61?78, Helsinki, Finland.
Lagerwerf, Luuk. 1998. Causal Connectives
Have Presuppositions. Holland Academic
Graphics, The Hague, the Netherlands.
Levinson, Stephen. 2000. Presumptive
Meanings: The Theory of Generalized
Conversational Implicature. MIT Press,
Cambridge, MA.
Luperfoy, Susann. 1992. The representation
of multimodal user interface dialogues
using discourse pegs. In Proceedings of the
30th Annual Meeting of the Association for
Computational Linguistics (ACL), pages
22?31, University of Delaware, Newark.
Mann, William and Sandra Thompson.
1988. Rhetorical structure theory: Toward
a functional theory of text organization.
Text, 8(3):243?281.
Marcu, Daniel. 1999. Instructions for
manually annotating the discourse
structure of texts. Available from
http://www.isi.edu/?marcu.
Marcu, Daniel and Abdessamad Echihabi.
2002. An unsupervised approach to
recognizing discourse relations. In
Proceedings of the 40th Annual Meeting,
Association for Computational Linguistics,
pages 368?375, University of
Pennsylvania, Philadelphia.
Modjeska, Natalia Nygren. 2001. Towards a
resolution of comparative anaphora: A
corpus study of ??other.? In PAPACOL,
Italy.
Modjeska, Natalia Nygren. 2002. Lexical
and grammatical role constraints in
resolving other-anaphora. In Proceedings of
586
Computational Linguistics Volume 29, Number 4
the Discourse Anaphora and Anaphor
Resolution Colloquium, Lisbon, Portugal.
Moens, Marc and Mark Steedman. 1988.
Temporal ontology and temporal
reference. Computational Linguistics,
14(1):15?28.
Moore, Johanna and Martha Pollack. 1992.
A problem for RST: The need for
multi-level discouse analysis.
Computational Linguistics, 18(4):537?544.
Moser, Megan and Johanna Moore. 1995.
Investigating cue selection and placement
in tutorial discourse. In Proceedings of the
33rd Annual Meeting, Association for
Computational Linguistics, pages 130?135,
MIT, Cambridge, MA.
Moser, Megan and Johanna Moore. 1996.
Toward a synthesis of two accounts of
discourse structure. Computational
Linguistics, 22(3):409?419.
Not, Elena, Lucia Tovena, and Massimo
Zancanaro. 1999. Positing and resolving
bridging anaphora in deverbal NPs. In
ACL?99 Workshop on the Relationship between
Discourse/Dialogue Structure and Reference,
College Park, MD.
Partee, Barbara. 1984. Nominal and
temporal anaphora. Linguistics and
Philosophy, 7(3):287?324.
Polanyi, Livia and Martin H. van den Berg.
1996. Discourse structure and discourse
interpretation. In P. Dekker and
M. Stokhof, editors, Proceedings of the Tenth
Amsterdam Colloquium, pages 113?131,
University of Amsterdam.
Prince, Ellen. 1992. The ZPG letter: Subjects,
definiteness and information-status. In
Susan Thompson and William Mann,
editors, Discourse Description: Diverse
Analyses of a Fundraising Text. John
Benjamins, Amsterdam, pages 295?325.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartik. 1972. A
Grammar of Contemporary English.
Longman, Harlow, England.
Scha, Remko and Livia Polanyi. 1988. An
augmented context free grammar for
discourse. In Proceedings of the 12th
International Conference on Computational
Linguistics (COLING?88), pages 573?577,
Budapest, Hungary, August.
Schilder, Frank. 1997a. Towards a theory of
discourse processing: Flashback sequences
described by D-trees. In Proceedings of the
Formal Grammar Conference (ESSLLI?97),
Aix-en-Provence, France, August.
Schilder, Frank. 1997b. Tree discourse
grammar, or how to get attached to a
discourse. In Proceedings of the Second
International Workshop on Computational
Semantics, Tilburg, the Netherlands,
January.
Steedman, Mark. 1996. Surface Structure and
Interpretation. Volume 30 of Linguistic
Inquiry Monograph, 5, MIT Press,
Cambridge, MA.
Steedman, Mark. 2000a. Information
structure and the syntax-phonology
interface. Linguistic Inquiry, 34:649?689.
Steedman, Mark. 2000b. The Syntactic
Process. MIT Press, Cambridge, MA.
Stokhof, Martin and Jeroen Groenendijk.
1999. Dynamic semantics. In Robert
Wilson and Frank Keil, editors, MIT
Encyclopedia of Cognitive Science. MIT Press.
Cambridge, MA, pages 247?249
Stone, Matthew, Christine Doran, Bonnie
Webber, Tonia Bleam, and Martha Palmer.
2001. Microplanning from communicative
intentions: Sentence planning using
descriptions (SPUD). Technical Report no.
RUCCS TR68, Department of Cognitive
Science, Rutgers University, New
Brunswick, NJ.
Stone, Matthew and Daniel Hardt. 1999.
Dynamic discourse referents for tense and
modals. In Harry Bunt, editor,
Computational Semantics. Kluwer,
Dordrecht, the Netherlands, pages
287?299.
Strube, Michael. 1998. Never look back: An
alternative to centering. In Proceedings,
COLING/ACL?98, pages 1251?1257,
Montreal, Quebec, Canada.
Traugott, Elizabeth. 1995. The role of the
development of discourse markers in a
theory of grammaticalization. Paper
presented at ICHL XII, Manchester,
England. Revised version of (1997)
available at http://www.stanford.
edu/traugott/ect-papersonline.html.
Traugott, Elizabeth. 1997. The discourse
connective after all: A historical
pragmatic account. Paper presented at
ICL, Paris. Available at http://www.
stanford.edu/traugott/ect-
papersonline.html.
van den Berg, Martin H. 1996. Discourse
grammar and dynamic logic. In P. Dekker
and M. Stokhof, editors, Proceedings of the
Tenth Amsterdam Colloquium, pages 93?111,
ILLC/Department of Philosophy,
University of Amsterdam.
van Eijck, Jan and Hans Kamp. 1997.
Representing discourse in context. In Jan
van Benthem and Alice ter Meulen,
editors, Handbook of Logic and Language.
Elsevier Science B.V., Amsterdam, pages
181?237.
Venditti, Jennifer J., Matthew Stone,
Preetham Nanda, and Paul Tepper. 2002.
Discourse constraints on the
587
Webber et al Anaphora and Discourse Structure
interpretation of nuclear-accented
pronouns. In Proceedings of Symposium on
Speech Prosody, Aix-en-Provence, France.
Available at http://www.lpl.
univ-aix.fr/sp2002/papers.htm.
Vendler, Zeno. 1967. Linguistics in Philosophy.
Cornell University Press, Ithaca, NY.
Webber, Bonnie. 1988. Tense as discourse
anaphor. Computational Linguistics,
14(2):61?73.
Webber, Bonnie. 1991. Structure and
ostension in the interpretation of
discourse deixis. Language and Cognitive
Processes, 6(2):107?135.
Webber, Bonnie and Breck Baldwin. 1992.
Accommodating context change. In
Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 96?103, University of
Delaware, Newark.
Webber, Bonnie and Aravind Joshi. 1998.
Anchoring a lexicalized tree-adjoining
grammar for discourse. In COLING/ACL
Workshop on Discourse Relations and
Discourse Markers, pages 86?92, Montreal,
Quebec, Canada.
Webber, Bonnie, Alistair Knott, and Aravind
Joshi. 2001. Multiple discourse
connectives in a lexicalized grammar for
discourse. In Harry Bunt, Reinhard
Muskens, and Elias Thijsse, editors,
Computing Meaning, volume 2. Kluwer,
Dordrecht, the Netherlands, pages
229?249.
Webber, Bonnie, Alistair Knott, Matthew
Stone, and Aravind Joshi. 1999a.
Discourse relations: A structural and
presuppositional account using lexicalised
TAG. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics, pages 41?48, College Park,
MD.
Webber, Bonnie, Alistair Knott, Matthew
Stone, and Aravind Joshi. 1999b. What
are little trees made of: A structural and
presuppositional account using lexicalised
TAG. In Proceedings of International
Workshop on Levels of Representation in
Discourse (LORID?99), pages 151?156,
Edinburgh.
Wiebe, Janyce. 1993. Issues in linguistic
segmentation. In Workshop on Intentionality
and Structure in Discourse Relations,
Association for Computational Linguistics,
pages 148?151, Ohio State University.
Woods, William. 1978. Semantics and
quantification in natural language
question answering. In Marshall C. Yovits,
editor, Advances in Computers, volume 17.
Academic Press, New York, pages 1?87.
XTAG-Group. 2001. A lexicalized tree
adjoining grammar for English. Technical
Report no. IRCS 01-03, University
of Pennsylvania, Philadelphia. Available at
ftp://ftp.cis.upenn.edu/pub/ircs/technical-
reports/01-03.
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 674?682,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Genre distinctions for Discourse in the Penn TreeBank
Bonnie Webber
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW, UK
bonnie.webber@ed.ac.uk
Abstract
Articles in the Penn TreeBank were iden-
tified as being reviews, summaries, let-
ters to the editor, news reportage, correc-
tions, wit and short verse, or quarterly
profit reports. All but the latter three
were then characterised in terms of fea-
tures manually annotated in the Penn Dis-
course TreeBank ? discourse connectives
and their senses. Summaries turned out
to display very different discourse features
than the other three genres. Letters also
appeared to have some different features.
The two main findings involve (1) differ-
ences between genres in the senses asso-
ciated with intra-sentential discourse con-
nectives, inter-sentential discourse con-
nectives and inter-sentential discourse re-
lations that are not lexically marked; and
(2) differences within all four genres be-
tween the senses of discourse relations
not lexically marked and those that are
marked. The first finding means that genre
should be made a factor in automated
sense labelling of non-lexically marked
discourse relations. The second means
that lexically marked relations provide a
poor model for automated sense labelling
of relations that are not lexically marked.
1 Introduction
It is well-known that texts differ from each other in
a variety of ways, including their topic, the read-
ing level of their intended audience, and their in-
tended purpose (eg, to instruct, to inform, to ex-
press an opinion, to summarize, to take issue with
or disagree, to correct, to entertain, etc.). This
paper considers differences in texts in the well-
known Penn TreeBank (hereafter, PTB) and in
particular, how these differences show up in the
Penn Discourse TreeBank (Prasad et al, 2008).
It first describes ways in which texts can vary
(Section 2). It then illustrates the variety of texts
to be found in the the PTB and suggests their
grouping into four broad genres (Section 3). After
a brief introduction to the Penn Discourse Tree-
Bank (hereafter, PDTB) in Section 4, Sections 5
and 6 show that these four genres display differ-
ences in connective frequency and in terms of the
senses associated with intra-sentential connectives
(eg, subordinating conjunctions), inter-sentential
connectives (eg, inter-sentential coordinating con-
junctions) and those inter-sentential relations that
are not lexically marked. Section 7 considers re-
cent efforts to induce effective procedures for au-
tomated sense labelling of discourse relations that
are not lexically marked (Elwell and Baldridge,
2008; Marcu and Echihabi, 2002; Pitler et al,
2009; Wellner and Pustejovsky, 2007; Wellner,
2008). It makes two points. First, because gen-
res differ from each other in the senses associated
with such relations, genre should be made a factor
in their automated sense labelling. Secondly, be-
cause different senses are being conveyed when a
relation is lexically marked than when it isn?t, lex-
ically marked relations provide a poor model for
automated sense labelling of relations that are not
lexically marked.
2 Two Perspectives on Genre
The dimension of text variation of interest here is
genre, which can be viewed externally, in terms
of the communicative purpose of a text (Swales,
1990), or internally, in terms of features com-
mon to texts sharing a communicative purpose.
(Kessler et al, 1997) combine these views by say-
ing that a genre should not be so broad that the
texts belonging to it don?t share any distinguish-
ing properties ?
. . .we would probably not use the term
?genre? to describe merely the class of
674
texts that have the objective of persuad-
ing someone to do something, since that
class ? which would include editorials,
sermons, prayers, advertisements, and
so forth ? has no distinguishing formal
properties (Kessler et al, 1997, p. 33).
A balanced corpus like the Brown Corpus of
American English or the British National Corpus,
will sample texts from different genres, to give a
representative view of how the language is used.
For example, the fifteen categories of published
material sampled for the Brown Corpus include
PRESS REPORTAGE, PRESS EDITORIALS, PRESS
REVIEWS and five different types of FICTION.
In contrast, experiments on what genres would
be helpful in web search for particular types of in-
formation on a topic led (Rosso, 2008), to 18 class
labels that his subjects could reliably apply to web
pages (here, ones from an .edu domain) with over
50% agreement. These class labels included ARTI-
CLE, COURSE DESCRIPTION, COURSE LIST, DI-
ARY, WEBLOG OR BLOG, FAQ/HELP and FORM.
In both Brown?s published material and Rosso?s
web pages, the selected class labels (genres) re-
flect external purpose rather than distinctive inter-
nal features.
Such features are, however, of great interest in
both text analysis and text processing. Text an-
alysts have shown that there are indeed interest-
ing features that correlate more strongly with cer-
tain genres than with others. For example, (Biber,
1986) considered 41 linguistic features previously
mentioned in the literature, including type/token
ratio, average word length, and such frequencies
as that of particular words (eg, I/you, it, the pro-
verb do), particular word types (eg, place adverbs,
hedges), particular parts-of-speech (eg, past tense
verbs, adjectives), and particular syntactic con-
structions (eg, that-clauses, if -clauses, reduced
relative clauses). He found certain clusters of
these features (i.e. their presense or absense) cor-
related well with certain text types. For example,
press reportage scored the highest with respect to
high frequency of that-clauses and contractions,
and low type-token ratio (i.e. a varied vocabu-
lary for a given length of text), while general and
romantic fiction scored much lower on these fea-
tures. (Biber, 2003) showed significant differences
in the internal structure of noun phrases used in
fiction, news, academic writing and face-to-face
conversations.
Such features are of similar interest in text pro-
cessing ? in particular, automated genre classifi-
cation (Dewdney et al, 2001; Finn and Kushmer-
ick, 2006; Kessler et al, 1997; Stamatatos et al,
2000; Wolters and Kirsten, 1999) ? which relies
on there being reliably detectable features that can
be used to distinguish one class from another. This
is where the caveat from (Kessler et al, 1997) be-
comes relevant: A particular genre shouldn?t be
taken so broadly as to have no distinguishing fea-
tures, nor so narrowly as to have no general appli-
cability. But this still allows variability in what is
taken to be a genre. There is no one ?right set?.
3 Genre in the Penn TreeBank
Although the files in the Penn TreeBank (PTB)
lack any classificatory meta-data, leading the PTB
to be treated as a single homogeneous collection
of ?news articles?, researchers who have manually
examined it in detail have noted that it includes a
variety of ?financial reports, general interest sto-
ries, business-related news, cultural reviews, ed-
itorials and letters to the editor? (Carlson et al,
2002, p. 7).
To date, ignoring this variety hasn?t really mat-
tered since the PTB has primarily been used in
developing word-level and sentence-level tools
for automated language analysis such as wide-
coverage part-of-speech taggers, robust parsers
and statistical sentence generators. Any genre-
related differences in word usage and/or syntax
have just meant a wider variety of words and sen-
tences shaping the covereage of these tools. How-
ever, ignoring this variety may actually hinder the
development of robust language technology for
analysing and/or generating multi-sentence text.
As such, it is worth considering genre in the PTB,
since doing so can allow texts from different gen-
res to be weighted differently when tools are being
developed.
This is a start on such an undertaking. In lieu
of any informative meta-data in the PTB files1, I
looked at line-level patterns in the 2159 files that
make up the Penn Discourse TreeBank subset of
the PTB, and then manually confirmed the text
types I found.2 The resulting set includes all the
1Subsequent to this paper, I discovered that the TIPSTER
Collection (LDC Catalog entry LDC93T3B) contains a small
amount of meta-data that can be projected onto the PTB files,
to refine the semi-automatic, manually-verified analysis done
here. This work is now in progress.
2Similar patterns can also be found among the 153 files in
675
genres noted by Carlson et al (2002) and others as
well:
1. Op-Ed pieces and reviews ending with a by-
line (73 files): wsj 0071, wsj 0087, wsj 0108,
wsj 0186, wsj 0207, wsj 0239, wsj 0257, etc.
2. Sourced articles from another newspaper or
magazine (8 files): wsj 1453, wsj 1569, wsj 1623,
wsj 1635, wsj 1809, wsj 1970, wsj 2017, wsj 2153
3. Editorials and other reviews, similar to the
above, but lacking a by-line or source (11
files): wsj 0039, wsj 0456, wsj 0765, wsj 0794,
wsj 0819, wsj 0972, wsj 1259 wsj 1315, etc.
4. Essays on topics commemorating the WSJ?s
centennial (12 files): wsj 0022, wsj 0339,
wsj 0406, wsj 0676, wsj 0933, 2sj 1164, etc.
5. Daily summaries of offerings and pricings in
U.S. and non-U.S. capital markets (13 files):
wsj 0125, wsj 0271, wsj 0476, wsj 0612, wsj 0704,
wsj 1001, wsj 1161, wsj 1312, wsj 1441, etc.
6. Daily summaries of financially significant
events, ending with a summary of the day?s
market figures (14 files): wsj 0178, wsj 0350,
wsj 0493, wsj 0675, wsj 1043, wsj 1217, etc.
7. Daily summaries of interest rates (12 files):
wsj 0219, wsj 0457, wsj 0602, wsj 0986, etc.
8. Summaries of recent SEC filings (4 files):
wsj 0599, wsj 0770, wsj 1156, wsj 1247
9. Weekly market summaries (12 files):
wsj 0137, wsj 0231, wsj 0374, wsj 0586, wsj 1015,
wsj 1187, wsj 1337, wsj 1505, wsj 1723, etc.
10. Letters to the editor (49 files3): wsj 0091,
wsj 0094, wsj 0095, wsj 0266, wsj 0268, wsj 0360,
wsj 0411, wsj 0433, wsj 0508, wsj 0687, etc.
11. Corrections (24 files): wsj 0104, wsj 0200,
wsj 0211, wsj 0410, wsj 0603, wsj 0605, etc.
12. Wit and short verse (14 files): wsj 0139,
wsj 0312, wsj 0594, wsj 0403, wsj 0757, etc.
13. Quarterly profit reports ? introductory para-
graphs alone (11 files): wsj 0190, wsj 0364,
wsj 0511, wsj 0696, wsj 1056, wsj 1228, etc.
the Penn TreeBank that aren?t included in the PDTB. How-
ever, such files were excluded so that all further analyses
could be carried out on the same set of files.
3The relation between letters and files is not one-to-one:
13 (26.5%) of these files contain between two and six letters.
This is relevant at the end of this section when considering
length as a potentially distinguishing feature of a text.
14. News reports (1902 files)
A complete listing of these classes can be found in
an electronic appendix to this article at the PDTB
home page (http://www.seas.upenn.edu/?pdtb).
In order to consider discourse-level features dis-
tinctive to genres within the PTB, I have ignored,
for the time being, both CORRECTIONS and WIT
AND SHORT VERSE since they are so obviously
different from the other texts, and also QUAR-
TERLY PROFIT REPORTS, since they turn out to
be multiple simply copies of the same text be-
cause the distinguishing company listings have
been omitted.
The remaining eleven classes have been ag-
gregated into four broad genres: ESSAYS (104
files, classes 1-4), SUMMARIES (55 files, classes
5-9), LETTERS (49 files, class 10) and NEWS
(1902 files, class 14). The latter corre-
sponds to the Brown Corpus class PRESS RE-
PORTAGE and the class NEWS in the New
York Times annotated corpus (Evan Sandhaus,
2008), excluding CORRECTIONS and OBITUAR-
IES. The LETTERS class here corresponds to
the NYT class OPINION/LETTERS, while ES-
SAYS here spans both Brown Corpus classes
PRESS REVIEWS and PRESS EDITORIALS, and
the NYT corpus classes OPINION/EDITORIALS,
OPINION/OPED, FEATURES/XXX/COLUMNS and
FEATURES/XXX/REVIEWS, where XXX ranges
over Arts, Books, Dining and Wine, Movies,
Style, etc. The class called SUMMARIES has no
corresponding class in Brown. In the NYT Cor-
pus, it corresponds to those articles whose tax-
onomic classifiers field is NEWS/BUSINESS and
whose types of material field is SCHEDULE.
There are two things to note here. First, no
claim is being made that these are the only classes
to be found in the PTB. For example, the class
labelled NEWS contains a subset of 80 short (1-3
sentence) articles announcing personnel changes
? eg, promotions, appointments to supervisory
boards, etc. (eg, wsj 0001, wsj 0014, wsj 0066,
wsj 0069, wsj 0218, etc.) I have not looked
for more specific classes because even classes at
this level of specificity show that ignoring genre-
specific discourse features can hinder the devel-
opment of robust language technology for either
analysing or generating multi-sentence text. Sec-
ondly, no claim is being made that the four se-
lected classes comprise the ?right? set of genres
for future use of the PTB for discourse-related
676
language technology, just that some sensitivity to
genre will lead to better performance.
Some simple differences between the four broad
genre can be seen in Figure 1, in terms of the av-
erage length of a file in words, sentences or para-
graphs4, and the average number of sentences per
paragraph. Figure 1 shows that essays are, on aver-
age, longer than texts from the other three classes,
and have longer paragraphs. The relevance of the
latter will become clear in the next section, when
I describe PDTB annotation as background for
genre differences related to this annotation.
4 The Penn Discourse TreeBank
Genre differences at the level of discourse in the
PTB can be seen in the manual annotations of the
Penn Discourse TreeBank (Prasad et al, 2008).
There are several elements to PDTB annotation.
First, the PDTB annotates the arguments of ex-
plicit discourse connectives:
(1) Even so, according to Mr. Salmore, the ad
was ?devastating? because it raised ques-
tions about Mr. Courter?s credibility. But it?s
building on a long tradition. (0041)
Here, the explicit connective (?but?) is underlined.
Its first argument, ARG1, is shown in italics and
its second, ARG2, in boldface. The number 0041
indicates that the example comes from subsection
wsj 0041 of the PTB.
Secondly, the PDTB annotates implicit dis-
course relations between adjacent sentences
within the same paragraph, where the second does
not contain an explicit inter-sentential connective:
(2) The projects already under construction will
increase Las Vegas?s supply of hotel rooms by
11,795, or nearly 20%, to 75,500. [Implicit
?so?] By a rule of thumb of 1.5 new jobs for
each new hotel room, Clark County will
have nearly 18,000 new jobs. (0994)
With implicit discourse relations, annotators were
asked to identify one or more explicit connectives
that could be inserted to lexicalize the relation be-
tween the arguments. Here, they have been identi-
fied as the connective ?so?.
Where annotators could not identify such an im-
plicit connective, they were asked if they could
identify a non-connective phrase in ARG2 (e.g.
4A file usually contains a single article, except (as noted
earlier) files in the class LETTERS, which may contain more
than one letter.
?this means?) that realised the implicit discourse
relation instead (ALTLEX), or a relation holding
between the second sentence and an entity men-
tioned in the first (ENTREL), rather than the inter-
pretation of the previous sentence itself:
(3) Rated triple-A by Moody?s and S&P, the issue
will be sold through First Boston Corp. The
issue is backed by a 12% letter of credit
from Credit Suisse.
If the annotators couldn?t identify either, they
would assert that no discourse relation held be-
tween the adjacent sentences (NOREL). Note that
because resource limitations meant that implicit
discourse relations (comprising implicit connec-
tives, ALTLEX, ENTREL and NOREL) were only
annotated within paragraphs, longer paragraphs
(as there were in ESSAYS) could potentially mean
more implicit discourse relations were annotated.
The third element of PDTB annotation is that
of the senses of connectives, both explicit and im-
plicit. These have been manually annotated using
the three-level sense hierarchy described in detail
in (Miltsakaki et al, 2008). Briefly, there are four
top-level classes:
? TEMPORAL, where the situations described
in the arguments are related temporally;
? CONTINGENCY, where the situation de-
scribed in one argument causally influences
that described in the other;
? COMPARISON, used to highlight some
prominent difference that holds between the
situations described in the two arguments;
? EXPANSION, where one argument expands
the situation described in the other and moves
the narrative or exposition forward.
TEMPORAL relations can be further specified to
ASYNCHRONOUS and SYNCHRONOUS, depend-
ing on whether or not the situations described by
the arguments are temporally ordered. CONTIN-
GENCY can be further specified to CAUSE and
CONDITION, depending on whether or not the ex-
istential status of the arguments depends on the
connective (i.e. no for CAUSE, and yes for CON-
DITION).
COMPARISON can be further specified to CON-
TRAST, where the two arguments share a predicate
or property whose difference is being highlighted,
and CONCESSION, where ?the highlighted differ-
ences are related to expectations raised by one
677
Total Total Total Total Avg. words Avg. sentences Avg. ?s Avg. sentences
Genre files paragraphs sentences words per file per file per file per ?
ESSAYS 104 1580 4774 98376 945.92 45.9 15.2 3.02
SUMMARIES 55 1047 2118 37604 683.71 38.5 19.1 2.02
LETTERS 49 339 739 15613 318.63 15.1 7.1 2.14
NEWS 1902 18437 40095 837367 440.26 21.1 9.7 2.17
Figure 1: Distribution of Words, Sentences and Paragraphs by Genre (? stands for ?paragraph?.)
argument which are then denied by the other?
(Miltsakaki et al, 2008, p.282). Finally, EX-
PANSION has six subtypese, including CONJUNC-
TION, where the situation described in ARG2, pro-
vides new information related to the situation de-
scribed in ARG1; RESTATEMENT, where ARG2
restates or redescribes the situation described in
ARG1; and ALTERNATIVE, where the two argu-
ments evoke situations taken to be alternatives.
These two levels are sufficient to show signifi-
cant differences between genres. The only other
thing to note is that annotators could be as specific
as they chose in annotating the sense of a connec-
tive: If they could not decide on the specific type
of COMPARISON holding between the two argu-
ments of a connective, or they felt that both sub-
types of COMPARISON were being expressed, they
could simply sense annotate the connective with
the label COMPARISON. I will comment on this in
Section 6.
The fourth element of PDTB annotation is at-
tribution (Prasad et al, 2007; Prasad et al, 2008).
This was not considered in the current analysis,
although here too, genre-related differences are
likely.
5 Connective Frequency by Genre
The analysis that follows distinguishes between
two kinds of relations associated with explicit con-
nectives in the PDTB: (1) intra-sentential dis-
course relations, which hold between clauses
within the same sentence and are associated with
subordinating conjunctions, intra-sentential coor-
dinating conjunctions, and discourse adverbials
whose arguments occur within the same sen-
tence5); and (2) explicit inter-sentential discourse
relations, which hold across sentences and are
associated with explicit inter-sentential connec-
tives (inter-sentential coordinating conjunctions
and discourse adverbials whose arguments are not
5Limited resources meant that intra-sentential discourse
relations associated with subordinators like ?in order to? and
?so that? or with free adjuncts were not annotated in the
PDTB.
in the same sentence).
It is the latter that are effectively in complemen-
tary distribution with implicit discourse relations
in the PDTB6, and Figures 2 and 3 show their dis-
tribution across the four genres.7 Figure 2 shows
that among explicit inter-sentential connectives,
S-initial coordinating conjunctions (?And?, ?Or?
and ?But?) are a feature of ESSAYS, SUMMARIES
and NEWS but not of LETTERS. LETTERS are writ-
ten by members of the public, not by the journal-
ists or editors working for the Wall Street Journal.
This suggests that the use of S-initial coordinating
conjunctions is an element of Wall Street Journal
?house style?, as opposed to a common feature of
modern writing.
Figure 3 shows several things about the dif-
ferent patterning across genres of implicit dis-
course relations (Columns 4?7 for implicit con-
nectives, ALTLEX, ENTREL and NOREL) and
explicit inter-sentential connectives (Column 3).
First, SUMMARIES are distinctive in two ways:
While the ratio of implicit connectives to explicit
inter-sentential connectives is around 3:1 in the
other three genres, for SUMMARIES it is around
4:1 ? there are just many fewer explicit inter-
sentential connectives. Secondly, while the ra-
tio of ENTREL relations to implicit connectives
ranges from 0.19 to 0.32 in the other three gen-
res, in SUMMARIES, ENTREL predominates (as in
Example 3 from one of the daily summaries of of-
ferings and pricings). In fact, there are nearly as
6This is not quite true for two reasons ? first, because the
first argument of a discourse adverbial is not restricted to the
immediately adjacent sentence and secondly, because a sen-
tence can have both an initial coordinating conjunction and a
discourse adverbial, as in ?So, for example, he?ll eat tofu with
fried pork rinds.? But it?s a reasonable first approximation.
7Although annotated in the PDTB, throughout this paper
I have ignored the S-medial discourse adverbial also, as in
?John also eats fish?, since such instances are better regarded
as presuppositional. That is, as well as a textual antecedent,
they can be licensed through inference (e.g. ?John claims
to be a vegetarian, but he also eats fish.?) or accommodated
by listeners with respect to the spatio-temporal context (e.g.
Watching John dig into a bowl of tofu, one might remark
?Don?t worry. He also eats fish.?) The other discourse ad-
verbials annotated in the PDTB do not have this property.
678
Total Explicit Density of Explicit S-initial S-initial S-medial
Total Inter-Sentential Inter-Sentential Coordinating Discourse Inter-Sentential
Genre Sentences Connectives Connectives/Sentence Conjunctions Adverbials Disc Advs
ESSAYS 4774 691 0.145 334 (48.3%) 244 (35.3%) 113 (16.4%)
SUMMARIES 2118 95 0.045 46 (48.4%) 39 (41.1%) 10 (10.5%)
LETTERS 739 85 0.115 26 (30.6%) 37 (43.5%) 18 (21.2%)
NEWS 40095 4709 0.117 2389 (50.7%) 1610 (34.2%) 718 (15.3%)
Figure 2: Distribution of Explicit Inter-Sentential Connectives.
Total Total Explicit
Inter-Sentential Inter-Sentential Implicit
Genre Discourse Rels Connectives Connectives ENTREL ALTLEX NOREL
ESSAYS 3302 691 (20.9%) 2112 (64.0%) 397 (12.0%) 86 (2.6%) 16 (0.5%)
SUMMARIES 916 95 (10.4%) 363 (39.6%) 434 (47.4%) 12 (1.3%) 12 (1.3%)
LETTERS 433 85 (19.6%) 267 (61.7%) 58 (13.4%) 22 (5.1%) 1 (0.2%)
NEWS 23017 4709 (20.5%) 13287 (57.7%) 4293 (18.7%) 504 (2.2%) 224 (1%)
Figure 3: Distribution of Inter-Sentential Discourse Relations, including Explicits from Figure 2.
many ENTREL relations in summaries as the total
of explicit and implicit connectives combined.
Finally, it is possible that the higher frequency
of alternative lexicalizations of discourse connec-
tives (ALTLEX) in LETTERS than in the other three
genres means that they are not part of Wall Street
Journal ?house style?. (Other elements of WSJ
?house style? ? or possibly, news style in general
? are observable in the significantly higher fre-
quency of direct and indirect quotations in news
than in the other three genres. This property is not
discussed further here, but is worth investigating
in the future.)
With respect to explicit intra-sentential con-
nectives, the main point of interest in Figure 4
is that SUMMARIES display a significantly lower
density of intra-sentential connectives overall than
the other three genres, as well as a significantly
lower relative frequency of intra-sentential dis-
course adverbials. As the next section will show,
these intra-sentential connectives, while few, are
selected most often to express CONTRAST and sit-
uations changing over time, reflecting the nature
of SUMMARIES as regular periodic summaries of
a changing world.
6 Connective Sense by Genre
(Pitler et al, 2008) show a difference across Level
1 senses (COMPARISON, CONTINGENCY, TEM-
PORAL and EXPANSION) in the PDTB in terms of
their tendency to be realised by explicit connec-
tives (a tendency of COMPARISON and TEMPO-
RAL relations) or by Implicit Connectives (a ten-
dency of CONTINGENCY and EXPANSION). Here
I show differences (focussing on Level 2 senses,
which are more informative) in their frequency
of occurance in the four genres, by type of con-
nective: explicit intra-sentential connectives (Fig-
ure 5), explicit inter-sentential connectives (Fig-
ure 6), and implicit inter-sentential connectives
(Figure 7). SUMMARIES and LETTERS are each
distinctly different from ESSAYS and NEWS with
respect to each type of connective.
One difference in sense annotation across the
four genres harkens back to a comment made in
Section 4 ? that annotators could be as specific
as they chose in annotating the sense of a con-
nective. If they could not decide between spe-
cific level n+1 labels for the sense of a connective,
they could simply assign it a level n label. It is
perhaps suggestive then of the relative complexity
of ESSAYS and LETTERS, as compared to NEWS,
that the top-level label COMPARISON was used
approximately twice as often in labelling explicit
inter-sentential connectives in ESSAYS (7.2%) and
LETTERS (9.4%) than in news (4.3%). (The top-
level labels EXPANSION, TEMPORAL and CON-
TINGENCY were used far less often, as to be sim-
ply noise.) In any case, this aspect of readabil-
ity may be worth further investigation (Pitler and
Nenkova, 2008).
7 Automated Sense Labelling of
Discourse Connectives
The focus here is on automated sense labelling
of discourse connectives (Elwell and Baldridge,
2008; Marcu and Echihabi, 2002; Pitler et al,
2009; Wellner and Pustejovsky, 2007; Wellner,
679
Total Density of Intra-Sentential Intra-Sentential
Total Intra-Sentential Intra-Sentential Subordinating Coordinating Discourse
Genre Sentences Connectives Connectives/Sentence Conjunctions Conjunctions Adverbials
ESSAYS 4774 1397 0.293 808 (57.8%) 438 (31.4%) 151 (10.8%)
SUMMARIES 2118 275 0.130 166 (60.4%) 99 (36.0%) 10 (3.6%)
LETTERS 739 200 0.271 126 (63.0%) 56 (28.0%) 18 (9.0%)
NEWS 40095 9336 0.233 5514 (59.1%) 3015 (32.3%) 807 (8.6%)
Figure 4: Distribution of Explicit Intra-Sentential Connectives.
Relation Essays Summaries Letters News
Expansion.Conjunction 253 (18.1%) 50 (18.2%) 31 (15.5%) 1907 (20.4%)
Contingency.Cause 208 (14.9%) 37 (13.5%) 32 (16%) 1354 (14.5%)
Contingency.Condition 205 (14.7%) 15 (5.5%) 22 (11%) 1082 (11.6%)
Temporal.Asynchronous 187 (13.4%) 54 (19.6%) 19 (9.5%) 1444 (15.5%)
Comparison.Contrast 187 (13.4%) 56 (20.4%) 29 (14.5%) 1416 (15.2%)
Temporal.Synchrony 165 (11.8%) 32 (11.6%) 27 (13.5%) 1061 (11.4%)
Total 1397 275 200 9336
Figure 5: Explicit Intra-Sentential Connectives: Most common Level 2 Senses
Relation Essays Summaries Letters News
Comparison.Contrast 231 (33.4%) 47 (49.5%) 20 (23.5%) 1853 (39.4%)
Expansion.Conjunction 156 (22.6%) 24 (25.3%) 20 (23.5%) 1144 (24.3%)
Comparison.Concession 75 (10.9%) 11 (11.6%) 5 (5.9%) 462 (9.8%)
Comparison 50 (7.2%) ? 8 (9.4%) 204 (4.3%)
Temporal.Asynchronous 40 (5.8%) 1 (1.1%) 5 (5.8%) 265 (5.6%)
Expansion.Instantiation 37 (5.4%) 3 (3.2%) 3 (3.5%) 236 (5.0%)
Contingency.Cause 32 (4.6%) 1 (1.1%) 12 (14.1%) 136 (2.9%)
Expansion.Restatement 27 (3.9%) ? 6 (7.1%) 93 (2.0%)
Total 691 95 85 4709
Figure 6: Explicit Inter-Sentential Connectives: Most common Level 2 Senses
Relation Essays Summaries Letters News
Contingency.Cause 577 (27.3%) 70 (19.28%) 75 (28.1%) 3389 (25.5%)
Expansion.Restatement 395 (18.7%) 62 (17.07%) 55 (20.6%) 2591 (19.5%)
Expansion.Conjunction 362 (17.1%) 126 (34.7%) 40 (15.0%) 2908 (21.9%)
Comparison.Contrast 254 (12.0%) 53 (14.60%) 42 (15.7%) 1704 (12.8%)
Expansion.Instantiation 211 (10.0%) 18 (4.96%) 14 (5.2%) 1152 (8.7%)
Temporal.Asynchronous 110 (5.2%) 7 (1.93%) 6 (2.3%) 524 (3.9%)
Total 2112 363 267 13287
Figure 7: Implicit Connectives: Most common Level 2 Senses
Essays Summaries
Relation: Implicit Inter-Sent Intra-Sent Implicit Inter-Sent Intra-Sent
Contingency.Cause 577 (27.3%) 32 (4.6%) 208 (14.9%) 70 (19.28%) 1 (1.1%) 37 (13.5%)
Expansion.Restatement 395 (18.7%) 27 (3.9%) 4 (0.3%) 62 (17.07%) ? ?
Expansion.Conjunction 362 (17.1%) 156 (22.6%) 253 (18.1%) 126 (34.7%) 24 (25.3%) 50 (18.2%)
Comparison.Contrast 254 (12.0%) 231 (33.4%) 187 (13.4%) 53 (14.60%) 47 (49.5%) 56 (20.4%)
Expansion.Instantiation 211 (10.0%) 37 (5.4%) 5 (0.3%) 18 (5.0%) 3 (3.2%) ?
Total: 2112 691 1397 363 95 275
Figure 8: Essays and Summaries: Connective sense frequency
680
Letters News
Relation: Implicit Inter-Sent Intra-Sent Implicit Inter-Sent Intra-Sent
Contingency.Cause 75 (28.1%) 12 (14.1%) 32 (16%) 3389 (25.5%) 136 (2.9%) 1354 (14.5%)
Expansion.Restatement 55 (20.6%) 6 (7.1%) 4 (2%) 2591 (19.5%) 93 (2.0%) 20 (0.2%)
Expansion.Conjunction 40 (15.0%) 20 (23.5%) 31 (15.5%) 2908 (21.9%) 1144 (24.3%) 1907 (20.4%)
Comparison.Contrast 42 (15.7%) 20 (23.5%) 29 (14.5%) 1704 (12.8%) 1853 (39.4%) 1416 (15.2%)
Expansion.Instantiation 14 (5.2%) 3 (3.5%) ? 1152 (8.7%) 236 (5.0%) 18 (0.2%)
Total 267 85 200 13287 4709 9336
Figure 9: Letters and News: Connective sense frequency
2008). There are two points to make. First, Fig-
ure 7 provides evidence (in terms of differences
between genres in the senses associated with inter-
sentential discourse relations that are not lexically
marked) for taking genre as a factor in automated
sense labelling of those relations.
Secondly, Figures 8 and 9 summarize Figures 5,
6 and 7 with respect to the five senses that oc-
cur most frequently in the four genre with dis-
course relations that are not lexically marked,
covering between 84% and 91% of those rela-
tions. These Figures show that, no matter what
genre one considers, different senses tend to be
expressed with (explicit) intra-sentential connec-
tives, with explicit inter-sentential connectives and
with implicit connectives. This means that lexi-
cally marked relations provide a poor model for
automated sense labelling of relations that are not
lexically marked. This is new evidence for the
suggestion (Sporleder and Lascarides, 2008) that
intrinsic differences between explicit and implicit
discourse relations mean that the latter have to be
learned independently of the former.
8 Conclusion
This paper has, for the first time, provided genre
information about the articles in the Penn Tree-
Bank. It has characterised each genre in terms of
features manually annotated in the Penn Discourse
TreeBank, and used this to show that genre should
be made a factor in automated sense labelling of
discourse relations that are not explicitly marked.
There are clearly other potential differences that
one might usefully investigate: For example, fol-
lowing (Pitler et al, 2008), one might look at
whether connectives with multiple senses occur
with only one of those senses (or mainly so) in
a particular genre. Or one might investigate how
patterns of attribution vary in different genres,
since this is relevant to subjectivity in text. Other
aspects of genre may be even more significant for
language technology. For example, whereas the
first sentence of a news article might be an effec-
tive summary of its contents ? e.g.
(4) Singer Bette Midler won a $400,000 federal
court jury verdict against Young & Rubicam
in a case that threatens a popular advertising
industry practice of using ?sound-alike? per-
formers to tout products. (wsj 0485)
it might be less so in the case of an essay, even one
of about the same length ? e.g.
(5) On June 30, a major part of our trade deficit
went poof! (wsj 0447)
Of course, to exploit these differences, it is im-
portant to be able to automatically identify what
genre or genres a text belongs to. Fortunately,
there is a growing body of work on genre-based
text classification, including (Dewdney et al,
2001; Finn and Kushmerick, 2006; Kessler et al,
1997; Stamatatos et al, 2000; Wolters and Kirsten,
1999). Of particular interest in this regard is
whether other news corpora, such as the New York
Times Annotated Corpus (Linguistics Data Con-
sortium Catalog Number: LDC2008T19) manifest
similar properties to theWSJ in their different gen-
res. If so, then genre-specific extrapolation from
the WSJ Corpus may enable better performance
on a wider range of corpora.
Acknowledgments
I thank my three anonymous reviewers for their
useful comments. Additional thoughtful com-
ments came from Mark Steedman, Alan Lee,
Rashmi Prasad and Ani Nenkova.
References
Douglas Biber. 1986. Spoken and written textual di-
mensions in english. Language, 62(2):384?414.
Douglas Biber. 2003. Compressed noun-phrase struc-
tures in newspaper discourse. In Jean Aitchison and
Diana Lewis, editors, New Media Language, pages
169?181. Routledge.
681
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2002. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In Proceedings of the 2nd SIGdial Workshop on Dis-
course and Dialogue, Aalborg, Denmark.
Nigel Dewdney, Carol VanEss-Dykema, and Richard
MacMillan. 2001. The form is the substance:
classification of genres in text. In Proceedings of
the Workshop on Human Language Technology and
Knowledge Management, pages 1?8.
Robert Elwell and Jason Baldridge. 2008. Discourse
connective argument identication with connective
specic rankers. In Proceedings of the IEEE Con-
ference on Semantic Computing.
Evan Sandhaus. 2008. New york times corpus: Corpus
overview. Provided with the corpus, LDC catalogue
entry LDC2008T19.
Aidan Finn and Nicholas Kushmerick. 2006. Learning
to classify documents according to genre. Journal
of the American Society for Information Science and
Technology, 57.
Brett Kessler, Geoffrey Numberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Pro-
ceedings of the 35th Annual Meeting of the ACL,
pages 32?38.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the Association for Com-
putational Linguistics.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-
avind Joshi. 2008. Sense annotation in the penn
discourse treebank. In Computational Linguistics
and Intelligent Text Processing, pages 275?286.
Springer.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of EMNLP.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Eas-
ily identifiable discourse relations. In Proceedings
of COLING, Manchester.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of ACL-IJCNLP, Sin-
gapore.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind
Joshi, and Bonnie Webber. 2007. Attribution and
its annotation in the Penn Discourse TreeBank. TAL
(Traitement Automatique des Langues), 42(2).
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank
2.0. In Proceedings, 6th International Conference
on Language Resources and Evaluation, Marrakech,
Morocco.
Mark Rosso. 2008. User-based identification of web
genres. J American Society for Information Science
and Technology, 59(7):1053?1072.
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: an assessment. Natural Language En-
gineering, 14(3):369?416.
Efstathios Stamatatos, Nikos Fakotakis, and George
Kokkinakis. 2000. Text genre detection using com-
mon word frequencies. In Proceedings of the 18th
Annual Conference of the ACL, pages 808?814.
John Swales. 1990. Genre Analysis. Cambridge Uni-
versity Press, Cambridge.
Ben Wellner and James Pustejovsky. 2007. Automati-
cally identifying the arguments to discourse connec-
tives. In Proceedings of the 2007 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Prague CZ.
Ben Wellner. 2008. Sequence Models and Ranking
Methods for Discourse Parsing. Ph.D. thesis, Bran-
deis University.
Maria Wolters and Mathias Kirsten. 1999. Exploring
the use of linguistic features in domain and genre
classification. In Proceedings of the 9th Meeting of
the European Chapter of the Assoc. for Computa-
tional Linguistics, pages 142?149, Bergen, Norway.
682
  	
 
 Enhanced Free Text Access to Anatomically-Indexed Data
Gail Sinclair
Informatics Division
University of Edinburgh
80 South Bridge
Edinburgh, UK EH1 1HN
carols@dai.ed.ac.uk
Bonnie Webber
Informatics Division
University of Edinburgh
2 Buccleuch Place
Edinburgh, UK EH8 9LW
bonnie.webber@ed.ac.uk
Duncan Davidson
MRC Human Genetics Unit
Western General Hospital
Crewe Road
Edinburgh, UK EH4 2XU
duncan.davidson@hgu.mrc.ac.uk
Abstract
We describe our use of an existing re-
source, the Mouse Anatomical Nomen-
clature, to improve a symbolic interface
to anatomically-indexed gene expression
data. The goal is to reduce user effort in
specifying anatomical structures of inter-
est and increase precision and recall.
1 Introduction
Language Technology (LT) resources are time-
consuming and expensive to develop, and appli-
cations rarely have the luxury of calling upon re-
sources specially designed for the task at hand. For
LT applications in developmental biology such as
robust interfaces to anatomically-indexed gene ex-
pression data and text mining tools to assist in build-
ing such databases, resources already exist in the
form of anatomical nomenclatures for several model
organisms including mouse, zebrafish, drosophila
and human. (Others may follow.) These nomen-
clatures have been developed by biologists for biol-
ogists, to record in a clear, intuitive and structured
way the structures that can be distinguished at each
stage of an embryo?s development. The challenge
for LT applications in developmental biology is to
stretch them to serve other purposes as well.
In this paper, we describe how we have taken one
of these anatomical nomenclatures (mouse) and ex-
tracted from it a new resource to facilitate free text
access to anatomically-indexed data. The techniques
we have used are applicable to anatomical nomen-
clatures for other model organisms as well.
The paper is organised as follows: In Section 2,
we describe the Mouse Atlas, which is the particular
context for the interface we are developing. Sec-
tion 3 describes what we are doing to reduce the
amount of effort a user has to expend in specifying
anatomical structures of interest to them. In Sec-
tion 4, we describe how what we did to reduce user
effort also serves to provide a clearer display of the
results of searching. Then in Sections 5 and 6, we
describe what we are doing to increase the precision
and recall of user queries.
2 The Mouse Atlas
The Mouse Atlas, developed by researchers at the
Medical Research Council?s Human Genetics Unit
(MRC HGU) in Edinburgh, is a 3D atlas of mouse
embryo development (http://genex.hgu.mrc.ac.uk).
Anatomical structures within each of the 26 Theiler
Stages of embryo development are labelled, and 3D
reconstructions of each stage can be displayed in
transverse, frontal, sagittal or arbitrary planes.
The Mouse Atlas is now being used to support in-
dexing of gene expression data, allowing the results
of gene expression experiments to be indexed with
respect to where genes are expressed in the devel-
oping embryo. There are at least two ways of using
anatomy to index gene expression data. In spatial
indexing, data is associated directly with volume el-
ements, voxels of the anatomical model. In symbolic
indexing, gene expression data is associated with a
label specifying a pre-defined region of the embryo.
A database of spatially indexed gene expres-
sion data (the EMAGE database) is being devel-
oped at the MRC HGU. A database of symboli-
cally indexed gene expression data (the Gene Ex-
pression Database or GXD) is being developed
by the Jackson Laboratory in Bar Harbor, Maine
                                            Association for Computational Linguistics.
                            the Biomedical Domain, Philadelphia, July 2002, pp. 45-52.
                         Proceedings of the Workshop on Natural Language Processing in
(http://www.informatics.jax.org). Indexing in the
GXD uses the Mouse Anatomical Nomenclature, a
set of 26 trees of anatomical terms (one tree per
Theiler Stage) structured primarily by part-whole re-
lations (and some set-member relations).
The root node of each Theiler Stage tree corre-
sponds to the entire embryo at that stage, while other
nodes correspond to organ systems, subsystems,
spatially-localised parts of subsystems, or anatom-
ical structures. Each node within a tree has a la-
bel (its component term), but component terms are
not meant to serve as unique designators: the only
thing guaranteed to denote an anatomical structure
uniquely is the sequence of component terms that
comprises a path from the root node. Thus paths
(and only paths) can serve as keys for symbolic in-
dexing of data. For example the component term
CRANIAL labels both a child of GANGLION (i.e.,
ganglia located in the head) and a child of NERVE
(i.e., nerves located in the head). The path to this
latter child
EMBRYO.ORGANSYSTEM.NERVOUSSYSTEM.
CENTRALNERVOUSSYSTEM.NERVE.
CRANIAL.TRIGEMINALV
uniquely denotes the trigeminal, or fifth cranial,
nerve1 within the Theiler Stage denoted by its root
and is used as a key for relevant data.
For accessing gene expression data, both spatial
and symbolic means are again both possible. An
elegant spatial interface is being completed at the
MRC HGU, that pairs an active window contain-
ing a view of the embryo stage of interest, with a
window containing the corresponding nomenclature
tree2. Clicking at a point in the embryo view high-
lights the most specific corresponding node of the
nomenclature being displayed (i.e., sub-trees of a
node can be either hidden or exploded). Similarly,
clicking on a term in the nomenclature highlights the
corresponding structure within the embryo along the
plane currently being displayed. A screen-shot from
the interface is shown in Figure 2. On the left of the
figure is an outline frontal drawing of the embryo, on
which a sagittal section plane is marked in red. The
centre panel shows a digital, sagittal section through
1Full stop is used to separate component names along a path.
2http://genex.hgu.mrc.ac.uk/Resources/GXDQuery1
the volumetric embryo model with the delineated
left dorsal aorta coloured blue. The correspond-
ing component term is highlighted on the Mouse
Anatomical Nomenclature on the right. Users can
access the gene expression data on the highlighted
structure by another mouse click.
The current project aims at improving symbolic
access to gene expression data by providing a robust
free-text interface. In the current GXD interface3 ,
users can tree-walk through the Mouse Anatomical
Nomenclature for a given stage, to find the anatomi-
cal structure whose associated gene expression data
is of interest to them, or they can enter a term which
is matched against single component names, with all
possible substring matches returned for the user to
choose among.
Problems exist with both forms of access. It is
well known that navigating through a tree is tedious.
A more subtle problem arises from anatomy being
forced into a tree-structure that it doesn?t have. This
leads to structures being divided and the resulting
sub-structures realised in different parts of the tree ?
for example, the endocardial tube is divided into one
part that is a daughter of COMMON ATRIAL CHAM-
BER (i.e., its location), and another part which is a
daughter of OUTFLOW TRACT. So appearing to find
a structure of interest by a tree-walk, in addition to
being tedious, doesn?t by itself guarantee the user
that s/he has found it all.
In contrast, access by sub-string matching on in-
dividual component terms has problems of both re-
call and precision. A user may enter a string that
matches nothing (0% recall): (1) it may be neither a
component term nor a substring within a component
term ? e.g., while HEART is a common synonym
for the modifier ?cardiac? (and a component term
in its own right) and CARDIAC MUSCLE is a compo-
nent term, the string ?heart muscle? yields no match;
or (2) it may span multiple component terms in the
nomenclature ? e.g., while GLAND is a component
term, and PITUITARY is the component term of one
of its children (i.e., a member of the set of glands),
the string ?pituitary gland? yields no match. Or the
opposite may happen: 100% recall with low preci-
sion. For example, a search on ?hindbrain? yields
22 matches, while ?mesenchyme? yields as many as
3http://www.informatics.jax.org/menus/expression menu
Figure 1: Screen shot of Mouse Atlas interface, displaying a Theiler Stage 14 embryo.
1056 matches.
The current project aims to provide a robust free
text interface that will avoid these and other prob-
lems, (1) reducing the effort that a user needs to
expend in finding anatomical structures of inter-
est; (2) better organising search results; (3) improv-
ing recall by reducing the number of times that no
match is found; and (4) improving precision over
that which is possible using substring matching on
individual component names. To do this, we have
been extracting from the Mouse Anatomical Nomen-
clature another resource comprising a set of Natu-
ral Language (NL) phrases that uniquely denote the
parts of the embryo. To expand this set, we have
semi-automatically culled other anatomical speci-
fiers from a textbook on developmental anatomy
made available to us in electronic form. The inter-
esting challenges this has posed are described in the
remainder of the paper.
3 Reducing User Effort
As already noted, component terms in the Mouse
Anatomical Nomenclature (as in those for other
model organisms) are not unique designators for
anatomical structures: the only unique designators
are path specifications. Thus technically, the only
way a user can uniquely select an anatomical struc-
ture of interest is to enter the entire path name (or to
find it through navigating an embryo stage tree down
from its root).
However, there are developmentally valid no-
tions of uniqueness with respect to which some
of the 1416 component terms associated with the
13737 nodes in the 26 Theiler Stage trees of the
Mouse Anatomical Nomenclature can be taken to be
unique.4
The first such notion of uniqueness can be as-
sociated with an anatomical structure that develops
by some Theiler Stage j and then persists under
the same name through subsequent stages. In the
Mouse Anatomical Nomenclature, this situation cor-
responds, first off, to path specifications that differ
only in their root note (which designates the embryo
4The size of Theiler Stage trees ranges from 3 nodes in stage
2 (early development) to 1739 nodes in stage 26 (pre-birth),
with the average size being 528 nodes.
at the corresponding Theiler Stage). If the compo-
nent term at the leaf does not occur elsewhere in the
nomenclature outside this path specification, then
this component name can be classified as unique.
This notion of uniqueness was found to hold
of 1017 of the 1416 component terms, including
CARDIOGENIC PLATE, CRANIUM, etc. This meant
that approximately 11200 nodes were covered, with
somewhat over 2500 still remaining. These 1017
component terms could potentially be used to access
gene expression data associated with some or all of
the Theiler Stages through which the uniquely des-
ignated structure exists, except for a problem that we
will mention shortly.
The second notion of uniqueness is an extension
of the first. Before anatomical structures are fully
formed, they tend to be referred to by names that de-
note the same anatomical structure but also convey
that it is not fully formed ? for example, the FU-
TURE FOREBRAIN. Such component terms can be
linked with the component term of the structure they
develop into, treating the two together as a unique
designator across the extended sequence of Theiler
Stages. A user seeking gene expression data for the
FOREBRAIN without specifying a particular Theiler
Stage, could then select from stages 15-16, which
contain the FUTURE FOREBRAIN, as well as from
stages 17-26, which contain the FOREBRAIN.
In some cases, finding such terms is easy ? specif-
ically, when paths in adjacent stages differ only in
their root node and their potentially co-designating
leaf terms. In other cases, the process is complicated
by the fact that their containing anatomical struc-
tures are themselves developing and changing. This
creates additional differences in paths that should
be taken to co-designate in this lineage sense. The
difference may simply be in the particular compo-
nent term associated with a non-terminal node ? e.g.
FUTURE FOREBRAIN is a child of FUTURE BRAIN
in stages 15-16, while FOREBRAIN descends from
BRAIN in stages 17-26. These cases can be iden-
tified by verifying that the intermediate structures
are themselves in a lineage relation. But the paths
may also differ in length, the earlier stage path be-
ing longer than its corresponding path in the next
stage. This is because the earlier stage specifies the
tissue from which the structure is developing from ?
for example, the fibula develops from the lower leg
mesenchyme. So the path
EMBRYO.LIMB.HINDLIMB.LEG.LOWERLEG.
MESENCHYME.FIBULA
is a unique designator in Stage23, becoming
EMBRYO.LIMB.HINDLIMB.LEG.LOWERLEG.
FIBULA
in Stages 24 to 26. To recognise such cases, we
need to analyse what nodes contribute to differences
in path length and decide whether two component
terms co-specify on that basis.
Again, when these patterns are encountered, the
component names, providing they are not involved
in any other initial tree paths, can be classified as
being unique, further reducing the number of names
to be disambiguated. So far 44 of these lineage pat-
terns have been identified, further eliminating ap-
proximately 118 from the set of ambiguous compo-
nent terms.
The third notion of uniqueness that can be used
for identifying component terms that can serve as
unique designators can be called group uniqueness.
For example, although the component term TOOTH
appears in different path specifications, whose corre-
sponding internal nodes are not pairwise equivalent
? e.g.
EMBRYO.ORGAN SYSTEM.VISCERAL
ORGAN.ALIMENTARY SYSTEM.ORAL
REGION.LOWER JAW.TOOTH
EMBRYO.ORGAN SYSTEM.VISCERAL
ORGAN.ALIMENTARY SYSTEM.ORAL
REGION.UPPER JAW.TOOTH
where LOWER JAW does not co-specify with UPPER
JAW, the pairwise different nodes may correspond to
structures whose anatomical/developmental proper-
ties can, in the context of gene expression, be con-
sidered the same. This notion of group uniqueness
was found to hold of nine component terms, cover-
ing approximately 260 of the remaining nodes.
Before moving from individual component terms
that turn out to be unique designators for anatomi-
cal structures, to short sequences of such terms, we
need to point out a separate problem in actually us-
ing them in a user interface. The problem follows
from a design decision made in the development of
these anatomical nomenclatures that supports their
intended use by biologists as clear and succinct
structural descriptions of an embryo. Specifically,
while an individual component term may turn out
to be a unique specifier with respect to the Nomen-
clature, outside the context of its tree path, it may
not signify to a biologist what it is intended to. For
example, while the term LOOP has been found to
uniquely denote the same anatomical structure as
EMBRYO.ORGAN SYSTEM.VISCERAL ORGAN.
ALIMENTARY SYSTEM.GUT.MIDGUT.LOOP
a biologist would never simply use ?loop? to refer
to the the loop of the midgut. Similarly, while the
term DISTAL uniquely designates the same anatom-
ical structure as
EMBRYO.LIMB.FORELIMB.JOINT.
RADIO-ULNARJOINT.DISTAL
?distal? on its own is not how any biologist would
refer to the joint of the radius and ulna bones that is
furthest from the shoulder.
For the interface we are developing, we need to
replace these albeit unique component terms with
phrases that are more natural to use in specifying
these structures.
Turning now to component terms that are not
unique in any of the senses discussed so far, it still
does not appear to be the case that a user need en-
ter an entire path specification to refer to its associ-
ated anatomical structure. In many cases, a sub-path
specification of two, or in some cases, three compo-
nent terms appears sufficient to specify the anatom-
ical structure of interest.
To find where shorter sub-paths would serve as
a source of unique designators, we enumerated all
sub-paths from nodes with a non-unique associated
component term (either leaf or internal node) to
the root of their corresponding Theiler Stage tree
(i.e., paths being specified in child-parent order).
This revealed many cases where a unique two- or
three-component path specification would disam-
biguate an otherwise ambiguous term, and allowed
us to cover another 156 component terms via the 2-
component terms and 50 via the 3-component terms.
This has left only 58 of the original 1416 component
terms (and 1159 corresponding nodes in the Nomen-
clature out of the original 13737) for us to inves-
tigate other methods of finding unique designators
for.
The question is what Natural Language phrases
these multi-component terms correspond to, since it
is such phrases that would be used in an interface,
not sequences of component terms. Slight varia-
tions in what the parent-child relations correspond
to mean there are three different phrasal patterns for
two-component sub-paths: (1) In cases where the
child and parent nodes are in a part-whole relation
and both are realised as nouns ? e.g., a child with
component term CAPSULE descending from a par-
ent LENS, or a parent CORTEX or a parent OVARY
EMBRYO.ORGANSYSTEM.SENSORYORGAN
.EYE.LENS.CAPSULE
EMBRYO.ORGANSYSTEM.VISCERAL ORGAN.
RENAL/URINARYSYSTEM.METANEPHROS.
EXCRETORY COMPONENT.CORTEX.
CAPSULE
EMBRYO.ORGAN SYSTEM.VISCERAL ORGAN.
REPRODUCTIVE SYSTEM.FEMALE.
OVARY.CAPSULE
the multi-component term can be realised as a
phrase CHILD OF PARENT, generating the three
uniquely specifying phrases ?capsule of lens?, ?cap-
sule of cortex? and ?capsule of ovary?. Alterna-
tively, a natural phrase of the form PARENT CHILD,
(i.e. ?lens capsule?, ?cortex capsule? and ?ovary
capsule? can also be constructed as a natural way
of describing the anatomical structure that the path
denotes.
(2) In cases where the child and parent nodes are
in a part-whole relation, but the component term as-
sociated with the child is an adjective such as LEFT,
UPPER or ANTERIOR, then the pattern CHILD PAR-
ENT can be used to form an appropriate phrase. For
example, the path specification
EMBRYO.ORGANSYSTEM.CARDIOVASCULAR
SYSTEM.VENOUSSYSTEM.VENACAVA.
INFERIOR
can be accessed by the phrase ?inferior vena cava?.
(3) In cases where the child and parent nodes are
in a set-instance relation, as in the case of
EMBRYO.ORGAN SYSTEM.VISCERAL ORGAN.
ALIMENTARY SYSTEM.ORAL REGION.
GLAND.PITUITARY
again the pattern CHILD PARENT can be used to
form an appropriate phrase ? for example ?pituitary
gland? in this case. Phrases thus formed from multi-
component sub-paths may again be unique with re-
spect to an interval of Theiler Stages, or with respect
to lineage within the Theiler Stages, or with respect
to a group.
4 Improving the Display of Search Results
Currently, within the interface to the Gene Expres-
sion Database, one can search for an anatomical
structure of interest within a single tree or across all
stages. A query across all Theiler Stages (TS) results
in a list of all stages with a matching component, and
associated with each stage is a path specification ter-
minating at a matching component name. This does
not easily enable the user to locate the specific entity
they are interested in. If the term is not unique, then
the results contain all possible anatomical structures
the query could represent. For example, a sub-string
match on the phrase ?lumen? results in
4 TS12 term(s) matching query ?lumen?:
future spinal cord;neural tube;neural lumen
foregut diverticulum;lumen
hindgut diverticulum;lumen
midgut;lumen
5 TS13 term(s) matching query ?lumen?:
future spinal cord;neural tube;neural lumen
foregut diverticulum;lumen
hindgut diverticulum;lumen
midgut;lumen
foregut-midgut junction;lumen
7 TS14 term(s) matching query ?lumen?:
future spinal cord;neural tube;neural lumen (future
spinal canal, spinal canal)
hindgut diverticulum;lumen
midgut;lumen
foregut-midgut junction;lumen
rest of foregut;lumen
foregut;pharyngeal region;lumen
otic pit;lumen
10 TS15 term(s) matching query ?lumen?:
optic recess (lumen of optic stalk)
future spinal cord;neural tube;neural lumen (future
spinal canal, spinal canal)
hindgut diverticulum;lumen
midgut;lumen
pharynx;lumen
foregut-midgut junction;lumen
hindgut;lumen
rest of foregut;lumen
foregut;oesophageal region;lumen
otic pit;lumen
Locating an entity of interest amongst all these tree
paths can be an arduous task. Even if the term is
unique, the same entry will be repeated across mul-
tiple stages, leading to a visual search problem.
An alternative, cleaner way of presenting search
results is to take the matching component terms as
the primary display key and associate it with a list
of stages where its corresponding path specification
occurs. For non-unique search queries such as the
example above, this displays as
future spinal cord; neural tube; neural lumen:
Stages 12, 13, 14, 15, ...
foregut diverticulum; lumen:
Stages 12, 13
hindgut diverticulum; lumen:
Stages 12, 13, 14, 15, ...
midgut;lumen:
Stages 12, 13, 14, 15, ...
foregut-midgut junction; lumen:
Stages 13, 14, 15, ...
rest of foregut; lumen:
Stages 14, 15, ...
otic pit; lumen:
Stages 14, 15, ...
optic recess:
Stages 15, ...
pharynx; lumen:
Stages 15, ...
foregut; oesophageal region; lumen:
Stages 15, ...
foregut; pharyngeal region; lumen:
Stages 14
We will, of course, have to verify that this form
of display better facilitates users finding the struc-
ture(s) and stage(s) of interest to them.
5 Increasing Precision
The introduction of phrases based on more than
one component term within the Mouse Anatomy
Nomenclature significantly reduces the number of
irrelevant matches compared to searches based on
a single comonent term.
To continue our example with CAPSULE from
Section 3, the current situation is that no results
will be found if ?cortex capsule? is entered as a
search query. If the user then simply searches for
?capsule? across all stages, 31 instances will be re-
turned. However, although all sub-paths leading to
CORTEX.CAPSULE are returned, the other 87% of
the results are irrelevant to the user?s intention. If
the multi-component terms are included as uniquely
designating replacements of existing terms, 100%
recall would be maintained, while increasing preci-
sion to 100% percent.
As there is some systematicity involving parent
and child component terms, these terms can be auto-
matically generated. Within the nomenclature there
are three typical patterns. These can be formalised
as:
  the child being a descriptor of the parent e.g.
superior vena cava
  the child being a part of the parent e.g. ovary
capsule or capsule of ovary
  the child being a member of the parent set e.g.
pituitary gland.
Of course, recognising which tree path belongs to
which of the patterns above requires biological ex-
pertise, but once identified new terms can be gener-
ated that are more likely to be used naturally to re-
fer to the relevant anatomical components. Once in
place these new terms can increase recall, with high
precision.
6 Increasing Recall
While the Mouse Anatomical Nomenclature was de-
signed to specify every anatomical structure within
the developing mouse embryo, it does not contain
all the terms that developmental biologists might use
to refer to anatomical entities. Although some syn-
onyms have been explicitly recorded in the nomen-
clature, no attempt has been made to be exhaustive.
In order to increase the recall of user searches
for anatomical structures, we have undertaken to in-
crease the number and range of synonyms for ele-
ments of the nomenclature, by semi-automatically
Figure 2: Schematic of a Theiler Stage 20 embryo
analysing texts likely to contain terms related to the
developmental anatomy of the mouse.
To demonstrate the potential value of this ap-
proach, we first manually reviewed the short tex-
tual description that accompanies each Theiler stage
within the Mouse Atlas, highlighting the main fea-
tures of the stage ? for example, this text accompa-
nies the schematic of TS20 (Figure 6):
The handplate (anterior footplate) is no
longer circular but develops angles which
correspond to the future digits. The poste-
rior footplate is also distinguishable from
the lower part of the leg. It is possible
to see the pigmentation of the pigmented
layer of the retina through the transparent
cornea. The tongue and brain vesicles are
clearly visible.
We collected all the noun phrases (NPs) that could
potentially refer to an anatomical structure and
found, within the  1400 words comprising the de-
scriptions, 25 anatomical terms that were not in-
cluded in the nomenclature either as component
terms or as synonyms for component terms. Since
the same people wrote these textual descriptions as
developed the Nomenclature, it shows how difficult
it is to record all terms used for anatomical structures
without systematic effort.
To support such a systematic effort, we have
been applying basic text analysis software to
a textbook on developmental anatomy (Kaufman
and Bard, 1999), including a tokenizer, part-of-
speech tagger and NP chunker, the latter two from
the Language Technology Group (LTG) in Edin-
burgh (http://www.ltg.ed.ac.uk), as well as addi-
tional scripts ? in order to identify noun phrases,
from which we then extract those most likely to re-
fer to an anatomical structure. The latter have then
been discussed with our domain expert, Davidson.
Because neither the POS tagger nor the chun-
ker were specially trained for this type of techni-
cal text, their performance was rather weak. Chun-
ker output from the chapter on the heart, produced
5547 phrases, of which 2465 were considered to be
NPs. 2.4% (i.e. 74) of the 3082 claimed non-NPs
were obvious false negatives involving the terms ve-
nacava, septum primum/secundum and ductus arte-
riosus/venous. Of the terms classified as NPs, 3.7%
(i.e. 92) were found to be false positives. Most of
these errors involved words that could be classed as
verbs or nouns adjacent to true NPs, regarded as plu-
ral nouns but which, in context, were acting as verbs,
e.g. the ostium secundum forms.
From the true NPs, we removed pronouns, num-
bers, authors names, and plural terms whose singu-
lar was also present. This left 451 NPs, of which
82 were found to be exact matches for component
terms and eight, for the synonyms in the Nomencla-
ture. These were also removed, leaving 361 possible
anatomical terms not found in the Nomenclature.
We then used a common technique to reduce this
set by only considering NPs headed by or modified
by a frequent head or modifier from within the set of
component terms (Bodenreider et al, 2002). Here,
frequent meant   3 times. For example, CAROTID,
FIBROUS and ENDOCARDIAL are frequent modi-
fiers, while ARTERY, SEPTUM and TISSUE are fre-
quent head nouns. Of the 361 remaining NPs from
the heart chapter, 115 shared a high frequency head
noun with terms already in the Nomenclature, while
105 shared a high frequency modifier with terms in
the Nomenclature. We considered these 220 NPs
probable anatomical terms, with the remaining 141
being possible anatomical terms. These two sets are
now being reviewed to identify which are synonyms
for anatomical structures identified in the Nomen-
clature, which denote structures or groups of struc-
tures that have not be recorded in the Nomenclature,
which are reduced co-referring NPs, and which are
not anatomical terms after all.
7 Future Work
One result of this work has been to catch both
structural and terminological inconsistencies in the
Mouse Anatomical Nomenclature because our ex-
tractions allow biologists to easily see differences
between one branch and another or between one tree
and another in the Nomenclature.
With respect to an enhanced interface to the gene
expression data, we are now ready to take the results
of our analyses and use them to provide a potentially
more effective way of searching for relevant anatom-
ical structures and displaying the results. We have a
method to mine for additional synonyms for anatom-
ical terms, that we will apply to additional texts, ide-
ally after re-training the POS-tagger and chunker to
better reflect the types of texts we are dealing with.
Similar Nomenclatures of developmental
anatomy exist for other model organisms, including
drosophila, zebrafish and human. These too will
be used to index gene expression data for these
organisms, eventually supporting cross-species
comparison of gene expression patterns and further
understanding of development. So we believe that
developmental anatomy provides a rich domain
in which to apply (and learn to extend) Natural
Language tools and techniques.
Acknowledgements
The authors would like to thank Dr. Jonathan
Bard for supplying us with electronic versions of
two chapters from The Anatomical Basis of Mouse
Development and answering many of our questions.
References
Olivier Bodenreider, Thomas Rindflesch, and Anita Bur-
gun. 2002. Unsupervised, corpus-based method for
extending a biomedical terminology. In Proceed-
ings of the ACL Workshop on Biomedical Language,
Philadelphia PA.
Matthew H. Kaufman and Jonathan Bard. 1999. The
anatomical basis of mouse development. Academic
Press.
Grounding spatial named entities for information extraction
and question answering
Jochen L. Leidner Gail Sinclair Bonnie Webber
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
Scotland, UK
jochen.leidner@ed.ac.uk, csincla1@inf.ed.ac.uk, bonnie@inf.ed.ac.uk
Abstract
The task of named entity annotation of unseen
text has recently been successfully automated
with near-human performance.
But the full task involves more than annotation,
i.e. identifying the scope of each (continuous)
text span and its class (such as place name). It
also involves grounding the named entity (i.e.
establishing its denotation with respect to the
world or a model). The latter aspect has so far
been neglected.
In this paper, we show how geo-spatial named
entities can be grounded using geographic co-
ordinates, and how the results can be visual-
ized using off-the-shelf software. We use this
to compare a ?textual surrogate? of a newspa-
per story, with a ?visual surrogate? based on
geographic coordinates.
1 Introduction
The task of named entity annotation of unseen text
has recently been successfully automated, achiev-
ing near-human performance using machine learning
(Zheng and Su, 2002). But many applications also re-
quire grounding ? i.e., associating each classified text
span with a referent in the world or some model thereof.
The current paper discusses spatial grounding of named
entities that may be referentially ambiguous, using a min-
imality heuristic that is informed by external geographic
knowledge sources. We then apply these ideas to the cre-
ation of ?visual surrogates? for news articles.
This paper is structured as follows: Section 2 discusses
how spatial named entities can be grounded and how this
interacts with their extraction and applications. Section
3 describes a geo-spatial resolution algorithm. Section 4
shows how maps can be automatically constructed from
named-entity tagged newswire text using resolved place
names, hence introducing a new, graphical document sur-
rogate. Section 5 deals with the usefulness of grounded
named entities for question answering. Section 6 presents
some related work, and Section 7 concludes this paper.
2 Spatial Grounding
Gazetteers are large lists of names of geographic entities,
usually enriched with further information, such as their
class (e.g., town, river, dam, etc.), their size, and their
location (i.e. with respect to some relative or absolute
coordinate system such as longitude and latitude).
Appendix A identifies some publicly available sources.
UN-LOCODE is the official gazetteer by the United
Nations; it is also freely available from the UNECE Web
site1 and contains more than 36 000 locations in 234
countries (UNECE, 1998). The Alexandria Gazetteer
(Smith et al, 1996; Frew et al, 1998) is another database
of geographical entities, including both their coor-
dinates and relationships such as: in-state-of,
in-province-of, in-county-of,
in-country-of, in-region-of, part-of
and formerly-known-as.
To date, Named Entity Recognition (NER) has only
used gazetteers as evidence that a text span could be some
kind of place name (LOCATION), even though their finite
nature makes lists of names of limited use for classifica-
tion (Mikheev et al, 1999). Here we use them for spatial
grounding ? relating linguistic entities of subtype LOCA-
TION (Grishman and Sundheim, 1998) to their real-world
counterparts.
?World Atlases? and the gazetteers that index them are
not the only resources than can be used for grounding
spatial terms. In biomedicine, there are are several
brain atlases of different species, using various different
techniques, and focussing on both normal and disease
state; as well as a digital atlas of the human body
1 http://www.unece.org/cefact/locode/service/main.htm
Figure 1: Grounding an XML Ontology in Voxels: The
Mouse Atlas (Baldock et al, 1999).
based on data from the Visible Human project. Such
atlases and the nomenclatures that label their parts,
provide an important resource for biomedical research
and clinical diagnosis. For example, the Mouse Atlas
(Ringwald et al, 1994) comprises a sequence of 3D
(volumetric) reconstructions of the mouse embryo in
each of its 26 Theiler States of development. Indexing
it is an part-of hierarchy of anatomical terms (such
as embryo.organsystem.cardiovascularsystem-
.heart.atrium), called the Mouse Anatomical
Nomenclature (MAN). Each term is mapped to one or
more sets of adjacent voxels2 that constitute the term?s
denotation in the embryo. Figure 1 illustrate this linkage
(using 2D cross-sections) in the EMAGE database.3
Just as one might find it useful for information extrac-
tion or question answering to ground grographic terms
found in previously unseen text, one may also find it use-
ful to ground anatomical terms in previously unseen text.
One example of this would be in providing support for the
curation of the Gene Expression Database (GXD).4 This
support could come in the form of a named entity recog-
nizer for anatomical parts in text, with grounding against
the Mouse Atlas, using the gazetteer-like information in
the MAN.
So what is the relationship between a place name
gazetteer like UN-LOCODE and the Mouse Atlas? The
MAN is structured in a similar part-of hierarchy to
that of geographical locations:
USA embryo
California organ system
San Mateo County cardiovascular system
Redwood City heart
Because both gazetteers like UN-LOCODE and biomed-
ical atlases like the Mouse Atlas provide spatial ground-
ing for linguistic terms (Figure 2), both can be used to
reason about spatio-temporal settings of a discourse, for
instance, to resolve referential ambiguity.
2 Pixels are points in the 2D plane   x, y  ; voxels are 3D gener-
alizations of pixels   x  y  z  .
3 http://genex.hgu.mrc.ac.uk/Emage/database/intro.html
4 http://genex.hgu.mrc.ac.uk/Resources/GXDQuery1/
3 Place-Name Resolution for Information
Extraction
There are many places that share the same (Berlin, Ger-
many  Berlin, WI, USA) or similar names (York, UK
 New York, USA), usually because historically, the
founders of a new town had given it a similar or the same
name as the place they emigrated from.
When ambiguous place names are used in conversa-
tion or in text, it is usually clear to the hearer what spe-
cific referent is intended. First, speaker and hearer usu-
ally share some extra-linguistic context and implicitly ad-
here to Grice?s Cooperative Principle and the ?maxims?
that follow, which require a speaker to provide more iden-
tifying information about a location that the recipient is
believed to be unfamiliar with. Secondly, linguistic con-
text can provide clues: an accident report on the road
between Perth and Dundee promotes an interpretation of
Perth in Scotland, while an accident on the road between
Perth and Freemantle promotes an interpretation of Perth
in Western Australia. Computers, which are bound to se-
lect referents algorithmically, can exploit linguistic con-
text more easily than extra-linguistic context, but even the
use of linguistic context requires (as always) some subtle
heuristic reasoning.
Grounding place names mentioned in a text can sup-
port effective visualization ? for instance, in a multimedia
document surrogate that contains textual, video and map
elements (e.g. in a question answering scenario), where
we want to ensure that the video shows the region and the
map is centered around the places mentioned.
To make use of linguistic context in resolving am-
biguous place names, we apply two different minimal-
ity heuristics (Gardent and Webber, 2001). The first we
borrow (slightly modified) from work in automatic word
sense disambiguation (Gale et al, 1992), calling it ?one
referent per discourse?. It assumes that a place name
mentioned in a discourse refers to the same location
throughout the discourse, just as a word is assumed to
be used in the same one sense throughout the discourse.
Neither is logically necessary, and hence both are simply
interpretational biases.
The second minimality heuristic assumes that, in cases
where there is more than one place name mentioned in
some span of text, the smallest region that is able to
ground the whole set is the one that gives them their inter-
pretation.5 This can be used to resolve referential ambi-
guity by proximity: i.e., not only is the place name Berlin
taken to denote the same Berlin throughout a discourse
unless mentioned otherwise,6 but so does a Potsdam men-
5 Probably the smaller the span, the more often valid will this
heuristic be.
6 This paper is a rare exception due to its meta-linguistic na-
ture.
Gazetteer Named Entity Grounding Real World
UN-LOCODE Novosibirsk  55?02  N; 82?55  E  (Novosibirsk)
(longitude/latitude)
Mouse Atlas atrium  345, 2, 345  ; 	
		 (part of the heart)
(set of voxels)
Figure 2: Comparison between Spatial Grounding in UN-LOCODE and the Mouse Atlas.
A?
A??
C
D
E
F
G
HI
J
K
B
Places mentioned in discourse
Potential Referents
Places NOT mentioned in discourse
Figure 3: A Place-Name Resolution Method.
tioned together with a Berlin uniquely select the capital
of Germany as the likely referent from the set of all can-
didate Berlins.7
To illustrate this ?spatial minimality? heuristic, con-
sider Figure 3: Assume that a mention of place A in a
text could either refer to A

or A
 
. If the text also con-
tains terms that ground unambiguously to I, J, and K, we
assume the referent of A is A

rather than A
 
because the
former leads to a smaller spatial context.
To use this ?spatial minimality? heuristic, we start by
extracting all place names using a named entity recog-
nizer. We then look up the ?confusion set? of potential
referents for each place name, e.g. for Berlin:  Berlin,
FRG (German capital); Berlin, WI, USA; Berlin, NJ,
USA; Berlin, CT, USA; Berlin, NH, USA; Berlin, GA,
USA; Berlin, IL, USA; Berlin, NY, USA; Berlin, ND,
USA; Berlin, NJ, USA  . Each member of the set of po-
tential referents is associated with its spatial coordinates
(longitude/latitude), using a gazetteer. We then compute
the cross-product of all the confusion sets. (Each mem-
ber of the cross-product contains one potential referent
for each place name, along with its spatial coordinates.)
For each member of the cross-product, we compute the
area of the minimal polygon bounding all the potential
referents, and select as the intended interpretation, the
one with the smallest area.8 The resulting behaviour is
7 despite the fact that most places named Berlin are in the
United States
8 One can approximate this, either by computing the sum of
 Berlin; Potsdam  Berlin, FRG (Germany)
 Fairburn; Berlin  Berlin, WI, USA
 West Berlin; Bishops; Dicktown  Berlin, NJ, USA
 Kensington; Berlin; New Britain  Berlin, CT, USA
 Copperville; Berlin; Gorham  Berlin, NH, USA
 Moultrie; Berlin  Berlin, GA, USA
 Berlin; Prouty  Berlin, IL, USA
 Berlin; Berlin Center; Cherryplain  Berlin, NY, USA
 Medberry; Berlin  Berlin, ND, USA
Figure 4: Spatial Reference Resolution Using Spatial
Minimality.
shown in Figure 4: depending on contextually mentioned
other places, a different Berlin is selected.
The value of this heuristic needs to be assessed quanti-
tatively against various types of text.
In resolving anatomical designators in text, we may
employ a variation of the spatial minimality heuristic,
based on the fact that no listing will ever be complete
with respect to all the existing or new-minted synonyms
for anatomical terms.
When grounding the anatomical terms in the text
In subsequent stages until birth, cytokeratin 8
continues to be expressed in embryonic taste
buds distributed in punctuate patterns at regu-
lar intervals along rows that are symmetrically
located on both sides of the median sulcus in
the dorsal anterior developing tongue.
we find no median sulcus within the MAN, only alveo-
lar sulcus, optic sulcus, pre-otic sulcus, sulcus limitans
and sulcus terminalis. We just assume that all anatomi-
cal terms refer to previously recognized anatomical enti-
ties, just as we assume that all geographic terms refer to
existing geographic entities and not, for example, some
new town called ?Berlin? or ?London? that is not yet in
the gazetteer. Hence median sulcus is assumed to be a
synonym for one of the five sulci given in the MAN. At
this point, we can invoke the spatial minimality heuris-
tic, looking for the minimal bounding space that includes
tongue and one of the five sulci, here yielding ?sulcus
terminalis?. Thus the spatial minimality heuristic is here
pairwise point-point distances, or symbolically, using a hierar-
chical gazetteer?s relations, such as in-region-of.
used with other assumptions to resolve missing or previ-
ously unseen terms.
4 Visualization of Geo-Spatial Aspects in
Narrative
The usefulness of visual representations to
convey information is widely recognized (cf.
(Larkin and Simon, 1987)). Here, we use the grounding
of named entities in news stories to create a visual
surrogate that represents their ?spatial aboutness?.
Two news stories were selected from online newspa-
pers on the same day (2003-02-21): one story (Appendix
B) reports the tragic death of a baby from London in a
Glasgow hospital despite flying it to a Glasgow special-
ist hospital in the Royal aircraft (BBC News, 2003), and
the other report (Appendix C) describes the search of the
Californian police for a pregnant women from Modesto,
CA, who has disappeared (The Mercury News, 2003).
We use the term ?surrogate? to refer to a partial view
of a text (e.g. (Leidner, 2002)). Figure 5 shows a tex-
tual surrogate in the form of all place names found in a
text: an analyst who wants to get a quick overview about
the locations involved in some item of news reportage, to
decide its local interest or relevance, might find such a
surrogate helpful, although the source would still have to
be skim-read.
Story A
... Scotland ... Tooting ... London ... Glasgow ... London
... Glasgow ... Northolt ... Glasgow ... Britain ... Prestwick
... Tooting ... Glasgow ... UK ... (Glasgow) ...
Story B
Modesto ... (Southern California) ... (Modesto) ... Los
Angeles ... Sacramento ... Berkeley (Marina) ... Fresno
... Oakland ... Modesto ... Los Angeles ... Southern
California ... Modesto ... Southern California ... New York
... Long Island ...
Figure 5: A Textual Geo-Spatial Document Surrogate for
the Stories in Appendices B and C.
We now compare this ?baseline? textual surrogate to a
graphical map representation that draws on the algorithm
introduced before. Our simple visualisation method com-
prises the following components (Figure 6): an (open-
domain) news item is fed into locotagger, our sim-
ple named entity tagger for place names based on UN-
LOCODE.9 It recognises location names, resolves mul-
tireferential place names and looks up the coordinates:
9 For the experiment reported here, we also used data from
http://www.astro.com/cgi/aq.cgi?lang=e.
Named Entity
Tagging
Generator
Map
Newswire Text
Graphical Map
Longitute/
LookupLatitude
Placename
Resolution
Figure 6: System Architecture.
Scott, more than a dozen news crews
from <ENAMEX type="LOCATION" longitude=
"-118.25" latitude="34.05">Los
Angeles</ENAMEX> to <ENAMEX type=
"LOCATION" longitude="121.5"
latitude="38.583333">Sacramento
</ENAMEX> camped out front.
From the text we obtain a vector of types of all spatial
named entities with their frequency of occurrence in the
text:









UK : 1
Scotland : 1
Tooting : 2
London : 2
Glasgow : 5
Northolt : 2
Prestwick : 1
Britain : 1








Annotation and Data Mining of the Penn Discourse TreeBank
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104 USA
rjprasad@linc.cis.upenn.edu
Eleni Miltsakaki
University of Pennsylvania
Philadelphia, PA 19104 USA
elenimi@linc.cis.upenn.edu
Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
joshi@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
The Penn Discourse TreeBank (PDTB) is a new re-
source built on top of the Penn Wall Street Journal
corpus, in which discourse connectives are anno-
tated along with their arguments. Its use of stand-
off annotation allows integration with a stand-off
version of the Penn TreeBank (syntactic structure)
and PropBank (verbs and their arguments), which
adds value for both linguistic discovery and dis-
course modeling. Here we describe the PDTB and
some experiments in linguistic discovery based on
the PDTB alone, as well as on the linked PTB and
PDTB corpora.
1 Introduction
Large scale annotated corpora such as the Penn
TreeBank (Marcus et al, 1993) have played a cen-
tral role in speech and natural language research.
However, with the demand for more powerful NLP
applications comes a need for greater richness in
annotation ? hence, the development of PropBank
(Kingsbury and Palmer, 2002), which adds basic se-
mantics to the PTB in the form of verb predicate-
argument annotation and eventually similar annota-
tion of nominalizations. We have been developing
yet another annotation layer above these both. The
Penn Discourse TreeBank (PDTB) adds low-level
discourse structure and semantics through the anno-
tation of discourse connectives and their arguments,
using connective-specific semantic role labels. With
this added knowledge, the PDTB (together with the
PTB and PropBank) should support more in-depth
NLP research and more powerful applications.
Work on the PDTB is grounded in a lexical-
ized approach to discourse ? DLTAG (Webber and
Joshi, 1998; Webber et al, 1999a; Webber et al,
2000; Webber et al, 2003). Here, low-level dis-
course structure and semantics are taken to re-
sult (in part) from composing elementary predicate-
argument relations whose predicates come mainly
from discourse connectives1 and whose arguments
1Despite this, we have deliberately adopted a policy of hav-
come from units of discourse ? clausal, sentential
or multi-sentential units. The PDTB therefore dif-
fers from the RST-annotated corpus (Carlson et al,
2003) which starts with (abstract) rhetorical rela-
tions (Mann and Thompson, 1988) and annotates a
subset of the Penn WSJ corpus with those relations
that can be taken to hold between (primarily) pairs
of discourse spans identified in the corpus.
The current paper focuses on what can be dis-
covered through analyzing PDTB annotation, both
on its own and together with the Penn TreeBank.
Section 2 of the paper briefly reviews the theo-
retical background of the project, its current state,
the guidelines given to annotators, the annotation
tool they used (WordFreak), and the extent of inter-
annotator agreement. Section 3 shows how we have
used PDTB annotation, along with the PTB, to ex-
tract several features pertaining to discourse con-
nectives and their arguments, and discusses the rel-
evance of these features for NLP research and ap-
plications. Section 4 concludes with the summary.
2 Project overview
2.1 Theoretical background
The PDTB project builds on basic ideas presented
in Webber and Joshi (1998), Webber et al (1999b)
and Webber et al (2003) ? that connectives are
discourse-level predicates which project predicate-
argument structure on a par with verbs at the sen-
tence level. Webber and Joshi (1998) propose a
tree-adjoining grammar for discourse (DLTAG) in
which compositional aspects of discourse meaning
are formally defined, thus teasing apart composi-
tional from non-compositional layers of meaning.
In this framework, connectives are grouped into nat-
ural classes depending on the structure that they
project at the discourse level. Subordinate and coor-
dinating conjunctions, for example, require two ar-
ing the annotations independent of the DLTAG structural de-
scriptions for two reasons: (1) to make the annotated cor-
pus useful to researchers working in different frameworks and
(2) to simplify the annotators? task, thereby increasing inter-
annotator reliability.
guments that can be identified structurally from ad-
jacent units of discourse. What Webber et al (2003)
call anaphoric connectives (discourse adverbials,
such as otherwise, instead, furthermore, etc.) also
require two arguments ? one derived structurally,
and the other derived anaphorically from the pre-
ceding discourse. The crucial contribution of this
framework to the design of PDTB is what can be
seen as a bottom-up approach to discourse structure.
Specifically, instead of appealing to an abstract (and
arbitrary) set of discourse relations whose identifi-
cation may confound multiple sources of discourse
meaning, we start with the annotation of discourse
connectives and their arguments, thus exposing a
clearly defined level of discourse representation.
2.2 Project description
The PTDB project began in November 2002. The
first phase, including pilot annotations and prelim-
inary development of guidelines, was completed in
May 2003, and we expect to release the PDTB by
November 2005. Intermediate versions of the an-
notated corpus will be made available for feedback
from the community.
The PDTB corpus will include annotations of
four types of connectives: subordinating and co-
ordinating conjunctions, adverbial connectives and
implicit connectives. The final number of annota-
tions will amount to approximately 30K: 20K anno-
tations of the 250 types explicit connectives identi-
fied in the corpus and 10K annotations of implicit
connectives. The final version of the corpus will
also characterize the semantic role of each argu-
ment.
To date, we have annotated 10 explicit connec-
tives (therefore, as a result, instead, otherwise, nev-
ertheless, because, although, even though, when, so
that), amounting to a total of 2717 annotations, as
well as 386 tokens of implicit connectives. Anno-
tations have been performed by two to four annota-
tors.
2.3 Annotation guidelines
The annotation guidelines for PDTB have
been revised considerably since the pilot
phase of the project in May 2003. The cur-
rent version of the guidelines is available at
http://www.cis.upenn.edu/   pdtb. Below
we outline basic points from the guidelines.
What counts as a discourse connective? We
count as discourse connectives (1) all subordinat-
ing and coordinating conjunctions, (2) all discourse
adverbials, and (3) all inter-sentential implicit con-
nectives. Discourse adverbials include only those
adverbials which convey relationships between two
abstract objects such as events, states, propositions,
etc. (Asher, 1993). For instance, in Example 1, as
a result conveys a cause-effect relation between the
event of limiting the size of industries and that of
industries operating out of small, expensive, and in-
efficient units. In contrast, the semantic interpreta-
tion of the clausal adverbial strangely in Example 2
only requires a single event/state which it classifies
in the set of strange events/states.2
(1) [In the past, the socialist policies of the govern-
ment strictly limited the size of new steel mills,
petrochemical plants, car factories and other in-
dustrial concerns to conserve resources and re-
strict the profits businessmen could make]. As
a result, [industry operated out of small, expen-
sive, highly inefficient industrial units].
(2) Strangely, conventional wisdom inside the Belt-
way regards these transfer payments as ?uncon-
trollable? or ?nondiscretionary.?
Implicit connectives are taken to occur between
adjacent sentences not related by any explicit con-
nective. They are annotated with whatever explicit
connective the annotator feels could be inserted,
with the original meaning retained. Assessment of
inter-annotator agreement groups these annotations
into five coarse classes (Miltsakaki et al, 2004).
Currently, we are not annotating implicit connec-
tives intra-sententially (such as between a main
clause and a free adjunct) or across paragraphs.
What counts as a legal argument? The sim-
plest argument to a connective is what we take to
be the minimum unit of discourse. Because we
take discourse relations to hold between abstract
objects, we require that an argument contain at least
one clause-level predication (usually a verb ? tensed
or untensed), though it may span as much as a se-
quence of clauses or sentences. The two exceptions
are nominal phrases that express an event or a state,
and discourse deictics that denote an abstract ob-
ject.
What we describe to annotators as arguments to
discourse connectives are actually the textual span
from which the argument is derived (Webber et al,
1999a; Webber et al, 2003). This is especially clear
in the case of the first argument of instead in (3),
which does not actually include the negation, al-
though it is part of the selected text.3
2For a more detailed discussion of how discourse adver-
bials can be distinguished from clausal adverbials, see Forbes
(2003).
3For a corpus-based study of the arguments of instead, see
(Miltsakaki et al, 2003).
(3) [No price for the new shares has been set]. In-
stead, [the companies will leave it up to the mar-
ketplace to decide].
How far does an argument extend? One par-
ticularly significant addition to the guidelines came
as a result of differences among annotators as to
how large a span constituted the argument of a con-
nective. During pilot annotations, annotators used
three annotation tags: CONN for the connective
and ARG1 and ARG2 for the two arguments. To
this set, we have added two optional tags, SUP1
and SUP2 (supplementary), for cases when the an-
notator wants to mark textual spans s/he considers
to be useful, supplementary information for the in-
terpretation of an argument. Examples (4) and (5)
demonstrate its use. The arguments are shown in
square brackets, while spans providing supplemen-
tary information are shown in parentheses.4
(4) Although [started in 1965], [Wedtech didn?t re-
ally get rolling until 1975] (when Mr. Neuberger
discovered the Federal Government?s Section 8
minority business program).
(5) Because [mutual fund trades don?t take effect un-
til the market close] (? in this case, at 4 p.m. ?)
[these shareholders effectively stayed put].
2.4 Inter-Annotation Reliability
An extensive discussion of inter-annotator reliabil-
ity in the PDTB is presented in (Miltsakaki et al,
2004). The three things that are relevant to the dis-
cussion here of using the PDTB for linguistic dis-
covery are (1) the agreement criterion, (2) the level
of inter-annotator agreement, and (3) the types of
inter-annotator variation.
With respect to agreement, we did not use the
kappa statistic (Siegel and Castellan, 1988) because
it requires the data tokens to be classified into dis-
crete categories and PDTB annotation involves se-
lecting a span of text whose length is not prescribed
a priori.5 Instead of kappa, we assessed inter-
annotator agreement using an exact match crite-
rion: for any ARG1 or ARG2 token, agreement was
recorded as 1 when both annotators made identical
4SUP annotations have not been used in the current
experiments.
5Carlson et al (2003) avoid this by using two sets of cat-
egories: one set in which there is a separate category for each
span that could constitute an elementary discourse unit, and one
set in which there is only a separate category for each span that
at least one annotator has selected. Because the arguments of
connectives tend to be longer and hence more variable than the
elementary spans used in the RST-corpus, we do not see any
gain from introducing the first set of categories, and the second
set is equivalent to our exact match criterion.
textual selections for the annotation and 0 when the
annotators made non-identical selections.
Treating ARG1 and ARG2 annotations as inde-
pendent tokens for assessment, the total number of
inter-annotator judgments assessed for explicit con-
nectives was twice the number of connective tokens,
i.e, 5434. In this measure, we achieved a high-level
of agreement on the arguments to subordinate con-
junctions (92.4%), while lower agreement on ad-
verbials (71.8%).6 This difference between the two
types is not surprising, since locating the anaphoric
(ARG1) argument of adverbial connectives is be-
lieved to be a harder task than that of locating the
arguments of subordinating conjunctions. For ex-
ample, the anaphoric argument of the adverbial con-
nectives may be located in some non-adjacent span
of text, even several paragraphs away.
A detailed analysis of inter-annotator variation
shows that most of the disagreements (79%) in-
volved Partial Overlap ? that is, text that is com-
mon to what is selected separately by each annota-
tor. Partial overlap subsumes categories such as (a)
higher verb, where one of the annotators included
some extra clausal material that contained a higher
governing predicate, (b) dependent clause, where
one of the annotators included extra clausal mate-
rial which was syntactically dependent on the clause
selected by both, and (c) parenthetical, where one
of the annotators included text that occurred in the
middle of the other annotator?s selection. Example 6
illustrates a case of higher verb disagreement.
(6) a. [he knew the RDF was neither rapid nor de-
ployable nor a force] ? even though [it cost
$8 billion or $10 billion a year].
b. he knew [the RDF was neither rapid nor de-
ployable nor a force] ? even though [it cost
$8 billion or $10 billion a year].
The partial overlap disagreements are important
with respect to the experiments described in the next
section, because most of this variation turns out to
be irrelevant to the experiments. We will elaborate
on this further in the next section.
3 Data Mining
PDTB annotation indicates two things: the argu-
ments of each explicit discourse connective and the
lexical tokens that actually play a role as discourse
connectives. It should be clear that the former
6In Miltsakaki et al (2004), we have reported on the anno-
tation of implicit connectives as well. We achieved 72% agree-
ment on the use of explicit expressions in place of the implicit
connectives. More details on the implicit connective annotation
can be found in this work.
cannot be derived automatically from existing re-
sources, since determining the size and location of
the arguments is not simply a matter of sentential
syntax or verb predicate argument relations. But
the latter is also a non-trivial feature because every
lexical item that functions as a discourse connective
also has a range of other functions. While some of
these functions correlate with POS-tags other than
those used in annotating connectives, the PTB POS-
tags themselves cannot always be reliably distin-
guished, given inconsistencies in how the lexical
items are analyzed.
We believe that the PDTB annotation can con-
tribute to a range of linguistic discovery and lan-
guage modeling tasks, such as
 providing empirical evidence for the DLTAG
claim that discourse adverbials get one argu-
ment anaphorically, while structural connec-
tives such as conjunctions establish relations
between adjacent units of text (Creswell et al,
2002).
 acquiring common usage patterns of connec-
tives and identifying their dependencies, in or-
der to support ?natural? choices in Natural
Language Generation (di Eugenio et al, 1997;
Moser and Moore, 1995; Williams and Reiter,
2003).
 developing decision procedures for resolving
and interpreting discourse adverbials (Milt-
sakaki et al, 2003) which can be built on top of
discourse parsing systems (Forbes et al, 2003).
 developing ?word sense disambiguation? pro-
cedures for distinguishing among different
senses of a connective and hence interpret-
ing connectives correctly (e.g., distinguishing
between temporal and explanatory since, be-
tween hypothetical and counterfactual if, be-
tween epistemic and semantic because, etc.)
 providing empirical evidence for theories of
anaphoric phenomena such as verb phrase el-
lipsis that see them as sensitive to the type of
discourse relation in which they are expressed
(Hardt and Romero, 2002; Kehler, 2002).
The value of carrying out such studies using a sin-
gle corpus with multiple layers of annotation is that
relationships between phenomena are clearer. (The
downside is focusing on a single genre ? newspa-
per text ? and a particular ?house style? ? that of
the Wall Street Journal. However, developing the
PDTB may help facilitate the production of more
such corpora, through an initial pass of automatic
annotation, followed by manual correction, much
as was done in developing the PTB (Marcus et al,
1993).)
Here we present some preliminary experiments
we have carried out on the current version of the
PDTB. We automatically extracted features asso-
ciated with discourse connectives and their argu-
ments, both from the PDTB annotation alone as well
as from the integrated annotation of the PDTB and
PTB. The findings reveal novel patterns regarding
the location and size of the arguments of discourse
connectives and suggest additional experiments.
The multi-layered annotations for PDTB, PTB
(and soon to be available PropBank) are rendered in
XML within a ?stand-off? annotation architecture
in which multiple (independently conducted) anno-
tations refer to the same primary document. Word-
Freak directly renders the PDTB annotations in the
stand-off XML representation, but for the syntactic
layer, the PTB phrase structure constituent annota-
tions had to first be converted to the XML stand-off
representation.7
For preparing the connective tokens for data min-
ing, we started with the 2717 annotations for the
10 explicit connectives reported in Section 2.2 and
extracted those tokens on which we achieved full
?exact match? agreement as well as ?partial over-
lap? agreement on both the arguments (cf. Sec-
tion 2.4). We felt justified in combining both sets
because ?partial overlap? disagreements, which oc-
curred mostly within sentences, did not make any
overall difference to the features that were extracted.
The total number of tokens we obtained from this
was 2688. 51 tokens on this set had to be thrown out
since the official release of the Penn TreeBank did
not have the corresponding syntactic annotations for
these tokens.8 From the remaining 2637 tokens, we
extracted two sets of features, one for adverbials
(229 tokens) and the other for subordinating con-
junctions (2408 tokens).
For the adverbials, we wanted to determine
whether the results reported in earlier work
(Creswell et al, 2002) held up. Among other
things, this work examined whether (1) anaphoric
arguments could be reliably annotated, to facili-
tate the development of robust anaphora resolu-
tion algorithms, and (2) there were differences be-
7Thanks to Jeremy Lacivita for implementing the represen-
tation of PTB in stand-off XML form. The stand-off represen-
tation of PTB will be released together with the PDTB corpus.
8Researchers who are currently conducting or are planning
to conduct multi-layered annotations or experiments with the
Penn TreeBank should be aware that the official release con-
tains more source and PoS-tagged files than the parsed files.
Future annotations of the PDTB will only be performed on texts
that are parsed.
tween the type, size and location of the arguments
of anaphoric (adverbial) connectives and those of
structural connectives.
The high inter-annotator agreement reported in
this earlier study has now been confirmed by the
PDTB annotation (cf. Section 2.4). As for the other,
we automatically extracted some of the same fea-
tures that were hand-annotated in Creswell et al
(2002) to determine the distribution of these con-
nectives with respect to their position (POS) and
the size and location (LOC) of their anaphoric argu-
ments. These features are further described below:
POS: pertains to the position of the connective in
its host argument, i.e., the argument in which it oc-
curs.9 POS can take three defined values: INIT for
argument-initial position (Examples 7-9), MED for
argument-medial position (Examples 10-11), and
FINAL for argument-final position (Examples 12
and 13). Note that the host argument of the con-
nective is a sentence in Example 8 and 9, a VP con-
junct in Example 7, a free adjunct in Example 10,
the main clause of a sentence in Example 11, a sub-
ordinate clause in Example 12, and finally, the first
of the two coordinated sentences in Example 13.
LOC: pertains to the size and location of the
anaphoric argument of the connective. LOC can
take four defined values: SS for when the anaphoric
argument occurs in the same sentence as the con-
nective (Examples 7, 10 and 11), PS for when the
argument occurs in the immediately previous sen-
tence (Examples 12 and 13), PP for when the argu-
ment occurs in the immediately preceding sequence
of sentences (Example 8), and NC for when the ar-
gument occurs in some non-contiguous sentence(s)
(Example 9). A sentence is defined as minimally
a main clause and all of its attached subordinate
clauses, if any. Coordinated main clauses, by this
definition, are treated as separate sentences. Note
that according to the definition of the LOC feature,
the anaphoric argument may constitute the entire
sentence(s), as in Examples 8, 9 and 13, or it may be
part of the sentence(s), as in Examples 7 and 10-12.
An important aspect of the LOC feature is that
it involved the multi-layering of PDTB and PTB,
since the PDTB itself contains no information about
syntactic constituency or even sentence boundaries.
For deriving the LOC feature values, we needed in-
formation not only about the sentence boundaries
of texts, but also about coordinated clause bound-
aries, which requires accessing sentence-internal
constituents.
9We achieved 94.1% agreement on the host argument
(ARG2) annotations.
(7) INIT-SS: Rep. John LaFalce (D., N.Y.) said Mr.
Johnson refused [to testify jointly with Mr. Mul-
ford] and instead [asked to appear after the Trea-
sury official had completed his testimony].
(8) INIT-PP: [But Mr. Treybig questions whether
that will be enough to stop Tandem?s first main-
frame from taking on some of the functions that
large organizations previously sought from Big
Blue?s machines. ?The answer isn?t price re-
ductions, but new systems,? he said]. Never-
theless, [Tandem faces a variety of challenges,
the biggest being that customers generally view
the company?s computers as complementary to
IBM?s mainframes].
(9) INIT-NC: [For years, costume jewelry makers
fought a losing battle]. Jewelry displays in de-
partment stores were often cluttered and unin-
spired. And the merchandise was, well, fake.
As a result, [marketers of faux gems steadily lost
space in department stores to more fashionable
rivals ? cosmetics makers].
(10) MED-SS: Investors usually don?t want [to take
physical delivery of a contract], [preferring in-
stead to profit from its price swings and then end
any obligation to take delivery or make delivery
as it nears expiration].
(11) MED-SS: Although [bond prices weren?t as
volatile on Tuesday trading as stock prices],
[traders nevertheless said action also was much
slower yesterday in the Treasury market].
(12) FIN-PS: Buyers can look forward to double-
digit annual returns if [they are right]. But they
will have disappointing returns or even losses if
[interest rates rise] instead.
(13) FIN-PS: [Tons of delectably rotting potatoes,
barley and wheat will fill damp barns across the
land as thousands of farmers turn the state?s buy-
ers away]. [Many a piglet won?t be born] as a re-
sult, and many a ham will never hang in a butcher
shop.
The distribution of the POS feature values across
the different connectives, given in Table 1, shows
that the connectives in this set occurred predomi-
nantly in the initial position of their host argument.
The question of whether or not these different po-
sitions correlate with any aspect of the informa-
tion structure of the arguments (Forbes et al, 2003;
Kruijff-Korbayova? and Webber, 2001) is, however,
an open one and will need to be explored further
with the PDTB annotations.
INIT MED FIN TOTAL
201 (87.8%) 13 (5.7%) 15 (6.5%) 229
Table 1: Distribution of the Position (POS) of Dis-
course Adverbials
CONN SS PS PP NC Total
nevertheless 3 (9.7%) 17 (54.8%) 3 (9.7%) 8 (25.8%) 31
otherwise 2 (11.1%) 14 (77.8%) 1 (5.6%) 1 (5.6%) 18
as a result 3 (4.8%) 44 (69.8%) 5 (7.9%) 12 (19%) 63
therefore 11 (55%) 7 (35%) 1 (5%) 1 (5%) 20
instead 22 (22.7%) 62 (63.9%) 2 (2.1%) 11 (11.3%) 97
TOTAL 41 (17.9%) 144 (62.9%) 12 (5.2%) 33 (14.4%) 229
Table 2: Distribution for Location (LOC) of Anaphoric Argument of Adverbial Connectives
The distribution of the LOC values across the dif-
ferent connectives is shown in Table 2. We first look
at all the connectives taken together (i.e., the final
TOTAL row) and focus on differences in LOC and
what such differences suggest.
The first thing that is evident from the TOTAL
row in Table 2 is the significant proportion of ARG1
tokens that occur in a position non-adjacent to the
discourse adverbial (NC = 14.4%). This accords
with the results in (Creswell et al, 2002), in terms
of providing evidence that discourse adverbials (un-
like structural connectives) are not getting both their
arguments from structurally defined positions.
The second point that is evident from the TOTAL
row is the significant proportion of ARG1 tokens
in SS location. This includes instances of ARG1
in complement clauses (Example 7), subordinate
clauses (Example 11), relative clauses (both restric-
tive and non-restrictive, as in Example 14), pre-
ceding VP conjuncts (Example 15), and from main
clauses, where the adverbial is attached to a free ad-
junct, as in Example 16.
(14) [  The British pound], [pressured by last week?s
resignations of key Thatcher administration of-
ficials], nevertheless [  rose Monday to $1.5820
from Friday?s $1.5795].10
(15) Appealing to a young audience, [he scraps an
old reference to Ozzie and Harriet] and instead
[quotes the Grateful Dead].
(16) [The transmogrified brokers never let the C-word
cross their lips], instead [stressing such terms as
?safe,? ?insured? and ?guaranteed?].
While one might want to argue that the latter is
no different from adjacent full clauses and hence
should be treated the same as a location in the pre-
vious sentence (i.e., LOC=PS), the other SS cases
provide additional evidence for an anaphoric anal-
ysis of these discourse adverbials since there al-
ready exists a separate structural relation in each
case. Furthermore, in Example 7, the arguments of
the conjunction and, though not yet addressed by
our annotators, differ from the arguments of instead.
10The subscripts on the bracketed spans in this example indi-
cate discontinuous parts of the host argument of nevertheless.
Any attempt to treat instead as a structural connec-
tive will produce a syntactic analysis with crossing
branches ? a source of both theoretical and practical
(parsing) problems (Forbes et al, 2003).
Turning now to the individual analysis of adver-
bials, Table 2 shows that the 4 connectives other
than therefore pattern rather similarly with respect
to the location of the anaphoric argument (SS,
PS, PP, NC). All of them except therefore have
their antecedent predominantly in the previous sen-
tence (between 54.8% and 77.8%). The question
is whether the difference in how therefore patterns
? i.e., drawing its antecedent 55% of the time from
the same sentence ? is simply a consequence of hav-
ing such few data points (i.e., only 20) or a matter of
?house style? (with all the examples from the Wall
Street Journal) or a difference that is theoretically
motivated. If the answer lies in house style or the-
ory, then it is relevant to work in natural language
generation. Further annotation and analysis of ad-
verbials and their arguments in the PDTB will pro-
vide more information as to this puzzle.
At the start of this section, we indicated five dif-
ferent areas in which PDTB annotation could con-
tribute to linguistic discovery and language model-
ing. This data mining experiment illustrates the first
three, as well as providing information relevant to
further development of discourse parsing systems
and natural language generation systems. For fu-
ture work, we intend to explore further the extrac-
tion and study of other features related to discourse
adverbials. Two features that we are currently work-
ing to extract automatically pertain to (a) the co-
occurrence of discourse adverbials with other con-
nectives in the host argument, and (b) the syntac-
tic type and depth of the anaphoric arguments, such
as whether the argument was a finite or non-finite
complement clause, a relative clause, or a finite or
non-finite subordinate clause etc.
For the subordinating conjunctions (Table 3), we
extracted features pertaining to the relative position
of the two arguments of the conjunction. Subordi-
nating conjunctions often take their arguments in
the same sentence with the subordinate clause as
one argument and the main clause as its other ar-
gument. However, the subordinate clause can either
occur to the right of the main clause, i.e., postposed,
as in Example 17, or it can occur preposed, i.e., be-
fore the main clause, as in Example 18.
(17) ARG1-ARG2: But Sen. McCain says [Mr.
Keating broke off their friendship abruptly in
1987], because [the senator refused to press the
thrift executive?s case as vigorously as Mr. Keat-
ing wanted].
(18) ARG2-ARG1: Because [Swiss and EC insurers
are widely present on each other?s markets], [the
accord isn?t expected to substantially increase
near-term competition].
The distribution of the relative position of the
arguments of these connectives, given in Table 3,
shows significant differences across the connec-
tives.
CONN ARG1-ARG2 ARG2-ARG1 Total
when 545 (54%) 465 (46%) 1010
because 822 (90%) 93 (10%) 915
even though 77 (75%) 26 (25%) 103
although 129 (37%) 218 (63%) 347
so that 33 (100%) 0 (0%) 33
Total 1606 (67%) 802 (33%) 2408
Table 3: Distribution for Argument order for Subor-
dinating Conjunctions
There are a few interesting things to note here.
First, even if one considers only the four subordi-
nating conjunctions with  100 tokens, no two of
them pattern in the same way.
Second, with when, the almost equal distribution
of preposed and postposed tokens suggests either
free variation of the two patterns or different uses
of the two patterns, with each use favoring a differ-
ent pattern. The latter would accord with a theo-
retical distinction that has been made between post-
posed when expressing a purely temporal relation
between the two clauses, and preposed when ex-
pressing a contingent relation between them (Moens
and Steedman, 1988). Integrated evidence from the
PTB and PropBank may help distinguish the two
possibilities.
Third, there is a striking contrast between the pat-
terning of although and even though, especially if
one assumes that even though (like even when, even
after, even if, etc.) involves application of the topi-
calizer even to the subordinate clause, just as it can
apply to other constituents. Further annotation and
analysis of the PDTB will reveal whether all subor-
dinating conjunctions that co-occur with even pat-
tern like even though, or whether this is specific to
the concessive.
Finally, when Williams and Reiter (2003) exam-
ined 342 texts from the RST annotation of the Penn
TreeBank corpus (Carlson et al, 2003), they re-
ported that 77% of the instances of concessive re-
lations that they examined appeared in the order
ARG2-ARG1. (The eleven instances of although
that they examined and the three instances of even
though appeared in concessive relations, along with
instances of but, despite, however, etc.) If we were
to collapse together all instances of although and
even though annotated in the PDTB (totalling 450),
we would find that 46% (206) patterned as ARG1-
ARG2, and 54% of them (244) patterned as ARG2-
ARG1. This might lead us to draw a similar con-
clusion to Williams and Reiter (2003). But it would
also disguise the fact noted above that although and
even though pattern oppositely to one another. This
suggests (1) that making the feature extraction pro-
cedure specific to particular connectives, as in the
PDTB, will reveal distributional patterns that are
lost when more abstract relations are the focus of the
annotation, and (2) that a larger set of annotated to-
kens can show more reliable distributional patterns.
In sum, data mining of PDTB with respect to sub-
ordinating conjunctions has shown radically differ-
ent distribution patterns regarding the relative po-
sition of the arguments. Some of these have con-
firmed and strengthened previous theoretical claims
and some have suggested new and promising re-
search directions. Further work in this area will also
be extremely relevant for NLG sentence planning
components employing discourse relations (Walker
et al (2003), Stent et al (2004), among others),
where the sentence planner needs to make decisions
regarding cue placement. Finally, while our ap-
proach is ?syntactic?, with the distribution of the
connectives and their arguments being explored in
terms of whether they are subordinating conjunc-
tions, coordinating conjunctions, or adverbial con-
nectives, one can also explore the patterning of
connectives in terms of semantic categories, once
their semantic role annotation is complete (cf. Sec-
tion 2.2). The latter could be especially interesting
to cross-linguistic studies of discourse, as well as
to applications such as multilingual generation and
MT are envisaged.11
4 Summary
In this paper we have presented the Penn Dis-
course TreeBank (PDTB), a large-scale discourse-
level annotated corpus that is being developed to-
wards the creation of a multi-layered annotated cor-
pus, integrating the Penn TreeBank, PropBank and
11We thank an anonymous reviewer for pointing this out.
the PDTB. The PDTB encodes low-level discourse
structure information, marking discourse connec-
tives as indicators of discourse relations, and their
arguments. We have reported high inter-annotator
agreement for the PDTB annotation. Our data min-
ing experience and preliminary results show that the
multi-layered corpora is a rich source of information
that can be exploited towards the development of
powerful and efficient natural language understand-
ing and generation systems as well as towards large-
scale corpus-based research.
Acknowledgments
We are very grateful to Tom Morton and Jeremy
Lacivita for the development and modification of
the WordFreak annotation tool. Special thanks to
Jeremy for providing continuous technical support.
Thanks are also due to our annotators, Cassandre
Creswell, Driya Amandita, John Laury, Emily Paw-
ley, Alan Lee, Alex Derenzy and Steve Pettington.
References
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a Discourse-Tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Jan van Kuppevelt and Ronnie Smith, edi-
tors, Current Directions in Discourse and Dialogue.
Kluwer Academic Publishers.
Cassandre Creswell, Katherine Forbes, Eleni Miltsakaki,
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2002. The Discourse Anaphoric Properties of Con-
nectives. In Proceedings of DAARC2002. Edic?o?es
Colibri.
Barbara di Eugenio, Johanna D. Moore, and Massimo
Paolucci. 1997. Learning Features that Predict Cue
Usage. In Proceedings of ACL/EACL 97.
Kate Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop
Sarkar, Aravind Joshi, and Bonnie Webber. 2003. D-
LTAG System: Discourse Parsing with a Lexicalized
Tree-Adjoining Grammar. Journal of Logic, Lan-
guage and Information, 12(3):261?279.
Kate Forbes. 2003. Discourse Semantics of S-Modifying
Adverbials. Ph.D. thesis, Department of Linguistics,
University of Pennsylvania.
Dan Hardt and Maribel Romero. 2002. Ellipsis and the
Structure of Discourse. In Proceedings of Sinn und
Bedeutung VI.
Andrew Kehler. 2002. Coherence, Reference and the
Theory of Grammar. CSLI Publications.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Proceedings of LREC-02.
Ivana Kruijff-Korbayova? and Bonnie Webber. 2001. In-
formation Structure and the Semantics of ?otherwise?.
In Proceedings of ESSLLI 2001: Workshop on Infor-
mation Structure, Discourse Structure and Discourse
Semantics.
William Mann and Sandra Thompson. 1988. Rhetorical
Structure Theory. Toward a Functional Theory of Text
Organization. Text, 8(3):243?281.
Mitch Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Eleni Miltsakaki, Cassandre Creswell, Kate Forbes, Ar-
avind Joshi, and Bonnie Webber. 2003. Anaphoric
Arguments of Discourse Connectives: Semantic
Properties of Antecedents versus Non-Antecedents.
In Proceedings of the Computational Treatment of
Anaphora Workshop, EACL 2003.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating Discourse Con-
nectives and their Arguments. In Proceedings of the
NAACL/HLT Workshop: Frontiers in Corpus Annota-
tion.
Marc Moens and Mark Steedman. 1988. Temporal On-
tology and Temporal Reference. Computational Lin-
guistics, 14(2):15?28.
Megan G. Moser and Johanna Moore. 1995. Investi-
gating Cue Selection and Placement in Tutorial Dis-
course. In Proceedings of ACL95.
Sidney Siegel and N. J. Castellan. 1988. Nonparama-
teric Statistics for the Behavioral Sciences. McGraw-
Hill, 2nd edition.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialog systems. In Pro-
ceedings of ACL-2004.
Marilyn Walker, Rashmi Prasad, and Amanda Stent.
2003. A Trainable Generator for Recommendations
in Multimodal Dialogue. In Eurospeech, 2003.
Bonnie Webber and Aravind Joshi. 1998. Anchoring a
Lexicalized Tree-Adjoining Grammar for Discourse.
In ACL/COLING Workshop on Discourse Relations
and Discourse Markers, Montreal.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999a. Discourse Relations: A Struc-
tural and Presuppositional Account Using Lexicalized
TAG. In Proceedings of ACL-99.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999b. What are Little Texts Made of? A
Structural and Presuppositional Account Using Lex-
icalized TAG. In Proceedings of the International
Workshop on Levels of Representation in Discourse
(LORID ?99).
Bonnie Webber, Alistair Knott, and Aravind Joshi.
2000. Multiple Discourse Connectives in a Lexi-
calized Grammar for Discourse. In Proceedings of
IWCS-00.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and Discourse Struc-
ture. Computational Linguistics, 29:545?587.
Sandra Williams and Ehud Reiter. 2003. A Corpus
Analysis of Discourse Relations for Natural Language
Generation. In Proceedings of Corpus Linguistics.
Classification from Full Text: A Comparison of Canonical Sections of
Scientific Papers
Gail Sinclair Bonnie Webber
School of Informatics,
University of Edinburgh
 	
Annotating Discourse Connectives And Their Arguments
Eleni Miltsakaki
University of Pennsylvania
Philadelphia, PA 19104 USA
elenimi@linc.cis.upenn.edu
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104 USA
rjprasad@linc.cis.upenn.edu
Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
joshi@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
This paper describes a new, large scale
discourse-level annotation project ? the Penn
Discourse TreeBank (PDTB). We present an
approach to annotating a level of discourse
structure that is based on identifying discourse
connectives and their arguments. The PDTB
is being built directly on top of the Penn Tree-
Bank and Propbank, thus supporting the extrac-
tion of useful syntactic and semantic features
and providing a richer substrate for the devel-
opment and evaluation of practical algorithms.
We provide a detailed preliminary analysis of
inter-annotator agreement ? both the level of
agreement and the types of inter-annotator vari-
ation.
1 Introduction
Large scale annotated corpora have played a critical role
in speech and natural language research. The Penn Tree-
Bank (PTB) is an example of such a resource with world-
wide impact on natural language processing (Marcus et
al., 1993). However, the PTB deals with text only at
the sentence level: with the demand for more power-
ful NLP applications comes a need for greater richness
in annotation. At the sentence level, Penn Propbank
is adding predicate-argument annotation to sentences in
PTB (Kingsbury and Palmer, 2002). At the discourse-
level are efforts to produce corpora annotated with rhetor-
ical relations (Carlson et al, 2003). This paper describes
a more basic discourse-level annotation project ? the
Penn Discourse TreeBank (PDTB) ? that aims to produce
a large-scale corpus in which discourse connectives are
annotated, along with their arguments.
There have been several approaches to describing dis-
course in terms of discourse relations (Mann and Thomp-
son, 1988; Asher and Lascarides, 1998; Polanyi and
van den Berg, 1996). In these approaches, the additional
meaning the discourse contributes beyond the sentence
derives from discourse relations. Specification of the dis-
course relations for a discourse thus constitutes a descrip-
tion of a certain level of discourse structure.
Rather than starting from (abstract) discourse rela-
tions, we describe an approach to annotating a large-
scale corpus in terms of a more basic characterisation
of discourse structure in terms of discourse connectives
and their arguments. The motivation for such an ap-
proach stems from work by Webber and Joshi (1998),
Webber et al (1999a), Webber et al (2000) which inte-
grates sentence level structures with discourse level struc-
ture (using tree-adjoining grammars for both cases, LTAG
and DLTAG, respectively).1 This allows structural com-
position and its associated semantic composition at the
sentence level to be smoothly carried over to the dis-
course level, a goal also shared by Gardent (1997),
Schilder (1997) and Polanyi and van den Berg (1996),
among others.2
Discourse connectives and their arguments can be suc-
cessfully annotated with high reliability (cf. Section
4). This is not surprising, given that the task resem-
bles that of annotating verbs and their arguments at
the sentence level (Kingsbury and Palmer, 2002). In
fact, we use a fine-grained, lexically grounded annota-
tion in which argument labels are specific to the dis-
1In the PDTB annotations, we have deliberately adopted
a policy to make the annotations independent of the DLTAG
framework for two reasons: (1) to make the annotated corpus
widely useful to researchers working in different frameworks
and (2) to make the annotators? task easier, thereby increasing
interannotator reliability.
2However, the approaches in Gardent (1997),
Schilder (1997), and Polanyi and van den Berg (1996) are
different in two ways: a) the process by which discourse
derives compositional aspects of meaning is considered entirely
separate from how clauses do so, and b) only two mechanisms
are used for deriving discourse semantics ? compositional
semantics and inference.
course connectives involved, in much the same way as
in Kingsbury and Palmer (2002). In contrast, a recent
attempt (Carlson et al, 2003) at using RST-type rela-
tions for annotating a much smaller corpus has already
revealed difficulties involved in reliably annotating more
abstract discourse relations. Moreover, this type of anno-
tation does not contain any record of the basis on which
a relation was assigned.
The paper is organized as follows. Section 2 provides
a brief overview of the fundamental ideas that provide
the basis for the design of the PDTB annotation. Section
3 gives a detailed description of the annotation project,
including information about the size of the corpus, com-
pleted annotations as well as annotation instructions as
formulated in the guidelines. Section 4 presents data
analysis based on current annotations as well as results
from inter-annotator agreement. Section 5 wraps up with
a summary of the work.
2 Theoretical background
The annotation project presented in this paper builds
on basic ideas presented in Webber and Joshi (1998),
Webber et al (1999b) and Webber et al (2003) ? that
connectives are discourse-level predicates which project
predicate-argument structure on a par with verbs at the
sentence level. Webber and Joshi (1998) propose a tree-
adjoining grammar for discourse (DLTAG) in which
compositional aspects of discourse meaning are for-
mally defined, thus teasing apart compositional from non-
compositional layers of meaning. In this framework, con-
nectives are grouped into natural classes depending on the
structure that they project at the discourse level. Subordi-
nate and coordinating conjunctions, for example, require
two arguments that can be identified structurally from ad-
jacent units of discourse. What Webber et al (2003) call
anaphoric discourse connectives (some, but not all, dis-
course adverbials, such as ?otherwise?, ?instead?, ?fur-
thermore?, etc.) also require two arguments, but only one
of them derives structurally. For the complete interpreta-
tion of these connectives, their other argument needs to
be recovered. The crucial contribution of this framework
to the design of the current project is what can be seen
as a bottom-up approach to discourse structure. Specifi-
cally, instead of appealing to an abstract (and arbitrary)
set of discourse relations whose identification involves
confounding multiple sources of discourse meaning, we
start with the annotation of discourse connectives and
their arguments, thus exposing a clearly defined level of
discourse representation.
3 Project description
The PTDB project began in November 2002. The first
phase, including pilot annotations and preliminary devel-
opment of guidelines, was completed in May 2003. The
PDTB is expected to be released by November 2005. In-
termediate versions of the annotated corpus will be made
available for receiving feedback.
The PDTB corpus will include annotations of four
types of connectives: subordinating conjunctions, coor-
dinating conjunctions, adverbial connectives and implicit
connectives. We specify each of these types in more de-
tail in Section 3.1. The final number of annotations in
the corpus will amount to approximately 30,000; 10,000
implicit connectives and 20,000 annotations of the 250
explicit connectives identified in the corpus. The final
version of the corpus will also contain characterizations
of the semantic roles associated with the arguments of
each type of connective.
In this paper we present the results of annotating 10
explicit connectives, amounting to a total of 2717 anno-
tations, as well as 386 tokens of implicit connectives. The
set of 10 connectives comprises the adverbial connectives
?therefore?, ?as a result?, ?instead?, ?otherwise?, ?never-
theless?, and the subordinate conjunctions ?because?, ?al-
though?, ?even though?, ?when?, and ?so that?. In all
cases, annotations have been performed by four annota-
tors. While this slows down the annotation process con-
siderably, the nature, significance and magnitude of the
project as well as the well-known complexity of discourse
annotation tasks impels us to strive for maximum relia-
bility, achieved by having the task performed by multiple
annotators.3
Individual annotation proceeds one connective at a
time. The annotation tool WordFreak4 is used to iden-
tify all instances of the given connective in the corpus,
and these are then annotated independently and manu-
ally by four annotators. This way, the annotators quickly
gain experience with that connective and develop a better
understanding of its predicate-argument characteristics.
Similarly, for the annotation of implicit connectives, all
instances (as specified in the guidelines, see Section 3.2)
are identified one file at a time. For this task, the anno-
tators are required to read the entire file so that they can
make well-informed and reliable decisions about the im-
plicit connectives and their arguments. In addition, after
the arguments of each implicit connective have been iden-
tified, the annotators provide, if possible, an explicit con-
nective (or other suitable expression) that best expresses
the inferred relation. As with explicit connectives, anno-
tations of implicit connectives are done by four annota-
3When inter-annotator consistency has stabilized, we intend
to reduce the number of annotators to three, or maybe two at the
minimum.
4WordFreak was developed by Tom Morton at the University
of Pennsylvania. It has been substantially modified by Jeremy
Lacivita to fit the needs of the PDTB project. A snapshot of the
tool can be seen at http://www.cis.upenn.edu/?pdtb.
tors.
Compared with Propbank?s annotation of verb
predicate-argument structures, annotation of arguments
of discourse predicates is different in interesting ways.
Propbank annotators have to determine the number of ar-
guments required by each verb. In contrast, discourse
connectives exhibit a clear predicate-argument structure
requiring only two arguments. The main challenge we
have discovered for annotating discourse connectives is
determining the extent of their arguments. Even subor-
dinate conjunctions whose arguments never cross a sen-
tence boundary may sometimes be the source of disagree-
ment between annotators.
In what follows, we present a brief overview of the
classes of connectives that we annotate, followed by
highlights of the annotation manual and relevant corpus
examples.
3.1 Discourse connectives
We classify discourse connectives into four classes: sub-
ordinate and coordinating conjunctions, adverbials and
implicit connectives. Examples of each type are given be-
low, with their arguments shown in square brackets and
the connectives, in italics.
3.1.1 Subordinate conjunctions
Subordinate conjunctions introduce clauses that are
syntactically dependent on a main clause. The most com-
mon types of relations that they express are temporal
(e.g., ?when?, ?as soon as?), causal e.g., ?because?), con-
cessive (e.g., ?although?, ?even though?), purpose (e.g.,
?so that?, ?in order that?) and conditional (e.g., ?if?, ?un-
less). Clauses introduced with a subordinate conjunction
may be preposed (or, more rarely, interposed) with re-
spect to the main clause, as shown in (1).
(1) Because [the drought reduced U.S. stockpiles], [they
have more than enough storage space for their new
crop], and that permits them to wait for prices to rise.
3.1.2 Coordinating conjunctions
Coordinating conjunctions are ones such as ?and?,
?but?, and ?or?. Example (2) shows the annotation of an
instance of the conjunction ?and?.
(2) [William Gates and Paul Allen in 1975 developed
an early language-housekeeper system for PCs], and
[Gates became an industry billionaire six years after
IBM adapted one of these versions in 1981].
Instances of coordinating conjunctions which coordi-
nate nominal or other non-clausal constituents are ex-
cluded from annotation. We also exclude cases of VP-
coordination because in such cases the arguments of the
connective can be retrieved automatically from the syn-
tactic layer.
3.1.3 Adverbial connectives
Adverbial connectives are sentence-modifying adverbs
which express a discourse relation (Forbes, 2003). The
class of adverbial connectives includes ?however?, ?there-
fore?, ?then?, ?otherwise?, etc. In this class, we have also
included prepositional phrases with a similar sentence
modifying function such as ?as a result?, ?in addition?,
?in fact?, etc. Example (3) shows the annotation of an
instance of the adverbial connective ?as a result?.
(3) ...[many analysts expected energy prices to rise at the
consumer level too]. As a result, [many economists
were expecting the consumer price index to increase
significantly more than it did].
The arguments of adverbial connectives may or may
not be adjacent to the sentence containing the connective.
In a few cases, an argument may be found one or two
paragraphs away from the connective.
3.1.4 Implicit connectives
Implicit connectives are identified between adjacent
sentences with no explicit connectives.5 The annotation
of implicit connectives is intended to capture the connec-
tion between two sentences appearing in adjacent posi-
tions. For example, in (4), the two adjacent sentences
are connected in a way similar to having the explicit con-
nective ?but? contrasting them. Indeed, for implicit con-
nectives, annotators are asked to provide, when possible,
an explicit connective that best describes the inferred re-
lation. The explicit connective provided in (4) was ?in
contrast?.
(4) ...[The $6 billion that some 40 companies are looking to
raise in the year ending March 31 compares with only
$2.7 billion raised on the capital market in the previous
fiscal year]. IMPLICIT-(In contrast) [In fiscal 1984 be-
fore Mr. Gandhi came to power, only $810 million was
raised].
3.2 Annotation guidelines
The annotation guidelines for PDTB have been revised
considerably since the pilot phase of the project in May
2003. The current version of the guidelines is available at
http://www.cis.upenn.edu/?pdtb. Below we out-
line the basic points.
3.2.1 What counts as a discourse connective?
We count as discourse connectives (1) all subordinat-
ing and coordinating conjunctions, (2) certain adverbials,
and (3) implicit connectives. The adverbials include only
those which convey a relation between events or states.
For example, in (5) ?as a result? conveys a cause-effect re-
lation between the event of limiting the size of new steel
5There are, of course, other implicit connectives that we are
not taking into account.
mills and that of the industry operating out of small, ex-
pensive and highly inefficient units. In contrast, the se-
mantic interpretation of ?strangely? in (6) only requires a
single event/state which it classifies in the set of strange
events/states.6
(5) [In the past, the socialist policies of the government
strictly limited the size of new steel mills, petrochem-
ical plants, car factories and other industrial concerns to
conserve resources and restrict the profits businessmen
could make]. As a result, industry operated out of small,
expensive, highly inefficient industrial units.
(6) Strangely, conventional wisdom inside the Beltway re-
gards these transfer payments as ?uncontrollable? or
?nondiscretionary.?
The guidelines also highlight instances of lexical items
with multiple functions, only one of which is as a dis-
course connective. For example, ?when? can either serve
as a subordinate conjunction or introduce a relative clause
modifying a nominal phrase, as in (7), where the when-
clause modifies the nominal ?1985?.7Here we again ben-
efit from building discourse annotation on top of Penn
TreeBank because the syntactic annotation of when-
clauses distinguishes the two functions: When-relatives
are marked as NP-modifiers adjoining to an NP, whereas
adverbial when-clauses adjoin to a sentential node.
(7) Attorneys have argued since 1985, when the law took
effect.
Similarly, some since-clauses function as NP modifiers
as shown in (8). In such cases, ?since? is not annotated as
a connective. As in the case of when-clauses, instances of
NP modifying since-clauses can be identified in the Penn
TreeBank by virtue of their syntactic annotation.
(8) In the decade since the communist nation emerged from
isolation, its burgeoning trade with the West has lifted
Hong Kong?s status as a regional business partner.
Finally, implicit connectives count as connectives.
They are identified between adjacent sentences which do
not contain any other explicit connectives. Currently, we
are not annotating implicit connectives intra-sententially,
such as between the matrix clause and free adjunct in Ex-
ample (9). We plan to incorporate annotations of implicit
intra-sentential connectives at a later stage of the project.
(9) Second, they channel monthly mortgage payments into
semiannual payments, reducing the administrative bur-
den on investors.
6For a more detailed discussion of the basis for distin-
guishing discourse adverbials from clausal adverbials, see
Forbes (2003).
7In cases of when-relatives, a when-clause can be annotated
as SUP (see Section 3.2.3).
3.2.2 What counts as a legal argument?
Because we take discourse relations to hold between
abstract objects, we require that an argument contains at
least one predicate along with its arguments. Of course,
a sequence of clauses or sentences may also form a legal
argument, containing multiple predicates.
Because our annotations are done directly on top of
the Penn TreeBank, annotators may select as an argument
certain textual spans that appear to exclude one or more
arguments of the predicate. These are cases in which
these arguments are directly retrievable from the syntac-
tic annotation. Thus, we are able to select only the pred-
icates that are required for the interpretation of the dis-
course connective and simultaneously access their argu-
ments for the complete interpretation of the clause while
keeping the annotations of single arguments simple and
maximally contiguous. In (10), for example, the relative
clause is marked as one of the two arguments of the con-
nective ?even though?. The subject of the verb in the rela-
tive clause is directly retrievable from the Penn TreeBank
annotation. Similarly, in (11) the subject of the infinitival
clause is also available from the syntactic representation.
(10) Workers described ?clouds of dust? [that hung over
parts of the factory] even though [exhaust fans venti-
lated the air].
(11) The average maturity for funds open only to institutions,
considered by some [to be a stronger indicator] because
[those managers watch the market closely], reached a
high point for the year ? 33 days.
There are two exceptions to the requirement that an
argument include a verb ? these are nominal phrases that
express an event or a state, and discourse deictics that
denote an event or state. In (12), for example, the nominal
phrase ?fainting spells? can be marked as a legal argument
of the connective ?when? because the phrase expresses an
event of fainting.
(12) Its symptoms include a cold sweat at the sound of de-
bate, clammy hands in the face of congressional crit-
icism, and [fainting spells] when [someone writes the
word ?controversy.?]
Discourse deictic expressions are forms such as ?this?
and ?that? that can be used to denote the interpretation
of clausal textual spans from the preceding discourse.
In (13), for example, ?that? denotes the interpretation of
the sentence immediately preceding it. Our annotators
are guided to make argument selections that assume that
anaphoric and deictic expressions have been resolved.
Thus, in (13), they are able to select ?That?s? as one ar-
gument of the connective ?because?.
(13) Airline stocks typically sell at a discount of about one-
third to the stock market?s price-earnings ratio ? which
is currently about 13 times earnings. [That?s] because
[airline earnings, like those of auto makers, have been
subject to the cyclical ups-and-downs of the economy].
The annotators are also informed that in some cases,
an argument of a connective must be derived from the
selected textual span (Webber et al, 1999a; Webber et al,
2003). This is the case for the first argument of ?instead?
in (14), which does not include the negation, although it
is contained in the selected text.8
(14) [No price for the new shares has been set]. Instead, [the
companies will leave it up to the marketplace to decide].
In sum, legal arguments can be groups of sentences,
single sentences (a main clause and its subordinate
clauses), single clauses (tensed or non-tensed), NPs that
specify events or situations, and discourse deictic expres-
sions.
3.2.3 How far does an argument extend?
One particularly significant addition to the guidelines
came as a result of differences among annotators as to
how large a span constituted the argument of a connec-
tive. During pilot annotations, annotators used three an-
notation tags: CONN for the connective and ARG1 and
ARG2 for the two arguments. To this set, we have added
the optional tags SUP1, SUP2 (supplementary) for cases
when the annotator wants to mark textual spans s/he con-
siders to be useful, supplementary information for the
interpretation an argument. Example (15) demonstrates
the use of SUP1. Arguments are shown in square brack-
ets, while spans providing supplementary information are
shown in parentheses.
(15) Although [started in 1965], [Wedtech didn?t really get
rolling until 1975] (when Mr. Neuberger discovered the
Federal Government?s Section 8 minority business pro-
gram).
4 Data analysis
To test the reliability of the annotation, we first con-
sidered the kappa statistic (Siegel and Castellan, 1988)
which is used extensively in empirical studies of dis-
course (Carletta, 1996). The kappa coefficient provides
an inter-annotator agreement figure for any number of an-
notators by measuring pairwise agreement between them
and by correcting for chance expected agreement. How-
ever, the statistic requires the data tokens to be classified
into discrete categories, and as a result, we could not ap-
ply it to our data since the PDTB annotation tokens can-
not be classified as such. Rather, annotation in the PDTB
constitutes either selection of a span of text for the ar-
guments of connectives which can be of indeterminate
length or providing explicit expressions for implicit con-
nectives from an open-ended class of expressions.
8For a preliminary corpus-based analysis of the arguments
of ?instead?, see Miltsakaki et al (2003).
Instead, we have assessed inter-annotator agreement in
terms of agreement/disagreement on span or named ex-
pression identity for each token as a percentage of the
pairs of spans or expressions that actually matched ver-
sus those that should have. For the argument annotations,
we use a most conservative measure - the exact match
criterion. In addition, we also used different diagnostics
for the argument annotations for the explicit connectives,
reporting percentage agreement on different classes of to-
kens, such as those in which the first argument (ARG1)
annotations and second argument (ARG2) annotations
were counted independently, as well as those in which the
ARG1 and ARG2 annotations (for each connective) were
counted together as a single token. For all the argument
annotations, the computation of agreement excluded the
supplementary annotations (cf. Section 3.2.3).
We present here agreement results on ARG1 and
ARG2 annotations by two annotators for the annotation
of ten explicit connectives, amounting to a total of 2717
annotations, and 368 annotations of implicit connectives,
including agreement results on the explicit expression the
annotators used in in place of the implicit connectives as
well as the ARG1 and ARG2 annotations of the implicit
connectives.9 The ten explicit connectives include 5 sub-
ordinating conjunctions (when, because, even though, al-
though, and so that) and 5 adverbials (nevertheless, oth-
erwise, instead, therefore, and as a result).
4.1 Inter-annotator Agreement
4.1.1 Explicit connectives
For the explicit connective annotations, we used two
diagnostics for measuring inter-annotator agreement. In
the first diagnostic , we took the class of tokens as the to-
tal number of argument annotations, treating ARG1 and
ARG2 annotations as independent tokens. The total num-
ber of tokens in this class is therefore twice the number
of connective tokens, i.e, 5434. We recorded agreement
using the exact match criterion. That is, for any ARG1
or ARG2 token, agreement was recorded as 1 when both
annotators made identical textual selections for the an-
notation and 0 when the annotators made non-identical
selections.
We achieved 90.2% agreement (4900/5434 tokens)
on the annotations for this class. Agreement on only
ARG1 tokens was 86.3%, and agreement on only ARG2
tokens was 94.1%. Further distribution of the agree-
ments by connective is given in Table 1. Connectives
are grouped in the table by type (subordinating conjunc-
tion (SUBCONJ) and adverbial (ADV)). The second col-
9Right now SUP1 and SUP2 annotations are for our use only
and are not included in the current evaluations. Additional an-
notations by another 2 annotators are currently underway. The
2 annotators of the explicit connectives are different from the 2
annotators of the implicit connectives.
umn gives the number of agreeing tokens for each con-
nective and the third column gives the total number of
(ARG1+ARG2) tokens available for that connective. The
last column gives the percent agreement for the connec-
tive in that row, i.e., as a percentage of tokens for which
agreement was 1 (column 2) versus the total number of
tokens for that connective (column 3).
CONNECTIVES AGR No. Conn. Total %AGR
when 1877 2032 92.4%
because 1703 1824 93.4%
even though 194 206 94.1%
although 635 704 90.1%
so that 66 74 89.2%
TOTAL SUBCONJ 4469 4834 92.4%
nevertheless 56 94 59.6%
otherwise 44 46 95.7%
instead 172 236 72.9%
as a result 110 168 65.5%
therefore 49 56 87.5%
TOTAL ADV. 431 600 71.8%
OVERALL TOTAL 4900 5434 90.2%
Table 1: Distribution of Agreement by Connective, with
ARG1 and ARG2 Annotations Counted Independently
The table shows that we achieved high agreement
on argument annotations of subordinating conjunctions
(92.4%). Average agreement on the adverbials was lower
(71.8%). This difference between the two types is not sur-
prising, since locating the anaphoric (ARG1) argument of
adverbial connectives is believed to be a harder task than
that of locating the arguments of subordinating conjunc-
tions. For example, the anaphoric argument of the ad-
verbial connectives may be located in some non-adjacent
span of text, even several paragraphs away. Arguments of
subordinating conjunctions, on the other hand, can most
often be found in spans of text adjacent to the connective.
The table also shows that there was uniform agreement
across the different subordinating conjunctions (roughly
90%), whereas the adverbials showed more variation.
In particular, agreement on otherwise and therefore was
high (95.7% and 87.5% respectively), while lower for
the other three adverbials, instead (72.9%), as a result
(65.5%), and nevertheless (59.6%). This suggests either
greater variability in how these adverbials are interpreted
or greater complexity in their interpretation, which results
in more variability when people are forced to associate an
interpretation with a particular text span.
We also computed agreement using a second more
conservative diagnostic in which we took the class of to-
kens as the total number of connective tokens (2717) so
that the ARG1 and ARG2 annotations for each connec-
tive were treated together as part of the same token. Here
again, we recorded agreement using the exact match mea-
sure. That is, for any connective token, agreement was
recorded as 1 when both annotators made identical tex-
tual selections for the annotation of both arguments and
0 when the annotators made non-identical selections for
any one or both arguments.
We achieved 82.8% agreement (2249/2717 tokens) on
the annotations for this class. Table 2 gives the distribu-
tion of the agreements by connective. The table shows
relatively lower agreements when compared with the first
diagnostic, for both subordinating conjunctions (86%) as
well as adverbials (57%). However, this difference is un-
derstandable since the token class as defined for this di-
agnostic yields a stricter measure of agreement.
CONNECTIVES AGR No. Conn. Total %AGR
when 868 1016 86.4%
because 804 912 88.2%
even though 91 103 88.3%
although 288 352 81.8%
so that 27 34 79.4%
TOTAL SUBCONJ 2078 2417 86.0%
nevertheless 18 47 38.3%
otherwise 21 23 91.3%
instead 72 118 61.0%
as a result 38 84 45.2%
therefore 22 28 78.6%
TOTAL ADV. 171 300 57.0%
OVERALL TOTAL 2249 2717 82.8%
Table 2: Distribution of Agreement by Connective, with
ARG1 and ARG2 Annotations Counted Together
We classified disagreements into 4 major types. The
result of classifying the 534 disagreements from Diag-
nostic 1 (Table 1) is given in Table 3. The third column
gives the percent of the total disagreements for each type.
DISAGREEMENT TYPE No. %
Missing Annotations 72 13.5%
No Overlap 30 5.6%
Partial Overlap
Parentheticals 53 9.9%
higher verb 181 33.9%
dependent clause 182 34.1%
Other 6 1.1%
Unresolved 10 1.9%
TOTAL 534 100%
Table 3: Disagreement Classification
The majority of disagreements (79%) were due to
Partial Overlap, which subsumes the categories Higher
Verb, Dependent Clause, Parenthetical and Other. Par-
tial Overlap means that there was partial overlap in the
annotations selected by the two annotators. Higher verb
includes tokens where one of the annotators included the
governing predicate for the clause marked by both anno-
tators. The higher clause occurred on the left or right pe-
riphery of the lower clause. Dependent Clause includes
tokens where one of the annotators included extra clausal
material that is syntactically dependent on the clause that
was selected by both, and that occurs on the left or right
periphery of the common text. Parenthetical means
that one of the annotators included a medial parentheti-
cal, while the other did not. The intervening text could be
the main as well as the dependent clause. An example is
provided below:
(16) Bankers said [warrants for Hong Kong stocks are at-
tractive] because [they give foreign investors], wary of
volatility in the colony?s stock market, [an opportunity
to buy shares without taking too great a risk].
(17) Bankers said [warrants for Hong Kong stocks are at-
tractive] because [they give foreign investors, wary of
volatility in the colony?s stock market, an opportunity
to buy shares without taking too great a risk].
Other included tokens with partial overlap between an-
notations, but in addition included a combination of more
than type, such as higher verb+dependent clause.
Note that disagreements that contain a partial over-
lap could be counted as agreeing tokens if we relaxed
the more conservative exact match measure to a partial
match measure. Our subjective view was that in several
cases, the ?extra? textual material, especially those fit-
ting the dependent clause and parenthetical category did
not make any significant semantic contribution in terms
of their inclusion or exclusion in the argument. With the
partial match measure, excluding these cases reduces the
disagreements to half the given number, giving us 94.5%
agreement overall.
The No Overlap tokens were cases of true disagree-
ment in that there was no overlap in the annotations se-
lected by the annotators. These tokens constituted 5.6%
of the disagreements. Examples (18) and (19) shows the
two annotations for a token in which there was no over-
lap in the ARG1 annotation. Missing Annotations also
constituted a substantial proportion of the disagreements
(13.5%) and was used for tokens where the annotation
was missing for one annotator. Note that these don?t re-
ally count as disagreement, since all connectives are pre-
theoretically assumed to require two arguments. Unre-
solved includes tokens which have introduced new issues
for the annotation guidelines and cannot be resolved at
this time. These include issues such as how to treat com-
paratives, certain types of adjunct clauses, certain types
of nominalizations etc.
(18) [The word ?death? cannot be escaped entirely by the
industry], but salesmen dodge it wherever possible or
cloak it in euphemisms, [preferring to talk about ?sav-
ings? and ?investment?] instead.
(19) The word ?death? cannot be escaped entirely by the
industry, but salesmen dodge it wherever possible or
[cloak it in euphemisms], preferring [to talk about ?sav-
ings? and ?investment?] instead.
4.1.2 Implicit connectives
For the 386 tokens of implicit connectives, we ana-
lyzed inter-annotator agreement between two annotators
for (a) the explicit connectives they provided in place of
an implicit connective, and (b) the argument annotations
of the implicit connectives.
As a preliminary step in analyzing agreement on the
type of explicit connective provided by the annotators in
place of an implicit connective, we considered 5 groups
of connectives conveying : a) additional information
(e.g., ?furthermore?, ?in addition?) b) cause-effect rela-
tions (e.g., ?because?, ?as a result?), c) temporal relations
(e.g., ?then?, ?simultaneously?), d) contrastive relations
(e.g., ?however?, ?although?), and e) restatement or sum-
marization (e.g., ?in other words?, ?in sum?). 10 Agree-
ment was then computed on these basic groups of con-
nectives.11 From the total of 386 tokens of implicit con-
nectives, 9 were excluded from the analysis due to tech-
nical error (missing annotation). For the remaining 307
tokens, we achieved 72% agreement on the type of ex-
plicit connective that best conveyed the interpretation of
the implicit connective.
For the argument annotations of the implicit connec-
tives, we present agreement results from using the first
diagnostic used for the explicit connectives. That is, we
counted ARG1 and ARG2 annotations as independent to-
kens and computed percent agreement using the exact
match criterion. On the 772 ARG1 and ARG2 tokens,
we achieved 85.1% (657/772) agreement between 2 an-
notators. The analysis of the 115 disagreements is given
in Table 4. Note that here again, the number of disagree-
ments reduces to half using the partial match measure for
the parenthetical and dependent clause classes, giving us
92.6% agreement overall.
DISAGREEMENT TYPE No. %
Missing Annotations 6 5.2%
No Overlap 2 1.7%
Partial Overlap
parenthetical 13 11.3%
higher verb 24 20.9%
dependent clause 44 38.3%
sentence 19 16.5%
other 3 2.6%
Unresolved 4 3.5%
TOTAL 115 100%
Table 4: Disagreement Classification for Implicit Con-
nective ARG Annotations
10These groups are based on types of coherence relations de-
rived from corpus-based distributions of connectives presented
in (Knott, 1996). Initially, we also considered a group of con-
nectives expressing hypothetical relations but no such connec-
tives were identified in the annotations.
11Some polysemous connectives such as ?while? and ?in fact?
appeared in more than one group.
5 Summary
In this paper we presented a new and innovative
discourse-level annotation project, the Penn Discourse
TreeBank (PDTB), in which discourse connectives and
their arguments are annotated, thereby defining a clear
level of discourse structure that can be reliably annotated
for a large corpus. Our inter-annotator results confirm our
expectations of high agreement and annotation reliability.
At a later stage of the project, we plan to provide seman-
tic characterizations of the arguments of connectives and
resolve any cases of polysemy that might arise.
Acknowledgments
We are very grateful to Tom Morton and Jeremy Lacivita
for the development and special modification of the
WordFreak annotation tool. Special thanks to Jeremy for
providing continuous technical support. Thanks are also
due to our annotators, Cassie Creswell, Driya Amandita,
John Laury, Emily Pawley, Alan Lee, Alex Derenzy and
Steve Pettington. Also, many thanks to Katherine Forbes
Riley and Jean Carletta for their comments and sugges-
tions. Finally, we would like to thank the reviewers for
their very useful comments. This work was partially sup-
ported by NSF Grant EIA 02-24417.
References
Nicholas Asher and Alex Lascarides. 1998. The seman-
tics and pragmatics of presupposition. Journal of Se-
mantics, 15(3):239?300.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks. Computational Linguistics, 22:249?254.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski,
2003. Current Directions in Discourse and Dialogue,
chapter Building a Discourse-Tagged Corpus in the
Framework of Rhetorical Structure Theory. Kluwer
Academic Publishers.
Kate Forbes. 2003. Discourse Semantics of S-Modifying
Adverbials. Ph.D. thesis, Department of Linguistics,
University of Pennsylvania.
Claire Gardent. 1997. Discourse tree adjoining gram-
mars. Claus 89, University of the Saarlandes, Saar-
brucken.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Third International Confer-
ence on Language Resources and Evaluation, LREC-
02, Las Palmas, Canary Islands, Spain.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. thesis,
University of Edinburgh.
William Mann and Sandra Thompson. 1988. Rhetori-
cal structure theory. toward a functional theory of text
organization. Text, 8(3):243?281.
Mitch Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: the Penn Treebank. Computational
Linguistics, 19:313?330.
Eleni Miltsakaki, Cassandre Creswell, Kate Forbes, Ar-
avind Joshi, and Bonnie Webber. 2003. Anaphoric
arguments of discourse connectives: Semantic prop-
erties of antecedents versus non-antecedents. In Pro-
ceedings of the Computational Treatment of Anaphora
Workshop, EACL 2003, Budapest.
Livia Polanyi and Martin van den Berg. 1996. Discourse
structure and discourse interpretation. In Proceedings
of the Tenth Amsterdam Colloquium, University of Am-
sterdam, pages 113?131.
Frank Schilder. 1997. Discourse tree grammar or how
to get attached to a discourse? In Proceedings of the
the second International Workshop on Computational
Semantics (IWCS-II), Tilburg, The Netherlands, pages
261?273.
Sidney Siegel and N. J. Castellan. 1988. Nonparama-
teric Statistics for the Behavioral Sciences. McGraw-
Hill, 2nd edition.
Bonnie Webber and Aravind Joshi. 1998. Anchoring a
lexicalized tree adjoining grammar for discourse. In
ACL/COLING Workshop on Discourse Relations and
Discourse Markers, Montreal, pages 8?92. Montreal,
Canada.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999a. Discourse relations: A struc-
tural and presuppositional account using lexicalized
TAG. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, Mary-
land, pages 41?48. College Park MD.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999b. What are little texts made of? A
structural and presuppositional account using lexical-
ized TAG. In Proceedings of the International Work-
shop on Levels of Representation in Discourse (LORID
?99), Edinburgh, pages 145?149.
Bonnie Webber, Alistair Knott, and Aravind Joshi. 2000.
Multiple discourse connectives in a lexicalized gram-
mar for discourse. In Proceedings of the Third Interna-
tional Workshop on Computational Semantics, Tilburg,
The Netherlands.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29:545?587.
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29?36,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Attribution and the (Non-)Alignment of Syntactic and Discourse Arguments
of Connectives
Nikhil Dinesh and Alan Lee and Eleni Miltsakaki and Rashmi Prasad and Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
fnikhild,aleewk,elenimi,rjprasad,joshig@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
The annotations of the Penn Discourse
Treebank (PDTB) include (1) discourse
connectives and their arguments, and (2)
attribution of each argument of each con-
nective and of the relation it denotes. Be-
cause the PDTB covers the same text as
the Penn TreeBank WSJ corpus, syntac-
tic and discourse annotation can be com-
pared. This has revealed significant dif-
ferences between syntactic structure and
discourse structure, in terms of the argu-
ments of connectives, due in large part to
attribution. We describe these differences,
an algorithm for detecting them, and fi-
nally some experimental results. These re-
sults have implications for automating dis-
course annotation based on syntactic an-
notation.
1 Introduction
The overall goal of the Penn Discourse Treebank
(PDTB) is to annotate the million word WSJ cor-
pus in the Penn TreeBank (Marcus et al, 1993) with
a layer of discourse annotations. A preliminary re-
port on this project was presented at the 2004 work-
shop on Frontiers in Corpus Annotation (Miltsakaki
et al, 2004a), where we described our annotation
of discourse connectives (both explicit and implicit)
along with their (clausal) arguments.
Further work done since then includes the an-
notation of attribution: that is, who has expressed
each argument to a discourse connective (the writer
or some other speaker or author) and who has ex-
pressed the discourse relation itself. These ascrip-
tions need not be the same. Of particular interest is
the fact that attribution may or may not play a role
in the relation established by a connective. This may
lead to a lack of congruence between arguments at
the syntactic and the discourse levels. The issue of
congruence is of interest both from the perspective
of annotation (where it means that, even within a
single sentence, one cannot merely transfer the an-
notation of syntactic arguments of a subordinate or
coordinate conjunction to its discourse arguments),
and from the perspective of inferences that these an-
notations will support in future applications of the
PDTB.
The paper is organized as follows. We give a brief
overview of the annotation of connectives and their
arguments in the PDTB in Section 2. In Section 3,
we describe the annotation of the attribution of the
arguments of a connective and the relation it con-
veys. In Sections 4 and 5, we describe mismatches
that arise between the discourse arguments of a con-
nective and the syntactic annotation as provided by
the Penn TreeBank (PTB), in the cases where all the
arguments of the connective are in the same sen-
tence. In Section 6, we will discuss some implica-
tions of these issues for the theory and practice of
discourse annotation and their relevance even at the
level of sentence-bound annotation.
2 Overview of the PDTB
The PDTB builds on the DLTAG approach to dis-
course structure (Webber and Joshi, 1998; Webber
et al, 1999; Webber et al, 2003) in which con-
nectives are discourse-level predicates which project
predicate-argument structure on a par with verbs at
29
the sentence level. Initial work on the PDTB has
been described in Miltsakaki et al (2004a), Milt-
sakaki et al (2004b), Prasad et al (2004).
The key contribution of the PDTB design frame-
work is its bottom-up approach to discourse struc-
ture: Instead of appealing to an abstract (and arbi-
trary) set of discourse relations whose identification
may confound multiple sources of discourse mean-
ing, we start with the annotation of discourse con-
nectives and their arguments, thus exposing a clearly
defined level of discourse representation.
The PDTB annotates as explicit discourse connec-
tives all subordinating conjunctions, coordinating
conjunctions and discourse adverbials. These pred-
icates establish relations between two abstract ob-
jects such as events, states and propositions (Asher,
1993).1
We use Conn to denote the connective, and Arg1
and Arg2 to denote the textual spans from which the
abstract object arguments are computed.2 In (1), the
subordinating conjunction since establishes a tem-
poral relation between the event of the earthquake
hitting and a state where no music is played by a
certain woman. In all the examples in this paper, as
in (1), Arg1 is italicized, Arg2 is in boldface, and
Conn is underlined.
(1) She hasn?t played any music since the earthquake
hit.
What counts as a legal argument? Since we take
discourse relations to hold between abstract objects,
we require that an argument contains at least one
clause-level predication (usually a verb ? tensed or
untensed), though it may span as much as a sequence
of clauses or sentences. The two exceptions are
nominal phrases that express an event or a state, and
discourse deictics that denote an abstract object.
1For example, discourse adverbials like as a result are dis-
tinguished from clausal adverbials like strangely which require
only a single abstract object (Forbes, 2003).
2Each connective has exactly two arguments. The argument
that appears in the clause syntactically associated with the con-
nective, we call Arg2. The other argument is called Arg1. Both
Arg1 and Arg2 can be in the same sentence, as is the case for
subordinating conjunctions (e.g., because). The linear order of
the arguments will be Arg2 Arg1 if the subordinate clause ap-
pears sentence initially; Arg1 Arg2 if the subordinate clause ap-
pears sentence finally; and undefined if it appears sentence me-
dially. For an adverbial connective like however, Arg1 is in the
prior discourse. Hence, the linear order of its arguments will be
Arg1 Arg2.
Because our annotation is on the same corpus as
the PTB, annotators may select as arguments textual
spans that omit content that can be recovered from
syntax. In (2), for example, the relative clause is
selected as Arg1 of even though, and its subject can
be recovered from its syntactic analysis in the PTB.
In (3), the subject of the infinitival clause in Arg1 is
similarly available.
(2) Workers described ?clouds of blue dust? that hung
over parts of the factory even though exhaust fans
ventilated the air.
(3) The average maturity for funds open only to institu-
tions, considered by some to be a stronger indicator
because those managers watch the market closely,
reached a high point for the year ? 33 days.
The PDTB also annotates implicit connectives be-
tween adjacent sentences where no explicit connec-
tive occurs. For example, in (4), the two sentences
are contrasted in a way similar to having an explicit
connective like but occurring between them. Anno-
tators are asked to provide, when possible, an ex-
plicit connective that best describes the relation, and
in this case in contrast was chosen.
(4) The $6 billion that some 40 companies are looking to
raise in the year ending March 21 compares with only
$2.7 billion raise on the capital market in the previous
year. IMPLICIT - in contrast In fiscal 1984, before
Mr. Gandhi came into power, only $810 million
was raised.
When complete, the PDTB will contain approxi-
mately 35K annotations: 15K annotations of the 100
explicit connectives identified in the corpus and 20K
annotations of implicit connectives.3
3 Annotation of attribution
Wiebe and her colleagues have pointed out the
importance of ascribing beliefs and assertions ex-
pressed in text to the agent(s) holding or making
them (Riloff and Wiebe, 2003; Wiebe et al, 2004;
Wiebe et al, 2005). They have also gone a consid-
erable way towards specifying how such subjective
material should be annotated (Wiebe, 2002). Since
we take discourse connectives to convey semantic
predicate-argument relations between abstract ob-
jects, one can distinguish a variety of cases depend-
ing on the attribution of the discourse relation or its
3The annotation guidelines for the PDTB are available at
http://www.cis.upenn.edu/pdtb.
30
arguments; that is, whether the relation or arguments
are ascribed to the author of the text or someone
other than the author.
Case 1: The relation and both arguments are at-
tributed to the same source. In (5), the concessive
relation between Arg1 and Arg2, anchored on the
connective even though is attributed to the speaker
Dick Mayer, because he is quoted as having said
it. Even where a connective and its arguments are
not included in a single quotation, the attribution can
still be marked explicitly as shown in (6), where only
Arg2 is quoted directly but both Arg1 and Arg2 can
be attibuted to Mr. Prideaux. Attribution to some
speaker can also be marked in reported speech as
shown in the annotation of so that in (7).
(5) ?Now, Philip Morris Kraft General Foods? parent
company is committed to the coffee business and to
increased advertising for Maxwell House,? says Dick
Mayer, president of the General Foods USA division.
?Even though brand loyalty is rather strong for cof-
fee, we need advertising to maintain and strengthen
it.?
(6) B.A.T isn?t predicting a postponement because the
units ?are quality businesses and we are en-
couraged by the breadth of inquiries,? said Mr.
Prideaux.
(7) Like other large Valley companies, Intel also noted
that it has factories in several parts of the nation,
so that a breakdown at one location shouldn?t leave
customers in a total pinch.
Wherever there is a clear indication that a relation
is attributed to someone other than the author of the
text, we annotate the relation with the feature value
SA for ?speaker attribution? which is the case for
(5), (6), and (7). The arguments in these examples
are given the feature value IN to indicate that they
?inherit? the attribution of the relation. If the rela-
tion and its arguments are attributed to the writer,
they are given the feature values WA and IN respec-
tively.
Relations are attributed to the writer of the text by
default. Such cases include many instances of re-
lations whose attribution is ambiguous between the
writer or some other speaker. In (8), for example,
we cannot tell if the relation anchored on although
is attributed to the spokeswoman or the author of the
text. As a default, we always take it to be attributed
to the writer.
Case 2: One or both arguments have a different at-
tribution value from the relation. While the default
value for the attribution of an argument is the attribu-
tion of its relation, it can differ as in (8). Here, as in-
dicated above, the relation is attributed to the writer
(annotated WA) by default, but Arg2 is attributed to
Delmed (annotated SA, for some speaker other than
the writer, and other than the one establishing the
relation).
(8) The current distribution arrangement ends in March
1990 , although Delmed said it will continue to pro-
vide some supplies of the peritoneal dialysis prod-
ucts to National Medical, the spokeswoman said.
Annotating the corpus with attribution is neces-
sary because in many cases the text containing the
source of attribution is located in a different sen-
tence. Such is the case for (5) where the relation
conveyed by even though, and its arguments are at-
tributed to Dick Mayer.
We are also adding attribution values to the anno-
tation of the implicit connectives. Implicit connec-
tives express relations that are inferred by the reader.
In such cases, the author intends for the reader to
infer a discourse relation. As with explicit connec-
tives, we have found it useful to distinguish implicit
relations intended by the writer of the article from
those intended by some other author or speaker. To
give an example, the implicit relation in (9) is at-
tributed to the writer. However, in (10) both Arg1
and Arg2 have been expressed by the speaker whose
speech is being quoted. In this case, the implicit re-
lation is attributed to the speaker.
(9) Investors in stock funds didn?t panic the week-
end after mid-October?s 190-point market plunge.
IMPLICIT-instead Most of those who left stock
funds simply switched into money market funds.
(10) ?People say they swim, and that may mean they?ve
been to the beach this year,? Fitness and Sports. ?It?s
hard to know if people are responding truthfully.
IMPLICIT-because People are too embarrassed to
say they haven?t done anything.?
The annotation of attribution is currently under-
way. The final version of the PDTB will include an-
notations of attribution for all the annotated connec-
tives and their arguments.
Note that in the Rhetorical Structure Theory
(RST) annotation scheme (Carlson et al, 2003), at-
tribution is treated as a discourse relation. We, on
the other hand, do not treat attribution as a discourse
31
relation. In PDTB, discourse relations (associated
with an explicit or implicit connective) hold between
two abstracts objects, such as events, states, etc. At-
tribution relates a proposition to an entity, not to an-
other proposition, event, etc. This is an important
difference between the two frameworks. One conse-
quence of this difference is briefly discussed in Foot-
note 4 in the next section.
4 Arguments of Subordinating
Conjunctions in the PTB
A natural question that arises with the annotation
of arguments of subordinating conjunctions (SUB-
CONJS) in the PDTB is to what extent they can be
detected directly from the syntactic annotation in the
PTB. In the simplest case, Arg2 of a SUBCONJ is its
complement in the syntactic representation. This is
indeed the case for (11), where since is analyzed as
a preposition in the PTB taking an S complement
which is Arg2 in the PDTB, as shown in Figure 1.
(11) Since the budget measures cash flow, a new $1 di-
rect loan is treated as a $1 expenditure.
Furthermore, in (11), since together with its com-
plement (Arg2) is analyzed as an SBAR which mod-
ifies the clause a new $1 direct loan is treated as a
$1 expenditure, and this clause is Arg1 in the PDTB.
Can the arguments always be detected in this
way? In this section, we present statistics showing
that this is not the case and an analysis that shows
that this lack of congruence between the PDTB and
the PTB is not just a matter of annotator disagree-
ment.
Consider example (12), where the PTB requires
annotators to include the verb of attribution said
and its subject Delmed in the complement of al-
though. But although as a discourse connective de-
nies the expectation that the supply of dialysis prod-
ucts will be discontinued when the distribution ar-
rangement ends. It does not convey the expectation
that Delmed will not say such things. On the other
hand, in (13), the contrast established by while is be-
tween the opinions of two entities i.e., advocates and
their opponents.4
4This distinction is hard to capture in an RST-based pars-
ing framework (Marcu, 2000). According to the RST-based an-
notation scheme (Carlson et al, 2003) ?although Delmed said?
and ?while opponents argued? are elementary discourse units
(12) The current distribution arrangement ends in March
1990, although Delmed said it will continue to pro-
vide some supplies of the peritoneal dialysis prod-
ucts to National Medical, the spokeswoman said.
(13) Advocates said the 90-cent-an-hour rise, to $4.25 an
hour by April 1991, is too small for the working poor,
while opponents argued that the increase will still
hurt small business and cost many thousands of
jobs.
In Section 5, we will identify additional cases. What
we will then argue is that it will be insufficient to
train an algorithm for identifying discourse argu-
ments simply on the basis of syntactically analysed
text.
We now present preliminary measurements of
these and other mismatches between the two corpora
for SUBCONJS. To do this we describe a procedural
algorithm which builds on the idea presented at the
start of this section. The statistics are preliminary in
that only the annotations of a single annotator were
considered, and we have not attempted to exclude
cases in which annotators disagree.
We consider only those SUBCONJS for which both
arguments are located in the same sentence as the
connective (which is the case for approximately 99%
of the annotated instances). The syntactic configura-
tion of such relations pattern in a way shown in Fig-
ure 1. Note that it is not necessary for any of Conn,
Arg1, or Arg2 to have a single node in the parse tree
that dominates it exactly. In Figure 1 we do obtain a
single node for Conn, and Arg2 but for Arg1, it is
the set of nodes fNP; V Pg that dominate it exactly.
Connectives like so that, and even if are not domi-
nated by a single node, and cases where the annota-
tor has decided that a (parenthetical) clausal element
is not minimally necessary to the interpretation of
Arg2 will necessitate choosing multiple nodes that
dominate Arg2 exactly.
Given the node(s) in the parse tree that dominate
Conn (fINg in Figure 1), the algorithm we present
tries to find node(s) in the parse tree that dominate
Arg1 and Arg2 exactly using the operation of tree
subtraction (Sections 4.1, and 4.2). We then discuss
its execution on (11) in Section 4.3.
annotated in the same way: as satellites of the relation Attribu-
tion. RST does not recognize that satellite segments, such as
the ones given above, sometimes participate in a higher RST
relation along with their nuclei and sometimes not.
32
S12
SBAR NP
A new $1 direct
loan
VP
is treated as a
$1 expenditure
IN S
2
the budget mea-
sures cash flowsince
Given N
Conn
= fINg, our goal is to find N
Arg1
=
fNP; V Pg, and N
Arg2
= fS
2
g. Steps:
 h
Conn
= IN
 x
Conn+Arg2
= SBAR  parent(h
Conn
)
 x
Conn+Arg1+Arg2
= S
12
 lowest Ancestor
parent(x
Conn+Arg2
)
with la-
bel S or SBAR. Note that x 2 Ancestor
x
 N
Arg2
= x
Conn+Arg2
 N
Conn
= SBAR  fINg
= fS
2
g
 N
Arg1
= x
Conn+Arg1+Arg2
  fx
Conn+Arg2
g
= S
12
  fSBARg
= fNP; V Pg
Figure 1: The syntactic configuration for (11), and the execution of the tree subtraction algorithm on this configuration.
4.1 Tree subtraction
We will now define the operation of tree subtraction
the graphical intuition for which is given in Figure
2. Let T be the set of nodes in the tree.
Definition 4.1. The ancestors of any node t 2 T ,
denoted by Ancestor
t
 T is a set of nodes such
that t 2 Ancestor
t
and parent(u; t) ) ([u 2
Ancestor
t
] ^ [Ancestor
u
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 31?38,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Annotating Attribution in the Penn Discourse TreeBank
Rashmi Prasad and Nikhil Dinesh and Alan Lee and Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
 
rjprasad,nikhild,aleewk,joshi  @linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
An emerging task in text understanding
and generation is to categorize information
as fact or opinion and to further attribute
it to the appropriate source. Corpus an-
notation schemes aim to encode such dis-
tinctions for NLP applications concerned
with such tasks, such as information ex-
traction, question answering, summariza-
tion, and generation. We describe an anno-
tation scheme for marking the attribution
of abstract objects such as propositions,
facts and eventualities associated with dis-
course relations and their arguments an-
notated in the Penn Discourse TreeBank.
The scheme aims to capture the source and
degrees of factuality of the abstract ob-
jects. Key aspects of the scheme are anno-
tation of the text spans signalling the attri-
bution, and annotation of features record-
ing the source, type, scopal polarity, and
determinacy of attribution.
1 Introduction
News articles typically contain a mixture of infor-
mation presented from several different perspec-
tives, and often in complex ways. Writers may
present information as known to them, or from
some other individual?s perspective, while further
distinguishing between, for example, whether that
perspective involves an assertion or a belief. Re-
cent work has shown the importance of recogniz-
ing such perspectivization of information for sev-
eral NLP applications, such as information extrac-
tion, summarization, question answering (Wiebe
et al, 2004; Stoyanov et al, 2005; Riloff et al,
2005) and generation (Prasad et al, 2005). Part of
the goal of such applications is to distinguish be-
tween factual and non-factual information, and to
identify the source of the information. Annotation
schemes (Wiebe et al, 2005; Wilson and Wiebe,
2005; PDTB-Group, 2006) encode such distinc-
tions to facilitate accurate recognition and repre-
sentation of such perspectivization of information.
This paper describes an extended annotation
scheme for marking the attribution of discourse re-
lations and their arguments annotated in the Penn
Discourse TreeBank (PDTB) (Miltsakaki et al,
2004; Prasad et al, 2004; Webber et al, 2005), the
primary goal being to capture the source and de-
grees of factuality of abstract objects. The scheme
captures four salient properties of attribution: (a)
source, distinguishing between different types of
agents to whom AOs are attributed, (b) type, re-
flecting the degree of factuality of the AO, (c) sco-
pal polarity of attribution, indicating polarity re-
versals of attributed AOs due to surface negated
attributions, and (d) determinacy of attribution, in-
dicating the presence of contexts canceling the en-
tailment of attribution. The scheme also describes
annotation of the text spans signaling the attri-
bution. The proposed scheme is an extension of
the core scheme used for annotating attribution
in the first release of the PDTB (Dinesh et al,
2005; PDTB-Group, 2006). Section 2 gives an
overview of the PDTB, Section 3 presents the ex-
tended annotation scheme for attribution, and Sec-
tion 4 presents the summary.
2 The Penn Discourse TreeBank (PDTB)
The PDTB contains annotations of discourse rela-
tions and their arguments on the Wall Street Jour-
nal corpus (Marcus et al, 1993). Following the
approach towards discourse structure in (Webber
et al, 2003), the PDTB takes a lexicalized ap-
31
proach towards the annotation of discourse rela-
tions, treating discourse connectives as the an-
chors of the relations, and thus as discourse-level
predicates taking two abstract objects (AOs) as
their arguments. For example, in (1), the subordi-
nating conjunction since is a discourse connective
that anchors a TEMPORAL relation between the
event of the earthquake hitting and a state where
no music is played by a certain woman. (The 4-
digit number in parentheses at the end of examples
gives the WSJ file number of the example.)
(1) She hasn?t played any music since the earthquake
hit. (0766)
There are primarily two types of connectives
in the PDTB: ?Explicit? and ?Implicit?. Explicit
connectives are identified form four grammati-
cal classes: subordinating conjunctions (e.g., be-
cause, when, only because, particularly since),
subordinators (e.g., in order that), coordinating
conjunctions (e.g., and, or), and discourse adver-
bials (e.g., however, otherwise). In the examples
in this paper, Explicit connectives are underlined.
For sentences not related by an Explicit connec-
tive, annotators attempt to infer a discourse rela-
tion between them by inserting connectives (called
?Implicit? connectives) that best convey the in-
ferred relations. For example, in (2), the inferred
CAUSAL relation between the two sentences was
annotated with because as the Implicit connective.
Implicit connectives together with their sense clas-
sification are shown here in small caps.
(2) Also unlike Mr. Ruder, Mr. Breeden appears to
be in a position to get somewhere with his agenda.
Implicit=BECAUSE (CAUSE) As a former White
House aide who worked closely with Congress, he
is savvy in the ways of Washington. (0955)
Cases where a suitable Implicit connective
could not be annotated between adjacent sentences
are annotated as either (a) ?EntRel?, where the
second sentence only serves to provide some fur-
ther description of an entity in the first sentence
(Example 3); (b) ?NoRel?, where no discourse re-
lation or entity-based relation can be inferred; and
(c) ?AltLex?, where the insertion of an Implicit
connective leads to redundancy, due to the rela-
tion being alternatively lexicalized by some ?non-
connective? expression (Example 4).
(3) C.B. Rogers Jr. was named chief executive officer of
this business information concern. Implicit=EntRel
Mr. Rogers, 60 years old, succeeds J.V. White, 64,
who will remain chairman and chairman of the ex-
ecutive committee (0929).
(4) One in 1981 raised to $2,000 a year from $1,500
the amount a person could put, tax-deductible,
into the tax-deferred accounts and widened cov-
erage to people under employer retirement plans.
Implicit=AltLex (consequence) [This caused] an ex-
plosion of IRA promotions by brokers, banks, mu-
tual funds and others. (0933)
Arguments of connectives are simply labelled
Arg2, for the argument appearing in the clause
syntactically bound to the connective, and Arg1,
for the other argument. In the examples here, Arg1
appears in italics, while Arg2 appears in bold.
The basic unit for the realization of an AO ar-
gument of a connective is the clause, tensed or un-
tensed, but it can also be associated with multiple
clauses, within or across sentences. Nominaliza-
tions and discourse deictics (this, that), which can
also be interpreted as AOs, can serve as the argu-
ment of a connective too.
The current version of the PDTB also contains
attribution annotations on discourse relations and
their arguments. These annotations, however, used
the earlier core scheme which is subsumed in the
extended scheme described in this paper.
The first release of the Penn Discourse
TreeBank, PDTB-1.0 (reported in PDTB-
Group (2006)), is freely available from
http://www.seas.upenn.edu/?pdtb.
PDTB-1.0 contains 100 distinct types of Explicit
connectives, with a total of 18505 tokens, anno-
tated across the entire WSJ corpus (25 sections).
Implicit relations have been annotated in three
sections (Sections 08, 09, and 10) for the first
release, totalling 2003 tokens (1496 Implicit
connectives, 19 AltLex relations, 435 EntRel
tokens, and 53 NoRel tokens). The corpus also
includes a broadly defined sense classification for
the implicit relations, and attribution annotation
with the earlier core scheme. Subsequent releases
of the PDTB will include Implicit relations
annotated across the entire corpus, attribution
annotation using the extended scheme proposed
here, and fine-grained sense classification for both
Explicit and Implicit connectives.
3 Annotation of Attribution
Recent work (Wiebe et al, 2005; Prasad et al,
2005; Riloff et al, 2005; Stoyanov et al, 2005),
has shown the importance of recognizing and rep-
resenting the source and factuality of information
in certain NLP applications. Information extrac-
tion systems, for example, would perform better
32
by prioritizing the presentation of factual infor-
mation, and multi-perspective question answering
systems would benefit from presenting informa-
tion from different perspectives.
Most of the annotation approaches tackling
these issues, however, are aimed at performing
classifications at either the document level (Pang
et al, 2002; Turney, 2002), or the sentence or word
level (Wiebe et al, 2004; Yu and Hatzivassiloglou,
2003). In addition, these approaches focus primar-
ily on sentiment classification, and use the same
for getting at the classification of facts vs. opin-
ions. In contrast to these approaches, the focus
here is on marking attribution on more analytic se-
mantic units, namely the Abstract Objects (AOs)
associated with predicate-argument discourse re-
lations annotated in the PDTB, with the aim of
providing a compositional classification of the fac-
tuality of AOs. The scheme isolates four key prop-
erties of attribution, to be annotated as features:
(1) source, which distinguishes between different
types of agents (Section 3.1); (2) type, which en-
codes the nature of relationship between agents
and AOs, reflecting the degree of factuality of the
AO (Section 3.2); (3) scopal polarity, which is
marked when surface negated attribution reverses
the polarity of the attributed AO (Section 3.3), and
(4) determinacy, which indicates the presence of
contexts due to which the entailment of attribu-
tion gets cancelled (Section 3.4). In addition, to
further facilitate the task of identifying attribution,
the scheme also aims to annotate the text span
complex signaling attribution (Section 3.5)
Results from annotations using the earlier attri-
bution scheme (PDTB-Group, 2006) show that a
significant proportion (34%) of the annotated dis-
course relations have some non-Writer agent as
the source for either the relation or one or both ar-
guments. This illustrates the simplest case of the
ambiguity inherent for the factuality of AOs, and
shows the potential use of the PDTB annotations
towards the automatic classification of factuality.
The annotations also show that there are a variety
of configurations in which the components of the
relations are attributed to different sources, sug-
gesting that recognition of attributions may be a
complex task for which an annotated corpus may
be useful. For example, in some cases, a rela-
tion together with its arguments is attributed to the
writer or some other agent, whereas in other cases,
while the relation is attributed to the writer, one
or both of its arguments is attributed to different
agent(s). For Explicit connectives. there were 6
unique configurations, for configurations contain-
ing more than 50 tokens, and 5 unique configura-
tions for Implicit connectives.
3.1 Source
The source feature distinguishes between (a) the
writer of the text (?Wr?), (b) some specific agent
introduced in the text (?Ot? for other), and (c)
some generic source, i.e., some arbitrary (?Arb?)
individual(s) indicated via a non-specific reference
in the text. The latter two capture further differ-
ences in the degree of factuality of AOs with non-
writer sources. For example, an ?Arb? source for
some information conveys a higher degree of fac-
tuality than an ?Ot? source, since it can be taken
to be a ?generally accepted? view.
Since arguments can get their attribution
through the relation between them, they can be an-
notated with a fourth value ?Inh?, to indicate that
their source value is inherited from the relation.
Given this scheme for source, there are broadly
two possibilities. In the first case, a relation
and both its arguments are attributed to the same
source, either the writer, as in (5), or some other
agent (here, Bill Biedermann), as in (6). (At-
tribution feature values assigned to examples are
shown below each example; REL stands for the
discourse relation denoted by the connective; At-
tribution text spans are shown boxed.)
(5) Since the British auto maker became a takeover
target last month, its ADRs have jumped about
78%. (0048)
REL Arg1 Arg2
[Source] Wr Inh Inh
(6) ?The public is buying the market when in re-
ality there is plenty of grain to be shipped,?
said Bill Biedermann  (0192)
REL Arg1 Arg2
[Source] Ot Inh Inh
As Example (5) shows, text spans for im-
plicit Writer attributions (corresponding to im-
plicit communicative acts such as I write, or I say),
are not marked and are taken to imply Writer attri-
bution by default (see also Section 3.5).
In the second case, one or both arguments have
a different source from the relation. In (7), for
example, the relation and Arg2 are attributed to
the writer, whereas Arg1 is attributed to another
agent (here, Mr. Green). On the other hand, in (8)
and (9), the relation and Arg1 are attributed to the
writer, whereas Arg2 is attributed to another agent.
33
(7) When Mr. Green won a $240,000 verdict in a land
condemnation case against the state in June 1983,
he says Judge O?Kicki unexpectedly awarded him
an additional $100,000. (0267)
REL Arg1 Arg2
[Source] Wr Ot Inh
(8) Factory orders and construction outlays were largely
flat in December while purchasing agents said
manufacturing shrank further in October. (0178)
REL Arg1 Arg2
[Source] Wr Inh Ot
(9) There, on one of his first shopping trips, Mr.
Paul picked up several paintings at stunning prices.
 Afterward, Mr. Paul is said by Mr. Guterman
to have phoned Mr. Guterman, the New York de-
veloper selling the collection, and gloated. (2113)
REL Arg1 Arg2
[Source] Wr Inh Ot
Example (10) shows an example of a generic
source indicated by an agentless passivized attri-
bution on Arg2 of the relation. Note that pas-
sivized attributions can also be associated with
a specific source when the agent is explicit, as
shown in (9). ?Arb? sources are also identified
by the occurrences of adverbs like reportedly, al-
legedly, etc.
(10) Although index arbitrage is said to add liquidity to
markets, John Bachmann,  says too much liq-
uidity isn?t a good thing. (0742)
REL Arg1 Arg2
[Source] Wr Ot Arb
We conclude this section by noting that ?Ot?
is used to refer to any specific individual as the
source. That is, no further annotation is provided
to indicate who the ?Ot? agent in the text is. Fur-
thermore, as shown in Examples (11-12), multiple
?Ot? sources within the same relation do not indi-
cate whether or not they refer to the same or differ-
ent agents. However, we assume that the text span
annotations for attribution, together with an inde-
pendent mechanism for named entity recognition
and anaphora resolution can be employed to iden-
tify and disambiguate the appropriate references.
(11) Suppression of the book, Judge Oakes observed ,
would operate as a prior restraint and thus involve
the First Amendment. Moreover, and
here Judge Oakes went to the heart of the question ,
?Responsible biographers and historians con-
stantly use primary sources, letters, diaries, and
memoranda. (0944)
REL Arg1 Arg2
[Source] Wr Ot Ot
(12) The judge was considered imperious, abrasive and
ambitious, those who practiced before him say .
Yet, despite the judge?s imperial bearing, no one
ever had reason to suspect possible wrongdoing,
says John Bognato, president of Cambria  .(0267)
REL Arg1 Arg2
[Source] Wr Ot Ot
3.2 Type
The type feature signifies the nature of the rela-
tion between the agent and the AO, leading to dif-
ferent inferences about the degree of factuality of
the AO. In order to capture the factuality of the
AOs, we start by making a three-way distinction
of AOs into propositions, facts and eventualities
(Asher, 1993). This initial distinction allows for
a more semantic, compositional approach to the
annotation and recognition of factuality. We de-
fine the attribution relations for each AO type as
follows: (a) Propositions involve attribution to an
agent of his/her (varying degrees of) commitment
towards the truth of a proposition; (b) Facts in-
volve attribution to an agent of an evaluation to-
wards or knowledge of a proposition whose truth
is taken for granted (i.e., a presupposed proposi-
tion); and (c) Eventualities involve attribution to
an agent of an intention/attitude towards an even-
tuality. In the case of propositions, a further dis-
tinction is made to capture the difference in the de-
gree of the agent?s commitment towards the truth
of the proposition, by distinguishing between ?as-
sertions? and ?beliefs?. Thus, the scheme for the
annotation of type ultimately uses a four-way dis-
tinction for AOs, namely between assertions, be-
liefs, facts, and eventualities. Initial determination
of the degree of factuality involves determination
of the type of the AO.
AO types can be identified by well-defined se-
mantic classes of verbs/phrases anchoring the at-
tribution. We consider each of these in turn.
Assertions are identified by ?assertive predi-
cates? or ?verbs of communication? (Levin, 1993)
such as say, mention, claim, argue, explain etc.
They take the value ?Comm? (for verbs of Com-
munication). In Example (13), the Ot attribution
on Arg1 takes the value ?Comm? for type. Im-
plicit writer attributions, as in the relation of (13),
also take (the default) ?Comm?. Note that when an
argument?s attribution source is not inherited (as
in Arg1 in this example) it also takes its own inde-
pendent value for type. This example thus conveys
that there are two different attributions expressed
within the discourse relation, one for the relation
and the other for one of its arguments, and that
both involve assertion of propositions.
34
(13) When Mr. Green won a $240,000 verdict in a land
condemnation case against the state in June 1983,
he says Judge O?Kicki unexpectedly awarded him
an additional $100,000. (0267)
REL Arg1 Arg2
[Source] Wr Ot Inh
[Type] Comm Comm Null
In the absence of an independent occurrence of
attribution on an argument, as in Arg2 of Exam-
ple (13), the ?Null? value is used for the type on
the argument, meaning that it needs to be derived
by independent (here, undefined) considerations
under the scope of the relation. Note that unlike
the ?Inh? value of the source feature, ?Null? does
not indicate inheritance. In a subordinate clause,
for example, while the relation denoted by the sub-
ordinating conjunction may be asserted, the clause
content itself may be presupposed, as seems to be
the case for the relation and Arg2 of (13). How-
ever, we found these differences difficult to deter-
mine at times, and consequently leave this unde-
fined in the current scheme.
Beliefs are identified by ?propositional attitude
verbs? (Hintikka, 1971) such as believe, think, ex-
pect, suppose, imagine, etc. They take the value
?PAtt? (for Propostional Attitude). An example of
a belief attribution is given in (14).
(14) Mr. Marcus believes spot steel prices will continue
to fall through early 1990 and then reverse them-
selves. (0336)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
Facts are identified by the class of ?factive and
semi-factive verbs? (Kiparsky and Kiparsky, 1971;
Karttunen, 1971) such as regret, forget, remember,
know, see, hear etc. They take the value ?Ftv?
(for Factive) for type (Example 15). In the current
scheme, this class does not distinguish between
the true factives and semi-factives, the former in-
volving an attitute/evaluation towards a fact, and
the latter involving knowledge of a fact.
(15) The other side , he argues knows Giuliani has al-
ways been pro-choice, even though he has personal
reservations. (0041)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Ftv Null Null
Lastly, eventualities are identified by a class of
verbs which denote three kinds of relations be-
tween agents and eventualities (Sag and Pollard,
1991). The first kind is anchored by verbs of influ-
ence like persuade, permit, order, and involve one
agent influencing another agent to perform (or not
perform) an action. The second kind is anchored
by verbs of commitment like promise, agree, try,
intend, refuse, decline, and involve an agent com-
mitting to perform (or not perform) an action. Fi-
nally, the third kind is anchored by verbs of ori-
entation like want, expect, wish, yearn, and in-
volve desire, expectation, or some similar mental
orientation towards some state(s) of affairs. These
sub-distinctions are not encoded in the annotation,
but we have used the definitions as a guide for
identifying these predicates. All these three types
are collectively referred to and annotated as verbs
of control. Type for these classes takes the value
?Ctrl? (for Control). Note that the syntactic term
control is used because these verbs denote uni-
form structural control properties, but the primary
basis for their definition is nevertheless semantic.
An example of the control attribution relation an-
chored by a verb of influence is given in (16).
(16) Eward and Whittington had planned to leave the bank
earlier, but Mr. Craven had persuaded them to re-
main until the bank was in a healthy position.
(1949)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Ctrl Null Null
Note that while our use of the term source ap-
plies literally to agents responsible for the truth of
a proposition, we continue to use the same term
for the agents for facts and eventualities. Thus,
for facts, the source represents the bearers of atti-
tudes/knowledge, and for considered eventualities,
the source represents intentions/attitudes.
3.3 Scopal Polarity
The scopal polarity feature is annotated on re-
lations and their arguments to primarily identify
cases when verbs of attribution are negated on the
surface - syntactically (e.g., didn?t say, don?t think)
or lexically (e.g., denied), but when the negation in
fact reverses the polarity of the attributed relation
or argument content (Horn, 1978). Example (17)
illustrates such a case. The ?but? clause entails an
interpretation such as ?I think it?s not a main con-
sideration?, for which the negation must take nar-
row scope over the embedded clause rather than
the higher clause. In particular, the interpretation
of the CONTRAST relation denoted by but requires
that Arg2 should be interpreted under the scope
of negation.
35
(17) ?Having the dividend increases is a supportive ele-
ment in the market outlook, but I don?t think it?s a
main consideration,? he says. (0090)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Comm Null PAtt
[Polarity] Null Null Neg
To capture such entailments with surface nega-
tions on attribution verbs, an argument of a con-
nective is marked ?Neg? for scopal polarity when
the interpretation of the connective requires the
surface negation to take semantic scope over the
lower argument. Thus, in Example (17), scopal
polarity is marked as ?Neg? for Arg2.
When the neg-lowered interpretations are not
present, scopal polarity is marked as the default
?Null? (such as for the relation and Arg1 of Ex-
ample 17).
It is also possible for the surface negation of at-
tribution to be interpreted as taking scope over the
relation, rather than an argument. We have not ob-
served this in the corpus yet, so we describe this
case with the constructed example in (18). What
the example shows is that in addition to entailing
(18b) - in which case it would be annotated par-
allel to Example (17) above - (18a) can also en-
tail (18c), such that the negation is intrepreted as
taking semantic scope over the ?relation? (Lasnik,
1975), rather than one of the arguments. As the
scopal polarity annotations for (18c) show, low-
ering of the surface negation to the relation is
marked as ?Neg? for the scopal polarity of the re-
lation.
(18) a. John doesn?t think Mary will get cured because
she took the medication.
b.   John thinks that because Mary took the
medication, she will not get cured.
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
[Polarity] Null Neg Null
c.   John thinks that Mary will get cured
not because she took the medication (but be-
cause she has started practising yoga.)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
[Polarity] Neg Null Null
We note that scopal polarity does not capture
the appearance of (opaque) internal negation that
may appear on arguments or relations themselves.
For example, a modified connective such as not
because does not take ?Neg? as the value for sco-
pal polarity, but rather ?Null?. This is consistent
with our goal of marking scopal polarity only for
lowered negation, i.e., when surface negation from
the attribution is lowered to either the relation or
argument for interpretation.
3.4 Determinacy
The determinacy feature captures the fact that the
entailment of the attribution relation can be made
indeterminate in context, for example when it ap-
pears syntactically embedded in negated or condi-
tional contexts.. The annotation attempts to cap-
ture such indeterminacy with the value ?Indet?.
Determinate contexts are simply marked as the de-
fault ?Null?. For example, the annotation in (19)
conveys the idea that the belief or opinion about
the effect of higher salaries on teachers? perfor-
mance is not really attributed to anyone, but is
rather only being conjectured as a possibility.
(19) It is silly libel on our teachers to think they would
educate our children better if only they got a few
thousand dollars a year more. (1286)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] PAtt Null Null
[Polarity] Null Null Null
[Determinacy] Indet Null Null
3.5 Attribution Spans
In addition to annotating the properties of attribu-
tion in terms of the features discussed above, we
also propose to annotate the text span associated
with the attribution. The text span is annotated as
a single (possibly discontinuous) complex reflect-
ing three of the annotated features, namely source,
type and scopal polarity. The attribution span also
includes all non-clausal modifiers of the elements
contained in the span, for example, adverbs and
appositive NPs. Connectives, however, are ex-
cluded from the span, even though they function
as modifiers. Example (20) shows a discontinu-
ous annotation of the attribution, where the paren-
thetical he argues is excluded from the attribution
phrase the other side knows, corresponding to the
factive attribution.
(20) The other side , he argues knows Giuliani has al-
ways been pro-choice, even though he has personal
reservations. (0041)
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Ftv Null Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
Inclusion of the fourth feature, determinacy,
is not ?required? to be included in the current
scheme because the entailment cancelling contexts
36
can	 be very complex. For example, in Exam-
ple (19), the conditional interpretation leading to
the indeterminacy of the relation and its arguments
is due to the syntactic construction type of the en-
tire sentence. It is not clear how to annotate the
indeterminacy induced by such contexts. In the
example, therefore, the attribution span only in-
cludes the anchor for the type of the attribution.
Spans for implicit writer attributions are left un-
marked since there is no corresponding text that
can be selected. The absence of a span annota-
tion is simply taken to reflect writer attribution,
together with the ?Wr? value on the source fea-
ture.
Recognizing attributions is not trivial since they
are often left unexpressed in the sentence in which
the AO is realized, and have to be inferred from the
prior discourse. For example, in (21), the relation
together with its arguments in the third sentence
are attributed to Larry Shapiro, but this attribution
is implicit and must be inferred from the first sen-
tence.
(21) ?There are certain cult wines that can command these
higher prices,? says Larry Shapiro of Marty?s, 
?What?s different is that it is happening with young
wines just coming out. We?re seeing it partly because
older vintages are growing more scarce.? (0071)
REL Arg1 Arg2
[Source] Ot Inh Inh
The spans for such implicit ?Ot? attributions
mark the text that provides the inference of the
implicit attribution, which is just the closest occur-
rence of the explicit attribution phrase in the prior
text.
The final aspect of the span annotation is that
we also annotate non-clausal phrases as the an-
chors attribution, such as prepositional phrases
like according to X, and adverbs like reportedly,
allegedly, supposedly. One such example is shown
in (22).
(22) No foreign companies bid on the Hiroshima project,
according to the bureau . But the Japanese prac-
tice of deep discounting often is cited by Ameri-
cans as a classic barrier to entry in Japan?s mar-
ket. (0501)
REL Arg1 Arg2
[Source] Wr Ot Inh
[Type] Comm Comm Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
Note that adverbials are free to pick their own type
of attribution. For example, supposedly as an at-
tribution adverb picks ?PAtt? as the value for type.
3.6 Attribution of Implicit Relations
Implicit connectives and their arguments in the
PDTB are also marked for attribution. Implicit
connectives express relations that are inferred by
the reader. In such cases, the writer intends for
the reader to infer a discourse relation. As with
Explicit connectives, implicit relations intended
by the writer of the article are distinguished from
those intended by some other agent introduced by
the writer. For example, while the implicit rela-
tion in Example (23) is attributed to the writer, in
Example (24), both Arg1 and Arg2 have been
expressed by someone else whose speech is be-
ing quoted: in this case, the implicit relation is at-
tributed to the other agent.
(23) The gruff financier recently started socializing in
upper-class circles. Implicit = FOR EXAMPLE
(ADD.INFO) Although he says he wasn?t keen on go-
ing, last year he attended a New York gala where
his daughter made her debut. (0800)
REL Arg1 Arg2
[Source] Wr Inh Inh
[Type] Comm Null Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
(24) ?We asked police to investigate why they are
allowed to distribute the flag in this way.
Implicit=BECAUSE (CAUSE) It should be con-
sidered against the law,?
said Danny Leish, a spokesman for the association .
REL Arg1 Arg2
[Source] Ot Inh Inh
[Type] Comm Null Null
[Polarity] Null Null Null
[Determinacy] Null Null Null
For implicit relations, attribution is also anno-
tated for AltLex relations but not for EntRel and
NoRel, since the former but not the latter refer to
the presense of discourse relations.
4 Summary
In this paper, we have proposed and described an
annotation scheme for marking the attribution of
both explicit and implicit discourse connectives
and their arguments in the Penn Discourse Tree-
Bank. We discussed the role of the annotations for
the recognition of factuality in natural language
applications, and defined the notion of attribution.
The scheme was presented in detail with exam-
ples, outlining the ?feature-based annotation? in
terms of the source, type, scopal polarity, and
determinacy associated with attribution, and the
?span annotation? to highlight the text reflecting
the attribution features.
37
Ackno


wledgements
The Penn Discourse TreeBank project is partially
supported by NSF Grant: Research Resources,
EIA 02-24417 to the University of Pennsylva-
nia (PI: A. Joshi). We are grateful to Lukasz
Abramowicz and the anonymous reviewers for
useful comments.
References
Nicholas. Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer, Dordrecht.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005.
Attribution and the (non)-alignment of syntactic and
discourse arguments of connectives. In Proceedings
of the ACL Workshop on Frontiers in Corpus Anno-
tation II: Pie in the Sky, Ann Arbor, Michigan.
Jaakko Hintikka. 1971. Semantics for propositional at-
titudes. In L. Linsky, editor, Reference and Modal-
ity, pages 145?167. Oxford.
Laurence Horn. 1978. Remarks on neg-raising. In
Peter Cole, editor, Syntax and Semantics 9: Prag-
matics. Academic Press, New York.
Lauri Karttunen. 1971. Some observations on factiv-
ity. Papers in Linguistics, 4:55?69.
Carol Kiparsky and Paul Kiparsky. 1971. Fact. In
D. D. Steinberg and L. A. Jakobovits, editors, Se-
mantics: An Interdisciplinary Reader in Philosophy,
Linguistics and Psychology, pages 345?369. Cam-
bridge University Press, Cambridge.
Howard Lasnik. 1975. On the semantics of nega-
tion. In Contemporary Research in Philosophi-
cal Logic and Linguistic Semantics, pages 279?313.
Dordrecht: D. Reidel.
Beth Levin. 1993. English Verb Classes And Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating discourse con-
nectives and their arguments. In Proceedings of the
HLT/NAACL Workshop on Frontiers in Corpus An-
notation, pages 9?16, Boston, MA.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), pages 79?86.
Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and
Bonnie Webber. 2004. Annotation and data mining
of the Penn Discourse Treebank. In Proceedings of
the ACL Workshop on Discourse Annotation, pages
88?95, Barcelona, Spain.
Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan
Lee, Eleni Miltsakaki, and Bonnie Webber. 2005.
The Penn Discourse TreeBank as a resource for nat-
ural language generation. In Proceedings of the
Corpus Linguistics Workshop on Using Corpora for
NLG.
Ellen Riloff, Janyce Wiebe, and Willian Phillips. 2005.
Exploiting subjectivity classification to improve in-
formation extraction. In Proceedings of the 20th Na-
tional Conference on Artificial Intelligence (AAAI-
2005).
Ivan A. Sag and Carl Pollard. 1991. An integrated
theory of complement control. Language, 67(1):63?
113.
The PDTB-Group. 2006. The Penn Discourse Tree-
Bank 1.0 Annotation Manual. Technical Report
IRCS-06-01, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania.
Veseli Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using
the OpQA corpus. In Proceedings of HLT-EMNLP.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of ACL 2002,
pages 417?424.
Bonnie Webber, Aravind Joshi, M. Stone, and Alis-
tair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29(4):545?587.
Bonnie Webber, Aravind Joshi, Eleni Miltsakaki,
Rashmi Prasad, Nikhil Dinesh, Alan Lee, and
K. Forbes. 2005. A short introduction to the PDTB.
In Copenhagen Working Papers in Language and
Speech Processing.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277?308.
Janyce Wiebe, Theresa Wilson, , and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 1(2).
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In Proceedings of the
ACL Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky, Ann Arbor, Michigan.
Hon Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of EMNLP-2003, pages
129?136, Saporo, Japan.
38
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 46?53,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Marking Time in Developmental Biology: Annotating Developmental
Events and their Links with Molecular Events
Gail Sinclair
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW
c.g.sinclair@ed.ac.uk
Bonnie Webber
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW
bonnie@inf.ed.ac.uk
Duncan Davidson
MRC Human Genetics Unit
Western General Hospital
Edinburgh EH4 2XU
Duncan.Davidson@hgu.mrc.ac.uk
Abstract
Current research in developmental biology
aims to link developmental genetic path-
ways with the processes going on at cel-
lular and tissue level. Normal processes
will only take place under specific sequen-
tial conditions at the level of the pathways.
Disrupting or altering pathways may mean
disrupted or altered development.
This paper is part of a larger work explor-
ing methods of detecting and extracting in-
formation on developmental events from
free text and on their relations in space and
time.
1 Introduction
Most relation extraction work to date on biomedi-
cal articles has focused on genetic and protein in-
teractions, e.g. the extraction of the fact that ex-
pression of Gene A has an effect on the expression
of Gene B. However where genetic interactions are
tissue- or stage-specific, the conditions that govern
the types of interactions often depend on where in
the body the interaction is happening (space) and
at what stage of life/development (time).
For genetic pathways involved in development,
it is critical to link what is happening at the molec-
ular level to changes in the developing tissues,
usually described in terms of processes such as
tubulogenesis and epithelialization (both involved
in the development of the kidney) and where they
are happening.
The processes themselves are usually linked
to stages rather than precise time points and
spans like ?6.15pm EST?, ?March 3?, ?last year?.
Within the developmental mouse community,
there are at least two different ways of specifying
the developmental stage of an embryo - Theiler
stages (TS), and days post coitum/embryonic
day (d.p.c./E). However, these cannot be simply
mapped to one another as can days, weeks and
years. Embryonic days are real time stages in-
dependent of the state of the embryo and dated
from an assumption about when (approximately)
the relevant coitus must have taken place, while
Theiler stages are relative stages dependent on the
processes an embryo is undergoing.
Developmental stages can be also be referred to
implicitly, by the state of the embryo or the pro-
cesses currently taking place within it. This is be-
cause during development, tissues form, change,
merge or even disappear. So if the embryo is un-
dergoing tubulogenesis, one can assume that its
developmental stage is (loosely) somewhere be-
tween TS20 and birth. If the text refers to induced
mesenchyme during a description of tubulogene-
sis, one can assume that this change in the mes-
enchyme is the (normal) consequence of the Wolf-
fian duct invading the metanephric mesenchyme.
The invasion is known to occur around 10.5 d.p.c
so the induced mesenchyme must come into exis-
tence soon after this time.
Temporal links between developmental events
may be indicated explicitly (e.g. first, a tubule de-
velops into a comma-shaped body, which then de-
velops into an S-shaped body), but they are more
likely to be indicated implicitly by their order-
ing in the text and by associative (or ?bridging?)
anaphora where the anaphor refers to the result of
a previously mentioned process, e.g. the induction
of metanephric mesenchyme as one event, and a
subsequent mention of induced mesenchyme (an
?associative? or ?bridging? reference) within an-
other event, suggesting the former event occurred
before the latter.
46
Figure 1: Partial genetic pathway for early kid-
ney morphogenesis. The arrows show directed in-
teractions between genes that are required for the
specified processes. E.g Pax2 interacts with (acti-
vates) Six2 which, together with Sall1 and Wt1,
is required for differentiation of the mesenchy-
mal cells in the metanephric mesenchyme. Image
taken from (Ribes et al, 2003).
This work on linking molecular and develop-
mental events mentioned in text on development
is also meant to deal with the problem that no
one article ever fully describes a topic. The par-
tial genetic interaction network in Figure 1 has
been built from several different studies and not
determined from just a single experiment. So
not only does the information within one article
need to be mined for useful information - the in-
formation across articles needs to be associated
with each other with respect to temporal, spatial
and experimental grounding. Eccles et al (2002)
states that Pax2 is required for differentiation of
mesenchymal cells during kidney morphogenesis,
while Sajithlal et al (2005) states that Eya1 is re-
quired. However these two results by themselves
do not help us determine whether these require-
ments are independent of one another or whether
they are required at different stages or in different
parts of metanephric mesenchyme or whether the
two genes interact. The conditions involved in the
experiments, most importantly the temporal con-
ditions, can help to link the two events.
This work aims to develop methods for extract-
ing information from text that will ground ge-
netic pathways (molecular events) with regard to
tissue location, developmental process and stage
of embryonic development - that is, their spatio-
temporal context. The task at hand is to recognise
how biologists write about developmental events
and then adapt existing or formulate new natu-
ral language processing techniques to extract these
events and temporally relate them to each other.
The resultant information can then be used both
for database curation purposes and for visualisa-
tion, i.e. to enrich pathway diagrams such as Fig-
ure 1, with information such as when and where
the interactions take place, what type of inter-
actions are involved (physical, activation, inhibi-
tion), the origin of this information and other as-
sociated information.
2 Notions of Time
As previously mentioned, there are different ways
of calibrating for developmental stages, and they
cannot simply be mapped to one another. The two
most common stage notations for mouse develop-
ment are Theiler stages, TS, and Embryonic days,
E (equivalent to days post coitum, d.p.c.). The lat-
ter are self explanatory in that they denote the 24
hour day and can be considered real-time staging.
47
The convention was originally that E11 would rep-
resent the 24 hour period of the 11th day. It is,
however, now common to find E11.5 representing
the same time period, but this is merely a change
in convention due to standard practices of experi-
mentation.
A Theiler stage on the other hand represents
a non-fixed relative time period defined by the
progress of development rather than directly in
terms of the passage of time. Theiler Stages
(Theiler, 1989) divide mouse development into 26
prenatal and 2 postnatal stages. In general, Theiler
used external features that can be directly assessed
by visual inspection of the live embryo as devel-
opmental landmarks to define stages. The Edin-
burgh Mouse Atlas Project (EMAP)1 uses Theiler
stages to organise anatomical terms in their Mouse
Atlas Nomenclature (MAN). EMAP gives a brief
description of each Theiler stage with TS25 as an
example as follows:
Skin wrinkled
The skin has thickened and formed wrinkles and
the subcutaneous veins are less visible. The fin-
gers and toes have become parallel and the um-
bilical hernia has disappeared. The eyelids have
fused. Whiskers are just visible.
Absent: ear extending over auditory meatus, long
whiskers.
An embryo is in TS25 at approximately 17 d.p.c.
As can be seen in Figure 2, an embryo at E11
could be considered in Theiler stage 17, 18 or 19,
i.e. Theiler stages can overlap one another with
respect to Embryonic day. Indeed, here, TS17 can
fully encompass TS18 in the dpc timeline.
The development of internal structures is ap-
proximately correlated with external develop-
ments, so except for fine temporal differences,
the Theiler stages can be assumed to apply to the
whole embryo. Theiler stages provide only gross
temporal resolution of developmental events, and
the development of internal structures often take
place within the boundaries of one of these stages
or overlapping stage boundaries. Thus, internal
developmental processes can also have their own
finer relative timeline or staging.
There is no ontology or reference book that
comprehensively specifies this finer staging and
the knowledge of the biologist as the reader of ar-
1Edinburgh Mouse Atlas Project -
http://genex.hgu.mrc.ac.uk/
Figure 2: Graphic of kidney morphogenesis an-
notated with the two standard staging notations
for mouse development. At E10.5 the Wolffian
duct invades the metanephric mesenchyme form-
ing the ureteric bud around E11. The bud then
branches around E11.5 and continues to do so un-
til birth, forming the ultimate functional units of
the kidney - the nephrons. TS = Theiler Stage, E
= Embryonic day/dpc. This image is adapted from
http://www.sciencemuseum.org.uk/
ticles is relied upon. This work will contribute to
making this deeper staging criteria explicit.
3 Annotation
3.1 Event Classification
As a first step, a Gold Standard corpus of 988 sen-
tences was developed with each sentence being
classified as containing the description of a devel-
opmental and/or molecular event or not. 385 sen-
tences were classified as positive, with 603 neg-
ative. Named entities within all these sentences
were also annotated. Among these element types
were stage, process and tissue. A Naive Bayes
automatic classifier for sentence classification was
developed using this Gold Standard resulting in
a balanced F-score of 72.3% for event classifica-
tion. (A manual rule-based approach resulted in
an F-score of 86.6%, but this has yet to be fully
investigated for automation. Guessing positive for
all sentences would give a balanced F-score of
58.4%)
3.2 Event Specifications
Two event types are of interest in this work -
molecular and tissue events. The former involve
the action (and possible effect) of molecules dur-
48
ing development and the latter involves the devel-
opment of the tissues themselves. A description of
an event can be expected to contain the following
elements:
  molecular or tissue event type (e.g. expres-
sion, inhibition)
  stage or temporal expression (e.g. after X,
subsequent to X, E11)
  at least one of
? molecule name, anatomical term, bio-
logical process term
The informational elements included within an
event description can then be used to relate events
to each other. Specifically, processes involve
known tissues and are known to happen during
certain stages, just as the relative order of pro-
cesses, tissue formations and stages are known.
While an initial specification of an event may
be associated with a single sentence, clause or
phrase, not all the elements of relevance to this
work may be specified there. In particular, an in-
formational element of the event may be explic-
itly and fully stated in this initial event specifica-
tion, or it may be underspecified or it may be miss-
ing. For those that are underspecified or missing,
background knowledge about other elements and
events may need to be taken into consideration in
order for them to be fully resolved (see Section
4.2).
The following is a straightforward example
where the given sentence specifies all the main el-
ements required for a molecular event.
1. At E11, the integrin ?8 subunit was expressed
throughout the mesenchyme of the nephro-
genic cord.
  Molecular Event : expression
  molecule name: integrin ?8
  anatomical term: mesenchyme of the
nephrogenic cord
  stage: E11
Example 2 shows that a single sentence may
specify more than one event.
2. Prior to formation of the ureteric bud,
no ?8 expression was evident within the
mesenchyme that separates the urogenital
ridge from the metanephric mesenchyme and
within the metanephric mesenchyme itself.
  EVENT-0
? Tissue Event : formation of anatom-
ical term
? anatomical term: ureteric bud
? stage/temporal expression = missing
  EVENT-1
? Molecular Event: absence of expres-
sion
? molecule name: ?8
? anatomical term: mesenchyme that
separates the urogenital ridge from
the metanephric mesenchyme
? temporal expression:Prior to
EVENT-0
  EVENT-2
? Molecular Event: absence of expres-
sion
? molecule name: ?8
? anatomical term: metanephric mes-
enchyme
? temporal expression: Prior to
EVENT-0
EVENT-0 is not the focus of this sentence, but
rather a reference event. Its attributes need to be
recorded so that the stage of the other events can
be determined.
TimeML (Pustejovsky et al, 2004) is a spec-
ification language designed for the annotation
of temporal and event information. Although
TimeML is not currently being used as a method
of representation for this work, Example 1 above
could be represented as follows:
 SIGNAL sid=?s1? type=?temporal? 
At
 /SIGNAL 
 TIMEX tid=?t1? type=?STAGE?
value=?E11? 
E11
 /TIMEX 
the integrin ?8
 EVENT eid=?e1? class=?molecular? 
was expressed
 /EVENT 
throughout the mesenchyme of the
 SIGNAL sid=?s2? type=?tissue? 
nephrogenic cord
 /SIGNAL 
nephrogenic cord can be considered a signal of
type ?tissue? as it does not exist throughout the
49
whole of development and so can indicate or rule
out time periods for this event description.
3.3 Event Time-Stamping
The relative timing of any biological processes
mentioned in the event descriptions first needs to
be determined before we can work out when the
actual events described are taking place.
Schilder and Habel (2001) looked beyond the
core temporal expressions and into prepositional
phrases that contained temporal relations, i.e. be-
fore, during, etc and introduced the notion of noun
phrases as event-denoting expressions. An event
that is described as occurring ?after the election?
does not have an explicit time-stamp attached to it,
but the knowledge about the timing of the election
mentioned gives the reader a notion of when in ab-
solute time the event occurred. This is similar to
Example 2 above where Event-0 is the reference
event, thus biological processes can be considered
event-denoting expressions.
While Schilder and Habel rely on prepositional
phrases to designate their event-denoting noun
phrases, for this work propositional phrases are
not necessarily required. The mention of a noun
phrase by itelf may be enough. In developmen-
tal biology, tissues may only be extant for a lim-
ited period before they form into some other tissue
and these can also be used as event-denoting ex-
pressions - for example, comma-shaped bodies are
structures within the developing kidney that are
only in existence for a relatively short time period -
before the existence of the S-shaped bodies and af-
ter epithelialization. Therefore the mention of tis-
sues as well as processes can help to pinpoint the
timing of the event being described. While they
may not ultimately bring us to the exact stage the
event is occurring in, it can at least rule out some
spans of time. We discuss this further in Section
4.2.
In order for events to be linked to one another,
it is necessary to uniquely index each event and its
elements. Mapping across indices will be utilised
so that known relationships between elements can
be represented. For example, E10 comes before
E12, tubulogenesis occurs during kidney morpho-
genesis, and the proximal tubule is part of the
nephron.
Of the elements types listed in Section 3.2, only
the molecule element cannot be used to resolve
developmental stage while tissue, process, stage
and, of course, temporal expression can. Other el-
ements are also of interest to the biologist and inte-
gral to development and molecular function, how-
ever they are not of use in the grounding of events
in time.
4 Initial Investigations
This section demonstrates that one must look be-
yond the sentence in order to resolve the temporal
aspects of events.
4.1 Evidence for Developmental Stage
Evidence sufficient to resolve developmental stage
can come from many places. 314 positive sen-
tences from the Gold Standard corpus and their
context were examined, and the evidence required
to resolve developmental stage for each of the
events mentioned there was determined as shown
in Table 1.
As can be seen from the table, only 48 out of
the 314 event sentences (i.e. 15%) have the de-
velopmental stage in which the event is occurring
explicitly stated in the given sentence, (e.g. Ex-
ample 1 in Section 3). So other means need to be
explored in order to ground events with respect to
developmental stage. An event sentence may be a
continuation of a topic, and so the specific devel-
opmental stage involved may well be stated in the
immediately surrounding or related text.
Information in the immediately surrounding
text (rows labelled Following Sentence, Previous
Sentence and Current Paragraph) resolves the de-
velopmental stage of the event in 64 cases (i.e.
21%). This most commonly occurs by looking for
the immediately previously mentioned stage, and
in one case the next encountered stage.
Event sentences also often refer to figures, and
so the stage being described in the caption (i.e.
legend) of the referenced figure will often be the
same as the one relevant to the sentence. (This was
true of all sentences looked at that referenced a fig-
ure.) Figures, however, are generally only found in
the Results sections and so this type of evidence is
not often going to be of use for sentences found in
other sections of an article.
Similarly, events can be described within the
figure legends themselves. The concise and simple
way in which legends are generally written mean
that the explicit stage is commonly referred to, and
so stage can be resolved using this referenced in-
formation(43 out of 47 cases, i.e. 91%).
50
Source of Evidence Abstract Introduction Results Discussion Methods Totals
Time Irrelevant 7 12 22 23 1 65
Prior Knowledge 17 33 31 45 0 126
Following Sentence 0 0 1 0 0 1
Previous Sentence 0 0 7 0 0 7
Current Paragraph 0 0 18 1 0 19
Reference to Figure 0 0 38 0 0 38
Within Fig Legend 0 0 43 0 0 43
(time not resolved) 0 3 1 0 0 4
Explicitly Stated 0 1 41 5 1 48
(not relevant) 0 0 1 0 0 1
Totals 24 49 165 74 2 314
Table 1: Location and type of evidence sufficient to resolve developmental stage in sentences. Time
Irrelevant indicates that the event being described is not time critical, i.e. event is a constant over devel-
opmental timeline, or end result. Prior knowledge means temporal information other than that found in
the current paragraph but associated with current event such as tissue and process is required for temporal
resolution. This may be found in the current article or from previously curated information (assuming
accurate terminology mapping.) Text from outside the current paragraph cannot be relied upon to be rel-
evant to the current sentence without additional information. time not resolved means the stage could not
be pinpointed using the figure legend. not relevant indicates that although an explicit stage was referred
to within the sentence, this was not relevant to the event being described, e.g. event and stage in different
clauses of the sentence.
Table 2 shows a similar table to Table 1, but
deals only with those sentences found within fig-
ure legends. It shows where within the figure leg-
end the required evidence for developmental stage
can be found. As can be seen, in 80% of these
cases the relevant developmental stage can be as-
certained directly from the legend. It should be
noted that figure legends in biological articles tend
to be much lengthier than those from NLP articles.
In 21% of the event sentences, a specific devel-
opmental stage is not relevant to the fact being de-
scribed (first row of Table 1), e.g. the kidneys of
the double mutants were located more caudal and
medial than normal. This sentence is describing
an end result, i.e. an affected or normal kidney
at birth (although this could, of course, be con-
sidered a developmental stage.) Alternatively, the
time-irrelevant event being described could be a
non-event, e.g. the fact that a gene is never ex-
pressed in a particular tissue. Similarly, this could
be considered as the developmental stage range
from conception to birth.
The significantly small proportion of event sen-
tences located in Abstracts (24 of 314 total event
sentences, less than 8%) demonstrates the need to
use full text. Even where an event is described
within an Abstract, it is rarely accompanied by
associated processes or tissues specific enough to
suggest the stage of development never mind an
explicit timestamp, as it is, by necessity, only gen-
erally describing the whole article. The majority
of BioNLP work is being done with the use of Ab-
stracts only. This is because of their relative ease
of access compared with full text, but methods de-
veloped using Abstracts only will not necessarily
be as effective when applied to full text.
As can be seen, the majority of temporally-
underspecified event sentences are situated in the
Results section of the articles. Indeed, this is
the section where most event sentences are to be
found. This work is initially focussing on event
descriptions found in Results sections of articles as
these will focus on the work done by the authors
and their findings and will not generally include
modality in the event descriptions as Introduction
and Discussion sections might. As shown above,
the Methods section rarely contains event descrip-
tions and when they do they are usually about what
the experiment aims to show and so this should be
repeated in the Results section.
4.2 Prior Knowledge
As mentioned earlier, if none of the above sources
reveal the relevant stage of an event, then other el-
ements within the sentence, such as tissue or pro-
cess, need to be looked at so that prior knowledge
51
Source of Evidence Figure Legends
Time Irrelevant 4
Prior Knowledge 4
Following Sentence 0
Previous Sentence 14
Current Paragraph 13
Explicitly Stated 11
Total 47
Table 2: Location and type of evidence sufficient to resolve developmental stage in sentences within
figure legends. Rows as in Table 1, with Current Paragraph being equal to the whole of the legend.
about those elements can be exploited for devel-
opmental stage to be resolved. For example, given
the sentence
Prior to formation of the ureteric bud,
no ?8 expression was evident within the
mesenchyme that separates the urogen-
ital ridge from the metanephric mes-
enchyme and within the metanephric
mesenchyme itself.
the developmental stage can be resolved if we
know when the ureteric bud forms (TS17/E10.5).
It could also be the case that the other tissues
or processes mentioned have a specific lifetime
within development and these could help to fur-
ther pinpoint the timeline involved for the lack of
?8 expression. For example,
Pax2 was initiating in the metanephric
mesenchyme undergoing induction.
It is not so straightforward to assign a stage here,
since the mesenchyme is constantly being induced
from E11 (TS18) until birth (TS26), but we have
at least discounted E1-E10 (TS1-TS17) as relevant
stages.
Resources such as the Mouse Atlas Nomencla-
ture (MAN) (Ringwald et al, 1994) will provide
the initial prior knowledge in order to resolve de-
velopmental stage of events. This describes the
different stages of development and the tissues in
evidence at each stage, giving what is known as the
abstract mouse. From this abstract mouse, we can
ascertain the normal stage ranges where tissues ex-
ist and use this knowledge for temporal resolution,
taking care not to assume that tissues do not neces-
sarily exist within the same stage range in mutant
mice than in wild-type. The prior knowledge data-
bank can be recursively added to with facts from
events already extracted from papers for use in fur-
ther event extraction and their anchoring in time.
5 Future Work
5.1 Term Normalisation
There is no point extracting events descriptions if
we cannot relate the events and their elements to
each other. The event-denoting expressions iden-
tified need to be normalised so that it can be recog-
nised when two terms are referring to the same el-
ement.
Inconsistent terminology in the biomedical field
is a known problem (Sinclair et al, 2002). One
gene can have several names (synonymy) just as
the same name can be used for more than one gene
(homonymy). Very often the synonyms bear no re-
lation to one another since they were perhaps con-
currently discovered in different laboratories and
named. For example, the gene insomnia can also
be known as cheap date, since experiments found
that organisms without this gene have a tendency
to fall asleep and are particularly susceptible to
alcohol. The same anatomical part can also be
referred to by different terms, e.g. the Wolffian
duct is also known as the nephric duct, and the
metanephros is another name for the kidney. There
is also a lineage issue, where a tissue with one
name (or perhaps more) develops into something
with another name (e.g. the intermediate meso-
derm gives rise to both the Wolffian duct and the
metanephric mesenchyme which in turn both de-
velop into the metanephros. The MAN includes
this type of information.
Term normalisation is particularly important for
the process and tissue elements. If these terms
are not normalised, temporal knowledge about the
terms may not be exploited and it may not be de-
termined that events involving them are linked.
52
5.2 Event Elements
If the elements required to fully describe an event
are explicitly stated within a simple sentence, then
temporal grounding will be straightforward. How-
ever, this is unlikely to often be the case. More
complex sentences will dictate the need for de-
pendency relations to be determined so that each
event?s elements can be identified. Methods for
dealing with missing or underspecified elements
that are not resolved within the event description
itself will be investigated.
A naive approach will first be investigated to
fill these gaps: find the closest appropriate ele-
ment in the previous context (varying the size of
the window for how far back to look, such as cur-
rent paragraph or last 3 sentences). An error anal-
ysis on this simple method will help to guide the
amount of further work necessary to achieve equal
success across all elements. For those elements
that this method is ineffective, other methods will
be developed incorporating features such as sen-
sitivity to syntax, event type and location within
article. Similarly, it will be established whether
different techniques are required for missing infor-
mation than for underspecified information. They
will first be treated in the same manner with anal-
ysis determining whether they should be treated
differently.
6 Conclusion
This ongoing work has shown the importance of
relative time lines in order to link events to one
another. The identification of event elements and
their normalisation will then form a basis for rea-
soning over these elements with regards to first
time-stamping of events and then temporally relat-
ing the events. The aim of many BioNLP studies
is ultimately to reason over extracted events and,
as such, the relative timing of these events is cru-
cial. For example, if we know
1. tissue X is transformed into tissue Y at stage S
and
2. molecule M is expressed in X at stage S-1,
then it can be reasoned that event 2 has an im-
pact on event 1. This reasoning can be made more
successful if we know as much about the events
as possible, not just that tissue Y is formed and
molecule M is expressed.
It has also been demonstrated that we not only
need to look beyond the sentence level for tempo-
ral resolution but also beyond the article in order to
replicate the reader?s assumed level of background
knowledge.
References
J. F. Allen, Towards a general theory of action andtime, Artificial Intelligence, vol 23, pp 123-154,
1984.
M. R. Eccles, S. He, M. Legge, R. Kumar, J. Fox, C.Zhou, M. French and R. W. Tsai, .PAX genes indevelopment and disease: the role of PAX2 in uro-
genital tract development. Int J Dev Biol, vol 46, no4, pp 535-44, 2002.
J. Pustejovsky, I. Mani, L. Belanger, B. Bogurev, B.Knippen, J. Littman, A. Rumshisky, A. See, S.
Symonen, J. Van Guilder, L. Van Guilder and M.Verhagen, The Specification Langeuage TimeML.in The Language of Time: A Reader, Oxford Uni-
versity Press, 2004.
D. Ribes, E. Fischer, A. Calmont and J. Rossert, Tran-scriptional Control of Epithelial Differentiation dur-ing Kidney Development. J Am Soc Nephrol, vol
14, pp S9-S15, 2003.
M. Ringwald, R. A. Baldock, J. Bard, M. H. Kaufman,J. T. Eppig, J. E. Richardson, J. H. Nadeau and D.Davidson, A database for mouse development. Sci-ence, vol 265, pp 2033-2034, 1994.
G. Sajithlal, D. Zou, D. Silvius and P. X. Xu, Eya 1 actsas a critical regulator for specifying the metanephricmesenchyme. Dev Biol, vol 284, no 2, pp 323-36,
2005.
F. Schilder and C. Habel, From Temporal Expres-sions to Temporal Information: Semantic Taggingof News Message., in Proceedings of the ACL 2001Workshop on Temporal and Spatial Information Pro-cessing, Toulouse, France, pp 88-95.
G. Sinclair, B. Webber and D. Davidson, EnhancedNatural Language Access to Anatomically Indexed
Data. in Proceedings of the ACL 2002 Workshopon Natural Language Processing in the BiomedicalDomain, Philadelphia, pp 45-52.
K. Theiler, The House Mouse. Atlas of Embryonic De-velopment. Springer Verlag New York, 1989.
53
BioNLP 2007: Biological, translational, and clinical language processing, pages 197?198,
Prague, June 2007. c?2007 Association for Computational Linguistics
Marking Time in Developmental Biology
Gail Sinclair and Bonnie Webber
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW
c.g.sinclair@ed.ac.uk, bonnie@inf.ed.ac.uk
1 Introduction
In developmental biology, to support reasoning
about cause and effect, it is critical to link genetic
pathways with processes at the cellular and tissue
level that take place beforehand, simultaneously or
subsequently. While researchers have worked on re-
solving with respect to absolute time, events men-
tioned in medical texts such as clinical narratives
(e.g. Zhou et al 2006), events in developmental bi-
ology are primarily resolved relative to other events.
In this regard, I am developing a system to extract
and time-stamp event sentences in articles on devel-
opmental biology, looking beyond the sentence that
describes the event and considering ranges of times
rather than just single timestamps.
I started by creating four gold standard corpora
for documents, event sentences, entities and times-
tamped events (for future public release). These
datasets are being used to develop an automated
pipeline to (1) retrieve relevant documents; (2) iden-
tify sentences within the documents that describe de-
velopmental events; and (3) associate these events
with the developmental stage(s) that the article links
them with or they are known to be linked with
through prior knowledge.
Different types of evidence are used in each step.
For determining the relevant developmental stage(s),
the text surrounding an event-containing sentence is
an efficient source of temporal grounding due of its
immediate accessibility. However, this does not al-
ways yield the correct stage and other sources need
to be used. Information within the sentence, such
as the entities under discussion, can also be used
to help with temporal grounding using mined back-
ground knowledge about the period of existence of
an entity.
2 Creation of Datasets
In creating the four new data sets mentioned above,
I annotated 1200 documents according to relevance
to murine kidney development. From 5 relevant
documents, 1200 sentences were annotated as to
whether they contained an event description. (Two
annotators - one biologist, one computer scientist -
achieved an inter-annotator agreement kappa score
of 95%.) A sentence is considered a positive one if
it contains a description of the following event types:
? molecular expression within tissue/during pro-
cess/at stage X (molecular event)
? tissue process, i.e. what forms from what (tis-
sue event)
? requirement of a molecule for a process
(molecular or tissue event)
? abnormality in a process/tissue/stage (molecu-
lar or tissue event)
? negation of the above e.g. was not expressed,
did not form, formed normally (molecular or
tissue event).
A negative sentence is one that does not fall under at
least one of the above categories.
In addition, 6 entities (tissue, process, species,
stage, molecule and event verb) were annotated in
1800 sentences (1200 described above + 600 from
197
relevant documents not yet annotated at sentence
level) and 347 entity-annotated positive event sen-
tences were marked with their associated develop-
mental stage.
Example: At E11, the integrin ?8 subunit was ex-
pressed throughout the mesenchyme of the nephro-
genic cord. Entities annotated: E11(stage), integrin
?8 (molecule), expressed (event verb), mesenchyme
of the nephrogenic cord (tissue).
3 Evidence for Temporal Resolution
Developmental biology is not as concerned with the
absolute time of events in a specific embryo as it
is with events that generally happen under the same
circumstances in developmental time. These are re-
ferred to with respect to stages from conception to
birth. The evidence sufficient to resolve the devel-
opmental stage of an event sentence can come from
many places. The two significant areas of evidence
are local context (i.e. surrounding text) and prior
(i.e. background) knowledge.
Local context can further be classified as:
? explicit: evidence of stage is mentioned within
current (event) sentence,
? previous sentence: evidence is found in sen-
tence immediately previous to current sentence,
? following sentence: evidence is found in sen-
tence immediately following current sentence,
? current paragraph: evidence is found in para-
graph containing current sentence but not in ad-
jacent sentences,
? referenced to figure: evidence is found in fig-
ure legend referenced in current sentence.
Evidence Source # Event Sentences
Explicitly Stated 48
Immed Prev Sentence 7
Following Sentence 1
Current Paragraph 19
Referenced Figure Legend 38
Within Figure Legend 43
Time Irrelevant 65
Prior Knowledge 126
Total 347
When local context does not provide evidence, prior
knowledge can be used about when entities men-
tioned within the sentence normally appear within
development. Event sentences can also be irrel-
evant of individual time ranges and apply to the
whole of development. The table above shows the
frequency with which each evidence type is used to
resolve developmental stage.
4 Experiments
Event sentence retrieval experiments (using separate
training and test data) resulted in a F-score of 72.3%
and 86.6% for Naive Bayes and rule-based classifi-
cation approaches respectively (relying upon perfect
entity recognition). A baseline method (classifying
all sentences as positive) achieves 58.4% F-score.
Experiments were also carried out to assign devel-
opmental stage to sentences already known to con-
tain events. The baseline approach is to use the last
mentioned stage in the text and any methods devel-
oped should score higher than this baseline. Rules
were developed to assign developmental stage based
on the knowledge gained from two fifths of the in-
vestigations into temporal evidence described above.
The other three fifths were annotated after the rules
had been defined. Precision scores for all 347 sen-
tences can be seen in the following table with the
Naive method representing the baseline and Local
representing the use of rules.
Paper Naive Prec. Local Prec.
1 75.7 97.3
2 89.6 90.9
3 89.1 100
4 95.6 92.3
5 95.5 91.3
Average 89.1 94.5
Experiments are currently ongoing into exploiting
the use of background knowledge of the develop-
mental processes and tissues mentioned within event
descriptions in order to assign developmental stage
to events sentences not already assigned by the lo-
cal context rules and to increase confidence in those
stages already assigned.
References
L. Zhou, G. B. Melton, S. Parsons and G Hripcsak, A tempo-
ral constraint structure for extracting temporal information from
clinical narrative, J Biomed Inf 39(4), Aug 2006, 424-439
198
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 41?48,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Question Answering based on Semantic Roles
Michael Kaisser Bonnie Webber
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
Scotland
m.kaisser@sms.ed.ac.uk, bonnie@inf.ed.ac.uk
Abstract
This paper discusses how lexical resources
based on semantic roles (i.e. FrameNet,
PropBank, VerbNet) can be used for Ques-
tion Answering, especially Web Question
Answering. Two algorithms have been im-
plemented to this end, with quite different
characteristics. We discuss both approaches
when applied to each of the resources and a
combination of these and give an evaluation.
We argue that employing semantic roles can
indeed be highly beneficial for a QA system.
1 Introduction
A large part of the work done in NLP deals with
exploring how different tools and resources can be
used to improve performance on a task. The quality
and usefulness of the resource certainly is a major
factor for the success of the research, but equally so
is the creativity with which these tools or resources
are used. There usually is more than one way to
employ these, and the approach chosen largely de-
termines the outcome of the work.
This paper illustrates the above claims with re-
spect to three lexical resources ? FrameNet (Baker
et al, 1998), PropBank (Palmer et al, 2005) and
VerbNet (Schuler, 2005) ? that convey information
about lexical predicates and their arguments. We de-
scribe two new and complementary techniques for
using these resources and show the improvements to
be gained when they are used individually and then
together. We also point out problems that must be
overcome to achieve these results.
Compared with WordNet (Miller et al, 1993)?
which has been used widely in QA?FrameNet, Prop-
Bank and VerbNet are still relatively new, and there-
fore their usefulness for QA has still to be proven.
They offer the following features which can be used
to gain a better understanding of questions, sen-
tences containing answer candidates, and the rela-
tions between them:
? They all provide verb-argument structures for a
large number of lexical entries.
? FrameNet and PropBank contain semantically
annotated sentences that exemplify the under-
lying frame.
? FrameNet contains not only verbs but also lex-
ical entries for other part-of-speeches.
? FrameNet provides inter-frame relations that
can be used for more complex paraphrasing to
link the question and answer sentences.
In this paper we describe two methods that use
these resources to annotate both questions and sen-
tences containing answer candidates with seman-
tic roles. If these annotations can successfully be
matched, an answer candidate can be extracted. We
are able, for example, to give a complete frame-
semantic analysis of the following sentences and to
recognize that they all contain an answer to the ques-
tion ?When was Alaska purchased??:
The United States purchased Alaska in 1867.
Alaska was bought from Russia in 1867.
In 1867, Russia sold Alaska to the United States.
The acquisition of Alaska by the United States
in 1867 is known as ?Seward?s Folly.
41
The first algorithm we present uses the three
lexical resources to generate potential answer-
containing templates. While the templates contain
holes ? in particular, for the answer ? the parts that
are known can be used to create exact quoted search
queries. Sentences can then be extracted from the
output of the search engine and annotated with re-
spect to the resource being used. From this, an an-
swer candidate (if present) can be extracted. The
second algorithm analyzes the dependency structure
of the annotated example sentences in FrameNet and
PropBank. It then poses rather abstract queries to the
web, but can in its candidate sentence analysis stage
deal with a wider range of syntactic possibilities. As
we will see, the two algorithms are nicely comple-
mentary.
2 Method 1: Question Answering by
Natural Language Generation
The first method implemented uses the data avail-
able in the resources to generate potential answer
sentences to the question. While at least one com-
ponent of such a sentence (the answer) is yet un-
known, the remainder of the sentence can be used to
query a web search engine. The results can then be
analyzed, and if they match the originally-proposed
answer sentence structure, an answer candidate can
be extracted.
The first step is to annotate the question with its
semantic roles. For this task we use a classical se-
mantic role labeler combined with a rule-based ap-
proach. Keep in mind that our task is to annotate
questions, not declarative sentences. This is impor-
tant for several reasons:
1. The role labeler we use is trained on FrameNet
and PropBank data, i.e. mostly on declarative
sentences, whose syntax often differs consider-
ably from the sytax of questions. Aa a result,
the training and test set differ substantially in
nature.
2. Questions tend to be shorter and simpler syn-
tactically than declarative sentences?especially
those occurring in news corpora.
3. Questions contain one semantic role that has to
be annotated but which is not or is only implic-
itly (through the question word) mentioned ?
the answer.
Because of these reasons and especially because
many questions tend to be gramatically simple, we
found that a few simple rules can help the question
annotation process dramatically. We rely on Mini-
Par (Lin, 1998) to find the question?s head verb, e.g.
?purchase? for ?Who purchased YouTube?? (In the
following we will often refer to this question to il-
lustrate our approach.) We then look up all entries
in one of the resources, and for FrameNet and Prop-
Bank we simplify the annotated sentences until we
achieve a set of abstract frame structures, similar to
those in VerbNet. By doing this we intentionally re-
move certain levels of information that were present
in the original data, i.e. tense, voice, mood and nega-
tion. (In a later step we will reintroduce some of it.)
Here is what we find in FrameNet for ?purchase?:
Buyer[Subj,NP] VERB Goods[Obj,NP]
Buyer[Subj,NP] VERB Goods[Obj,NP]
Seller[Dep,PP-from]
Buyer[Subj,NP] VERB Goods[Obj,NP]
Money[Dep,PP-for]
Buyer[Subj,NP] VERB Goods[Obj,NP]
Recipient[Dep,PP-for]
...
A syntactic analysis of the question (also obtained
from MiniPar) shows that ?Who? is the (deep) sub-
ject and ?YouTube?, the (deep) object. The first of
the above frames fits this analysis best, because it
lists only the two roles with the desired grammatical
functions. By mapping the question analysis to this
frame, we can assign the roles Goods to ?YouTube?
and Buyer to ?Who?. From this we can conclude
that the question asks for the Buyer role.
An additional point suitable to illustrate why a
few simple rules can achieve in many cases more
that a statistical classifier, are When- and Where-
questions. Here, the hint that leads to the correct de-
tection of the answer role lies in the question word,
which is of course not present in the answer sen-
tence. Furthermore, the answer role in an answer
sentence will usually be realized as a PP with a to-
tally different dependency path than the one of ques-
tion?s question word. In contrast, a rule that states
that whenever a temporal or location question is de-
tected the answer role becomes, in FrameNet terms,
Place or Time, respectively, is very helpful here.
Once the role assignment is complete, we use
all abstract frames which contain the roles found in
the question to generate potential answer templates.
42
This is also the point where we reintroduce tense and
voice information:1 If the question was asked in the
a past tense, we will now create from each abstract
frame, all surface realizations in all past tenses, both
in active and passive voice. If we had used the an-
notated data directly without the detour over the ab-
stract frames, we would have difficulty sorting out
negated sentences, those in undesired moods and
those in unsuitable tenses. In contrast our approach
guarantees that all possible tenses in both voices are
generated, and no meaning-altering information like
mood and negation is present. For the example given
above we would create inter alia the following an-
swer templates:
ANSWER[NP] purchased YouTube
YouTube was purchased by ANSWER[NP]
ANSWER[NP] had purchased YouTube
...
The part (or parts) of the templates that are
known are quoted and sent to a search en-
gine. For the second example, this would be
"YouTube was purchased by". From the snippets
returned by the search engine, we extract candi-
date sentences and match them against the abstract
frame structure from which the queries were origi-
nally created. In this way, we annotate the candidate
sentences and are now able to identify the filler of
the answer role. For example, the above query re-
turns ?On October 9, 2006, YouTube was purchased
by Google for an incredible US$1.65 billion?, from
which we can extract ?Google?, because it is the NP
filling the buyer role.
So far, we have mostly discussed questions whose
answer role is an argument of the head verb. How-
ever, for questions like ?When was YouTube pur-
chased?? this assumption does not hold. Here, the
question asks for an adjunct. This is an important
difference for at least three reasons:
1. FrameNet and VerbNet do not or only sparsely
annotate peripheral adjuncts. (PropBank how-
ever does.)
2. In English, the position of adjuncts varies much
more than those of arguments.
3. In English, different kinds of adjuncts can oc-
cupy the same position in a sentence, although
naturally not at the same time.
1While we strip off mood and negation during the creation
of the abstract frames, we have not yet reintroduced them.
The following examples illustrate point 2:
YouTube was purchased by Google on October 9.
On October 9, YouTube was purchased by Google.
YouTube was purchased on October 9 by Google.
All variations are possible, although they may dif-
fer in frequency. PPs conveying other peripheral ad-
juncts ( e.g. ?for $1.65 billion?) could replace all the
above temporals PPs, or they could be added at other
positions.
The special behavior of these types of questions
has not only to be accounted for when annotating
the question with semantic roles, but also and when
creating and processing potential answer sentences.
We use an abstract frame structure like the following
to create the queries:
Buyer[Subj,NP,unknown]
VERB Goods[Obj,NP,"YouTube"]
While this lacks a role for the answer, we
can still use it to create, for example, the query
"has purchased YouTube". When sentences re-
turned from the search engine are then matched
against the abstract structure, we can extract all PPs
directly before the Buyer role, between the Buyer
role and the verb and directly behind the Goods role.
Then we can check all these PPs on their semantic
types and keep only those that match the answer type
of the question (if any).
3 Making use of FrameNet Frames and
Inter-Frame Relations
The method presented so far can be used with all
three resources. But FrameNet goes a step further
than just listing verb-argument structures: It orga-
nizes all of its lexical entries in frames2, with rela-
tions between frames that can be used for a wider
paraphrasing and inference. This section will ex-
plain how we make use of these relations.
The purchase.v entry is organized in a frame
called Commerce buy which also contains the
entries for buy.v and purchase ((act)).n. Both
these entries are annotated with the same frame
elements as purchase.v. This makes it possible to
formulate alternative answer templates, for exam-
ple: YouTube was bought by ANSWER[NP] and
2Note the different meaning of frame in FrameNet and Prop-
Bank/VerbNet respectively.
43
ANSWER[NP-Genitive] purchase of YouTube.
The latter example illustrates that we can also
generate target paraphrases with heads which are
not verbs. Handling these is usually easier than
sentences based on verbs, because no tense/voice
information has to be introduced.
Furthermore, frames themselves can stand in
different relations. The frame Commerce goods-
transfer, for example, relates both to the already
mentioned Commerce buy frame and to Com-
merce sell in an is perspectivized in relation. The
latter contains the lexical entries retail.v, retailer.n,
sale.n, sell.v, vend.v and vendor.n. Again, the
frame elements used are the same as for pur-
chase.v. Thus we can now create answer templates
like YouTube was sold to ANSWER[NP]. Other
templates created from this frame seem odd, e.g.
YouTube has been retailed to ANSWER[NP].
because the verb ?to retail? usually takes mass-
products as its object argument and not a company.
But FrameNet does not make such fine-grained
distinctions. Interestingly, we did not come across
a single example in our experiments where such
a phenomenon caused an overall wrong answer.
Sentences like the one above will most likely not be
found on the web (just because they are in a narrow
semantic sense not well-formed). Yet even if we
would get a hit, it probably would be a legitimate to
count the odd sentence ?YouTube had been retailed
to Google? as evidence for the fact that Google
bought YouTube.
4 Method 2: Combining Semantic Roles
and Dependency Paths
The second method we have implemented com-
pares the dependency structure of example sentences
found in PropBank and FrameNet with the depen-
dency structure of candidate sentences. (VerbNet
does not list example sentences for lexical entries,
so could not be used here.)
In a pre-processing step, all example sentences in
PropBank and FrameNet are analyzed and the de-
pendency paths from the head to each of the frame
elements are stored. For example, in the sentence
?The Soviet Union has purchased roughly eight mil-
lion tons of grain this month? (found in PropBank),
?purchased? is recognized as the head, ?The So-
viet Union? as ARG0, ?roughly eight million tons of
grain? as ARG1, and ?this month? as an adjunct of
type TMP. The stored paths to each are as follows:
headPath = ? i
role = ARG0, paths = {?s, ?subj}
role = ARG1, paths = {?obj}
role = TMP, paths = {?mod}
This says that the head is at the root, ARG0 is at both
surface subject (s) and deep subject (subj) position3,
ARG1 is the deep object (obj), and TMP is a direct
adjunct (mod) of the head.
Questions are annotated as described in Section 2.
Sentences that potentially contain answer candidates
are then retrieved by posing a rather abstract query
consisting of key words from the question. Once
we have obtained a set of candidate-containing sen-
tences, we ask the following questions of their de-
pendency structures compared with those of the ex-
ample sentences from PropBank4:
1a Does the candidate-containing sentence share
the same head verb as the example sentence?
1b Do the candidate sentence and the example sen-
tence share the same path to the head?
2a In the candidate sentence, do we find one or
more of the example?s paths to the answer role?
2b In the candidate sentence, do we find all of the
example?s paths to the answer role?
3a Can some of the paths for the other roles be
found in the candidate sentence?
3b Can all of the paths for the other roles be found
in the candidate sentence?
4a Do the surface strings of the other roles par-
tially match those of the question?
4b Do the surface strings of the other roles com-
pletely match those of the question?
Tests 1a and 2a of the above are required criteria:
If the candidate sentence does not share the same
head verb or if we can find no path to the answer
role, we exclude it from further processing.
3MiniPar allows more than one path between nodes due, for
example, to traces. The given example is MiniPar?s way of in-
dicating that this is a sentence in active voice.
4Note that our proceeding is not too different from what a
classical role labeler would do: Both approaches are primarily
based on comparing dependency paths. However, a standard
role labeler would not take tests 3a, 3b, 4a and 4b into account.
44
Each sentence that passes steps 1a and 2a is
assigned a weight of 1. For each of the remaining
tests that succeeds, we multiply that weight by
2. Hence a candidate sentence that passes all the
tests is assigned a weight 64 times higher than a
candidate that only passes tests 1a and 2a. We take
this as reasonable, as the evidence for having found
a correct answer is indeed very weak if only tests 1a
and 2a succeeded and very high if all tests succeed.
Whenever condition 2a holds, we can extract an
answer candidate from the sentence: It is the phrase
that the answer role-path points to. All extracted
answers are stored together with their weights, if
we retrieve the same answer more than once, we
simple add the new weight to the old ones. After
all candidate sentences have been compared with
all pre-extracted structures, the ones that do not
show the correct semantic type are removed. This
is especially important for answers that are realized
as adjuncts, see Section 2. We choose the answer
candidate with the highest score as the final answer.
We now illustrate this method with respect to our
question ?Who purchased YouTube?? The roles as-
signment process produces this result: ?YouTube?
is ARG1 and the answer is ARG0. From the web
we retrieve inter alia the following sentence: ?Their
aim is to compete with YouTube, which Google re-
cently purchased for more than $1 billion.? The de-
pendency analysis of the relevant phrases is:
headPath = ?i?i?pred?i?mod?pcom-n?rel?i
phrase = ?Google?, paths = {?s, ?subj}
phrase = ?which?, paths = {?obj}
phrase = ?YouTube?, paths = {?i?rel}
phrase = ?for more than $1 billion?, paths = {?mod}
If we annotate this sentence by using the analy-
sis from the above example sentence (?The Soviet
Union has purchased ...?) we get the following (par-
tially correct) role assignment: ?Google? is ARG0,
?which? is ARG1, ?for more than $1 billion? is TMP.
The following table shows the results of the 8 tests
described above:
1a OK
1b ?
2a OK
2b OK
3a OK
3b OK
4a ?
4b ?
Test 1a and 2a succeeded, so this sentence is as-
signed an initial weight of 1. However, only three
other tests succeed as well, so its final weight is
8. This rather low weight for a positive candi-
date sentence is due to the fact that we compared
it against a dependency structure which it only par-
tially matched. However, it might very well be the
case that another of the annotated sentences shows a
perfect fit. In such a case this comparison would
result in a weight of 64. If these were the only
two sentences that produce a weight of 1 or greater,
the final weight for this answer candidate would be
8 + 64 = 72.
5 Evaluation
We choose to evaluate our experiments with the
TREC 2002 QA test set because test sets from 2004
and beyond contain question series that pose prob-
lems that are separate from the research described
in this paper. While we participated in TREC 2004,
2005 and 2006, with an anaphora-resolution com-
ponent that performed quite well, we feel that if
one wants to evaluate a particular method, adding an
additional module, unrelated to the actual problem,
can distort the results. Additionally, because we are
searching for answers on the web rather than in the
AQUAINT corpus, we do not distinguish between
supported and unsupported judgments.
Of the 500 questions in the TREC 2002 test set,
236 have be as their head verb. As the work de-
scribed here essentially concerns verb semantics,
such questions fall outside its scope. Evaluation
has thus been carried out on only the remaining 264
questions.
For the first method (cf. Section 2), we evaluated
system accuracy separately for each of the three re-
sources, and then together, obtaining the following
values:
FrameNet PropBank VerbNet combined
0.181 0.227 0.223 0.261
For the combined run we looked up the verb
in all three resources simultaneously and all en-
tries from every resource were used. As can
be seen, PropBank and VerbNet perform equally
well, while FrameNet?s performance is significantly
lower. These differences are due to coverage issues:
FrameNet is still in development, and further ver-
sions with a higher coverage will be released. How-
ever, a closer look shows that coverage is a problem
for all of the resources. The following table shows
the percentage of the head verbs that were looked
45
up during the above experiments based on the 2002
question set, that could not be found (not found). It
also lists the percentage of lexical entries that con-
tain no annotated sentences (s = 0), five or fewer
(s <= 5), ten or fewer (s <= 10), or more than
50 (s > 50). Furthermore, the table lists the aver-
age number of lexical entries found per head verb
(avg senses) and the average number of annotated
sentences found per lexical entry (avg sent). 5
FrameNet PropBank
not found 11% 8%
s = 0 41% 7%
s <= 5 48% 35%
s <= 10 57% 45%
s > 50 8% 23%
avg senses 2.8 4.4
avg sent. 16.4 115.0
The problem with lexical entires only containing
a small number of annotated sentences is that these
sentences often do not exemplify common argument
structures, but rather seldom ones. As a solution to
this coverage problem, we experimented with a cau-
tious technique for expanding coverage. Any head
verb, we assumed displays the following three pat-
terns:
intransitive: [ARG0] VERB
transitive: [ARG0] VERB [ARG1]
ditransitive: [ARG0] VERB [ARG1] [ARG2]
During processing, we then determined whether
the question used the head verb in a standard in-
transitive, transitive or ditransitive way. If it did,
and that pattern for the head verb was not contained
in the resources, we temporarily added this abstract
frame to the list of abstract frames the system used.
This method rarely adds erroneous data, because the
question shows that such a verb argument structure
exists for the verb in question. By applying this tech-
nique, the combined performance increased from
0.261 to 0.284.
In Section 2 we reported on experiments that
make use of FrameNet?s inter-frame relations. The
next table lists the results we get when (a) using only
the question head verb for the reformulations, (b) us-
ing the other entries in the same frame as well, (c)
using all entries in all frames to which the starting
5As VerbNet contains no annotated sentences, it is not listed.
Note also, that these figures are not based on the resources in
total, but on the head verbs we looked up for our evaluation.
frame is related via the Inheritance, Perspective on
and Using relations (by using only those frames
which show the same frame elements).
(a) only question head verb 0.181
(b) all entries in frame 0.204
all entries in related frames(c) (with same frame elements) 0.215
Our second method described in Section 4, can
only be used with FrameNet and PropBank, because
VerbNet does not give annotated example sentences.
Here are the results:
FrameNet PropBank
0.030 0.159
Analysis shows that PropBank dramatically out-
performs FrameNet for three reasons:
1. PropBank?s lexicon contains more entries.
2. PropBank provides many more example sen-
tences for each entry.
3. FrameNet does not annotate peripheral ad-
juncts, and so does not apply to When- or
Where-questions.
The methods we have described above are com-
plementary. When they are combined so that when
method 1 returns an answer it is always chosen
as the final one, and only if method 1 did not
return an answer were the results from method
2 used, we obtain a combined accuracy of 0.306
when only using PropBank. When using method 1
with all three resources and our cautious coverage-
extension strategy, with all additional reformulations
that FrameNet can produce and method 2, using
PropBank and FrameNet, we achieve an accuracy of
0.367.
We also evaluated how much increase the de-
scribed approaches based on semantic roles bring to
our existing QA system. This system is completly
web-based and employs two answer finding strate-
gies. The first is based on syntactic reformulation
rules, which are similar to what we described in sec-
tion 2. However, in contrast to the work described
in this paper, these rules are manually created. The
second strategy uses key words from the question as
queries, and looks for frequently occuring n-grams
in the snippets returned by the search engine. The
system received the fourth best result for factoids in
TREC 2004 (Kaisser and Becker, 2004) (where both
46
just mentioned approaches are described in more de-
tail) and TREC 2006 (Kaisser et al, 2006), so it in
itself is a state-of-the-art, high performing QA sys-
tem. We observe an increase in performance by 21%
over the mentioned baseline system. (Without the
components based on semantic roles 130 out of 264
questions are answered correct, with these compo-
nents 157.)
6 Related Work
So far, there has been little work at the intersection
of QA and semantic roles. Fliedner (2004) describes
the functionality of a planned system based on the
German version of FrameNet, SALSA, but no so far
no paper describing the completed system has been
published.
Novischi and Moldovan (2006) use a technique
that builds on a combination of lexical chains and
verb argument structures extracted from VerbNet to
re-rank answer candidates. The authors? aim is to
recognize changing syntactic roles in cases where
an answer sentence shows a head verb different from
the question (similar to work described here in Sec-
tion 2). However, since VerbNet is based on the-
matic rather than semantic roles, there are problems
in using it for this purpose, illustrated by the follow-
ing VerbNet pattern for buy and sell:
[Agent] buy [Theme] from [Source]
[Agent] sell [Recipient] [Theme]
Starting with the sentence ?Peter bought a guitar
from Johnny?, and mapping the above roles for buy
to those for sell, the resulting paraphrase in terms
of sell would be ?Peter sold UNKNOWN a guitar?.
That is, there is nothing blocking the Agent role of
buy being mapped to the Agent role of sell, nor any-
thing linking the Source role of buy to any role in
sell. There is also a coverage problem: The authors
report that their approach only applies to 15 of 230
TREC 2004 questions. They report a performance
gain of 2.4% (MMR for the top 50 answers), but it
does not become clear whether that is for these 15
questions or for the complete question set.
The way in which we use the web in our first
method is somewhat similar to (Dumais et al, 2002).
However, our system allows control of verb argu-
ment structures, tense and voice and thus we can
create a much larger set of reformulations.
Regarding our second method, two papers de-
scribe related ideas: Firstly, in (Bouma et al, 2005)
the authors describe a Dutch QA system which
makes extensive use of dependency relations. In a
pre-processing step they parsed and stored the full
text collection for the Dutch CLEF QA-task. When
their system is asked a question, they match the de-
pendency structure of the question against the de-
pendency structures of potential answer candidates.
Additionally, a set of 13 equivalence rules allows
transformations of the kind ?the coach of Norway,
Egil Olsen? ? ?Egil Olsen, the coach of Norway?.
Secondly, Shen and Klakow (2006) use depen-
dency relation paths to rank answer candidates. In
their work, a candidate sentence supports an answer
if relations between certain phrases in the candidate
sentence are similar to the corresponding ones in the
question.
Our work complements that described in both
these papers, based as it is on a large collection of
semantically annotated example sentences: We only
require a candidate sentence to match one of the an-
notated example sentences. This allows us to deal
with a much wider range of syntactic possibilities, as
the resources we use do not only document verb ar-
gument structures, but also the many ways they can
be syntactically realized.
7 Discussion
Both methods presented in this paper employ se-
mantic roles but with different aims in mind: The
first method focuses on creating obvious answer-
containing sentences. Because in these sentences,
the head and the semantic roles are usually adjacent,
it is possible to create exact search queries that will
lead to answer candidates of a high quality. Our
second method can deal with a wider range of syn-
tactic variations but here the link to the answer sen-
tences? surface structure is not obvious, thus no ex-
act queries can be posed.
The overall accuracy we achieved suggests that
employing semantic roles for question answering is
indeed useful. Our results compare nicely to re-
cent TREC evaluation results. This is an especially
strong point, because virtually all high performing
TREC systems combine miscellaneous strategies,
which are already know to perform well. Because
47
the research question driving this work was to deter-
mine how semantic roles can benefit QA, we deliber-
ately designed our system to only build on semantic
roles. We did not chose to extend an already exist-
ing system, using other methods with a few features
based on semantic roles.
Our results are convincing qualitatively as well as
quantitavely: Detecting paraphrases and drawing in-
ferences is a key challenge in question answering,
which our methods achieve in various ways:
? They both recognize different verb-argument
structures of the same verb.
? Method 1 controls for tense and voice: Our sys-
tem will not take a future perfect sentence for
an answer to a present perfect question.
? For method 1, no answer candidates altered by
mood or negation are accepted.
? Method 1 can create and recognize answer sen-
tences, whose head is synonymous or related in
meaning to the answers head. In such transfor-
mations, we are also aware of potential changes
in the argument structure.
? The annotated sentences in the resources en-
ables method 2 to deal with a wide range of
syntactic phenomena.
8 Conclusion
This paper explores whether lexical resources like
FrameNet, PropBank and VerbNet are beneficial for
QA and describes two different methods in which
they can be used. One method uses the data in these
resources to generate potential answer-containing
sentences that are searched for on the web by using
exact, quoted search queries. The second method
uses only a keyword-based search, but it can anno-
tate a larger set of candidate sentences. Both meth-
ods perform well solemnly and they nicely comple-
ment each other. Our methods based on semantic
roles alone achieves an accuracy of 0.39. Further-
more adding the described features to our already
existing system boosted accuracy by 21%.
Acknowledgments
This work was supported by Microsoft Research
through the European PhD Scholarship Programme.
References
Colin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING-ACL.
Gosse Bouma, Jori Mur, Gertjan van Noord, Lonneke
van der Plas, and Jo?rg Tiedemann. 2005. Question
Answering for Dutch using Dependency Relations. In
Proceedings of the CLEF 2005 Workshop.
Susan Dumais, Michele Bankom, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web Question Answering: Is
More Always Better? Proceedings of UAI 2003.
Gerhard Fliedner. 2004. Towards Using FrameNet for
Question Answering. In Proceedings of the LREC
2004 Workshop on Building Lexical Resources from
Semantically Annotated Corpora.
Michael Kaisser and Tilman Becker. 2004. Question An-
swering by Searching Large Corpora with Linguistic
Methods. In The Proceedings of the 2004 Edition of
the Text REtrieval Conference, TREC 2004.
Michael Kaisser, Silke Scheible, and Bonnie Webber.
2006. Experiments at the University of Edinburgh for
the TREC 2006 QA track. In The Proceedings of the
2006 Edition of the Text REtrieval Conference, TREC
2006.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993. In-
troduction to WordNet: An On-Line Lexical Database.
Adrian Novischi and Dan Moldovan. 2006. Question
Answering with Lexical Chains Propagating Verb Ar-
guments. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Karin Kipper Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
Dan Shen and Dietrich Klakow. 2006. Exploring Corre-
lation of Dependency Relation Paths for Answer Ex-
traction. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL.
48
Proceedings of the Linguistic Annotation Workshop, pages 191?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Panel Session: Discourse Annotation
Manfred Stede
Dept. of Linguistics
University of Potsdam
stede@ling.uni-potsdam.de
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Eva Hajic?ova?
Faculty of Math. and Physics
Charles University
hajicova@ufal.ms.mff.cuni.cz
Brian Reese
Dept. of Linguistics
Univ. of Texas at Austin
bjreese@mail.utexas.edu
Simone Teufel
Computer Laboratory
Univ. of Cambridge
sht25@cl.cam.uk
Bonnie Webber
School of Informatics
Univ. of Edinburgh
bonnie@inf.ed.ac.uk
Theresa Wilson
Dept. of Comp. Science
Univ. of Pittsburgh
twilson@cs.pitt.edu
1 Introduction
The classical ?success story? of corpus annotation
are the various syntax treebanks that provide struc-
tural analyses of sentences and have enabled re-
searchers to develop a range of new and highly suc-
cessful data-oriented approaches to sentence pars-
ing. In recent years, however, a number of corpora
have been constructed that provide annotations on
the discourse level, i.e. information that reaches be-
yond the sentence boundaries. Phenomena that have
been annotated include coreference links, the scope
of connectives, and coherence relations. Many of
these are phenomena on whose handling there is
not a general agreement in the research community,
and therefore the question of ?recycling? corpora by
other people and for other purposes is often diffi-
cult. (To some extent, this is due to the fact that dis-
course annotation deals ?only? with surface reflec-
tions of underlying, abstract objects.) At the same
time, the efforts needed for building high-quality
discourse corpora are considerable, and thus one
should be careful in deciding how to invest those ef-
forts. One aspect of providing added-value with an-
notation projects is that of shared corpora: If a vari-
ety of annotation efforts is executed on the same pri-
mary data, the series of annotation levels can yield
insights that the creators of the individual levels had
not explicitly planned for. A clear case is the rela-
tionship between coherence relations and connective
use: When both levels are marked individually and
with independent annotation guidelines, then after-
wards the correlations between coherence relations,
cue usage (and possibly other factors, if annotated)
can be studied systematically. This conception of
multi-level annotation presupposes, of course, that
the technical problems of setting annotation levels
in correspondence to one another be resolved.
The panel on discourse annotation is organized
by Manfred Stede and Janyce Wiebe. It aims at
surveying the scene of discourse corpora, exploring
chances for synergy, and identifying desiderata for
future corpus creation projects. In preparation for
the panel, the participants have provided the follow-
ing short descriptions of the various copora in whose
construction they have been involved.
2 Prague Dependency Treebank
(Eva Hajic?ova?, Prague)
One of the maxims of the work on the Prague De-
pendency Treebank is that one should not overlook,
disregard and thus lose what the sentence structure
offers when one attempts to analyze the structure of
discourse, thus moving from ?the trees? to ?the for-
est?. Therefore, we emphasize that discourse anno-
tation should make use of every possible detail the
annotation of the component parts of the discourse,
namely the sentences, puts at our disposal. This
is, of course, not only true for the surface shape of
the sentence (i.e., the surface means of expression),
but (and most importantly) for the underlying repre-
sentation of sentences. The panel contribution will
introduce the (multilayered) annotation scenario of
the Prague Dependency Treebank and illustrate the
point using some of the particular features of the un-
derlying structure of sentences that can be made use
of in planning the scenario of discourse ?treebanks?.
191
3 SDRT in Newspaper Text
(Brian Reese, Austin)
We are currently working under the auspices of
an NSF grant to build and train a discourse parser
and codependent anaphora resolution program to
test discourse theories empirically. The training re-
quires the construction of a corpus annotated with
discourse structure and coreference information. So
far, we have annotated the MUC61 corpus for dis-
course structure and are in the process of annotating
the ACE22 corpus; both corpora are already anno-
tated for coreference. One of the goals of the project
is to investigate whether using the right frontier con-
straint improves the system?s performance in resolv-
ing anaphors. Here we detail some experiences we
have had with the discourse annotation process.
An implementation of the extant SDRT (Asher and
Lascarides, 2003) glue logic for building discourse
structures is insufficient to deal with open domain
text, and we cannot envision an extended version
at the present time able to deal with the problem.
Thus, we have opted for a machine learning based
approach to discourse parsing based on superficial
features, like BNL. To build an implementation to
test these ideas, we have had to devise a corpus of
texts annotated for discourse structure in SDRT.
Each of the 60 texts in the MUC6 corpus, and now
18 of the news stories in ACE2, were annotated by
two people familiar with SDRT. The annotators then
conferred and agreed upon a gold standard. Our
annotation effort took the hierarchical structure of
SDRT seriously and built graphs in which the nodes
are discourse units and the arcs represent discourse
relations between the units. The units could either be
simple (elementary discourse units: EDUs) or they
could be complex. We assumed that in principle the
units were recursively generated and could have an
arbitrary though finite degree of complexity.
4 Potsdam Commentary Corpus
(Manfred Stede, Potsdam)
Construction of the Potsdam Commentary Corpus
(PCC) began in 2003 and is still ongoing. It is a
1The Message Understanding Conference, www-nlpir.
nist.gov/related projects/muc/.
2The Automated Content Extraction program,
www.nist.gov/speech/tests/ace/.
genre-specific corpus of German newspaper com-
mentaries, taken from the daily papers Ma?rkische
Allgemeine Zeitung and Tagesspiegel. One central
aim is to provide a tool for studying mechanisms
of argumentation and how they are reflected on the
linguistic surface. The corpus on the one hand is a
collection of ?raw? data, which is used for genre-
oriented statistical explorations. On the other hand,
we have identified two sub-corpora that are subject
to a rich multi-level annotation (MLA).
The PCC176 (Stede, 2004) is a sub-corpus that
is available upon request for research purposes. It
consists of 176 relatively short commentaries (12-
15 sentences), with 33.000 tokens in total. The
sentences have been PoS-tagged automatically (and
manually checked); sentence syntax was anno-
tated semi-automatically using the TIGER scheme
(Brants et al, 2002) and Annotate3 tool. In addition,
we annotated coreference (PoCos (Krasavina and
Chiarcos, 2007)) and rhetorical structure according
to RST (Mann and Thompson, 1988). Our anno-
tation software architecture consists of a variety of
standard, external tools that can be used effectively
for the different annotation types. Their XML output
is then automatically converted to a generic format
(PAULA, (Dipper, 2005)), which is read into the lin-
guistic database ANNIS (Dipper et al, 2004), where
the annotations are aligned, so that the data can be
viewed and queried across annotation levels.
The PCC10 is a sub-corpus of 10 commentaries
that serves as ?testbed? for further developing the
annotation levels. On the one hand, we are apply-
ing recent guidelines on annotation of information
structure (Go?tze et al, 2007). On the other hand,
based on experiences with the RST annotation, we
are replacing the rhetorical trees with a set of dis-
tinct, simpler annotation layers: thematic structure,
conjunctive relations (Martin, 1992), and argumen-
tation structure (Freeman, 1991); these are comple-
mented by the other levels mentioned above for the
PCC176. The primary motivation for this step is the
high degree of arbitrariness that annotators reported
when producing the RST trees (see (Stede, 2007)).
By separating the thematic from the intentional in-
formation, and accounting for the surface-oriented
3www.coli.uni-saarland.de/projects/
sfb378/negra-corpus/annotate.html
192
conjunctive relations (which are similar to what is
annotated in the PDTB, see Section 6), we hope to
? make annotation easier: handling several ?sim-
ple? levels individually should be more effec-
tive than a single, very complex annotation
step;
? end up with less ambiguity in the annotations,
since the reasons for specific decisions can be
made explicit (by annotations on ?simpler? lev-
els);
? be more explicit than a single tree can be: if a
discourse fulfills, for example, a function both
for thematic development and for the writer?s
intention, they can both be accounted for;
? provide the central information that a ?tradi-
tional? rhetorical tree conveys, without loosing
essential information.
5 AZ Corpus
(Simone Teufel, Cambridge)
The Argumentative Zoning (AZ) annotation scheme
(Teufel, 2000; Teufel and Moens, 2002) is con-
cerned with marking argumentation steps in scien-
tific articles. One example for an argumentation step
is the description of the research goal, another an
overt comparison of the authors? work with rival ap-
proaches. In our scheme, these argumentation steps
have to be associated with text spans (sentences or
sequences of sentences). AZ?Annotation is the la-
belling of each sentence in the text with one of these
labels (7 in the original scheme in (Teufel, 2000)).
The AZ labels are seen as relations holding between
the meanings of these spans, and the rhetorical act
of the entire paper. (Teufel et al, 1999) reports on
interannotator agreement studies with this scheme.
There is a strong interrelationship between the ar-
gumentation in a paper, and the citations writers use
to support their argument. Therefore, a part of the
computational linguistics corpus has a second layer
of annotation, called CFC (Teufel et al, 2006) or
Citation Function Classification. CFC? annotation
records for each citation which rhetorical function it
plays in the argument. This is following the spirit of
research in citation content analysis (e.g., (Moravc-
sik and Murugesan, 1975)). An example for a ci-
tation function would be ?motivate that the method
used is sound?. The annotation scheme contains
12 functions, clustered into ?superiority?, ?neutral
comparison/contrast?, ?praise or usage? and ?neu-
tral?.
One type of research we hope to do in the future
is to study the relationship between these rhetori-
cal phonemena with more traditional discourse phe-
nomena, e.g. anaphoric expressions.
The CmpLg/ACL Anthology corpora consist of
320/9000 papers in computational linguistics. They
are partially annotated with AZ and CFC markup. A
subcorpus of 80 parallelly annotated papers (AZ and
CFF) can be obtained from us for research (12000
sentences, 1756 citations). We are currently port-
ing both schemes to chemistry in the framework
of the EPSRC-sponsored project SciBorg. In the
course of this work a larger, more general AZ an-
notation scheme was developed. The SciBorg effort
will result in an AZ/CFC?annotated chemistry cor-
pus available to the community in 2009.
In terms of challenges, the most time-consuming
aspects of creating this annotated corpus were for-
mat conversions on the corpora, and cyclic adapta-
tions of scheme and guidelines. Another problem is
the simplification of annotating only full sentences;
sometimes, annotators would rather mark a clause
or sometimes even just an NP. However, we found
these cases to be relatively rare.
6 Penn Discourse Treebank
(Bonnie Webber, Edinburgh)
The Penn Discourse TreeBank (Miltsakaki et al,
2004; Prasad et al, 2004; Webber, 2005) anno-
tates discourse relations over the Wall Street Jour-
nal corpus (Marcus et al, 1993), in terms of dis-
course connectives and their arguments. Following
the approach towards discourse structure in (Webber
et al, 2003), the PDTB takes a lexicalized approach,
treating discourse connectives as the anchors of the
relations and thus as discourse-level predicates tak-
ing two Abstract Objects as their arguments. An-
notated are the text spans that give rise to these ar-
guments. There are primarily two types of connec-
tives in the PDTB: explicit and implicit, the latter
being inserted between adjacent paragraph-internal
sentence pairs not related by an explicit connective.
193
Also annotated in the PDTB is the attribution of
each discourse relation and of its arguments (Dinesh
et al, 2005; Prasad et al, 2007). (Attribution itself
is not considered a discourse relation.) A prelimi-
nary version of the PDTB was released in April 2006
(PDTB-Group, 2006), and is available for download
at http://www.seas.upenn.edu/?pdtb. This release only has
implicit connectives annotated in three sections of
the corpus. The annotation of all implicit connec-
tives, along with a hierarchical semantic classifica-
tion of all connectives (Miltsakaki et al, 2005), will
appear in the final release of the PDTB in August
2007.
Here I want to mention three of the challenges we
have faced in developing the PDTB:
(I) Words and phrases that can function as con-
nectives can also serve other roles. (Eg, when can be
a relative pronoun, as well as a subordinating con-
junction.) It has been difficult to identify all and
only those cases where a token functions as a dis-
course connective, and in many cases, the syntactic
analysis in the Penn TreeBank (Marcus et al, 1993)
provides no help. For example, is as though always a
subordinating conjunction (and hence a connective)
or do some tokens simply head a manner adverbial
(eg, seems as though . . . versus seems more rushed
as though . . . )? Is also sometimes a discourse con-
nective relating two abstract objects and other times,
an adverb that presupposes that a particular property
holds of some other entity? If so, when one and
when the other? In the PDTB, annotation has erred
on the side of false positives.
(II) In annotating implicit connectives, we discov-
ered systematic non-lexical indicators of discourse
relations. In English, these include cases of marked
syntax (eg, Had I known the Queen would be here,
I would have dressed better.) and cases of sentence-
initial PPs and adjuncts with anaphoric or deictic
NPs such as at the other end of the spectrum, adding
to that speculation. These cases labelled ALTLEX,
for ?alternative lexicalisation? have not been anno-
tated as connectives in the PDTB because they are
fully productive (ie, not members of a more eas-
ily annotated closed set of tokens). They comprise
about 1% of the cases the annotators have consid-
ered. Future discourse annotation will benefit from
further specifying the types of these cases.
(III) The way in which spans are annotated as ar-
guments to connectives also raises a challenge. First,
because the PDTB annotates both structural and
anaphoric connectives (Webber et al, 2003), a span
can serve as argument to >1 connective. Secondly,
unlike in the RST corpus (Carlson et al, 2003) or the
Discourse GraphBank (Wolf and Gibson, 2005), dis-
course segments are not separately annotated, with
annotators then identifying what discourse relations
hold between them. Instead, in annotating argu-
ments, PDTB annotators have selected the minimal
clausal text span needed to interpret the relation.
This could comprise an embedded, subordinate or
coordinate clause, an entire sentence, or a (possi-
bly disjoint) sequence of sentences. As a result,
there are fairly complex patterns of spans within and
across sentences that serve as arguments to differ-
ent connectives, and there are parts of sentences that
don?t appear within the span of any connective, ex-
plicit or implicit. The result is that the PDTB pro-
vides only a partial but complexly-patterned cover
of the corpus. Understanding what?s going on and
what it implies for discourse structure (and possibly
syntactic structure as well) is a challenge we?re cur-
rently trying to address (Lee et al, 2006).
7 MPQA Opinion Corpus
(Theresa Wilson, Pittsburgh)
Our opinion annotation scheme (Wiebe et al, 2005)
is centered on the notion of private state, a gen-
eral term that covers opinions, beliefs, thoughts, sen-
timents, emotions, intentions and evaluations. As
Quirk et al (1985) define it, a private state is a state
that is not open to objective observation or verifica-
tion. We can further view private states in terms of
their functional components ? as states of experi-
encers holding attitudes, optionally toward targets.
For example, for the private state expressed in the
sentence John hates Mary, the experiencer is John,
the attitude is hate, and the target is Mary.
We create private state frames for three main types
of private state expressions (subjective expressions)
in text:
? explicit mentions of private states, such as
?fears? in ?The U.S. fears a spill-over?
? speech events expressing private states, such as
?said? in ?The report is full of absurdities,?
194
Xirao-Nima said.
? expressive subjective elements, such as ?full of
absurdities? in the sentence just above.
Frames include the source (experiencer) of the
private state, the target, and various properties such
as polarity (positive, negative, or neutral) and inten-
sity (high, medium, or low). Sources are nested. For
example, for the sentence ?China criticized the U.S.
report?s criticism of China?s human rights record?,
the source is ?writer, China, U.S. report?, reflecting
the facts that the writer wrote the sentence and the
U.S. report?s criticism is the target of China?s criti-
cism. It is common for multiple frames to be created
for a single clause, reflecting various levels of nest-
ing and the type of subjective expression.
The annotation scheme has been applied to a
corpus, called the ?Multi-Perspective Question An-
swering (MPQA) Corpus,? reflecting its origins in
the 2002 NRRC Workshop on Multi-Perspective
Question Answering (MPQA) (Wiebe et al, 2003)
sponsored by ARDA AQUAINT (it is also called
?OpinionBank?). It contains 535 documents and a
total of 11,114 sentences. The articles in the cor-
pus are from 187 different foreign and U.S. news
sources, dating from June 2001 to May 2002. Please
see (Wiebe et al, 2005) and Theresa Wilson?s forth-
coming PhD dissertation for further information, in-
cluding the results of inter-coder agreement studies.
References
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In J. van
Kuppevelt & R. Smith, editor, Current Directions in
Discourse and Dialogue. Kluwer, New York.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005. At-
tribution and the (non-)alignment of syntactic and dis-
course arguments of connectives. In ACL Workshop
on Frontiers in Corpus Annotation, Ann Arbor MI.
Stefanie Dipper, Michael Go?tze, Manfred Stede, and Till-
mann Wegst. 2004. Annis: A linguistic database for
exploring information structure. In Interdisciplinary
Studies on Information Structure, ISIS Working papers
of the SFB 632 (1), pages 245?279.
Stefanie Dipper. 2005. XML-based stand-off represen-
tation and exploitation of multi-level linguistic annota-
tion. In Rainer Eckstein and Robert Tolksdorf, editors,
Proceedings of Berliner XML Tage, pages 39?50.
James B. Freeman. 1991. Dialectics and the
Macrostructure of Argument. Foris, Berlin.
Michael Go?tze, Cornelia Endriss, Stefan Hinterwimmer,
Ines Fiedler, Svetlana Petrova, Anne Schwarz, Stavros
Skopeteas, Ruben Stoel, and Thomas Weskott. 2007.
Information structure. In Information structure in
cross-linguistic corpora: annotation guidelines for
morphology, syntax, semantics, and information struc-
ture, volume 7 of ISIS Working papers of the SFB 632,
pages 145?187.
Olga Krasavina and Christian Chiarcos. 2007. Potsdam
Coreference Scheme. In this volume.
Alan Lee, Rashmi Prasad, Aravind Joshi, Nikhil Dinesh,
and Bonnie Webber. 2006. Complexity of dependen-
cies in discourse. In Proc. 5th Workshop on Treebanks
and Linguistic Theory (TLT?06), Prague.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. TEXT, 8:243?281.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large scale anno-
tated corpus of English: The Penn TreeBank. Compu-
tational Linguistics, 19:313?330.
James R. Martin. 1992. English text: system and struc-
ture. John Benjamins, Philadelphia/Amsterdam.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating discourse connec-
tives and their arguments. In NAACL/HLT Workshop
on Frontiers in Corpus Annotation, Boston.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Experiments
on sense annotation and sense disambiguation of dis-
course connectives. In 4t Workshop on Treebanks and
Linguistic Theory (TLT?05), Barcelona, Spain.
Michael J. Moravcsik and Poovanalingan Murugesan.
1975. Some results on the function and quality of ci-
tations. Soc. Stud. Sci., 5:88?91.
The PDTB-Group. 2006. The Penn Discourse TreeBank
1.0 annotation manual. Technical Report IRCS 06-01,
University of Pennsylvania.
195
Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and
Bonnie Webber. 2004. Annotation and data mining
of the Penn Discourse TreeBank. In ACL Workshop
on Discourse Annotation, Barcelona, Spain, July.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind Joshi,
and Bonnie Webber. 2007. Attribution and its annota-
tion in the Penn Discourse TreeBank. TAL (Traitement
Automatique des Langues.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, New York.
Manfred Stede. 2004. The Potsdam commentary corpus.
In Proceedings of the ACL Workshop on Discourse An-
notation, pages 96?102, Barcelona.
Manfred Stede. 2007. RST revisited: disentangling nu-
clearity. In Cathrine Fabricius-Hansen and Wiebke
Ramm, editors, ?Subordination? versus ?coordination?
in sentence and text ? from a cross-linguistic perspec-
tive. John Benjamins, Amsterdam. (to appear).
Simone Teufel and Marc Moens. 2002. Summaris-
ing scientific articles ? experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?446.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumenta-
tion in research articles. In Proceedings of the 9th Eu-
ropean Conference of the ACL (EACL-99), pages 110?
117, Bergen, Norway.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. An annotation scheme for citation function. In
Proceedings of SIGDIAL-06, Sydney, Australia.
Simone Teufel. 2000. Argumentative Zoning: Infor-
mation Extraction from Scientific Text. Ph.D. thesis,
School of Cognitive Science, University of Edinburgh,
Edinburgh, UK.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29:545?587.
Bonnie Webber. 2005. A short introduction to the Penn
Discourse TreeBank. In Copenhagen Working Papers
in Language and Speech Processing.
Janyce Wiebe, Eric Breck, Chris Buckley, Claire Cardie,
Paul Davis, Bruce Fraser, Diane Litman, David Pierce,
Ellen Riloff, Theresa Wilson, David Day, and Mark
Maybury. 2003. Recognizing and organizing opinions
expressed in the world press. In Working Notes of the
AAAI Spring Symposium in New Directions in Ques-
tion Answering, pages 12?19, Palo Alto, California.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31:249?287.
196
Coling 2010: Poster Volume, pages 1023?1031,
Beijing, August 2010
Realization of Discourse Relations by Other Means: Alternative
Lexicalizations
Rashmi Prasad and Aravind Joshi
University of Pennsylvania
rjprasad,joshi@seas.upenn.edu
Bonnie Webber
University of Edinburgh
bonnie@inf.ed.ac.uk
Abstract
Studies of discourse relations have not, in
the past, attempted to characterize what
serves as evidence for them, beyond lists
of frozen expressions, or markers, drawn
from a few well-defined syntactic classes.
In this paper, we describe how the lexical-
ized discourse relation annotations of the
Penn Discourse Treebank (PDTB) led to
the discovery of a wide range of additional
expressions, annotated as AltLex (alterna-
tive lexicalizations) in the PDTB 2.0. Fur-
ther analysis of AltLex annotation sug-
gests that the set of markers is open-
ended, and drawn from a wider variety
of syntactic types than currently assumed.
As a first attempt towards automatically
identifying discourse relation markers, we
propose the use of syntactic paraphrase
methods.
1 Introduction
Discourse relations that hold between the content
of clauses and of sentences ? including relations
of cause, contrast, elaboration, and temporal or-
dering ? are important for natural language pro-
cessing tasks that require sensitivity to more than
just a single sentence, such as summarization, in-
formation extraction, and generation. In written
text, discourse relations have usually been con-
sidered to be signaled either explicitly, as lexical-
ized with some word or phrase, or implicitly due
to adjacency. Thus, while the causal relation be-
tween the situations described in the two clauses
in Ex. (1) is signalled explicitly by the connective
As a result, the same relation is conveyed implic-
itly in Ex. (2).
(1) John was tired. As a result he left early.
(2) John was tired. He left early.
This paper focusses on the problem of how to
characterize and identify explicit signals of dis-
course relations, exemplified in Ex. (1). To re-
fer to all such signals, we use the term ?discourse
relation markers? (DRMs). Past research (e.g.,
(Halliday and Hasan, 1976; Martin, 1992; Knott,
1996), among others) has assumed that DRMs
are frozen or fixed expressions from a few well-
defined syntactic classes, such as conjunctions,
adverbs, and prepositional phrases. Thus the lit-
erature presents lists of DRMs, which researchers
try to make as complete as possible for their cho-
sen language. In annotating lexicalized discourse
relations of the Penn Discourse Treebank (Prasad
et al, 2008), this same assumption drove the ini-
tial phase of annotation. A list of ?explicit con-
nectives? was collected from various sources and
provided to annotators, who then searched for
these expressions in the text and annotated them,
along with their arguments and senses. The same
assumption underlies methods for automatically
identifying DRMs (Pitler and Nenkova, 2009).
Since expressions functioning as DRMs can also
have non-DRM functions, the task is framed as
one of classifying given individual tokens as DRM
or not DRM.
In this paper, we argue that placing such syn-
tactic and lexical restrictions on DRMs limits
a proper understanding of discourse relations,
which can be realized in other ways as well. For
example, one should recognize that the instantia-
tion (or exemplification) relation between the two
sentences in Ex. (3) is explicitly signalled in the
second sentence by the phrase Probably the most
egregious example is, which is sufficient to ex-
press the instantiation relation.
(3) Typically, these laws seek to prevent executive
branch officials from inquiring into whether cer-
tain federal programs make any economic sense or
proposing more market-oriented alternatives to reg-
ulations. Probably the most egregious example is a
1023
proviso in the appropriations bill for the executive
office that prevents the president?s Office of Man-
agement and Budget from subjecting agricultural
marketing orders to any cost-benefit scrutiny.
Cases such as Ex. (3) show that identifying
DRMs cannot simply be a matter of preparing a
list of fixed expressions and searching for them in
the text. We describe in Section 2 how we identi-
fied other ways of expressing discourse relations
in the PDTB. In the current version of the cor-
pus (PDTB 2.0.), they are labelled as AltLex (al-
ternative lexicalizations), and are ?discovered? as
a result of our lexically driven annotation of dis-
course relations, including explicit as well as im-
plicit relations. Further analysis of AltLex anno-
tations (Section 3) leads to the thesis that DRMs
are a lexically open-ended class of elements which
may or may not belong to well-defined syntactic
classes. The open-ended nature of DRMs is a
challenge for their automated identification, and
in Section 4, we point to some lessons we have
already learned from this annotation. Finally, we
suggest that methods used for automatically gen-
erating candidate paraphrases may help to expand
the set of recognized DRMs for English and for
other languages as well (Section 5).
2 AltLex in the PDTB
The Penn Discourse Treebank (Prasad et al,
2008) constitutes the largest available resource of
lexically grounded annotations of discourse rela-
tions, including both explicit and implicit rela-
tions.1 Discourse relations are assumed to have
two and only two arguments, called Arg1 and
Arg2. By convention, Arg2 is the argument syn-
tactically associated with the relation, while Arg1
is the other argument. Each discourse relation is
also annotated with one of the several senses in the
PDTB hierarchical sense classification, as well as
the attribution of the relation and its arguments.
In this section, we describe how the annotation
methodology of the PDTB led to the identification
of the AltLex relations.
Since one of the major goals of the annota-
tion was to lexically ground each relation, a first
step in the annotation was to identify the explicit
1http://www.seas.upenn.edu/?pdtb
markers of discourse relations. Following stan-
dard practice, a list of such markers ? called ?ex-
plicit connectives? in the PDTB ? was collected
from various sources (Halliday and Hasan, 1976;
Martin, 1992; Knott, 1996; Forbes-Riley et al,
2006).2 These were provided to annotators, who
then searched for these expressions in the corpus
and marked their arguments, senses, and attribu-
tion.3 In the pilot phase of the annotation, we
also went through several iterations of updating
the list, as and when annotators reported seeing
connectives that were not in the current list. Im-
portantly, however, connectives were constrained
to come from a few well-defined syntactic classes:
? Subordinating conjunctions: e.g., because,
although, when, while, since, if, as.
? Coordinating conjunctions: e.g., and, but,
so, either..or, neither..nor.
? Prepositional phrases: e.g., as a result, on
the one hand..on the other hand, insofar as,
in comparison.
? adverbs: e.g., then, however, instead, yet,
likewise, subsequently
Ex. (4) illustrates the annotation of an explicit
connective. (In all PDTB examples in the paper,
Arg2 is indicated in boldface, Arg1 is in italics,
the DRM is underlined, and the sense is provided
in parentheses at the end of the example.)
(4) U.S. Trust, a 136-year-old institution that is one of
the earliest high-net worth banks in the U.S., has
faced intensifying competition from other firms that
have established, and heavily promoted, private-
banking businesses of their own. As a result,
U.S. Trust?s earnings have been hurt. (Contin-
gency:Cause:Result)
After all explicit connectives in the list were
annotated, the next step was to identify implicit
discourse relations. We assumed that such rela-
tions are triggered by adjacency, and (because of
resource limitations) considered only those that
held between sentences within the same para-
graph. Annotators were thus instructed to supply
a connective ? called ?implicit connective? ? for
2All explicit connectives annotated in the PDTB are listed
in the PDTB manual (PDTB-Group, 2008).
3These guidelines are recorded in the PDTB manual.
1024
each pair of adjacent sentences, as long as the re-
lation was not already expressed with one of the
explicit connectives provided to them. This proce-
dure led to the annotation of implicit connectives
such as because in Ex. (5), where a causal relation
is inferred but no explicit connective is present in
the text to express the relation.
(5) To compare temperatures over the past 10,000
years, researchers analyzed the changes in concen-
trations of two forms of oxygen. (Implicit=because)
These measurements can indicate temperature
changes, . . . (Contingency:Cause:reason)
Annotators soon noticed that in many cases,
they were not able to supply an implicit connec-
tive. Reasons supplied included (a) ?there is a re-
lation between these sentences but I cannot think
of a connective to insert between them?, (b) ?there
is a relation between the sentences for which I
can think of a connective, but it doesn?t sound
good?, and (c) ?there is no relation between the
sentences?. For all such cases, annotators were
instructed to supply ?NONE? as the implicit con-
nective. Later, we sub-divided these ?NONE? im-
plicits into ?EntRel?, for the (a) type above (an
entity-based coherence relation, since the second
sentence seemed to continue the description of
some entity mentioned in the first); ?NoRel? (no
relation) for the (c) type; and ?AltLex?, for the (b)
type, which we turn to next.
Closer investigation of the (b) cases revealed
that the awkwardness perceived by annotators
when inserting an implicit connective was due to
redundancy in the expression of the relation: Al-
though no explicit connective was present to re-
late the two sentences, some other expression ap-
peared to be doing the job. This is indeed what
we found. Subsequently, instances of AltLex were
annotated if:
1. A discourse relation can be inferred between
adjacent sentences.
2. There is no explicit connective present to re-
late them.
3. The annotator is not able to insert an im-
plicit connective to express the inferred rela-
tion (having used ?NONE? instead), because
inserting it leads to an awkward redundancy
in expressing the relation.
Under these conditions, annotators were in-
structed to look for and mark as Altlex, whatever
alternative expression appeared to denote the re-
lation. Thus, for example, Ex. (6) was annotated
as AltLex because although a causal relation is in-
ferred between the sentences, inserting a connec-
tive like because makes expression of the relation
redundant. Here the phrase One reason is is taken
to denote the relation and is marked as AltLex.
(6) Now, GM appears to be stepping up the pace of its
factory consolidation to get in shape for the 1990s.
One reason is mounting competition from new
Japanese car plants in the U.S. that are pour-
ing out more than one million vehicles a year
at costs lower than GM can match. (Contin-
gency:Cause:reason)
The result of this procedure led to the annota-
tion of 624 tokens of AltLex in the PDTB. We
turn to our analysis of these expressions in the
next section.
3 What is found in AltLex?
Several questions arise when considering the Alt-
Lex annotations. What kind of expressions are
they? What can we learn from their syntax?
Do they project discourse relations of a different
sort than connectives? How can they be identi-
fied, both during manual annotation and automat-
ically? To address these questions, we examined
the AltLex annotation for annotated senses, and
for common lexico-syntactic patterns extracted
using alignment with the Penn Treebank (Marcus
et al, 1993).4
3.1 Lexico-syntactic Characterization
We found that we could partition AltLex annota-
tion into three groups by (a) whether or not they
belonged to one of the syntactic classes admit-
ted as explicit connectives in the PDTB, and (b)
whether the expression was frozen (ie, blocking
free substitution, modification or deletion of any
of its parts) or open-ended. The three groups are
shown in Table 1 and discussed below.
4The source texts of the PDTB come from the Penn
Treebank (PTB) portion of the Wall Street Journal corpus.
The PDTB corpus provides PTB tree alignments of all its
text span annotations, including connectives, AltLex?s, argu-
ments of relations, and attribution spans.
1025
AltLex Group No (%) Examples
Syntactically
admitted, lexi-
cally frozen
92 (14.7%) quite the contrary (ADVP), for one thing (PP), as well (ADVP),
too (ADVP), soon (ADVP-TMP), eventually (ADVP-TMP),
thereafter (RB), even (ADVP), especially (ADVP), actually
(ADVP), still (ADVP), only (ADVP), in response (PP)
Syntactically
free, lexically
frozen
54 (8.7%) What?s more (SBAR-ADV), Never mind that (ADVP-
TMP;VB;DT), To begin with (VP), So (ADVP-PRD-TPC),
Another (DT), further (JJ), As in (IN;IN), So what if
(ADVP;IN), Best of all (NP)
Syntactically
and lexically
free
478 (76.6%) That compares with (NP-SBJ;VBD;IN), After these payments
(PP-TMP), That would follow (NP-SBJ;MD;VB), The plunge
followed (NP-SBJ;VBD), Until then (PP-TMP), The increase
was due mainly to (NP-SBJ;VBD;JJ;RB;TO), That is why (NP-
SBJ;VBZ;WHADVP), Once triggered (SBAR-TMP)
TOTAL 624 ?
Table 1: Breakdown of AltLex by Syntactic and Lexical Flexibility. Examples in the third column are
accompanied (in parentheses) with their PTB POS tags and constituent phrase labels obtained from the
PDTB-PTB alignment.
Syntactically admitted and lexically frozen:
The first row shows that 14.7% of the strings an-
notated as AltLex belong to syntactic classes ad-
mitted as connectives and are similarly frozen.
(Syntactic class was obtained from the PDTB-
PTB alignment.) So, despite the effort in prepar-
ing a list of connectives (cf. Section 1), additional
ones were still found in the corpus through AltLex
annotation. This suggests that any pre-defined list
of connectives should only be used to guide anno-
tators in a strategy for ?discovering? connectives.
Syntactically free and lexically frozen: AltLex
expressions that were frozen but belonged to syn-
tactic classes other than those admitted for the
PDTB explicit connectives accounted for 8.7%
(54/624) of the total (Table 1, row 2). For exam-
ple, the AltLex What?s more (Ex. 7) is parsed as
a clause (SBAR) functioning as an adverb (ADV).
It is also frozen, in not undergoing any change (eg,
What?s less, What?s bigger, etc.5
(7) Marketers themselves are partly to blame: They?ve
increased spending for coupons and other short-
term promotions at the expense of image-building
advertising. What?s more, a flood of new prod-
ucts has given consumers a dizzying choice of
5Apparently similar headless relative clauses such as
What?s more exciting differ from What?s more in not func-
tioning as adverbials, just as NPs.
brands, many of which are virtually carbon
copies of one other. (Expansion:Conjunction)
Many of these AltLex annotations do not con-
stitute a single constituent in the PTB, as with
Never mind that. These cases suggest that ei-
ther the restrictions on connectives as frozen ex-
pressions should be relaxed to admit all syntactic
classes, or the syntactic analyses of these multi-
word expressions is irrelevant to their function.
Both syntactically and lexically free: This
third group (Table 1, row 3) constitutes the major-
ity of AltLex annotations ? 76.6% (478/624). Ad-
ditional examples are shown in Table 2. Common
syntactic patterns here include subjects followed
by verbs (Table 2a-c), verb phrases with comple-
ments (d), adverbial clauses (e), and main clauses
with a subordinating conjunction (f).
All these AltLex annotations are freely modifi-
able, with their fixed and modifiable parts shown
in the regular expressions defined for them in Ta-
ble 2. Each has a fixed ?core? phrase shown as
lexical tokens in the regular expression, e.g, con-
sequence of, attributed to, plus obligatory and op-
tional elements shown as syntactic labels. Op-
tional elements are shown in parentheses. <NX>
indicates any noun phrase, <PPX>, any prepo-
sitional phrase, <VX>, any verb phrase, and
1026
AltLex String AltLex Pattern
(a) A consequence of their departure could be ... <DTX> consequence (<PPX>) <VX>
(b) A major reason is ... <DTX> (<JJX>) reason (<PPX>) <VX>
(c) Mayhap this metaphorical connection made ... (<ADVX>) <NX> made
(d) ... attributed the increase to ... attributed <NX> to
(e) Adding to that speculation ... Adding to <NX>
(f) That may be because ... <NX> <VX> because
Table 2: Complex AltLex strings and their patterns
<JJX>, any adjectival phrase
These patterns show, for example, that other
variants of the identified AltLex A major reason
is include The reason is, A possible reason for the
increase is, A reason for why we should consider
DRMs as an open class is, etc. This is robust sup-
port for our claim that DRMs should be regarded
as an open class: The task of identifying them can-
not simply be a matter of checking an a priori list.
Note that the optional modification seen here
is clearly also possible with many explicit con-
nectives such as if (eg, even if just if, only if ),
as shown in Appendix C of the PDTB manual
(PDTB-Group, 2008). This further supports the
thesis that DRMs should be treated as an open
class that includes explicit connectives.
3.2 Semantic Characterization
AltLex strings were annotated as denoting the dis-
course relation that held between otherwise un-
marked adjacent utterances (Section 2). We found
them to convey this relation in much the same
way as anaphoric discourse adverbials. Accord-
ing to (Forbes-Riley et al, 2006), discourse ad-
verbials convey both the discourse relation and an
anaphoric reference to its Arg1. The latter may be
either explicit (e.g., through the use of a demon-
strative like ?this? or ?that?), or implicit. Thus,
both as a result of that and as a result are dis-
course adverbials in the same way: the latter refers
explicitly to Arg1 via the pronoun ?that?, while
former does so via an implicit internal argument.
(A result must be a result of something.)
The examples in Table 2 make this same two?
part semantic contribution, albeit with more com-
plex expressions referring to Arg1 and more com-
plex modification of the expression denoting the
relation. For example, in the AltLex shown in
(Table 2c), Mayhap this metaphorical connection
made (annotated in Ex. (8)), the relation is de-
noted by the causal verb made, while Arg1 is
referenced through the definite description this
metaphorical connection. In addition, the adverb
Mayhap further modifies the relational verb.
(8) Ms. Bartlett?s previous work, which
earned her an international reputation
in the non-horticultural art world, of-
ten took gardens as its nominal subject.
Mayhap this metaphorical connection made
the BPC Fine Arts Committee think she had a
literal green thumb. (Contingency:Cause:Result)
These complex AltLex?s also raise the question
of why we find them at all in language. One part of
the answer is that these complex AltLex?s are used
to convey more than just the meaning of the rela-
tion. In most cases, we found that substituting the
AltLex with an adverbial connective led to some
aspect of the meaning being lost, as in Ex. (9-
10). Substituting For example for the AltLex with
an (necessary) accompanying paraphrase of Arg2
loses the information that the example provided as
Arg2 is possibly the most egregious one. The con-
nective for example does not allow similar modi-
fication. This means that one must use a different
strategy such as an AltLex expression.
(9) Typically, these laws seek to prevent exec-
utive branch officials from inquiring into
whether certain federal programs make
any economic sense or proposing more
market-oriented alternatives to regulations.
Probably the most egregious example is a pro-
viso in the appropriations bill for the executive
office that prevents the president?s Office of
Management and Budget from subjecting agri-
cultural marketing orders to any cost-benefit
scrutiny. (Expansion:Instantiation)
(10) For example, a proviso in the appropriations bill
for the executive office prevents the president?s Of-
1027
fice of Management and Budget from subjecting
agricultural marketing orders to any cost-benefit
scrutiny.
Another part of the answer to Why AltLex? is
that it can serve to convey a relation for which the
lexicon lacks an adverbial connective. For exam-
ple, while English has several adverbial connec-
tives that express a ?Cause:Consequence? relation
(eg, as a result, consequently, etc.), it lacks an
adverbial connective expressing ?Cause:Reason?
(or explanation) albeit having at least two sub-
ordinating conjunctions that do so (because and
since). Thus, we find an AltLex whenever this re-
lation needs to be expressed between sentences, as
shown in Ex. (11).
(11) But a strong level of investor withdrawals is
much more unlikely this time around, fund man-
agers said. A major reason is that investors al-
ready have sharply scaled back their purchases
of stock funds since Black Monday. (Contin-
gency:Cause:reason)
Note, however, that even for such relations such
as Cause:Reason, it is still not the case that a list of
canned expressions will be sufficient to generate
the Altlex or to identify them, since this relation
can itself be further modified. In Ex. (12), for ex-
ample, the writer intends to convey that there are
multiple reasons for the walkout, although only
one of them is eventually specified in detail.
(12) In Chile, workers at two copper mines, Los
Bronces and El Soldado, which belong to the
Exxon-owned Minera Disputada, yesterday voted
to begin a full strike tomorrow, an analyst
said. Reasons for the walkout, the analyst said,
included a number of procedural issues, such as
a right to strike. (Contingency:Cause:reason)
4 Lessons learned from AltLex
Like all lexical phenomena, DRMs appear to
have a power-law distribution, with some very
few high-frequency instances like (and, but), a
block of mid-frequency instances (eg, after, be-
cause, however), and many many low-frequency
instances in the ?long tail? (eg, much as, on the
contrary, in short, etc.). Given the importance
of DRMs for recognizing and classifying dis-
course relations and their arguments, what have
we learned from the annotation of AltLex?
First, the number of expressions found through
AltLex annotation, that belong to syntactic classes
admitted as connectives and also similarly frozen
(Table 1, row 1) shows that even in the PDTB,
there are additional instances of what we have
taken to be explicit connectives. By recognizing
them and unambiguously labelling their senses,
we will start to reduce the number of ?hard cases?
of implicit connectives whose sense has to be rec-
ognized (Marcu and Echihabi, 2002; Sporleder
and Lascarides, 2008; Pitler et al, 2009; Lin et al,
2009). Secondly, the number of tokens of expres-
sions from other syntactic classes that have been
annotated as AltLex (Table 1, rows 2 and 3) may
actually be higher than was caught via our Alt-
Lex annotation, thus making them even more im-
portant for discourse processing. To assess this,
we selected five of them and looked for all their
tokens in the WSJ raw files underlying both the
PTB and the PDTB. After eliminating those to-
kens that had already been annotated, we judged
whether the remaining ones were functioning as
connectives. Table 3 shows the expressions we
used in the first column, with the second and third
columns reporting the number of tokens annotated
in PDTB, and the number of additional tokens in
the WSJ corpus functioning as connectives. (The
asterisk next to the expressions is a wild card to al-
low for variations along the lines discussed for Ta-
ble 2.) These results show that these DRMs occur
two to three times more frequently than already
annotated.
Increased frequencies of AltLex occurrence are
also observed in discourse annotation projects un-
dertaken subsequent to the PDTB, since they were
able to be more sensitive to the presence of Alt-
Lex. The Hindi Discourse Relation Bank (HDRB)
(Oza et al, 2009), for example, reports that 6.5%
of all discourse relations in the HDRB have been
annotated as AltLex, compared to 1.5% in the
PDTB. This also provides cross-linguistic evi-
dence of the importance of recognizing the full
range of DRMs in a language.
5 Identifying DRMs outside the PDTB
As the set of DRMs appears to be both open-ended
and distributed like much else in language, with
a very long tail, it is likely that many are miss-
ing from the one-million word WSJ corpus anno-
tated in the PDTB 2.0. Indeed, in annotating En-
1028
AltLex Annotated Unannotated
The reason* 8 15
That?s because 11 16
The result* 12 18
That/This would* 5 16
That means 11 17
TOTAL 47 82
Table 3: Annotated and Unannotated instances of AltLex
glish biomedical articles with discourse relations,
Yu et al(2008) report finding many DRMs that
don?t appear in the WSJ (e.g., as a consequence).
If one is to fully exploit DRMs in classifying
discourse relations, one must be able to identify
them all, or at least many more of them than we
have to date. One method that seems promising
is Callison-Burch?s paraphrase generation through
back-translation on pairs of word-aligned corpora
(Callison-Birch, 2007). This method exploits the
frequency with which a word or phrase is back
translated (from texts in language A to texts in
language B, and then back from texts in language
B to texts in language A) across a range of pivot
languages, into other words or phrases.
While there are many factors that introduce
low-frequency noise into the process, including
lexical ambiguity and errors in word alignment,
Callison-Burch?s method benefits from being able
to use the many existing word-aligned translation
pairs developed for creating translation models for
SMT. Recently, Callison-Burch showed that para-
phrase errors could be reduced by syntactically
constraining the phrases identified through back-
translation to ones with the same syntactic cat-
egory as assigned to the source (Callison-Birch,
2008), using a large set of syntactic categories
similar to those used in CCG (Steedman, 2000).
For DRMs, the idea is to identify through back-
translation, instances of DRMs that were neither
included in our original set of explicit connec-
tive nor subsequently found through AltLex an-
notation. To allow us to carry out a quick pi-
lot study, Callison-Burch provided us with back-
translations of 147 DRMs (primarily explicit con-
nectives annotated in the PDTB 2.0, but also in-
cluding a few from other syntactic classes found
through AltLex annotation). Preliminary analysis
of the results reveals many DRMs that don?t ap-
pear anywhere in the WSJ Corpus (eg, as a con-
sequence, as an example, by the same token), as
well as additional DRMs that appear in the cor-
pus but were not annotated as AltLex (e.g., above
all, after all, despite that). Many of these latter
instances appear in the initial sentence of a para-
graph, but the annotation of implicit connectives
? which is what led to AltLex annotation in the
first place (Section 2) ? was not carried out on
these sentences.
There are two further things to note before clos-
ing this discussion. First, there is an additional
source of noise in using back-translation para-
phrase to expand the set of identified DRMs. This
arises from the fact that discourse relations can
be conveyed either explicitly or implicitly, and
a translated text may not have made the same
choices vis-a-vis explicitation as its source, caus-
ing additional word alignment errors (some of
which are interesting, but most of which are not).
Secondly, this same method should prove useful
for languages other English, although there will be
an additional problem to overcome for languages
(such as Turkish) in which DRMs are conveyed
through morphology as well as through distinct
words and phrases.
6 Related work
We are not the first to recognize that discourse re-
lations can realized by more than just one or two
syntactic classes. Halliday and Hasan (1976) doc-
ument prepositional phrases like After that being
used to express conjunctive relations. More im-
portantly, they note that any definite description
can be substituted for the demonstrative pronoun.
1029
Similarly, Taboada (2006), in looking at how of-
ten RST-based rhetorical relations are realized by
discourse markers, starts by considering only ad-
verbials, prepositional phrases, and conjunctions,
but then notes the occurrence of a single instance
of a nominal fragment The result in her corpus.
Challenging the RST assumption that the basic
unit of a discourse is a clause, with discourse rela-
tions holding between adjacent clausal units, Kib-
ble (1999) provides evidence that informational
discourse relations (as opposed to intentional dis-
course relations) can hold intra-clausally as well,
with the relation ?verbalized? and its arguments
realized as nominalizations, as in Early treatment
with Brand X can prevent a cold sore developing.
Since his focus is intra-clausal, he does not ob-
serve that verbalized discourse relations can hold
across sentences as well, where a verb and one
of its arguments function similarly to a discourse
adverbial, and in the end, he does not provide a
proposal for how to systematically identify these
alternative realizations. Le Huong et al (2003),
in developing an algorithm for recognizing dis-
course relations, consider non-verbal realizations
(called NP cues) in addition to verbal realizations
(called VP cues). However, they provide only one
example of such a cue (?the result?). Like Kib-
ble (1999), Danlos (2006) and Power (2007) also
focus only on identifying verbalizations of dis-
course relations, although they do consider cases
where such relations hold across sentences.
What has not been investigated in prior work
is the basis for the alternation between connec-
tives and AltLex?s, although there are several ac-
counts of why a language may provide more than
one connective that conveys the same relation.
For example, the alternation in Dutch between
dus (?so?), daardoor (?as a result?), and daarom
(?that?s why?) is explained by Pander Maat and
Sanders (2000) as having its basis in ?subjectiv-
ity?.
7 Conclusion and Future Work
Categorizing and identifying the range of ways in
which discourse relations are realized is impor-
tant for both discourse understanding and gener-
ation. In this paper, we showed that existing prac-
tices of cataloguing these ways as lists of closed
class expressions is problematic. We drew on our
experience in creating the lexically grounded an-
notations of the Penn Discourse Treebank, and
showed that markers of discourse relations should
instead be treated as open-class items, with uncon-
strained syntactic possibilities. Manual annota-
tion and automatic identification practices should
develop methods in line with this finding if they
aim to exhaustively identify all discourse relation
markers.
Acknowledgments
We want to thank Chris Callison-Burch, who
graciously provided us with EuroParl back-
translation paraphrases for the list of connectives
we sent him. This work was partially supported
by NSF grant IIS-07-05671.
References
Callison-Birch, Chris. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, School of Informatics, Univer-
sity of Edinburgh.
Callison-Birch, Chris. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Danlos, Laurence. 2006. Discourse verbs. In Pro-
ceedings of the 2nd Workshop on Constraints in Dis-
course, pages 59?65, Maynooth, Ireland.
Forbes-Riley, Katherine, Bonnie Webber, and Aravind
Joshi. 2006. Computing discourse semantics: The
predicate-argument semantics of discourse connec-
tives in D-LTAG. Journal of Semantics, 23:55?106.
Halliday, M. A. K. and Ruqaiya Hasan. 1976. Cohe-
sion in English. London: Longman.
Huong, LeThanh, Geetha Abeysinghe, and Christian
Huyck. 2003. Using cohesive devices to recog-
nize rhetorical relations in text. In Proceedings of
4th Computational Linguistics UK Research Collo-
quium (CLUK 4), University of Edinburgh, UK.
Kibble, Rodger. 1999. Nominalisation and rhetorical
structure. In Proceedings of ESSLLI Formal Gram-
mar conference, Utrecht.
Knott, Alistair. 1996. A Data-Driven Methodology
for Motivating a Set of Coherence Relations. Ph.D.
thesis, University of Edinburgh, Edinburgh.
1030
Lin, Ziheng, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, Singapore.
Maat, Henk Pander and Ted Sanders. 2000. Do-
mains of use or subjectivity? the distribution of
three dutch causal connectives explained. TOPICS
IN ENGLISH LINGUISTICS, pages 57?82.
Marcu, Daniel and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the Association for Com-
putational Linguistics.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Martin, James R. 1992. English text: System and
structure. Benjamins, Amsterdam.
Oza, Umangi, Rashmi Prasad, Sudheer Kolachina,
Dipti Mishra Sharma, and Aravind Joshi. 2009.
The hindi discourse relation bank. In Proceedings
of the ACL 2009 Linguistic Annotation Workshop III
(LAW-III), Singapore.
Pitler, Emily and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of the Joint Conference of the 47th
Meeting of the Association for Computational Lin-
guistics and the 4th International Joint Conference
on Natural Language Processing, Singapore.
Pitler, Emily, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse
relations in text. In Proceedings of the Joint Con-
ference of the 47th Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing.
Power, Richard. 2007. Abstract verbs. In ENLG ?07:
Proceedings of the Eleventh European Workshop on
Natural Language Generation, pages 93?96, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of 6th International Conference on
Language Resources and Evaluation (LREC 2008).
PDTB-Group. 2008. The Penn Discourse TreeBank
2.0 Annotation Manual. Technical Report IRCS-08-
01, Institute for Research in Cognitive Science, Uni-
versity of Pennsylvania.
Sporleder, Caroline and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: an assessment. Natural Language En-
gineering, 14(3):369?416.
Steedman, Mark. 2000. The Syntactic Process. MIT
Press, Cambridge MA.
Taboada, Maite. 2006. Discourse markers as signals
(or not) of rhetorical relations. Journal of Pragmat-
ics, 38(4):567?592.
Yu, Hong, Nadya Frid, Susan McRoy, P Simpson,
Rashmi Prasad, Alan Lee, and Aravind Joshi. 2008.
Exploring discourse connectivity in biomedical text
for text mining. In Proceedings of the 16th Annual
International Conference on Intelligent Systems for
Molecular Biology BioLINK SIG Meeting, Toronto,
Canada.
1031
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 155?163,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Structured and Unstructured Cache Models for SMT Domain Adaptation
Annie Louis
School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB
alouis@inf.ed.ac.uk
Bonnie Webber
School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB
bonnie@inf.ed.ac.uk
Abstract
We present a French to English transla-
tion system for Wikipedia biography ar-
ticles. We use training data from out-
of-domain corpora and adapt the system
for biographies. We propose two forms
of domain adaptation. The first biases
the system towards words likely in biogra-
phies and encourages repetition of words
across the document. Since biographies in
Wikipedia follow a regular structure, our
second model exploits this structure as a
sequence of topic segments, where each
segment discusses a narrower subtopic of
the biography domain. In this structured
model, the system is encouraged to use
words likely in the current segment?s topic
rather than in biographies as a whole.
We implement both systems using cache-
based translation techniques. We show
that a system trained on Europarl and news
can be adapted for biographies with 0.5
BLEU score improvement using our mod-
els. Further the structure-aware model out-
performs the system which treats the entire
document as a single segment.
1 Introduction
This paper explores domain adaptation of statisti-
cal machine translation (SMT) systems to contexts
where the target documents have predictable reg-
ularity in topic and document structure. Regular-
ities can take the form of high rates of word rep-
etition across documents, similarities in sentence
syntax, similar subtopics and discourse organiza-
tion. Domain adaptation for such documents can
exploit these similarities. In this paper we focus
on topic (lexical) regularities in a domain. We
present a system that translates Wikipedia biogra-
phies from French to English by adapting a system
trained on Europarl and news commentaries. This
task is interesting for the following two reasons.
Many techniques for SMT domain adaption
have focused on rather diverse domains such as us-
ing systems trained on Europarl or news to trans-
late medical articles (Tiedemann, 2010a), blogs
(Su et al., 2012) and transcribed lectures (Federico
et al., 2012). The main challenge for such systems
is translating out-of-vocabulary words (Carpuat et
al., 2012). In contrast, words in biographies are
closer to a training corpus of news commentaries
and parlimentary proceedings and allow us to ex-
amine how well domain adaptation techniques can
disambiguate lexical choices. Such an analysis is
harder to do on very divergent domains.
In addition, biographies have a fairly regu-
lar discourse structure: a central entity (person
who is the topic of the biography), recurring
subtopics such as ?childhood?, ?schooling?, ?ca-
reer? and ?later life?, and a likely chronological
order to these topics. These regularities become
more predictable in documents from sources such
as Wikipedia. This setting allows us to explore the
utility of models which make translation decisions
depending on the discourse structure. Translation
methods for structured documents have only re-
cently been explored in Foster et al. (2010). How-
ever, their system was developed for parlimentary
proceedings and translations were adapted using
separate language models based upon the identity
of the speaker, text type (questions, debate, etc.)
and the year when the proceedings took place.
Biographies constitute a more realistic discourse
context to develop structured models.
This paper introduces a new corpus consisting
of paired French-English translations of biography
articles from Wikipedia.
1
We translate this cor-
pus by developing cache-based domain adaptation
methods, a technique recently proposed by Tiede-
1
Corpus available at http://homepages.inf.ed.
ac.uk/alouis/wikiBio.html.
155
mann (2010a). In such methods, cache(s) can be
filled with relevant items for translation and trans-
lation hypotheses that match a greater number of
cache items are scored higher. These cache scores
are used as additional features during decoding.
We use two types of cache?one which encour-
ages the use of words more indicative of the biog-
raphy domain and another which encourages word
repetition in the same document.
We also show how cache models allow
for straightforward implementation of structured
translation by refreshing the cache in response to
topic segment boundaries. We fill caches with
words relevant to the topic of the current segment
which is being translated. The cache contents are
obtained from an unsupervised topic model which
induces clusters of words that are likely to ap-
pear in the same topic segment. Evaluation re-
sults show that cache-based models give upto 0.5
BLEU score improvements over an out-of-domain
system. In addition, models that take topical struc-
ture into account score 0.3 BLEU points higher
than those which ignore discourse structure.
2 Related work
The study that is closest to our work is that of
Tiedemann (2010a), which proposed cache mod-
els to adapt a Europarl-trained system to medical
documents. The system used caching in two ways:
a cache-based language model (stores target lan-
guage words from translations of preceding sen-
tences in the same document) and a cache-based
translation model (stores phrase pairs from pre-
ceding sentence translations). These caches en-
couraged the system to imitate the ?consistency?
aspect of domain-specific texts i.e., the property
that words or phrases are likely to be repeated in a
domain and within the same document.
Cache models developed in later work, Tiede-
mann (2010b) and Gong et al. (2011), were ap-
plied for translating in-domain documents. Gong
et al. (2011) introduced additional caches to store
(i) words and phrase pairs from training docu-
ments most similar to a current source article,
and (ii) words from topical clusters created on the
training set. However, a central issue in these sys-
tems is that caches become noisy over time, since
they ignore topic shifts in the documents. This pa-
per presents cache models which not only take ad-
vantage of likely words in the domain and consis-
tency, but which also adapt to topic shifts.
A different line of work very relevant to our
study is the creation of topic-specific translations
by either inferring a topic for the source document
as a whole, or at the other extreme, finer topics for
individual sentences (Su et al., 2012; Eidelman et
al., 2012). Neither of these granularities seem in-
tuitive in natural discourse. In this work, we pro-
pose that tailoring translations to topics associated
with discourse segments in the article is likely to
be beneficial for two reasons: a) subtopics of such
granularity can be assumed with reasonable con-
fidence to re-occur in documents from the same
domain and b) we can hypothesize that a domain
will have a small number of segment-level topics.
3 System adaptation for biographies
We introduce two types of translation systems
adapted for biographies:
General domain models (domain-) that use in-
formation about biographies but treat the docu-
ment as a whole.
Structured models (struct-) that are sensitive to
topic segment boundaries and the specific topic of
the segment currently being translated.
We implement both models using caches. Since
we do not have parallel corpora for the biography
domain, our caches contain items in the target lan-
guage only. We use two types of caches:
Topic cache stores target language words (uni-
grams) likely in a particular topic. Each unigram
has an associated score.
Consistency cache favours repetition of words in
the sentences from the same document. It stores
target language words (unigrams) from the 1-best
translations of previous sentences in the same doc-
ument. Each word is associated with an age value
and a score. Age indicates when a word entered
the cache and introduces a ?decay effect?. Words
used in immediately previous sentences have a
low age value while higher age values indicate
words from sentences much prior in the document.
Scores are inversely proportional to age.
Both the types of caches are present in both
the general domain and structured models, but the
cache words and scores are computed differently.
3.1 A general domain model
This system seeks to bias translations towards
words which occur often in biography articles.
The topic cache is filled with word unigrams
that are more likely to occur in biographies com-
156
pared to general news documents. We compare
the words from 1,475 English Wikipedia biogra-
phies articles to those in a large collection (64,875
articles) of New York Times (NYT) news articles
(taken from the NYT Annotated Corpus (Sand-
haus, 2008)). We use a log-likelihood ratio test
(Lin and Hovy, 2000) to identify words which oc-
cur with significantly higher probability in biogra-
phies compared to NYT. We collect only words
indicated with 0.0001 significance by the test to
be more likely in biographies. We rank this set of
18,597 words in decreasing order of frequency in
the biography article set and assign to each word
a score equal to 1/rank of the word. These words
with their associated scores form the contents of
the topic cache. In the general domain model,
these same words are assumed to be useful for the
full document and so the cache contents remain
constant during translation of the full document.
The consistency cache stores words from the
translations of preceding sentences of the same
document. After each sentence is translated, we
collect the words from the 1-best translation and
filter out punctuation marks and out of vocabu-
lary words. The remaining words are assigned an
age of 1. Words already present in the cache have
their age incremented by one. The new words with
age 1 are added to the cache
2
and the scores for
all cache words are recomputed as e
1/age
. The
age therefore gets incremented as each sentence?s
words are inserted into the cache creating a decay.
The cache is cleared at the end of each document.
During decoding, a candidate phrase is split into
unigrams and checked against each cache. Scores
for matching unigrams are summed up to obtain a
score for the phrase. Separate scores are computed
for matches with the topic and consistency caches.
3.2 A structured model
Here we consider topic and consistency at a nar-
rower level?within topic segments of the article.
The topic cache is filled with words likely in
individual topic segments of an article. To do this,
we need to identify the topic of smaller segments
of the article and also store a set of most probable
words for each topic. The topics should also have
bilingual mappings which will allow us to infer for
every French document segment, words that are
likely in such a segment in the English language.
We designed and implemented an unsupervised
2
If the word already exists in the cache, it is first removed.
topic model based on Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) to induce such word clus-
ters. In a first step, we induce subtopics from
monolingual articles in English and French sep-
arately. The topics are subsequently aligned be-
tween the languages as explained below.
In the first step, we learn a topic model which
incorporates two main ideas a) adds sensitivity
to topic boundaries by assigning a single topic
per topic segment b) allows for additional flex-
ibility by not only drawing the words of a seg-
ment from the segment-level topic, but also al-
lows some words to be either specific to the doc-
ument (such as named entities) or stop words. To
address idea b), we have a ?switching variable?
to switch between document-specific word, stop-
word or domain-words.
The generative story to create a monolingual
dataset of biographies is as follows:
? Draw a distribution ? for the proportion of the
three word types in the full corpus (domain
subtopic, document-specific, stopwords) ?
Dirichlet(?)
? For each domain subtopic ?
l
, 1 ? l ? T ,
draw a distribution over word vocabulary ?
Dirichlet(?)
? Draw a distribution ? over word vocabulary
for stopwords ? Dirichlet()
? For each document D
i
:
? Draw a distribution pi
i
over vocab-
ulary for document-specific words ?
Dirichlet(?)
? Draw a distribution ?
i
giving the mix-
ture of domain subtopics for this docu-
ment ? Dirichlet(?)
? For each topic segment M
ij
in D
i
:
? Draw a domain subtopic z
ij
?
Multinomial(?
i
)
? For each word w
ijk
in segment M
ij
:
? Draw a word type s
ijk
?
Multinomial(?)
? Depending on the chosen switch
value s
ijk
, draw the word from
the subtopic of the segment ?
z
ij
or document-specific vocabulary
pi
i
, or stopwords ?
We use the section markings in the Wikipedia
articles as topic segment boundaries while learn-
ing the model. We use symmetric Dirichlet priors
157
for the vocabulary distributions associated with
domain subtopics, document-specific words and
stopwords. The concentration parameters are set
to 0.001 to encourage sparsity. The distribution
?
i
for per-document subtopics is also drawn from
a symmetric Dirichlet distribution with concentra-
tion parameter 0.01. We use asymmetric Dirich-
let priors for ? set to (5, 3, 2) for (domain topic,
document-specific, stopwords). The hyperparam-
eter values were minimally tuned so that the differ-
ent vocabulary distributions behaved as intended.
We perform inference using collapsed Gibbs
sampling where we integrate out many multinomi-
als. The sampler chooses a topic z
ij
for every seg-
ment and then samples a word type s
ijk
for each
word in the segment. We initialize these variables
randomly and the assignment after 1000 Gibbs it-
erations are taken as the final ones. We create
these models separately for English and French,
in each case obtaining T domain subtopics.
The second step creates an alignment between
the source and target topics using a bilingual dic-
tionary
3
. For each French topic, we find the top
matching English topic by scoring the number
of dictionary matches. It is unlikely for every
French topic to have a closely corresponding En-
glish topic. Based on observations about the qual-
ity of topic alignment, we select the top 60% (out
of T ) pairs of French-English aligned topics only.
Note that our method uses two steps to learn
bilingual topics in contrast to some multilingual
topic models which learn aligned topics directly
from parallel or comparable corpora (Zhao and
Xing, 2006; Boyd-Graber and Blei, 2009; Jagar-
lamudi and Daum?e III, 2010). These methods in-
duce topic-specific translations of words. Rather
we choose a less restrictive pairing of word clus-
ters by topic since (i) we have monolingual bi-
ographies in the two languages which could be
quite heterogenous in the types of personalities
discussed, (ii) we seek to identify words likely in a
topic segment for example ?career-related? words
rather than specific translations for source words.
During translation, for each topic segment in the
source document, we infer the French topic most
likely to have produced the segment and find the
corresponding English-side topic. The most prob-
able words for that English topic are then loaded
into the topic cache. The score for a word is its
probability in that topic. When a topic segment
3
A filtered set of 13,400 entries from www.dict.cc
boundary is reached, the topic cache is cleared and
the topic words for the new segment are filled.
The consistency cache?s contents are computed
similarly to the general domain case. However, the
cache gets cleared at segment boundaries.
4 Training and test data
We distinguish two resources for data. The out-
of-domain system is trained using the WMT?12
datasets comprising Europarl and news commen-
tary texts. It has 2,144,820 parallel French-
English sentence pairs. The language model is
trained using the English side of the training cor-
pus. The tuning set has 2,489 sentence pairs.
Our test set is a corpus of French to En-
glish translations of biographies compiled from
Wikipedia. To create the biography corpus, we
collect articles which are marked with a ?Trans-
lation template? in Wikipedia metadata. These
markings indicate a page which is translated from
a corresponding page in a different language and
also contains a link to the source article. (Note
that these article pairs are not those written on
the same topic separately in the two languages.)
We collect pairs of French-English pages with this
template and filter those which do not belong to
the Biography topic (using Wikipedia metadata).
Note, however, that these article pairs are not
very close translations. During translation an edi-
tor may omit or add information and also reorga-
nize parts of the article. So we filter out the paired
documents which differ significantly in length. We
use LFAligner
4
to create sentence alignments for
the remaining document pairs. We constrain the
alignments to be within documents but since sec-
tion headings were not maintained in translations,
we did not further constrain alignments within sec-
tions. We manually corrected the resulting align-
ments and keep only documents which have good
alignments and have manually marked topic seg-
ments (Wikipedia section headings). Unaligned
sentences were filtered out. Table 1 shows a sum-
mary of this data and the split for tuning and test.
The articles are 12 to 87 sentences long and con-
tain 5 topic segments on average.
We also collect a larger set of monolingual
French and English Wikipedia biographies to cre-
ate the domain subtopics. We select only articles
that have at least 10 segments (sections) to ensure
4
http://sourceforge.net/projects/
aligner/
158
Tuning Test
No. of article pairs 15 30
Total sentences pairs 430 1008
Min. article size (in sentences) 13 12
Max. article size (in sentences) 59 85
Average no. of segments per article 4.7 5.3
Table 1: Summary of Wikipedia biographies data
that they are comprehensive ones. This collection
contains 1000 French and 1000 English articles.
5 Experimental settings
We use the Moses phrase-based translation system
(Koehn et al., 2007) to implement our models.
5.1 Out-of-domain model
This baseline model is trained on the WMT 2012
training sets described in the previous section and
uses the six standard features from Koehn et al.
(2003). We build a 5-gram language model us-
ing SRILM. The features were tuned using MERT
(Och, 2003) on the WMT 2012 tuning sets. This
system does not use any data about biographies.
5.2 Biography-adapted models
First we perform experiments using the manually
marked sections in Wikipedia as topic segments.
We also report results with automatic segmenta-
tion in Section 7.
The domain and structured models have two ex-
tra features ?topic cache? and ?consistency cache?.
For the structured model, topic segment bound-
aries and inferred topic is passed as XML markup
on the source documents. For the consistency
cache, we use a wrapper which passes the 1-
best translation (also using XML markup) of the
preceding sentence and updates the cache before
translating every next sentence.
We tune the weights for these new cache fea-
tures as follows. The weights for the baseline fea-
tures from the out-of-domain model are kept con-
stant. The weights for the new cache features are
set using a grid search. This tuning uses the bi-
ographies documents listed in Table 1 as tuning
data. We run the decoding using the baseline fea-
ture weights and a weight for a cache feature and
compute the (case-insensitive) BLEU (Papineni et
al., 2002) scores of each tuning document. The
weight for the cache feature which maximizes the
average BLEU value over the tuning documents
is chosen. We have not tuned the features us-
ing MERT in this study since a grid search al-
lowed us to quantify the influence of increasing
Figure 1: Effect of feature weights and number of
topics on accuracy for structured topic cache
weights on the new features directly. Previous
work has noted that MERT fails to find good set-
tings for cache models (Tiedemann, 2010b). In
future work, we will explore how successful op-
timization of baseline and cache feature weights
could be done jointly. We present the findings
from our grid search below.
The struct-topic cache has two parameters, the
number of topics T and the number of most prob-
able words from each topic which get loaded into
the cache. We ran the tuning for T = 25, 50,
100 and 200 topics (note that 60% of the topics
will be kept after bilingual alignment, see Section
3.2). We also varied the number of topic words
chosen?50, 100, 250 and 500.
The performance did not vary with the number
of topic words used and 50 words gave the same
performance as 500 words for topic models with
any number of topics. This interesting result sug-
gests that only the most likely and basic words
from each topic are useful. The top 50 words from
two topics (one capturing early life and the other
an academic career) taken from the 50-topic model
on English biographies are shown in Table 2.
In Figure 1, we show the performance of sys-
tems using different number of topics. In each
case, the same number of topic words (50) was
added to the cache. We find that 50 topics model
performs best confirming our hypothesis that only
a small number of domain subtopics is plausible.
We choose the 50 topic model with top 50 words
for each topic for the structured topic cache.
The best weights and average document level
BLEU scores on the tuning set are given in Table
3. The scores were computed using the mteval-
v13a.pl script in Moses. BLEU scores for the
159
his a s family on life She child St mother
of in married children They death became whom friends attended
and had He that daughter son marriage lived later work
to was born died wife years met couple I age
he her at she father home moved about husband house
of is He received has included National original Academy French
and The by its used works study list book College
his work Award Medal award His Institute life contributed Year
in he are awarded also title Arts Royal edition awards
s University Prize Society A honorary Library include Sciences recognition
Table 2: Top 50 words from 2 topics of the T = 50 topic model
Cache type weight BLEU-doc
Domain-topic 0.075 19.79
Domain-consistency 0.05 19.70
Domain-topic + consis. 0.05, 0.05 19.80
Struct-topic (50 topics) 1.75 19.94
Struct-consistency 0.125 19.70
Struct-topic + consis. 0.4, 0.1 19.84
Domain-consis. + struct-topic 0.1, 0.25 19.86
Out-of-domain 19.33
Table 3: Best weights for cache features and
BLEU scores (averaged for tuning documents).
out-of-domain model are shown on the last line.
Note that these scores are overall on a lower scale
for a French-English system due to out-of-domain
differences and because the reference translations
from Wikipedia are not very close ones.
These numbers show that cache models have the
potential to provide better translations compared
to an out-of-domain baseline. The structured topic
model system is the best system outperforming the
out-of-domain system and also the domain-topic
system. Hence, treating documents as composed
of topical segments is a useful setting for auto-
matic translation.
The domain and structured versions of the con-
sistency cache however, show no difference. This
result could arise due to the decay factor incor-
porated in the consistency cache. Higher scores
are given to words from immediately previous
sentences compared to those far off. This decay
implicitly gives lower scores to words from ear-
lier topic segments than those from recent ones.
Explicitly refreshing the cache in the structured
model does not give additional benefits.
When consistency and topic caches are used to-
gether in both general domain and structured set-
tings, the combination is not better than individual
caches. We also tried a setting where the consis-
tency cache is document-range and the topic cache
works at segment level (domain-consis. + struct-
topic). This combination also does not outperform
using the structured topic cache alone.
Model BLEU-doc BLEU-sent
Domain-topic 17.63 17.61
Domain-consistency 17.70 17.75
Domain-topic + consis. 17.63 17.63
Struct-topic (50 topics) 17.76 17.84
Struct-consistency 17.33 17.34
Struct-topic + consis. 17.47 17.51
Struct-topic + dom-consis. 17.29 17.25
Out-of-domain 17.37 17.43
Table 4: BLEU scores on the test set. ?doc? in-
dicates BLEU scores averaged over documents,
?sent? indicates sentence-level BLEU
6 Results on the test corpus
The best weights chosen on the tuning corpus are
used to decode the biographies test corpus (sum-
marized in Table 1). Table 4 reports the av-
erage BLEU of documents as well as sentence
level BLEU scores of the corpus. We used the
paired bootstrap resampling method (Koehn 2004)
to compute significance.
The struct-topic model gives the highest im-
provement of 0.4 sentence level BLEU over the
out-of-domain model. Struct-topic is also 0.23
BLEU points better compared to the domain-
topic model confirming the usefulness of model-
ing structure regularities. These improvements at
significant at 95% confidence level.
The second best model is the domain-
consistency model (significantly better than out-
of-domain model at 90% confidence level). But
the performance of this cache decreases in the
structured setting. Moreover, combinations of
caches fail to improve over individual caches.
One hypothesis for this result is that biogra-
phy subtopic words which give good performance
in the topic cache differ from the words which
provide benefits in the consistency cache. For
example, words related to named entities and
other document-specific content words could be
ones that are more consistent within the docu-
ment. Then clearing the consistency cache at topic
boundaries would remove such words from the
160
cache leading to low performance of the ?struc-
tured? version. In our current model, we do not
distinguish between words making up the consis-
tency cache. In future, we plan to experiment
with consistency caches of different ranges and
which hold different types of words. This ap-
proach would require identifying named entities
and parts of speech on the automatic translations
of previous sentences, which is likely to be error-
prone and so require methods for associating a
confidence measure with the cache words.
7 Understanding factors that influence
structured cache models
The documents in our test corpus have varying
lengths, number of segments and segment sizes.
This section explores the behavior of structured
models on these different document types. For
this analysis, we compare the BLEU scores from
the domain and the structured versions of the two
caches. We do not consider the out-of-domain sys-
tem here since we are interested in quantifying
gains from using document structure.
For each document in our test corpus, we com-
pute (i) the difference between the BLEU scores
of struct-topic and domain-topic systems (BLEU-
gain-topic), and (ii) the difference in BLEU
scores between the struct-consistency and domain-
consistency systems (BLEU-gain-consis). Table 5
reports the average BLEU gains binned by a) the
document length (in sentences) b) number of topic
segments in the document and c) the average size
of topic segments in a document (in sentences).
The numbers clearly indicate that performance
is not uniform across different types of docu-
ments. The struct-topic cache performs much bet-
ter on longer documents of over 30 sentences giv-
ing 0.3 to 0.4 BLEU points increase compared to
the general domain model. On the other hand, the
performance worsens when the structured cache
is applied on documents with less than 20 sen-
tences. Similarly, the struct-topic cache is benefi-
cial for documents where the average segment size
is larger than 5 sentences and when the number of
topic segments is around 5 to 7.
The struct-consistency cache generally per-
forms worse than the unstructured version and
there does not appear to be a niche set according
to any of the properties?document length, num-
ber of segments and segment size.
Given these findings, it is possible that the
struct-topic cache can benefit by modifying the
(a) Average BLEU gains and document length
doc. length no. docs gain-topic gain-consis
12 to 19 7 -0.41 -0.20
20 to 29 10 0.17 -0.63
30 to 49 8 0.44 -0.16
50 to 85 5 0.34 -0.45
(b) Average BLEU gains and no. of topic segments
no. segments no. docs gain-topic gain-consis
3 to 4 9 -0.09 -0.21
5 13 0.24 -0.37
6 to 7 5 0.34 -0.74
9 3 -0.03 -0.26
(c) Average BLEU gains and topic segment size
avg. segment size no. docs gain-topic gain-consis
< 5 10 -0.23 -0.41
5 to 10 18 0.33 -0.37
11 to 17 2 0.39 -0.24
Table 5: Average BLEU score gains from a struc-
tured cache (compared to domain caches) split by
different properties of documents in the test set
document structure to match that handled better
by the structured model. We test this hypothe-
sis by segmenting all test documents with an ideal
segment size. The model seems to perform better
when each segment has around 5 to 10 sentences
(longer segments are also preferred but we have
few very long documents in our corpus), so we
try to re-segment the articles to contain approxi-
mately 7 sentences in each segment. We use an
automatic topic segmentation method (Eisenstein
and Barzilay, 2008) to segment the source arti-
cles in our test corpus. For each article we request
(document length)/7 segments to be created.
5
We then run the structured topic and consis-
tency models on the automatically segmented cor-
pus using the same feature weights as before. The
results are shown in Table 6.
Model BLEU (doc) BLEU (sent)
Struct-topic 17.94 17.94
Struct-consistency 17.51 17.46
Table 6: Translation performance on automati-
cally segmented test corpus
The struct-topic cache now reaches our best re-
sult of 0.5 BLEU improvement over the out-of-
domain model and 0.3 improvement over the un-
structured domain model. The consistency cache
is also slightly better using the automatic segmen-
tation than the manual sections. Choosing the
right granularity appears to be important for struc-
tured caches and coarse section headers may not
be ideal. This result also shows automatic segmen-
5
Note that we only specify the number of segments, but
the system could create long or short segments.
161
of (42) he (36) his (36) the (22) to (11) in (9) was (7) one (6) a (3) at (3)
head (3) that (3) construction (3) empire office french bases reconstruction only such
all ban marseille main charged have well researchers openness retreat
an two mechanical events army iron class surrender order thirty
and black objectives factory disciple largest close budget part time
as who ceremony figure majority level even sentence project trained
on seat diplomatic wheat working winner life archaeological 9 during
Table 7: Impact words computed on the test corpus. The number of times each word was found in the
impact list is indicated within parentheses. Words listed without parentheses appeared once in the list.
(1) (S) Pendant la Premi`ere Guerre mondiale, mobilis?e dans les troupes de marine, il combat dans les Balkans et les
Dardanelles.
(R) During the First World War, conscripted into the navy, he fought in the Balkans and the Dardanelles.
(B) During World War I, mobilized in troops navy, it fight in the Balkans and Dardanelles.
(C) During World War I, mobilized troops in the navy, he fight in the Balkans and the Dardanelles.
(2) (S)
`
A l??age de 15 ans, elle a ?et?e choisie par la troupe d?op?era de l?arm?ee chinoise pour ?etre form?ee au chant.
(R) At the age of 15, she was selected by the Chinese Armys Operatic troupe to be trained as a singer.
(B) In the age of 15 years, she was chosen by the pool of opera of the Chinese military to be formed the call.
(C) In the age of 15 years, she was chosen by the pool of opera of the Chinese military to be trained to call.
(3) (S) La figure de la Corriveau n?a cess?e, depuis, d?inspirer romans, chansons et pi`eces de th?e?atre et d?alimenter les
controverses.
(R) The figure of Corriveau still inspires novels, songs and plays and is the subject of argument.
(B) The perceived the Corriveau has stopped, since, inspire novels, songs and parts of theater and fuel controversies.
(C) The figure of the Corriveau has stopped, since, inspire novels, songs and parts of theater and fuel controversies.
Table 8: Three examples of impact words in test translations. Abbreviations: S - source sentence, R -
reference translation, B - baseline translation, C - structured topic cache translation
tation can be successfully used in these models.
8 Changes made by the cache models
Here we examine the kinds of changes made by
the cache models which have lead to the im-
proved BLEU scores. We focus on the the topic
cache since its changes are straightforward to
compute compared to consistency. We analyze
the struct-topic cache translations on automati-
cally segmented documents as that provided the
best performance overall.
To do this analysis, we define the notion of an
impact word. An impact word is one which satis-
fies three conditions: (i) the word is not present in
the out-of-domain translation of a sentence, (ii) it
is present in the translation produced by the topic
cache model (iii) the word matches the reference
translation for the sentence.
These impact words provide a simple (albeit ap-
proximate) way to analyze useful changes made
by the topic cache over the out-of-domain system.
On the test corpus (30 documents), 231 impact
word tokens were found and they come from 70
unique word types. So topic cache model signif-
icantly affects translation decisions and over 200
useful word changes were made in the 30 doc-
uments. The impact word types and counts are
shown in Table 7. Several of these changes relate
to function words and pronouns. For example, the
pronoun ?he? and the past tense verb ?was? were
correctly introduced in several sentences such as
Example (1) in Table 8. A content word change is
indicated in examples (2) and (3). These changes
appear to be appropriate for biographies.
9 Conclusions
We have introduced a new corpus of biography
translations which we propose as suitable for ex-
amining discourse-motivated SMT methods. We
showed that cache-based techniques which also
take the topic organization into account, make
more appropriate lexical choices for the domain.
In future work, we plan to explore how other do-
main similarities such as sentence syntax and en-
tity reference, for example biographies have a cen-
tral entity (person), can be used to improve transla-
tion performance. We also plan to take advantage
of recent methods to do document level decoding
(Hardmeier et al., 2012).
Acknowledgements
The first author was supported by a Newton Inter-
national Fellowship (NF120479) from the Royal
Society and The British Academy. We also thank
the NLP group at Edinburgh for their comments
and feedback on this work.
162
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multi-
lingual topic models for unaligned text. In Proceed-
ings of UAI, pages 75?82.
Marine Carpuat, Hal Daum?e III, Alexander Fraser,
Chris Quirk, Fabienne Braune, Ann Clifton, Ann
Irvine, Jagadeesh Jagarlamudi, John Morgan, Ma-
jid Razmara, Ale?s Tamchyna, Katharine Henry, and
Rachel Rudinger. 2012. Domain adaptation in ma-
chine translation: Final report. In 2012 Johns Hop-
kins Summer Workshop Final Report.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of ACL, pages
115?119.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP, pages 334?343.
Marcello Federico, Mauro Cettolo, Luisa Bentivogli,
Michael Paul, and Sebastian Stueker. 2012.
Overview of the IWSLT 2012 evaluation campaign.
Proceedings of IWSLT.
George Foster, Pierre Isabelle, and Roland Kuhn.
2010. Translating structured documents. In Pro-
ceedings of AMTA.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of EMNLP, pages
909?919.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the EMNLP-CoNLL, pages 1179?1190.
Jagadeesh Jagarlamudi and Hal Daum?e III. 2010. Ex-
tracting multilingual topics from unaligned compa-
rable corpora. In Advances in Information Retrieval,
Lecture Notes in Computer Science, pages 444?456.
Springer Berlin Heidelberg.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-HLT, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
open source toolkit for statistical machine transla-
tion. In Proceedings of the ACL meeting on Interac-
tive Poster and Demonstration Sessions, pages 177?
180.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In Proceedings of COLING, pages 495?501.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Evan Sandhaus. 2008. The New York Times Anno-
tated Corpus. Corpus number LDC2008T19, Lin-
guistic Data Consortium, Philadelphia.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of ACL, pages 459?468.
J?org Tiedemann. 2010a. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing.
J?org Tiedemann. 2010b. To cache or not to cache?:
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 189?194.
Bing Zhao and Eric P. Xing. 2006. Bitam: bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING-ACL, pages 969?976.
163
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 598?606,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Applying the semantics of negation to SMT through n-best list re-ranking
Federico Fancellu
Centre for Global Intelligent Content
School of Computer Science and Statistics
Trinity College Dublin
ffancellu@cngl.ie
Bonnie Webber
School of Informatics
University of Edinburgh
Edinburgh, UK, EH8 9AB
bonnie@inf.ed.ac.uk
Abstract
Although the performance of SMT sys-
tems has improved over a range of differ-
ent linguistic phenomena, negation has not
yet received adequate treatment.
Previous works have considered the prob-
lem of translating negative data as one of
data sparsity (Wetzel and Bond (2012)) or
of structural differences between source
and target language with respect to the
placement of negation (Collins et al.
(2005)). This work starts instead from the
questions ofwhat is meant by negation and
what makes a good translation of negation.
These questions have led us to explore the
use of semantics of negation in SMT ?
specifically, identifying core semantic el-
ements of negation (cue, event and scope)
in a source-side dependency parse and re-
ranking hypotheses on the n-best list pro-
duced after decoding according to the ex-
tent to which an hypothesis realises these
elements.
The method shows considerable improve-
ment over the baseline as measured by
BLEU scores and Stanford?s entailment-
based MT evaluation metric (Pad? et al.
(2009)).
1 Introduction
Translating negation is a task that involves more
than the correct rendering of a negation marker in
the target sentence. For instance, translating Italy
did not defeat France in 1909 differs from trans-
lating Italy defeated France in 1909, or France
did not defeat Italy in 1909, or Italy did not con-
quer France in 1909. These examples show that
translating negation also involves placing in the
right position the semantic arguments as well as
the event directly negated. Moreover, if the source
sentence was uttered in response to the statement I
think Italy defeated France in 1911, where the fo-
cus is the temporal argument in 1911, one can see
that the system should not lose track of the focus
of negation when producing the hypothesis trans-
lation.
Although negation must be appropriately ren-
dered to ensure correct representation of the se-
mantics of the source sentence in the machine out-
put, only some of the efforts to improve the transla-
tion of negation-bearing sentences in SMT address
the problem.
Wetzel and Bond (2012) considered negation as
a problem of data sparsity and so attempted to en-
rich the training data with negative paraphrases
of positive sentences. Collins et al. (2005) and
Li et al. (2009) both addressed differences in the
placement of negation in source and target texts,
by re-ordering negative elements in the source sen-
tence to better resemble their position in the corre-
sponding target text. Although these approaches
show improvement over the baseline, neither con-
siders negation as a linguistic phenomenon with
specific characteristics.
This we do in the work presented here: We iden-
tify the elements of negation that an MT system
has to reproduce and then devise a strategy to en-
sure that they are output correctly. These elements
we take to be the cue, event and scope of nega-
tion1. Unlike previous works, we first validate
the hypothesis that if the top-ranked translation in
the n-best list does not replicate elements of nega-
tion from the source, there may be a more accurate
translation after decoding, somewhere else on the
n-best list. If the hypothesis is false, then problems
in the translation of negation lie elsewhere.
1Due to its ambiguity and the fact that it is already in-
cluded in the scope, we have ignored the focus of negation.
That does not mean it may not be important to correctly re-
produce the focus; there might be cases where, although not
fully-capturing the scope, we want to translate correctly the
part that is directly negated or emphasised.
598
We use dependency parsing as a basis for N-
best list re-ranking. Dependencies between lexical
elements appear to encode all elements of nega-
tion, offering a robust and easily-applicable way
to extract negation-related information from a sen-
tence. We carry out our exploration of N-best list
re-ranking in two steps:
? First, an oracle translation is computed both
to assess the validity of the approach and to
understand the maximal extent to which it
could possibly enhance performance. An or-
acle translation is obtained by performing n-
best list re-ranking using reference transla-
tions as a gold-standard.
To avoid the problem in Chinese-English Hi-
erarchical Phrase-Based (HPB) translation of
loss and/or misplacement of negation-related
elements when hierarchical phrases are built,
Chinese source sentences are first broken into
sub-clauses Yang and Xue (2012), then trans-
lated and finally ?stitched? back together for
evaluation.
? Standard n-best list re-ranking is then per-
formed using only source-side information.
Hypotheses are re-ranked according to the
degree of similarity between the negation-
related elements in the hypotheses and those
in the source sentence. Here the correspon-
dence between source and target text is es-
tablished through lexical translation probabil-
ities output after training.
Results of this method show that n-best list rerank-
ing does lead to a significant improvement in
BLEU score. However, BLEU says nothing about
semantics, so we also evaluate the method using
Stanford?s entailment basedMTmetrics Pad? et al.
(2009), and also show improvement here. In the
final section of the paper, we note the value of de-
veloping a custom metric that actually assesses the
components of negation.
2 Related works
Negation has been a widely discussed topic out-
side the field of SMT, with recent works focused
mainly on automatic detection of negation. Blanco
and Moldovan (2011) have established the distri-
bution of negative cues and the syntactic structures
in which they appear in the WSJ section of the
Penn Treebank, as a basis for automatically de-
tecting scope and focus of negation using simple
heuristics.
Machine-learning has been used by systems
participating in the *SEM 2012 shared task on
automatically detecting the scope and focus of
negation. Those systems with the best F1 mea-
sures (Chowdhury and Mahbub (2012), Read et al.
(2012) and Lapponi et al. (2012) all use a mix-
ture of SVM (Support Vector Machines) and CRF.
Their performance improves significantly when
syntactic features are also considered. In partic-
ular, Lapponi et al. (2012) use features extracted
from a dependency parse to guide their system to
detect the correct scope boundary.
In translation, only few efforts have focussed on
the problem of translating negation. Wetzel and
Bond (2012) treat it as resulting from data spar-
sity. To remedy this, they enrich their Japanese-
to-English training set with negative paraphrases
of positive sentences, where negation is inserted
as a ?handle? to the main verb after a sentence is
parsed using MSR (Minimal Recursion Semantics
Copestake et al. (2005)). Results show that BLEU
score improves on a test sub-set containing only
negative sentences when extra negative data is ap-
pended to the original training data and the lan-
guage model is enriched as well. However, system
performance deteriorates on both the original test
set and on positive sentences. Moreover, generat-
ing paraphrases with negation expressed only on
the main verb does not allow to fully capture the
various ways negation can be expressed.
Other works considered negation in the frame-
work of clause restructuring. Collins et al. (2005)
pre-process the German source to resemble the
structure of English while Li et al. (2009) tried
to swap the order of the words in a Chinese sen-
tence to resemble Korean. Rosa (2013) takes a
post-processing approach to negation in English-
Czech translation, ?fixing? common errors such as
the loss of a negation cue by either generating the
morphologically negative form of the relevant verb
(if the verb has such a form) or prefixing the verb
with the negative prefix ne. Despite the improve-
ments, these approaches do not really address what
is special about negation.
3 Decomposing negation
Correctly translating negation involves more than
placing a negative marker in the right position. We
follow Blanco andMoldovan (2011) in decompos-
ing negation into three main components:
599
? a negation cue, including negative markers,
affixes and all the words or multiwords units
that inherently express negation.
? a negation event, i.e. the event that is directly
negated. Events can be either verbs (e.g. ?I do
not go to the cinema) or adjectives (e.g. ?He
is not clever?).
? a negation scope, i.e. the part of the state-
ment whose meaning is negated (Blanco and
Moldovan, 2011, 229). The scope contains all
those words that, if negated, would make the
statement true. We follow here the guidelines
for annotating negative data released during
the *SEM 2012 Shared Task Morante et al.
(2011) for a more detailed understanding on
what to consider part of the negation scope.
In addition to these three components, formal se-
manticists identify a negation focus, i.e. the part
of the scope that is directly negated or more em-
phasized. Focus is the most difficult part to detect
since it is the most ambiguous. In the sentence ?he
does not want to go to school by car? the speaker
emphasized the fact that ?he does not want to go
to school by car? or that ?he does not want to go
to school by car? (but he wants to go somewhere
else) or that ?he does not want to go to school by
car? (but by other means of transportation).
Translating negation is therefore a matter of en-
suring that the cue is present, that its attachment to
the corresponding event follows language-specific
rules and that all the elements included in the scope
are placed in the right order. Correctly reproduc-
ing the focus is left for future works.
4 Methodology
4.1 N-best list re-ranking
N-best list re-ranking is used in SMT to deal with
sentence-level phenomena whose locality goes be-
yond n-grams or single hierarchical rules. It in-
volves re-ranking the list of target-language hy-
potheses produced by decoding, using additional
features extracted from the source sentence. In the
case of negation, N-best list re-ranking allows us to
assess whether a system is able to correctly trans-
late the elements of negation, while failing to place
the best hypothesis on these grounds at the top of
the n-best list.
The current work follows the same approach as
other n-best list re-rankers (Och et al. (2004); Spe-
cia et al. (2008); Apidianaki et al. (2012)) but using
negation as the additional feature. Negation is here
defined as the degree of overlap of cue, event and
scope between the hypothesis translation and the
source sentence.
Following Hasan et al. (2007), we use an n-best
list of 10000 sentences but we do not initially tune
the negation feature using MERT or interpolate it
with other features. This is because in order to as-
sess the degree of overlap between the scope in
the source and the hypothesis sentence, a n-gram
based score is used which conveys the same in-
formation as that of the language model score in
the log-linear model. Moreover, our re-ranking ex-
ploits lexical translation probabilities, thereby re-
sembling a simple translation model.
4.2 Extract negation using dependency
parsing
The degree of overlap between the source sentence
and the hypothesis translation is measured in terms
of the overlap between their negation cue, event
and scope. These must therefore be correctly ex-
tracted. Dependency parsing provides an efficient
way to do so, with several advantages:
? Dependency parsing encodes the notions of
cue and event as the dependant and the head
respectively of a ?neg? relation. Scope can
be approximated through recursive retrieval
of all the descendants of the verb-event. The
following example shows how these elements
are extracted from the dependency parse:
Peter and
conj

conj

Mary did not buy
nsubj

punct

obj

aux

neg

a blue car
det

amod

.
nsubj(buy-6, and-2) , conj(and-2, Peter-1), conj (and-2, Mary-3),
aux(buy-6, did-4), neg(buy-6, not-5), root(ROOT-0, buy-6),
det(car-9, a-7), amod(car-9, blue-8) , dobj(buy-6, car-9)
The ?neg? dependency relation conveys both
the negation cue (not-5) and the negation
event (buy-6) of the sentence ?Peter andMary
did not buy a blue car?. An approximate scope
can be recovered by following the path from
the event (included) to the terminal nodes and
collecting all the lexical elements along the
way.
Also in the case of a sentence containing
a subordinate clause, dependency parsing is
600
able to correctly capture the latter as part of
the scope given that the relative pronoun de-
pends directly on the event of the main clause.
On the other hand, recursion from the negated
event excludes coordinate clauses that are not
considered part of the scope, given that the
event is a dependant of the connective.
One problem with this method is that it is
unable to capture the entire scope when the
head is nominal. For instance, ?no reasons
were given?, the ?neg? dependency holds be-
tween no and reasons but it needs to climb
the hierarchy further to get to the verbal head
given. The same holds for negation on ob-
ject nominals. We leave this to future work
(along with affix-conveyed negation), need-
ing to show first that the current approach is
a good one.
? A dependency parser can be developed for
any language for which a Treebank or Prop-
bank is available for training. This extends
the range of source languages to which the ap-
proach can be applied.
4.3 Computing an oracle translation
In order to test the validity of the method and to
assess its maximum contribution, we first use it
with an oracle translation in which n-best list re-
ranking relies on a comparison with negation cue,
event and scope extracted from the reference trans-
lation(s), here assumed to correctly contain all el-
ements of negation.
Each hypothesis on the n-best list is assigned
an overlap score with these reference-translation-
derived elements, and the hypothesis with the
highest score is re-ranked at the top and used for
evaluation.
The overlap score is obtained by summing up
three sub-scores: (i) the cue overlap score mea-
sures how many cues in reference are repre-
sented in the hypothesis, normalised by the num-
ber of cues in the reference; (ii) the event over-
lap score measures how many events in the refer-
ence are represented in the hypothesis, normalised
by the number of the events in the reference;
and (iii) the scope overlap score is a weighted n-
gram overlap between hypothesis scope and ref-
erence scope, with higher weight for higher-order
n-grams. Given less-than-perfect machine out-
put, breaking down the score into subscores al-
lows us to consider different degrees of correct-
ness in translating negation. When multiple ref-
erence translations are available, the hypothesis is
matched with each, and only the best score taken
into consideration.
4.4 Re-ranking using lexical translation
probabilities
After the oracle translation is computed, tradi-
tional n-best list re-ranking is performed relying
on source side information only. We then bridge
the gap between source and target language using
lexical translation probabilities to render source-
side cue, event, scope into the target language. Re-
ranking involves three separate steps:
? The source sentence is parsed and dependen-
cies extracted. Since the present work tack-
les Chinese-to-English translation, we had to
enhance the representation of negative depen-
dencies in the Chinese source, where only the
adverb ? bu4 is flagged as ?neg? dependant.
To do this, we follow the same intuition used
to isolate negation-bearing sentences in the
test set (see section 5).
? To obtain a rough translation of cue, event and
scope in the target language, the top ten lex-
ical translation probabilities for each lexical
item, available in the lexical translations (in
order of probability) table output after train-
ing, are considered.
? Hypotheses in the n-best list are re-scored tak-
ing into consideration the information above.
Scoring cue and event is straightforward; the
words for the cue and the event are assigned
the lexical probability of being the translation
of the cue and the event in the Chinese sen-
tence by looking up the lexical translation ta-
ble. If the cue or the event do not figure as
translations of the negation cue and event in
the Chinese sentence, a score of 0 is assigned
to them.
The scope is instead scored by looping
through the words in the hypothesis; for each
such hypothesis word, the process identifies
which source-side scopeword it is most likely
to be the translation of. If no scope can be re-
trieved, a score of 0 is given for scope match-
ing. For each word the best translation proba-
bility is taken into account and these are then
summed together to score how likely is the
601
scope in the hypothesis to be the translation
of the scope in the source.
5 System
A hierarchical phrase based model (HPBM) was
trained on a corpus of 625000 (? 18.200.000 to-
kens) length-ratio filtered sentences. 56949 sen-
tences (? 9.11%) of the Chinese side and 48941
sentences (? 7.83%) of the English side of the
training set were found to include at least one
instance of negation. 2500 sentences were in-
stead included in the dev set to tune the log-linear
model using the MERT algorithm. 3725 sentences
from the Multiple-Translation Chinese Corpus 2.0
(LDC2002T01) were used as test set. The test
set comes divided into four sub-sets; in this paper
these sub-sets are referred as test set 1 to 4. The
source side was tokenized using the Stanford Chi-
neseWord Segmenter (Tseng et al. (2005)) and en-
coded in ?utf-8? so to serve as input to the system.
In order to focus on the problem of translating neg-
ative data, the 563 sentence pairs containing nega-
tion were extracted from the original test set. This
test set constitutes the true baseline improvements
will be measured upon. Reducing the number
of test sentences also eases the computation load
when involved in dependency parsing on 10.000
sentences in each n-best list. Negated sentence
were isolated by means of both regular expres-
sions and dependency parsing; this is because, as
pointed out above, the Chinese side does not flag
all negative dependencies as such.2
6 Results
6.1 Baseline
BLEU scores for the baseline systems are given in
Table 1, where the negative subset is compared to
the original (all sentences) and only positive sen-
tences conditions.
Table 2 shows instead the result for the nega-
tive baseline across three different metrics. Along
with the BLEU scores, we also took into consider-
ation an entailment-based MT evaluation metric,
2While the English dependency parser is able to iden-
tify almost all negative markers and their dependencies, the
Chinese dependency parser here deployed (the Stanford Chi-
nese Dependency parser) only captures sentences contain-
ing the adverb ?bu4. For this reason, we exploited the
list of negation adverbs included in the Chinese Propbank
(LDC2005T23) documentation and look for each of them via
regular expressions. Moreover we also looked for words con-
taining ? as component since they are most likely to carry
negative meaning (e.g. ??, ?not-long?).
the RTE score3 Pad? et al. (2009). The RTE score
assesses to what extent the hypothesis entails the
reference translation across a wide variety of se-
mantic and syntactic features. Another reason we
chose this metric is because it contains a feature for
polarity as well as features to check the degree of
similarity between the syntactic tree and the depen-
dencies between hypothesis and reference transla-
tion, the latter being what we used to recover the
elements of negation. We expect this metric to give
a further insight on the quality of the machine out-
put.
Baseline results are in line with the results of
Wetzel and Bond (2012), where there is a drop in
BLEU scores between positive and negative sen-
tences, and between the overall test set and the one
containing negative data only.
When analysing the results from the baseline, we
noticed that words were being deleted or moved
inappropriately when the hierarchy of phrases was
being built. This might be detrimental to the trans-
lation of negation since elements might end up out-
side the correct negation scope. The following ex-
ample illustrates this problem.
(1) Source : ? ? ? ? ?? ?? ?? ??
?????????????????
?? ? ? ? ? ? ? ?
:::
?
:::
?
:::
?
::
?
:::
?
::
?
:::
?
::
?
:::
?
::
?
:
??????????
????? ?
Baseline : Investment in fixed assets
investment in the three years ,?????
yuan , floor is not high , ? the former
border city , road , and communication
conditions have not been completed ,
will not change .
Due to unrestricted rule application, mainly
guided by the language model, the underlined
clauses containing negation on the source side
have been deleted. Moreover, the polarity of the
last clause, positive in the source, is changed into
negative in the target translation, most probably
because a negative cue is moved from somewhere
else in the sentence.
In order to solve these two problems, we exploit
the syntactic feature of the Chinese language of
grouping clauses into a single sentence. We fol-
low the intuition of Yang and Xue (2012) in using
3The entailment-based MT metric also outputs an
RTE+MT score, where the RTE score is interpolated with tra-
ditional MT metrics (e.g. BLEU, NIST, TER, METEOR).
602
Test set Original Positive Negative Orig. ? Neg. Pos. ? Neg.
Test 1 32.92 32.95 29.64 - 3.28 - 3.31
Test 2 25.88 26.21 24.31 - 1.57 - 1.9
Test 3 19.00 19.78 16.11 - 2.89 - 3.67
Test 4 28.64 29.71 27.14 - 1.5 - 2.57
Average 26.61 27.16 24.3 -2.31 - 2.96
Table 1: BLEU scores for the baseline system. The difference in BLEU scores between the positive, the
original and the negative conditions is also reported.
Test set BLEU RTE RTE+MT
Test 1 29.64 0.22 0.837
Test 2 24.31 0.307 0.732
Test 3 16.11 -0.603 -0.095
Test 4 27.14 -0.25 0.33
Average 24.3 -0.08 0.451
Table 2: BLEU, RTE and RTE+MT scores for the baseline system as tested on the sub-set only containing
negative sentences.
commas to guide the segmentation of a sentence
into constituent sub-clauses. Moreover, we also
use other syntactic clues to segment the test sen-
tences, including quotes in direct quotation, to re-
duce the size of the test sentences.
The constituent sub-clauses are then translated sin-
gularly and ?stitched? back together into the origi-
nal sentence for evaluation.
6.2 Re-ranking results
Table 3 and 4 shows the performance of the system
when n-best list re-ranking is performed. Table 3
shows the results for the oracle translation, while
Table 4 the results for actual n-best list re-ranking.
Two conditions are here compared: a short con-
dition where test sentences are chunked into con-
stituent sub-clauses prior to translation and a orig-
inal (orig.) condition where no chunking is per-
formed.
Results shows considerable improvements over
the baseline when re-ranking is performed ?
an average BLEU score improvement of 1.75
points. As hypothesised, we get further improve-
ment when Chinese source sentences are translated
through their constituent sub-clauses ? an aver-
age BLEU score improvement of 3.07 points. A
similar improvement is shown in Table 5 where
the original test sets comprising both positive and
negative sentences are considered. This proves the
validity of n-best list re-ranking using syntactic de-
pendencies as a method to improve the quality of
the translation of negative data. The following ex-
ample shows the improvement in detail:
(2) Source : ?? ???? ? ?? ? ?
??? ??? ?? ? ?? ?? ,??
?? ?? ?? ? ?? ?? ? ?? ??
? ?? ?? ? ?? ? ??
Ex. reference : When asked about in-
flation, he said : ?The overall inflation
rate in the Euro area still exhibits a down
trend. At present, there is no sign to show
economic development in the medium term
will create risks of price instability?.
Baseline : on the inflation he said
the euro dropped overall medium
term economic development will in
no signs of inflation risks .
Oracle : on inflation , said
the euro dropped overall
there is no signs of economic development
in the medium term prices will not risks .
Source-only re-ranking : on inflation ,
said the euro dropped overall there is no
signs of economic development in the
medium term will price risks .
In (2) the baseline translation shows the problems
mentioned earlier, where movement leaves nega-
tion with the wrong scope, changing the overall
meaning of the sentence. Decomposing sentences
into constituent clauses and then re-ranking the
translations permits negation to retain its correct
scope so that the meaning is the same as the refer-
ence sentence.
7 Conclusion
We have presented an approach to translating neg-
ative sentences that is based on the semantics of
negation and applying it to n-best list re-ranking.
603
BLEU RTE RTE+MT
1
Baseline 29.64 0.22 0.837
Orig. 33.73 (+4.09) 0.64 (+0.42) 1.396 (+0.559)
Short 35.39 (+5.75) 0.74 (+ 0.52) 1.508 (+ 0.671)
2
Baseline 24.31 0.307 0.732
Orig. 27.43 (+3.12) 0.457 (+0.15) 1.12 (+0.388)
Short 27.29 (+3.18) 0.6 (+ 0.293) 1.175 (+ 0.443)
3
Baseline 16.11 -0.603 -0.095
Orig. 17.97 (+1.86) 0.356 (+ 0.959) 0.958 (+ 1.053)
Short 18.19 (+2.08) 0.243 (+ 0.84) 0.78 (+ 0.875)
4
Baseline 27.14 -0.25 0.33
Orig. 31.97 (+ 4.83) 0.42 (+ 0.67) 1.024 (+ 0.694)
Short 32.50 (+ 5.36) 0.57 (+ 0.82) 1.36 (+ 1.03)
Avg. Baseline 24.3 - 0.08 0.45Orig. 27.78 (+ 3.48) 0.47 (+ 0.55) 1.12 (+ 0.67)
Short 29.09 (+ 4.79) 0.52 (+ 0.60) 1.23 (+ 0.78)
Table 3: BLEU, RTE and RTE+MT scores for the oracle translation. The test sets evaluated are marked
from 1 to 4. Improvement over the baseline is reported.
BLEU RTE RTE+MT
1
Baseline 29.64 0.22 0.837
Orig. 31.96 (+ 2.32) 0.62 (+ 0.4) 1.382 (+ 0.545)
Short 34.20 (+ 4.56) 0.68 (+0.46) 1.452 (+ 0.615)
2
Baseline 24.31 0.307 0.732
Orig. 26.65 (+2.34) 0.48 (+ 0.173) 1.159 (+ 0.427)
Short 26.94 (+ 2.63) 0.49 (+0.183) 1.172 (+ 0.44)
3
Baseline 16.11 -0.603 -0.095
Orig. 17.20 (+ 1.09) 0.35 (+ 0.953) 0.935 (+ 1.03)
Short 17.41 (+ 1.3) 0.226 (+0.829) 0.87 (+ 0.965)
4
Baseline 27.14 -0.25 0.33
Orig. 28.42 (+ 1.28) 0.302 (+ 0.552) 1.01 (+ 0.68)
Short 30.96 (+ 3.82) 0.55 (+ 0.8) 1.36 (+ 1.03)
Avg. Baseline 24.3 -0.08 0.45Orig. 26.05 (+ 1.75) 0.438 (+ 0.518) 1.12 (+ 0.669)
Short 27.37 (+ 3.07) 0.51 (+ 0.59) 1.21 (+ 0.759)
Table 4: BLEU, RTE and RTE+MT scores for the sentences re-ranked using source side information
only. Improvement over the baseline is reported.
Dependency parsing and lexical translations are
here considered as easily applicable methods to
extract and translate negation related information
across different language pairs. Improvements
across different automatic evaluationmetrics show
that the above method is useful when translating
negative data. In particular, the entailment-based
RTE metric is here used as an alternative to the
BLEU score given the semantic and syntactic fea-
tures assessed, polarity included. Given the pos-
itive results, one can conclude that the problem
is neither one of data sparsity nor syntactic mis-
match.
We have also demonstrated that when dealing
with sentences containing multiple sub-clauses,
translating the constituent sub-clauses separately
and then stitching them back together before eval-
uation avoids the loss or excessive movement of
negation during decoding. This was evident in the
case of Chinese and HPBMs but there is no reason
why this does not hold also for other languages.
8 Future works
Given the validity of the present approach, future
works should be focused in extending it to differ-
ent language pairs. Also, it would be useful to re-
search more in detail into language typology and
try to devise a method which is language indepen-
dent.
Although leading to an overall improvement, n-
best list re-ranking does not always guarantee a
perfect translation. It is therefore useful in the fu-
ture to investigate ways of always ensuring that the
n-best list contains a good translation of negation
by, for instance, enriching the hypotheses list with
paraphrases. Post-editing rules can also be consid-
ered to further correct the final output.
Finally, although we can show considerable im-
provement with respect to both n-gram overlap
604
BLEU RTE RTE+MT
1
Baseline 32.92 -0.49 -0.073
Orig. 33.54 (+ 0.62) -0.38 (+ 0.11) 0.046 (+ 0.119)
Short 34.02 (+ 1.1) -0.33 (+ 0.16) 0.057 (+ 0.13)
2
Baseline 25.88 -2.173 -1.726
Orig. 26.3 (+ 0.42) -1.851 (+ 0.322) -1.376 (+ 0.35)
Short 26.42 (+ 0.54) -1.80 (+ 0.373) -1.339 (+ 0.387)
3
Baseline 19.00 -0.897 -0.644
Orig. 19.20 (+ 0.20) -0.731 (+ 0.166) -0.474 (+ 0.17)
Short 19.23 (+ 0.23) -0.743 (+ 0.154) -0.488 (+ 0.156)
4
Baseline 28.64 -3.43 -3.16
Orig. 29.56 (+ 0.92) -3.01 (+ 0.42) -2.72 (+ 0.44)
Short 29.95 (+ 1.31) -2.94 (+ 0.49) -2.67 (+ 0.49)
Avg. Baseline 26.61 -1.747 -1.4Orig. 27.15(+ 0.54) -1.488 (+ 0.259) -1.131 (+ 0.269)
Short 27.41(+ 0.8) -1.453 (+ 0.294) -1.11 (+ 0.29)
Table 5: BLEU, RTE and RTE+MT scores for the the original test set, containing both positive and neg-
ative sentences re-ranked using source side information only. Improvement over the baseline is reported.
with the reference translation (BLEU score) and
overall semantic similarity, it remains to be de-
termined the extent to which the machine output
captures elements of negation present in the ref-
erence translation and on which system improve-
ment depends. A more targeted metric is needed,
that can effectively determine the extent to which
cue, event and scope are captured in hypothesis
translation as compared to the reference gold stan-
dard. That is the subject of current and future
work (Fancellu (2013)), which should implement
the new customized metric to include measures of
precision, recall and a F1 measure.
References
Apidianaki, M., Wisniewski, G., Sokolov, A.,
Max, A., and Yvon, F. (2012). Wsd for n-best
reranking and local language modeling in smt.
In Proceedings of the Sixth Workshop on Syntax,
Semantics and Structure in Statistical Transla-
tion, pages 1?9. Association for Computational
Linguistics.
Blanco, E. and Moldovan, D. I. (2011). Some is-
sues on detecting negation from text. In FLAIRS
Conference.
Chowdhury, M. and Mahbub, F. (2012). Fbk: Ex-
ploiting phrasal and contextual clues for nega-
tion scope detection. In Proceedings of the
First Joint Conference on Lexical and Computa-
tional Semantics-Volume 1: Proceedings of the
main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 340?
346. Association for Computational Linguistics.
Collins, M., Koehn, P., and Ku?erov?, I. (2005).
Clause restructuring for statistical machine
translation. In Proceedings of the 43rd annual
meeting on association for computational lin-
guistics, pages 531?540. Association for Com-
putational Linguistics.
Copestake, A., Flickinger, D., Pollard, C., and Sag,
I. A. (2005). Minimal recursion semantics: An
introduction. Research on Language and Com-
putation, 3(2-3):281?332.
Fancellu, F. (2013). Improving the performance
of chinese-to-english hierarchical phrase based
models (hpbm) on negative data using n-best list
re-ranking. Master?s thesis, School of Informat-
ics - University of Edinburgh.
Hasan, S., Zens, R., and Ney, H. (2007). Are very
large n-best lists useful for smt? In Human
Language Technologies 2007: The Conference
of the North American Chapter of the Associ-
ation for Computational Linguistics; Compan-
ion Volume, Short Papers, pages 57?60. Asso-
ciation for Computational Linguistics.
Lapponi, E., Velldal, E., ?vrelid, L., and Read, J.
(2012). Uio 2: sequence-labeling negation us-
ing dependency features. In Proceedings of the
First Joint Conference on Lexical and Computa-
tional Semantics-Volume 1: Proceedings of the
main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 319?
327. Association for Computational Linguistics.
Li, J.-J., Kim, J., Kim, D.-I., and Lee, J.-H. (2009).
Chinese syntactic reordering for adequate gen-
eration of korean verbal phrases in chinese-to-
605
korean smt. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages
190?196. Association for Computational Lin-
guistics.
Morante, R., Schrauwen, S., and Daelemans, W.
(2011). Annotation of negation cues and their
scope: Guidelines v1. Technical report, 0. Tech-
nical report, University of Antwerp. CLIPS:
Computational Linguistics & Psycholinguistics
technical report series.
Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A.,
Yamada, K., Fraser, A., Kumar, S., Shen, L.,
Smith, D., Eng, K., et al. (2004). A smorgasbord
of features for statistical machine translation. In
HLT-NAACL, pages 161?168.
Pad?, S., Galley, M., Jurafsky, D., and Manning,
C. D. (2009). Textual entailment features for
machine translation evaluation. In Proceedings
of the Fourth Workshop on Statistical Machine
Translation, pages 37?41. Association for Com-
putational Linguistics.
Read, J., Velldal, E., ?vrelid, L., and Oepen, S.
(2012). Uio 1: Constituent-based discrimina-
tive ranking for negation resolution. InProceed-
ings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Pro-
ceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation,
pages 310?318. Association for Computational
Linguistics.
Rosa, R. (2013). Automatic post-editing of phrase-
basedmachine translation outputs. Master?s the-
sis, Institute of Formal and Applied Linguistics,
Charles University, Prague.
Specia, L., Sankaran, B., and Nunes, M. d. G. V.
(2008). N-best reranking for the efficient inte-
gration of word sense disambiguation and statis-
tical machine translation. InComputational Lin-
guistics and Intelligent Text Processing, pages
399?410. Springer.
Tseng, H., Chang, P., Andrew, G., Jurafsky, D.,
and Manning, C. (2005). A conditional random
field word segmenter for sighan bakeoff 2005.
In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, volume
171. Jeju Island, Korea.
Wetzel, D. and Bond, F. (2012). Enriching parallel
corpora for statistical machine translation with
semantic negation rephrasing. In Proceedings
of the Sixth Workshop on Syntax, Semantics and
Structure in Statistical Translation, pages 20?
29. Association for Computational Linguistics.
Yang, Y. and Xue, N. (2012). Chinese comma
disambiguation for discourse analysis. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Long
Papers-Volume 1, pages 786?794. Association
for Computational Linguistics.
606
Book Reviews
Discourse Processing
Manfred Stede
University of Potsdam
Morgan & Claypool (Synthesis Lectures on Human Language Technologies, edited
by Graeme Hirst, volume 15), 2011, ix+155 pp; paperbound, ISBN 978-1-60845-734-2,
$40.00; ebook, ISBN 978-1-60845-735-9, $30.00 or by subscription
Reviewed by
Bonnie Webber
University of Edinburgh
Discourse is coming in from the cold. After years of being ignored by researchers in
other areas of computational linguistics and language technology, many of these same
researchers are beginning to think that their own work could benefit from treating text
as more than just a bag of sentences. That is, they are beginning to think that discourse
offers some low-hanging fruit?achievable improvements in system performance that
exploit either aspects of text structure or the context that text establishes and uses for
efficient referring and/or predicational expressions.
? 2012 Association for Computational Linguistics
This new monograph on Discourse Processing by Manfred Stede both reflects this
new zeitgeist and provides an introduction to discourse for researchers in computational
linguistics or language technology with little or no background in the area. This clear
and timely monograph consists of a brief introduction to discourse, a meaty chapter on
each of the three aspects of discourse processing that hold most promise for language
technology, and a brief conclusion on where discourse research might go in the future.
I will go through the three major chapters, and then make some general remarks.
Chapter 2
Chapter 2 addresses two distinct types of large-scale discourse structure: structure that
follows from a text belonging to a particular genre, and structure that follows from
the topic (or topic mix) of a text. The genre of a text affects features such as style
and register. What is relevant here is structure that genre may confer on a text. Stede
suggests that some, but not all, texts inherit large-scale structure from their genre,
calling some unstructured, some structured, and some semi-structured. As a reader, I
did not find this distinction useful, because all text that belongs to a genre seems to
get some large-scale structure from it. On the other hand, all or part of this structure
might simply not be manifest in the kind of lexico-syntactic features that automated
systems regularly rely on for text segmentation. As a case in point, although Stede offers
the text Suffering (used as a running example throughout the book) as an example of
unstructured text, like other instances of Comments in the Talk of the Town section of
the New Yorker magazine, its large-scale structure comprises a ?hook? aimed at getting
the reader?s attention, followed by a short essay that concludes with a serious point.
Although ways of attracting a reader?s attention may not have specific lexico-syntactic
features, it might still be possible to recognize the transition between ?hook? and essay,
and essay structure itself is what ETS?s eRater system (Burstein and Chodorow 2010)
aims to recognize and evaluate.
Computational Linguistics Volume 38, Number 4
This first half of Chapter 2 focuses on the genre-based structure of scientific texts
and of film reviews. Here researchers have already shown that language technologies
such as information extraction and sentiment analysis benefit from taking such structure
into account, so this is entirely appropriate for the book?s target audience. More on
genre-based functional structure and its use in producing structured biomedical
abstracts can be found in the recent survey of research on discourse structure and
language technology by Webber, Egg, and Kordoni (2012).
The second half of Chapter 2 discusses large-scale discourse structure associated
with patterns of topics. Such structure is often found in expository writing such as
encyclopedia articles and travel pieces. Here, changing patterns of content words corre-
late well with changes in topic, rendering them useful for the many approaches to text
segmentation that are well-described in this half of the chapter. Because the discussion
here of probabilistic models for topic segmentation is rather short, the reader whowants
to know more should consult the excellent survey of topic segmentation methods by
Purver (2011).
Chapter 3
Chapter 3, entitled Coreference Resolution, addresses more than this, dealing with the
resolution of other expressions whose reduction is licensed by the discourse context,
such as bridging reference and ?other? reference, which Halliday and Hasan (1976)
call comparative reference because it occurs with comparative forms such as ?larger
fish? and ?a more impressive poodle,? as well as with ?other,? ?another,? and ?such.?
Stede justifies inclusion of this chapter for two reasons?the close connection between
coreference resolution and topic segmentation and the benefits to text analysis provided
by having its pronouns resolved. But another reason must be the link mentioned earlier
between text and context: Discourse creates the context in which context-reduced
expressions make sense, so it falls naturally within the tasks of discourse processing
to resolve them, either through modeling context explicitly or through the use of
proxies.
The chapter starts with an overview of coreference and anaphora that covers both
their forms and their functions. This is followed by an important section on corpus
annotation (Section 3.2), included because (as Stede notes) what has been annotated and
why it has been annotated strongly determines what expressions are resolved and how.
This section identifies many of the problems in coreference annotation that have been
raised in the literature, but recognizes that research has to make use of the resources
that exist and not just the resources it wants. Several of these are indicated at the end
of the section, reminding one that it would have been useful to have some pointers in
Chapter 2 to corpora available for genre-based segmentation (such as Liakata?s ART
corpus)1 or for topic-based segmentation.
Stede then links the current chapter to the previous one through a discussion of
entity-based coherence (Section 3.3) and then discusses how to identify when a pronoun
or definite noun phrase should be treated as anaphoric (Section 3.4) as groundwork
for discussion of anaphora resolution (Sections 3.5?3.7). Missing from the discussion of
detecting non-anaphoric (pleonastic) pronouns is mention of Bergsma?s recent system
NADA for doing this (Bergsma and Yarowsky 2011).2
1 Downloadable from http://www.aber.ac.uk/en/ns/research/cb/projects/art/art-corpus/
2 Downloadable from http://code.google.com/p/nada-nonref-pronoun-detector/
918
Book Reviews
The discussion of anaphora resolution covers rule-based approaches to resolving
nominal anaphora (Section 3.5) and then supervised machine learning methods for
anaphora resolution (Section 3.6). The latter follows the structure (albeit not the con-
tent) of Ng?s survey (2010), in discussing mention-pair models, and then entity-mention
models. Whereas Ng then discusses ranking models, including his cluster ranker (Rahman
and Ng 2009), which is conceptually similar to the Lappin and Leass (1994) approach
described in Section 3.5, Stede discusses a range of more recent models, most of which
are subsequent to Ng?s survey.
Section 3.8 surveys methods evaluating coreference resolution and some of the
known problems in doing so. A good complement to this is Byron?s too-little-known
discussion of problems in the consistent reporting of such results (Byron 2001). Chap-
ter 3 concludes with a section on Recent Trends, which would also have been useful in
Chapter 2.
Chapter 4
The fourth and longest chapter deals with semantic or pragmatically oriented coherence
relations that hold between adjacent text spans or discourse units. Whereas the previous
two chapters were essentially theory-neutral, the presentation in Chapter 4 largely
reflects the perspective of Rhetorical Structure Theory (Mann and Thompson 1988). RST
takes a text to be a sequence of elementary discourse units that comprise the leaves of
a tree structure of coherence relations between recursively defined discourse units. RST
also assumes that one of the arguments to a coherence relation may be more important
to the speaker?s purpose than the other, calling the former the nucleus and the latter,
the satellite.
This RST framework dictates the structure of the chapter: Following an introductory
section that explains andmotivates coherence relations, each subsequent section consid-
ers the next task in an RST analysis?segmenting a text into elementary discourse units
(Section 4.2), recognizing which (adjacent) units stand in a coherence relation and what
(single) relation holds between them (Section 4.3), and finally, inducing the overall tree
structure of coherence relations that hold between recursively defined discourse units
(Section 4.4). All these tasks are well described, both from a theoretical perspective and
in terms of automated procedures for carrying them out. Coverage of relevant work is
very high.
Where the reader may get confused, however, is that a good proportion of the
more recent work on identifying coherence relations does not fall within the framework
of RST, and thus doesn?t adhere to several of its assumptions?in particular, that a
text is divisible into a covering sequence of elementary discourse units, that only one
relation can hold between discourse units, that the arguments to a coherence relation
must be adjacent, that one argument to a coherence relation may intrinsically convey
information that is more important to the speaker?s purpose than the other, and that
coherence relations impose an overall tree structure on a text in terms of recursively
defined discourse units.
Although Chapter 4 discusses the Penn Discourse TreeBank (Prasad et al 2008) and
its ?somewhat modest annotations? (page 126), the discussion is framed in terms of
RST tasks, whereas the assumptions underlying the Penn Discourse TreeBank reflect its
concerns with a quite different set of tasks involved in recognizing coherence relations.
The first task requires finding evidence for a coherence relation (in the form of a
discourse connective such as a coordinating or subordinating conjunction or a discourse
adverbial, or in the form of sentence adjacency) and then determining (1) if the evidence
919
Computational Linguistics Volume 38, Number 4
does indeed signal a coherence relation, given that evidence is often ambiguous; (2) if
it does, what constitutes its arguments; and (3) what is its sense. Although Chapter 4
covers some of this work (Dinesh et al 2005; Wellner and Pustejovsky 2007; Elwell
and Baldridge 2008; Pitler and Nenkova 2009; Prasad, Joshi, and Webber 2010), its
appearance within the context of a discussion of RST-tasks may lead to some confusion.
Chapter 4 concludes with a brief discussion of some important open issues regard-
ing coherence relations, including problems with associating a large text span with
a single recursive structure of coherence relations and problems with inter-annotator
agreement.
Summary
For its intended audience, this monograph will serve as a compact, readable intro-
duction to the subject of discourse processing. The relevant phenomena are presented
clearly, as are many of the computational methods for dealing with them. What readers
won?t get is criteria for choosing among the methods or an understanding of what each
method is good for. This problem may reflect the absence of comparable performance
results and useful error analyses in the original publications, however.
Also missing from the monograph is discussion of applications of discourse pro-
cessing, and pointers to more of the resources available to researchers interested in
discourse structure. This is where the additional resources I have mentioned may prove
complementary.
Finally, a plea to the series editor: Monographs such as this one really need an index.
Some monographs in the series have one, whereas others (like this one) don?t. Because
the series appears in both electronic and physical format, one could excuse the former
not having an explicit index, since in most cases, one can get away with the basic search
facility in the Adobe Reader. Nothing similar is available for the nicely sized physical
monographs. Their authors should be strongly encouraged to provide them.
References
Bergsma, Shane and David Yarowsky. 2011.
Nada: A robust system for non-referential
pronoun detection. In Proceedings of
DAARC, 12 pages, Faro.
Burstein, Jill and Martin Chodorow. 2010.
Progress and new directions in technology
for automated essay evaluation.
In R. Kaplan, editor, The Oxford Handbook
of Applied Linguistics. Oxford University
Press, 2nd edition, pages 487?497.
Byron, Donna. 2001. The uncommon
denominator: A proposal for consistent
reporting of pronoun resolution results.
Computational Linguistics, 27(4):569?577.
Dinesh, Nikhil, Alan Lee, Eleni Miltsakaki,
Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2005. Attribution
and the (non-)alignment of syntactic
and discourse arguments of connectives.
In ACL Workshop on Frontiers in Corpus
Annotation, pages 29?36, Ann Arbor, MI.
Elwell, Robert and Jason Baldridge.
2008. Discourse connective argument
identification with connective specific
rankers. In Proceedings of the IEEE
Conference on Semantic Computing,
8 pages, Santa Clara, CA.
Halliday, Michael and Ruqaiya Hasan.
1976. Cohesion in English. Longman.
Lappin, Shalom and Herbert Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20(4):535?561.
Mann, William and Sandra Thompson.
1988. Rhetorical structure theory: Toward
a functional theory of text organization.
Text, 8(3):243?281.
Ng, Vincent. 2010. Supervised noun
phrase coreference research: The
first 15 years. In Proceedings of the
48th Annual Meeting of the Association
for Computational Linguistics,
pages 1396?1411, Uppsala.
Pitler, Emily and Ani Nenkova. 2009. Using
syntax to disambiguate explicit discourse
connectives in text. In ACL-IJCNLP ?09:
Proceedings of the 47th Meeting of the
920
Book Reviews
Association for Computational Linguistics
and the 4th International Joint Conference on
Natural Language Processing, pages 13?16,
Singapore.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo, Aravind
Joshi, and Bonnie Webber. 2008. The Penn
Discourse TreeBank 2.0. In Proceedings,
6th International Conference on Language
Resources and Evaluation, pages 2961?2968,
Marrakech.
Prasad, Rashmi, Aravind Joshi, and Bonnie
Webber. 2010. Exploiting scope for shallow
discourse parsing. In Proceedings of the
7th International Conference on Language
Resources and Evaluation (LREC 2010),
pages 2076?2083, Malta.
Purver, Matthew. 2011. Topic segmentation.
In Gokhan Tur and Renato de Mori,
editors, Spoken Language Understanding:
Systems for Extracting Semantic
Information from Speech, Chapter 11.
Wiley, Hoboken, NJ.
Rahman, Altaf and Vincent Ng. 2009.
Supervised models for coreference
resolution. In Proceedings of the 2009
Conference on Empirical Methods in
Natural Language Processing,
pages 968?977, Singapore.
Webber, Bonnie, Markus Egg, and
Valia Kordoni. 2012. Discourse
structure and language technology.
Natural Language Engineering,
doi:10.1017/S1351324911000337.
Wellner, Ben and James Pustejovsky. 2007.
Automatically identifying the arguments
to discourse connectives. In Proceedings of
the 2007 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 92?101, Prague.
This book review was edited by Pierre Isabelle.
Bonnie Webber is a Professor of Informatics at Edinburgh University. She received both her MSc
and PhD from Harvard University. She is a Fellow of the Royal Society of Edinburgh and of
the American Association for Artificial Intelligence. Both her early and her recent research have
focused on computational approaches to discourse and question answering. In between, she
has carried out research on animation from instructions, medical decision support systems, and
biomedical text processing. Webber?s e-mail address is bonnie.webber@ed.ac.uk.
921

Tutorial Abstracts of ACL 2010, page 3,
Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational Linguistics
Discourse Structure: Theory, Practice and Use
Bonnie Webber,? Markus Egg,? Valia Kordoni?
? University of Edinburgh ? Humboldt University ? Saarland University
bonnie@inf.ed.ac.uk markus.egg@anglistik.hu-berlin.de kordoni@dfki.de
1 Introduction
This tutorial aims to provide attendees with a clear
notion of how discourse structure is relevant for
language technology (LT), what is needed for ex-
ploiting discourse structure, what methods and re-
sources are available to support its use, and what
more could be done in the future.
2 Content Overview
This tutorial consists of four parts. Part I starts
with a brief introduction to different bases for dis-
course structuring, properties of discourse struc-
ture that are relevant to LT, and accessible evi-
dence for discourse structure.
For discourse structure to be useful for lan-
guage technologies, one must be able to automati-
cally recognize or generate with it. Hence, Part II
surveys computational approaches to recognizing
and generating discourse structure, both manually-
authored approaches and ones developed through
Machine Learning.
Part III of the tutorial describes applications
of discourse structure recognition and generation
in LT, as well as discourse-related resources be-
ing made available in English, German, Turkish,
Hindi, Czech, Arabic and Chinese. Part IV con-
cludes with a list of future possibilities.
3 Tutorial Outline
1. PART I ? General Overview
(a) Bases for structure in monologic, dia-
logic and multiparty discourse
(b) Aspects of discourse structure relevant
to Language Technology
(c) Evidence for discource structure
2. PART II ? Computational Recognition and
Generation of discourse structure
(a) Discourse chunking and parsing
(b) Recognizing arguments and sense of
discourse connectives
(c) Recognizing and generating entity-
based discourse structure
(d) Dialogue parsing
3. PART III ? Applications and Resources
(a) Applications to Language Technology
(b) Discourse structure resources (mono-
lingual and multilingual)
4. PART IV ? Future Developments
4 References
? Regina Barzilay and Lillian Lee (2004). Catching the Drift:
Probabilistic Content Models, with Applications to Genera-
tion and Summarization. Proc. 2nd Human Language Tech-
nology Conference and Annual Meeting of the North Ameri-
can Chapter, Association for Computational Linguistics, pp.
113-120.
? Regina Barzilay and Mirella Lapata (2008). Modeling Lo-
cal Coherence: An Entity-based Approach. Computational
Linguistics 34(1), pp. 1-34.
? Daniel Marcu (2000). The theory and practice of discourse
parsing and summarization. Cambridge: MIT Press.
? Umangi Oza, Rashmi Prasad, Sudheer Kolachina, Dipti
Misra Sharma and Aravind Joshi (2009). The Hindi Dis-
course Relation Bank. Proc. Third Linguistic Annotation
Workshop (LAW III). Singapore.
? Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki
et al (2008). The Penn Discourse TreeBank 2.0. Proc. 6th
Int?l Conference on Language Resources and Evaluation.
? Manfred Stede (2008). RST revisited: Disentangling nu-
clearity. In Cathrine Fabricius-Hansen and Wiebke Ramm
(eds.), Subordination versus Coordination in Sentence and
Text. Amsterdam: John Benjamins.
? Ben Wellner (2008). Sequence Models and Ranking Meth-
ods for Discourse Parsing. Brandeis University.
? Deniz Zeyrek, ?Umit Deniz Turan, Cem Bozsahin, Ruket
C?akici et al (2009). Annotating Subordinators in the Turkish
Discourse Bank. Proc. Third Linguistic Annotation Work-
shop (LAW III). Singapore.
3
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1660?1668,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Evaluating a City Exploration Dialogue System Combining
Question-Answering and Pedestrian Navigation
Srinivasan Janarthanam1, Oliver Lemon1, Phil Bartie2, Tiphaine Dalmas2,
Anna Dickinson2, Xingkun Liu1, William Mackaness2, and Bonnie Webber2
1 The Interaction Lab, Heriot-Watt University
2 Edinburgh University
sc445@hw.ac.uk
Abstract
We present a city navigation and tourist
information mobile dialogue app with in-
tegrated question-answering (QA) and ge-
ographic information system (GIS) mod-
ules that helps pedestrian users to nav-
igate in and learn about urban environ-
ments. In contrast to existing mobile apps
which treat these problems independently,
our Android app addresses the prob-
lem of navigation and touristic question-
answering in an integrated fashion using
a shared dialogue context. We evaluated
our system in comparison with Samsung
S-Voice (which interfaces to Google nav-
igation and Google search) with 17 users
and found that users judged our system to
be significantly more interesting to inter-
act with and learn from. They also rated
our system above Google search (with the
Samsung S-Voice interface) for tourist in-
formation tasks.
1 Introduction
We present a mobile dialogue system (an Android
app) called Spacebook that addresses the problem
of pedestrian navigation and tourist information in
urban environments. There has been little prior
work that addresses these two problems - naviga-
tion and tourist information provision - in an inte-
grated way. By navigation, we refer to the prob-
lem of finding appropriate destinations to go to
and the task of wayfinding to reach them and by
tourist information provision we refer to the prob-
lem of meeting the informational needs of a user
about entities such as museums, statues and fa-
mous personalities. A dialogue system such as this
could serve as a personal tour guide to pedestrian
tourists as they walk around unknown cities. With
the proliferation of smartphones, there has been a
number of mobile apps developed to address these
problems. However these apps have the following
problems: first, they demand the user?s visual at-
tention because they predominantly present infor-
mation on a mobile screen. This can be dangerous
in urban environments, as well as being distract-
ing. Second, these apps address the problems of
navigation and tourist information independently
and therefore do not have a shared interaction con-
text. This means that users cannot switch between
information and navigation tasks in a natural and
fluid manner.
User1: Take me to the National Museum.
System2: The National Museum is about 300m away..
System3: At the KFC, turn left on to South Bridge
System4 : Near you is the statue of David Hume.
User2: Who is David Hume.
System5: David Hume was a Scottish philosopher....
User3: Tell me more about David Hume.
System6: He was one of the most important figures in..
System7: You should be able to see the museum ...
User4: Tell me more about the museum.
System8: The National Museum of Scotland is a....
Table 1: An example interaction with the evalu-
ated system
In contrast to many existing mobile apps,
Spacebook has a speech-only interface and ad-
dresses both problems in an integrated way. We
conjecture that with a speech-only interface, users
can immerse themselves in exploring the city,
and that because of the shared context they can
switch between navigation and tourist information
tasks more easily. Using the navigational context,
Spacebook pushes point-of-interest information
which can then initiate tourist information tasks
using the QA module. Table 1 presents an example
interaction with our system showing the integrated
use of navigation and question-answering capabil-
1660
ities. Utterances System4-8 show the system?s ca-
pability to push information about nearby points-
of-interest (PoI) during a navigation task and an-
swer followup questions using the QA system (in
utterances User2 and User3). The final 3 utter-
ances show a natural switch between navigation to
an entity and QA about that entity.
We investigate whether our system using a com-
bination of geographical information system (GIS)
and natural language processing (NLP) technolo-
gies would be a better companion to pedestrian
city explorers than the current state-of-the-art mo-
bile apps. We hypothesize that, (1) users will find
our speech-only interface to navigation efficient as
it allows them to navigate without having to re-
peatedly look at a map and (2), that users will
find a dialogue interface which integrates touris-
tic question-answering and navigation within a
shared context to be useful for finding information
about entities in the urban environment. We first
present some related work in section 2. We de-
scribe the architecture of the system in section 3.
We then present our experimental design, results
and analysis in sections 5, 6 and 7.
2 Related work
Mobile apps such as Siri, Google Maps Naviga-
tion, Sygic, etc. address the problem of naviga-
tion while apps like Triposo, Guidepal, Wikihood,
etc. address the problem of tourist information by
presenting the user with descriptive information
about various points of interest (PoI) in the city.
While some exploratory apps present snippets of
information about a precompiled list of PoIs, other
apps dynamically generate a list of PoIs arranged
based on their proximity to the users. Users can
also obtain specific information about PoIs using
Search apps. Also, since these navigation and ex-
ploratory/search apps do not address both prob-
lems in an integrated way, users need to switch
between them and therefore lose interaction con-
text.
While most apps address these two problems
independently, some like Google Now, Google
Field Trip, etc, mix navigation with exploration.
But such apps present information primarily vi-
sually on the screen for the user to read. Some
of these are available for download at the Google
Play Android app store1. Several dialogue and
natural language systems have addressed the issue
1https://play.google.com/store
of pedestrian navigation (Malaka and Zipf, 2000;
Raubal and Winter, 2002; Dale et al, 2003; Bar-
tie and Mackaness, 2006; Shroder et al, 2011;
Dethlefs and Cuaya?huitl, 2011). There has also
been recent interest in shared tasks for generat-
ing navigation instructions in indoor and urban en-
vironments (Byron et al, 2007; Janarthanam and
Lemon, 2011). Some dialogue systems deal with
presenting information concerning points of inter-
est (Ko et al, 2005; Kashioka et al, 2011) and in-
teractive question answering (Webb and Webber,
2009).
In contrast, Spacebook has the objective of
keeping the user?s cognitive load low and prevent-
ing users from being distracted (perhaps danger-
ously so) from walking in the city (Kray et al,
2003). Also, it allows users to interleave the two
sub-tasks seamlessly and can keep entities dis-
cussed in both tasks in shared context (as shown
in Table 1).
3 Architecture
The architecture of the Spacebook system is
shown in figure 1. Our architecture brings to-
gether Spoken Dialogue Systems (SDS), Geo-
graphic Information Systems (GIS) and Question-
Answering (QA) technologies (Janarthanam et al,
2012). Its essentially a spoken dialogue system
(SDS) consisting of an automatic speech recog-
niser (ASR), a semantic parser, an Interaction
Manager, an utterance generator and a text-to-
speech synthesizer (TTS). The GIS modules in
this architecture are the City Model, the Visibility
Engine, and the Pedestrian tracker. Users commu-
nicate with the system using a smartphone-based
client app (an Android app) that sends users? po-
sition, pace rate, and spoken utterances to the sys-
tem, and delivers synthesised system utterances to
the user.
Figure 1: System Architecture
1661
3.1 Dialogue interface
The dialogue interface consists of a speech recog-
nition module, an utterance parser, an interaction
manager, an utterance generator and a speech syn-
thesizer. The Nuance 9 speech recogniser with
a domain specific language model was used for
speech recognition. The recognised speech is cur-
rently parsed using a rule-based parser into dia-
logue acts and semantic content.
The Interaction Manager (IM) is the central
component of this architecture, which provides
the user with navigational instructions, pushes PoI
information and manages QA questions. It re-
ceives the user?s input in the form of a dialogue
act (DA), the user?s location (latitude and longi-
tude) and pace rate. Based on these inputs and the
dialogue context, it responds with system output
dialogue act, based on a dialogue policy. The IM
initiates the conversation with a calibration phase
where the user?s initial location and orientation are
obtained. The user can then initiate tasks that in-
terest him/her. These tasks include searching for
an entity (e.g. a museum or a restaurant), request-
ing navigation instructions to a destination, ask-
ing questions about the entities in the City Model,
and so on. When the user is mobile, the IM iden-
tifies points of interest2 on the route proximal to
the user. We call this ?PoI push?. The user is en-
couraged to ask for more information if he/she is
interested. The system also answers adhoc ques-
tions from the user (e.g. ?Who is David Hume??,
?What is the Old College??, etc) (see section 3.4).
Navigation instructions are given in-situ by ob-
serving user?s position continuously, in relation
to the next node (street junction) on the current
planned route, and they are given priority if in con-
flict with a PoI push at the same time. Navigation
instructions use landmarks near route nodes when-
ever possible (e.g. ?When you reach Clydesdale
Bank , keep walking forward?). The IM also in-
forms when users pass by recognisable landmarks,
just to reassure them that they are on track (e.g.
?You will pass by Tesco on the right?). In addition
to navigation instructions, the IM also answers
users? questions concerning the route, his/her lo-
cation, and location of and distance to the various
entities. Finally, the IM uses the city model?s Vis-
ibility Engine (VE) to determine whether the des-
tination is visible to the user (see section 3.3).
2Using high scoring ones when there are many, based on
tourist popularity ratings in the City Model.
The shared spatial and dialogue context em-
ploys a feature-based representation which is up-
dated every 1 second (for location), and after every
dialogue turn. Spatial context such as the user?s
coordinates, street names, PoIs and landmarks
proximal to the user, etc are used by PoI push-
ing and navigation. The dialogue context main-
tains the history of landmarks and PoIs pushed,
latest entities mentioned, etc to resolve anaphoric
references in navigation and QA requests, and to
deliver coherent dialogue responses. The IM re-
solves anaphoric references by keeping a record
of entities mentioned in the dialogue context. It
also engages in clarification sub-dialogues when
the speech recognition confidence scores are low.
The IM stores the name and type information for
each entity (such as landmark, building, etc) men-
tioned in navigation instructions and PoI pushes.
Subsequent references to these entities using ex-
pressions such as ?the museum?, ?the cafe? etc
are resolved by searching for the latest entity of
the given type. Pronouns are resolved to the last
mentioned entity.
The IM also switches between navigation, PoI
push, and QA tasks in an intelligent manner by
using the shared context to prioritise its utterances
from these different tasks. The utterance genera-
tor is a Natural Language Generation module that
translates the system DA into surface text which is
converted into speech using the Cereproc Text-to-
Speech Synthesizer using a Scottish female voice.
The only changes made were minor adjustments
to the pronunciation of certain place names.
3.2 Pedestrian tracker
Urban environments can be challenging with lim-
ited sky views, and hence limited line of sight
to satellites, in deep urban corridors. There is
therefore significant uncertainty about the user?s
true location reported by GNSS sensors on smart-
phones (Zandbergen and Barbeau, 2011). This
module improves on the reported user position
by combining smartphone sensor data (e.g. ac-
celerometer) with map matching techniques, to
determine the most likely location of the pedes-
trian (Bartie and Mackaness, 2012).
3.3 City Model
The City Model is a spatial database containing
information about thousands of entities in the city
of Edinburgh (Bartie and Mackaness, 2013). This
data has been collected from a variety of exist-
1662
ing resources such as Ordnance Survey, Open-
StreetMap, Google Places, and the Gazetteer for
Scotland. It includes the location, use class, name,
street address, and where relevant other properties
such as build date and tourist ratings. The model
also includes a pedestrian network (streets, pave-
ments, tracks, steps, open spaces) which is used
by an embedded route planner to calculate min-
imal cost routes, such as the shortest path. The
city model also consists of a Visibility Engine
that identifies the entities that are in the user?s
vista space (Montello, 1993). To do this it ac-
cesses a digital surface model, sourced from Li-
DAR, which is a 2.5D representation of the city
including buildings, vegetation, and land surface
elevation. The Visibility Engine uses this dataset
to offer a number of services, such as determining
the line of sight from the observer to nominated
points (e.g. which junctions are visible), and de-
termining which entities within the city model are
visible. Using these services, the IM determines if
the destination is visible or not.
3.4 Question-Answering server
The QA server currently answers a range of def-
inition and biographical questions such as, ?Tell
me more about the Scottish Parliament?, ?Who
was David Hume??, ?What is haggis??, and re-
quests to resume (eg. ?Tell me more?). QA
is also capable of recognizing out of scope re-
quests, that is, either navigation-related questions
that can be answered by computations from the
City Model and dealt with elsewhere in the sys-
tem (?How far away is the Scottish Parliament??,
?How do I get there??), or exploration queries
that cannot be handled yet (?When is the can-
non gun fired from the castle??). Question clas-
sification is entirely machine learning-based using
the SMO algorithm (Keerthi et al, 1999) trained
over 2013 annotated utterances. Once the question
has been typed, QA proceeds to focus detection
also using machine learning techniques (Mikhail-
sian et al, 2009). Detected foci include possi-
bly anaphoric expressions (?Who was he??, ?Tell
me more about the castle?). These expressions
are resolved against the dialogue history and ge-
ographical context. QA then proceeds to a tex-
tual search on texts from the Gazetteer of Scotland
(Gittings, 2012) and Wikipedia, and definitions
from WordNet glosses. The task is similar to TAC
KBP 2013 Entity Linking Track and named en-
tity disambiguation (Cucerzan, 2007). Candidate
answers are reranked using a trained confidence
score with the top candidate used as the final an-
swer. These are usually long, descriptive answers
and are provided as a flow of sentence chunks that
the user can interrupt (see table 2). The Interaction
Manager queries the QA model and pushes infor-
mation when a salient PoI is in the vicinity of the
user.
?Edinburgh?s most famous and historic thoroughfare,
which has formed the heart of the Old Town since
mediaeval times. The Royal Mile includes Castlehill,
the Lawnmarket, the Canongate and the Abbey Strand,
but, is officially known simply as the High Street.?
Table 2: QA output: query on ?Royal Mile?
3.5 Mobile client
The mobile client app, installed on an Android
smartphone (Samsung Galaxy S3), connects the
user to the dialogue system using a 3G data con-
nection. The client senses the user?s location us-
ing positioning technology using GNSS satellites
(GPS and GLONASS) which is sent to the dia-
logue system at the rate of one update every two
seconds. It also sends pace rate of the user from
the accelerometer sensor. In parallel, the client
also places a phone call using which the user com-
municates with the dialogue system.
4 Baseline system
The baseline system chosen for evaluation was
Samsung S-Voice, a state-of-the-art commercial
smartphone speech interface. S-Voice is a Sam-
sung Android mobile phone app that allows a user
to use the functionalities of device using a speech
interface. For example, the user can say ?Call
John? and it will dial John from the user?s con-
tacts. It launches the Google Navigation app when
users request directions and it activates Google
Search for open ended touristic information ques-
tions. The Navigation app is capable of providing
instructions in-situ using speech. We used the S-
Voice system for comparison because it provided
an integrated state-of-the-art interface to use both
a navigation app and also an information-seeking
app using the same speech interface. Users were
encouraged to use these apps using speech but
were allowed to use the GUI interface when us-
ing speech wasn?t working (e.g. misrecognition of
local names). Users obtained the same kind of in-
1663
formation (i.e. navigation directions, descriptions
about entities such as people, places, etc) from the
baseline system as they would from our system.
However, our system interacted with the user us-
ing the speech modality only.
5 Experimental design
Spacebook and the baseline were evaluated in the
summer of 2012. We evaluated both systems with
17 subjects in the streets of Edinburgh. There
were 11 young subjects (between 20 and 26 years,
mean=22 ? 2) and 6 older subjects (between 50
and 71 years, mean=61 ? 11). They were mostly
native English speakers (88%). 59% of the users
were regular smartphone users and their mean
overall time spent in the city was 76 months. The
test subjects had no previous experience with the
proposed system. They were recruited via email
adverts and mail shots. Subjects were given a task
sheet with 8 tasks in two legs (4 tasks per leg).
These tasks included both navigation and tourist
information tasks (see table 3). Subjects used our
system for one of the legs and the baseline system
for the other and the order was balanced. Each leg
took up to 30 mins to finish and the total duration
including questionnaires was about 1.5 hours. Fig-
ure 2 shows the route taken by the subjects. The
route is about 1.3 miles long. Subjects were fol-
lowed by the evaluator who made notes on their
behaviour (e.g. subject looks confused, subject
looks at or manipulates the phone, subject looks
around, etc).
Subjects filled in a demographic questionnaire
prior to the experiment. After each leg, they filled
in a system questionnaire (see appendix) rating
their experience. After the end of the experi-
ment, they filled out a comparative questionnaire
and were debriefed. They were optionally asked
to elaborate on their questionnaire ratings. Users
were paid ?20 after the experiment was over.
6 Results
Subjects were asked to identify tasks that they
thought were successfully completed. The per-
ceived task success rates of the two systems were
compared for each task using the Chi square test.
The results show that there is no statistically sig-
nificant difference between the two systems in
terms of perceived task success although the base-
line system had a better task completion rate in
tasks 1-3, 5 and 6. Our system performed better in
Figure 2: Task route
tourist information tasks (4, 7) (see table 4).
Task Our system Baseline p
T1 (N) 77.7 100 0.5058
T2 (TI) 88.8 100 0.9516
T3 (N) 100 100 NA
T4 (TI) 100 87.5 0.9516
T5 (N+TI) 62.5 100 0.1654
T6 (N+TI) 87.5 100 0.9516
T7 (TI) 100 55.5 0.2926
T8 (N) 75.0 88.8 0.9105
Table 4: % Perceived Task success - task wise
comparison (N - navigation task, TI - Tourist In-
formation task)
The system questionnaires that were filled out
by users after each leg were analysed. These
consisted of questions concerning each system to
be rated on a six point Likert scale (1-Strongly
Disagree, 2-Disagree, 3-Somewhat Disagree, 4-
Somewhat Agree, 5-Agree, 6-Strongly Agree).
The responses were paired and tested using a
Wilcoxon Sign Rank test. Median and Mode for
each system and significance in differences are
shown in table 5. Results show that although
our system is not performing significantly better
than the baseline system (SQ1-SQ10 except SQ7),
users seem to find it more understanding (SQ7)
and more interesting to interact with (SQ11) than
the baseline. We grouped the subjects by age
group and tested their responses. We found that
the young subjects (age group 20-26), also felt that
1664
Leg 1
(Task 1) Ask the system to guide you to the Red Fort restaurant.
(Task 2) You?ve heard that Mary Queen of Scots lived in Edinburgh. Find out about her.
(Task 3) Walk to the university gym.
(Task 4) Near the gym there is an ancient wall with a sign saying ?Flodden Wall?. Find out what that is.
Leg 2
(Task 5) Try to find John Knox House and learn about the man.
(Task 6) Ask the system to guide you to the Old College. What can you learn about this building?
(Task 7) Try to find out more about famous Edinburgh people and places, for example, David Hume,
John Napier, and Ian Rankin. Try to find information about people and places that you are personally
interested in or that are related to what you see around you.
(Task 8) Ask the system to guide you back to the Informatics Forum.
Table 3: Tasks for the user
they learned something new about the city using it
(SQ12) (p < 0.05) while the elderly (age group
50-71) didn?t. We also found statistically signifi-
cant differences in smartphone users rating for our
system on their learning compared to the baseline
(SQ12).
Subjects were also asked to choose between the
two systems given a number of requirements such
as ease of use, use for navigation, tourist infor-
mation, etc. There was an option to rank the sys-
tems equally (i.e. a tie). They were presented with
the same requirements as the system questionnaire
with one additional question - ?Overall which sys-
tem do you prefer?? (CQ0). Users? choice of sys-
tem based on a variety of requirements is shown
in table 6. Users? choice counts were tested us-
ing Chi-square test. Significant differences were
found in users? choice of system for navigation
and tourist information requirements. Users pre-
ferred the baseline system for navigation (CQ2)
and our system for touristic information (CQ3) on
the city. Although there was a clear choice of sys-
tems based on the two tasks, there was no signifi-
cant preference of one system over the other over-
all (CQ0). They chose our system as the most in-
teresting system to interact with (CQ11) and that
it was more informative than the baseline (CQ12).
Figure 3 shows the relative frequency between
user choices on comparative questions.
7 Analysis
Users found it somewhat difficult to navigate using
Spacebook (see comments in table 7). Although
the perceived task success shows that our system
was able to get the users to their destination and
there was no significant difference between the
two systems based on their questionnaire response
on navigation, they pointed out a number of issues
and suggested a number of modifications. Many
Figure 3: Responses to comparative questions
users noted that a visual map and the directional
arrow in the baseline system was helpful for nav-
igation. In addition, they noted that our system?s
navigation instructions were sometimes not satis-
factory. They observed that there weren?t enough
instructions coming from the system at street junc-
tions. They needed more confirmatory utterances
(that they are walking in the right direction) (5
users) and quicker recovery and notification when
walking the wrong way (5 users). They observed
that the use of street names was confusing some-
times. Some users also wanted a route summary
before the navigation instructions are given.
The problem with Spacebook?s navigation pol-
icy was that it did not, for example, direct the
user via easily visible landmarks (e.g. ?Head to-
wards the Castle?), and relies too much on street
names. Also, due to the latency in receiving GPS
information, the IM sometimes did not present in-
structions soon enough during evaluation. Some-
times it received erroneous GPS information and
therefore got the user?s orientation wrong. These
problems will be addressed in the future version.
Some users did find navigation instructions use-
ful because of the use of proximal landmarks such
1665
Question B Mode B Median S Mode S Median p
SQ1 - Ease of use 4 4 5 4 0.8207
SQ2 - Navigation 4 4 5 4 0.9039
SQ3 - Tourist Information 2 3 4 4 0.07323
SQ4 - Easy to understand 5 5 5 5 0.7201
SQ5 - Useful messages 5 4 5 4 1
SQ6 - Response time 5 5 2 2 0.2283
SQ7 - Understanding 3 3 5 4 0.02546
SQ8 - Repetitive 2 3 2 3 0.3205
SQ9 - Aware of user environment 5 5 4 4 0.9745
SQ10 - Cues for guidance 5 5 5 5 0.1371
SQ11 - Interesting to interact with 5 4 5 5 0.01799
SQ12 - Learned something new 5 4 5 5 0.08942
Table 5: System questionnaire responses (B=Baseline, S=our system)
Task Baseline Our system Tie p-
Preferred Preferred value
CQ0 23.52 35.29 41.17 0.66
CQ1 35.29 29.41 35.29 0.9429
CQ2 64.70 0 35.29 0.004
CQ3 17.64 64.70 17.64 0.0232
CQ4 35.29 29.41 23.52 0.8187
CQ5 23.52 52.94 23.52 0.2298
CQ6 23.52 29.41 35.29 0.8187
CQ7 17.64 47.05 35.29 0.327
CQ8 29.41 23.52 47.05 0.4655
CQ9 29.41 52.94 17.64 0.1926
CQ10 47.05 29.41 23.52 0.4655
CQ11 5.88 76.47 17.64 0.0006
CQ12 0 70.58 29.41 0.005
Table 6: User?s choice on comparative questions
(CQ are the same questions as SQ but requesting
a ranking of the 2 systems)
as KFC, Tesco, etc. (popular chain stores). Some
users also suggested that our system should have
a map and that routes taken should be plotted on
them for reference. Based on the ratings and ob-
servations made by the users, we conclude that our
first hypothesis that Spacebook would be more ef-
ficient for navigation than the baseline because of
its speech-only interface was inconclusive. We be-
lieve so because users? poor ratings for Spacebook
may be due to the current choice of dialogue pol-
icy for navigation. It may be possible to reassure
the user with a better dialogue policy with just the
speech interface. However, this needs further in-
vestigation.
Users found the information-search task inter-
esting and informative when they used Spacebook
(see sample user comments in table 8). They
also found push information on nearby PoIs un-
expected and interesting as they would not have
found them otherwise. Many users believed that
this could be an interesting feature that could help
tourists. They also found that asking questions and
finding answers was much easier with Spacebook
compared to the baseline system, where some-
times users needed to type search keywords in.
Another user observation was that they did not
have to stop to listen to information presented
by our system (as it was in speech) and could
carry on walking. However, with the baseline sys-
tem, they had to stop to read information off the
screen. Although users in general liked the QA
feature, many complained that Spacebook spoke
too quickly when it was presenting answers. Some
users felt that the system might lose context of the
navigation task if presented with a PoI question.
In contrast, some others noted Spacebook?s ability
to interleave the two tasks and found it to be an
advantage.
Users? enthusiasm for our system was observed
when (apart from the points of interest that were
in the experimental task list) they also asked spon-
taneous questions about James Watt, the Talbot
Rice gallery, the Scottish Parliament and Edin-
burgh Castle. Some of the PoIs that the system
pushed information about were the Royal College
of Surgeons, the Flodden Wall, the Museum of
Childhood, and the Scottish Storytelling Centre.
Our system answered a mean of 2.5 out of 6.55
questions asked by users in leg 1 and 4.88 out of
8.5 questions in leg 2. Please note that an utter-
ance is sent to QA if it is not parsed by the parser
and therefore some utterances may not be legit-
mate questions themselves. Users were pushed a
mean of 2.88 and 6.37 PoIs during legs 1 and 2.
There were a total of 17 ?tell me more? requests
requesting the system to present more information
(mean=1.35 ? 1.57).
Evaluators who followed the subjects noted that
the subjects felt difficulty using the baseline sys-
tem as they sometimes struggled to see the screen
1666
1. ?It?s useful when it says ?Keep walking? but it should say it more often.?
2. ?[Your system] not having a map, it was sometimes difficult to check how aware it was of my environment.?
3. ?[Google] seemed to be easier to follow as you have a map as well to help.?
4. ?It told me I had the bank and Kentucky Fried Chicken so I crossed the road because I knew it?d be somewhere over
beside them. I thought ?OK, great. I?m going the right way.? but then it didn?t say anything else. I like those kind of
directions because when it said to go down Nicolson Street I was looking around trying to find a street sign.?
5. ?The system keeps saying ?when we come to a junction, I will tell you where to go?, but I passed junctions and it
didn?t say anything. It should say ?when you need to change direction, I will tell you.??
6. ?I had to stop most of the times for the system to be aware of my position. If walking very slowly, its awareness of
both landmarks and streets is excellent.?
Table 7: Sample user comments on the navigation task
1. ?Google doesn?t *offer* any information. I would have to know what to ask for...?
2. ?Since many information is given without being asked for (by your system), one can discover new places and
landmarks even if he lives in the city. Great feature!!?
3. ?I didn?t feel confident to ask [your system] a question and still feel it would remember my directions?
4. ?Google could only do one thing at a time, you couldn?t find directions for a place whilst learning more.?
5. ?If she talked a little bit slower [I would use the system for touristic purposes]. She just throws masses of information
really, really quickly.?
Table 8: Sample user comments on the tourist information task
in bright sunlight. They sometimes had difficulty
identifying which way to go based on the route
plotted on the map. In comparison, subjects did
not have to look at the screen when they used
our system. Based on the ratings and observa-
tions made by the users about our system?s tourist
information features such as answering questions
and pushing PoI information, we have support for
our second hypothesis: that users find a dialogue
interface which integrates question-answering and
navigation within a shared context to be useful for
finding information about entities in the urban en-
vironment.
8 Future plans
We plan to extend Spacebook?s capabilities to ad-
dress other challenges in pedestrian navigation and
tourist information. Many studies have shown
that visible landmarks provide better cues for nav-
igation than street names (Ashweeni and Steed,
2006; Hiley et al, 2008). We will use visible
landmarks identified using the visibility engine to
make navigation instructions more effective, and
we plan to include entities in dialogue and visual
context as candidates for PoI push, and to imple-
ment an adaptive strategy that will estimate user
interests and push information that is of interest
to them. We are also taking advantage of user?s
local knowledge of the city to present navigation
instructions only for the part of the route that the
user does not have any knowledge of. These fea-
tures, we believe, will make users? experience of
the interface more pleasant, useful and informa-
tive.
9 Conclusion
We presented a mobile dialogue app called Space-
book to support pedestrian users in navigation
and tourist information gathering in urban envi-
ronments. The system is a speech-only interface
and addresses navigation and tourist information
in an integrated way, using a shared dialogue con-
text. For example, using the navigational context,
Spacebook can push point-of-interest information
which can then initiate touristic exploration tasks
using the QA module.
We evaluated the system against a state-of-the-
art baseline (Samsung S-Voice with Google Navi-
gation and Search) with a group of 17 users in the
streets of Edinburgh. We found that users found
Spacebook interesting to interact with, and that
it was their system of choice for touristic infor-
mation exploration tasks. These results were sta-
tistically significant. Based on observations and
user ratings, we conclude that our speech-only
system was less preferred for navigation and more
preferred for tourist information tasks due to fea-
tures such as PoI pushing and the integrated QA
module, when compared to the baseline system.
Younger users, who used Spacebook, even felt that
they learned new facts about the city.
Acknowledgments
The research leading to these results was funded by the Eu-
ropean Commission?s Framework 7 programme under grant
1667
agreement no. 270019 (SPACEBOOK project).
References
K. B. Ashweeni and A. Steed. 2006. A natural
wayfinding exploiting photos in pedestrian naviga-
tion systems. In Proceedings of the 8th conference
on Human-computer interaction with mobile devices
and services.
P. Bartie and W. Mackaness. 2006. Development
of a speech-based augmented reality system to sup-
port exploration of cityscape. Transactions in GIS,
10:63?86.
P. Bartie and W. Mackaness. 2012. D3.4 Pedestrian
Position Tracker. Technical report, The SPACE-
BOOK Project (FP7/2011-2014 grant agreement no.
270019).
P. Bartie and W. Mackaness. 2013. D3.1.2 The Space-
Book City Model. Technical report, The SPACE-
BOOK Project (FP7/2011-2014 grant agreement no.
270019).
D. Byron, A. Koller, J. Oberlander, L. Stoia, and
K. Striegnitz. 2007. Generating Instructions in Vir-
tual Environments (GIVE): A challenge and evalua-
tion testbed for NLG. In Proceedings of the Work-
shop on Shared Tasks and Comparative Evaluation
in Natural Language Generation.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings
of EMNLP-CoNLL.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL : Using
Natural Language Generation for Navigational As-
sistance. In Proceedings of ACSC2003, Australia.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011. Hierar-
chical Reinforcement Learning and Hidden Markov
Models for Task-Oriented Natural Language Gener-
ation. In Proc. of ACL.
B. Gittings. 2012. The Gazetteer for Scotland -
http://www.scottish-places.info.
H. Hiley, R. Vedantham, G. Cuellar, A. Liuy,
N. Gelfand, R. Grzeszczuk, and G. Borriello. 2008.
Landmark-based pedestrian navigation from collec-
tions of geotagged photos. In Proceedings of the
7th Int. Conf. on Mobile and Ubiquitous Multimedia
(MUM).
S. Janarthanam and O. Lemon. 2011. The GRUVE
Challenge: Generating Routes under Uncertainty in
Virtual Environments. In Proceedings of ENLG.
S. Janarthanam, O. Lemon, X. Liu, P. Bartie, W. Mack-
aness, T. Dalmas, and J. Goetze. 2012. Integrat-
ing location, visibility, and Question-Answering in
a spoken dialogue system for Pedestrian City Explo-
ration. In Proc. of SIGDIAL 2012, S. Korea.
H. Kashioka, T. Misu, E. Mizukami, Y. Shiga,
K. Kayama, C. Hori, and H. Kawai. 2011. Multi-
modal Dialog System for Kyoto Sightseeing Guide.
In Asia-Pacific Signal and Information Processing
Association Annual Summit and Conference.
S.S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 1999. Improvements to Platt?s
SMO Algorithm for SVM Classifier Design. Neural
Computation, 3:637?649.
J. Ko, F. Murase, T. Mitamura, E. Nyberg, M. Tateishi,
I. Akahori, and N. Hataoka. 2005. CAMMIA: A
Context-Aware Spoken Dialog System for Mobile
Environments. In IEEE ASRU Workshop.
C. Kray, K. Laakso, C. Elting, and V. Coors. 2003.
Presenting Route Instructions on Mobile Devices.
In Proceedings of IUI 03, Florida.
R. Malaka and A. Zipf. 2000. Deep Map - challenging
IT research in the framework of a tourist information
system. In Information and Communication Tech-
nologies in Tourism 2000, pages 15?27. Springer.
A. Mikhailsian, T. Dalmas, and R. Pinchuk. 2009.
Learning foci for question answering over topic
maps. In Proceedings of ACL 2009.
D. Montello. 1993. Scale and multiple psychologies
of space. In A. U. Frank and I. Campari, editors,
Spatial information theory: A theoretical basis for
GIS.
M. Raubal and S. Winter. 2002. Enriching wayfinding
instructions with local landmarks. In Second Inter-
national Conference GIScience. Springer, USA.
C.J. Shroder, W. Mackaness, and B. Gittings. 2011.
Giving the Right Route Directions: The Require-
ments for Pedestrian Navigation Systems. Transac-
tions in GIS, pages 419?438.
N. Webb and B. Webber. 2009. Special Issue on Inter-
active Question Answering: Introduction. Natural
Language Engineering, 15(1):1?8.
P. A. Zandbergen and S. J. Barbeau. 2011. Posi-
tional Accuracy of Assisted GPS Data from High-
Sensitivity GPS-enabled Mobile Phones. Journal of
Navigation, 64(3):381?399.
1668
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 66?73
Manchester, UK. August 2008
Topic Indexing and Retrieval for Factoid QA
Kisuh Ahn
School of Informatics
University of Edinburgh
Edinburgh, UK
k.ahn@sms.ed.ac.uk
Bonnie Webber
School of Informatics
University of Edinburgh
Edinburgh, UK
bonnie@inf.ed.ac.uk
Abstract
The method of Topic Indexing and Re-
trieval for QA persented in this paper
enables fast and efficent QA for ques-
tions with named entity answers. This is
achieved by identifying all possible named
entity answers in a corpus off-line and
gathering all possible evidence for their di-
rect retrieval as answer candidates using
standard IR techniques. An evaluation of
this method on 377 TREC questions pro-
duced a score of 0.342 in Accuracy and
0.413 in Mean Reciprocal Rank (MRR).
1 Introduction
Many textual QA systems use Information
Retrieval to retrieve a subset of the docu-
ments/passages from the source corpus in order to
reduce the amount of text that needs to be inves-
tigated in finding the correct answers. This use
of Information Retrieval (IR) plays an important
role, since it imposes an upper bound on the per-
formance of the entire QA system: Subsequent an-
swer extraction operations cannot make up for the
failure of IR to fetch text that contains correct an-
swers. Several techniques have been developed to
cut down the amount of text that must be retrieved
in order to ensure against the loss of answer mate-
rial, but processing any text for downstream oper-
ations still takes up valuable on-line time.
In this paper, we present a method, Topic Index-
ing and Retrieval for QA, that turns factoid Ques-
tion Answering into fine-grained Information Re-
trieval, where answer candidates are directly re-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
trieved instead of documents/passages. The pri-
mary claim here is that for simple named entity
answers, this can make for fast and accurate re-
trieval.
2 The Overall Idea
The answers to many factoid questions are named
entities ? eg, ?Who is the president of India??,
?Where was Eric Clapton born??, etc. The basic
idea of this paper?s central method, Topic Indexing
and Retrieval for Question Answering (or TOQA
subsequently), is to extract such expressions off-
line from a textual corpus as potential answers and
gather evidence that supports their direct retrieval
as answers to questions using off-the-shelf IR.
Central here is the notion of topics. Under
this method, any named entities (proper names)
found in a corpus are regarded as potential an-
swers. However, named entities are not just treated
as words or phrases but as topics with three kinds
of information useful for Question Answering.
First, as a locus of information, a topic has tex-
tual content which talks about this topic. This
comprises the set of all sentences from the cor-
pus that mention this topic. Textual content is im-
portant because it provides the means to judge the
topic?s suitability as an answer to a question via
textual similarity between the question and some
part of the topic?s textual content.
Second, a topic has an ontological type (or
types). This type information is very important for
QA because the question requires the answer to be
of certain type. A topic must be of the same type
(or some compatible type via ISA relation) in or-
der to be considered as an answer candidate. For
example, the question, ?Who is the president of In-
dia?? requires the answer to be of type PERSON
(or more specifically, PRESIDENT).
66
Finally, a topic has relations to other topics. For
example, the topic, ?Dolly the sheep?, is closely
related to the topic, ?Ian Wilmut?. While the pre-
cise nature of this relation may vary, the frequent
co-occurence of two topics in sentences can be re-
garded as an evidence that the two are related. Re-
lated topics are useful for question answering be-
cause they reduce the search space. For exam-
ple, the answer to the question, e.g. ?Who cre-
ated Dolly the sheep?? can be found among all the
topics that are related to the topic contained in the
question (or question topic), e.g. ?Dolly? here.
These three kinds of information are the base
material for Question Answering using topics:
they provide the means to directly retrieve answers
to questions.
3 Preprocessing
This section describes the technical details of how
to collect these three kinds of information used for
topic based QA, and how to process and store them
off-line in order to enable fast and efficient on-
line question answering. The stored material con-
sists of (1) a Topic Repository, which stores topics
with their variant names and ontological types, (2)
a topic document collection that stores the textual
content of topics, and (3) a set of indices created
by indexing the topic document collection for fast
and efficient retrieval.
3.1 The Make Up of Topic Repository
The Topic Repository stores topics, along their
variant names and their ontological types, in hash
tables for fast look-up. Building a topic repos-
itory requires identifying topics within the given
corpus. For this we have used the C&C named en-
tity recogniser (Curran and Clark, 2003), which is
run on pos-tagged and chunked documents in the
corpus to identify and extract named entities as po-
tential topics. This also identifies the base type of a
subset of named entities as PERSON, LOCATION
and ORGANISATION. This is stored for later use
in building type-separated indices. When a named
entity is identified, we first check whether it repre-
sents a topic already found in the topic repository.
This is done by checking the topic-name hash ta-
ble in the repository, which serves as the main data
storage for the variant names of topics.
To resolve a target named entity to the appro-
priate topic, we use Wikipedia?s Redirect table,
which contains many common variant names for
the same topic. The topic-name hash table is up-
dated accordingly. Hash table entries consist of
pairs like (?George Clooney?, 1745442), where the
name ?George Clooney? is one of the names that
belong to the unique topic with the ID number of
1745442. We currently do nothing to disambiguate
topics, so different individuals with the same name
will all be considered the same topic.
Fine-grained ontological types of topics are
identified and stored as well in a separate topic-
type table. In order to discover fine-grained
topic types, the ontology database Yago is used
(Suchanek et al, 2007). Yago contains such infor-
mation for Wikipedia topics, derived by mapping
the category information about target topic sup-
plied by a Wikipedia user to the appropriate Word-
Net concept. (Wikipedia categories are not consis-
tent and uniform, and they are more like tags that
characterise a topic rather than strictly classify it.)
Using this ontology to look up the type(s) of each
topic-type (i.e. the corresponding WordNet con-
cept) and by tracing up the WordNet concept hier-
archy, we created a fine-grained, multi-level (with
respect to ISA) topic-type hash table for all the top-
ics in the topic repository.
The topic-type hash table not only contains the
ontological type of a topic, but also a significant
amount of world knowledge typically associated to
the topic, due to the nature of Wikipedia categories
as descriptive tags. For example, ?Bill Gates? is
identified as ?CEO? (a title-role), and ?Pusan? as
?a province of Korea? (geographical knowledge).
Such diverse and significant knowledge, as well as
the breadth and the depth of the fine types con-
tained in the topic-type hash table, enable a very
powerful match between the answer type from a
question to that of a candidate topic.
The set of fine-grained answer types used here
differs from the set of answer types such as Li and
Roth (2002) used elsewhere in that the set is open-
ended, and new types can be added for an entity at
any time.
The topic repository is used in re-ranking an-
swer candidates by the fine-grained anwer type
and for question topic identification, as well as in
building topic document collection to be explained
next.
3.2 The Topic Document Collection
As noted, the textual content of a topic is the set of
all sentences in a corpus that mentions this topic.
67
(Since anaphora resolution is not yet performed,
the sentences that only mention a particular topic
anaphorically are missed.) Such set of sentences is
assembled into one file per topic. This can then
be regarded as a document on its own with the
topic name as its title. We henceforth call such
a document, a topic document. Figure 1 illustrates
a topic document for the topic, Dolly the sheep.
The topic document collection thus created for all
topics identified can be regarded as a reorganised
corpus with respect to the original corpus as the
Figure ?? illustrates.
Figure 1: An Example Topic Document: Dolly the
sheep
The topic document collection for the full set of
topics is a subset of the original corpus, reorga-
nized around topics. The process of creating the
topic document collection (which we refer to as
the topic document method) is actually performed
at the same time as the creation of Topic Reposi-
tory. Any sentence that contains identifiable topics
is appended to the topic document of each topic
it contains. The topic document collection so cre-
ated is central to our Question Answering because
retrieving a topic document (specifically, its topic)
equates to generating an answer candidate for a
given question. Hence, via topic documents, fine-
grained IR can be used to retrieve answers directly.
In order to facilitate such retrieval, however, a
topic document collection needs to be indexed.
In our implemented system (described in Section
5), this is done using the indexing module of the
Lemur Toolkit. For type specific retrieval, three
separate indices corresponding to PERSON, LO-
CATION and ORGANISATION are created ac-
cording to the base types of the topics identified
at the time of Named Entity Recognition. In ad-
dition, an index for all topic documents regardless
of types, TOTAL, is also created for questions from
which the answer type cannot be determined or for
which their answer types differ from the three base
types. Of note here is that separate indices are
created only for these base types, as we have not
explored separate indexing by fine-grained answer
types. These fine-types are only used for reranking
after the candidate topics have been retrieved from
the base indices.
At the time of retrieval, an appropriate index is
to be chosen depending on the answer type iden-
tified from the question. This is discussed in the
next section.
4 Topic Retrieval and Reranking for QA
The goal is to retrieve a ranked list of topic doc-
uments (indicated by their topics) as answers to a
given question. In order to do this, the query for
the IR operation must be formulated from the ques-
tion, and the specific answer type must be identi-
fied both for retrieval and for any re-ranking of the
retrieved list of topics.
Thus, the first necessary operation is Question
Analysis. Question Analysis identifies the ques-
tion type (eg, definition question, factoid question,
list question, etc); the answer type, and the ques-
tion topics (if any) and produces a shallow parse
of the question text (pos-tagged and chunked) for
query formulation. (Identifying the question type
is a formality since the method only deals with fac-
toid questions.)
The question topic identification is straightfor-
ward: Any proper name present in a particular
question is a question topic. For answer type
identification, we use a simple rule based algo-
rithm that looks at the WH-word (e.g. ?Where?
means location), the head noun of a WH-phrase
with ?Which? or ?What? (e.g. ?Which president?
means the answer type is of president), and if the
main verb is a copula, the head of the post-copula
noun phrase (e.g. for ?Who is the president ..?,
here again ?president? is the answer type.) Word-
Net is used to identify the base type of the an-
swer type identified from the question when it is
not one of the base types (PERSON, LOCATION,
ORGANISATION). For example, ?president? is
traced to its base type, ?PERSON?.
68
Next is the retrieval of topics as answer candi-
dates for a given question. This involves: (1) iden-
tifying the appropriate index, (2) formulating the
query, and (3) the actual retrieval operation. An ap-
propriate index is chosen based on the base answer
type. For example, for the question, ?Who is the
president of Germany??, the answer type is iden-
tified as ?president?. But since the answer type,
?president?, is not the base type, WordNet is used
to trace from ?president? to a base type (PERSON)
and the corresponding index is selected (because
separate indices exist only for base types). If none
of the three base types is found by this process, the
total index is used.
Retrieval uses the InQuery retrieval system
within the Lemur Tool Kit (Ogilvie and Callan,
2002). InQuery supports a powerful and flexible
structured query language. Taking advantage of
this, a structured query is formulated from the tar-
get question. So for example, the parsed form of
the question, ?Who is the president of Germany??
is used to generate the following query
\sum(is president of germany
\phrase(president of germany)).
In this example, ?president of germany? forms a
phrase, and it is inserted as part of the query el-
ement with the ?\phrase? operator. However, the
individual keywords are also included as bag of
words since we have found it to give better perfor-
mance in the trials that we have run. The overall
operator is then enclosed by the ?\sum? operator
that gives the overall score of the query with re-
spect to a document. With this query, search is
performed and a ranked list of topics is retrieved.
This ranked list is then run through the following
operations:
1. Filtering the retrieved list of topics to remove
question topics if present.
2. Re-ranking with respect to topic type, prefer-
ring the topic that matches the fine answer
type.
3. Choosing the highest ranking topic as the an-
swer to the question.
The question topic, in the above example, ?Ger-
many?, is filtered out if it is found in the list of top-
ics retrieved (using topic-name hash table), which
can happen as it is one of the keywords in the
query. For the remaining topics in the list, the types
of each topic are fetched using the topic-type hash
table and matched up to the specific answer type.
Re-ranking is performed according to the follow-
ing rules:
? Topics whose type precisely matches the an-
swer type are ranked higher than any other
topics whose types do not precisely match the
answer type.
? Topics whose type do not precisely match
the answer type but still matches the base
type traced from the answer type are ranked
higher than any other topics whose types do
not match the answer type at all.
Based on these simple rules, the highest-ranking
topic is chosen as the answer. Because of the de-
tailed and precise type information stored for each
topic, we find this simple procedure works well
enough. However, a more sophisticated answer
candidate reranking strategy is conceivable based
on giving different weights to different degree of
match for an answer type.
5 Bi-Topic Indexing
The method described thus far ignores question
topics except for filtering them out during post-
processing. However, we mentioned in Section 2
that related topics can be exploited in answering
questions.
To take advantage of question topics within
Topic Indexing and Retrieval, we have adopted
the solution of constructing bi-topic documents in
contrast to the original topic documents with sin-
gle topics. An example of a bi-topic document is
the following Figure 2, which represents the two
topics (Dolly, Ian Wilmut). Such a bi-topic docu-
ment represents the general relation between two
topics via the context in which they co-occur. (As
already noted, the precise character of the rela-
tion is ignored.) The terms that more frequently
appear in such document characterise the relation
between the two topics in statistical fashion, and
this document would be given a higher score for
retrieval with respect to a question, if the ques-
tion contains such a relatively frequently appear-
ing term. For example, in scoring the bi-topic doc-
ument pertaining to (Dolly, Ian Wilmut) bi-topic
document with respect to the question, ?Who cre-
ated the first cloned sheep, Dolly??, the frequently
appearing term in the document, ?cloned? would
69
give a very high mark for this document with re-
spect to this question.
Figure 2: A Bi-Topic Document: (Dolly, Ian
Wilmut)
We construct a bi-topic document collection
is a recursive application of the topic document
method first on the original documents and then
to the resulting topic documents. So given a single
topic document, e.g. for ?Dolly?, the same topic
document generating process is then applied to this
document. This generates a new set of topic doc-
uments that, in addition to having their own top-
ics, e.g. ?Ian Wilmut?, will also contain the topic
?Dolly? since the original topic document has the
topic ?Dolly? in its every sentence. The result-
ing bi-topic documents would comprise (Dolly, Ian
Wilmut), (Dolly, Bonnie), (Dolly, Roslin), etc., all
as bi-topic documents. These topic documents all
concern the topic ?Dolly?, which we call the an-
chor topic, and indexing these amounts to creating
a ?Dolly? (anchored) index. Separate indices for
base types as in the case of the single topic doc-
uments need not be created since the number of
bi-topic documents anchored to one topic is some
magnitude smaller compared to the number of to-
tal single topic documents.
QA using a bi-topic document index is essen-
tially the same as for the single topic document in-
dex, except in selecting the appropriate anchored
index using the question topic identified from the
question. So the ?Dolly? index is chosen if the
question topic is ?Dolly?, as in the question, e.g.
?Who created the fist cloned sheep, Dolly??. Re-
ranking based on fine-grained answer types can
still be performed although question topic filtering
is no longer necessary.
This bi-topic method has the draw-back of gen-
erating a lot of documents and corresponding in-
dices since the number of bi-topic documents is
the product of the number of topics with all the
associated topics. This takes a lot of space for stor-
age and time for generating such a collection. For
the evaluation to be described in the next section,
we have created bi-topic documents and indices
that only pertain to questions (ie only for the ques-
tion topics within the test set) due to the limitation
of space. To be able to scale this method gener-
ally, XML information retrieval technique might
be applicable as this supports richer retrieval ele-
ments other than whole documents and therefore
the bi-topic documents pertaining to one anchor
topic could be all embedded within one topic doc-
ument. This is one area we would like to explore
further in the future.
The next section characterises and compares the
performance of single topic and bi-topic document
based methods.
6 Evaluation
6.1 The Evaluation Settings
Evaluation has been carried out to determine
whether Topic Indexing and Retrieval using a sim-
ple and efficient IR technique for direct answer re-
trieval can indeed make for an accurate QA sys-
tem. This has also iluminated those features of the
method that contribute to QA performance.
The questions and the corpus (AQUAINT) used
for the evaluation are taken from the TREC QA
track. 377 questions that have single proper names
as answers (ie, excluding list questions, ?other?
questions and questions without answers) were se-
lected from the TREC 2003/2004/2005 questions.
Questions from TREC 2004 and TREC 2005 are
grouped around what are called targets. A tar-
get is basically the question topic, e.g. ?When
was he born?? where ?he? refers to the target,
e.g. ?Fred Durst?. One of the experimental setups
takes account of these targets by employing the Bi-
topic method discussed in Section 5. This retrieval
strategy is also applied to questions from TREC
2003 (that come with no targets), by identifying
the question topic in a question and extracting it
as a target automatically, in order to see whether
it can benefit the QA performance even when the
target is not provided manually.
70
The actual evaluation of the method consists of
three experiments, each of which tests a different
setting. The common elements for all three are
the core answer retrieval system. The aspects that
differentiate the three settings are: (1) whether or
not a fine-grained answer type is used for rerank-
ing, (2) whether single topic documents or bi-topic
documents are retrieved.
6.2 The Core Evaluation System
The common core system that implements the an-
swer retrieval method comprises (1) a question
analysis module that analyses the question and
produces the question type, answer type, the ques-
tion topics and the shallow parse of the question
text and (2) a retrieval module that generates the
structured query, selects the appropriate index and
retrieves the top 100 topics as answer candidates.
This core system performs the basic retrieval op-
erations, to which we add further operations such
as answer-type based reranking and target specific
retrieval. The addition of some of these features
distinguish different setups for the evaluation.
Setup A involves just the core system on single
topic document indexing of the AQUAINT corpus,
as described in Section 3.2. The resulting topic
documents are divided into the three base types
(PERSON, LOCATION, ORGANISATION), plus
OTHER, as summarised in Table 1. Some ex-
amples of entities belong to type OTHER include
medicines, roller coasters and software.
KIND NUM
PERSON 117370
ORGANISATION 67559
LOCATION 48194
OTHER 17942
TOTAL 251065
Table 1: Number of Topic Docs per Types
Setup B is basically the same as setup A, ex-
cept for the addition of fine-grained answer type
re-ranking on the one hundred topics retrieved as
answer candidates. That is, elements of this list
are re-ranked depending on whether their fine-
grained answer type matches the fine-grained an-
swer type identified from the question. Note here
that only the coarse answer type (PERSON, LO-
CATION, ORGANISATION, TOTAL) was used
for retrieval, as opposed to the fine-grained type
such as PRESIDENT or COMPANY, due to the
A@N A B C
1 0.233:88 0.340:128 0.342:129
2 0.316:119 0.406:153 0.443:167
3 0.366:138 0.438:165 0.485:183
4 0.401:151 0.467:176 0.501:189
5 0.430:162 0.491:185 0.515:194
10 0.472:178 0.523:197 0.549:207
15 0.496:187 0.533:201 0.560:211
20 0.512:193 0.541:204 0.560:211
ACC 0.233 0.340 0.342
MRR 0.306 0.395 0.413
Table 2: Results for all setups for all questions
fact that separate indices exist only for these coarse
types. The identification of the fine type of a can-
didate topic is done by looking up this information
in the topic-type hash table as mentioned in Sec-
tion 3. Again the resulting top candidate is picked
as the definite answer.
The final setup is setup C. Setup C exploits ques-
tion topics (targets), as described in Section 5. Tar-
gets are explicitly provided in TREC 2004 and
TREC 2005 question set. For the TREC 2003, the
questions, which do not come with explicit targets,
the system automatically extracts a target from the
question using a very simple rule: any proper name
in the question is regarded as a target. The point of
this setup is to test the effectiveness of the bi-topic
method discussed in Section 5. The core retrieval
procedure is the same as in setup B, except that
the index on which the retrieval is performed is se-
lected based on the question topic. In Section 5,
we mentioned that a set of indices were built with
respect to ?anchor topics?. So the question topic
identified from the question (or provided as de-
fault) acts as the anchor topic and the index that
corresponds to this anchor topic gets chosen. The
rest of the process is the same as setup B, and re-
trieved topics are re-ranked according to the fine-
grained answer type.
6.3 Overall Results
Table 2 summarises the results of the experiments
across all setups and across all the questions eval-
uated. The leftmost column indicates the cut-off
point (ie, 5 indicates the top-5 answer candidates,
10 indicates the top-10 answer candidates, etc.).
The other columns indicate the A@N performance
score data for setup A, setup B and setup C respec-
tively at each cut-off point. Each entry comprises
71
A@N B-C C-B C ? B
1 60 61 68
2 61 75 92
3 61 79 104
4 63 76 113
5 66 75 119
10 69 79 128
15 68 78 133
20 69 76 135
Table 3: Overlap between B and C
two scores separated by a colon, representing the
ratio of correctly answered questions over all ques-
tions and the number of correctly answered ques-
tions. The last two rows summarise the results by
giving the accuracy (ACC), which is equivalent to
the correctness rate at A@1 and the Mean Recip-
rocal Rank score (MRR).
From this table, it can be seen that both setup B
and setup C produced results that are superior to
setup A in all measures: accuracy, A@N (for N up
to 20) and MRR.
In order to verify whether the differences in
scores indicate statistical significance, we have
performed Wilcoxon Matched Signed Rank Test
(Wilcoxon, 1945) on the test data (the differences
in ranks for all the questions between setups). This
test is suited for testing two related samples when
an underlying distribution cannot be assumed (un-
like t-test) as with the data here. The statistical
test shows that the difference between setup B and
setup A is indeed significant (p = 1.763e ? 08,
for P threshold at 0.05) and that the difference
between setup C and setup A is also significant
(p = 4.244e?08). So setup B and setup C perform
significantly better than setup A.
Setup C performs slightly better than setup B,
both in accuracy (0.342 vs. 0.340) and in MRR
(0.413 vs 0.395), but the statistical test shows,
this difference is not statistically significant (p =
0.5729). However, as the Table 3 shows, setup
B and setup C correctly answered different ques-
tions. (Setup B answered most of the questions
that were correctly answered by setup A, as well
as questions that were not correctly answered by
setup A). Thus, a further investigation is needed
to understand performance differences between se-
tups B and C.
The execution time for each question takes less
than one second for both single-topic and bi-topic
document indices based retrieval on a single CPU
(P4 3.2 Mhz HT) with 512 MB of memory, and
the reranking operation did not add any significant
amount of time to it.
7 Related Work
In this section, we discuss some of the works on
novel indexing techniques for QA that relate to this
work.
In predictive annotation (Prager et al, 1999), the
text of the target corpus is pre-processed in such
a way that phrases that are potential answers are
marked and annotated with respect to their answer
types (or QA-tokens as they call them) including
PERSON$
1
, DURATION$, etc. Then the text is
indexed not only with ordinary terms but also with
these QA-tokens as indexing elements. The main
advantage of this approach is that QA-tokens are
used as part of the query enhancing the passage re-
trieval performance. Our work in this paper uses
the same predictive annotation technique but dif-
fers in that the named entities are indexed as topics
and are retrieved directly as answer candidates.
Similar to our approach, Kim et al (2001) ap-
plies predictive annotation method to retrieve an-
swers directly rather than supporting text. For ev-
ery potential answer in the corpus, a set of text
spans up to three sentences long (the sentence in
which it appears, plus whatever following sen-
tences that are linked to this sentence via lexical
chain totalling no more than three sentences in
size) is stored and later sued to retrieve a potential
answer. Although similar to our work, the main
difference is in the way the textual evidence is ag-
gregated. In Topic Indexing and Retrieval, all the
evidence (aka textual content) available through-
out the corpus for a possible answer is aggregated,
whereas Kim uses text spans up three sentences
long from a single document connected by a co-
reference chain for each answer candidate. Also,
topic relations are not exploited as in our work (via
Bi-topic documents).
Fleischman et al (2002) also retrieves answers
directly. In what they call the answer repository
approach to Question Answering, highly precise
relational information is extracted from the text
collection using text mining techniques based on
part of speech patterns. The extracted concept-
instance pairs of person name-title such as (Bill
1
In their notation, the Dollar sign at the end indicates that
this is a QA token rather than a term.
72
Gates, Chairman of Microsoft) are used either
solely or in conjunction with a common QA sys-
tem in producing answers. (Jijkoun et al (2004)
follows a similar approach.) This basically Infor-
mation Extraction approach taken here can com-
plement our own work for the benefit of increased
precision for select types of questions.
In Clifton and Teahan (2004), their knowledge
framework based QA system, QITEKAT, prestores
possible answers along with their corresponding
question templates based on manual and automatic
regular expression patterns. That the potential
questions are stored as well the answers make this
approach different from our approach.
The bi-topic method in this paper has some sim-
ilarity to Katz and Lin (2000). Here, ternary re-
lations are extracted off-line using manually con-
structed regular expression patterns on a target text
and stored in a database for the use in Question
Answering such as in the START QA system (Katz
et al, 2002). With bi-topic documents in this pa-
per, instead of the precise relations between the
two topics, the aggregate context between two par-
ticular topics are captured by assembling all state-
ments that mention these two topics together in one
file. While this does not give the exact character-
istics of the relations involved, it does give some
statistical characterization between the two topics
to the benefit for QA.
8 Conclusion
In this paper, we have presented the method of
Topic Indexing and Retrieval for QA. The method
effectively turns document retrieval of IR into di-
rect answer retrieval by indexing potential answers
(topics) via topic documents. We claimed that
the method can be applied in answering simple
named-entity questions. The evaluation results in-
deed show that the method is effective for this type
of question, with MRR of 0.413 and accuracy of
0.342 (best run: setup C).
References
Clifton, Terence and William Teahan. 2004. Bangor at
TREC 2004: Question answering track. In Proceed-
ings TREC 2004.
Curran, J. and S. Clark. 2003. Language independent
ner using a maximum entropy tagger. In Proceed-
ings of the Seventh Conference on Natual Language
Learning (CoNLL-03), pages 164?167.
Fleischman, Michael, Eduard Hovy, and Abdessamad
Echihabi. 2002. Offline strategies for online ques-
tion answering: Answering questions before they are
asked. In Proceedings of the 41th Annual Meeting of
the Association for Computational Linguistics (ACL-
2003).
Jijkoun, Valentin, Maarten de Rijke, and Jori Mur.
2004. Information extraction for question answer-
ing: improving recall through syntactic patterns.
In COLING ?04: Proceedings of the 20th inter-
national conference on Computational Linguistics,
page 1284, Morristown, NJ, USA. Association for
Computational Linguistics.
Katz, Boris and Jimmy Lin. 2000. REXTOR: A system
for generating relations from natural language. In
Proceedings of the ACL 2000 Workshop on Recent
Advances in NLP and IR.
Katz, B., S. Felshin, D. Yuret, A. Ibrahim, J. Lin,
G. Marton, A. McFarland, and B. Temelkuran. 2002.
Omnibase: Uniform access to heterogeneous data for
question answering.
Kim, Harksoo, Kyungsun Kim, Gary Geunbae Lee,
and Jungyun Seo. 2001. MAYA: A fast question-
answering system based on a predictive answer in-
dexer. In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics (ACL-
2001) Workshop on Open-Domain Question Answer-
ing.
Li, X. and D. Roth. 2002. Learning question clas-
sifiers. In Proceeding of the 19th International
Conference on Computational Linguistics (COL-
ING?02).
Ogilvie, P. and J. Callan. 2002. Experiments using
the lemur toolkit. In Proceeding of the 2001 Text
Retrieval Conference (TREC 2001), pages 103?108.
Prager, John, Dragomir Radev, Eric Brown, Anni Co-
den, and Valerie Samn. 1999. The use of predic-
tive annotation for question answering in TREC8. In
Proceedings of the Eighth Text REtrieval Conference
(TREC-8).
Suchanek, Fabian M., Gjergji Kasneci, and Ger-
hard Weikum. 2007. Yago: A core of seman-
tic knowledge - unifying WordNet and Wikipedia.
In Williamson, Carey L., Mary Ellen Zurko, and
Prashant J. Patel-Schneider, Peter F. Shenoy, edi-
tors, 16th International World Wide Web Conference
(WWW 2007), pages 697?706, Banff, Canada. ACM.
Wilcoxon, F. 1945. Individual comparisons by ranking
methods. Biometrics, (1):80?83.
73
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 42?54,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Discourse Structure and Computation: Past, Present and Future
Bonnie Webber
School of Informatics
University of Edinburgh
Edinburgh UK EH8 9AB
bonnie.webber@ed.ac.uk
Aravind Joshi
Dept of Computer & Information Science
University of Pennsylvania
Philadelphia PA 19104-6228
joshi@seas.upenn.edu
Abstract
The discourse properties of text have long
been recognized as critical to language tech-
nology, and over the past 40 years, our un-
derstanding of and ability to exploit the dis-
course properties of text has grown in many
ways. This essay briefly recounts these de-
velopments, the technology they employ, the
applications they support, and the new chal-
lenges that each subsequent development has
raised. We conclude with the challenges faced
by our current understanding of discourse, and
the applications that meeting these challenges
will promote.
1 Why bother with discourse?
Research in Natural Language Processing (NLP) has
long benefitted from the fact that text can often be
treated as simply a bag of words or a bag of sen-
tences. But not always: Position often matters ?
e.g., It is well-known that the first one or two sen-
tences in a news report usually comprise its best ex-
tractive summary. Order often matters ? e.g., very
different events are conveyed depending on how
clauses and sentences are ordered.
(1) a. I said the magic words, and a genie ap-
peared.
b. A genie appeared, and I said the magic
words.
Adjacency often matters ? e.g., attributed mate-
rial may span a sequence of adjacent sentences,
and contrasts are visible through sentence juxtaposi-
tion. Context always matters ? e.g., All languages
achieve economy through minimal expressions that
can only convey intended meaning when understood
in context.
Position, order, adjacency and context are intrin-
sic features of discourse, and research on discourse
processing attempts to solve the challenges posed by
context-bound expressions and the discourse struc-
tures that give rise, when linearized, to position, or-
der and adjacency.
But challenges are not why Language Technol-
ogy (LT) researchers should care about discourse:
Rather, discourse can enable LT to overcome known
obstacles to better performance. Consider auto-
mated summarization and machine translation: Hu-
mans regularly judge output quality in terms that in-
clude referential clarity and coherence. Systems can
only improve here by paying attention to discourse
? i.e., to linguistic features above the level of n-
grams and single sentences. (In fact, we predict that
as soon as cheap ? i.e., non-manual ? methods are
found for reliably assessing these features ? for ex-
ample, using proxies like those suggested in (Pitler
et al, 2010) ? they will supplant, or at least com-
plement today?s common metrics, Bleu and Rouge
that say little about what matters to human text un-
derstanding (Callison-Burch et al, 2006).)
Consider also work on automated text simplifica-
tion: One way that human editors simplify text is
by re-expressing a long complex sentence as a dis-
course sequence of simple sentences. Researchers
should be able to automate this through understand-
ing the various ways that information is conveyed
in discourse. Other examples of LT applications
already benefitting from recognizing and applying
discourse-level information include automated as-
sessment of student essays (Burstein and Chodorow,
2010); summarization (Thione et al, 2004), infor-
42
mation extraction (Patwardhan and Riloff, 2007;
Eales et al, 2008; Maslennikov and Chua, 2007),
and more recently, statistical machine translation
(Foster et al, 2010). These are described in more
detail in (Webber et al, 2012).
Our aim here then, on this occasion of ACL?s 50th
Annual Meeting, is to briefly describe the evolution
of computational approaches to discourse structure,
reflect on where the field currently stands, and what
new challenges it faces in trying to deliver on its
promised benefit to Language Technology.
2 Background
2.1 Early Methods
The challenges mentioned above are not new.
Question-Answering systems like LUNAR (Woods,
1968; Woods, 1978) couldn?t answer successive
questions without resolving context-bound expres-
sions such as pronouns:
(2) What is the concentration of silicon in brec-
cias?
?breccia1, parts per million?
?breccia2, parts per million?
? . . . ?
What is it in volcanics? (Woods, 1978)
Early systems for human interaction with animated
agents, including SHRDLU (Winograd, 1973) and
HOMER (Vere and Bickmore, 1990), faced the
same challenge. And early message understand-
ing systems couldn?t extract relevant information
(like when a sighted submarine submerged ? ?went
sinker?) without recognizing relations implicit in the
structure of a message, as in
(3) VISUAL SIGHTING OF PERISCOPE FOL-
LOWED BY ATTACK WITH ASROC AND TOR-
PEDO. WENT SINKER. LOOSEFOOT 722/723
CONTINUE SEARCH. (Palmer et al, 1993)
The same was true of early systems for processing
narrative text (under the rubric story understanding).
They took on the problem of recognizing events that
had probably happened but hadn?t been mentioned
in the text, given the sequence of events that had
been (Lehnert, 1977; Rumelhart, 1975; Schank and
Abelson, 1977; Mandler, 1984).
Since these early systems never saw more than
a handful of examples, they could successfully em-
ploy straight-forward, but ad hoc methods to handle
the discourse problems the examples posed. For ex-
ample, LUNAR used a single 10-position ring buffer
to store discourse entities associated with both the
user?s and the system?s referring expressions, resolv-
ing pronouns by looking back through the buffer
for an appropriate entity and over-writing previous
buffer entries when the buffer was full.
The next wave of work in computational dis-
course processing sought greater generality through
stronger theoretical grounding, appealing to then-
current theories of discourse such as Centering The-
ory (Grosz et al, 1986; Grosz et al, 1995), used as
a basis for anaphor resolution (Brennan et al, 1987;
Walker et al, 1997; Tetreault, 2001) and text genera-
tion (Kibble and Power, 2000), Rhetorical Structure
Theory (Mann and Thompson, 1988), used as a ba-
sis for text generation (Moore, 1995) and document
summarization (Marcu, 2000b), and Grosz and Sid-
ner?s theory of discourse based on intentions (Grosz
and Sidner, 1986a) and shared plans (Grosz and
Sidner, 1990), used in developing animated agents
(Johnson and Rickel, 2000). Issues related to fully
characterizing centering are explored in great detail
in (Kehler, 1997) and (Poesio et al, 2004).
The approaches considered during this period
never saw more than a few handfuls of examples.
But, as has been clear from developments in PoS-
tagging, Named Entity Recognition and parsing,
Language Technology demands approaches that can
deal with whatever data are given them. So subse-
quent work in computational discourse processing
has similarly pursued robustness through the use of
data-driven approaches that are usually able to cap-
ture the most common forms of any phenomenon
(ie, the 80% at the high end of the Zipfian distri-
bution), while giving up on the long tail. This is
described in Section 3.
2.2 Early Assumptions
While early work focussed on the correct assump-
tion that much was implicit in text and had to be
inferred from the explicit sequence of sentences that
constituted a text, work during the next period fo-
cussed on the underlying structure of discourse and
its consequences. More specificaly, it assumed that
the sequence of sentences constituting the text were
covered by a single tree structure, similar to the sin-
gle tree structure of phrases and clauses covering the
43
s1 s2
s3
condition
condition
s1
s2 s3motivation
motivation
(a) (b)
Figure 1: Proposed discourse structures for Ex. 4: (a) In
terms of informational relations; (b) in terms of inten-
tional relations
words in a sentence. At issue though was the nature
of the structure.
One issue concerned the nature of the relation be-
tween parent and child nodes in a discourse tree,
and/or the relation between siblings. While Rhetor-
ical Structure Theory (Mann and Thompson, 1988)
posited a single discourse relation holding between
any two discourse units (i.e., units projecting to ad-
jacent text spans), Moore and Pollack (1992) gave an
example of a simple discourse (Ex. 4) in which dif-
ferent choices about the discourse relation holding
between pairs of units, implied different and non-
isomorphic structures.
(4) Come home by 5:00. s1 Then we can go to the
hardware store before it closes. s2 That way we
can finish the bookshelves tonight. s3
Example 4 could be analysed purely in terms of
information-based discourse relations, in which s1
specified the CONDITION under which s2 held,
which in turn specified the CONDITION under which
s3 held. This would make s1 subordinate to s2,
which in turn would be subordinate to s3, as in Fig-
ure 1a. Alternatively, Example 4 could be analysed
purely in terms of intention-based (pragmatic) rela-
tions, in which s2 would be MOTIVATION for s1,
while s3 would be MOTIVATION for s2. This would
make s3 subordinate to s2, which in turn would be
subordinate to s1, as in Figure 1b. In short, the
choice of relation was not merely a matter of labels,
but had structural implications as well.
Another issue during this period concerned the
nature of discourse structure: Was it really a tree?
Sibun (1992), looking at people?s descriptions of the
layout of their house or apartment, argued that they
resembled different ways of linearizing a graph of
the rooms and their connectivity through doors and
halls. None of these linearizations were trees. Sim-
ilarly, Knott et al (2001), looking at transcriptions
of museum tours, argued that each resembled a lin-
ear sequence of trees, with one or more topic-based
connections between their root nodes ? again, not
a single covering tree structure. Wiebe (1993), look-
ing at simple examples such as
(5) The car was finally coming toward him. s1
He finished his diagnostic tests, s2
feeling relief. s3
But then the car started to turn right. s4
pointed multiple lexical items explicitly relating a
clause to multiple other clauses. Here, but would
relate s4 to s3 via a CONTRAST relation, while then
would relate s4 to s2 via a temporal SUCCESSION
relation.
The most well-known of work from this period
is that of Mann and Thompson (1988), Grosz and
Sidner (1986b), Moore and Moser (1996), Polanyi
and van den Berg (1996), and Asher and Lascarides
(2003).1
The way out of these problems was also a way
to achieve the robustness required of any Language
Technology, and that lay in the growing consensus
towards the view that discourse does not have a sin-
gle monolithic hierarchical structure. Rather, dif-
ferent aspects of a discourse give rise to different
structures, possibly with different formal properties
(Stede, 2008; Stede, 2012; Webber et al, 2012).
These different structures we describe in the next
section, while the fact that this can?t be the end of
the story, we take up in Section 4.
3 The Situation Today
Recent years have seen progress to differing degrees
on at least four different types of discourse struc-
tures: topic structure, functional structure, event
structure, and a structure of coherence relations.
First we say a bit about the structures, and then about
the resources employed in recognizing and labelling
them.
3.1 Types of discourse structures
Topic structure and automated topic segmentation
aims to break a discourse into a linear sequence of
1For a historical account and assessent of work in automated
anaphora resolution in this period and afterwards, we direct the
reader to Strube (2007), Ng (2010) and Stede (2012).
44
topics such the geography of a country, followed by
its history, its demographics, its economy, its legal
structures, etc. Segmentation is usually done on a
sentence-by-sentence basis, with segments not as-
sumed to overlap. Methods for topic segmenation
emply semantic, lexical and referential similarity
or, more recently, language models (Bestgen, 2006;
Chen et al, 2009; Choi et al, 2001; Eisenstein and
Barzilay, 2008; Galley et al, 2003; Hearst, 1997;
Malioutov and Barzilay, 2006; Purver et al, 2006;
Purver, 2011).
Functional structure and automated functional
segmentation aims to identify sections within a dis-
course that serve different functions. These func-
tions are genre-specific. In the case of scien-
tific journals, high-level sections generally include
the Background (work that motivates the objec-
tives of the work and/or the hypothesis or claim
being tested), followed by its Methods, and Re-
sults, ending with a Discussion of the results or out-
comes, along with conclusions to be drawn. Finer-
grained segments might include the advantage of
a new method (method-new-advantage) or of an
old method (method-old-advantage) or the disad-
vantage of one or the other (Liakata et al, 2010).
Again, segmentation is usually done on a sentence-
by-sentence basis, with sentences not assumed to
fill more than one function. Methods for functional
segmentation have employed specific cue words and
phrases, as well as more general language models
(Burstein et al, 2003; Chung, 2009; Guo et al,
2010; Kim et al, 2010; Lin et al, 2006; McKnight
and Srinivasan, 2003; Ruch et al, 2007; Mizuta
et al, 2006; Palau and Moens, 2009; Teufel and
Moens, 2002; Teufel et al, 2009; Agarwal and Yu,
2009). The BIO approach to sequential classica-
tion (Beginning/Inside/Outside) used in Named En-
tity Recognition has also proved useful (Hirohata
et al, 2008), recognizing that the way the start of
a functional segement is signalled may differ from
how it is continued.
Note that topic segmentation and functional seg-
mentation are still not always distinguished. For
example, in (Jurafsky and Martin, 2009), the term
discourse segmentation is used to refer to any seg-
mentation of a discourse into a ?high-level? linear
structure. Nevertheless, segmentation by function
exploits different features (and in some cases, dif-
ferent methods) than segmentation by topic, so they
are worth keeping distinct.
Attention to event structure and the identification
of events within a text is a more recent phenomena,
after a hiatus of over twenty years. Here we just
point to work by (Bex and Verheij, 2010; Cham-
bers and Jurafsky, 2008; Do et al, 2011; Finlayson,
2009).
The automated identification of discourse rela-
tions aims to identify discourse relations such as
CONDITION and MOTIVATION, as in Example 4,
and CONTRAST and SUCCESSION, as in Exam-
ple 5. These have also been called coherence re-
lations or rhetorical relations. Methods used de-
pend on whether or not a text is taken to be divisible
into a covering sequence of a non-overlapping dis-
course units related to adjacent units by discourse
relations as in Rhetorical Structure Theory (Mann
and Thompson, 1988) or to both adjacent and non-
adjacent units as in the Discourse GraphBank (Wolf
and Gibson, 2005). If such a cover is assumed,
methods involve parsing a text into units using lex-
ical and punctuational cues, followed by labelling
the relation holding between them (Marcu, 2000a;
Marcu, 2000b; Wolf and Gibson, 2005). If text is
not assumed to be divisible into discourse units, then
methods involve finding evidence for discourse re-
lations (including both explicit words and phrases,
and clausal and sentential adjacency) and their ar-
guments, and then labelling the sense of the iden-
tified relation (Elwell and Baldridge, 2008; Ghosh
et al, 2011; Lin et al, 2010; Lin, 2012; Prasad et
al., 2010a; Wellner, 2008; Wellner and Pustejovsky,
2007).
3.2 Resources for discourse structure
All automated systems for segmenting and labelling
text are grounded in data ? whether the data has
informed the manual creation of rules or has been
a source of features for an approach based on ma-
chine learning. In the case of topic structure and
high-level functional structure, there is now a sub-
stantial amount of data that is freely available. For
other types of discourse structure, manual annota-
tion has been required and, depending on the type of
structure, different amounts are currently available.
More specifically, work on topic structure and
segmentation has been able to take advantage of the
45
large, free, still-growing wikipedia, where articles
on similar topics tend to show similar explicit seg-
mentation into sub-topics. This is certainly the case
with the English wikipedia. If similar wikipedia
evolving in other languages lack explicit segmenta-
tion, it may be that cross-lingual techniques may be
able to project explicit segmentation from English-
language articles.
With respect to high-level functional structure,
some work on automated segmentation has been
able to exploit explicit author-provided indicators
of structure, such as the author-structured abstracts
now required by bio-medical journals indexed by
MedLine. Researchers have used these explicitly
structured abstracts to segment abstracts that lack
explicit structure (Chung, 2009; Guo et al, 2010;
Hirohata et al, 2008; Lin et al, 2006).
For all other kinds of discourse structures, ded-
icated manual annotation has been required, both
for segmentation and labelling, and many of these
resources have been made available for other re-
searchers. For fine-grained functional structure,
there is the ART corpus (Liakata et al, 2010)2.
For discourse relations annotated in the RST
framework, there is the RST Discourse TreeBank of
English text (Carlson et al, 2003), available through
the Linguistic Data Consortium (LDC), as well as
similarly annotated corpora in Spanish (da Cunha et
al., 2011), Portugese (Pardo et al, 2008) and Ger-
man (Stede, 2004).
For discourse relations annotated in the lexically-
grounded approach first described in (Webber and
Joshi, 1998), there is the Penn Discourse TreeBank
(Prasad et al, 2008) in English, as well as corpora
in Modern Standard Arabic (Al-Saif and Markert,
2010; Al-Saif and Markert, 2011), Chinese (Xue,
2005; Zhou and Xue, 2012), Czech (Mladova? et al,
2008), Danish (Buch-Kromann et al, 2009; Buch-
Kromann and Korzen, 2010), Dutch (van der Vliet
et al, 2011), Hindi (Oza et al, 2009), and Turk-
ish (Zeyrek and Webber, 2008; Zeyrek et al, 2009;
Zeyrek et al, 2010). Also available are discourse-
annotated journal articles in biomedicine (Prasad et
al., 2011) and discourse-annotated dialogue (Tonelli
et al, 2010).
2http://www.aber.ac.uk/en/cs/research/cb/projects/art/art-
corpus/
4 New challenges
Although the largely empirically-grounded, multi-
structure view of discourse addresses some of the
problems that previous computational approaches
encountered, it also reveals new ones, while leaving
some earlier problems still unaddressed.
4.1 Evidence for discourse structures
The first issue has to do with what should be taken as
evidence for a particular discourse structure. While
one could simply consider all features that can be
computed reliably and just identify the most accu-
rate predictors, this is both expensive and, in the end,
unsatisfying.
With topic structure, content words do seem to
provide compelling evidence for segmentation, ei-
ther using language models or semantic relatedness.
On the other hand, this might be improved through
further evidence in the form of entity chains, as ex-
plored earlier in (Kan et al, 1998), but using to-
day?s more accurate approaches to automated coref-
erence recognition (Strube, 2007; Charniak and El-
sner, 2009; Ng, 2010).
Whatever the genre, evidence for function struc-
ture seems to come from the frequency and distri-
bution of closed-class words, particular phrases (or
phrase patterns), and in the case of speech, into-
nation. So, for example, Niekrasz (2012) shows
that what he calls participant-relational features
that indicate the participants relationships to the
text provide convincing evidence for segmenting
oral narrative by the type of narrative activity tak-
ing place. These features include the distribution
and frequency of first and second person pronouns,
tense, and intonation. But much work remains to
be done in this area, in establishing what provides
reliable evidence within a genre and what evidence
might be stable across genres.
Evidence for discourse relations is what we have
given significant thought to, as the Penn Discourse
TreeBank (Prasad et al, 2008) and related corpora
mentioned in Section 3.2 aim to ground each in-
stance of a discourse relation in the evidence that
supports it. The issue of evidence is especially
important because none of these corpora has yet
been completely annotated with discourse relations.
Completing the annotation and developing robust
46
automated segmentation techniques requires iden-
tifying what elements of the language provide evi-
dence for coherence relations, and under what con-
ditions.
The two main types of evidence for discourse re-
lations in English are the presence of a discourse
connective and sentence adjacency. Discourse con-
nectives annotated in the PDTB 2.0 come from
a list of subordinating and coordinating conjunc-
tions, and discourse adverbials ? a subset of those
identified by Forbes-Riley et al(2006). Subse-
quently, Prasad et al (2010b) used Callison-Burch?s
technique for identifying syntax-constrained para-
phrases (Callison-Burch, 2008) to identify addi-
tional discourse connectives, some of which don?t
appear in the PDTB corpus and some of which
appear in the corpus but were not identified and
annotated as discourse connectives. English isn?t
alone in lacking a complete list of discourse con-
nectives: While German has the massive Hand-
buch de deutschen Konnektoren (Pasch et al, 2003),
even this resource has been found to be incomplete
through clever application of automated tagging and
word-alignment of parallel corpora (Versley, 2010).
Evidence for discourse relations in the PDTB also
comes from lexical or phrasal elements that are out-
side the initial set of conjunctions and discourse ad-
verbials. This evidence has been called alternative
lexicalization or AltLex (Prasad et al, 2010b), and
includes (in English) clause-initial what?s more (Ex-
ample 6) and that means (Example 7).
(6) A search party soon found the unscathed air-
craft in a forest clearing much too small to have
allowed a conventional landing. What?s more,
the seven mail personnel aboard were missing.
[wsj 0550]
(7) The two companies each produce market
pulp, containerboard and white paper. That
means goods could be manufactured closer
to customers, saving shipping costs, he said.
[wsj 0317]
The discovery of these other forms of evidence3
raises the question of when it is that a word or phrase
signals a discourse relation. For example, only 15 of
the 33 tokens of that means in the PDTB were anno-
tated as evidence of a discourse relation. While the
3which English is not alone in having, cf. (Rysova, 2012)
three paragraph-initial instances were left unanno-
tated due to resource limitations (ie, no paragraph
initial sentences were annotated unless they con-
tained an explicit discourse connective), the major-
ity were ignored because they followed an explicit
connective.
As Wiebe?s example (5) showed, there can be
multiple explicit discourse connectives in a clause,
each of which is evidence for a separate discourse re-
lation (albeit possibly between the same arguments).
All of these are annotated in the PDTB ? eg, both but
and then in
(8) Congress would have 20 days to reject the
package with a 50% majority, but then a Presi-
dent could veto that rejection. [wsj 1698]
The question is whether an AltLex in the context of
an explicit connective also provides evidence of a
distinct discourse relation ? for example, with the
conjunction with But in
(9) At a yearling sale, a buyer can go solo and get
a horse for a few thousand dollars. But that
means paying the horse?s maintenance; on av-
erage, it costs $25,000 a year to raise a horse.
[wsj 1174]
As noted above, the PDTB 2.0 also admits sen-
tence adjacency as evidence for one, or even two,
implicit discourse relations, as in
(10) And some investors fault Mr. Spiegel?s life
style; [Implicit = because, for instance] he
earns millions of dollars a year and flies
around in Columbia?s jet planes. [wsj 0179]
Here, the implicit token of because is associ-
ated with a discourse relation labelled CONTIN-
GENCY.CAUSE.REASON, while the implicit token
of for instance is associated with one labelled EX-
PANSION.RESTATEMENT.SPECIFICATION.
The question is whether sentence adjacency could
also serve as evidence for a distinct discourse rela-
tion, even when there is also an explicit discourse
adverbial, as in the following three instances of in-
stead. Here, Ex. 11 can be paraphrased as And in-
stead, Ex. 12 as But instead, and Ex.13 as So in-
stead.
(11) But many banks are turning away from strict
price competition. Instead, they are trying to
47
build customer loyalty by bundling their ser-
vices into packages and targeting them to small
segments of the population. [wsj 0085]
(12) The tension was evident on Wednesday
evening during Mr. Nixon?s final banquet
toast, normally an opportunity for reciting plat-
itudes about eternal friendship. Instead, Mr.
Nixon reminded his host, Chinese President
Yang Shangkun, that Americans haven?t for-
given China?s leaders for the military assault
of June 3-4 that killed hundreds, and perhaps
thousands, of demonstrators. [wsj 0093]
(13) Since stars are considerably more massive than
planets, such wobbles are small and hard to
see directly. Instead, Dr Marcy and others like
him look for changes that the wobbles cause in
the wavelength of the light from the star. [The
Economist, 10 November 2007]
These examples suggest that the presence of an
explicit connective should not, in all cases, be con-
sidered evidence for the absense of an implicit con-
nective. Once the set of explicit connectives have
been identified that can co-occur with each other
(including for example and for instance, as well as
instead), automated parsers for coherence relations
can be made to consider the presence of an implicit
connective whenever one of these is seen.
4.2 Variability in discourse annotation
Another issue relates to variability in annotating dis-
course structure: Inter-annotator agreement can be
very low in annotating pragmatic and discourse-
related phenomena. While we will illustrate the
point here in terms of annotating coherence rela-
tions, for other examples, the general point is illus-
trated in papers from the DGfS Workshop on Beyond
Semantics4 and in an upcoming special issue of the
journal Discourse and Dialogue devoted to the same
topic.
The Penn Wall Street Journal corpus contains
twenty-four (24) reports of errata in previously-
appearing articles. Twenty-three (23) consist of a
single pair of sentences, with no explicit discourse
connective signalling the relation between them.5
4http://www.linguistics.ruhr-uni-bochum.de/beyondsem/
5The other report contains three sentences, again with no
explicit connectives.
One sentence reports the error, and the other, the cor-
rect statement ? e.g.
(14) VIACOM Inc.?s loss narrowed to $21.7 million
in the third quarter from $56.9 million a year
ago. Thursday?s edition misstated the narrow-
ing. [wsj 1747]
In twenty of the errata (class C1), the correct
statement is given in the first sentence and the er-
ror, in the second; In the other three (class C2), it is
the other way around. One might think that the two
sentences in the twenty C1 reports would be anno-
tated as having the same discourse relation holding
between them, and the same with the two sentences
in the three C2 reports. But that is not the case: The
twenty C1 reports presented to annotators at differ-
ent times ended up being labelled with six different
discourse relations. There was even variability in
labelling the three members of the C2 class: They
were labelled with one discourse relation, and one
with a completely different one.
What should one conclude from this variability?
One possibility is that there is one right answer,
and annotators just vary in their ability to iden-
tify it. This would mean it would be beneficial to
have a large troop of annotators (so that the major-
ity view could prevail). Another possibility is that
there is more than one right answer, which would
imply multi-label classification so that multiple la-
bels could hold to different degrees. A third possi-
bility reflects the view from Beyond Semantics that
it is often very hard to transfer results from theoreti-
cal linguistics based on toy examples to naturally-
occurring texts. In this case, variability is a con-
sequence of the still exploratory nature of much
discourse annotation. In the case of errata, while
clearly some relation holds between the pair of sen-
tences, it may actually not be any of those used in
annotating the PDTB. That is, as Grosz and Sidner
(1986b) argued several years ago, the sentences may
only be related by their communicative intentions ?
one sentence intended to draw the reader?s attention
to the specific error that was made (so that the reader
knows what was mis-stated), the other intended to
correct it. One might then take the sense annotation
of discourse relations as still exploratory in the wide
range of corpora being annotated with this informa-
tion (cf. Section 3.2).
48
4.3 Systematic relations between discourse
structures
Fortunately for approaches to automated discourse
structure recognition, the lack of isomorphism be-
tween different discourse structures does not neces-
sarily mean that they are completely independent.
This belief that different aspects of discourse would
be related, is what led Grosz and Sidner (1986b) to
propose a theory that linked what they called the in-
tentional structure of discourse, with its linguistic
structure and with the reader or listener?s cognitive
attentional structure.
With respect to the different types of discourse
structure considered here, (Lin, 2012) has consid-
ered the possibility of systematic relations between
Teufel?s Argumentative Zone labelling of scientific
texts in a corpus developed for her PhD thesis
(Teufel, 1999) and PDTB-style discourse relations,
both within and across sentences. This is certainly
worth additional study, for the value it can bring to
automated methods of discourse structure recogni-
tion.
4.4 Intentional structure
When computational discourse processing turned
to machine learning methods based on reliably-
identifiable features, it abandoned (at least temporar-
ily) the centrality of pragmatics and speaker in-
tentions to discourse. That is, there were few or
no features that directly indicated or could serve
as reliable proxies for what role speaker intended
his/her utterance to play in the larger discourse. But
both Niekrasz? work on meeting segmentation (Sec-
tion 4.1) and the discussion in Section 4.2 of errata
and variability in their labelling draws new attention
to this old question, and not just to Moore and Pol-
lack?s observation (Section 3) that intentional and
informational characterizations may confer differ-
ent, non-isomorphic structures over a text. It may
also be the case that neither structure may provide a
complete cover: A new visit is warranted.
4.5 Discourse and inference
Not only were intentions abandoned in the move
to data-intensive methods, so was inference and is-
sues of how readers and listeners recover informa-
tion that isn?t explicit. What?s missing can be an
unmentioned event, with classic examples coming
from the restaurant script (Lehnert, 1977), where
someone enters a restaurant, sits down at a ta-
ble and gives their order to a waiter, where un-
mentioned inter alia is an event in which the per-
son becomes informed of what items the restaurant
has to offer, say through being given a menu. Or
it can be an unmentioned fact, such as that pro-
gram trading involves the computer-executed trad-
ing of a basket of fifteen or more stocks. The
latter explains the annotation of an implicit EX-
PANSION.RESTATEMENT.GENERALIZATION rela-
tion between the two sentences in
(15) ?You?re not going to stop the idea of trading a
basket of stocks,? says Vanderbilt?s Prof. Stoll.
?Program trading is here to stay, and computers
are here to stay, and we just need to understand
it.? [wsj 0118]
The problem here with inference is when labelling
an implicit coherence relation requires inferred in-
formation about its arguments, those arguments may
have quite different features than when all the infor-
mation needed to label the relation is explicit.
5 Conclusion
There are still large challenges ahead for compu-
tational discourse modelling. But we are hopeful
that greater openness to how information is con-
veyed through discourse, as well as richer modelling
techniques developed for other problems, will allow
needed progress to be made. If we can improve sys-
tem performance in recognizing the roles that utter-
ances are meant to play in discourse in one genre,
perhaps it will help us generalize and transport this
intention recognition between genres. We also hope
for progress in finding more ways to take advantage
of unannotated data in discourse research; in un-
derstanding more about inter-dependencies between
features of different types of discourse structure; in
continuing to carry out related computational dis-
course research and development in multiple lan-
guages and genres, so as to widen the access to the
knowledge gained; and in exploiting discourse in
Language Technology applications, including infor-
mation extraction and SMT.
49
References
Shashank Agarwal and Hong Yu. 2009. Automati-
cally classifying sentences in full-text biomedical arti-
cles into introduction, methods, results and discussion.
Bioinformatics, 25(23):3174?3180.
Amal Al-Saif and Katja Markert. 2010. The Leeds Ara-
bic Discourse Treebank: Annotating discourse con-
nectives for Arabic. In Proceedings, 7th International
Conference on Language Resources and Evaluation
(LREC 2010).
Amal Al-Saif and Katja Markert. 2011. Modelling dis-
course relations for Arabic. In Proceedings, Empirical
Methods in Natural Language Processing, pages 736?
747.
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge UK.
Yves Bestgen. 2006. Improving text segmentation us-
ing Latent Semantic Analysis: A reanalysis of Choi,
Wiemer-Hastings, and Moore (2001). Computational
Linguistics, 32(1):5?12.
Floris Bex and Bart Verheij. 2010. Story schemes for
argumentation about the facts of a crime. In Proceed-
ings, AAAI Fall Symposium on Computational Narra-
tives, Menlo Park CA. AAAI Press.
Susan E. Brennan, Marilyn Walker Friedman, and Carl J.
Pollard. 1987. A centering approach to pronouns. In
Proceedings of the 25th Annual Meeting, Association
for Computational Linguistics, pages 155?162, Stan-
ford University, Stanford CA.
Matthias Buch-Kromann and I?rn Korzen. 2010. The
unified annotation of syntax and discourse in the
Copenhagen Dependency Treebanks. In Proceedings
of the Fourth Linguistic Annotation Workshop, pages
127?131, July.
Matthias Buch-Kromann, I?rn Korzen, and Henrik H?eg
Mu?ller. 2009. Uncovering the ?lost? structure of
translations with parallel treebanks. In Fabio Alves,
Susanne Go?pferich, and Inger Mees, editors, Copen-
hagen Studies of Language: Methodology, Technol-
ogy and Innovation in Translation Process Research,
Copenhagen Studies of Language, vol. 38, pages 199?
224. Copenhagen Business School.
Jill Burstein and Martin Chodorow. 2010. Progress and
new directions in technology for automated essay eval-
uation. In R Kaplan, editor, The Oxford Handbook of
Applied Linguistics, pages 487?497. Oxford Univer-
sity Press, 2 edition.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the WRITE stuff: Automatic identification of
discourse structure in student essays. IEEE Intelligent
Systems: Special Issue on Advances in Natural Lan-
guage Processing, 18:32?39.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 249?256, Trento, Italy.
Chris Callison-Burch. 2008. Syntactic constraints
on paraphrases extracted from parallel corpora. In
EMNLP ?08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In J. van
Kuppevelt & R. Smith, editor, Current Directions in
Discourse and Dialogue. Kluwer, New York.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings, Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 789?797.
Eugene Charniak and Micha Elsner. 2009. Em works
for pronoun anaphora resolution. In Proc. European
Chapter of the Association for Computational Linguis-
tics.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David Karger. 2009. Global models of document
structure using latent permutations. In Proceedings,
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), pages 371?379.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for text seg-
mentation. In EMNLP ?01: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 109?117.
Grace Chung. 2009. Sentence retrieval for abstracts of
randomized controlled trials. BMC Medical Informat-
ics and Decision Making, 10(9), February.
Iria da Cunha, Juan-Manuel Torres-Moreno, and Gerardo
Sierra. 2011. On the development of the rst spanish
treebank. In Proc. 5th Linguistic Annotation Work-
shop, pages 1?10, Portland OR.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings, Conference on Empirical Methods in
Natural Language Processing, pages 294?303.
James Eales, Robert Stevens, and David Robertson.
2008. Full-text mining: Linking practice, protocols
and articles in biological research. In Proceedings of
the BioLink SIG, ISMB 2008.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In EMNLP ?08:
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 334?343.
50
Robert Elwell and Jason Baldridge. 2008. Dis-
course connective argument identication with connec-
tive specic rankers. In Proceedings of the IEEE Con-
ference on Semantic Computing (ICSC-08).
Mark Finlayson. 2009. Deriving narrative morphologies
via analogical story merging. In Proceedings, 2nd In-
ternational Conference on Analogy, pages 127?136.
Katherine Forbes-Riley, Bonnie Webber, and Aravind
Joshi. 2006. Computing discourse semantics: The
predicate-argument semantics of discourse connec-
tives in D-LTAG. Journal of Semantics, 23:55?106.
George Foster, Pierre Isabelle, and Roland Kuhn. 2010.
Translating structured documents. In Proceedings of
AMTA.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of the 41st
Annual Conference of the Association for Computa-
tional Linguistics.
Sucheta Ghosh, Sara Tonelli, Giuseppe Riccardi, and
Richard Johansson. 2011. End-to-end discourse
parser evaluation. In Proceedings, IEEE Conference
on Semantic Computing (ICSC-11).
Barbara Grosz and Candace Sidner. 1986a. Attention,
intention and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Barbara Grosz and Candace Sidner. 1986b. Attention,
intention and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Barbara Grosz and Candace Sidner. 1990. Plans for dis-
course. In Philip Cohen, Jerry Morgan, and Martha
Pollack, editors, Intentions in Communication, pages
417?444. MIT Press.
Barbara Grosz, Aravind Joshi, and Scott Weinstein.
1986. Towards a computational theory of dis-
course interpretation. Widely circulated unpublished
manuscript.
Barbara Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins,
Lin Sun, and Ulla Stenius. 2010. Identifying the infor-
mation structure of scientific abstracts. In Proceedings
of the 2010 BioNLP Workshop, July.
Marti Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Kenji Hirohata, Naoki Okazaki, Sophia Ananiadou, and
Mitsuru Ishizuka. 2008. Identifying sections in scien-
tic abstracts using conditional random fields. In Pro-
ceedings of the 3rd International Joint Conference on
Natural Language Processing, pages 381?388.
W. Lewis Johnson and Jeff Rickel. 2000. Animated ped-
agogical agents: Face-to-face interaction in interactive
learning environments. Int?l J. Artificial Intelligence
in Education, 11:47?78.
Dan Jurafsky and James Martin. 2009. Speech and Lan-
guage Processing. Prentice-Hall, Englewood Cliffs
NJ, 2 edition.
Min-Yen Kan, Judith Klavans, and Kathleen McKeown.
1998. Linear segmentation and segment significance.
In Proceedings of the Sixth Workshop on Very Large
Corpora.
Andrew Kehler. 1997. Current theories of centering for
pronoun intepretation: A critical evaluation. Compu-
tational Linguistics, 23(3):467?475.
Rodger Kibble and Richard Power. 2000. An integrated
framework for text planning and pronominalisation. In
Proc. of the First International Conference on Natural
Language Generation, pages 77?84, Mitzpe Ramon,
Israel, June.
Su Nam Kim, David Martinez, and Lawrence Cavedon.
2010. Automatic classification of sentences for evi-
dence based medicine. In Proc. ACM 4th Int?l Work-
shop on Data and Text Mining in Biomedical informat-
ics, pages 13?22.
Alistair Knott, Jon Oberlander, Mick O?Donnell, and
Chris Mellish. 2001. Beyond elaboration: The
interaction of relations and focus in coherent text.
In T Sanders, J Schilperoord, and W Spooren, ed-
itors, Text Representation:Linguistic and psycholin-
guistic aspects, pages 181?196. John Benjamins Pub-
lishing.
Wendy Lehnert. 1977. A conceptual theory of question
answering. In Proc 5th International Joint Conference
on Artificial Intelligence, pages 158?164.
Maria Liakata, Simone Teufel, Advaith Siddharthan, and
Colin Batchelor. 2010. Corpora for the conceptuali-
sation and zoning of scientific papers. In Proceedings
of the 7th International Conference on Language Re-
sources and Evaluation (LREC 2010).
Jimmy Lin, Damianos Karakos, Dina Demner-Fushman,
and Sanjeev Khudanpur. 2006. Generative content
models for structural analysis of medical abstracts. In
Proceedings of the HLT-NAACL Workshop on BioNLP,
pages 65?72.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan.
2010. A PDTB-styled end-to-end discourse
parser. Technical report, Department of Com-
puting, National University of Singapore, November.
http://arxiv.org/abs/1011.0835.
Ziheng Lin. 2012. Discourse Parsing: Inferring Dis-
course Structure, Modelling Coherence, and its Appli-
cations. Ph.D. thesis, National University of Singa-
pore.
51
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics.
Jean Mandler. 1984. Stories, scripts, and scenes: As-
pects of schema theory. Lawrence Erlbaum Asso-
ciates, Hillsdale NJ.
William Mann and Sandra Thompson. 1988. Rhetorical
Structure Theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 2000a. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395?448.
Daniel Marcu. 2000b. The theory and practice of dis-
course parsing and summarization. MIT Press.
Mstislav Maslennikov and Tat-Seng Chua. 2007. A
multi-resolution framework for information extraction
from free text. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 592?599. Association for Computational
Linguistics.
Larry McKnight and Padmini Srinivasan. 2003. Cate-
gorization of sentence types in medical abstracts. In
Proceedings of the AMIA Annual Symposium, pages
440?444.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Journal
of Medical Informatics, 75:468487.
Lucie Mladova?, S?a?rka Zika?nova?, and Eva Hajic?ova?.
2008. From sentence to discourse: Building an an-
notation scheme for discourse based on the Prague De-
pendency Treebank. In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2008).
Johanna Moore and Martha Pollack. 1992. A problem
for RST: The need for multi-level discouse analysis.
Computational Linguistics, 18(4):537?544.
Johanna Moore. 1995. Participating in Explanatory Di-
alogues. MIT Press, Cambridge MA.
Megan Moser and Johanna Moore. 1996. Toward a syn-
thesis of two accounts of discourse structure. Compu-
tational Linguistics, 22(3):409?419.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first 15 years. In Proc. 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1396?1411, Uppsala, Sweden.
John Niekrasz. 2012. Toward Summarization of Com-
municative Activities in Spoken Conversation. Ph.D.
thesis, University of Edinburgh.
Umangi Oza, Rashmi Prasad, Sudheer Kolachina,
Dipti Misra Sharma, and Aravind Joshi. 2009. The
hindi discourse relation bank. In Proc. 3rd ACL Lan-
guage Annotation Workshop (LAW III), Singapore, Au-
gust.
Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: the detection, classifi-
cation and structure of arguments in text. In Proc. 12th
International Conference on Artificial Intelligence and
Law, ICAIL ?09, pages 98?107. ACM.
Martha Palmer, Carl Weir, Rebecca Passonneau, and Tim
Finin. 1993. The kernal text understanding system.
Artificial Intelligence, 63:17?68.
Thiago Alexandre Salgueiro Pardo, Maria das Gracas
Volpe Nunes, and Lucia Helena Machado Rino. 2008.
Dizer: An automatic discourse analyzer for brazilian
portuguese. Lecture Notes in Artificial Intelligence,
3171:224?234.
Renate Pasch, Ursula Brausse, Eva Breindl, and Ulrich
Wassner. 2003. Handbuch der deutschen Konnek-
toren. Walter de Gruyter, Berlin.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
information extraction with semantic affinity patterns
and relevant regions. In Proceedings of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-07).
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proc., 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 544?554, Uppsala, Sweden.
Massimo Poesio, Rosemary Stevenson, Barbara Di Eu-
genio, and Janet Hitzeman. 2004. Centering: A para-
metric theory and its instantiations. Computational
Linguistics, 30(3):300?363.
Livia Polanyi and Martin H. van den Berg. 1996.
Discourse structure and discourse interpretation. In
P. Dekker and M. Stokhof, editors, Proceedings of the
Tenth Amsterdam Colloquium, pages 113?131, Uni-
versity of Amsterdam.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, and et al 2008. The Penn Discourse Treebank
2.0. In Proceedings of the 6th International Confer-
ence on Language Resources and Evaluation.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010a. Exploiting scope for shallow discourse pars-
ing. In Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation (LREC
2010).
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010b. Realization of discourse relations by other
means: Alternative lexicalizations. In Proceed-
ings, International Conf. on Computational Linguis-
tics (COLING).
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The Biomedical Discourse
52
Relation Bank. BMC Bioinformatics, 12(188):18
pages. http://www.biomedcentral.com/1471-
2015/12/188.
Matthew Purver, Tom Griffiths, K.P. Ko?rding, and Joshua
Tenenbaum. 2006. Unsupervised topic modelling for
multi-party spoken discourse. In Proceedings, Inter-
national Conf. on Computational Linguistics (COL-
ING) and the Annual Meeting of the Association for
Computational Linguistics, pages 17?24.
Matthew Purver. 2011. Topic segmentation. In Gokhan
Tur and Renato de Mori, editors, Spoken Language
Understanding: Systems for Extracting Semantic In-
formation from Speech. Wiley, Hoboken NJ.
Patrick Ruch, Celia Boyer, Christine Chichester, Imad
Tbahriti, Antoine Geissbu?hler, Paul Fabry, and et al
2007. Using argumentation to extract key sentences
from biomedical abstracts. International Journal of
Medical Informatics, 76(2?3):195?200.
David Rumelhart. 1975. Notes on a schema for stories.
In Dan Bobrow and Alan Collins, editors, Representa-
tion and Understanding: Studies in Cognitive Science.
Academic Press, New York.
Magdalena Rysova. 2012. Alternative lexicalizations of
discourse connectives in czech. In Proc. 8th Int?l Conf.
Language Resources and Evaluation (LREC 2012).
Roger Schank and Robert Abelson. 1977. Scripts, Plans,
Goals and Understanding: an Inquiry into Human
Knowledge Structures. Lawrence Erlbaum, Hillsdale
NJ.
Penni Sibun. 1992. Generating text without trees. Com-
putational Intelligence, 8(1):102?122.
Manfred Stede. 2004. The Potsdam Commentary Cor-
pus. In ACL Workshop on Discourse Annotation,
Barcelona, Spain, July.
Manfred Stede. 2008. RST revisted: Disentangling nu-
clearity. In Cathrine Fabricius-Hansen and Wiebke
Ramm, editors, ?Subordination? versus ?Coordination?
in Sentence and Text, pages 33?59. John Benjamins,
Amsterdam.
Manfred Stede. 2012. Discourse Processing. Morgan &
Claypool Publishers.
Michael Strube. 2007. Corpus-based and ma-
chine learning approaches to anaphora resolution.
In Monika Schwarz-Friesel, Manfred Consten, and
Mareile Knees, editors, Anaphors in Text: Cognitive,
formal and applied approaches to anaphoric refer-
ence, pages 207?222. John Benjamins Publishing.
Joel Tetreault. 2001. A corpus-based evaluation of cen-
tering and pronoun resolution. Computational Lin-
guistics, 27(4):507?520.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles - experiments with relevance and
rhetorical status. Computational Linguistics, 28:409?
445.
Simone Teufel, Advaith Siddharthan, and Colin Batche-
lor. 2009. Towards discipline-independent argumen-
tative zoning: evidence from chemistry and compu-
tational linguistics. In Proceedings, Conference on
Empirical Methods in Natural Language Processing,
pages 1493?1502.
Simone Teufel. 1999. Argumentative Zoning: Informa-
tion Extraction from Scientific Text. Ph.D. thesis, Uni-
versity of Edinburgh.
Gian Lorenzo Thione, Martin van den Berg, Livia
Polanyi, and Chris Culy. 2004. Hybrid text summa-
rization: combining external relevance measures with
structural analysis. In Proceedings of the ACL 2004
Workshop Text Summarization Branches Out.
Sara Tonelli, Guiseppe Riccardi, Rashmi Prasad, and
Aravind Joshi. 2010. Annotation of discourse re-
lations for conversational spoken dialogs. In Proc.
7th Int?l Conf. Language Resources and Evaluation
(LREC 2010).
Nynke van der Vliet, Ildiko? Berzla?novich, Gosse Bouma,
Markus Egg, and Gisela Redeker. 2011. Building a
discourse-annotated Dutch text corpus. In Bochumer
Linguistische Arbeitsberichte, pages 157?171.
Steven Vere and Timothy Bickmore. 1990. A basic
agent. Computational Intelligence, 6(1):41?60.
Yannick Versley. 2010. Discovery of ambiguous and un-
ambiguous discourse connectives via annotation pro-
jection. In Workshop on the Annotation and Exploita-
tion of Parallel Corpora (AEPC). NODALIDA.
Marilyn Walker, Aravind Joshi, and Ellen Prince. 1997.
Centering in Discourse. Oxford University Press, Ox-
ford, England.
Bonnie Webber and Aravind Joshi. 1998. Anchor-
ing a lexicalized tree-adjoining grammar for discourse.
In Coling/ACL Workshop on Discourse Relations and
Discourse Markers, pages 86?92, Montreal, Canada.
Bonnie Webber, Markus Egg, and Valia Kordoni. 2012.
Discourse structure and language technology. Natural
Language Engineering.
Ben Wellner and James Pustejovsky. 2007. Automati-
cally identifying the arguments of discourse connec-
tives. In Proceedings of the 2007 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-07).
Ben Wellner. 2008. Sequence Models and Ranking
Methods for Discourse Parsing. Ph.D. thesis, Bran-
deis University.
Janyce Wiebe. 1993. Issues in linguistic segmentation.
In Workshop on Intentionality and Structure in Dis-
course Relations, Association for Computational Lin-
guistics, pages 148?151, Ohio StateUniversity.
Terry Winograd. 1973. A procedural model of language
understanding. In Roger Schank and Ken Colby, ed-
itors, Computer Models of Thought and Language,
53
pages 152?186. W.H. Freeman. Reprinted in Grosz
et al (eds), Readings in Natural Language Processing.
Los Altos CA: Morgan Kaufmann Publishers, 1986,
pp.249-266.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31:249?287.
William Woods. 1968. Procedural semantics for a
question-answering machine. In Proceedings of the
AFIPS National Computer Conference, pages 457?
471, Montvale NJ. AFIPS Press.
William Woods. 1978. Semantics and quantification in
natural language question answering. In Advances in
Computers, volume 17, pages 1?87. Academic Press,
New York.
Nianwen Xue. 2005. Annotating discourse connectives
in the chinese treebank. In ACL Workshop on Frontiers
in Corpus Annotation II, Ann Arbor MI.
Deniz Zeyrek and Bonnie Webber. 2008. A discourse re-
source for Turkish: Annotating discourse connectives
in the METU corpus. In Proceedings of the 6th Work-
shop on Asian Language Resources (ALR6).
Deniz Zeyrek, U?mut Deniz Turan, Cem Bozsahin, Ruket
C?ak?c?, and et al 2009. Annotating Subordinators in
the Turkish Discourse Bank. In Proceedings of the 3rd
Linguistic Annotation Workshop (LAW III).
Deniz Zeyrek, Is??n Demirs?ahin, Ay?s??g?? Sevdik-
C?all?, Hale O?gel Balaban, I?hsan Yalc??nkaya, and
U?mut Deniz Turan. 2010. The annotation scheme
of the Turkish Discourse Bank and an evaluation of
inconsistent annotations. In Proceedings of the 4th
Linguistic Annotation Workshop (LAW III).
Yuping Zhou and Nianwen Xue. 2012. Pdtb-style dis-
course annotation of chinese text. In Proc. 50th An-
nual Meeting of the ACL, Jeju Island, Korea.
54
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 19?26,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Implicitation of Discourse Connectives in (Machine) Translation
Thomas Meyer
Idiap Research Institute and EPFL
Martigny and Lausanne, Switzerland
thomas.meyer@idiap.ch
Bonnie Webber
University of Edinburgh
Edinburgh, UK
bonnie@inf.ed.ac.uk
Abstract
Explicit discourse connectives in a source
language text are not always translated to
comparable words or phrases in the tar-
get language. The paper provides a corpus
analysis and a method for semi-automatic
detection of such cases. Results show
that discourse connectives are not trans-
lated into comparable forms (or even any
form at all), in up to 18% of human refer-
ence translations from English to French
or German. In machine translation, this
happens much less frequently (up to 8%
only). Work in progress aims to cap-
ture this natural implicitation of discourse
connectives in current statistical machine
translation models.
1 Introduction
Discourse connectives (DCs), a class of frequent
cohesive markers, such as although, however, for
example, in addition, since, while, yet, etc., are es-
pecially prone to ?translationese?, i.e. the use of
constructions in the target language (TL) that dif-
fer in frequency or position from how they would
be found in texts born in the language. That is,
?translationese? makes DCs prone to being trans-
lated in ways that can differ markedly from their
use in the source language. (Blum-Kulka, 1986;
Cartoni et al, 2011; Ilisei et al, 2010; Halverson,
2004; Hansen-Schirra et al, 2007; Zufferey et al,
2012). For cohesive markers and DCs, Koppel and
Ordan (2011) and Cartoni et al (2011) have shown
that they may be more explicit (increased use) or
less explicit (decreased use) in translationese. The
paper focuses on the latter case, but the same de-
tection method can be applied in reverse, in order
to find increased use (explicitation) as well.
In English about 100 types of explicit DCs have
been annotated in the Penn Discourse TreeBank,
or PDTB (Prasad et al, 2008) (We say more about
this in Section 3.1). The actual set of markers or
connectives is however rather open-ended (Prasad
et al, 2010). DCs signal discourse relations that
connect two spans of text and can be ambiguous
with respect to the discourse relation they convey.
Moreover, the same DC can simultaneously con-
vey more than one discourse relation. For exam-
ple, while can convey contrast or temporality, or
both at the same time. On the other hand, dis-
course relations can also be conveyed implicitly,
without an explicit DC.
Human translators can chose to not translate a
SL DC with a TL DC, where the latter would be re-
dundant or where the SL discourse relation would
more naturally be conveyed in the TL by other
means (cf. Section 2). We will use the term ?zero-
translation? or ?implicitation? for a valid transla-
tion that conveys the same sense as a lexically ex-
plicit SL connective, but not with the same form.
As we will show, current SMT models either learn
the explicit lexicalization of a SL connective to
a TL connective, or treat the former as a ran-
dom variation, realizing it or not. Learning other
valid ways of conveying the same discourse rela-
tion might not only result in more fluent TL text,
but also help raise its BLEU score by more closely
resembling its more implicit human reference text.
The paper presents work in progress on a cor-
pus study where zero-translations of DCs have
been semi-automatically detected in human refer-
ence and machine translations from English (EN)
to French (FR) and German (DE) (Section 3).
Two types of discourse relations that are very fre-
quently omitted in FR and DE translations are
studied in detail and we outline features on how
these omissions could be modeled into current
SMT systems (Section 4).
19
2 Implicitation of connectives in
translation
Figure 1 is an extract from a news article in the
newstest2010 data set (see Section 3.2). It con-
tains two EN connectives ? as and otherwise ?
that were annotated in the PDTB1. Using the set of
discourse relations of the PDTB, as can be said to
signal the discourse relation CAUSE (subtype Rea-
son), and otherwise the discourse relation ALTER-
NATIVE. This is discussed further in Section 3.1.
EN: The man with the striking bald head was
still needing a chauffeur, 1. as the town was still
unknown to him. 2. Otherwise he could have
driven himself ? 3. after all, no alcohol was
involved and the 55-year-old was not drunk.
FR-REF: L?homme, dont le cra?ne chauve
attirait l?attention, se laissa conduire 1. 0
dans la ville qui lui e?tait encore e?trange`re. 2.
Autrement notre quinquage?naire aurait pu
prendre lui-me?me le volant ? 3. 0 il n?avait
pas bu d?alcool et il n?e?tait pas non plus ivre de
bonheur.
DE-REF: Der Mann mit der markanten
Glatze liess sich 1. wegen/Prep der ihm noch
fremden Stadt chauffieren. 2. Ansonsten ha?tte
er auch selbst fahren ko?nnen ? Alkohol war 3.
schliesslich/Adv nicht im Spiel, und besoffen
vor Glu?ck war der 55-ja?hrige genauso wenig.
Figure 1: Examples of EN source connectives
translated as zero or by other means in human ref-
erence translations.
The human reference translations do not trans-
late the first connective as explicitly. In FR there
is no direct equivalent, and the reason why the
man needed a driver is given with a relative clause:
...dans la ville qui... (lit.: in the town that was still
foreign to him). In DE as is realized by means of
a preposition, wegen (lit.: because of). The sec-
ond EN connective otherwise, maintains its form
in translation to the target connective autrement in
FR and ansonsten in DE.
On the other hand, baseline SMT systems for
1The excerpt contains a third possible connective after all
that was not annotated in the PDTB, and our data as a whole
contains other possible connectives not yet annotated there,
including given that and at the same time. We did not analyse
such possible connectives in the work described here.
EN/FR and EN/DE (Section 3.2) both translated
the two connectives as and otherwise explicitly by
the usual target connectives, in FR: comme, sinon
and in DE wie, sonst.
3 Semi-automatic detection of
zero-translations
3.1 Method
The semi-automatic method that identifies zero- or
non-connective translations in human references
and machine translation output is based on a list
of 48 EN DCs with a frequency above 20 in the
Penn Discourse TreeBank Version 2.0 (Prasad et
al., 2008). In order to identify which discourse re-
lations are most frequently translated as zero, we
have assigned each of the EN DCs the level-2 dis-
course relation that it is most frequently associated
with in the PDTB corpus. The total list of EN con-
nectives is given in Table 1.
For every source connective, we queried its
most frequent target connective translations from
the online dictionary Linguee2 and added them to
dictionaries of possible FR and DE equivalents.
With these dictionaries and Giza++ word align-
ment (Och and Ney, 2003), the SL connectives
can be located and the sentences of its transla-
tion (reference and/or automatic) can be scanned
for an aligned occurrence of the TL dictionary
entries. If more than one DC appears in the
source sentence and/or a DC is not aligned with a
connective or connective-equivalent found in the
dictionaries, the word position (word index) of
the SL connective is compared to the word in-
dexes of the translation in order to detect whether
a TL connective (or connective-equivalent from
the dictionaries) appears in a 5-word window to
its left and right.3. This also helps filtering out
cases of non-connective uses of e.g. separately
or once as adverbs. Finally, if no aligned entry
is present and the alignment information remains
empty, the method counts a zero-translation and
collects statistics on these occurrences.
After a first run where we only allowed for ac-
tual connectives as translation dictionary entries,
we manually looked through 400 cases for each,
FR and DE reference translations, that were output
2http://www.linguee.com
3The method extends on the ACT metric (Hajlaoui and
Popescu-Belis, 2013) that measures MT quality in terms of
connectives in order to detect more types of DCs and their
equivalents.
20
Figure 2: Percentage of zero-translations in newstest2010+2012 for EN/FR per discourse relation and
translation type: human reference (Ref) or MT output (MT).
as zero-translations (in the newtest2012 data, see
Section 3.2). We found up to 100 additional cases
that actually were not implicitations, but conveyed
the SL connective?s meaning by means of a para-
phrase, e.g. EN: if ? FR: dans le cas ou` (lit.: in
case where) ? DE: im Falle von (lit.: in case of).
For example, the EN connective otherwise ended
up with the dictionary entries in Figure 3.
EN: otherwise ALTERNATIVE :
FR: autrement|sinon|car|dans un autre
cas|d?une autre manie`re
DE: ansonsten|andernfalls|anderenfalls
|anderweitig|widrigenfalls|andrerseits|
andererseits|anders|sonst
Figure 3: Dictionary entries of FR and DE connec-
tives and equivalents for the EN connective other-
wise.
3.2 Data
For the experiments described here, we con-
catenated two data sets, the newstest2010 and
newstest2012 parallel texts as publicly available
by the Workshop on Machine Translation4. The
texts consist of complete articles from various
daily news papers that have been translated from
EN to FR, DE and other languages by translation
agencies.
In total, there are 5,492 sentences and 117,799
words in the SL texts, of which 2,906 are tokens
4http://www.statmt.org/wmt12/
of the 48 EN connectives. See Table 1 for the con-
nectives and their majority class, which aggregate
to the detailed statistics given in Table 2.
Rel. TC Rel. TC
Alternative 30 Conjunction 329
Asynchrony 588 Contrast 614
Cause 308 Instantiation 43
Concession 140 Restatement 14
Condition 159 Synchrony 681
Table 2: Total counts (TC) of English dis-
course connectives (2,906 tokens) from the
newstest2010+2012 corpora, whose majority
sense conveys one of the 10 PDTB level-2 dis-
course relations (Rel.) listed here.
To produce machine translations of the same
data sets we built EN/FR and EN/DE base-
line phrase-based SMT systems, by using the
Moses decoder (Koehn et al, 2007), with the Eu-
roparl corpus v7 (Koehn, 2005) as training and
newtest2011 as tuning data. The 3-gram language
model was built with IRSTLM (Federico et al,
2008) over Europarl and the rest of WMT?s news
data for FR and DE.
3.3 Results
In order to group the individual counts of zero-
translations per DC according to the discourse re-
lation they signal, we calculated the relative fre-
quency of zero-translations per relation as percent-
ages, see Figures 2 for EN/FR, and 4 for EN/DE.
21
Figure 4: Percentage of zero-translations in newstest2010+2012 for EN/DE per discourse relation and
translation type: human reference (Ref) or MT output (MT).
The total percentage of zero-translations in the ref-
erences and the baseline MT output is given in
Table 3.
A first observation is that an MT system seems
to produce zero-translations for DCs significantly
less often than human translators do. Human FR
translations seem to have a higher tendency to-
ward omitting connectives than the ones in DE.
Figures 2 and 4 also show that the discourse re-
lations that are most often rendered as zero are de-
pendent on the TL. In the FR reference transla-
tions, SYNCHRONY, ALTERNATIVE and CONCES-
SION account for most implicitations, while in the
DE reference translations, CONDITION, ALTER-
NATIVE and CONCESSION are most often left im-
plicit.
Translation Type C %
EN/FR Ref 508 17.5
MT 217 7.5
EN/DE Ref 392 13.5
MT 129 4.4
Table 3: Counts (C) and relative frequency (%)
of zero-translations for EN/FR and EN/DE in hu-
man references (Ref) and MT output (MT) over
newstest2010+2012.
The results are to some extent counterintuitive
as one would expect that semantically dense dis-
course relations like CONCESSION would need to
be explicit in translation in order to convey the
same meaning. Section 4 presents some non-
connective means available in the two TLs, by
which the discourse relations are still established.
We furthermore looked at the largest implicita-
tion differences per discourse relation in the hu-
man reference translations and the MT output. For
EN/FR for example, 13.8% of all CONDITION re-
lations are implicitated in the references, by mak-
ing use of paraphrases such as dans le moment
ou` (lit.: in the moment where) or dans votre cas
(lit.: in your case) in place of the EN connective
if. The MT system translates if in 99.4% of all
cases to the explicit FR connective si. Similarly,
for INSTANTIATION relations and the EN connec-
tive for instance in the references, the translators
made constrained use of verbal paraphrases such
as on y trouve (lit.: among which we find). MT on
the other hand outputs the explicit FR connective
par exemple in all cases of for instance.
For EN/DE, there is the extreme case, where
ALTERNATIVE relations are, in human reference
translations, quite often implicitated (in 23.3% of
all cases), whereas the MT system translates all
the instances explicitly to DE connectives: wenn
(unless), sonst (otherwise) and statt, stattdessen,
anstatt (instead). The translators however make
use of constructions with a sentence-initial verb in
conditional mood (cf. Section 4.2) for otherwise
and unless, but not for instead, which is, as with
MT, always explicitly translated by humans, most
often to the DE connective statt. The very op-
posite takes place for the RESTATEMENT relation
22
and the EN connective in fact. Here, MT leaves
implicit just as many instances as human transla-
tors do, i.e. 14.3% of all cases. Translators use
paraphrases such as in Wahrheit (lit.: in truth) or
u?brigens (lit.: by the way), while the translation
model tends to use im Gegenteil (lit.: opposite),
which is not a literal translation of in fact (usually
in der Tat or tatsa?chlich in DE), but reflects the
contrastive function this marker frequently had in
the Europarl training data of the baseline MT sys-
tem.
4 Case studies
4.1 Temporal connectives from EN to FR
The most frequent implicitated discourse relation
for EN/FR translation is SYNCHRONY, i.e. con-
nectives conveying that their arguments describe
events that take place at the same time. However,
since the situations in which SYNCHRONY rela-
tions are implicitated are similar to those in which
CONTRAST relations are implicitated, we discuss
the two together.
We exemplify here cases where EN DCs that
signal SYNCHRONY and/or CONTRAST are trans-
lated to FR with a ?en/Preposition + Verb in
Gerund? construction without a TL connective.
The EN source instances giving rise to such im-
plicitations in FR are usually of the form ?DC +
Verb in Present Continuous? or ?DC + Verb in Sim-
ple Past?, see sentences 1 and 2 in Figure 5.
Out of 13 cases of implicitations for while in the
data, 8 (61.5%) have been translated to the men-
tioned construction in FR, as illustrated in the first
example in Figure 5, with a reference and machine
translation from newstest2010. The DC while here
ambiguously signals SYNCHRONY and/or CON-
TRAST, but there is a second temporal marker
(at the same time, a connective-equivalent not yet
considered in this paper or in the PDTB), that dis-
ambiguates while to its CONTRAST sense only or
to the composite sense SYNCHRONY/CONTRAST.
The latter is conveyed in FR by en me?prisant, with
CONTRAST being reinforced by tout (lit.: all).
In Example 2, from newstest2012, the sentence-
initial connective when, again signaling SYN-
CHRONY, is translated to the very same construc-
tion of ?en/Preposition + Verb in Gerund? in the
FR reference.
In the baseline MT output for Example 1, nei-
ther of the two EN DCs is deleted, while is literally
translated to alors que and at the same time to dans
1. EN: In her view, the filmmaker ?is asking
a favour from the court, while at the same time
showing disregard for its authority?.
FR-REF: Pour elle, le cine?aste ?demande une
faveur a` la cour, tout en/Prep me?prisant/V/Ger
son autorite??.
FR-MT*: Dans son avis, le re?alisateur de
?demande une faveur de la cour, alors que dans
le me?me temps une marque de me?pris pour son
autorite??.
2. EN: When Meder looked through the
weather-beaten windows of the red, white and
yellow Art Nouveau building, she could see
weeds growing up through the tiles.
FR-REF: En/Prep jetant/V/Ger un coup
d??il par la fene?tre de l?immeuble-art nou-
veau en rouge-blanc-jaune, elle a observe?
l?e?panouissement des mauvaises herbes entre les
carreaux.
FR-MT*: Lorsque Meder semblait weather-
beaten a` travers les fene?tres du rouge, jaune et
blanc de l?art nouveau ba?timent, elle pourrait
voir les mauvaises herbes qui grandissent par les
tuiles.
Figure 5: Translation examples for the EN tempo-
ral connectives while and when, rendered in the FR
reference as a ?preposition + Verb in Gerund? con-
struction. MT generates the direct lexical equiva-
lents alors que and lorsque.
le me?me temps. While the MT output is not totally
wrong, it sounds disfluent, as dans le me?me temps
after alors que is neither necessary nor appropri-
ate.
In the baseline MT output for Example 2, the di-
rect lexical equivalent for when ? lorsque is gen-
erated, which is correct, although the translation
has other mistakes such as the wrong verb sem-
blait and the untranslated weather-beaten.
To model such cases for SMT one could use
POS tags to detect the ?DC + Present Continu-
ous/Simple Past? in EN and apply a rule to trans-
late it to ?Preposition + Gerund? in FR. Further-
more, when two DCs follow each other in EN,
and both can signal the same discourse relations,
a word-deletion feature (as it is available in the
Moses decoder via sparse features), could be used
to trigger the deletion of one of the EN connec-
tives, so that only one is translated to the TL. We
23
will examine in future work whether there are sys-
tematic patterns in the translation of such ?dou-
ble? connectives in SL and TL. Another possibility
would be to treat cases like while at the same time
as a multi-word phrase that is then translated to the
corresponding prepositional construction in FR.
4.2 Conditional connectives from EN to DE
Out of the 41 cases involving a CONDITION re-
lation (10.5% of all DE implicitations), 40 or
97.6% were due to the EN connective if not be-
ing translated to its DE equivalents wenn, falls,
ob. Instead, in 21 cases (52.5%), the human
reference translations made use of a verbal con-
struction which obviates the need for a connec-
tive in DE when the verb in the if -clause is
moved to sentence-initial position and its mood
is made conditional, as in Figure 6, a refer-
ence translation from newstest2012, with the DE
verb wa?re (lit.: were) (VMFIN=modal finite verb,
Konj=conditional). This construction is also avail-
able in EN (Were you here, I would...), but seems
to be much more formal and less frequent than in
DE where it is ordinarily used across registers. In
the baseline MT output for this sentence, if was
translated explicitly to the DE connective wenn,
which is in principle correct, but the syntax of the
translation is wrong, mainly due to the position of
the verb tun, which should be at the end of the sen-
tence.
The remaining 19 cases of EN if were either
translated to DE prepositions (e.g. bei, wo, lit.: at,
where) or the CONDITION relation is not expressed
at all and verbs in indicative mood make the use of
a conditional DE connective superfluous.
Of the 21 tokens of if whose reference transla-
tions used a verbal construction in DE, 14 (66.7%)
were tokens of if whose argument clause explic-
itly referred to the preceding context ? e.g., if they
were, if so, if this is true etc. These occurrences
could therefore be identified in EN and could be
modeled for SMT as re-ordering rules on the ver-
bal phrase in the DE syntax tree after constituent
parsing in syntax-based translation models.
5 Conclusion
This study showed that human translators do not
translate explicit EN discourse connectives as FR
or DE discourse connectives in up to 18% of all
cases. In MT output this happens about 3 times
less often. We thus plan to examine how to pro-
EN: If not for computer science, they would be
doing amazing things in other fields.
DE-REF: 0 Wa?re/VMFIN/Konj es
nicht die Computerbranche gewesen, wu?rden
sie in anderen Bereichen fantastische Dinge
schaffen.
DE-MT*: Wenn nicht fu?r die Informatik,
wu?rden sie tun, erstaunlich, Dinge auf anderen
Gebieten.
Figure 6: Translation example for the EN connec-
tive if, rendered in the DE reference as a construc-
tion with a sentence-initial verb in conditional
mood. MT generates the direct lexical equivalent
wenn.
duce higher-scoring translations without a target
language connective but with some other syntactic
pattern that conveys the same source language dis-
course relation. Depending on the features identi-
fied, movements of syntactical constituents or re-
ordering of POS tags at the phrase and/or sub-tree
level will be implemented for hierarchical syntac-
tic or phrase-based SMT models.
Acknowledgments
We are grateful to the Swiss National Science
Foundation (SNSF) for partially funding this work
and the research visit to Edinburgh with the
COMTIS Sinergia project, n. CRSI22 127510
(see www.idiap.ch/comtis/) and to the
three anonymous reviewers for their helpful com-
ments.
References
Shoshana Blum-Kulka. 1986. Shifts of Cohesion and
Coherence in Translation. In Juliane House and
Shoshana Blum-Kulka, editors, Interlingual and In-
tercultural Communication. Discourse and cogni-
tion in translation and second language acquisition,
pages 17?35. Narr Verlag, Tu?bingen, Germany.
Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, and
Andrei Popescu-Belis. 2011. How Comparable
are Parallel Corpora? Measuring the Distribution of
General Vocabulary and Connectives. In Proceed-
ings of 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), pages 78?86, Portland, OR.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an Open Source Toolkit for
24
Handling Large Scale Language Models. In Pro-
ceedings of Interspeech, Brisbane, Australia.
Najeh Hajlaoui and Andrei Popescu-Belis. 2013.
Assessing the Accuracy of Discourse Connective
Translations: Validation of an Automatic Met-
ric. In 14th International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLING), Samos, Greece.
Sandra Halverson. 2004. Connectives as a Translation
Problem. In H. et al (Eds.) Kittel, editor, Encyclo-
pedia of Translation Studies, pages 562?572. Walter
de Gruyter, Berlin/New York.
Silvia Hansen-Schirra, Stella Neumann, and Erich
Steiner. 2007. Cohesive Explicitness and Explicita-
tion in an English-German Translation Corpus. Lan-
guages in Contrast, 7:241?265.
Iustina Ilisei, Diana Inkpen, Gloria Pastor Corpas, and
Ruslan Mitkov. 2010. Identifcation of Transla-
tionese: A Machine Learning Approach. In A. Gel-
bukh, editor, Computational Linguistics and Intelli-
gent Text Processing Lecture Notes in Computer Sci-
ence. Springer-Verlag, Berlin, Heidelberg, Germany.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbs. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of 45th Annual Meeting of the
Association for Computational Linguistics (ACL),
Demonstration Session, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its Dialects. In Proceedings of ACL-HLT 2011
(49th Annual Meeting of the ACL: Human Language
Technologies, pages 1318?1326, Portland, OR.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of 6th International Conference on
Language Resources and Evaluation (LREC), pages
2961?2968, Marrakech, Morocco.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Realization of Discourse Relations by Other
Means: Alternative Lexicalizations. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics (COLING), pages 1023?1031,
Beijing, China.
Sandrine Zufferey, Liesbeth Degand, Andrei Popescu-
Belis, and Ted Sanders. 2012. Empirical Valida-
tions of Multilingual Annotation Schemes for Dis-
course Relations. In Proceedings of ISA-8 (8th
Workshop on Interoperable Semantic Annotation),
pages 77?84, Pisa, Italy.
25
EN conn. Majority rel. Tokens EN conn. Majority rel. Tokens
after Asynchrony 575/577 just as Synchrony 13/14
also Conjunction 1735/1746 later Asynchrony 90/91
although Contrast *157/328 meanwhile Synchrony 148/193
as Synchrony 543/743 moreover Conjunction 100/101
as a result Cause 78/78 nevertheless Concession *19/44
as if Concession *4/16 nonetheless Concession 17/27
as long as Condition 20/24 now that Cause 20/22
as soon as Asynchrony 11/20 once Asynchrony 78/84
because Cause 854/858 on the other hand Contrast 35/37
before Asynchrony 326/326 otherwise Alternative 22/24
but Contrast 2427/3308 previously Asynchrony 49/49
by contrast Contrast 27/27 separately Conjunction 73/74
even if Concession *41/83 since Cause 104/184
even though Concession 72/95 so that Cause 31/31
finally Asynchrony *14/32 still Concession 83/190
for example Instantiation 194/196 then Asynchrony 312/340
for instance Instantiation 98/98 therefore Cause 26/26
however Contrast 355/485 though Concession *156/320
if Condition 1127/1223 thus Cause 112/112
in addition Conjunction 165/165 unless Alternative 94/95
indeed Conjunction 54/104 until Asynchrony 140/162
in fact Restatement *39/82 when Synchrony 594/989
instead Alternative 109/112 while Contrast 455/781
in turn Asynchrony 20/30 yet Contrast 53/101
Table 1: English connectives with a frequency above 20 in the PDTB. Also listed are the level-2 majority
relations with the number of tokens out of the total tokens of the connective in the PDTB (counts includ-
ing the majority relation being part of a composite sense tag). *For some connectives there is no level-2
majority because some instances have only been annotated with level-1 senses. We did not consider the
connectives and and or (too many non-connective occurrences for automatic detection).
26
Proceedings of the SIGDIAL 2013 Conference, page 1,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Discourse Relations, Discourse Structure,
Discourse Semantics
Bonnie Webber
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, Scotland, UK
bonnie@inf.ed.ac.uk
It is generally accepted that a discourse connective expresses a semantic
and/or pragmatic relation between its matrix sentence or clause and some-
thing in the previous discourse. Usually the sense of this relation is expressed
as a label, often within a hierarchy of sense labels. But the meaning of these
labels may vary from system to system, and the same connective may be as-
signed different labels in different systems. Given this, we might learn more
and make better predictions if (i) sense labels were associated with (some
of) their entailments and (ii) connectives were characterized in terms of both
their formal properties and their use conditions. I?ll give examples of both.
The above-mentioned predictions tie in with an interesting property of
Penn Discourse TreeBank annotation. Annotators were allowed to assign
multiple sense labels to a single connective, to imply that all the senses held
simultaneously. For those cases where adjacent sentences lacked an inter-
vening connective, annotators were instructed to try to insert one or more
connectives that (together) expressed the relation(s) between the sentences.
Here too, in many cases, annotators inserted a single connective to which
they assigned multiple meanings, Other times they inserted multiple connec-
tives to convey the relation(s) they took as being expressed. Some of this
will be shown to make more sense in terms of the entailments and formal
properties of the connectives than in terms of any sense labels.
I?ll close by trying to distinguish discourse connectives that are associated
with coordinating or subordinating relations between sentences or clauses,
which is an feature of discourse structure, from those connectives that simply
convey additional relevant semantic or pragmatic content.
1
