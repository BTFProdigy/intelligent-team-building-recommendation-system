Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 947?954, Vancouver, October 2005. c?2005 Association for Computational Linguistics
SEARCHING THE AUDIO NOTEBOOK:
KEYWORD SEARCH IN RECORDED CONVERSATIONS
Peng Yu, Kaijiang Chen, Lie Lu, and Frank Seide
Microsoft Research Asia, 5F Beijing Sigma Center, 49 Zhichun Rd., 100080 Beijing, P.R.C.
{rogeryu,kaijchen,llu,fseide}@microsoft.com
Abstract
MIT?s Audio Notebook added great value to the
note-taking process by retaining audio record-
ings, e.g. during lectures or interviews. The key
was to provide users ways to quickly and easily
access portions of interest in a recording. Sev-
eral non-speech-recognition based techniques
were employed. In this paper we present a
system to search directly the audio record-
ings by key phrases. We have identified the
user requirements as accurate ranking of phrase
matches, domain independence, and reasonable
response time. We address these requirements
by a hybrid word/phoneme search in lattices,
and a supporting indexing scheme. We will in-
troduce the ranking criterion, a unified hybrid
posterior-lattice representation, and the index-
ing algorithm for hybrid lattices. We present
results for five different recording sets, includ-
ing meetings, telephone conversations, and in-
terviews. Our results show an average search
accuracy of 84%, which is dramatically better
than a direct search in speech recognition tran-
scripts (less than 40% search accuracy).
1 Introduction
Lisa Stifelman proposed in her thesis the idea of the
?Audio Notebook,? where audio recordings of lectures
and interviews are retained along with the notes (Stifel-
man, 1997). She has shown that the audio recordings are
valuable to users if portions of interest can be accessed
quickly and easily.
Stifelman explored various techniques for this, includ-
ing user-activity based techniques (most noteworthy time-
stamping notes so they can serve as an index into the
recording) and content-based ones (signal processing for
accelerated playback, ?snap-to-grid? (=phrase boundary)
based on prosodic cues). The latter are intended for sit-
uations where the former fail, e.g. when the user has no
time for taking notes, does not wish to pay attention to it,
or cannot keep up with complex subject matter, and as a
consequence the audio is left without index. In this pa-
per, we investigate technologies for searching the spoken
content of the audio recording.
Several approaches have been reported in the litera-
ture for the problem of indexing spoken words in au-
dio recordings. The TREC (Text REtrieval Conference)
Spoken-Document Retrieval (SDR) track has fostered re-
search on audio-retrieval of broadcast-news clips. Most
TREC benchmarking systems use broadcast-news recog-
nizers to generate approximate transcripts, and apply text-
based information retrieval to these. They achieve re-
trieval accuracy similar to using human reference tran-
scripts, and ad-hoc retrieval for broadcast news is consid-
ered a ?solved problem? (Garofolo, 2000). Noteworthy
are the rather low word-error rates (20%) in the TREC
evaluations, and that recognition errors did not lead to
catastrophic failures due to redundancy of news segments
and queries.
However, in our scenario, requirements are rather dif-
ferent. First, word-error rates are much higher (40-
60%). Directly searching such inaccurate speech recog-
nition transcripts suffers from a poor recall. Second, un-
like broadcast-news material, user recordings of conver-
sations will not be limited to a few specific domains. This
not only poses difficulties for obtaining domain-specific
training data, but also implies an unlimited vocabulary of
query phrases users want to use. Third, audio recordings
will accumulate. When the audio database grows to hun-
dreds or even thousands of hours, a reasonable response
time is still needed.
A successful way to deal with high word error rates is
the use of recognition alternates (lattices). For example,
(Seide and Yu, 2004; Yu and Seide, 2004) reports a sub-
stantial 50% improvement of FOM (Figure Of Merit) for
a word-spotting task in voicemails. Improvements from
using lattices were also reported by (Saraclar and Sproat,
2004) and (Chelba and Acero, 2005).
To address the problem of domain independence, a
subword-based approach is needed. In (Logan, 2002)
the authors address the problem by indexing phonetic or
word-fragment based transcriptions. Similar approaches,
e.g. using overlapping M -grams of phonemes, are dis-
cussed in (Scha?uble, 1995) and (Ng, 2000). (James
and Young, 1994) introduces the approach of searching
phoneme lattices. (Clements, 2001) proposes a similar
idea called ?phonetic search track.? In previous work
(Seide and Yu, 2004), promising results were obtained
with phonetic lattice search in voicemails. In (Yu and
947
wordrecognizer
phoneticrecognizer
posteriorconversion & merging
wordlattice latticeindexer
latticestore
indexlookupresult list linear search query
phonemelattice
audiostream
promisingsegmentsranker
indexing
search
hit list
h
y
b
r
i
d
l
a
t
t
i
c
e
query
invertedindex letter to sound
word stringpho
n
e
m
e
s
t
r
i
n
g
hybridlattice
Figure 1: System architecture.
Seide, 2004), it was found that even better re ult can be
achieved by combining a phonetic search with a word-
level search.
For the third problem, quick response time is com-
monly achieved by indexing techniques. However, in
the context of phonetic lattice search, the concept of ?in-
dexing? becomes a non-trivial problem, because due to
the unknown-word nature, we need to deal with an open
set of index keys. (Saraclar and Sproat, 2004) proposes
to store the individual lattice arcs (inverting the lattice).
(Allauzen et al, 2004) introduces a general indexation
framework by indexing expected term frequencies (?ex-
pected counts?) instead of each individual keyword oc-
currence or lattice arcs. In (Yu et al, 2005), a similar idea
of indexing expected term frequencies is proposed, sug-
gesting to approximate expected term frequencies by M -
gram phoneme language models estimated on segments
of audio.
In this paper, we combine previous work on pho-
netic lattice search, hybrid search and lattice indexing
into a real system for searching recorded conversations
that achieves high accuracy and can handle hundreds of
hours of audio. The main contributions of this paper
are: a real system for searching conversational speech, a
novel method for combining phoneme and word lattices,
and experimental results for searching recorded conver-
sations.
The paper is organized as follows. Section 2 gives an
overview of the system. Section 3 introduces the over-
all criterion, based on which the system is developed,
Section 4 introduces our implementation for a hybrid
word/phoneme search system, and Section 5 discusses the
lattice indexing mechanism. Section 6 presents the exper-
imental results, and Section 7 concludes.
2 A System For Searching Conversations
A system for searching the spoken content of recorded
conversations has several distinct properties. Users are
searching their own meetings, so most searches will be
known-item searches with at most a few correct hits in the
archive. Users will often search for specific phrases that
they remember, possibly with boolean operators. Rele-
vance weighting of individual query terms is less of an
issue in this scenario.
We identified three user requirements:
? high recall and accurate ranking of phrase matches;
? domain independence ? it should work for any topic,
ideally without need to adapt vocabularies or lan-
guage models;
? reasonable response time ? a few seconds at most,
independent of the size of the conversation archive.
We address them as follows. First, to increase recall
we search recognition alternates based on lattices. Lat-
tice oracle word-error rates1 are significantly lower than
word-error rates of the best path. For example, (Chelba
and Acero, 2005) reports a lattice oracle error rate of 22%
for lecture recordings at a top-1 word-error rate of 45%2.
To utilize recognizer scores in the lattices, we formulat-
ing the ranking problem as one of risk minimization and
derive that keyword hits should be ranked by their word
(phrase) posterior probabilities.
Second, domain independence is achieved by combin-
ing large-vocabulary recognition with a phonetic search.
This helps especially for proper names and specialized
terminology, which are often either missing in the vocab-
ulary or not well-predicted by the language model.
Third, to achieve quick response time, we use an M -
gram based indexing approach. It has two stages, where
the first stage is a fast index lookup to create a short-list of
candidate lattices. In the second stage, a detailed lattice
match is applied to the lattices in the short-list. We call
the second stage linear search because search time grows
linearly with the duration of the lattices searched.
1The ?oracle word-error rate? of a lattice is the word error
rate of the path through the lattice that has the least errors.
2Note that this comparison was for a reasonably well-tuned
recognizer setup. Any arbitrary lattice oracle error rate can be
obtained by adjusting the recognizer?s pruning setup and in-
vesting enough computation time (plus possibly adapting the
search-space organization).
948
The resulting system architecture is shown in Fig. 1. In
the following three sections, we will discuss our solutions
in these three aspects in details respectively.
3 Ranking Criterion
For ranking search results according to ?relevance? to the
user?s query, several relevance measures have been pro-
posed in the text-retrieval literature. The key element of
these measures is weighting the contribution of individual
keywords to the relevance-ranking score. Unfortunately,
text ranking criteria are not directly applicable to retrieval
of speech because recognition alternates and confidence
scores are not considered.
Luckily, this is less of an issue in our known-item style
search, because the simplest of relevance measures can
be used: A search hit is assumed relevant if the query
phrase was indeed said there (and fulfills optional boolean
constraints), and it is not relevant otherwise.
This simple relevance measure, combined with a vari-
ant of the probability ranking principle (Robertson,
1977), leads to a system where phrase hits are ranked by
their phrase posterior probability. This is derived through
a Bayes-risk minimizing approach as follows:
1. Let the relevance be R(Q,hiti) of a returned audio
hit ? hiti to a user?s query Q formally defined is 1
(match) if the hit is an occurrence of the query term
with time boundaries (thitis , thit
i
e ), or 0 if not.
2. The user expects the system to return a list of audio
hits, ranked such that the accumulative relevance of
the top n hits (hit1...hitn), averaged over a range of
n = 1...nmax, is maximal:
1
nmax
nmax?
n=1
n?
i=1
R(Q,hiti) != max . (1)
Note that this is closely related to popular word-
spotting metrics, such as the NIST (National Insti-
tute of Standards & Technology) Figure Of Merit.
To the retrieval system, the true transcription of each
audio file is unknown, so it must maximize Eq. (1) in the
sense of an expected value
EWT |O
{
1
nmax
nmax?
n=1
n?
i=1
RWT (Q,hiti)
}
!= max,
where O denotes the totality of all audio files (O
for observation), W = (w1, w2, ..., wN ) a hypothe-
sized transcription of the entire collection, and T =
(t1, t2, ..., tN+1) the associated time boundaries on a
shared collection-wide time axis.
RWT (?) shall be relevance w.r.t. the hypothesized tran-
scription and alignment. The expected value is taken
w.r.t. the posterior probability distribution P (WT |O)
provided by our speech recognizer in the form of scored
lattices. It is easy to see that this expression is max-
imal if the hits are ranked by their expected relevance
EWT |O{RWT (Q,hiti)}. In our definition of relevance,
RWT (Q,hiti) is written as
RWT (Q,hiti) =
?
???
???
1 ?k, l : tk = thitis
?tk+l = thitie
?wk, ..., wk+l?1 = Q
0 otherwise
and the expected relevance is computed as
EWT |O{RWT (Q,hiti)} =
?
WT
RWT (Q,hiti)P (WT |O)
= P (?, thitis , Q, thit
i
e , ?|O)
with
P (?, ts, Q, te, ?|O) =
?
WT :?k,l:tk=ts?tk+l=te
?wk,...,wk+l?1=Q
P (WT |O). (2)
For single-word queries, this is the well-known word pos-
terior probability (Wessel et al, 2000; Evermann et al,
2000). To cover multi-label phrase queries, we will call it
phrase posterior probability.
The formalism in this section is applicable to all sorts
of units, such as fragments, syllables, or words. The tran-
scription W and its units wk, as well as the query string
Q, should be understood in this sense. For a regular word-
level search, W and Q are just strings of words In the con-
text of phonetic search, W and Q are strings of phonemes.
For simplicity of notation, we have excluded the issue of
multiple pronunciations of a word. Eq. (2) can be trivially
extended by summing up over all alternative pronuncia-
tions of the query. And in a hybrid search, there would
be multiple representations of the query, which are just as
pronunciation variants.
4 Word/Phoneme Hybrid Search
For a combined word and phoneme based search, two
problems need to be considered:
? Recognizer configuration. While established solu-
tions exist for word-lattice generation, what needs
to be done for generating high-quality phoneme lat-
tices?
? How should word and phoneme lattices be jointly
represented for the purpose of search, and how
should they be searched?
4.1 Speech Recognition
4.1.1 Large-Vocabulary Recognition
Word lattices are generated by a common speaker-
independent large-vocabulary recognizer. Because the
speaking style of conversations is very different from, say,
949
your average speech dictation system, specialized acous-
tic models are used. These are trained on conversational
speech to match the speaking style. The vocabulary and
the trigram language model are designed to cover a broad
range of topics.
The drawback of large-vocabulary recognition is, of
course, that it is infeasible to have the vocabulary cover
all possible keywords that a user may use, particularly
proper names and specialized terminology.
One way to address this out-of-vocabulary problem is
to mine the user?s documents or e-mails to adapt the rec-
ognizer?s vocabulary. While this is workable for some
scenarios, it is not a good solution e.g. when new words
are frequently introduced in the conversations themselves
rather than preceding written conversations, where the
spelling of a new word is not obvious and thus inconsis-
tent, or when documents with related documents are not
easily available on the user?s hard disk but would have to
be specifically gathered by the user.
A second problem is that the performance of state-of-
the-art speech recognition relies heavily on a well-trained
domain-matched language model. Mining user data can
only yield a comparably small amount of training data.
Adapting a language model with it would barely yield a
robust language model for newly learned words, and their
usage style may differ in conversational speech.
For the above reasons, we decided not to attempt to
adapt vocabulary and language model. Instead, we use
a fixed broad-domain vocabulary and language model
for large-vocabulary recognition, and augment this sys-
tem with maintenance-free phonetic search to cover new
words and mismatched domains.
4.1.2 Phonetic Recognition
The simplest phonetic recognizer is a regular recog-
nizer with the vocabulary replaced by the list of phone-
mes of the language, and the language model replaced by
a phoneme M -gram. However, such phonetic language
model is much weaker than a word language model. This
results in poor accuracy and inefficient search.
Instead, our recognizer uses ?phonetic word frag-
ments? (groups of phonemes similar to syllables or half-
syllables) as its vocabulary and in the language model.
This provides phonotactic constraints for efficient decod-
ing and accurate phoneme-boundary decisions, while re-
maining independent of any specific vocabulary. A set
of about 600 fragments was automatically derived from
the language-model training set by a bottom-up group-
ing procedure (Klakow, 1998; Ng, 2000; Seide and Yu,
2004). Example fragments are /-k-ih-ng/ (the syllable -
king), /ih-n-t-ax-r-/ (inter-), and /ih-z/ (the word is).
With this, lattices are generated using the common
Viterbi decoder with word-pair approximation (Schwartz
et al, 1994; Ortmanns et al, 1996). The decoder has been
modified to keep track of individual phoneme boundaries
and scores. These are recorded in the lattices, while
fragment-boundary information is discarded. This way,
phoneme lattices are generated.
In the results section we will see that, even with a well-
trained domain-matching word-level language model,
searching phoneme lattices can yield search accuracies
comparable with word-level search, and that the best per-
formance is achieved by combining both into a hybrid
word/phoneme system.
4.2 Unified Hybrid Lattice Representation
Combining word and phonetic search is desirable because
they are complementary: Word-based search yields bet-
ter precision, but has a recall issue for unknown and rare
words, while phonetic search has very good recall but suf-
fers from poor precision especially for short words.
Combining the two is not trivial. Several strategies are
discussed in (Yu and Seide, 2004), including using a hy-
brid recognizer, combining lattices from two separate rec-
ognizers, and combining the results of two separate sys-
tems. Both hybrid recognizer configuration and lattice
combination turned out difficult because of the different
dynamic range of scores in word and phonetic paths.
We found it beneficial to convert both lattices into
posterior-based representations called posterior lattices
first, which are then merged into a hybrid posterior lat-
tice. Search is performed in a hybrid lattice in a unified
manner using both phonetic and word representations as
?alternative pronunciation? of the query, and summing up
the resulting phrase posteriors.
Posterior lattices are like regular lattices, except that
they do not store acoustic likelihoods, language model
probabilities, and precomputed forward/backward proba-
bilities, but arc and node posteriors. An arc?s posterior
is the probability that the arc (with its associated word
or phoneme hypothesis) lies on the correct path, while a
node posterior is the probability that the correct path con-
nects two word/phoneme hypotheses through this node.
In our actual system, a node is only associated with a
point in time, and the node posterior is the probability
of having a word or phoneme boundary at its associated
time point.
The inclusion of node posteriors, which to our knowl-
edge is a novel contribution of this paper, makes an exact
computation of phrase posteriors from posterior lattices
possible. In the following we will explain this in detail.
4.2.1 Arc and Node Posteriors
A lattice L = (N ,A, nstart, nend) is a directed acyclic
graph (DAG) with N being the set of nodes, A is the
set of arcs, and nstart, nend ? N being the unique ini-
tial and unique final node, respectively. Nodes represent
times and possibly context conditions, while arcs repre-
sent word or phoneme hypotheses.3
Each node n ? N has an associated time t[n] and pos-
sibly an acoustic or language-model context condition.
Arcs are 4-tuples a = (S[a], E[a], I[a], w[a]). S[a], E[a]
3Alternative definitions of lattices are possible, e.g. nodes
representing words and arcs representing word transitions.
950
? N denote the start and end node of the arc. I[a] is
the arc label4, which is either a word (in word lattices)
or a phoneme (in phonetic lattices). Last, w[a] shall be
a weight assigned to the arc by the recognizer. Specifi-
cally, w[a] = pac(a)1/? ?PLM(a) with acoustic likelihood
pac(a), language model probability PLM, and language-
model weight ?.
In addition, we define paths pi = (a1, ? ? ? , aK) as
sequences of connected arcs. We use the symbols S,
E, I , and w for paths as well to represent the respec-
tive properties for entire paths, i.e. the path start node
S[pi] = S[a1], path end node E[pi] = E[aK ], path la-
bel sequence I[pi] = (I[a1], ? ? ? , I[aK ]), and total path
weight w[pi] = ?Kk=1 w[ak].
Finally, we define ?(n1, n2) as the entirety of all paths
that start at node n1 and end in node n2: ?(n1, n2) =
{pi|S[pi] = n1 ? E[pi] = n2}.
With this, the phrase posteriors defined in Eq. 2 can be
written as follows.
In the simplest case, Q is a single word token. Then,
the phrase posterior is just the word posterior and, as
shown in e.g. (Wessel et al, 2000) or (Evermann et al,
2000), can be computed as
P (?, ts, Q, te, ?|O) =
?
pi=(a1,??? ,aK )??(nstart,nend):
?l:[S[al]]=ts?t[E[al]]=te?I[al]=Q
w[pi]
?
pi??(nstart,nend)
w[pi]
=
?
a?A:t[S[a]]=ts
?t[E[a]]=te?I[a]=Q
Parc[a] (3)
with Parc[a] being the arc posterior defined as
Parc[a] =
?S[a] ? w[a] ? ?E[a]
?nend
with the forward/backward probabilities ?n and ?n de-
fined as:
?n =
?
pi??(nstart,n)
w[pi]
?n =
?
pi??(n,nend)
w[pi].
?n and ?n can conveniently be computed from the word
lattices by the well-known forward/backward recursion:
?n =
{ 1.0 n = nstart?
a:E[a]=n
?S[a] ? w[a] otherwise
?n =
{ 1.0 n = nend?
a:S[a]=n
w[a] ? ?E[a] otherwise.
4Lattices are often interpreted as weighted finite-state accep-
tors, where the arc labels are the input symbols, hence the sym-
bol I .
Now, in the general case of multi-label queries, the phrase
posterior can be computed as
P (?, ts, Q, te, ?|O)
=
?
pi=(a1,??? ,aK ):
t[S[pi]]=ts?t[E[pi]]=te?I[pi]=Q
Parc[a1] ? ? ?Parc[aK ]
Pnode[S[a2]] ? ? ?Pnode[S[aK ]]
with Pnode[n], the node posterior5, defined as
Pnode[n] = ?n ? ?n?nend
. (4)
4.2.2 Advantages of Posterior Lattices
The posterior-lattice representation has several advan-
tages over traditional lattices. First, lattice storage is re-
duced because only one value (node posterior) needs to be
stored per node instead of two (?, ?)6. Second, node and
arc posteriors have a smaller and similar dynamic range
than ?n, ?n, and w[a], which is beneficial when the val-
ues should be stored with a small number of bits.
Further, for the case of word-based search, the summa-
tion in Eq. 3 can also be precomputed by merging all lat-
tice nodes that carry the same time label, and merging the
corresponding arcs by summing up their arc posteriors.
In such a ?pinched? lattice, word posteriors for single-
label queries can now be looked up directly. However,
posteriors for multi-label strings cannot be computed pre-
cisely anymore. Our experiments have shown that the im-
pact on ranking accuracy caused by this approximation is
neglectable. Unfortunately, we have also found that the
same is not true for phonetic search.
The most important advantage of posterior lattices for
our system is that they provide a way of combining the
word and phoneme lattices into a single structure ? by
simply merging their start nodes and their end nodes. This
allows to implement hybrid queries in a single unified
search, treating the phonetic and the word-based repre-
sentation of the query as alternative pronunciations.
5 Lattice Indexing
Searching lattices is time-consuming. It is not feasible to
search large amounts of audio. To deal with hundreds or
even thousands of hours of audio, we need some form of
inverted indexing mechanism.
This is comparably straight-forward when indexing
text. It is also not difficult for indexing word lattices. In
both case, the set of words to index is known. However,
indexing phoneme lattices is very different, because the-
oretically any phoneme string could be an indexing item.
5Again, mind that in our lattice formulation word/phoneme
hypotheses are represented by arcs, while nodes just represent
connection points. The node posterior is the probability that the
correct path passes through a connection point.
6Note, however, that storage for the traditional lattice can
also be reduced to a single number per node by weight push-
ing (Saraclar and Sproat, 2004), using an algorithm that is very
similar to the forward/backward procedure.
951
We address this by our M -gram lattice-indexing
scheme. It was originally designed for phoneme lattices,
but can be ? and is actually ? used in our system for in-
dexing word lattices.
First, audio files are clipped into homogeneous seg-
ments. For an audio segment i, we define the expected
term frequency (ETF) of a query string Q as summation
of phrase posteriors of all hits in this segment:
ETFi(Q) =
?
?ts,te
P (?, ts, Q, te, ?|Oi)
=
?
pi??i:I[pi]=Q
p[pi]
with ?i being the set of all paths of segment i.
At indexing time, ETFs of a list of M -grams for each
segment are calculated. They are stored in an inverted
structure that allows retrieval by M -gram.
In search time, the ETFs of the query string are es-
timated by the so-called ?M -gram approximation?. In
order to explain this concept, we need to first introduce
P (Q|Oi) ? the probability of observing query string Q at
any word boundary in the recording Oi. P (Q|Oi) has a
relationship with ETF as
ETFi(Q) = N?i ? P (Q|Oi)
with N?i being the expected number of words in the seg-
ment i. It can also be computed as
N?i =
?
n?Ni
p[n],
where Ni is the node set for segment i.
Like the M -gram approximation in language-model
theory, we approximate P (Q|Oi) as
P (Q|Oi) ? P? (Q|Oi)
=
l?
k=1
P? (qk|qk?M+1, ? ? ? , qk?1, Oi),
while the right-hand items can be calculated from M -
gram ETFs:
P? (qk|qk?M+1, ? ? ? , qk?1, Oi)
= ETFi(qk?M+1, ? ? ? , qk)ETFi(qk?M+1, ? ? ? , qk?1) .
The actual implementation uses only M -grams extracted
from a large background dictionary, with a simple backoff
strategy for unseen M -grams, see (Yu et al, 2005) for
details.
The resulting index is used in a two stage-search man-
ner: The index itself is only used as the first stage to de-
termine a short-list of promising segments that may con-
tain the query. The second stage involves a linear lattice
search to get final results.
Table 1: Test corpus summary.
test set dura- #seg- keyword set
tion ments (incl. OOV)
ICSI meetings 2.0h 429 1878 (96)
SWBD eval2000 3.6h 742 2420 (215)
SWBD rt03s 6.3h 1298 2325 (236)
interviews (phone) 1.1h 267 1057 (49)
interviews (lapel) 1.0h 244 1629 (107)
6 Results
6.1 Setup
We have evaluated our system on five different corpora of
recorded conversations:
? one meeting corpus (NIST ?RT04S? development
data set, ICSI portion, (NIST, 2000-2004))
? two eval sets from the switchboard (SWBD) data
collection (?eval 2000? and ?RT03S?, (NIST, 2000-
2004))
? two in-house sets of interview recordings of about
one hour each, one recorded over the telephone, and
one using a single microphone mounted in the inter-
viewee?s lapel.
For each data set, a keyword list was selected by an
automatic procedure (Seide and Yu, 2004). Words and
multi-word phrases were selected from the reference tran-
scriptions if they occurred in at most two segments. Ex-
ample keywords are overseas, olympics, and ?automated
accounting system?. For the purpose of evaluation, those
data sets are cut into segments of about 15 seconds each.
The size of the corpora, their number of segments, and
the size of the selected keyword set are given in Table 1.
The acoustic model we used is trained on 309h of the
Switchboard corpus (SWBD-1). The LVCSR language
model was trained on the transcriptions of the Switch-
board training set, the ICSI-meeting training set, and the
LDC Broadcast News 96 and 97 training sets. No ded-
icated training data was available for the in-house inter-
view recordings. The recognition dictionary has 51388
words. The phonetic language model was trained on the
phonetic version of the transcriptions of SWBD-1 and
Broadcast News 96 plus about 87000 background dictio-
nary entries, a total of 11.8 million phoneme tokens.
To measure the search accuracy, we use the ?Figure
Of Merit? (FOM) metric defined by NIST for word-
spotting evaluations. In its original form, it is the aver-
age of detection/false-alarm curve taken over the range
[0..10] false alarms per hour per keyword. Because man-
ual word-level alignments of our test sets were not avail-
able, we modified the FOM such that a correct hit is a
15-second segment that contains the key phrase.
Besides FOM, we use a second metric ? ?Top Hit Pre-
cision? (THP), defined as the correct rate of the best
ranked hit. If no hit is returned for an existing query term,
it is counted as an error. Both of these metrics are relevant
measures in our known-item search.
952
Table 2: Baseline transcription word-error rates (WER)
as well as precision (P), recall (R), FOM and THP for
searching the transcript.
test set WER P R FOM THP
[%] [%] [%] [%] [%]
ICSI meetings 44.1 80.6 43.8 43.6 43.6
SWBD eval2000 39.0 79.6 41.1 41.1 41.1
SWBD rt03s 45.2 72.6 36.3 36.3 36.0
interviews (phone) 57.7 68.8 31.6 29.3 31.3
interviews (lapel) 62.8 80.1 32.0 30.2 32.1
average 49.8 76.3 37.0 36.1 36.8
Table 3: Comparison of search accuracy for word,
phoneme, and hybrid lattices.
test set word phoneme hybrid
Figure Of Merit (FOM) [%]
ICSI meetings 72.1 81.2 88.2
SWBD eval2000 71.3 80.4 87.3
SWBD rt03s 66.4 76.9 84.2
interviews (phone) 60.6 73.7 83.3
interviews (lapel) 59.0 70.2 77.7
average 65.9 76.5 84.1
INV words only 69.4 77.0 84.7
OOV words only 0 73.8 73.8
Top Hit Precision (THP) [%]
ICSI meetings 67.2 65.0 78.7
SWBD eval2000 67.1 63.6 77.9
SWBD rt03s 59.6 59.1 71.7
interviews (phone) 55.7 64.4 73.1
interviews (lapel) 55.6 59.7 71.2
average 61.0 62.4 74.5
INV words only 64.5 62.4 75.3
OOV words only 0 60.5 60.5
6.2 Word/Phoneme Hybrid Search
Table 2 gives the LVSCR transcription word-error rates
for each set. Almost all sets have a word-error rates above
40%. Searching those speech recognition transcriptions
results in FOM and THP values below 40%.
Table 3 gives results of searching in word, phoneme,
and hybrid lattices. First, for all test sets, word-lattice
search is drastically better than transcription-only search.
Second, comparing word-lattice and phoneme-lattice
search, phoneme lattices outperforms word lattices on
all tests in terms of FOM. This is because phoneme lat-
tice has better recall rate. For THP, word lattice search
is slightly better except on the interview sets for which
the language model is not well matched. Hybrid search
leads to a substantial improvement over each (27.6% av-
erage FOM improvement and 16.2% average THP im-
provement over word lattice search). This demonstrates
the complementary nature of word and phoneme search.
We also show results separately for known words (in-
vocabulary, INV) and out-of-vocabulary words (OOV).
Interestingly, even for known words, hybrid search leads
to a significant improvement (get 22.0% for FOM and
16.7% for THP) compared to using word lattices only.
6.3 Effect of Node Posterior
In Section 4.2, we have shown that phrase posteriors can
be computed from posterior lattices if they include both
arc and node posteriors (Eq. 4). However, posterior rep-
resentations of lattices found in literature only include
word (arc) posteriors, and some posterior-based systems
simply ignore the node-posterior term, e.g. (Chelba and
Acero, 2005). In Table 4, we evaluate the impact on ac-
curacy when this term is ignored. (In this experiment,
we bypassed the index-lookup step, thus the numbers are
slightly different from Table 3.)
We found that for word-level search, the effect of node
posterior compensation is indeed neglectable. However,
for phonetic search it is not: We observe a 4% relative
FOM loss.
6.4 Index Lookup and Linear Search
Section 5 introduced a two-stage search approach using
an M -gram based indexing scheme. How much accuracy
is lost from incorrectly eliminating correct hits in the first
(index-based) stage? Table 5 compares three setups. The
first column shows results for linear search only: no index
lookup used at all, a complete linear search is performed
on all lattices. This search is optimal but does not scale
up to large database. The second column shows index
lookup only. Segments are ranked by the approximate
M -gram based ETF score obtained from the index. The
third column shows the two-stage results.
The index-based two-stage search is indeed very close
to a full linear search (average FOM loss of 1.2% and
THP loss of 0.2% points). A two-stage search takes under
two seconds and is mostly independent of the database
size. In other work, we have applied this technique suc-
cessfully to search a database of nearly 200 hours.
6.5 The System
Fig. 2 shows a screenshot of a research prototype for a
search-enabled audio notebook. In addition to a note-
taking area (bottom) and recording controls, it includes
a rich audio browser showing speaker segmentation and
automatically identified speaker labels (both not scope of
this paper). Results of keyword searches are shown as
color highlights, which are clickable to start playback at
that position.
7 Conclusion
In this paper, we have presented a system for searching
recordings of conversational speech, particularly meet-
Table 4: Effect of ignoring the node-posterior term in
phrase-posterior computation (shown for ICSI meeting
set only).
FOM word phoneme
exact computation 72.1 82.3
node posterior ignored 72.0 79.2
relative change [%] -0.1 -3.8
953
Table 5: Comparing the effect of lattice indexing. Shown
is unindexed ?linear search,? index lookup only (seg-
ments selected via the index without subsequent linear
search), and the combination of both.
test set linear index two-
search lookup stage
Figure Of Merit (FOM) [%]
ICSI meetings 88.6 86.4 88.2
SWBD eval2000 88.7 86.5 87.3
SWBD rt03s 87.3 85.1 84.2
interviews (phone) 83.8 81.2 83.3
interviews (lapel) 78.3 76.1 77.7
average 85.3 83.1 84.1
Top Hit Precision (THP) [%]
ICSI meetings 78.8 70.7 78.7
SWBD eval2000 78.0 71.4 77.9
SWBD rt03s 71.9 65.7 71.7
interviews (phone) 73.8 64.6 73.1
interviews (lapel) 70.8 65.9 71.2
average 74.7 67.7 74.5
ings and telephone conversations. We identified user re-
quirements as accurate ranking of phrase matches, do-
main independence, and reasonable response time. We
have addressed these by hybrid word/phoneme lattice
search and a supporting indexing scheme. Unlike many
other spoken-document retrieval systems, we search
recognition alternates instead of only speech recognition
transcripts. This yields a significant improvement of key-
word spotting accuracy. We have combined word-level
search with phonetic search, which not only enables the
system to handle the open-vocabulary problem, but also
substantially improves in-vocabulary accuracy. We have
proposed a posterior-lattice representation that allows for
unified word and phoneme indexing and search. To speed
up the search process, we proposed M -gram based lat-
tice indexing, which extends our open vocabulary search
ability for large collection of audio. Tested on five dif-
ferent recording sets including meetings, conversations,
and interviews, a search accuracy (FOM) of 84% has
been achieved ? dramatically better than searching speech
recognition transcripts (under 40%).
Figure 2: Screenshot of our research prototype of a
search-enabled audio notebook.
8 Acknowledgements
The authors wish to thank our colleagues Asela Gunawar-
dana, Patrick Nguyen, Yu Shi, and Ye Tian for sharing
their Switchboard setup and models with us; and Frank
Soong for his valuable feedback on this paper.
References
C. Allauzen, M. Mohri, M. Saraclar, General indexation of
weighted automata ? application to spoken utterance re-
trieval. Proc. HLT?2004, Boston, 2004.
C. Chelba and A. Acero, Position specific posterior lattices for
indexing speech. Proc. ACL?2005, Ann Arbor, 2005.
Mark Clements et al, Phonetic Searching vs. LVCSR:
How to find what you really want in audio archives.
Proc. AVIOS?2001, San Jose, 2001.
G. Evermann et al, Large vocabulary decoding and con-
fidence estimation using word posterior probabilities.
Proc. ICASSP?2000, Istanbul, 2000
J. Garofolo, TREC-9 Spoken Document Retrieval Track.
National Institute of Standards and Technology, http://
trec.nist.gov/pubs/trec9/sdrt9_slides/
sld001.htm.
D. A. James and S. J. Young, A fast lattice-based approach to
vocabulary-independent wordspotting. Proc. ICASSP?1994,
Adelaide, 1994.
D. Klakow. Language-model optimization by mapping of cor-
pora. Proc. ICASSP?1998.
Beth Logan et al, An experimental study of an audio indexing
system for the web. Proc. ICSLP?2000, Beijing, 2000.
Beth Logan et al, Word and subword indexing approaches
for reducing the effects of OOV queries on spoken audio.
Proc. HLT?2002, San Diego, 2002.
Kenney Ng, Subword-based approaches for spoken document
retrieval. PhD thesis, Massachusetts Institute of Technology,
2000.
NIST Spoken Language Technology Evaluations, http://
www.nist.gov/speech/tests/.
S. Ortmanns, H. Ney, F. Seide, and I. Lindam, A comparison of
the time conditioned and word conditioned search techniques
for large-vocabulary speech recognition. Proc. ICSLP?1996,
Philadelphia, 1996.
S. E. Robertson, The probability ranking principle in IR. Journal
of Documentation 33 (1977).
M. Saraclar, R. Sproat, Lattice-based search for spoken utter-
ance retrieval. Proc. HLT?2004, Boston, 2004.
P. Scha?uble et al, First experiences with a system for con-
tent based retrieval of information from speech recordings.
Proc. IJCAI?1995, Montreal, 1995.
R. Schwartz et al, A comparison of several approximate al-
gorithms for finding multiple (n-best) sentence hypotheses.
Proc. ICSLP?1994, Yokohama, 1994.
F. Seide, P. Yu, et al, Vocabulary-independent search in sponta-
neous speech. Proc. ICASSP?2004, Montreal, 2004.
Lisa Joy Stifelman, The Audio Notebook. PhD thesis, Mas-
sachusetts Institute of Technology, 1997.
F. Wessel, R. Schlu?ter, and H. Ney, Using posterior
word probabilities for improved speech recognition.
Proc. ICASSP?2000, Istanbul, 2000.
P. Yu, F. Seide, A hybrid word / phoneme-based approach
for improved vocabulary-independent search in spontaneous
speech. Proc. ICLSP?04, Jeju, 2004.
P. Yu, K. J. Chen, C. Y. Ma, F. Seide, Vocabulary-Independent
Indexing of Spontaneous Speech, to appear in IEEE transac-
tion on Speech and Audio Processing, Special Issue on Data
Mining of Speech, Audio and Dialog.
954
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 415?422,
New York, June 2006. c?2006 Association for Computational Linguistics
Towards Spoken-Document Retrieval for the Internet:
Lattice Indexing For Large-Scale Web-Search Architectures
Zheng-Yu Zhou?, Peng Yu, Ciprian Chelba+, and Frank Seide
?Chinese University of Hong Kong, Shatin, Hong Kong
Microsoft Research Asia, 5F Beijing Sigma Center, 49 Zhichun Road, 100080 Beijing
+Microsoft Research, One Microsoft Way, Redmond WA 98052
zyzhou@se.cuhk.edu.hk, {rogeryu,chelba,fseide}@microsoft.com
Abstract
Large-scale web-search engines are generally
designed for linear text. The linear text repre-
sentation is suboptimal for audio search, where
accuracy can be significantly improved if the
search includes alternate recognition candi-
dates, commonly represented as word lattices.
This paper proposes a method for indexing
word lattices that is suitable for large-scale
web-search engines, requiring only limited
code changes.
The proposed method, called Time-based
Merging for Indexing (TMI), first converts the
word lattice to a posterior-probability represen-
tation and then merges word hypotheses with
similar time boundaries to reduce the index
size. Four alternative approximations are pre-
sented, which differ in index size and the strict-
ness of the phrase-matching constraints.
Results are presented for three types of typi-
cal web audio content, podcasts, video clips,
and online lectures, for phrase spotting and rel-
evance ranking. Using TMI indexes that are
only five times larger than corresponding linear-
text indexes, phrase spotting was improved over
searching top-1 transcripts by 25-35%, and rel-
evance ranking by 14%, at only a small loss
compared to unindexed lattice search.
1 Introduction
Search engines have become the essential tool for find-
ing and accessing information on the Internet. The re-
cent runaway success of podcasting has created a need
for similar search capabilities to find audio on the web.
As more news video clips and even TV shows are offered
for on-demand viewing, and educational institutions like
MIT making lectures available online, a need for audio
search arises as well, because the most informative part
of many videos is its dialogue.
There is still a significant gap between current web au-
dio/video search engines and the relatively mature text
search engines, as most of today?s audio/video search en-
gines rely on the surrounding text and metadata of an au-
dio or video file, while ignoring the actual audio content.
This paper is concerned with technologies for searching
the audio content itself, in particular how to represent the
speech content in the index.
Several approaches have been reported in the litera-
ture for indexing spoken words in audio recordings. The
TREC (Text REtrieval Conference) Spoken-Document
Retrieval (SDR) track has fostered research on audio-
retrieval of broadcast-news clips. Most TREC bench-
marking systems use broadcast-news recognizers to gen-
erate approximate transcripts, and apply text-based infor-
mation retrieval to these. They achieve retrieval accuracy
similar to using human reference transcripts, and ad-hoc
retrieval for broadcast news is considered a ?solved prob-
lem? (Garofolo, 2000). Noteworthy are the rather low
word-error rates (20%) in the TREC evaluations, and that
recognition errors did not lead to catastrophic failures due
to redundancy of news segments and queries. However, in
our scenario, unpredictable, highly variable acoustic con-
ditions, non-native and accented speaker, informal talk-
ing style, and unlimited-domain language cause word-
error rates to be much higher (40-60%). Directly search-
ing such inaccurate speech recognition transcripts suffers
from a poor recall.
A successful way for dealing with high word error rates
is the use of recognition alternates (lattices) (Saraclar,
2004; Yu, 2004; Chelba, 2005). For example, (Yu, 2004)
reports a 50% improvement of FOM (Figure Of Merit) for
a word-spotting task in voice-mails, and (Yu, HLT2005)
adopted the approach for searching personal audio collec-
tions, using a hybrid word/phoneme lattice search.
Web-search engines are complex systems involving
substantial investments. For extending web search to au-
dio search, the key problem is to find a (approximate)
415
representation of lattices that can be implemented in a
state-of-the-art web-search engine with as little changes
as possible to code and index store and without affecting
its general architecture and operating characteristics.
Prior work includes (Saraclar, 2004), which proposed
a direct inversion of raw lattices from the speech recog-
nizer. No information is lost, and accuracy is the same
as for directly searching the lattice. However, raw lattices
contain a large number of similar entries for the same spo-
ken word, conditioned on language-model (LM) state and
phonetic cross-word context, leading to inefficient usage
of storage space.
(Chelba, 2005) proposed a posterior-probability based
approximate representation in which word hypotheses are
merged w.r.t. word position, which is treated as a hidden
variable. It easily integrates with text search engines, as
the resulting index resembles a normal text index in most
aspects. However, it trades redundancy w.r.t. LM state
and context for uncertainty w.r.t. word position, and only
achieves a small reduction of index entries. Also, time
information for individual hypotheses is lost, which we
consider important for navigation and previewing.
(Mangu, 2000) presented a method to align a speech
lattice with its top-1 transcription, creating so-called
?confusion networks? or ?sausages.? Sausages are a par-
simonious approximation of lattices, but due to the pres-
ence of null links, they do not lend themselves naturally
for matching phrases. Nevertheless, the method was a key
inspiration for the present paper.
This paper is organized as follows. The next section
states the requirements for our indexing method and de-
scribes the overall system architecture. Section 3 intro-
duces our method, and Section 4 the results. Section 5
briefly describes a real prototype built using the approach.
2 Indexing Speech Lattices, Internet Scale
Substantial investments are necessary to create and oper-
ate a web search engine, in software development and op-
timization, infrastructure, as well as operation and main-
tainance processes. This poses constraints on what can
practically be done when integrating speech-indexing ca-
pabilities to such an engine.
2.1 Requirements
We have identified the following special requirements for
speech indexing:
? realize best possible accuracy ? speech alternates
must be indexed, with scores;
? provide time information for individual hits ? to fa-
cilitate easy audio preview and navigation in the UI;
? encode necessary information for phrase matching ?
phrase matching is a basic function of a search en-
gine and an important feature for document ranking.
This is non-trivial because boundaries of recognition
alternates are generally not aligned.
None of these capabilities are provided by text search
engines. To add these capabilities to an existing web en-
gine, we are facing practical constraints. First, the struc-
ture of the index store cannot be changed fundamentally.
But we can reinterpret existing fields. We also assume
that the index attaches a few auxiliary bits to each word
hit. E.g., this is done in (early) Google (Brin, 1998) and
MSN Search. These can be used for additional data that
needs to be stored.
Secondly, computation and disk access should remain
of similar order of magnitude as for text search. Extra
CPU cycles for phrase-matching loops are possible as
long as disk access remains the dominating factor. The
index size cannot be excessively larger than for indexing
text. This precludes direct inversion of lattices (and un-
fortunately also the use of phonetic lattices).
Last, while local code changes are possible, the over-
all architecture and dataflow cannot be changed. E.g.,
this forbids the use of a two-stage method as in (Yu,
HLT2005).
2.2 Approach
We take a three-step approach. First, following (Chelba,
2005), we use a posterior-probability representation, as
posteriors are resilient to approximations and can be
quantized with only a few bits. Second, we reduce the in-
herent redundancy of speech lattices by merging word hy-
potheses with same word identity and similar time bound-
aries, hence the name ?Time-based Merging for Indexing?
(TMI). Third, the resulting hypothesis set is represented
in the index by reinterpreting existing data fields and re-
purposing auxiliary bits.
2.3 System Architecture
Fig. 1 shows the overall architecture of a search engine
for audio/video search. At indexing time, a media de-
coder first extracts the raw audio data from different for-
mats of audio found on the Internet. A music detector
prevents music from being indexed. The speech is then
fed into a large-vocabulary continuous-speech recognizer
(LVCSR), which outputs word lattices. The lattice in-
dexer converts the lattices into the TMI representation,
which is then merged into the inverted index. Available
textual metadata is also indexed.
At search time, all query terms are looked up in the in-
dex. For each document containing all query terms (deter-
mined by intersection), individual hit lists of each query
term are retrieved and fed into a phrase matcher to iden-
tify full and partial phrase hits. Using this information,
the ranker computes relevance scores. To achieve accept-
able response times, a full-scale web engine would split
this process up for parallel execution on multiple servers.
Finally the result presentation module will create snippets
416
mediadecoder speechstream speechrecognizer
indexlookupresult page
query
audiostream
resultpresentation
indexing
searchinvertedindex
wavestream latticeindexer
speechlattice
TMI representationmetadata textindexer
ranker
time information
doclist phrasematch hitinformationhitlist
musicdetector
Figure 1: System Architecture.
for the returned documents and compose the result page.
In audio search, snippets would contain time information
for individual word hits to allow easy navigation and pre-
view.
3 Time-based Merging for Indexing
Our previous work (Yu, IEEE2005) has shown that in a
word spotting task, ranking by phrase posteriors is in the-
ory optimal if (1) a search hit is considered relevant if the
query phrase was indeed said there, and (2) the user ex-
pects a ranked list of results such that the accumulative
relevance of the top-n entries of the list, averaged over
a range of n, is maximized. In the following, we will
first recapitulate the lattice notation and how phrase pos-
teriors are calculated from the lattice. We then introduce
time-based merging, which leads to an approximate rep-
resentation of the original lattice. We will describe two
strategies of merging, one by directly clustering word hy-
potheses (arc-based merging) and one by grouping lattice
nodes (node-based merging).
3.1 Posterior Lattice Representation
A lattice L = (N ,A, nstart, nend) is a directed acyclic
graph (DAG) with N being the set of nodes, A is the
set of arcs, and nstart, nend ? N being the unique ini-
tial and unique final node, respectively. Nodes represent
times and possibly context conditions, while arcs repre-
sent word or phoneme hypotheses.1
Each node n ? N has an associated time t[n] and
possibly an acoustic or language-model context condi-
tion. Arcs are 4-tuples a = (S[a], E[a], I[a], w[a]). S[a],
E[a] ? N denote the start and end node of the arc. I[a]
is the word identity. Last, w[a] shall be a weight as-
signed to the arc by the recognizer. Specifically, w[a] =
pac(a)1/? ? PLM(a) with acoustic likelihood pac(a), LM
probability PLM, and LM weight ?.
1Alternative definitions of lattices are possible, e.g. nodes
representing words and arcs representing word transitions.
In addition, we define paths pi = (a1, ? ? ? , aK) as
sequences of connected arcs. We use the symbols S,
E, I , and w for paths as well to represent the respec-
tive properties for entire paths, i.e. the path start node
S[pi] = S[a1], path end node E[pi] = E[aK ], path la-
bel sequence I[pi] = (I[a1], ? ? ? , I[aK ]), and total path
weight w[pi] = ?Kk=1 w[ak].
Based on this, we define arc posteriors Parc[a] and
node posteriors Pnode[n] as
Parc[a] =
?S[a] ? w[a] ? ?E[a]
?nend
; Pnode[n] = ?n ? ?n?nend
,
with forward-backward probabilities ?n, ?n defined as:
?n =
?
pi:S[pi]=nstart?E[pi]=n
w[pi] ; ?n =
?
pi:S[pi]=n?E[pi]=nend
w[pi]
?n and ?n can be conveniently computed using the well-
known forward-backward recursion, e.g. (Wessel, 2000).
With this, an alternative equivalent representation is
possible by using word posteriors as arc weights. The
posterior lattice representation stores four fields with
each edge: S[a], E[a], I[a], and Parc[a], and two fields
with each node: t[n], and Pnode[a].
With the posterior lattice representation, the phrase
posterior of query string Q is computed as
P (?, ts, Q, te, ?|O)
=
?
pi=(a1,??? ,aK ):
t[S[pi]]=ts?t[E[pi]]=te?I[pi]=Q
Parc[a1] ? ? ?Parc[aK ]
Pnode[S[a2]] ? ? ?Pnode[S[aK ]] . (1)
This posterior representation is lossless. Its advantage is
that posteriors are much more resiliant to approximations
than acoustic likelihoods. This paves the way for lossy
approximations aiming at reducing lattice size.
3.2 Time-based Merging for Indexing
First, (Yu, HLT2005) has shown that node posteriors can
be replaced by a constant, with no negative effect on
417
search accuracy. This approximation simplifies the de-
nominator in Eq. 1 to pK?1node .
We now merge all nodes associated with the same time
points. As a result, the connection condition for two arcs
depends only on the boundary time point. This operation
gave the name Time-based Merging for Indexing.
TMI stores arcs with start and end time, while dis-
carding the original node information that encoded de-
pendency on LM state and phonetic context. This form
is used, e.g., by (Wessel, 2000). Lattices are viewed as
sets of items h = (ts[h], dur[h], I[h], P [h]), with ts[h]
being the start time, dur[h] the time duration, I[h] the
word identity, and P [h] the posterior probability. Arcs
with same word identity and time boundaries but differ-
ent start/end nodes are merged together, their posteriors
being summed up.
These item sets can be organized in an inverted index,
similar to a text index, for efficient search. A text search
engine stores at least two fields with each word hit: word
position and document identity. For TMI, two more fields
need to be stored: duration and posterior. Start times can
be stored by repurposing the word-position information.
Posterior and duration go into auxiliary bits. If the index
has the ability to store side information for documents,
bits can be saved in the main index by recording all time
points in a look-up table, and storing start times and du-
rations as table indices instead of absolute times. This
works because the actual time values are only needed for
result presentation. Note that the TMI index is really an
extension of a linear-text index, and the same code base
can easily accomodate indexing both speech content and
textual metadata.
With this, multi-word phrase matches are defined as
a sequence of items h1...hK matching the query string
(Q = (I[h1], ? ? ? , I[hK ])) with matching boundaries
(ts[hi] + dur[hi] = ts[hi+1]). The phrase posterior is
calculated (using the approximate denominator) as
P (?, ts, Q, te, ?|O) ?
? P [h1] ? ? ?P [hK ]
pK?1node
, (2)
summing over all item sequences with ts = ts[h1] and
te = ts[hK ] + dur[hK ].
Regular text search engines can not directly support
this, but the code modification and additional CPU cost
is small. The major factor is disk access, which is still
linear with the index size.
We call this index representation ?TMI-base.? It pro-
vides a substantial reduction of number of index entries
compared to the original lattices. However, it is obviously
an approximative representation. In particular, there are
now conditions under which two word hypotheses can be
matched as part of a phrase that were not connected in
the original lattice. This approximation seems sensible,
though, as the words involved are still required to have
Table 1: Test corpus summary.
test set dura- #seg- #keywords WER
tion ments (#multi-word) [%]
podcasts 1.5h 367 3223 (1709) 45.8
videos 1.3h 341 2611 (1308) 50.8
lectures 169.6h 66102 96 (74) 54.8
precisely matching word boundaries. In fact it has been
shown that this representation can be used for direct word-
error minimization during decoding (Wessel, 2000).
For further reduction of the index size, we are now re-
laxing the merging condition. The next two sections will
introduce two alternate ways of merging.
3.3 Arc-Based Merging
A straightforward way is to allow tolerance of time
boundaries. Practically, this is done by the following
bottom-up clustering procedure:
? collect arcs with same word identity;
? find the arc a? with the best posterior, set the result-
ing item time boundary same as a?;
? merge all overlapping arcs a satisfying t[S[a?]] ?
41 ? t[S[a]] ? t[S[a?]] +41 and t[E[a?]]?41 ?
t[E[a]] ? t[E[a?]] +41;
? repeat with remaining arcs.
We call this method ?TMI-arc? to denote its origin from
direct clustering of arcs.
Note that the resulting structure can generally not be
directly represented as a lattice anymore, as formally con-
nected hypotheses now may have slightly mismatching
time boundaries. To compensate for this, the item connec-
tion condition in phrase matching needs to be relaxed as
well: ts[hi+1]?41 ? ts[hi]+dur[hi] ? ts[hi+1]+41.
The storage cost for each TMI-arc item is same as for
TMI-base, while the number of items will be reduced.
3.4 Node-Based Merging
An alternative way is to group ranges of time points,
and then merge hypotheses whose time boundaries got
grouped together.
The simplest possibility is to quantize time points into
fixed intervals, such as 250 ms. Hypotheses are merged
if their quantized time boundaries are identical. This
method we call ?TMI-timequant.?
Besides reducing index size by allowing more item
merging, TMI-timequant has another important property:
since start times and duration are heavily quantized, the
number of bits used for storing the information with the
items in the index can be significantly reduced.
The disadvantage of this method is that loops are fre-
quently being generated this way (quantized duration of
0), providing sub-optimal phrase matching constraints.
To alleviate for this problem, we modify the merging
by forbidding loops to be created: Two time points can be
418
Table 2: Lattice search accuracy on different dataset.
setup best path raw lattice
keywords all sing. mult. all sing. mult.
Phrase spotting, FOM[%]
podcasts 55.0 59.9 50.1 69.5 74.7 64.2
videos 47.0 50.6 43.0 64.4 67.4 61.1
lectures 65.5 69.5 47.1 77.0 80.8 58.8
Relevance ranking, mAP[%]
lectures 52.6 52.7 52.6 61.6 66.4 60.2
grouped together if (1) their difference is below a thresh-
old (like 250 ms); and (2) if there is no word hypothesis
starting and ending in the same group. As a refinement,
the second point is relaxed by a pruning threshold in that
hypotheses with posteriors below the threshold will not
block nodes merging.
Amongst the manifold of groupings that satisfy these
two conditions, the one leading to the smallest number of
groups is considered the optimal solution. It can be found
using dynamic programming:
? line up all existing time boundaries in ascending or-
der, ti < ti+1, i = 1, ? ? ? , N ;
? for each time point ti, find out the furthest time point
that it can be grouped with given the constraints, de-
noting its index as T [ti];
? set group count C[t0] = 1; C[ti] = ?, i > 0;
? set backpointer B[t0] = ?1; B[ti] = ti, i > 0;
? for i = 1, ? ? ? , N :
? for j = i+1, ? ? ? , T [ti]: if C[tj+1] > C[ti]+1:
? C[tj+1] = C[ti] + 1;
? B[tj+1] = ti;
? trace back and merge nodes:
? set k = N , repeat until k = ?1:
? group time points from B[tk] to tk?1;
? k = B[tk].
This method can be applied to the TMI-base represen-
tation, or alternatively directly to the posterior lattice. In
this case, the above algorithm needs to be adapted to op-
erate on nodes rather than time points. The above method
is called ?TMI-node.?
If, as mentioned before, times and durations are stored
as indexes into a look-up table, TMI-node is highly space
efficient. In most cases, the index difference between end
and start point is 1, and in practical terms, the index dif-
ference can be capped by a small number below 10.
4 Results
4.1 Setup
We have evaluated our system on three different corpora,
in an attempt to represent popular types of audio currently
found on the Internet:
? podcasts: short clips ranging from mainstream me-
dia like ABC and CNN to non-professionally pro-
duced edge content;
? video clips, acquired from MSN Video;
? online lectures: a subset of the MIT iCampus lecture
collection (Glass, 2004).
In relation to our goal of web-scale indexing, the pod-
cast and video sets are miniscule in size (about 1.5 hours
each). Nevertheless they are suitable for investigating the
effectiveness of the TMI method w.r.t. phrase spotting
accuracy. Experiments on relevance ranking were con-
ducted only on the much larger lecture set (170 hours).
For the iCampus lecture corpus, the same set of queries
was used as in (Chelba, 2005), which was collected from
a group of users. Example keywords are computer science
and context free grammar. On the other two sets, an au-
tomatic procedure described in (Seide, 2004) was used to
select keywords. Example keywords are playoffs, beach
Florida, and American Express financial services.
A standard speaker-independent trigram LVCSR sys-
tem was used to generate raw speech lattices. For video
and podcasts, models were trained on a combination of
telephone conversations (Switchboard), broadcast news,
and meetings, downsampled to 8 kHz, to accomodate for
a wide range of audio types and speaking styles. For lec-
tures, an older setup was used, based on a dictation engine
without adaptation to the lecture task. Due to the larger
corpus size, lattices for lectures were pruned much more
sharply. Word error rates (WER) and corpus setups are
listed in Table 1. It should be noted that the word-error
rates vary greatly within the podcast and video corpora,
ranging from 30% (clean broadcast news) to over 80%
(accented reverberated speech with a cheering crowd).
Each indexing method is evaluated by a phrase spotting
task and a document retrieval task.
4.1.1 Phrase Spotting
We use the ?Figure Of Merit? (FOM) metric defined by
NIST for word-spotting evaluations. In its original form,
FOM is the detection/false-alarm curve averaged over the
range of [0..10] false alarms per hour per keyword. We
generalized this metric to spotting of phrases, which can
be multi-word or single-word. A multi-word phrase is
matched if all of its words match in order.
Since automatic word alignment can be troublesome
for long audio files in the presence of errors in the ref-
erence transcript, we reduced the time resolution of the
FOM metric and used the sentence as the basic time unit.
A phrase hit is considered correct if an actual occurence
of the phrase is found in the same sentence. Multiple hits
of the same phrase within one sentence are counted as a
single hit, their posterior probabilities being summed up
for ranking.
The segmentation of the audio files is based on the ref-
erence transcript. Segments are on average about 10 sec-
onds long. In a real system, sentence boundaries are of
course unknown, but previous experiments have shown
419
Table 3: Comparison of different indexing methods. Only results for multi-words queries are shown, because results
for single-word queries are identical across lattice-indexing methods (approximately identical in the case of pruning.)
dataset podcasts videos lectures
FOM [%] size FOM [%] size FOM [%] mAP [%] size
bestpath 50.1 1.1 43.0 1.0 47.1 52.6 1.0
raw lattice 64.2 527.6 61.1 881.7 58.8 60.2 23.3
Pnode = const 64.3 527.6 61.1 881.7 58.8 60.3 23.3
no pruning
TMI-base 65.3 55.2 62.6 78.8 58.8 60.2 7.7
TMI-arc 62.9 16.1 58.5 20.7 57.9 60.1 4.4
TMI-timequant 66.7 15.4 64.2 19.5 58.8 60.3 4.5
TMI-node 66.5 20.7 63.4 27.6 58.7 59.7 4.4
PSPL 68.9 182.0 66.2 212.0 58.7 61.0 21.2
pruned to about 5 entries per spoken word
TMI-base 62.1 5.6 54.1 5.1 57.0 60.3 4.5
TMI-arc 60.7 4.6 53.6 5.0 57.9 60.1 4.4
TMI-timequant 63.1 4.7 57.1 5.1 58.8 60.3 4.5
TMI-node 63.7 4.6 57.7 5.1 58.7 59.7 4.4
PSPL 57.3 6.0 49.8 5.8 53.6 61.0 4.4
that the actual segmentation does not have significant im-
pact on the results.
4.1.2 Relevance Ranking
The choice and optimization of a relevance ranking for-
mula is a difficult problem that is beyond the scope of this
paper. We chose a simple document ranking method as
described in (Chelba, 2005):
Given query Q = (q1, ? ? ? , qL), for each document
D, expected term frequencies (ETF) of all sub-strings
Q[i,j] = (qi, ? ? ? , qj) are calculated:
ETF(Q[i,j]|D)=
?
ts,te
P (?, ts, Q[i,j], te, ?|O,D) (3)
A document is returned if all query words are present. The
relevance score is calculated as
S(D,Q)=
L?
i=1
L?
j=i
wj?i log[1+ETF(Q[i,j]|D)] (4)
where the weights w` have the purpose to give higher
weight to longer sub-strings. They were chosen as w` =
1 + 1000 ? `, no further optimization was performed.
Only the lecture set is used for document retrieval eval-
uation. The whole set consists of 169 documents, with an
average of 391 segments in each document. The eval-
uation metric is the mean average precision (mAP) as
computed by the standard trec_eval package used by
the TREC evaluations (NIST, 2005). Since actual rele-
vance judgements were not available for this corpus, we
use the output of a state-of-the-art text retrieval engine on
the ground truth transcripts as the reference. The idea is
that if human judgements are not available, the next best
thing to do is to assess how close our spoken-document
retrieval system gets to a text engine applied to reference
transcripts. Although one should take the absolute mAP
scores with a pinch of salt, we believe that comparing the
relative changes of these mAP scores is meaningful.
4.2 Lattice Search and Best Path Baseline
Table 2 lists the word spotting and document retrieval re-
sult of direct search in the original raw lattice, as well
as for searching the top-1 path. Results are listed sepa-
rately for single- and multi-word queries. For the phrase-
spotting task, a consistent about 15% improvement is
observed on all sets, re-emphasizing the importance of
searching alternates. For document retrieval, the accuracy
(mAP) is also significantly improved from 53% to 62%.
4.2.1 Comparing Indexing Methods
Table 3 compares different indexing methods with re-
spect to search accuracy and index size. We only show
results for multi-words queries results, as it can be shown
that results for single-word queries must be identical. The
index size is measured as index entries per spoken word,
i.e. it does not reflect that different indexing methods may
require different numbers of bits in the actual index store.
In addition to four types of TMI methods, we include
an alternative posterior-lattice indexing method in our
comparison called PSPL (position-specific posterior lat-
tices) (Chelba, 2005). A PSPL index is constructed by
enumerating all paths through a lattice, representing each
path as a linear text, and adding each text to the index,
each time starting over from word position 1. Each word
hypothesis on each path is assigned the posterior proba-
bility of the entire path. Instances of the same word oc-
curing at the same text position are merged, accumulating
their posterior probabilities. This way, each index entry
represents the posterior probability that a word occurs at
a particular position in the actual spoken word sequence.
PSPL is an attractive alternative to the work presented in
420
02
46
810
1214
1618
20
48 53 58 63 68Phrase Spotting Accuracy (Figure Of Merit [%])(a) podcasts
i
n
d
e
x
 
e
n
t
r
i
e
s
 
/
 
s
p
o
k
e
n
 
w
o
r
d bestpathbaselinePSPLTMI-baseTMI-arcTMI-tqTMI-node
02
46
810
1214
1618
20
42 47 52 57 62Phrase Spotting Accuracy (Figure Of Merit [%])(b) videos
i
n
d
e
x
 
e
n
t
r
i
e
s
 
/
 
s
p
o
k
e
n
 
w
o
r
d bestpathbaselinePSPLTMI-baseTMI-arcTMI-tqTMI-node
02
46
810
1214
1618
20
40 45 50 55 60Phrase Spotting Accuracy (Figure Of Merit [%])(c) lectures
i
n
d
e
x
 
e
n
t
r
i
e
s
 
/
 
s
p
o
k
e
n
 
w
o
r
d bestpathbaselinePSPLTMI-baseTMI-arcTMI-tqTMI-node
02
46
810
1214
1618
20
52 54 56 58 60 62 64Relevance Ranking Accuracy (mAP [%])(d) lectures
i
n
d
e
x
 
e
n
t
r
i
e
s
 
/
 
s
p
o
k
e
n
 
w
o
r
d bestpathbaselinePSPLTMI-baseTMI-arcTMI-tqTMI-node
Figure 2: Index size vs. accuracy for different pruning thresholds for word-spotting on (a) podcasts, (b) videos, (c)
lectures, and (d) relevance ranking for lectures.
this paper because it continues to use the notion of a word
position instead of time, with the advantage that exist-
ing implementations of phrase-matching conditions apply
without modification.
The results show that, comparing with the direct raw-
lattice search, all indexing methods have only slight im-
pact on both word spotting and document retrieval accu-
racies. Against our expectation, in many cases improved
accuracies are observed. These are caused by creating ad-
ditonal paths compared to the original lattice, improving
recall. It is not yet clear how to exploit this in a systematic
manner.
W.r.t. storage efficiency, the TMI merging methods
have about 5 times less index entries than the original lat-
tice for lectures (and an order of magnitude less for pod-
casts and videos that were recognized with rather waste-
ful pruning thresholds). This can be further improved by
pruning.
4.2.2 Pruning
Index size and accuracy can be balanced by pruning
low-scoring index entries. Experiments have shown that
the optimal pruning strategy differs slightly from method
to method. For the TMI set, the index is pruned by remov-
ing all entries with posterior probabilities below a certain
fixed threshold. In addition, for TMI-node we enforce
that the best path is not pruned. For PSPL, an index entry
at a particular word position is removed if its posterior is
worse by a fixed factor compared to the best index entry
for the same word position. This also guarantees that the
best path is never pruned.
Fig. 2 depicts the trade-off of size and accuracy for
different indexing methods. TMI-node provides the best
trade-off. The last block of Table 3 shows results for all
indexing methods when pruned with the respective prun-
ing thresholds adjusted such that the number of index en-
tries is approximately five times that for the top-1 tran-
script. We chose this size because reducing the index size
still has limited impact on accuracy (0.5-points for pod-
casts, 3.5 for videos, and none for lectures) while keeping
operating characteristics (storage size, CPU, disk) within
an order of magnitude from text search.
5 The System
The presented technique was implemented in a research
prototype shown in Fig. 3. About 780 hours of audio doc-
uments, including video clips from MSN Video and audio
files from most popular podcasts, were indexed. The in-
dex is disk-based, its size is 830 MB, using a somewhat
wasteful XML representation for research convenience.
Typically, searches are executed within 0.5 seconds.
The user interface resembles a typical text search en-
gine. A media player is embedded for immediate within-
page playback. Snippets are generated for previewing the
search results. Each word in a snippet has its original
time point associated, and a click on it positions the me-
dia player to the corresponding time in the document.
6 Conclusion
We targeted the paper to the task of searching audio con-
tent from the Internet. Aiming at maximizing reuse of
existing web-search engines, we investigated how best to
421
Figure 3: Screenshot of the video/audio-search prototype. For each document, in addition to the title and description
text from meta-data, the system displays recognition-transcript snippets around the audio hits, e.g. ?... bird flu has
been a ...? in the first document. Clicking on a word in a snippet starts playing back the video at that position using
the embedded video player.
represent important lattice properties ? recognition alter-
nates with scores, time boundaries, and phrase-matching
constraints ? in a form suitable for large-scale web-search
engines, while requiring only limited code changes.
The proposed method, Time-based Merging for Index-
ing (TMI), first converts the word lattice to a posterior-
probability representation and then merges word hypothe-
ses with similar time boundaries to reduce the index size.
Four approximations were presented, which differ in size
and the strictness of phrase-matching constraints.
Results were presented for three typical types of web
audio content ? podcasts, video clips, and online lectures
? for phrase spotting and relevance ranking. Using TMI
indexes that are only five times larger than corresponding
linear-text indexes, accuracy was improved over search-
ing top-1 transcripts by 25-35% for word spotting and
14% for relevance ranking, very close to what is gained
by a direct search of unindexed lattices.
Practical feasibility has been demonstrated by a re-
search prototype with 780 hours indexed audio, which
completes searches within 0.5 seconds.
To our knowledge, this is also the first paper to report
speech recognition results for podcasts.
7 Acknowledgements
The authors wish to thank Jim Glass and T. J. Hazen at
MIT for providing the iCampus data.
References
S. Brin and L. Page, The anatomy of a large-scale hypertextual
Web search engine. Computer Networks and ISDN Systems,
30(1-7):107-117.
C. Chelba and A. Acero, Position specific posterior lattices for
indexing speech. Proc. ACL?2005, Ann Arbor, 2005.
J. Garofolo, TREC-9 Spoken Document Retrieval Track.
National Institute of Standards and Technology, http://
trec.nist.gov/pubs/trec9/sdrt9_slides/
sld001.htm.
J. Glass, T. J. Hazen, L. Hetherington, C. Wang, Analysis and
Processing of Lecture Audio data: Preliminary investiga-
tion. Proc. HLT-NAACL?2004 Workshop: Interdisciplinary
Approaches to Speech Indexing and Retrieval, Boston, 2004.
L. Mangu, E. Brill, A. Stolcke, Finding Consensus in Speech
Recognition: Word Error Minimization and Other Applica-
tions of Confusion Networks. Computer, Speech and Lan-
guage, 14(4):373-400.
MSN Video. http:// video.msn.com.
The TREC evaluation package. http:// www - lpir . nist
. gov / projects / trecvid / trecvid . tools /
trec_eval.
M. Saraclar, R. Sproat, Lattice-based search for spoken utter-
ance retrieval. Proc. HLT?2004, Boston, 2004.
F. Seide, P. Yu, et al, Vocabulary-independent search in sponta-
neous speech. Proc. ICASSP?2004, Montreal, 2004.
F. Wessel, R. Schlu?ter, and H. Ney, Using posterior word proba-
bilities for improved speech recognition. Proc. ICASSP?2000,
Istanbul, 2000.
P. Yu, K. J. .Chen, L. Lu, F. Seide, Searching the Audio
Notebook: Keyword Search in Recorded Conversations.
Proc. HLT?2005, Vancouver, 2005.
P. Yu, K. J. Chen, C. Y. Ma, F. Seide, Vocabulary-Independent
Indexing of Spontaneous Speech, IEEE transaction on
Speech and Audio Processing, Vol.13, No.5, Special Issue
on Data Mining of Speech, Audio and Dialog.
P. Yu, F. Seide, A hybrid word / phoneme-based approach
for improved vocabulary-independent search in spontaneous
speech. Proc. ICLSP?04, Jeju, 2004.
422
