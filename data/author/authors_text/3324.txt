Finding Errors Automatically in
Semantically Tagged Dialogues
John Aberdeen, Christine Doran, Laurie Damianos,
Samuel Bayer and Lynette Hirschman
The MITRE Corporation
202 Burlington Road
Bedford, MA 01730 USA
+1.781.271.2000
{aberdeen,cdoran,laurie,sam,lynette}@mitre.org
ABSTRACT
We describe a novel method for detecting errors in task-based
human-computer (HC) dialogues by automatically deriving
them from semantic tags. We examined 27 HC dialogues from
the DARPA Communicator air travel domain, comparing user
inputs to system responses to look for slot value
discrepancies, both automatically and manually. For the
automatic method, we labeled the dialogues with semantic tags
corresponding to "slots" that would be filled in "frames" in
the course of the travel task. We then applied an automatic
algorithm to detect errors in the dialogues. The same dialogues
were also manually tagged (by a different annotator) to label
errors directly. An analysis of the results of the two tagging
methods indicates that it may be possible to detect errors
automatically in this way, but our method needs further work
to reduce the number of false errors detected. Finally, we
present a discussion of the differing results from the two
tagging methods.
Keywords
Dialogue, Error detection, DARPA Communicator.
1. INTRODUCTION
In studying the contrasts between human-computer (HC) and
human-human (HH) dialogues [1] it is clear that many HC
dialogues are plagued by disruptive errors that are rarely seen
in HH dialogues. A comparison of HC and HH dialogues may
help us understand such errors. Conversely, the ability to
detect errors in dialogues is critical to understanding the
differences between HC and HH communication.
Understanding HC errors is also crucial to improving HC
interaction, making it more robust, trustworthy and efficient.
The goal of the work described in this paper is to provide an
annotation scheme that allows automatic calculation of
misunderstandings and repairs, based on semantic information
presented at each turn. If we represent a dialogue as a sequence
of pairs of partially-filled semantic frames (one for the user?s
utterances, and one for the user?s view of the system state), we
can annotate the accumulation and revision of information in
the paired frames.  We hypothesized that, with such a
representation, it would be straightforward to detect when the
two views of the dialogue differ (a misunderstanding), where
the difference originated (source of error), and when the two
views reconverge (correction). This would be beneficial
because semantic annotation often is used for independent rea-
sons, such as measurements of concepts per turn [8],
information bit rate [9], and currently active concepts [10].
Given this, if our hypothesis is correct, then by viewing
semantic annotation as a representation of filling slots in user
and system frames, it should be possible to detect errors
automatically with little or no additional annotation.
2. SEMANTIC TAGGING
We tagged 27 dialogues from 4 different systems that
participated in a data collection conducted by the DARPA
Communicator  program in the summer of 2000. These are
dialogues between paid subjects and spoken language
dialogue systems operating in the air travel domain. Each
dialogue was labeled with semantic tags by one annotator. We
focused on just the surface information available in the
dialogues, to minimize inferences made by the annotator.
The semantic tags may be described along two basic
dimensions: slot and type. The slot dimension describes the
items in a semantic frame that are filled over the course of a
dialogue, such as DEPART_CITY and AIRLINE (see Table 1 for
the complete list).
The type dimension describes whether the tag is a PROMPT, a
FILL, or an OFFER. This type dimension is critical to semantic
analysis since it allows one to describe the effect a tag has on
slots in the frame. PROMPTs are attempts to gather values to
fill slots, e.g., "what city do you want to fly to". FILLs are
actual slot fills, e.g., "I?d like to fly to San Francisco". OFFERs
represent actual flight information based on previous slot
FILLs, e.g., "there is a 9:45 flight to San Francisco on Delta".
However, OFFERs often do not exactly match slot FILLs (e.g.,
the user requests a flight at 9:30, but the closest match flight
is at 9:45), and thus must be distinguished from FILLs.
In addition to the two basic dimensions of slot and type, each
tag takes a leg attribute to indicate which leg of a trip is being
discussed. There is also an initial USER_ID slot which has two
types (PROMPT_USER_ID and FILL_USER_ID), but no leg
attribute.
Our semantic tag set alo includes two special tags, YES and
NO, for annotating responses to offers and yes/no questions.
Finally, we have two tags, PROMPT_ERASE_ FRAMES and
FILL_ERASE_FRAMES, for annotating situations where the
frames are erased and the dialogue is restarted (e.g., the user
says "start over"). Figure 1 shows part of a sample dialogue
with semantic tags. Our semantic tagset is summarized in Table
1.
Table 1. Semantic Tagset
PROMPT FILL OFFER
DEPART_CITY X X X
ARRIVE_CITY X X X
DEPART_AIRPORT X X X
ARRIVE_AIRPORT X X X
DATE X X X
DEPART_TIME X X X
ARRIVE_TIME X X X
AIRLINE X X X
USER_ID X X
ERASE_FRAMES X X
YES (single bare tag)
NO (single bare tag)
3. ERROR DETECTION
To provide a baseline for comparison to an algorithm that
detects errors automatically, we had an annotator (not the same
person who did the semantic tagging described above)
manually tag the problem areas. This annotator marked four
items:
(1) occurrence: where the problem first occurs in the
dialogue (e.g. where the user says the item which the
system later incorporates incorrectly)
(2) detection: where the user could first be aware that
there is a problem (e.g. where the system reveals its
mistake)
(3) correction attempt: where the user attempts to repair
the error
(4) correction detection: where the user is first able to
detect that the repair has succeeded
We next developed an algorithm for automatically finding
errors in our semantically tagged dialogues. In this phase of
the research, we concentrated on deriving an automatic method
for assigning the first two of the four error categories,
occurrence and detection (in a later phase we plan to develop
automatic methods for correction attempt and correction
detection). First, the algorithm derives the turn-by-turn frame
states for both the user's utterances and the system's utterances
(i.e., what the user heard the system say), paying special
attention to confirmation tags such as YES or deletion tags
like FILL_ERASE_FRAMES. Then, the algorithm compares
patterns of user and system events to hypothesize errors.
Occurences and detections are hypothesized for three types of
errors: hallucinations (system slot fill without user slot fill),
mismatches (system slot fill does not match user slot fill), and
prompts after fills (system prompt after user slot fill).
Figure 2 shows a sample dialogue that illustrates several error
types. Utterance S12 shows a prompt after fill error ? the user
has already supplied (in utterance U11) the information the
system is requesting. In utterance U13 the user supplies
contradictory information, and the system catches this and
tries to resolve it in utterances S14 and S16. Next a mismatch
error is illustrated ? the user specifies ARRIVE_CITY in
utterance U17, and the system shows that it has misrecognized
Figure 1. Portion of a Sample Dialogue Labeled with Semantic Tags
 S0: System said: Welcome to our travel planning system.
<prompt_user_id>Please say your PIN number</prompt_user_id>.
U1: User said: My PIN is <fill_user_id>four four four four</fill_user_id>.
 S2: Sys tem said: What are your travel plans?
U3: User said: I want to go to <fill_arrive_city leg=?1?>San Francisco</fill_arrive_city>.
 S4: System said: Traveling to <fill_arrive_city leg=?1?>San Francisco</fill_arrive_city>.
<prompt_depart_city leg=?1?>Where are you departing from?</prompt_depart_city>
U5: User said: <fill_depart_city leg=?1?>Boston</fill_depart_city>.
 S6: System said: <prompt_date leg=?1?>When would you like to leave</prompt_date>
<fill_depart_city leg=?1?>Boston</fill_depart_city>?
U7: User said: <fill_date leg=?1?>July twenty-fifth</fill_date> at
<fill_depart_time leg=?1?>12 noon</fill_depart_time>.
 S8: System said: I have a <offer_airline leg=?1?>Delta</offer_airline> flight that leaves
<offer_depart_city leg=?1?>Boston</offer_depart_city> at
<offer_depart_time leg=?1?>11:10AM</offer_depart_time> on
<offer_date leg=?1?>July twenty-fifth</offer_date> and arrives in
<offer_arrive_city leg=?1?>San Francisco</offer_arrive_city> at
<offer_arrive_time>5:19PM</offer_arrive_time>. Is that OK?
it in utterance S18. The user attempts to correct this
misrecognition in utterance U21, and as can be seen from
utterance S22, the system again has misrecognized the user?s
utterance.
Below we describe the results from running the automatic
algorithm on our 27 semantically tagged dialogues.
4. RESULTS
In the 27 dialogues considered, a total of 131 items were
flagged by one or both of the methods as error items (60 occur,
71 detect). A breakdown of these errors and which method
found them is in Table 2.
Table 2. Unique Errors Identified
# errors found by: Occur Detect Total
Both Methods 14 23 37
Automatic Only 28 38 66
Manual Only 18 10 28
Totals 60 71 131
As can be seen in Table 2 the automatic method flagged many
more items as errors than the manual method.
Table 3. Error Judgements
Occur Detect
E NE Q E NE Q
Auto 48% 40% 12% 52% 38% 10%
Man 84% 13% 3% 82% 15% 3%
We carefully examined each of the items flagged as errors by
the two methods. Three judges (the semantic tagging
annotator, the manual error tagging annotator, and a third
person who did not participate in the annotation) determined
which of the errors found by each of the two methods were real
errors (E), not real errors (NE), or questionable (Q). For
calculations in the present analysis, we used E as the baseline
of real errors, rather than E+Q. Table 3 shows the judgements
made for both the automatic and manual method, which are
discussed in the next section. It is important to note that
human annotators do not perform this task perfectly, with error
rates of 13% and 15%. This is also shown in the precision and
recall numbers for the two methods in Table 4.
Table 4. Precision & Recall
Occur DetectPrecision
& Recall P R P R
Automatic 0.48 0.57 0.52 0.84
Manual 0.84 0.77 0.82 0.71
5. ANALYSIS
The automatic method flagged 40 items as errors that the
judges determined were not errors (17 occur, 23 detect). These
40 false errors can be classified as follows:
A. 10 were due to bugs in the algorithm or source data
B. 19 were false errors that can be eliminated with non-
trivial changes to the semantic tagset and/or algorithm
C. 3 were false errors that could not be eliminated
without the ability to make inferences about world
knowledge
D. 8 were due to mistakes made by the semantic
annotator
One example of the 19 false errors above in B is when the first
user utterance in a dialogue is a bare location, it is unclear
whether the user intends it to be a departure or arrival location.
Our semantic tagset currently has no tags for ambiguous
situations such as these. Adding underspecified tags to our
tagset (and updating the automatic algorithm appropriately)
would solve this problem. Another example is a situation
where a system was legitimately asking for clarification about
a slot fill, but the algorithm flagged it as prompting for keys
that had already been filled. This could be fixed by adding a
CLARIFY element to the type dimension (currently PROMPT,
FILL, and OFFER). We believe that making these changes
would not compromise the generality of our semantic tagset.
However, as the point of our approach is to derive errors
without much additional annotation, additions to the semantic
tagset should only be made when there is substantial
justification.
There were also 21 errors (15 occur, 6 detect) that were not
detected by the automatic method, but were judged as real
errors. These 21 errors may be categorized as follows:
A. 2 were due to bugs in the algorithm
B. 8 were situations where the algorithm correctly
flagged the detect point of an error, but missed the
associated occur point
C. 6 were situations that could be fixed by
modifications to the semantic tagset
D. 1 was an error that could be fixed either by a
revision to the semantic tagset or a revision to the
algorithm
E. 2 were situations where the system ignored a user
fill, and the automatic algorithm interpreted it as no
confirmation (not an error). Human judgement is
required to detect these errors
F. 2 were due to mistakes made by the semantic
annotator
6. PREVIOUS WORK
In Hirschman & Pao [5], annotation was done by manual
inspection of the exchanges in the dialogue. Each exchange
was evaluated based on the portion of information "visible to
the other party". Errors and problems were identified manually
and traced back to their point of origin. This is quite similar to
our baseline manual annotation described in section 3.
There have been other approaches to detecting and
characterizing errors in HC dialogues. Danieli [2] used
expectations to model future user ut terances, and Levow [6][7]
used utterance and pause duration, as well as pitch variability
to characterize errors and corrections. Dybkj?r, Bernsen &
Dybkj?r [4] developed a set of principles of cooperative HC
dialogue, as well as a taxonomy of errors typed according to
which of the principles are violated. Finally, Walker et. al.
[11][12] have trained an automatic classifier that identifies
and predicts problems in HC dialogues.
7. DISCUSSION
It is clear that our algorithm and semantic tagset, as they stand
now, need improvements to reduce the number of false errors
detected. However, even now the automatic method offers some
advantages over tagging errors manually, the most important
of which is that many researchers already annotate their
dialogues with semantic tags for other purposes and thus
many errors can be detected with no additional annotation.
Also, the automatic method associates errors with particular
slots, enabling researchers to pinpoint aspects of their
dialogue management strategy that need the most work.
Finally, Day et. al. [3] have shown that correcting existing
annotations is more time efficient than annotating from
scratch. In this way, the automatic method may be used to
"seed" an annotation effort, with later hand correction.
8. ACKNOWLEDGMENTS
This work was funded by the DARPA Communicator program
under contract number DAAB07-99-C201.  ? 2001 The MITRE
Corporation. All rights reserved.
9. REFERENCES
[1] Aberdeen, J. and Doran, C. Human-computer and human-
human dialogues. DARPA Communicator Principle
Investigators Meeting (Philadelphia, PA USA 2000).
http://www.dsic-web.net/ito/meetings/communicator
_sep2000/
[2] Danieli, M. On the use of expectations for detecting and
repairing human-machine miscommunication.
Proceedings of AAAI Workshop on Detecting, Repairing
and Preventing Human-Machine Miscommunication
(Portland OR, USA 1996).
[3] Day, D., Aberdeen, J., Hirschman, L., Kozierok, R.,
Robinson, P. and Vilain, M. Mixed-initiative
development of language processing systems. In
Proceedings of the Fifth Conference on Applied Natural
Language Processing (Washington DC, USA 1997).
[4] Dybkj?r, L., Bernsen, N.O. and Dybkj?r, H. Reducing
miscommunication in spoken human-machine dialogue.
Proceedings of AAAI Workshop on Detecting, Repairing
and Preventing Human-Machine Miscommunication
(Portland OR, USA 1996).
[5] Hirschman, L. and Pao, C. The cost of errors in a spoken
language system. Proceedings of the Third European
Figure 2. Portion of a Sample Dialogue Illustrating Several Errors
   U11: User said: I want to fly from <fill_depart_city leg=?1?>Seattle</fill_depart_city> to
<fill_arrive_city leg=?1?>Tokyo</fill_arrive_city>
     S12: System said: <prompt_arrive_city leg="1">to which city would you like to fly</prompt_arrive_city>
from <fill_depart_city leg="1">seattle</fill_depart_city>
U13: User said: <fill_arrive_city leg="1">seattle washington</fill_arrive_city>
    S14: System said: excuse me, i didn't hear you well. I think you want to fly from
<fill_depart_city leg="1">seattle</fill_depart_city> If this is okay, let's continue. Otherwise,
please specify the locations again.
  U15: User said: <yes>that's okay</yes>
    S16: System said: <prompt_arrive_city leg="1">what city would you like to fly to?</prompt_arrive_city>
    U17: User said: <fill_arrive_city leg="1">tokyo japan</fill_arrive_city>
     S18: System said: flying to <fill_arrive_city leg="1">togiak</fill_arrive_city>
<prompt_date leg="1">on what date?</prompt_date>
  U19: User said: <fill_date leg="1">monday September twenty fifth</fill_date>
  S20: System said: there don't seem to be any flights from <fill_depart_city leg="1">seattle</fill_depart_city>
to <fill_arrive_city leg="1">togiak</fill_arrive_city> which depart
<fill_depart_time leg="1">about ten PM</fill_depart_time> on
<fill_date leg="1">september twenty fifth</fill_date>.
  U21: User said:  destination <fill_arrive_city leg="1">tokyo japan</fill_arrive_city>
  S22: System said: did you say you want to fly to
<prompt_arrive_city leg="1">san diego</prompt_arrive_city>?
Conference on Speech Communication and Technology
(Berlin, Germany 1993).
[6] Levow, G.A. Characterizing and recognizing spoken
corrections in human-computer dialogue. Proceedings of
COLING-ACL (Montreal, Canada 1998).
[7] Levow, G.A. Understanding recognition failures in spoken
corrections in human-computer dialogue. Proceedings of
ECSA Workshop on Dialogue and Prosody (Eindhoven,
The Netherlands 1999).
[8] Luo, X. and Papineni, K. IBM DARPA Communicator v1.0.
DARPA Communicator Principle Investigators Meeting
(Philadelphia, PA USA 2000). http://www.dsic-web.net
/ito/meetings/communicator_sep2000/
[9] Polifroni, J. and Seneff, S. Galaxy-II as an architecture for
spoken dialogue evaluation. Proceedings of the Second
International Conference on Language Resources and
Evaluation (Athens, Greece 2000).
[10] Rudnicky, A. CMU Communicator. DARPA Communicator
Principle Investigators Meeting (Philadelphia, PA USA
2000). http://www.dsic-web.net/ito/meetings
/communicator_sep2000/
[11] Walker, M., Langkilde, I., Wright, J., Gorin, A. and Litman,
D. Learning to predict problematic situations in a spoken
dialogue system: experiments with how may I help you?
Proceedings of the Seventeenth International Conference
on Machine Learning (Stanford, CA USA 2000).
[12] Walker, M., Wright, J. and Langkilde, I. Using natural
language processing and discourse features to identify
understanding errors in a spoken dialogue system.
Proceedings of the North American Meeting of the
Association of Computational Linguistics (Seattle, WA
USA 2000).
Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
MiTAP for SARS Detection 
 
 
Laurie E. Damianos, Samuel Bayer, 
Michael A. Chisholm, John Henderson,  
Lynette Hirschman, William Morgan, 
Marc Ubaldino, Guido Zarrella 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730 
{laurie, sam, chisholm, 
jhndrsn, lynette, wmorgan, 
ubaldino,jzarrella}@mitre.org 
James M. Wilson, V, MD and  
Marat G. Polyak 
Division of Integrated Biodefense 
ISIS Center, Georgetown University 
2115 Wisconsin Avenue Suite 603 
Washington, DC 20007 
{wilson, mgp5} 
@isis.imac.georgetown.edu 
 
Abstract 
The MiTAP prototype for SARS detection 
uses human language technology for detect-
ing, monitoring, and analyzing potential indi-
cators of infectious disease outbreaks and 
reasoning for issuing warnings and alerts. Mi-
TAP focuses on providing timely, multi-
lingual information access to analysts, domain 
experts, and decision-makers worldwide. Data 
sources are captured, filtered, translated, 
summarized, and categorized by content. 
Critical information is automatically extracted 
and tagged to facilitate browsing, searching, 
and scanning, and to provide key terms at a 
glance. The processed articles are made avail-
able through an easy-to-use news server and 
cross-language information retrieval system 
for access and analysis anywhere, any time. 
Specialized newsgroups and customizable fil-
ters or searches on incoming stories allow us-
ers to create their own view into the data 
while a variety of tools summarize, indicate 
trends, and provide alerts to potentially rele-
vant spikes of activity. 
1 
2 
Background 
Potentially catastrophic biological events that threaten 
US national security are steadily increasing in fre-
quency. These events pose immediate danger to ani-
mals, plants, and humans. Current disease surveillance 
systems are inadequate for detecting indicators early 
enough to ensure the rapid response needed to combat 
these biological events and corresponding public reac-
tion. Recent examples of outbreaks include both the 
HIV/AIDS and foot and mouth pandemics, the spread of 
West Nile virus to and across the US, the escape of Rift 
Valley Fever from Africa, SARS, and the translocation 
of both mad cow disease (BSE) and monkey pox to the 
United States.  
Biological surveillance systems in the United States 
rely most heavily on human medical data for signs of 
epidemic activity. These systems span multiple organi-
zations and agencies, are often not integrated, and have 
no alerting capability. As a result, responders have an 
insufficient amount of lead time to prepare for biologi-
cal events or catastrophes. 
Indications and Warnings (I&Ws) provide the poten-
tial for early alert of impending biological events, per-
haps weeks to months in advance. Sources of I&Ws 
include transportation data, telecommunication traffic, 
economic indices, Internet news, RSS feeds (RSS) in-
cluding weblogs, commerce, agricultural surveillance, 
weather, and other environmental data. Retrospective 
analyses of major infectious disease outbreaks (e.g., 
West Nile Virus and SARS) show that I&Ws were pre-
sent weeks to months in advance, but these indicators 
were missed because data sources were difficult to ob-
tain and hard to integrate. As a result, the available in-
formation was not utilized for appropriate national and 
international response. This illuminates a critical need in 
biodefense for an integrated system linking I&Ws for 
biological events from multiple and disparate sources 
with the response community. 
Introduction 
MiTAP (Damianos et al 2002) was originally devel-
oped by the MITRE Corporation under the Defense 
Advanced Research Projects Agency (DARPA) 
Translingual Information Detection Extraction and 
Summarization (TIDES) program. TIDES aims to revo-
lutionize the way that information is obtained from hu-
man language by enabling people to find and interpret 
relevant information quickly and effectively, regardless 
of language or medium. MiTAP was initially created for 
tracking and monitoring infectious disease outbreaks 
and other biological threats as part of a DARPA Inte-
grated Feasibility Experiment in biosecurity to explore 
the integration of synergistic TIDES language process-
ing technologies applied to a real world domain. The 
system has since been expanded to other domains such 
as weapons of mass destruction, satellite monitoring, 
and suspect terrorist activity. In addition, researchers 
and analysts are examining hundreds of MiTAP data 
sources for differing perspectives on conflict and hu-
manitarian relief efforts. 
Our newest MiTAP prototype explores the integra-
tion of outputs from operational data mining (anomaly 
detection), human language technology (information 
extraction, temporal tagging, machine translation, cross-
language information retrieval), and visualization tools 
to detect SARS-specific I&Ws in Asia, with relevance 
to pathogen translocation to the United States. Using 
feeds from English and Chinese language newswire, 
weblogs, and other Internet data, the system translates 
Chinese text data and tracks keyword combinations 
thought to represent I&Ws specific to SARS outbreaks 
in China. Analysts can use cross-language information 
retrieval for retrospective analysis and improving the 
I&W model, save searches to use as filters on incoming 
data, view trends, and visualize the data along a time-
line. Figure 1 shows an overview of the prototype. 
Warnings generated by this MiTAP prototype are in-
tended to complement traditional biosurveillance and 
communications already in use by the international pub-
lic health community. This system represents an expan-
sion of current US surveillance capabilities to detect 
biological agents of catastrophic potential.
 
 
Figure 1 Overview of the MiTAP prototype for SARS detection. 
3 Component Technologies 
The MiTAP prototype relies extensively on human 
language technology and expert system reasoning. 
Below, MiTAP capabilities are described briefly 
along with their contributing component 
technologies. 
3.1 
3.2 
3.3 
3.4 
3.5 
Information Processing 
After Internet news sources are captured and 
normalized, they are passed through a zoner using 
human-generated rules to identify source, date, and 
other information such as headline, or title, and 
content. The Alembic natural language analyzer (Ab-
erdeen et al 1995; Vilain and Day 1996) processes 
the zoned messages to identify paragraph, sentence, 
and word boundaries as well as part-of-speech tags. 
The messages then pass through the Alembic named 
entity recognizer for identification and tagging of 
person, organization, location, and disease names. 
Finally, the article is processed by the TempEx 
normalizing time expression tagger (Mani and Wil-
son 2000). 
For Chinese and other non-English sources, the 
CyberTrans machine translation system (Miller et al 
2001) is used to translate articles automatically into 
English. CyberTrans wraps commercial and research 
translation engines to produce a common set of 
interfaces; the current prototype makes use of the 
SYSTRAN Chinese-English system.  
RSS feeds can provide a high volume textual ge-
stalt.  Weblogs, in particular, are a good source of 
timely text, some of which is topical and all of which 
is based on personal observations and experiences. 
Aggregate measurements on these feeds can provide 
indications of public health-related phenom-
ena.  Consider the relative rates of words and phrases 
such as "stay home from" or "pneumonia.?  Geotem-
poral location of non-seasonal spikes in relative rank 
of these strings can establish suspicion for further 
investigation by I&W experts. 
Browsing 
English language data and pairs of foreign language 
documents and their translated versions are made 
available on a news server (INN 2001) for browsing. 
The system categorizes and bins articles into 
newsgroups based on their content. To do this, the 
system relies on a combination of the information 
extraction results as well as human-generated rules 
for pattern matching. Newsgroups are created to 
provide multiple perspectives on the data; analysts 
can subscribe to specific disease tracking 
newsgroups, regional newsgroups, specific data 
source newsgroups, or to customized topic tracking 
newsgroups that may be based on several related 
subjects. 
Tagged entities in each article are color-coded to 
enable rapid scanning of information and easy identi-
fication of key names. The five most frequently men-
tioned locations in each article as well as the top five 
people are presented as a list for quick reference. 
Information Retrieval 
To supplement access to the articles on the news 
server and to allow for retrospective analysis, articles 
are indexed using the Lucene information retrieval 
system (The Jakarta Project 2001) for English 
language documents and using PSE (Darwish 2002) 
for foreign language documents. Web links are 
maintained between foreign language documents and 
their translated versions to allow for more accurate 
human translations of selected documents. 
Analysts can perform full text, source-specific 
queries over the entire set of archived documents and 
view the retrieved results as a relevance-ranked list or 
as a plot across a timeline. A cross-language informa-
tion retrieval interface allows users to search in Eng-
lish across the Chinese language sources. 
Users can also save specific search constraints to 
be used as filters on incoming data. These saved 
searches provide a simple analytic capability as well 
as an alerting feature. (See below.) 
Analysis 
To assist analysts in identifying relevant and related 
articles, we have integrated multi-document summa-
rization and watch lists. Columbia University?s 
Newsblaster (McKeown et al 2002) automatically 
detects daily topics, clusters MiTAP articles around 
those topics, and generates multi-document summari-
zations which are made available on the news server. 
Multiple technologies (e.g., coreference, information 
extraction) from Alias I, Inc. (Baldwin et al 2002) 
produces comprehensive views on specific named 
entities (i.e., people or disease) across MiTAP docu-
ments. These views are summarized through ranked 
lists, highlighting important topics of the day and 
activities which might indicate disease outbreak.  
Finely-tuned searches can be saved and applied as 
filters or topic tracking mechanisms. These saved 
searches are automatically updated at specific inter-
vals and can be aggregated and displayed visually as 
bar graphs to reveal spikes of activity that otherwise 
might go undetected. 
Alerting 
The MiTAP prototype has two separate alerting ca-
pabilities: saved searches and an integrated expert 
system. The saved search functionality allows ana-
lysts to set thresholds for alerting purposes. For ex-
ample, MiTAP can send email when any new article 
arrives, when a specified maximum number of arti-
cles arrives, or when the daily number of new articles 
increases by some percentage of the total or moving 
average. 
The Human Language Indication Detector 
(HLID) performs data fusion on a number of dispa-
rate sources, compressing a large volume of informa-
tion into a smaller but more significant set of alerts. 
HLID monitors a variety of sources including MiTAP 
articles, information events in RSS feeds, and other 
dynamically updated information on the World Wide 
Web. HLID analyzes events from these sources in 
real time and generates an estimate of significance 
for each, complete with an audit trail of supporting 
and negating evidence. This allows an analyst to di-
rect a search for indicators towards interesting data 
while reducing the time spent investigating false 
alarms and insignificant events.  
HLID is composed of four major components. 
The first is an event collector, which monitors a data 
source and triggers action when an event is observed. 
These events are sent to the rule based reasoning en-
gine, an expert system shell (JESS 2004) with hand 
authored rules. The engine performs vetting and ini-
tial investigation of each event by identifying corre-
lated events, corroborating or invalidating evidence, 
and references to supporting information. The engine 
can also supplement its knowledge base by perform-
ing a directed search via the query management sys-
tem, which allows retrieval of information from a 
wide variety of sources including databases and web 
pages. Lastly, the alerting mechanism disseminates 
the conclusions reached by the system and provides 
an interface that allows an analyst to launch a deeper 
search for indicators and warnings. 
4 
5 
Acknowledgments 
This work has been funded, in part, by the Defense 
Advanced Research Projects Agency Translingual 
Information Detection Extraction and Summarization 
program under contract numbers DAAB07-01-C-
C201 and W15P7T-04-C-D001, the Office of the 
Secretary of Defense in support of the Coalition Pro-
visional Authority in Baghdad, and a MITRE Special 
Initiative for Rapid Integration of Novel Indications 
and Warnings for SARS. 
References 
Aberdeen, J., Burger, J., Day, D., Hirschman, L., 
Robinson, P., and Vilain, M. 1995. MITRE: De-
scription of the Alembic System as Used for 
MUC-6. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). 
Baldwin, B,, Moore, M., Ross, A., Shah, D.  2002. 
Trinity Information Access System. Proceedings of 
Human Lanuage Technology Conference, San 
Diego, CA. 
Damianos, L., Ponte, J., Wohlever, S., Reeder, F., 
Day, D., Wilson, G., Hirschman, L. 2002. MiTAP, 
Text and Audio Processing for Bio-Security: A 
Case Study In Proceedings of IAAI-2002: The 
Fourteenth Innovative Applications of Artificial 
Intelligence Conference, Edmonton, Alberta, Can-
ada. 
Darwish, K. PSE: A Small Search Engine written in 
Perl 2002 
http://tides.umiacs.umd.edu/software.html 
INN: InterNetNews, Internet Software Consortium 
2001, http://www.isc.org/products/INN.  
The Jakarta Project, 2001 
http://jakarta.apache.org/lucene/docs/index.html. 
JESS: the Rule Engine for the Java? Platform 2004 
http://herzberg.ca.sandia.gov/jess/  
Mani, I. and Wilson, G. 2000. Robust Temporal 
Processing of News. In Proceedings of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL'2000), 69-76. 
McKeown, K., Barzilay, R., Evan, D., Hatzivassi-
loglou, V., Klavans, J., Sable, C., Schiffman, B., 
Sigelman, S. 2002. Tracking and Summarizing 
News on a Daily Basis with Columbia's Newsblas-
ter. In Proceedings of HLT 2002: Human Lan-
guage Technology Conference. 
Miller, K., Reeder, F., Hirschman, L., Palmer, D. 
2001. Multilingual Processing for Operational 
Users, NATO Workshop on Multilingual Process-
ing at EUROSPEECH. 
RSS RDF Site Summary http://purl.org/rss/1.0/spec 
Vilain, M. and Day, D. 1996. Finite-state phrase 
parsing by rule sequences. In Proceedings of the 
1996 International Conference on Computational 
Linguistics (COLING-96), Copenhagen, Denmark. 
Comparing Several Aspects of Human-Computer and
Human-Human Dialogues
Christine Doran, John Aberdeen, Laurie Damianos and Lynette Hirschman
The MITRE Corporation
202 Burlington Road
Bedford, MA 01730 USA
{cdoran,aberdeen,laurie,lynette}@mitre.org
Abstract
While researchers have many intuitions
about the differences between human-
computer and human-human interac-
tions, most of these have not previously
been subject to empirical scrutiny. This
work presents some initial experiments
in this direction, with the ultimate goal
being to use what we learn to improve
computer dialogue systems. Working
with data from the air travel domain,
we identified a number of striking dif-
ferences between the human-human and
human-computer interactions.
1 Introduction
In our initial experiments comparing human-
human (HH) and human-computer (HC) inter-
action we have annotated dialogues from the air
travel domain with several sets of tags: dialogue
act, initiative and unsolicited information. Our
aim is to begin an empirical exploration of how
these aspects of the dialogue shed light on dif-
ferences between HH and HC interactions. We
found striking differences between the human-
human and human-computer interactions. With
many of the issues we examine here, researchers
have voiced strong intuitions about the differences
between HH and HC communication, but these in-
tuitions have not previously been subject to em-
pirical scrutiny.
Why do we want to compare HH and HC in-
teractions? We believe that an examination of
the differences between HH and HC dialogues can
help those working on the HC interactions to im-
prove their systems. This will not necessarily
mean making the HC interactions ?more like? HH
interactions; rather, we believe that such analy-
sis can give us insights about the appropriateness
and success of various communicative approaches
in different settings. We are also interested in
quantifying what it means for a dialogue to be
?mixed-initiative?. There is liberal use of this
term in work on human-computer dialogues, but
there does not seem to be a clear sense of what it
really means and how to define it.
This work is being done in the context of the
Communicator Travel task. Communicator is a
DARPA-funded program involving major indus-
try and academic sites, established to provide the
next generation of intelligent conversational inter-
faces to distributed information. The goal of the
program is to move beyond the current system
initiated voice menu style of dialogue interaction
towards a more flexible strategy of shared con-
versational control between human and system.
Work up to the fall of 2000 concentrated on the
travel task, but groups are now moving into other
domains with their dialogue systems.
2 Our Data
We have tagged 20 HH dialogues and 40
HC dialogues; the HH dialogues consist of
25,208 words and 1742 turns, and the HC
dialogues consist of 8313 words and 957
turns. The HH dialogues are from the SRI
ATIS collection (available at www.ai.sri.com/?
communic/amex/amex.html) and the CMU Travel
data (www.speech.cs.cmu.edu). The HC dia-
logues are from a subset of the systems partici-
pating in the DARPA Communicator data collec-
tion which was run in June and July of 2000.1
Subjects were selected by the U.S. National In-
stitute of Standards and Technology (NIST) and
each subject was paid to call all of the 9 partic-
ipating sites with either a fixed or open travel
itinerary. The fixed itineraries were intended to
balance flight types (one-way, round-trip, multi-
leg) and destinations (large US airport, small for-
1Ideally, of course, we would want HH dialogues
collected under similar circumstances to the HC dia-
logues. However, no such data are available and col-
lecting new HH data is not currently a feasible en-
deavor for us.
eign airport, etc.).2 We made no attempt to bal-
ance the subset of data we analyzed; we merely
selected 10 random users. We then identified four
sites whose systems we expected to differ quite
widely (based on researchers? descriptions of their
systems), and extracted the 10 users? dialogues for
each site. Each dialogue was tagged with three
sets of annotations?for dialogue acts, for initia-
tive and for provision of unsolicited information
by users (HC only). Sections 2.1, 2.2 and 3.5 de-
scribe these in more detail. The passage below
is excerpted from one of the human-computer in-
teractions, using the transcribed versions of the
user?s turns.3
User said: i would like to fly from miami
florida to sydney australia on wednesday
october eleventh please
System said: okay, from Miami to Sydney on
Wednesday October 11. Can you provide the
approximate departure time or airline?
User said: [um] i prefer american airlines
and i can leave anytime
System said: There are no nonstop flights.
I have a flight connecting in Los Angeles
leaving at 7:15 a.m. a flight connecting
in Los Angeles leaving at 5:20 p.m. and a
flight connecting in Los Angeles leaving
at 7:30 p.m. Do you like any of these?
User said: yes how about the morning one
System said: American flight 912 connecting
to American flight 7362 in Los Angeles has
been added to your itinerary. you depart
Miami at 7:15 am and arrive in Sydney at
9:40 pm the following day. Where would you
like to go next?
2.1 Dialogue Act Tagging
We have done two rounds of tagging. Our start-
ing point was a 20-tag subset of the CSTAR Con-
sortium tags (Consortium, 1999). We found the
CSTAR tag set to be the most suitable for our
task of the tagging schemes we considered, based
on information provided in the survey of dialogue
act annotation schemes assembled by the MATE
project (Klein et al, 1998). We picked the CSTAR
tags because they had been designed for task-
2It is important to note that the fixed itineraries
were assigned, i.e. not real trips, and it was also evi-
dent from the transcripts that few if any of the open
itineraries represented real trips. This may well have
had some impact on the data that were collected.
3Both human-human and human-system dialogue
transcripts are reproduced with their native format-
ting, i.e. all caps, no punctuation, etc. The system
side contains punctuation resulting from the genera-
tion process.
oriented dialogues, the tag categories were suffi-
ciently clear and simple that we believed we would
be able to tag the data reliably and, finally, the
categories captured the sorts of distinctions we be-
lieved would be relevant. We rejected the DAMSL
tag set (Core and Allen, 1997; Core et al, 1999)
on the grounds that is was too sophisticated for
our purposes, covering many aspects of dialogue
structure that were not necessarily relevant for our
task such as intentionality, grounding and context
tracking. In addition, the interannotator agree-
ment levels reported for this scheme are quite low.
Some of the other tag sets we considered were
(Carletta et al, 1995; Nakatani et al, 1995; van
Vark et al, 1996; Di Eugenio et al, 1998; Jurafsky
et al, 1997).
In collaboration with AT&T, we arrived at a
set of changes to our tag set that would make
it compatible with their efforts to tag system ut-
terances automatically (Walker and Passonneau,
2001), in the hopes of being able to share re-
sults with them more easily. We added a sit-
uation/conversation/task distinction to a num-
ber of our tags (e.g. give-information split
into give-task-info, give-situation-info and
give-conversation-info). We also added a
not-understand tag and collapsed some orig-
inal tags into super-categories. Our revised tag
set had 26 tags, and two people (one who had
also done the first round of tagging) tagged the
same data set. The situation/conversation/task
distinction turned out to be extremely difficult for
the taggers to make; we believe that revisions to
the tagging guidelines could lead to some improve-
ment on this front, but without enumerating the
kinds of utterances which fall into each category,
this will remain a difficult task.
We tagged each utterance that contained some
speech, i.e. was not composed entirely of non-
speech annotation like *pause* or [click], and
we split turns4 into utterances using guidelines
that had been developed internally for another
purpose. Utterances on this definition were
roughly clause-sized units, and possibly fragmen-
tary.5 This meant that there were often multi-
ple dialogue acts (DAs) per turn, and where there
were multiple sequential DAs of the same type, we
collapsed them under a single tag on the assump-
tion that they were combining to ?perform? that
DA. We initially split some of the CSTAR tags
4Chunk of text labelled with either User said or
Expert said. It was possible for a single speaker to have
more than one sequential turn, i.e. turn 6= speaker
change.
5In hindsight, it would have been preferable to seg-
ment the dialogues in a separate step.
into implicit and explicit versions, but found
that the implicit cases were so hard to identify
that we were not using those tags, and they were
dropped from the tag set.
Tables 1 and 2 show roughly parallel sub-
dialogues from the HH and HC data.6 Each turn
is tagged with its DA, and the first expert turn
in Table 2 shows multiple DAs within a turn, a
give-information followed by an offer.
Expert:WHAT TIME DO [req-task-info]
YOU NEED TO DEPART
User:AS SOON AS [give-task-info]
POSSIBLE AFTER FIVE P.M.
Expert:THE FIRST FLIGHT [give-task-info]
AFTER FIVE P.M. ON THAT DATE IS
AT FIVE THIRTY FIVE P.M. ARRIVING
IN CHICAGO AT SIX OH SIX P.M.
ON U.S. AIR
User: IS THAT O?HARE [req-task-info]
Table 1: DA tagging in an HH Exchange
Expert: i have an American [give-task-info]
Airlines flight departing Seattle at
twelve fifty five p.m., arrives Tokyo
at three p.m. the next day.
Is that OK? [offer]
User: yes I?ll take it [accept]
Expert: Will you return to seattle[req-task-info]
from tokyo?
User: what airport [req-task-info]
Expert: Will you return to seattle[req-task-info]
from tokyo?
Table 2: DA tagging in an HC Exchange
With our first tag set, our Kappa score for
interannotator agreement on these dialogues is
0.90 (with two annotators). Not surprisingly, our
Kappa score on the second, more complex tag set
(cf. Table 10 for a list of the tags) was lower,
0.71 (0.74 on the HC data and 0.66 on the HH
data). Both scores are in line with scores re-
ported in similar tagging tasks (Klein et al, 1998):
0.56 for DAMSL (overall average), 0.83 for Map-
task (experienced coders), 0.8-0.84 for Switch-
board DAMSL and 0.83 for VerbMobil. The drop
in score between our two tag sets emphasizes an
issue which we continue to wrestle with?the trade-
off between tag set complexity and tagging accu-
racy. At what point is it more useful to have re-
6Throughout the paper, we will use expert to refer
to either the human or the computer travel agent, sys-
tem to refer exclusively to the computer travel agent,
and user to refer to the travelers.
liable results from an impoverished tag set than
results of questionable value from a sophisticated
tag set?
2.2 Initiative Tagging
There is not a clearly agreed upon definition of ini-
tiative in the literature on dialogue analysis (but
see e.g., (Chu-Carroll and Brown, 1998; Jordan
and Di Eugenio, 1997; Flammia and Zue, 1997)),
despite the fact the terms initiative and mixed-
initiative are widely used. Intuitively, it seems
that control rests with the participant who is mov-
ing a conversation ahead at a given point, or se-
lecting new topics for conversation.
After experimenting with several tagging meth-
ods, we concluded that the approach presented
in Walker and Whittaker (1990) adopted from
(Whittaker and Stenton, 1988) best captured the
aspects of the dialogue we were interested in and,
as with the DAs, could be tagged reliably on our
data.
Each turn is tagged with which participant has
control at the end of that turn, based on the utter-
ance type. Again, we did not tag turns composed
entirely of non-speech annotation, and we also ex-
cluded conventional openings and closings, follow-
ing Walker and Whittaker. Below, we list the
rules for tagging each utterance type; a prompt
is an utterance ?which did not express proposi-
tional content, such as Yeah, Okay, Uh-huh, . . . .?
(Op cit, p. 3) The classification refers to the il-
locutionary force of the item, rather than to its
particular syntactic form.
Assertion: speaker has initiative unless it is a
response to a question or command7
Question: speaker has initiative unless it is a re-
sponse to a question or command
Command: speaker has initiative
Prompt: hearer has initiative
Tables 3 and 4 show the same passages used
above, but this time tagged for initiative. To give
a sense of how the tagging rules are applied, let us
step through the HC example (Table 4). Turn (1)
is assigned expert-initiative, because it is an
assertion which is not a response to any preceding
question or command. Turn (2) is still expert-
initiative, because it is an answer to the ques-
tion Is that OK?. The third turn is a question
and expert-initiative, but turn (4) is user-
initiative because it is a question that is not a
response to the previous question. The system
7Italics show our modification to the rule.
does not address the user?s question, but rather
repeats its own question, so the final turn (5) is
expert-initiative.
Expert:WHAT TIME DO YOU [exp-init]
NEED TO DEPART
User:AS SOON AS POSSIBLE [exp-init]
AFTER FIVE P.M.
Expert:THE FIRST FLIGHT AFTER [exp-init]
FIVE P.M. ON THAT DATE IS AT
FIVE THIRTY FIVE P.M.
ARRIVING IN CHICAGO AT
SIX OH SIX P.M. ON U.S. AIR
User:IS THAT O?HARE [user-init]
Table 3: Initiative tagging in an HH Exchange
(1)Expert: i have an American [exp-init]
Airlines flight departing Seattle at
twelve fifty five p.m. , arrives Tokyo
at three p.m. the next day.
Is that OK?
(2)User: yes I?ll take it [exp-init]
(3)Expert: Will you return to seattle [exp-init]
from tokyo?
(4)User: what airport [user-init]
(5)Expert: Will you return to seattle [exp-init]
from tokyo?
Table 4: Initiative tagging in an HC Exchange
Our Kappa scores for interannotator agreement
on the initiative tagging were somewhat lower
than for DA tagging. Here, ?=0.68. In fact, our
agreement was rather high, at 87%, but because
there were so few instances of user initiative in
the HC dialogues, our agreement would have to
be exceptional to be reflected in a higher Kappa
score. While we had believed this to be the easier
task, with quite clear guidelines and only a binary
tagging choice, it in fact proved to be quite diffi-
cult. We still believe that this tag set can give
us useful insights into our data, but we would
be interested in attempting further revisions to
the tagging guidelines, particularly as regards the
definition of an ?answer?, i.e. when an answer is
responsive and when it is not.
3 Analysis
We found a number of interesting differences be-
tween the HH and HC dialogues. While we have
not yet been able to test our hypotheses about
why these differences appear, we will discuss our
ideas about them and what sorts of further work
we would like to do to subject those ideas to em-
pirical validation.
3.1 Initiative Distribution
Based on researchers? descriptions of their systems
(i.e. for the most part, ?highly mixed-initiative?),
we had expected to find some variance in the dis-
tribution of initiative across systems. As is ev-
ident from Table 5, the HC systems do not dif-
fer much from each other, but taken as whole,
the dialogues differ dramatically from the HH di-
alogues. In the HH dialogues, users and expert
share the initiative relatively equitably, while in
the HC data the experts massively dominate in
taking the initiative. Here, we are simply counting
the number of turns tagged as user-initiative or
expert-initiative.8
We also show turns to completion and overall
user satisfaction scores for each system as a refer-
ence point. User satisfaction was calculated from
five questions asked of each user after each dia-
logue. The questions use a 5-point Likert scale.
Turns to completion measures the total number
of on-task turns. We found no significant corre-
lations here, but cf. Walker et al (2001) which
provides more detailed analyses of the Communi-
cator dialogues using user satisfaction and other
metrics, within the PARADISE framework. It is
worth noting, however, that the HC D has both
the highest percentage of expert initiative and the
highest satisfaction scores, so we should not con-
clude that more initiative will necessarily lead to
happier users.
% Exp % User Turns to User
Init Init Comp Sat
HC A 86.8% 13.2% 40.5 60.0%
HC B 89.9% 10.1% 41.4 71.5%
HC C 90.6% 9.4% 36.0 68.5%
HC D 93.7% 6.3% 43.9 82.8%
HH SRI 48.3% 51.7% N/A N/A
HH CMU 54.0% 46.0% N/A N/A
Table 5: Percentages of User and Expert Initiative
in HH and HC Dialogues
In the HC dialogues, we also see a difference in
success rate for user-initiative turns. By our defi-
nition, the user ?succeeds? in taking the initiative
in the dialogue if the system responds to the initia-
tive on the first possible turn. The rate of success
8A cautionary note is warranted here. We are
not suggesting that more user-initiative is intrinsically
preferable; it may well turn out to be the case that
a completely system-directed dialogue is more pleas-
ant/efficient/etc. Rather, we are seeking to quantify
and assess what it means to be ?mixed-initiative? so
that we can better evaluate the role of initiative in
effective (task-oriented) dialogues.
is the ratio of successful user-initiatives attempts
to total user-initiatives attempts. There appears
to be a negative relationship between number of
initiative attempts and their success rate. See
Figure 1, below. HC D has a high success rate
for a relatively small number of user-initiative at-
tempts. HC A has many more occurrences of user
initiative, but does not incorporate them as well.
Figure 1: User-Initiative and Success Rate per
System
There is no determinable relationship between
user experience (i.e., the number of calls per sys-
tems) and either the amount of user-initiative or
the success rate of user-initiative.
We also looked at user-initiative with re-
spect to dialogue act type. Most user-initiatives
are request-action (26%) and request-
information (19%). Request-information
dialogue acts (e.g., What cities do you know in
Texas?, Are there any other flights?, Which air-
port is that?) are handled well by the systems
(83% success rate) while request-action dia-
logue acts (e.g., start over, scratch that, book that
flight) are not (48%). Most of the user-initiatives
that are request-action dialogue acts are the
start over command (16% of the total user-
initiatives). Corrections to flight information pre-
sented by the systems consist of 20% of the total
user-initiatives.
3.2 Overall Verbosity
In counting the number of words used, we find
that the computer experts are much more verbose
than their human users, and are relatively more
verbose than their human travel agent counter-
parts. In the HH dialogues, experts average 10.1
words/turn, while users average 7.2. In the HC di-
alogues on average, system have from 16.65-33.1
words/turn vs. the users? 2.8-4.8 words/turn. Fig-
ure 2 shows these differences for each of the four
systems and for the combined HH data.
Figure 2: Words per turn for users and experts in
the HH and HC dialogues
3.2.1 Short vs. Long Confirmations
One DA which is a basic conversational tool and
therefore an interesting candidate for analysis is
the use of confirmations. Instances of short con-
firmation, typically back-channel utterances such
as okay and uh huh were tagged as acknowl-
edge, while instances of long confirmation, as
when one participant explicitly repeats something
that the other participant has said, were tagged
as verify-X, where X=conversation-action,
task-information and task-action, This tag-
ging allows us to easily calculate the distribution
of short and long confirmations.
Overall we found in the HC dialogues a rather
different confirmation profile from the HH dia-
logues. In the HC dialogues, the systems use both
types of confirmation far more than the users do
(246 total system, 8 total user). Moreover, sys-
tems use long confirmation about five times more
often (210 vs. 36) than they use short confirma-
tion. In contrast, the experts in the HH dialogues
use somewhat more confirmations than users (247
vs. 173), but both parties use far more short than
long confirmations (340 vs. 80), just the reverse
of the HC situation. This difference partially ac-
counts for the total word count differences we saw
in the previous section. Tables 6 and 7 show the
breakdowns in these numbers for each system and
for the two sets of HH data, and begin to quantify
the striking contrasts between human and com-
puter confirmation strategies.
3.3 Number of Dialogue Acts
Another observation is that the computer experts
appear to be trying to do more. They have sig-
nificantly more DAs per turn than do their hu-
man users, whereas in the HH dialogues, the two
participants have nearly the same number of DAs
per turn (just over 1.3). In the HC dialogues, sys-
Site Expert User Total
HC A 3 (0.5%) 4 (0.7%) 7 (1.2%)
HC B 13 (1.9%) 0 (0.0%) 13 (1.9%)
HC C 20 (3.1%) 3 (0.5%) 23 (3.6%)
HC D 0 (0.0%) 0 (0.0%) 0 (0.0%)
HH SRI 95 (16.1%) 79 (13.3%) 174 (29.4%)
HH CMU 94 (12.1%) 72 (9.3%) 166 (21.4%)
Table 6: Number of short confirmations, i.e. ac-
knowledge (percentage of total dialogue acts)
Site Expert User Total
HC A 32 (5.7%) 0 (0.0%) 32 (5.7%)
HC B 74 (10.6%) 0 (0.0%) 74 (10.6%)
HC C 59 (9.2%) 1 (0.2%) 60 (9.4%)
HC D 45 (8.6%) 0 (0.0%) 45 (8.6%)
HH SRI 11 (1.9%) 11 (1.9%) 22 (3.7%)
HH CMU 47 (6.1%) 11 (1.4%) 58 (7.5%)
Table 7: Number of long confirmations i.e.
verify-X (percentage of total dialogue acts)
tems have, on average 1.6 DAs per turn where
users have just 1.0, as Figure 3 shows. If we take
a DA as representing a single dialogue ?move?,
then users in the HC dialogues are managing one
move per turn, where the systems have at least one
and often more. A common sequence for the com-
puter experts is a verify-task-information fol-
lowed by a request-task-information, such as
A flight to Atlanta. What city are you departing
from?.
Figure 3: Dialogue acts per turn for users and
experts in the HH and HC dialogues
3.4 Types of Dialogue Acts
One of our main questions going into this work
was whether there would be interestingly differ-
ent distributions of DAs in the HH and HC dia-
logues, and whether different distributions of DAs
across systems would be correlated with user sat-
isfaction. Unfortunately, we do not have user sat-
isfaction scores for the HH data, but if new data
were to be collected, this would be an essential
addition.
Tables 8 and 9 illustrate some of the main dif-
ferences between the HH and HC dialogues, and
as regards our first research question, definitely
give an interesting view of the differences between
the HH and HC conversations.
DA Overall Expert User
GiveTaskInfo 27.7% 29.7% 25.5%
Acknowledge 24.9% 26.9% 22.7%
RequestTaskInfo 11.0% 10.7% 11.4%
VerifyTaskInfo 5.4% 7.5% 3.2%
Affirm 4.8% 4.3% 5.4%
Table 8: Five most frequent DAs in Human-
Human dialogues, by percent of total DAs for col-
umn
DA Overall Expert User
GiveTaskInfo 23.7% 12.9% 46.3%
RequestTaskInfo 15.3% 22.1% 1.3%
Offer 7.7% 11.5% 0.0%
VerifyTaskInfo 7.1% 10.5% 0.1%
Apology 4.5% 6.6% 0.1%
Table 9: Five most frequent DAs in Human-
Computer dialogues, by percent of total DAs for
column
As expected in this domain, all DAs involving
exchange of task information (give-task-info,
request-task-info, and verify-task-info are
frequent in both sets of dialogues. However, in the
HH dialogues, acknowledge (e.g. the tag for
back-channel responses and general confirmations
such as right, uh huh and okay) is the second most
common DA, and does not even appear in the top
five for the HC dialogues. The DA for positive re-
sponses, affirm, is also in the top ranking for the
HH dialogues, but does not appear in the list for
the HC dialogues. Finally, offer and apology
appear frequently in the HC dialogues and not in
the top HH DAs. The appearance of these two is a
clear indication that the systems are doing things
quite differently from their human counterparts.
Turning to differences between experts and
users in these top categories, we can see that hu-
man users and experts are about equally likely
to ask for or give task-related information (give-
task-info and request-task-info). In con-
trast, in the HC dialogues nearly half of the users?
DAs are giving task information and hardly any
are requesting such information, while almost a
quarter of expert DAs are requesting information.
There is some inequity in the use of verify-task-
info in the HH dialogues, where experts perform
about twice as many verifications as users; how-
ever, in the HC dialogues, virtually all verification
is done by the expert. All of these patterns rein-
force our finding about initiative distribution; in
the HC dialogues, one disproportionately finds the
expert doing the asking and verification of task in-
formation, and the user doing the answering, while
in the HH dialogues the exchange of information
is much more balanced.
DA HC A HC B HC C HC D
accept 3.9% 3.1% 4.8% 3.4%
acknowledge 1.2% 1.9% 3.6% 0.0%
affirm 1.8% 2.4% 0.8% 9.5%
apologize 4.6% 3.7% 8.9% 0.0%
demand-conv-info 1.1% 0.0% 0.0% 0.0%
demand-sit-info 0.0% 1.6% 1.4% 1.3%
demand-task-info 3.4% 0.3% 0.0% 1.3%
give-sit-info 5.7% 6.3% 4.7% 1.9%
give-task-info 34.8% 16.0% 24.8% 20.8%
negate 2.1% 1.7% 0.8% 5.2%
not-understand 2.5% 3.7% 7.2% 0.0%
offer 3.5% 8.4% 9.4% 9.4%
open-close 2.3% 3.1% 4.8% 3.4%
please-wait 0.0% 6.2% 1.6% 3.1%
reject 1.1% 4.1% 0.3% 2.5%
req-conv-action 2.7% 4.4% 2.5% 1.0%
req-sit-action 1.1% 1.4% 0.2% 1.9%
req-sit-info 0.0% 3.3% 0.2% 3.2%
req-task-action 1.1% 1.4% 0.3% 0.2%
req-task-info 17.9% 12.6% 10.9% 21.6%
suggest-conv-action 1.6% 0.1% 2.0% 0.0%
thank 2.1% 3.4% 1.4% 1.7%
verify-conv-action 0.7% 0.7% 0.0% 0.0%
verify-task-action 2.5% 0.4% 1.9% 0.0%
verify-task-info 2.5% 9.4% 7.5% 8.6%
user satisfaction9 60.0% 71.5% 68.5% 82.8%
Table 10: Distribution of DAs by System
Table 10 gives an interesting snapshot of each
system, in terms of its overall distribution of DAs.
These numbers are reflective of the system design-
ers? decisions for their systems, and that means all
DAs are not going to be used by all systems (i.e.
0.0% may mean that that DA is not part of the
system?s repertoire).
We will concentrate here on the best and worst
9This figure combines the scores on five user satis-
faction questions. A perfect score is 100%.
received systems in terms of their overall user sat-
isfaction, HC D and HC A; the relevant numbers
are boldfaced. They also have very different di-
alogue strategies, and that is partially reflected
in the table. HC D?s dialogue strategy does not
make use of the ?social nicety? DAs employed by
other systems (acknowledge, apologize, not-
understand), and yet it still had the highest user
satisfaction of the four. This system also has the
highest proportion of affirm (more than three
times as many as the next highest system) and
req-task-info DAs, which suggests that quite a
lot of information is being solicited and the users
(because we know from Table 9 that it is primarily
the users responding) are more often than average
responding affirmatively. The fact that the per-
centage of give-task-infos is somewhere in the
middle of the range and affirms is so high may
indicate that the HC D uses more yes/no than
content questions.
Looking at the lower scoring system, HC A, we
see very different patterns. HC A has most of
the demand-task-infos, the second highest per-
centage of req-task-infos and by far the most
give-task-infos, so its dialogue strategy must
involve a large number of attempts to extract in-
formation from the user, and yet it has the fewest
offer DAs, so these don?t appear to be resulting
in suggestions of particular travel options.
Turning to correlations between DA use by
expert and user (combined across systems) and
user satisfaction, we see some expected results
but also some rather surprising correlations.
Not unexpectedly, apologies and signals of non-
understanding by the system are highly negatively
correlated with satisfaction (-0.7 and -0.9, respec-
tively). While it may seem counter-intuitive that
open-close by the user is negatively correlated
(at -0.8), those familiar with this data will un-
doubtedly have noticed that users often try to say
Goodbye repeatedly to try to end a dialogue that
is going badly. Discussion of situational informa-
tion (e.g. phone use) by the expert is highly neg-
atively correlated, but by the user, the DA req-
situation-info is perfectly positively correlated.
We cannot account for this finding.
3.5 Unsolicited Information
In the HC data we noticed that users often
provided more information than was explicitly
solicited?we call this ?unsolicited information?.
For example, when a system asks for one piece
of information, On what day would you be depart-
ing Portland?, the user might respond with ad-
ditional information such as, Thursday, October
5th before six pm from Portland back to Seattle.
78% of that unsolicited information is offered in
response to open-ended questions (e.g., How can I
help you? or What are your travel plans?). While
our initiative tagging partially captures this, there
are cases where the answer may be considered re-
sponsive (i.e. initiative does not shift away from
the participant asking the question) and yet un-
solicited information has been offered. Thus, this
category is somewhat orthogonal to our charac-
terization of initiative, although it is clearly one
way of seizing control of the conversation.10
To get at this information, we developed a third
tagging scheme for annotating unsolicited infor-
mation. We began examining just the HC doc-
uments, because the phenomenon is prevalent in
these data; we hope to perform a similar analysis
on the HH data as well. We found that the sys-
tems we examined in general handle unsolicited in-
formation well. 70% of all unsolicited information
is handled correctly by the systems, 22% is han-
dled incorrectly, and the rest could not be accu-
rately classified. Information offered in response
to open-ended questions is handled correctly more
often by the systems than unsolicited information
offered at other points in the dialogue (74% versus
56%). The former figure is not surprising, since
the systems are designed to handle ?unsolicited?
information following open-prompts. However, we
were surprised the systems did as well as they did
on unsolicited information in contexts where it
was not expected. Figure 4 shows the relationship
between frequency of various types of unsolicited
information and how well the system incorporates
that information. There appears to be some cor-
relation between the frequency of unsolicited in-
formation and the rate of success, but we do not
have enough data to make a stronger claim.
Furthermore, systems vary in response delay to
pieces of unsolicited information. We define re-
sponse delay as the number of system turns it
takes before the information is acknowledged by
the system (either correctly or incorrectly.) If a
system responds immediately to the unsolicited
information, a count of zero turns is recorded.
Figure 5 shows the difference among systems in re-
sponding to unsolicited information. We graphed
both the average total number of system turns as
well as the average number of turns minus rep-
etitions. HC B responds almost immediately to
10This issue may also be related to where in the
dialogue errors occur. We are pursuing another line
of research which looks at automatic error detection,
described in (Aberdeen et al, 2001). We believe we
may also be able to detect unsolicited information au-
tomatically, as well as to see whether it is likely to
trigger errors by the system.
Figure 4: Unsolicited Fields vs. Success Rate of
Incorporation
unsolicited information while HCs A and C take
more turns to respond. HC D has trouble under-
standing the unsolicited information, and either
keeps asking for clarification or continues to ig-
nore the human and prompts for some other piece
of information multiple times.
Figure 5: Variation of System Response to Unso-
licited Information
Figure 6 shows the different rates at which sys-
tems acknowledge unsolicited information for dif-
ferent fields. For example, departure city is recog-
nized and validated almost immediately. Return
date and flight type are incorporated fairly quickly
when the system understands what is being said.
If we look at the effects of experience on
the amount of unsolicited information offered, as
shown in Figure 7, we can see that users tend
to provide more unsolicited information over time
(i.e., as they make more calls to the systems).
This effect may be the result of increased user
confidence in the systems at handling unsolicited
information. It also may be attributed to user
boredom; as time goes on, users may be trying
to finish the task as quickly as possible. Even if
this is true, however, it demonstrates attempts by
users to take more control of the interactions as
Figure 6: System Response to Different Types of
Unsolicited Information
they become more experienced.
Figure 7: Effect of Experience on Unsolicited In-
formation
Our data also show that the success rate of in-
corporating unsolicited information improves with
user experience. The ratio of successes to failures
increases in later calls to the systems (Figure 8).
4 Discussion
This was a relatively small study, but many of
the results are sufficiently striking that we expect
them to hold over large sets of dialogues. First,
it is clear that (for our definition of the term) ini-
tiative is skewed towards the computer expert in
the human-computer dialogues, despite claims of
developers to the contrary. Whether this is de-
sirable or not is a separate issue, but we believe
it is a move forward to be able to quantify this
difference. Second, there are clear differences in
dialogue act patterns between the HH and HC di-
alogues. When the DAs correspond to basic di-
alogue moves, like questions or signals of agree-
ment, we can begin to see how the dialogue dy-
namic is different in the human computer situa-
Figure 8: Experience versus Success Rate of In-
corporating Unsolicited Information
tion. In general, the conversation was much more
balanced between traveler and expert in the HH
setting, in terms of amount of speech, types of di-
alogue acts and with respect to initiative. In the
HC conversations, the system dominated, in num-
ber of words and dialogue acts and in initiative.
We are very interested in the selection of the
?right? tag set for a given task. As we noted in
our discussion of DA tagging, we had very dif-
ferent outcomes with two closely related tag sets.
Clearly the choice of tag set is highly dependent
on the use the tagged data will be put to, how
easily the task can be characterized in the set of
tagging guidelines, and what trade-offs in accu-
racy vs. richness of representation are acceptable.
A central question we are left with is ?Why
don?t the users talk more in HC dialogues?? Is
it that they are happy to just give short, specific
answers to very directed questions? Or do they
?learn? that longer answers are likely to cause the
systems problems? Or perhaps users have pre-
conceived notions (often justified) that the com-
puter will not understand long utterances? We
may speculate that poor speech recognition per-
formance is a major factor shaping this behav-
ior, leading system designers to attempt to con-
strain what users can say, while simultneously at-
tempting to hold onto the initiative. (Walker et
al. (2001) found sentence accuracy to be one of
the significant predictors of user satisfaction in the
Summer 2000 DARPA Communicator data collec-
tion.) There are some cases where the experts in
the HC dialogues say things their human counter-
parts need not. One obvious case, which appears
in even the small example dialogues we are us-
ing here, is that the systems tend to repeat utter-
ances when there is some processing difficulty. In
the same vein, errors and misunderstandings are
more frequent in the HC data, resulting in (some
fairly verbose) efforts by the systems to identify
the problem and get the conversation back on
track.
5 Future Work
We are currently working with other Communica-
tor sites who are also looking at dialogue issues.
In addition, we are beginning to look at two new
aspects of these dialogues: task complexity and
conversational failure analysis (at the turn level,
(Aberdeen et al, 2001)). We are also interested
in examining patterns of initiative tags, i.e. con-
trol shift types and length of initiative runs, and
at relations between DAs and user satisfaction.
6 Acknowledgments
Thanks to Lori Levin and Alon Lavie at CMU for
sharing the CSTAR tagging guidelines and their
sample tagged corpus.
References
J. Aberdeen, C. Doran, L. Damianos, S. Bayer, and
L. Hirschman. 2001. Finding errors automatically
in semantically tagged dialogues. In Notebook Pro-
ceedings of the First International Conference on
Human Language Technology Research, San Diego,
CA, March.
J. C. Carletta, A. Isard, S. Isard, J. Kowtko,
G. Doherty-Sneddon, and A. Anderson. 1995.
The coding of dialogue structure in a corpus. In
J. A. Andernach, S. P. van de Burgt, and G. F.
van der Hoeven, editors, Proceedings of the Twente
Workshop on Language Technology: Corpus-based
approaches to dialogue modelling, Enschede, The
Netherlands. Universiteit Twente.
Jennifer Chu-Carroll and Michael K. Brown. 1998.
An evidential model for tracking initiative in col-
laborative dialogue interactions. User Modeling and
User-Adapted Interaction, 8(3-4):215?253.
CSTAR Consortium. 1999. Dialogue act annotation.
Unpublished Manuscript, October.
Mark Core and James Allen. 1997. Coding dialogs
with the damsl annotation scheme. In Proceed-
ings of the AAAI Fall Symposium on Communica-
tive Action in Humans and Machines, Boston, MA,
November.
Mark Core, Masato Ishizaki, Johanna Moore, Chris-
tine Nakatani, Nobert Reithinger, David Traum,
and Syun Tutiya, editors. 1999. The Report of The
Third Workshop of the Discourse Resource Initia-
tive, Chiba Univeristy. Technical Report No.3 CC-
TR-99-1.
Barbara Di Eugenio, Pamela W. Jordan, and Liina
Pylkknen. 1998. The COCONUT project: dialogue
annotation manual. Technical Report ISP Techni-
cal Report 98-1, University of Pittsburgh, Decem-
ber.
Giovanni Flammia and Victor Zue. 1997. Learn-
ing the structure of mixed initiative dialogues us-
ing a corpus of annotated conversations. In Proc.
Eurospeech 97, pages 1871?1874, Rhodes, Greece,
September.
Pamela W. Jordan and Barbara Di Eugenio. 1997.
Control and initiative in collaborative problem solv-
ing dialogues. In AAAI Spring Symposium on Com-
putational Models for Mixed Initiative Interaction,
Stanford, CA.
Daniel Jurafsky, Elizabeth Shriberg, and De-
bra Biasca. 1997. Switchboard swbd-damsl
shallow-discourse-function annotation coders man-
ual. Technical Report Technical Report 97-02, Uni-
versity of Colorado Institute of Cognitive Science,
August.
Marion Klein, Niels Ole Bernsen, Sarah Davies, Laila
Dybkj?r, Juanma Garrido, Henrik Kasch, An-
dreas Mengel, Vito Pirelli, Massimo Poesio, Sil-
via Quazza, and Claudia Soria, 1998. Supported
Coding Schemes, MATE Deliverable D1.1, July.
http://mate.nis.sdu.dk/.
Christine H. Nakatani, Barbara J. Grosz, David D.
Ahn, and Julia Hirschberg. 1995. Instructions for
annotating discourse. Technical Report TR-21-95,
Harvard University.
R.J. van Vark, J.P.M. de Vreught, and L.J.M.
Rothkrantz. 1996. Analysing ovr dialogues, coding
scheme 1.0. Technical Report 96-137, Delft Univer-
sity of Technology.
Marilyn Walker and Rebecca Passonneau. 2001. Di-
alogue act tags as qualitative dialogue metrics for
spoken dialogue systems. In Notebook Proceedings
of the First International Conference on Human
Language Technology Research, San Diego, CA,
March.
Marilyn Walker and Steve Whittaker. 1990. Mixed
initiative in dialogue: An investigation into dis-
course segmentation. In Proceedings of ACL90.
M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garo-
folo, L. Hirschman, A. Le, S. Lee, S. Narayan,
K. Papineni, B. Pellom, J. Polifroni, A. Potamianos,
P. Prabhu, A. Rudnicky, G. Sanders, S. Seneff,
D. Stallard, and S. Whittaker. 2001. DARPA Com-
municator Dialog Travel Planning Systems: The
June 2000 Data Collection. Submitted., April.
Steve Whittaker and Phil Stenton. 1988. Cues and
control in expert client dialogues. In Proceedings
of the 26th Annual Meeting of the Association for
Computational Linguistics (ACL88), pages 123?
130.
Gene Name Extraction Using FlyBase Resources 
Alex Morgan 
amorgan@mitre.org 
Lynette Hirschman 
lynette@mitre.org 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730-1420 
Alexander Yeh 
asy@mitre.org 
Marc Colosimo 
mcolosim@brandeis.edu  
 
 
Abstract 
Machine-learning based entity extraction re-
quires a large corpus of annotated training to 
achieve acceptable results.  However, the cost 
of expert annotation of relevant data, coupled 
with issues of inter-annotator variability, 
makes it expensive and time-consuming to 
create the necessary corpora. We report here 
on a simple method for the automatic creation 
of large quantities of imperfect training data 
for a biological entity (gene or protein) extrac-
tion system. We used resources available in 
the FlyBase model organism database; these 
resources include a curated lists of genes and 
the articles from which the entries were 
drawn, together a synonym lexicon.  We ap-
plied simple pattern matching to identify gene 
names in the associated abstracts and filtered 
these entities using the list of curated entries 
for the article.  This process created a data set 
that could be used to train a simple Hidden 
Markov Model (HMM) entity tagger. The re-
sults from the HMM tagger were comparable 
to those reported by other groups (F-measure 
of 0.75). This method has the advantage of be-
ing rapidly transferable to new domains that 
have similar existing resources. 
1 
                                                          
Introduction: Biological Databases 
 
There is currently an information explosion in 
biomedical research.  The growth of literature is 
roughly exponential, as can be seen in Figure 1 
which shows the number of literature references in 
FlyBase1 organized by date of publication over a 
hundred year span.2  This growth of literature 
makes it daunting for researchers to keep track of 
the information, even in very small subfields of 
biology. 
1 FlyBase is a database that focuses on research in the genetics 
and molecular biology of the fruit fly (Drosophila melangas-Figure 1: FlyBase References, 1900-2000 
 
Increasingly, biological databases serve to collect 
and organize published experimental results.  A 
wide range of biological databases exist, including 
model organism databases (e.g., for mouse3 and 
yeast4) as well as various protein databases (e.g., 
Protein Information Resource5 (PIR) or SWISS-                                                                                          
tor), a model organism for genetics research: 
http://www.flybase.org. 
PROT6 and   interaction databases such as the 
Biomolecular Interaction Network Database7 
(BIND). These databases are created by a process 
of curation, which is done by Ph.D. biologists who 
read the published literature to cull experimental 
findings and relations. These facts are organized 
into a set of structured fields of a database and 
 
2 Of course most of these early references in FlyBase are not 
in electronic form. The FlyBase database has been in existence 
since 1993. 
3 http://www.informatics.jax.org/ 
4 http://genome-www.stanford.edu/Saccharomyces/ 
5 http://pir.georgetown.edu/pirwww/pirhome3.shtml 
6 http://us.expasy.org/sprot/ 
7 http://www.bind.ca/ 
linked to the source of information (the journal 
article).  As a result, curation is a time-consuming 
and expensive process; database curators are in-
creasingly eager to adopt text mining and natural 
language processing techniques to make curation 
faster and more consistent. As a result, there has 
been growing interest in the application of entity 
extraction and text classification techniques to the 
problem of biological database curation [Hirsch-
man02]. 
2 
                                                          
Entity Extraction Methods 
There are two approaches to entity extraction.  The 
first requires manual or heuristic creation of rules 
to identify the names mentioned in text; the second 
uses machine learning to create the rules that drive 
the entity tagging. Heuristic systems require expert 
developers to create the rules, and these rules must 
be manually changed to handle new domains. Ma-
chine-learning based systems are dependent on 
large quantities of tagged data, consisting of both 
positive and negative examples.8  Figure 2 shows 
results from the IdentiFinder system [Bikel99] il-
lustrating that performance increases roughly with 
the log of quantity of training data. Given the ex-
pense of manual annotation of large quantities of 
data, the challenge for the machine learning ap-
proach is to find ways of creating sufficient quanti-
ties of training data cheaply. 
     Overall, hand-crafted systems seem to outper-
form learning-based systems for biology. How-
ever, it is clear that the quantities of training have 
been small, relative to the results reported for en-
tity extraction in e.g., newswire [Hirschman03]. 
There are several published sets of performance 
results for automatic named biological entity ex-
traction systems.  The system of Collier et al [Col-
lier00] uses a hidden Markov model to achieve an 
F-measure9 of 0.73 when trained on a corpus of 
29,940 words of text from 100 MEDLINE ab-
stracts.   Contrast this with Figure 2, which reports 
results using over 600,000 words of training data, 
and an F-measure of 0.95 for English newswire 
entity extraction (and 0.91 for Spanish).   
                                                          
8 For negative examples, the "closed world" assumption gen-
erally is taken to apply: if an entity is not tagged, it is assumed 
to be a negative example. 
 
Krauthammer et al [Krauthammer00] have taken a 
somewhat different approach which encodes char-
acters as 4-tuples of  DNA bases; they then use 
BLAST together with a lexicon of gene names to 
search for 'gene name homologies'. They report an 
F-measure of 0.75 without the use of a large set of 
rules or annotated training data. 
 
The PASTA system [Gaizauskas03] uses a combi-
nation of heuristic and machine-learned rules to 
achieve a higher F-measure over a larger number 
of classes: F-measure of 0.83 for the task of identi-
fying 12 classes of entities involved in the descrip-
tion of roles of residues in protein molecules. 
Because they used heuristic rules, they were able 
to get these results with a relatively small training 
corpus of 52 MEDLINE abstracts (roughly 12,000 
words). 
Figure 2: Performance of BBN's IdentiFinder named entity 
recognition system relative to the amount of training data, from 
[Bikel99] 
 
These results suggest that machine learning meth-
ods will not be able to compete with heuristic rules 
until there is a way to generate large quantities of 
annotated training data. Biology has the advantage 
that there are rich resources available, such as lexi-
cons, ontologies and hand-curated databases.  
What is missing is a way to convert these into 
training corpora for text mining and natural lan-
guage processing.  Craven and Kumlien [Cra-
ven99] developed an innovative approach that used 
fields in a biological database to locate abstracts 
which mention physiological localization of pro-
teins. Then via a simple pattern matching algo-
9 
Recall) (Precision
Recall)Precision2(
+
??=F   
Manning D, Schutze H. Foundations of Statistical Natural 
Language Processing, 2002: p 269. 
rithm, they identified those sentences where the 
relation was mentioned and matched these with 
entries in the Yeast Protein Database (YPD).  In 
this way, they were able to automatically create an 
annotated gold standard, consisting of sentences 
paired with the curated relations derived from 
those sentences. They then used these for training 
and testing a machine-learning based system.  This 
approach inspired our interest in using existing 
resources to create an annotated corpus automati-
cally.   
3 
r 
3.1 
                                                          
FlyBase: Organization and Resources 
We focused on FlyBase because we had access to 
FlyBase resources from our work in the creation of 
the KDD 2002 Cup Challenge Task 1 [Yeh03].   
Through this work, we had become familiar with 
the multi-stage process of curation.  An early task 
in the curation pipeline is to determine, for a given 
article, whether there are experimental results that 
need to be added to the database. This was the task 
used as the basis for the KDD text data mining 
"challenge evaluation". A later task in the pipeline 
creates a list of the Drosophila genes discussed in 
each curated article. This is the task we focus on in 
this paper.   
 
An example of a FlyBase entry can be seen in Fig-
ure 3 which shows part of the record for the gene 
Toll. Under Molecular Function and Biological 
Process we see that the gene is responsible for en-
coding a transmembrane receptor protein involved 
in antimicrobial humoral response (part of the 
innate immune system of the fly).  We see furthe
that  ?Tl? and ?CG5490? are synonymous for Toll 
(top of the entry next to Symbol), and the link 
Synonyms leads to a long synonym list which in-
cludes: ?Fs(1)Tl?, ?dToll?, ?CT17414?, ?Toll-1?, 
?Fs(3)Tl?, ?mat(3)9?, ?mel(3)10?, and ?mel(3)9?.  
Many of these facts about Toll are linked to a par-
ticular literature reference in the database.  For ex-
ample, following the link for Transcripts will lead 
to a page with links to the abstract of a paper by 
Tauszig et al [Tauszig00] which reports on ex-
periments which measured the lengths of RNA 
transcribed from the Toll gene. 
 
For FlyBase, Drosophila genes are the key bio-
logical entities; each entity (e.g., gene) is associ-
ated with a unique identifier for the underlying 
physical entity. If there were a one-to-one relation-
ship between gene name and unique identifier, the 
gene identification task would be straightforward.  
However, both polysemy and synonymy occur fre-
quently in the naming of biological entities, and 
the gene names of Drosophila are considered to be 
particularly problematic because of creative nam-
ing conventions10.  For example, ?18 wheeler?, 
?batman?, and ?rutabaga? are all Drosophila gene 
names. A single entity (as represented by a unique 
identifier) may have a number of names like Toll 
or even ATP?, which has 38 synonyms listed in 
FlyBase.     
 
Figure 3: FlyBase entry for Toll 
 
Resources 
We obtained a copy of part the FlyBase database,11 
including the lists of genes discussed in each paper 
examined by the curators.  Using the BioPython12 
modules, we were able to obtain MEDLINE ab-
stracts for 15,144 for these papers.  We decided to 
10 At the other end of the spectrum is the yeast nomenclature 
which is strictly controlled ? see <http://genome- 
www.stanford.edu/Saccharomyces/gene_guidelines.shtml> for 
nomenclature conventions. 
11 Special thanks to William Gelbart, David Emmert, Beverly 
Matthews, Leyla Bayraktaroglu, and Don Gilbert. 
12 http://www.biopython.org/ 
set aside the same articles used in the KDD Cup 
Challenge [Yeh03] for evaluation purposes.  This 
left a training set of 14,033 abstracts, consisting of 
a total of 2,664,324 lexemes identified by our 
tokenizer. 
4 
4.1 
                                                          
 
It was only with some reluctance that we decided 
to focus on journal abstracts. From our earlier 
work, we recognized that the majority of the in-
formation entered into FlyBase is missing from the 
abstracts and can be found only in the full text of 
the article [Hirschman03]. However, due to copy-
right restrictions, there is a paucity of freely avail-
able full text for journal articles.  What articles are 
available in electronic form vary in their format-
ting, which can cause considerable difficulty in 
automatic processing. MEDLINE abstracts have a 
uniform format and are readily available. Many 
other experiments have been performed on 
MEDLINE abstracts for similar reasons. 
 
We also created a synonym lexicon from FlyBase.  
We found 35,971 genes with associated ?gene 
symbols? (e.g. Tl is the gene symbol for Toll) and 
48,434 synonyms; therefore, each gene has an av-
erage of 2.3 alternate naming forms, including the 
gene symbol.  The lexicon also allowed us to asso-
ciate each gene with one a unique FlyBase gene 
identifier, providing "term normalization." 
Experiments 
For purposes of evaluation, our task was the identi-
fication of mentions of Drosophila genes in the 
text of abstracts.  We also included mentions of 
protein or transcript where the associated gene 
shared the same name. This occurs when, for ex-
ample, the gene name appears as a pre-nominal 
modifier, as in "the zygotic Toll protein".  We did 
not include mentions of protein complexes because 
these are created out of multiple polypeptide 
chains with multiple genes (e.g., hemoglobin). We 
also did not include families of proteins or genes 
(e.g. lectin), particular alleles of a gene, genes 
which are not part of the natural Drosophila ge-
nome such as reporter genes (e.g. LacZ), and the 
names of genes from other organisms (e.g. sonic 
hedgehog, the mammalian gene homologous to the 
Drosophila hedgehog gene).13 
Background 
Our initial experiment [Hirschman03] had looked 
at creating a gene name finder by simple pattern 
matching, using the extensive FlyBase list of genes 
and their synonyms and identifying each mention 
which occurred in the lexicon with the appropriate 
unique identifier. This yielded spectacularly poor 
results: recall14 on the full papers was quite high 
(84%), but precision was 2%!  For abstracts, the 
recall was predictably lower (31%) and precision 
remained low at 7%.  Our analysis showed that 
polysemy (described in Section 5) and the large 
intersection of gene names with common English 
words caused most of the performance problems. 
In the initial run, where a name was ambiguous, 
we recorded all gene identifiers; this raised recall 
but lowered precision.  After removing all the 
names which were ambiguous for a gene, precision 
climbed to 5% for full papers and 17% in abstracts, 
with a corresponding drop in recall (77% for full 
papers, 28% for abstracts).  We also tried a few 
simple filters, such as ignoring all terms three 
characters or less in length, but the best precision 
we could achieve was 29% in abstracts, certainly 
unacceptable. 
 
We were, however, encouraged by the relatively 
high recall in full papers. Analysis showed that 
many of the missing names were contained only in 
figures or tables that had not been downloaded.  
While these were counted as recall errors when 
compared to the FlyBase curation, there were, in 
fact, no mentions of these genes in the text that had 
been downloaded for this experiment.  Similarly, 
for abstracts, while the recall appeared low com-
pared to the complete set of genes discussed in the 
full paper, these genes were simply not mentioned 
in the abstract.  So from an information extraction 
13 There are no curated lists of complexes or families in Fly-
Base, so we did not train a tagger for these tasks. In our man-
ual curation, we did create separate tags for complexes and 
families, since we believe that these will be important for fu-
ture tasks.  
14 Note that these measures of recall and precision are based 
on the list of unique Drosophila genes curated in a paper. This 
is quite different from recall and precision measuring the men-
tions of gene names in a paper. We used the measure of 
unique genes in a paper because this allowed us to take advan-
tage of the existing FlyBase expert curated resources. 
point of view, the simple pattern matching 
achieved a very high recall for genes mentioned in 
the text being processed. 
4.2 
4.3 
Generating Noisy Training Data 
The initial experiment demonstrated that exact 
match using rich lexical resources was not useful 
on its own. However, we realized that we could 
use the lists of curated genes from FlyBase to con-
strain the possible matches within an abstract ? that 
is, to "license" the tagging of only those genes 
known to occur in the curated full article.  Our 
hope was that this filtered data would provide large 
quantities of cheap but imperfect or noisy training 
data.  
 
Our next experiment focused on generating this 
large but noisy training corpus.  We used our inter-
nal tokenizer, punctoker, originally designed for 
use with newswire data.  There were some errors in 
tokenization, since biological terms have a very 
different morphology from newswire? see 
[Cohen02] for an interesting discussion of tokeni-
zation issues. Among the problems in tokenization 
were uses of "-" instead of white space, or "/" to 
separate recombinant genes.  However, an informal 
examination of errors did not show tokenization 
errors to be a significant contributor to the overall 
performance of the entity extraction system. 
 
To perform the pattern matching, we created a suf-
fix tree of all the synonyms known to FlyBase for 
those genes. This was important, since many bio-
logical entity names are multi-word terms.   We 
then used longest-extent pattern matching to find 
candidate mentions in the abstract of the paper.  
The system tagged only terms licensed by the as-
sociated list of genes for the abstract, assigning the 
appropriate unique gene identifier. Even with the 
FlyBase filtering, this method resulted in some 
errors.  For example, an examination of an abstract 
describing the gene to revealed the unsurprising 
result that all the uses of the word "to" did not refer 
to the gene.  However, the aim was to create data 
of sufficient quantity to lessen the effects of this 
noise. 
Evaluation 
In order to measure performance, we created a 
small doubly annotated test corpus.  We selected a 
sample of 86 abstracts and had two annotators 
mark these abstracts for gene name mentions as 
previously described.  Mentions of families and 
foreign genes were also identified with different 
tags during this process, but not evaluated.   One 
curator was a professional researcher in biology 
with experience as a model organism genome da-
tabase curator (Colosimo).  This set of annotations 
was taken as the "gold-standard". The second an-
notator was the system developer with no particu-
lar annotation experience (Morgan). With two 
annotators, we were able to measure inter-
annotator agreement (F-measure of 0.87). We also 
measured the quality of the automatically created 4.4
     
training data by using the lexical pattern matching 
procedure with filtering to generate annotations for 
86 abstracts in the test set.  The F-measure was 
0.83, when compared against the gold standard, 
shown in Table 1 below. 
F-measure Precision Recall
Training Data
Quality
0.83 0.78 0.88
Inter-
annotator
Agreement
0.87 0.83 0.91
 
 
Ta
We  
tha s 
me
the
[Pa
trai
and
wa
Fig
15 P
http 
ble 1: Training data quality and inter-annotator agreement  
HMM Tagging With Noisy Training Data 
 now had a large quantity of noisy training data
t we could use to train a statistical tagger.   Thi                                                     
thodology is illustrated in Figure 4.  We chose 
 HMM-based trainable entity tagger phrag15 
lmer99] to extract the names in text.  We 
ned phrag on different amounts of training data 
 measured performance.  Our evaluation metric 
s the standard metric used in named entity 
Abstracts
from
PubMed
Lexicon
FlyBase
Large Quantity
of Noisy
Training Data
Plain Text
Genes Tagged
Gene1 Gene2
Other1 Other2
Start End
Text automatically tagged using
FlyBase references and a lexicon is
used to train up a tagger capable of
tagging gene names in new text,
including gene names never observed
before.
Trainable
Tagger
ure 4: Schematic of  Methodology 
hrag is available for download at 
://www.openchannelfoundation.org/projects/Qanda 
Training Data F-measure Precision Recall
531522 0.62 0.73 0.54
529760 0.64 0.75 0.56
1342039 0.72 0.80 0.65
2664324 0.73 0.79 0.67
No Orthographic Correction
  
Table 2: Performance as a function of training data 
 
Training Data F-measure Precision Recall
531522 0.65 0.76 0.56
529760 0.66 0.74 0.59
522825 0.67 0.76 0.59
1322285 0.72 0.77 0.67
1342039 0.75 0.80 0.70
2664324 0.75 0.78 0.71
Orthographic Correction
 
Table 3: Improved performance with orthographical correction 
for Greek letters and case folding for term matching in training 
data  
 
-
f 
-
  
-
", 
p-
m 
n 
.6 
 
entity identification F-measure of 73%.  We then 
made a simple modification of the algorithm to 
correct for variations in orthography due to capi-
talization and representation of Greek letters:  we 
simply expanded the search for letters such as "?" 
to include "Delta" and "delta".  By expanding the 
matching of terms using the orthographical and 
case variants, performance of phrag improved 
slightly, shown in Table 3, improving our best 
performance to an F-measure of 75%.   
5 
 
Figure 5 shows these results in a graphical form.  
Two things are apparent from this graph.  Based on 
the results shown in Figure 2, we might expect the 
performance to be linear with the logarithm of the 
amount of training data, and in this case there is a 
rough fit with a correlation coefficient of .88.  The 
other result which stands out is that there is con-
siderable variation in the performance when train-
ed on different training sets of the same size.  We 
believe that this is due to the very limited amount 
of testing data. 
Error Analysis 
We have identified three types of polysemy in 
Drosophila gene names in FlyBase.  In some cases, 
one name (e.g., ?Clock?) can refer to two distinct 
genes: period or Clock.  The term with the most 
polysemy is ?P450? which is a family of genes and 
is listed as a synonym for 20 different genes in 
FlyBase.  In addition, the same term is often used 
interchangeably to refer to the gene, RNA tran-
script, or the protein. [Hazivassloglou01] presents 
interesting results that demonstrate that experts 
only agree 78% of the time on whether a particular 
mention refers to a gene or a protein.16  The most 
problematic type of polysemy occurs because 
many Drosophila gene names are also regular Eng-
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
100000 1000000 10000000
Training Data (# of Lexemes)
F
-m
ea
su
re
Figure 5: Performance as a function of the amount of train-
ing data.  The line is a least-squares logarithmic fit with an 
R2 value of .8814. 
                                                           
lish words such as "white", ?cycle?, and "bizarre". 
There are some particularly troublesome examples 
that occur because of frequent use of short forms 
(abbreviations) of gene names, e.g., "we", "a", 
"not?, and even ?and? each occur as gene names.  
These short forms are often abbreviations for the 
full gene name.  For example, the gene symbol of 
the gene takeout is "to", and the symbol for the 
16 The entity tagging task for FlyBase was defined to extract 
gene-or-protein names; however, in cases where the article 
talks only about the protein and not about the gene, the protein
name may not appear on the list of curated genes for the arti-
cle, leading to apparent false positives in tagging. evaluation, requiring the matching of a name's ex
tent and tag (except that for our experiment, we 
were only concerned with one tag, Drosophila 
gene).   Extent matching meant exact matching o
gene name boundaries at the level of tokens:   Ex
actly matching boundaries were considered a hit.
Inexact answers are considered a miss.  For exam
ple, a multiword gene name such as "fas receptor
which has been tagged for "fas" but not for "rece
tor" would constitute a miss (recall error) and a 
false alarm (precision error).  
 
Table 2 shows the performance of the basic syste
as a function of the amount of training data.  As 
with Figure 2, we see there is a diminishing retur
as the amount of training data is increased.   At 2
million words or training data, phrag achieved an
gene wee is "we".  It may be that more sophisti-
cated handling of abbreviations can address some 
of these issues. 
An error analysis looking at the results of our sta-
tistical tagger demonstrated some unusual behav-
ior.  Because our gene name tagger phrag uses a 
first order Markov model, it relies on local context 
and occasionally makes errors such as not tagging 
all of the occurrences of the term "rutabaga" in an 
abstract about rutabaga as gene names.  This cer-
tainly opens up the opportunity for some sort of 
post processing step to resolve these problems. 
 
The fact that phrag uses this local context can 
sometimes be a strength, enabling it to identify 
gene names it has never seen.  We estimated the 
ability of the system to identify new terms as gene 
names by substituting strings unknown to phrag in 
place of all the occurrences of gene names in the 
evaluation data.  The performance of the system at 
correctly identifying terms it had never observed 
gave a precision of 68%, a recall of 22% and an F-
measure of 33%.  This result is relatively encour-
aging, compared with the 3.3% precision and 4.4% 
recall for novel gene names reported by Krau-
thammer.  Recognizing novel names is important 
because the nomenclature of biological entities is 
constantly changing and entity tagging systems 
should to be able to rapidly adapt and recognize 
new terms.   
6 Conclusion and Future Directions 
We have demonstrated that we can automatically 
produce large quantities of relatively high quality 
training data; these data were good enough to train 
an HMM-based tagger to identify gene mentions 
with an F-measure of 75% (precision of 78% and 
recall of 71%), evaluated on our small develop-
ment test set of 86 abstracts.    This compares fa-
vorably with other reported results as described in 
Section 2, and as discussed below, we believe that 
we can improve upon these results in various ways.  
These results are still considerably below the re-
sults from [Gaizauskas03] and may be too low to 
be useful as a building block for further automated 
processing, such as relation extraction.  However, 
in the absence of any shared benchmark evaluation 
sets, cross-system performance cannot be evalu-
ated since the task definition and evaluation cor-
pora differ from system to system.   
 
We plan to take this work in several directions.  
First, we believe that we can improve the quality of 
the underlying automatically generated data, and 
with this, the quality of the entity tagging. There 
are several things that could be improved.  
 
A morphological analyzer trained for biological 
text would eliminate some of the tokenization er-
rors and perhaps capture some of the underlying 
regularities, such as addition of Greek letters or 
numbers (with or without preceding hyphen) to 
specify sub-types within a gene family. There can 
also be considerable semantic content in gene 
names and their formatting.  For example, many 
Drosophila genes are differentiated from the genes 
of other organisms by prepending a "d" or "D", 
such as "dToll".  Gene names can also be explicit 
descriptions of their chromosomal location or even 
function (e.g. Dopamine receptor). 
 
The problem of matching abbreviations has been 
tackled by a number of researchers [e.g. Puste-
jovsky02 and Liu03].  As was mentioned above, it 
seems that ambiguity for "short forms" of gene 
names could be partially resolved by detecting lo-
cal definitions for abbreviations.  It should also be 
possible to apply part of speech tagging and corpus 
statistics to avoid mis-tagging of common words, 
such as ?to? or ?and?.  
 
In the longer term, this methodology provides an 
opportunity to go beyond gene name tagging for 
Drosophila. It can be extended to other domains 
that have comparable resources (e.g. other model 
organism genome databases, other biological enti-
ties), and entity tagging itself provides the founda-
tion for more complex tasks, such as relation 
extraction (e.g. using the BIND database) or attrib-
ute extraction (e.g. using FlyBase to identify at-
tributes such as RNA transcript length, associated 
with protein coding genes). 
 
Second, the existence of a synonym lexicon with 
unique identifiers provides data for term normali-
zation, a task of potentially greater utility to biolo-
gists than the tagging of every mention in an 
article.  There are currently few corpora with anno-
tated term normalization; using the methodology 
outlined here makes it possible to produce large 
quantities of normalized data.  The identification 
and characterization of abbreviations and other 
transformations would be particularly important in 
normalization.   
By exploiting the rich set of biological resources 
that already exist, it should be possible to generate 
many kinds of corpora useful for training high-
quality information extraction and text mining 
components. 
References 
 
Bikel D, Schwartz R, Weischedel R. An Algorithm that 
Learns What's in a Name. Machine Learning, Special 
Issue on Natural Language Learning 34 (1999):211-31. 
 
Cohen KB, Dolbey A, Hunter L. ?Contrast and variabil-
ity in gene names.? Proceedings of the workshop on 
natural language processing in the biomedical domain, 
Association for Computational Linguistics, 2002 
 
Collier N, Nobata C, Tsujii J. ?Extracting the Names of 
Genes and Gene Products with a Hidden Markov 
Model.? Proceedings of COLING '2000 (2000): 201-07. 
 
Craven M, Kumlien J. ?Constructing Biological Knowl-
edge Bases by Extracting Information from Text 
Sources.? Proceedings of the Seventh International 
Conference on Intelligent Systems for Molecular Biol-
ogy 1999: 77-86. 
 
Gaizauskas R, Demetriou G, Artymiuk PJ, Willett P. 
?Protein Structures and Information Extraction from 
Biological Texts: The PASTA System.? Bioinformatics. 
19  (2003): 135-43. 
 
Hatzivassiloglou V, Duboue P, Rzhetsky A. ?Disam-
biguating Proteins, Genes, and RNA in Text: A Ma-
chine Learning Approach.? Bioinformatics 2001: 97-
106. 
 
Hirschman L, Park J, Tsujii J, Wong L, Wu C. "Accom-
plishments and Challenges in Literature Data Mining 
for Biology," Bioinformatics 17 (2002):1553-61. 
 
Hirschman L, Morgan A, Yeh A.  ?Rutabaga by Any 
Other Name: Extracting Biological Names." Accepted, 
Journal of Biomedical Informatics, Spring 2003.  
 
Krauthammer M, Rzhetsky A, Morosov P, Friedman C. 
?Using BLAST for Identifying Gene and Protein Names 
in Journal Articles.? Gene 259 (2000): 245-52. 
 
Liu H, Friedman C.  ?Mining Terminological Knowl-
edge in Large Biomedical Corpora.?  Proceedings of the 
Pacific Symposium on Biocomputing.  2003. 
 
Palmer D, Burger J, and Ostendorf M. "Information 
Extraction from Broadcast News Speech Data." Pro-
ceedings of the DARPA Broadcast News and Under-
standing Workshop, 1999. 
 
Pustejovsky J, Casta?o J, Saur? R, Rumshisky A, Zhang 
J, Luo W. ?Medstract: Creating Large-scale Information 
Servers for Biomedical Libraries.? Proceedings of the 
ACL 2002 Workshop on Natural Language Processing 
in the Biomedical Domain. 2002. 
 
Tauszig et al ?Toll-related receptors and the control of 
antimicrobial peptide expression in Drosophila.? Pro-
ceedings of the  National Academy of  Sciences 97 
(2000): 10520-5. 
 
Yeh A., Hirschman L,  Morgan A.  "Evaluation of Text 
Data Mining for Database Curation: Lessons Learned 
from the KDD Challenge Cup." Accepted, Intelligent 
Systems in Molecular Biology, Brisbane, June 2003.  
 
 
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 72?75,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Measuring Risk and Information Preservation: Toward New Metrics for De-identification of Clinical Texts 
 Lynette Hirschman & John Aberdeen The MITRE Corporation 202 Burlington Rd. Bedford, MA 01730 {lynette,aberdeen}@mitre.org 
 
 
 
Abstract 
Current metrics for de-identification are based on information extraction metrics, and do not address the real-world questions ?how good are current systems?, and ?how good do they need to be?. Metrics are needed that quantify both the risk of re-identification and informa-tion preservation. We review the challenges in de-identifying clinical texts and the current metrics for assessing clinical de-identification systems. We then introduce three areas to ex-plore that can lead to metrics that quantify re-identification risk and information preserva-tion. 1 Introduction Our current metrics do not address the ques-tions ?how good are current free-text de-identification systems?, and ?how good do they need to be?? We need measures that quantify risk of re-identification based on type and amount of personal health identifier (PHI) leakage (PHI terms not redacted in the de-identification process), and measures that quantify information preservation or readability. The metrics in current use were developed originally for entity extraction (the correct labeling of types of phrases in free text, such as person name, date, organization). Entity extraction per-formance is typically measured in terms of preci-sion, recall and balanced f-measure at both the token (word) and phrase level. The top de-identification systems (Szarvas, Farkas, & Busa-Fekete, 2007; Wellner, et al, 2007) performed well on these measures, as reported at the first i2b2 De-
identification challenge evaluation (Uzuner, Luo, & Szolovits, 2007), achieving accuracies of over 97% token-level f-measure, with recall (sensitivity) of over 95%. Over the past several years, these results have been extended to more record types and record formats; for example, (Friedlin & McDonald, 2008) reported that their MeDS system successfully removed 99.5% of HIPAA-specified identifiers from HL7 records. These are encourag-ing numbers, but recall and precision do not tell us how good a de-identification system needs to be for a particular intended use. 2 Challenges in Clinical Text Removing PHI from unstructured text poses new challenges: in contrast to structured information (e.g., fields in a table or a database), we do not know in advance where PHI will appear in a free text record, and we do not know what kinds of PHI will occur. This problem is made more challenging for medical records because the types of record vary greatly in content and in amount of PHI ? for example, a lab report will likely contain very little PHI, while a social work note will be likely to con-tain much more. Medical records have internal structure that is dependent on the medical record system and the medical record type; there is typically a mix of structured fields (e.g., for patient identifier, patient name, doctor name), along with unstructured fields for free text. This means that de-identification of records must handle a combination of structured and unstructured information. The excerpt below shows two free text fields (CLINICAL HISTORY and IMPRESSION) from a (fictitious) radiology 
72
report, with several types of PHI, including dates, locations and ages (shown in bold).  RADIOLOGY REPORT CLINICAL HISTORY: Patient is a 4-year 5-month old male who presented to Oak Valley Health Center on 10/11/2007 with a cough of 10 days duration and fever. Patient lives in a densely populated section of Knoxville. Rule out pneumonia. IMPRESSION: Scattered lung densities likely to represent either scattered atelectasis or acute viral illness with no definite lobar pneumonia identified.  This example illustrates how PHI is distributed in the free text portions of a medical record. Re-moving PHI from these free text portions requires application of techniques from natural language processing that are capable of identifying phrases of specific types based on the lexical content (the words that make up the phrases) and the surround-ing words.  3 Current Methods and Metrics Fortunately, the problem of identifying types of information in free text is a well-studied problem in the natural language processing community. We can leverage several decades of research on infor-mation extraction and the named entity identifica-tion problem in particular, including multiple community evaluations such as the Message Un-derstanding Conferences (MUC) (Grishman & Sundheim, 1996) and the subsequent Automated Content Extraction (ACE) evaluations1 ? both fo-cused on extraction from newswire -- as well as evaluations of biomedical entity extraction from the published literature e.g., in the BioCreative evaluations (Krallinger, et al, 2008). In addition, starting in 2006, there have been a series of evalua-tions for clinical natural language processing, with data sets of clinical records provided by the i2b2 consortium (Uzuner, et al, 2007). It has been criti-cally important to have corpora of medical records, because medical records represent a very different style of text compared to news articles or journal articles. Medical records are characterized by their formulaic and telegraphic style, that is, the use of                                                 1 http://www.itl.nist.gov/iad/mig/tests/ace 
phrases or incomplete sentences rather than fluent prose, along with heavy use of abbreviations and domain-specific terminology (e.g., ?93 yo w NVD?). The systems developed for newswire or for journal articles must be explicitly adapted (or trained) to handle the categories required for de-identification as well as the telegraphic language of medical records.  De-identification of free text medical records consists of two steps: recognition and redaction. The phrase recognition stage corresponds to the named entity recognition problem mentioned above, namely the ability to identify a sequence of words in running text that constitutes the mention of an entity of a specified type ? such as the phrase Oak Valley Health Center in the example above. For newswire, types of named entities include per-son, organization, location, time, date, and money; for biomedical tasks, entities have included genes, proteins, drugs, diseases, etc. For de-identification, the critical elements are the 18 types of protected health information identified by HIPAA,2 includ-ing names, dates, locations, zip codes, phone num-bers, social security numbers, ages ninety and above, URLs and other identifying information. Interestingly, most institutions have developed their own set of protected classes of information, e.g., some institutions distinguish between DOCTOR and PATIENT identifiers, which both fall into the more general HIPAA category of NAME. The techniques used to recognize named enti-ties in text include: ? Lexically-based approaches that rely on matching words (or phrases) against the words or phrases contained in a lexicon;  ? Pattern based approaches that are particu-larly useful for HIPAA-relevant PHI such as telephone numbers, social security numbers, dates, etc.  ? Machine learning approaches that are based on statistical models of word se-quences. These approaches require training exemplars that are used to associate se-quences of words with probabilities of types of phrase, e.g., the word(s) following ?Dr.? or ?DR? will likely be a doctor?s name.                                                  2 Health Insurance Portability and Accountability Act of 1996, Pub. L. No. 104-191, 110 Stat. 1936 (1996). 
73
All three approaches have been used and often combined (Beckwith, Mahaadevan, Balis, & Kuo, 2006; Berman, 2003; Friedlin & McDonald, 2008; Gupta, Saul, & Gilbertson, 2004; Morrison, Li, Lai, & Hripcsak, 2009; Szarvas, et al, 2007; Uzuner, Sibanda, Luo, & Szolovits, 2008; Wellner, et al, 2007) to provide high quality recognition of PHI. The 2006 i2b2 challenge evaluation for auto-matic de-identification of free text clinical records provided an opportunity for groups to benchmark their automated de-identification systems against a carefully prepared gold standard corpus of medical discharge summaries. The top systems performed well with scores of over 0.97 token-level f-measure and recall (sensitivity) of over 0.95 (Uzuner, et al, 2007). 4 Toward New Metrics The Uzuner et al (2007) paper concludes with two important (and as yet unanswered) questions (p. 562):  1. Does success on this challenge problem ex-trapolate to similar performance on other, untested data sets?  2. Can health policy makers rely on this level of performance to permit automated or semi-automated disclosure of health data for research purposes without undue risk to patients?  We have been particularly concerned with the second question, because it will be very difficult to release automatically de-identified data until we can provide an answer. The metrics used to date have been measures of technology performance, but they do not address the key issues of risk of PHI exposure and readability/preservation of in-formation in the de-identified record.  Recall errors are clearly correlated with risk of PHI exposure, but not all recall errors lead to PHI exposure. For example, the name ?Washington, George? might be mistakenly redacted to ?LOCATION, NAME? leading to both a recall and a precision error for the word ?Washington? but no PHI exposure. Also, some kinds of PHI exposure errors contain much more information (e.g., a pa-tient?s last name) than others (a first name; or a telephone extension where the telephone number has been redacted). Friedlin and McDonald (2008) 
report that MeDS did not miss any full patient identifiers, but it did miss an average of 2.13 pa-tient identifier fragments per report. However, they concluded that none of these fragments were true patient identifiers. Similarly, precision errors cause mislabeling of results and are correlated with loss of readability. In the extreme case, a system that redacts all words would achieve perfect recall, but very low preci-sion and no information content. A system that replaces real PHI with synthetic (fictitious) PHI might be more resistant to re-identification because it would be difficult for an attacker to distinguish real from fictitious information.  We need new measures that quantify risk of re-identification based on type and amount of PHI leakage (PHI terms or parts of terms not redacted in the de-identification process); and we need measures that quantify information preservation or readability. We plan to explore three areas that may yield more useful metrics for de-identification. The first is to quantify the re-identification risk through a detailed analysis of PHI in different record types. Given a set of records and a de-identification sys-tem, we can generate quantitative data on PHI dis-tribution in different record types, rate of exposure of different classes of PHI (e.g., names vs. loca-tions vs. phone numbers), and likelihood of com-binations of exposed PHI. We can distinguish between partial exposure of PHI (e.g., just a first name or just a room number), and combinations of such exposures within a single record (room num-ber and institution provides much more identifying information than room number alone). Using this information, we can develop analyses of risk using methods developed for structured data (Machanavajjhala, Kifer, Gehrke, & Venkitasubramaniam, 2007; Malin, 2007; Sweeney, 2002) by combining statistics from de-identified records with publicly available informa-tion (census data, voter registration, etc). The second area to explore is how to measure information preservation or readability. One ap-proach would be to apply one or more available medical information extraction systems such as the Mayo Clinic cTAKES system (Savova, Kipper-Schuler, Buntrock, & Chute, 2008) to compare information correctly extracted from de-identified data vs. original data. This would provide a rea-sonable proxy for measuring information loss due 
74
to de-identification. Alternatively, Friedlin and McDonald (2008) developed a measure of inter-pretability in their de-identification experiments, defined as preserving test type and test results (for lab reports) or type of report, specimen and con-clusion (for pathology reports).  A third area to explore is protection by hiding in plain sight. We can determine the reduction in risk from applying resynthesis (Yeniterzi, et al, 2010) to de-identified data, which would have the effect of hiding exposed PHI in plain sight ? since such elements would be interspersed with fictitious but realistic looking identifiers (particularly names) inserted as replacements of PHI. 5 Conclusion Current metrics for de-identification have their origins in information extraction; they neither ade-quately assess the risk of re-identification, nor do they provide a good measure of information pres-ervation. We plan to address these shortcomings by 1) applying risk analysis methods derived for structured data, 2) using medical extraction sys-tems to assess information preservation, and 3) exploring hiding in plain sight protection by using resynthesis to replace identifiers with false by real-istic fillers. Once we have alternative measures for risk of re-identification and information preserva-tion, we can also explore the correlation of preci-sion and recall to these new measures. Accurately quantifying and balancing risk of re-identification and information preservation will enable health policy makers to make better decisions about the use of automated de-identification, and sharing of clinical data for research. References  Beckwith, B. A., Mahaadevan, R., Balis, U. J., & Kuo, F. (2006). Development and evaluation of an open source software tool for deidentification of pathology reports. BMC Med Inform Decis Mak, 6, 12. Berman, J. (2003). Concept-Match Medical Data Scrubbing. Arch Pathol Lab Med, 127, 680-686. Friedlin, F. J., & McDonald, C. J. (2008). A software tool for removing patient identifying information from clinical documents. J Am Med Inform Assoc, 15(5), 601-610. Grishman, R., & Sundheim, B. (1996). Message Understanding Conference - 6: A Brief History. Paper 
presented at the 16th International Conference on Computational Linguistics, Copenhagen. Gupta, D., Saul, M., & Gilbertson, J. (2004). Evaluation of a deidentification (DE-ID) software engine to share pathology reports and clinical documents for research. Am J Clin Pathol, 121(2), 176-186. Krallinger, M., Morgan, A., Smith, L., Leitner, F., Tanabe, L., Wilbur, J., Hirschman, L., & Valencia, A. (2008). Evaluation of text-mining systems for biology: overview of the Second BioCreative community challenge. Genome Biol, 9 Suppl 2, S1. Machanavajjhala, A., Kifer, D., Gehrke, J., & Venkitasubramaniam, M. (2007). l-diversity: Privacy beyond k-anonymity. ACM Transactions on Knowledge Discovery from Data (TKDD), 1(1), 3. Malin, B. (2007). A computational model to protect patient data from location-based re-identification. Artif Intell Med, 40(3), 223-239. Morrison, F. P., Li, L., Lai, A. M., & Hripcsak, G. (2009). Repurposing the clinical record: can an existing natural language processing system de-identify clinical notes? J Am Med Inform Assoc, 16(1), 37-39. Savova, G., Kipper-Schuler, K., Buntrock, J., & Chute, C. (2008). UIMA-based Clinical Information Extraction System. Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP, 39. Sweeney, L. (2002). k-anonymity: A model for protecting privacy. International Journal of Uncertainty Fuzziness and Knowledge Based Systems, 10(5), 557-570. Szarvas, G., Farkas, R., & Busa-Fekete, R. (2007). State-of-the-art anonymization of medical records using an iterative machine learning framework. J Am Med Inform Assoc, 14(5), 574-580. Uzuner, ?., Luo, Y., & Szolovits, P. (2007). Evaluating the state-of-the-art in automatic de-identification. J Am Med Inform Assoc, 14(5), 550-563. Uzuner, ?., Sibanda, T. C., Luo, Y., & Szolovits, P. (2008). A de-identifier for medical discharge summaries. Artif Intell Med, 42(1), 13-35. Wellner, B., Huyck, M., Mardis, S., Aberdeen, J., Morgan, A., Peshkin, L., Yeh, A., Hitzeman, J., & Hirschman, L. (2007). Rapidly retargetable approaches to de-identification in medical records. J Am Med Inform Assoc, 14(5), 564-573. Yeniterzi, R., Aberdeen, J., Bayer, S., Wellner, B., Clark, C., Hirschman, L., & Malin, B. (2010). Effects of Personal Identifier Resynthesis on Clinical Text De-identification. J Am Med Inform Assoc, 17(2), 159-168.   
75
