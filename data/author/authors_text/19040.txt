Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 560?567,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Fast Semantic Extraction Using a Novel Neural Network Architecture
Ronan Collobert
NEC Laboratories America, Inc.
4 Independence Way
Suite 200, Princeton, NJ 08540
collober@nec-labs.com
Jason Weston
NEC Laboratories America, Inc.
4 Independence Way
Suite 200, Princeton, NJ 08540
jasonw@nec-labs.com
Abstract
We describe a novel neural network archi-
tecture for the problem of semantic role la-
beling. Many current solutions are compli-
cated, consist of several stages and hand-
built features, and are too slow to be applied
as part of real applications that require such
semantic labels, partly because of their use
of a syntactic parser (Pradhan et al, 2004;
Gildea and Jurafsky, 2002). Our method in-
stead learns a direct mapping from source
sentence to semantic tags for a given pred-
icate without the aid of a parser or a chun-
ker. Our resulting system obtains accuracies
comparable to the current state-of-the-art at
a fraction of the computational cost.
1 Introduction
Semantic understanding plays an important role in
many end-user applications involving text: for infor-
mation extraction, web-crawling systems, question
and answer based systems, as well as machine trans-
lation, summarization and search. Such applications
typically have to be computationally cheap to deal
with an enormous quantity of data, e.g. web-based
systems process large numbers of documents, whilst
interactive human-machine applications require al-
most instant response. Another issue is the cost of
producing labeled training data required for statisti-
cal models, which is exacerbated when those models
also depend on syntactic features which must them-
selves be learnt.
To achieve the goal of semantic understanding,
the current consensus is to divide and conquer the
[The company]ARG0 [bought]REL [sugar]ARG1 [on the world
market]ARGM-LOC [to meet export commitments]ARGM-PNC
Figure 1: Example of Semantic Role Labeling from
the PropBank dataset (Palmer et al, 2005). ARG0
is typically an actor, REL an action, ARG1 an ob-
ject, and ARGM describe various modifiers such as
location (LOC) and purpose (PNC).
problem. Researchers tackle several layers of pro-
cessing tasks ranging from the syntactic, such as
part-of-speech labeling and parsing, to the semantic:
word-sense disambiguation, semantic role-labeling,
named entity extraction, co-reference resolution and
entailment. None of these tasks are end goals in
themselves but can be seen as layers of feature ex-
traction that can help in a language-based end ap-
plication, such as the ones described above. Un-
fortunately, the state-of-the-art solutions of many of
these tasks are simply too slow to be used in the ap-
plications previously described. For example, state-
of-the-art syntactic parsers theoretically have cubic
complexity in the sentence length (Younger, 1967)1
and several semantic extraction algorithms use the
parse tree as an initial feature.
In this work, we describe a novel type of neural
network architecture that could help to solve some
of these issues. We focus our experimental study on
the semantic role labeling problem (Palmer et al,
2005): being able to give a semantic role to a syn-
1Even though some parsers effectively exhibit linear be-
havior in sentence length (Ratnaparkhi, 1997), fast statistical
parsers such as (Henderson, 2004) still take around 1.5 seconds
for sentences of length 35 in tests that we made.
560
tactic constituent of a sentence, i.e. annotating the
predicate argument structure in text (see for exam-
ple Figure 1). Because of its nature, role labeling
seems to require the syntactic analysis of a sentence
before attributing semantic labels. Using this intu-
ition, state-of-the-art systems first build a parse tree,
and syntactic constituents are then labeled by feed-
ing hand-built features extracted from the parse tree
to a machine learning system, e.g. the ASSERT sys-
tem (Pradhan et al, 2004). This is rather slow, tak-
ing a few seconds per sentence at test time, partly
because of the parse tree component, and partly be-
cause of the use of Support Vector Machines (Boser
et al, 1992), which have linear complexity in test-
ing time with respect to the number of training ex-
amples. This makes it hard to apply this method to
interesting end user applications.
Here, we propose a radically different approach
that avoids the more complex task of building a full
parse tree. From a machine learning point of view, a
human does not need to be taught about parse trees
to talk. It is possible, however, that our brains may
implicitly learn features highly correlated with those
extracted from a parse tree. We propose to develop
an architecture that implements this kind of implicit
learning, rather than using explicitly engineered fea-
tures. In practice, our system also provides semantic
tags at a fraction of the computational cost of other
methods, taking on average 0.02 seconds to label a
sentence from the Penn Treebank, with almost no
loss in accuracy.
The rest of the article is as follows. First, we de-
scribe the problem of shallow semantic parsing in
more detail, as well as existing solutions to this prob-
lem. We then detail our algorithmic approach ? the
neural network architecture we employ ? followed
by experiments that evaluate our method. Finally,
we conclude with a summary and discussion of fu-
ture work.
2 Shallow Semantic Parsing
FrameNet (Baker et al, 1998) and the Proposition
Bank (Palmer et al, 2005), or PropBank for short,
are the two main systems currently developed for
semantic role-labeling annotation. We focus here
on PropBank. PropBank encodes role labels by se-
mantically tagging the syntactic structures of hand
annotated parses of sentences. The current version
of the dataset gives semantic tags for the same sen-
tences as in the Penn Treebank (Marcus et al, 1993),
which are excerpts from theWall Street Journal. The
central idea is that each verb in a sentence is la-
beled with its propositional arguments, where the
abstract numbered arguments are intended to fill typ-
ical roles. For example, ARG0 is typically the actor,
and ARG1 is typically the thing acted upon. The
precise usage of the numbering system is labeled for
each particular verb as so-called frames. Addition-
ally, semantic roles can also be labeled with one of
13 ARGM adjunct labels, such as ARGM-LOC or
ARGM-TMP for additional locational or temporal
information relative to some verb.
Shallow semantic parsing has immediate applica-
tions in tasks such as meta-data extraction (e.g. from
web documents) and question and answer based sys-
tems (e.g. call center systems), amongst others.
3 Previous Work
Several authors have already attempted to build ma-
chine learning approaches for the semantic role-
labeling problem. In (Gildea and Jurafsky, 2002)
the authors presented a statistical approach to learn-
ing (for FrameNet), with some success. They pro-
posed to take advantage of the syntactic tree struc-
ture that can be predicted by a parser, such as Char-
niak?s parser (Charniak, 2000). Their aim is, given
a node in the parse tree, to assign a semantic role
label to the words that are the children of that node.
They extract several key types of features from the
parse tree to be used in a statistical model for pre-
diction. These same features also proved crucial to
subsequent approaches, e.g. (Pradhan et al, 2004).
These features include:
? The parts of speech and syntactic labels of
words and nodes in the tree.
? The node?s position (left or right) in relation to
the verb.
? The syntactic path to the verb in the parse tree.
? Whether a node in the parse tree is part of a
noun or verb phrase (by looking at the parent
nodes of that node).
561
? The voice of the sentence: active or passive
(part of the PropBank gold annotation);
as well as several other features (predicate, head
word, verb sub-categorization, . . . ).
The authors of (Pradhan et al, 2004) used a
similar structure, but added more features, notably
head word part-of-speech, the predicted named en-
tity class of the argument, word sense disambigua-
tion of the verb and verb clustering, and others (they
add 25 variants of 12 new feature types overall.)
Their system also uses a parser, as before, and then a
polynomial Support Vector Machine (SVM) (Boser
et al, 1992) is used in two further stages: to clas-
sify each node in the tree as being a semantic ar-
gument or not for a given verb; and then to clas-
sify each semantic argument into one of the classes
(ARG1, ARG2, etc.). The first SVM solves a two-
class problem, the second solves a multi-class prob-
lem using a one-vs-the-rest approach. The final sys-
tem, called ASSERT, gives state-of-the-art perfor-
mance and is also freely available at: http://
oak.colorado.edu/assert/. We compare
to this system in our experimental results in Sec-
tion 5. Several other competing methods exist, e.g.
the ones that participated in the CONLL 2004 and
2005 challenges (http://www.lsi.upc.edu/
?srlconll/st05/st05.html). In this paper
we focus on a comparison with ASSERT because
software to re-run it is available online. This also
gives us a timing result for comparison purposes.
The three-step procedure used in ASSERT (calcu-
lating a parse tree and then applying SVMs twice)
leads to good classification performance, but has
several drawbacks. First in speed: predicting a
parse tree is extremely demanding in computing re-
sources. Secondly, choosing the features necessary
for SVM classification requires extensive research.
Finally, the SVM classification algorithm used in ex-
isting approaches is rather slow: SVM training is at
least quadratic in time with respect to the number
of training examples. The number of support vec-
tors involved in the SVM decision function also in-
creases linearly with the number of training exam-
ples. This makes SVMs slow on large-scale prob-
lems, both during training and testing phases.
To alleviate the burden of parse tree computation,
several attempts have been made to remove the full
parse tree information from the semantic role label-
ing system, in fact the shared task of CONLL 2004
was devoted to this goal, but the results were not
completely satisfactory. Previously, in (Gildea and
Palmer, 2001), the authors tried to show that the
parse tree is necessary for good generalization by
showing that segments derived from a shallow syn-
tactic parser or chunker do not perform as well for
this goal. A further analysis of using chunkers, with
improved results was also given in (Punyakanok et
al., 2005), but still concluded the full parse tree is
most useful.
4 Neural Network Architecture
Ideally, we want an end-to-end fast learning system
to output semantic roles for syntactic constituents
without using a time consuming parse tree.
Also, as explained before, we are interesting in
exploring whether machine learning approaches can
learn structure implicitly. Hence, even if there is a
deep relationship between syntax and semantics, we
prefer to avoid hand-engineered features that exploit
this, and see if we can develop a model that can learn
these features instead. We are thus not interested
in chunker-based techniques, even though they are
faster than parser-based techniques.
We propose here a neural network based architec-
ture which achieves these two goals.
4.1 Basic Architecture
The type of neural network that we employ is aMulti
Layer Perceptron (MLP). MLPs have been used for
many years in the machine learning field and slowly
abandoned for several reasons: partly because of
the difficulty of solving the non-convex optimization
problems associated with learning (LeCun et al,
1998), and partly because of the difficulty of their
theoretical analysis compared to alternative convex
approaches.
An MLP works by successively projecting the
data to be classified into different spaces. These
projections are done in what is called hidden lay-
ers. Given an input vector z, a hidden layer applies
a linear transformation (matrix M ) followed by a
squashing function h:
z 7? Mz 7? h(Mz) . (1)
562
A typical squashing function is the hyperbolic tan-
gent h(?) = tanh(?). The last layer (the output
layer) linearly separates the classes. The composi-
tion of the projections in the hidden layers could be
viewed as the work done by the kernel in SVMs.
However there is a very important difference: the
kernel in SVM is fixed and arbitrarily chosen, while
the hidden layers in an MLP are trained and adapted
to the classification task. This allows us to create
much more flexible classification architectures.
Our method for semantic role labeling classifies
each word of a sentence separately. We do not use
any semantic constituent information: if the model
is powerful enough, words in the same semantic
constituent should have the same class label. This
means we also do not separate the problem into
an identification and classification phase, but rather
solve in a single step.
4.1.1 Notation
We represent words as indices. We consider a fi-
nite dictionary of words D ? N. Let us represent a
sentence of nw words to be analyzed as a function
s(?). The ith word in the sentence is given by the
index s(i):
1 ? i ? nw s(i) ? D .
We are interested in predicting the semantic role la-
bel of the word at position posw, given a verb at po-
sition posv (1 ? posw, posv ? nw). A mathemati-
cal description of our network architecture schemat-
ically shown in Figure 2 follows.
4.1.2 Transforming words into feature vectors
Our first concern in semantic role labeling is that
we have to deal with words, and that a simple in-
dex i ? D does not carry any information specific
to a word: for each word we need a set of features
relevant for the task. As described earlier, previous
methods construct a parse tree, and then compute
hand-built features which are then fed to a classi-
fication algorithm. In order to bypass the use of a
parse tree, we convert each word i ? D into a par-
ticular vector wi ? Rd which is going to be learnt
for the task we are interested in. This approach has
already been used with great success in the domain
of language models (Bengio and Ducharme, 2001;
Schwenk and Gauvain, 2002).
 
 


     
 


Lookup Table d
.
.
.
d
Linear Layer with sentence?adapted columns
d
C(position w.r.t. cat, position w.r.t. sat)
Softmax Squashing Layer
...
ARG1 ARG2 ARGMLOC ARGMTMP
Classical Linear Layer
Tanh Squashing Layer
nhu Ci
ws(6)
ws(2)
s(1)w
...C1 C2 C6
Classical Linear Layer
ws(6)...ws(2)s(1)w
s(1)  s(2)   ...                   s(6)
satthe
Input Sentence
on the matcat
Figure 2: MLP architecture for shallow semantic
parsing. The input sequence is at the top. The out-
put class probabilities for the word of interest (?cat?)
given the verb of interest (?sat?) are given at the bot-
tom.
The first layer of our MLP is thus a lookup table
which replaces the word indices into a concatenation
of vectors:
{s(1), . . . , s(nw)}
7? (ws(1) . . . ws(nw)) ? R
nw d .
(2)
The weights {wi | i ? D} for this layer are consid-
ered during the backpropagation phase of the MLP,
and thus adapted automatically for the task we are
interested in.
4.1.3 Integrating the verb position
Feeding word vectors alone to a linear classifica-
tion layer as in (Bengio and Ducharme, 2001) leads
563
to very poor accuracy because the semantic classifi-
cation of a given word also depends on the verb in
question. We need to provide the MLP with infor-
mation about the verb position within the sentence.
For that purpose we use a kind of linear layer which
is adapted to the sentence considered. It takes the
form:
(ws(1) . . . ws(nw)) 7? M
?
?
?
wTs(1)
...
wTs(nw)
?
?
? ,
where M ? Rnhu?nw d, and nhu is the number of
hidden units. The specific nature of this layer is
that the matrix M has a special block-column form
which depends on the sentence:
M = (C1| . . . |Cnw) ,
where each column Ci ? Rnhu?d depends on the
position of the ith word in s(?), with respect to the
position posw of the word of interest, and with re-
spect to the position posv of the verb of interest:
Ci = C(i? posw, i? posv) ,
where C(?, ?) is a function to be chosen.
In our experiments C(?, ?) was a linear layer with
discretized inputs (i ? posw, i ? posv) which were
transformed into two binary vectors of size wsz,
where a bit is set to 1 if it corresponds to the po-
sition to encode, and 0 otherwise. These two binary
vectors are then concatenated and fed to the linear
layer. We chose the ?window size? wsz = 11. If
a position lies outside the window, then we still set
the leftmost or rightmost bit to 1. The parameters in-
volved in this function are also considered during the
backpropagation. With such an architecture we al-
low our MLP to automatically adapt the importance
of a word in the sentence given its distance to the
word we want to classify, and to the verb we are in-
terested in.
This idea is the major novelty in this work, and is
crucial for the success of the entire architecture, as
we will see in the experiments.
4.1.4 Learning class probabilities
The last layer in our MLP is a classical linear
layer as described in (1), with a softmax squashing
function (Bridle, 1990). Considering (1) and given
z? = Mz, we have
hi(z?) =
exp z?i
?
j exp z?j
.
This allows us to interpret outputs as probabilities
for each semantic role label. The training of the
whole system is achieved using a normal stochastic
gradient descent.
4.2 Word representation
As we have seen, in our model we are learning one
d dimensional vector to represent each word. If the
dataset were large enough, this would be an elegant
solution. In practice many words occur infrequently
within PropBank, so (independent of the size of d)
we can still only learn a very poor representation for
words that only appear a few times. Hence, to con-
trol the capacity of our model we take the original
word and replace it with its part-of-speech if it is
a verb, noun, adjective, adverb or number as deter-
mined by a part-of-speech classifier, and keep the
words for all other parts of speech. This classifier is
itself a neural network. This way we keep linking
words which are important for this task. We do not
do this replacement for the predicate itself.
5 Experiments
We used Sections 02-21 of the PropBank dataset
version 1 for training and validation and Section
23 for testing as standard in all our experiments.
We first describe the part-of-speech tagger we em-
ploy, and then describe our semantic role labeling
experiments. Software for our method, SENNA (Se-
mantic Extraction using a Neural Network Archi-
tecture), more details on its implementation, an on-
line applet and test set predictions of our system
in comparison to ASSERT can be found at http:
//ml.nec-labs.com/software/senna.
Part-Of-Speech Tagger The part-of-speech clas-
sifier we employ is a neural network architecture of
the same type as in Section 4, where the function
Ci = C(i ? posw) depends now only on the word
position, and not on a verb. More precisely:
Ci =
{
0 if 2 |i? posw| > wsz ? 1
Wi?posw otherwise ,
564
where Wk ? Rnhu?d and wsz is a window size.
We chose wsz = 5 in our experiments. The
d-dimensional vectors learnt take into account the
capitalization of a word, and the prefix and suf-
fix calculated using Porter-Stemmer. See http:
//ml.nec-labs.com/software/senna for
more details. We trained on the training set of Prop-
Bank supplemented with the Brown corpus, result-
ing in a test accuracy on the test set of PropBank of
96.85% which compares to 96.66% using the Brill
tagger (Brill, 1992).
Semantic Role Labeling In our experiments we
considered a 23-class problem of NULL (no la-
bel), the core arguments ARG0-5, REL, ARGA, and
ARGM- along with the 13 secondary modifier labels
such as ARGM-LOC and ARGM-TMP. We simpli-
fied R-ARGn and C-ARGn to be written as ARGn,
and post-processed ASSERT to do this as well.
We compared our system to the freely available
ASSERT system (Pradhan et al, 2004). Both sys-
tems are fed only the input sentence during testing,
with traces removed, so they cannot make use of
many PropBank features such as frameset identiti-
fier, person, tense, aspect, voice, and form of the
verb. As our algorithm outputs a semantic tag for
each word of a sentence, we directly compare this
per-word accuracy with ASSERT. Because ASSERT
uses a parser, and because PropBank was built by la-
beling the nodes of a hand-annotated parse tree, per-
node accuracy is usually reported in papers such as
(Pradhan et al, 2004). Unfortunately our approach
is based on a completely different premise: we tag
words, not syntactic constituents coming from the
parser. We discuss this further in Section 5.2.
The per-word accuracy comparison results can be
seen in Table 5. Before labeling the semantic roles
of each predicate, one must first identify the pred-
icates themselves. If a predicate is not identified,
NULL tags are assigned to each word for that pred-
icate. The first line of results in the table takes into
account this identification process. For the neural
network, we used our part-of-speech tagger to per-
form this as a verb-detection task.
We noticed ASSERT failed to identify relatively
many predicates. In particular, it seems predicates
such as ?is? are sometimes labeled as AUX by
the part-of-speech tagger, and subsequently ignored.
We informed the authors of this, but we did not re-
ceive a response. To deal with this, we considered
the additional accuracy (second row in the table)
measured over only those sentences where the pred-
icate was identified by ASSERT.
Timing results The per-sentence compute time is
also given in Table 5, averaged over all sentences in
the test set. Our method is around 250 times faster
than ASSERT. It is not really feasible to run AS-
SERT for most applications.
Measurement NN ASSERT
Per-word accuracy
(all verbs) 83.64% 83.46%
Per-word accuracy
(ASSERT verbs) 84.09% 86.06%
Per-sentence
compute time (secs) 0.02 secs 5.08 secs
Table 1: Experimental comparison with ASSERT
5.1 Analysis of our MLP
While we gave an intuitive justification of the archi-
tecture choices of our model in Section 4, we now
give a systematic empirical study of those choices.
First of all, providing the position of the word and
the predicate in function C(?, ?) is essential: the best
model we obtained with a window around the word
only gave 51.3%, assuming correct identification of
all predicates. Our best model achieves 83.95% in
this setting.
If we do not cluster the words according to their
part-of-speech, we also lose some performance, ob-
taining 78.6% at best. On the other hand, clustering
all words (such as CC, DT, IN part-of-speech tags)
also gives weaker results (81.1% accuracy at best).
We believe that including all words would give very
good performance if the dataset was large enough,
but training only on PropBank leads to overfitting,
many words being infrequent. Clustering is a way
to fight against overfitting, by grouping infrequent
words: for example, words with the label NNP, JJ,
RB (which we cluster) appear on average 23, 22 and
72 times respectively in the training set, while CC,
DT, IN (which we do not cluster) appear 2420, 5659
and 1712 times respectively.
565
Even though some verbs are infrequent, one can-
not cluster all verbs into a single group, as each verb
dictates the types of semantic roles in the sentence,
depending on its frame. Clustering all words into
their part-of-speech, including the predicate, gives
a poor 73.8% compared with 81.1%, where every-
thing is clustered apart from the predicate.
Figure 3 gives some anecdotal examples of test set
predictions of our final model compared to ASSERT.
5.2 Argument Classification Accuracy
So far we have not used the same accuracy measures
as in previous work (Gildea and Jurafsky, 2002;
Pradhan et al, 2004). Currently our architecture is
designed to label on a per-word basis, while existing
systems perform a segmentation process, and then
label segments. While we do not optimize our model
for the same criteria, it is still possible to measure the
accuracy using the same metrics. We measured the
argument classification accuracy of our network, as-
suming the correct segmentation is given to our sys-
tem, as in (Pradhan et al, 2004), by post-processing
our per-word tags to form a majority vote over each
segment. This gives 83.18% accuracy for our net-
work when we suppose the predicate must also be
identified, and 80.53% for the ASSERT software.
Measuring only on predicates identified by ASSERT
we instead obtain 84.32% accuracy for our network,
and 87.02% for ASSERT.
6 Discussion
We have introduced a neural network architecture
that can provide computationally efficient semantic
role tagging. It is also a general architecture that
could be applied to other problems as well. Because
our network currently outputs labels on a per-word
basis it is difficult to assess existing accuracy mea-
sures. However, it should be possible to combine
our approach with a shallow parser to enhance per-
formance, and make comparisons more direct.
We consider this work as a starting point for dif-
ferent research directions, including the following
areas:
? Incorporating hand-built features Currently,
the only prior knowledge our system encodes
comes from part-of-speech tags, in stark con-
trast to other methods. Of course, performance
TRUTH: He camped out at a high-tech nerve center
on the floor of [the Big Board, where]ARGM-LOC [he]ARG0
[could]ARGM-MOD [watch]REL [updates on prices and pend-
ing stock orders]ARG1.
ASSERT (68.7%): He camped out at a high-tech nerve
center on the floor of the Big Board, [ where]ARGM-LOC
[he]ARG0 [could]ARGM-MOD [watch]REL [updates]ARG1 on
prices and pending stock orders.
NN (100%): He camped out at a high-tech nerve center
on the floor of [the Big Board, where]ARGM-LOC [he]ARG0
[could]ARGM-MOD [watch]REL [updates on prices and pend-
ing stock orders]ARG1.
TRUTH: [United Auto Workers Local 1069, which]ARG0
[represents]REL [3,000 workers at Boeing?s helicopter unit
in Delaware County, Pa.]ARG1 , said it agreed to extend its
contract on a day-by-day basis, with a 10-day notification
to cancel, while it continues bargaining.
ASSERT (100%): [United Auto Workers Local 1069,
which]ARG0 [represents]REL [3,000 workers at Boeing?s
helicopter unit in Delaware County, Pa.]ARG1 , said it agreed
to extend its contract on a day-by-day basis, with a 10-day
notification to cancel, while it continues bargaining.
NN (89.1%): [United Auto Workers Local 1069,
which]ARG0 [represents]REL [3,000 workers at Boeing?s
helicopter unit]ARG1 [ in Delaware County]ARGM-LOC , Pa. ,
said it agreed to extend its contract on a day-by-day basis,
with a 10-day notification to cancel, while it continues
bargaining.
Figure 3: Two examples from the PropBank test set,
showing Neural Net and ASSERT and gold standard
labelings, with per-word accuracy in brackets. Note
that even though our labeling does not match the
hand-annotated one in the second sentence it still
seems to make some sense as ?in Delaware County?
is labeled as a location modifier. The complete set
of predictions on the test set can be found at http:
//ml.nec-labs.com/software/senna.
would improve with more hand-built features.
For example, simply adding whether each word
is part of a noun or verb phrase using the hand-
annotated parse tree (the so-called ?GOV? fea-
ture from (Gildea and Jurafsky, 2002)) im-
proves the performance of our system from
83.95% to 85.8%. One must trade the gener-
ality of the model with its specificity, and also
take into account how long the features take to
compute.
? Incorporating segment information Our system
has no prior knowledge about segmentation in
text. This could be encoded in many ways:
most obviously by using a chunker, but also by
566
designing a different network architecture, e.g.
by encoding contiguity constraints. To show
the latter is useful, using hand-annotated seg-
ments to force contiguity by majority vote leads
to an improvement from 83.95% to 85.6%.
? Incorporating known invariances via virtual
training data. In image recognition problems
it is common to create artificial training data by
taking into account invariances in the images,
e.g. via rotation and scale. Such data improves
generalization substantially. It may be possible
to achieve similar results for text, by ?warp-
ing? training data to create new sentences, or
by constructing sentences from scratch using a
hand-built grammar.
? Unlabeled data. Our representation of words
is as d dimensional vectors. We could try to
improve this representation by learning a lan-
guage model from unlabeled data (Bengio and
Ducharme, 2001). As many words in Prop-
Bank only appear a few times, the representa-
tion might improve, even though the learning is
unsupervised. This may also make the system
generalize better to types of data other than the
Wall Street Journal.
? Transductive Inference. Finally, one can also
use unlabeled data as part of the supervised
training process, which is called transduction
or semi-supervised learning.
In particular, we find the possibility of using un-
labeled data, invariances and the use of transduc-
tion exciting. These possibilities naturally fit into
our framework, whereas scalability issues will limit
their application in competing methods.
References
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The
Berkeley FrameNet project. Proceedings of COLING-
ACL, 98.
Y. Bengio and R. Ducharme. 2001. A neural probabilis-
tic language model. In Advances in Neural Informa-
tion Processing Systems, NIPS 13.
B.E. Boser, I.M. Guyon, and V.N. Vapnik. 1992. A train-
ing algorithm for optimal margin classifiers. Proceed-
ings of the fifth annual workshop on Computational
learning theory, pages 144?152.
J.S. Bridle. 1990. Probabilistic interpretation of feed-
forward classification network outputs, with relation-
ships to statistical pattern recognition. In F. Fogelman
Soulie? and J. He?rault, editors, Neurocomputing: Al-
gorithms, Architectures and Applications, pages 227?
236. NATO ASI Series.
E. Brill. 1992. A simple rule-based part of speech tag-
ger. Proceedings of the Third Conference on Applied
Natural Language Processing, pages 152?155.
E. Charniak. 2000. A maximum-entropy-inspired parser.
Proceedings of the first conference on North American
chapter of the Association for Computational Linguis-
tics, pages 132?139.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
D. Gildea and M. Palmer. 2001. The necessity of pars-
ing for predicate argument recognition. Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 239?246.
J. Henderson. 2004. Discriminative training of a neural
network statistical parser. In Proceedings of the 42nd
Meeting of Association for Computational Linguistics.
Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Mu?ller. 1998.
Efficient backprop. In G.B. Orr and K.-R. Mu?ller, ed-
itors, Neural Networks: Tricks of the Trade, pages 9?
50. Springer.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the penn treebank. Computational Linguistics,
19(2):313?330.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Comput. Linguist., 31(1):71?106.
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Ju-
rafsky. 2004. Shallow semantic parsing using support
vector machines. Proceedings of HLT/NAACL-2004.
V. Punyakanok, D. Roth, and W. Yih. 2005. The ne-
cessity of syntactic parsing for semantic role labeling.
Proceedings of IJCAI?05, pages 1117?1123.
A. Ratnaparkhi. 1997. A linear observed time statistical
parser based on maximum entropy models. Proceed-
ings of EMNLP.
H. Schwenk and J.L. Gauvain. 2002. Connection-
ist language modeling for large vocabulary continu-
ousspeech recognition. Proceedings of ICASSP?02.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10.
567
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1366?1371,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Connecting Language and Knowledge Bases with Embedding Models
for Relation Extraction
Jason Weston
Google
111 8th avenue
New York, NY, USA
jweston@google.com
Antoine Bordes
Heudiasyc
UT de Compi?gne
& CNRS
Compi?gne, France
bordesan@utc.fr
Oksana Yakhnenko
Google
111 8th avenue
New York, NY, USA
oksana@google.com
Nicolas Usunier
Heudiasyc
UT de Compi?gne
& CNRS
Compi?gne, France
nusunier@utc.fr
Abstract
This paper proposes a novel approach for rela-
tion extraction from free text which is trained
to jointly use information from the text and
from existing knowledge. Our model is based
on scoring functions that operate by learning
low-dimensional embeddings of words, enti-
ties and relationships from a knowledge base.
We empirically show on New York Times ar-
ticles aligned with Freebase relations that our
approach is able to efficiently use the extra in-
formation provided by a large subset of Free-
base data (4M entities, 23k relationships) to
improve over methods that rely on text fea-
tures alone.
1 Introduction
Information extraction (IE) aims at generating struc-
tured data from free text in order to populate Knowl-
edge Bases (KBs). Hence, one is given an incom-
plete KB composed of a set of triples of the form
(h, r , t); h is the left-hand side entity (or head), t
the right-hand side entity (or tail) and r the relation-
ship linking them. An example from the Freebase
KB1 is (/m/2d3rf ,<director-of>, /m/3/324), where
/m/2d3rf refers to the director ?Alfred Hitchcock"
and /m/3/324 to the movie ?The Birds".
This paper focuses on the problem of learning to
perform relation extraction (RE) under weak super-
vision from a KB. RE is sub-task of IE that consid-
ers that entities have already been detected by a dif-
ferent process, such as a named-entity recognizer.
RE then aims at assigning to a relation mention m
1www.freebase.com
(i.e. a sequence of text which states that some rela-
tion is true) the corresponding relationship from the
KB, given a pair of extracted entities (h, t) as con-
text. For example, given the triple (/m/2d3rf ,?wrote
and directed", /m/3/324), a system should predict
<director-of>. The task is said to be weakly super-
vised because for each pair of entities (h, t) detected
in the text, all relation mentions m associated with
them are labeled with all the relationships connect-
ing h and t in the KB, whether they are actually ex-
pressed by m or not.
Our key contribution is a novel model that em-
ploys not only weakly labeled text mention data, as
most approaches do, but also leverages triples from
the known KB. The model thus learns the plausi-
bility of new (h, r , t) triples by generalizing from
the KB, even though this triple is not present. A
ranking-based embedding framework is used to train
our model. Thereby, relation mentions, entities and
relationships are all embedded into a common low-
dimensional vector space, where scores are com-
puted. We show that our method can successfully
take into account information from a large-scale KB
(Freebase: 4M entities, 23k relationships) to im-
prove over systems that are only using text features.
This paper is organized as follows: Section 2
presents related work, Section 3 introduces our
model and its main influences, and experimental re-
sults are displayed in Section 4.
2 Previous Work
Learning under weak supervision is common in nat-
ural language processing, especially for tasks where
the annotation costs are significant such as in se-
1366
mantic parsing (Kate and Mooney, 2007; Liang et
al., 2009; Bordes et al, 2010; Matuszek et al,
2012). This is also naturally used in IE, since it
allows to train large-scale systems without requir-
ing to label numerous texts. The idea was intro-
duced by (Craven et al, 1999), which matched the
Yeast Protein Database with PubMed abstracts. It
was also used to train open extractors based on
Wikipedia infoboxes and corresponding sentences
(Wu and Weld, 2007; Wu and Weld, 2010). Large-
scale open IE projects (e.g. (Banko et al, 2007))
also rely on weak supervision, since they learn mod-
els from a seed KB in order to extend it.
Weak supervision is also a popular option for RE:
Mintz et al (2009) used Freebase to train weakly su-
pervised relational extractors on Wikipedia, an ap-
proach generalized by the multi-instance learning
frameworks (Riedel et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012). All these works only
use textual information to perform extraction.
Lao et al (2012) proposed the first work aiming to
perform RE employing both KB data and text, using
a rule-based random walk method. Recently, Riedel
et al (2013) proposed another joint approach based
on collaborative filtering for learning entity embed-
dings. This approach connects text with Freebase
by learning shared embeddings of entities through
weak supervision, in contrast to our method where
no joint learning is performed. We do not compare
to these two approaches since they use two different
evaluation protocols that greatly differ from those
used in all aforementioned previous works. Never-
theless, our method is easier to integrate into exist-
ing systems than those, since KB data is used via
the addition of a scoring term, which is trained sepa-
rately beforehand (with no shared embeddings). Be-
sides, we demonstrate in our experimental section
that our system can handle a large number of rela-
tionships, significantly larger than that presented in
(Lao et al, 2012; Riedel et al, 2013).
3 Embedding-based Framework
Our work concerns energy-based methods, which
learn low-dimensional vector representations (em-
beddings) of atomic symbols (words, entities, re-
lationships, etc.). In this framework, we learn two
models: one for predicting relationships given re-
lation mentions and another one to encode the in-
teractions among entities and relationships from the
KB. The joint action of both models in prediction
allows us to use the connection between the KB and
text to perform relation extraction. One could also
share parameters between models (via shared em-
beddings), but this is not implemented in this work.
This approach is inspired by previous work designed
to connect words and Wordnet (Bordes et al, 2012).
Both submodels end up learning vector embed-
dings of symbols, either for entities or relationships
in the KB, or for each word/feature of the vocabulary
(denoted V). The set of entities and relationships in
the KB are denoted by E and R, and nv , ne and nr
denote the size of V , E and R respectively. Given
a triple (h, r , t) the embeddings of the entities and
the relationship (vectors in Rk ) are denoted with the
same letter, in boldface characters (i.e. h, r, t).
3.1 Connecting Text and Relationships
The first part of the framework concerns the learn-
ing of a function Sm2r (m, r), based on embeddings,
that is designed to score the similarity of a relation
mention m and a relationship r .
Our scoring approach is inspired by previous
work for connecting word labels and images (We-
ston et al, 2010), which we adapted, replacing im-
ages by mentions and word labels by relationships.
Intuitively, it consists of first projecting words and
features into the embedding space and then comput-
ing a similarity measure (the dot product in this pa-
per) between this projection and a relationship em-
bedding. The scoring function is then:
Sm2r (m, r) = f(m)>r
with f a function mapping words and features into
Rk , f(m) = W>?(m). W is the matrix of Rnv?k
containing all word embeddings w, ?(m) is the
(sparse) binary representation of m (? Rnv ) indi-
cating absence or presence of words/features, and
r ? Rk is the embedding of the relationship r .
This approach can be easily applied at test time to
score (mention, relationship) pairs. Since this type
of learning problem is weakly supervised, Bordes et
al. (2010) showed that a convenient way to train it
is by using a ranking loss. Hence, given a data set
D = {(mi , ri ), i = 1, ... , |D|} consisting of (men-
tion, relationship) training pairs, one could learn the
1367
embeddings using constraints of the form:
?i , ?r ? 6= ri , f(mi )>ri > 1 + f(mi )>r? , (1)
where 1 is the margin. That is, we want the re-
lation that (weakly) labels a given mention to be
scored higher than other relation by a margin of 1.
Then, given any mention m one can predict the cor-
responding relationship r?(m) with:
r?(m) = arg max
r ??R
Sm2r (m, r ?) = arg max
r ??R
(
f(m)>r?
)
.
Learning Sm2r (?) under constraints (1) is well
suited when one is interested in building a per-
mention prediction system. However, performance
metrics of relation extraction are sometimes mea-
sured using precision recall curves aggregated for
all mentions concerning the same pair of entities,
as in (Riedel et al, 2010). In that case the scores
across predictions for different mentions need to be
calibrated so that the most confident ones have the
higher scores. This can be better encoded with con-
straints of the following form:
?i , j , ?r ? 6= ri , rj , f(mi )>ri > 1 + f(mj)>r? .
In this setup, scores of pairs observed in the training
set should be larger than that of any other prediction
across all mentions. In practice, we use ?soft? rank-
ing constraints (optimizing the hinge loss), i.e. we
minimize:
?i , j , ?r ? 6= ri , rj , max(0, 1?f(mi )>ri +f(mj)>r?).
Finally, we also enforce a (hard) constraint on the
norms of the columns of W and r, i.e. ?i , ||Wi ||2 ?
1 and ?j , ||rj ||2 ? 1. Training is carried out by
Stochastic Gradient Descent (SGD), updating W
and r at each step, following (Weston et al, 2010;
Bordes et al, 2013). That is, at the start of training
the parameters to be learnt (the nv ? k word/feature
embeddings in W and the nr ? k relation embed-
dings r ) are initialized to random weights. We ini-
tialize each k-dimensional embedding vector ran-
domly with mean 0, standard deviation 1k . Then, we
iterate the following steps to train them:
1. Select at random a positive training pair
(mi , ri ).
2. Select at random a secondary training pair
(mj , rj), used to calibrate the scores.
3. Select at random a negative relation r ? such that
r ? 6= ri and r ? 6= rj .
4. Make a stochastic gradient step to minimize
max(0, 1? f(mi )>ri + f(mj)>r?).
5. Enforce the constraint that each embedding
vector is normalized, i.e. if ||Wi ||2 > 1 then
Wi ?Wi/||Wi ||2.
3.2 Encoding Structured Data of KBs
Using only weakly labeled text mentions for train-
ing ignores much of the prior knowledge we can
leverage from a large KB such as Freebase. In or-
der to connect this relational data with our model,
we propose to encode its information into entity and
relationship embeddings. This allows us to build a
model which can score the plausibility of new en-
tity relationship triples which are missing from Free-
base. Several models have been recently developed
for that purpose (e.g. in (Nickel et al, 2011; Bor-
des et al, 2011; Bordes et al, 2012)): we chose in
this work to follow the approach of (Bordes et al,
2013), which is simple, flexible and has shown very
promising results on Freebase data.
Given a training set S = {(hi , ri , ti ), i =
1, ... , |S|} of relations extracted from the KB, this
model learns vector embeddings of the entities and
of the relationships using the idea that the func-
tional relation induced by the r -labeled arcs of the
KB should correspond to a translation of the em-
beddings. That is, given a k-dimensional embed-
ding of the left-hand side (head) entity, adding the
k-dimensional embedding of a given relation should
yield the point (or close to the point) of the k-
dimensional embedding of the right-hand side (tail)
entity. Hence, this method enforces that h + r ? t
when (h, r , t) holds, while h + r should be far away
from t otherwise. The model thus gives the follow-
ing score for the plausibility of a relation:
Skb(h, r , t) = ?||h + r ? t||22 .
A ranking loss is also used for training Skb. The
ranking objective is designed to assign higher scores
1368
to existing relations versus any other possibility:
?i ,?h? 6= hi , Skb(hi , ri , ti ) ? 1 + Skb(h?, ri , ti ),
?i ,?r ? 6= ri , Skb(hi , ri , ti ) ? 1 + Skb(hi , r ?, ti ),
?i ,?t ? 6= ti , Skb(hi , ri , ti ) ? 1 + Skb(hi , ri , t ?).
That is, for each known triple (h, r , t), if we re-
placed the (i) head, (ii) relation or (iii) tail with some
other possibility, the modified triple should have a
lower score (i.e. be less plausible) than the original
triple. The three sets of constraints defined above
encode the three types of modification. As in Sec-
tion 3.1 we use soft constraints via the hinge loss,
enforce constraints on the norm of embeddings, i.e.
?h,r ,t , ||h||2 ? 1, ||r ||2 ? 1, ||t||2 ? 1, and training
is performed using SGD, as in (Bordes et al, 2013).
At test time, one may again need to calibrate the
scores Skb across entity pairs. We propose a sim-
ple approach: we convert the scores by ranking all
relationshipsR by Skb and instead output:
S?kb(h, r , t)=?
(?
r ? 6=r
?(Skb(h, r , t)>Skb(h, r
?, t))
)
,
i.e. a function of the rank of r . We chose the simpli-
fied model ?(x) = 1 if x < ? and 0 otherwise; ?(?)
is the Kronecker function.
3.3 Implementation for Relation Extraction
Our framework can be used for relation extraction
in the following way. First, for each pair of entities
(h, t) that appear in the test set, all the correspond-
ing mentionsMh,t in the test set are collected and a
prediction is performed with:
r?h,t = argmax
r?R
?
m?Mh,t
Sm2r (m, r) .
The predicted relationship can either be a valid re-
lationship or NA ? a marker that means that there is
no relation between h and t (NA is added to R dur-
ing training and is treated like other relationships).
If r?h,t is a relationship, a composite score is defined:
Sm2r+kb(h, r?h,t , t)=
?
m?Mh,t
Sm2r (m, r?h,t)+S?kb(h, r?h,t , t)
That is, only the top scoring non-NA predictions are
re-scored. Hence, our final composite model favors
predictions that agree with both the mentions and the
KB. If r?h,t is NA, the score is unchanged.
4 Experiments
We use the training and test data, evaluation frame-
work and baselines from (Riedel et al, 2010; Hoff-
mann et al, 2011; Surdeanu et al, 2012).
NYT+FB This dataset, developed by (Riedel et
al., 2010), aligns Freebase relations with the New
York Times corpus. Entities were found using the
Stanford named entity tagger (Finkel et al, 2005),
and were matched to their name in Freebase. For
each mention, sentence level features are extracted
which include part of speech, named entity and de-
pendency tree path properties. Unlike some of the
previous methods, we do not use features that aggre-
gate properties across multiple mentions. We kept
the 100,000 most frequent features.There are 52 pos-
sible relationships and 121,034 training mentions of
which most are labeled as no relation (labeled ?NA?)
? there are 4700 Freebase relations mentioned in the
training set, and 1950 in the test set.
Freebase Freebase is a large-scale KB that has
around 80M entities, 23k relationships and 1.2B re-
lations. We used a subset restricted to the top 4M
entities for scalability reasons ? where top is defined
as the ones with the largest number of relations to
other entities. We used all the 23k possible relation-
ships in Freebase. To make a realistic setting, we
did not choose the entity set using the NYT+FB data
set, so it may not overlap completely. For that rea-
son, we needed to keep the set rather large. Keeping
the top 4M entities gives an overlap of 80% with the
entities in the NYT+FB test set. Most importantly,
we then removed all the entity pairs present in the
NYT+FB test set from Freebase, i.e. all relations
they are involved in independent of the relationship.
This ensures that we cannot just memorize the true
relations for an entity pair ? we have to learn to gen-
eralize from other entities and relations.
As the NYT+FB dataset was built on an earlier
version of Freebase we also had to translate the dep-
recated relationships into their new variants (e.g.
?/p/business/company/place_founded ? ? ?/orga-
nization/organization/place_founded?) to make the
two datasets link (then, the 52 relationships in
NYT+FB are now a subset of the 23k from Free-
base). We then trained the Skb model on the remain-
ing triples.
1369
recall
prec
ision
0 0.1 0.20
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Wsabie M2R+FBMIMLREHoffmannWsabie M2RRiedelMintz
recall
prec
ision
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10.4
0.5
0.6
0.7
0.8
0.9 Wsabie M2R+FBMIMLREHoffmannWsabie M2RRiedelMintz
Figure 1: Top: Aggregate extraction precision/recall
curves for a variety of methods. Bottom: the
same plot zoomed to the recall [0-0.1] region.
WsabieM2R is our method trained only on mentions,
WsabieM2R+FB uses Freebase annotations as well.
Modeling Following (Bordes et al, 2013) we set
the embedding dimension k to 50. The learning rate
for SGD was selected using a validation set: we ob-
tained 0.001 for Sm2r , and 0.1 for Skb. For the cal-
ibration of S?kb, ? = 10 (note, here we are ranking
all 23k Freebase relationships). Training Sm2r took
5 minutes, whilst training Skb took 2 days due to the
large scale of the data set.
Results Figure 1 displays the aggregate precision
/ recall curves of our approach WSABIEM2R+FB
which uses the combination of Sm2r + Skb, as well
as WSABIEM2R , which only uses Sm2r , and existing
state-of-the-art approaches: HOFFMANN (Hoffmann
et al, 2011)2, MIMLRE (Surdeanu et al, 2012).
RIEDEL (Riedel et al, 2010) and MINTZ (Mintz et
al., 2009).
WSABIEM2R is comparable to, but slightly worse
than, the MIMLRE and HOFFMANN methods, possi-
bly due to its simplified assumptions (e.g. predict-
ing a single relationship per entity pair). However,
the addition of extra knowledge from other Freebase
entities in WSABIEM2R+FB provides superior per-
formance to all other methods, by a wide margin, at
least between 0 and 0.1 recall (see bottom plot).
Performance of WSABIEM2R and
WSABIEM2R+FB for recall > 0.1 degrades rapidly,
faster than that of other methods. This is also
caused by the simplifications of WSABIEM2R that
prevent it from reaching high precision when the
recall is greater than 0.1. We recall that Freebase
data is not used to detect relationships i.e. to
discriminate between NA and the rest, but only to
select the best relationship in case of detection.
That is WSABIEM2R+FB only improves precision,
not recall, so both versions of Wsabie are similar
w.r.t. recall. This could be improved by borrowing
ideas from HOFFMANN (Hoffmann et al, 2011) or
MIMLRE (Surdeanu et al, 2012) for dealing with
the multi-label case. Our approach, which uses
Freebase to increase precision, is general and could
improve any other method.
5 Conclusion
In this paper we described a framework for leverag-
ing large scale knowledge bases to improve relation
extraction by training not only on (mention, relation-
ship) pairs but using all other KB triples as well. We
empirically showed that it allows to significantly im-
prove precision on extracted relations. Our model-
ing approach is general and should apply to other
settings, e.g. for the task of entity linking.
Acknowledgments
This work was carried out in the framework of
the Labex MS2T (ANR-11-IDEX-0004-02), and
funded by the French National Agency for Research
(EVEREST-12-JS02-005-01).
2There is an error in the plot from (Hoffmann et al, 2011),
which we have corrected. The authors acknowledged this issue.
1370
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In IJCAI, vol-
ume 7, pages 2670?2676.
Antoine Bordes, Nicolas Usunier, and Jason Weston.
2010. Label ranking under ambiguous supervision for
learning semantic correspondences. In Proceedings of
the 27th International Conference on Machine Learn-
ing (ICML-10), pages 103?110.
Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Proc. of the 25th Conf.
on Artif. Intel. (AAAI).
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words and
meaning representations for open-text semantic pars-
ing. In Proc. of the 15th Intern. Conf. on Artif. Intel.
and Stat., volume 22, pages 127?135. JMLR.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,
Jason Weston, and Oksana Yakhnenko. 2013. Trans-
lating embeddings for modeling multi-relational data.
In Advances in Neural Information Processing Sys-
tems (NIPS 26).
Mark Craven, Johan Kumlien, et al 1999. Constructing
biological knowledge bases by extracting information
from text sources. In ISMB, volume 1999, pages 77?
86.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363?370.
Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, vol-
ume 1, pages 541?550.
Rohit J Kate and Raymond J Mooney. 2007. Learning
language semantics from ambiguous supervision. In
AAAI, volume 7, pages 895?900.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1017?
1026. Association for Computational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1-Volume 1, pages 91?99.
Association for Computational Linguistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume 2,
pages 1003?1011. Association for Computational Lin-
guistics.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings of
the 28th International Conference on Machine Learn-
ing (ICML-11), pages 809?816.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowledge
Discovery in Databases, pages 148?163. Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT, pages 74?84.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 455?465. Associa-
tion for Computational Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier. 2010.
Large scale image annotation: learning to rank with
joint word-image embeddings. Machine learning,
81(1):21?35.
Fei Wu and Daniel S Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 41?50. ACM.
Fei Wu and Daniel S Weld. 2010. Open information ex-
traction using wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 118?127. Association for Computa-
tional Linguistics.
1371
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615?620,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Question Answering with Subgraph Embeddings
Antoine Bordes
Facebook AI Research
112 avenue de Wagram,
Paris, France
abordes@fb.com
Sumit Chopra
Facebook AI Research
770 Broadway,
New York, USA
spchopra@fb.com
Jason Weston
Facebook AI Research
770 Broadway,
New York, USA
jase@fb.com
Abstract
This paper presents a system which learns
to answer questions on a broad range of
topics from a knowledge base using few
hand-crafted features. Our model learns
low-dimensional embeddings of words
and knowledge base constituents; these
representations are used to score natural
language questions against candidate an-
swers. Training our system using pairs of
questions and structured representations of
their answers, and pairs of question para-
phrases, yields competitive results on a re-
cent benchmark of the literature.
1 Introduction
Teaching machines how to automatically answer
questions asked in natural language on any topic
or in any domain has always been a long stand-
ing goal in Artificial Intelligence. With the rise
of large scale structured knowledge bases (KBs),
this problem, known as open-domain question an-
swering (or open QA), boils down to being able
to query efficiently such databases with natural
language. These KBs, such as FREEBASE (Bol-
lacker et al., 2008) encompass huge ever growing
amounts of information and ease open QA by or-
ganizing a great variety of answers in a structured
format. However, the scale and the difficulty for
machines to interpret natural language still makes
this task a challenging problem.
The state-of-the-art techniques in open QA can
be classified into two main classes, namely, infor-
mation retrieval based and semantic parsing based.
Information retrieval systems first retrieve a broad
set of candidate answers by querying the search
API of KBs with a transformation of the ques-
tion into a valid query and then use fine-grained
detection heuristics to identify the exact answer
(Kolomiyets and Moens, 2011; Unger et al., 2012;
Yao and Van Durme, 2014). On the other hand,
semantic parsing methods focus on the correct in-
terpretation of the meaning of a question by a se-
mantic parsing system. A correct interpretation
converts a question into the exact database query
that returns the correct answer. Interestingly, re-
cent works (Berant et al., 2013; Kwiatkowski et
al., 2013; Berant and Liang, 2014; Fader et al.,
2014) have shown that such systems can be ef-
ficiently trained under indirect and imperfect su-
pervision and hence scale to large-scale regimes,
while bypassing most of the annotation costs.
Yet, even if both kinds of system have shown the
ability to handle large-scale KBs, they still require
experts to hand-craft lexicons, grammars, and KB
schema to be effective. This non-negligible hu-
man intervention might not be generic enough to
conveniently scale up to new databases with other
schema, broader vocabularies or languages other
than English. In contrast, (Fader et al., 2013) pro-
posed a framework for open QA requiring almost
no human annotation. Despite being an interesting
approach, this method is outperformed by other
competing methods. (Bordes et al., 2014b) in-
troduced an embedding model, which learns low-
dimensional vector representations of words and
symbols (such as KBs constituents) and can be
trained with even less supervision than the system
of (Fader et al., 2013) while being able to achieve
better prediction performance. However, this ap-
proach is only compared with (Fader et al., 2013)
which operates in a simplified setting and has not
been applied in more realistic conditions nor eval-
uated against the best performing methods.
In this paper, we improve the model of (Bor-
des et al., 2014b) by providing the ability to an-
swer more complicated questions. The main con-
tributions of the paper are: (1) a more sophisti-
cated inference procedure that is both efficient and
can consider longer paths ((Bordes et al., 2014b)
considered only answers directly connected to the
615
question in the graph); and (2) a richer represen-
tation of the answers which encodes the question-
answer path and surrounding subgraph of the KB.
Our approach is competitive with the current state-
of-the-art on the recent benchmark WEBQUES-
TIONS (Berant et al., 2013) without using any lex-
icon, rules or additional system for part-of-speech
tagging, syntactic or dependency parsing during
training as most other systems do.
2 Task Definition
Our main motivation is to provide a system for
open QA able to be trained as long as it has ac-
cess to: (1) a training set of questions paired with
answers and (2) a KB providing a structure among
answers. We suppose that all potential answers are
entities in the KB and that questions are sequences
of words that include one identified KB entity.
When this entity is not given, plain string match-
ing is used to perform entity resolution. Smarter
methods could be used but this is not our focus.
We use WEBQUESTIONS (Berant et al., 2013)
as our evaluation bemchmark. Since it contains
few training samples, it is impossible to learn on
it alone, and this section describes the various data
sources that were used for training. These are sim-
ilar to those used in (Berant and Liang, 2014).
WebQuestions This dataset is built using FREE-
BASE as the KB and contains 5,810 question-
answer pairs. It was created by crawling questions
through the Google Suggest API, and then obtain-
ing answers using Amazon Mechanical Turk. We
used the original split (3,778 examples for train-
ing and 2,032 for testing), and isolated 1k ques-
tions from the training set for validation. WE-
BQUESTIONS is built on FREEBASE since all an-
swers are defined as FREEBASE entities. In each
question, we identified one FREEBASE entity us-
ing string matching between words of the ques-
tion and entity names in FREEBASE. When the
same string matches multiple entities, only the en-
tity appearing in most triples, i.e. the most popular
in FREEBASE, was kept. Example questions (an-
swers) in the dataset include ?Where did Edgar
Allan Poe died?? (baltimore) or ?What degrees
did Barack Obama get?? (bachelor of arts,
juris doctor).
Freebase FREEBASE (Bollacker et al., 2008)
is a huge and freely available database of
general facts; data is organized as triplets
(subject, type1.type2.predicate, object),
where two entities subject and object (identi-
fied by mids) are connected by the relation type
type1.type2.predicate. We used a subset, cre-
ated by only keeping triples where one of the
entities was appearing in either the WEBQUES-
TIONS training/validation set or in CLUEWEB ex-
tractions. We also removed all entities appearing
less than 5 times and finally obtained a FREEBASE
set containing 14M triples made of 2.2M entities
and 7k relation types.
1
Since the format of triples
does not correspond to any structure one could
find in language, we decided to transform them
into automatically generated questions. Hence, all
triples were converted into questions ?What is the
predicate of the type2 subject?? (using the
mid of the subject) with the answer being object.
An example is ?What is the nationality of the
person barack obama?? (united states). More
examples and details are given in a longer version
of this paper (Bordes et al., 2014a).
ClueWeb Extractions FREEBASE data allows
to train our model on 14M questions but these have
a fixed lexicon and vocabulary, which is not real-
istic. Following (Berant et al., 2013), we also cre-
ated questions using CLUEWEB extractions pro-
vided by (Lin et al., 2012). Using string match-
ing, we ended up with 2M extractions structured
as (subject, ?text string?, object) with both
subject and object linked to FREEBASE. We
also converted these triples into questions by using
simple patterns and FREEBASE types. An exam-
ple of generated question is ?Where barack obama
was allegedly bear in?? (hawaii).
Paraphrases The automatically generated ques-
tions that are useful to connect FREEBASE triples
and natural language, do not provide a satisfac-
tory modeling of natural language because of their
semi-automatic wording and rigid syntax. To
overcome this issue, we follow (Fader et al., 2013)
and supplement our training data with an indirect
supervision signal made of pairs of question para-
phrases collected from the WIKIANSWERS web-
site. On WIKIANSWERS, users can tag pairs of
questions as rephrasings of each other: (Fader et
al., 2013) harvested a set of 2M distinct questions
from WIKIANSWERS, which were grouped into
350k paraphrase clusters.
1
WEBQUESTIONS contains ?2k entities, hence restrict-
ing FREEBASE to 2.2M entities does not ease the task for us.
616
3 Embedding Questions and Answers
Inspired by (Bordes et al., 2014b), our model
works by learning low-dimensional vector embed-
dings of words appearing in questions and of enti-
ties and relation types of FREEBASE, so that repre-
sentations of questions and of their corresponding
answers are close to each other in the joint embed-
ding space. Let q denote a question and a a can-
didate answer. Learning embeddings is achieved
by learning a scoring function S(q, a), so that S
generates a high score if a is the correct answer to
the question q, and a low score otherwise. Note
that both q and a are represented as a combina-
tion of the embeddings of their individual words
and/or symbols; hence, learning S essentially in-
volves learning these embeddings. In our model,
the form of the scoring function is:
S(q, a) = f(q)
>
g(a). (1)
Let W be a matrix of R
k?N
, where k is the di-
mension of the embedding space which is fixed a-
priori, andN is the dictionary of embeddings to be
learned. LetN
W
denote the total number of words
and N
S
the total number of entities and relation
types. WithN = N
W
+N
S
, the i-th column ofW
is the embedding of the i-th element (word, entity
or relation type) in the dictionary. The function
f(.), which maps the questions into the embed-
ding spaceR
k
is defined as f(q) =W?(q), where
?(q) ? N
N
, is a sparse vector indicating the num-
ber of times each word appears in the question q
(usually 0 or 1). Likewise the function g(.) which
maps the answer into the same embedding space
R
k
as the questions, is given by g(a) = W?(a).
Here ?(a) ? N
N
is a sparse vector representation
of the answer a, which we now detail.
3.1 Representing Candidate Answers
We now describe possible feature representations
for a single candidate answer. (When there are
multiple correct answers, we average these rep-
resentations, see Section 3.4.) We consider three
different types of representation, corresponding to
different subgraphs of FREEBASE around it.
(i) Single Entity. The answer is represented as
a single entity from FREEBASE: ?(a) is a 1-
of-N
S
coded vector with 1 corresponding to
the entity of the answer, and 0 elsewhere.
(ii) Path Representation. The answer is
represented as a path from the entity
mentioned in the question to the answer
entity. In our experiments, we consid-
ered 1- or 2-hops paths (i.e. with either
1 or 2 edges to traverse): (barack obama,
people.person.place of birth, honolulu)
is a 1-hop path and (barack obama,
people.person.place of birth, location.
location.containedby, hawaii) a 2-hops
path. This results in a ?(a) which is a
3-of-N
S
or 4-of-N
S
coded vector, expressing
the start and end entities of the path and the
relation types (but not entities) in-between.
(iii) Subgraph Representation. We encode both
the path representation from (ii), and the en-
tire subgraph of entities connected to the can-
didate answer entity. That is, for each entity
connected to the answer we include both the
relation type and the entity itself in the repre-
sentation ?(a). In order to represent the an-
swer path differently to the surrounding sub-
graph (so the model can differentiate them),
we double the dictionary size for entities, and
use one embedding representation if they are
in the path and another if they are in the sub-
graph. Thus we now learn a parameter matrix
R
k?N
where N = N
W
+ 2N
S
(N
S
is the to-
tal number of entities and relation types). If
there areC connected entities withD relation
types to the candidate answer, its representa-
tion is a 3+C+D or 4+C+D-of-N
S
coded
vector, depending on the path length.
Our hypothesis is that including more informa-
tion about the answer in its representation will lead
to improved results. While it is possible that all
required information could be encoded in the k di-
mensional embedding of the single entity (i), it is
unclear what dimension k should be to make this
possible. For example the embedding of a country
entity encoding all of its citizens seems unrealis-
tic. Similarly, only having access to the path ig-
nores all the other information we have about the
answer entity, unless it is encoded in the embed-
dings of either the entity of the question, the an-
swer or the relations linking them, which might be
quite complicated as well. We thus adopt the sub-
graph approach. Figure 1 illustrates our model.
3.2 Training and Loss Function
As in (Weston et al., 2010), we train our model
using a margin-based ranking loss function. Let
D = {(q
i
, a
i
) : i = 1, . . . , |D|} be the training set
617
?Who did Clooney marry in 1987?? 
Embedding	 ?matrix	 ?W	 ?
G. Clooney K. Preston 
1987 
J. Travolta 
Model 
Honolulu 
Detec?on	 ?of	 ?Freebase	 ?en?ty	 ?in	 ?the	 ?ques?on	 ?
Embedding model 
Freebase subgraph 
Binary	 ?encoding	 ?of	 ?the	 ?subgraph	 ??(a)	 ?
Embedding	 ?of	 ?the	 ?subgraph	 ?g(a)	 ?
Binary	 ?encoding	 ?of	 ?the	 ?ques?on	 ??(q)	 ?
Embedding	 ?of	 ?the	 ?ques?n	 ? f(q)	 ?
Ques?n 	 ?q	 ?
Subgraph	 ?of	 ?a	 ?candidate	 ?answer	 ?a	 ?(here	 ?K.	 ?Preston)	 ?
Score S(q,a) How	 ?the	 ?candidate	 ?answer	 ?fits	 ?the	 ?ques?on	 ?
Dot	 ?product	 ? Embedding	 ?matrix	 ?W	 ?
Figure 1: Illustration of the subgraph embedding model scoring a candidate answer: (i) locate entity in
the question; (ii) compute path from entity to answer; (iii) represent answer as path plus all connected
entities to the answer (the subgraph); (iv) embed both the question and the answer subgraph separately
using the learnt embedding vectors, and score the match via their dot product.
of questions q
i
paired with their correct answer a
i
.
The loss function we minimize is
|D|
?
i=1
?
a??
?
A(a
i
)
max{0,m?S(q
i
, a
i
)+S(q
i
, a?)}, (2)
where m is the margin (fixed to 0.1). Minimizing
Eq. (2) learns the embedding matrix W so that
the score of a question paired with a correct an-
swer is greater than with any incorrect answer a?
by at least m. a? is sampled from a set of incor-
rect candidates
?
A. This is achieved by sampling
50% of the time from the set of entities connected
to the entity of the question (i.e. other candidate
paths), and by replacing the answer entity by a ran-
dom one otherwise. Optimization is accomplished
using stochastic gradient descent, multi-threaded
with Hogwild! (Recht et al., 2011), with the con-
straint that the columns w
i
of W remain within
the unit-ball, i.e., ?
i
, ||w
i
||
2
? 1.
3.3 Multitask Training of Embeddings
Since a large number of questions in our training
datasets are synthetically generated, they do not
adequately cover the range of syntax used in natu-
ral language. Hence, we also multi-task the train-
ing of our model with the task of paraphrase pre-
diction. We do so by alternating the training of
S with that of a scoring function S
prp
(q
1
, q
2
) =
f(q
1
)
>
f(q
2
), which uses the same embedding ma-
trix W and makes the embeddings of a pair of
questions (q
1
, q
2
) similar to each other if they are
paraphrases (i.e. if they belong to the same para-
phrase cluster), and make them different other-
wise. Training S
prp
is similar to that of S except
that negative samples are obtained by sampling a
question from another paraphrase cluster.
We also multitask the training of the embed-
dings with the mapping of the mids of FREEBASE
entities to the actual words of their names, so that
the model learns that the embedding of the mid of
an entity should be similar to the embedding of the
word(s) that compose its name(s).
3.4 Inference
OnceW is trained, at test time, for a given ques-
tion q the model predicts the answer with:
a? = argmax
a
?
?A(q)
S(q, a
?
) (3)
where A(q) is the candidate answer set. This can-
didate set could be the whole KB but this has both
speed and potentially precision issues. Instead, we
create a candidate set A(q) for each question.
We recall that each question contains one identi-
fied FREEBASE entity. A(q) is first populated with
all triples from FREEBASE involving this entity.
This allows to answer simple factual questions
whose answers are directly connected to them (i.e.
1-hop paths). This strategy is denoted C
1
.
Since a system able to answer only such ques-
tions would be limited, we supplement A(q) with
examples situated in the KB graph at 2-hops from
the entity of the question. We do not add all such
quadruplets since this would lead to very large
candidate sets. Instead, we consider the follow-
ing general approach: given that we are predicting
a path, we can predict its elements in turn using
618
Method P@1 F1 F1
(%) (Berant) (Yao)
Baselines
(Berant et al., 2013) ? 31.4 ?
(Bordes et al., 2014b) 31.3 29.7 31.8
(Yao and Van Durme, 2014) ? 33.0 42.0
(Berant and Liang, 2014) ? 39.9 43.0
Our approach
Subgraph & A(q) = C
2
40.4 39.2 43.2
Ensemble with (Berant & Liang, 14) ? 41.8 45.7
Variants
Without multiple predictions 40.4 31.3 34.2
Subgraph & A(q) = All 2-hops 38.0 37.1 41.4
Subgraph & A(q) = C
1
34.0 32.6 35.1
Path & A(q) = C
2
36.2 35.3 38.5
Single Entity & A(q) = C
1
25.8 16.0 17.8
Table 1: Results on the WEBQUESTIONS test set.
a beam search, and hence avoid scoring all can-
didates. Specifically, our model first ranks rela-
tion types using Eq. (1), i.e. selects which rela-
tion types are the most likely to be expressed in
q. We keep the top 10 types (10 was selected on
the validation set) and only add 2-hops candidates
to A(q) when these relations appear in their path.
Scores of 1-hop triples are weighted by 1.5 since
they have one less element than 2-hops quadru-
plets. This strategy, denotedC
2
, is used by default.
A prediction a
?
can commonly actually be
a set of candidate answers, not just one an-
swer, for example for questions like ?Who are
David Beckham?s children??. This is achieved
by considering a prediction to be all the en-
tities that lie on the same 1-hop or 2-hops
path from the entity found in the question.
Hence, all answers to the above question are
connected to david beckham via the same path
(david beckham, people.person.children,
*
).
The feature representation of the prediction is then
the average over each candidate entity?s features
(see Section 3.1), i.e. ?
all
(a
?
) =
1
|a
?
|
?
a
?
j
:a
?
?(a
?
j
)
where a
?
j
are the individual entities in the over-
all prediction a
?
. In the results, we compare to a
baseline method that can only predict single can-
didates, which understandly performs poorly.
4 Experiments
We compare our system in terms of F1 score as
computed by the official evaluation script
2
(F1
(Berant)) but also with a slightly different F1 def-
inition, termed F1 (Yao) which was used in (Yao
and Van Durme, 2014) (the difference being the
way that questions with no answers are dealt with),
2
Available from www-nlp.stanford.edu/software/sempre/
and precision @ 1 (p@1) of the first candidate en-
tity (even when there are a set of correct answers),
comparing to recently published systems.
3
The
upper part of Table 1 indicates that our approach
outperforms (Yao and Van Durme, 2014), (Berant
et al., 2013) and (Bordes et al., 2014b), and per-
forms similarly as (Berant and Liang, 2014).
The lower part of Table 1 compares various ver-
sions of our model. Our default approach uses
the Subgraph representation for answers and C
2
as the candidate answers set. Replacing C
2
by
C
1
induces a large drop in performance because
many questions do not have answers thatare di-
rectly connected to their inluded entity (not in
C
1
). However, using all 2-hops connections as
a candidate set is also detrimental, because the
larger number of candidates confuses (and slows
a lot) our ranking based inference. Our results
also verify our hypothesis of Section 3.1, that a
richer representation for answers (using the local
subgraph) can store more pertinent information.
Finally, we demonstrate that we greatly improve
upon the model of (Bordes et al., 2014b), which
actually corresponds to a setting with the Path rep-
resentation and C
1
as candidate set.
We also considered an ensemble of our ap-
proach and that of (Berant and Liang, 2014). As
we only had access to their test predictions we
used the following combination method. Our ap-
proach gives a score S(q, a) for the answer it pre-
dicts. We chose a threshold such that our approach
predicts 50% of the time (when S(q, a) is above
its value), and the other 50% of the time we use
the prediction of (Berant and Liang, 2014) instead.
We aimed for a 50/50 ratio because both meth-
ods perform similarly. The ensemble improves the
state-of-the-art, and indicates that our models are
significantly different in their design.
5 Conclusion
This paper presented an embedding model that
learns to perform open QA using training data
made of questions paired with their answers and
of a KB to provide a structure among answers, and
can achieve promising performance on the com-
petitive benchmark WEBQUESTIONS.
3
Results of baselines except (Bordes et al., 2014b) have
been extracted from the original papers. For our experiments,
all hyperparameters have been selected on the WEBQUES-
TIONS validation set: k was chosen among {64, 128, 256},
the learning rate on a log. scale between 10
?4
and 10
?1
and
we used at most 100 paths in the subgraph representation.
619
References
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (ACL?14), Baltimore, USA.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?13), Seattle, USA.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, Vancouver, Canada. ACM.
Antoine Bordes, Sumit Chopra, and Jason Weston.
2014a. Question answering with subgraph embed-
dings. CoRR, abs/1406.3676.
Antoine Bordes, Jason Weston, and Nicolas Usunier.
2014b. Open question answering with weakly su-
pervised embedding models. In Proceedings of the
7th European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery
in Databases (ECML-PKDD?14), Nancy, France.
Springer.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL?13), Sofia, Bulgaria.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In Proceedings of 20th
SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD?14), New York City, USA.
ACM.
Oleksandr Kolomiyets and Marie-Francine Moens.
2011. A survey on question answering technology
from an information retrieval perspective. Informa-
tion Sciences, 181(24):5412?5434.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP?13), Seattle, USA,
October.
Thomas Lin, Mausam, and Oren Etzioni. 2012. En-
tity linking at web scale. In Proceedings of the Joint
Workshop on Automatic Knowledge Base Construc-
tion and Web-scale Knowledge Extraction (AKBC-
WEKEX?12), Montreal, Canada.
Benjamin Recht, Christopher R?e, Stephen J Wright,
and Feng Niu. 2011. Hogwild!: A lock-free ap-
proach to parallelizing stochastic gradient descent.
In Advances in Neural Information Processing Sys-
tems (NIPS 24)., Vancouver, Canada.
Christina Unger, Lorenz B?uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over RDF data. In Proceedings of the
21st international conference on World Wide Web
(WWW?12), Lyon, France. ACM.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2010. Large scale image annotation: learning to
rank with joint word-image embeddings. Machine
learning, 81(1).
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?14), Baltimore, USA.
620
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1822?1827,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
#TAGSPACE: Semantic Embeddings from Hashtags
Jason Weston
Facebook AI Research
jase@fb.com
Sumit Chopra
Facebook AI Research
spchopra@fb.com
Keith Adams
Facebook AI Research
kma@fb.com
Abstract
We describe a convolutional neural net-
work that learns feature representations for
short textual posts using hashtags as a su-
pervised signal. The proposed approach is
trained on up to 5.5 billion words predict-
ing 100,000 possible hashtags. As well as
strong performance on the hashtag predic-
tion task itself, we show that its learned
representation of text (ignoring the hash-
tag labels) is useful for other tasks as well.
To that end, we present results on a docu-
ment recommendation task, where it also
outperforms a number of baselines.
1 Introduction
Hashtags (single tokens often composed of nat-
ural language n-grams or abbreviations, prefixed
with the character ?#?) are ubiquitous on social
networking services, particularly in short textual
documents (a.k.a. posts). Authors use hashtags to
diverse ends, many of which can be seen as labels
for classical NLP tasks: disambiguation (chips
#futurism vs. chips #junkfood); identi-
fication of named entities (#sf49ers); sentiment
(#dislike); and topic annotation (#yoga).
Hashtag prediction is the task of mapping text to
its accompanying hashtags. In this work we pro-
pose a novel model for hashtag prediction, and
show that this task is also a useful surrogate for
learning good representations of text.
Latent representations, or embeddings, are vec-
torial representations of words or documents, tra-
ditionally learned in an unsupervised manner over
large corpora. For example LSA (Deerwester et
al., 1990) and its variants, and more recent neural-
network inspired methods like those of Bengio et
al. (2006), Collobert et al. (2011) and word2vec
(Mikolov et al., 2013) learn word embeddings. In
the word embedding paradigm, each word is rep-
resented as a vector in R
n
, where n is a hyper-
parameter that controls capacity. The embeddings
of words comprising a text are combined using a
model-dependent, possibly learned function, pro-
ducing a point in the same embedding space. A
similarity measure (for example, inner product)
gauges the pairwise relevance of points in the em-
bedding space.
Unsupervised word embedding methods train
with a reconstruction objective in which the em-
beddings are used to predict the original text. For
example, word2vec tries to predict all the words
in the document, given the embeddings of sur-
rounding words. We argue that hashtag predic-
tion provides a more direct form of supervision:
the tags are a labeling by the author of the salient
aspects of the text. Hence, predicting them may
provide stronger semantic guidance than unsuper-
vised learning alone. The abundance of hashtags
in real posts provides a huge labeled dataset for
learning potentially sophisticated models.
In this work we develop a convolutional net-
work for large scale ranking tasks, and apply it
to hashtag prediction. Our model represents both
words and the entire textual post as embeddings as
intermediate steps. We show that our method out-
performs existing unsupervised (word2vec) and
supervised (WSABIE (Weston et al., 2011)) em-
bedding methods, and other baselines, at the hash-
tag prediction task.
We then probe our model?s generality, by trans-
fering its learned representations to the task of per-
sonalized document recommendation: for each of
M users, given N previous positive interactions
with documents (likes, clicks, etc.), predict the
N + 1?th document the user will positively inter-
act with. To perform well on this task, the rep-
resentation should capture the user?s interest in
textual content. We find representations trained
on hashtag prediction outperform representations
from unsupervised learning, and that our convolu-
1822
w1
w2
N ? d
word
lookup 
table
convolution
layer
tanh
layer
max
pooling
tanh
layer
linear
layer
wl
f(w, t)
hashtag
lookup 
table 
t
(l + K   1)? d
l ?H l ?H
H H
d
d
Figure 1: #TAGSPACE convolutional network f(w, t) for scoring a (document, hashtag) pair.
tional architecture performs better than WSABIE
trained on the same hashtag task.
2 Prior Work
Some previous work (Davidov et al., 2010; Godin
et al., 2013; She and Chen, 2014) has addressed
hashtag prediction. Most such work applies to
much smaller sets of hashtags than the 100,000 we
consider, with the notable exception of Ding et al.
(2012), which uses an unsupervised method.
As mentioned in Section 1, many approaches
learn unsupervised word embeddings. In our
experiments we use word2vec (Mikolov et al.,
2013) as a representative scalable model for un-
supervised embeddings. WSABIE (Weston et al.,
2011) is a supervised embedding approach that has
shown promise in NLP tasks (Weston et al., 2013;
Hermann et al., 2014). WSABIE is shallow, linear,
and ignores word order information, and so may
have less modeling power than our approach.
Convolutional neural networks (CNNs), in
which shared weights are applied across the in-
put, are popular in the vision domain and have re-
cently been applied to semantic role labeling (Col-
lobert et al., 2011) and parsing (Collobert, 2011).
Neural networks in general have also been applied
to part-of-speech tagging, chunking, named en-
tity recognition (Collobert et al., 2011; Turian et
al., 2010), and sentiment detection (Socher et al.,
2013). All these tasks involve predicting a limited
(2-30) number of labels. In this work, we make
use of CNNs, but apply them to the task of rank-
ing a very large set of tags. We thus propose a
model and training scheme that can scale to this
class of problem.
3 Convolutional Embedding Model
Our model #TAGSPACE (see Figure 1), like other
word embedding models, starts by assigning a d-
dimensional vector to each of the l words of an
input document w
1
, . . . , w
l
, resulting in a matrix
of size l ? d. This is achieved using a matrix of
N ? d parameters, termed the lookup-table layer
(Collobert et al., 2011), where N is the vocabulary
size. In this work N is 10
6
, and each row of the
matrix represents one of the million most frequent
words in the training corpus.
A convolution layer is then applied to the l ? d
input matrix, which considers all successive win-
dows of text of size K, sliding over the docu-
ment from position 1 to l. This requires a fur-
ther Kd?H weights and H biases to be learned.
To account for words at the two boundaries of the
document we also apply a special padding vector
at both ends. In our experiments K was set to 5
and H was set to 1000. After the convolutional
step, a tanh nonlinearity followed by a max op-
eration over the l ? H features extracts a fixed-
size (H-dimensional) global feature vector, which
is independent of document size. Finally, another
tanh non-linearity followed by a fully connected
linear layer of size H?d is applied to represent the
entire document in the original embedding space
of d-dimensions.
Hashtags are also represented using d-
dimensional embeddings using a lookup-table.
We represent the top 100,000 most frequent tags.
For a given document w we then rank any given
hashtag t using the scoring function:
f(w, t) = e
conv
(w) ? e
lt
(t)
1823
where e
conv
(w) is the embedding of the document
by the CNN just described and e
lt
(t) is the em-
bedding of a candidate tag t. We can thus rank all
candidate hashtags via their scores f(w, t), largest
first.
To train the above scoring function, and hence
the parameters of the model we minimize a rank-
ing loss similar to the one used in WSABIE as
a training objective: for each training example,
we sample a positive tag, compute f(w, t
+
), then
sample random tags
?
t up to 1000 times until
f(w,
?
t) > m + f(w, t
+
), where m is the mar-
gin. A gradient step is then made to optimize the
pairwise hinge loss:
L = max{0,m? f(w, t
+
) + f(w,
?
t)}.
We use m = 0.1 in our experiments. This loss
function is referred to as the WARP loss in (We-
ston et al., 2011) and is used to approximately
optimizing the top of the ranked list, useful for
metrics like precision and recall@k. In particu-
lar, the search for a negative candidate tag means
that more energy is spent on improving the rank-
ing performance of positive labels already near the
top of the ranked list, compared to only randomly
sampling of negatives, which would optimize the
average rank instead.
Minimizing our loss is achieved with parallel
stochastic gradient descent using the hogwild al-
gorithm (Niu et al., 2011). The lookup-table lay-
ers are initialized with the embeddings learned by
WSABIE to expedite convergence. This kind of
?pre-training? is a standard trick in the neural net-
work literature, see e.g. (Socher et al., 2011).
The ranking loss makes our model scalable to
100,000 (or more) hashtags. At each training ex-
ample only a subset of tags have to be computed,
so it is far more efficient than a standard classifi-
cation loss that considers them all.
4 Experiments
4.1 Data
Our experiments use two large corpora of posts
containing hashtags from a popular social net-
work.
1
The first corpus, which we call people,
consists of 201 million posts from individual user
accounts, comprising 5.5 billion words.
The second corpus, which we call pages, con-
sists of 35.3 million page posts, comprising 1.6
1
Both corpora were de-identified during collection.
Posts Words
Dataset (millions) (billions) Top 4 tags
Pages 35.3 1.6
#fitness,
#beauty,
#luxury, #cars
People 201 5.5
#FacebookIs10,
#love, #tbt,
#happy
Table 1: Datasets used in hashtag prediction.
billion words. These posts? authorial voice is a
public entity, such as a business, celebrity, brand,
or product. The posts in the pages dataset are pre-
sumably intended for a wider, more general audi-
ence than the posts in the people dataset. Both are
summarized in Table 1.
Both corpora comprise posts between February
1st and February 17th, 2014. Since we are not at-
tempting a multi-language model, we use a simple
trigram-based language prediction model to con-
sider only posts whose most likely language is En-
glish.
The two datasets use hashtags very differently.
The pages dataset has a fatter head, with popular
tags covering more examples. The people dataset
uses obscure tags more heavily. For example, the
top 100 tags account for 33.9% of page tags, but
only 13.1% of people tags.
4.2 Hashtag prediction
The hashtag prediction task attempts to rank a
post?s ground-truth hashtags higher than hash-
tags it does not contain. We trained models on
both the people and page datasets, and collected
precision at 1, recall at 10, and mean rank for
50,000 randomly selected posts withheld from
training. A further 50,000 withheld posts are
used for selecting hyperparameters. We compare
#TAGSPACE with the following models:
Frequency This simple baseline ignores input
text, always ranking hashtags by their frequency
in the training data.
#words This baseline assigns each tag a static
score based on its frequency plus a large bonus if
it corresponds to a word in the input text. For ex-
ample, on input ?crazy commute this am?, #words
ranks #crazy, #commute, #this and #am
highest, in frequency order.
Word2vec We trained the unsupervised model
of Mikolov et al. (2013) on both datasets, treat-
ing hashtags the same as all other words. To ap-
1824
Crazy commute this am, #nyc, #snow, #puremichigan, #snowday, #snowstorm,
was lucky to even get in to work. #tubestrike, #blizzard, #commute, #snowpocalypse, #chiberia
This can?t go on anymore, #samelove, #equalrights, #equality, #equalityforall, #loveislove,
we need marriage equality now! #lgbt, #marriageequality, #noh8, #gayrights, #gaymarriage
Kevin spacey what a super hottie :) #houseofcards, #hoc, #houseofcardsseason2, #season2, #kevinspacey,
#frankunderwood, #netflix, #suits, #swoon, #hubbahubba
Went shopping today and found a really #mango, #shopping, #heaven, #100happydays, #yummy,
good place to get fresh mango. #lunch, #retailtherapy, #yum, #cravings, #wholefoods
Went running today -- #running, #ouch, #pain, #nopainnogain, #nike
my feet hurt so much! #marathontraining, #sore, #outofshape, #nikeplus, #runnerproblems
Wow, what a goal that was, #arsenal, #coyg, #ozil, #afc, #arsenalfc
just too fast, Mesut Ozil is the best! #lfc, #ynwa, #mesut, #gunners, #ucl
Working really hard on the paper #thestruggle, #smh, #lol, #collegelife, #homework
all last night. #sad, #wtf, #confused, #stressed, #work
The restaurant was too expensive #ripoff, #firstworldproblems, #smh, #fail, #justsaying
and the service was slow. #restaurant, #badservice, #food, #middleclassproblems, #neveragain
The restaurant had great food #dinner, #restaurant, #yum, #food, #delicious
and was reasonably priced. #stuffed, #goodtimes, #foodporn, #yummy, #winning
He has the longest whiskers, #cat, #kitty, #meow, #cats, #catsofinstagram
omg so sweet! #crazycatlady, #cute, #kitten, #catlady, #adorable
Table 2: #TAGSPACE (256 dim) predictions for some example posts.
ply these word embeddings to ranking, we first
sum the embeddings of each word in the text (as
word2vec does), and then rank hashtags by simi-
larity of their embedding to that of the text.
2
WSABIE (Weston et al., 2011) is a supervised
bilinear embedding model. Each word and tag has
an embedding. The words in a text are averaged
to produce an embedding of the text, and hash-
tags are ranked by similarity to the text embed-
ding. That is, the model is of the form:
f(w, t) = w
>
U
>
V t
where the post w is represented as a bag of words
(a sparse vector in R
N
), the tag is a one-hot-vector
in R
N
, and U and V are k ?N embedding matri-
ces. The WARP loss, as described in section 3, is
used for training.
Performance of all these models at hashtag pre-
diction is summarized in Tables 3 and 4. We find
similar results for both datasets. The frequency
and #words baselines perform poorly across the
2
Note that the unsupervised Word2vec embeddings could
be used as input to a supervised classifier, which we did not
do. For a supervised embedding baseline we instead use WS-
ABIE. WSABIE trains word embeddings U and hashtag em-
beddings V in a supervised fashion, whereas Word2vec trains
them both unsupervised. Adding supervision to Word2vec
would effectively do something in-between: U would still be
unsupervised, but V would then be supervised.
board, establishing the need to learn from text.
Among the learning models, the unsupervised
word2vec performs the worst. We believe this
is due to it being unsupervised ? adding super-
vision better optimizes the metric we evaluate.
#TAGSPACE outperforms WSABIE at all dimen-
sionalities. Due to the relatively large test sets,
the results are statistically significant; for example,
comparing #TAGSPACE (64 dim) beats Wsabie (64
dim) for the page dataset 56% of the time, and
draws 23% of the time in terms of the rank met-
ric, and is statistically significant with a Wilcoxon
signed-rank test.
Some example predictions for #TAGSPACE are
given for some constructed examples in Table 2.
We also show nearest word embeddings to the
posts. Training data was collected at the time of
the pax winter storm, explaining predictions for
the first post, and Kevin Spacey appears in the
show ?House of Cards,?. In all cases the hash-
tags reveal labels that capture the semantics of the
posts, not just syntactic similarity of individual
words.
Comparison to Production System We also
compare to a proprietary system in production in
Facebook for hashtag prediction. It trains a lo-
gistic regression model for every hashtag, using
a bag of unigrams, bigrams, and trigrams as the
1825
Method dim P@1 R@10 Rank
Freq. baseline - 1.06% 2.48% 11277
#words baseline - 0.90% 3.01% 11034
Word2Vec 256 1.21% 2.85% 9973
Word2Vec 512 1.14% 2.93% 8727
WSABIE 64 4.55% 8.80% 6921
WSABIE 128 5.25% 9.33% 6208
WSABIE 256 5.66% 10.34% 5519
WSABIE 512 5.92% 10.74% 5452
#TAGSPACE 64 6.69% 12.42% 3569
#TAGSPACE 128 6.91% 12.57% 3858
#TAGSPACE 256 7.37% 12.58% 3820
Table 3: Hashtag test results for people dataset.
Method dim P@1 R@10 Rank
Freq. baseline - 4.20% 1.59% 11103
#words baseline - 2.63% 5.05% 10581
Word2Vec 256 4.66% 8.15% 10149
Word2Vec 512 5.26% 9.33% 9800
WSABIE 64 24.45% 29.64% 2619
WSABIE 128 27.47% 32.94% 2325
WSABIE 256 29.76% 35.28% 1992
WSABIE 512 30.90% 36.96% 1184
#TAGSPACE 64 34.08% 38.96% 1184
#TAGSPACE 128 36.27% 41.42% 1165
#TAGSPACE 256 37.42% 43.01% 1155
Table 4: Hashtag test results for pages dataset.
input features. Unlike the other models we con-
sider here, this baseline has been trained using a
set of approximately 10 million posts. Engineer-
ing constraints prevent measuring mean rank per-
formance. We present it here as a serious effort
at solving the same problem from outside the em-
bedding paradigm. On the people dataset this sys-
tem achieves 3.47% P@1 and 5.33% R@10. On
the pages dataset it obtains 5.97% P@1 and 6.30%
R@10. It is thus outperformed by our method.
However, we note the differences in experimen-
tal setting mean this comparison is perhaps not
completely fair (different training sets). We expect
performance of linear models such as this to be
similar to WSABIE as that has been in the case in
other datasets (Gupta et al., 2014), but at the cost
of more memory usage. Note that models like lo-
gistic regression and SVM do not scale well if you
have millions of hashtags, which we could handle
in our models.
4.3 Personalized document recommendation
To investigate the generality of these learned rep-
resentations, we apply them to the task of recom-
mending documents to users based on the user?s
interaction history. The data for this task comprise
anonymized day-long interaction histories for a
tiny subset of people on a popular social network-
Method dim P@1 R@10 R@50
Word2Vec 256 0.75% 1.96% 3.82%
BoW - 1.36% 4.29% 8.03%
WSABIE 64 0.98% 3.14% 6.65%
WSABIE 128 1.02% 3.30% 6.71%
WSABIE 256 1.01% 2.98% 5.99%
WSABIE 512 1.01% 2.76% 5.19%
#TAGSPACE 64 1.27% 4.56% 9.64%
#TAGSPACE 128 1.48% 4.74% 9.96%
#TAGSPACE 256 1.66% 5.29% 10.69%
WSABIE+ BoW 64 1.61% 4.83% 9.00%
#TAGSPACE+ BoW 64 1.80% 5.90% 11.22%
#TAGSPACE+ BoW 256 1.92% 6.15% 11.53%
Table 5: Document recommendation task results.
ing service. For each of the 34 thousand people
considered, we collected the text of between 5 and
167 posts that she has expressed previous positive
interactions with (likes, clicks, etc.). Given the
person?s trailing n?1 posts, we use our models to
predict the n?th post by ranking it against 10,000
other unrelated posts, and measuring precison and
recall. The score of the n
th
post is obtained by
taking the max similarity over the n? 1 posts. We
use cosine similarity between post embeddings in-
stead of the inner product that was used for hash-
tag training so that the scores are not unduly influ-
enced by document length. All learned hashtag
models were trained on the people dataset. We
also consider a TF-IDF weighted bag-of-words
baseline (BoW). The results are given in Table 5.
Hashtag-based embeddings outperform BoW
and unsupervised embeddings across the board,
and #TAGSPACE outperforms WSABIE. The best
results come from summing the bag-of-words
scores with those of #TAGSPACE.
5 Conclusion
#TAGSPACE is a convolutional neural network
that learns to rank hashtags with a minimum of
task-specific assumptions and engineering. It per-
forms well, beating several baselines and an in-
dustrial system engineered for hashtag prediction.
The semantics of hashtags cause #TAGSPACE to
learn a representation that captures many salient
aspects of text. This representation is general
enough to port to the task of personalized docu-
ment recommendation, where it outperforms other
well-known representations.
Acknowledgements
We thank Ledell Wu and Jeff Pasternak for their
help with datasets and baselines.
1826
References
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ?10, pages 241?249, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Zhuoye Ding, Qi Zhang, and Xuanjing Huang. 2012.
Automatic hashtag recommendation for microblogs
using topic-specific translation model. In COLING
(Posters)?12, pages 265?274.
Fr?ederic Godin, Viktor Slavkovikj, Wesley De Neve,
Benjamin Schrauwen, and Rik Van de Walle. 2013.
Using topic models for twitter hashtag recommen-
dation. In Proceedings of the 22nd international
conference on World Wide Web companion, pages
593?596. International World Wide Web Confer-
ences Steering Committee.
Maya R Gupta, Samy Bengio, and Jason Weston.
2014. Training highly multiclass classifiers. Jour-
nal of Machine Learning Research, 15:1?48.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
Proceedings of ACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Feng Niu, Benjamin Recht, Christopher R?e, and
Stephen J Wright. 2011. Hogwild!: A lock-free ap-
proach to parallelizing stochastic gradient descent.
Advances in Neural Information Processing Sys-
tems, 24:693?701.
Jieying She and Lei Chen. 2014. Tomoha: Topic
model-based hashtag recommendation on twitter. In
Proceedings of the companion publication of the
23rd international conference on World wide web
companion, pages 371?372. International World
Wide Web Conferences Steering Committee.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631?1642.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary
image annotation. In Proceedings of the Twenty-
Second international joint conference on Artificial
Intelligence-Volume Volume Three, pages 2764?
2770. AAAI Press.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for
relation extraction. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
1827
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1448?1458,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Semantic Frame Identification with Distributed Word Representations
Karl Moritz Hermann
??
Dipanjan Das
?
Jason Weston
?
Kuzman Ganchev
?
?
Department of Computer Science, University of Oxford, Oxford OX1 3QD, United Kingdom
?
Google Inc., 76 9th Avenue, New York, NY 10011, United States
karl.moritz.hermann@cs.ox.ac.uk
{dipanjand,kuzman}@google.com jaseweston@gmail.com
Abstract
We present a novel technique for semantic
frame identification using distributed rep-
resentations of predicates and their syntac-
tic context; this technique leverages auto-
matic syntactic parses and a generic set
of word embeddings. Given labeled data
annotated with frame-semantic parses, we
learn a model that projects the set of word
representations for the syntactic context
around a predicate to a low dimensional
representation. The latter is used for se-
mantic frame identification; with a stan-
dard argument identification method in-
spired by prior work, we achieve state-of-
the-art results on FrameNet-style frame-
semantic analysis. Additionally, we report
strong results on PropBank-style semantic
role labeling in comparison to prior work.
1 Introduction
Distributed representations of words have proved
useful for a number of tasks. By providing richer
representations of meaning than what can be en-
compassed in a discrete representation, such ap-
proaches have successfully been applied to tasks
such as sentiment analysis (Socher et al, 2011),
topic classification (Klementiev et al, 2012) or
word-word similarity (Mitchell and Lapata, 2008).
We present a new technique for semantic frame
identification that leverages distributed word rep-
resentations. According to the theory of frame se-
mantics (Fillmore, 1982), a semantic frame rep-
resents an event or scenario, and possesses frame
elements (or semantic roles) that participate in the
?
The majority of this research was carried out during an
internship at Google.
event. Most work on frame-semantic parsing has
usually divided the task into two major subtasks:
frame identification, namely the disambiguation of
a given predicate to a frame, and argument iden-
tification (or semantic role labeling), the analysis
of words and phrases in the sentential context that
satisfy the frame?s semantic roles (Das et al, 2010;
Das et al, 2014).
1
Here, we focus on the first sub-
task of frame identification for given predicates;
we use our novel method (?3) in conjunction with
a standard argument identification model (?4) to
perform full frame-semantic parsing.
We present experiments on two tasks. First, we
show that for frame identification on the FrameNet
corpus (Baker et al, 1998; Fillmore et al, 2003),
we outperform the prior state of the art (Das et al,
2014). Moreover, for full frame-semantic parsing,
with the presented frame identification technique
followed by our argument identification method,
we report the best results on this task to date. Sec-
ond, we present results on PropBank-style seman-
tic role labeling (Palmer et al, 2005; Meyers et al,
2004; M`arquez et al, 2008), that approach strong
baselines, and are on par with prior state of the art
(Punyakanok et al, 2008).
2 Overview
Early work in frame-semantic analysis was pio-
neered by Gildea and Jurafsky (2002). Subsequent
work in this area focused on either the FrameNet
or PropBank frameworks, and research on the lat-
ter has been more popular. Since the CoNLL
2004-2005 shared tasks (Carreras and M`arquez,
1
There are exceptions, wherein the task has been modeled
using a pipeline of three classifiers that perform frame iden-
tification, a binary stage that classifies candidate arguments,
and argument identification on the filtered candidates (Baker
et al, 2007; Johansson and Nugues, 2007).
1448
John     bought    a   car   .
COMMERCE_BUY
buy.V
Buyer Goods
John     bought    a   car   .
buy.01
buy.V
A0 A1
Mary      sold        a   car   .
COMMERCE_BUY
sell.V
Seller Goods
Mary      sold        a   car   .
sell.01
sell.V
A0 A1
(a) (b)
Figure 1: Example sentences with frame-semantic analyses.
FrameNet annotation conventions are used in (a) while (b)
denotes PropBank conventions.
2004; Carreras and M`arquez, 2005) on PropBank
semantic role labeling (SRL), it has been treated
as an important NLP problem. However, research
has mostly focused on argument analysis, skipping
the frame disambiguation step, and its interaction
with argument identification.
2.1 Frame-Semantic Parsing
Closely related to SRL, frame-semantic parsing
consists of the resolution of predicate sense into
a frame, and the analysis of the frame?s argu-
ments. Work in this area exclusively uses the
FrameNet full text annotations. Johansson and
Nugues (2007) presented the best performing sys-
tem at SemEval 2007 (Baker et al, 2007), and Das
et al (2010) improved performance, and later set
the current state of the art on this task (Das et al,
2014). We briefly discuss FrameNet, and subse-
quently PropBank annotation conventions here.
FrameNet The FrameNet project (Baker et al,
1998) is a lexical database that contains informa-
tion about words and phrases (represented as lem-
mas conjoined with a coarse part-of-speech tag)
termed as lexical units, with a set of semantic
frames that they could evoke. For each frame,
there is a list of associated frame elements (or
roles, henceforth), that are also distinguished as
core or non-core.
2
Sentences are annotated us-
ing this universal frame inventory. For exam-
ple, consider the pair of sentences in Figure 1(a).
COMMERCE BUY is a frame that can be evoked by
morphological variants of the two example lexical
units buy.V and sell.V. Buyer, Seller and Goods are
some example roles for this frame.
2
Additional information such as finer distinction of the
coreness properties of roles, the relationship between frames,
and that of roles are also present, but we do not leverage that
information in this work.
PropBank The PropBank project (Palmer et al,
2005) is another popular resource related to se-
mantic role labeling. The PropBank corpus has
verbs annotated with sense frames and their ar-
guments. Like FrameNet, it also has a lexi-
cal database that stores type information about
verbs, in the form of sense frames and the possi-
ble semantic roles each frame could take. There
are modifier roles that are shared across verb
frames, somewhat similar to the non-core roles
in FrameNet. Figure 1(b) shows annotations for
two verbs ?bought? and ?sold?, with their lemmas
(akin to the lexical units in FrameNet) and their
verb frames buy.01 and sell.01. Generic core role
labels (of which there are seven, namely A0-A5 and
AA) for the verb frames are marked in the figure.
3
A key difference between the two annotation sys-
tems is that PropBank uses a local frame inven-
tory, where frames are predicate-specific. More-
over, role labels, although few in number, take spe-
cific meaning for each verb frame. Figure 1 high-
lights this difference: while both sell.V and buy.V
are members of the same frame in FrameNet, they
evoke different frames in PropBank. In spite of
this difference, nearly identical statistical models
could be employed for both frameworks.
Modeling In this paper, we model the frame-
semantic parsing problem in two stages: frame
identification and argument identification. As
mentioned in ?1, these correspond to a frame dis-
ambiguation stage,
4
and a stage that finds the var-
ious arguments that fulfill the frame?s semantic
roles within the sentence, respectively. This re-
sembles the framework of Das et al (2014), who
solely focus on FrameNet corpora, unlike this pa-
per. The novelty of this paper lies in the frame
identification stage (?3). Note that this two-stage
approach is unusual for the PropBank corpora
when compared to prior work, where the vast ma-
jority of published papers have not focused on the
verb frame disambiguation problem at all, only fo-
cusing on the role labeling stage (see the overview
paper of M`arquez et al (2008) for example).
3
NomBank (Meyers et al, 2004) is a similar resource for
nominal predicates, but we do not consider it in our experi-
ments.
4
For example in PropBank, the lexical unit buy.V has
three verb frames and in sentential context, we want to disam-
biguate its frame. (Although PropBank never formally uses
the term lexical unit, we adopt its usage from the frame se-
mantics literature.)
1449
2.2 Distributed Frame Identification
We present a model that takes word embeddings
as input and learns to identify semantic frames.
A word embedding is a distributed representa-
tion of meaning where each word is represented
as a vector in R
n
. Such representations allow a
model to share meaning between similar words,
and have been used to capture semantic, syntac-
tic and morphological content (Collobert and We-
ston, 2008; Turian et al, 2010, inter alia). We use
word embeddings to represent the syntactic con-
text of a particular predicate instance as a vector.
For example, consider the sentence ?He runs the
company.? The predicate runs has two syntac-
tic dependents ? a subject and direct object (but
no prepositional phrases or clausal complements).
We could represent the syntactic context of runs as
a vector with blocks for all the possible dependents
warranted by a syntactic parser; for example, we
could assume that positions 0 . . . n in the vector
correspond to the subject dependent, n+1 . . . 2n
correspond to the clausal complement dependent,
and so forth. Thus, the context is a vector in R
nk
with the embedding of He at the subject position,
the embedding of company in direct object posi-
tion and zeros everywhere else. Given input vec-
tors of this form for our training data, we learn a
matrix that maps this high dimensional and sparse
representation into a lower dimensional space. Si-
multaneously, the model learns an embedding for
all the possible labels (i.e. the frames in a given
lexicon). At inference time, the predicate-context
is mapped to the low dimensional space, and we
choose the nearest frame label as our classifica-
tion. We next describe this model in detail.
3 Frame Identification with Embeddings
We continue using the example sentence from
?2.2: ?He runs the company.? where we want to
disambiguate the frame of runs in context. First,
we extract the words in the syntactic context of
runs; next, we concatenate their word embeddings
as described in ?2.2 to create an initial vector space
representation. Subsequently, we learn a map-
ping from this initial representation into a low-
dimensional space; we also learn an embedding
for each possible frame label in the same low-
dimensional space. The goal of learning is to
make sure that the correct frame label is as close as
possible to the mapped context, while competing
frame labels are farther away.
Formally, let x represent the actual sentence
with a marked predicate, along with the associated
syntactic parse tree; let our initial representation
of the predicate context be g(x). Suppose that the
word embeddings we start with are of dimension
n. Then g is a function from a parsed sentence
x to R
nk
, where k is the number of possible syn-
tactic context types. For example g selects some
important positions relative to the predicate, and
reserves a block in its output space for the embed-
ding of words found at that position. Suppose g
considers clausal complements and direct objects.
Then g : X ? R
2n
and for the example sentence
it has zeros in positions 0 . . . n and the embedding
of the word company in positions n+1 . . . 2n.
g(x) = [0, . . . , 0, embedding of company].
Section 3.1 describes the context positions we use
in our experiments. Let the low dimensional space
we map to be R
m
and the learned mapping be M :
R
nk
? R
m
. The mapping M is a linear trans-
formation, and we learn it using the WSABIE algo-
rithm (Weston et al, 2011). WSABIE also learns an
embedding for each frame label (y, henceforth).
In our setting, this means that each frame corre-
sponds to a point in R
m
. If we have F possi-
ble frames we can store those parameters in an
F ?m matrix, one m-dimensional point for each
frame, which we will refer to as the linear map-
ping Y . Let the lexical unit (the lemma conjoined
with a coarse POS tag) for the marked predicate
be `. We denote the frames that associate with
` in the frame lexicon
5
and our training corpus
as F
`
. WSABIE performs gradient-based updates
on an objective that tries to minimize the distance
between M(g(x)) and the embedding of the cor-
rect label Y (y), while maintaining a large distance
between M(g(x)) and the other possible labels
Y (y?) in the confusion set F
`
. At disambiguation
time, we use a simple dot product similarity as our
distance metric, meaning that the model chooses
a label by computing the argmax
y
s(x, y) where
s(x, y) = M(g(x)) ?Y (y), where the argmax iter-
ates over the possible frames y ? F
`
if ` was seen
in the lexicon or the training data, or y ? F , if it
was unseen.
6
Model learning is performed using
the margin ranking loss function as described in
5
The frame lexicon stores the frames, corresponding se-
mantic roles and the lexical units associated with the frame.
6
This disambiguation scheme is similar to the one adopted
by Das et al (2014), but they use unlemmatized words to
define their confusion set.
1450
Figure 2: Context representation extraction for the
embedding model. Given a dependency parse (1)
the model extracts all words matching a set of paths
from the frame evoking predicate and its direct de-
pendents (2). The model computes a composed rep-
resentation of the predicate instance by using dis-
tributed vector representations for words (3) ? the
(red) vertical embedding vectors for each word are
concatenated into a long vector. Finally, we learn a
linear transformation function parametrized by the
context blocks (4).
Weston et al (2011), and in more detail in section
3.2.
Since WSABIE learns a single mapping from g(x)
to R
m
, parameters are shared between different
words and different frames. So for example ?He
runs the company? could help the model disam-
biguate ?He owns the company.? Moreover, since
g(x) relies on word embeddings rather than word
identities, information is shared between words.
For example ?He runs the company? could help
us to learn about ?She runs a corporation?.
3.1 Context Representation Extraction
In principle g(x) could be any feature function, but
we performed an initial investigation of two partic-
ular variants. In both variants, our representation
is a block vector where each block corresponds to
a syntactic position relative to the predicate, and
each block?s values correspond to the embedding
of the word at that position.
Direct Dependents The first context function we
considered corresponds to the examples in ?3. To
elaborate, the positions of interest are the labels of
the direct dependents of the predicate, so k is the
number of labels that the dependency parser can
produce. For example, if the label on the edge be-
tween runs and He is nsubj, we would put the em-
bedding of He in the block corresponding to nsubj.
If a label occurs multiple times, then the embed-
dings of the words below this label are averaged.
Unfortunately, using only the direct dependents
can miss a lot of useful information. For exam-
ple, topicalization can place discriminating infor-
mation farther from the predicate. Consider ?He
runs the company.? vs. ?It was the company that
he runs.? In the second sentence, the discrim-
inating word, company dominates the predicate
runs. Similarly, predicates in embedded clauses
may have a distant agent which cannot be captured
using direct dependents. Consider ?The athlete
ran the marathon.? vs. ?The athlete prepared him-
self for three months to run the marathon.? In the
second example, for the predicate run, the agent
The athlete is not a direct dependent, but is con-
nected via a longer dependency path.
Dependency Paths To capture more relevant
context, we developed a second context function
as follows. We scanned the training data for a
given task (either the PropBank or the FrameNet
domains) for the dependency paths that connected
the gold predicates to the gold semantic argu-
ments. This set of dependency paths were deemed
as possible positions in the initial vector space rep-
resentation. In addition, akin to the first context
function, we also added all dependency labels to
the context set. Thus for this context function, the
block cardinality k was the sum of the number of
scanned gold dependency path types and the num-
ber of dependency labels. Given a predicate in its
sentential context, we therefore extract only those
context words that appear in positions warranted
by the above set. See Figure 2 for an illustration
of this process.
We performed initial experiments using con-
text extracted from 1) direct dependents, 2) de-
pendency paths, and 3) both. For all our experi-
ments, setting 3) which concatenates the direct de-
pendents and dependency path always dominated
the other two, so we only report results for this
setting.
3.2 Learning
We model our objective function following We-
ston et al (2011), using a weighted approximate-
rank pairwise loss, learned with stochastic gradi-
ent descent. The mapping from g(x) to the low
dimensional space R
m
is a linear transformation,
so the model parameters to be learnt are the matrix
M ? R
nk?m
as well as the embedding of each
possible frame label, represented as another ma-
trix Y ? R
F?m
where there are F frames in total.
The training objective function minimizes:
?
x
?
y?
L
(
rank
y
(x)
)
max(0, ?+s(x, y)?s(x, y?)).
1451
where x, y are the training inputs and their cor-
responding correct frames, and y? are negative
frames, ? is the margin. Here, rank
y
(x) is the
rank of the positive frame y relative to all the neg-
ative frames:
rank
y
(x) =
?
y?
I(s(x, y) ? ? + s(x, y?)),
and L(?) converts the rank to a weight. Choos-
ing L(?) = C? for any positive constant C opti-
mizes the mean rank, whereas a weighting such as
L(?) =
?
?
i=1
1/i (adopted here) optimizes the
top of the ranked list, as described in (Usunier
et al, 2009). To train with such an objective,
stochastic gradient is employed. For speed the
computation of rank
y
(x) is then replaced with a
sampled approximation: sample N items y? until
a violation is found, i.e. max(0, ? + s(x, y?) ?
s(x, y))) > 0 and then approximate the rank with
(F ? 1)/N , see Weston et al (2011) for more
details on this procedure. For the choices of the
stochastic gradient learning rate, margin (?) and
dimensionality (m), please refer to ?5.4-?5.5.
Note that an alternative approach could learn
only the matrixM , and then use a k-nearest neigh-
bor classifier in R
m
, as in Weinberger and Saul
(2009). The advantage of learning an embedding
for the frame labels is that at inference time we
need to consider only the set of labels for classi-
fication rather than all training examples. Addi-
tionally, since we use a frame lexicon that gives
us the possible frames for a given predicate, we
usually only consider a handful of candidate la-
bels. If we used all training examples for a given
predicate for finding a nearest-neighbor match at
inference time, we would have to consider many
more candidates, making the process very slow.
4 Argument Identification
Here, we briefly describe the argument identifi-
cation model used in our frame-semantic parsing
experiments, post frame identification. Given x,
the sentence with a marked predicate, the argu-
ment identification model assumes that the pred-
icate frame y has been disambiguated. From a
frame lexicon, we look up the set of semantic roles
R
y
that associate with y. This set alo contains the
null role r
?
. From x, a rule-based candidate argu-
ment extraction algorithm extracts a set of spans
A that could potentially serve as the overt
7
argu-
7
By overtness, we mean the non-null instantiation of a
semantic role in a frame-semantic parse.
? starting word of a ? POS of the starting word of a
? ending word of a ? POS of the ending word of a
? head word of a ? POS of the head word of a
? bag of words in a ? bag of POS tags in a
? a bias feature ? voice of the predicate use
? word cluster of a?s head
? word cluster of a?s head conjoined with word cluster
of the predicate
?
? dependency path between a?s head and the predicate
? the set of dependency labels of the predicate?s children
? dependency path conjoined with the POS tag of a?s
head
? dependency path conjoined with the word cluster of
a?s head
? position of a with respect to the predicate (before, after,
overlap or identical)
? whether the subject of the predicate is missing (miss-
ingsubj)
? missingsubj, conjoined with the dependency path
? missingsubj, conjoined with the dependency path from
the verb dominating the predicate to a?s head
Table 1: Argument identification features. The span in con-
sideration is termed a. Every feature in this list has two ver-
sions, one conjoined with the given role r and the other con-
joined with both r and the frame y. The feature with a
?
su-
perscript is only conjoined with the role to reduce its sparsity.
mentsA
y
for y (see ?5.4-?5.5 for the details of the
candidate argument extraction algorithms).
Learning Given training data of the form
??x
(i)
, y
(i)
,M
(i)
??
N
i=1
, where,
M = {(r, a} : r ? R
y
, a ? A ?A
y
}, (1)
a set of tuples that associates each role r in R
y
with a span a according to the gold data. Note that
this mapping associates spans with the null role r
?
as well. We optimize the following log-likelihood
to train our model:
max
?
N
?
i=1
|M
(i)
|
?
j=1
log p?
(
(r, a)
j
|x, y,R
y
)
? C???
2
2
where p? is a log-linear model normalized over the
set R
y
, with features described in Table 1. We
set C = 1.0 and use L-BFGS (Liu and Nocedal,
1989) for training.
Inference Although our learning mechanism
uses a local log-linear model, we perform infer-
ence globally on a per-frame basis by applying
hard structural constraints. Following Das et al
(2014) and Punyakanok et al (2008) we use the
log-probability of the local classifiers as a score in
an integer linear program (ILP) to assign roles sub-
ject to hard constraints described in ?5.4 and ?5.5.
We use an off-the-shelf ILP solver for inference.
1452
5 Experiments
In this section, we present our experiments and
the results achieved. We evaluate our novel frame
identification approach in isolation and also con-
joined with argument identification resulting in
full frame-semantic structures; before presenting
our model?s performance we first focus on the
datasets, baselines and the experimental setup.
5.1 Data
We evaluate our models on both FrameNet- and
PropBank-style structures. For FrameNet, we use
the full-text annotations in the FrameNet 1.5 re-
lease
8
which was used by Das et al (2014, ?3.2).
We used the same test set as Das et al contain-
ing 23 documents with 4,458 predicates. Of the
remaining 55 documents, 16 documents were ran-
domly chosen for development.
9
For experiments with PropBank, we used the
Ontonotes corpus (Hovy et al, 2006), version 4.0,
and only made use of the Wall Street Journal doc-
uments; we used sections 2-21 for training, sec-
tion 24 for development and section 23 for testing.
This resembles the setup used by Punyakanok et
al. (2008). All the verb frame files in Ontonotes
were used for creating our frame lexicon.
5.2 Frame Identification Baselines
For comparison, we implemented a set of baseline
models, with varying feature configurations. The
baselines use a log-linear model that models the
following probability at training time:
p(y|x, `) =
e
??f(y,x,`)
?
y??F
`
e
??f(y?,x,`)
(2)
At test time, this model chooses the best frame as
argmax
y
? ? f(y, x, `) where argmax iterates over
the possible frames y ? F
`
if ` was seen in the
lexicon or the training data, or y ? F , if it was un-
seen, like the disambiguation scheme of ?3. We
train this model by maximizing L
2
regularized
log-likelihood, using L-BFGS; the regularization
constant was set to 0.1 in all experiments.
For comparison with our model from ?3, which
we call WSABIE EMBEDDING, we implemented two
baselines with the log-linear model. Both the
baselines use features very similar to the input rep-
resentations described in ?3.1. The first one com-
putes the direct dependents and dependency paths
8
See https://framenet.icsi.berkeley.edu.
9
These documents are listed in appendix A.
as described in ?3.1 but conjoins them with the
word identity rather than a word embedding. Ad-
ditionally, this model uses the un-conjoined words
as backoff features. This would be a standard NLP
approach for the frame identification problem, but
is surprisingly competitive with the state of the art.
We call this baseline LOG-LINEAR WORDS. The sec-
ond baseline, tries to decouple the WSABIE training
from the embedding input, and trains a log linear
model using the embeddings. So the second base-
line has the same input representation as WSABIE
EMBEDDING but uses a log-linear model instead of
WSABIE. We call this model LOG-LINEAR EMBED-
DING.
5.3 Common Experimental Setup
We process our PropBank and FrameNet training,
development and test corpora with a shift-reduce
dependency parser that uses the Stanford conven-
tions (de Marneffe and Manning, 2013) and uses
an arc-eager transition system with beam size of 8;
the parser and its features are described by Zhang
and Nivre (2011). Before parsing the data, it is
tagged with a POS tagger trained with a condi-
tional random field (Lafferty et al, 2001) with the
following emission features: word, the word clus-
ter, word suffixes of length 1, 2 and 3, capitaliza-
tion, whether it has a hyphen, digit and punctua-
tion. Beyond the bias transition feature, we have
two cluster features for the left and right words in
the transition. We use Brown clusters learned us-
ing the algorithm of Uszkoreit and Brants (2008)
on a large English newswire corpus for cluster fea-
tures. We use the same word clusters for the argu-
ment identification features in Table 1.
We learn the initial embedding representations
for our frame identification model (?3) using a
deep neural language model similar to the one pro-
posed by Bengio et al (2003). We use 3 hidden
layers each with 1024 neurons and learn a 128-
dimensional embedding from a large corpus con-
taining over 100 billion tokens. In order to speed
up learning, we use an unnormalized output layer
and a hinge-loss objective. The objective tries to
ensure that the correct word scores higher than a
random incorrect word, and we train with mini-
batch stochastic gradient descent.
5.4 Experimental Setup for FrameNet
Hyperparameters For our frame identification
model with embeddings, we search for the WSA-
BIE hyperparameters using the development data.
1453
SEMAFOR LEXICON FULL LEXICON
Development Data
Model All Ambiguous Rare All Ambiguous Rare
LOG-LINEAR WORDS 96.21 90.41 95.75 96.37 90.41 96.07
LOG-LINEAR EMBEDDING 96.06 90.56 95.38 96.19 90.49 95.70
WSABIE EMBEDDING (?3) 96.90 92.73 96.44 96.99 93.12 96.39
SEMAFOR LEXICON FULL LEXICON
Model All Ambiguous Rare Unseen All Ambiguous Rare
Test Data
Das et al (2014) supervised 82.97 69.27 80.97 23.08
Das et al (2014) best 83.60 69.19 82.31 42.67
LOG-LINEAR WORDS 84.71 70.97 81.70 27.27 87.44 70.97 87.10
LOG-LINEAR EMBEDDING 83.42 68.70 80.95 27.97 86.20 68.70 86.03
WSABIE EMBEDDING (?3) 86.58 73.67 85.04 44.76 88.73 73.67 89.38
Table 2: Frame identification results for FrameNet. See ?5.6.
SEMAFOR LEXICON FULL LEXICON
Model Precision Recall F
1
Precision Recall F
1
Development Data
LOG-LINEAR WORDS 89.43 75.98 82.16 89.41 76.05 82.19
WSABIE EMBEDDING (?3) 89.89 76.40 82.59 89.94 76.27 82.54
Test Data
Das et al supervised 67.81 60.68 64.05
Das et al best 68.33 61.14 64.54
LOG-LINEAR WORDS 71.16 63.56 67.15 73.35 65.27 69.08
WSABIE EMBEDDING (?3) 72.79 64.95 68.64 74.44 66.17 70.06
Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We
skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
We search for the stochastic gradient learning
rate in {0.0001, 0.001, 0.01}, the margin ? ?
{0.001, 0.01, 0.1, 1} and the dimensionality of the
final vector space m ? {256, 512}, to maximize
the frame identification accuracy of ambiguous
lexical units; by ambiguous, we imply lexical units
that appear in the training data or the lexicon with
more than one semantic frame. The underlined
values are the chosen hyperparameters used to an-
alyze the test data.
Argument Candidates The candidate argument
extraction method used for the FrameNet data, (as
mentioned in ?4) was adapted from the algorithm
of Xue and Palmer (2004) applied to dependency
trees. Since the original algorithm was designed
for verbs, we added a few extra rules to handle
non-verbal predicates: we added 1) the predicate
itself as a candidate argument, 2) the span ranging
from the sentence position to the right of the pred-
icate to the rightmost index of the subtree headed
by the predicate?s head; this helped capture cases
like ?a few months? (where few is the predicate and
months is the argument), and 3) the span ranging
from the leftmost index of the subtree headed by
the predicate?s head to the position immediately
before the predicate, for cases like ?your gift to
Goodwill? (where to is the predicate and your gift
is the argument).
10
10
Note that Das et al (2014) describe the state of the art
in FrameNet-based analysis, but their argument identifica-
tion strategy considered all possible dependency subtrees in
Frame Lexicon In our experimental setup, we
scanned the XML files in the ?frames? directory
of the FrameNet 1.5 release, which lists all the
frames, the corresponding roles and the associ-
ated lexical units, and created a frame lexicon to
be used in our frame and argument identification
models. We noted that this renders every lexical
unit as seen; in other words, at frame disambigua-
tion time on our test set, for all instances, we only
had to score the frames in F
`
for a predicate with
lexical unit ` (see ?3 and ?5.2). We call this setup
FULL LEXICON. While comparing with prior state
of the art on the same corpus, we noted that Das et
al. (2014) found several unseen predicates at test
time.
11
For fair comparison, we took the lexical
units for the predicates that Das et al considered
as seen, and constructed a lexicon with only those;
training instances, if any, for the unseen predicates
under Das et al?s setup were thrown out as well.
We call this setup SEMAFOR LEXICON.
12
We also
experimented on the set of unseen instances used
by Das et al
ILP constraints For FrameNet, we used three
ILP constraints during argument identification
(?4). 1) each span could have only one role, 2)
each core role could be present only once, and 3)
all overt arguments had to be non-overlapping.
a parse, resulting in a much larger search space.
11
Instead of using the frame files, Das et al built a frame
lexicon from FrameNet?s exemplars and the training corpus.
12
We got Das et al?s seen predicates from the authors.
1454
Model All Ambiguous Rare
LOG-LINEAR WORDS 94.21 90.54 93.33
LOG-LINEAR EMBEDDING 93.81 89.86 93.73
WSABIE EMBEDDING (?3) 94.79 91.52 92.55
Dev data ? ? Test data
Model All Ambiguous Rare
LOG-LINEAR WORDS 94.74 92.07 91.32
LOG-LINEAR EMBEDDING 94.04 90.95 90.97
WSABIE EMBEDDING (?3) 94.56 91.82 90.62
Table 4: Frame identification accuracy results for PropBank.
The model and the column names have the same semantics
as Table 2.
Model P R F
1
LOG-LINEAR WORDS 80.02 75.58 77.74
WSABIE EMBEDDING (?3) 80.06 75.74 77.84
Dev data ? ? Test data
Model P R F
1
LOG-LINEAR WORDS 81.55 77.83 79.65
WSABIE EMBEDDING (?3) 81.32 77.97 79.61
Table 5: Full frame-structure prediction results for Propbank.
This is a metric that takes into account frames and arguments
together. See ?5.7 for more details.
5.5 Experimental Setup for PropBank
Hyperparameters As in ?5.4, we made a hyper-
parameter sweep in the same space. The chosen
learning rate was 0.01, while the other values were
? = 0.01 and m = 512. Ambiguous lexical units
were used for this selection process.
Argument Candidates For PropBank we use
the algorithm of Xue and Palmer (2004) applied
to dependency trees.
Frame Lexicon For the PropBank experiments
we scanned the frame files for propositions in
Ontonotes 4.0, and stored possible core roles for
each verb frame. The lexical units were simply
the verb associating with the verb frames. There
were no unseen verbs at test time.
ILP constraints We used the constraints of Pun-
yakanok et al (2008).
5.6 FrameNet Results
Table 2 presents accuracy results on frame iden-
tification.
13
We present results on all predicates,
ambiguous predicates seen in the lexicon or the
training data, and rare ambiguous predicates that
appear ? 11 times in the training data. The WS-
ABIE EMBEDDING model from ?3 performs signif-
icantly better than the LOG-LINEAR WORDS base-
line, while LOG-LINEAR EMBEDDING underperforms
in every metric. For the SEMAFOR LEXICON setup,
we also compare with the state of the art from Das
13
We do not report partial frame accuracy that has been
reported by prior work.
Model P R F
1
LOG-LINEAR WORDS 77.29 71.50 74.28
WSABIE EMBEDDING (?3) 77.13 71.32 74.11
Dev data ? ? Test data
Model P R F
1
LOG-LINEAR WORDS 79.47 75.11 77.23
WSABIE EMBEDDING (?3) 79.36 75.04 77.14
Punyakanok et al Collins 75.92 71.45 73.62
Punyakanok et al Charniak 77.09 75.51 76.29
Punyakanok et al Combined 80.53 76.94 78.69
Table 6: Argument only evaluation (semantic role labeling
metrics) using the CoNLL 2005 shared task evaluation script
(Carreras and M`arquez, 2005). Results from Punyakanok et
al. (2008) are taken from Table 11 of that paper.
et al (2014), who used a semi-supervised learn-
ing method to improve upon a supervised latent-
variable log-linear model. For unseen predicates
from the Das et al system, we perform better as
well. Finally, for the FULL LEXICON setting, the ab-
solute accuracy numbers are even better for our
best model. Table 3 presents results on the full
frame-semantic parsing task (measured by a reim-
plementation of the SemEval 2007 shared task
evaluation script) when our argument identifica-
tion model (?4) is used after frame identification.
We notice similar trends as in Table 2, and our re-
sults outperform the previously published best re-
sults, setting a new state of the art.
5.7 PropBank Results
Table 4 shows frame identification results on the
PropBank data. On the development set, our best
model performs with the highest accuracy on all
and ambiguous predicates, but performs worse on
rare ambiguous predicates. On the test set, the
LOG-LINEAR WORDS baseline performs best by a
very narrow margin. See ?6 for a discussion.
Table 5 presents results where we measure pre-
cision, recall and F
1
for frames and arguments to-
gether; this strict metric penalizes arguments for
mismatched frames, like in Table 3. We see the
same trend as in Table 4. Finally, Table 6 presents
SRL results that measures argument performance
only, irrespective of the frame; we use the eval-
uation script from CoNLL 2005 (Carreras and
M`arquez, 2005). We note that with a better frame
identification model, our performance on SRL im-
proves in general. Here, too, the embedding model
barely misses the performance of the best baseline,
but we are at par and sometimes better than the sin-
gle parser setting of a state-of-the-art SRL system
(Punyakanok et al, 2008).
14
14
The last row of Table 6 refers to a system which used the
1455
6 Discussion
For FrameNet, the WSABIE EMBEDDING model we
propose strongly outperforms the baselines on all
metrics, and sets a new state of the art. We be-
lieve that the WSABIE EMBEDDING model performs
better than the LOG-LINEAR EMBEDDING baseline
(that uses the same input representation) because
the former setting allows examples with differ-
ent labels and confusion sets to share informa-
tion; this is due to the fact that all labels live in
the same label space, and a single projection ma-
trix is shared across the examples to map the input
features to this space. Consequently, the WSABIE
EMBEDDING model can share more information be-
tween different examples in the training data than
the LOG-LINEAR EMBEDDING model. Since the LOG-
LINEAR WORDS model always performs better than
the LOG-LINEAR EMBEDDING model, we conclude
that the primary benefit does not come from the
input embedding representation.
15
On the PropBank data, we see that the LOG-
LINEAR WORDS baseline has roughly the same per-
formance as our model on most metrics: slightly
better on the test data and slightly worse on the
development data. This can be partially explained
with the significantly larger training set size for
PropBank, making features based on words more
useful. Another important distinction between
PropBank and FrameNet is that the latter shares
frames between multiple lexical units. The ef-
fect of this is clearly observable from the ?Rare?
column in Table 4. WSABIE EMBEDDING performs
poorly in this setting while LOG-LINEAR EMBEDDING
performs well. Part of the explanation has to do
with the specifics of WSABIE training. Recall that
the WSABIE EMBEDDING model needs to estimate
the label location in R
m
for each frame. In other
words, it must estimate 512 parameters based on
at most 10 training examples. However, since the
input representation is shared across all frames,
every other training example from all the lexical
units affects the optimal estimate, since they all
modify the joint parameter matrixM . By contrast,
in the log-linear models each label has its own
set of parameters, and they interact only via the
normalization constant. The LOG-LINEAR WORDS
model does not have this entanglement, but cannot
share information between words. For PropBank,
combination of two syntactic parsers as input.
15
One could imagine training a WSABIE model with word
features, but we did not perform this experiment.
these drawbacks and benefits balance out and we
see similar performance for LOG-LINEAR WORDS
and LOG-LINEAR EMBEDDING. For FrameNet, esti-
mating the label embedding is not as much of a
problem because even if a lexical unit is rare, the
potential frames can be frequent. For example, we
might have seen the SENDING frame many times,
even though telex.V is a rare lexical unit.
In comparison to prior work on FrameNet, even
our baseline models outperform the previous state
of the art. A particularly interesting comparison is
between our LOG-LINEAR WORDS baseline and the
supervised model of Das et al (2014). They also
use a log-linear model, but they incorporate a la-
tent variable that uses WordNet (Fellbaum, 1998)
to get lexical-semantic relationships and smooths
over frames for ambiguous lexical units. It is
possible that this reduces the model?s power and
causes it to over-generalize. Another difference is
that when training the log-linear model, they nor-
malize over all frames, while we normalize over
the allowed frames for the current lexical unit.
This would tend to encourage their model to ex-
pend more of its modeling power to rule out pos-
sibilities that will be pruned out at test time.
7 Conclusion
We have presented a simple model that outper-
forms the prior state of the art on FrameNet-
style frame-semantic parsing, and performs at par
with one of the previous-best single-parser sys-
tems on PropBank SRL. Unlike Das et al (2014),
our model does not rely on heuristics to con-
struct a similarity graph and leverage WordNet;
hence, in principle it is generalizable to varying
domains, and to other languages. Finally, we pre-
sented results on PropBank-style semantic role la-
beling with a system that included the task of au-
tomatic verb frame identification, in tune with the
FrameNet literature; we believe that such a sys-
tem produces more interpretable output, both from
the perspective of human understanding as well as
downstream applications, than pipelines that are
oblivious to the verb frame, only focusing on ar-
gument analysis.
Acknowledgments
We thank Emily Pitler for comments on an early
draft, and the anonymous reviewers for their valu-
able feedback.
1456
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The berkeley framenet project. In Proceedings of
COLING-ACL.
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction.
In Proceedings of SemEval.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling.
In Proceedings of CoNLL.
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 shared task: semantic role labeling. In
Proceedings of CoNLL.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of
ICML.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010.
Probabilistic frame-semantic parsing. In Proceed-
ings of NAACL-HLT.
D. Das, D. Chen, A. F. T. Martins, N. Schneider, and
N. A. Smith. 2014. Frame-semantic parsing. Com-
putational Linguistics, 40(1):9?56.
M.-C. de Marneffe and C. D. Manning, 2013. Stanford
typed dependencies manual.
C. Fellbaum, editor. 1998. WordNet: an electronic
lexical database.
C. J. Fillmore, C. R. Johnson, and M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3):235?250.
C. J. Fillmore. 1982. Frame Semantics. In Linguis-
tics in the Morning Calm, pages 111?137. Hanshin
Publishing Co., Seoul, South Korea.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90 In Pro-
ceedings of NAACL-HLT.
R. Johansson and P. Nugues. 2007. LTH: semantic
structure extraction using nonprojective dependency
trees. In Proceedings of SemEval.
A. Klementiev, I. Titov, and B. Bhattarai. 2012. In-
ducing crosslingual distributed representations of
words. In Proceedings of COLING.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
D. C. Liu and J. Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503 ? 528.
L. M`arquez, X. Carreras, K. C. Litkowski, and
S. Stevenson. 2008. Semantic role labeling: an in-
troduction to the special issue. Computational Lin-
guistics, 34(2):145?159.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In Pro-
ceedings of NAACL/HLT Workshop on Frontiers in
Corpus Annotation.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-
HLT.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2):257?287.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions.
In Proceedings of EMNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word
representations: A simple and general method for
semi-supervised learning. In Proceedings of ACL,
Stroudsburg, PA, USA.
N. Usunier, D. Buffoni, and P. Gallinari. 2009. Rank-
ing with ordered weighted pairwise classification. In
ICML.
J. Uszkoreit and T. Brants. 2008. Distributed word
clustering for large scale class-based language mod-
eling in machine translation. In Proceedings of
ACL-HLT.
K. Q. Weinberger and L. K. Saul. 2009. Distance met-
ric learning for large margin nearest neighbor clas-
sification. Journal of Machine Learning Research,
10:207?244.
J. Weston, S. Bengio, and N. Usunier. 2011. Wsabie:
Scaling up to large vocabulary image annotation. In
Proceedings of IJCAI.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of EMNLP
2004.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Pro-
ceedings of ACL-HLT.
1457
Number Filename
dev-1 LUCorpus-v0.3 20000420 xin eng-NEW.xml
dev-2 NTI SouthAfrica Introduction.xml
dev-3 LUCorpus-v0.3 CNN AARONBROWN ENG 20051101 215800.partial-NEW.xml
dev-4 LUCorpus-v0.3 AFGP-2002-600045-Trans.xml
dev-5 PropBank TicketSplitting.xml
dev-6 Miscellaneous Hijack.xml
dev-7 LUCorpus-v0.3 artb 004 A1 E1 NEW.xml
dev-8 NTI WMDNews 042106.xml
dev-9 C-4 C-4Text.xml
dev-10 ANC EntrepreneurAsMadonna.xml
dev-11 NTI LibyaCountry1.xml
dev-12 NTI NorthKorea NuclearOverview.xml
dev-13 LUCorpus-v0.3 20000424 nyt-NEW.xml
dev-14 NTI WMDNews 062606.xml
dev-15 ANC 110CYL070.xml
dev-16 LUCorpus-v0.3 CNN ENG 20030614 173123.4-NEW-1.xml
Table 7: List of files used as development set for the FrameNet 1.5 corpus.
A Development Data
Table 7 features a list of the 16 randomly selected
documents from the FrameNet 1.5 corpus, which
we used for development. The resultant develop-
ment set consists of roughly 4,500 predicates. We
use the same test set as in Das et al (2014), con-
taining 23 documents and 4,458 predicates.
1458
