Modern Natural Language Interfaces to Databases:
Composing Statistical Parsing with Semantic Tractability
Ana-Maria Popescu Alex Armanasu Oren Etzioni
University of Washington
{amp, alexarm, etzioni, daveko, ayates}@cs.washington.edu
David Ko Alexander Yates
Abstract
Natural Language Interfaces to Databases
(NLIs) can benefit from the advances in statis-
tical parsing over the last fifteen years or so.
However, statistical parsers require training on
a massive, labeled corpus, and manually cre-
ating such a corpus for each database is pro-
hibitively expensive. To address this quandary,
this paper reports on the PRECISE NLI, which
uses a statistical parser as a ?plug in?. The pa-
per shows how a strong semantic model cou-
pled with ?light re-training? enables PRECISE
to overcome parser errors, and correctly map
from parsed questions to the corresponding
SQL queries. We discuss the issues in using
statistical parsers to build database-independent
NLIs, and report on experimental results with
the benchmark ATIS data set where PRECISE
achieves 94% accuracy.
1 Introduction and Motivation
Over the last fifteen years or so, much of the NLP
community has focused on the use of statistical
and machine learning techniques to solve a wide
range of problems in parsing, machine translation,
and more. Yet, classical problems such as building
Natural Language Interfaces to Databases (NLIs)
(Grosz et al, 1987) are far from solved.
There are many reasons for the limited success of
past NLI efforts (Androutsopoulos et al, 1995). We
highlight several problems that are remedied by our
approach. First, manually authoring and tuning a se-
mantic grammar for each new database is brittle and
prohibitively expensive. In response, we have im-
plemented a ?transportable? NLI that aims to mini-
mize manual, database-specific configuration. Sec-
ond, NLI systems built in the 70s and 80s had lim-
ited syntactic parsing capabilities. Thus, we have an
opportunity to incorporate the important advances
made by statistical parsers over the last two decades
in an NLI.
However, attempting to use a statistical parser in
a database-independent NLI leads to a quandary. On
the one hand, to parse questions posed to a particu-
lar database, the parser has to be trained on a corpus
of questions specific to that database. Otherwise,
many of the parser?s decisions will be incorrect. For
example, the Charniak parser (trained on the 40,000
sentences in the WSJ portion of the Penn Treebank)
treats ?list? as a noun, but in the context of the ATIS
database it is a verb.1 On the other hand, manually
creating and labeling a massive corpus of questions
for each database is prohibitively expensive.
We consider two methods of resolving this
quandary and assess their performance individually
and in concert on the ATIS data set. First, we use
a strong semantic model to correct parsing errors.
We introduce a theoretical framework for discrim-
inating between Semantically Tractable (ST) ques-
tions and difficult ones, and we show that ST ques-
tions are prevalent in the well-studied ATIS data
set (Price, 1990). Thus, we show that the seman-
tic component of the NLI task can be surprisingly
easy and can be used to compensate for syntactic
parsing errors. Second, we re-train the parser using
a relatively small set of 150 questions, where each
word is labeled by its part-of-speech tag.
To demonstrate how these methods work in prac-
tice, we sketch the fully-implemented PRECISE
NLI, where a parser is a modular ?plug in?. This
modularity enables PRECISE to leverage continuing
advances in parsing technology over time by plug-
ging in improved parsers as they become available.
The remainder of this paper is organized as fol-
lows. We describe PRECISE in Section 2, sketch our
theory in Section 3, and report on our experiments
in Section 4. We consider related work in Section 5,
and conclude in Section 6.
2 The PRECISE System Overview
Our recent paper (Popescu et al, 2003) introduced
the PRECISE architecture and its core algorithm for
1This is an instance of a well known machine learning prin-
ciple ? typically, a learning algorithm is effective when its test
examples are drawn from roughly the same distribution as its
training examples.
reducing semantic interpretation to a graph match-
ing problem that is solved by MaxFlow. In this sec-
tion we provide a brief overview of PRECISE, focus-
ing on the components necessary to understanding
its performance on the ATIS data set in Section 4.
To discuss PRECISE further, we must first intro-
duce some terminology. We say that a database is
made up of three types of elements: relations, at-
tributes and values. Each element is unique: an at-
tribute element is a particular column in a particular
relation and each value element is the value of a
particular attribute. A value is compatible with its
attribute and also with the relation containing this
attribute. An attribute is compatible with its rela-
tion. Each attribute in the database has associated
with it a special value, which we call a wh-value,
that corresponds to a wh-word (what, where, etc.).
We define a lexicon as a tuple (T, E, M), where
T is a set of strings, called tokens (intuitively, tokens
are strings of one or more words, like ?New York?);
E is a set of database elements, wh-values, and join
paths; 2 and M is a subset of T ? E ? a binary
relation between tokens and database elements.
PRECISE takes as input a lexicon and a parser.
Then, given an English question, PRECISE maps it
to one (or more) corresponding SQL queries. We
concisely review how PRECISE works through a
simple example. Consider the following question
q: ?What are the flights from Boston to Chicago??
First, the parser plug-in automatically derives a
dependency analysis for q from q?s parse tree,
represented by the following compact syntactic log-
ical form: LF (q) = what(0), is(0, 1), f light(1),
from(1, 2), boston(2), to(1, 3), chicago(3).
LF (q) contains a predicate for each question word.
Head nouns correspond to unary predicates whose
arguments are constant identifiers.
Dependencies are encoded by equality con-
straints between arguments to different predicates.
The first type of dependency is represented by noun
and adjective pre-modifiers corresponding to unary
predicates whose arguments are the identifiers for
the respective modified head nouns. A second type
of dependency is represented by noun postmodifiers
and mediated by prepositions (in the above exam-
ple, ?from? and ?to?). The prepositions correspond
to binary predicates whose arguments specify the at-
tached noun phrases. For instance, ?from? attaches
?flight? to ?boston?. Finally, subject/predicate,
predicate/direct object and predicate/indirect object
dependency information is computed for the various
2A join path is a set of equality constraints between the at-
tributes of two or more tables. See Section 3 for more details
and a formal definition.
verbs present in the question. Verbs correspond to
binary or tertiary predicates whose arguments indi-
cate what noun phrases play the subject and object
roles. In our example, the verb ?is? mediates the
dependency between ?what? and ?flight?. 3
PRECISE?s lexicon is generated by automatically
extracting value, attribute, and relation names from
the database. We manually augmented the lexicon
with relevant synonyms, prepositions, etc..
The tokenizer produces a single complete
tokenization of this question and lemmatizes
the tokens: (what, is, flight, from,
boston, to, chicago). By looking up the
tokens in the lexicon, PRECISE efficiently retrieves
the set of potentially matching database elements
for every token. In this case, what, boston and
chicago are value tokens, to and from are at-
tribute tokens and flight is a relation token.
In addition to this information, the lexicon also
contains a set of restrictions for tokens that are
prepositions or verbs. The restrictions specify the
database elements that are allowed to match to the
arguments of the respective preposition or verb. For
example, from can take as arguments a flight and
a city. The restrictions also specify the join paths
connecting these relations/attributes. The syntactic
logical form is used to retrieve the relevant set of
restrictions for a given question.
The matcher takes as input the information de-
scribed above and reduces the problem of satisfy-
ing the semantic constraints imposed by the defi-
nition of a valid interpretation to a graph matching
problem (Popescu et al, 2003). In order for each
attribute token to match a value token, Boston
and Chicago map to the respective values of the
database attribute city.cityName, from maps to
flight.fromAirport or fare.fromAirport and to
maps to flight.toAirport or fare.toAirport. The
restrictions validate the output of the matcher and
are then used in combination with the syntactic in-
formation to narrow down even further the possi-
ble interpretations for each token by enforcing lo-
cal dependencies. For example, the syntactic in-
formation tells us that ?from? refers to ?flight? and
since ?flight? uniquely maps to flight, this means
that from will map to flight.fromAirport rather
than fare.fromAirport (similarly, to maps to
flight.toAirport and whatmaps to flight.flightId).
Finally, the matcher compiles a list of all relations
satisfying all the clauses in the syntactic logical
form using each constant and narrows down the set
3PRECISE uses a larger set of constraints on dependency
relations, but for brevity, we focus on those relevant to our ex-
amples.
of possible interpretations for each token accord-
ingly. Each set of (constant, corresponding database
element) pairs represents a semantic logical form.
The query generator takes each semantic logical
form and uses the join path information available in
the restrictions to form the final SQL queries corre-
sponding to each semantic interpretation.
pronoun verb noun prep noun prep noun prep noun
NP NP
NP
PP
PP
PP
NPNP NP
VP
S
NP
WhatareflightsfromBostontoChicagoonMonday?
Figure 1: Example of an erroneous parse tree corrected
by PRECISE?s semantic over-rides. PRECISE detects that the
parser attached the PP ?on Monday? to ?Chicago? in error.
PRECISE attempts to re-attach ?on Monday? first to the PP
?to Chicago?, and then to the NP ?flights from Boston to
Chicago?, where it belongs.
2.1 Parser Enhancements
We used the Charniak parser (Charniak, 2000) for
the experiments reported in this paper. We found
that the Charniak parser, which was trained on
the WSJ corpus, yielded numerous syntactic errors.
Our first step was to hand tag a set of 150 questions
with Part Of Speech (POS) tags, and re-train the
parser?s POS tagger. As a result, the probabilities
associated with certain tags changed dramatically.
For example, initially, ?list? was consistently tagged
as a noun, but after re-training it was consistently la-
beled as a verb. This change occurs because, in the
ATIS domain, ?list? typically occurs in imperative
sentences, such as ?List all flights.?
Focusing exclusively on the tagger drastically re-
duced the amount of data necessary for re-training.
Whereas the Charniak parser was originally trained
on close to 40,000 sentences, we only required 150
sentences for re-training. Unfortunately, the re-
trained parser still made errors when solving dif-
ficult syntactic problems, most notably preposition
attachment and preposition ellipsis. PRECISE cor-
rects both types of errors using semantic informa-
tion.
We refer to PRECISE?s use of semantic informa-
tion to correct parser errors as semantic over-rides.
Specifically, PRECISE detects that an attachment de-
cision made by the parser is inconsistent with the
semantic information in its lexicon.4 When this oc-
curs, PRECISE attempts to repair the parse tree as
follows. Given a noun phrase or a prepositional
phrase whose corresponding node n in the parse tree
has the wrong parent p, PRECISE traverses the path
in the parse tree from p to the root node, search-
ing for a suitable node to attach n to. PRECISE
chooses the first ancestor of p such that when n is
attached to the new node, the modified parse tree
agrees with PRECISE?s semantic model. Thus, the
semantic over-ride procedure is a generate-and-test
search where potential solutions are generated in the
order of ancestors of node n in the parse tree. The
procedure?s running time is linear in the depth of the
parse tree.
Consider, for example, the question ?What are
flights from Boston to Chicago on Monday?? The
parser attaches the prepositional phrase ?on Mon-
day? to ?Chicago? whereas it should be attached to
?flights? (see Figure 1). The parser merely knows
that ?flights?, ?Boston?, and ?Chicago? are nouns. It
then uses statistics to decide that ?on Monday? is
most likely to attach to ?Chicago?. However, this
syntactic decision is inconsistent with the semantic
information in PRECISE?s lexicon ? the preposition
?on? does not take a city and a day as arguments,
rather it takes a flight and a day.
Thus, PRECISE decides to over-ride the parser
and attach ?on? elsewhere. As shown in Figure
1, PRECISE detects that the parser attached the PP
?on Monday? to ?Chicago? in error. PRECISE at-
tempts to re-attach ?on Monday? first to the PP ?to
Chicago?, and then to the NP ?flights from Boston
to Chicago?, where it belongs. While in our ex-
ample the parser violated a constraint in PRECISE?s
lexicon, the violation of any semantic constraint will
trigger the over-ride procedure.
In the above example, we saw how semantic over-
rides help PRECISE fix prepositional attachment er-
rors; they also enable it to correct parser errors
in topicalized questions (e.g., ?What are Boston to
Chicago flights??) and in preposition ellipsis (e.g.,
when ?on? is omitted in the question ?What are
flights from Boston to Chicago Monday??).
Unfortunately, semantic over-rides do not correct
all of the parser?s errors. Most of the remaining
parser errors fall into the following categories: rel-
ative clause attachment, verb attachment, numeric
4We say that node n is attached to node p if p is the parent
of n in the parse tree.
noun phrases, and topicalized prepositional phrases.
In general, semantic over-rides can correct local at-
tachment errors, but cannot over-come more global
problems in the parse tree. Thus, PRECISE can be
forced to give up and ask the user to paraphrase her
question.
3 PRECISE Theory
The aim of this section is to explain the theoretical
under-pinnings of PRECISE?s semantic model. We
show that PRECISE always answers questions from
the class of Semantically Tractable (ST) questions
correctly, given correct lexical and syntactic infor-
mation.5
We begin by introducing some terminology that
builds on the definitions given Section 2.
3.1 Definitions
A join path is a set of equality constraints between
a sequence of database relations. More formally, a
join path for relations R1, . . . , Rn is a set of con-
straints C ? {Ri.a = Ri+1.b|1 ? i ? n?1}. Here
the notation Ri.a refers to the value of attribute a in
relation Ri.
We say a relation between token set T and a set
of database elements and join paths E respects a
lexicon L if it is a subset of M .
A question is simply a string of characters. A to-
kenization of a question (with respect to a lexicon)
is an ordered set of strings such that each element
of the tokenization is an element of the lexicon?s to-
ken set, and the concatenation of the elements of the
tokenization, in order, is equal to the original ques-
tion. For a given lexicon and question, there may
be zero, one, or several tokenizations. Any question
that has at least one tokenization is tokenizable.
An attachment function is a function FL,q : T ?
T , where L is the lexicon, q is a question, and T
is the set of tokens in the lexicon. The attachment
function is meant to represent dependency informa-
tion available to PRECISE through a parser. For
example, if a question includes the phrase ?restau-
rants in Seattle?, the attachment function would at-
tach ?Seattle? to ?restaurants? for this question. Not
all tokens are attached to something in every ques-
tion, so the attachment function is not a total func-
tion. We say that a relation R between tokens
in a question q respects the attachment function if
?t1, t2, R(t1, t2) ? (FL,q(t1) = t2) ? (FL,q does
not take on a value for t1).
5We do not claim that NLI users will restrict their questions
to the ST subset of English in practice, but rather that identify-
ing classes of questions as semantically tractable (or not), and
experimentally measuring the prevalence of such questions, is
a worthwhile avenue for NLI research.
In an NLI, interpretations of a question are SQL
statements. We define a valid interpretation of a
question as being an SQL statement that satisfies a
number of conditions connecting it to the tokens in
the question. Because of space constraints, we pro-
vide only one such constraint as an example: There
exists a tokenization t of the question and a set of
database elements E such that there is a one-to-one
map from t to E respecting the lexicon, and for each
value element v ? E, there is exactly one equality
constraint in the SQL clause that uses v.
For a complete definition of a valid interpretation,
see (Popescu et al, 2003).
3.2 Semantic Tractability Model
In this section we formally define the class of
ST questions, and show that PRECISE can prov-
ably map such questions to the corresponding SQL
queries. Intuitively, ST questions are ?easy to un-
derstand? questions where the words or phrases
correspond to database elements or constraints on
join paths. Examining multiple questions sets and
databases, we have found that nouns, adjectives, and
adverbs in ?easy? questions refer to database rela-
tions, attributes, or values.
Moreover, the attributes and values in a question
?pair up? naturally to indicate equality constraints in
SQL. However, values may be paired with implicit
attributes that do not appear in the question (e.g., the
attribute ?cuisine? in ?What are the Chinese restau-
rants in Seattle?? is implicit). Interestingly, there is
no notion of ?implicit value? ? the question ?What
are restaurants with cuisine in Seattle?? does not
make sense.
A preposition indicates a join between the rela-
tions corresponding to the arguments of the prepo-
sition. For example, consider the preposition ?from?
in the question ?what airlines fly from Boston to
Chicago?? ?from? connects the value ?Boston? (in
the relation ?cities?) to the relation ?airlines?. Thus,
we know that the corresponding SQL query will join
?airlines? and ?cities?.
We formalize these observations about questions
below. We say that a question q is semantically
tractable using lexicon L and attachment function
FL,q if:
1. It is possible to split q up into words and
phrases found in L. (More formally, q is to-
kenizable according to L.)
2. While words may have multiple meanings in
the lexicon, it must be possible to find a one-
to-one correspondence between tokens in the
question and some set of database elements.
(More formally, there exists a tokenization t
and a set of database elements and join paths
Et such that there is a bijective function f from
t to Et that respects L.)
3. There is at least one such set Et that has exactly
one wh-value.
4. It is possible to add ?implicit? attributes to Et
to get a set E ?t with exactly one compatible
attribute for every value. (More formally, for
some Et with a wh-value there exist attributes
a1, . . . , an such that E ?t = Et ? {a1, . . . , an}
and there is a bijective function g from the set
of value elements (including wh-values) V to
the set of attribute elements A in E ?t.)
5. At least one such E ?t obeys the syntactic
restrictions of FL,q. (More formally, let
A? = A ? Et. Then we require that
{(f?1(g?1(a)), f?1(a)) | a ? A?} respects
FL,q.)
3.3 Results and Discussion
We say that an NLI is sound for a class of questions
Q using lexicon L and attachment function FL if
for every input q ? Q, every output of the NLI is a
valid interpretation. We say the NLI is complete if
it returns all valid interpretations. Our main result is
the following:
Theorem 1 Given a lexicon L and attachment
function FL, PRECISE is sound and complete for the
class of semantically tractable questions.
In practical terms, the theorem states that given
correct and complete syntactic and lexical informa-
tion, PRECISE will return exactly the set of valid
interpretations of a question. If PRECISE is missing
syntactic or semantic constraints, it can generate ex-
traneous interpretations that it ?believes? are valid.
Also, if a person uses a term in a manner incon-
sistent with PRECISE?s lexicon, then PRECISE will
interpret her question incorrectly. Finally, PRECISE
will not answer a question that contains words ab-
sent from its lexicon.
The theorem is clearly an idealization, but the ex-
periments reported in Section 4 provide evidence
that it is a useful idealization. PRECISE, which em-
bodies the model of semantic tractability, achieves
very high accuracy because in practice it either has
correct and complete lexical and syntactic informa-
tion or it has enough semantic information to com-
pensate for its imperfect inputs. In fact, as we ex-
plained in Section 2.1, PRECISE?s semantic model
enables it to correct parser errors in some cases.
Finding all the valid interpretations for a question
is computationally expensive in the worst case (even
just tokenizing a question is NP-complete (Popescu
et al, 2003)). Moreover, if the various syntac-
tic and semantic constraints are fed to a standard
constraint solver, then the problem of finding even
a single valid interpretation is exponential in the
worst case. However, we have been able to formu-
late PRECISE?s constraint satisfaction problem as a
graph matching problem that is solved in polyno-
mial time by the MaxFlow algorithm:
Theorem 2 For lexicon L, PRECISE finds one valid
interpretation for a tokenization T of a semantically
tractable question in time O(Mn2), where n is the
number of tokens in T and M is the maximum num-
ber of interpretations that a token can have in L.
4 Experimental Evaluation
Semantic Tractability (ST) theory and PRECISE?s
architecture raise a four empirical questions that
we now address via experiments on the ATIS data
set (Price, 1990): how prevalent are ST questions?
How effective is PRECISE in mapping ATIS ques-
tions to SQL queries? What is the impact of se-
mantic over-rides? What is the impact of parser re-
training? Our experiments utilized the 448 context-
independent questions in the ATIS ?Scoring Set A?.
We chose the ATIS data set because it is a standard
benchmark (see Table 2) where independently gen-
erated questions are available to test the efficacy of
an NLI.
We found that 95.8% of the ATIS questions were
ST questions. We classified each question as ST
(or not) by running PRECISE on the question and
System Setup PRECISE PRECISE-1
ParserORIG 61.9% 60.3%
ParserORIG+ 89.7% 85.5%
ParserTRAINED 92.4% 88.2%
ParserTRAINED+ 94.0% 89.2%
ParserCORRECT 95.8% 91.9%
Table 1: Impact of Parser Enhancements. The PRECISE
column records the percentage of questions where the small
set of SQL queries returned by PRECISE contains the cor-
rect query; PRECISE-1 refers to the questions correctly in-
terpreted if PRECISE is forced to return exactly one SQL
query. ParserORIG is the original version of the parser,
ParserTRAINED is the version re-trained for the ATIS do-
main, and ParserCORRECT is the version whose output is
corrected manually. System configurations marked by +
indicate the automatic use of semantic over-rides to correct
parser errors.
PRECISE PRECISE-1 AT&T CMU MIT SRI BBN UNISYS MITRE HEY
94.0% 89.1% 96.2% 96.2% 95.5% 93% 90.6% 76.4% 69.4% 92.5%
Table 2: Accuracy Comparison between PRECISE , PRECISE-1 and the major ATIS NLIs. Only PRECISE and the HEY NLI
are database independent. All results are for performance on the context-independent questions in ATIS.
recording its response. Intractable questions were
due to PRECISE?s incomplete semantic informa-
tion. Consider, for example, the ATIS request ?List
flights from Oakland to Salt Lake City leaving after
midnight Thursday.? PRECISE fails to answer this
question because it lacks a model of time, and so
cannot infer that ?after midnight Thursday? means
?early Friday morning.?
In addition, we found that the prevalence of ST
questions in the ATIS data is consistent with our ear-
lier results on the set of 1,800 natural language ques-
tions compiled by Ray Mooney in his experiments
in three domains (Tang and Mooney, 2001). As re-
ported in (Popescu et al, 2003), we found that ap-
proximately 80% of Mooney?s questions were ST.
PRECISE performance on the ATIS data was also
comparable to its performance on the Mooney data
sets.
Table 1 quantifies the impact of the parser en-
hancements discussed in Section 2.1. Since PRE-
CISE can return multiple distinct SQL queries when
it judges a question to be ambiguous, we report its
results in two columns. The left column (PRECISE)
records the percentage of questions where the set
of returned SQL queries contains the correct query.
The right column (PRECISE-1) records the percent-
age of questions where PRECISE is correct if it is
forced to return exactly one query per question. In
our experiments, PRECISE returned a single query
92.4% of the time, and returned two queries the rest
of the time. Thus, the difference between the two
columns is not great.
Initially, plugging the Charniak parser into PRE-
CISE yielded only 61.9% accuracy. Introducing se-
mantic over-rides to correct prepositional attach-
ment and preposition ellipsis errors increased PRE-
CISE?s accuracy to 89.7% ? the parser?s erroneous
POS tags still led PRECISE astray in some cases.
After re-training the parser on 150 POS-tagged
ATIS questions, but without utilizing semantic over-
rides, PRECISE achieved 92.4% accuracy. Combin-
ing both re-training and semantic over-rides, PRE-
CISE achieved 94.0% accuracy. This accuracy is
close to the maximum that PRECISE can achieve,
given its incomplete semantic information? we
found that, when all parsing errors are corrected by
hand, PRECISE?s accuracy is 95.8%.
To assess PRECISE?s performance, we compared
it with previous work. Table 2 shows PRECISE?s
accuracy compared with the most successful ATIS
NLIs (Minker, 1998). We also include, for com-
parison, the more recent database-independent HEY
system (He and Young, 2003). All systems were
compared on the ATIS scoring set ?A?, but we
did ?clean? the questions by introducing sentence
breaks, removing verbal errors, etc.. Since we could
add modules to PRECISE to automatically handle
these various cases, we don?t view this as signifi-
cant.
Given the database-specific nature of most previ-
ous ATIS systems, it is remarkable that PRECISE is
able to achieve comparable accuracy. PRECISE does
return two interpretations a small percentage of the
time. However, even when restricted to returning
a single interpretation, PRECISE-1 still achieved an
impressive 89.1% accuracy (Table 1).
5 Related Work
We discuss related work in three categories:
Database-independent NLIs, ATIS-specific NLIs,
and sublanguages.
Database-independent NLIs There has been ex-
tensive previous work on NLIs (Androutsopoulos et
al., 1995), but three key elements distinguish PRE-
CISE. First, we introduce a model of ST questions
and show that it produces provably correct inter-
pretations of questions (subject to the assumptions
of the model). We measure the prevalence of ST
questions to demonstrate the practical import of our
model. Second, we are the first to use a statistical
parser as a ?plug in?, experimentally measure its
efficacy, and analyze the attendant challenges. Fi-
nally, we show how to leverage our semantic model
to correct parser errors in difficult syntactic cases
(e.g., prepositional attachment). A more detailed
comparison of PRECISE with a wide range of NLI
systems appears in (Popescu et al, 2003). The
advances in this paper over our previous one in-
clude: reformulation of ST THEORY, the parser re-
training, semantic over-rides, and the experiments
testing PRECISE on the ATIS data.
ATIS NLIs The typical ATIS NLIs used either
domain-specific semantic grammars (Seneff, 1992;
Ward and Issar, 1996) or stochastic models that re-
quired fully annotated domain-specific corpora for
reliable parameter estimation (Levin and Pieraccini,
1995). In contrast, since it uses its model of se-
mantically tractable questions, PRECISE does not
require heavy manual processing and only a small
number of annotated questions. In addition, PRE-
CISE leverages existing domain-independent pars-
ing technology and offers theoretical guarantees ab-
sent from other work. Improved versions of ATIS
systems such as Gemini (Moore et al, 1995) in-
creased their coverage by allowing an approximate
question interpretation to be computed from the
meanings of some question fragments. Since PRE-
CISE focuses on high precision rather than recall, we
analyze every word in the question and interpret the
question as a whole. Most recently, (He and Young,
2003) introduced the HEY system, which learns a
semantic parser without requiring fully-annotated
corpora. HEY uses a hierarchical semantic parser
that is trained on a set of questions together with
their corresponding SQL queries. HEY is similar to
(Tang and Mooney, 2001). Both learning systems
require a large set of questions labeled by their SQL
queries?an expensive input that PRECISE does not
require?and, unlike PRECISE, both systems can-
not leverage continuing improvements to statistical
parsers.
Sublanguages The early work with the most sim-
ilarities to PRECISE was done in the field of sublan-
guages. Traditional sublanguage work (Kittredge,
1982) has looked at defining sublanguages for var-
ious domains, while more recent work (Grishman,
2001; Sekine, 1994) suggests using AI techniques
to learn aspects of sublanguages automatically. Our
work can be viewed as a generalization of tradi-
tional sublanguage research. We restrict ourselves
to the semantically tractable subset of English rather
than to a particular knowledge domain. Finally, in
addition to offering formal guarantees, we assess the
prevalence of our ?sublanguage? in the ATIS data.
6 Conclusion
This paper is the first to provide evidence that sta-
tistical parsers can support NLIs such as PRECISE.
We identified the quandary associated with appro-
priately training a statistical parser: without special
training for each database, the parser makes numer-
ous errors, but creating a massive, labeled corpus of
questions for each database is prohibitively expen-
sive. We solved this quandary via light re-training
of the parser?s tagger and via PRECISE?s semantic
over-rides, and showed that in concert these meth-
ods enable PRECISE to rise from 61.9% accuracy to
94% accuracy on the ATIS data set. Even though
PRECISE is database independent, its accuracy is
comparable to the best of the database-specific ATIS
NLIs developed in previous work (Table 2).
References
I. Androutsopoulos, G. D. Ritchie, and P. Thanisch.
1995. Natural Language Interfaces to Databases - An
Introduction. In Natural Language Engineering, vol
1, part 1, pages 29?81.
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of NAACL-2000.
R. Grishman. 2001. Adaptive information extraction
and sublanguage analysis. In Proc. of IJCAI 2001.
B.J. Grosz, D. Appelt, P. Martin, and F. Pereira. 1987.
TEAM: An Experiment in the Design of Trans-
portable Natural Language Interfaces. In Artificial In-
telligence 32, pages 173?243.
Y. He and S. Young. 2003. A data-driven spoken lan-
guage understanding system. In IEEE Workshop on
Automatic Speech Recognition and Understanding.
R. Kittredge. 1982. Variation and homogeneity of sub-
languages. In R. Kittredge and J. Lehrberger, editors,
Sublanguage: Studies of Language in Restricted Se-
mantic Domains, pages 107?137. de Gruyter, Berlin.
E. Levin and R. Pieraccini. 1995. Chronus, the next gen-
eration. In Proc. of the DARPA Speech and Natural
Language Workshop, pages 269?271.
W. Minker. 1998. Evaluation methodologies for inter-
active speech systems. In First International Confer-
ence on Language Resources and Evaluation, pages
801?805.
R. Moore, D. Appelt, J. Dowding, J. M. Gawron, and
D. Moran. 1995. Combining linguistic and statistical
knowledge sources in natural-language processing for
atis. In Proc. of the ARPA Spoken Language Technol-
ogy Workshop.
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards a
theory of natural language interfaces to databases. In
Proc. of IUI-2003.
P. Price. 1990. Evaluation of spoken language systems:
the atis domain. In Proc. of the DARPA Speech and
Natural Language Workshop, pages 91?95.
S. Sekine. 1994. A New Direction For Sublanguage
NLP. In Proc. of the International Conference on New
Methods in Language Processing, pages 165?177.
S. Seneff. 1992. Robust parsing for spoken language
systems. In Proc. of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing.
L.R. Tang and R.J. Mooney. 2001. Using Multiple
Clause Constructors in Inductive Logic Programming
for Semantic Parsing. In Proc. of the 12th Eu-
ropean Conference on Machine Learning (ECML-
2001), Freiburg, Germany, pages 466?477.
W. Ward and S. Issar. 1996. Recent improvements in the
cmu spoken language understanding system. In Proc.
of the ARPA Human Language Technology Workshop,
pages 213?216.
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 11?20,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
It?s a Contradiction?No, it?s Not:
A Case Study using Functional Relations
Alan Ritter, Doug Downey, Stephen Soderland and Oren Etzioni
Turing Center
Department of Computer Science and Engineering
University of Washington
Box 352350
Seattle, WA 98195, USA
{aritter,ddowney,soderlan,etzioni}@cs.washington.edu
Abstract
Contradiction Detection (CD) in text is a
difficult NLP task. We investigate CD
over functions (e.g., BornIn(Person)=Place),
and present a domain-independent algorithm
that automatically discovers phrases denoting
functions with high precision. Previous work
on CD has investigated hand-chosen sentence
pairs. In contrast, we automatically harvested
from the Web pairs of sentences that appear
contradictory, but were surprised to find that
most pairs are in fact consistent. For example,
?Mozart was born in Salzburg? does not con-
tradict ?Mozart was born in Austria? despite
the functional nature of the phrase ?was born
in?. We show that background knowledge
about meronyms (e.g., Salzburg is in Austria),
synonyms, functions, and more is essential for
success in the CD task.
1 Introduction and Motivation
Detecting contradictory statements is an important
and challenging NLP task with a wide range of
potential applications including analysis of politi-
cal discourse, of scientific literature, and more (de
Marneffe et al, 2008; Condoravdi et al, 2003;
Harabagiu et al, 2006). De Marneffe et al present a
model of CD that defines the task, analyzes different
types of contradictions, and reports on a CD system.
They report 23% precision and 19% recall at detect-
ing contradictions in the RTE-3 data set (Voorhees,
2008). Although RTE-3 contains a wide variety of
contradictions, it does not reflect the prevalence of
seeming contradictions and the paucity of genuine
contradictions, which we have found in our corpus.
1.1 Contradictions and World Knowledge
Our paper is motivated in part by de Marneffe et al?s
work, but with some important differences. First,
we introduce a simple logical foundation for the CD
task, which suggests that extensive world knowl-
edge is essential for building a domain-independent
CD system. Second, we automatically generate a
large corpus of apparent contradictions found in ar-
bitrary Web text. We show that most of these appar-
ent contradictions are actually consistent statements
due to meronyms (Alan Turing was born in London
and in England), synonyms (George Bush is mar-
ried to both Mrs. Bush and Laura Bush), hypernyms
(Mozart died of both renal failure and kidney dis-
ease), and reference ambiguity (one John Smith was
born in 1997 and a different John Smith in 1883).
Next, we show how background knowledge enables
a CD system to discard seeming contradictions and
focus on genuine ones.
De Marneffe et al introduced a typology of con-
tradiction in text, but focused primarily on contra-
dictions that can be detected from linguistic evi-
dence (e.g. negation, antonymy, and structural or
lexical disagreements). We extend their analysis to
a class of contradictions that can only be detected
utilizing background knowledge. Consider for ex-
ample the following sentences:
1) ?Mozart was born in Salzburg.?
2) ?Mozart was born in Vienna.?
3) ?Mozart visited Salzburg.?
4) ?Mozart visited Vienna.?
Sentences 1 & 2 are contradictory, but 3 & 4 are
not. Why is that? The distinction is not syntactic.
Rather, sentences 1 and 2 are contradictory because
11
the relation expressed by the phrase ?was born in?
can be characterized here as a function from peo-
ple?s names to their unique birthplaces. In contrast,
?visited? does not denote a functional relation.1
We cannot assume that a CD system knows, in
advance, all the functional relations that might ap-
pear in a corpus. Thus, a central challenge for a
function-based CD system is to determine which re-
lations are functional based on a corpus. Intuitively,
we might expect that ?functional phrases? such as
?was born in? would typically map person names
to unique place names, making function detection
easy. But, in fact, function detection is surprisingly
difficult because name ambiguity (e.g., John Smith),
common nouns (e.g., ?dad? or ?mom?), definite de-
scriptions (e.g., ?the president?), and other linguistic
phenomena can mask functions in text. For example,
the two sentences ?John Smith was born in 1997.?
and ?John Smith was born in 1883.? can be viewed
as either evidence that ?was born in? does not de-
note a function or, alternatively, that ?John Smith?
is ambiguous.
1.2 A CD System Based on Functions
We report on the AUCONTRAIRE CD system, which
addresses each of the above challenges. First, AU-
CONTRAIRE identifies ?functional phrases? statis-
tically (Section 3). Second, AUCONTRAIRE uses
these phrases to automatically create a large cor-
pus of apparent contradictions (Section 4.2). Fi-
nally, AUCONTRAIRE sifts through this corpus to
find genuine contradictions using knowledge about
synonymy, meronymy, argument types, and ambi-
guity (Section 4.3).
Instead of analyzing sentences directly, AUCON-
TRAIRE relies on the TEXTRUNNER Open Informa-
tion Extraction system (Banko et al, 2007; Banko
and Etzioni, 2008) to map each sentence to one or
more tuples that represent the entities in the sen-
tences and the relationships between them (e.g.,
was born in(Mozart,Salzburg)). Using extracted tu-
ples greatly simplifies the CD task, because nu-
merous syntactic problems (e.g., anaphora, rela-
tive clauses) and semantic challenges (e.g., quantifi-
cation, counterfactuals, temporal qualification) are
1Although we focus on function-based CD in our case study,
we believe that our observations apply to other types of CD as
well.
delegated to TEXTRUNNER or simply ignored. Nev-
ertheless, extracted tuples are a convenient approxi-
mation of sentence content, which enables us to fo-
cus on function detection and function-based CD.
Our contributions are the following:
? We present a novel model of the Contradiction
Detection (CD) task, which offers a simple log-
ical foundation for the task and emphasizes the
central role of background knowledge.
? We introduce and evaluate a new EM-style al-
gorithm for detecting whether phrases denote
functional relations and whether nouns (e.g.,
?dad?) are ambiguous, which enables a CD sys-
tem to identify functions in arbitrary domains.
? We automatically generate a corpus of seem-
ing contradictions from Web text, and report
on a set of experiments over this corpus, which
provide a baseline for future work on statistical
function identification and CD. 2
2 A Logical Foundation for CD
On what basis can a CD system conclude that two
statements T and H are contradictory? Logically,
contradiction holds when T |= ?H . As de Marneffe
et al point out, this occurs when T and H contain
antonyms, negation, or other lexical elements that
suggest that T and H are directly contradictory. But
other types of contradictions can only be detected
with the help of a body of background knowledge
K: In these cases, T and H alone are mutually con-
sistent. That is,
T |=\ ?H ?H |=\ ?T
A contradiction between T and H arises only in
the context of K. That is:
((K ? T ) |= ?H) ? ((K ?H) |= ?T )
Consider the example of Mozart?s birthplace in
the introduction. To detect a contradiction, a CD
system must know that A) ?Mozart? refers to the
same entity in both sentences, that B) ?was born in?
denotes a functional relation, and that C) Vienna and
Salzburg are inconsistent locations.
2The corpus is available at http://www.cs.
washington.edu/research/aucontraire/
12
Of course, world knowledge, and reasoning about
text, are often uncertain, which leads us to associate
probabilities with a CD system?s conclusions. Nev-
ertheless, the knowledge base K is essential for CD.
We now turn to a probabilistic model that helps
us simultaneously estimate the functionality of re-
lations (B in the above example) and ambiguity of
argument values (A above). Section 4 describes the
remaining components of AUCONTRAIRE.
3 Detecting Functionality and Ambiguity
This section introduces a formal model for comput-
ing the probability that a phrase denotes a function
based on a set of extracted tuples. An extracted tuple
takes the form R(x, y) where (roughly) x is the sub-
ject of a sentence, y is the object, and R is a phrase
denoting the relationship between them. If the re-
lation denoted by R is functional, then typically the
object y is a function of the subject x. Thus, our dis-
cussion focuses on this possibility, though the anal-
ysis is easily extended to the symmetric case.
Logically, a relation R is functional in a vari-
able x if it maps it to a unique variable y:
?x, y1, y2 R(x, y1) ? R(x, y2) ? y1 = y2. Thus,
given a large random sample of ground instances of
R, we could detect with high confidence whether R
is functional. In text, the situation is far more com-
plex due to ambiguity, polysemy, synonymy, and
other linguistic phenomena. Deciding whether R is
functional becomes a probabilistic assessment based
on aggregated textual evidence.
The main evidence that a relation R(x, y) is func-
tional comes from the distribution of y values for
a given x value. If R denotes a function and x is
unambiguous, then we expect the extractions to be
predominantly a single y value, with a few outliers
due to noise. We aggregate the evidence that R is
locally functional for a particular x value to assess
whether R is globally functional for all x.
We refer to a set of extractions with the same
relation R and argument x as a contradiction set
R(x, ?). Figure 1 shows three example contradic-
tion sets. Each example illustrates a situation com-
monly found in our data. Example A in Figure 1
shows strong evidence for a functional relation. 66
out of 70 TEXTRUNNER extractions for was born in
(Mozart, PLACE) have the same y value. An am-
biguous x argument, however, can make a func-
tional relation appear non-functional. Example B
depicts a distribution of y values that appears less
functional due to the fact that ?John Adams? refers
to multiple, distinct real-world individuals with that
name. Finally, example C exhibits evidence for a
non-functional relation.
A. was born in(Mozart, PLACE):
Salzburg(66), Germany(3), Vienna(1)
B. was born in(John Adams, PLACE):
Braintree(12), Quincy(10), Worcester(8)
C. lived in(Mozart, PLACE):
Vienna(20), Prague(13), Salzburg(5)
Figure 1: Functional relations such as example A have a
different distribution of y values than non-functional rela-
tions such as C. However, an ambiguous x argument as in
B, can make a functional relation appear non-functional.
3.1 Formal Model of Functions in Text
To decide whether R is functional in x for all x,
we first consider how to detect whether R is lo-
cally functional for a particular value of x. The local
functionality of R with respect to x is the probabil-
ity that R is functional estimated solely on evidence
from the distribution of y values in a contradiction
set R(x, ?).
To decide the probability that R is a function, we
define global functionality as the average local func-
tionality score for each x, weighted by the probabil-
ity that x is unambiguous. Below, we outline an EM-
style algorithm that alternately estimates the proba-
bility that R is functional and the probability that x
is ambiguous.
Let R?x indicate the event that the relation R is
locally functional for the argument x, and that x is
locally unambiguous for R. Also, let D indicate
the set of observed tuples, and define DR(x,?) as the
multi-set containing the frequencies for extractions
of the form R(x, ?). For example the distribution of
extractions from Figure 1 for example A is
Dwas born in(Mozart,?) = {66, 3, 1}.
Let ?fR be the probability that R(x, ?) is locally
functional for a random x, and let ?f be the vector
of these parameters across all relations R. Likewise,
?ux represents the probability that x is locally unam-
biguous for random R, and ?u the vector for all x.
13
We wish to determine the maximum a pos-
teriori (MAP) functionality and ambiguity pa-
rameters given the observed data D, that is
arg max?f ,?u P (?
f ,?u|D). By Bayes Rule:
P (?f ,?u|D) =
P (D|?f ,?u)P (?f ,?u)
P (D)
(1)
We outline a generative model for the data,
P (D|?f ,?u). Let us assume that the event R?x de-
pends only on ?fR and ?
u
x , and further assume that
given these two parameters, local ambiguity and lo-
cal functionality are conditionally independent. We
obtain the following expression for the probability
of R?x given the parameters:
P (R?x|?
f ,?u) = ?fR?
u
x
We assume each set of data DR(x,?) is gener-
ated independently of all other data and parameters,
given R?x. From this and the above we have:
P (D|?f ,?u) =
?
R,x
(
P (DR(x,?)|R
?
x)?
f
R?
u
x
+P (DR(x,?)|?R
?
x)(1? ?
f
R?
u
x)
)
(2)
These independence assumptions allow us to ex-
press P (D|?f ,?u) in terms of distributions over
DR(x,?) given whether or not R
?
x holds. We use the
URNS model as described in (Downey et al, 2005)
to estimate these probabilities based on binomial
distributions. In the single-urn URNS model that we
utilize, the extraction process is modeled as draws of
labeled balls from an urn, where the labels are either
correct extractions or errors, and different labels can
be repeated on varying numbers of balls in the urn.
Let k = maxDR(x,?), and let n =
?
DR(x,?);
we will approximate the distribution over DR(x,?)
in terms of k and n. If R(x, ?) is locally func-
tional and unambiguous, there is exactly one cor-
rect extraction label in the urn (potentially repeated
multiple times). Because the probability of correct-
ness tends to increase with extraction frequency, we
make the simplifying assumption that the most fre-
quently extracted element is correct.3 In this case, k
is the number of correct extractions, which by the
3As this assumption is invalid when there is not a unique
maximal element, we default to the prior P (R?x) in that case.
URNS model has a binomial distribution with pa-
rameters n and p, where p is the precision of the ex-
traction process. If R(x, ?) is not locally functional
and unambiguous, then we expect k to typically take
on smaller values. Empirically, the underlying fre-
quency of the most frequent element in the?R?x case
tends to follow a Beta distribution.
Under the model, the probability of the evidence
given R?x is:
P (DR(x,?)|R
?
x) ? P (k, n|R
?
x) =
(
n
k
)
pk(1? p)n?k
And the probability of the evidence given ?R?x is:
P (DR(x,?)|?R
?
x) ? P (k, n|?R
?
x)
=
(n
k
) ? 1
0
p?k+?f?1(1?p?)n+?f?1?k
B(?f ,?f )
dp?
=
(n
k
)
?(n? k + ?f )?(?f + k)
B(?f , ?f )?(?f + ?f + n)
(3)
where n is the sum over DR(x,?), ? is the Gamma
function and B is the Beta function. ?f and ?f are
the parameters of the Beta distribution for the ?R?x
case. These parameters and the prior distributions
are estimated empirically, based on a sample of the
data set of relations described in Section 5.1.
3.2 Estimating Functionality and Ambiguity
Substituting Equation 3 into Equation 2 and apply-
ing an appropriate prior gives the probability of pa-
rameters ?f and ?u given the observed data D.
However, Equation 2 contains a large product of
sums?with two independent vectors of coefficients,
?f and ?u?making it difficult to optimize analyti-
cally.
If we knew which arguments were ambiguous,
we would ignore them in computing the function-
ality of a relation. Likewise, if we knew which rela-
tions were non-functional, we would ignore them in
computing the ambiguity of an argument. Instead,
we initialize the ?f and ?u arrays randomly, and
then execute an algorithm similar to Expectation-
Maximization (EM) (Dempster et al, 1977) to arrive
at a high-probability setting of the parameters.
Note that if ?u is fixed, we can compute the ex-
pected fraction of locally unambiguous arguments x
for which R is locally functional, using DR(x?,?) and
14
Equation 3. Likewise, for fixed ?f , for any given
x we can compute the expected fraction of locally
functional relations R that are locally unambiguous
for x.
Specifically, we repeat until convergence:
1. Set ?fR =
1
sR
?
x P (R
?
x|DR(x,?))?
u
x for all R.
2. Set ?ux =
1
sx
?
R P (R
?
x|DR(x,?))?
f
R for all x.
In both steps above, the sums are taken over only
those x or R for which DR(x,?) is non-empty. Also,
the normalizer sR =
?
x ?
u
x and likewise sx =?
R ?
f
R.
As in standard EM, we iteratively update our pa-
rameter values based on an expectation computed
over the unknown variables. However, we alter-
nately optimize two disjoint sets of parameters (the
functionality and ambiguity parameters), rather than
just a single set of parameters as in standard EM.
Investigating the optimality guarantees and conver-
gence properties of our algorithm is an item of future
work.
By iteratively setting the parameters to the expec-
tations in steps 1 and 2, we arrive at a good setting
of the parameters. Section 5.2 reports on the perfor-
mance of this algorithm in practice.
4 System Overview
AUCONTRAIRE identifies phrases denoting func-
tional relations and utilizes these to find contradic-
tory assertions in a massive, open-domain corpus of
text.
AUCONTRAIRE begins by finding extractions of
the form R(x, y), and identifies a set of relations
R that have a high probability of being functional.
Next, AUCONTRAIRE identifies contradiction sets
of the form R(x, ?). In practice, most contradiction
sets turned out to consist overwhelmingly of seem-
ing contradictions?assertions that do not actually
contradict each other for a variety of reasons that
we enumerate in section 4.3. Thus, a major chal-
lenge for AUCONTRAIRE is to tease apart which
pairs of assertions in R(x, ?) represent genuine con-
tradictions.
Here are the main components of AUCONTRAIRE
as illustrated in Figure 2:
Extractor: Create a set of extracted assertions E
from a large corpus of Web pages or other docu-
ments. Each extraction R(x, y) has a probability p
Figure 2: AUCONTRAIRE architecture
of being correct.
Function Learner: Discover a set of functional re-
lations F from among the relations in E . Assign to
each relation in F a probability pf that it is func-
tional.
Contradiction Detector: Query E for assertions
with a relation R in F , and identify sets C of po-
tentially contradictory assertions. Filter out seeming
contradictions in C by reasoning about synonymy,
meronymy, argument types, and argument ambigu-
ity. Assign to each potential contradiction a proba-
bility pc that it is a genuine contradiction.
4.1 Extracting Factual Assertions
AUCONTRAIRE needs to explore a large set of
factual assertions, since genuine contradictions are
quite rare (see Section 5). We used a set of extrac-
tions E from the Open Information Extraction sys-
tem, TEXTRUNNER (Banko et al, 2007), which was
run on a set of 117 million Web pages.
TEXTRUNNER does not require a pre-defined set
of relations, but instead uses shallow linguistic anal-
ysis and a domain-independent model to identify
phrases from the text that serve as relations and
phrases that serve as arguments to that relation.
TEXTRUNNER creates a set of extractions in a sin-
gle pass over the Web page collection and provides
an index to query the vast set of extractions.
Although its extractions are noisy, TEXTRUNNER
provides a probability that the extractions are cor-
15
rect, based in part on corroboration of facts from
different Web pages (Downey et al, 2005).
4.2 Finding Potential Contradictions
The next step of AUCONTRAIRE is to find contra-
diction sets in E .
We used the methods described in Section 3 to
estimate the functionality of the most frequent rela-
tions in E . For each relation R that AUCONTRAIRE
has judged to be functional, we identify contradic-
tion sets R(x, ?), where a relation R and domain ar-
gument x have multiple range arguments y.
4.3 Handling Seeming Contradictions
For a variety of reasons, a pair of extractions
R(x, y1) and R(x, y2) may not be actually contra-
dictory. The following is a list of the major sources
of false positives?pairs of extractions that are not
genuine contradictions, and how they are handled
by AUCONTRAIRE. The features indicative of each
condition are combined using Logistic Regression,
in order to estimate the probability that a given pair,
{R(x, y1), R(x, y2)} is a genuine contradiction.
Synonyms: The set of potential contradictions
died from(Mozart,?) may contain assertions that
Mozart died from renal failure and that he died from
kidney failure. These are distinct values of y, but
do not contradict each other, as the two terms are
synonyms. AUCONTRAIRE uses a variety of knowl-
edge sources to handle synonyms. WordNet is a re-
liable source of synonyms, particularly for common
nouns, but has limited recall. AUCONTRAIRE also
utilizes synonyms generated by RESOLVER (Yates
and Etzioni, 2007)? a system that identifies syn-
onyms from TEXTRUNNER extractions. Addition-
ally, AUCONTRAIRE uses edit-distance and token-
based string similarity (Cohen et al, 2003) between
apparently contradictory values of y to identify syn-
onyms.
Meronyms: For some relations, there is no con-
tradiction when y1 and y2 share a meronym,
i.e. ?part of? relation. For example, in the set
born in(Mozart,?) there is no contradiction be-
tween the y values ?Salzburg? and ?Austria?, but
?Salzburg? conflicts with ?Vienna?. Although this
is only true in cases where y occurs in an up-
ward monotone context (MacCartney and Manning,
2007), in practice genuine contradictions between
y-values sharing a meronym relationship are ex-
tremely rare. We therefore simply assigned contra-
dictions between meronyms a probability close to
zero. We used the Tipster Gazetteer4 and WordNet
to identify meronyms, both of which have high pre-
cision but low coverage.
Argument Typing: Two y values are not contra-
dictory if they are of different argument types. For
example, the relation born in can take a date or a
location for the y value. While a person can be
born in only one year and in only one city, a per-
son can be born in both a year and a city. To avoid
such false positives, AUCONTRAIRE uses a sim-
ple named-entity tagger5 in combination with large
dictionaries of person and location names to as-
sign high-level types (person, location, date, other)
to each argument. AUCONTRAIRE filters out ex-
tractions from a contradiction set that do not have
matching argument types.
Ambiguity: As pointed out in Section 3, false con-
tradictions arise when a single x value refers to mul-
tiple real-world entities. For example, if the con-
tradiction set born in(John Sutherland, ?) includes
birth years of both 1827 and 1878, is one of these a
mistake, or do we have a grandfather and grandson
with the same name? AUCONTRAIRE computes the
probability that an x value is unambiguous as part
of its Function Learner (see Section 3). An x value
can be identified as ambiguous if its distribution of
y values is non-functional for multiple functional re-
lations.
If a pair of extractions, {R(x, y1), R(x, y2)}, does
not fall into any of the above categories and R is
functional, then it is likely that the sentences under-
lying the extractions are indeed contradictory. We
combined the various knowledge sources described
above using Logistic Regression, and used 10-fold
cross-validation to automatically tune the weights
associated with each knowledge source. In addi-
tion, the learning algorithm also utilizes the follow-
ing features:
? Global functionality of the relation, ?fR
? Global unambiguity of x, ?ux
4http://crl.nmsu.edu/cgi-bin/Tools/CLR/
clrcat
5http://search.cpan.org/?simon/
Lingua-EN-NamedEntity-1.1/NamedEntity.pm
16
? Local functionality of R(x, ?)
? String similarity (a combination of token-based
similarity and edit-distance) between y1 and y2
? The argument types (person, location, date, or
other)
The learned model is then used to estimate how
likely a potential contradiction {R(x, y1), R(x, y2)}
is to be genuine.
5 Experimental Results
We evaluated several aspects of AUCONTRAIRE:
its ability to detect functional relations and to de-
tect ambiguous arguments (Section 5.2); its preci-
sion and recall in contradiction detection (Section
5.3); and the contribution of AUCONTRAIRE?s key
knowledge sources (Section 5.4).
5.1 Data Set
To evaluate AUCONTRAIRE we used TEXTRUN-
NER?s extractions from a corpus of 117 million Web
pages. We restricted our data set to the 1,000 most
frequent relations, in part to keep the experiments
tractable and also to ensure sufficient statistical sup-
port for identifying functional relations.
We labeled each relation as functional or not,
and computed an estimate of the probability it is
functional as described in section 3.2. Section 5.2
presents the results of the Function Learner on this
set of relations. We took the top 2% (20 relations)
as F , the set of functional relations in our exper-
iments. Out of these, 75% are indeed functional.
Some examples include: was born in, died in, and
was founded by.
There were 1.2 million extractions for all thou-
sand relations, and about 20,000 extractions in 6,000
contradiction sets for all relations in F .
We hand-tagged 10% of the contradiction sets
R(x, ?) where R ? F , discarding any sets with over
20 distinct y values since the x argument for that
set is almost certainly ambiguous. This resulted in a
data set of 567 contradiction sets containing a total
of 2,564 extractions and 8,844 potentially contradic-
tory pairs of extractions.
We labeled each of these 8,844 pairs as contradic-
tory or not. In each case, we inspected the original
sentences, and if the distinction was unclear, con-
sulted the original source Web pages, Wikipedia ar-
ticles, and Web search engine results.
In our data set, genuine contradictions over func-
tional relations are surprisingly rare. We found only
110 genuine contradictions in the hand-tagged sam-
ple, only 1.2% of the potential contradiction pairs.
5.2 Detecting Functionality and Ambiguity
We ran AUCONTRAIRE?s EM algorithm on the
thousand most frequent relations. Performance con-
verged after 5 iterations resulting in estimates of the
probability that each relation is functional and each
x argument is unambiguous. We used these proba-
bilities to generate the precision-recall curves shown
in Figure 3.
The graph on the left shows results for function-
ality, while the graph on the right shows precision at
finding unambiguous arguments. The solid lines are
results after 5 iterations of EM, and the dashed lines
are from computing functionality or ambiguity with-
out EM (i.e. assuming uniform values of ?c when
computing ?f and vice versa). The EM algorithm
improved results for both functionality and ambigu-
ity, increasing area under curve (AUC) by 19% for
functionality and by 31% for ambiguity.
Of course, the ultimate test of how well AUCON-
TRAIRE can identify functional relations is how well
the Contradiction Detector performs on automati-
cally identified functional relations.
5.3 Detecting Contradictions
We conducted experiments to evaluate how well
AUCONTRAIRE distinguishes genuine contradic-
tions from false positives.
The bold line in Figure 4 depicts AUCONTRAIRE
performance on the distribution of contradictions
and seeming contradictions found in actual Web
data. The dashed line shows the performance of AU-
CONTRAIRE on an artificially ?balanced? data set
that we constructed to contain 50% genuine contra-
dictions and 50% seeming ones.
Previous research in CD presented results on
manually selected data sets with a relatively bal-
anced mix of positive and negative instances. As
Figure 4 suggests, this is a much easier problem than
CD ?in the wild?. The data gathered from the Web
is badly skewed, containing only 1.2% genuine con-
tradictions.
17
Functionality
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
AuContraireNo Iteration
Ambiguity
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
AuContraireNo Iteration
Figure 3: After 5 iterations of EM, AUCONTRAIRE achieves a 19% boost to area under the precision-recall curve
(AUC) for functionality detection, and a 31% boost to AUC for ambiguity detection.
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Web DistributionBalanced Data
Figure 4: Performance of AUCONTRAIRE at distinguish-
ing genuine contradictions from false positives. The bold
line is results on the actual distribution of data from the
Web. The dashed line is from a data set constructed to
have 50% positive and 50% negative instances.
5.4 Contribution of Knowledge Sources
We carried out an ablation study to quantify how
much each knowledge source contributes to AU-
CONTRAIRE?s performance. Since most of the
knowledge sources do not apply to numeric argu-
ment values, we excluded the extractions where y
is a number in this study. As shown in Figure 5,
performance of AUCONTRAIRE degrades with no
knowledge of synonyms (NS), with no knowledge
of meronyms (NM), and especially without argu-
ment typing (NT). Conversely, improvements to any
of these three components would likely improve the
performance of AUCONTRAIRE.
The relatively small drop in performance from
no meronyms does not indicate that meronyms are
not essential to our task, only that our knowledge
sources for meronyms were not as useful as we
hoped. The Tipster Gazetteer has surprisingly low
coverage for our data set. It contains only 41% of
the y values that are locations. Many of these are
matches on a different location with the same name,
which results in incorrect meronym information. We
estimate that a gazetteer with complete coverage
would increase area under the curve by approxi-
mately 40% compared to a system with meronyms
from the Tipster Gazetteer and WordNet.
AuContraire NS NM NT
Percentage AUC
0
20
40
60
80
100
Figure 5: Area under the precision-recall curve for the
full AUCONTRAIRE and for AUCONTRAIRE with knowl-
edge removed. NS has no synonym knowledge; NM has
no meronym knowledge; NT has no argument typing.
To analyze the errors made by AUCONTRAIRE,
we hand-labeled all false-positives at the point of
maximum F-score: 29% Recall and 48% Precision.
18
Figure 6 reveals the central importance of world
knowledge for the CD task. About half of the errors
(49%) are due to ambiguous x-arguments, which we
found to be one of the most persistent obstacles to
discovering genuine contradictions. A sizable por-
tion is due to missing meronyms (34%) and missing
synonyms (14%), suggesting that lexical resources
with broader coverage than WordNet and the Tipster
Gazetteer would substantially improve performance.
Surprisingly, only 3% are due to errors in the extrac-
tion process.
Extraction Errors (3%)
Missing Synonyms (14%)
Missing Meronyms (34%)
Ambiguity (49%)
Figure 6: Sources of errors in contradiction detection.
All of our experimental results are based on the
automatically discovered set of functions F . We
would expect AUCONTRAIRE?s performance to im-
prove substantially if it were given a large set of
functional relations as input.
6 Related Work
Condoravdi et al (2003) first proposed contradiction
detection as an important NLP task, and Harabagiu
et al (2006) were the first to report results on con-
tradiction detection using negation, although their
evaluation corpus was a balanced data set built
by manually negating entailments in a data set
from the Recognizing Textual Entailment confer-
ences (RTE) (Dagan et al, 2005). De Marneffe et
al. (2008) reported experimental results on a contra-
diction corpus created by annotating the RTE data
sets.
RTE-3 included an optional task, requiring sys-
tems to make a 3-way distinction: {entails, contra-
dicts, neither} (Voorhees, 2008). The average per-
formance for contradictions on the RTE-3 was preci-
sion 0.11 at recall 0.12, and the best system had pre-
cision 0.23 at recall 0.19. We did not run AUCON-
TRAIRE on the RTE data sets because they contained
relatively few of the ?functional contradictions? that
AUCONTRAIRE tackles. On our Web-based data
sets, we achieved a precision of 0.62 at recall 0.19,
and precision 0.92 at recall 0.51 on the balanced data
set. Of course, comparisons across very different
data sets are not meaningful, but merely serve to un-
derscore the difficulty of the CD task.
In contrast to previous work, AUCONTRAIRE is
the first to do CD on data automatically extracted
from the Web. This is a much harder problem than
using an artificially balanced data set, as shown in
Figure 4.
Automatic discovery of functional relations has
been addressed in the database literature as Func-
tional Dependency Mining (Huhtala et al, 1999;
Yao and Hamilton, 2008). This focuses on dis-
covering functional relationships between sets of at-
tributes, and does not address the ambiguity inherent
in natural language.
7 Conclusions and Future Work
We have described a case study of contradiction de-
tection (CD) based on functional relations. In this
context, we introduced and evaluated the AUCON-
TRAIRE system and its novel EM-style algorithm
for determining whether an arbitrary phrase is func-
tional. We also created a unique ?natural? data set
of seeming contradictions based on sentences drawn
from a Web corpus, which we make available to the
research community.
We have drawn two key lessons from our case
study. First, many seeming contradictions (approx-
imately 99% in our experiments) are not genuine
contradictions. Thus, the CD task may be much
harder on natural data than on RTE data as sug-
gested by Figure 4. Second, extensive background
knowledge is necessary to tease apart seeming con-
tradictions from genuine ones. We believe that these
lessons are broadly applicable, but verification of
this claim is a topic for future work.
Acknowledgements
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, ONR grant N00014-
08-1-0431 as well as gifts from the Utilika Founda-
tion and Google, and was carried out at the Univer-
sity of Washington?s Turing Center.
19
References
M. Banko and O. Etzioni. 2008. The tradeoffs between
traditional and open relation extraction. In Proceed-
ings of ACL.
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
W.W. Cohen, P. Ravikumar, and S.E. Fienberg. 2003.
A comparison of string distance metrics for name-
matching tasks. In IIWeb.
Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. Entailment,
intensionality and text understanding. In Proceedings
of the HLT-NAACL 2003 workshop on Text meaning,
pages 38?45, Morristown, NJ, USA. Association for
Computational Linguistics.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
Marie-Catherine de Marneffe, Anna Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In ACL 2008.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society Se-
ries B, 39(1):1?38.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Prob-
abilistic Model of Redundancy in Information Extrac-
tion. In Procs. of IJCAI.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In AAAI.
Yka? Huhtala, Juha Ka?rkka?inen, Pasi Porkka, and Hannu
Toivonen. 1999. TANE: An efficient algorithm for
discovering functional and approximate dependencies.
The Computer Journal, 42(2):100?111.
B. MacCartney and C.D. Manning. 2007. Natural Logic
for Textual Inference. In Workshop on Textual Entail-
ment and Paraphrasing.
Ellen M. Voorhees. 2008. Contradictions and justifica-
tions: Extensions to the textual entailment task. In
Proceedings of ACL-08: HLT, pages 63?71, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hong Yao and Howard J. Hamilton. 2008. Mining func-
tional dependencies from data. Data Min. Knowl. Dis-
cov., 16(2):197?219.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
20
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 79?88,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Scaling Textual Inference to the Web
Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld
Turing Center
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195, USA
stef,etzioni,weld@cs.washington.edu
Abstract
Most Web-based Q/A systems work by find-
ing pages that contain an explicit answer to
a question. These systems are helpless if the
answer has to be inferred from multiple sen-
tences, possibly on different pages. To solve
this problem, we introduce the HOLMES sys-
tem, which utilizes textual inference (TI) over
tuples extracted from text.
Whereas previous work on TI (e.g., the lit-
erature on textual entailment) has been ap-
plied to paragraph-sized texts, HOLMES uti-
lizes knowledge-based model construction to
scale TI to a corpus of 117 million Web pages.
Given only a few minutes, HOLMES doubles
recall for example queries in three disparate
domains (geography, business, and nutrition).
Importantly, HOLMES?s runtime is linear in
the size of its input corpus due to a surprising
property of many textual relations in the Web
corpus?they are ?approximately? functional
in a well-defined sense.
1 Introduction and Motivation
Numerous researchers have identified the Web as
a rich source of answers to factual questions, e.g.,
(Kwok et al, 2001; Brill et al, 2002), but often the
desired information is not stated explicitly even in a
textual corpus as massive as the Web. Consider the
question ?What vegetables help prevent osteoporo-
sis?? Since there is likely no sentence on the Web
directly stating ?Kale prevents osteoporosis?, a sys-
tem must infer that kale is an answer by combining
facts from multiple sentences, possibly from differ-
ent pages, which justify that conclusion: i.e., that
kale is a vegetable, kale contains calcium, and cal-
cium helps prevent osteoporosis.
Figure 1: The architecture of HOLMES.
Textual Inference (TI) methods have advanced in
recent years. For example, textual entailment tech-
niques aim to determine whether one textual frag-
ment (the hypothesis) follows from another (the text)
(Dagan et al, 2005). While most TI researchers have
focused on high-quality inferences from a small
source text, we seek to utilize sizable chunks of the
Web corpus as our source text. In order to do this,
we must confront two major challenges. The first is
uncertainty: TI is an imperfect process, particularly
when applied to the Web corpus, hence probabilistic
methods help to assess the confidence in inferences.
The second challenge is scalability: how does infer-
ence time scale given increasingly large corpora as
input?
1.1 HOLMES: A Scalable TI System
This paper describes HOLMES, an implemented sys-
tem, which addresses both challenges by carrying
out scalable, probabilistic inference over ground
assertions extracted from the Web. The input to
HOLMES is a conjunctive query, a set of inference
rules expressed as Horn clauses, and large sets of
ground assertions extracted from theWeb, WordNet,
and other knowledge bases. As shown in Figure 1,
HOLMES chains backward from the query, using the
inference rules to construct a forest of proof trees
from the ground assertions. This forest is converted
79
into a Markov network (a form of Knowledge-
Based Model Construction (KBMC) (Wellman et
al., 1992)) and evaluated using approximate prob-
abilistic inference. HOLMES operates in an anytime
fashion ? if desired it can keep iterating: search-
ing for more proofs, and elaborating the Markov net-
work.
HOLMES makes some important simplifying as-
sumptions. Specifically, we use simple ground
tuples to represent extracted assertions (e.g.,
contains(kale, calcium)). Syntactic prob-
lems (e.g., anaphora, relative clauses) and seman-
tic challenges (e.g., quantification, counterfactuals,
temporal qualification) are delegated to the extrac-
tion system or simply ignored. This paper focuses
on scalability for this subset of the TI task.
1.2 Summary of Experimental Results
We tested HOLMES on 183 million distinct ground
assertions extracted from the Web by the TEX-
TRUNNER system (Banko et al, 2007), coupled
with 159 thousand ground assertions from Word-
Net (Miller et al, 1990), and a compact set of hand-
coded inference rules. Given a total of 55 to 145
seconds, HOLMES was able to produce high-quality
inferences that doubled the number of answers to
example queries in three disparate domains: geog-
raphy, business, and nutrition.
We also evaluated how the speed of HOLMES
scaled with the size of its input corpus. In the
general case, logical inference over a Horn theory
(needed in order to produce the probabilistic net-
work) is polynomial in the number of ground asser-
tions, and hence in the size of the textual corpus.1
Unfortunately, this is prohibitive, since even low-
order polynomial growth is fatal on a 117 million-
page corpus, let alne the full Web.
1.3 Why HOLMES Scales Linearly
Fortunately, the Web?s long tail works in our favor.
The relations we extract from text are approximately
pseudo-functional (APF), as we formalize in Sec-
tion 3, and this property leads to runtime that scales
linearly with the corpus. To see the underlying in-
tuition, consider the APF relation denoted by the
phrase ?is married to;? most of the time it maps a
person?s name to a small number of spousal names
1In fact, it is P-complete ? as hard as any polynomial-time
problem.
so this relation is APF. Section 3 shows why this
APF property ensures linear scaling, and Section 4
demonstrates linear scaling in practice.
2 An Overview of HOLMES
HOLMES is a system designed to answer complex
queries over large, noisy knowledge bases. As a mo-
tivating example, we consider the question ?What
vegetables help prevent osteoporosis?? As of this
writing, Google has no pages explicitly stating ?kale
helps prevent osteoporosis?, making it challenging
to return ?kale? as an answer. However, there are
numerous web pages stating that ?kale is high in cal-
cium? and others declaring that ?calcium helps pre-
vent osteoporosis?. If we could combine those facts
we could easily infer that ?kale? is an answer to the
question ?What vegetables help prevent osteoporo-
sis?? HOLMES was designed to make such infer-
ences while accounting for uncertainty in the pro-
cess.
Given a query, expressed as a conjunctive
Datalog rule, HOLMES generates a probabilistic
model using knowledge-based model construction
(KBMC) (Wellman et al, 1992). Specifically,
HOLMES utilizes fast, logical inference to find the
subset of ground assertions and inference rules that
may influence the answers to the query ? enabling
the construction of a small and focused Markov net-
work. Since this graphical model is much smaller
than one incorporating all ground assertions, prob-
abilistic inference will be much faster than if naive
compilation were used.
Figure 1 summarizes the operation of HOLMES.
As with many theorem provers or KBMC systems,
HOLMES takes three inputs:
1. A set of knowledge bases ? databases of
ground relational assertions, each with an
estimate of its probability, which can be
generated by TextRunner (Banko et al,
2007) or Kylin (Wu and Weld, 2007). In
our example, we would extract the as-
sertions IsHighIn(kale, calcium) and
Prevents(calcium, osteoporosis) from
those sentences.
2. A domain theory ? A set of probabilis-
tic inference rules written as Markov logic
Horn clauses, which can be used to de-
rive new assertions. The weight associ-
ated with each clause specifies its reliability.
80
kaleis high incalcium(TextRunner : 0.39)kaleis high inmagnesium(TextRunner : 0.39) magnesiumhelps preventosteoporosis(TextRunner : 0.39) calciumhelps preventosteoporosis(TextRunner : 0.68) broccoliis high incalcium(TextRunner : 0.39)
kalehelps preventosteoporosis(Inferred : 0.88) broccolihelps preventosteoporosis(Inferred : 0.49)kaleIS-Avegetable(WordNet : 0.9) broccoliIS-Avegetable(WordNet : 0.9)Inf. Rule:Transitive-Throughhigh in Inf. Rule:Transitive-Throughhigh in Inf. Rule:Transitive-Throughhigh in
kale matches the query(Inferred : 0.91) broccoli matches the query(Inferred : 0.58)Query Result Query Result
Figure 2: Partial proof ?tree? (DAG) for the query ?What
vegetables help prevent osteoporosis?? Rectangles de-
pict ground assertions from a knowledge base, rounded
boxes are inferred assertions, and shaded squared repre-
sent the application of inference rules. HOLMES converts
this DAG into a Markov network in order to estimate the
probability of each node.
In Section 2.3 we identify several domain-
independent rules, but a user may (optionally)
specify additional, domain-specific rules if de-
sired. In our example, we assume we are given
the domain-specific rule: Prevents(X,Z) :-
IsHighIn(X,Y) ? Prevents(Y,Z)
3. A conjunctive query is specified as a Datalog
rule. For example, the question ?What vegeta-
bles help prevent osteoporosis?? could be writ-
ten as: query(X) :- IS-A(X,Vegetable)
? Prevents(X,osteoporosis)
and returns a set of answers to the query, each with
an associated probability.
2.1 Basic Operation
To find these answers and their associated proba-
bilities, HOLMES first finds all ground assertions in
the knowledge bases that are potentially relevant to
the query. This is efficiently done using the infer-
ence rules to chain backwards from the query. Note
that the generated candidate answers, themselves,
are less important than the associated proof trees.
Furthermore, since HOLMES uses these ?trees? (ac-
tually, DAGs) to generate a probabilistic graphical
model, HOLMES seeks to find as many proof trees
as possible for each query result ? each may influ-
ence the final belief in that result. Figure 2 shows a
partial proof tree for our example query.
To handle uncertainty, HOLMES now constructs a
ground Markov network from the proof trees and the
Markov-logic-encoded inference rules. Markov net-
works (Pearl, 1988) model the joint distribution of a
set of variables by creating an undirected graph with
one node for each random variable, and represent-
ing dependencies between variables with cliques in
the graph. Each clique has a corresponding poten-
tial function ?k, which returns a non-negative value
based on the state of variables in the clique. The
probability of a state, x, is given by
P (x) =
1
Z
?
?k(x{k})
where the partition functionZ is a normalizing term,
and x{k} denotes the state of all the variables in
clique k.
HOLMES converts the proof trees into a Markov
network in a manner pioneered by the Markov Logic
framework of Richardson and Domingos (2006). A
Boolean variable is created to represent the truth of
each assertion in the proof forest. Next, HOLMES
adds edges to the Markov network to create a clique
corresponding to each application of an inference
rule in the proof forest.
Following the Markov Logic framework, the po-
tential function of a clique has form ?(x) = ew if all
member nodes are true (w denotes the weight of the
inference rule), and ?(x) = 1 otherwise. The proba-
bilities of leaf nodes are derived from the underlying
knowledge base,2 and inferred nodes are biased with
an exponential prior.
Finally, HOLMES computes the approximate
probability of each answer by running a variant
of loopy belief propagation (Pearl, 1988) over the
Markov network. In our experience this method
performs well on networks derived from our Horn
clause proof forest, but one could use Monte Carlo
techniques or even exact methods if desired.
Note that this architecture allows HOLMES to
combine information from multiple web pages to in-
fer assertions not explicitly seen in the textual cor-
pus. Because this inference is done using a Markov
network, it correctly handles uncertain extractions
and probabilistic dependencies. By using KBMC to
create a custom, focused network for each query, the
2In our experiments, ground assertions from WordNet get
a uniformly high probability of correctness (0.9), but those ex-
tracted from the Web are assigned probabilities derived from
redundancy statistics, following the intuition that frequently ex-
tracted facts are more likely to be true (Etzioni et al, 2005).
81
amount of probabilistic inference is reduced to man-
ageable proportions.
2.2 Anytime, Incremental Expansion
Because exact probabilistic inference is #P-
complete, HOLMES uses approximate methods, but
even these techniques have problems if the Markov
network gets too large. As a result, HOLMES creates
the network incrementally. After the first proof trees
are generated, HOLMES creates the model and per-
forms approximate probabilistic inference. If more
time is available then HOLMES searches for addi-
tional proof trees and updates the network (Fig-
ure 1). This incremental process allows HOLMES
to return initial results (with preliminary probability
estimates) as soon as they are discovered.
For efficiency, HOLMES exploits standard Data-
log optimizations (e.g., it only expands proofs of re-
cently added nodes and it uses an approximation to
magic sets (Ullman, 1989), rather than simple back-
wards chaining). For tractability, we also allow the
user to limit the number of transitive inference steps
for any inference rule.
HOLMES also includes a few enhancements for
dealing with information extracted from natural lan-
guage. For example, HOLMES?s inference rules sup-
port substring/regex matching of ground assertions,
to accommodate simple variations in text. HOLMES
also can be restricted to only operate over proper
nouns, which is useful for queries involving named
entities.
2.3 Markov Logic Inference Rules
HOLMES is given the following set of six domain-
independent rules, which are similar to the up-
ward monotone rules introduced by (MacCartney
and Manning, 2007).
1. Observed relations are likely to be true:
R(X,Y) :- ObservedInCorpus(X, R, Y)
2. Synonym substitution preserves meaning:
RTR(X?,Y) :- RTR(X,Y) ? Synonym(X, X?)
3. RTR(X,Y?) :- RTR(X,Y) ? Synonym(Y, Y?)
4. Generalizations preserve meaning:
RTR(X?,Y) :- RTR(X,Y) ? IS-A(X, X?)
5. RTR(X,Y?) :- RTR(X,Y) ? IS-A(Y, Y?)
6. Transitivity of Part Meronyms:
RTR(X,Y?) :- RTR(X,Y) ? Part-Of(Y, Y?)
where RTR matches ?* in? (e.g., ?born in?).
For example, if Q(X):-BornIn(X,?France?),
and we know from WordNet that Paris is in
France, then by inference rule 6, we know that
BornIn(X,?Paris?) will yield valid results for
Q(X). Although all of these rules contain at most
two relations in the body, HOLMES allows an
arbitrary number of relations in the query and rule
bodies. However, we have found that even simple
rules can dramatically improve some queries.
We set the rule weights to capture the intuition
that deeper inferences decrease the likelihood (as
there are more chances to make mistakes), whereas
additional, independent proof trees increase the
likelihood (as there is more supporting evidence).
Specifically, in our experiments we set the prior on
inferred facts to -0.75, the weight on rule 1 to 1.5,
and the weights on all other rules to 0.6.
At present, we define these weights manually, but
we expect to learn the parameter values in the future.
3 Scaling Inference to the Web
If TI is applied to a corpus containing hundreds of
millions or even billions of pages, its run time has to
be at most linear in the size of the corpus. This sec-
tion shows that under some reasonable assumptions
inference does scale linearly.
We start our analysis with two simplifications.
First, we assume that the number of distinct, ground
assertions in the KBs, |A|, grows at most linearly
with the size of the textual corpus. This is cer-
tainly true for assertions extracted by TextRunner
and Kylin, and follows from our exclusion of texts
with complex quantified sentences. Our analysis
now proceeds to consider scaling with respect to |A|
for a fixed query and set of inference rules.
Our second assumption is that the size of every
proof tree is bounded by some constant, m. This
is a strong assumption and one that depends on the
precise set of inference rules and pattern of ground
assertions. However, it holds in our experience, and
if necessary could be enforced by terminating the
search for proof trees at a certain depth, e.g., log(m).
HOLMES?s knowledge-based model construction
has two parts: construction of the proof forest and
conversion of the forest into a Markov network.
Since the Markov network is essentially isomorphic
to the proof forest, the conversion will be O(|A|) if
the forest is linear in size, which is ensured if the
time to construct the proof trees isO(|A|). We show
82
this in the remainder of this section.
Recall that HOLMES requires inference rules to
be function-free Horn clauses. While this limits ex-
pressivity to some degree, it provides a huge speed
benefit ? logical inference over Horn clauses can
be done in polynomial time, whereas general propo-
sitional inference (i.e., from grounded first-order
rules) is NP-complete.
Alas, even low-order polynomial blowup is un-
acceptable when the textual corpus reaches Web
scale; we seek linear growth. Intuitively, there are
two places where polynomial expansion could cause
trouble. First, the number of different types of proofs
(i.e., first order proofs) could grow too quickly, and
secondly, a given type of proof tree might apply
to too many ground assertions (?tuples? in database
lingo). We treat these issues in turn.
Under our assumptions, each proof tree can be
represented as an expression in relational algebra
with at most m equijoins (Ullman, 1989),3 each
stemming from the application of an inference rule.
Since the number of rules is fixed, as is m, there are
a constant number of possible first-order proof trees.
The bigger concern is that any one of these first-
order trees might result in a polynomial number of
ground trees; if so, the size of the ground forest
(and corresponding Markov network) could grow
too quickly. In fact, polynomial growth is a common
phenomena in database query evaluation. Luckily,
most relations in the Web corpus behave more fa-
vorably. We introduce a property of relations that
ensures m-way joins, and therefore all proof trees
up to size m, can be computed in O(|A|) time.
The intuition is that most relations derived from
large corpora have a ?heavy-tailed? distribution,
wherein a few objects appear many times in a rela-
tion, but most appear only once or twice, thus joins
involving rare objects lead to a small number of re-
sults, and so the main limitation on scalability is
common objects. We now prove that if these com-
mon objects account for a small enough fraction of
the relation, then joins will still scale linearly. We
focus on binary relations, but these results can eas-
ily be extended to relations of larger arity.
3Note that an inference rule of the form H(X) :-
R1(X,Y),R2(Y,Z) is equivalent to the algebraic expression
piX(R1 ./ R2). First a join is performed between R1 and R2
testing for equality between values of Y ; then a projection elim-
inates all columns besides X .
Definition 1 A relation, R = {(xi, yi)} ? X ?
Y , is pseudo-functional (PF) in x with degree k, if
?x ? X : |{y|(x, y) ? R}| ? k. When the precise
variable and degree is irrelevant to discussion, we
simply say ?R is PF.?
An m-way equijoin over relations that are PF in
the join variables will have at most km ? |R| results.
Since km is constant for a given join and |R| scales
linearly in the size of the textual corpus, proof tree
construction over PF relations also scales linearly.
However, due to their heavy-tailed distributions,
most relations extracted from theWeb fit the pseudo-
functional definition in most, but not all values of
X . Fortunately, it turns out that in most cases these
?bad? values ofX are rare and hence don?t influence
the join size significantly. We formalize this intu-
ition by defining a class of approximately pseudo-
functional (APF) relations and proving that joining
two APF relations produces at most a linear number
of results.
Definition 2 A relation, R, is approximately
pseudo-functional (APF) in x with degree k, if X
can be partitioned into two sets XG and XB such
that for all x ? XG R is PF with degree k and?
x?XB
|{y|(x, y) ? R}| ? k ? log(|R|)
Theorem 1. If relation R1 is APF in y with de-
gree k1 and R2 is APF in y with degree k2 then
the relation Q = R1 ./ R2 has size at most
O(max(|R1|, |R2|)).
Proof. Since R1 and R2 are APF, we know that
Y can be partitioned into four groups: YBB =
YB1
?
YB2, YBG = YB1
?
YG2, YGB = YG1
?
YB2,
YGG = YG1
?
YG2.4 We can show that each group
leads to at most O(|A|) entries in Q. For y ? YBB
there are at most k1 ? k2 ? log(|R1|) ? log(|R2|) en-
tries in Q. The y ? YGB and y ? YBG lead to at
most k1 ? k2 ? log(|R2|) and k1 ? k2 ? log(|R1|)
entries, respectively. For y ? YGG there are at
most k1 ? k2 ? max(|R1|, |R2|). Summing the re-
sults from the four partitions, we see that |Q| is
O(max(|R1|, |R2|)), thus it is O(|A|).
This theorem and proof can easily be extended to
4YBB are the ?doubly bad? values of y that violate the PF
definition for both relations, YGG are the values that do not vio-
late the PF definition for either relation, and YBG and YGB are
the values that violate it in only R1 or R2, resp.
83
an m-way equijoin, as long as each relation is APF
in all arguments that are being joined.
Theorem 2. IfQ is the relation obtained by an equi-
join over m relationsR1..m, each having size at most
O(|A|), and if all R1..m are APF in all arguments
that they are joined in with degree at most kmax, and
if
?
1?i?m
log(|Ri|) ? |A|, then |Q| is O(|A|).
The inequality in Theorem 2 relates the sizes of
the relations (|R|), the join (m) and the number of
ground assertions (|A|). However, in many cases we
are interested in much smaller values of m than the
inequality enables. We can relax the APF definition
to allow a broader, but still scalable, class ofm-way-
APF relations.
Corollary 3. If Q is the relation obtained by an m-
way join, and if each participating relation is APF
in their joined variables with a bound of ki ? m
?
|Ri|
instead of ki ? log(|Ri|), then the join is O(|A|).
The final step in our scaling argument concerns
probabilistic inference, which is #P-Complete if per-
formed exactly. This is addressed in two ways. First,
HOLMES uses approximate methods, e.g., loopy be-
lief propagation, which avoids the cost of exact in-
ference ? at the cost of reduced precision. Sec-
ondly, at a practical level, HOLMES?s incremental
construction of the graphical model (Figure 1) al-
lows it to bound the size of the network by terminat-
ing the search for additional proofs.
4 Experimental Results
This section reports on measurements that confirm
that linear scaling with |A| occurs in practice, and
that HOLMES?s inference is not only scalable but
also improves precision/recall on sample queries in
a diverse set of domains. After describing the exper-
imental domains and queries, Section 4.2 reports on
the boost to the area under the precision/recall curve
for a set of example queries in three domains: ge-
ography, business, and nutrition. Section 4.3 then
shows that APF relations are very common in the
Web corpus, and finally Section 4.4 demonstrates
empirically that HOLMES?s inference time scales
linearly with the number of pages in the corpus.
4.1 Experimental Setup
HOLMES utilized two knowledge bases in these ex-
periments: TEXTRUNNER and WordNet. TEX-
TRUNNER contains approximately 183 million dis-
tinct ground assertions extracted from over 117 mil-
lion web pages, and WordNet contains 159 thousand
manually created IS-A, Part-Of, and Synonym asser-
tions.
In all queries, HOLMES utilizes the domain-
independent inference rules described in Sec-
tion 2.3. HOLMES additionally makes use of two
domain-specific inference rules in the Nutrition
domain, to demonstrate the benefits of including
domain-specific information. Estimating the preci-
sion and relative recall of HOLMES requires exten-
sive and careful manual tagging of HOLMES output.
To make this feasible, we restricted ourselves to a
set of twenty queries in three domains, but made the
domains diverse to illustrate the broad scope of the
system.
We now describe each domain briefly.
Geography: the query issued is: ?Who was born in
one of the following countries?? More formally,
Q(X) :- BornIn(X,{country}) where {country}
is bound to each of the following nine countries
in turn {France, Germany, China, Thailand, Kenya,
Morocco, Peru, Columbia, Guatemala}, yielding a
total of nine queries.
Because Web text often refers to a person?s
birth city rather than birth country, this query il-
lustrates how combining an ground assertion (e.g.,
BornIn(Alberto Fujimori, Lima)) with back-
ground knowledge (e.g., LocatedIn(Lima, Peru))
enables the system to draw new conclusions (e.g.,
BornIn(Alberto Fujimori, Peru)).
Business: we issued the following two queries.
1) Which companies are acquiring software com-
panies? Formally, Q(X) :- Acquired(X, Y)
? Develops(Y, ?software?) This query tests
HOLMES?s ability to scalably join a large number of
assertions from multiple pages.
2) Which companies are headquartered in the
USA? Q(X) :- HeadquarteredIn(X, ?USA?)
? IS-A(X, ?company?)
Answering this query comprehensively requires
HOLMES to combine a join (over the relations Head-
quarteredIn and IS-A) with transitive inference on
PartOf (e.g., Seattle is PartOf Washington which is
PartOf the USA) and on IS-A (e.g., Microsoft IS-A
software company which IS-A company). The IS-
A assertions came from both TEXTRUNNER (using
patterns from (Hearst, 1992)) and WordNet.
84
0
0.2
0.4
0.6
0.8
1
0 1000 2000 3000 4000 5000Estimated Recall
Precis
ion
BaselineHolmes Increase in AuC
Figure 3: PR Curve for BornIn(X, {country}). Inference
boosts the Area under the PR Curve (AuC) by 102 %.
Domain Increase Total Inference
in AuC Time
Geography +102% 55 s
Business +2,643% 145 s
Nutrition +5,595% 64 s
Table 1: Improvement in the AuC of HOLMES over the
BASELINE and total inference time taken by HOLMES.
Results are summed over all queries in the geography,
business, and nutrition domains. Inference time mea-
sured on unoptimized prototype.
Nutrition: the nine queries issued are instances
of ?What foods prevent disease?? Where a food is
a member of one of the classes: fruit, vegetable, or
grain, and a disease is one of: anemia, scurvy, or
osteoporosis. More formally, Q(X, {disease}) :-
Prevents(X, {disease}) ? IS-A(X, {food})
Our experiments in the nutrition domain utilized
two domain-specific inference rules in addition to
the ones presented in Section 2.3:
Prevents(X,Y):-HighIn(X,Z) ? Prevents(Z,Y)
Prevents(X,Y):-Contains(X,Z) ? Prevents(Z,Y)
4.2 Effect of Inference on Recall
To measure the cost and benefit of HOLMES?s in-
ference we need to define a baseline for compar-
ison. Answering the conjunctive queries in the
business and nutrition domains requires computing
joins, which TEXTRUNNER does not do. Thus, we
defined a baseline system, BASELINE, which has
access to the underlying Knowledge Bases (KBs)
(TEXTRUNNER and WordNet), and the ability to
compute joins using information explicitly stated in
either KB, but does not have the ability to infer new
assertions.
We compared HOLMES with BASELINE in all
three domains. Figure 3 depicts the combined pre-
cision/relative recall curves for the nine Geography
queries. HOLMES yields substantially higher re-
call (the shaded region) at modestly lower preci-
sion, doubling the area under the precision/recall
curve (AuC). The other precision/recall curves also
showed a slight drop in precision for substantial
gains in recall. Table 1 summarizes the results, along
with the total runtime needed for inference. Because
relations in the business domain are much larger
than in the other domains (i.e., 100x ground asser-
tions), inference is slower in this domain.
We note that inference is particularly helpful with
rarely mentioned instances. However, inference can
lead to errors when the proof tree contains joins on
generic terms (e.g., ?company?) or common extrac-
tion errors (e.g., ?LLC? as a company name). This
is a key area for future work.
4.3 Prevalence of APF Relations
To determine the prevalence of APF relations inWeb
text, we examined a sample of 500 binary relations
selected randomly from TEXTRUNNER?s ground as-
sertions. The surface forms of the relations and ar-
guments may misrepresent the true properties of the
underlying concepts, so to better estimate the true
properties we merged synonymous values as given
by Resolver (Yates and Etzioni, 2007) or the most
frequent sense of the word in WordNet. For exam-
ple, we would consider BornIn(baby, hospital)
and BornAt(infant, infirmary) to represent the
same concept, and so would merge them into one
instance of the ?Born In? relation. The largest two re-
lations had over 1.25 million unique instances each,
and 52% of the relations had more than 10,000 in-
stances.
For each relation R, we first found all instances
of R extracted by TEXTRUNNER and merged all
synonymous instances as described above. Then,
for each argument of R we computed the smallest
value, Kmin, such that R is APF with degree Kmin.
Since many interesting assertions can be inferred by
simply joining two relations, we also considered the
special case of 2-way joins using Corollary 3. We
computed the smallest value, K2./, such that the re-
lation is two-way-APF with degree K2./.
Figure 4 shows the fraction of relations with
Kmin andK2./ of at mostK as a function of varying
85
0%
20%
40%
60%
80%
100%
0 1000 2000 3000 4000 5000 6000Degree of Approximate Pseudo-Functionality
APFAPF for two-way join
Figure 4: Prevalence of APF relations in Web text. The
x-axis depicts the degree of pseudo-functionality, e.g.,
Kmin and K2./, (see definition 2); the y-axis lists the
percent of relations that are APF with that degree. Re-
sults are averaged over both arguments.
values of K. The results are averaged over both ar-
guments of each binary relation. For arbitrary joins
in this KB, 80% of the relations are APF with de-
gree less than 496; for 2-way joins (like the ones in
our inference rules and test queries), 80% of the rela-
tions are APF with degree less than 65. These results
indicate that the majority of relations TEXTRUNNER
extracted from text are APF, and so we can expect
HOLMES?s techniques will allow efficient inference
over most relations.
While Theorem 2 guarantees that joins over those
relations will beO(|R|), that notation hides a poten-
tially large constant factor of Kminm. Fortunately
the constant factor is significantly smaller in prac-
tice. To see why, we re-examine the proof: the large
factor comes from assuming that all of R?s first ar-
guments which meet the PF definition are associated
with exactly Kmin distinct second arguments. How-
ever, in our corpus 83% of first arguments are as-
sociated with only one second argument. Clearly,
our worst-case analysis substantially over-estimates
inference time for most queries. Moreover, in ad-
ditional experiments (omitted due to space limita-
tions), measured join sizes grew linearly in the size
of the corpus, but were on average two to three or-
ders of magnitude smaller than the bounds given in
the theory. This observation held across relations
with different sizes and values of Kmin.
While the results in Figure 4 may vary for other
sets of relations, we believe the general trends
hold. This is promising for Question Answering and
Textual Inference systems, since if true it implies
R2 = 0.9881
R2 = 0.9808
R2 = 0.9931
020
4060
80100
120140
160
0% 20% 40% 60% 80% 100%Fraction of Corpus
GeographyBusinessNutrition
Figure 5: The effects of corpus size on total inference
time. We see approximately linear growth in all domains,
and display the best fit lines and coefficient of determina-
tion (R2) of each.
that combining information frommultiple difference
source is feasible, and can allow such systems to in-
fer answers not explicitly seen in any source.
4.4 Scalability of Inference Speed
Since the previous subsection showed that most re-
lations are APF in their arguments, our theory pre-
dicts HOLMES?s inference will scale linearly. We
tested this hypothesis empirically by running infer-
ence over the test queries in our three domains, while
varying the number of pages in the textual corpus.
Figure 5 shows how the inference time HOLMES
used to answer all queries in each domain scales
with KB size. For these queries, and several oth-
ers we tested (not shown here), inference time grows
linearly with the size of the KB. Based on these re-
sults we believe that HOLMES can provide scalable
inference over a wide variety of domains.
5 Related Work
Textual Entailment systems are given two textual
fragments, text T and hypothesis H , and attempt to
decide if the meaning of H can be inferred from
the meaning of T (Dagan et al, 2005). While
many approaches have addressed this problem, our
work is most closely related to that of (Raina et al,
2005; MacCartney and Manning, 2007; Tatu and
Moldovan, 2006; Braz et al, 2005), which convert
the inputs into logical forms and then attempt to
?prove? H from T plus a set of axioms. For in-
stance, (Braz et al, 2005) represents T , H , and a
set of rewrite rules in a description logic framework,
and determines entailment by solving an integer lin-
86
ear program derived from that representation.
These approaches and related ones (e.g.,
(Van Durme and Schubert, 2008)) use highly
expressive representations, enabling them to ex-
press negation, temporal information, and more.
HOLMES?s representation is much simpler?
Markov Logic Horn Clauses for inference rules
coupled with a massive database of ground asser-
tions. However, this simplification allows HOLMES
to tackle a ?text? of enormously larger size: 117
million Web pages versus a single paragraph. A sec-
ond, if smaller, difference stems from the fact that
instead of determining whether a single hypothesis
sentence, H , follows from the text, HOLMES tries to
find all consequents that match a conjunctive query.
HOLMES is also related to open-domain question-
answering systems such as Mulder (Kwok et al,
2001), AskMSR (Brill et al, 2002), and others
(Harabagiu et al, 2000; Brill et al, 2001). How-
ever, these Q/A systems attempt to find individual
documents or sentences containing the answer. They
often perform deep analysis on promising texts, and
back off to shallower, less reliable methods if those
fail. In contrast, HOLMES utilizes TI and attempts
to combine information from multiple different sen-
tences in a scalable way.
While its ability to combine information from
multiple sources is promising, HOLMES has several
limitations these Q/A systems do not have. Since
HOLMES relies on an information extraction sys-
tem to convert sentences into ground predicates,
any limitations of the IE system will be propagated
to HOLMES. Additionally, the logical representa-
tion HOLMES uses limits the reasoning and types
of questions it can answer. HOLMES is geared to-
wards answering questions which are naturally ex-
pressed as properties and relations of entities, and is
not well suited to answering more abstract or open
ended questions. Although we have demonstrated
that HOLMES is scalable, further work is needed to
make it to run at interactive speeds.
Finally, research in statistical relational learning
such as MLNs (Richardson and Domingos, 2006),
RMNs (Taskar et al, 2002), and others (Getoor
and Taskar, 2007) have studied techniques for com-
bining logical and probabilistic inference. Our in-
ference rules are more restrictive than those al-
lowed in MLNs, but this trade-off allows us to ef-
ficiently scale inference to large, open domain cor-
pora. By constructing only cliques for satisfied in-
ference rules, HOLMES explicitly models the intu-
ition behind LazySAT inference (Singla and Domin-
gos, 2006) as used in MLNs. I.e., most Horn clause
inference rules will be trivially satisfied since their
antecedents will be false, so we only need to worry
about ones where the antecedent is true.
6 Conclusions
This paper makes three main contributions:
1. We introduce and evaluate the HOLMES sys-
tem, which leverages KBMC methods in order
to scale a class of TI methods to the Web.
2. We define the notion of Approximately Pseudo-
Functional (APF) relations and prove that, for
a APF relations, HOLMES?s inference time in-
creases linearly with the size of the input cor-
pus. We show empirically that APF relations
appear to be prevalent in our Web corpus (Fig-
ure 4), and that HOLMES?s runtime does scale
linearly with the size of its input (Figure 5), tak-
ing only a few CPU minutes when run over 183
million distinct ground assertions.
3. We present experiments demonstrating that, for
a set of queries in the domains of geography,
business, and nutrition, HOLMES substantially
improves the quality of answers (measured by
AuC) relative to a ?no inference? baseline.
In the future, we plan more extensive tests to char-
acterize when HOLMES?s inference is helpful. We
also hope to examine in what cases jointly perform-
ing extraction and inference (as opposed to perform-
ing them separately) is feasible at scale. Finally, we
plan to examine methods for HOLMES to learn both
rule weights and new inference rules.
Acknowledgements
We thank the following for helpful comments on
previous drafts: Fei Wu, Michele Banko, Mausam,
Doug Downey, and Alan Ritter. This research was
supported in part by NSF grants IIS-0535284, IIS-
0312988, and IIS-0307906, ONR grants N00014-
08-1-0431 and N00014-06-1-0147, CALO grant 03-
000225, the WRF / TJ Cable Professorship as well
as gifts from Google. The work was performed at
the University of Washington?s Turing Center.
87
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
R. Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sam-
mons. 2005. An inference model for semantic en-
tailment in natural language. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 1678?1679.
E. Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng.
2001. Data-intensive question answering. In Procs.
of Text REtrieval Conference (TREC-10), pages 393?
400.
Eric Brill, Susan Dumais, and Michele Banko. 2002. An
analysis of the AskMSR question-answering system.
In EMNLP ?02: Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 257?264, Morristown, NJ, USA. Association for
Computational Linguistics.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
L. Getoor and B. Taskar. 2007. Introduction to Statistical
Relational Learning. MIT Press.
S. Harabagiu, M. Pasca, and S. Maiorano. 2000. Exper-
iments with open-domain textual question answering.
In Procs. of the COLING-2000.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545, Nantes, France.
C.C.T. Kwok, O. Etzioni, and D.S. Weld. 2001. Scal-
ing question answering to the Web. Proceedings of
the 10th international conference on World Wide Web,
pages 150?161.
B. MacCartney and C.D. Manning. 2007. Natural Logic
for Textual Inference. In Workshop on Textual Entail-
ment and Paraphrasing.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to wordnet: An on-line
lexical database. International Journal of Lexicogra-
phy, 3(4):235?312.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann Publishers Inc. San Francisco, CA, USA.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI 2005.
AAAI Press.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107?136.
Parag Singla and Pedro Domingos. 2006. Memory-
efficient inference in relational domains. In AAAI.
B. Taskar, P. Abbeel, and D. Koller. 2002. Discrimi-
native probabilistic models for relational data. Eigh-
teenth Conference on Uncertainty in Artificial Intelli-
gence (UAI02).
Marta Tatu and Dan Moldovan. 2006. A logic-based
semantic approach to recognizing textual entailment.
In Proceedings of the COLING/ACL on Main confer-
ence poster sessions, pages 819?826, Morristown, NJ,
USA. Association for Computational Linguistics.
J. Ullman. 1989. Database and knowledge-base systems.
Computer Science Press.
B. Van Durme and L.K. Schubert. 2008. Open knowl-
edge extraction through compositional language pro-
cessing. In Symposium on Semantics in Systems for
Text Processing.
M. Wellman, J. Breese, and R. Goldman. 1992. From
knowledge bases to decision models. The Knowledge
Engineering Review, 7(1):35?53.
F. Wu and D. Weld. 2007. Autonomously semantifying
Wikipedia. In Proceedings of the ACM Sixteenth Con-
ference on Information and Knowledge Management
(CIKM-07), Lisbon, Porgugal.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
88
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 339?346, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Extracting Product Features and Opinions from Reviews
Ana-Maria Popescu and Oren Etzioni
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350
{amp, etzioni}@cs.washington.edu
Abstract
Consumers are often forced to wade
through many on-line reviews in
order to make an informed prod-
uct choice. This paper introduces
OPINE, an unsupervised information-
extraction system which mines re-
views in order to build a model of im-
portant product features, their evalu-
ation by reviewers, and their relative
quality across products.
Compared to previous work, OPINE
achieves 22% higher precision (with
only 3% lower recall) on the feature
extraction task. OPINE?s novel use of
relaxation labeling for finding the se-
mantic orientation of words in con-
text leads to strong performance on
the tasks of finding opinion phrases
and their polarity.
1 Introduction
The Web contains a wealth of opinions about products,
politicians, and more, which are expressed in newsgroup
posts, review sites, and elsewhere. As a result, the prob-
lem of ?opinion mining? has seen increasing attention
over the last three years from (Turney, 2002; Hu and Liu,
2004) and many others. This paper focuses on product
reviews, though our methods apply to a broader range of
opinions.
Product reviews on Web sites such as amazon.com
and elsewhere often associate meta-data with each review
indicating how positive (or negative) it is using a 5-star
scale, and also rank products by how they fare in the re-
views at the site. However, the reader?s taste may differ
from the reviewers?. For example, the reader may feel
strongly about the quality of the gym in a hotel, whereas
many reviewers may focus on other aspects of the ho-
tel, such as the decor or the location. Thus, the reader is
forced to wade through a large number of reviews looking
for information about particular features of interest.
We decompose the problem of review mining into the
following main subtasks:
I. Identify product features.
II. Identify opinions regarding product features.
III. Determine the polarity of opinions.
IV. Rank opinions based on their strength.
This paper introduces OPINE, an unsupervised infor-
mation extraction system that embodies a solution to each
of the above subtasks. OPINE is built on top of the Know-
ItAll Web information-extraction system (Etzioni et al,
2005) as detailed in Section 3.
Given a particular product and a corresponding set of
reviews, OPINE solves the opinion mining tasks outlined
above and outputs a set of product features, each accom-
panied by a list of associated opinions which are ranked
based on strength (e.g., ?abominable? is stronger than
?bad). This output information can then be used to gen-
erate various types of opinion summaries.
This paper focuses on the first 3 review mining sub-
tasks and our contributions are as follows:
1. We introduce OPINE, a review-mining system whose
novel components include the use of relaxation labeling
to find the semantic orientation of words in the context of
given product features and sentences.
2. We compare OPINE with the most relevant previous
review-mining system (Hu and Liu, 2004) and find that
OPINE?s precision on the feature extraction task is 22%
better though its recall is 3% lower on Hu?s data sets. We
show that 1/3 of this increase in precision comes from
using OPINE?s feature assessment mechanism on review
data while the rest is due to Web PMI statistics.
3. While many other systems have used extracted opin-
ion phrases in order to determine the polarity of sentences
or documents, OPINE is the first to report its precision and
recall on the tasks of opinion phrase extraction and opin-
ion phrase polarity determination in the context of known
product features and sentences. On the first task, OPINE
has a precision of 79% and a recall of 76%. On the sec-
ond task, OPINE has a precision of 86% and a recall of
89%.
339
Input: product class C, reviews R.
Output: set of [feature, ranked opinion list] tuples
R?? parseReviews(R);
E? findExplicitFeatures(R?, C);
O? findOpinions(R?, E);
CO? clusterOpinions(O);
I? findImplicitFeatures(CO, E);
RO? rankOpinions(CO);
{(f , oi, ...oj)...}?outputTuples(RO, I ? E);
Figure 1: OPINE Overview.
The remainder of this paper is organized as follows:
Section 2 introduces the basic terminology, Section 3
gives an overview of OPINE, describes and evaluates its
main components, Section 4 describes related work and
Section 5 presents our conclusion.
2 Terminology
A product class (e.g., Scanner) is a set of products (e.g.,
Epson1200). OPINE extracts the following types of prod-
uct features: properties, parts, features of product parts,
related concepts, parts and properties of related concepts
(see Table 1 for examples of such features in the Scan-
ner domains). Related concepts are concepts relevant to
the customers? experience with the main product (e.g.,
the company that manufactures a scanner). The relation-
ships between the main product and related concepts are
typically expressed as verbs (e.g., ?Epson manufactures
scanners?) or prepositions (?scanners from Epson?). Fea-
tures can be explicit (?good scan quality?) or im-
plicit (?good scans? implies good ScanQuality).
OPINE also extracts opinion phrases, which are adjec-
tive, noun, verb or adverb phrases representing customer
opinions. Opinions can be positive or negative and vary
in strength (e.g., ?fantastic? is stronger than ?good?).
3 OPINE Overview
This section gives an overview of OPINE (see Figure 1)
and describes its components and their experimental eval-
uation.
Goal Given product class C with instances I and re-
views R, OPINE?s goal is to find a set of (feature, opin-
ions) tuples {(f, oi, ...oj)} s.t. f ? F and oi, ...oj ? O,
where:
a) F is the set of product class features in R.
b) O is the set of opinion phrases in R.
c) f is a feature of a particular product instance.
d) o is an opinion about f in a particular sentence.
d) the opinions associated with each feature f are
ranked based on their strength.
Solution The steps of our solution are outlined in Fig-
ure 1 above. OPINE parses the reviews using MINI-
PAR (Lin, 1998) and applies a simple pronoun-resolution
module to parsed review data. OPINE then uses the data
to find explicit product features (E). OPINE?s Feature As-
sessor and its use of Web PMI statistics are vital for the
extraction of high-quality features (see 3.2). OPINE then
identifies opinion phrases associated with features in E
and finds their polarity. OPINE?s novel use of relaxation-
labeling techniques for determining the semantic orien-
tation of potential opinion words in the context of given
features and sentences leads to high precision and recall
on the tasks of opinion phrase extraction and opinion
phrase polarity extraction (see 3.3).
In this paper, we only focus on the extraction of ex-
plicit features, identifying corresponding customer opin-
ions about these features and determining their polarity.
We omit the descriptions of the opinion clustering, im-
plicit feature generation and opinion ranking algorithms.
3.0.1 The KnowItAll System.
OPINE is built on top of KnowItAll, a Web-based,
domain-independent information extraction system (Et-
zioni et al, 2005). Given a set of relations of interest,
KnowItAll instantiates relation-specific generic extrac-
tion patterns into extraction rules which find candidate
facts. KnowItAll?s Assessor then assigns a probability to
each candidate. The Assessor uses a form of Point-wise
Mutual Information (PMI) between phrases that is esti-
mated from Web search engine hit counts (Turney, 2001).
It computes the PMI between each fact and automatically
generated discriminator phrases (e.g., ?is a scanner? for
the isA() relationship in the context of the Scanner
class). Given fact f and discriminator d, the computed
PMI score is:
PMI(f, d) = Hits(d+ f )Hits(d)?Hits(f )
The PMI scores are converted to binary features for a
Naive Bayes Classifier, which outputs a probability asso-
ciated with each fact (Etzioni et al, 2005).
3.1 Finding Explicit Features
OPINE extracts explicit features for the given product
class from parsed review data. First, the system recur-
sively identifies both the parts and the properties of the
given product class and their parts and properties, in turn,
continuing until no candidates are found. Then, the sys-
tem finds related concepts as described in (Popescu et
al., 2004) and extracts their parts and properties. Table 1
shows that each feature type contributes to the set of final
features (averaged over 7 product classes).
Explicit Features Examples % Total
Properties ScannerSize 7%
Parts ScannerCover 52%
Features of Parts BatteryLife 24%
Related Concepts ScannerImage 9%
Related Concepts? Features ScannerImageSize 8%
Table 1: Explicit Feature Information
340
In order to find parts and properties, OPINE first ex-
tracts the noun phrases from reviews and retains those
with frequency greater than an experimentally set thresh-
old. OPINE?s Feature Assessor, which is an instantia-
tion of KnowItAll?s Assessor, evaluates each noun phrase
by computing the PMI scores between the phrase and
meronymy discriminators associated with the product
class (e.g., ?of scanner?, ?scanner has?, ?scanner comes
with?, etc. for the Scanner class). OPINE distin-
guishes parts from properties using WordNet?s IS-A hi-
erarchy (which enumerates different kinds of properties)
and morphological cues (e.g., ?-iness?, ?-ity? suffixes).
3.2 Experiments: Explicit Feature Extraction
In our experiments we use sets of reviews for 7 prod-
uct classes (1621 total reviews) which include the pub-
licly available data sets for 5 product classes from (Hu
and Liu, 2004). Hu?s system is the review mining sys-
tem most relevant to our work. It uses association rule
mining to extract frequent review noun phrases as fea-
tures. Frequent features are used to find potential opin-
ion words (only adjectives) and the system uses Word-
Net synonyms/antonyms in conjunction with a set of seed
words in order to find actual opinion words. Finally, opin-
ion words are used to extract associated infrequent fea-
tures. The system only extracts explicit features.
On the 5 datasets in (Hu and Liu, 2004), OPINE?s pre-
cision is 22% higher than Hu?s at the cost of a 3% re-
call drop. There are two important differences between
OPINE and Hu?s system: a) OPINE?s Feature Assessor
uses PMI assessment to evaluate each candidate feature
and b) OPINE incorporates Web PMI statistics in addition
to review data in its assessment. In the following, we
quantify the performance gains from a) and b).
a) In order to quantify the benefits of OPINE?s Feature
Assessor, we use it to evaluate the features extracted by
Hu?s algorithm on review data (Hu+A/R). The Feature
Assessor improves Hu?s precision by 6%.
b) In order to evaluate the impact of using Web PMI
statistics, we assess OPINE?s features first on reviews
(OP/R) and then on reviews in conjunction with the
Web (the corresponding methods are Hu+A/R+W and
OPINE). Web PMI statistics increase precision by an av-
erage of 14.5%.
Overall, 1/3 of OPINE?s precision increase over Hu?s
system comes from using PMI assessment on reviews and
the other 2/3 from the use of the Web PMI statistics.
In order to show that OPINE?s performance is robust
across multiple product classes, we used two sets of re-
views downloaded from tripadvisor.com for Ho-
tels and amazon.com for Scanners. Two annotators la-
beled a set of unique 450 OPINE extractions as correct
or incorrect. The inter-annotator agreement was 86%.
The extractions on which the annotators agreed were used
to compute OPINE?s precision, which was 89%. Fur-
Data Explicit Feature Extraction: Precision
Hu Hu+A/R Hu+A/R+W OP/R OPINE
D1 0.75 +0.05 +0.17 +0.07 +0.19
D2 0.71 +0.03 +0.19 +0.08 +0.22
D3 0.72 +0.03 +0.25 +0.09 +0.23
D4 0.69 +0.06 +0.22 +0.08 +0.25
D5 0.74 +0.08 +0.19 +0.04 +0.21
Avg 0.72 +0.06 + 0.20 +0.07 +0.22
Table 2: Precision Comparison on the Explicit Feature-
Extraction Task. OPINE?s precision is 22% better than Hu?s
precision; Web PMI statistics are responsible for 2/3 of the pre-
cision increase. All results are reported with respect to Hu?s.
Data Explicit Feature Extraction: Recall
Hu Hu+A/R Hu+A/R+W OP/R OPINE
D1 0.82 -0.16 -0.08 -0.14 -0.02
D2 0.79 -0.17 -0.09 -0.13 -0.06
D3 0.76 -0.12 -0.08 -0.15 -0.03
D4 0.82 -0.19 -0.04 -0.17 -0.03
D5 0.80 -0.16 -0.06 -0.12 -0.02
Avg 0.80 -0.16 -0.07 -0.14 -0.03
Table 3: Recall Comparison on the Explicit Feature-
Extraction Task. OPINE?s recall is 3% lower than the recall
of Hu?s original system (precision level = 0.8). All results are
reported with respect to Hu?s.
thermore, the annotators extracted explicit features from
800 review sentences (400 for each domain). The inter-
annotator agreement was 82%. OPINE?s recall on the
set of 179 features on which both annotators agreed was
73%.
3.3 Finding Opinion Phrases and Their Polarity
This subsection describes how OPINE extracts potential
opinion phrases, distinguishes between opinions and non-
opinions, and finds the polarity of each opinion in the
context of its associated feature in a particular review sen-
tence.
3.3.1 Extracting Potential Opinion Phrases
OPINE uses explicit features to identify potential opin-
ion phrases. Our intuition is that an opinion phrase as-
sociated with a product feature will occur in its vicinity.
This idea is similar to that of (Kim and Hovy, 2004) and
(Hu and Liu, 2004), but instead of using a window of size
k or the output of a noun phrase chunker, OPINE takes
advantage of the syntactic dependencies computed by the
MINIPAR parser. Our intuition is embodied by 10 ex-
traction rules, some of which are shown in Table 4. If
an explicit feature is found in a sentence, OPINE applies
the extraction rules in order to find the heads of potential
opinion phrases. Each head word together with its modi-
341
fiers is returned as a potential opinion phrase1.
Extraction Rules Examples
if ?(M,NP = f)? po = M (expensive) scanner
if ?(S = f, P,O)? po = O lamp has (problems)
if ?(S, P,O = f)? po = P I (hate) this scanner
if ?(S = f, P,O)? po = P program (crashed)
Table 4: Examples of Domain-independent Rules for
the Extraction of Potential Opinion Phrases. Nota-
tion: po=potential opinion, M=modifier, NP=noun phrase,
S=subject, P=predicate, O=object. Extracted phrases are en-
closed in parentheses. Features are indicated by the typewriter
font. The equality conditions on the left-hand side use po?s
head.
Rule Templates Rules
dep(w,w?) m(w,w?)
?v s.t. dep(w, v), dep(v, w?) ?v s.t. m(w, v), o(v, w?)
?v s.t. dep(w, v), dep(w?, v) ?v s.t. m(w, v), o(w?, v)
Table 5: Dependency Rule Templates For Finding Words
w, w? with Related SO Labels . OPINE instantiates these
templates in order to obtain extraction rules. Notation:
dep=dependent, m=modifier, o=object, v,w,w?=words.
OPINE examines the potential opinion phrases in order
to identify the actual opinions. First, the system finds the
semantic orientation for the lexical head of each poten-
tial opinion phrase. Every phrase whose head word has a
positive or negative semantic orientation is then retained
as an opinion phrase. In the following, we describe how
OPINE finds the semantic orientation of words.
3.3.2 Word Semantic Orientation
OPINE finds the semantic orientation of a word w in
the context of an associated feature f and sentence s. We
restate this task as follows:
Task Given a set of semantic orientation (SO) labels
({positive, negative, neutral}), a set of reviews and a
set of tuples (w, f , s), where w is a potential opinion
word associated with feature f in sentence s, assign a SO
label to each tuple (w, f , s).
For example, the tuple (sluggish, driver, ?I am not
happy with this sluggish driver?) would be assigned a
negative SO label.
Note: We use ?word? to refer to a potential opinion
word w and ?feature? to refer to the word or phrase which
represents the explicit feature f .
Solution OPINE uses the 3-step approach below:
1. Given the set of reviews, OPINE finds a SO label for
each word w.
2. Given the set of reviews and the set of SO labels for
words w, OPINE finds a SO label for each (w, f ) pair.
1The (S,P,O) tuples in Table 4 are automatically generated
from MINIPAR?s output.
3. Given the set of SO labels for (w, f ) pairs, OPINE
finds a SO label for each (w, f , s) input tuple.
Each of these subtasks is cast as an unsupervised col-
lective classification problem and solved using the same
mechanism. In each case, OPINE is given a set of ob-
jects (words, pairs or tuples) and a set of labels (SO la-
bels); OPINE then searches for a global assignment of la-
bels to objects. In each case, OPINE makes use of local
constraints on label assignments (e.g., conjunctions and
disjunctions constraining the assignment of SO labels to
words (Hatzivassiloglou and McKeown, 1997)).
A key insight in OPINE is that the problem of searching
for a global SO label assignment to words, pairs or tuples
while trying to satisfy as many local constraints on as-
signments as possible is analogous to labeling problems
in computer vision (e.g., model-based matching). OPINE
uses a well-known computer vision technique, relaxation
labeling (Hummel and Zucker, 1983), in order to solve
the three subtasks described above.
3.3.3 Relaxation Labeling Overview
Relaxation labeling is an unsupervised classification
technique which takes as input:
a) a set of objects (e.g., words)
b) a set of labels (e.g., SO labels)
c) initial probabilities for each object?s possible labels
d) the definition of an object o?s neighborhood (a set of
other objects which influence the choice of o?s label)
e) the definition of neighborhood features
f) the definition of a support function for an object label
The influence of an object o?s neighborhood on its la-
bel L is quantified using the support function. The sup-
port function computes the probability of the label L be-
ing assigned to o as a function of o?s neighborhood fea-
tures. Examples of features include the fact that a certain
local constraint is satisfied (e.g., the word nice partic-
ipates in the conjunction and together with some other
word whose SO label is estimated to be positive).
Relaxation labeling is an iterative procedure whose
output is an assignment of labels to objects. At each itera-
tion, the algorithm uses an update equation to reestimate
the probability of an object label based on its previous
probability estimate and the features of its neighborhood.
The algorithm stops when the global label assignment
stays constant over multiple consecutive iterations.
We employ relaxation labeling for the following rea-
sons: a) it has been extensively used in computer-vision
with good results b) its formalism allows for many types
of constraints on label assignments to be used simulta-
neously. As mentioned before, constraints are integrated
into the algorithm as neighborhood features which influ-
ence the assignment of a particular label to a particular
object.
OPINE uses the following sources of constraints:
342
a) conjunctions and disjunctions in the review text
b) manually-supplied syntactic dependency rule tem-
plates (see Table 5). The templates are automatically in-
stantiated by our system with different dependency re-
lationships (premodifier, postmodifier, subject, etc.) in
order to obtain syntactic dependency rules which find
words with related SO labels.
c) automatically derived morphological relationships
(e.g., ?wonderful? and ?wonderfully? are likely to have
similar SO labels).
d) WordNet-supplied synonymy, antonymy, IS-A and
morphological relationships between words. For exam-
ple, clean and neat are synonyms and so they are likely
to have similar SO labels.
Each of the SO label assignment subtasks previously
identified is solved using a relaxation labeling step. In the
following, we describe in detail how relaxation labeling
is used to find SO labels for words in the given review
sets.
3.3.4 Finding SO Labels for Words
For many words, a word sense or set of senses is used
throughout the review corpus with a consistently positive,
negative or neutral connotation (e.g., ?great?, ?awful?,
etc.). Thus, in many cases, a word w?s SO label in the
context of a feature f and sentence s will be the same as
its SO label in the context of other features and sentences.
In the following, we describe how OPINE?s relaxation la-
beling mechanism is used to find a word?s dominant SO
label in a set of reviews.
For this task, a word?s neighborhood is defined as
the set of words connected to it through conjunctions,
disjunctions and all other relationships previously intro-
duced as sources of constraints.
RL uses an update equation to re-estimate the prob-
ability of a word label based on its previous probabil-
ity estimate and the features of its neighborhood (see
Neighborhood Features). At iteration m, let q(w,L)(m)
denote the support function for label L of w and let
P (l(w) = L)(m) denote the probability that L is the label
of w. P (l(w) = L)(m+1) is computed as follows:
RL Update Equation (Rangarajan, 2000)
P (l(w) = L)(m+1) =
P (l(w) = L)(m)(1 + ?q(w,L)(m))
P
L? P (l(w) = L
?)(m)(1 + ?q(w,L?)(m))
where L? ? {pos, neg, neutral} and ? > 0 is an
experimentally set constant keeping the numerator and
probabilities positive. RL?s output is an assignment of
dominant SO labels to words.
In the following, we describe in detail the initialization
step, the derivation of the support function formula and
the use of neighborhood features.
RL Initialization Step OPINE uses a version of Tur-
ney?s PMI-based approach (Turney, 2003) in order to de-
rive the initial probability estimates (P (l(w) = L)(0))
for a subset S of the words. OPINE computes a SO
score so(w) for each w in S as the difference between
the PMI of w with positive keywords (e.g., ?excellent?)
and the PMI of w with negative keywords (e.g., ?awful?).
When so(w) is small, or w rarely co-occurs with the key-
words, w is classified as neutral. If so(w) > 0, then
w is positive, otherwise w is negative. OPINE then uses
the labeled S set in order to compute prior probabilities
P (l(w) = L), L ? {pos, neg, neutral} by computing
the ratio between the number of words in S labeled L
and |S|. Such probabilities are used as initial probabil-
ity estimates associated with the labels of the remaining
words.
Support Function The support function computes the
probability of each label for word w based on the labels
of objects in w?s neighborhood N .
Let Ak = {(wj , Lj)|wj ? N} , 0 < k ? 3|N | rep-
resent one of the potential assignments of labels to the
words in N . Let P (Ak)(m) denote the probability of this
particular assignment at iteration m. The support for la-
bel L of word w at iteration m is :
q(w,L)(m) =
3|N|X
k=1
P (l(w) = L|Ak)(m) ? P (Ak)(m)
We assume that the labels of w?s neighbors are inde-
pendent of each other and so the formula becomes:
q(w,L)(m) =
3|N|X
k=1
P (l(w) = L|Ak)(m)?
|N|Y
j=1
P (l(wj) = Lj)(m)
Every P (l(wj) = Lj)(m) term is the estimate for the
probability that l(wj) = Lj (which was computed at it-
eration m using the RL update equation).
The P (l(w) = L|Ak)(m) term quantifies the influence
of a particular label assignment to w?s neighborhood over
w?s label. In the following, we describe how we estimate
this term.
Neighborhood Features
Each type of word relationship which constrains the
assignment of SO labels to words (synonymy, antonymy,
etc.) is mapped by OPINE to a neighborhood feature. This
mapping allows OPINE to use simultaneously use multi-
ple independent sources of constraints on the label of a
particular word. In the following, we formalize this map-
ping.
Let T denote the type of a word relationship in R (syn-
onym, antonym, etc.) and let Ak,T represent the labels
assigned by Ak to neighbors of a word w which are con-
nected to w through a relationship of type T . We have
Ak =
?
T Ak,T and
P (l(w) = L|Ak)(m) = P (l(w) = L|
[
T
Ak,T )(m)
For each relationship type T , OPINE defines a
neighborhood feature fT (w,L,Ak,T ) which computes
P (l(w) = L|Ak,T ), the probability that w?s label is L
given Ak,T (see below). P (l(w) = L|
?
T Ak,T )(m) is
estimated combining the information from various fea-
tures about w?s label using the sigmoid function ?():
343
P (l(w) = L|Ak)(m) = ?(
jX
i=1
f i(w,L,Ak,i)(m) ? ci)
where c0, ...cj are weights whose sum is 1 and which
reflect OPINE ?s confidence in each type of feature.
Given word w, label L, relationship type T and neigh-
borhood label assignment Ak, let NT represent the subset
of w?s neighbors connected to w through a type T rela-
tionship. The feature fT computes the probability that
w?s label is L given the labels assigned by Ak to words
in NT . Using Bayes?s Law and assuming that these la-
bels are independent given l(w), we have the following
formula for fT at iteration m:
fT (w,L,Ak,T )(m) = P (l(w) = L)(m)?
|NT |Y
j=1
P (Lj |l(w) = L)
P (Lj |l(w) = L) is the probability that word wj has label
Lj if wj and w are linked by a relationship of type T and
w has label L. We make the simplifying assumption that
this probability is constant and depends only of T , L and
L?, not of the particular words wj and w. For each tuple
(T , L, Lj), L,Lj ? {pos, neg, neutral}, OPINE builds
a probability table using a small set of bootstrapped pos-
itive, negative and neutral words.
3.3.5 Finding (Word, Feature) SO Labels
This subtask is motivated by the existence of frequent
words which change their SO label based on associated
features, but whose SO labels in the context of the respec-
tive features are consistent throughout the reviews (e.g.,
in the Hotel domain, ?hot water? has a consistently posi-
tive connotation, whereas ?hot room? has a negative one).
In order to solve this task, OPINE first assigns each
(w, f) pair an initial SO label which is w?s SO label. The
system then executes a relaxation labeling step during
which syntactic relationships between words and, respec-
tively, between features, are used to update the default
SO labels whenever necessary. For example, (hot, room)
appears in the proximity of (broken, fan). If ?room?and
?fan? are conjoined by and, this suggests that ?hot? and
?broken? have similar SO labels in the context of their
respective features. If ?broken? has a strongly negative
semantic orientation, this fact contributes to OPINE?s be-
lief that ?hot? may also be negative in this context. Since
(hot, room) occurs in the vicinity of other such phrases
(e.g., stifling kitchen), ?hot? acquires a negative SO label
in the context of ?room?.
3.3.6 Finding (Word, Feature, Sentence) SO Labels
This subtask is motivated by the existence of (w,f )
pairs (e.g., (big, room)) for which w?s orientation changes
based on the sentence in which the pair appears (e.g., ? I
hated the big, drafty room because I ended up freezing.?
vs. ?We had a big, luxurious room?.)
In order to solve this subtask, OPINE first assigns each
(w, f, s) tuple an initial label which is simply the SO la-
bel for the (w, f) pair. The system then uses syntactic
relationships between words and, respectively, features
in order to update the SO labels when necessary. For
example, in the sentence ?I hated the big, drafty room
because I ended up freezing.?, ?big? and ?hate? satisfy
condition 2 in Table 5 and therefore OPINE expects them
to have similar SO labels. Since ?hate? has a strong neg-
ative connotation, ?big? acquires a negative SO label in
this context.
In order to correctly update SO labels in this last step,
OPINE takes into consideration the presence of negation
modifiers. For example, in the sentence ?I don?t like a
large scanner either?, OPINE first replaces the positive
(w, f) pair (like, scanner) with the negative labeled pair
(not like, scanner) and then infers that ?large? is likely to
have a negative SO label in this context.
3.3.7 Identifying Opinion Phrases
After OPINE has computed the most likely SO labels
for the head words of each potential opinion phrase in the
context of given features and sentences, OPINE can ex-
tract opinion phrases and establish their polarity. Phrases
whose head words have been assigned positive or nega-
tive labels are retained as opinion phrases. Furthermore,
the polarity of an opinion phrase o in the context of a fea-
ture f and sentence s is given by the SO label assigned to
the tuple (head(o), f, s) (3.3.6 shows how OPINE takes
into account negation modifiers).
3.4 Experiments
In this section we evaluate OPINE?s performance on the
following tasks: finding SO labels of words in the con-
text of known features and sentences (SO label extrac-
tion); distinguishing between opinion and non-opinion
phrases in the context of known features and sentences
(opinion phrase extraction); finding the correct polarity
of extracted opinion phrases in the context of known fea-
tures and sentences (opinion phrase polarity extraction).
While other systems, such as (Hu and Liu, 2004; Tur-
ney, 2002), have addressed these tasks to some degree,
OPINE is the first to report results. We first ran OPINE on
13841 sentences and 538 previously extracted features.
OPINE searched for a SO label assignment for 1756 dif-
ferent words in the context of the given features and sen-
tences. We compared OPINE against two baseline meth-
ods, PMI++ and Hu++.
PMI++ is an extended version of (Turney, 2002)?s
method for finding the SO label of a phrase (as an at-
tempt to deal with context-sensitive words). For a given
(word, feature, sentence) tuple, PMI++ ignores the sen-
tence, generates a phrase based on the word and the fea-
ture (e.g., (clean, room): ?clean room?) and finds its SO
label using PMI statistics. If unsure of the label, PMI++
tries to find the orientation of the potential opinion word
instead. The search engine queries use domain-specific
keywords (e.g., ?scanner?), which are dropped if they
344
lead to low counts.
Hu++ is a WordNet-based method for finding a word?s
context-independent semantic orientation. It extends
Hu?s adjective labeling method in a number of ways in
order to handle nouns, verbs and adverbs in addition to
adjectives and in order to improve coverage. Hu?s method
starts with two sets of positive and negative words and
iteratively grows each one by including synonyms and
antonyms from WordNet. The final sets are used to pre-
dict the orientation of an incoming word.
Type PMI++ Hu++ OPINE
P R P R P R
adj 0.73 0.91 +0.02 -0.17 +0.07 -0.03
nn 0.63 0.92 +0.04 -0.24 +0.11 -0.08
vb 0.71 0.88 +0.03 -0.12 +0.01 -0.01
adv 0.82 0.92 +0.02 -0.01 +0.06 +0.01
Avg 0.72 0.91 +0.03 -0.14 +0.06 -0.03
Table 6: Finding SO Labels of Potential Opinion Words
in the Context of Given Product Features and Sentences.
OPINE?s precision is higher than that of PMI++ and Hu++.
All results are reported with respect to PMI++ . Notation:
adj=adjectives, nn=nouns, vb=verbs, adv=adverbs
3.4.1 Experiments: SO Labels
On the task of finding SO labels for words in the con-
text of given features and review sentences, OPINE obtains
higher precision than both baseline methods at a small
loss in recall with respect to PMI++. As described be-
low, this result is due in large part to OPINE?s ability to
handle context-sensitive opinion words.
We randomly selected 200 (word, feature, sentence)
tuples for each word type (adjective, adverb, etc.) and
obtained a test set containing 800 tuples. Two annota-
tors assigned positive, negative and neutral labels to each
tuple (the inter-annotator agreement was 78%). We re-
tained the tuples on which the annotators agreed as the
gold standard. We ran PMI++ and Hu++ on the test data
and compared the results against OPINE?s results on the
same data.
In order to quantify the benefits of each of the three
steps of our method for finding SO labels, we also com-
pared OPINE with a version which only finds SO la-
bels for words and a version which finds SO labels for
words in the context of given features, but doesn?t take
into account given sentences. We have learned from this
comparison that OPINE?s precision gain over PMI++ and
Hu++ is mostly due to to its ability to handle context-
sensitive words in a large number of cases.
Although Hu++ does not handle context-sensitive SO
label assignment, its average precision was reasonable
(75%) and better than that of PMI++. Finding a word?s
SO label is good enough in the case of strongly positive
or negative opinion words, which account for the major-
ity of opinion instances. The method?s loss in recall is
due to not recognizing words absent from WordNet (e.g.,
?depth-adjustable?) or not having enough information to
classify some words in WordNet.
PMI++ typically does well in the presence of strongly
positive or strongly negative words. Its high recall is
correlated with decreased precision, but overall this sim-
ple approach does well. PMI++?s main shortcoming is
misclassifying terms such as ?basic? or ?visible? which
change orientation based on context.
3.4.2 Experiments: Opinion Phrases
In order to evaluate OPINE on the tasks of opinion
phrase extraction and opinion phrase polarity extraction
in the context of known features and sentences, we used a
set of 550 sentences containing previously extracted fea-
tures. The sentences were annotated with the opinion
phrases corresponding to the known features and with the
opinion polarity. We compared OPINE with PMI++ and
Hu++ on the tasks of interest. We found that OPINE had
the highest precision on both tasks at a small loss in re-
call with respect to PMI++. OPINE?s ability to identify
a word?s SO label in the context of a given feature and
sentence allows the system to correctly extract opinions
expressed by words such as ?big? or ?small?, whose se-
mantic orientation varies based on context.
Measure PMI++ Hu++ OPINE
OP Extraction: Precision 0.71 +0.06 +0.08
OP Extraction: Recall 0.78 -0.08 -0.02
OP Polarity: Precision 0.80 -0.04 +0.06
OP Polarity: Recall 0.93 +0.07 -0.04
Table 7: Extracting Opinion Phrases and Opinion Phrase
Polarity Corresponding to Known Features and Sentences.
OPINE?s precision is higher than that of PMI++ and of Hu++.
All results are reported with respect to PMI++.
4 Related Work
The key components of OPINE described in this paper are
the PMI feature assessment which leads to high-precision
feature extraction and the use of relaxation-labeling in or-
der to find the semantic orientation of potential opinion
words. The review-mining work most relevant to our re-
search is that of (Hu and Liu, 2004) and (Kobayashi et
al., 2004). Both identify product features from reviews,
but OPINE significantly improves on both. (Hu and Liu,
2004) doesn?t assess candidate features, so its precision
is lower than OPINE?s. (Kobayashi et al, 2004) employs
an iterative semi-automatic approach which requires hu-
man input at every iteration. Neither model explicitly ad-
dresses composite (feature of feature) or implicit features.
Other systems (Morinaga et al, 2002; Kushal et al, 2003)
also look at Web product reviews but they do not extract
345
opinions about particular product features. OPINE?s use
of meronymy lexico-syntactic patterns is similar to that
of many others, from (Berland and Charniak, 1999) to
(Almuhareb and Poesio, 2004).
Recognizing the subjective character and polarity of
words, phrases or sentences has been addressed by many
authors, including (Turney, 2003; Riloff et al, 2003;
Wiebe, 2000; Hatzivassiloglou and McKeown, 1997).
Most recently, (Takamura et al, 2005) reports on the
use of spin models to infer the semantic orientation of
words. The paper?s global optimization approach and use
of multiple sources of constraints on a word?s semantic
orientation is similar to ours, but the mechanism differs
and they currently omit the use of syntactic information.
Subjective phrases are used by (Turney, 2002; Pang and
Vaithyanathan, 2002; Kushal et al, 2003; Kim and Hovy,
2004) and others in order to classify reviews or sentences
as positive or negative. So far, OPINE?s focus has been on
extracting and analyzing opinion phrases corresponding
to specific features in specific sentences, rather than on
determining sentence or review polarity.
5 Conclusion
OPINE is an unsupervised information extraction system
which extracts fine-grained features, and associated opin-
ions, from reviews. OPINE?s use of the Web as a cor-
pus helps identify product features with improved preci-
sion compared with previous work. OPINE uses a novel
relaxation-labeling technique to determine the semantic
orientation of potential opinion words in the context of
the extracted product features and specific review sen-
tences; this technique allows the system to identify cus-
tomer opinions and their polarity with high precision and
recall.
6 Acknowledgments
We would like to thank the KnowItAll project and the
anonymous reviewers for their comments. Michael Ga-
mon, Costas Boulis and Adam Carlson have also pro-
vided valuable feedback. We thank Minquing Hu and
Bing Liu for providing their data sets and for their com-
ments. Finally, we are grateful to Bernadette Minton and
Fetch Technologies for their help in collecting additional
reviews. This research was supported in part by NSF
grant IIS-0312988, DARPA contract NBCHD030010,
ONR grant N00014-02-1-0324 as well as gifts from
Google and the Turing Center.
References
A. Almuhareb and M. Poesio. 2004. Attribute-based and value-
based clustering: An evaluation. In EMNLP, pages 158?165.
M. Berland and E. Charniak. 1999. Finding parts in very large
corpora. In ACL, pages 57?64.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Un-
supervised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting the se-
mantic orientation of adjectives. In ACL/EACL, pages 174?
181.
M. Hu and B. Liu. 2004. Mining and Summarizing Customer
Reviews. In KDD, pages 168?177, Seattle, WA.
R.A. Hummel and S.W. Zucker. 1983. On the foundations of
relaxation labeling processes. In PAMI, pages 267?287.
S. Kim and E. Hovy. 2004. Determining the sentiment of opin-
ions. In COLING.
N. Kobayashi, K. Inui, K. Tateishi, and T. Fukushima. 2004.
Collecting Evaluative Expressions for Opinion Extraction.
In IJCNLP, pages 596?605.
D. Kushal, S. Lawrence, and D. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and semantic classifica-
tion of product reviews. In WWW.
D. Lin. 1998. Dependency-based evaluation of MINIPAR. In
Workshop on Evaluation of Parsing Systems at ICLRE.
S. Morinaga, K. Yamanishi, K. Tateishi, and T. Fukushima.
2002. Mining product reputations on the web. In KDD.
Lee L. Pang, B and S. Vaithyanathan. 2002. Thumbs up? sen-
timent classification using machine learning techniques. In
EMNLP, pages 79?86.
A. Popescu, A. Yates, and O. Etzioni. 2004. Class extraction
from the World Wide Web. In AAAI-04 Workshop on Adap-
tive Text Extraction and Mining, pages 68?73.
A. Rangarajan. 2000. Self annealing and self annihilation: uni-
fying deterministic annealing and relaxation labeling. In Pat-
tern Recognition, 33:635-649.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning Subjective
Nouns Using Extraction Pattern Bootstrapping. In CoNLL,
pages 25?32s.
H. Takamura, T. Inui, and M. Okumura. 2005. Extracting Se-
mantic Orientations of Words using Spin Model. In ACL,
pages 133?141.
P. D. Turney. 2001. Mining the Web for Synonyms: PMI-IR
versus LSA on TOEFL. In Procs. of the Twelfth European
Conference on Machine Learning (ECML-2001), pages 491?
502, Freiburg, Germany.
P. D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of reviews.
In Procs. of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL?02), pages 417?424.
P. Turney. 2003. Inference of Semantic Orientation from Asso-
ciation. In CoRR cs. CL/0309034.
J. Wiebe. 2000. Learning subjective adjectives from corpora.
In AAAI/IAAI, pages 735?740.
346
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 563?570, Vancouver, October 2005. c?2005 Association for Computational Linguistics
KnowItNow: Fast, Scalable Information Extraction from the Web
Michael J. Cafarella, Doug Downey, Stephen Soderland, Oren Etzioni
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350
{mjc,ddowney,soderlan,etzioni}@cs.washington.edu
Abstract
Numerous NLP applications rely on
search-engine queries, both to ex-
tract information from and to com-
pute statistics over the Web corpus.
But search engines often limit the
number of available queries. As a
result, query-intensive NLP applica-
tions such as Information Extraction
(IE) distribute their query load over
several days, making IE a slow, off-
line process.
This paper introduces a novel archi-
tecture for IE that obviates queries to
commercial search engines. The ar-
chitecture is embodied in a system
called KNOWITNOW that performs
high-precision IE in minutes instead
of days. We compare KNOWITNOW
experimentally with the previously-
published KNOWITALL system, and
quantify the tradeoff between re-
call and speed. KNOWITNOW?s ex-
traction rate is two to three orders
of magnitude higher than KNOW-
ITALL?s.
1 Background and Motivation
Numerous modern NLP applications use the Web as their
corpus and rely on queries to commercial search engines
to support their computation (Turney, 2001; Etzioni et al,
2005; Brill et al, 2001). Search engines are extremely
helpful for several linguistic tasks, such as computing us-
age statistics or finding a subset of web documents to an-
alyze in depth; however, these engines were not designed
as building blocks for NLP applications. As a result,
the applications are forced to issue literally millions of
queries to search engines, which limits the speed, scope,
and scalability of the applications. Further, the applica-
tions must often then fetch some web documents, which
at scale can be very time-consuming.
In response to heavy programmatic search engine use,
Google has created the ?Google API? to shunt program-
matic queries away from Google.com and has placed hard
quotas on the number of daily queries a program can is-
sue to the API. Other search engines have also introduced
mechanisms to limit programmatic queries, forcing ap-
plications to introduce ?courtesy waits? between queries
and to limit the number of queries they issue.
To understand these efficiency problems in more detail,
consider the KNOWITALL information extraction sys-
tem (Etzioni et al, 2005). KNOWITALL has a generate-
and-test architecture that extracts information in two
stages. First, KNOWITALL utilizes a small set of domain-
independent extraction patterns to generate candidate
facts (cf. (Hearst, 1992)). For example, the generic pat-
tern ?NP1 such as NPList2? indicates that the head of
each simple noun phrase (NP) in NPList2 is a member of
the class named in NP1. By instantiating the pattern for
class City, KNOWITALL extracts three candidate cities
from the sentence: ?We provide tours to cities such as
Paris, London, and Berlin.? Note that it must also fetch
each document that contains a potential candidate.
Next, extending the PMI-IR algorithm (Turney, 2001),
KNOWITALL automatically tests the plausibility of the
candidate facts it extracts using pointwise mutual in-
formation (PMI) statistics computed from search-engine
hit counts. For example, to assess the likelihood that
?Yakima? is a city, KNOWITALL will compute the PMI
between Yakima and a set of k discriminator phrases that
tend to have high mutual information with city names
(e.g., the simple phrase ?city?). Thus, KNOWITALL re-
quires at least k search-engine queries for every candidate
extraction it assesses.
Due to KNOWITALL?s dependence on search-engine
queries, large-scale experiments utilizing KNOWITALL
take days and even weeks to complete, which makes re-
search using KNOWITALL slow and cumbersome. Pri-
vate access to Google-scale infrastructure would provide
563
sufficient access to search queries, but at prohibitive cost,
and the problem of fetching documents (even if from a
cached copy) would remain (as we discuss in Section
2.1). Is there a feasible alternative Web-based IE system?
If so, what size Web index and how many machines are
required to achieve reasonable levels of precision/recall?
What would the architecture of this IE system look like,
and how fast would it run?
To address these questions, this paper introduces a
novel architecture for web information extraction. It
consists of two components that supplant the generate-
and-test mechanisms in KNOWITALL. To generate ex-
tractions rapidly we utilize our own specialized search
engine, called the Bindings Engine (or BE), which ef-
ficiently returns bindings in response to variabilized
queries. For example, in response to the query ?Cities
such as ProperNoun(Head(?NounPhrase?))?, BE will
return a list of proper nouns likely to be city names. To
assess these extractions, we use URNS, a combinatorial
model, which estimates the probability that each extrac-
tion is correct without using any additional search engine
queries.1 For further efficiency, we introduce an approx-
imation to URNS, based on frequency of extractions? oc-
currence in the output of BE, and show that it achieves
comparable precision/recall to URNS.
Our contributions are as follows:
1. We present a novel architecture for Information Ex-
traction (IE), embodied in the KNOWITNOW sys-
tem, which does not depend on Web search-engine
queries.
2. We demonstrate experimentally that KNOWITNOW
is the first system able to extract tens of thousands
of facts from the Web in minutes instead of days.
3. We show that KNOWITNOW?s extraction rate is two
to three orders of magnitude greater than KNOW-
ITALL?s, but this increased efficiency comes at the
cost of reduced recall. We quantify this tradeoff for
KNOWITNOW?s 60,000,000 page index and extrap-
olate how the tradeoff would change with larger in-
dices.
Our recent work has described the BE search engine
in detail (Cafarella and Etzioni, 2005), and also analyzed
the URNS model?s ability to compute accurate probability
estimates for extractions (Downey et al, 2005). However,
this is the first paper to investigate the composition of
these components to create a fast IE system, and to com-
pare it experimentally to KNOWITALL in terms of time,
1In contrast, PMI-IR, which is built into KNOWITALL, re-
quires multiple search engine queries to assess each potential
extraction.
recall, precision, and extraction rate. The frequency-
based approximation to URNS and the demonstration of
its success are also new.
The remainder of the paper is organized as follows.
Section 2 provides an overview of BE?s design. Sec-
tion 3 describes the URNS model and introduces an ef-
ficient approximation to URNS that achieves similar pre-
cision/recall. Section 4 presents experimental results. We
conclude with related and future work in Sections 5 and
6.
2 The Bindings Engine
This section explains how relying on standard search en-
gines leads to a bottleneck for NLP applications, and pro-
vides a brief overview of the Bindings Engine (BE)?our
solution to this problem. A comprehensive description of
BE appears in (Cafarella and Etzioni, 2005).
Standard search engines are computationally expen-
sive for IE and other NLP tasks. IE systems issue multiple
queries, downloading all pages that potentially match an
extraction rule, and performing expensive processing on
each page. For example, such systems operate roughly as
follows on the query (?cities such as ?NounPhrase??):
1. Perform a traditional search engine query to find
all URLs containing the non-variable terms (e.g.,
?cities such as?)
2. For each such URL:
(a) obtain the document contents,
(b) find the searched-for terms (?cities such as?) in
the document text,
(c) run the noun phrase recognizer to determine
whether text following ?cities such as? satisfies
the linguistic type requirement,
(d) and if so, return the string
We can divide the algorithm into two stages: obtaining
the list of URLs from a search engine, and then process-
ing them to find the ?NounPhrase? bindings. Each stage
poses its own scalability and speed challenges. The first
stage makes a query to a commercial search engine; while
the number of available queries may be limited, a single
one executes relatively quickly. The second stage fetches
a large number of documents, each fetch likely resulting
in a random disk seek; this stage executes slowly. Nat-
urally, this disk access is slow regardless of whether it
happens on a locally-cached copy or on a remote doc-
ument server. The observation that the second stage is
slow, even if it is executed locally, is important because
it shows that merely operating a ?private? search engine
does not solve the problem (see Section 2.1).
The Bindings Engine supports queries contain-
ing typed variables (such as NounPhrase) and
564
string-processing functions (such as ?head(X)? or
?ProperNoun(X)?) as well as standard query terms. BE
processes a variable by returning every possible string
in the corpus that has a matching type, and that can be
substituted for the variable and still satisfy the user?s
query. If there are multiple variables in a query, then all
of them must simultaneously have valid substitutions.
(So, for example, the query ?<NounPhrase> is located
in <NounPhrase>? only returns strings when noun
phrases are found on both sides of ?is located in?.) We
call a string that meets these requirements a binding for
the variable in question. These queries, and the bindings
they elicit, can usefully serve as part of an information
extraction system or other common NLP tasks (such as
gathering usage statistics). Figure 1 illustrates some of
the queries that BE can handle.
president Bush <Verb>
cities such as ProperNoun(Head(<NounPhrase>))
<NounPhrase> is the CEO of <NounPhrase>
Figure 1: Examples of queries that can be handled by
BE. Queries that include typed variables and string-
processing functions allow NLP tasks to be done ef-
ficiently without downloading the original document
during query processing.
BE?s novel neighborhood index enables it to process
these queries with O(k) random disk seeks and O(k) se-
rial disk reads, where k is the number of non-variable
terms in its query. As a result, BE can yield orders of
magnitude speedup as shown in the asymptotic analysis
later in this section. The neighborhood index is an aug-
mented inverted index structure. For each term in the cor-
pus, the index keeps a list of documents in which the term
appears and a list of positions where the term occurs, just
as in a standard inverted index (Baeza-Yates and Ribeiro-
Neto, 1999). In addition, the neighborhood index keeps
a list of left-hand and right-hand neighbors at each posi-
tion. These are adjacent text strings that satisfy a recog-
nizer for one of the target types, such as NounPhrase.
As with a standard inverted index, a term?s list is pro-
cessed from start to finish, and can be kept on disk as a
contiguous piece. The relevant string for a variable bind-
ing is included directly in the index, so there is no need
to fetch the source document (thus causing a disk seek).
Expensive processing such as part-of-speech tagging or
shallow syntactic parsing is performed only once, while
building the index, and is not needed at query time. It
is important to note that simply preprocessing the corpus
and placing the results in a database would not avoid disk
seeks, as we would still have to explicitly fetch these re-
sults. The run-time efficiency of the neighborhood index
Query Time Index Space
BE O(k) O(N)
Standard engine O(k + B) O(N)
Table 1: BE yields considerable savings in query time
over a standard search engine. k is the number of con-
crete terms in the query, B is the number of variable
bindings found in the corpus, and N is the number of
documents in the corpus. N and B are typically ex-
tremely large, while k is small.
comes from integrating the results of corpus processing
with the inverted index (which determines which of those
results are relevant).
The neighborhood index avoids the need to return to
the original corpus, but it can consume a large amount
of disk space, as parts of the corpus text are folded into
the index several times. To conserve space, we perform
simple dictionary-lookup compression of strings in the
index. The storage penalty will, of course, depend on the
exact number of different types added to the index. In our
experiments, we created a useful IE system with a small
number of types (including NounPhrase) and found that
the neighborhood index increased disk space only four
times that of a standard inverted index.
Asymptotic Analysis:
In our asymptotic analysis of BE?s behavior, we count
query time as a function of the number of random disk
seeks, since these seeks dominate all other processing
tasks. Index space is simply the number of bytes needed
to store the index (not including the corpus itself).
Table 1 shows that BE requires only O(k) random disk
seeks to process queries with an arbitrary number of vari-
ables whereas a standard engine takes O(k + B), where
k is the number of concrete query terms, and B is the
number of bindings found in a corpus of N documents.
Thus, BE?s performance is the same as that of a standard
search engine for queries containing only concrete terms.
For variabilized queries, N may be in the billions and B
will tend to grow with N . In our experiments, eliminating
the B term from our query processing time has resulted
in speedups of two to three orders of magnitude over a
standard search engine. The speedup is at the price of a
small constant multiplier to index size.
2.1 Discussion
While BE has some attractive properties for NLP compu-
tations, is it necessary? Could fast, large-scale informa-
tion extraction be achieved merely by operating a ?pri-
vate? search engine?
The release of open-source search engines such as
Nutch2, coupled with the dropping price of CPUs and
2http://lucene.apache.org/nutch/
565
8.16
0.06
0
1
2
3
4
5
6
7
8
9
10
BE Nutch
El
ap
se
d 
m
in
u
te
s
Figure 2: Average time to return the relevant bindings
in response to a set of queries was 0.06 CPU minutes
for BE, compared to 8.16 CPU minutes for the com-
parable processing on Nutch. This is a 134-fold speed
up. The CPU resources, network, and index size were
the same for both systems.
disks, makes it feasible for NLP researchers to operate
their own large-scale search engines. For example, Tur-
ney operates a search engine with a terabyte-sized index
of Web pages, running on a local eight-machine Beowulf
cluster (Turney, 2004). Private search engines have two
advantages. First, there is no query quota or need for
?courtesy waits? between queries. Second, since the en-
gine is local, network latency is minimal.
However, to support IE, we must also execute the sec-
ond stage of the algorithm (see the beginning of this sec-
tion). In this stage, each document that matches a query
has to be retrieved from an arbitrary location on a disk.3
Thus, the number of random disk seeks scales linearly
with the number of documents retrieved. Moreover, many
NLP applications require the extraction of strings match-
ing particular syntactic or semantic types from each page.
The lack of linguistic data in the search engine?s index
means that many pages are fetched only to be discarded
as irrelevant.
To quantify the speedup due to BE, we compared it to a
standard search index built on the open-source Nutch en-
gine. All of our Nutch and BE experiments were carried
out on the same corpus of 60 million Web pages and were
run on a cluster of 23 dual-Xeon machines, each with two
local 140 Gb disks and 4 Gb of RAM. We set al config-
uration values to be exactly the same for both Nutch and
BE. BE gave a 134-fold speed up on average query pro-
cessing time when compared to the same queries with the
Nutch index, as shown in Figure 2.
3Moving the disk head to an arbitrary location on the disk
is a mechanical operation that takes about 5 milliseconds on
average.
3 The URNS Model
To realize the speedup from BE, KNOWITNOW must also
avoid issuing search engine queries to validate the cor-
rectness of each extraction, as required by PMI compu-
tation. We have developed a probabilistic model obviat-
ing search-engine queries for assessment. The intuition
behind this model is that correct instances of a class or
relation are likely to be extracted repeatedly, while ran-
dom errors by an IE system tend to have low frequency
for each distinct incorrect extraction.
Our probabilistic model, which we call URNS, takes the
form of a classic ?balls-and-urns? model from combina-
torics. We think of IE abstractly as a generative process
that maps text to extractions. Each extraction is modeled
as a labeled ball in an urn. A label represents either an
instance of the target class or relation, or represents an
error. The information extraction process is modeled as
repeated draws from the urn, with replacement.
Formally, the parameters that characterize an urn are:
? C ? the set of unique target labels; |C| is the number
of unique target labels in the urn.
? E ? the set of unique error labels; |E| is the number
of unique error labels in the urn.
? num(b) ? the function giving the number of balls
labeled by b where b ? C ? E. num(B) is the
multi-set giving the number of balls for each label
b ? B.
The goal of an IE system is to discern which of the
labels it extracts are in fact elements of C, based on re-
peated draws from the urn. Thus, the central question we
are investigating is: given that a particular label x was
extracted k times in a set of n draws from the urn, what
is the probability that x ? C? We can express the prob-
ability that an element extracted k of n times is of the
target relation as follows.
P (x ? C|x appears k times in n draws) =
?
r?num(C)( rs )k(1 ? rs )n?k
?
r??num(C?E)( r
?
s )k(1 ? r
?
s )n?k
(1)
where s is the total number of balls in the urn, and the
sum is taken over possible repetition rates r.
A few numerical examples illustrate the behavior of
this equation. Let |C| = |E| = 2, 000 and assume
for simplicity that all labels are repeated on the same
number of balls (num(ci) = RC for all ci ? C, and
num(ei) = RE for all ei ? E). Assume that the ex-
traction rules have precision p = 0.9, which means that
RC = 9 ? RE ? target balls are nine times as common
in the urn as error balls. Now, for k = 3 and n = 10, 000
we have P (x ? C) = 93.0%. Thus, we see that a small
number of repetitions can yield high confidence in an ex-
traction. However, when the sample size increases so that
566
n = 20, 000, and the other parameters are unchanged,
then P (x ? C) drops to 19.6%. On the other hand, if
C balls repeat much more frequently than E balls, say
RC = 90?RE (with |E| set to 20,000, so that p remains
unchanged), then P (x ? C) rises to 99.9%.
The above examples enable us to illustrate the advan-
tages of URNS over the noisy-or model used in previous
work. The noisy-or model assumes that each extraction is
an independent assertion that the extracted label is ?true,?
an assertion that is correct a fraction p of the time. The
noisy-or model assigns the following probability to ex-
tractions:
Pnoisy?or(x ? C|x appears k times) = 1 ? (1 ? p)k
Therefore, the noisy-or model will assign the same
probability ? 99.9% ? in all three of the above exam-
ples, although this is only correct in the case for which
n = 10, 000 and RC = 90?RE . As the other two exam-
ples show, for different sample sizes or repetition rates,
the noisy-or model can be highly inaccurate. This is not
surprising given that the noisy-or model ignores the sam-
ple size and the repetition rates.
URNS uses an EM algorithm to estimate its parameters,
and currently the algorithm takes roughly three minutes
to terminate.4 Fortunately, we determined experimen-
tally that we can approximate URNS?s precision and recall
using a far simpler frequency-based assessment method.
This is true because good precision and recall merely re-
quire an appropriate ordering of the extractions for each
relation, and not accurate probabilities for each extrac-
tion. For unary relations, we use the simple approxima-
tion that items extracted more often are more likely to
be true, and order the extractions from most to least ex-
tracted. For binary relations like CapitalOf(X,y),
in which we extract several different candidate capitals y
for each known country X, we use a smoothed frequency
estimate to order the extractions. Let freq(R(X, y)) de-
note the number of times that the binary relation R(X, y)
is extracted; we define:
smoothed freq(R(X, y)) = freq(R(X, y))maxy? freq(R(X, y?)) + 1
We found that sorting by smoothed frequency (in de-
scending order) performed better than simply sorting by
freq for relations R(X, y) in which different known X val-
ues may have widely varying Web presence.
Unlike URNS, our frequency-based assessment does
not yield accurate probabilities to associate with each ex-
traction, but for the purpose of returning a ranked list of
high-quality extractions it is comparable to URNS (see
4This code has not been optimized at all. We believe that
we can easily reduce its running time to less than a minute on
average, and perhaps substantially more.
0.75
0.8
0.85
0.9
0.95
1
0 50 100 150 200 250
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowItNow-URNS
KnowItAll-PMI
Figure 3: Country: KNOWITALL maintains some-
what higher precision than KNOWITNOW throughout
the recall-precision curve.
Figures 3 through 6), and it has the advantage of being
much faster. Thus, in the experiments reported on below,
we use frequency-based assessment as part of KNOWIT-
NOW.
4 Experimental Results
This section contrasts the performance of KNOWITNOW
and KNOWITALL experimentally. Before considering the
experiments in detail, we note that a key advantage of
KNOWITNOW is that it does not make any queries to Web
search engines. As a result, KNOWITNOW?s scale is not
limited by a query quota, though it is limited by the size
of its index.
We report on the following metrics:
? Recall: how many distinct extractions does each
system return at high precision?5
? Time: how long did each system take to produce
and rank its extractions?
? Extraction Rate: how many distinct high-quality
extractions does the system return per minute? The
extraction rate is simply recall divided by time.
We contrast KNOWITALL and KNOWITNOW?s preci-
sion/recall curves in Figures 3 through 6. We com-
pared KNOWITNOW with KNOWITALL on four rela-
tions: Corp, Country, CeoOf(Corp,Ceo), and
CapitalOf(Country,City). The unary relations
were chosen to examine the difference between a relation
with a small number of correct instances (Country) and
one with a large number of extractions (Corp). The bi-
nary relations were chosen to cover both functional rela-
tions (CapitalOf) and set-valued relations (CeoOf?
we treat former CEOs as correct instances of the relation).
5Since we cannot compute ?true recall? for most relations
on the Web, the paper uses the term ?recall? to refer to the size
of the set of facts extracted.
567
0.75
0.8
0.85
0.9
0.95
1
0 50 100 150 200
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowItNow-URNS
KnowItAll-PMI
Figure 4: CapitalOf: KNOWITNOW does nearly as
well as KNOWITALL, but has more difficulty than
KNOWITALL with sparse data for capitals of more ob-
scure countries.
For the two unary relations, both systems created ex-
traction rules from eight generic patterns. These are hy-
ponym patterns like ?NP1 {,} such as NPList2? or ?NP2
{,} and other NP1?, which extract members of NPList2
or NP2 as instances of NP1. For the binary relations,
the systems instantiated rules from four generic patterns.
These are patterns for a generic ?of? relation. They are
?NP1 , rel of NP2?, ?NP1 the rel of NP2?, ?rel of NP2
, NP1?, and ?NP2 rel NP1?. When rel is instantiated for
CeoOf, these patterns become ?NP1 , CEO of NP2? and
so forth.
Both KNOWITNOW and KNOWITALL merge extrac-
tions with slight variants in the name, such as those dif-
fering only in punctuation or whitespace, or in the pres-
ence or absence of a corporate designator. For binary
extractions, CEOs with the same last name and same
company were also merged. Both systems rely on the
OpenNlp maximum-entropy part-of-speech tagger and
chunker (Ratnaparkhi, 1996), but KNOWITALL applies
them to pages downloaded from the Web based on the re-
sults of Google queries, whereas KNOWITNOW applies
them once to crawled and indexed pages.6 Overall, each
of the above elements of KNOWITALL and KNOWIT-
NOW are the same to allow for controlled experiments.
Whereas KNOWITNOW runs a small number of vari-
abilized queries (one for each extraction pattern, for
each relation), KNOWITALL requires a stopping crite-
rion. Otherwise, KNOWITALL will continue to query
Google and download URLs found in its result pages over
many days and even weeks. We allowed a total of 6 days
of search time for KNOWITALL, allocating more search
for the relations that continued to be most productive. For
CeoOf KNOWITNOW returned all pairs of Corp,Ceo
6Our time measurements for KNOWITALL are not affected
by the tagging and chunking time because it is dominated
by time required to query Google, waiting a second between
queries.
0.75
0.8
0.85
0.9
0.95
1
0 5,000 10,000 15,000 20,000 25,000
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowItNow-URNS
KnowItAll-PMI
Figure 5: Corp: KNOWITALL?s PMI assessment main-
tains high precision. KNOWITNOW has low recall up
to precision 0.85, then catches up with KNOWITALL.
in its corpus; KNOWITALL searched for CEOs of a ran-
dom selection of 10% of the corporations it found, and
we projected the total extractions and search effort for all
corporations. For CapitalOf, both KNOWITNOW and
KNOWITALL looked for capitals of a set of 195 coun-
tries.
Table 2 shows the number of queries, search time, dis-
tinct correct extractions at precision 0.8, and extraction
rate for each relation. Search time for KNOWITNOW is
measured in seconds and search time for KNOWITALL
is measured in hours. The number of extractions per
minute counts the distinct correct extractions. Since we
limit KNOWITALL to one Google query per second, the
time for KNOWITALL is proportional to the number of
queries. KNOWITNOW?s extraction rate is from 275 to
4,707 times that of KNOWITALL at this level of preci-
sion.
While the number of distinct correct extractions from
KNOWITNOW at precision 0.8 is roughly comparable to
that of 6 days search effort from KNOWITALL, the sit-
uation is different at precision 0.9. KNOWITALL?s PMI
assessor is able to maintain higher precision than KNOW-
ITNOW?s frequency-based assessor. The number of cor-
rect corporations for KNOWITNOW drops from 23,128 at
precision 0.8 to 1,116 at precision 0.9. KNOWITALL is
able to identify 17,620 correct corporations at precision
0.9. Even with the drop in recall, KNOWITNOW?s ex-
traction rate is still 305 times higher than KNOWITALL?s.
The reason for KNOWITNOW?s difficulty at precision 0.9
is due to extraction errors that occur with high frequency,
particularly generic references to companies (?the Seller
is a corporation ...?, ?corporations such as Banks?, etc.)
and truncation of certain company names by the extrac-
tion rules. The more expensive PMI-based assessment
was not fooled by these systematic extraction errors.
Figures 3 through 6 show the recall-precision curves
for KNOWITNOW with URNS assessment, KNOWIT-
NOW with the simpler frequency-based assessment, and
568
Google Queries Time Extractions Extractions per minute
NOW ALL NOW (sec) ALL (hrs) NOW ALL NOW ALL ratio
Corp 0 (16) 201,878 42 56.1 23,128 23,617 33,040 7.02 4,707
Country 0 (16) 35,480 42 9.9 161 203 230 0.34 672
CeoOf 0 (6) 263,646 51 73.2 2,402 5,823 2,836 1.33 2,132
CapitalOf 0 (6) 17,216 55 4.8 169 192 184 0.67 275
Table 2: Comparison of KNOWITNOW with KNOWITALL for four relations, showing number of Google queries
(local BE queries in parentheses), search time, correct extractions at precision 0.8, and extraction rate (the
number of correct extractions at precision 0.8 per minute of search). Overall, KNOWITNOW took a total of
slightly over 3 minutes as compared to a total of 6 days of search for KNOWITALL.
0.75
0.8
0.85
0.9
0.95
1
0 2,000 4,000 6,000
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowitNow-URNS
KnowItAll-PMI
Figure 6: CeoOf: KNOWITNOW has difficulty dis-
tinguishing low frequency correct extractions from
noise. KNOWITALL is able to cope with the sparse
data more effectively.
KNOWITALL with PMI-based assessment. For each of
the four relations, PMI is able to maintain a higher pre-
cision than either frequency-based or URNS assessment.
URNS and frequency-based assessment give roughly the
same levels of precision.
For the relations with a small number of correct in-
stances, Country and CapitalOf, KNOWITNOW is
able to identify 70-80% as many instances as KNOW-
ITALL at precision 0.9. In contrast, Corp and CeoOf
have a huge number of correct instances and a long tail
of low frequency extractions that KNOWITNOW has dif-
ficulty distinguishing from noise. Over one fourth of
the corporations found by KNOWITALL had Google hit
counts less than 10,500, a sparseness problem that was
exacerbated by KNOWITNOW?s limited index size.
Figure 7 shows projected recall from larger KNOW-
ITNOW indices, fitting a sigmoid curve to the recall
from index size of 10M, 20M, up to 60M pages. The
curve was fitted using logistic regression, and is restricted
to asymptote at the level reported for Google-based
KNOWITALL for each relation. We report re-
call at precision 0.9 for capitals of 195 coun-
tries and CEOs of a random selection of the
top 5,000 corporations as ranked by PMI.
Recall is defined as the percent of countries with a
0
0.2
0.4
0.6
0.8
1
0 100 200 300 400
KnowItNow index size (millions)
R
ec
al
l a
t 0
.
9 
pr
ec
is
io
n
KnowItNow CeoOf
Google CeoOf
KnowItNow CapitalOf
Google CapitalOf
Figure 7: Projections of recall (at precision 0.9) as a
function of KNOWITNOW index size. At 400 million
pages, KNOWITNOW?s recall rapidly approaches the
recall achieved by KNOWITALL using roughly 300,000
Google queries.
correct capital or the number of correct CEOs divided by
the number of corporations.
The curve for CeoOf is rising steeply enough that a
400 million page KNOWITNOW index may approach the
same level of recall yielded by KNOWITALL when it uses
300,000 Google queries. As shown in Table 2, KNOW-
ITALL takes slightly more than three days to generate
these results. KNOWITNOW would operate over a cor-
pus 6.7 times its current one, but the number of required
random disk seeks (and the asymptotic run time analy-
sis) would remain the same. We thus expect that with a
larger corpus we can construct a KNOWITNOW system
that reproduces KNOWITALL levels of precision and re-
call while still executing in the order of a few minutes.
5 Related Work
There has been very little work published on how to make
NLP computations such as PMI-IR and IE fast for large
corpora. Indeed, extraction rate is not a metric typically
used to evaluate IE systems, but we believe it is an im-
portant metric if IE is to scale.
Hobbs et al point out the advantage of fast text
processing for rapid system development (Hobbs et al,
1992). They could test each change to system parameters
569
and domain-specific patterns on a large sample of docu-
ments, having moved from a system that took 36 hours to
process 100 documents to FASTUS, which took only 11
minutes. This allowed them to develop one of the highest
performing MUC-4 systems in only one month.
While there has been extensive work in the IR and
Web communities on improvements to the standard in-
verted index scheme, there has been little work on effi-
cient large-scale search to support natural language ap-
plications. One exception is Resnik?s Linguist?s Search
Engine (Elkiss and Resnik, 2004), a tool for searching
large corpora of parse trees. There is little published in-
formation about its indexing system, but the user man-
ual suggests its corpus is a combination of indexed sen-
tences and user-specific document collections driven by
the user?s AltaVista queries. In contrast, the BE system
has a single index, constructed just once, that serves all
queries. There is no published performance data avail-
able for Resnik?s system.
6 Conclusions and Future Directions
In previous work, statistical NLP computation over large
corpora has been a slow, offline process, as in KNOW-
ITALL (Etzioni et al, 2005) and also in PMI-IR appli-
cations such as sentiment classification (Turney, 2002).
Technology trends, and open source search engines such
as Nutch, have made it feasible to create ?private? search
engines that index large collections of documents; but as
shown in Figure 2, firing large numbers of queries at pri-
vate search engines is still slow.
This paper described a novel and practical approach
towards substantially speeding up IE. We described
KNOWITNOW, which extracts thousands of facts in min-
utes instead of days. Furthermore, we sketched URNS,
a probabilistic model that both obviates the need for
search-engine queries and outputs more accurate prob-
abilities than PMI-IR. Finally, we introduced a simple,
efficient approximation to URNS, whose probability esti-
mates are not as good, but which has comparable preci-
sion/recall to URNS, making it an appropriate assessor for
KNOWITNOW.
The speed and massively improved extraction rate of
KNOWITNOW come at the cost of reduced recall. We
quantified this tradeoff in Table 2, and also argued that as
KNOWITNOW?s index size increases from 60 million to
400 million pages, KNOWITNOW would achieve in min-
utes the same precision/recall that takes KNOWITALL
days to obtain. Of course, a hybrid approach is possi-
ble where KNOWITNOW has, say, a 100 million page
index and, when necessary, augments its results with a
limited number of queries to Google. Investigating the
extraction-rate/recall tradeoff in such a hybrid system is
a natural next step.
While our experiments have used the Web corpus, our
approach transfers readily to other large corpora; exper-
imentation with other corpora is another topic for future
work. In conclusion, we believe that our techniques trans-
form IE from a slow, offline process to an online one.
They could open the door to a new class of interactive IE
applications, of which KNOWITNOW is merely the first.
7 Acknowledgments
This research was supported in part by NSF grant IIS-
0312988, DARPA contract NBCHD030010, ONR grant
N00014-02-1-0324, and gifts from Google and the Tur-
ing Center.
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern Informa-
tion Retrieval. Addison Wesley.
E. Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng. 2001.
Data-intensive question answering. In Procs. of Text RE-
trieval Conference (TREC-10), pages 393?400.
M. Cafarella and O. Etzioni. 2005. A Search Engine for Nat-
ural Language Applications. In Procs. of the 14th Interna-
tional World Wide Web Conference (WWW 2005).
D. Downey, O. Etzioni, and S. Soderland. 2005. A Probabilistic
Model of Redundancy in Information Extraction. In Procs.
of the 19th International Joint Conference on Artificial Intel-
ligence (IJCAI 2005).
E. Elkiss and P. Resnik, 2004. The Linguist?s Search Engine
User?s Guide. University of Maryland.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Un-
supervised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
M. Hearst. 1992. Automatic Acquisition of Hyponyms from
Large Text Corpora. In Procs. of the 14th International
Conference on Computational Linguistics, pages 539?545,
Nantes, France.
J.R. Hobbs, D. Appelt, M. Tyson, J. Bear, and D. Israel. 1992.
Description of the FASTUS system used for MUC-4. In
Procs. of the Fourth Message Understanding Conference,
pages 268?275.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In Procs. of the Empirical Methods in Natural Language
Processing Conference, Univ. of Pennsylvania.
P. D. Turney. 2001. Mining the Web for Synonyms: PMI-IR
versus LSA on TOEFL. In Procs. of the Twelfth European
Conference on Machine Learning (ECML-2001), pages 491?
502, Freiburg, Germany.
P. D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of reviews.
In Procs. of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL?02), pages 417?424.
P. D. Turney, 2004. Waterloo MultiText System. Institute for
Information Technology, Nat?l Research Council of Canada.
570
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 32?33,
Vancouver, October 2005.
OPINE: Extracting Product Features and Opinions from Reviews
Ana-Maria Popescu Bao Nguyen
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350
{amp,omicron,etzioni}@cs.washington.edu
Oren Etzioni
Abstract
Consumers have to often wade
through a large number of on-line re-
views in order to make an informed
product choice. We introduce OPINE,
an unsupervised, high-precision in-
formation extraction system which
mines product reviews in order to
build a model of product features and
their evaluation by reviewers.
1 Introduction
The Web contains a wealth of customer reviews - as a
result, the problem of ?review mining? has seen increas-
ing attention over the last few years from (Turney, 2003;
Hu and Liu, 2004) and many others. We decompose the
problem of review mining into the following subtasks:
a) Identify product features, b) Identify opinions re-
garding product features, c) Determine the polarity of
each opinion and d) Rank opinions according to their
strength (e.g., ?abominable? is stronger than ?bad?).
We introduce OPINE, an unsupervised information ex-
traction system that embodies a solution to each of the
above subtasks. The remainder of this paper is organized
as follows: Section 2 describes OPINE?s components to-
gether with their experimental evaluation and Section 3
describes the related work.
2 OPINE Overview
OPINE is built on top of KNOWITALL, a Web-based,
domain-independent information extraction system (Et-
zioni et al, 2005). Given a set of relations of inter-
est, KNOWITALL instantiates relation-specific generic
extraction patterns into extraction rules which find can-
didate facts. The Assessor module then assigns a proba-
bility to each candidate using a form of Point-wise Mu-
tual Information (PMI) between phrases that is estimated
from Web search engine hit counts (Turney, 2003). It
Input: product class C, reviews R.
Output: set of [feature, ranked opinion list] tuples
R?? parseReviews(R);
E? findExplicitFeatures(R?, C);
O? findOpinions(R?, E);
CO? clusterOpinions(O);
I? findImplicitFeatures(CO, E);
RO? rankOpinions(CO);
{(f , oi, ...oj)}?outputTuples(RO, I?E);
Figure 1: OPINE Overview.
computes the PMI between each fact and discriminator
phrases (e.g., ?is a scanner? for the isA() relationship
in the context of the Scanner class). Given fact f and
discriminator d, the computed PMI score is:
PMI(f, d) = Hits(d + f )Hits(d)?Hits(f )
The PMI scores are converted to binary features for a
Naive Bayes Classifier, which outputs a probability asso-
ciated with each fact.
Given product class C with instances I and reviews R,
OPINE?s goal is to find the set of (feature, opinions) tuples
{(f, oi, ...oj)} s.t. f ? F and oi, ...oj ? O, where:
a) F is the set of product class features in R.
b) O is the set of opinion phrases in R.
c) opinions associated with a particular feature are
ranked based on their strength.
OPINE?s solution to this task is outlined in Figure 1. In
the following, we describe in detail each step.
Explicit Feature Extraction OPINE parses the re-
views using the MINIPAR dependency parser (Lin, 1998)
and applies a simple pronoun-resolution module to the
parsed data. The system then finds explicitly men-
tioned product features (E) using an extended version
of KNOWITALL?s extract-and-assess strategy described
above. OPINE extracts the following types of product fea-
tures: properties, parts, features of product parts (e.g.,
ScannerCoverSize), related concepts (e.g., Image
32
is related to Scanner) and parts and properties of re-
lated concepts (e.g., ImageSize). When compared on
this task with the most relevant previous review-mining
system in (Hu and Liu, 2004), OPINE obtains a 22% im-
provement in precision with only a 3% reduction in recall
on the relevant 5 datasets. One third of this increase is due
to OPINE?s feature assessment step and the rest is due to
the use of Web PMI statistics.
Opinion Phrases OPINE extracts adjective, noun, verb
and adverb phrases attached to explicit features as poten-
tial opinion phrases. OPINE then collectively assigns pos-
itive, negative or neutral semantic orientation (SO) labels
to their respective head words. This problem is similar to
labeling problems in computer vision and OPINE uses a
well-known computer vision technique, relaxation label-
ing, as the basis of a 3-step SO label assignment proce-
dure. First, OPINE identifies the average SO label for a
word w in the context of the review set. Second, OPINE
identifies the average SO label for each word w in the
context of a feature f and of the review set (?hot? has
a negative connotation in ?hot room?, but a positive one
in ?hot water?). Finally, OPINE identifies the SO label of
word w in the context of feature f and sentence s. For ex-
ample, some people like large scanners (?I love this large
scanner?) and some do not (?I hate this large scanner?).
The phrases with non-neutral head words are retained as
opinion phrases and their polarity is established accord-
ingly. On the task of opinion phrase extraction, OPINE
obtains a precision of 79% and a recall of 76% and on the
task of opinion phrase polarity extraction OPINE obtains
a precision of 86% and a recall of 84%.
Implicit Features Opinion phrases refer to properties,
which are sometimes implicit (e.g., ?tiny phone? refers to
the phone size). In order to extract such properties, OPINE
first clusters opinion phrases (e.g., tiny and small will
be placed in the same cluster), automatically labels the
clusters with property names (e.g., Size) and uses them
to build implicit features (e.g., PhoneSize). Opinion
phrases are clustered using a mixture of WordNet infor-
mation (e.g., antonyms are placed in the same cluster) and
lexical pattern information (e.g., ?clean, almost spotless?
suggests that ?clean? and ?spotless? are likely to refer to
the same property). (Hu and Liu, 2004) doesn?t handle
implicit features, so we have evaluated the impact of im-
plicit feature extraction on two separate sets of reviews
in the Hotels and Scanners domains. Extracting implicit
features (in addition to explicit features) has resulted in a
2% increase in precision and a 6% increase in recall for
OPINE on the task of feature extraction.
Ranking Opinion Phrases Given an opinion cluster,
OPINE uses the final probabilities associated with the SO
labels in order to derive an initial opinion phrase strength
ranking (e.g., great > good > average) in the manner
of (Turney, 2003). OPINE then uses Web-derived con-
straints on the relative strength of phrases in order to im-
prove this ranking. Patterns such as ?a1, (*) even a2? are
good indicators of how strong a1 is relative to a2. OPINE
bootstraps a set of such patterns and instantiates them
with pairs of opinions in order to derive constraints such
as strength(deafening) > strength(loud). OPINE
also uses synonymy and antonymy-based constraints
such as strength(clean) = strength(dirty). The con-
straint set induces a constraint satisfaction problem
whose solution is a ranking of the respective cluster opin-
ions (the remaining opinions maintain their default rank-
ing). OPINE?s accuracy on the opinion ranking task is
87%. Finally, OPINE outputs a set of (feature, ranked
opinions) tuples for each product.
3 Related Work
The previous review-mining systems most relevant to
our work are (Hu and Liu, 2004) and (Kobayashi et
al., 2004). The former?s precision on the explicit fea-
ture extraction task is 22% lower than OPINE?s while
the latter employs an iterative semi-automatic approach
which requires significant human input; neither handles
implicit features. Unlike previous research on identifying
the subjective character and the polarity of phrases and
sentences ((Hatzivassiloglou and Wiebe, 2000; Turney,
2003) and many others), OPINE identifies the context-
sensitive polarity of opinion phrases. In contrast to super-
vised methods which distinguish among strength levels
for sentences or clauses ((Wilson et al, 2004) and oth-
ers), OPINEuses an unsupervised constraint-based opin-
ion ranking approach.
References
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Un-
supervised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
V. Hatzivassiloglou and J. Wiebe. 2000. Effects of Adjec-
tive Orientation and Gradability on Sentence Subjectivity. In
COLING, pages 299?305.
M. Hu and B. Liu. 2004. Mining and Summarizing Customer
Reviews. In KDD, pages 168?177, Seattle, WA.
N. Kobayashi, K. Inui, K. Tateishi, and T. Fukushima. 2004.
Collecting Evaluative Expressions for Opinion Extraction.
In IJCNLP, pages 596?605.
D. Lin. 1998. Dependency-based evaluation of MINIPAR. In
Workshop on Evaluation of Parsing Systems at ICLRE.
P. Turney. 2003. Inference of Semantic Orientation from Asso-
ciation. In CoRR cs. CL/0309034.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you?
finding strong and weak opinion clauses. In AAAI, pages
761?769.
33
Proceedings of NAACL HLT 2007, pages 121?130,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Unsupervised Resolution of Objects and Relations on the Web
Alexander Yates
Turing Center
Computer Science and Engineering
University of Washington
Box 352350
Seattle, WA 98195, USA
ayates@cs.washington.edu
Oren Etzioni
Turing Center
Computer Science and Engineering
University of Washington
Box 352350
Seattle, WA 98195, USA
etzioni@cs.washington.edu
Abstract
The task of identifying synonymous re-
lations and objects, or Synonym Resolu-
tion (SR), is critical for high-quality infor-
mation extraction. The bulk of previous
SR work assumed strong domain knowl-
edge or hand-tagged training examples.
This paper investigates SR in the con-
text of unsupervised information extrac-
tion, where neither is available. The pa-
per presents a scalable, fully-implemented
system for SR that runs in O(KN log N)
time in the number of extractions N and
the maximum number of synonyms per
word, K. The system, called RESOLVER,
introduces a probabilistic relational model
for predicting whether two strings are
co-referential based on the similarity of
the assertions containing them. Given
two million assertions extracted from the
Web, RESOLVER resolves objects with
78% precision and an estimated 68% re-
call and resolves relations with 90% pre-
cision and 35% recall.
1 Introduction
Web Information Extraction (WIE) sys-
tems extract assertions that describe a rela-
tion and its arguments from Web text (e.g.,
(is capital of,D.C.,United States)). WIE systems
can extract hundreds of millions of assertions
containing millions of different strings from the
Web (e.g., the TEXTRUNNER system (Banko et al,
2007)).1 WIE systems often extract assertions that
describe the same real-world object or relation using
different names. For example, a WIE system might
extract (is capital city of,Washington,U.S.),
which describes the same relationship as above but
contains a different name for the relation and each
argument.
Synonyms are prevalent in text, and the Web cor-
pus is no exception. Our data set of two million as-
sertions extracted from a Web crawl contained over
a half-dozen different names each for the United
States and Washington, D.C., and three for the ?is
capital of? relation. The top 80 most commonly
extracted objects had an average of 2.9 extracted
names per entity, and several had as many as 10
names. The top 100 most commonly extracted re-
lations had an average of 4.9 synonyms per relation.
We refer to the problem of identifying synony-
mous object and relation names as Synonym Res-
olution (SR).2 An SR system for WIE takes a set of
assertions as input and returns a set of clusters, with
each cluster containing coreferential object strings
or relation strings. Previous techniques for SR have
focused on one particular aspect of the problem, ei-
ther objects or relations. In addition, the techniques
either depend on a large set of training examples, or
are tailored to a specific domain by assuming knowl-
edge of the domain?s schema. Due to the number
and diversity of the relations extracted, these tech-
1For a demo see www.cs.washington.edu/research/textrunner.
2Ironically, SR has a number of synonyms in the literature,
including Entity Resolution, Record Linkage, and Deduplica-
tion.
121
niques are not feasible for WIE systems. Schemata
are not available for the Web, and hand-labeling
training examples for each relation would require a
prohibitive manual effort.
In response, we present RESOLVER, a novel,
domain-independent, unsupervised synonym resolu-
tion system that applies to both objects and relations.
RESOLVER clusters coreferential names together us-
ing a probabilistic model informed by string similar-
ity and the similarity of the assertions containing the
names. Our contributions are:
1. A scalable clustering algorithm that runs in
time O(KN log N) in the number of extrac-
tions N and maximum number of synonyms
per word, K, without discarding any poten-
tially matching pair, under exceptionally weak
assumptions about the data.
2. An unsupervised probabilistic model for pre-
dicting whether two object or relation names
co-refer.
3. An empirical demonstration that RESOLVER
can resolve objects with 78% precision and
68% recall, and relations with 90% precision
and 35% recall.
The next section discusses previous work. Section
3 introduces our probabilistic model for SR. Section
4 describes our clustering algorithm. Section 5 de-
scribes extensions to our basic SR system. Section
6 presents our experiments, and section 7 discusses
our conclusions and areas for future work.
2 Previous Work
The DIRT algorithm (Lin and Pantel, 2001) ad-
dresses a piece of the unsupervised SR problem.
DIRT is a heuristic method for finding synonymous
relations, or ?inference rules.? DIRT uses a depen-
dency parser and mutual information statistics over
a corpus to identify relations that have similar sets of
arguments. In contrast, our algorithm provides a for-
mal probabilistic model that applies equally well to
relations and objects, and we provide an evaluation
of the algorithm in terms of precision and recall.
There are many unsupervised approaches for ob-
ject resolution in databases, but unlike our algo-
rithm these approaches depend on a known, fixed
schema. Ravikumar and Cohen (Ravikumar and Co-
hen, 2004) present an unsupervised approach to ob-
ject resolution using Expectation-Maximization on
a hierarchical graphical model. Several other re-
cent approaches leverage domain-specific informa-
tion and heuristics for object resolution. For ex-
ample, many (Dong et al, 2005; Bhattacharya and
Getoor, 2005; Bhattacharya and Getoor, 2006) rely
on evidence from observing which strings appear as
arguments to the same relation simultaneously (e.g.,
co-authors of the same publication). While this is
useful information when resolving authors in the ci-
tation domain, it is extremely rare to find relations
with similar properties in extracted assertions. None
of these approaches applies to the problem of resolv-
ing relations. See (Winkler, 1999) for a survey of
this area.
Several supervised learning techniques make en-
tity resolution decisions (Kehler, 1997; McCallum
and Wellner, 2004; Singla and Domingos, 2006), but
of course these systems depend on the availability
of training data, and often on a significant number
of labeled examples per relation of interest. These
approaches also depend on complex probabilistic
models and learning algorithms, and they have order
O(n3) time complexity, or worse. They currently do
not scale to the amounts of data extracted from the
Web. Previous systems were tested on at most a few
thousand examples, compared with millions or hun-
dreds of millions of extractions from WIE systems
such as TEXTRUNNER.
Coreference resolution systems (e.g., (Lappin and
Leass, 1994; Ng and Cardie, 2002)), like SR sys-
tems, try to merge references to the same object (typ-
ically pronouns, but potentially other types of noun
phrases). This problem differs from the SR problem
in several ways: first, it deals with unstructered text
input, possibly with syntactic annotation, rather than
relational input. Second, it deals only with resolv-
ing objects. Finally, it requires local decisions about
strings; that is, the same word may appear twice in a
text and refer to two different things, so each occur-
rence of a word must be treated separately.
The PASCAL Recognising Textual Entailment
Challenge proposes the task of recognizing when
two sentences entail one another, and many authors
have submitted responses to this challenge (Dagan et
al., 2006). Synonym resolution is a subtask of this
problem. Our task differs significantly from the tex-
tual entailment task in that it has no labeled training
122
data, and its input is in the form of relational extrac-
tions rather than raw text.
Two probabilistic models for information extrac-
tion have a connection with ours. Our probabilistic
model is partly inspired by the ball-and-urns abstrac-
tion of information extraction presented by Downey
et al (2005) Our task and probability model are dif-
ferent from theirs, but we make many of the same
modeling assumptions. Second, we follow Snow et
al.?s work (2006) on taxonomy induction in incorpo-
rating transitive closure constraints in our probabil-
ity calculations, as explained below.
3 Probabilistic Model
Our probabilistic model provides a formal, rigorous
method for resolving synonyms in the absence of
training data. It has two sources of evidence: the
similarity of the strings themselves (i.e., edit dis-
tance) and the similarity of the assertions they ap-
pear in. This second source of evidence is some-
times referred to as ?distributional similarity? (Hin-
dle, 1990).
Section 3.2 presents a simple model for predict-
ing whether a pair of strings co-refer based on string
similarity. Section 3.3 then presents a model called
the Extracted Shared Property (ESP) Model for pre-
dicting whether a pair of strings co-refer based on
their distributional similarity. Finally, a method is
presented for combining these models to come up
with an overall prediction for coreference decisions
between two clusters of strings.
3.1 Terminology and Notation
We use the following notation to describe the proba-
bilistic models. The input is a data set D containing
extracted assertions of the form a = (r, o1, . . . , on),
where r is a relation string and each oi is an object
string representing the arguments to the relation. In
our data, all of the extracted assertions are binary, so
n = 2. The subset of all assertions in D containing
a string s is called Ds.
For strings si and sj , let Ri,j be the random vari-
able for the event that si and sj refer to the same
entity. Let Rti,j denote the event that Ri,j is true,
and Rfi,j denote the event that it is false.
A pair of strings (r, s2) is called a property of
a string s1 if there is an assertion (r, s1, s2) ? D
or (r, s2, s1) ? D. A pair of strings (s1, s2) is
an instance of a string r if there is an assertion
(r, s1, s2) ? D. Equivalently, the property p =
(r, s2) applies to s1, and the relation r applies to
the instance i = (s1, s2). Finally, two strings x and
y share a property (or instance) if both x and y are
extracted with the same property (or instance).
3.2 String Similarity Model
Many objects appear with multiple names that are
substrings, acronyms, abbreviations, or other sim-
ple variations of one another. Thus string similarity
can be an important source of evidence for whether
two strings co-refer. Our probabilistic String Sim-
ilarity Model (SSM) assumes a similarity function
sim(s1, s2): STRING? STRING ? [0, 1]. The
model sets the probability of s1 co-referring with s2
to a smoothed version of the similarity:
P (Rti,j |sim(s1, s2)) =
? ? sim(s1, s2) + 1
?+ ?
The particular choice of ? and ? make little differ-
ence to our results, so long as they are chosen such
that the resulting probability can never be one or
zero. In our experiments ? = 20 and ? = 5, and we
use the well-known Monge-Elkan string similarity
function for objects and the Levenshtein string edit-
distance function for relations (Cohen et al, 2003).
3.3 The Extracted Shared Property Model
The Extracted Shared Property (ESP) Model out-
puts the probability that s1 and s2 co-refer
based on how many properties (or instances) they
share. As an example, consider the strings
?Mars? and ?Red Planet?, which appear in our
data 659 and 26 times respectively. Out of
these extracted assertions, they share four proper-
ties. For example, (lacks,Mars, ozone layer) and
(lacks,Red P lanet, ozone layer) both appear as
assertions in our data. The ESP model determines
the probability that ?Mars? and ?Red Planet? refer
to the same entity after observing k, the number of
properties that apply to both, n1, the total number
of extracted properties for ?Mars?, and n2, the total
number of extracted properties for ?Red Planet.?
ESP models the extraction of assertions as a
generative process, much like the URNS model
(Downey et al, 2005). For each string si, a certain
123
number, Pi, of properties of the string are written on
balls and placed in an urn. Extracting ni assertions
that contain si amounts to selecting a subset of size
ni from these labeled balls.3 Properties in the urn are
called potential properties to distinguish them from
extracted properties.
To model coreference decisions, ESP uses a pair
of urns, containing Pi and Pj balls respectively, for
the two strings si and sj . Some subset of the Pi
balls have the exact same labels as an equal-sized
subset of the Pj balls. Let the size of this sub-
set be Si,j . The ESP model assumes that corefer-
ential strings share as many potential properties as
possible, though only a few of the potential proper-
ties will be extracted for both. For non-coreferential
strings, the number of shared potential properties is a
strict subset of the potential properties of each string.
Thus if Ri,j is true then Si,j = min(Pi, Pj), and if
Ri,j is false then Si,j < min(Pi, Pj).
The ESP model makes several simplifying as-
sumptions in order to make probability predictions.
As is suggested by the ball-and-urn abstraction, it
assumes that each ball for a string is equally likely
to be selected from its urn. Because of data sparsity,
almost all properties are very rare, so it would be dif-
ficult to get a better estimate for the prior probability
of selecting a particular potential property. Second,
it assumes that without knowing the value of k, ev-
ery value of Si,j is equally likely, since we have no
better information. Finally, it assumes that all sub-
sets of potential properties are equally likely to be
shared by two non-coreferential objects, regardless
of the particular labels on the balls, given the size of
the shared subset.
Given these assumptions, we can derive an ex-
pression for P (Rti,j). First, note that there are(Pi
ni
)(Pj
nj
) total ways of extracting ni and nj asser-
tions for si and sj . Given a particular value of Si,j ,
the number of ways in which ni and nj assertions
can be extracted such that they share exactly k is
given by
Count(k, ni, nj |Pi, Pj , Si,j) =
(Si,j
k
)?
r,s?0
(Si,j?k
r+s
)(r+s
r
)( Pi?Si,j
ni?(k+r)
)( Pj?Si,j
nj?(k+s)
)
By our assumptions,
3Unlike the URNS model, balls are drawn without replace-
ment because each extracted property is distinct in our data.
P (k|ni, nj , Pi, Pj , Si,j) =
Count(k, ni, nj |Pi, Pj , Si,j)(Pi
ni
)(Pj
nj
) (1)
Let Pmin = min(Pi, Pj). The result below fol-
lows from Bayes? Rule and our assumptions above:
Proposition 1 If two strings si and sj have Pi and
Pj potential properties (or instances), and they ap-
pear in extracted assertions Di and Dj such that
|Di| = ni and |Dj | = nj , and they share k extracted
properties (or instances), the probability that si and
sj co-refer is:
P (Rti,j |Di, Dj , Pi, Pj) =
P (k|ni, nj , Pi, Pj , Si,j = Pmin)?
k?Si,j?Pmin P (k|ni, nj , Pi, Pj , Si,j)
(2)
Substituting equation 1 into equation 2 gives us a
complete expression for the probability we are look-
ing for.
Note that the probability for Ri,j depends on just
two hidden parameters, Pi and Pj . Since we have
no labeled data to estimate these parameters from,
we tie these parameters to the number of times the
respective strings si and sj are extracted. Thus we
set Pi = N ? ni, and we set N = 50 in our experi-
ments.
3.4 Combining the Evidence
For each potential coreference relationship Ri,j ,
there are now two pieces of probabilistic evidence.
Let Eei,j be the evidence for ESP, and let Esi,j be the
evidence for SSM. Our method for combining the
two uses the Na??ve Bayes assumption that each piece
of evidence is conditionally independent, given the
coreference relation:
P (Esi,j , Eei,j |Ri,j) = P (Esi,j |Ri,j)P (Eei,j |Ri,j)
Given this simplifying assumption, we can com-
bine the evidence to find the probability of a cofer-
ence relationship by applying Bayes? Rule to both
sides (we omit the i, j indices for brevity):
P (Rt|Es, Ee) =
P (Rt|Es)P (Rt|Ee)(1? P (Rt))?
i?{t,f} P (Ri|Es)P (Ri|Ee)(1? P (Ri))
124
3.5 Comparing Clusters of Strings
Our algorithm merges clusters of strings with one
another, using one of the above models. However,
these models give probabilities for coreference deci-
sions between two individual strings, not two clus-
ters of strings.
We follow the work of Snow et al (2006) in in-
corporating transitive closure constraints in proba-
bilistic modeling, and make the same independence
assumptions. The benefit of this approach is that the
calculation for merging two clusters depends only
on coreference decisions between individual strings,
which can be calculated independently.
Let a clustering be a set of coreference relation-
ships between pairs of strings such that the corefer-
ence relationships obey the transitive closure prop-
erty. We let the probability of a set of assertions D
given a clustering C be:
P (D|C) =
?
Rti,j?C
P (Di ?Dj |Rti,j)?
?
Rfi,j?C
P (Di ?Dj |Rfi,j)
The metric used to determine if two clusters
should be merged is the likelihood ratio, or the prob-
ability for the set of assertions given the merged
clusters over the probability given the original clus-
tering. Let C ? be a clustering that differs from C
only in that two clusters in C have been merged in
C ?, and let ?C be the set of coreference relation-
ships in C ? that are true, but the corresponding ones
in C are false. This metric is given by:
P (D|C ?)/P (D|C) =
?
Rti,j??C P (R
t
i,j |Di ?Dj)(1? P (Rti,j))?
Rti,j??C(1? P (R
t
i,j |Di ?Dj))P (Rti,j)
The probability P (Rti,j |Di?Dj) may be supplied
by the SSM, ESP, or combination model. In our ex-
periments, we let the prior for the SSM model be
0.5. For the ESP and combined models, we set the
prior to P (Rti,j) = 1min(P1,P2) .
4 RESOLVER?s Clustering Algorithm
Our clustering algorithm iteratively merges clusters
of co-referential names, making each iteration in
S := set of all strings
For each property or instance p,
Sp := {s ? S|s has property p}
1. Scores := {}
2. Build index mapping properties (and instances)
to strings with those properties (instances)
3. For each property or instance p:
If |Sp| < Max:
For each pair {s1, s2} ? Sp:
Add mergeScore(s1, s2) to Scores
4. Repeat until no merges can be performed:
Sort Scores
UsedClusters := {}
While score of top clusters c1, c2
is above Threshold:
Skip if either is in UsedClusters
Merge c1 and c2
Add c1, c2 to UsedClusters
Merge properties containing c1, c2
Recalculate merge scores as in Steps 1-3
Figure 1: RESOLVER?s Clustering Algorithm
time O(N log N) in the number of extracted as-
sertions. The algorithm requires only basic assump-
tions about which strings to compare. Previous work
on speeding up clustering algorithms for SR has ei-
ther required far stronger assumptions, or else it has
focused on heuristic methods that remain, in the
worst case, O(N2) in the number of distinct objects.
Our algorithm, a greedy agglomerative clustering
method, is outlined in Figure 1. The first novel part
of the algorithm, step 3, compares pairs of strings
that share the same property or instance, so long as
no more than Max strings share that same property
or instance. After the scores for all comparisons are
made, each string is assigned its own cluster. Then
the scores are sorted and the best cluster pairs are
merged until no pair of clusters has a score above
threshold. The second novel aspect of this algorithm
is that as it merges clusters in Step 4, it merges prop-
erties containing those clusters in a process we call
mutual recursion, which is discussed below.
This algorithm compares every pair of clusters
that have the potential to be merged, assuming two
properties of the data. First, it assumes that pairs
of clusters with no shared properties are not worth
125
comparing. Since the number of shared properties
is a key source of evidence for our approach, these
clusters almost certainly will not be merged, even if
they are compared, so the assumption is quite rea-
sonable. Second, the approach assumes that clus-
ters sharing only properties that apply to very many
strings (more than Max) need not be compared.
Since properties shared by many strings provide lit-
tle evidence that the strings are coreferential, this as-
sumption is reasonable for SR. We use Max = 50
in our experiments. Less than 0.1% of the properties
are thrown out using this cutoff.
4.1 Algorithm Analysis
Let D be the set of extracted assertions. The follow-
ing analysis shows that one iteration of merges takes
time O(N log N), where N = |D|. Let NC be
the number of comparisons between strings in step
3. To simplify the analysis, we consider only those
properties that contain a relation string and an argu-
ment 1 string. Let A be the set of all such properties.
NC is linear in N :4
NC =
?
p?A
|Sp| ? (|Sp| ? 1)
2
? (Max? 1)2 ?
?
p?A
|Sp|
= (Max? 1)2 ?N
Note that this bound is quite loose because most
properties apply to only a few strings. Step 4 re-
quires time O(N log N) to sort the comparison
scores and perform one iteration of merges. If the
largest cluster has size K, in the worst case the al-
gorithm will take K iterations. In our experiments,
the algorithm never took more than 9 iterations.
4.2 Relation to other speed-up techniques
The merge/purge algorithm (Hernandez and Stolfo,
1995) assumes the existence of a particular attribute
such that when the data set is sorted on this attribute,
matching pairs will all appear within a narrow win-
dow of one another. This algorithm is O(M log M)
where M is the number of distinct strings. However,
there is no attribute or set of attributes that comes
4If the Max parameter is allowed to vary with log|D|,
rather than remaining constant, the same analysis leads to a
slightly looser bound that is still better than O(N2).
close to satisfying this assumption in the context of
domain-independent information extraction.
There are several techniques that often provide
speed-ups in practice, but in the worst case they
make O(M2) comparisons at each merge iteration,
where M is the number of distinct strings. This can
cause problems on very large data sets. Notably,
McCallum et al (2000) use a cheap comparison
metric to place objects into overlapping ?canopies,?
and then use a more expensive metric to cluster ob-
jects appearing in the same canopy. The RESOLVER
clustering algorithm is in fact an adaptation of the
canopy method; it adds the restriction that strings are
not compared when they share only high-frequency
properties. The canopy method works well on high-
dimensional data with many clusters, which is the
case with our problem, but its time complexity is
worse than ours.
For information extraction data, a complexity of
O(M2) in the number of distinct strings turns out
to be considerably worse than our algorithm?s com-
plexity of O(N log N) in the number of extracted
assertions. This is because the data obeys a Zipf law
relationship between the frequency of a string and its
rank, so the number of distinct strings grows linearly
or almost linearly with the number of assertions.5
4.3 Mutual Recursion
Mutual recursion refers to the novel property of
our algorithm that as it clusters relation strings to-
gether into sets of synonyms, it collapses proper-
ties together for object strings and potentially finds
more shared properties between coreferential object
strings. Likewise, as it clusters objects together into
sets of coreferential names, it collapses instances of
relations together and potentially finds more shared
instances between coreferential relations. Thus the
clustering decisions for relations and objects mutu-
ally depend on one another.
For example, the strings ?Kennedy? and ?Pres-
ident Kennedy? appear in 430 and 97 assertions
in our data, respectively, but none of their ex-
tracted properties match exactly. Many properties,
5The exact relationship depends on the shape parameter z
of the Zipf curve. If z < 1, as it is for our data set, the num-
ber of total extractions grows linearly with the number of dis-
tinct strings extracted. If z = 1, then n extractions will contain
?( nln n ) distinct strings.
126
however, almost match. For example, the asser-
tions (challenged,Kennedy,Premier Krushchev)
and (stood up to,President Kennedy,Kruschev)
both appear in our data. Because ?challenged? and
?stood up to? are similar, and ?Krushchev? and ?Pre-
mier Krushchev? are similar, our algorithm is able
to merge these pairs into two clusters, thereby creat-
ing a new shared property between ?Kennedy? and
?President Kennedy.? Eventually it can merge these
two strings as well.
5 Extensions to RESOLVER
While the basic RESOLVER system can cluster syn-
onyms accurately and quickly, there is one type of
error that it frequently makes. In some cases, it has
difficulty distinguishing between similar pairs of ob-
jects and identical pairs. For example, ?Virginia?
and ?West Virginia? share several extractions be-
cause they have the same type, and they have high
string similarity. As a result, RESOLVER clusters
these two together. The next two sections describe
two extensions to RESOLVER that address the prob-
lem of similarity vs. identity.
5.1 Function Filtering
RESOLVER can use functions and one-to-one rela-
tions to help distinguish between similar and identi-
cal pairs. For example, West Virginia and Virginia
have different capitals: Richmond and Charleston,
respectively. If both of these facts are extracted, and
if RESOLVER knows that the ?capital of? relation is
functional, it should prevent Virginia and West Vir-
ginia from merging.
The Function Filter prevents merges between
strings that have different values for the same func-
tion. More precisely, it decides that two strings y1
and y2 match if their string similarity is above a high
threshold. It prevents a merge between strings x1
and x2 if there exist a function f and extractions
f(x1, y1) and f(x2, y2), and there are no such ex-
tractions such that y1 and y2 match (and vice versa
for one-to-one relations). Experiments described in
section 6 show that the Function Filter can improve
the precision of RESOLVER without significantly af-
fecting its recall.
While the Function Filter currently uses func-
tions and one-to-one relations as negative evidence,
it is also possible to use them as positive evidence.
For example, the relation ?married? is not strictly
one-to-one, but for most people the set of spouses
is very small. If a pair of strings are extracted
with the same spouse?e.g., ?FDR? and ?President
Roosevelt? share the property (?married?, ?Eleanor
Roosevelt?)?this is far stronger evidence that the
two strings are identical than if they shared some
random property.
Unfortunately, various techniques that attempted
to model this insight, including a TF-IDF weighting
of properties, yielded essentially no improvement of
RESOLVER. One major reason is that there are rel-
atively few examples of shared functional or one-
to-one properties because of sparsity. This idea de-
serves more investigation, however, and is an area
for future work.
5.2 Using Web Hitcounts
While names for two similar objects may often ap-
pear together in the same sentence, it is relatively
rare for two different names of the same object to
appear in the same sentence. RESOLVER exploits
this fact by querying the Web to determine how often
a pair of strings appears together in a large corpus.
When the hitcount is high, RESOLVER can prevent
the merge.
Specifically, the Coordination-Phrase Filter
searches for hitcounts of the phrase ?s1 and s2?,
where s1 and s2 are a candidate pair for merging.
It then computes a variant of pointwise mutual
information, given by
coordination score(s1, s2) = hits(s1 and s2)
2
hits(s1)? hits(s2)
The filter prevents any merge for which the coor-
dination score is above a threshold, which is de-
termined on a development set. The results of
Coordination-Phrase filtering are discussed in the
next section.
6 Experiments
Our experiments demonstrate that the ESP model
is significantly better at resolving synonyms than a
widely-used distributional similarity metric, the co-
sine similarity metric (CSM) (Salton and McGill,
1983), and that RESOLVER is significantly better at
127
resolving synonyms than either of its components,
SSM or ESP.
We test these models on a data set of 2.1 million
assertions extracted from a Web crawl.6 All models
ran over all assertions, but compared only those ob-
jects or relations that appeared at least 25 times in
the data, to give the ESP and CSM models sufficient
data for estimating similarity. However, the mod-
els do use strings that appear less than 25 times as
features. In all, the data contains 9,797 distinct ob-
ject strings and 10,151 distinct relation strings that
appear at least 25 times.
We judged the precision of each model by manu-
ally labeling all of the clusters that each model out-
puts. Judging recall would require inspecting not
just the clusters that the system outputs, but the en-
tire data set, to find all of the true clusters. Be-
cause of the size of the data set, we instead esti-
mated recall over a smaller subset of the data. We
took the top 200 most frequent object strings and top
200 most frequent relation strings in the data. For
each one of these high-frequency strings, we man-
ually searched through all strings with frequency
over 25 that shared at least one property, as well
as all strings that contained one of the keywords in
the high-frequency strings or obvious variations of
them. We manually clustered the resulting matches.
The top 200 object strings formed 51 clusters of size
greater than one, with an average cluster size of 2.9.
For relations, the top 200 strings and their matches
formed 110 clusters with size greater than one, with
an average cluster size of 4.9. We measured the re-
call of our models by comparing the set of all clus-
ters containing at least one of the high-frequency
words against these gold standard clusters.
For our precision and recall measures, we only
compare clusters of size two or more, in order to
focus on the interesting cases. Using the term hy-
pothesis cluster for clusters created by one of the
models, we define the precision of a model to be the
number of elements in all hypothesis clusters which
are correct divided by the total number of elements
in hypothesis clusters. An element s is marked cor-
rect if a plurality of the elements in s?s cluster refer
to the same entity as s; we break ties arbitrarily, as
6The data is made available at
http://www.cs.washington.edu/homes/ayates/.
they do not affect results. We define recall as the
sum over gold standard clusters of the most num-
ber of elements found in a single hypothesis cluster,
divided by the total number of elements in gold stan-
dard clusters.
For the ESP and SSM models in our experiment,
we prevented mutual recursion by clustering rela-
tions and objects separately. Only the full RE-
SOLVER system uses mutual recursion. For the CSM
model, we create for each distinct string a row vec-
tor, with each column representing a property. If that
property applies to the string, we set the value of
that column to the inverse frequency of the property
and zero otherwise. CSM finds the cosine of the an-
gle between the vectors for each pair of strings, and
merges the best pairs that score above threshold.
Each model requires a threshold parameter to de-
termine which scores are suitable for merging. For
these experiments we arbitrarily chose a threshold
of 3 for the ESP model (that is, the data needs to
be 3 times more likely given the merged cluster than
the unmerged clusters in order to perform the merge)
and chose thresholds for the other models by hand so
that the difference between them and ESP would be
roughly even between precision and recall, although
for relations it was harder to improve the recall. It is
an important item for future work to be able to esti-
mate these thresholds and perhaps other parameters
of our models from unlabeled data, but the chosen
parameters worked well enough for the experiments.
Table 1 shows the precision and recall of our models.
6.1 Discussion
ESP significantly outperforms CSM on both object
and relation clustering. CSM had particular trouble
with lower-frequency strings, judging far too many
of them to be co-referential on too little evidence. If
the threshold for clustering using CSM is increased,
however, the recall begins to approach zero.
ESP and CSM make predictions based on a very
noisy signal. ?Canada,? for example, shares more
properties with ?United States? in our data than
?U.S.? does, even though ?Canada? appears less of-
ten than ?U.S.? The results show that both models
perform below the SSM model on its own for object
merging, and both perform slightly better than SSM
on relations because of SSM?s poor recall.
We found a significant improvement in both pre-
128
Objects Relations
Model Prec. Rec. F1 Prec. Rec. F1
CSM 0.51 0.36 0.42 0.62 0.29 0.40
ESP 0.56 0.41 0.47 0.79 0.33 0.47
SSM 0.62 0.53 0.57 0.85 0.25 0.39
RESOLVER 0.71 0.66 0.68 0.90 0.35 0.50
Table 1: Comparison of the cosine similarity metric (CSM), RESOLVER components (SSM and ESP), and the RESOLVER
system. Bold indicates the score is significantly different from the score in the row above at p < 0.05 using the chi-squared test
with one degree of freedom. Using the same test, RESOLVER is also significantly different from ESP and CSM in recall on objects,
and from CSM and SSM in recall on relations. RESOLVER?s F1 on objects is a 19% increase over SSM?s F1. RESOLVER?s F1 on
relations is a 28% increase over SSM?s F1.
cision and recall when using a combined model over
using SSM alone. RESOLVER?s F1 is 19% higher
than SSM?s on objects, and 28% higher on relations.
In a separate experiment we found that mutual re-
cursion provides mixed results. A combination of
SSM and ESP without mutual recursion had a preci-
sion of 0.76 and recall of 0.59 on objects, and a pre-
cision of 0.91 and recall of 0.35 on relations. Mutual
recursion increased recall and decreased precision
for both objects and relations. None of the differ-
ences were statistically significant, however.
There is clearly room for improvement on the SR
task. Except for the problem of confusing similar
and identical pairs (see section 5), error analysis
shows that most of RESOLVER?s mistakes are be-
cause of two kinds of errors:
1. Extraction errors. For example, ?US News?
gets extracted separately from ?World Report?, and
then RESOLVER clusters them together because they
share almost all of the same properties.
2. Multiple word senses. For example, there are two
President Bushes; also, there are many terms like
?President? and ?Army? that can refer to many dif-
ferent entities.
6.2 Experiments with Extensions
The extensions to RESOLVER attempt to address
the confusion between similar and identical pairs.
Experiments with the extensions, using the same
datasets and metrics as above, demonstrate that the
Function Filter (FF) and the Coordination-Phrase
Filter (CPF) boost RESOLVER?s performance.
FF requires as input the set of functional and one-
to-one relations in the data. Table 2 contains a sam-
is capital of is capital city of
named after was named after
headquartered in is headquartered in
Table 2: A sample of the set of functions used by the Func-
tion Filter.
Model Prec. Rec. F1
RESOLVER 0.71 0.66 0.68
RESOLVER+FF 0.74 0.66 0.70
RESOLVER+CPF 0.78 0.68 0.73
RESOLVER+FF+CPF 0.78 0.68 0.73
Table 3: Comparison of object merging results for the
RESOLVER system, RESOLVER plus Function Filtering (RE-
SOLVER+FF), RESOLVER plus Coordination-Phrase Filter-
ing (RESOLVER+CPF), and RESOLVER plus both types of fil-
tering (RESOLVER+FF+CPF). Bold indicates the score is sig-
nificantly different from RESOLVER?s score at p < 0.05 us-
ing the chi-squared test with one degree of freedom. RE-
SOLVER+CPF?s F1 on objects is a 28% increase over SSM?s
F1, and a 7% increase over RESOLVER?s F1.
pling of the manually-selected functions used in our
experiment. Automatically discovering such func-
tions from extractions has been addressed in Ana-
Maria Popescu?s dissertation (Popescu, 2007), and
we did not attempt to duplicate this effort in RE-
SOLVER.
Table 3 contains the results of our experiments.
With coordination-phrase filtering, RESOLVER?s F1
is 28% higher than SSM?s on objects, and 6% higher
than RESOLVER?s F1 without filtering. While func-
tion filtering is a promising idea, FF provides a
smaller benefit than CPF on this dataset, and the
129
merges that it prevents are, with a few exceptions,
a subset of the merges prevented by CPF. This is in
part due to the limited number of functions available
in the data. In addition to outperforming FF on this
dataset, CPF has the added advantage that it does not
require additional input, like a set of functions.
7 Conclusion and Future Work
We have shown that the unsupervised and scalable
RESOLVER system is able to find clusters of co-
referential object names in extracted relations with
a precision of 78% and a recall of 68% with the aid
of coordination-phrase filtering, and can find clus-
ters of co-referential relation names with precision
of 90% and recall of 35%. We have demonstrated
significant improvements over using simple similar-
ity metrics for this task by employing a novel prob-
abilistic model of coreference.
In future work, we plan to use RESOLVER on a
much larger data set of over a hundred million as-
sertions, further testing its scalability and its abil-
ity to improve in accuracy given additional data.
We also plan to add techniques for handling mul-
tiple word senses. Finally, to make the probabilistic
model more accurate and easier to use, we plan to
investigate methods for automatically estimating its
parameters from unlabeled data.
Acknowledgements
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, DARPA contract
NBCHD030010, ONR grant N00014-05-1-0185 as
well as gifts from Google, and carried out at the Uni-
versity of Washington?s Turing Center. We thank
Doug Downey, Michele Banko, Stef Schoenmack-
ers, Dan Weld, Fei Wu, and the anonymous review-
ers for their helpful comments on previous drafts.
References
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In IJCAI.
I. Bhattacharya and L. Getoor. 2005. Relational Clus-
tering for Multi-type Entity Resolution. In 11th ACM
SIGKDD Workshop on Multi Relational Data Mining.
I. Bhattacharya and L. Getoor. 2006. Query-time entity
resolution. In KDD.
W.W. Cohen, P. Ravikumar, and S.E. Fienberg. 2003.
A comparison of string distance metrics for name-
matching tasks. In IIWeb.
I. Dagan, O. Glickman, and B. Magnini. 2006. The PAS-
CAL Recognising Textual Entailment Challenge. Lec-
ture Notes in Computer Science, 3944:177?190.
X. Dong, A.Y. Halevy, and J. Madhavan. 2005. Refer-
ence reconciliation in complex information spaces. In
SIGMOD.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Prob-
abilistic Model of Redundancy in Information Extrac-
tion. In IJCAI.
M. A. Hernandez and S. J. Stolfo. 1995. The
merge/purge problem for large databases. In SIG-
MOD.
D. Hindle. 1990. Noun classification from predicage-
argument structures. In ACL.
A. Kehler. 1997. Probabilistic coreference in informa-
tion extraction. In EMNLP.
S. Lappin and H. J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational Lin-
guistics, 20(4):535?561.
D. Lin and P. Pantel. 2001. DIRT ? Discovery of Infer-
ence Rules from Text. In KDD.
A. McCallum and B. Wellner. 2004. Conditional models
of identity uncertainty with application to noun coref-
erence. In NIPS.
A. McCallum, K. Nigam, and L. Ungar. 2000. Efficient
clustering of high-dimensional data sets with applica-
tion to reference matching. In KDD.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.
Ana-Maria Popescu. 2007. Information Extraction from
Unstructured Web Text. University of Washington.
P. Ravikumar and W. W. Cohen. 2004. A hierarchical
graphical model for record linkage. In UAI.
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
P. Singla and P. Domingos. 2006. Entity Resolution with
Markov Logic. In ICDM.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
COLING/ACL.
W.E. Winkler. 1999. The state of record linkage and cur-
rent research problems. Technical report, U.S. Bureau
of the Census, Washington, D.C.
130
NAACL HLT Demonstration Program, pages 25?26,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
TextRunner: Open Information Extraction on the Web
Alexander Yates
Michael Cafarella
Michele Banko
Oren Etzioni
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195-2350
{ayates,banko,hastur,mjc,etzioni,soderlan}@cs.washington.edu
Matthew Broadhead
Stephen Soderland
1 Introduction
Traditional information extraction systems
have focused on satisfying precise, narrow,
pre-specified requests from small, homoge-
neous corpora. In contrast, the TextRunner
system demonstrates a new kind of informa-
tion extraction, called Open Information Ex-
traction (OIE), in which the system makes a
single, data-driven pass over the entire cor-
pus and extracts a large set of relational
tuples, without requiring any human input.
(Banko et al, 2007) TextRunner is a fully-
implemented, highly scalable example of OIE.
TextRunner?s extractions are indexed, al-
lowing a fast query mechanism.
Our first public demonstration of the Text-
Runner system shows the results of perform-
ing OIE on a set of 117 million web pages. It
demonstrates the power of TextRunner in
terms of the raw number of facts it has ex-
tracted, as well as its precision using our novel
assessment mechanism. And it shows the abil-
ity to automatically determine synonymous re-
lations and objects using large sets of extrac-
tions. We have built a fast user interface for
querying the results.
2 Previous Work
The bulk of previous information extraction
work uses hand-labeled data or hand-crafted
patterns to enable relation-specific extraction
(e.g., (Culotta et al, 2006)). OIE seeks to
avoid these requirements for human input.
Shinyama and Sekine (Shinyama and
Sekine, 2006) describe an approach to ?un-
restricted relation discovery? that does away
with many of the requirements for human in-
put. However, it requires clustering of the doc-
uments used for extraction, and thus scales in
quadratic time in the number of documents.
It does not scale to the size of the Web.
For a full discussion of previous work, please
see (Banko et al, 2007), or see (Yates and Et-
zioni, 2007) for work relating to synonym res-
olution.
3 Open IE in TextRunner
OIE presents significant new challenges for in-
formation extraction systems, including
Automation of relation extraction, which in
traditional information extraction uses hand-
labeled inputs.
Corpus Heterogeneity on the Web, which
makes tools like parsers and named-entity tag-
gers less accurate because the corpus is differ-
ent from the data used to train the tools.
Scalability and efficiency of the system.
Open IE systems are effectively restricted to
a single, fast pass over the data so that they
can scale to huge document collections.
In response to these challenges, Text-
Runner includes several novel components,
which we now summarize (see (Banko et al,
2007) for details).
1. Single Pass Extractor
The TextRunner extractor makes a sin-
gle pass over all documents, tagging sen-
tences with part-of-speech tags and noun-
phrase chunks as it goes. For each pair of noun
phrases that are not too far apart, and subject
to several other constraints, it applies a clas-
sifier described below to determine whether or
not to extract a relationship. If the classifier
25
deems the relationship trustworthy, a tuple of
the form t = (ei, rj , ek) is extracted, where
ei, ek are entities and rj is the relation between
them. For example, TextRunner might ex-
tract the tuple (Edison, invented, light bulbs).
On our test corpus (a 9 million document sub-
set of our full corpus), it took less than 68
CPU hours to process the 133 million sen-
tences. The process is easily parallelized, and
took only 4 hours to run on our cluster.
2. Self-Supervised Classifier
While full parsing is too expensive to apply to
the Web, we use a parser to generate training
examples for extraction. Using several heuris-
tic constraints, we automatically label a set
of parsed sentences as trustworthy or untrust-
worthy extractions (positive and negative ex-
amples, respectively). The classifier is trained
on these examples, using features such as the
part of speech tags on the words in the re-
lation. The classifier is then able to decide
whether a sequence of POS-tagged words is a
correct extraction with high accuracy.
3. Synonym Resolution
Because TextRunner has no pre-defined re-
lations, it may extract many different strings
representing the same relation. Also, as with
all information extraction systems, it can ex-
tract multiple names for the same object. The
Resolver system performs an unsupervised
clustering of TextRunner?s extractions to
create sets of synonymous entities and rela-
tions. Resolver uses a novel, unsupervised
probabilistic model to determine the probabil-
ity that any pair of strings is co-referential,
given the tuples that each string was extracted
with. (Yates and Etzioni, 2007)
4. Query Interface
TextRunner builds an inverted index of
the extracted tuples, and spreads it across a
cluster of machines. This architecture sup-
ports fast, interactive, and powerful relational
queries. Users may enter words in a relation or
entity, and TextRunner quickly returns the
entire set of extractions matching the query.
For example, a query for ?Newton? will return
tuples like (Newton, invented, calculus). Users
may opt to query for all tuples matching syn-
onyms of the keyword input, and may also opt
to merge all tuples returned by a query into
sets of tuples that are deemed synonymous.
4 Experimental Results
On our test corpus of 9 million Web doc-
uments, TextRunner extracted 7.8 million
well-formed tuples. On a randomly selected
subset of 400 tuples, 80.4% were deemed cor-
rect by human reviewers.
We performed a head-to-head compari-
son with a state-of-the-art traditional in-
formation extraction system, called Know-
ItAll. (Etzioni et al, 2005) On a set of ten
high-frequency relations, TextRunner found
nearly as many correct extractions as Know-
ItAll (11,631 to 11,476), while reducing the
error rate of KnowItAll by 33% (18% to
12%).
Acknowledgements
This research was supported in part by NSF
grants IIS-0535284 and IIS-0312988, DARPA
contract NBCHD030010, ONR grant N00014-
05-1-0185 as well as gifts from Google, and
carried out at the University of Washington?s
Turing Center.
References
M. Banko, M. J. Cafarella, S. Soderland,
M. Broadhead, and O. Etzioni. 2007. Open In-
formation Extraction from the Web. In IJCAI.
A. Culotta, A. McCallum, and J. Betz. 2006. Inte-
grating Probabilistic Extraction Models and Re-
lational Data Mining to Discover Relations and
Patterns in Text. In HLT-NAACL.
O. Etzioni, M. Cafarella, D. Downey, S. Kok,
A. Popescu, T. Shaked, S. Soderland, D. Weld,
and A. Yates. 2005. Unsupervised Named-
Entity Extraction from the Web: An Experi-
mental Study. Artificial Intelligence, 165(1):91?
134.
Y. Shinyama and S. Sekine. 2006. Preemptive
Information Extraction Using Unrestricted Re-
lation Discovery. In HLT-NAACL.
A. Yates and O. Etzioni. 2007. Unsupervised Res-
olution of Objects and Relations on the Web. In
NAACL-HLT.
26
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 696?703,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Sparse Information Extraction:
Unsupervised Language Models to the Rescue
Doug Downey, Stefan Schoenmackers, and Oren Etzioni
Turing Center, Department of Computer Science and Engineering
University of Washington, Box 352350
Seattle, WA 98195, USA
{ddowney,stef,etzioni}@cs.washington.edu
Abstract
Even in a massive corpus such as the Web, a
substantial fraction of extractions appear in-
frequently. This paper shows how to assess
the correctness of sparse extractions by uti-
lizing unsupervised language models. The
REALM system, which combines HMM-
based and n-gram-based language models,
ranks candidate extractions by the likeli-
hood that they are correct. Our experiments
show that REALM reduces extraction error
by 39%, on average, when compared with
previous work.
Because REALM pre-computes language
models based on its corpus and does not re-
quire any hand-tagged seeds, it is far more
scalable than approaches that learn mod-
els for each individual relation from hand-
tagged data. Thus, REALM is ideally suited
for open information extraction where the
relations of interest are not specified in ad-
vance and their number is potentially vast.
1 Introduction
Information Extraction (IE) from text is far from in-
fallible. In response, researchers have begun to ex-
ploit the redundancy in massive corpora such as the
Web in order to assess the veracity of extractions
(e.g., (Downey et al, 2005; Etzioni et al, 2005;
Feldman et al, 2006)). In essence, such methods uti-
lize extraction patterns to generate candidate extrac-
tions (e.g., ?Istanbul?) and then assess each candi-
date by computing co-occurrence statistics between
the extraction and words or phrases indicative of
class membership (e.g., ?cities such as?).
However, Zipf?s Law governs the distribution of
extractions. Thus, even the Web has limited redun-
dancy for less prominent instances of relations. In-
deed, 50% of the extractions in the data sets em-
ployed by (Downey et al, 2005) appeared only
once. As a result, Downey et al?s model, and re-
lated methods, had no way of assessing which ex-
traction is more likely to be correct for fully half of
the extractions. This problem is particularly acute
when moving beyond unary relations. We refer to
this challenge as the task of assessing sparse extrac-
tions.
This paper introduces the idea that language mod-
eling techniques such as n-gram statistics (Manning
and Schu?tze, 1999) and HMMs (Rabiner, 1989) can
be used to effectively assess sparse extractions. The
paper introduces the REALM system, and highlights
its unique properties. Notably, REALM does not
require any hand-tagged seeds, which enables it to
scale to Open IE?extraction where the relations of
interest are not specified in advance, and their num-
ber is potentially vast (Banko et al, 2007).
REALM is based on two key hypotheses. The
KnowItAll hypothesis is that extractions that oc-
cur more frequently in distinct sentences in the
corpus are more likely to be correct. For exam-
ple, the hypothesis suggests that the argument pair
(Giuliani, New York) is relatively likely to be
appropriate for the Mayor relation, simply because
this pair is extracted for the Mayor relation rela-
tively frequently. Second, we employ an instance of
the distributional hypothesis (Harris, 1985), which
696
can be phrased as follows: different instances of
the same semantic relation tend to appear in sim-
ilar textual contexts. We assess sparse extractions
by comparing the contexts in which they appear to
those of more common extractions. Sparse extrac-
tions whose contexts are more similar to those of
common extractions are judged more likely to be
correct based on the conjunction of the KnowItAll
and the distributional hypotheses.
The contributions of the paper are as follows:
? The paper introduces the insight that the sub-
field of language modeling provides unsuper-
vised methods that can be leveraged to assess
sparse extractions. These methods are more
scalable than previous assessment techniques,
and require no hand tagging whatsoever.
? The paper introduces an HMM-based tech-
nique for checking whether two arguments are
of the proper type for a relation.
? The paper introduces a relational n-gram
model for the purpose of determining whether
a sentence that mentions multiple arguments
actually expresses a particular relationship be-
tween them.
? The paper introduces a novel language-
modeling system called REALM that combines
both HMM-based models and relational n-
gram models, and shows that REALM reduces
error by an average of 39% over previous meth-
ods, when applied to sparse extraction data.
The remainder of the paper is organized as fol-
lows. Section 2 introduces the IE assessment task,
and describes the REALM system in detail. Section
3 reports on our experimental results followed by a
discussion of related work in Section 4. Finally, we
conclude with a discussion of scalability and with
directions for future work.
2 IE Assessment
This section formalizes the IE assessment task and
describes the REALM system for solving it. An IE
assessor takes as input a list of candidate extractions
meant to denote instances of a relation, and outputs
a ranking of the extractions with the goal that cor-
rect extractions rank higher than incorrect ones. A
correct extraction is defined to be a true instance of
the relation mentioned in the input text.
More formally, the list of candidate extrac-
tions for a relation R is denoted as ER =
{(a1, b1), . . . , (am, bm)}. An extraction (ai, bi) is
an ordered pair of strings. The extraction is correct
if and only if the relation R holds between the argu-
ments named by ai and bi. For example, for R =
Headquartered, a pair (ai, bi) is correct iff there
exists an organization ai that is in fact headquartered
in the location bi.1
ER is generated by applying an extraction mech-
anism, typically a set of extraction ?patterns?, to
each sentence in a corpus, and recording the results.
Thus, many elements of ER are identical extractions
derived from different sentences in the corpus.
This task definition is notable for the minimal
inputs required?IE assessment does not require
knowing the relation name nor does it require hand-
tagged seed examples of the relation. Thus, an IE
Assessor is applicable to Open IE.
2.1 System Overview
In this section, we describe the REALM system,
which utilizes language modeling techniques to per-
form IE Assessment.
REALM takes as input a set of extractions ER,
and outputs a ranking of those extractions. The
algorithm REALM follows is outlined in Figure 1.
REALM begins by automatically selecting from ER
a set of bootstrapped seeds SR intended to serve as
correct examples of the relation R. REALM utilizes
the KnowItAll hypothesis, setting SR equal to the
h elements in ER extracted most frequently from
the underlying corpus. This results in a noisy set of
seeds, but the methods that use these seeds are noise
tolerant.
REALM then proceeds to rank the remaining
(non-seed) extractions by utilizing two language-
modeling components. An n-gram language model
is a probability distribution P (w1, ..., wn) over con-
secutive word sequences of length n in a corpus.
Formally, if we assume a seed (s1, s2) is a correct
extraction of a relation R, the distributional hypoth-
esis states that the context distribution around the
seed extraction, P (w1, ..., wn|wi = s1, wj = s2)
for 1 ? i, j ? n tends to be ?more similar? to
1For clarity, our discussion focuses on relations between
pairs of arguments. However, the methods we propose can be
extended to relations of any arity.
697
P (w1, ..., wn|wi = e1, wj = e2) when the extrac-
tion (e1, e2) is correct. Naively comparing context
distributions is problematic, however, because the
arguments to a relation often appear separated by
several intervening words. In our experiments, we
found that when relation arguments appear together
in a sentence, 75% of the time the arguments are
separated by at least three words. This implies that
n must be large, and for sparse argument pairs it is
not possible to estimate such a large language model
accurately, because the number of modeling param-
eters is proportional to the vocabulary size raised to
the nth power. To mitigate sparsity, REALM utilizes
smaller language models in its two components as a
means of ?backing-off? from estimating context dis-
tributions explicitly, as described below.
First, REALM utilizes an HMM to estimate
whether each extraction has arguments of the proper
type for the relation. Each relation R has a set
of types for its arguments. For example, the rela-
tion AuthorOf(a, b) requires that its first ar-
gument be an author, and that its second be some
kind of written work. Knowing whether extracted
arguments are of the proper type for a relation can
be quite informative for assessing extractions. The
challenge is, however, that this type information is
not given to the system since the relations (and the
types of the arguments) are not known in advance.
REALM solves this problem by comparing the dis-
tributions of the seed arguments and extraction ar-
guments. Type checking mitigates data sparsity by
leveraging every occurrence of the individual extrac-
tion arguments in the corpus, rather than only those
cases in which argument pairs occur near each other.
Although argument type checking is invalu-
able for extraction assessment, it is not suf-
ficient for extracting relationships between ar-
guments. For example, an IE system us-
ing only type information might determine that
Intel is a corporation and that Seattle is
a city, and therefore erroneously conclude that
Headquartered(Intel, Seattle) is cor-
rect. Thus, REALM?s second step is to employ an
n-gram-based language model to assess whether the
extracted arguments share the appropriate relation.
Again, this information is not given to the system,
so REALM compares the context distributions of the
extractions to those of the seeds. As described in
REALM(Extractions ER = {e1, ..., em})
SR = the h most frequent extractions in ER
UR = ER - SR
TypeRankings(UR)? HMM-T(SR, UR)
RelationRankings(UR)? REL-GRAMS(SR, UR)
return a ranking of ER with the elements of SR at the
top (ranked by frequency) followed by the elements of
UR = {u1, ..., um?h} ranked in ascending order of
TypeRanking(ui) ?RelationRanking(ui).
Figure 1: Pseudocode for REALM at run-time.
The language models used by the HMM-T and
REL-GRAMS components are constructed in a pre-
processing step.
Section 2.3, REALM employs a relational n-gram
language model in order to accurately compare con-
text distributions when extractions are sparse.
REALM executes the type checking and relation
assessment components separately; each component
takes the seed and non-seed extractions as arguments
and returns a ranking of the non-seeds. REALM then
combines the two components? assessments into a
single ranking. Although several such combinations
are possible, REALM simply ranks the extractions in
ascending order of the product of the ranks assigned
by the two components. The following subsections
describe REALM?s two components in detail.
We identify the proper nouns in our corpus us-
ing the LEX method (Downey et al, 2007). In ad-
dition to locating the proper nouns in the corpus,
LEX also concatenates each multi-token proper noun
(e.g.,Los Angeles) together into a single token.
Both of REALM?s components construct language
models from this tokenized corpus.
2.2 Type Checking with HMM-T
In this section, we describe our type-checking com-
ponent, which takes the form of a Hidden Markov
Model and is referred to as HMM-T. HMM-T ranks
the set UR of non-seed extractions, with a goal of
ranking those extractions with arguments of proper
type for R above extractions containing type errors.
Formally, let URi denote the set of the ith arguments
of the extractions in UR. Let SRi be defined simi-
larly for the seed set SR.
Our type checking technique exploits the distri-
butional hypothesis?in this case, the intuition that
698
Intel , headquartered in Santa+Clara
Figure 2: Graphical model employed by HMM-
T. Shown is the case in which k = 2. Corpus
pre-processing results in the proper noun Santa
Clara being concatenated into a single token.
extraction arguments in URi of the proper type will
likely appear in contexts similar to those in which
the seed arguments SRi appear. In order to iden-
tify terms that are distributionally similar, we train
a probabilistic generative Hidden Markov Model
(HMM), which treats each token in the corpus as
generated by a single hidden state variable. Here, the
hidden states take integral values from {1, . . . , T},
and each hidden state variable is itself generated by
some number k of previous hidden states.2 For-
mally, the joint distribution of the corpus, repre-
sented as a vector of tokens w, given a correspond-
ing vector of states t is:
P (w|t) =
?
i
P (wi|ti)P (ti|ti?1, . . . , ti?k) (1)
The distributions on the right side of Equation 1
can be learned from a corpus in an unsupervised
manner, such that words which are distributed sim-
ilarly in the corpus tend to be generated by simi-
lar hidden states (Rabiner, 1989). The generative
model is depicted as a Bayesian network in Figure 2.
The figure also illustrates the one way in which our
implementation is distinct from a standard HMM,
namely that proper nouns are detected a priori and
modeled as single tokens (e.g., Santa Clara is
generated by a single hidden state). This allows
the type checker to compare the state distributions
of different proper nouns directly, even when the
proper nouns contain differing numbers of words.
To generate a ranking of UR using the learned
HMM parameters, we rank the arguments ei accord-
ing to how similar their state distributions P (t|ei)
2Our implementation makes the simplifying assumption that
each sentence in the corpus is generated independently.
are to those of the seed arguments.3 Specifically, we
define a function:
f(e) =
?
ei?e
KL(
?
w??SRi
P (t|w?)
|SRi|
, P (t|ei)) (2)
where KL represents KL divergence, and the outer
sum is taken over the arguments ei of the extraction
e. We rank the elements of UR in ascending order of
f(e).
HMM-T has two advantages over a more tradi-
tional type checking approach of simply counting
the number of times in the corpus that each extrac-
tion appears in a context in which a seed also ap-
pears (cf. (Ravichandran et al, 2005)). The first
advantage of HMM-T is efficiency, as the traditional
approach involves a computationally expensive step
of retrieving the potentially large set of contexts in
which the extractions and seeds appear. In our ex-
periments, using HMM-T instead of a context-based
approach results in a 10-50x reduction in the amount
of data that is retrieved to perform type checking.
Secondly, on sparse data HMM-T has the poten-
tial to improve type checking accuracy. For exam-
ple, consider comparing Pickerington, a sparse
candidate argument of the type City, to the seed
argument Chicago, for which the following two
phrases appear in the corpus:
(i) ?Pickerington, Ohio?
(ii) ?Chicago, Illinois?
In these phrases, the textual contexts surrounding
Chicago and Pickerington are not identical,
so to the traditional approach these contexts offer
no evidence that Pickerington and Chicago
are of the same type. For a sparse token like
Pickerington, this is problematic because the
token may never occur in a context that precisely
matches that of a seed. In contrast, in the HMM, the
non-sparse tokens Ohio and Illinois are likely
to have similar state distributions, as they are both
the names of U.S. States. Thus, in the state space
employed by the HMM, the contexts in phrases (i)
and (ii) are in fact quite similar, allowing HMM-
T to detect that Pickerington and Chicago
are likely of the same type. Our experiments quan-
tify the performance improvements that HMM-T of-
3The distribution P (t|ei) for any ei can be obtained from
the HMM parameters using Bayes Rule.
699
fers over the traditional approach for type checking
sparse data.
The time required to learn HMM-T?s parameters
scales proportional to T k+1 times the corpus size.
Thus, for tractability, HMM-T uses a relatively small
state space of T = 20 states and a limited k value
of 3. While these settings are sufficient for type
checking (e.g., determining that Santa Clara is
a city) they are too coarse-grained to assess relations
between arguments (e.g., determining that Santa
Clara is the particular city in which Intel is
headquartered). We now turn to the REL-GRAMS
component, which performs the latter task.
2.3 Relation Assessment with REL-GRAMS
REALM?s relation assessment component, called
REL-GRAMS, tests whether the extracted arguments
have a desired relationship, but given REALM?s min-
imal input it has no a priori information about the
relationship. REL-GRAMS relies instead on the dis-
tributional hypothesis to test each extraction.
As argued in Section 2.1, it is intractable to build
an accurate language model for context distributions
surrounding sparse argument pairs. To overcome
this problem, we introduce relational n-gram mod-
els. Rather than simply modeling the context distri-
bution around a given argument, a relational n-gram
model specifies separate context distributions for an
arguments conditioned on each of the other argu-
ments with which it appears. The relational n-gram
model allows us to estimate context distributions for
pairs of arguments, even when the arguments do not
appear together within a fixed window of n words.
Further, by considering only consecutive argument
pairs, the number of distinct argument pairs in the
model grows at most linearly with the number of
sentences in the corpus. Thus, the relational n-gram
model can scale.
Formally, for a pair of arguments (e1, e2), a re-
lational n-gram model estimates the distributions
P (w1, ..., wn|wi = e1, e1 ? e2) for each 1 ? i ?
n, where the notation e1 ? e2 indicates the event
that e2 is the next argument to either the right or the
left of e1 in the corpus.
REL-GRAMS begins by building a relational n-
gram model of the arguments in the corpus. For
notational convenience, we represent the model?s
distributions in terms of ?context vectors? for each
pair of arguments. Formally, for a given sentence
containing arguments e1 and e2 consecutively, we
define a context of the ordered pair (e1, e2) to be
any window of n tokens around e1. Let C =
{c1, c2, ..., c|C|} be the set of all contexts of all ar-
gument pairs found in the corpus.4 For a pair of ar-
guments (ej , ek), we model their relationship using
a |C| dimensional context vector v(ej ,ek), whose i-th
dimension corresponds to the number of times con-
text ci occurred with the pair (ej , ek) in the corpus.
These context vectors are similar to document vec-
tors from Information Retrieval (IR), and we lever-
age IR research to compare them, as described be-
low.
To assess each extraction, we determine how sim-
ilar its context vector is to a canonical seed vec-
tor (created by summing the context vectors of the
seeds). While there are many potential methods
for determining similarity, in this work we rank ex-
tractions by decreasing values of the BM25 dis-
tance metric. BM25 is a TF-IDF variant intro-
duced in TREC-3(Robertson et al, 1992), which
outperformed both the standard cosine distance and
a smoothed KL divergence on our data.
3 Experimental Results
This section describes our experiments on IE assess-
ment for sparse data. We start by describing our
experimental methodology, and then present our re-
sults. The first experiment tests the hypothesis that
HMM-T outperforms an n-gram-based method on
the task of type checking. The second experiment
tests the hypothesis that REALM outperforms multi-
ple approaches from previous work, and also outper-
forms each of its HMM-T and REL-GRAMS compo-
nents taken in isolation.
3.1 Experimental Methodology
The corpus used for our experiments consisted of a
sample of sentences taken from Web pages. From
an initial crawl of nine million Web pages, we se-
lected sentences containing relations between proper
nouns. The resulting text corpus consisted of about
4Pre-computing the set C requires identifying in advance
the potential relation arguments in the corpus. We consider the
proper nouns identified by the LEX method (see Section 2.1) to
be the potential arguments.
700
three million sentences, and was tokenized as de-
scribed in Section 2. For tractability, before and after
performing tokenization, we replaced each token oc-
curring fewer than five times in the corpus with one
of two ?unknown word? markers (one for capital-
ized words, and one for uncapitalized words). This
preprocessing resulted in a corpus containing about
sixty-five million total tokens, and 214,787 unique
tokens.
We evaluated performance on four relations:
Conquered, Founded, Headquartered, and
Merged. These four relations were chosen because
they typically take proper nouns as arguments, and
included a large number of sparse extractions. For
each relationR, the candidate extraction listER was
obtained using TEXTRUNNER (Banko et al, 2007).
TEXTRUNNER is an IE system that computes an in-
dex of all extracted relationships it recognizes, in the
form of (object, predicate, object) triples. For each
of our target relations, we executed a single query
to the TEXTRUNNER index for extractions whose
predicate contained a phrase indicative of the rela-
tion (e.g., ?founded by?, ?headquartered in?), and
the results formed our extraction list. For each rela-
tion, the 10 most frequent extractions served as boot-
strapped seeds. All of the non-seed extractions were
sparse (no argument pairs were extracted more than
twice for a given relation). These test sets contained
a total of 361 extractions.
3.2 Type Checking Experiments
As discussed in Section 2.2, on sparse data HMM-T
has the potential to outperform type checking meth-
ods that rely on textual similarities of context vec-
tors. To evaluate this claim, we tested the HMM-T
system against an N-GRAMS type checking method
on the task of type-checking the arguments to a re-
lation. The N-GRAMS method compares the context
vectors of extractions in the same way as the REL-
GRAMS method described in Section 2.3, but is not
relational (N-GRAMS considers the distribution of
each extraction argument independently, similar to
HMM-T). We tagged an extraction as type correct iff
both arguments were valid for the relation, ignoring
whether the relation held between the arguments.
The results of our type checking experiments are
shown in Table 1. For all types, HMM-T outper-
forms N-GRAMS, and HMM-T reduces error (mea-
Type HMM-T N-GRAMS
Conquered 0.917 0.767
Founded 0.827 0.636
Headquartered 0.734 0.589
Merged 0.920 0.854
Average 0.849 0.712
Table 1: Type Checking Performance. Listed is area
under the precision/recall curve. HMM-T outper-
forms N-GRAMS for all relations, and reduces the
error in terms of missing area under the curve by
46% on average.
sured in missing area under the precision/recall
curve) by 46%. The performance difference on each
relation is statistically significant (p < 0.01, two-
sampled t-test), using the methodology for measur-
ing the standard deviation of area under the preci-
sion/recall curve given in (Richardson and Domin-
gos, 2006). N-GRAMS, like REL-GRAMS, employs
the BM-25 metric to measure distributional similar-
ity between extractions and seeds. Replacing BM-
25 with cosine distance cuts HMM-T?s advantage
over N-GRAMS, but HMM-T?s error rate is still 23%
lower on average.
3.3 Experiments with REALM
The REALM system combines the type checking
and relation assessment components to assess ex-
tractions. Here, we test the ability of REALM to
improve the ranking of a state of the art IE system,
TEXTRUNNER. For these experiments, we evalu-
ate REALM against the TEXTRUNNER frequency-
based ordering, a pattern-learning approach, and the
HMM-T and REL-GRAMS components taken in iso-
lation. The TEXTRUNNER frequency-based order-
ing ranks extractions in decreasing order of their ex-
traction frequency, and importantly, for our task this
ordering is essentially equivalent to that produced by
the ?Urns? (Downey et al, 2005) and Pointwise Mu-
tual Information (Etzioni et al, 2005) approaches
employed in previous work.
The pattern-learning approach, denoted as PL, is
modeled after Snowball (Agichtein, 2006). The al-
gorithm and parameter settings for PL were those
manually tuned for the Headquartered relation
in previous work (Agichtein, 2005). A sensitivity
analysis of these parameters indicated that the re-
701
Conquered Founded Headquartered Merged Average
Avg. Prec. 0.698 0.578 0.400 0.742 0.605
TEXTRUNNER 0.738 0.699 0.710 0.784 0.733
PL 0.885 0.633 0.651 0.852 0.785
PL+ HMM-T 0.883 0.722 0.727 0.900 0.808
HMM-T 0.830 0.776 0.678 0.864 0.787
REL-GRAMS 0.929 (39%) 0.713 0.758 0.886 0.822
REALM 0.907 (19%) 0.781 (27%) 0.810 (35%) 0.908 (38%) 0.851 (39%)
Table 2: Performance of REALM for assessment of sparse extractions. Listed is area under the preci-
sion/recall curve for each method. In parentheses is the percentage reduction in error over the strongest
baseline method (TEXTRUNNER or PL) for each relation. ?Avg. Prec.? denotes the fraction of correct
examples in the test set for each relation. REALM outperforms its REL-GRAMS and HMM-T components
taken in isolation, as well as the TEXTRUNNER and PL systems from previous work.
sults are sensitive to the parameter settings. How-
ever, we found no parameter settings that performed
significantly better, and many settings performed
significantly worse. As such, we believe our re-
sults reasonably reflect the performance of a pattern
learning system on this task. Because PL performs
relation assessment, we also attempted combining
PL with HMM-T in a hybrid method (PL+ HMM-T)
analogous to REALM.
The results of these experiments are shown in Ta-
ble 2. REALM outperforms the TEXTRUNNER and
PL baselines for all relations, and reduces the miss-
ing area under the curve by an average of 39% rel-
ative to the strongest baseline. The performance
differences between REALM and TEXTRUNNER are
statistically significant for all relations, as are differ-
ences between REALM and PL for all relations ex-
cept Conquered (p < 0.01, two-sampled t-test).
The hybrid REALM system also outperforms each
of its components in isolation.
4 Related Work
To our knowledge, REALM is the first system to use
language modeling techniques for IE Assessment.
Redundancy-based approaches to pattern-based
IE assessment (Downey et al, 2005; Etzioni et al,
2005) require that extractions appear relatively fre-
quently with a limited set of patterns. In contrast,
REALM utilizes all contexts to build a model of ex-
tractions, rather than a limited set of patterns. Our
experiments demonstrate that REALM outperforms
these approaches on sparse data.
Type checking using named-entity taggers has
been previously shown to improve the precision of
pattern-based IE systems (Agichtein, 2005; Feld-
man et al, 2006), but the HMM-T type-checking
component we develop differs from this work in im-
portant ways. Named-entity taggers are limited in
that they typically recognize only small set of types
(e.g., ORGANIZATION, LOCATION, PERSON),
and they require hand-tagged training data for each
type. HMM-T, by contrast, performs type check-
ing for any type. Finally, HMM-T does not require
hand-tagged training data.
Pattern learning is a common technique for ex-
tracting and assessing sparse data (e.g. (Agichtein,
2005; Riloff and Jones, 1999; Pas?ca et al, 2006)).
Our experiments demonstrate that REALM outper-
forms a pattern learning system closely modeled af-
ter (Agichtein, 2005). REALM is inspired by pat-
tern learning techniques (in particular, both use the
distributional hypothesis to assess sparse data) but
is distinct in important ways. Pattern learning tech-
niques require substantial processing of the corpus
after the relations they assess have been specified.
Because of this, pattern learning systems are un-
suited to Open IE. Unlike these techniques, REALM
pre-computes language models which allow it to as-
sess extractions for arbitrary relations at run-time.
In essence, pattern-learning methods run in time lin-
ear in the number of relations whereas REALM?s run
time is constant in the number of relations. Thus,
REALM scales readily to large numbers of relations
whereas pattern-learning methods do not.
702
A second distinction of REALM is that its type
checker, unlike the named entity taggers employed
in pattern learning systems (e.g., Snowball), can be
used to identify arbitrary types. A final distinction is
that the language models REALM employs require
fewer parameters and heuristics than pattern learn-
ing techniques.
Similar distinctions exist between REALM and a
recent system designed to assess sparse extractions
by bootstrapping a classifier for each target relation
(Feldman et al, 2006). As in pattern learning, con-
structing the classifiers requires substantial process-
ing after the target relations have been specified, and
a set of hand-tagged examples per relation, making
it unsuitable for Open IE.
5 Conclusions
This paper demonstrated that unsupervised language
models, as embodied in the REALM system, are an
effective means of assessing sparse extractions.
Another attractive feature of REALM is its scal-
ability. Scalability is a particularly important con-
cern forOpen Information Extraction, the task of ex-
tracting large numbers of relations that are not spec-
ified in advance. Because HMM-T and REL-GRAMS
both pre-compute language models, REALM can be
queried efficiently to perform IE Assessment. Fur-
ther, the language models are constructed indepen-
dently of the target relations, allowing REALM to
perform IE Assessment even when relations are not
specified in advance.
In future work, we plan to develop a probabilistic
model of the information computed by REALM. We
also plan to evaluate the use of non-local context for
IE Assessment by integrating document-level mod-
eling techniques (e.g., Latent Dirichlet Allocation).
Acknowledgements
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, DARPA contract
NBCHD030010, ONR grant N00014-05-1-0185 as
well as a gift from Google. The first author is sup-
ported by an MSR graduate fellowship sponsored by
Microsoft Live Labs. We thank Michele Banko, Jeff
Bilmes, Katrin Kirchhoff, and Alex Yates for helpful
comments.
References
E. Agichtein. 2005. Extracting Relations From Large
Text Collections. Ph.D. thesis, Department of Com-
puter Science, Columbia University.
E. Agichtein. 2006. Confidence estimation methods for
partially supervised relation extraction. In SDM 2006.
M. Banko, M. Cararella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Procs. of IJCAI 2007.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Prob-
abilistic Model of Redundancy in Information Extrac-
tion. In Procs. of IJCAI 2005.
D. Downey, M. Broadhead, and O. Etzioni. 2007. Locat-
ing complex named entities in web text. In Procs. of
IJCAI 2007.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
R. Feldman, B. Rosenfeld, S. Soderland, and O. Etzioni.
2006. Self-supervised relation extraction from the
web. In ISMIS, pages 755?764.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics, pages 26?47.
New York: Oxford University Press.
C. D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Procs. of ACL/COLING 2006.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE, 77(2):257?286.
D. Ravichandran, P. Pantel, and E. H. Hovy. 2005. Ran-
domized Algorithms and NLP: Using Locality Sensi-
tive Hash Functions for High Speed Noun Clustering.
In Procs. of ACL 2005.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107?136.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-level Boot-strapping.
In Procs. of AAAI-99, pages 1044?1049.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC-3. In
Text REtrieval Conference, pages 21?30.
703
Proceedings of ACL-08: HLT, pages 28?36,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The Tradeoffs Between Open and Traditional Relation Extraction
Michele Banko and Oren Etzioni
Turing Center
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195, USA
banko,etzioni@cs.washington.edu
Abstract
Traditional Information Extraction (IE) takes
a relation name and hand-tagged examples of
that relation as input. Open IE is a relation-
independent extraction paradigm that is tai-
lored to massive and heterogeneous corpora
such as theWeb. An Open IE system extracts a
diverse set of relational tuples from text with-
out any relation-specific input. How is Open
IE possible? We analyze a sample of English
sentences to demonstrate that numerous rela-
tionships are expressed using a compact set
of relation-independent lexico-syntactic pat-
terns, which can be learned by an Open IE sys-
tem.
What are the tradeoffs between Open IE and
traditional IE? We consider this question in
the context of two tasks. First, when the
number of relations is massive, and the rela-
tions themselves are not pre-specified, we ar-
gue that Open IE is necessary. We then present
a new model for Open IE called O-CRF and
show that it achieves increased precision and
nearly double the recall than the model em-
ployed by TEXTRUNNER, the previous state-
of-the-art Open IE system. Second, when the
number of target relations is small, and their
names are known in advance, we show that
O-CRF is able to match the precision of a tra-
ditional extraction system, though at substan-
tially lower recall. Finally, we show how to
combine the two types of systems into a hy-
brid that achieves higher precision than a tra-
ditional extractor, with comparable recall.
1 Introduction
Relation Extraction (RE) is the task of recognizing
the assertion of a particular relationship between two
or more entities in text. Typically, the target relation
(e.g., seminar location) is given to the RE system as
input along with hand-crafted extraction patterns or
patterns learned from hand-labeled training exam-
ples (Brin, 1998; Riloff and Jones, 1999; Agichtein
and Gravano, 2000). Such inputs are specific to the
target relation. Shifting to a new relation requires a
person to manually create new extraction patterns or
specify new training examples. This manual labor
scales linearly with the number of target relations.
In 2007, we introduced a new approach to the
RE task, called Open Information Extraction (Open
IE), which scales RE to the Web. An Open IE sys-
tem extracts a diverse set of relational tuples without
requiring any relation-specific human input. Open
IE?s extraction process is linear in the number of
documents in the corpus, and constant in the num-
ber of relations. Open IE is ideally suited to corpora
such as the Web, where the target relations are not
known in advance, and their number is massive.
The relationship between standard RE systems
and the new Open IE paradigm is analogous to the
relationship between lexicalized and unlexicalized
parsers. Statistical parsers are usually lexicalized
(i.e. they make parsing decisions based on n-gram
statistics computed for specific lexemes). However,
Klein and Manning (2003) showed that unlexical-
ized parsers are more accurate than previously be-
lieved, and can be learned in an unsupervised man-
ner. Klein and Manning analyze the tradeoffs be-
28
tween the two approaches to parsing and argue that
state-of-the-art parsing will benefit from employing
both approaches in concert. In this paper, we exam-
ine the tradeoffs between relation-specific (?lexical-
ized?) extraction and relation-independent (?unlexi-
calized?) extraction and reach an analogous conclu-
sion.
Is it, in fact, possible to learn relation-independent
extraction patterns? What do they look like? We first
consider the task of open extraction, in which the
goal is to extract relationships from text when their
number is large and identity unknown. We then con-
sider the targeted extraction task, in which the goal
is to locate instances of a known relation. How does
the precision and recall of Open IE compare with
that of relation-specific extraction? Is it possible to
combine Open IE with a ?lexicalized? RE system
to improve performance? This paper addresses the
questions raised above and makes the following con-
tributions:
? We present O-CRF, a new Open IE system that
uses Conditional Random Fields, and demon-
strate its ability to extract a variety of rela-
tions with a precision of 88.3% and recall of
45.2%. We compare O-CRF to O-NB, the ex-
traction model previously used by TEXTRUN-
NER (Banko et al, 2007), a state-of-the-art
Open IE system. We show that O-CRF achieves
a relative gain in F-measure of 63% over O-NB.
? We provide a corpus-based characterization of
how binary relationships are expressed in En-
glish to demonstrate that learning a relation-
independent extractor is feasible, at least for the
English language.
? In the targeted extraction case, we compare the
performance of O-CRF to a traditional RE sys-
tem and find that without any relation-specific
input, O-CRF obtains the same precision with
lower recall compared to a lexicalized extractor
trained using hundreds, and sometimes thou-
sands, of labeled examples per relation.
? We present H-CRF, an ensemble-based extrac-
tor that learns to combine the output of the
lexicalized and unlexicalized RE systems and
achieves a 10% relative increase in precision
with comparable recall over traditional RE.
The remainder of this paper is organized as fol-
lows. Section 2 assesses the promise of relation-
independent extraction for the English language by
characterizing how a sample of relations is ex-
pressed in text. Section 3 describes O-CRF, a new
Open IE system, as well as R1-CRF, a standard RE
system; a hybrid RE system is then presented in Sec-
tion 4. Section 5 reports on our experimental results.
Section 6 considers related work, which is then fol-
lowed by a discussion of future work.
2 The Nature of Relations in English
How are relationships expressed in English sen-
tences? In this section, we show that many rela-
tionships are consistently expressed using a com-
pact set of relation-independent lexico-syntactic pat-
terns, and quantify their frequency based on a sam-
ple of 500 sentences selected at random from an IE
training corpus developed by (Bunescu andMooney,
2007).1 This observation helps to explain the suc-
cess of open relation extraction, which learns a
relation-independent extraction model as described
in Section 3.1.
Previous work has noted that distinguished re-
lations, such as hypernymy (is-a) and meronymy
(part-whole), are often expressed using a small num-
ber of lexico-syntactic patterns (Hearst, 1992). The
manual identification of these patterns inspired a
body of work in which this initial set of extraction
patterns is used to seed a bootstrapping process that
automatically acquires additional patterns for is-a or
part-whole relations (Etzioni et al, 2005; Snow et
al., 2005; Girju et al, 2006), It is quite natural then
to consider whether the same can be done for all bi-
nary relationships.
To characterize how binary relationships are ex-
pressed, one of the authors of this paper carefully
studied the labeled relation instances and produced
a lexico-syntactic pattern that captured the relation
for each instance. Interestingly, we found that 95%
of the patterns could be grouped into the categories
listed in Table 1. Note, however, that the patterns
shown in Table 1 are greatly simplified by omitting
the exact conditions under which they will reliably
produce a correct extraction. For instance, while
many relationships are indicated strictly by a verb,
1For simplicity, we restrict our study to binary relationships.
29
Simplified
Relative Lexico-Syntactic
Frequency Category Pattern
37.8 Verb E1 Verb E2
X established Y
22.8 Noun+Prep E1 NP Prep E2
X settlement with Y
16.0 Verb+Prep E1 Verb Prep E2
X moved to Y
9.4 Infinitive E1 to Verb E2
X plans to acquire Y
5.2 Modifier E1 Verb E2 Noun
X is Y winner
1.8 Coordinaten E1 (and|,|-|:) E2 NP
X-Y deal
1.0 Coordinatev E1 (and|,) E2 Verb
X , Y merge
0.8 Appositive E1 NP (:|,)? E2
X hometown : Y
Table 1: Taxonomy of Binary Relationships: Nearly 95%
of 500 randomly selected sentences belongs to one of the
eight categories above.
detailed contextual cues are required to determine,
exactly which, if any, verb observed in the context
of two entities is indicative of a relationship between
them. In the next section, we show how we can use a
Conditional Random Field, a model that can be de-
scribed as a finite state machine with weighted tran-
sitions, to learn a model of how binary relationships
are expressed in English.
3 Relation Extraction
Given a relation name, labeled examples of the re-
lation, and a corpus, traditional Relation Extraction
(RE) systems output instances of the given relation
found in the corpus. In the open extraction task, re-
lation names are not known in advance. The sole
input to an Open IE system is a corpus, along with
a small set of relation-independent heuristics, which
are used to learn a general model of extraction for
all relations at once.
The task of open extraction is notably more diffi-
cult than the traditional formulation of RE for sev-
eral reasons. First, traditional RE systems do not
attempt to extract the text that signifies a relation in
a sentence, since the relation name is given. In con-
trast, an Open IE system has to locate both the set of
entities believed to participate in a relation, and the
salient textual cues that indicate the relation among
them. Knowledge extracted by an open system takes
the form of relational tuples (r, e1, . . . , en) that con-
tain two or more entities e1, . . . , en, and r, the name
of the relationship among them. For example, from
the sentence, ?Microsoft is headquartered in beau-
tiful Redmond?, we expect to extract (is headquar-
tered in, Microsoft, Redmond). Moreover, following
extraction, the system must identify exactly which
relation strings r correspond to a general relation of
interest. To ensure high-levels of coverage on a per-
relation basis, we need, for example to deduce that
? ?s headquarters in?, ?is headquartered in? and ?is
based in? are different ways of expressing HEAD-
QUARTERS(X,Y).
Second, a relation-independent extraction process
makes it difficult to leverage the full set of features
typically used when performing extraction one re-
lation at a time. For instance, the presence of the
words company and headquarters will be useful in
detecting instances of the HEADQUARTERS(X,Y)
relation, but are not useful features for identifying
relations in general. Finally, RE systems typically
use named-entity types as a guide (e.g., the second
argument to HEADQUARTERS should be a LOCA-
TION). In Open IE, the relations are not known in
advance, and neither are their argument types.
The unique nature of the open extraction task has
led us to develop O-CRF, an open extraction sys-
tem that uses the power of graphical models to iden-
tify relations in text. The remainder of this section
describes O-CRF, and compares it to the extraction
model employed by TEXTRUNNER, the first Open
IE system (Banko et al, 2007). We then describe
R1-CRF, a RE system that can be applied in a typi-
cal one-relation-at-a-time setting.
3.1 Open Extraction with Conditional Random
Fields
TEXTRUNNER initially treated Open IE as a clas-
sification problem, using a Naive Bayes classifier to
predict whether heuristically-chosen tokens between
two entities indicated a relationship or not. For the
remainder of this paper, we refer to this model as
O-NB. Whereas classifiers predict the label of a sin-
gle variable, graphical models model multiple, in-
30
K a f k a
E N T O E N TO E N T B 	 R E L I 	 R E L
, P r
a g u ea w
r i t
e
r b o r n i n
Figure 1: Relation Extraction as Sequence Labeling: A
CRF is used to identify the relationship, born in, between
Kafka and Prague
terdependent variables. Conditional Random Fields
(CRFs) (Lafferty et al, 2001), are undirected graphi-
cal models trained to maximize the conditional prob-
ability of a finite set of labels Y given a set of input
observations X . By making a first-order Markov as-
sumption about the dependencies among the output
variables Y , and arranging variables sequentially in
a linear chain, RE can be treated as a sequence la-
beling problem. Linear-chain CRFs have been ap-
plied to a variety of sequential text processing tasks
including named-entity recognition, part-of-speech
tagging, word segmentation, semantic role identifi-
cation, and recently relation extraction (Culotta et
al., 2006).
3.1.1 Training
As with O-NB, O-CRF?s training process is self-
supervised. O-CRF applies a handful of relation-
independent heuristics to the PennTreebank and ob-
tains a set of labeled examples in the form of rela-
tional tuples. The heuristics were designed to cap-
ture dependencies typically obtained via syntactic
parsing and semantic role labelling. For example,
a heuristic used to identify positive examples is the
extraction of noun phrases participating in a subject-
verb-object relationship, e.g., ?<Einstein> received
<the Nobel Prize> in 1921.? An example of a
heuristic that locates negative examples is the ex-
traction of objects that cross the boundary of an ad-
verbial clause, e.g. ?He studied <Einstein?s work>
when visiting <Germany>.?
The resulting set of labeled examples are de-
scribed using features that can be extracted without
syntactic or semantic analysis and used to train a
CRF, a sequence model that learns to identify spans
of tokens believed to indicate explicit mentions of
relationships between entities.
O-CRF first applies a phrase chunker to each doc-
ument, and treats the identified noun phrases as can-
didate entities for extraction. Each pair of enti-
ties appearing no more than a maximum number of
words apart and their surrounding context are con-
sidered as possible evidence for RE. The entity pair
serves to anchor each end of a linear-chain CRF, and
both entities in the pair are assigned a fixed label of
ENT. Tokens in the surrounding context are treated
as possible textual cues that indicate a relation, and
can be assigned one of the following labels: B-REL,
indicating the start of a relation, I-REL, indicating
the continuation of a predicted relation, or O, indi-
cating the token is not believed to be part of an ex-
plicit relationship. An illustration is given in Fig-
ure 1.
The set of features used by O-CRF is largely
similar to those used by O-NB and other state-
of-the-art relation extraction systems, They in-
clude part-of-speech tags (predicted using a sepa-
rately trained maximum-entropy model), regular ex-
pressions (e.g.detecting capitalization, punctuation,
etc.), context words, and conjunctions of features
occurring in adjacent positions within six words to
the left and six words to the right of the current
word. A unique aspect of O-CRF is that O-CRF
uses context words belonging only to closed classes
(e.g. prepositions and determiners) but not function
words such as verbs or nouns. Thus, unlike most RE
systems, O-CRF does not try to recognize semantic
classes of entities.
O-CRF has a number of limitations, most of which
are shared with other systems that perform extrac-
tion from natural language text. First, O-CRF only
extracts relations that are explicitly mentioned in
the text; implicit relationships that could inferred
from the text would need to be inferred from O-
CRF extractions. Second, O-CRF focuses on rela-
tionships that are primarily word-based, and not in-
dicated solely from punctuation or document-level
features. Finally, relations must occur between en-
tity names within the same sentence.
O-CRF was built using the CRF implementation
provided by MALLET (McCallum, 2002), as well
as part-of-speech tagging and phrase-chunking tools
available from OPENNLP.2
2http://opennlp.sourceforge.net
31
3.1.2 Extraction
Given an input corpus, O-CRF makes a single pass
over the data, and performs entity identification us-
ing a phrase chunker. The CRF is then used to label
instances relations for each possible entity pair, sub-
ject to the constraints mentioned previously.
Following extraction, O-CRF applies the RE-
SOLVER algorithm (Yates and Etzioni, 2007) to find
relation synonyms, the various ways in which a re-
lation is expressed in text. RESOLVER uses a prob-
abilistic model to predict if two strings refer to the
same item, based on relational features, in an unsu-
pervised manner. In Section 5.2 we report that RE-
SOLVER boosts the recall of O-CRF by 50%.
3.2 Relation-Specific Extraction
To compare the behavior of open, or ?unlexicalized,?
extraction to relation-specific, or ?lexicalized? ex-
traction, we developed a CRF-based extractor under
the traditional RE paradigm. We refer to this system
as R1-CRF.
Although the graphical structure of R1-CRF is the
same as O-CRF R1-CRF differs in a few ways. A
given relation R is specified a priori, and R1-CRF is
trained from hand-labeled positive and negative in-
stances of R. The extractor is also permitted to use
all lexical features, and is not restricted to closed-
class words as is O-CRF. Since R is known in ad-
vance, if R1-CRF outputs a tuple at extraction time,
the tuple is believed to be an instance of R.
4 Hybrid Relation Extraction
Since O-CRF and R1-CRF have complementary
views of the extraction process, it is natural to won-
der whether they can be combined to produce a
more powerful extractor. In many machine learn-
ing settings, the use of an ensemble of diverse clas-
sifiers during prediction has been observed to yield
higher levels of performance compared to individ-
ual algorithms. We now describe an ensemble-based
or hybrid approach to RE that leverages the differ-
ent views offered by open, self-supervised extraction
in O-CRF, and lexicalized, supervised extraction in
R1-CRF.
4.1 Stacking
Stacked generalization, or stacking, (Wolpert,
1992), is an ensemble-based framework in which the
goal is learn a meta-classifier from the output of sev-
eral base-level classifiers. The training set used to
train the meta-classifier is generated using a leave-
one-out procedure: for each base-level algorithm, a
classifier is trained from all but one training example
and then used to generate a prediction for the left-
out example. The meta-classifier is trained using the
predictions of the base-level classifiers as features,
and the true label as given by the training data.
Previous studies (Ting and Witten, 1999; Zenko
and Dzeroski, 2002; Sigletos et al, 2005) have
shown that the probabilities of each class value as
estimated by each base-level algorithm are effective
features when training meta-learners. Stacking was
shown to be consistently more effective than voting,
another popular ensemble-based method in which
the outputs of the base-classifiers are combined ei-
ther through majority vote or by taking the class
value with the highest average probability.
4.2 Stacked Relation Extraction
We used the stacking methodology to build an
ensemble-based extractor, referred to as H-CRF.
Treating the output of an O-CRF and R1-CRF as
black boxes, H-CRF learns to predict which, if any,
tokens found between a pair of entities (e1, e2), in-
dicates a relationship. Due to the sequential nature
of our RE task, H-CRF employs a CRF as the meta-
learner, as opposed to a decision tree or regression-
based classifier.
H-CRF uses the probability distribution over the
set of possible labels according to each O-CRF and
R1-CRF as features. To obtain the probability at
each position of a linear-chain CRF, the constrained
forward-backward technique described in (Culotta
andMcCallum, 2004) is used. H-CRF also computes
the Monge Elkan distance (Monge and Elkan, 1996)
between the relations predicted by O-CRF and R1-
CRF and includes the result in the feature set. An
additional meta-feature utilized by H-CRF indicates
whether either or both base extractors return ?no re-
lation? for a given pair of entities. In addition to
these numeric features, H-CRF uses a subset of the
base features used by O-CRF and R1-CRF. At each
32
O-CRF O-NB
Category P R F1 P R F1
Verb 93.9 65.1 76.9 100 38.6 55.7
Noun+Prep 89.1 36.0 51.3 100 9.7 55.7
Verb+Prep 95.2 50.0 65.6 95.2 25.3 40.0
Infinitive 95.7 46.8 62.9 100 25.5 40.6
Other 0 0 0 0 0 0
All 88.3 45.2 59.8 86.6 23.2 36.6
Table 2: Open Extraction by Relation Category. O-CRF
outperforms O-NB, obtaining nearly double its recall and
increased precision. O-CRF?s gains are partly due to its
lower false positive rate for relationships categorized as
?Other.?
given position i between e1 and e2, the presence of
the word observed at i as a feature, as well as the
presence of the part-of-speech-tag at i.
5 Experimental Results
The following experiments demonstrate the benefits
of Open IE for two tasks: open extraction and tar-
geted extraction.
Section 5.1, assesses the ability of O-CRF to lo-
cate instances of relationships when the number of
relationships is large and their identity is unknown.
We show that without any relation-specific input, O-
CRF extracts binary relationships with high precision
and a recall that nearly doubles that of O-NB.
Sections 5.2 and 5.3 compare O-CRF to tradi-
tional and hybrid RE when the goal is to locate in-
stances of a small set of known target relations. We
find that while single-relation extraction, as embod-
ied by R1-CRF, achieves comparatively higher lev-
els of recall, it takes hundreds, and sometimes thou-
sands, of labeled examples per relation, for R1-
CRF to approach the precision obtained by O-CRF,
which is self-trained without any relation-specific
input. We also show that the combination of unlex-
icalized, open extraction in O-CRF and lexicalized,
supervised extraction in R1-CRF improves precision
and F-measure compared to a standalone RE system.
5.1 Open Extraction
This section contrasts the performance of O-CRF
with that of O-NB on an Open IE task, and shows
that O-CRF achieves both double the recall and in-
creased precision relative to O-NB. For this exper-
iment, we used the set of 500 sentences3 described
in Section 2. Both IE systems were designed and
trained prior to the examination of the sample sen-
tences; thus the results on this sentence sample pro-
vide a fair measurement of their performance.
While the TEXTRUNNER system was previously
found to extract over 7.5 million tuples from a cor-
pus of 9 million Web pages, these experiments are
the first to assess its true recall over a known set of
relational tuples. As reported in Table 2, O-CRF ex-
tracts relational tuples with a precision of 88.3% and
a recall of 45.2%. O-CRF achieves a relative gain
in F1 of 63.4% over the O-NB model employed by
TEXTRUNNER, which obtains a precision of 86.6%
and a recall of 23.2%. The recall of O-CRF nearly
doubles that of O-NB.
O-CRF is able to extract instances of the four
most frequently observed relation types ? Verb,
Noun+Prep, Verb+Prep and Infinitive. Three of the
four remaining types ? Modifier, Coordinaten and
Coordinatev ? which comprise only 8% of the sam-
ple, are not handled due to simplifying assumptions
made by both O-CRF and O-NB that tokens indicat-
ing a relation occur between entity mentions in the
sentence.
5.2 O-CRF vs. R1-CRF Extraction
To compare performance of the extractors when a
small set of target relationships is known in ad-
vance, we used labeled data for four different re-
lations ? corporate acquisitions, birthplaces, inven-
tors of products and award winners. The first two
datasets were collected from the Web, and made
available by Bunescu and Mooney (2007). To aug-
ment the size of our corpus, we used the same tech-
nique to collect data for two additional relations, and
manually labelled positive and negative instances by
hand over all collections. For each of the four re-
lations in our collection, we trained R1-CRF from
labeled training data, and ran each of R1-CRF and
O-CRF over the respective test sets, and compared
the precision and recall of all tuples output by each
system.
Table 3 shows that from the start, O-CRF achieves
a high level of precision ? 75.0% ? without any
3Available at http://www.cs.washington.edu/research/
knowitall/hlt-naacl08-data.txt
33
O-CRF R1-CRF
Relation P R P R Train Ex
Acquisition 75.6 19.5 67.6 69.2 3042
Birthplace 90.6 31.1 92.3 64.4 1853
InventorOf 88.0 17.5 81.3 50.8 682
WonAward 62.5 15.3 73.6 52.8 354
All 75.0 18.4 73.9 58.4 5930
Table 3: Precision (P) and Recall (R) of O-CRF and R1-
CRF.
O-CRF R1-CRF
Relation P R P R Train Ex
Acquisition 75.6 19.5 67.6 69.2 3042?
Birthplace 90.6 31.1 92.3 53.3 600
InventorOf 88.0 17.5 81.3 50.8 682?
WonAward 62.5 15.3 65.4 61.1 50
All 75.0 18.4 70.17 60.7 >4374
Table 4: For 4 relations, a minimum of 4374 hand-tagged
examples is needed for R1-CRF to approximately match
the precision of O-CRF for each relation. A ??? indicates
the use of all available training data; in these cases, R1-
CRF was unable to match the precision of O-CRF.
relation-specific data. Using labeled training data,
the R1-CRF system achieves a slightly lower preci-
sion of 73.9%.
Exactly how many training examples per relation
does it take R1-CRF to achieve a comparable level
of precision? We varied the number of training ex-
amples given to R1-CRF, and found that in 3 out of
4 cases it takes hundreds, if not thousands of labeled
examples for R1-CRF to achieve acceptable levels
of precision. In two cases ? acquisitions and inven-
tions ? R1-CRF is unable to match the precision of
O-CRF, even with many labeled examples. Table 4
summarizes these findings.
Using labeled data, R1-CRF obtains a recall of
58.4%, compared to O-CRF, whose recall is 18.4%.
A large number of false negatives on the part of O-
CRF can be attributed to its lack of lexical features,
which are often crucial when part-of-speech tagging
errors are present. For instance, in the sentence, ?Ya-
hoo To Acquire Inktomi?, ?Acquire? is mistaken for
a proper noun, and sufficient evidence of the exis-
tence of a relationship is absent. The lexicalized R1-
CRF extractor is able to recover from this error; the
presence of the word ?Acquire? is enough to recog-
R1-CRF Hybrid
Relation P R F1 P R F1
Acquisition 67.6 69.2 68.4 76.0 67.5 71.5
Birthplace 93.6 64.4 76.3 96.5 62.2 75.6
InventorOf 81.3 50.8 62.5 87.5 52.5 65.6
WonAward 73.6 52.8 61.5 75.0 50.0 60.0
All 73.9 58.4 65.2 79.2 56.9 66.2
Table 5: A hybrid extractor that uses O-CRF improves
precision for all relations, at a small cost to recall.
nize the positive instance, despite the incorrect part-
of-speech tag.
Another source of recall issues facing O-CRF is
its ability to discover synonyms for a given relation.
We found that while RESOLVER improves the rela-
tive recall of O-CRF by nearly 50%, O-CRF locates
fewer synonyms per relation compared to its lexical-
ized counterpart. With RESOLVER, O-CRF finds an
average of 6.5 synonyms per relation compared to
R1-CRF?s 16.25.
In light of our findings, the relative tradeoffs of
open versus traditional RE are as follows. Open IE
automatically offers a high level of precision without
requiring manual labor per relation, at the expense
of recall. When relationships in a corpus are not
known, or their number is massive, Open IE is es-
sential for RE.When higher levels of recall are desir-
able for a small set of target relations, traditional RE
is more appropriate. However, in this case, one must
be willing to undertake the cost of acquiring labeled
training data for each relation, either via a computa-
tional procedure such as bootstrapped learning or by
the use of human annotators.
5.3 Hybrid Extraction
In this section, we explore the performance of H-
CRF, an ensemble-based extractor that learns to per-
form RE for a set of known relations based on the
individual behaviors of O-CRF and R1-CRF.
As shown in Table 5, the use of O-CRF as part
of H-CRF, improves precision from 73.9% to 79.2%
with only a slight decrease in recall. Overall, F1
improved from 65.2% to 66.2%.
One disadvantage of a stacking-based hybrid sys-
tem is that labeled training data is still required. In
the future, we would like to explore the development
of hybrid systems that leverage Open IE methods,
34
like O-CRF, to reduce the number of training exam-
ples required per relation.
6 Related Work
TEXTRUNNER, the first Open IE system, is part
of a body of work that reflects a growing inter-
est in avoiding relation-specificity during extrac-
tion. Sekine (2006) developed a paradigm for ?on-
demand information extraction? in order to reduce
the amount of effort involved when porting IE sys-
tems to new domains. Shinyama and Sekine?s ?pre-
emptive? IE system (2006) discovers relationships
from sets of related news articles.
Until recently, most work in RE has been carried
out on a per-relation basis. Typically, RE is framed
as a binary classification problem: Given a sentence
S and a relation R, does S assert R between two
entities in S? Representative approaches include
(Zelenko et al, 2003) and (Bunescu and Mooney,
2005), which use support-vector machines fitted
with language-oriented kernels to classify pairs of
entities. Roth and Yih (2004) also described a
classification-based framework in which they jointly
learn to identify named entities and relations.
Culotta et al (2006) used a CRF for RE, yet
their task differs greatly from open extraction. RE
was performed from biographical text in which the
topic of each document was known. For every en-
tity found in the document, their goal was to pre-
dict what relation, if any, it had relative to the page
topic, from a set of given relations. Under these re-
strictions, RE became an instance of entity labeling,
where the label assigned to an entity (e.g. Father) is
its relation to the topic of the article.
Others have also found the stacking framework to
yield benefits for IE. Freitag (2000) used linear re-
gression to model the relationship between the con-
fidence of several inductive learning algorithms and
the probability that a prediction is correct. Over
three different document collections, the combined
method yielded improvements over the best individ-
ual learner for all but one relation. The efficacy of
ensemble-based methods for extraction was further
investigated by (Sigletos et al, 2005), who experi-
mented with combining the outputs of a rule-based
learner, a Hidden Markov Model and a wrapper-
induction algorithm in five different domains. Of a
variety ensemble-based methods, stacking proved to
consistently outperform the best base-level system,
obtaining more precise results at the cost of some-
what lower recall. (Feldman et al, 2005) demon-
strated that a hybrid extractor composed of a statis-
tical and knowledge-based models outperform either
in isolation.
7 Conclusions and Future Work
Our experiments have demonstrated the promise of
relation-independent extraction using the Open IE
paradigm. We have shown that binary relationships
can be categorized using a compact set of lexico-
syntactic patterns, and presented O-CRF, a CRF-
based Open IE system that can extract different re-
lationships with a precision of 88.3% and a recall of
45.2%4. Open IE is essential when the number of
relationships of interest is massive or unknown.
Traditional IE is more appropriate for targeted ex-
traction when the number of relations of interest is
small and one is willing to incur the cost of acquir-
ing labeled training data. Compared to traditional
IE, the recall of our Open IE system is admittedly
lower. However, in a targeted extraction scenario,
Open IE can still be used to reduce the number of
hand-labeled examples. As Table 4 shows, numer-
ous hand-labeled examples (ranging from 50 for one
relation to over 3,000 for another) are necessary to
match the precision of O-CRF.
In the future, O-CRF?s recall may be improved
by enhancements to its ability to locate the various
ways in which a given relation is expressed. We also
plan to explore the capacity of Open IE to automati-
cally provide labeled training data, when traditional
relation extraction is a more appropriate choice.
Acknowledgments
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, ONR grant N00014-
08-1-0431 as well as gifts from Google, and carried
out at the University of Washington?s Turing Center.
Doug Downey, Stephen Soderland and Dan Weld
provided helpful comments on previous drafts.
4The TEXTRUNNER Open IE system now indexes extrac-
tions found by O-CRF from millions of Web pages, and is lo-
cated at http://www.cs.washington.edu/research/textrunner
35
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In
Procs. of the Fifth ACM International Conference on
Digital Libraries.
M. Banko, M. Cararella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Procs. of IJCAI.
S. Brin. 1998. Extracting Patterns and Relations from the
WorldWideWeb. InWebDBWorkshop at 6th Interna-
tional Conference on Extending Database Technology,
EDBT?98, pages 172?183, Valencia, Spain.
R. Bunescu and R. Mooney. 2005. Subsequence kernels
for relation extraction. In In Procs. of Neural Informa-
tion Processing Systems.
R. Bunescu and R. Mooney. 2007. Learning to extract
relations from the web using minimal supervision. In
Proc. of ACL.
A. Culotta and A. McCallum. 2004. Confidence es-
timation for information extraction. In Procs of
HLT/NAACL.
A. Culotta, A. McCallum, and J. Betz. 2006. Integrat-
ing probabilistic extraction models and data mining
to discover relations and patterns in text. In Procs of
HLT/NAACL, pages 296?303.
P. Domingos. 1996. Unifying instance-based and rule-
based induction. Machine Learning, 24(2):141?168.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
R. Feldman, B. Rosenfeld, and M. Fresko. 2005. Teg - a
hybrid approach to information extraction. Knowledge
and Information Systems, 9(1):1?18.
D. Freitag. 2000. Machine learning for information
extraction in informal domains. Machine Learning,
39(2-3):169?202.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 32(1).
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Procs. of
ICML.
A. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
A. E. Monge and C. P. Elkan. 1996. The field matching
problem: Algorithms and applications. In Procs. of
KDD.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-level Boot-strapping.
In Procs. of AAAI-99, pages 1044?1049.
D. Roth and W. Yih. 2004. A linear progamming formu-
lation for global inference in natural language tasks.
In Procs. of CoNLL.
S. Sekine. 2006. On-demand information extraction. In
Proc. of COLING.
Y. Shinyama and S. Sekine. 2006. Preemptive informa-
tion extraction using unrestricted relation discovery.
In Proc. of the HLT-NAACL.
G. Sigletos, G. Paliouras, C. D. Spyropoulos, andM. Hat-
zopoulos. 2005. Combining infomation extraction
systems using voting and stacked generalization. Jour-
nal of Machine Learning Research, 6:1751,1782.
R. Snow, D. Jurafsky, and A. Ng. 2005. Learning syn-
tactic patterns for automatic hypernym discovery. In
Advances in Neural Information Processing Systems
17. MIT Press.
K.M. Ting and I. H. Witten. 1999. Issues in stacked gen-
eralization. Artificial Intelligence Research, 10:271?
289.
D. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5(2):241?260.
A. Yates and O. Etzioni. 2007. Unsupervised resolu-
tion of objects and relations on the web. In Procs of
NAACL/HLT.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. JMLR, 3:1083?1106.
B. Zenko and S. Dzeroski. 2002. Stacking with an ex-
tended set of meta-level attributes and mlr. In Proc. of
ECML.
36
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 262?270,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Compiling a Massive, Multilingual Dictionary via Probabilistic Inference
Mausam Stephen Soderland Oren Etzioni
Daniel S. Weld Michael Skinner* Jeff Bilmes
University of Washington, Seattle *Google, Seattle
{mausam,soderlan,etzioni,weld,bilmes}@cs.washington.edu mskinner@google.com
Abstract
Can we automatically compose a large set
of Wiktionaries and translation dictionar-
ies to yield a massive, multilingual dic-
tionary whose coverage is substantially
greater than that of any of its constituent
dictionaries?
The composition of multiple translation
dictionaries leads to a transitive inference
problem: if word A translates to word
B which in turn translates to word C,
what is the probability that C is a trans-
lation of A? The paper introduces a
novel algorithm that solves this problem
for 10,000,000 words in more than 1,000
languages. The algorithm yields PANDIC-
TIONARY, a novel multilingual dictionary.
PANDICTIONARY contains more than four
times as many translations than in the
largest Wiktionary at precision 0.90 and
over 200,000,000 pairwise translations in
over 200,000 language pairs at precision
0.8.
1 Introduction and Motivation
In the era of globalization, inter-lingual com-
munication is becoming increasingly important.
Although nearly 7,000 languages are in use to-
day (Gordon, 2005), most language resources are
mono-lingual, or bi-lingual.1 This paper investi-
gates whether Wiktionaries and other translation
dictionaries available over the Web can be auto-
matically composed to yield a massive, multilin-
gual dictionary with superior coverage at compa-
rable precision.
We describe the automatic construction of a
massive multilingual translation dictionary, called
1The English Wiktionary, a lexical resource developed by
volunteers over the Internet is one notable exception that con-
tains translations of English words in about 500 languages.
Figure 1: A fragment of the translation graph for two senses
of the English word ?spring?. Edges labeled ?1? and ?3? are
for spring in the sense of a season, and ?2? and ?4? are for
the flexible coil sense. The graph shows translation entries
from an English dictionary merged with ones from a French
dictionary.
PANDICTIONARY, that could serve as a resource
for translation systems operating over a very
broad set of language pairs. The most immedi-
ate application of PANDICTIONARY is to lexical
translation?the translation of individual words or
simple phrases (e.g., ?sweet potato?). Because
lexical translation does not require aligned cor-
pora as input, it is feasible for a much broader
set of languages than statistical Machine Transla-
tion (SMT). Of course, lexical translation cannot
replace SMT, but it is useful for several applica-
tions including translating search-engine queries,
library classifications, meta-data tags,2 and recent
applications like cross-lingual image search (Et-
zioni et al, 2007), and enhancing multi-lingual
Wikipedias (Adar et al, 2009). Furthermore,
lexical translation is a valuable component in
knowledge-based Machine Translation systems,
e.g., (Bond et al, 2005; Carbonell et al, 2006).
PANDICTIONARY currently contains over 200
million pairwise translations in over 200,000 lan-
guage pairs at precision 0.8. It is constructed from
information harvested from 631 online dictionar-
ies and Wiktionaries. This necessitates match-
2Meta-data tags appear in community Web sites such as
flickr.com and del.icio.us.
262
ing word senses across multiple, independently-
authored dictionaries. Because of the millions of
translations in the dictionaries, a feasible solution
to this sense matching problem has to be scalable;
because sense matches are imperfect and uncer-
tain, the solution has to be probabilistic.
The core contribution of this paper is a princi-
pled method for probabilistic sense matching to in-
fer lexical translations between two languages that
do not share a translation dictionary. For exam-
ple, our algorithm can conclude that Basque word
?udaherri? is a translation of Maori word ?koanga?
in Figure 1. Our contributions are as follows:
1. We describe the design and construction of
PANDICTIONARY?a novel lexical resource
that spans over 200 million pairwise transla-
tions in over 200,000 language pairs at 0.8
precision, a four-fold increase when com-
pared to the union of its input translation dic-
tionaries.
2. We introduce SenseUniformPaths, a scal-
able probabilistic method, based on graph
sampling, for inferring lexical translations,
which finds 3.5 times more inferred transla-
tions at precison 0.9 than the previous best
method.
3. We experimentally contrast PANDIC-
TIONARY with the English Wiktionary and
show that PANDICTIONARY is from 4.5 to
24 times larger depending on the desired
precision.
The remainder of this paper is organized as fol-
lows. Section 2 describes our earlier work on
sense matching (Etzioni et al, 2007). Section 3
describes how the PANDICTIONARY builds on and
improves on their approach. Section 4 reports on
our experimental results. Section 5 considers re-
lated work on lexical translation. The paper con-
cludes in Section 6 with directions for future work.
2 Building a Translation Graph
In previous work (Etzioni et al, 2007) we intro-
duced an approach to sense matching that is based
on translation graphs (see Figure 1 for an exam-
ple). Each vertex v ? V in the graph is an or-
dered pair (w, l) where w is a word in a language
l. Undirected edges in the graph denote transla-
tions between words: an edge e ? E between (w1,
l1) and (w2, l2) represents the belief that w1 and
w2 share at least one word sense.
Construction: The Web hosts a large num-
ber of bilingual dictionaries in different languages
and several Wiktionaries. Bilingual dictionaries
translate words from one language to another, of-
ten without distinguishing the intended sense. For
example, an Indonesian-English dictionary gives
?light? as a translation of the Indonesian word ?en-
teng?, but does not indicate whether this means il-
lumination, light weight, light color, or the action
of lighting fire.
The Wiktionaries (wiktionary.org) are sense-
distinguished, multilingual dictionaries created by
volunteers collaborating over the Web. A transla-
tion graph is constructed by locating these dictio-
naries, parsing them into a common XML format,
and adding the nodes and edges to the graph.
Figure 1 shows a fragment of a translation
graph, which was constructed from two sets of
translations for the word ?spring? from an English
Wiktionary, and two corresponding entries from
a French Wiktionary for ?printemps? (spring sea-
son) and ?ressort? (flexible spring). Translations of
the season ?spring? have edges labeled with sense
ID=1, the flexible coil sense has ID=2, translations
of ?printemps? have ID=3, and so forth.3
For clarity, we show only a few of the actual
vertices and edges; e.g., the figure doesn?t show
the edge (ID=1) between ?udaherri? and ?primav-
era?.
Inference: In our previous system we had
a simple inference procedure over translation
graphs, called TRANSGRAPH, to find translations
beyond those provided by any source dictionary.
TRANSGRAPH searched for paths in the graph be-
tween two vertices and estimated the probability
that the path maintains the same word sense along
all edges in the path, even when the edges come
from different dictionaries. For example, there are
several paths between ?udaherri? and ?koanga? in
Figure 1, but all shift from sense ID 1 to 3. The
probability that the two words are translations is
equivalent to the probability that IDs 1 and 3 rep-
resent the same sense.
TRANSGRAPH used two formulae to estimate
these probabilities. One formula estimates the
probability that two multi-lingual dictionary en-
tries represent the same word sense, based on the
proportion of overlapping translations for the two
entries. For example, most of the translations of
3Sense-distinguished multi-lingual entries give rise to
cliques all of which share a common sense ID.
263
French ?printemps? are also translations of the sea-
son sense of ?spring?. A second formula is based
on triangles in the graph (useful for bilingual dic-
tionaries): a clique of 3 nodes with an edge be-
tween each pair of nodes. In such cases, there is
a high probability that all 3 nodes share a word
sense.
Critique: While TRANSGRAPH was the first
to present a scalable inference method for lexical
translation, it suffers from several drawbacks. Its
formulae operate only on local information: pairs
of senses that are adjacent in the graph or triangles.
It does not incorporate evidence from longer paths
when an explicit triangle is not present. Moreover,
the probabilities from different paths are com-
bined conservatively (either taking the max over
all paths, or using ?noisy or? on paths that are
completely disjoint, except end points), thus lead-
ing to suboptimal precision/recall.
In response to this critique, the next section
presents an inference algorithm, called SenseUni-
formPaths (SP), with substantially improved recall
at equivalent precision.
3 Translation Inference Algorithms
In essence, inference over a translation graph
amounts to transitive sense matching: if word A
translates to word B, which translates in turn to
word C, what is the probability that C is a trans-
lation of A? If B is polysemous then C may not
share a sense with A. For example, in Figure 2(a)
if A is the French word ?ressort? (the flexible-
coil sense of spring) and B is the English word
?spring?, then Slovenian word ?vzmet? may or may
not be a correct translation of ?ressort? depending
on whether the edge (B,C) denotes the flexible-
coil sense of spring, the season sense, or another
sense. Indeed, given only the knowledge of the
path A ? B ? C we cannot claim anything with
certainty regarding A to C.
However, if A, B, and C are on a circuit that
starts at A, passes through B and C and re-
turns to A, there is a high probability that all
nodes on that circuit share a common word sense,
given certain restrictions that we enumerate later.
Where TRANSGRAPH used evidence from circuits
of length 3, we extend this to paths of arbitrary
lengths.
To see how this works, let us begin with the sim-
plest circuit, a triangle of three nodes as shown in
Figure 2(b). We can be quite certain that ?vzmet?
shares the sense of coil with both ?spring? and
?ressort?. Our reasoning is as follows: even
though both ?ressort? and ?spring? are polysemous
they share only one sense. For a triangle to form
we have two choices ? (1) either ?vzmet? means
spring coil, or (2) ?vzmet? means both the spring
season and jurisdiction, but not spring coil. The
latter is possible but such a coincidence is very un-
likely, which is why a triangle is strong evidence
for the three words to share a sense.
As an example of longer paths, our inference
algorithms can conclude that in Figure 2(c), both
?molla? and ?vzmet? have the sense coil, even
though no explicit triangle is present. To show
this, let us define a translation circuit as follows:
Definition 1 A translation circuit from v?1 with
sense s? is a cycle that starts and ends at v?1 with
no repeated vertices (other than v?1 at end points).
Moreover, the path includes an edge between v?1
and another vertex v?2 that also has sense s
?.
All vertices on a translation circuit are mutual
translations with high probability, as in Figure
2(c). The edge from ?spring? indicates that ?vzmet?
means either coil or season, while the edge from
?ressort? indicates that ?molla? means either coil
or jurisdiction. The edge from ?vzmet? to ?molla?
indicates that they share a sense, which will hap-
pen if all nodes share the sense season or if either
?vzmet? has the unlikely combination of coil and
jurisdiction (or ?molla? has coil and season).
We also develop a mathematical model of
sense-assignment to words that lets us formally
prove these insights. For more details on the the-
ory please refer to our extended version. This pa-
per reports on our novel algorithm and experimen-
tal results.
These insights suggest a basic version of our al-
gorithm: ?given two vertices, v?1 and v
?
2 , that share
a sense (say s?) compute all translation circuits
from v?1 in the sense s
?; mark all vertices in the
circuits as translations of the sense s??.
To implement this algorithm we need to decide
whether a vertex lies on a translation circuit, which
is trickier than it seems. Notice that knowing
that v is connected independently to v?1 and v
?
2
doesn?t imply that there exists a translation circuit
through v, because both paths may go through a
common node, thus violating of the definition of
translation circuit. For example, in Figure 2(d) the
Catalan word ?ploma? has paths to both spring and
ressort, but there is no translation circuit through
264
spring
English
ressort
French
vzmet
Slovenian
spring
English
ressort
French
vzmet
Slovenian
spring
English
vzmet
Slovenian
ressort
French
molla
Italian
spring
English
ressort
French
ploma
Catalan
Feder
German
???? 
Russian
spring
English
ressort
French
fj?der
Swedish
penna
Italian
Feder
German
(a)                         (b)                                   (c)                                (d)                     (e)
season
coil
jurisdiction
coil
s* s*
s* s*
s*
? ? ?
? ?
feather
coil
?
?
Figure 2: Snippets of translation graphs illustrating various inference scenarios. The nodes in question mark represent the
nodes in focus for each illustration. For all cases we are trying to infer translations of the flexible coil sense of spring.
it. Hence, it will not be considered a transla-
tion. This example also illustrates potential errors
avoided by our algorithm ? here, German word
?Feder? mean feather and spring coil, but ?ploma?
means feather and not the coil.
An exhaustive search to find translation circuits
would be too slow, so we approximate the solution
by a random walk scheme. We start the random
walk from v?1 (or v
?
2) and choose random edges
without repeating any vertices in the current path.
At each step we check if the current node has an
edge to v?2 (or v
?
1). If it does, then all the ver-
tices in the current path form a translation circuit
and, thus, are valid translations. We repeat this
random walk many times and keep marking the
nodes. In our experiments for each inference task
we performed a total of 2,000 random walks (NR
in pseudo-code) of max circuit length 7. We chose
these parameters based on a development set of 50
inference tasks.
Our first experiments with this basic algorithm
resulted in a much higher recall than TRANS-
GRAPH, albeit, at a significantly lower precision.
A closer examination of the results revealed two
sources of error ? (1) errors in source dictionary
data, and (2) correlated sense shifts in translation
circuits. Below we add two new features to our
algorithm to deal with each of these error sources,
respectively.
3.1 Errors in Source Dictionaries
In practice, source dictionaries contain mistakes
and errors occur in processing the dictionaries to
create the translation graph. Thus, existence of a
single translation circuit is only limited evidence
for a vertex as a translation. We wish to exploit
the insight that more translation circuits constitute
stronger evidence. However, the different circuits
may share some edges, and thus the evidence can-
not be simply the number of translation circuits.
We model the errors in dictionaries by assigning
a probability less than 1.0 to each edge4 (pe in the
4In our experiments we used a flat value of 0.6, chosen by
pseudo-code). We assume that the probability of
an edge being erroneous is independent of the rest
of the graph. Thus, a translation graph with pos-
sible data errors converts into a distribution over
accurate translation graphs.
Under this distribution, we can use the proba-
bility of existence of a translation circuit through a
vertex as the probability that the vertex is a trans-
lation. This value captures our insights, since a
larger number of translation circuits gives a higher
probability value.
We sample different graph topologies from our
given distribution. Some translation circuits will
exist in some of the sampled graphs, but not in
others. This, in turn, means that a given vertex v
will only be on a circuit for a fraction of the sam-
pled graphs. We take the proportion of samples in
which v is on a circuit to be the probability that v
is in the translation set. We refer to this algorithm
as Unpruned SenseUniformPaths (uSP).
3.2 Avoiding Correlated Sense-shifts
The second source of errors are circuits that in-
clude a pair of nodes sharing the same polysemy,
i.e., having the same pair of senses. A circuit
might maintain sense s? until it reaches a node that
has both s? and a distinct si. The next edge may
lead to a node with si, but not s?, causing an ex-
traction error. The path later shifts back to sense
s? at a second node that also has s? and si. An ex-
ample for this is illustrated in Figure 2(e), where
both the German and Swedish words mean feather
and spring coil. Here, Italian ?penna? means only
the feather and not the coil.
Two nodes that share the same two senses oc-
cur frequently in practice. For example, many
languages use the same word for ?heart? (the or-
gan) and center; similarly, it is common for lan-
guages to use the same word for ?silver?, the metal
and the color. These correlations stem from com-
parameter tuning on a development set of 50 inference tasks.
In future we can use different values for different dictionaries
based on our confidence in their accuracy.
265
Figure 3: The set {B, C} has a shared ambiguity - each
node has both sense 1 (from the lower clique) and sense 2
(from the upper clique). A circuit that contains two nodes
from the same ambiguity set with an intervening node not in
that set is likely to create translation errors.
mon metaphor and the shared evolutionary roots
of some languages.
We are able to avoid circuits with this type of
correlated sense-shift by automatically identifying
ambiguity sets, sets of nodes known to share mul-
tiple senses. For instance, in Figure 2(e) ?Feder?
and ?fj?der? form an ambiguity set (shown within
dashed lines), as they both mean feather and coil.
Definition 2 An ambiguity set A is a set of ver-
tices that all share the same two senses. I.e.,
?s1, s2, with s1 6= s2 s.t. ?v ? A, sense(v, s1)?
sense(v, s2), where sense(v, s) denotes that v has
sense s.
To increase the precision of our algorithm we
prune the circuits that contain two nodes in the
same ambiguity set and also have one or more in-
tervening nodes that are not in the ambiguity set.
There is a strong likelihood that the intervening
nodes will represent a translation error.
Ambiguity sets can be detected from the graph
topology as follows. Each clique in the graph rep-
resents a set of vertices that share a common word
sense. When two cliques intersect in two or more
vertices, the intersecting vertices share the word
sense of both cliques. This may either mean that
both cliques represent the same word sense, or that
the intersecting vertices form an ambiguity set. A
large overlap between two cliques makes the for-
mer case more likely; a small overlap makes it
more likely that we have found an ambiguity set.
Figure 3 illustrates one such computation.
All nodes of the clique V1, V2, A,B,C,D share
a word sense, and all nodes of the clique
B,C,E, F,G,H also share a word sense. The set
{B,C} has nodes that have both senses, forming
an ambiguity set. We denote the set of ambiguity
sets by A in the pseudo-code.
Having identified these ambiguity sets, we mod-
ify our random walk scheme by keeping track of
whether we are entering or leaving an ambiguity
set. We prune away all paths that enter the same
ambiguity set twice. We name the resulting algo-
rithm SenseUniformPaths (SP), summarized at a
high level in Algorithm 1.
Comparing Inference Algorithms Our evalua-
tion demonstrated that SP outperforms uSP. Both
these algorithms have significantly higher recall
than TRANSGRAPH algorithm. The detailed re-
sults are presented in Section 4.2. We choose SP
as our inference algorithm for all further research,
in particular to create PANDICTIONARY.
3.3 Compiling PanDictionary
Our goal is to automatically compile PANDIC-
TIONARY, a sense-distinguished lexical transla-
tion resource, where each entry is a distinct word
sense. Associated with each word sense is a list of
translations in multiple languages.
We use Wiktionary senses as the base senses
for PANDICTIONARY. Recall that SP requires two
nodes (v?1 and v
?
2) for inference. We use the Wik-
tionary source word as v?1 and automatically pick
the second word from the set of Wiktionary trans-
lations of that sense by choosing a word that is
well connected, and, which does not appear in
other senses of v?1 (i.e., is expected to share only
one sense with v?1).
We first run SenseUniformPaths to expand the
approximately 50,000 senses in the English Wik-
tionary. We further expand any senses from the
other Wiktionaries that are not yet covered by
PANDICTIONARY, and add these to PANDIC-
TIONARY. This results in the creation of the
world?s largest multilingual, sense-distinguished
translation resource, PANDICTIONARY. It con-
tains a little over 80,000 senses. Its construction
takes about three weeks on a 3.4 GHz processor
with a 2 GB memory.
Algorithm 1 S.P.(G, v?1, v
?
2,A)
1: parameters NG: no. of graph samples, NR: no. of ran-
dom walks, pe: prob. of sampling an edge
2: createNG versions ofG by sampling each edge indepen-
dently with probability pe
3: for all i = 1..NG do
4: for all vertices v : rp[v][i] = 0
5: perform NR random walks starting at v?1 (or v
?
2 ) and
pruning any walk that enters (or exits) an ambiguity
set in A twice. All walks that connect to v?2 (or v
?
1 )
form a translation circuit.
6: for all vertices v do
7: if(v is on a translation circuit) rp[v][i] = 1
8: return
?
i
rp[v][i]
NG
as the prob. that v is a translation
266
4 Empirical Evaluation
In our experiments we investigate three key ques-
tions: (1) which of the three algorithms (TG, uSP
and SP) is superior for translation inference (Sec-
tion 4.2)? (2) how does the coverage of PANDIC-
TIONARY compare with the largest existing mul-
tilingual dictionary, the English Wiktionary (Sec-
tion 4.3)? (3) what is the benefit of inference over
the mere aggregation of 631 dictionaries (Section
4.4)? Additionally, we evaluate the inference algo-
rithm on two other dimensions ? variation with the
degree of polysemy of source word, and variation
with original size of the seed translation set.
4.1 Experimental Methodology
Ideally, we would like to evaluate a random sam-
ple of the more than 1,000 languages represented
in PANDICTIONARY.5 However, a high-quality
evaluation of translation between two languages
requires a person who is fluent in both languages.
Such people are hard to find and may not even
exist for many language pairs (e.g., Basque and
Maori). Thus, our evaluation was guided by our
ability to recruit volunteer evaluators. Since we
are based in an English speaking country we were
able to recruit local volunteers who are fluent in
a range of languages and language families, and
who are also bilingual in English.6
The experiments in Sections 4.2 and 4.3 test
whether translations in a PANDICTIONARY have
accurate word senses. We provided our evalua-
tors with a random sample of translations into their
native language. For each translation we showed
the English source word and gloss of the intended
sense. For example, a Dutch evaluator was shown
the sense ?free (not imprisoned)? together with the
Dutch word ?loslopende?. The instructions were
to mark a word as correct if it could be used to ex-
press the intended sense in a sentence in their na-
tive language. For experiments in Section 4.4 we
tested precision of pairwise translations, by having
informants in several pairs of languages discuss
whether the words in their respective languages
can be used for the same sense.
We use the tags of correct or incorrect to com-
pute the precision: the percentage of correct trans-
5The distribution of words in PANDICTIONARY is highly
non-uniform ranging from 182,988 words in English to 6,154
words in Luxembourgish and 189 words in Tuvalu.
6The languages used was based on the availability of na-
tive speakers. This varied between the different experiments,
which were conducted at different times.
Figure 4: The SenseUniformPaths algorithm (SP) more
than doubles the number of correct translations at precision
0.95, compared to a baseline of translations that can be found
without inference.
lations divided by correct plus incorrect transla-
tions. We then order the translations by probabil-
ity and compute the precision at various probabil-
ity thresholds.
4.2 Comparing Inference Algorithms
Our first evaluation compares our SenseUniform-
Paths (SP) algorithm (before and after pruning)
with TRANSGRAPH on both precision and num-
ber of translations.
To carry out this comparison, we randomly sam-
pled 1,000 senses from English Wiktionary and
ran the three algorithms over them. We evalu-
ated the results on 7 languages ? Chinese, Danish,
German, Hindi, Japanese, Russian, and Turkish.
Each informant tagged 60 random translations in-
ferred by each algorithm, which resulted in 360-
400 tags per algorithm7. The precision over these
was taken as a surrogate for the precision across
all the senses.
We compare the number of translations for each
algorithm at comparable precisions. The baseline
is the set of translations (for these 1000 senses)
found in the source dictionaries without inference,
which has a precision 0.95 (as evaluated by our
informants).8
Our results are shown in Figure 4. At this high
precision, SP more than doubles the number of
baseline translations, finding 5 times as many in-
ferred translations (in black) as TG.
Indeed, both uSP and SP massively outperform
TG. SP is consistently better than uSP, since it
performs better for polysemous words, due to its
pruning based on ambiguity sets. We conclude
7Some translations were marked as ?Don?t know?.
8Our informants tended to underestimate precision, often
marking correct translations in minor senses of a word as in-
correct.
267
0.5
0.6
0.7
0.8
0.9
1
0.0 4.0 8.0 12.0 16.0
Pr
ec
is
io
n
Translations in Millions
PanDictionary
English Wiktionary
Figure 5: Precision vs. coverage curve for PANDIC-
TIONARY. It quadruples the size of the English Wiktionary at
precision 0.90, is more than 8 times larger at precision 0.85
and is almost 24 times the size at precision 0.7.
that SP is the best inference algorithm and employ
it for PANDICTIONARY construction.
4.3 Comparison with English Wiktionary
We now compare the coverage of PANDIC-
TIONARY with the English Wiktionary at varying
levels of precision. The English Wiktionary is the
largest Wiktionary with a total of 403,413 transla-
tions. It is also more reliable than some other Wik-
tionaries in making word sense distinctions. In this
study we use only the subset of PANDICTIONARY
that was computed starting from the English Wik-
tionary senses. Thus, this subsection under-reports
PANDICTIONARY?s coverage.
To evaluate a huge resource such as PANDIC-
TIONARY we recruited native speakers of 14 lan-
guages ? Arabic, Bulgarian, Danish, Dutch, Ger-
man, Hebrew, Hindi, Indonesian, Japanese, Ko-
rean, Spanish, Turkish, Urdu, and Vietnamese. We
randomly sampled 200 translations per language,
which resulted in about 2,500 tags. Figure 5
shows the total number of translations in PANDIC-
TIONARY in senses from the English Wiktionary.
At precision 0.90, PANDICTIONARY has 1.8 mil-
lion translations, 4.5 times as many as the English
Wiktionary.
We also compare the coverage of PANDIC-
TIONARY with that of the English Wiktionary in
terms of languages covered. Table 1 reports, for
each resource, the number of languages that have
a minimum number of distinct words in the re-
source. PANDICTIONARY has 1.4 times as many
languages with at least 1,000 translations at pre-
cision 0.90 and more than twice at precision 0.7.
These observations reaffirm our faith in the pan-
lingual nature of the resource.
PANDICTIONARY?s ability to expand the lists
of translations provided by the EnglishWiktionary
is most pronounced for senses with a small num-
0.75
0.8
0.85
0.9
0.95
1 2 3,4 >4
Pre
cis
ion
Avg precision 0.90
Avg precision 0.85
Polysemy of the English source word
3-4
Figure 6: Variation of precision with the degree of poly-
semy of the source English word. The precision decreases as
polysemy increases, still maintaining reasonably high values.
ber of translations. For example, at precision 0.90,
senses that originally had 3 to 6 translations are in-
creased 5.3 times in size. The increase is 2.2 times
when the original sense size is greater than 20.
For closer analysis we divided the English
source words (v?1) into different bins based on the
number of senses that English Wiktionary lists for
them. Figure 6 plots the variation of precision with
this degree of polysemy. We find that translation
quality decreases as degree of polysemy increases,
but this decline is gradual, which suggests that SP
algorithm is able to hold its ground well in difficult
inference tasks.
4.4 Comparison with All Source Dictionaries
We have shown that PANDICTIONARY has much
broader coverage than the English Wiktionary, but
how much of this increase is due to the inference
algorithm versus the mere aggregation of hundreds
of translation dictionaries in PANDICTIONARY?
Since most bilingual dictionaries are not sense-
distinguished, we ignore the word senses and
count the number of distinct (word1, word2) trans-
lation pairs.
We evaluated the precision of word-word trans-
lations by a collaborative tagging scheme, with
two native speakers of different languages, who
are both bi-lingual in English. For each sug-
gested translation they discussed the various
senses of words in their respective languages
and tag a translation correct if they found some
sense that is shared by both words. For this
study we tagged 7 language pairs: Hindi-Hebrew,
# languages with distinct words
? 1000 ? 100 ? 1
English Wiktionary 49 107 505
PanDictionary (0.90) 67 146 608
PanDictionary (0.85) 75 175 794
PanDictionary (0.70) 107 607 1066
Table 1: PANDICTIONARY covers substantially more lan-
guages than the English Wiktionary.
268
050
100
150
200
250
EW 631D PD(0.9) PD(0.85) PD(0.8)
Inferred transl. Direct transl.
Tra
nsl
ati
on
s(i
n m
illio
ns)
Figure 7: The number of distinct word-word translation
pairs from PANDICTIONARY is several times higher than the
number of translation pairs in the English Wiktionary (EW)
or in all 631 source dictionaries combined (631 D). A major-
ity of PANDICTIONARY translations are inferred by combin-
ing entries from multiple dictionaries.
Japanese-Russian, Chinese-Turkish, Japanese-
German, Chinese-Russian, Bengali-German, and
Hindi-Turkish.
Figure 7 compares the number of word-word
translation pairs in the English Wiktionary (EW),
in all 631 source dictionaries (631 D), and in PAN-
DICTIONARY at precisions 0.90, 0.85, and 0.80.
PANDICTIONARY increases the number of word-
word translations by 73% over the source dictio-
nary translations at precision 0.90 and increases it
by 2.7 times at precision 0.85. PANDICTIONARY
also adds value by identifying the word sense of
the translation, which is not given in most of the
source dictionaries.
5 Related Work
Because we are considering a relatively new prob-
lem (automatically building a panlingual transla-
tion resource) there is little work that is directly re-
lated to our own. The closest research is our previ-
ous work on TRANSGRAPH algorithm (Etzioni et
al., 2007). Our current algorithm outperforms the
previous state of the art by 3.5 times at precision
0.9 (see Figure 4). Moreover, we compile this in a
dictionary format, thus considerably reducing the
response time compared to TRANSGRAPH, which
performed inference at query time.
There has been considerable research on meth-
ods to acquire translation lexicons from either
MRDs (Neff and McCord, 1990; Helmreich et
al., 1993; Copestake et al, 1994) or from par-
allel text (Gale and Church, 1991; Fung, 1995;
Melamed, 1997; Franz et al, 2001), but this has
generally been limited to a small number of lan-
guages. Manually engineered dictionaries such as
EuroWordNet (Vossen, 1998) are also limited to
a relatively small set of languages. There is some
recent work on compiling dictionaries from mono-
lingual corpora, which may scale to several lan-
guage pairs in future (Haghighi et al, 2008).
Little work has been done in combining mul-
tiple dictionaries in a way that maintains word
senses across dictionaries. Gollins and Sanderson
(2001) explored using triangulation between alter-
nate pivot languages in cross-lingual information
retrieval. Their triangulation essentially mixes
together circuits for all word senses, hence, is un-
able to achieve high precision.
Dyvik?s ?semantic mirrors? uses translation
paths to tease apart distinct word senses from
inputs that are not sense-distinguished (Dyvik,
2004). However, its expensive processing and
reliance on parallel corpora would not scale to
large numbers of languages. Earlier (Knight and
Luk, 1994) discovered senses of Spanish words by
matching several English translations to a Word-
Net synset. This approach applies only to specific
kinds of bilingual dictionaries, and also requires a
taxonomy of synsets in the target language.
Random walks, graph sampling and Monte
Carlo simulations are popular in literature, though,
to our knowledge, none have applied these to our
specific problems (Henzinger et al, 1999; Andrieu
et al, 2003; Karger, 1999).
6 Conclusions
We have described the automatic construction of
a unique multilingual translation resource, called
PANDICTIONARY, by performing probabilistic in-
ference over the translation graph. Overall, the
construction process consists of large scale in-
formation extraction over the Web (parsing dic-
tionaries), combining it into a single resource (a
translation graph), and then performing automated
reasoning over the graph (SenseUniformPaths) to
yield a much more extensive and useful knowl-
edge base.
We have shown that PANDICTIONARY has
more coverage than any other existing bilingual
or multilingual dictionary. Even at the high preci-
sion of 0.90, PANDICTIONARY more than quadru-
ples the size of the English Wiktionary, the largest
available multilingual resource today.
We plan to make PANDICTIONARY available
to the research community, and also to the Wik-
tionary community in an effort to bolster their ef-
forts. PANDICTIONARY entries can suggest new
translations for volunteers to add to Wiktionary
entries, particularly if combined with an intelli-
gent editing tool (e.g., (Hoffmann et al, 2009)).
269
Acknowledgments
This research was supported by a gift from the
Utilika Foundation to the Turing Center at Uni-
versity of Washington. We acknowledge Paul
Beame, Nilesh Dalvi, Pedro Domingos, Rohit
Khandekar, Daniel Lowd, Parag, Jonathan Pool,
Hoifung Poon, Vibhor Rastogi, Gyanit Singh for
fruitful discussions and insightful comments on
the research. We thank the language experts who
donated their time and language expertise to eval-
uate our systems. We also thank the anynomous
reviewers of the previous drafts of this paper for
their valuable suggestions in improving the evalu-
ation and presentation.
References
E. Adar, M. Skinner, and D. Weld. 2009. Information
arbitrage in multi-lingual Wikipedia. In Procs. of
Web Search and Data Mining(WSDM 2009).
C. Andrieu, N. De Freitas, A. Doucet, and M. Jor-
dan. 2003. An Introduction to MCMC for Machine
Learning. Machine Learning, 50:5?43.
F. Bond, S. Oepen, M. Siegel, A. Copestake, and
D D. Flickinger. 2005. Open source machine trans-
lation with DELPH-IN. In Open-Source Machine
Translation Workshop at MT Summit X.
J. Carbonell, S. Klein, D. Miller, M. Steinbaum,
T. Grassiany, and J. Frey. 2006. Context-based ma-
chine translation. In AMTA.
A. Copestake, T. Briscoe, P. Vossen, A. Ageno,
I. Castellon, F. Ribas, G. Rigau, H. Rodriquez, and
A. Samiotou. 1994. Acquisition of lexical trans-
lation relations from MRDs. Machine Translation,
3(3?4):183?219.
H. Dyvik. 2004. Translation as semantic mirrors: from
parallel corpus to WordNet. Language and Comput-
ers, 49(1):311?326.
O. Etzioni, K. Reiter, S. Soderland, and M. Sammer.
2007. Lexical translation with application to image
search on the Web. In Machine Translation Summit
XI.
M. Franz, S. McCarly, and W. Zhu. 2001. English-
Chinese information retrieval at IBM. In Proceed-
ings of TREC 2001.
P. Fung. 1995. A pattern matching method for finding
noun and proper noun translations from noisy paral-
lel corpora. In Proceedings of ACL-1995.
W. Gale and K.W. Church. 1991. A Program for
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of ACL-1991.
T. Gollins and M. Sanderson. 2001. Improving cross
language retrieval with triangulated translation. In
SIGIR.
Raymond G. Gordon, Jr., editor. 2005. Ethnologue:
Languages of the World (Fifteenth Edition). SIL In-
ternational.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In ACL.
S. Helmreich, L. Guthrie, and Y. Wilks. 1993. The
use of machine readable dictionaries in the Pangloss
project. In AAAI Spring Symposium on Building
Lexicons for Machine Translation.
Monika R. Henzinger, Allan Heydon, Michael Mitzen-
macher, and Marc Najork. 1999. Measuring index
quality using random walks on the web. In WWW.
R. Hoffmann, S. Amershi, K. Patel, F. Wu, J. Foga-
rty, and D. S. Weld. 2009. Amplifying commu-
nity content creation with mixed-initiative informa-
tion extraction. In ACM SIGCHI (CHI2009).
D. R. Karger. 1999. A randomized fully polynomial
approximation scheme for the all-terminal network
reliability problem. SIAM Journal of Computation,
29(2):492?514.
K. Knight and S. Luk. 1994. Building a large-scale
knowledge base for machine translation. In AAAI.
I.D. Melamed. 1997. A Word-to-Word Model of
Translational Equivalence. In Proceedings of ACL-
1997 and EACL-1997, pages 490?497.
M. Neff and M. McCord. 1990. Acquiring lexical data
from machine-readable dictionary resources for ma-
chine translation. In 3rd Intl Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion of Natural Language.
P. Vossen, editor. 1998. EuroWordNet: A multilingual
database with lexical semantic networds. Kluwer
Academic Publishers.
270
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 193?196,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Rose is a Roos is a Ruusu: Querying Translations for Web Image Search
Janara Christensen Mausam Oren Etzioni
Turing Center
Dept. of Computer Science and Engineering
University of Washington, Seattle, WA 98105 USA
{janara, mausam, etzioni}@cs.washington.edu
Abstract
We query Web Image search engines with
words (e.g., spring) but need images that
correspond to particular senses of the word
(e.g., flexible coil). Querying with poly-
semous words often yields unsatisfactory
results from engines such as Google Im-
ages. We build an image search engine,
IDIOM, which improves the quality of re-
turned images by focusing search on the
desired sense. Our algorithm, instead of
searching for the original query, searches
for multiple, automatically chosen trans-
lations of the sense in several languages.
Experimental results show that IDIOM out-
performs Google Images and other com-
peting algorithms returning 22% more rel-
evant images.
1 Introduction
One out of five Web searches is an image search
(Basu, 2009). A large subset of these searches
is subjective in nature, where the user is looking
for different images for a single concept (Linsley,
2009). However, it is a common user experience
that the images returned are not relevant to the in-
tended concept. Typical reasons include (1) exis-
tence of homographs (other words that share the
same spelling, possibly in another language), and
(2) polysemy, several meanings of the query word,
which get merged in the results.
For example, the English word ?spring? has sev-
eral senses ? (1) the season, (2) the water body, (3)
spring coil, and (4) to jump. Ten out of the first fif-
teen Google images for spring relate to the season
sense, three to water body, one to coil and none to
the jumping sense. Simple modifications to query
do not always work. Searching for spring water
results in many images of bottles of spring water
and searching for spring jump returns only three
images (out of fifteen) of someone jumping.
Polysemous words are common in English. It
is estimated that average polysemy of English is
more than 2 and average polysemy of common
English words is much higher (around 4). Thus,
it is not surprising that polysemy presents a signif-
icant limitation in the context of Web Search. This
is especially pronounced for image search where
query modification by adding related words may
not help, since, even though the new words might
be present on the page, they may not be all associ-
ated with an image.
Recently Etzioni et al (2007) introduced PAN-
IMAGES, a novel approach to image search, which
presents the user with a set of translations. E.g., it
returns 38 translations for the coil sense of spring.
The user can query one or more translations to get
the relevant images. However, this method puts
the onus of choosing a translation on the user. A
typical user is unaware of most properties of lan-
guages and has no idea whether a translation will
make a good query. This results in an added bur-
den on the user to try different translations before
finding the one that returns the relevant images.
Our novel system, IDIOM, removes this addi-
tional burden. Given a desired sense it automati-
cally picks the good translations, searches for as-
sociated images and presents the final images to
the user. For example, it automatically queries the
French ressort when looking for images of spring
coil. We make the following contributions:
? We automatically learn a predictor for "good"
translations to query given a desired sense. A
good translation is one that is monosemous
and is in a major language, i.e., is expected to
yield a large number of images.
? Given a sense we run our predictor on all its
translations to shortlist a set of three transla-
tions to query.
? We evaluate our predictor by comparing the
images that its shortlists return against the
193
images that several competing methods re-
turn. Our evaluation demonstrates that ID-
IOM returns at least one good image for 35%
more senses (than closest competitor) and
overall returns 22% better images.
2 Background
IDIOM makes heavy use of a sense disambiguated,
vastly multilingual dictionary called PANDIC-
TIONARY (Mausam et al, 2009). PANDIC-
TIONARY is automatically constructed by prob-
abilistic inference over a graph of translations,
which is compiled from a large number of multi-
lingual and bilingual dictionaries. For each sense
PANDICTIONARY provides us with a set of trans-
lations in several languages. Since it is gener-
ated by inference, some of the asserted transla-
tions may be incorrect ? it additionally associates
a probability score with each translation. For
our work we choose a probability threshold such
that the overall precision of the dictionary is 0.9
(evaluated based on a random sample). PANDIC-
TIONARY has about 80,000 senses and about 1.8
million translations at precision 0.9.
We use Google Image Search as our underlying
image search engine, but our methods are indepen-
dent of the underlying search engine used.
3 The IDIOM Algorithm
At the highest level IDIOM operates in three main
steps: (1) Given a new query q it looks up its vari-
ous senses in PANDICTIONARY. It displays these
senses and asks the user to select the intended
sense, s
q
. (2) It runs Algorithm 1 to shortlist three
translations of s
q
that are expected to return high
quality images. (3) It queries Google Images us-
ing the three shortlisted translations and displays
the images. In this fashion IDIOM searches for
images that are relevant to the intended concept
as opposed to using a possibly ambiguous query.
The key technical component is the second step
? shortlisting the translations. We first use PAN-
DICTIONARY to acquire a set of high probability
translations of s
q
. We run each of these transla-
tions through a learned classifier, which predicts
whether it will make a good query, i.e., whether
we can expect images relevant to this sense if
queried using this translation. The classifier ad-
ditionally outputs a confidence score, which we
use to rank the various translations. We pick the
top three translations, as long as they are above a
minimum confidence score, and return those as the
shortlisted queries. Algorithm 1 describes this as
a pseudo-code.
Algorithm 1 findGoodTranslationsToQuery(s
q
)
1: translations = translations of s
q
in PANDICTIONARY
2: for all w ? translations do
3: pd = getPanDictionaryFeatures(w, s
q
)
4: g = getGoogleFeatures(w, s
q
)
5: conf[w] = confidence in Learner.classify(pd, g)
6: sort all words w in decreasing order of conf scores
7: return top three w from the sorted list
3.1 Features for Classifier
What makes a translation w good to query? A
desired translation is one that (1) is in a high-
coverage language, so that the number of images
returned is large, (2) monosemously expresses the
intended sense s
q
, or at least has this sense as
its dominant sense, and (3) does not have homo-
graphs in other languages. Such a translation is
expected to yield images relevant to only the in-
tended sense. We construct several features that
provide us evidence for these desired characteris-
tics. Our features are automatically extracted from
PANDICTIONARY and Google.
For the first criterion we restrict the transla-
tions to a set of high-coverage languages includ-
ing English, French, German, Spanish, Chinese,
Japanese, Arabic, Russian, Korean, Italian, and
Portuguese. Additionally, we include the lan-
guage as well as number of documents returned by
Google search of w as features for the classifier.
To detect if w is monosemous we add a feature
reflecting the degree of polysemy of w: the num-
ber of PANDICTIONARY senses thatw belongs to.
The higher this number the more polysemous w
is expected to be. We also include the number of
languages that have w in their vocabulary, thus,
adding a feature for the degree of homography.
PANDICTIONARY is arranged such that each
sense has an English source word. If the source
word is part of many senses but s
q
is much more
popular than others or s
q
is ordered before the
other senses then we can expect s
q
to be the dom-
inant sense for this word. We include features like
size of the sense and order of the sense.
Part of speech of s
q
is another feature. Finally
we also add the probability score that w is a trans-
lation of s
q
in our feature set.
3.2 Training the Classifier
To train our classifier we used Weka (Witten and
Frank, 2005) on a hand labeled dataset of 767 ran-
194
0 100 200 300 4000.
00
0.10
0.20
Number of Good Images Returned
Prec
ision IDIOMSWSW+GRSW+R
IDIOM SW SW+G SW+R R
Perc
enta
ge C
orrec
t
0
20
40
60
IDIOM SW SW+G SW+R R
Perc
enta
ge C
orrec
t
0
20
40
60
Figure 1: (a): Precision of images vs. the number of relevant images returned. IDIOM covers the maximum area. (b,c) The
percentage of senses for which at least one relevant result was returned, for (b) all senses and (c) for minor senses of the queries.
domly chosen word sense pairs (e.g., pair of ?pri-
mavera,? and ?the season spring?). We labeled a
pair as positive if googling the word returns at least
one good image for the sense in the top three. We
compared performance among a number of ma-
chine learning algorithms and found that Random
Forests (Breiman, 2001) performed the best over-
all with 69% classification accuracy using ten fold
cross validation versus 63% for Naive Bayes and
62% for SVMs. This high performance of Ran-
dom Forests mirrors other past experiments (Caru-
ana and Niculescu-Mizil, 2006).
Because of the ensemble nature of Random
Forests it is difficult to inspect the learned clas-
sifier for analysis. Still, anecdotal evidence sug-
gests that the classifier is able to learn an effective
model of good translations. We observe that it fa-
vors English whenever the English word is part of
one or few senses ? it picks out auction when the
query is ?sale? in the sense of ?act of putting up
for auction to highest bidder". In cases where En-
glish is more ambiguous it chooses a relatively less
ambiguous word in another language. It chooses
the French word ressort for finding ?spring? in the
sense of coil. For the query ?gift? we notice that it
does not choose the original query. This matches
our intuition, since gift has many homographs ?
the German word ?Gift? means poison or venom.
4 Experiments
Can querying translations instead of the original
query improve the quality of image search? If so,
then how much does our classifier help compared
to querying random translations? We also analyze
our results and study the variation of image qual-
ity along various dimensions, like part of speech,
abstractness/concreteness of the sense, and ambi-
guity of the original query.
As a comparison, we are interested in how ID-
IOM performs in relation to other methods for
querying Google Images. We compare IDIOM to
several methods. (1) Source Word (SW): Querying
with only the source word. This comparison func-
tions as our baseline. (2) Source Word + Gloss
(SW+G): Querying with the source word and the
gloss for the sense
1
. This method is one way to fo-
cus the source word towards the desired sense. (3)
Source Word + Random (SW+R): Querying with
three pairs of source word and a random transla-
tion. This is another natural way to extend the
baseline for the intended sense. (4) Random (R):
Querying with three random translations. This
tests the extent to which our classifier improves
our results compared to randomly choosing trans-
lations shown to the user in PANIMAGES.
We randomly select fifty English queries from
PANDICTIONARY and look up all senses contain-
ing these in PANDICTIONARY, resulting in a total
of 134 senses. These queries include short word
sequences (e.g., ?open sea?), mildly polysemous
queries like ?pan? (means Greek God and cooking
vessel) and highly polysemous ones like ?light?.
For each sense of each word, we query Google
Images with the query terms suggested by each
method and evaluate the top fifteen results. For
methods in which we have three queries, we eval-
uate the top five results for each query. We evalu-
ate a total of fifteen results because Google Images
fits fifteen images on each page for our screen size.
Figure 1(a) compares the precision of the five
methods with the number of good images re-
turned. We vary the number of images in con-
sideration from 1 to 15 to generate various points
in the graph. IDIOM outperforms the others by
wide margins overall producing a larger number of
good images and at higher precision. Surprisingly,
the closest competitor is the baseline method as
opposed to other methods that try to focus the
search towards the intended sense. This is prob-
ably because the additional words in the query (ei-
ther from gloss or a random translation) confuse
Google Images rather than focusing the search.
IDIOM covers 41% more area than SW. Overall
1
PANDICTIONARY provides a gloss (short explanation)
for each sense. E.g., a gloss for ?hero? is ?role model.?
195
1 sense 2 or 3 senses >3 senses
Perc
enta
ge C
orrec
t
02
04
06
08
0 IDIOMSWSW+GSW+RR
Noun Verb AdjectiveP
erce
ntag
e Co
rrect
0
20
40
60
80 IDIOMSWSW+GSW+RR
Concrete Abstract
Perc
enta
ge C
orrec
t
0
20
40
60
80 IDIOMSWSW+GSW+RR
Figure 2: The percentage of senses for which at least one relevant result was returned varied along several dimensions: (a)
polysemy of original query, and (b) part of speech of the sense, (c) abstractness/concreteness of the sense.
IDIOM produces 22% better images compared to
SW (389 vs 318).
We also observe that random translations return
much worse images than IDIOM suggesting that a
classifier is essential for high quality images.
Figure 1(b) compares the percentage of senses
for which at least one good result was returned in
the fifteen. Here IDIOM performs the best at 51%.
Each other method performs at about 40%. The re-
sults are statistically highly significant (p < 0.01).
Figure 1(c) compares the performance just on
the subset of the non-dominant senses of the query
words. All methods perform worse than in Figure
1(b) but IDIOM outperforms the others.
We also analyze our results across several di-
mensions. Figure 2(a) compares the performance
as a function of polysemy of the original query. As
expected, the disparity in methods is much more
for high polysemy queries. Most methods perform
well for the easy case of unambiguous queries.
Figure 2(b) compares along the different parts
of speech. For nouns and verbs, IDIOM returns the
best results. For adjectives, IDIOM and SW per-
form the best. Overall, nouns are the easiest for
finding images and we did not find much differ-
ence between verbs and adjectives.
Finally, Figure 2(c) reports how the methods
perform on abstract versus concrete queries. We
define a sense as abstract if it does not have a nat-
ural physical manifestation. For example, we clas-
sify ?nest? (a bird built structure) as concrete, and
?confirm? (to strengthen) as abstract. IDIOM per-
forms better than the other methods, but the results
vary massively between the two categories.
Overall, we find that our new system consis-
tently produces better results across the several di-
mensions and various metrics.
5 Related Work and Conclusions
Related Work: The popular paradigm for image
search is keyword-based, but it suffers due to pol-
ysemy and homography. An alternative paradigm
is content based (Datta et al, 2008), which is very
slow and works on simpler images. The field
of cross-lingual information retrieval (Ballesteros
and Croft, 1996) often performs translation-based
search. Other than PANIMAGES (which we out-
perform), no one to our knowledge has used this
for image search.
Conclusions: The recent development of PAN-
DICTIONARY (Mausam et al, 2009), a sense-
distinguished, massively multilingual dictionary,
enables a novel image search engine called ID-
IOM. We show that querying unambiguous trans-
lations of a sense produces images for 35% more
concepts compared to querying just the English
source word. In the process we learn a classi-
fier that predicts whether a given translation is a
good query for the intended sense or not. We
plan to release an image search website based
on IDIOM. In the future we wish to incorporate
knowledge from WordNet and cross-lingual links
in Wikipedia to increase IDIOM?s coverage beyond
the senses from PANDICTIONARY.
References
L. Ballesteros and B. Croft. 1996. Dictionary methods for
cross-lingual information retrieval. In DEXA Conference
on Database and Expert Systems Applications.
Dev Basu. 2009. How To Leverage Rich Me-
dia SEO for Small Businesses. In Search En-
gine Journal. http://www.searchenginejournal.com/rich -
media-small-business-seo/9580.
L. Breiman. 2001. Random forests. Machine Learning,
45(1):5?32.
R. Caruana and A. Niculescu-Mizil. 2006. An empiri-
cal comparison of supervised learning algorithms. In
ICML?06, pages 161?168.
R. Datta, D. Joshi, J. Li, and J. Wang. 2008. Image retrieval:
Ideas, influences, and trends of the new age. ACM Com-
puting Surveys, 40(2):1?60.
O. Etzioni, K. Reiter, S. Soderland, and M. Sammer. 2007.
Lexical translation with application to image search on the
Web. In Machine Translation Summit XI.
Peter Linsley. 2009. Google Image Search. In SMX West.
Mausam, S. Soderland, O. Etzioni, D. Weld, M. Skinner, and
J. Bilmes. 2009. Compiling a massive, multilingual dic-
tionary via probabilistic inference. In ACL?09.
I. Witten and E. Frank. 2005. Data Mining: Practical Ma-
chine Learning Tools and Techniques. Morgan Kaufmann.
196
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 27?34,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Detecting Parser Errors Using Web-based Semantic Filters
Alexander Yates Stefan Schoenmackers
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195-2350
{ayates, stef, etzioni} @cs.washington.edu
Oren Etzioni
Abstract
NLP systems for tasks such as question
answering and information extraction typ-
ically rely on statistical parsers. But the ef-
ficacy of such parsers can be surprisingly
low, particularly for sentences drawn from
heterogeneous corpora such as the Web.
We have observed that incorrect parses of-
ten result in wildly implausible semantic
interpretations of sentences, which can be
detected automatically using semantic in-
formation obtained from the Web.
Based on this observation, we introduce
Web-based semantic filtering?a novel,
domain-independent method for automat-
ically detecting and discarding incorrect
parses. We measure the effectiveness of
our filtering system, called WOODWARD,
on two test collections. On a set of TREC
questions, it reduces error by 67%. On
a set of more complex Penn Treebank
sentences, the reduction in error rate was
20%.
1 Introduction
Semantic processing of text in applications such
as question answering or information extraction
frequently relies on statistical parsers. Unfortu-
nately, the efficacy of state-of-the-art parsers can
be disappointingly low. For example, we found
that the Collins parser correctly parsed just 42%
of the list and factoid questions from TREC 2004
(that is, 42% of the parses had 100% precision and
100% recall on labeled constituents). Similarly,
this parser produced 45% correct parses on a sub-
set of 100 sentences from section 23 of the Penn
Treebank.
Although statistical parsers continue to improve
their efficacy over time, progress is slow, par-
ticularly for Web applications where training the
parsers on a ?representative? corpus of hand-
tagged sentences is not an option. Because of the
heterogeneous nature of text on the Web, such a
corpus would be exceedingly difficult to generate.
In response, this paper investigates the possibil-
ity of detecting parser errors by using semantic in-
formation obtained from the Web. Our fundamen-
tal hypothesis is that incorrect parses often result
in wildly implausible semantic interpretations of
sentences, which can be detected automatically in
certain circumstances. Consider, for example, the
following sentence from the Wall Street Journal:
?That compares with per-share earnings from con-
tinuing operations of 69 cents.? The Collins parser
yields a parse that attaches ?of 69 cents? to ?op-
erations,? rather than ?earnings.? By computing
the mutual information between ?operations? and
?cents? on the Web, we can detect that this attach-
ment is unlikely to be correct.
Our WOODWARD system detects parser errors
as follows. First, it maps the tree produced by a
parser to a relational conjunction (RC), a logic-
based representation language that we describe in
Section 2.1. Second, WOODWARD employs four
distinct methods for analyzing whether a conjunct
in the RC is likely to be ?reasonable? as described
in Section 2.
Our approach makes several assumptions. First,
if the sentence is absurd to begin with, then a cor-
rect parse could be deemed incorrect. Second, we
require a corpus whose content overlaps at least in
part with the content of the sentences to be parsed.
Otherwise, much of our semantic analysis is im-
possible.
In applications such as Web-based question an-
swering, these assumptions are quite natural. The
27
questions are about topics that are covered exten-
sively on the Web, and we can assume that most
questions link verbs to nouns in reasonable com-
binations. Likewise, when using parsing for infor-
mation extraction, we would expect our assump-
tions to hold as well.
Our contributions are as follows:
1. We introduce Web-based semantic filtering?
a novel, domain-independent method for de-
tecting and discarding incorrect parses.
2. We describe four techniques for analyzing
relational conjuncts using semantic informa-
tion obtained from the Web, and assess their
efficacy both separately and in combination.
3. We find that WOODWARD can filter good
parses from bad on TREC 2004 questions for
a reduction of 67% in error rate. On a harder
set of sentences from the Penn Treebank, the
reduction in error rate is 20%.
The remainder of this paper is organized as fol-
lows. We give an overview of related work in Sec-
tion 1.1. Section 2 describes semantic filtering, in-
cluding our RC representation and the four Web-
based filters that constitute the WOODWARD sys-
tem. Section 3 presents our experiments and re-
sults, and section 4 concludes and gives ideas for
future work.
1.1 Related Work
The problem of detecting parse errors is most sim-
ilar to the idea of parse reranking. Collins (2000)
describes statistical techniques for reranking alter-
native parses for a sentence. Implicitly, a rerank-
ing method detects parser errors, in that if the
reranking method picks a new parse over the orig-
inal one, it is classifying the original one as less
likely to be correct. Collins uses syntactic and lex-
ical features and trains on the Penn Treebank; in
contrast, WOODWARD uses semantic features de-
rived from the web. See section 3 for a comparison
of our results with Collins?.
Several systems produce a semantic interpreta-
tion of a sentence on top of a parser. For example,
Bos et al (2004) build semantic representations
from the parse derivations of a CCG parser, and
the English Resource Grammar (ERG) (Toutanova
et al, 2005) provides a semantic representation us-
ing minimal recursion semantics. Toutanova et al
also include semantic features in their parse se-
lection mechanism, although it is mostly syntax-
driven. The ERG is a hand-built grammar and thus
does not have the same coverage as the grammar
we use. We also use the semantic interpretations
in a novel way, checking them against semantic
information on the Web to decide if they are plau-
sible.
NLP literature is replete with examples of sys-
tems that produce semantic interpretations and
use semantics to improve understanding. Sev-
eral systems in the 1970s and 1980s used hand-
built augmented transition networks or semantic
networks to prune bad semantic interpretations.
More recently, people have tried incorporating
large lexical and semantic resources like WordNet,
FrameNet, and PropBank into the disambiguation
process. Allen (1995) provides an overview of
some of this work and contains many references.
Our work focuses on using statistical techniques
over large corpora, reducing the need for hand-
built resources and making the system more robust
to changes in domain.
Numerous systems, including Question-
Answering systems like MULDER (Kwok et
al., 2001), PiQASso (Attardi et al, 2001), and
Moldovan et al?s QA system (2003), use parsing
technology as a key component in their analysis
of sentences. In part to overcome incorrect parses,
Moldovan et al?s QA system requires a complex
set of relaxation techniques. These systems
would greatly benefit from knowing when parses
are correct or incorrect. Our system is the first
to suggest using the output of a QA system to
classify the input parse as good or bad.
Several researchers have used pointwise mu-
tual information (PMI) over the Web to help make
syntactic and semantic judgments in NLP tasks.
Volk (2001) uses PMI to resolve preposition at-
tachments in German. Lapata and Keller (2005)
use web counts to resolve preposition attachments,
compound noun interpretation, and noun count-
ability detection, among other things. And Mark-
ert et al (2003) use PMI to resolve certain types of
anaphora. We use PMI as just one of several tech-
niques for acquiring information from the Web.
2 Semantic Filtering
This section describes semantic filtering as imple-
mented in the WOODWARD system. WOODWARD
consists of two components: a semantic interpreter
that takes a parse tree and converts it to a conjunc-
tion of first-order predicates, and a sequence of
four increasingly sophisticated methods that check
semantic plausibility of conjuncts on the Web. Be-
low, we describe each component in turn.
28
1. What(NP1) ? are(VP1, NP1, NP2) ? states(NP2) ? producing(VP2, NP2, NP3) ? oil(NP3) ? in(PP1, NP2, U.S.)
2. What(NP1) ? states(NP2) ? producing(VP1, NP3, NP2, NP1) ? oil(NP3) ? in(PP1, NP2, U.S.)
Figure 2: Example relational conjunctions. The first RC is the correct one for the sentence ?What are oil producing
states in the U.S.?? The second is the RC derived from the Collins parse in Figure 1. Differences between the two RCs
appear in bold.

	 



		 
	 

BE: A Search Engine for NLP Research
Michael J. Cafarella, Oren Etzioni
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350
{mjc,etzioni}@cs.washington.edu
Many modern natural language-processing applica-
tions utilize search engines to locate large numbers of
Web documents or to compute statistics over the Web
corpus. Yet Web search engines are designed and op-
timized for simple human queries?they are not well
suited to support such applications. As a result, these
applications are forced to issue millions of successive
queries resulting in unnecessary search engine load and
in slow applications with limited scalability.
In response, we have designed the Bindings Engine
(BE), which supports queries containing typed vari-
ables and string-processing functions (Cafarella and
Etzioni, 2005). For example, in response to the query
?powerful ?noun?? BE will return all the nouns in its
index that immediately follow the word ?powerful?,
sorted by frequency. (Figure 1 shows several possible
BE queries.) In response to the query ?Cities such as
ProperNoun(Head(?NounPhrase?))?, BE will return a
list of proper nouns likely to be city names.
president Bush <Verb>
cities such as ProperNoun(Head(<NounPhrase>))
<NounPhrase> is the CEO of <NounPhrase>
Figure 1: Examples of queries that can be handled
by BE. Queries that include typed variables and
string-processing functions allow certain NLP tasks
to be done very efficiently.
BE?s novel neighborhood index enables it to do so
with O(k) random disk seeks and O(k) serial disk
reads, where k is the number of non-variable terms in
its query. A standard search engine requires O(k + B)
random disk seeks, where B is the number of vari-
able ?bindings? found in the corpus. Since B is typ-
ically very large, BE vastly reduces the number of ran-
dom disk seeks needed to process a query. Such seeks
operate very slowly and make up the bulk of query-
processing time. As a result, BE can yield several
orders of magnitude speedup for large-scale language-
processing applications. The main cost is a modest in-
crease in space to store the index.
To illustrate BE?s capabilities, we have built an ap-
plication to support interactive information extraction
in response to simple user queries. For example, in re-
sponse to the user query ?insects?, the application re-
turns the results shown in Figure 2. The application
Figure 2: Most-frequently-seen extractions for
query ?insects?. The score for each extraction is the
number of times it was retrieved over several BE ex-
traction phrases.
generates this list by using the query term to instantiate
a set of generic extraction phrase queries such as ?in-
sects such as ?NounPhrase??. In effect, the application
is doing a kind of query expansion to enable naive users
to extract information. In an effort to find high-quality
extractions, we sort the list by the hit count for each
binding, summed over all the queries.
The key difference between this BE application,
called KNOWITNOW, and domain-independent infor-
mation extraction systems such as KNOWITALL (Et-
zioni et al, 2005) is that BE enables extraction at in-
teractive speeds ? the average time to expand and re-
spond to a user query is between 1 and 45 seconds.
With additional optimization, we believe we can reduce
that time to 5 seconds or less. A detailed description of
KNOWITNOW appears in (Cafarella et al, 2005).
References
M. Cafarella and O. Etzioni. 2005. A Search En-gine for Natural Language Applications. In Procs.
of the 14th International World Wide Web Confer-
ence (WWW 2005).
M. Cafarella, D. Downey, S. Soderland, and O. Etzioni.2005. Knowitnow: Fast, scalable information ex-
traction from the web. In Procs. of EMNLP.
O. Etzioni, M. Cafarella, D. Downey, S. Kok,A. Popescu, T. Shaked, S. Soderland, D. Weld, andA. Yates. 2005. Unsupervised named-entity extrac-
tion from the web: An experimental study. Articial
Intelligence, 165(1):91?134.
9
 10
Expanding the Recall of Relation Extraction by Bootstrapping
Junji Tomita
NTT Cyber Solutions Laboratories,
NTT Corporation
1-1 Hikarinooka Yokosuka-Shi,
Kanagawa 239-0847, Japan
tomita.junji@lab.ntt.co.jp
Stephen Soderland Oren Etzioni
Department of Computer Science
& Engineering
University of Washington
Seattle, WA 98195-2350
 soderlan,etzioni
@cs.washington.edu
Abstract
Most works on relation extraction assume
considerable human effort for making an
annotated corpus or for knowledge engi-
neering. Generic patterns employed in
KnowItAll achieve unsupervised, high-
precision extraction, but often result in low
recall. This paper compares two boot-
strapping methods to expand recall that
start with automatically extracted seeds
by KnowItAll. The first method is string
pattern learning, which learns string con-
texts adjacent to a seed tuple. The second
method learns less restrictive patterns that
include bags of words and relation-specific
named entity tags. Both methods improve
the recall of the generic pattern method. In
particular, the less restrictive pattern learn-
ing method can achieve a 250% increase
in recall at 0.87 precision, compared to the
generic pattern method.
1 Introduction
Relation extraction is a task to extract tu-
ples of entities that satisfy a given relation
from textual documents. Examples of rela-
tions include CeoOf(Company, Ceo) and Acquisi-
tion(Organization, Organization). There has been
much work on relation extraction; most of it em-
ploys knowledge engineering or supervised ma-
chine learning approaches (Feldman et al, 2002;
Zhao and Grishman, 2005). Both approaches are
labor intensive.
We begin with a baseline information extraction
system, KnowItAll (Etzioni et al, 2005), that does
unsupervised information extraction at Web scale.
KnowItAll uses a set of generic extraction pat-
terns, and automatically instantiates rules by com-
bining these patterns with user supplied relation
labels. For example, KnowItAll has patterns for a
generic ?of? relation:
NP1 ?s  relation , NP2
NP2 ,  relation of NP1
where NP1 and NP2 are simple noun phrases that
extract values of argument1 and argument2 of a
relation, and  relation is a user-supplied string
associated with the relation. The rules may also
constrain NP1 and NP2 to be proper nouns.
If a user supplies the relation labels ?ceo?
and ?chief executive officer? for the relation
CeoOf(Company, Ceo), KnowItAll inserts these
labels into the generic patterns shown above, to
create 4 extraction rules:
NP1 ?s ceo , NP2
NP1 ?s chief executive officer , NP2
NP2 , ceo of NP1
NP2 , chief executive officer of NP1
The same generic patterns with different la-
bels can also produce extraction rules for a May-
orOf relation or an InventorOf relation. These
rules have alternating context strings (exact string
match) and extraction slots (typically an NP or
head of an NP). This can produce rules with high
precision, but low recall, due to the wide variety
of contexts describing a relation. This paper looks
at ways to enhance recall over this baseline system
while maintaining high precision.
To enhance recall, we employ bootstrapping
techniques which start with seed tuples, i.e. the
most frequently extracted tuples by the baseline
system. The first method represents rules with
three context strings of tokens immediately adja-
cent to the extracted arguments: a left context,
56
middle context, and right context. These are in-
duced from context strings found adjacent to seed
tuples.
The second method uses a less restrictive pat-
tern representation such as bag of words, similar
to that of SnowBall(Agichtein, 2005). SnowBall is
a semi-supervised relation extraction system. The
input of Snowball is a few hand labeled correct
seed tuples for a relation (e.g. <Microsoft, Steve
Ballmer> for CeoOf relation). SnowBall clusters
the bag of words representations generated from
the context strings adjacent to each seed tuple, and
generates rules from them. It calculates the confi-
dence of candidate tuples and the rules iteratively
by using an EM-algorithm. Because it can extract
any tuple whose entities co-occur within a win-
dow, the recall can be higher than the string pat-
tern learning method. The main disadvantage of
SnowBall or a method which employs less restric-
tive patterns is that it requires Named Entity Rec-
ognizer (NER).
We introduce Relation-dependent NER (Rela-
tion NER), which trains an off-the-shelf super-
vised NER based on CRF(Lafferty et al, 2001)
with bootstrapping. This learns relation-specific
NE tags, and we present a method to use these tags
for relation extraction.
This paper compares the following two boot-
strapping strategies.
SPL: a simple string pattern learning method. It
learns string patterns adjacent to a seed tuple.
LRPL: a less restrictive pattern learning method.
It learns a variety of bag of words patterns,
after training a Relation NER.
Both methods are completely self-supervised ex-
tensions to the unsupervised KnowItAll. A user
supplies KnowItAll with one or more relation la-
bels to be applied to one or more generic extrac-
tion patterns. No further tagging or manual selec-
tion of seeds is required. Each of the bootstrapping
methods uses seeds that are automatically selected
from the output of the baseline KnowItAll system.
The results show that both bootstrapping meth-
ods improve the recall of the baseline system. The
two methods have comparable results, with LRPL
outperforms SPL for some relations and SPL out-
performs LRPL for other relations.
The rest of the paper is organized as follows.
Section 2 and 3 describe SPL and LRPL respec-
tively. Section 4 reports on our experiments, and
section 5 and 6 describe related works and conclu-
sions.
2 String Pattern Learning (SPL)
Both SPL and LRPL start with seed tuples that
were extracted by the baseline KnowItAll system,
with extraction frequency at or above a threshold
(set to 2 in these experiments). In these experi-
ments, we downloaded a set of sentences from the
Web that contained an occurrence of at least one
relation label and used this as our reservoir of un-
labeled training and test sentences. We created a
set of positive training sentences from those sen-
tences that contained both argument values of a
seed tuple.
SPL employs a method similar to that of
(Downey et al, 2004). It generates candidate ex-
traction rules with a prefix context, a middle con-
text, and a right context. The prefix is zero to 
 
tokens immediately to the left of extracted argu-
ment1, the middle context is all tokens between
argument1 and argument2, and the right context of
zero to 
 
tokens immediately to the right of ar-
gument2. It discards patterns with more than 

intervening tokens or without a relation label.
SPL tabulates the occurrence of such patterns
in the set of positive training sentences (all sen-
tences from the reservoir that contain both argu-
ment values from a seed tuple in either order), and
also tabulates their occurrence in negative training
sentences. The negative training are sentences that
have one argument value from a seed tuple and a
nearest simple NP in place of the other argument
value. This idea is based on that of (Ravichan-
dran and Hovy, 2002) for a QA system. SPL
learns a possibly large set of strict extraction rules
that have alternating context strings and extraction
slots, with no gaps or wildcards in the rules.
SPL selects the best patterns as follows:
1. Groups the context strings that have the exact
same middle string.
2. Selects the best pattern having the largest pat-
tern score, , for each group of context
strings having the same middle string.
  
 
 
    
 
 
     
	

    
(1)
3. Selects the patterns having  greater than

	
.
57
Figure 1: The architecture of LRPL (Less Restric-
tive Pattern Learning).
where 
 
  is a set of sentences that match
pattern  and include both argument values of a
seed tuple. 
	

  is a set of sentences that
match  and include just one argument value of
a seed tuple (e.g. just a company or a person for
CeoOf).  is a constant for smoothing.
3 Less Restrictive Pattern Learning
(LRPL)
LRPL uses a more flexible rule representation than
SPL. As before, the rules are based on a window of
tokens to the left of the first argument, a window
of middle tokens, and a window of tokens to the
right of the second argument. Rather than using
exact string match on a simple sequence of tokens,
LRPL uses a combination of bag of words and im-
mediately adjacent token. The left context is based
on a window of 
 
tokens immediately to the
left of argument1. It has two sets of tokens: the
token immediately to the left and a bag of words
for the remaining tokens. Each of these sets may
have zero or more tokens. The middle and right
contexts are similarly defined. We call this repre-
sentation extended bag of words.
Here is an example of how LRPL represents
the context of a training sentence with win-
dow size set to 4. ?Yesterday ,  Arg2Steve
Ballmer /Arg2, the Chief Executive Officer of
 Arg1Microsoft /Arg1 said that he is ...?.
order: arg2_arg1
values: Steve Ballmer, Microsoft
L: {yesterday} {,}
M: {,} {chief executive officer the} {of}
R: {said} {he is that}
Some of the tokens in these bags of words may
be dropped in merging this with patterns from
other training sentences. Each rule also has a con-
fidence score, learned from EM-estimation.
We experimented with simply using three bags
of words as in SnowBall, but found that precision
was increased when we distinguished the tokens
immediately adjacent to argument values from the
other tokens in the left, middle, and right bag of
words.
Less restrictive patterns require a Named Entity
Recognizer (NER), because the patterns can not
extract candidate entities by themselves1. LRPL
trains a supervised NER in bootstrapping for ex-
tracting candidate entities.
Figure 1 overviews LRPL. It consists of two
bootstrapping modules: Relation NER and Rela-
tion Assessor. LRPL trains the Relational NER
from seed tuples provided by the baseline Know-
ItAll system and unlabeled sentences in the reser-
voir. Then it does NE tagging on the sentences to
learn the less restrictive rules and to extract can-
didate tuples. The learning and extraction steps at
Relation Assessor are similar to that of SnowBall;
it generates a set of rules and uses EM-estimation
to compute a confidence in each rule. When these
rules are applied, the system computes a probabil-
ity for each tuple based on the rule confidence, the
degree of match between a sentence and the rule,
and the extraction frequency.
3.1 Relation dependent Named Entity
Recognizer
Relation NER leverages an off-the-shelf super-
vised NER, based on Conditional Random Fields
(CRF). In Figure 1, TrainSentenceGenerator auto-
matically generates training sentences from seeds
and unlabeled sentences in the reservoir. TrainEn-
tityRecognizer trains a CRF on the training sen-
tences and then EntityRecognizer applies the
trained CRF to all the unlabeled sentences, creat-
ing entity annotated sentences.
It can extract entities whose type matches an ar-
gument type of a particular relation. The type is
not explicitly specified by a user, but is automati-
cally determined according to the seed tuples. For
example, it can extract ?City? and ?Mayor? type en-
tities for MayorOf(City, Mayor) relation. We de-
scribe CRF in brief, and then how to train it in
bootstrapping.
1Although using all noun phrases in a sentence may be
possible, it apparently results in low precision.
58
3.1.1 Supervised Named Entity Recognizer
Several state-of-the-art supervised NERs are
based on a feature-rich probabilistic conditional
classifier such as Conditional Random Fields
(CRF) for sequential learning tasks(Lafferty et al,
2001; Rosenfeld et al, 2005). The input of CRF is
a feature sequence  of features 	

, and outputs a
tag sequence 
 of tags 

. In the training phrase, a
set of  

 


 is provided, and outputs a model


. In the applying phase, given  , it outputs a
tag sequence 
 by using 

. In the case of NE
tagging, given a sequence of tokens, it automat-
ically generates a sequence of feature sets; each
set is corresponding to a token. It can incorporate
any properties that can be represented as a binary
feature into the model, such as words, capitalized
patterns, part-of-speech tags and the existence of
the word in a dictionary. It works quite well on
NE tagging tasks (McCallum and Li, 2003).
3.1.2 How to Train Supervised NER in
Bootstrapping
We use bootstrapping to train CRF for relation-
specific NE tagging as follows: 1) select the sen-
tences that include all the entity values of a seed
tuple, 2) automatically mark the argument values
in each sentence, and 3)train CRF on the seed
marked sentences. An example of a seed marked
sentence is the following:
seed tuple: <Microsoft, Steve Ballmer>
seed marked sentence:
"Yesterday, <Arg2>Steve Ballmer</Arg2>,
CEO of <Arg1>Microsoft</Arg1>
announced that ..."
Because of redundancy, we can expect to gen-
erate a fairly large number of seed marked sen-
tences by using a few highly frequent seed tuples.
To avoid overfitting on terms from these seed tu-
ples, we substitute the actual argument values with
random characters for each training sentence, pre-
serving capitalization patterns and number of char-
acters in each token.
3.2 Relation Assessor
Relation Assessor employs several SnowBall-like
techniques including making rules by clustering
and EM-estimation for the confidence of the rules
and tuples.
In Figure 1, ContextRepresentationGenerator
generates extended bag of words contexts, from
entity annotated sentences, and classifies the con-
texts into two classes: training contexts 
	
(if
their entity values and their orders match a seed
tuple) and test contexts 
 
(otherwise). Train-
ConfidenceEstimator clusters 
	
based on the
match score between contexts, and generates a
rule from each cluster, that has average vectors
over contexts belonging to the cluster. Given a set
of generated rules and test contexts 
 
, Confi-
denceEstimator estimates each tuple confidence in

 
by using an EM algorithm. It also estimates
the confidence of the tuples extracted by the base-
line system, and outputs the merged result tuples
with confidence.
We describe the match score calculation
method, the EM-algorithm, and the merging
method in the following sub sections.
3.2.1 Match Score Calculation
The match score (or similarity)  of two ex-
tended bag of words contexts 

, 

is calculated
as the linear combination of the cosine values be-
tween the corresponding vectors.
 

 

 
 
 

 
 
 
 
 
 (2)
where,  is the index of left, middle, or right con-
texts.  is the index of left adjacent, right adjacent,
or other tokens. 
 
is the weight corresponding
to the context vector indexed by  and .
To achieve high precision, Relation Assessor
uses only the entity annotated sentences that have
just one entity for each argument (two entities
in total) and where those entities co-occur within


tokens window, and it uses at most 
 
left
and right tokens. It discards patterns without a re-
lation label.
3.2.2 EM-estimation for tuple and rule
confidence
Several rules generated from only positive ev-
idence result in low precision (e.g. rule ?of? for
MayorOf relation generated from ?Rudolph Giu-
liani of New York?). This problem can be im-
proved by estimating the rule confidence by the
following EM-algorithm.
1. For each 

in 
 
, identifies the best match
rule   

, based on the match score be-
tween 

and each rule . 

is the th con-
text that includes tuple 

.
 
 
 
 
  argmax

   
 
 (3)
59
2. Initializes seed tuple confidence, 
 
 
  
for all 
 
, where 
 
is a seed tuple.
3. Calculates tuple confidence, 
 , and rule
confidence,  , by using EM-algorithm. E
and M stages are iterated several times.
E stage:
  

 


 

  

  
	
  

  
(4)
M stage:
 
 
  (5)
  


    
 
 
 
  
 
 
 
 
 
(6)
where
 

  

 

 

   

  
 
 

 
  
 is a constant for smoothing.
This algorithm assigns a high confidence to the
rules that frequently co-occur with only high con-
fident tuples. It also assigns a high confidence to
the tuples that frequently co-occur with the con-
texts that match high confidence rules.
When it merges the tuples extracted by the base-
line system, the algorithm uses the following con-
stant value for any context that matches a baseline
pattern.
 

 

 

  

 
 
  
 
  
(7)
where 

denotes the context of tuple 

that
matches a baseline pattern, and 

is any baseline
pattern. With this calculation, the confidence of
any tuple extracted by a baseline pattern is always
greater than or equal to that of any tuple that is
extracted by the learned rules and has the same
frequency.
4 Evaluation
The focus of this paper is the comparison be-
tween bootstrapping strategies for extraction, i.e.,
string pattern learning and less restrictive pattern
learning having Relation NER. Therefore, we first
compare these two bootstrapping methods with
the baseline system. Furthermore, we also com-
pare Relation NER with a generic NER, which is
trained on a pre-existing hand annotated corpus.
Table 1: Weights corresponding to a context vector
(
 
).
adjacency
left other right total
left 0.067 0.133 0.2
context middle 0.24 0.12 0.24 0.6
right 0.133 0.067 0.2
4.1 Relation Extraction Task
We compare SPL and LRPL with the baseline sys-
tem on 5 relations: Acquisition, Merger, CeoOf,
MayorOf, and InventorOf. We downloaded about
from 100,000 to 220,000 sentences for each of
these relations from the Web, which contained a
relation label (e.g. ?acquisition?, ?acquired?, ?ac-
quiring? or ?merger?, ?merged?, ?merging?). We
used all the tuples that co-occur with baseline pat-
terns at least twice as seeds. The numbers of seeds
are between 33 (Acquisition) and 289 (CeoOf).
For consistency, SPL employs the same assess-
ment methods with LRPL. It uses the EM algo-
rithm in Section 3.2.2 and merges the tuples ex-
tracted by the baseline system. In the EM algo-
rithm, the match score    between a learned
pattern  and a tuple  is set to a constant 

.
LRPL uses MinorThird (Cohen, 2004) imple-
mentation of CRF for Relation NER. The features
used in the experiments are the lower-case word,
capitalize pattern, part of speech tag of the cur-
rent and +-2 tokens, and the previous state (tag)
referring to (Minkov et al, 2005; Rosenfeld et al,
2005). The parameters used for SPL and LRPL
are experimentally set as follows: 
	
 	,


 
, 

 	, 
 
 ,   ,
   and the context weights for LRPL shown in
Table 1.
Figure 2-6 show the recall-precision curves. We
use the number of correct extractions to serve as
a surrogate for recall, since computing actual re-
call would require extensive manual inspection of
the large data sets. Compared to the the baseline
system, both bootstrapping methods increases the
number of correct extractions for almost all the re-
lations at around 80% precision. For MayorOf re-
lation, LRPL achieves 250% increase in recall at
0.87 precision, while SPL?s precision is less than
the baseline system. This is because SPL can not
distinguish correct tuples from the error tuples that
60
Figure 2: The recall-precision curve of CeoOf re-
lation.
Figure 3: The recall-precision curve of MayorOf
relation.
co-occur with a short strict pattern, and that have a
wrong entity type value. An example of the error
tuples extracted by SPL is the following:
Learned Pattern: NP1 Mayor NP2
Sentence:
"When Lord Mayor Clover Moore spoke,..."
Tuple: <Lord, Clover Moore>
The improvement of Acquisition and Merger re-
lations is small for both methods; the rules learned
for Merger and Acquisition made erroneous ex-
tractions of mergers of geo-political entities, ac-
quisition of data, ball players, languages or dis-
eases. For InventorOf relation, LRPL does not
work well. This is because ?Invention? is not a
proper noun phrase, but a noun phrase. A noun
phrase includes not only nouns, but a particle,
a determiner, and adjectives in addition to non-
capitalized nouns. Our Relation NER was unable
to detect regularities in the capitalization pattern
and word length of invention phrases.
At around 60% precision, SPL achieves higher
recall for CeoOf and MayorOf relations, in con-
Figure 4: The recall-precision curve of Acquisi-
tion relation.
Figure 5: The recall-precision curve of Merger re-
lation.
trast, LRPL achieves higher recall for Acquisition
and Merger. The reason can be that nominal style
relations (CeoOf and MayorOf) have a smaller
syntactic variety for describing them. Therefore,
learned string patterns are enough generic to ex-
tract many candidate tuples.
4.2 Entity Recognition Task
Generic types such as person, organization, and
location cover many useful relations. One might
expect that NER trained for these generic types,
can be used for different relations without mod-
ifications, instead of creating a Relation NER.
To show the effectiveness of Relation NER, we
compare Relation NER with a generic NER
trained on a pre-existent hand annotated corpus
for generic types; we used MUC7 train, dry-run
test, and formal-test documents(Table 2) (Chin-
chor, 1997). We also incorporate the following
additional knowledge into the CRF?s features re-
ferring to (Minkov et al, 2005; Rosenfeld et al,
61
Figure 6: The recall-precision curve of InventorOf
relation.
Table 2: The number of entities and unique entities
in MUC7 corpus. The number of documents is
225.
entity all uniq
Organization 3704 993
Person 2120 1088
Location 2912 692
2005): first and last names, city names, corp des-
ignators, company words (such as ?technology?),
and small size lists of person title (such as ?mr.?)
and capitalized common words (such as ?Mon-
day?). The base features for both methods are the
same as the ones described in Section 4.1.
The ideal entity recognizer for relation extrac-
tion is recognizing only entities that have an ar-
gument type for a particular relation. Therefore,
a generic test set such as MUC7 Named Entity
Recognition Task can not be used for our evalu-
ation. We randomly selected 200 test sentences
from our dataset that had a pair of correct enti-
ties for CeoOf or MayorOf relations, and were not
used as training for the Relation NER. We mea-
sured the accuracy as follows.
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1088?1098,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Learning First-Order Horn Clauses from Web Text
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld
Turing Center
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98125, USA
stef,etzioni,weld@cs.washington.edu
Jesse Davis
Katholieke Universiteit Leuven
Department of Computer Science
POBox 02402 Celestijnenlaan 200a
B-3001 Heverlee, Belgium
jesse.davis@cs.kuleuven.be
Abstract
Even the entire Web corpus does not explic-
itly answer all questions, yet inference can un-
cover many implicit answers. But where do
inference rules come from?
This paper investigates the problem of learn-
ing inference rules from Web text in an un-
supervised, domain-independent manner. The
SHERLOCK system, described herein, is a
first-order learner that acquires over 30,000
Horn clauses from Web text. SHERLOCK em-
bodies several innovations, including a novel
rule scoring function based on Statistical Rel-
evance (Salmon et al, 1971) which is effec-
tive on ambiguous, noisy and incomplete Web
extractions. Our experiments show that in-
ference over the learned rules discovers three
times as many facts (at precision 0.8) as the
TEXTRUNNER system which merely extracts
facts explicitly stated in Web text.
1 Introduction
Today?s Web search engines locate pages that match
keyword queries. Even sophisticated Web-based
Q/A systems merely locate pages that contain an ex-
plicit answer to a question. These systems are help-
less if the answer has to be inferred from multiple
sentences, possibly on different pages. To solve this
problem, Schoenmackers et al(2008) introduced the
HOLMES system, which infers answers from tuples
extracted from text.
HOLMES?s distinction is that it is domain inde-
pendent and that its inference time is linear in the
size of its input corpus, which enables it to scale to
the Web. However, HOLMES?s Achilles heel is that
it requires hand-coded, first-order, Horn clauses as
input. Thus, while HOLMES?s inference run time
is highly scalable, it requires substantial labor and
expertise to hand-craft the appropriate set of Horn
clauses for each new domain.
Is it possible to learn effective first-order Horn
clauses automatically from Web text in a domain-
independent and scalable manner? We refer to the
set of ground facts derived from Web text as open-
domain theories. Learning Horn clauses has been
studied extensively in the Inductive Logic Program-
ming (ILP) literature (Quinlan, 1990; Muggleton,
1995). However, learning Horn clauses from open-
domain theories is particularly challenging for sev-
eral reasons. First, the theories denote instances of
an unbounded and unknown set of relations. Sec-
ond, the ground facts in the theories are noisy, and
incomplete. Negative examples are mostly absent,
and certainly we cannot make the closed-world as-
sumption typically made by ILP systems. Finally,
the names used to denote both entities and relations
are rife with both synonyms and polysymes making
their referents ambiguous and resulting in a particu-
larly noisy and ambiguous set of ground facts.
This paper presents a new ILP method, which is
optimized to operate on open-domain theories de-
rived from massive and diverse corpora such as the
Web, and experimentally confirms both its effective-
ness and superiority over traditional ILP algorithms
in this context. Table 1 shows some example rules
that were learned by SHERLOCK.
This work makes the following contributions:
1. We describe the design and implementation of
the SHERLOCK system, which utilizes a novel,
unsupervised ILP method to learn first-order
Horn clauses from open-domain Web text.
1088
IsHeadquarteredIn(Company, State) :-
IsBasedIn(Company, City) ? IsLocatedIn(City, State);
Contains(Food, Chemical) :-
IsMadeFrom(Food, Ingredient) ? Contains(Ingredient, Chemical);
Reduce(Medication, Factor) :-
KnownGenericallyAs(Medication, Drug) ? Reduce(Drug, Factor);
ReturnTo(Writer, Place) :- BornIn(Writer, City) ? CapitalOf(City, Place);
Make(Company1, Device) :- Buy(Company1, Company2) ? Make(Company2, Device);
Table 1: Example rules learned by SHERLOCK from Web extractions. Note that the italicized rules are unsound.
2. We derive an innovative scoring function that is
particularly well-suited to unsupervised learn-
ing from noisy text. For Web text, the scoring
function yields more accurate rules than several
functions from the ILP literature.
3. We demonstrate the utility of SHERLOCK?s
automatically learned inference rules. Infer-
ence using SHERLOCK?s learned rules identi-
fies three times as many high quality facts (e.g.,
precision ? 0.8) as were originally extracted
from the Web text corpus.
The remainder of this paper is organized as fol-
lows. We start by describing previous work. Sec-
tion 3 introduces the SHERLOCK rule learning sys-
tem, with Section 3.4 describing how it estimates
rule quality. We empirically evaluate SHERLOCK in
Section 4, and conclude.
2 Previous Work
SHERLOCK is one of the first systems to learn first-
order Horn clauses from open-domain Web extrac-
tions. The learning method in SHERLOCK belongs
to the Inductive logic programming (ILP) subfield
of machine learning (Lavrac and Dzeroski, 2001).
However, classical ILP systems (e.g., FOIL (Quin-
lan, 1990) and Progol (Muggleton, 1995)) make
strong assumptions that are inappropriate for open
domains. First, ILP systems assume high-quality,
hand-labeled training examples for each relation of
interest. Second, ILP systems assume that constants
uniquely denote individuals; however, in Web text
strings such as ?dad? or ?John Smith? are highly
ambiguous. Third, ILP system typically assume
complete, largely noise-free data whereas tuples ex-
tracted from Web text are both noisy and radically
incomplete. Finally, ILP systems typically utilize
negative examples, which are not available when
learning from open-domain facts. One system that
does not require negative examples is LIME (Mc-
Creath and Sharma, 1997); We compare SHERLOCK
with LIME?s methods in Section 4.3. Most prior ILP
and Markov logic structure learning systems (e.g.,
(Kok and Domingos, 2005)) are not designed to han-
dle the noise and incompleteness of open-domain,
extracted facts.
NELL (Carlson et al, 2010) performs coupled
semi-supervised learning to extract a large knowl-
edge base of instances, relations, and inference
rules, bootstrapping from a few seed examples of
each class and relation of interest and a few con-
straints among them. In contrast, SHERLOCK fo-
cuses mainly on learning inference rules, but does so
without any manually specified seeds or constraints.
Craven et al(1998) also used ILP to help infor-
mation extraction on the Web, but required training
examples and focused on a single domain.
Two other notable systems that learn inference
rules from text are DIRT (Lin and Pantel, 2001)
and RESOLVER (Yates and Etzioni, 2007). How-
ever, both DIRT and RESOLVER learn only a lim-
ited set of rules capturing synonyms, paraphrases,
and simple entailments, not more expressive multi-
part Horn clauses. For example, these systems may
learn the rule X acquired Y =? X bought Y ,
which captures different ways of describing a pur-
chase. Applications of these rules often depend on
context (e.g., if a person acquires a skill, that does
not mean they bought the skill). To add the neces-
sary context, ISP (Pantel et al, 2007) learned selec-
tional preferences (Resnik, 1997) for DIRT?s rules.
The selectional preferences act as type restrictions
1089
Figure 1: Architecture of SHERLOCK. SHERLOCK learns
inference rules offline and provides them to the HOLMES
inference engine, which uses the rules to answer queries.
on the arguments, and attempt to filter out incorrect
inferences. While these approaches are useful, they
are strictly more limited than the rules learned by
SHERLOCK.
The Recognizing Textual Entailment (RTE)
task (Dagan et al, 2005) is to determine whether
one sentence entails another. Approaches to RTE
include those of Tatu and Moldovan (2007), which
generates inference rules from WordNet lexical
chains and a set of axiom templates, and Pennac-
chiotti and Zanzotto (2007), which learns inference
rules based on similarity across entailment pairs. In
contrast with this work, RTE systems reason over
full sentences, but benefit by being given the sen-
tences and training data. SHERLOCK operates over
simpler Web extractions, but is not given guidance
about which facts may interact.
3 System Description
SHERLOCK takes as input a large set of open domain
facts, and returns a set of weighted Horn-clause in-
ference rules. Other systems (e.g., HOLMES) use the
rules to answer questions, infer additional facts, etc.
SHERLOCK?s basic architecture is depicted in
Figure 1. To learn inference rules, SHERLOCK per-
forms the following steps:
1. Identify a ?productive? set of classes and in-
stances of those classes
2. Discover relations between classes
3. Learn inference rules using the discovered rela-
tions and determine the confidence in each rule
The first two steps help deal with the synonyms,
homonyms, and noise present in open-domain the-
ories by identifying a smaller, cleaner, and more co-
hesive set of facts to learn rules over.
SHERLOCK learns inference rules from a collec-
tion of open-domain extractions produced by TEX-
TRUNNER (Banko et al, 2007). The rules learned
by SHERLOCK are input to an inference engine and
used to find answers to a user?s query. In this paper,
SHERLOCK utilizes HOLMES as its inference engine
when answering queries, and uses extracted facts
of the form R(arg1, arg2) provided by the authors
of TEXTRUNNER, but the techniques presented are
more broadly applicable.
3.1 Finding Classes and Instances
SHERLOCK first searches for a set of well-defined
classes and class instances. Instances of the same
class tend to behave similarly, so identifying a good
set of instances will make it easier to discover the
general properties of the entire class.
Options for identifying interesting classes include
manually created methods (WordNet (Miller et al,
1990)), textual patterns (Hearst, 1992), automated
clustering (Lin and Pantel, 2002), and combina-
tions (Snow et al, 2006). We use Hearst patterns
because they are simple, capture how classes and in-
stances are mentioned in Web text, and yield intu-
itive, explainable groups.
Hearst (1992) identified a set of textual patterns
which indicate hyponymy (e.g., ?Class such as In-
stance?). Using these patterns, we extracted 29 mil-
lion (instance, class) pairs from a large Web crawl.
We then cleaned them using word stemming, nor-
malization, and by dropping modifiers.
Unfortunately, the patterns make systematic er-
rors (e.g., extracting Canada as the name of a city
from the phrase ?Toronto, Canada and other cities.?)
To address this issue, we discard the low frequency
classes of each instance. This heuristic reduces the
noise due to systematic error while still capturing the
important senses of each word. Additionally, we use
the extraction frequency to estimate the probability
that a particular mention of an instance refers to each
of its potential classes (e.g., New York appears as a
city 40% of the time, a state 35% of the time, and a
place, area, or center the rest of the time).
1090
Ambiguity presents a significant obstacle when
learning inference rules. For example, the corpus
contains the sentences ?broccoli contains this vita-
min? and ?this vitamin prevents scurvy,? but it is un-
clear if the sentences refer to the same vitamin. The
two main sources of ambiguity we observed are ref-
erences to a more general class instead of a specific
instance (e.g., ?vitamin?), and references to a person
by only their first or last name. We eliminate the
first by removing terms that frequently appear as the
class name with other instances, and the second by
removing common first and last names.
The 250 most frequently mentioned class names
include a large number of interesting classes (e.g.,
companies, cities, foods, nutrients, locations) as
well as ambiguous concepts (e.g., ideas, things). We
focus on the less ambiguous classes by eliminating
any class not appearing as a descendant of physical
entity, social group, physical condition, or event in
WordNet. Beyond this filtering we make no use of a
type hierarchy and treat classes independently.
In our corpus, we identify 1.1 million distinct,
cleaned (instance, class) pairs for 156 classes.
3.2 Discovering Relations between Classes
Next, SHERLOCK discovers how classes relate to
and interact with each other. Prior work in relation
discovery (Shinyama and Sekine, 2006) has investi-
gated the problem of finding relationships between
different classes. However, the goal of this work is
to learn rules on top of the discovered typed rela-
tions. We use a few simple heuristics to automati-
cally identify interesting relations.
For every pair of classes (C1, C2), we find a set
of typed, candidate relations from the 100 most fre-
quent relations in the corpus where the first argu-
ment is an instance of C1 and the second argument
is an instance of C2. For extraction terms with mul-
tiple senses (e.g., New York), we split their weight
based on how frequently they appear with each class
in the Hearst patterns.
However, many discovered relations are rare and
meaningless, arising from either an extraction error
or word-sense ambiguity. For example, the extrac-
tion ?Apple is based in Cupertino? gives some evi-
dence that a fruit may possibly be based in a city.
We attempt to filter out incorrectly-typed relations
using two heuristics. We first discard any relation
whose weighted frequency falls below a threshold,
since rare relations are more likely to arise due to
extraction errors or word-sense ambiguity. We also
remove relations whose pointwise mutual informa-
tion (PMI) is below a threshold T=exp(2) ? 7.4:
PMI(R(C1, C2)) =
p(R,C1, C2)
p(R, ?, ?) ? p(?, C1, ?) ? p(?, ?, C2)
where p(R, ?, ?) is the probability a random extrac-
tion has relation R, p(?, C1, ?) is the probability a
random extraction has an instance of C1 as its first
argument, p(?, ?, C2) is similar for the second argu-
ment, and p(R,C1, C2) is the probability that a ran-
dom extraction has relation R and instances of C1
and C2 as its first and second arguments, respec-
tively. A low PMI indicates the relation occurred by
random chance, which is typically due to ambiguous
terms or extraction errors.
Finally, we use two TEXTRUNNER specific clean-
ing heuristics: we ignore a small set of stop-relations
(?be?, ?have?, and ?be preposition?) and extractions
whose arguments are more than four tokens apart.
This process identifies 10,000 typed relations.
3.3 Learning Inference Rules
SHERLOCK attempts to learn inference rules for
each typed relation in turn. SHERLOCK receives a
target relation, R, a set of observed examples of the
relation, E+, a maximum clause length k, a mini-
mum support, s, and an acceptance threshold, t, as
input. SHERLOCK generates all first-order, definite
clauses up to length k, where R appears as the head
of the clause. It retains each clause that:
1. Contains no unbound variables
2. Infers at least s examples from E+
3. Scores at least t according to the score function
We now propose a novel score function, and empir-
ically validate our choice in Sections 4.3 and 4.4.
3.4 Evaluating Rules by Statistical Relevance
The problem of evaluating candidate rules has been
studied by many researchers, but typically in either a
supervised or propositional context whereas we are
learning first-order Horn-clauses from a noisy set of
positive examples. Moreover, due to the incomplete
nature of the input corpus and the imperfect yield of
1091
extraction?many true facts are not stated explicitly
in the set of ground assertions used by the learner to
evaluate rules.
The absence of negative examples, coupled with
noise, means that standard ILP evaluation functions
(e.g., (Quinlan, 1990) and (Dzeroski and Bratko,
1992)) are not appropriate. Furthermore, when eval-
uating a particular rule with consequent C and an-
tecedent A, it is natural to consider p(C|A) but, due
to missing data, this absolute probability estimate is
often misleading: in many cases C will hold given
A but the fact C is not mentioned in the corpus.
Thus to evaluate rules over extractions, we need
to consider relative probability estimates. I.e., is
p(C|A)  p(C)? If so, then A is said to be sta-
tistically relevant to C (Salmon et al, 1971).
Statistical relevance tries to infer the simplest set
of factors which explain an observation. It can be
viewed as searching for the simplest propositional
Horn-clause which increases the likelihood of a goal
proposition g. The two key ideas in determining sta-
tistical relevance are discovering factors which sub-
stantially increase the likelihood of g (even if the
probabilities are small in an absolute sense), and dis-
missing irrelevant factors.
To illustrate these concepts, consider the follow-
ing example. Suppose our goal is to predict if New
York City will have a storm (S). On an arbitrary
day, the probability of having a storm is fairly low
(p(S)  1). However, if we know that the atmo-
spheric pressure on that day is low, this substantially
increases the probability of having a storm (although
that absolute probability may still be small). Ac-
cording to the principle of statistical relevance, low
atmospheric pressure (LP ) is a factor which predicts
storms (S :- LP ), since p(S|LP ) p(S) .
The principle of statistical relevance also identi-
fies and removes irrelevant factors. For example, let
M denote the gender of New York?s mayor. Since
p(S|LP,M) p(S), it na??vely appears that storms
in New York depend on the gender of the mayor in
addition to the air pressure. The statistical relevance
principle sidesteps this trap by removing any fac-
tors which are conditionally independent of the goal,
given the remaining factors. For example, we ob-
serve p(S|LP )=p(S|LP,M), and so we say that M
is not statistically relevant to S. This test applies Oc-
cam?s razor by searching for the simplest rule which
explains the goal.
Statistical relevance appears useful in the open-
domain context, since all the necessary probabilities
can be estimated from only positive examples. Fur-
thermore, approximating relative probabilities in the
presence of missing data is much more reliable than
determining absolute probabilities.
Unfortunately, Salmon defined statistical rele-
vance in a propositional context. One technical
contribution of our work is to lift statistical rele-
vance to first order Horn-clauses as follows. For
the Horn-clause Head(v1, ..., vn):-Body(v1, ..., vm)
(where Body(v1, ..., vm) is a conjunction of function-
free, non-negated, first-order relations, and vi ? V
is the set of typed variables used in the rule), we say
the body helps explain the head if:
1. Observing an instance of the body substantially
increases the probability of observing the head.
2. The body contains no irrelevant (conditionally
independent) terms.
We evaluate conditional independence of terms
using ILP?s technique of ?-subsumption, ensuring
there is no more general clause that is similarly
predictive of the head. Formally, clause C1 ?-
subsumes clause C2 if and only if there exists a sub-
stitution ? such thatC1? ? C2 where each clause is
treated as the set of its literals. For example, R(x, y)
?-subsumes R(x, x), since {R(x, y)}? ? {R(x, x)}
when ?={y/x}. Intuitively, if C1 ?-subsumes C2,
it means that C1 is more general than C2.
Definition 1 A first-order Horn-clause
Head(...):-Body(...) is statistically relevant if
p(Head(...)|Body(...))  p(Head(...)) and if there
is no clause body B?(...)? ? Body(...) such that
p(Head(...)|Body(...)) ? p(Head(...)|B?(...))
In practice it is difficult to determine the proba-
bilities exactly, so when checking for statistical rele-
vance we ensure that the probability of the rule is at
least a factor t greater than the probability of any
subsuming rule, that is, p(Head(...)|Body(...)) ?
t ? p(Head(...)|B?(...))
We estimate p(Head(...)|B(...)) from the observed
facts by assuming values of Head(...) are generated
by sampling values of B(...) as follows: for variables
vs shared between Head(...) and B(...), we sample
1092
values of vs uniformly from all observed ground-
ings of B(...). For variables vi, if any, that appear
in Head(...) but not in B(...), we sample their values
according to a distribution p(vi|classi). We estimate
p(vi|classi) based on the relative frequency that vi
was extracted using a Hearst pattern with classi.
Finally, we ensure the differences are statistically
significant using the likelihood ratio statistic:
2Nr
X
H(...)?
{Head(...),?Head(...)}
p(H(...)|Body(...)) ? log
p(H(...)|Body(...))
p(H(...)|B?(...))
where p(?Head(...)|B(...)) = 1?p(Head(...)|B(...))
and Nr is the number of results inferred by the
rule Head(...):-Body(...). This test is distributed ap-
proximately as ?2 with one degree of freedom. It
is similar to the statistical significance test used in
mFOIL (Dzeroski and Bratko, 1992), but has two
modifications since SHERLOCK doesn?t have train-
ing data. In lieu of positive and negative examples,
we use whether or not the inferred head value was
observed, and compare against the distribution of a
subsuming clause B?(...) rather than a known prior.
This method of evaluating rules has two impor-
tant differences from ILP under a closed world as-
sumption. First, our probability estimates consider
the fact that examples provide varying amounts of
information. Second, statistical relevance finds rules
with large increases in relative probability, not nec-
essarily a large absolute probability. This is crucial
in an open domain setting where most facts are false,
which means the trivial rule that everything is false
will have high accuracy. Even for true rules, the ob-
served estimates p(Head(...)|Body(...))  1 due to
missing data and noise.
3.5 Making Inferences
In order to benefit from learned rules, we need
an inference engine; with its linear-time scalabil-
ity, HOLMES is a natural choice (Schoenmackers
et al, 2008). As input HOLMES requires a target
atom H(...), an evidence set E and weighted rule
set R as input. It performs a form of knowledge
based model construction (Wellman et al, 1992),
first finding facts using logical inference, then esti-
mating the confidence of each using a Markov Logic
Network (Richardson and Domingos, 2006).
Prior to running inference, it is necessary to assign
a weight to each rule learned by SHERLOCK. Since
the rules and inferences are based on a set of noisy
and incomplete extractions, the algorithms used for
both weight learning and inference should capture
the following characteristics of our problem:
C1. Any arbitrary unknown fact is highly unlikely
to be true.
C2. The more frequently a fact is extracted from the
Web, the more likely it is to be true. However,
facts in E should have a confidence bounded
by a threshold pmax < 1. E contains system-
atic extraction errors, so we want uncertainty in
even the most frequently extracted facts.
C3. An inference that combines uncertain facts
should be less likely than each fact it uses.
Next, we describe the needed modifications to the
weight learning and inference algorithm to achieve
the desired behavior.
3.5.1 Weight Learning
We use the discriminative weight learning proce-
dure described by Huynh and Mooney (2008). Set-
ting the weights involves counting the number of
true groundings for each rule in the data (Richard-
son and Domingos, 2006). However, the noisy na-
ture of Web extractions will make this an overesti-
mate. Consequently, we compute ni(E), the number
of true groundings of rule i, as follows:
ni(E) =
?
j
max
k
?
B(...)?Bodyijk
p(B(...)) (1)
where E is the evidence, j ranges over heads of the
rule, Bodyijk is the body of the kth grounding for
jth head of rule i, and p(B(...)) is approximated us-
ing a logistic function of the number of times B(...)
was extracted,1 scaled to be in the range [0,0.75].
This models C2 by giving increasing but bounded
confidence for more frequently extracted facts. In
practice, this also helps address C3 by giving longer
rules smaller values of ni, which reflects that infer-
ences arrived at through a combination of multiple,
noisy facts should have lower confidence. Longer
rules are also more likely to have multiple ground-
ings that infer a particular head, so keeping only the
most likely grounding prevents a head from receiv-
ing undue weight from any single rule.
1We note that this approximation is equivalent to an MLN
which uses only the two rules defined in 3.5.2
1093
Finally, we place a very strong Gaussian prior
(i.e., L2 penalty) on the weights. Longer rules have a
higher prior to capture the notion that they are more
likely to make incorrect inferences. Without a high
prior, each rule would receive an unduly high weight
as we have no negative examples.
3.5.2 Probabilistic Inference
After learning the weights, we add the following
two rules to our rule set:
1. H(...) with negative weight wprior
2. H(...):-ExtractedFrom(H(...),sentencei)
with weight 1
The first rule models C1, by saying that most facts
are false. The second rule models C2, by stating the
probability of fact depends on the number of times it
was extracted. The weights of these rules are fixed.
We do not include these rules during weight learning
as doing so swamps the effects of the other inference
rules (i.e., forces them to zero).
HOLMES attempts to infer the truth value of each
ground atom H(...) in turn by treating all other ex-
tractionsE in our corpus as evidence. Inference also
requires computing ni(E) which we do according to
Equation 1 as in weight learning.
4 Experiments
One can attempt to evaluate a rule learner by esti-
mating the quality of learned rules, or by measuring
their impact on a system that uses the learned rules.
Since the notion of ?rule quality? is vague except
in the context of an application, we evaluate SHER-
LOCK in the context of the HOLMES inference-based
question answering system.
Our evaluation focuses on three main questions:
1. Does inference utilizing learned Horn rules im-
prove the precision/recall of question answer-
ing and by how much?
2. How do different rule-scoring functions affect
the performance of learning?
3. What role does each of SHERLOCK?s compo-
nents have in the resulting performance?
4.1 Methodology
Our objective with rule learning was to improve the
system?s ability to answer questions such as ?What
foods prevent disease?? So we focus our evaluation
on the task of computing as many instances as pos-
sible of an atomic pattern Rel(x, y). In this exam-
ple, Rel would be bound to ?Prevents?, xwould have
type ?Food? and y would have type ?Disease.?
But which relations should be used in the test?
There is a large variance in behavior across relations,
so examining any particular relation may give mis-
leading results. Instead, we examine the global per-
formance of the system by querying HOLMES for
all open-domain relations identified in Section 3.2
as follows:
1. Score all candidate rules according to the rule
scoring metric M , accept all rules with a score
at least tM (tuned on a small development set of
rules), and learn weights for all accepted rules.
2. Find all facts inferred by the rules and use the
rule weights to estimate the fact probabilities.
3. Reduce type information. For each fact, (e.g.,
BasedIn(Diebold, Ohio)) which has been de-
duced with multiple type signatures (e.g., Ohio
is both a state and a geographic location), keep
only the one with maximum probability (i.e.,
conservatively assuming dependence).
4. Place all results into bins based on their proba-
bilities, and estimate the precision and the num-
ber of correct facts in the bin using a random
sample.
In these experiments we consider rules with up to
k = 2 relations in the body. We use a corpus of
1 million raw extractions, corresponding to 250,000
distinct facts. SHERLOCK found 5 million candidate
rules that infer at least two of the observed facts. Un-
less otherwise noted, we use SHERLOCK?s rule scor-
ing function to evaluate the rules (Section 3.4).
The results represent a wide variety of domains,
covering a total of 10,672 typed relations. We ob-
serve between a dozen and 2,375 distinct, ground
facts for each relation. SHERLOCK learned a total
of 31,000 inference rules.2 Learning all rules, rule
2The learned rules are available at:
http://www.cs.washington.edu/research/sherlock-hornclauses/
1094
 0
 0.2
 0.4
 0.6
 0.8
 1
0 350000 700000 1050000 1400000
P
re
c
is
io
n
 o
f 
In
fe
rr
e
d
 F
a
c
ts
Estimated Number of Correct Facts
Benefits of Inference using Learned Rules
Sherlock With Complex Rules
Sherlock With Only Simple Entailments
No Inference
Inferred by Simple
Entailment Rules
Inferred by 
Multi-Part
Horn Rules
Extracted
Facts
Figure 2: Inference discovers many facts which are not
explicitly extracted, identifying 3x as many high quality
facts (precision 0.8) and more than 5x as many facts over-
all. Horn-clauses with multiple relations in the body in-
fer 30% more correct facts than are identified by simpler
entailment rules, inferring many facts not present in the
corpus in any form.
weights, and performing the inference took 50 min-
utes on a 72 core cluster. However, we note that for
half of the relations SHERLOCK accepts no inference
rules, and remind the reader that the performance on
any particular relation may be substantially differ-
ent, and depends on the facts observed in the corpus
and on the rules learned.
4.2 Benefits of Inference
We first evaluate the utility of the learned Horn rules
by contrasting the precision and number of correct
and incorrect facts identified with and without infer-
ence over learned rules. We compare against two
simpler variants of SHERLOCK. The first is a no-
inference baseline that uses no rules, returning only
facts that are explicitly extracted. The second base-
line only accepts rules of length k = 1, allowing it to
make simple entailments but not more complicated
inferences using multiple facts.
Figure 2 compares the precision and estimated
number of correct facts with and without inference.
As is apparent, the learned inference rules substan-
tially increase the number of known facts, quadru-
pling the number of correct facts beyond what are
explicitly extracted.
The Horn rules having a body-length of two iden-
tify 30% more facts than the simpler length-one
rules. Furthermore, we find the Horn rules yield
slightly increased precision at comparable levels of
recall, although the increase is not statistically sig-
nificant. This behavior can be attributed to learn-
ing smaller weights for the length-two rules than
the length-one rules, allowing the length-two rules
provide a small amount of additional evidence as
to which facts are true, but typically not enough to
overcome the confidence of a more reliable length-
one rule.
Analyzing the errors, we found that about
one third of SHERLOCK?s mistakes are due
to metonymy and word sense ambiguity (e.g.,
confusing Vancouver, British Columbia with
Vancouver, Washington), one third are due to
inferences based on incorrectly-extracted facts
(e.g., inferences based on the incorrect fact
IsLocatedIn(New York, Suffolk County),
which was extracted from sentences like ?Deer
Park, New York is located in Suffolk County?),
and the rest are due to unsound or incorrect
inference rules (e.g., BasedIn(Company, City):-
BasedIn(Company, Country)? CapitalOf(City,
Country)). Without negative examples it is difficult
to distinguish correct rules from these unsound
rules, since the unsound rules are correct more often
than expected by chance.
Finally, we note that although simple, length-one
rules capture many of the results, in some respects
they are just rephrasing facts that are extracted in
another form. However, the more complex, length-
two rules synthesize facts extracted from multiple
pages, and infer results that are not stated anywhere
in the corpus.
4.3 Effect of Scoring Function
We now examine how SHERLOCK?s rule scoring
function affects its results, by comparing it with
three rule scoring functions used in prior work:
LIME. The LIME ILP system (McCreath and
Sharma, 1997) proposed a metric that generalized
Muggleton?s (1997) positive-only score function
by modeling noise and limited sample sizes.
M-Estimate of rule precision. This is a common
approach for handling noise in ILP (Dzeroski and
Bratko, 1992). It requires negative examples,
which we generated by randomly swapping argu-
ments between positive examples.
1095
 0
 0.2
 0.4
 0.6
 0.8
 1
0 500000 1000000 1500000 2000000 2500000
P
re
c
is
io
n
 o
f 
In
fe
rr
e
d
 F
a
c
ts
Estimated Number of Correct Facts
Comparison of Rule Scoring Functions
Sherlock
LIME
M-Estimate
L1 Reg.
Figure 3: SHERLOCK identifies rules that lead to more
accurate inferences over a large set of open-domain ex-
tracted facts, deducing 2x as many facts at precision 0.8.
L1 Regularization. As proposed in (Huynh and
Mooney, 2008), this learns weights for all can-
didate rules using L1-regularization (encouraging
sparsity) instead of L2-regularization, and retains
only those with non-zero weight.
Figure 3 compares the precision and estimated
number of correct facts inferred by the rules of
each scoring function. SHERLOCK has consistently
higher precision, and finds twice as many correct
facts at precision 0.8.
M-Estimate accepted eight times as many rules as
SHERLOCK, increasing the number of inferred facts
at the cost of precision and longer inference times.
Most of the errors in M-Estimate and L1 Regulariza-
tion come from incorrect or unsound rules, whereas
most of the errors for LIME stem from systematic
extraction errors.
4.4 Scoring Function Design Decisions
SHERLOCK requires a rule to have statistical rele-
vance and statistical significance. We perform an
ablation study to understand how each of these con-
tribute to SHERLOCK?s results.
Figure 4 compares the precision and estimated
number of correct facts obtained when requiring
rules to be only statistically relevant, only statisti-
cally significant, or both. As is expected, there is
a precision/recall tradeoff. SHERLOCK has higher
precision, finding more than twice as many results at
precision 0.8 and reducing the error by 39% at a re-
call of 1 million correct facts. Statistical significance
finds twice as many correct facts as SHERLOCK, but
the extra facts it discovers have precision < 0.4.
 0
 0.2
 0.4
 0.6
 0.8
 1
0 500000 1000000 1500000 2000000 2500000 3000000
P
re
c
is
io
n
 o
f 
In
fe
rr
e
d
 F
a
c
ts
Estimated Number of Correct Facts
Design Decisions of Sherlock?s Scoring Function
Sherlock
Statistical Relevance
Statistical Significance
Figure 4: By requiring rules to have both statistical rel-
evance and statistical significance, SHERLOCK rejects
many error-prone rules that are accepted by the metrics
individually. The better rule set yields more accurate in-
ferences, but identifies fewer correct facts.
Comparing the rules accepted in each case, we
found that statistical relevance and statistical signifi-
cance each accepted about 180,000 rules, compared
to about 31,000 for SHERLOCK. The smaller set
of rules accepted by SHERLOCK not only leads to
higher precision inferences, but also speeds up in-
ference time by a factor of seven.
In a qualitative analysis, we found the statisti-
cal relevance metric overestimates probabilities for
sparse rules, which leads to a number of very high
scoring but meaningless rules. The statistical signif-
icance metric handles sparse rules better, but is still
overconfident in the case of many unsound rules.
4.5 Analysis of Weight Learning
Finally, we empirically validate the modifications of
the weight learning algorithm from Section 3.5.1.
The learned-rule weights only affect the probabil-
ities of the inferred facts, not the inferred facts them-
selves, so to measure the influence of the weight
learning algorithm we examine the recall at preci-
sion 0.8 and the area under the precision-recall curve
(AuC). We build a test set by holding SHERLOCK?s
inference rules constant and randomly sampling 700
inferred facts. We test the effects of:
? Fixed vs. Variable Penalty - Do we use the
same L2 penalty on the weights for all rules or
a stronger L2 penalty for longer rules?
? Full vs. Weighted Grounding Counts - Do we
count all unweighted rule groundings (as in
(Huynh and Mooney, 2008)), or only the best
weighted one (as in Equation 1)?
1096
Recall
(p=0.8) AuC
Variable Penalty, Weighted 0.35 0.735
Counts (used by SHERLOCK)
Variable Penalty, Full Counts 0.28 0.726
Fixed Penalty, Weighted Counts 0.27 0.675
Fixed Penalty, Full Counts 0.17 0.488
Table 2: SHERLOCK?s modified weight learning algo-
rithm gives better probability estimates over noisy and in-
complete Web extractions. Most of the gains come from
penalizing longer rules more, but using weighted ground-
ing counts further improves recall by 0.07, which corre-
sponds to almost 100,000 additional facts at precision 0.8.
We vary each of these independently, and give the
performance of all 4 combinations in Table 2.
The modifications from Section 3.5.1 improve
both the AuC and the recall at precision 0.8. Most
of the improvement is due to using stronger penal-
ties on longer rules, but using the weighted counts
in Equation 1 improves recall by a factor of 1.25 at
precision 0.8. While this may not seem like much,
the scale is such that it leads to almost 100,000 ad-
ditional correct facts at precision 0.8.
5 Conclusion
This paper addressed the problem of learning first-
order Horn clauses from the noisy and heteroge-
neous corpus of open-domain facts extracted from
Web text. We showed that SHERLOCK is able
to learn Horn clauses in a large-scale, domain-
independent manner. Furthermore, the learned rules
are valuable, because they infer a substantial number
of facts which were not extracted from the corpus.
While SHERLOCK belongs to the broad category
of ILP learners, it has a number of novel features that
enable it to succeed in the challenging, open-domain
context. First, SHERLOCK automatically identifies
a set of high-quality extracted facts, using several
simple but effective heuristics to defeat noise and
ambiguity. Second, SHERLOCK is unsupervised and
does not require negative examples; this enables it to
scale to an unbounded number of relations. Third, it
utilizes a novel rule-scoring function, which is toler-
ant of the noise, ambiguity, and missing data issues
prevalent in facts extracted from Web text. The ex-
periments in Figure 3 show that, for open-domain
facts, SHERLOCK?s method represents a substantial
improvement over traditional ILP scoring functions.
Directions for future work include inducing
longer inference rules, investigating better methods
for combining the rules, allowing deeper inferences
across multiple rules, evaluating our system on other
corpora and devising better techniques for handling
word sense ambiguity.
Acknowledgements
We thank Sonal Gupta and the anonymous review-
ers for their helpful comments. This research was
supported in part by NSF grant IIS-0803481, ONR
grant N00014-08-1-0431, the WRF / TJ Cable Pro-
fessorship and carried out at the University of Wash-
ington?s Turing Center. The University of Washing-
ton gratefully acknowledges the support of Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract nos. FA8750-
09-C-0179 and FA8750-09-C-0181. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government.
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
M. Craven, D. DiPasquo, D. Freitag, A.K. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to Extract Symbolic Knowledge from the World
Wide Web. In Procs. of the 15th Conference of the
American Association for Artificial Intelligence, pages
509?516, Madison, US. AAAI Press, Menlo Park, US.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
S. Dzeroski and I. Bratko. 1992. Handling noise in in-
ductive logic programming. In Proceedings of the 2nd
1097
International Workshop on Inductive Logic Program-
ming.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545, Nantes, France.
T.N. Huynh and R.J. Mooney. 2008. Discriminative
structure and parameter learning for Markov logic net-
works. In Proceedings of the 25th international con-
ference on Machine learning, pages 416?423. ACM.
Stanley Kok and Pedro Domingos. 2005. Learning the
structure of markov logic networks. In ICML ?05:
Proceedings of the 22nd international conference on
Machine learning, pages 441?448, New York, NY,
USA. ACM.
N. Lavrac and S. Dzeroski, editors. 2001. Relational
Data Mining. Springer-Verlag, Berlin, September.
D. Lin and P. Pantel. 2001. DIRT ? Discovery of Infer-
ence Rules from Text. In KDD.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1?
7.
E. McCreath and A. Sharma. 1997. ILP with noise
and fixed example size: a Bayesian approach. In Pro-
ceedings of the Fifteenth international joint conference
on Artifical intelligence-Volume 2, pages 1310?1315.
Morgan Kaufmann Publishers Inc.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to WordNet: An on-line
lexical database. International Journal of Lexicogra-
phy, 3(4):235?312.
S. Muggleton. 1995. Inverse entailment and Progol.
New Generation Computing, 13:245?286.
S. Muggleton. 1997. Learning from positive data. Lec-
ture Notes in Computer Science, 1314:358?376.
P. Pantel, R. Bhagat, B. Coppola, T. Chklovski, and
E. Hovy. 2007. ISP: Learning inferential selectional
preferences. In Proceedings of NAACL HLT, vol-
ume 7, pages 564?571.
M. Pennacchiotti and F.M. Zanzotto. 2007. Learning
Shallow Semantic Rules for Textual Entailment. Pro-
ceedings of RANLP 2007.
J. R. Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5:239?2666.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proc. of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why,
What, and How?
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107?136.
W.C. Salmon, R.C. Jeffrey, and J.G. Greeno. 1971. Sta-
tistical explanation & statistical relevance. Univ of
Pittsburgh Pr.
S. Schoenmackers, O. Etzioni, and D. Weld. 2008. Scal-
ing Textual Inference to the Web. In Procs. of EMNLP.
Y. Shinyama and S. Sekine. 2006. Preemptive informa-
tion extraction using unrestricted relation discovery.
In Procs. of HLT/NAACL.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
COLING/ACL 2006.
M. Tatu and D. Moldovan. 2007. COGEX at RTE3. In
Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing, pages 22?27.
M.P. Wellman, J.S. Breese, and R.P. Goldman. 1992.
From knowledge bases to decision models. The
Knowledge Engineering Review, 7(1):35?53.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
1098
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1266?1276,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Identifying Functional Relations in Web Text
Thomas Lin, Mausam, Oren Etzioni
Turing Center
University of Washington
Seattle, WA 98195, USA
{tlin,mausam,etzioni}@cs.washington.edu
Abstract
Determining whether a textual phrase denotes
a functional relation (i.e., a relation that maps
each domain element to a unique range el-
ement) is useful for numerous NLP tasks
such as synonym resolution and contradic-
tion detection. Previous work on this prob-
lem has relied on either counting methods or
lexico-syntactic patterns. However, determin-
ing whether a relation is functional, by ana-
lyzing mentions of the relation in a corpus,
is challenging due to ambiguity, synonymy,
anaphora, and other linguistic phenomena.
We present the LEIBNIZ system that over-
comes these challenges by exploiting the syn-
ergy between the Web corpus and freely-
available knowledge resources such as Free-
base. It first computes multiple typed function-
ality scores, representing functionality of the
relation phrase when its arguments are con-
strained to specific types. It then aggregates
these scores to predict the global functionality
for the phrase. LEIBNIZ outperforms previ-
ous work, increasing area under the precision-
recall curve from 0.61 to 0.88. We utilize
LEIBNIZ to generate the first public reposi-
tory of automatically-identified functional re-
lations.
1 Introduction
The paradigm of Open Information Extraction (IE)
(Banko et al, 2007; Banko and Etzioni, 2008) has
scaled extraction technology to the massive set of
relations expressed in Web text. However, additional
work is needed to better understand these relations,
and to place them in richer semantic structures. A
step in that direction is identifying the properties of
these relations, e.g., symmetry, transitivity and our
focus in this paper ? functionality. We refer to this
problem as functionality identification.
A binary relation is functional if, for a given arg1,
there is exactly one unique value for arg2. Exam-
ples of functional relations are father, death date,
birth city, etc. We define a relation phrase to be
functional if all semantic relations commonly ex-
pressed by that phrase are functional. For exam-
ple, we say that the phrase ?was born in? denotes
a functional relation, because the different seman-
tic relations expressed by the phrase (e.g., birth city,
birth year, etc.) are all functional.
Knowing that a relation is functional is helpful
for numerous NLP inference tasks. Previous work
has used functionality for the tasks of contradiction
detection (Ritter et al, 2008), quantifier scope dis-
ambiguation (Srinivasan and Yates, 2009), and syn-
onym resolution (Yates and Etzioni, 2009). It could
also aid in other tasks such as ontology generation
and information extraction. For example, consider
two sentences from a contradiction detection task:
(1) ?George Washington was born in Virginia.? and
(2) ?George Washington was born in Texas.?
As Ritter et al (2008) points out, we can only de-
termine that the two sentences are contradictory if
we know that the semantic relation referred to by
the phrase ?was born in? is functional, and that both
Virginia and Texas are distinct states.
Automatic functionality identification is essential
when dealing with a large number of relations as in
Open IE, or in complex domains where expert help
1266
Distributional
Difference
Assertions
Web
Corpus IE
Functionality prediction for Web relations
Combination Policy
Freebase
Clean 
Lists
type 
listsinstances per relation
functionality scores (typed)
Figure 1: Our system, LEIBNIZ, uses the Web and Free-
base to determine functionality of Web relations.
is scarce or expensive (e.g., biomedical texts). This
paper tackles automatic functionality identification
using Web text. While functionality identification
has been utilized as a module in various NLP sys-
tems, this is the first paper to focus exclusively on
functionality identification as a bona fide NLP infer-
ence task.
It is natural to identify functions based on triples
extracted from text instead of analyzing sentences
directly. Thus, as our input, we utilize tuples ex-
tracted by TEXTRUNNER (Banko and Etzioni, 2008)
when run over a corpus of 500 million webpages.
TEXTRUNNER maps sentences to tuples of the form
<arg1, relation phrase, arg2> and enables our
LEIBNIZ system to focus on the problem of decid-
ing whether the relation phrase is a function.
The naive approach, which classifies a relation
phrase as non-functional if several arg1s have multi-
ple arg2s in our extraction set, fails due to several
reasons: synonymy ? a unique entity may be re-
ferred by multiple strings, polysemy of both entities
and relations ? a unique string may refer to multiple
entities/relations, metaphorical usage, extraction er-
rors and more. These phenomena conspire to make
the functionality determination task inherently sta-
tistical and surprisingly challenging.
In addition, a functional relation phrase may ap-
pear non-functional until we consider the types of its
arguments. In our ?was born in? example, <George
Washington, was born in, 1732> does not contradict
<George Washington, was born in, Virginia> even
though we see two distinct arg2s for the same arg1.
To solve functionality identification, we need to con-
sider typed relations where the relations analyzed
are constrained to have specific argument types.
We develop several approaches to overcome these
challenges. Our first scheme employs approximate
argument merging to overcome the synonymy and
anaphora problems. Our second approach, DIS-
TRDIFF, takes a statistical view of the problem
and learns a separator for the typical count dis-
tributions of functional versus non-functional rela-
tions. Finally, our third and most successful scheme,
CLEANLISTS, identifies and processes a cleaner
subset of the data by intersecting the corpus with en-
tities in a secondary knowledge-base (in our case,
Freebase (Metaweb Technologies, 2009)). Utiliz-
ing pre-defined types, CLEANLISTS first identifies
typed functionality for suitable types for that rela-
tion phrase, and then combines them to output a final
functionality label. LEIBNIZ, a hybrid of CLEAN-
LISTS and DISTRDIFF, returns state-of-the-art re-
sults for our task.
Our work makes the following contributions:
1. We identify several linguistic phenomena that
make the problem of corpus-based functional-
ity identification surprisingly difficult.
2. We designed and implemented three novel
techniques for identifying functionality based
on instance-based counting, distributional dif-
ferences, and use of external knowledge bases.
3. Our best method, LEIBNIZ, outperforms the
existing approaches by wide margins, increas-
ing area under the precision-recall curve from
0.61 to 0.88. It is also capable of distinguishing
functionality of typed relation phrases, when
the arguments are restricted to specific types.
4. Utilizing LEIBNIZ, we created the first public
repository of functional relations.1
2 Related Work
There is a recent surge in large knowledge bases
constructed by human collaboration such as Free-
base (Metaweb Technologies, 2009) and VerbNet
(Kipper-Schuler, 2005). VerbNet annotates its
verbs with several properties but not functionality.
Freebase does annotate some relations with an ?is
unique? property, which is similar to functionality,
but the number of relations in Freebase is still much
1available at http://www.cs.washington.edu/
research/leibniz
1267
George Washington was born in :
Virginia
Westmoreland County
America
a town
a plantation
1732
February
the British colony of Virginia
Rudy Giuliani visited:
Florida
Boca Raton Synagogue
the Florida EvergladesSouth Carolina
Michigan
Republican headquarters
a famous cheesesteak restaurant
Philadelphia
Colonial Beach, Virginia
Figure 2: Sample arg2 values for a non-functional relation (visited) vs. a functional relation (was born in) illustrate
the challenge in discriminating functionality from Web text.
smaller than the hundreds of thousands of relations
existing on the Web, necessitating automatic ap-
proaches to functionality identification.
Discovering functional dependencies has been
recognized as an important database analysis tech-
nique (Huhtala et al, 1999; Yao and Hamilton,
2008), but the database community does not address
any of the linguistic phenomena which make this
a challenging problem in NLP. Three groups of re-
searchers have studied functionality identification in
the context of natural language.
AuContraire (Ritter et al, 2008) is a contradic-
tion detection system that also learns relation func-
tionality. Their approach combines a probabilis-
tic model based on (Downey et al, 2005) with es-
timates on whether each arg1 is ambiguous. The
estimates are used to weight each arg1?s contri-
bution to an overall functionality score for each
relation. Both argument-ambiguity and relation-
functionality are jointly estimated using an EM-like
method. While elegant, AuContraire requires sub-
stantial hand-engineered knowledge, which limits
the scalability of their approach.
Lexico-syntactic patterns: Srinivasan and Yates
(2009) disambiguate a quantifier?s scope by first
making judgments about relation functionality. For
functionality, they look for numeric phrases follow-
ing the relation. For example, the presence of the nu-
meric term ?four? in the sentence ?the fire destroyed
four shops? suggests that destroyed is not functional,
since the same arg1 can destroy multiple things.
The key problem with this approach is that it often
assigns different functionality labels for the present
tense and past tense phrases of the same semantic re-
lation. For example, it will consider ?lived in? to be
non-functional, but ?lives in? to be functional, since
we rarely say ?someone lives in many cities?. Since
both these phrases refer to the same semantic rela-
tion this approach has low precision. Moreover, it
performs poorly for relation phrases that naturally
expect numbers as the target argument (e.g., ?has an
atomic number of?).
While these lexico-syntactic patterns do not per-
form as well for our task, they are well-suited for
identifying whether a verb phrase can take multiple
objects or not. This can be understood as a function-
ality property of the verb phrase within a sentence,
as opposed to functionality of the semantic relation
the phrase represents.
WIE: In a preliminary study, Popescu (2007) ap-
plies an instance based counting approach, but her
relations require manually annotated type restric-
tions, which makes the approach less scalable.
Finally, functionality is just one property of rela-
tions that can be learned from text. A number of
other studies (Guarino and Welty, 2004; Volker et
al., 2005; Culotta et al, 2006) have examined detect-
ing other relation properties from text and applying
them to tasks such as ontology cleaning.
3 Challenges for Functionality Identification
A functional binary relation r is formally defined as
one such that ?x, y1, y2 : r(x, y1)?r(x, y2)? y1 =
y2. We define a relation string to be functional if all
semantic relations commonly expressed by the rela-
tion string are individually functional. Thus, under
our definition, ?was born in? and ?died in? are func-
tional, even though they can take different arg2s for
the same arg1, e.g., year, city, state, country, etc.
The definition of a functional relation suggests a
naive instance-based counting algorithm for identi-
fying functionality. ?Look for the number of arg2s
for each arg1. If all (or most) arg1s have exactly one
arg2, label the relation phrase functional, else, non-
functional.? Unfortunately, this naive algorithm fails
for our task exposing several linguistic phenomena
1268
that make our problem hard (see Figure 2):
Synonymy: Various arg2s for the same arg1 may
refer to the same entity. This makes many func-
tional relations seem non-functional. For instance,
<George Washington, was born in, Virginia> and
<George Washington, was born in, the British
colony of Virginia> are not in conflict. Other
examples of synonyms include ?Windy City? and
?Chicago?; ?3rd March? and ?03/03?, etc.
Anaphora: An entity can be referred to by using
several phrases. For instance,<George Washington,
was born in, a town> does not conflict with his be-
ing born in ?Colonial Beach, Virginia?, since ?town?
is an anaphora for his city of birth. Other examples
include ?The US President? for ?George W. Bush?,
and ?the superpower? to refer to ?United States?. The
effect is similar to that of synonyms ? many relations
incorrectly appear non-functional.
Argument Ambiguity: <George Washington, was
born in, ?Kortrijk, Belgium?> in addition to his be-
ing born in ?Virginia? suggests that ?was born in?
is non-functional. However, the real cause is that
?George Washington? is ambiguous and refers to dif-
ferent people. This ambiguity gets more pronounced
if the person is referred to just by their first (or last
name), e.g., ?Clinton? is commonly used to refer to
both Hillary and Bill Clinton.
Relation Phrase Ambiguity: A relation phrase can
have several senses. For instance ?weighs 80 kilos?
is a different weighs than ?weighs his options?.
Type Restrictions: A closely related problem
is type-variations in the argument. E.g., <George
Washington, was born in, America> vs. <George
Washington, born in, Virginia> both use the same
sense of ?was born in? but refer to different semantic
relations ? one that takes a country in arg2, and the
other that takes a state. Moreover, different argu-
ment types may result in different functionality la-
bels. For example, ?published in? is functional if the
arg2 is a year, but non-functional if it is a language,
since a book could be published in many languages.
We refer to this finer notion of functionality as typed
functionality.
Data Sparsity: There is limited data for more ob-
scure relations instances and non-functional relation
phrases appear functional due to lack of evidence.
Textually Functional Relations: Last but not least,
some relations that are not functional may appear
functional in text. An example is ?collects?. We col-
lect many things, but rarely mention it in text. Usu-
ally, someone?s collection is mentioned in text only
when it makes the news. We name such relations
textually functional. Even though we could build
techniques to reduce the impact of other phenomena,
no instance based counting scheme could overcome
the challenge posed by textually functional relations.
Finally, we note that our functionality predictor
operates over tuples generated by an Open IE sys-
tem. The extractors are not perfect and their errors
can also complicate our analysis.
4 Algorithms
To overcome these challenges, we design three al-
gorithms. Our first algorithm, IBC, applies several
rules to determine whether two arg2s are equal. Our
second algorithm, DISTRDIFF, takes a statistical ap-
proach, and tries to learn a discriminator between
typical count distributions for functional and non-
functional relations. Our final approach, CLEAN-
LISTS, applies counting over a cleaner subset of the
corpus, which is generated based on entities present
in a secondary KB such as Freebase.
From this section onwards, we gloss over the dis-
tinction between a semantic relation and a relation
phrase, since our algorithms do not have access to
relations and operate only at the phrase level. We
use ?relation? to refer to the phrases.
4.1 Instance Based Counting (IBC)
For each relation, IBC computes a global function-
ality score by aggregating local functionality scores
for each arg1. The local functionality for each arg1
computes the fraction of arg2 pairs that refer to the
same entity. To operationalize this computation we
need to identify which arg2s co-refer. Moreover, we
also need to pick an aggregation strategy to combine
local functionality scores.
Data Cleaning: Common nouns in arg1s are of-
ten anaphoras for other entities. For example, <the
company, was headquartered in, ...> refers to dif-
ferent companies in different extractions. To combat
this, IBC restricts arg1s to proper nouns. Secondly,
to counter extraction errors and data bias, it retains
1269
George Washington was born in :
Colonial
Beach
Colonial 
Beach,
Virginia
Westmoreland County, Virginia
February February 1732 1732
Figure 3: IBC judges that Colonial Beach and Westmore-
land County, Virginia refer to the same entity.
an extraction only once per unique sentence. This
reduces the disproportionately large frequencies of
some assertions that are generated from a single ar-
ticle published at multiple websites. Similarly, it al-
lows an extraction only once per website url. More-
over, it filters out any arg1 that does not appear at
least 10 times with that relation.
Equality Checking: This key component judges
if two arg2s refer to the same entity. It first em-
ploys weak typing by disallowing equality checks
across common nouns, proper nouns, dates and
numbers. This mitigates the relation ambiguity
problem, since we never compare ?born in(1732)?
and ?born in(Virginia)?. Within the same category it
judges two arg2s to co-refer if they share a content
word. It also performs a connected component anal-
ysis (Hopcraft and Tarjan, 1973) to take a transitive
closure of arg2s judged equal (see Figure 3).
For example, for the relation ?was named after?
and arg1=?Bluetooth? our corpus has three arg2s:
?Harald Bluetooth?, ?Harald Bluetooth, the King of
Denmark? and ?the King of Denmark?. Our equal-
ity method judges all three as referring to the same
entity. Note that this is a heuristic approach, which
could make mistakes. But for an error, there needs
to be extractions with the same arg1, relation and
similar arg2s. Such cases exist, but are not com-
mon. Our equality checking mitigates the problems
of anaphora, synonymy as well as some typing.
Aggregation: We try several methods to aggre-
gate local functionality scores for each arg1 into a
global score for the relation. These include, a simple
average, a weighted average weighted by frequency
of each arg1, a weighted average weighted by log
of frequency of each arg1, and a Bayesian approach
that estimates the probability that a relation is func-
tional using statistics over a small development set.
Overall, the log-weighting works the best: it assigns
a higher score for popular arguments, but not so high
that it drowns out all the other evidence.
4.2 DISTRDIFF
Our second algorithm, DISTRDIFF, takes a purely
statistical, discriminative view of the problem. It
recognizes that, due to aforementioned reasons,
whether a relation is functional or not, there are
bound to be several arg1s that look locally functional
and several that look locally non-functional. The
difference is in the number of such arg1s ? a func-
tional relation will have more of the former type.
DISTRDIFF studies the count distributions for a
small development set of functional relations (and
similarly for non-functional) and attempts to build
a separator between the two. As an illustration,
Figure 4(a) plots the arg2 counts for various arg1s
for a functional relation (?is headquartered in?).
Each curve represents a unique arg1. For an arg1,
the x-axis represents the rank (based on frequency)
of arg2s and y-axis represents the normalized fre-
quency of the arg2. For example, if an arg1 is found
with just one arg2, then x=1 will match with y=1
(the first point has all the mass) and x=2 will match
with y=0. If, on the other hand, an arg1 is found
with five arg2s, say, appearing ten times each, then
the first five x-points will map to 0.2 and the sixth
point will map to 0.
We illustrate the same plot for a non-functional
relation (?visited?) in Figure 4(b). It is evident from
the two figures that, as one would expect, curves for
most arg1s die early in case of a functional relation,
whereas the lower ranked arg2s are more densely
populated in case of a non-functional relation.
We aggregate this information using slope of the
best-fit line for each arg1 curve. For functional re-
lations, the best-fit lines have steep slopes, whereas
for non-functional the lines are flatter. We bucket the
slopes in integer bins and count the fraction of arg1s
appearing in each bin. This lets us aggregate the
information into a single slope-distribution for each
relation. Bold lines in Figure 4(c) illustrate the aver-
age slope-distributions, averaged over ten sample re-
lations of each kind ? dashed for non-functional and
solid for functional. Most non-functional relations
have a much higher probability of arg1s with low
magnitude slopes, whereas functional relations are
1270
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Arg
um
ent
 2 M
ass
 
Result Position 
is headquartered in (functional) 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Arg
um
ent
 2 M
ass
 
Result Position 
visited (not functional) 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 1 2 3 4 10
Ma
ss 
Floor(|Slope|) 
Average Slope Distributions 
functional average
nonfunctional average
was born on (sample func)
visited (sample nonfunc)
Figure 4: DISTRDIFF: Arg2 count distributions fall more sharply for (a) a sample functional relation, than (b) a
sample non-functional relation. (c) The distance of aggregated slope-distributions from average slope-distributions
can be used to predict the functionality.
the opposite. Notice that the aggregated curve for
?visited? in the figure is closer to the average curve
for non-functional than to functional and vice-versa
for ?was born on?.
We plot the aggregated slope-distributions for
each relation and use the distance from average dis-
tributions as a means to predict the functionality. We
use KL divergence (Kullback and Leibler, 1951) to
compute the distance between two distributions. We
score a relation?s functionality in three ways using:
(1) KLFUNC, its distance from average functional
slope-distribution Favg, (2) KLDIFF, its distance
from average functional minus its distance from av-
erage non-functional Navg, and (3) average of these
two scores. For a relation with slope distribution R,
the scores are computed as:
KLFUNC =
?
iR(i)ln
R(i)
Favg(i)
KLDIFF = KLFUNC - (
?
iR(i)ln
R(i)
Navg(i)
)
Section 5.2 compares the three scoring functions.
A purely statistical approach is resilient to noisy
data, and does not need to explicitly account for the
various issues we detailed earlier. A disadvantage
is that it cannot handle relation ambiguity and type
restrictions. Moreover, we may need to relearn the
separator if applying DISTRDIFF to a corpus with
very different count distributions.
4.3 CLEANLISTS
Our third algorithm, CLEANLISTS, is based on the
intuition that for identifying functionality we need
not reason over all the data in our corpus; instead,
a small but cleaner subset of the data may work
best. This clean subset should ideally be free of syn-
onyms, ambiguities and anaphora, and be typed.
Several knowledge-bases such as Wordnet,
Wikipedia, and Freebase (Fellbaum, 1998;
Wikipedia, 2004; Metaweb Technologies, 2009),
are readily and freely available and they all provide
clean typed lists of entities. In our experiments
CLEANLISTS employs Freebase as a source of
clean lists, but we could use any of these or other
domain-specific ontologies such as SNOMED
(Price and Spackman, 2000) as well.
CLEANLISTS takes the intersection of Freebase
entities with our corpus to generate a clean subset for
functionality analysis. Freebase currently has over
12 million entities in over 1,000 typed lists. Thus,
this intersection retains significant portions of the
useful data, and gets rid of most of anaphora and
synonymy issues. Moreover, by matching against
typed lists, many relation ambiguities are separated
as well, since ambiguous relations often take dif-
ferent types in the arguments (e.g., ?ran(Distance)?
vs. ?ran(Company)?). To mitigate the effect of argu-
ment ambiguity, we additionally get rid of instances
in which arg1s match multiple names in the Freebase
list of names.
As an example, consider the ?was born in? rela-
tion. CLEANLISTS will remove instances with only
?Clinton? in arg1, since it matches multiple people
in Freebase. It will treat the different types, e.g.,
cities, states, countries, months separately and ana-
lyze the functionality for each of these individually.
1271
By intersecting the relation data with argument lists
for these types, we will be left with a smaller, but
much cleaner, subset of relation data, one for each
type. CLEANLISTS analyzes each subset using sim-
ple, instance based counting and computes a typed
functionality score for each type. Thus, it first com-
putes typed functionality for each relation.
There are two subtleties in applying this algo-
rithm. First, we need to identify the set of types to
consider for each relation. Our algorithm currently
picks the types that occur most in each relation?s
observed data. In the future, we could also use a
selectional preferences system (Ritter et al, 2010;
Kozareva and Hovy, 2010). Note that we remove
Freebase types such as Written Work from consid-
eration for containing many entities whose primary
senses are not that type. For example, both ?Al Gore?
and ?William Clinton? are also names of books, but
references in text to these are rarely a reference to
the written work sense.
Secondly, an argument could belong to multiple
Freebase lists. For example, ?California? is both a
city and a state. We apply a simple heuristic: if a
string appears in multiple lists under consideration,
we assign it to the smallest of the lists (the list of
cities is much larger than states). This simple heuris-
tic usually assigns an argument to its intended type.
On a development set, the error rate of this heuristic
is<4%, though it varies a bit depending on the types
involved.
CLEANLISTS determines the overall functional-
ity of a relation string by aggregating the scores for
each type. It outputs functional if a majority of typed
senses for the relation are functional. For example,
CLEANLISTS judges ?was born in? to be functional,
since all relevant type restrictions are individually
typed functional ? everyone is born in exactly one
country, city, state, month, etc.
CLEANLISTS has a much higher precision due to
the intersection with clean lists, though at some cost
of recall. The reason for lower recall is that the ap-
proach has a bias towards types that are easy to enu-
merate. It does not have different distances (e.g., 50
kms, 20 miles, etc.) in its lists. Moreover, arguments
that do not correspond to a noun cannot be handled.
For example, in the sentence, ?He weighed eating
a cheeseburger against eating a salad?, the arg2 of
?weighed? can?t be matched to a Freebase list. To
increase the recall we back off to DISTRDIFF in the
cases when CLEANLISTS is unable to make a pre-
diction. This combination gives the best balance of
precision and recall for our task. We name our final
system LEIBNIZ.
One current limitation is that using only those
arg2s that exactly match clean lists leaves out some
good data (e.g., a tuple with an arg2 of ?Univ of
Wash? will not match against a list of universities
that spells it as ?University of Washington?). Be-
cause we have access to entity types, using typed
equality checkers (Prager et al, 2007) with the clean
lists would allow us to recapture much of this useful
data. Moreover, the knowledge of functions could
apply to building new type nanotheories and reduce
considerable manual effort. We wish to study this in
the future.
5 Evaluation
In our evaluation, we wish to answer three ques-
tions: (1) How do our three approaches, Instance
Based Counting (IBC), DISTRDIFF, and CLEAN-
LISTS, compare on the functionality identification
task? (2) How does our final system, LEIBNIZ,
compare against the existing state of the art tech-
niques? (3) How well is LEIBNIZ able to identify
typed functionality for different types in the same
relation phrase?
5.1 Dataset
For our experiments we test on the set of 887 re-
lations used by Ritter et al (2008) in their exper-
iments. We use the Open IE corpus generated by
running TEXTRUNNER on 500 million high quality
Webpages (Banko and Etzioni, 2008) as the source
of instance data for these relations. Extractor and
corpus differences lead to some relations not occur-
ring (or not occurring with sufficient frequency to
properly analyze, i.e.,? 5 arg1 with? 10 evidence),
leaving a dataset of 629 relations on which to test.
Two human experts tagged these relations for
functionality. Tagging the functionality of relation
phrases can be a bit subjective, as it requires the
experts to imagine the various senses of a phrase
and judge functionality over all those senses. The
inter-annotator agreement between the experts was
95.5%. We limit ourselves to the subset of the data
1272
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
Distr. Diff. Scoring Functions
KLfunc
KLdiff
Average
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
Internal Methods Comparison
IBC
Distr. Diff.
CleanLists
Figure 5: (a) The best scoring method for DISTRDIFF averages KLFUNC and KLDIFF. (b) CLEANLISTS performs
significantly better than DISTRDIFF, which performs significantly better than IBC.
on which the two experts agreed (a subset of 601
relation phrases).
5.2 Internal Comparisons
First, we compare the three scoring functions for
DISTRDIFF. We vary the score thresholds to gener-
ate the different points on the precision-recall curves
for each of the three. Figure 5(a) plots these curves.
It is evident that the hybrid scoring function, i.e.,
one which is an average of KLFUNC (distance from
average functional) and KLDIFF (distance from av-
erage functional minus distance from average non-
functional) performs the best. We use this scoring in
the further experiments involving DISTRDIFF.
Next, we compare our three algorithms on
the dataset. Figure 5(b) reports the results.
CLEANLISTS outperforms DISTRDIFF by vast mar-
gins, covering a 33.5% additional area under the
precision-recall curve. Overall, CLEANLISTS finds
the very high precision points, because of its use of
clean data. However, it is unable to make 23.1% of
the predictions, primarily because the intersection
between the corpus and Freebase entities results in
very little data for those relations. DISTRDIFF per-
forms better than IBC, due to its statistical nature,
but the issues described in Section 3 plague both
these systems much more than CLEANLISTS.
To increase the recall LEIBNIZ uses a combina-
tion of DISTRDIFF and CLEANLISTS, in which the
algorithm backs off to DISTRDIFF if CLEANLISTS
is unable to output a prediction.
5.3 External Comparisons
We next compare LEIBNIZ against the existing state
of the art approaches. Our competitors are AuCon-
traire and NumericTerms (Ritter et al, 2008; Srini-
vasan and Yates, 2009). Because we use the Au-
Contraire dataset, we report the results from their
best performing system. We reimplement a version
of NumericTerms using their list of numeric quanti-
fiers and extraction patterns that best correspond to
our relation format. We run our implementation of
NumericTerms on a dataset of 100 million English
sentences from a crawl of high quality Webpages to
generate the functionality labels.
Figure 6(a) reports the results of this experiment.
We find that LEIBNIZ outperforms AuContraire by
vast margins covering an additional 44% area in the
precision-recall curve. AuContraire?s AUC is 0.61
whereas LEIBNIZ covers 0.88. A Bootstrap Per-
centile Test (Keller et al, 2005) on F1 score found
the improvement of our techniques over AuCon-
traire to be statistically significant at ? = 0.05. Nu-
mericTerms does not perform well, because it makes
decisions based only on the local evidence in a sen-
tence, and does not integrate the knowledge from
different occurrences of the same relation. It returns
many false positives, such as ?lives in?, which ap-
pear functional to the lexico-syntactic pattern, but
are clearly non-functional, e.g., one could live in
many places over a lifetime.
An example of a LEIBNIZ error is the ?repre-
sented? relation. LEIBNIZ classifies this as func-
tional, because it finds several strongly functional
senses (e.g., when a person represents a country),
1273
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
Typed Functionality
AuContraire
NumericTerms
Leibniz
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
External Comparison
AuContraire
NumericTerms
Leibniz
Figure 6: (a) LEIBNIZ, which is a hybrid of CLEANLISTS and DISTRDIFF, achieves 0.88 AUC and outperforms the
0.61 AUC from AuContraire (Ritter et al, 2008) and the 0.05 AUC from NumericTerms (Srinivasan and Yates, 2009).
(b) LEIBNIZ is able to tease apart different senses of polysemous relations much better than other systems.
but the human experts might have had some non-
functional senses in mind while labeling.
5.4 Typed Functionality
Next, we conduct a study of the typed functional-
ity task. We test on ten common polysemous re-
lations, each having both a functional and a non-
functional sense. An example is the ?was pub-
lished in? relation. If arg2 is a year it is func-
tional, e.g. <Harry Potter 5, was published in,
2003>. However, ?was published in(Language)?
is not functional, e.g. <Harry Potter 5, was pub-
lished in, [French / Spanish / English]>. Simi-
larly, ?will become(Company)? is functional because
when a company is renamed, it transitions away
from the old name exactly once, e.g. <Cingular,
will become, AT&T Wireless>. However, ?will be-
come(government title)? is not functional, because
people can hold different offices in their life, e.g.,
<Obama, will become, [Senator / President]>.
In this experiment, a simple baseline of predict-
ing the same label for the two types of each rela-
tion achieves a precision of 0.5. Figure 6(b) presents
the results of this study. AuContraire achieves a flat
0.5, since it cannot distinguish between types. Nu-
mericTerms can be modified to distinguish between
basic types ? check the word just after the numeric
term to see whether it matches the type name. For
example, the modified NumericTerms will search
the Web for instances of ?was published in [nu-
meric term] years? vs. ?was published in [numeric
term] languages?. This scheme works better when
the type name is simple (e.g., languages) rather than
complex (e.g., government titles).
LEIBNIZ performs the best and is able to tease
apart the functionality of various types very well.
When LEIBNIZ did not work, it was generally be-
cause of textual functionality, which is a larger issue
for typed functionality than general functionality. Of
course, these results are merely suggestive ? we per-
form a larger-scale experiment and generate a repos-
itory of typed functions next.
6 A Repository of Functional Relations
We now report on a repository of typed functional
relations generated automatically by applying LEIB-
NIZ to a large collection of relation phrases. Instead
of starting with the most frequent relations from
TEXTRUNNER, we use OCCAM?s relations (Fader
et al, 2010) because they are more specific. For in-
stance, where TEXTRUNNER outputs an underspec-
ified tuple, <Gold, has, an atomic number of 79>,
OCCAM extracts <Gold, has an atomic number of,
79>. OCCAM enables LEIBNIZ to identify far more
functional relations than TEXTRUNNER.
6.1 Addressing Evidence Sparsity
Scaling up to a large collection of typed relations
requires us to consider the size of our data sets. For
example, consider which relation is more likely to be
functional?a relation with 10 instances all of which
indicate functionality versus a relation with 100 in-
stances where 95 behave functionally.
To address this problem, we adapt the likelihood
ratio approach from Schoenmackers et al (2010).
1274
For a typed relation with n instances, f of which in-
dicate functionality, the G-test (Dunning, 1993), G
= 2*(f*ln(f/k)+(n-f)*ln((n-f)/(n-k))), provides a mea-
sure for the likelihood that the relation is not func-
tional. Here k denotes the evidence indicating func-
tionality for the case where the relation is not func-
tional. Setting k = n*0.25 worked well for us. This
G-score replaces our previous metric for scoring
functional relations.
6.2 Evaluation of the Repository
In CLEANLISTS a factor that affects the quality of
the results is the exact set of lists that is used. If
the lists are not clean, results get noisy. For exam-
ple, Freebase?s list of films contains 73,000 entries,
many of which (e.g., ?Egg?) are not films in their pri-
mary senses. Even with heuristics such as assigning
terms to their smallest lists and disqualifying dictio-
nary words that occur from large type lists, there is
still significant noise left.
Using LEIBNIZ with a set of 35 clean lists on
OCCAM?s extraction corpus, we generated a repos-
itory of 5,520 typed functional relations. To eval-
uate this resource a human expert tagged a random
subset of the top 1,000 relations. Of these relations
22% were either ill-formed or had non-sensical type
constraints. From the well-formed typed relations
the precision was estimated to be 0.8. About half
the errors were due to textual functionality and the
rest were LEIBNIZ errors. Some examples of good
functions found include isTheSequelTo(videogame)
and areTheBirthstoneFor(month). An example of
a textually functional relation found is wasThe-
FounderOf(company).
This is the first public repository of automatically-
identified functional relations. Scaling up our data
set forced us to confront new sources of noise in-
cluding extractor errors, errors due to mismatched
types, and errors due to sparse evidence. Still, our
initial results are encouraging and we hope that our
resource will be valuable as a baseline for future
work.
7 Conclusions
Functionality identification is an important subtask
for Web-scale information extraction and other ma-
chine reading tasks. We study the problem of pre-
dicting the functionality of a relation phrase auto-
matically from Web text. We presented three algo-
rithms for this task: (1) instance-based counting, (2)
DISTRDIFF, which takes a statistical approach and
discriminatively classifies the relations using aver-
age arg-distributions, and (3) CLEANLISTS, which
performs instance based counting on a subset of
clean data generated by intersection of the corpus
with a knowledge-base like Freebase.
Our best approach, LEIBNIZ, is a hybrid of
DISTRDIFF and CLEANLISTS, and outperforms
the existing state-of-the-art approaches by covering
44% more area under the precision-recall curve. We
also observe that an important sub-component of
identifying a functional relation phrase is identifying
typed functionality, i.e., functionality when the ar-
guments of the relation phrase are type-constrained.
Because CLEANLISTS is able to use typed lists, it
can successfully identify typed functionality.
We run our techniques on a large set of relations to
output a first repository of typed functional relations.
We release this list for further use by the research
community.2
Future Work: Functionality is one of the sev-
eral properties a relation can possess. Others in-
clude selectional preferences, transitivity (Schoen-
mackers et al, 2008), mutual exclusion, symme-
try, etc. These properties are very useful in increas-
ing our understanding about these Open IE relation
strings. We believe that the general principles devel-
oped in this work, for example, connecting the Open
IE knowledge with an existing knowledge resource,
will come in very handy in identifying these other
properties.
Acknowledgements
We would like to thank Alan Ritter, Alex Yates and
Anthony Fader for access to their data sets. We
would like to thank Stephen Soderland, Yoav Artzi,
and the anonymous reviewers for helpful comments
on previous drafts. This research was supported
in part by NSF grant IIS-0803481, ONR grant
N00014-08-1-0431, DARPA contract FA8750-09-
C-0179, a NDSEG Fellowship, and carried out at
the University of Washington?s Turing Center.
2available at http://www.cs.washington.edu/
research/leibniz
1275
References
M. Banko and O. Etzioni. 2008. The tradeoffs between
open and traditional relation extraction. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL).
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI).
A. Culotta, A. McCallum, and J. Betz. 2006. Integrating
probabilistic extraction models and data mining to dis-
cover relations and patterns in text. In Proceedings of
the HLT-NAACL.
D. Downey, O. Etzioni, and S. Soderland. 2005. A prob-
abilistic model of redundancy in information extrac-
tion. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence (IJCAI).
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. In Computational Linguis-
tics, volume 19.
A. Fader, O. Etzioni, and S. Soderland. 2010. Identi-
fying well-specified relations for open information ex-
traction. (In preparation).
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
N. Guarino and C. Welty. 2004. An overview of On-
toClean. In Handbook of Ontologies in Information
Systems, pages 151?172.
J. Hopcraft and R. Tarjan. 1973. Efficient algorithms
for graph manipulation. Communications of the ACM,
16:372?378.
Y. Huhtala, J. Ka?rkka?inen, P. Porkka, and H. Toivonen.
1999. TANE: An efficient algorithm for discover-
ing functional and approximate dependencies. In The
Computer Journal.
M. Keller, S. Bengio, and S.Y. Wong. 2005. Bench-
marking non-parametric statistical tests. In Advances
in Neural Information Processing Systems (NIPS) 18.
K. Kipper-Schuler. 2005. Verbnet: A broad-coverage,
comprehensive verb lexicon. In Ph.D. thesis. Univer-
sity of Pennsylvania.
Z. Kozareva and E. Hovy. 2010. Learning arguments
and supertypes of semantic relations using recursive
patterns. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL).
S. Kullback and R.A. Leibler. 1951. On information
and sufficiency. Annals of Mathematical Statistics,
22(1):79?86.
Metaweb Technologies. 2009. Freebase data dumps. In
http://download.freebase.com/datadumps/.
A-M. Popescu. 2007. Information extraction from un-
structured web text. In Ph.D. thesis. University of
Washington.
J. Prager, S. Luger, and J. Chu-Carroll. 2007. Type nan-
otheories: a framework for term comparison. In Pro-
ceedings of the 16th ACM Conference on Information
and Knowledge Management (CIKM).
C. Price and K. Spackman. 2000. SNOMED clincal
terms. In British Journal of Healthcare Computing &
Information Management, volume 17.
A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
2008. It?s a contradiction - no, it?s not: A case study
using functional relations. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
A. Ritter, Mausam, and O. Etzioni. 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
S. Schoenmackers, O. Etzioni, and D. Weld. 2008. Scal-
ing textual inference to the web. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
S. Schoenmackers, J. Davis, O. Etzioni, and D. Weld.
2010. Learning first-order horn clauses from web text.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
P. Srinivasan and A. Yates. 2009. Quantifier scope
disambiguation using extracted pragmatic knowledge:
Preliminary results. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).
J. Volker, D. Vrandecic, and Y. Sure. 2005. Auto-
matic evaluation of ontologies (AEON). In Proceed-
ings of the 4th International Semantic Web Conference
(ISWC).
Wikipedia. 2004. Wikipedia: The free encyclopedia. In
http://www.wikipedia.org. Wikimedia Foundation.
H. Yao and H. Hamilton. 2008. Mining functional de-
pendencies from data. In Data Mining and Knowledge
Discovery.
A. Yates and O. Etzioni. 2009. Unsupervised methods
for determining object and relation synonyms on the
web. In Journal of Artificial Intelligence Research,
volume 34, pages 255?296.
1276
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524?1534,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Named Entity Recognition in Tweets:
An Experimental Study
Alan Ritter, Sam Clark, Mausam and Oren Etzioni
Computer Science and Engineering
University of Washington
Seattle, WA 98125, USA
{aritter,ssclark,mausam,etzioni}@cs.washington.edu
Abstract
People tweet more than 100 Million times
daily, yielding a noisy, informal, but some-
times informative corpus of 140-character
messages that mirrors the zeitgeist in an un-
precedented manner. The performance of
standard NLP tools is severely degraded on
tweets. This paper addresses this issue by
re-building the NLP pipeline beginning with
part-of-speech tagging, through chunking, to
named-entity recognition. Our novel T-NER
system doubles F1 score compared with the
Stanford NER system. T-NER leverages the
redundancy inherent in tweets to achieve this
performance, using LabeledLDA to exploit
Freebase dictionaries as a source of distant
supervision. LabeledLDA outperforms co-
training, increasing F1 by 25% over ten com-
mon entity types.
Our NLP tools are available at: http://
github.com/aritter/twitter_nlp
1 Introduction
Status Messages posted on Social Media websites
such as Facebook and Twitter present a new and
challenging style of text for language technology
due to their noisy and informal nature. Like SMS
(Kobus et al, 2008), tweets are particularly terse
and difficult (See Table 1). Yet tweets provide a
unique compilation of information that is more up-
to-date and inclusive than news articles, due to the
low-barrier to tweeting, and the proliferation of mo-
bile devices.1 The corpus of tweets already exceeds
1See the ?trending topics? displayed on twitter.com
the size of the Library of Congress (Hachman, 2011)
and is growing far more rapidly. Due to the vol-
ume of tweets, it is natural to consider named-entity
recognition, information extraction, and text mining
over tweets. Not surprisingly, the performance of
?off the shelf? NLP tools, which were trained on
news corpora, is weak on tweet corpora.
In response, we report on a re-trained ?NLP
pipeline? that leverages previously-tagged out-of-
domain text, 2 tagged tweets, and unlabeled tweets
to achieve more effective part-of-speech tagging,
chunking, and named-entity recognition.
1 The Hobbit has FINALLY started filming! I
cannot wait!
2 Yess! Yess! Its official Nintendo announced
today that they Will release the Nintendo 3DS
in north America march 27 for $250
3 Government confirms blast n nuclear plants n
japan...don?t knw wht s gona happen nw...
Table 1: Examples of noisy text in tweets.
We find that classifying named entities in tweets is
a difficult task for two reasons. First, tweets contain
a plethora of distinctive named entity types (Compa-
nies, Products, Bands, Movies, and more). Almost
all these types (except for People and Locations) are
relatively infrequent, so even a large sample of man-
ually annotated tweets will contain few training ex-
amples. Secondly, due to Twitter?s 140 character
limit, tweets often lack sufficient context to deter-
mine an entity?s type without the aid of background
2Although tweets can be written on any subject, following
convention we use the term ?domain? to include text styles or
genres such as Twitter, News or IRC Chat.
1524
knowledge.
To address these issues we propose a distantly su-
pervised approach which applies LabeledLDA (Ra-
mage et al, 2009) to leverage large amounts of unla-
beled data in addition to large dictionaries of entities
gathered from Freebase, and combines information
about an entity?s context across its mentions.
We make the following contributions:
1. We experimentally evaluate the performance of
off-the-shelf news trained NLP tools when ap-
plied to Twitter. For example POS tagging
accuracy drops from about 0.97 on news to
0.80 on tweets. By utilizing in-domain, out-
of-domain, and unlabeled data we are able to
substantially boost performance, for example
obtaining a 52% increase in F1 score on seg-
menting named entities.
2. We introduce a novel approach to distant super-
vision (Mintz et al, 2009) using Topic Models.
LabeledLDA is applied, utilizing constraints
based on an open-domain database (Freebase)
as a source of supervision. This approach in-
creases F1 score by 25% relative to co-training
(Blum and Mitchell, 1998; Yarowsky, 1995) on
the task of classifying named entities in Tweets.
The rest of the paper is organized as follows.
We successively build the NLP pipeline for Twitter
feeds in Sections 2 and 3. We first present our ap-
proaches to shallow syntax ? part of speech tagging
(?2.1), and shallow parsing (?2.2). ?2.3 describes a
novel classifier that predicts the informativeness of
capitalization in a tweet. All tools in ?2 are used
as features for named entity segmentation in ?3.1.
Next, we present our algorithms and evaluation for
entity classification (?3.2). We describe related work
in ?4 and conclude in ?5.
2 Shallow Syntax in Tweets
We first study two fundamental NLP tasks ? POS
tagging and noun-phrase chunking. We also discuss
a novel capitalization classifier in ?2.3. The outputs
of all these classifiers are used in feature generation
for named entity recognition in the next section.
For all experiments in this section we use a dataset
of 800 randomly sampled tweets. All results (Tables
Accuracy Error
Reduction
Majority Baseline (NN) 0.189 -
Word?s Most Frequent Tag 0.760 -
Stanford POS Tagger 0.801 -
T-POS(PTB) 0.813 6%
T-POS(Twitter) 0.853 26%
T-POS(IRC + PTB) 0.869 34%
T-POS(IRC + Twitter) 0.870 35%
T-POS(PTB + Twitter) 0.873 36%
T-POS(PTB + IRC + Twitter) 0.883 41%
Table 2: POS tagging performance on tweets. By training
on in-domain labeled data, in addition to annotated IRC
chat data, we obtain a 41% reduction in error over the
Stanford POS tagger.
2, 4 and 5) represent 4-fold cross-validation experi-
ments on the respective tasks.3
2.1 Part of Speech Tagging
Part of speech tagging is applicable to a wide range
of NLP tasks including named entity segmentation
and information extraction.
Prior experiments have suggested that POS tag-
ging has a very strong baseline: assign each word
to its most frequent tag and assign each Out of Vo-
cabulary (OOV) word the most common POS tag.
This baseline obtained a 0.9 accuracy on the Brown
corpus (Charniak et al, 1993). However, the appli-
cation of a similar baseline on tweets (see Table 2)
obtains a much weaker 0.76, exposing the challeng-
ing nature of Twitter data.
A key reason for this drop in accuracy is that Twit-
ter contains far more OOV words than grammatical
text. Many of these OOV words come from spelling
variation, e.g., the use of the word ?n? for ?in? in Ta-
ble 1 example 3. Although NNP is the most frequent
tag for OOV words, only about 1/3 are NNPs.
The performance of off-the-shelf news-trained
POS taggers also suffers on Twitter data. The state-
of-the-art Stanford POS tagger (Toutanova et al,
2003) improves on the baseline, obtaining an accu-
racy of 0.8. This performance is impressive given
that its training data, the Penn Treebank WSJ (PTB),
is so different in style from Twitter, however it is a
huge drop from the 97% accuracy reported on the
3We used Brendan O?Connor?s Twitter tokenizer
1525
Gold Predicted Stanford
Error
T-POS Error Error
Reduction
NN NNP 0.102 0.072 29%
UH NN 0.387 0.047 88%
VB NN 0.071 0.032 55%
NNP NN 0.130 0.125 4%
UH NNP 0.200 0.036 82%
Table 3: Most common errors made by the Stanford POS
Tagger on tweets. For each case we list the fraction of
times the gold tag is misclassified as the predicted for
both our system and the Stanford POS tagger. All verbs
are collapsed into VB for compactness.
PTB. There are several reasons for this drop in per-
formance. Table 3 lists common errors made by
the Stanford tagger. First, due to unreliable capi-
talization, common nouns are often misclassified as
proper nouns, and vice versa. Also, interjections
and verbs are frequently misclassified as nouns. In
addition to differences in vocabulary, the grammar
of tweets is quite different from edited news text.
For instance, tweets often start with a verb (where
the subject ?I? is implied), as in: ?watchng american
dad.?
To overcome these differences in style and vocab-
ulary, we manually annotated a set of 800 tweets
(16K tokens) with tags from the Penn TreeBank tag
set for use as in-domain training data for our POS
tagging system, T-POS.4 We add new tags for the
Twitter specific phenomena: retweets, @usernames,
#hashtags, and urls. Note that words in these cate-
gories can be tagged with 100% accuracy using sim-
ple regular expressions. To ensure fair comparison
in Table 2, we include a postprocessing step which
tags these words appropriately for all systems.
To help address the issue of OOV words and
lexical variations, we perform clustering to group
together words which are distributionally similar
(Brown et al, 1992; Turian et al, 2010). In particu-
lar, we perform hierarchical clustering using Jcluster
(Goodman, 2001) on 52 million tweets; each word
is uniquely represented by a bit string based on the
path from the root of the resulting hierarchy to the
word?s leaf. We use the Brown clusters resulting
from prefixes of 4, 8, and 12 bits. These clusters are
often effective in capturing lexical variations, for ex-
4Using MMAX2 (Mu?ller and Strube, 2006) for annotation.
ample, following are lexical variations on the word
?tomorrow? from one cluster after filtering out other
words (most of which refer to days):
?2m?, ?2ma?, ?2mar?, ?2mara?, ?2maro?,
?2marrow?, ?2mor?, ?2mora?, ?2moro?, ?2mo-
row?, ?2morr?, ?2morro?, ?2morrow?, ?2moz?,
?2mr?, ?2mro?, ?2mrrw?, ?2mrw?, ?2mw?,
?tmmrw?, ?tmo?, ?tmoro?, ?tmorrow?, ?tmoz?,
?tmr?, ?tmro?, ?tmrow?, ?tmrrow?, ?tm-
rrw?, ?tmrw?, ?tmrww?, ?tmw?, ?tomaro?,
?tomarow?, ?tomarro?, ?tomarrow?, ?tomm?,
?tommarow?, ?tommarrow?, ?tommoro?, ?tom-
morow?, ?tommorrow?, ?tommorw?, ?tomm-
row?, ?tomo?, ?tomolo?, ?tomoro?, ?tomorow?,
?tomorro?, ?tomorrw?, ?tomoz?, ?tomrw?,
?tomz?
T-POS uses Conditional Random Fields5 (Laf-
ferty et al, 2001), both because of their ability to
model strong dependencies between adjacent POS
tags, and also to make use of highly correlated fea-
tures (for example a word?s identity in addition to
prefixes and suffixes). Besides employing the Brown
clusters computed above, we use a fairly standard set
of features that include POS dictionaries, spelling
and contextual features.
On a 4-fold cross validation over 800 tweets,
T-POS outperforms the Stanford tagger, obtaining a
26% reduction in error. In addition we include 40K
tokens of annotated IRC chat data (Forsythand and
Martell, 2007), which is similar in style. Like Twit-
ter, IRC data contains many misspelled/abbreviated
words, and also more pronouns, and interjections,
but fewer determiners than news. Finally, we also
leverage 50K POS-labeled tokens from the Penn
Treebank (Marcus et al, 1994).
Overall T-POS trained on 102K tokens (12K from
Twitter, 40K from IRC and 50K from PTB) results
in a 41% error reduction over the Stanford tagger,
obtaining an accuracy of 0.883. Table 3 lists gains
on some of the most common error types, for ex-
ample, T-POS dramatically reduces error on inter-
jections and verbs that are incorrectly classified as
nouns by the Stanford tagger.
2.2 Shallow Parsing
Shallow parsing, or chunking is the task of identi-
fying non-recursive phrases, such as noun phrases,
5We use MALLET (McCallum, 2002).
1526
Accuracy Error
Reduction
Majority Baseline (B-NP) 0.266 -
OpenNLP 0.839 -
T-CHUNK(CoNLL) 0.854 9%
T-CHUNK(Twitter) 0.867 17%
T-CHUNK(CoNLL + Twitter) 0.875 22%
Table 4: Token-Level accuracy at shallow parsing tweets.
We compare against the OpenNLP chunker as a baseline.
verb phrases, and prepositional phrases in text. Ac-
curate shallow parsing of tweets could benefit sev-
eral applications such as Information Extraction and
Named Entity Recognition.
Off the shelf shallow parsers perform noticeably
worse on tweets, motivating us again to annotate in-
domain training data. We annotate the same set of
800 tweets mentioned previously with tags from the
CoNLL shared task (Tjong Kim Sang and Buchholz,
2000). We use the set of shallow parsing features de-
scribed by Sha and Pereira (2003), in addition to the
Brown clusters mentioned above. Part-of-speech tag
features are extracted based on cross-validation out-
put predicted by T-POS. For inference and learning,
again we use Conditional Random Fields. We utilize
16K tokens of in-domain training data (using cross
validation), in addition to 210K tokens of newswire
text from the CoNLL dataset.
Table 4 reports T-CHUNK?s performance at shal-
low parsing of tweets. We compare against the off-
the shelf OpenNLP chunker6, obtaining a 22% re-
duction in error.
2.3 Capitalization
A key orthographic feature for recognizing named
entities is capitalization (Florian, 2002; Downey et
al., 2007). Unfortunately in tweets, capitalization
is much less reliable than in edited texts. In addi-
tion, there is a wide variety in the styles of capital-
ization. In some tweets capitalization is informative,
whereas in other cases, non-entity words are capital-
ized simply for emphasis. Some tweets contain all
lowercase words (8%), whereas others are in ALL
CAPS (0.6%).
To address this issue, it is helpful to incorporate
information based on the entire content of the mes-
6http://incubator.apache.org/opennlp/
P R F1
Majority Baseline 0.70 1.00 0.82
T-CAP 0.77 0.98 0.86
Table 5: Performance at predicting reliable capitalization.
sage to determine whether or not its capitalization
is informative. To this end, we build a capitaliza-
tion classifier, T-CAP, which predicts whether or not
a tweet is informatively capitalized. Its output is
used as a feature for Named Entity Recognition. We
manually labeled our 800 tweet corpus as having
either ?informative? or ?uninformative? capitaliza-
tion. The criteria we use for labeling is as follows:
if a tweet contains any non-entity words which are
capitalized, but do not begin a sentence, or it con-
tains any entities which are not capitalized, then its
capitalization is ?uninformative?, otherwise it is ?in-
formative?.
For learning , we use Support Vector Ma-
chines.7 The features used include: the frac-
tion of words in the tweet which are capitalized,
the fraction which appear in a dictionary of fre-
quently lowercase/capitalized words but are not low-
ercase/capitalized in the tweet, the number of times
the word ?I? appears lowercase and whether or not
the first word in the tweet is capitalized. Results
comparing against the majority baseline, which pre-
dicts capitalization is always informative, are shown
in Table 5. Additionally, in ?3 we show that fea-
tures based on our capitalization classifier improve
performance at named entity segmentation.
3 Named Entity Recognition
We now discuss our approach to named entity recog-
nition on Twitter data. As with POS tagging and
shallow parsing, off the shelf named-entity recog-
nizers perform poorly on tweets. For example, ap-
plying the Stanford Named Entity Recognizer to one
of the examples from Table 1 results in the following
output:
[Yess]ORG! [Yess]ORG! Its official
[Nintendo]LOC announced today that they
Will release the [Nintendo]ORG 3DS in north
[America]LOC march 27 for $250
7http://www.chasen.org/?taku/software/
TinySVM/
1527
The OOV word ?Yess? is mistaken as a named en-
tity. In addition, although the first occurrence of
?Nintendo? is correctly segmented, it is misclassi-
fied, whereas the second occurrence is improperly
segmented ? it should be the product ?Nintendo
3DS?. Finally ?north America? should be segmented
as a LOCATION, rather than just ?America?. In gen-
eral, news-trained Named Entity Recognizers seem
to rely heavily on capitalization, which we know to
be unreliable in tweets.
Following Collins and Singer (1999), Downey et
al. (2007) and Elsner et al (2009), we treat classi-
fication and segmentation of named entities as sepa-
rate tasks. This allows us to more easily apply tech-
niques better suited towards each task. For exam-
ple, we are able to use discriminative methods for
named entity segmentation and distantly supervised
approaches for classification. While it might be ben-
eficial to jointly model segmentation and (distantly
supervised) classification using a joint sequence la-
beling and topic model similar to that proposed by
Sauper et al (2010), we leave this for potential fu-
ture work.
Because most words found in tweets are not part
of an entity, we need a larger annotated dataset to ef-
fectively learn a model of named entities. We there-
fore use a randomly sampled set of 2,400 tweets for
NER. All experiments (Tables 6, 8-10) report results
using 4-fold cross validation.
3.1 Segmenting Named Entities
Because capitalization in Twitter is less informative
than news, in-domain data is needed to train models
which rely less heavily on capitalization, and also
are able to utilize features provided by T-CAP.
We exhaustively annotated our set of 2,400 tweets
(34K tokens) with named entities.8 A convention on
Twitter is to refer to other users using the @ sym-
bol followed by their unique username. We deliber-
ately choose not to annotate @usernames as entities
in our data set because they are both unambiguous,
and trivial to identify with 100% accuracy using a
simple regular expression, and would only serve to
inflate our performance statistics. While there is am-
biguity as to the type of @usernames (for example,
8We found that including out-of-domain training data from
the MUC competitions lowered performance at this task.
P R F1 F1 inc.
Stanford NER 0.62 0.35 0.44 -
T-SEG(None) 0.71 0.57 0.63 43%
T-SEG(T-POS) 0.70 0.60 0.65 48%
T-SEG(T-POS, T-CHUNK) 0.71 0.61 0.66 50%
T-SEG(All Features) 0.73 0.61 0.67 52%
Table 6: Performance at segmenting entities varying the
features used. ?None? removes POS, Chunk, and capital-
ization features. Overall we obtain a 52% improvement
in F1 score over the Stanford Named Entity Recognizer.
they can refer to people or companies), we believe
they could be more easily classified using features
of their associated user?s profile than contextual fea-
tures of the text.
T-SEG models Named Entity Segmentation as a
sequence-labeling task using IOB encoding for rep-
resenting segmentations (each word either begins, is
inside, or is outside of a named entity), and uses
Conditional Random Fields for learning and infer-
ence. Again we include orthographic, contextual
and dictionary features; our dictionaries included a
set of type lists gathered from Freebase. In addition,
we use the Brown clusters and outputs of T-POS,
T-CHUNK and T-CAP in generating features.
We report results at segmenting named entities in
Table 6. Compared with the state-of-the-art news-
trained Stanford Named Entity Recognizer (Finkel
et al, 2005), T-SEG obtains a 52% increase in F1
score.
3.2 Classifying Named Entities
Because Twitter contains many distinctive, and in-
frequent entity types, gathering sufficient training
data for named entity classification is a difficult task.
In any random sample of tweets, many types will
only occur a few times. Moreover, due to their
terse nature, individual tweets often do not contain
enough context to determine the type of the enti-
ties they contain. For example, consider following
tweet:
KKTNY in 45min..........
without any prior knowledge, there is not enough
context to determine what type of entity ?KKTNY?
refers to, however by exploiting redundancy in the
data (Downey et al, 2010), we can determine it is
likely a reference to a television show since it of-
1528
ten co-occurs with words such as watching and pre-
mieres in other contexts.9
In order to handle the problem of many infre-
quent types, we leverage large lists of entities and
their types gathered from an open-domain ontology
(Freebase) as a source of distant supervision, allow-
ing use of large amounts of unlabeled data in learn-
ing.
Freebase Baseline: Although Freebase has very
broad coverage, simply looking up entities and their
types is inadequate for classifying named entities in
context (0.38 F-score, ?3.2.1). For example, accord-
ing to Freebase, the mention ?China? could refer to
a country, a band, a person, or a film. This prob-
lem is very common: 35% of the entities in our data
appear in more than one of our (mutually exclusive)
Freebase dictionaries. Additionally, 30% of entities
mentioned on Twitter do not appear in any Freebase
dictionary, as they are either too new (for example a
newly released videogame), or are misspelled or ab-
breviated (for example ?mbp? is often used to refer
to the ?mac book pro?).
Distant Supervision with Topic Models: To
model unlabeled entities and their possible types, we
apply LabeledLDA (Ramage et al, 2009), constrain-
ing each entity?s distribution over topics based on
its set of possible types according to Freebase. In
contrast to previous weakly supervised approaches
to Named Entity Classification, for example the Co-
Training and Na??ve Bayes (EM) models of Collins
and Singer (1999), LabeledLDA models each entity
string as a mixture of types rather than using a single
hidden variable to represent the type of each men-
tion. This allows information about an entity?s dis-
tribution over types to be shared across mentions,
naturally handling ambiguous entity strings whose
mentions could refer to different types.
Each entity string in our data is associated with a
bag of words found within a context window around
all of its mentions, and also within the entity itself.
As in standard LDA (Blei et al, 2003), each bag of
words is associated with a distribution over topics,
Multinomial(?e), and each topic is associated with a
distribution over words, Multinomial(?t). In addi-
tion, there is a one-to-one mapping between topics
and Freebase type dictionaries. These dictionaries
9Kourtney & Kim Take New York.
constrain ?e, the distribution over topics for each en-
tity string, based on its set of possible types, FB[e].
For example, ?Amazon could correspond to a distribu-
tion over two types: COMPANY, and LOCATION,
whereas ?Apple might represent a distribution over
COMPANY, and FOOD. For entities which aren?t
found in any of the Freebase dictionaries, we leave
their topic distributions ?e unconstrained. Note that
in absence of any constraints LabeledLDA reduces
to standard LDA, and a fully unsupervised setting
similar to that presented by Elsner et. al. (2009).
In detail, the generative process that models our
data for Named Entity Classification is as follows:
for each type: t = 1 . . . T do
Generate ?t according to symmetric Dirichlet
distribution Dir(?).
end for
for each entity string e = 1 . . . |E| do
Generate ?e over FB[e] according to Dirichlet
distribution Dir(?FB[e]).
for each word position i = 1 . . . Ne do
Generate ze,i from Mult(?e).
Generate the word we,i from Mult(?ze,i).
end for
end for
To infer values for the hidden variables, we apply
Collapsed Gibbs sampling (Griffiths and Steyvers,
2004), where parameters are integrated out, and the
ze,is are sampled directly.
In making predictions, we found it beneficial to
consider ?traine as a prior distribution over types forentities which were encountered during training. In
practice this sharing of information across contexts
is very beneficial as there is often insufficient evi-
dence in an isolated tweet to determine an entity?s
type. For entities which weren?t encountered dur-
ing training, we instead use a prior based on the dis-
tribution of types across all entities. One approach
to classifying entities in context is to assume that
?traine is fixed, and that all of the words inside theentity mention and context, w, are drawn based on
a single topic, z, that is they are all drawn from
Multinomial(?z). We can then compute the poste-
rior distribution over types in closed form with a
simple application of Bayes rule:
P (z|w) ?
?
w?w
P (w|z : ?)P (z : ?traine )
During development, however, we found that rather
than making these assumptions, using Gibbs Sam-
1529
Type Top 20 Entities not found in Freebase dictionaries
PRODUCT nintendo ds lite, apple ipod, generation black, ipod nano, apple iphone, gb black, xperia, ipods, verizon
media, mac app store, kde, hd video, nokia n8, ipads, iphone/ipod, galaxy tab, samsung galaxy, playstation
portable, nintendo ds, vpn
TV-SHOW pretty little, american skins, nof, order svu, greys, kktny, rhobh, parks & recreation, parks & rec, dawson
?s creek, big fat gypsy weddings, big fat gypsy wedding, winter wipeout, jersey shores, idiot abroad, royle,
jerseyshore, mr . sunshine, hawaii five-0, new jersey shore
FACILITY voodoo lounge, grand ballroom, crash mansion, sullivan hall, memorial union, rogers arena, rockwood
music hall, amway center, el mocambo, madison square, bridgestone arena, cat club, le poisson rouge,
bryant park, mandalay bay, broadway bar, ritz carlton, mgm grand, olympia theatre, consol energy center
Table 7: Example type lists produced by LabeledLDA. No entities which are shown were found in Freebase; these are
typically either too new to have been added, or are misspelled/abbreviated (for example rhobh=?Real Housewives of
Beverly Hills?). In a few cases there are segmentation errors.
pling to estimate the posterior distribution over types
performs slightly better. In order to make predic-
tions, for each entity we use an informative Dirich-
let prior based on ?traine and perform 100 iterations of
Gibbs Sampling holding the hidden topic variables
in the training data fixed (Yao et al, 2009). Fewer
iterations are needed than in training since the type-
word distributions, ? have already been inferred.
3.2.1 Classification Experiments
To evaluate T-CLASS?s ability to classify entity
mentions in context, we annotated the 2,400 tweets
with 10 types which are both popular on Twitter,
and have good coverage in Freebase: PERSON,
GEO-LOCATION, COMPANY, PRODUCT, FACIL-
ITY, TV-SHOW, MOVIE, SPORTSTEAM, BAND,
and OTHER. Note that these type annotations are
only used for evaluation purposes, and not used dur-
ing training T-CLASS, which relies only on distant
supervision. In some cases, we combine multi-
ple Freebase types to create a dictionary of entities
representing a single type (for example the COM-
PANY dictionary contains Freebase types /busi-
ness/consumer company and /business/brand). Be-
cause our approach does not rely on any manually
labeled examples, it is straightforward to extend it
for a different sets of types based on the needs of
downstream applications.
Training: To gather unlabeled data for inference,
we run T-SEG, our entity segmenter (from ?3.1), on
60M tweets, and keep the entities which appear 100
or more times. This results in a set of 23,651 dis-
tinct entity strings. For each entity string, we col-
lect words occurring in a context window of 3 words
from all mentions in our data, and use a vocabulary
of the 100K most frequent words. We run Gibbs
sampling for 1,000 iterations, using the last sample
to estimate entity-type distributions ?e, in addition
to type-word distributions ?t. Table 7 displays the
20 entities (not found in Freebase) whose posterior
distribution ?e assigns highest probability to selected
types.
Results: Table 8 presents the classification re-
sults of T-CLASS compared against a majority base-
line which simply picks the most frequent class
(PERSON), in addition to the Freebase baseline,
which only makes predictions if an entity appears
in exactly one dictionary (i.e., appears unambigu-
ous). T-CLASS also outperforms a simple super-
vised baseline which applies a MaxEnt classifier us-
ing 4-fold cross validation over the 1,450 entities
which were annotated for testing. Additionally we
compare against the co-training algorithm of Collins
and Singer (1999) which also leverages unlabeled
data and uses our Freebase type lists; for seed rules
we use the ?unambiguous? Freebase entities. Our
results demonstrate that T-CLASS outperforms the
baselines and achieves a 25% increase in F1 score
over co-training.
Tables 9 and 10 present a breakdown of F1 scores
by type, both collapsing types into the standard
classes used in the MUC competitions (PERSON,
LOCATION, ORGANIZATION), and using the 10
popular Twitter types described earlier.
Entity Strings vs. Entity Mentions: DL-Cotrain
and LabeledLDA use two different representations
for the unlabeled data during learning. LabeledLDA
groups together words across all mentions of an en-
1530
System P R F1
Majority Baseline 0.30 0.30 0.30
Freebase Baseline 0.85 0.24 0.38
Supervised Baseline 0.45 0.44 0.45
DL-Cotrain 0.54 0.51 0.53
LabeledLDA 0.72 0.60 0.66
Table 8: Named Entity Classification performance on the
10 types. Assumes segmentation is given as in (Collins
and Singer, 1999), and (Elsner et al, 2009).
Type LL FB CT SP N
PERSON 0.82 0.48 0.65 0.83 436
LOCATION 0.74 0.21 0.55 0.67 372
ORGANIZATION 0.66 0.52 0.55 0.31 319
overall 0.75 0.39 0.59 0.49 1127
Table 9: F1 classification scores for the 3 MUC types
PERSON, LOCATION, ORGANIZATION. Results are
shown using LabeledLDA (LL), Freebase Baseline (FB),
DL-Cotrain (CT) and Supervised Baseline (SP). N is the
number of entities in the test set.
Type LL FB CT SP N
PERSON 0.82 0.48 0.65 0.86 436
GEO-LOC 0.77 0.23 0.60 0.51 269
COMPANY 0.71 0.66 0.50 0.29 162
FACILITY 0.37 0.07 0.14 0.34 103
PRODUCT 0.53 0.34 0.40 0.07 91
BAND 0.44 0.40 0.42 0.01 54
SPORTSTEAM 0.53 0.11 0.27 0.06 51
MOVIE 0.54 0.65 0.54 0.05 34
TV-SHOW 0.59 0.31 0.43 0.01 31
OTHER 0.52 0.14 0.40 0.23 219
overall 0.66 0.38 0.53 0.45 1450
Table 10: F1 scores for classification broken down by
type for LabeledLDA (LL), Freebase Baseline (FB), DL-
Cotrain (CT) and Supervised Baseline (SP). N is the num-
ber of entities in the test set.
P R F1
DL-Cotrain-entity 0.47 0.45 0.46
DL-Cotrain-mention 0.54 0.51 0.53
LabeledLDA-entity 0.73 0.60 0.66
LabeledLDA-mention 0.57 0.52 0.54
Table 11: Comparing LabeledLDA and DL-Cotrain
grouping unlabeled data by entities vs. mentions.
System P R F1
COTRAIN-NER (10 types) 0.55 0.33 0.41
T-NER(10 types) 0.65 0.42 0.51
COTRAIN-NER (PLO) 0.57 0.42 0.49
T-NER(PLO) 0.73 0.49 0.59
Stanford NER (PLO) 0.30 0.27 0.29
Table 12: Performance at predicting both segmentation
and classification. Systems labeled with PLO are evalu-
ated on the 3 MUC types PERSON, LOCATION, ORGA-
NIZATION.
tity string, and infers a distribution over its possi-
ble types, whereas DL-Cotrain considers the entity
mentions separately as unlabeled examples and pre-
dicts a type independently for each. In order to
ensure that the difference in performance between
LabeledLDA and DL-Cotrain is not simply due to
this difference in representation, we compare both
DL-Cotrain and LabeledLDA using both unlabeled
datasets (grouping words by all mentions vs. keep-
ing mentions separate) in Table 11. As expected,
DL-Cotrain performs poorly when the unlabeled ex-
amples group mentions; this makes sense, since Co-
Training uses a discriminative learning algorithm,
so when trained on entities and tested on individual
mentions, the performance decreases. Additionally,
LabeledLDA?s performance is poorer when consid-
ering mentions as ?documents?. This is likely due
to the fact that there isn?t enough context to effec-
tively learn topics when the ?documents? are very
short (typically fewer than 10 words).
End to End System: Finally we present the end
to end performance on segmentation and classifica-
tion (T-NER) in Table 12. We observe that T-NER
again outperforms co-training. Moreover, compar-
ing against the Stanford Named Entity Recognizer
on the 3 MUC types, T-NER doubles F1 score.
4 Related Work
There has been relatively little previous work on
building NLP tools for Twitter or similar text styles.
Locke and Martin (2009) train a classifier to recog-
nize named entities based on annotated Twitter data,
handling the types PERSON, LOCATION, and OR-
GANIZATION. Developed in parallel to our work,
Liu et al (2011) investigate NER on the same 3
types, in addition to PRODUCTs and present a semi-
1531
supervised approach using k-nearest neighbor. Also
developed in parallel, Gimpell et al (2011) build a
POS tagger for tweets using 20 coarse-grained tags.
Benson et. al. (2011) present a system which ex-
tracts artists and venues associated with musical per-
formances. Recent work (Han and Baldwin, 2011;
Gouws et al, 2011) has proposed lexical normaliza-
tion of tweets which may be useful as a preprocess-
ing step for the upstream tasks like POS tagging and
NER. In addition Finin et. al. (2010) investigate
the use of Amazon?s Mechanical Turk for annotat-
ing Named Entities in Twitter, Minkov et. al. (2005)
investigate person name recognizers in email, and
Singh et. al. (2010) apply a minimally supervised
approach to extracting entities from text advertise-
ments.
In contrast to previous work, we have demon-
strated the utility of features based on Twitter-
specific POS taggers and Shallow Parsers in seg-
menting Named Entities. In addition we take a dis-
tantly supervised approach to Named Entity Classi-
fication which exploits large dictionaries of entities
gathered from Freebase, requires no manually anno-
tated data, and as a result is able to handle a larger
number of types than previous work. Although we
found manually annotated data to be very beneficial
for named entity segmentation, we were motivated
to explore approaches that don?t rely on manual la-
bels for classification due to Twitter?s wide range of
named entity types. Additionally, unlike previous
work on NER in informal text, our approach allows
the sharing of information across an entity?s men-
tions which is quite beneficial due to Twitter?s terse
nature.
Previous work on Semantic Bootstrapping has
taken a weakly-supervised approach to classifying
named entities based on large amounts of unla-
beled text (Etzioni et al, 2005; Carlson et al, 2010;
Kozareva and Hovy, 2010; Talukdar and Pereira,
2010; McIntosh, 2010). In contrast, rather than
predicting which classes an entity belongs to (e.g.
a multi-label classification task), LabeledLDA esti-
mates a distribution over its types, which is then use-
ful as a prior when classifying mentions in context.
In addition there has been been work on Skip-
Chain CRFs (Sutton, 2004; Finkel et al, 2005)
which enforce consistency when classifying multi-
ple occurrences of an entity within a document. Us-
ing topic models (e.g. LabeledLDA) for classifying
named entities has a similar effect, in that informa-
tion about an entity?s distribution of possible types
is shared across its mentions.
5 Conclusions
We have demonstrated that existing tools for POS
tagging, Chunking and Named Entity Recognition
perform quite poorly when applied to Tweets. To
address this challenge we have annotated tweets and
built tools trained on unlabeled, in-domain and out-
of-domain data, showing substantial improvement
over their state-of-the art news-trained counterparts,
for example, T-POS outperforms the Stanford POS
Tagger, reducing error by 41%. Additionally we
have shown the benefits of features generated from
T-POS and T-CHUNK in segmenting Named Entities.
We identified named entity classification as a par-
ticularly challenging task on Twitter. Due to their
terse nature, tweets often lack enough context to
identify the types of the entities they contain. In ad-
dition, a plethora of distinctive named entity types
are present, necessitating large amounts of training
data. To address both these issues we have presented
and evaluated a distantly supervised approach based
on LabeledLDA, which obtains a 25% increase in F1
score over the co-training approach to Named En-
tity Classification suggested by Collins and Singer
(1999) when applied to Twitter.
Our POS tagger, Chunker Named Entity Rec-
ognizer are available for use by the research
community: http://github.com/aritter/
twitter_nlp
Acknowledgments
We would like to thank Stephen Soderland, Dan
Weld and Luke Zettlemoyer, in addition to the
anonymous reviewers for helpful comments on a
previous draft. This research was supported in part
by NSF grant IIS-0803481, ONR grant N00014-11-
1-0294, Navy STTR contract N00014-10-M-0304, a
National Defense Science and Engineering Graduate
(NDSEG) Fellowship 32 CFR 168a and carried out
at the University of Washington?s Turing Center.
1532
References
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In The
49th Annual Meeting of the Association for Computa-
tional Linguistics, Portland, Oregon, USA. To appear.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn. Res.
Avrim Blum and Tom M. Mitchell. 1998. Combining
labeled and unlabeled sata with co-training. In COLT,
pages 92?100.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka, Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the third ACM interna-
tional conference on Web search and data mining,
WSDM ?10.
Eugene Charniak, Curtis Hendrickson, Neil Jacobson,
and Mike Perkowitz. 1993. Equations for part-of-
speech tagging. In AAAI, pages 784?789.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Empirical
Methods in Natural Language Processing.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating complex named entities in web text.
In Proceedings of the 20th international joint confer-
ence on Artifical intelligence.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2010. Analysis of a probabilistic model of redundancy
in unsupervised information extraction. Artif. Intell.,
174(11):726?748.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, NAACL ?09.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL Workshop on
Creating Speech and Text Language Data With Ama-
zon?s Mechanical Turk. Association for Computational
Linguistics, June.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?05.
Radu Florian. 2002. Named entity recognition as a house
of cards: classifier stacking. In Proceedings of the 6th
conference on Natural language learning - Volume 20,
COLING-02.
Eric N. Forsythand and Craig H. Martell. 2007. Lexical
and discourse analysis of online chat dialog. In Pro-
ceedings of the International Conference on Semantic
Computing.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. Technical report, Microsoft Research.
Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-
uard Hovy. 2011. Contextual bearing on linguistic
variation in social media. In ACL Workshop on Lan-
guage in Social Media, Portland, Oregon, USA. To
appear.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, April.
Mark Hachman. 2011. Humanity?s tweets: Just 20 ter-
abytes. In PCMAG.COM.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
The 49th Annual Meeting of the Association for Com-
putational Linguistics, Portland, Oregon, USA. To ap-
pear.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing sms: are two metaphors
better than one ? In COLING, pages 441?448.
Zornitsa Kozareva and Eduard H. Hovy. 2010. Not all
seeds are equal: Measuring the quality of text mining
seeds. In HLT-NAACL.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In ACL.
Brian Locke and James Martin. 2009. Named entity
recognition: Adapting to microblogging. In Senior
Thesis, University of Colorado.
1533
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. In http://mallet.
cs.umass.edu.
Tara McIntosh. 2010. Unsupervised discovery of nega-
tive categories in lexicon bootstrapping. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: apply-
ing named entity recognition to informal text. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, HLT ?05, pages 443?450, Morristown, NJ,
USA. Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-IJCNLP
2009.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197?214. Pe-
ter Lang, Frankfurt a.M., Germany.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: a super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 248?256,
Morristown, NJ, USA. Association for Computational
Linguistics.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text anal-
ysis applications. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 377?387, Morristown,
NJ, USA. Association for Computational Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology - Volume 1, NAACL ?03.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In Human Language Technologies:
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT).
Charles Sutton. 2004. Collective segmentation and la-
beling of distant entities in information extraction.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1473?1481. Associ-
ation for Computational Linguistics.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: chunking.
In Proceedings of the 2nd workshop on Learning lan-
guage in logic and the 4th conference on Computa-
tional natural language learning - Volume 7, ConLL
?00.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference on
streaming document collections. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, ACL ?95.
1534
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535?1545,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Identifying Relations for Open Information Extraction
Anthony Fader, Stephen Soderland, and Oren Etzioni
University of Washington, Seattle
{afader,soderlan,etzioni}@cs.washington.edu
Abstract
Open Information Extraction (IE) is the task
of extracting assertions from massive corpora
without requiring a pre-specified vocabulary.
This paper shows that the output of state-of-
the-art Open IE systems is rife with uninfor-
mative and incoherent extractions. To over-
come these problems, we introduce two sim-
ple syntactic and lexical constraints on bi-
nary relations expressed by verbs. We im-
plemented the constraints in the REVERB
Open IE system, which more than doubles the
area under the precision-recall curve relative
to previous extractors such as TEXTRUNNER
and WOEpos. More than 30% of REVERB?s
extractions are at precision 0.8 or higher?
compared to virtually none for earlier systems.
The paper concludes with a detailed analysis
of REVERB?s errors, suggesting directions for
future work.1
1 Introduction and Motivation
Typically, Information Extraction (IE) systems learn
an extractor for each target relation from la-
beled training examples (Kim and Moldovan, 1993;
Riloff, 1996; Soderland, 1999). This approach to IE
does not scale to corpora where the number of target
relations is very large, or where the target relations
cannot be specified in advance. Open IE solves this
problem by identifying relation phrases?phrases
that denote relations in English sentences (Banko
et al, 2007). The automatic identification of rela-
1The source code for REVERB is available at http://
reverb.cs.washington.edu/
tion phrases enables the extraction of arbitrary re-
lations from sentences, obviating the restriction to a
pre-specified vocabulary.
Open IE systems have achieved a notable measure
of success on massive, open-domain corpora drawn
from the Web, Wikipedia, and elsewhere. (Banko et
al., 2007; Wu and Weld, 2010; Zhu et al, 2009). The
output of Open IE systems has been used to support
tasks like learning selectional preferences (Ritter et
al., 2010), acquiring common sense knowledge (Lin
et al, 2010), and recognizing entailment (Schoen-
mackers et al, 2010; Berant et al, 2011). In ad-
dition, Open IE extractions have been mapped onto
existing ontologies (Soderland et al, 2010).
We have observed that two types of errors are fre-
quent in the output of Open IE systems such as TEX-
TRUNNER and WOE: incoherent extractions and un-
informative extractions.
Incoherent extractions are cases where the ex-
tracted relation phrase has no meaningful interpre-
tation (see Table 1 for examples). Incoherent ex-
tractions arise because the learned extractor makes a
sequence of decisions about whether to include each
word in the relation phrase, often resulting in incom-
prehensible predictions. To solve this problem, we
introduce a syntactic constraint: every multi-word
relation phrase must begin with a verb, end with a
preposition, and be a contiguous sequence of words
in the sentence. Thus, the identification of a relation
phrase is made in one fell swoop instead of on the
basis of multiple, word-by-word decisions.
Uninformative extractions are extractions that
omit critical information. For example, consider the
sentence ?Faust made a deal with the devil.? Previ-
1535
ous Open IE systems return the uninformative
(Faust, made, a deal)
instead of
(Faust, made a deal with, the devil).
This type of error is caused by improper handling
of relation phrases that are expressed by a combi-
nation of a verb with a noun, such as light verb
constructions (LVCs). An LVC is a multi-word ex-
pression composed of a verb and a noun, with the
noun carrying the semantic content of the predi-
cate (Grefenstette and Teufel, 1995; Stevenson et al,
2004; Allerton, 2002). Table 2 illustrates the wide
range of relations expressed this way, which are not
captured by existing open extractors. Our syntactic
constraint leads the extractor to include nouns in the
relation phrase, solving this problem.
Although the syntactic constraint significantly re-
duces incoherent and uninformative extractions, it
allows overly-specific relation phrases such as is of-
fering only modest greenhouse gas reduction targets
at. To avoid overly-specific relation phrases, we in-
troduce an intuitive lexical constraint: a binary rela-
tion phrase ought to appear with at least a minimal
number of distinct argument pairs in a large corpus.
In summary, this paper articulates two simple but
surprisingly powerful constraints on how binary re-
lationships are expressed via verbs in English sen-
tences, and implements them in the REVERB Open
IE system. We release REVERB and the data used in
our experiments to the research community.
The rest of the paper is organized as follows. Sec-
tion 2 analyzes previous work. Section 3 defines our
constraints precisely. Section 4 describes REVERB,
our implementation of the constraints. Section 5 re-
ports on our experimental results. Section 6 con-
cludes with a summary and discussion of future
work.
2 Previous Work
Open IE systems like TEXTRUNNER (Banko et al,
2007), WOEpos, and WOEparse (Wu and Weld, 2010)
focus on extracting binary relations of the form
(arg1, relation phrase, arg2) from text. These sys-
tems all use the following three-step method:
1. Label: Sentences are automatically labeled
with extractions using heuristics or distant su-
pervision.
Sentence Incoherent Relation
The guide contains dead links
and omits sites.
contains omits
The Mark 14 was central to the
torpedo scandal of the fleet.
was central torpedo
They recalled that Nungesser
began his career as a precinct
leader.
recalled began
Table 1: Examples of incoherent extractions. In-
coherent extractions make up approximately 13% of
TEXTRUNNER?s output, 15% of WOEpos?s output, and
30% of WOEparse?s output.
is is an album by, is the author of, is a city in
has has a population of, has a Ph.D. in, has a cameo in
made made a deal with, made a promise to
took took place in, took control over, took advantage of
gave gave birth to, gave a talk at, gave new meaning to
got got tickets to, got a deal on, got funding from
Table 2: Examples of uninformative relations (left) and
their completions (right). Uninformative relations oc-
cur in approximately 4% of WOEparse?s output, 6% of
WOEpos?s output, and 7% of TEXTRUNNER?s output.
2. Learn: A relation phrase extractor is learned
using a sequence-labeling graphical model
(e.g., CRF).
3. Extract: the system takes a sentence as in-
put, identifies a candidate pair of NP arguments
(arg1, arg2) from the sentence, and then uses
the learned extractor to label each word be-
tween the two arguments as part of the relation
phrase or not.
The extractor is applied to the successive sentences
in the corpus, and the resulting extractions are col-
lected.
This method faces several challenges. First,
the training phase requires a large number of la-
beled training examples (e.g., 200, 000 heuristically-
labeled sentences for TEXTRUNNER and 300, 000
for WOE). Heuristic labeling of examples obviates
hand labeling but results in noisy labels and distorts
the distribution of examples. Second, the extrac-
tion step is posed as a sequence-labeling problem,
where each word is assigned its own label. Because
each assignment is uncertain, the likelihood that the
extracted relation phrase is flawed increases with
the length of the sequence. Finally, the extractor
1536
chooses an extraction?s arguments heuristically, and
cannot backtrack over this choice. This is problem-
atic when a word that belongs in the relation phrase
is chosen as an argument (for example, deal from
the ?made a deal with? sentence).
Because of the feature sets utilized in previous
work, the learned extractors ignore both ?holistic?
aspects of the relation phrase (e.g., is it contiguous?)
as well as lexical aspects (e.g., how many instances
of this relation are there?). Thus, as we show in Sec-
tion 5, systems such as TEXTRUNNER are unable
to learn the constraints embedded in REVERB. Of
course, a learning system, utilizing a different hy-
pothesis space, and an appropriate set of training ex-
amples, could potentially learn and refine the con-
straints in REVERB. This is a topic for future work,
which we consider in Section 6.
The first Open IE system was TEXTRUNNER
(Banko et al, 2007), which used a Naive Bayes
model with unlexicalized POS and NP-chunk fea-
tures, trained using examples heuristically generated
from the Penn Treebank. Subsequent work showed
that utilizing a linear-chain CRF (Banko and Et-
zioni, 2008) or Markov Logic Network (Zhu et al,
2009) can lead to improved extraction. The WOE
systems introduced by Wu and Weld make use of
Wikipedia as a source of training data for their ex-
tractors, which leads to further improvements over
TEXTRUNNER (Wu and Weld, 2010). Wu and Weld
also show that dependency parse features result in a
dramatic increase in precision and recall over shal-
low linguistic features, but at the cost of extraction
speed.
Other approaches to large-scale IE have included
Preemptive IE (Shinyama and Sekine, 2006), On-
Demand IE (Sekine, 2006), and weak supervision
for IE (Mintz et al, 2009; Hoffmann et al, 2010).
Preemptive IE and On-Demand IE avoid relation-
specific extractors, but rely on document and en-
tity clustering, which is too costly for Web-scale IE.
Weakly supervised methods use an existing ontol-
ogy to generate training data for learning relation-
specific extractors. While this allows for learn-
ing relation-specific extractors at a larger scale than
what was previously possible, the extractions are
still restricted to a specific ontology.
Many systems have used syntactic patterns based
on verbs to extract relation phrases, usually rely-
ing on a full dependency parse of the input sentence
(Lin and Pantel, 2001; Stevenson, 2004; Specia and
Motta, 2006; Kathrin Eichler and Neumann, 2008).
Our work differs from these approaches by focus-
ing on relation phrase patterns expressed in terms
of POS tags and NP chunks, instead of full parse
trees. Banko and Etzioni (Banko and Etzioni, 2008)
showed that a small set of POS-tag patterns cover a
large fraction of relationships in English, but never
incorporated the patterns into an extractor. This pa-
per reports on a substantially improved model of bi-
nary relation phrases, which increases the recall of
the Banko-Etzioni model (see Section 3.3). Further,
while previous work in Open IE has mainly focused
on syntactic patterns for relation extraction, we in-
troduce a lexical constraint that boosts precision and
recall.
Finally, Open IE is closely related to semantic role
labeling (SRL) (Punyakanok et al, 2008; Toutanova
et al, 2008) in that both tasks extract relations and
arguments from sentences. However, SRL systems
traditionally rely on syntactic parsers, which makes
them susceptible to parser errors and substantially
slower than Open IE systems such as REVERB. This
difference is particularly important when operating
on the Web corpus due to its size and heterogeneity.
Finally, SRL requires hand-constructed semantic re-
sources like Propbank and Framenet (Martha and
Palmer, 2002; Baker et al, 1998) as input. In con-
trast, Open IE systems require no relation-specific
training data. ReVerb, in particular, relies on its ex-
plicit lexical and syntactic constraints, which have
no correlate in SRL systems. For a more detailed
comparison of SRL and Open IE, see (Christensen
et al, 2010).
3 Constraints on Relation Phrases
In this section we introduce two constraints on re-
lation phrases: a syntactic constraint and a lexical
constraint.
3.1 Syntactic Constraint
The syntactic constraint serves two purposes. First,
it eliminates incoherent extractions, and second, it
reduces uninformative extractions by capturing rela-
tion phrases expressed by a verb-noun combination,
including light verb constructions.
1537
V | V P | VW ?P
V = verb particle? adv?
W = (noun | adj | adv | pron | det)
P = (prep | particle | inf. marker)
Figure 1: A simple part-of-speech-based regular expres-
sion reduces the number of incoherent extractions like
was central torpedo and covers relations expressed via
light verb constructions like gave a talk at.
The syntactic constraint requires the relation
phrase to match the POS tag pattern shown in Fig-
ure 1. The pattern limits relation phrases to be either
a verb (e.g., invented), a verb followed immediately
by a preposition (e.g., located in), or a verb followed
by nouns, adjectives, or adverbs ending in a preposi-
tion (e.g., has atomic weight of). If there are multiple
possible matches in a sentence for a single verb, the
longest possible match is chosen. Finally, if the pat-
tern matches multiple adjacent sequences, we merge
them into a single relation phrase (e.g., wants to ex-
tend). This refinement enables the model to readily
handle relation phrases containing multiple verbs. A
consequence of this pattern is that the relation phrase
must be a contiguous span of words in the sentence.
The syntactic constraint eliminates the incoherent
relation phrases returned by existing systems. For
example, given the sentence
Extendicare agreed to buy Arbor Health Care for
about US $432 million in cash and assumed debt.
TEXTRUNNER returns the extraction
(Arbor Health Care, for assumed, debt).
The phrase for assumed is clearly not a valid rela-
tion phrase: it begins with a preposition and splices
together two distant words in the sentence. The syn-
tactic constraint prevents this type of error by sim-
ply restricting relation phrases to match the pattern
in Figure 1.
The syntactic constraint reduces uninformative
extractions by capturing relation phrases expressed
via LVCs. For example, the POS pattern matched
against the sentence ?Faust made a deal with the
Devil,? would result in the relation phrase made a
deal with, instead of the uninformative made.
Finally, we require the relation phrase to appear
between its two arguments in the sentence. This is a
common constraint that has been implicitly enforced
in other open extractors.
3.2 Lexical Constraint
While the syntactic constraint greatly reduces unin-
formative extractions, it can sometimes match rela-
tion phrases that are so specific that they have only a
few possible instances, even in a Web-scale corpus.
Consider the sentence:
The Obama administration is offering only modest
greenhouse gas reduction targets at the conference.
The POS pattern will match the phrase:
is offering only modest greenhouse gas reduction targets at
(1)
Thus, there are phrases that satisfy the syntactic con-
straint, but are not relational.
To overcome this limitation, we introduce a lexi-
cal constraint that is used to separate valid relation
phrases from overspecified relation phrases, like the
example in (1). The constraint is based on the in-
tuition that a valid relation phrase should take many
distinct arguments in a large corpus. The phrase in
(1) is specific to the argument pair (Obama admin-
istration, conference), so it is unlikely to represent a
bona fide relation. We describe the implementation
details of the lexical constraint in Section 4.
3.3 Limitations
Our constraints represent an idealized model of re-
lation phrases in English. This raises the question:
How much recall is lost due to the constraints?
To address this question, we analyzed Wu and
Weld?s set of 300 sentences from a set of random
Web pages, manually identifying all verb-based re-
lationships between noun phrase pairs. This resulted
in a set of 327 relation phrases. For each rela-
tion phrase, we checked whether it satisfies our con-
straints. We found that 85% of the relation phrases
do satisfy the constraints. Of the remaining 15%,
we identified some of the common cases where the
constraints were violated, summarized in Table 3.
Many of the example relation phrases shown in
Table 3 involve long-range dependencies between
words in the sentence. These types of dependen-
cies are not easily representable using a pattern over
POS tags. A deeper syntactic analysis of the input
sentence would provide a much more general lan-
guage for modeling relation phrases. For example,
one could create a model of relations expressed in
1538
Binary Verbal Relation Phrases
85% Satisfy Constraints
8% Non-Contiguous Phrase Structure
Coordination: X is produced and maintained by Y
Multiple Args: X was founded in 1995 by Y
Phrasal Verbs: X turned Y off
4% Relation Phrase Not Between Arguments
Intro. Phrases: Discovered by Y, X . . .
Relative Clauses: . . . the Y that X discovered
3% Do Not Match POS Pattern
Interrupting Modifiers: X has a lot of faith in Y
Infinitives: X to attack Y
Table 3: Approximately 85% of the binary verbal relation
phrases in a sample of Web sentences satisfy our con-
straints.
terms of dependency parse features that would cap-
ture the non-contiguous relation phrases in Table 3.
Previous work has shown that dependency paths do
indeed boost the recall of relation extraction systems
(Wu and Weld, 2010; Mintz et al, 2009). While us-
ing dependency path features allows for a more flex-
ible model of relations, it significantly increases pro-
cessing time, which is problematic for Web-scale ex-
traction. Further, we have found that this increased
recall comes at the cost of lower precision on Web
text (see Section 5).
The results in Table 3 are similar to Banko and Et-
zioni?s findings that a set of eight POS patterns cover
a large fraction of binary verbal relation phrases.
However, their analysis was based on a set of sen-
tences known to contain either a company acquisi-
tion or birthplace relationship, while our results are
on a random sample of Web sentences. We applied
Banko and Etzioni?s verbal patterns to our random
sample of 300 Web sentences, and found that they
cover approximately 69% of the relation phrases in
the corpus. The gap in recall between this and the
85% shown in Table 3 is largely due to LVC relation
phrases (made a deal with) and phrases containing
multiple verbs (refuses to return to), which their pat-
terns do not cover.
In sum, our model is by no means complete.
However, we have empirically shown that the ma-
jority of binary verbal relation phrases in a sample
of Web sentences are captured by our model. By
focusing on this subset of language, our model can
be used to perform Open IE at significantly higher
precision than before.
4 REVERB
This section introduces REVERB, a novel open ex-
tractor based on the constraints defined in the previ-
ous section. REVERB first identifies relation phrases
that satisfy the syntactic and lexical constraints, and
then finds a pair of NP arguments for each identified
relation phrase. The resulting extractions are then
assigned a confidence score using a logistic regres-
sion classifier.
This algorithm differs in three important ways
from previous methods (Section 2). First, the re-
lation phrase is identified ?holistically? rather than
word-by-word. Second, potential phrases are fil-
tered based on statistics over a large corpus (the
implementation of our lexical constraint). Finally,
REVERB is ?relation first? rather than ?arguments
first?, which enables it to avoid a common error
made by previous methods?confusing a noun in the
relation phrase for an argument, e.g. the noun deal in
made a deal with.
4.1 Extraction Algorithm
REVERB takes as input a POS-tagged and NP-
chunked sentence and returns a set of (x, r, y)
extraction triples.2 Given an input sentence s,
REVERB uses the following extraction algorithm:
1. Relation Extraction: For each verb v in s,
find the longest sequence of words rv such that
(1) rv starts at v, (2) rv satisfies the syntactic
constraint, and (3) rv satisfies the lexical con-
straint. If any pair of matches are adjacent or
overlap in s, merge them into a single match.
2. Argument Extraction: For each relation
phrase r identified in Step 1, find the nearest
noun phrase x to the left of r in s such that x is
not a relative pronoun, WHO-adverb, or exis-
tential ?there?. Find the nearest noun phrase y
to the right of r in s. If such an (x, y) pair could
be found, return (x, r, y) as an extraction.
We check whether a candidate relation phrase
rv satisfies the syntactic constraint by matching it
against the regular expression in Figure 1.
2REVERB uses OpenNLP for POS tagging and NP chunk-
ing: http://opennlp.sourceforge.net/
1539
To determine whether rv satisfies the lexical con-
straint, we use a large dictionary D of relation
phrases that are known to take many distinct argu-
ments. In an offline step, we construct D by find-
ing all matches of the POS pattern in a corpus of
500 million Web sentences. For each matching re-
lation phrase, we heuristically identify its arguments
(as in Step 2 above). We set D to be the set of all
relation phrases that take at least k distinct argument
pairs in the set of extractions. In order to allow for
minor variations in relation phrases, we normalize
each relation phrase by removing inflection, auxil-
iary verbs, adjectives, and adverbs. Based on ex-
periments on a held-out set of sentences, we found
that a value of k = 20 works well for filtering out
overspecified relations. This results in a set of ap-
proximately 1.7 million distinct normalized relation
phrases, which are stored in memory at extraction
time.
As an example of the extraction algorithm in ac-
tion, consider the following input sentence:
Hudson was born in Hampstead, which is a
suburb of London.
Step 1 of the algorithm identifies three relation
phrases that satisfy the syntactic and lexical con-
straints: was, born in, and is a suburb of. The first
two phrases are adjacent in the sentence, so they are
merged into the single relation phrase was born in.
Step 2 then finds an argument pair for each relation
phrase. For was born in, the nearest NPs are (Hud-
son, Hampstead). For is a suburb of, the extractor
skips over the NP which and chooses the argument
pair (Hampstead, London). The final output is
e1: (Hudson, was born in, Hampstead)
e2: (Hampstead, is a suburb of, London).
4.2 Confidence Function
The extraction algorithm in the previous section has
high recall, but low precision. Like with previous
open extractors, we want way to trade recall for pre-
cision by tuning a confidence threshold. We use a
logistic regression classifier to assign a confidence
score to each extraction, which uses the features
shown in Table 4. All of these features are efficiently
computable and relation independent. We trained
the confidence function by manually labeling the ex-
tractions from a set of 1, 000 sentences from the Web
and Wikipedia as correct or incorrect.
Weight Feature
1.16 (x, r, y) covers all words in s
0.50 The last preposition in r is for
0.49 The last preposition in r is on
0.46 The last preposition in r is of
0.43 len(s) ? 10 words
0.43 There is a WH-word to the left of r
0.42 r matches VW*P from Figure 1
0.39 The last preposition in r is to
0.25 The last preposition in r is in
0.23 10 words < len(s) ? 20 words
0.21 s begins with x
0.16 y is a proper noun
0.01 x is a proper noun
-0.30 There is an NP to the left of x in s
-0.43 20 words < len(s)
-0.61 r matches V from Figure 1
-0.65 There is a preposition to the left of x in s
-0.81 There is an NP to the right of y in s
-0.93 Coord. conjunction to the left of r in s
Table 4: REVERB uses these features to assign a confi-
dence score to an extraction (x, r, y) from a sentence s
using a logistic regression classifier.
Previous open extractors require labeled training
data to learn a model of relations, which is then used
to extract relation phrases from text. In contrast,
REVERB uses a specified model of relations for ex-
traction, and requires labeled data only for assigning
confidence scores to its extractions. Learning a con-
fidence function is a much simpler task than learning
a full model of relations, using two orders of magni-
tude fewer training examples than TEXTRUNNER or
WOE.
4.3 TEXTRUNNER-R
The model of relation phrases used by REVERB
is specified, but could a TEXTRUNNER-like sys-
tem learn this model from training data? While
it is difficult to answer such a question for all
possible permutations of features sets, training ex-
amples, and learning biases, we demonstrate that
TEXTRUNNER itself cannot learn REVERB?s model
even when re-trained using the output of REVERB
as labeled training data. The resulting system,
TEXTRUNNER-R, uses the same feature representa-
tion as TEXTRUNNER, but different parameters, and
a different set of training examples.
To generate positive instances, we ran REVERB
1540
on the Penn Treebank, which is the same dataset
that TEXTRUNNER is trained on. To generate neg-
ative instances from a sentence, we took each noun
phrase pair in the sentence that does not appear as
arguments in a REVERB extraction. This process
resulted in a set of 67, 562 positive instances, and
356, 834 negative instances. We then passed these
labeled examples to TEXTRUNNER?s training proce-
dure, which learns a linear-chain CRF using closed-
class features like POS tags, capitalization, punctu-
ation, etc.TEXTRUNNER-R uses the argument-first
extraction algorithm described in Section 2.
5 Experiments
We compare REVERB to the following systems:
? REVERB?lex - The REVERB system described
in the previous section, but without the lexical
constraint. REVERB?lex uses the same confi-
dence function as REVERB.
? TEXTRUNNER - Banko and Etzioni?s 2008 ex-
tractor, which uses a second order linear-chain
CRF trained on extractions heuristically gener-
ated from the Penn Treebank. TEXTRUNNER
uses shallow linguistic features in its CRF,
which come from the same POS tagger and NP-
chunker that REVERB uses.
? TEXTRUNNER-R - Our modification to
TEXTRUNNER, which uses the same extrac-
tion code, but with a model of relations trained
on REVERB extractions.
? WOEpos - Wu and Weld?s modification to
TEXTRUNNER, which uses a model of re-
lations learned from extractions heuristically
generated from Wikipedia.
? WOEparse - Wu and Weld?s parser-based ex-
tractor, which uses a large dictionary of depen-
dency path patterns learned from heuristic ex-
tractions generated from Wikipedia.
Each system is given a set of sentences as input,
and returns a set of binary extractions as output. We
created a test set of 500 sentences sampled from the
Web, using Yahoo?s random link service.3 After run-
3http://random.yahoo.com/bin/ryl
REVERB REVERB WOE TEXT- WOE TEXT-
0.0
0.1
0.2
0.3
0.4
0.5
A
r
e
a
U
n
d
e
r
P
R
C
u
r
v
e
?lex
parse
RUNNER-R
pos
RUNNER
A
r
e
a
U
n
d
e
r
P
R
C
u
r
v
e
Figure 2: REVERB outperforms state-of-the-art open
extractors, with an AUC more than twice that of
TEXTRUNNER or WOEpos, and 38% higher than
WOEparse.
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Comparison of REVERB-Based Systems
REVERB
REVERB
?lex
TEXTRUNNER-R
P
r
e
c
i
s
i
o
n
Figure 3: The lexical constraint gives REVERB
a boost in precision and recall over REVERB?lex.
TEXTRUNNER-R is unable to learn the model used by
REVERB, which results in lower precision and recall.
ning each extractor over the input sentences, two hu-
man judges independently evaluated each extraction
as correct or incorrect. The judges reached agree-
ment on 86% of the extractions, with an agreement
score of ? = 0.68. We report results on the subset
of the data where the two judges concur.
The judges labeled uninformative extractions con-
servatively. That is, if critical information was
dropped from the relation phrase but included in the
second argument, it is labeled correct. For example,
both the extractions (Ackerman, is a professor of, bi-
ology) and (Ackerman, is, a professor of biology) are
considered correct.
Each system returns confidence scores for its ex-
tractions. For a given threshold, we can measure
the precision and recall of the output. Precision
is the fraction of returned extractions that are cor-
rect. Recall is the fraction of correct extractions in
1541
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Extractions
REVERB
WOE
parse
WOE
pos
TEXTRUNNER
P
r
e
c
i
s
i
o
n
Figure 4: REVERB achieves significantly higher preci-
sion than state-of-the-art Open IE systems, and compara-
ble recall to WOEparse.
the corpus that are returned. We use the total num-
ber of extractions labeled as correct by the judges
as our measure of recall for the corpus. In order to
avoid double-counting, we treat extractions that dif-
fer superficially (e.g., different punctuation or drop-
ping inessential modifiers) as a single extraction. We
compute a precision-recall curve by varying the con-
fidence threshold, and then compute the area under
the curve (AUC).
5.1 Results
Figure 2 shows the AUC of each system. REVERB
achieves an AUC that is 30% higher than WOEparse
and is more than double the AUC of WOEpos or
TEXTRUNNER. The lexical constraint provides a
significant boost in performance, with REVERB
achieving an AUC 23% higher than REVERB?lex.
REVERB proves to be a useful source of train-
ing data, with TEXTRUNNER-R having an AUC
71% higher than TEXTRUNNER and performing
on par with WOEpos. From the training data,
TEXTRUNNER-R was able to learn a model that
predicts contiguous relation phrases, but still re-
turned incoherent relation phrases (e.g., starting with
a preposition) and overspecified relation phrases.
These errors are due to TEXTRUNNER-R overfitting
the training data and not having access to the lexical
constraint.
Figure 3 shows the precision-recall curves of the
systems introduced in this paper. TEXTRUNNER-R
has much lower precision than REVERB and
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Relations Only
REVERB
WOE
parse
WOE
pos
TEXTRUNNER
P
r
e
c
i
s
i
o
n
Figure 5: On the subtask of identifying relations phrases,
REVERB is able to achieve even higher precision and re-
call than other systems.
REVERB?lex at all levels of recall. The lexi-
cal constraint gives REVERB a boost in precision
over REVERB?lex, reducing overspecified extrac-
tions from 20% of REVERB?lex?s output to 1% of
REVERB?s. The lexical constraint also boosts recall
over REVERB?lex, since REVERB is able to find a
correct relation phrase where REVERB?lex finds an
overspecified one.
Figure 4 shows the precision-recall curves of
REVERB and the external systems. REVERB has
much higher precision than the other systems at
nearly all levels of recall. In particular, more than
30% of REVERB?s extractions are at precision 0.8
or higher, compared to virtually none for the other
systems. WOEparse achieves a slightly higher recall
than REVERB (0.62 versus 0.64), but at the cost of
lower precision.
In order to highlight the role of the relational
model of each system, we also evaluate their per-
formance on the subtask of extracting just the rela-
tion phrases from the input text. Figure 5 shows the
precision-recall curves for each system on the rela-
tion phrase-only evaluation. In this case, REVERB
has both higher precision and recall than the other
systems.
REVERB?s biggest improvement came from the
elimination of incoherent extractions. Incoher-
ent extractions were a large fraction of the errors
made by previous systems, accounting for approxi-
mately 13% of TEXTRUNNER?s extractions, 15% of
WOEpos?s, and 30% of WOEparse?s. Uninformative
1542
REVERB - Incorrect Extractions
65% Correct relation phrase, incorrect arguments
16% N-ary relation
8% Non-contiguous relation phrase
2% Imperative verb
2% Overspecified relation phrase
7% Other, including POS/chunking errors
Table 5: The majority of the incorrect extractions re-
turned by REVERB are due to errors in argument extrac-
tion.
extractions had a smaller effect on other systems?
precision, accounting for 4% of WOEparse?s extrac-
tions, 5% of WOEpos?s, and 7% of TEXTRUNNER?s,
while only appearing in 1% of REVERB?s extrac-
tions. REVERB?s reduction in uninformative extrac-
tions resulted in a boost in recall, capturing many
LVC relation phrases missed by other systems (like
those shown in Table 2).
To test the systems? speed, we ran each extrac-
tor on a set of 100, 000 sentences using a Pen-
tium 4 machine with 4GB of RAM. The process-
ing times were 16 minutes for REVERB, 21 min-
utes for TEXTRUNNER, 21 minutes for WOEpos, and
11 hours for WOEparse. The times for REVERB,
TEXTRUNNER, and WOEpos are all approximately
the same, since they all use the same POS-tagging
and NP-chunking software. WOEparse processes
each sentence with a dependency parser, resulting
in much longer processing time.
5.2 REVERB Error Analysis
To better understand the limitations of REVERB, we
performed a detailed analysis of its errors in pre-
cision (incorrect extractions returned by REVERB)
and its errors in recall (correct extractions that
REVERB missed).
Table 5 summarizes the types of incorrect extrac-
tions that REVERB returns. We found that 65% of
the incorrect extractions returned by REVERB were
cases where a relation phrase was correctly identi-
fied, but the argument-finding heuristics failed. The
remaining errors were cases where REVERB ex-
tracted an incorrect relation phrase. One common
mistake that REVERB made was extracting a rela-
tion phrase that expresses an n-ary relationship via
a ditransitive verb. For example, given the sentence
REVERB - Missed Extractions
52% Could not identify correct arguments
23% Relation filtered out by lexical constraint
17% Identified a more specific relation
8% POS/chunking error
Table 6: The majority of extractions that were missed by
REVERB were cases where the correct relation phrase
was found, but the arguments were not correctly identi-
fied.
?I gave him 15 photographs,? REVERB extracts (I,
gave, him). These errors are due to the fact that
REVERB only models binary relations.
Table 6 summarizes the correct extractions that
were extracted by other systems and were not ex-
tracted by REVERB. As with the false positive ex-
tractions, the majority of false negatives (52%) were
due to the argument-finding heuristics choosing the
wrong arguments, or failing to extract all possible ar-
guments (in the case of coordinating conjunctions).
Other sources of failure were due to the lexical con-
straint either failing to filter out an overspecified re-
lation phrase or filtering out a valid relation phrase.
These errors hurt both precision and recall, since
each case results in the extractor overlooking a cor-
rect relation phrase and choosing another.
5.3 Evaluation At Scale
Section 5.1 shows that REVERB outperforms ex-
isting Open IE systems when evaluated on a sam-
ple of sentences. Previous work has shown that
the frequency of an extraction in a large corpus is
useful for assessing the correctness of extractions
(Downey et al, 2005). Thus, it is possible a pri-
ori that REVERB?s gains over previous systems will
diminish when extraction frequency is taken into ac-
count.
In fact, we found that REVERB?s advantage over
TEXTRUNNER when run at scale is qualitatively
similar to its advantage on single sentences. We ran
both REVERB and TEXTRUNNER on Banko and Et-
zioni?s corpus of 500 million Web sentences and ex-
amined the effect of redundancy on precision.
As Downey?s work predicts, precision increased
in both systems for extractions found multiple
times, compared with extractions found only once.
However, REVERB had higher precision than
1543
TEXTRUNNER at all frequency thresholds. In fact,
REVERB?s frequency 1 extractions had a precision
of 0.75, which TEXTRUNNER could not approach
even with frequency 10 extractions, which had a
precision of 0.34. Thus, REVERB is able to return
more correct extractions at a higher precision than
TEXTRUNNER, even when redundancy is taken into
account.
6 Conclusions and Future Work
The paper?s contributions are as follows:
? We have identified and analyzed the problems
of incoherent and uninformative extractions for
Open IE systems, and shown their prevalence
for systems such as TEXTRUNNER and WOE.
? We articulated general, easy-to-enforce con-
straints on binary, verb-based relation phrases
in English that ameliorate these problems and
yield richer and more informative relations
(see, for example, Table 2).
? Based on these constraints, we designed, im-
plemented, and evaluated the REVERB extrac-
tor, which substantially outperforms previous
Open IE systems in both recall and precision.
? We make REVERB and the data used in our
experiments available to the research commu-
nity.4
In future work, we plan to explore utilizing our
constraints to improve the performance of learned
CRF models. Roth et al have shown how to incor-
porate constraints into CRF learners (Roth and Yih,
2005). It is natural, then, to consider whether the
combination of heuristically labeled training exam-
ples, CRF learning, and our constraints will result
in superior performance. The error analysis in Sec-
tion 5.2 also suggests natural directions for future
work. For instance, since many of REVERB?s errors
are due to incorrect arguments, improved methods
for argument extraction are in order.
Acknowledgments
We would like to thank Mausam, Dan Weld, Yoav
Artzi, Luke Zettlemoyer, members of the KnowItAll
4http://reverb.cs.washington.edu
group, and the anonymous reviewers for their help-
ful comments. This research was supported in part
by NSF grant IIS-0803481, ONR grant N00014-08-
1-0431, and DARPA contract FA8750-09-C-0179,
and carried out at the University of Washington?s
Turing Center.
References
David J. Allerton. 2002. Stretched Verb Constructions in
English. Routledge Studies in Germanic Linguistics.
Routledge (Taylor and Francis), New York.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In In the Proceedings
of the 20th International Joint Conference on Artificial
Intelligence, pages 2670?2676, January.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2010. Semantic role labeling for
open information extraction. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, FAM-LbR ?10, pages 52?60, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI, pages 1034?1041.
Gregory Grefenstette and Simone Teufel. 1995. Corpus-
based method for automatic identification of support
verbs for nominalizations. In Proceedings of the sev-
enth conference on European chapter of the Associa-
tion for Computational Linguistics, pages 98?103, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 286?
295, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
1544
Holmer Hemsen Kathrin Eichler and Gnter Neu-
mann. 2008. Unsupervised relation extraction
from web documents. In LREC. http://www.lrec-
conf.org/proceedings/lrec2008/.
J. Kim and D. Moldovan. 1993. Acquisition of semantic
patterns for information extraction from corpora. In
Procs. of Ninth IEEE Conference on Artificial Intelli-
gence for Applications, pages 171?176.
Dekang Lin and Patrick Pantel. 2001. DIRT-Discovery
of Inference Rules from Text. In Proceedings of
ACM Conference on Knowledge Discovery and Data
Mining(KDD-01), pages pp. 323?328.
Thomas Lin, Mausam, and Oren Etzioni. 2010. Identify-
ing Functional Relations in Web Text. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1266?1276, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In In Proceedings of LREC-
2002.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP ?09: Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
2, pages 1003?1011, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
E. Riloff. 1996. Automatically constructing extraction
patterns from untagged text. In Procs. of the Thir-
teenth National Conference on Artificial Intelligence
(AAAI-96), pages 1044?1049.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Pref-
erences. In ACL.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of the 22nd international conference on
Machine learning, ICML ?05, pages 736?743, New
York, NY, USA. ACM.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 1088?1098,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 731?738, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive Information Extraction using Unrestricted Rela-
tion Discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 304?311, New York City, USA,
June. Association for Computational Linguistics.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open in-
formation extraction to domain-specific relations. AI
Magazine, 31(3):93?102.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-Structured and Free Text. Machine
Learning, 34(1-3):233?272.
Lucia Specia and Enrico Motta. 2006. M.: A hybrid
approach for extracting semantic relations from texts.
In In. Proceedings of the 2 nd Workshop on Ontology
Learning and Population, pages 57?64.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.
2004. Statistical measures of the semi-productivity
of light verb constructions. In 2nd ACL Workshop on
Multiword Expressions, pages 1?8.
M. Stevenson. 2004. An unsupervised WordNet-based
algorithm for relation extraction. In Proceedings of
the ?Beyond Named Entity? workshop at the Fourth
International Conference on Language Resources and
Evalutaion (LREC?04).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Fei Wu and Daniel S. Weld. 2010. Open information ex-
traction using Wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 118?127, Morristown,
NJ, USA. Association for Computational Linguistics.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. StatSnowball: a statistical ap-
proach to extracting entity relationships. In WWW ?09:
Proceedings of the 18th international conference on
World wide web, pages 101?110, New York, NY, USA.
ACM.
1545
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 523?534, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Open Language Learning for Information Extraction
Mausam, Michael Schmitz, Robert Bart, Stephen Soderland, and Oren Etzioni
Turing Center
Department of Computer Science and Engineering
University of Washington, Seattle
{mausam,schmmd,rbart,soderlan,etzioni}@cs.washington.edu
Abstract
Open Information Extraction (IE) systems ex-
tract relational tuples from text, without re-
quiring a pre-specified vocabulary, by iden-
tifying relation phrases and associated argu-
ments in arbitrary sentences. However, state-
of-the-art Open IE systems such as REVERB
and WOE share two important weaknesses ?
(1) they extract only relations that are medi-
ated by verbs, and (2) they ignore context,
thus extracting tuples that are not asserted as
factual. This paper presents OLLIE, a sub-
stantially improved Open IE system that ad-
dresses both these limitations. First, OLLIE
achieves high yield by extracting relations me-
diated by nouns, adjectives, and more. Sec-
ond, a context-analysis step increases preci-
sion by including contextual information from
the sentence in the extractions. OLLIE obtains
2.7 times the area under precision-yield curve
(AUC) compared to REVERB and 1.9 times
the AUC of WOEparse.
1 Introduction
While traditional Information Extraction (IE)
(ARPA, 1991; ARPA, 1998) focused on identifying
and extracting specific relations of interest, there
has been great interest in scaling IE to a broader
set of relations and to far larger corpora (Banko et
al., 2007; Hoffmann et al2010; Mintz et al2009;
Carlson et al2010; Fader et al2011). However,
the requirement of having pre-specified relations of
interest is a significant obstacle. Imagine an intel-
ligence analyst who recently acquired a terrorist?s
laptop or a news reader who wishes to keep abreast
of important events. The substantial endeavor in
1. ?After winning the Superbowl, the Saints are now
the top dogs of the NFL.?
O: (the Saints; win; the Superbowl)
2. ?There are plenty of taxis available at Bali airport.?
O: (taxis; be available at; Bali airport)
3. ?Microsoft co-founder Bill Gates spoke at ...?
O: (Bill Gates; be co-founder of; Microsoft)
4. ?Early astronomers believed that the earth is the
center of the universe.?
R: (the earth; be the center of; the universe)
W: (the earth; be; the center of the universe)
O: ((the earth; be the center of; the universe)
AttributedTo believe; Early astronomers)
5. ?If he wins five key states, Romney will be elected
President.?
R,W: (Romney; will be elected; President)
O: ((Romney; will be elected; President)
ClausalModifier if; he wins five key states)
Figure 1: OLLIE (O) has a wider syntactic range and finds
extractions for the first three sentences where REVERB
(R) and WOEparse (W) find none. For sentences #4,5,
REVERB and WOEparse have an incorrect extraction by
ignoring the context that OLLIE explicitly represents.
analyzing their corpus is the discovery of important
relations, which are likely not pre-specified. Open
IE (Banko et al2007) is the state-of-the-art
approach for such scenarios.
However, the state-of-the-art Open IE systems,
REVERB (Fader et al2011; Etzioni et al2011)
and WOEparse (Wu and Weld, 2010) suffer from two
key drawbacks. Firstly, they handle a limited sub-
set of sentence constructions for expressing relation-
ships. Both extract only relations that are mediated
by verbs, and REVERB further restricts this to a sub-
set of verbal patterns. This misses important infor-
mation mediated via other syntactic entities such as
nouns and adjectives, as well as a wider range of
verbal structures (examples #1-3 in Figure 1).
523
Secondly, REVERB and WOEparse perform only
a local analysis of a sentence, so they often extract
relations that are not asserted as factual in the sen-
tence (examples #4,5). This often occurs when the
relation is within a belief, attribution, hypothetical
or other conditional context.
In this paper we present OLLIE (Open Language
Learning for Information Extraction), 1 our novel
Open IE system that overcomes the limitations of
previous Open IE by (1) expanding the syntactic
scope of relation phrases to cover a much larger
number of relation expressions, and (2) expand-
ing the Open IE representation to allow additional
context information such as attribution and clausal
modifiers. OLLIE extractions obtain a dramatically
higher yield at higher or comparable precision rela-
tive to existing systems.
The outline of the paper is as follows. First, we
provide background on Open IE and how it relates
to Semantic Role Labeling (SRL). Section 3 de-
scribes the syntactic scope expansion component,
which is based on a novel approach that learns open
pattern templates. These are relation-independent
dependency parse-tree patterns that are automati-
cally learned using a novel bootstrapped training set.
Section 4 discusses the context analysis component,
which is based on supervised training with linguistic
and lexical features.
Section 5 compares OLLIE with REVERB and
WOEparse on a dataset from three domains: News,
Wikipedia, and a Biology textbook. We find that
OLLIE obtains 2.7 times the area in precision-yield
curves (AUC) as REVERB and 1.9 times the AUC
as WOEparse. Moreover, for specific relations com-
monly mediated by nouns (e.g., ?is the president
of?) OLLIE obtains two order of magnitude higher
yield. We also compare OLLIE to a state-of-the-art
SRL system (Johansson and Nugues, 2008) on an
IE-related end task and find that they both have com-
parable performance at argument identification and
have complimentary strengths in sentence analysis.
In Section 6 we discuss related work on pattern-
based relation extraction.
2 Background
Open IE systems extract tuples consisting of argu-
ment phrases from the input sentence and a phrase
1Available for download at http://openie.cs.washington.edu
from the sentence that expresses a relation between
the arguments, in the format (arg1; rel; arg2). This is
done without a pre-specified set of relations and with
no domain-specific knowledge engineering. We
compare OLLIE to two state-of-the-art Open IE sys-
tems: (1) REVERB (Fader et al2011), which
uses shallow syntactic processing to identify rela-
tion phrases that begin with a verb and occur be-
tween the argument phrases;2 (2) WOEparse (Wu
and Weld, 2010), which uses bootstrapping from en-
tries in Wikipedia info-boxes to learn extraction pat-
terns in dependency parses. Like REVERB, the
relation phrases begin with verbs, but can handle
long-range dependencies and relation phrases that
do not come between the arguments. Unlike RE-
VERB, WOE does not include nouns within the re-
lation phrases (e.g., cannot represent ?is the presi-
dent of? relation phrase). Both systems ignore con-
text around the extracted relations that may indi-
cate whether it is a supposition or conditionally true
rather than asserted as factual (see #4-5 in Figure 1).
The task of Semantic role labeling is to identify
arguments of verbs in a sentence, and then to clas-
sify the arguments by mapping the verb to a se-
mantic frame and mapping the argument phrases to
roles in that frame, such as agent, patient, instru-
ment, or benefactive. SRL systems can also identify
and classify arguments of relations that are mediated
by nouns when trained on NomBank annotations.
Where SRL begins with a verb or noun and then
looks for arguments that play roles with respect to
that verb or noun, Open IE looks for a phrase that ex-
presses a relation between a pair of arguments. That
phrase is often more than simply a single verb, such
as the phrase ?plays a role in?, or ?is the CEO of?.
3 Relational Extraction in OLLIE
Figure 2 illustrates OLLIE?s architecture for learning
and applying binary extraction patterns. First, it uses
a set of high precision seed tuples from REVERB to
bootstrap a large training set. Second, it learns open
pattern templates over this training set. Next, OLLIE
applies these pattern templates at extraction time.
This section describes these three steps in detail. Fi-
nally, OLLIE analyzes the context around the tuple
(Section 4) to add information (attribution, clausal
modifiers) and a confidence function.
2Available for download at http://reverb.cs.washington.edu/
524
ReVerb 
Seed Tuples 
Training Data 
Open Pattern Learning Bootstrapper 
Pattern Templates 
Pattern Matching Context Analysis Sentence Tuples Ext. Tuples 
Extraction 
Learning 
Figure 2: System architecture: OLLIE begins with seed
tuples from REVERB, uses them to build a bootstrap
training set, and learns open pattern templates. These are
applied to individual sentences at extraction time.
3.1 Constructing a Bootstrapping Set
Our goal is to automatically create a large training
set, which encapsulates the multitudes of ways in
which information is expressed in text. The key ob-
servation is that almost every relation can also be ex-
pressed via a REVERB-style verb-based expression.
So, bootstrapping sentences based on REVERB?s tu-
ples will likely capture all relation expressions.
We start with over 110,000 seed tuples ? these are
high confidence REVERB extractions from a large
Web corpus (ClueWeb)3 that are asserted at least
twice and contain only proper nouns in the argu-
ments. These restrictions reduce ambiguity while
still covering a broad range of relations. For ex-
ample, a seed tuple may be (Paul Annacone; is the
coach of; Federer) that REVERB extracts from the
sentence ?Paul Annacone is the coach of Federer.?
For each seed tuple, we retrieve all sentences in a
Web corpus that contains all content words in the
tuple. We obtain a total of 18 million sentences.
For our example, we will retrieve all sentences that
contain ?Federer?, ?Paul?, ?Annacone? and some syn-
tactic variation of ?coach?. We may find sentences
like ?Now coached by Annacone, Federer is win-
ning more titles than ever.?
Our bootstrapping hypothesis assumes that all
these sentences express the information of the orig-
inal seed tuple. This hypothesis is not always true.
As an example, for a seed tuple (Boyle; is born in;
Ireland) we may retrieve a sentence ?Felix G. Whar-
ton was born in Donegal, in the northwest of Ireland,
a county where the Boyles did their schooling.?
3http://lemurproject.org/clueweb09.php/
To reduce bootstrapping errors we enforce addi-
tional dependency restrictions on the sentences. We
only allow sentences where the content words from
arguments and relation can be linked to each other
via a linear path of size four in the dependency parse.
To implement this restriction, we only use the sub-
set of content words that are headwords in the parse
tree. In the above sentence ?Ireland?, ?Boyle? and
?born? connect via a dependency path of length six,
and hence this sentence is rejected from the training
set. This reduces our set to 4 million (seed tuple,
sentence) pairs.
In our implementation, we use Malt Dependency
Parser (Nivre and Nilsson, 2004) for dependency
parsing, since it is fast and hence, easily applica-
ble to a large corpus of sentences. We post-process
the parses using Stanford?s CCprocessed algorithm,
which compacts the parse structure for easier extrac-
tion (de Marneffe et al2006).
We randomly sampled 100 sentences from our
bootstrapping set and found that 90 of them sat-
isfy our bootstrapping hypothesis (64 without de-
pendency constraints). We find this quality to be sat-
isfactory for our needs of learning general patterns.
Bootstrapped data has been previously used to
generate positive training data for IE (Hoffmann et
al., 2010; Mintz et al2009). However, previous
systems retrieved sentences that only matched the
two arguments, which is error-prone, since multiple
relations can hold between a pair of entities (e.g.,
Bill Gates is the CEO of, a co-founder of, and has a
high stake in Microsoft).
Alternatively, researchers have developed sophis-
ticated probabilistic models to alleviate the effect
of noisy data (Riedel et al2010; Hoffmann et al
2011). In our case, by enforcing that a sentence ad-
ditionally contains some syntactic form of the rela-
tion content words, our bootstrapping set is naturally
much cleaner.
Moreover, this form of bootstrapping is better
suited for Open IE?s needs, as we will use this data
to generalize to other unseen relations. Since the
relation words in the sentence and seed match, we
can learn general pattern templates that may apply
to other relations too. We discuss this process next.
3.2 Open Pattern Learning
OLLIE?s next step is to learn general patterns that
encode various ways of expressing relations. OL-
525
Extraction Template Open Pattern
1. (arg1; be {rel} {prep}; arg2) {arg1} ?nsubjpass? {rel:postag=VBN} ?{prep ?}? {arg2}
2. (arg1; {rel}; arg2) {arg1} ?nsubj? {rel:postag=VBD} ?dobj? {arg2}
3. (arg1; be {rel} by; arg2) {arg1} ?nsubjpass? {rel:postag=VBN} ?agent? {arg2}
4. (arg1; be {rel} of; arg2) {rel:postag=NN;type=Person} ?nn? {arg1} ?nn? {arg2}
5. (arg1; be {rel} {prep}; arg2) {arg1} ?nsubjpass? {slot:postag=VBN;lex ?announce|name|choose...}
?dobj? {rel:postag=NN} ?{prep ?}? {arg2}
Figure 3: Sample open pattern templates. Notice that some patterns (1-3) are purely syntactic, and others are seman-
tic/lexically constrained (in bold font). A dependency parse that matches pattern #1 is shown in Figure 4.
LIE learns open pattern templates ? a mapping from
a dependency path to an open extraction, i.e., one
that identifies both the arguments and the exact
(REVERB-style) relation phrase. Figure 3 gives ex-
amples of high-frequency pattern templates learned
by OLLIE. Note that some of the dependency
paths are completely unlexicalized (#1-3), whereas
in other cases some nodes have lexical or semantic
restrictions (#4, 5).
Open pattern templates encode the ways in
which a relation (in the first column) may
be expressed in a sentence (second column).
For example, a relation (Godse; kill; Gandhi)
may be expressed with a dependency path (#2)
{Godse}?nsubj?{kill:postag=VBD}?dobj?{Gandhi}.
To learn the pattern templates, we first extract the
dependency path connecting the arguments and re-
lation words for each seed tuple and the associated
sentence. We annotate the relation node in the path
with the exact relation word (as a lexical constraint)
and the POS (postag constraint). We create a re-
lation template from the seed tuple by normalizing
?is?/?was?/?will be? to ?be?, and replacing the rela-
tion content word with {rel}.4
If the dependency path has a node that is not part
of the seed tuple, we call it a slot node. Intuitively,
if slot words do not negate the tuple they can be
skipped over. As an example, ?hired? is a slot word
for the tuple (Annacone; is the coach of; Federer) in
the sentence ?Federer hired Annacone as a coach?.
We associate postag and lexical constraints with the
slot node as well. (see #5 in Figure 3).
Next, we perform several syntactic checks on
each candidate pattern. These checks are the con-
straints that we found to hold in very general pat-
terns, which we can safely generalize to other un-
seen relations. The checks are: (1) There are no slot
4Our current implementation only allows a single relation
content word; extending to multiple words is straightforward ?
the templates will require rel1, rel2,. . .
nodes in the path. (2) The relation node is in the
middle of arg1 and arg2. (3) The preposition edge
(if any) in the pattern matches the preposition in the
relation. (4) The path has no nn or amod edges.
If the checks hold true we accept it as a purely
syntactic pattern with no lexical constraints. Oth-
ers are semantic/lexical patterns and require further
constraints to be reliable as extraction patterns.
3.2.1 Purely Syntactic Patterns
For syntactic patterns, we aggressively general-
ize to unseen relations and prepositions. We remove
all lexical restrictions from the relation nodes. We
convert all preposition edges to an abstract {prep ?}
edge. We also replace the specific prepositions in
extraction templates with {prep}.
As an example, consider the sentences, ?Michael
Webb appeared on Oprah...? and ?...when Alexan-
der the Great advanced to Babylon.? and associ-
ated seed tuples (Michael Webb; appear on; Oprah)
and (Alexander; advance to; Babylon). Both these
data points return the same open pattern after gen-
eralization: ?{arg1} ?nsubj? {rel:postag=VBD}
?{prep ?}? {arg2}? with the extraction template
(arg1, {rel} {prep}, arg2). Other examples of syn-
tactic pattern templates are #1-3 in Figure 3.
3.2.2 Semantic/Lexical Patterns
Patterns that do not satisfy the checks are not as
general as those that do, but are still important. Con-
structions like ?Microsoft co-founder Bill Gates...?
work for some relation words (e.g., founder, CEO,
director, president, etc.) but would not work for
other nouns; for instance, from ?Chicago Symphony
Orchestra? we should not conclude that (Orchestra;
is the Symphony of; Chicago).
Similarly, we may conclude (Annacone; is the
coach of; Federer) from the sentence ?Federer hired
Annacone as a coach.?, but this depends on the se-
mantics of the slot word, ?hired?. If we replaced
526
?hired? by ?fired? or ?considered? then the extraction
would be false.
To enable such patterns we retain the lexical con-
straints on the relation words and slot words.5 We
collect all patterns together based only on the syn-
tactic restrictions and convert the lexical constraint
into a list of words with which the pattern was seen.
Example #5 in Figure 3 shows one such lexical list.
Can we generalize these lexically-annotated pat-
terns further? Our insight is that we can generalize
a list of lexical items to other similar words. For
example, if we see a list like {CEO, director, presi-
dent, founder}, then we should be able to generalize
to ?chairman? or ?minister?.
Several ways to compute semantically similar
words have been suggested in the literature like
Wordnet-based, distributional similarity, etc. (e.g.,
(Resnik, 1996; Dagan et al1999; Ritter et al
2010)). For our proof of concept, we use a simple
overlap metric with two important Wordnet classes
? Person and Location. We generalize to these types
when our list has a high overlap (> 75%) with hy-
ponyms of these classes. If not, we simply retain the
original lexical list without generalization. Example
#4 in Figure 3 is a type-generalized pattern.
We combine all syntactic and semantic patterns
and sort in descending order based on frequency of
occurrence in the training set. This imposes a natural
ranking on the patterns ? more frequent patterns are
likely to give higher precision extractions.
3.3 Pattern Matching for Extraction
We now describe how these open patterns are used
to extract binary relations from a new sentence. We
first match the open patterns with the dependency
parse of the sentence and identify the base nodes for
arguments and relations. We then expand these to
convey all the information relevant to the extraction.
As an example, consider the sentence: ?I learned
that the 2012 Sasquatch music festival is scheduled
for May 25th until May 28th.? Figure 4 illustrates the
dependency parse. To apply pattern #1 from Figure
3 we first match arg1 to ?festival?, rel to ?scheduled?
and arg2 to ?25th? with prep ?for?. However, (festi-
val, be scheduled for, 25th) is not a very meaningful
extraction. We need to expand this further.
5For highest precision extractions, we may also need seman-
tic constraints on the arguments. In this work, we increase our
yield by ignoring the argument-type constraints.
learned_VBD 
I_PRP scheduled_VBN 
that_IN festival_NN is_VBZ 25th_NNP 28th_NNP 
the_DET Sasquatch_NNP music_NN May_NNP_11 May_NNP_14 2012_CD 
nsubj ccomp 
complm nsubjpass auxpass prep_for prep_until 
det num nn nn nn nn 
Figure 4: A sample dependency parse. The col-
ored/greyed nodes represent all words that are extracted
from the pattern {arg1} ?nsubjpass? {rel:postag=VBN}
?{prep ?}? {arg2}. The extraction is (the 2012
Sasquatch Music Festival; is scheduled for; May 25th).
For the arguments we expand on amod, nn, det,
neg, prep of, num, quantmod edges to build the
noun-phrase. When the base noun is not a proper
noun, we also expand on rcmod, infmod, partmod,
ref, prepc of edges, since these are relative clauses
that convey important information. For relation
phrases, we expand on advmod, mod, aux, auxpass,
cop, prt edges. We also include dobj and iobj in the
case that they are not in an argument. After identi-
fying the words in arg/relation we choose their order
as in the original sentence. For example, these rules
will result in the extraction (the Sasquatch music fes-
tival; be scheduled for; May 25th).
3.4 Comparison with WOEparse
OLLIE?s algorithm is similar to that of WOEparse
? both systems follow the basic structure of boot-
strap learning of patterns based on dependency parse
paths. However, there are three significant differ-
ences. WOE uses Wikipedia-based bootstrapping,
finding a sentence in a Wikipedia article that con-
tains the infobox values. Since WOE does not have
access to a seed relation phrase, it heuristically as-
signs all intervening words between the arguments
in the parse as the relation phrase. This often results
in under-specified or nonsensical relation phrases.
For example, from the sentence ?David Miscavige
learned that after Tom Cruise divorced Mimi Rogers,
he was pursuing Nicole Kidman.? WOE?s heuristics
will extract the relation divorced was pursuing be-
tween ?Tom Cruise? and ?Nicole Kidman?. OLLIE,
in contrast, produces well-formed relation phrases
by basing its templates on REVERB relation phrases.
Secondly, WOE does not assign semantic/lexical
restrictions to its patterns, and thus, has lower preci-
sion due to aggressive syntactic generalization. Fi-
nally, WOE is designed to have verb-mediated rela-
527
tion phrases that do not include nouns, thus missing
important relations such as ?is the president of?. In
our experiments (see Figure 5) we find WOEparse to
have lower precision and yield than OLLIE.
4 Context Analysis in OLLIE
We now turn to the context analysis component,
which handles the problem of extractions that are not
asserted as factual in the text. In some cases, OLLIE
can handle this by extending the tuple representation
with an extra field that turns an otherwise incorrect
tuple into a correct one. In other cases, there is no re-
liable way to salvage the extraction, and OLLIE can
avoid an error by giving the tuple a low confidence.
Cases where OLLIE extends the tuple representa-
tion include conditional truth and attribution. Con-
sider sentence #4 in Figure 1. It is not asserting that
the earth is the center of the universe. OLLIE adds
an AttributedTo field, which makes the final extrac-
tion valid (see OLLIE extraction in Figure 1). This
field indicates who said, suggested, believes, hopes,
or doubts the information in the main extraction.
Another case is when the extraction is only condi-
tionally true. Sentence #5 in Figure 1 does not assert
as factual that (Romney; will be elected; President),
so it is an incorrect extraction. However, adding
a condition (?if he wins five states?) can turn this
into a correct extraction. We extend OLLIE to have
a ClausalModifier field when there is a dependent
clause that modifies the main extraction.
Our approach for extracting these additional fields
makes use of the dependency parse structure. We
find that attributions are marked by a ccomp (clausal
complement) edge. For example, in the parse of sen-
tence #4 there is a ccomp edge between ?believe?
and ?center?. Our algorithm first checks for the pres-
ence of a ccomp edge to the relation node. However,
not all ccomp edges are attributions. We match the
context verb (e.g., ?believe?) with a list of commu-
nication and cognition verbs from VerbNet (Schuler,
2006) to detect attributions. The context verb and its
subject then populate the AttributedTo field.
Similarly, the clausal modifiers are marked by ad-
vcl (adverbial clause) edge. We filter these lexically,
and add a ClausalModifier field when the first word
of the clause matches a list of 16 terms created using
a training set: {if, when, although, because, ...}.
OLLIE has high precision for AttributedTo and
ClausalModifier fields, nearly 98% on a develop-
ment set, however, these two fields do not cover all
the cases where an extraction is not asserted as fac-
tual. To handle others, we train OLLIE?s confidence
function to reduce the confidence of an extraction if
its context indicates it is likely to be non-factual.
We use a supervised logistic regression classifier
for the confidence function. Features include the
frequency of the extraction pattern, the presence of
AttributedTo or ClausalModifier fields, and the po-
sition of certain words in the extraction?s context,
such as function words or the communication and
cognition verbs used for the AttributedTo field. For
example, one highly predictive feature tests whether
or not the word ?if? comes before the extraction
when no ClausalModifier fields are attached. Our
training set was 1000 extractions drawn evenly from
Wikipedia, News, and Biology sentences.
5 Experiments
Our experiments evaluate three main questions. (1)
How does OLLIE?s performance compare with exis-
ting state-of-the-art open extractors? (2) What are
the contributions of the different sub-components
within OLLIE? (3) How do OLLIE?s extractions com-
pare with semantic role labeling argument identifi-
cation?
5.1 Comparison of Open IE Systems
Since Open IE is designed to handle a variety of
domains, we create a dataset of 300 random sen-
tences from three sources: News, Wikipedia and Bi-
ology textbook. The News and Wikipedia test sets
are a random subset of Wu and Weld?s test set for
WOEparse. We ran three systems, OLLIE, REVERB
and WOEparse on this dataset resulting in a total of
1,945 extractions from all three systems. Two an-
notators tagged the extractions as correct if the sen-
tence asserted or implied that the relation was true.
Inter-annotator agreement was 0.96, and we retained
the subset of extractions on which the two annotators
agree for further analysis.
All systems associate a confidence value with an
extraction ? ranking with these confidence values
generates a precision-yield curve for this dataset.
Figure 5 reports the curves for the three systems.
We find that OLLIE has a higher performance, ow-
ing primarily to its higher yield at comparable preci-
528
0.5  
0.6  
0.7  
0.8  
0.9  
1  
0  100  200  300  400  500  600  
OLLIE  
ReVerb 
WOE  
Yield 
Pre
cis
ion
 
parse 
Figure 5: Comparison of different Open IE systems. OL-
LIE achieves substantially larger area under the curve
than other Open IE systems.
sion. OLLIE finds 4.4 times more correct extractions
than REVERB and 4.8 times more than WOEparse at
a precision of about 0.75. Overall, OLLIE has 2.7
times larger area under the curve than REVERB and
1.9 times larger than WOEparse.6 We use the Boot-
strap test (Cohen, 1995) to find that OLLIE?s better
performance compared to the two systems is highly
statistically significant.
We perform further analysis to understand the rea-
sons behind the high yield from OLLIE. We find that
40% of the OLLIE extractions that REVERB misses
are due to OLLIE?s use of parsers ? REVERB misses
those because its shallow syntactic analysis cannot
skip over the intervening clauses or prepositional
phrases between the relation phrase and the argu-
ments. About 30% of the additional yield is those
extractions where the relation is not between its ar-
guments (see instance #1 in Figure 1). The rest are
due to other causes such as OLLIE?s ability to handle
relationships mediated by nouns and adjectives, or
REVERB?s shallow syntactic analysis, etc. In con-
trast, OLLIE misses very few extractions returned by
REVERB, mostly due to parser errors.
We find that WOEparse misses extractions found
by OLLIE for a variety of reasons. The primary
cause is that WOEparse does not include nouns in re-
lation phrases. It also misses some verb-based pat-
terns, probably due to training noise. In other cases,
WOEparse misses extractions due to ill-formed rela-
tion phrases (as in the example of Section 3.4: ?di-
vorced was pursuing? instead of the correct relation
?was pursuing?).
While the bulk of OLLIE?s extractions in our test
6Evaluating recall is difficult at this scale ? however, since
yield is proportional to recall, the area differences also hold for
the equivalent precision-recall curves.
Relation OLLIE REVERB incr.
is capital of 8,566 146 59x
is president of 21,306 1,970 11x
is professor at 8,334 400 21x
is scientist of 730 5 146x
Figure 6: OLLIE finds many more correct extractions for
relations that are typically expressed by noun phrases ?
up to 146 times that of REVERB. WOEparse outputs no
instances of these, because it does not allow nouns in the
relation. These results are at point of maximum yield
(with comparable precisions around 0.66).
set were verb-mediated, our intuition suggests that
there exist many relationships that are most natu-
rally expressed via noun phrases. To demonstrate
this effect, we chose four such relations ? is capi-
tal of, is president of, is professor at, and is scientist
of. We ran our systems on 100 million random sen-
tences from the ClueWeb corpus. Figure 6 reports
the yields of these four relations.7
OLLIE found up to 146 times as many extrac-
tions for these relations than REVERB. Because
WOEparse does not include nouns in relation phrases,
it is unable to extract any instance of these relations.
We examine a sample of the extractions to verify that
noun-mediated extractions are the main reason for
this large yield boost over REVERB (73% of OLLIE
extractions were noun-mediated). High-frequency
noun patterns like ?Obama, the president of the US?,
?Obama, the US president?, ?US President Obama?
far outnumber sentences of the form ?Obama is the
president of the US?. These relations are seldom the
primary information in a sentence, and are typically
mentioned in passing in noun phrases that express
the relation.
For some applications, noun-mediated relations
are important, as they associate people with work
places and job titles. Overall, we think of the results
in Figure 6 as a ?best case analysis? that illustrates
the dramatic increase in yield for certain relations,
due to syntactic scope expansion in Open IE.
5.2 Analysis of OLLIE
We perform two control experiments to understand
the value of semantic/lexical restrictions in pattern
learning and precision boost due to context analysis
component.
7We multiply the total number of extractions with precision
on a sample for that relation to estimate the yield.
529
0  
0.2  
0.4  
0.6  
0.8  
1  
0  10  20  30  40  50  60  
OLLIE  
OLLIE[Lex]  
OLLIE[syn]  
Yield 
Pre
cis
ion
 
Figure 7: Results on the subset of extractions from pat-
terns with semantic/lexical restrictions. Ablation study
on patterns with semantic/lexical restrictions. These pat-
terns without restrictions (OLLIE[syn]) result in low pre-
cision. Type generalization improves yield compared to
patterns with only lexical constraints (OLLIE[lex]).
Are semantic restrictions important for open pat-
tern learning? How much does type generalization
help? To answer these questions we compare three
systems ? OLLIE without semantic or lexical restric-
tions (OLLIE[syn]), OLLIE with lexical restrictions
but no type generalization (OLLIE[lex]) and the full
system (OLLIE). We restrict this experiment to the
patterns where OLLIE adds semantic/lexical restric-
tions, rather than dilute the result with patterns that
would be unchanged by these variants.
Figure 7 shows the results of this experiment on
our dataset from three domains. As the curves
show, OLLIE was correct to add lexical/semantic
constraints to these patterns ? precision is quite low
without the restrictions. This matches our intuition,
since these are not completely general patterns and
generalizing to all unseen relations results in a large
number of errors. OLLIE[lex] performs well though
at lower yield. The type generalization helps the
yield somewhat, without hurting the precision. We
believe that a more data-driven type generalization
that uses distributional similarity (e.g., (Ritter et al
2010)) may help much more. Also, notice that over-
all precision numbers are lower, since these are the
more difficult relations to extract reliably. We con-
clude that lexical/semantic restrictions are valuable
for good performance of OLLIE.
We also compare our full system to a version that
does not use the context analysis of Section 4. Fig-
ure 8 compares OLLIE to a version (OLLIE[pat]) that
does not add the AttributedTo and ClausalModifier
fields, and, instead of context-sensitive confidence
function, uses the pattern frequency in the training
0.5  
0.6  
0.7  
0.8  
0.9  
1  
0  100  200  300  400  500  600  
OLLIE  
OLLIE[pat]  
Yield 
Pre
cis
ion
 
Figure 8: Context analysis increases precision, raising the
area under the curve by 19%.
set as a ranking function. 10% of the sentences have
an OLLIE extraction with ClausalModifier and 6%
have AttributedTo fields. Adding ClausalModifier
corrects errors for 21% of extractions that have a
ClausalModifier and does not introduce any new er-
rors. Adding AttributedTo corrects errors for 55%
of the extractions with AttributedTo and introduces
an error for 3% of the extractions. Overall, we find
that OLLIE gives a significant boost to precision over
OLLIE[pat] and obtains 19% additional AUC.
Finally, we analyze the errors made by OLLIE.
Unsurprisingly, because of OLLIE?s heavy reliance
on the parsers, parser errors account for a large part
of OLLIE?s errors (32%). 18% of the errors are due
to aggressive generalization of a pattern to all un-
seen relations and 12% due to incorrect application
of lexically annotated patterns. About 14% of the er-
rors are due to important context missed by OLLIE.
Another 12% of the errors are because of the limita-
tions of binary representation, which misses impor-
tant information that can only be expressed in n-ary
tuples.
We believe that as parsers become more robust
OLLIE?s performance will improve even further. The
presence of context-related errors suggests that there
is more to investigate in context analysis. Finally, in
the future we wish to extend the representation to
include n-ary extractions.
5.3 Comparison with SRL
Our final evaluation suggests answers to two im-
portant questions. First, how does a state-of-the-art
Open IE system do in terms of absolute recall? Sec-
ond, how do Open IE systems compare against state-
of-the-art SRL systems?
SRL, as discussed in Section 2, has a very dif-
ferent goal ? analyzing verbs and nouns to identify
530
their arguments, then mapping the verb or noun to
a semantic frame and determining the role that each
argument plays in that frame. These verbs and nouns
need not make the full relation phrase, although, re-
cent work has shown that they may be converted
to Open IE style extractions with additional post-
processing (Christensen et al2011).
While a direct comparison between OLLIE and
a full SRL system is problematic, we can compare
performance of OLLIE and the argument identifica-
tion step of an SRL system. We set each system the
following task ? ?based on a sentence, find all noun-
pairs that have an asserted relationship.? This task is
permissive for both systems, as it does not require
finding an exact relation phrase or argument bound-
ary, or determining the argument roles in a relation.
We create a gold standard by tagging a random
50 sentences of our test set to identify all pairs of
NPs that have an asserted relation. We only counted
relation expressed by a verb or noun in the text, and
did not include relations expressed simply with ?of?
or apostrophe-s. Where a verb mediates between an
argument and multiple NPs, we represent this as a
binary relation for all pairs of NPs.
For example the sentence, ?Macromolecules
translocated through the phloem include proteins
and various types of RNA that enter the sieve tubes
through plasmodesmata.? has five binary relations.
arg1: arg2: relation term
Macromolecules phloem translocated
Macromolecules proteins include
Macromolecules types of RNA include
types of RNA sieve tubes enter
types of RNA plasmodesmata enter
We find an average of 4.0 verb-mediated relations
and 0.3 noun-mediated relations per sentence. Eval-
uating OLLIE against this gold standard helps to an-
swer the question of absolute recall: what percent-
age of binary relations expressed in a sentence can
our systems identify.
For comparison, we use a state-of-the-art SRL
system from Lund University (Johansson and
Nugues, 2008), which is trained on PropBank
(Martha and Palmer, 2002) for its verb-frames and
NomBank (Meyers et al2004) for noun-frames.
The PropBank version of the system won the very
competitive 2008 CONLL SRL evaluation.
We conduct this experiment by manually compar-
LUND OLLIE union
Verb relations 0.58 (0.69) 0.49 (0.55) 0.71 (0.83)
Noun relations 0.07 (0.33) 0.13 (0.13) 0.20 (0.33)
All relations 0.54 (0.67) 0.47 (0.52) 0.67 (0.80)
Figure 9: Recall of LUND and OLLIE on binary relations.
In parentheses is recall with oracle co-reference. Both
systems identify approximately half of all argument pairs,
but have lower recall on noun-mediated relations.
ing the outputs of LUND and OLLIE against the gold
standard. For each pair of NPs in the gold standard
we determine whether the systems find a relation
with that pair of NPs as arguments. Recall is based
on the percentage of NP pairs where the head nouns
matches head nouns of two different arguments in an
extraction or semantic frame. If the argument value
is conjunctive, we count a match against the head
noun of each item in the list. We also count cases
where system output would match the gold standard,
given perfect co-reference.
Figure 9 shows the recall for OLLIE and LUND,
with recall based on oracle co-referential matches
in parentheses. Our analysis shows strong recall
for both systems for verb-mediated relations: LUND
finding about two thirds of the argument pairs and
OLLIE finding over half. Both systems have low
recall for noun-mediated relations, with most of
LUND?s recall requiring co-reference. We observe
that a union of the two systems raises recall to
0.71 for verb-mediated relations and 0.83 with co-
reference, demonstrating that each system is identi-
fying argument pairs that the other missed.
It is not surprising that OLLIE has recall of ap-
proximately 0.5, since it is tuned for high precision
extraction, and avoids less reliable extractions from
constructions such as reduced relative clauses and
gerunds, or from noun-mediated relations with long-
range dependencies. In contrast, SRL is tuned to
identify the argument structure for nearly all verbs
and nouns in a sentence. The missing recall from
SRL is primarily where it does not identify both ar-
guments of a binary relation, or where the correct
argument is buried in a long argument phrase, but is
not its head noun.
It is surprising that LUND, trained on Nom-
Bank, identifies so few noun-mediated argument
pairs without co-reference. An example will make
this clear. For the sentence, ?Clarcor, a maker of
packaging and filtration products, said ...?, the tar-
531
get relation is between Clarcor and the products it
makes. LUND identifies a frame maker.01 in which
argument A0 has head noun ?maker? and A1 is a PP
headed by ?products?, missing the actual name of the
maker without co-reference post-processing. OLLIE
finds the extraction (Clarcor; be a maker of; packag-
ing and filtration products) where the heads of both
arguments matched those of the target. In another
example, LUND identifies ?his? and ?brother? as the
arguments of the frame brother.01, rather than the
actual names of the two brothers.
We can draw several conclusions from this exper-
iment. First, nouns, although less frequently mediat-
ing relations, are much harder and both systems are
failing significantly on those ? OLLIE is somewhat
better. Two, neither systems dominates the other;
in fact, recall is increased significantly by a union
of the two. Three, and probably most importantly,
significant information is still being missed by both
systems, and more research is warranted.
6 Related Work
There is a long history of bootstrapping and pat-
tern learning approaches in traditional informa-
tion extraction, e.g., DIPRE (Brin, 1998), Snow-
Ball (Agichtein and Gravano, 2000), Espresso (Pan-
tel and Pennacchiotti, 2006), PORE (Wang et al
2007), SOFIE (Suchanek et al2009), NELL (Carl-
son et al2010), and PROSPERA (Nakashole et
al., 2011). All these approaches first bootstrap data
based on seed instances of a relation (or seed data
from existing resources such as Wikipedia) and then
learn lexical or lexico-POS patterns to create an ex-
tractor. Other approaches have extended these to
learning patterns based on full syntactic analysis of
a sentence (Bunescu and Mooney, 2005; Suchanek
et al2006; Zhao and Grishman, 2005).
OLLIE has significant differences from the previ-
ous work in pattern learning. First, and most impor-
tantly, these previous systems learn an extractor for
each relation of interest, whereas OLLIE is an open
extractor. OLLIE?s strength is its ability to gener-
alize from one relation to many other relations that
are expressed in similar forms. This happens both
via syntactic generalization and type generalization
of relation words (sections 3.2.1 and 3.2.2). This ca-
pability is essential as many relations in the test set
are not even seen in the training set ? in early exper-
iments we found that non-generalized pattern learn-
ing (equivalent to traditional IE) had significantly
less yield at a slightly higher precision.
Secondly, previous systems begin with seeds that
consist of a pair of entities, whereas we also in-
clude the content words from REVERB relations in
our training seeds. This results in a much higher
precision bootstrapping set and high rule preci-
sion while still allowing morphological variants that
cover noun-mediated relations. A third difference is
in the scale of the training ? REVERB yields millions
of training seeds, where previous systems had orders
of magnitude less. This enables OLLIE to learn pat-
terns with greater coverage.
The closest to our work is the pattern learning
based open extractor WOEparse. Section 3.4 de-
tails the differences between the two extractors. An-
other extractor, StatSnowBall (Zhu et al2009), has
an Open IE version, which learns general but shal-
low patterns. Preemptive IE (Shinyama and Sekine,
2006) is a paradigm related to Open IE that first
groups documents based on pairwise vector cluster-
ing, then applies additional clustering to group en-
tities based on document clusters. The clustering
steps make it difficult for it to scale to large corpora.
7 Conclusions
Our work describes OLLIE, a novel Open IE ex-
tractor that makes two significant advances over
the existing Open IE systems. First, it expands
the syntactic scope of Open IE systems by identi-
fying relationships mediated by nouns and adjec-
tives. Our experiments found that for some rela-
tions this increases the number of correct extrac-
tions by two orders of magnitude. Second, by an-
alyzing the context around an extraction, OLLIE is
able to identify cases where the relation is not as-
serted as factual, but is hypothetical or conditionally
true. OLLIE increases precision by reducing con-
fidence in those extractions or by associating addi-
tional context in the extractions, in the form of at-
tribution and clausal modifiers. Overall, OLLIE ob-
tains 1.9 to 2.7 times more area under precision-
yield curves compared to existing state-of-the-art
open extractors. OLLIE is available for download at
http://openie.cs.washington.edu.
532
Acknowledgments
This research was supported in part by NSF grant IIS-0803481,
ONR grant N00014-08-1-0431, DARPA contract FA8750-09-
C-0179 and the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Air Force Research Laboratory (AFRL) con-
tract number FA8650-10-C-7058. The U.S. Government is au-
thorized to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily representing
the official policies or endorsements, either expressed or im-
plied, of IARPA, AFRL, or the U.S. Government. This research
is carried out at the University of Washington?s Turing Center.
We thank Fei Wu and Dan Weld for providing WOE?s code
and Anthony Fader for releasing REVERB?s code. Peter Clark,
Alan Ritter, and Luke Zettlemoyer provided valuable feedback
on the research and Dipanjan Das helped us with state-of-the-
art SRL systems. We also thank the anonymous reviewers for
their comments on an earlier draft.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In
Procs. of the Fifth ACM International Conference on
Digital Libraries.
ARPA. 1991. Proc. 3rd Message Understanding Conf.
Morgan Kaufmann.
ARPA. 1998. Proc. 7th Message Understanding Conf.
Morgan Kaufmann.
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
S. Brin. 1998. Extracting Patterns and Relations from the
World Wide Web. In WebDB Workshop at 6th Interna-
tional Conference on Extending Database Technology,
EDBT?98, pages 172?183, Valencia, Spain.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proc. of HLT/EMNLP.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Procs. of AAAI.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2011. An analysis of open informa-
tion extraction based on semantic role labeling. In
Proceedings of the 6th International Conference on
Knowledge Capture (K-CAP ?11).
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Lan-
guage Resources and Evaluation (LREC 2006).
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam. 2011. Open infor-
mation extraction: the second generation. In Proceed-
ings of the International Joint Conference on Artificial
Intelligence (IJCAI ?11).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 286?
295.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S.
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In ACL, pages 541?550.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role label-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING 08),
pages 393?400.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC 02).
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. Annotating
Noun Argument Structure for NomBank. In Proceed-
ings of LREC-2004, Lisbon, Portugal.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP ?09: Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
2, pages 1003?1011.
Ndapandula Nakashole, Martin Theobald, and Gerhard
Weikum. 2011. Scalable knowledge harvesting with
high precision and high recall. In Proceedings of the
Fourth International Conference on Web Search and
Web Data Mining (WSDM 2011), pages 227?236.
Joakim Nivre and Jens Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of the Conference
on Natural Language Learning (CoNLL-04), pages
49?56.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of 21st Interna-
tional Conference on Computational Linguistics and
533
44th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?06).
P. Resnik. 1996. Selectional constraints: an information-
theoretic model and its computational realization.
Cognition.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In ECML/PKDD (3), pages 148?163.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL ?10).
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
Y. Shinyama and S. Sekine. 2006. Preemptive informa-
tion extraction using unrestricted relation discovery.
In Procs. of HLT/NAACL.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Procs. of KDD, pages 712?717.
Fabian M. Suchanek, Mauro Sozio, and Gerhard
Weikum. 2009. Sofie: a self-organizing framework
for information extraction. In Proceedings of WWW,
pages 631?640.
Gang Wang, Yong Yu, and Haiping Zhu. 2007. Pore:
Positive-only relation extraction from wikipedia text.
In Proceedings of 6th International Semantic Web
Conference and 2nd Asian Semantic Web Conference
(ISWC/ASWC?07), pages 580?594.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL ?10).
Shubin Zhao and Ralph Grishman. 2005. Extracting re-
lations with integrated information using kernel meth-
ods. In Procs. of ACL.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. StatSnowball: a statistical ap-
proach to extracting entity relationships. In WWW
?09: Proceedings of the 18th international conference
on World Wide Web, pages 101?110, New York, NY,
USA. ACM.
534
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 893?903, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities
Thomas Lin, Mausam, Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{tlin, mausam, etzioni}@cs.washington.edu
Abstract
Entity linking systems link noun-phrase men-
tions in text to their corresponding Wikipedia
articles. However, NLP applications would
gain from the ability to detect and type all
entities mentioned in text, including the long
tail of entities not prominent enough to have
their own Wikipedia articles. In this paper we
show that once the Wikipedia entities men-
tioned in a corpus of textual assertions are
linked, this can further enable the detection
and fine-grained typing of the unlinkable en-
tities. Our proposed method for detecting un-
linkable entities achieves 24% greater accu-
racy than a Named Entity Recognition base-
line, and our method for fine-grained typing is
able to propagate over 1,000 types from linked
Wikipedia entities to unlinkable entities. De-
tection and typing of unlinkable entities can
increase yield for NLP applications such as
typed question answering.
1 Introduction
A key challenge in machine reading (Etzioni et al
2006) is to identify the entities mentioned in text,
and associate them with appropriate background in-
formation such as their type. Consider the sentence
?Some people think that pineapple juice is good for
vitamin C.? To analyze this sentence, a machine
should know that ?pineapple juice? refers to a bev-
erage, while ?vitamin C? refers to a nutrient.
Entity linking (Bunescu and Pas?ca, 2006;
Cucerzan, 2007) addresses this problem by link-
ing noun phrases within the sentence to entries
in a large, fixed entity catalog (almost always
example noun phrases status
?apple juice? ?orange juice? present
?prune juice? ?wheatgrass juice? absent
?radiation exposure? ?workplace stress? present
?asbestos exposure? ?financial stress? absent
?IJCAI? ?OOPSLA? present
?EMNLP? ?ICAPS? absent
Table 1: Wikipedia has entries for prominent entities,
while missing tail and new entities of the same types.
Wikipedia). Thus, entity linking has a limited and
somewhat arbitrary range. In our example, systems
by (Ferragina and Scaiella, 2010) and (Ratinov et
al., 2011) both link ?vitamin C? correctly, but link
?pineapple juice? to ?pineapple.? ?Pineapple juice?
is not entity linked as a beverage because it is not
prominent enough to have its own Wikipedia entry.
As Table 1 shows, Wikipedia often has prominent
entities, while missing tail and new entities of the
same types.1 (Wang et al2012) notes that there
are more than 900 different active shoe brands, but
only 82 exist in Wikipedia. In scenarios such as in-
telligence analysis and local search, non-Wikipedia
entities are often the most important.
Hence, we introduce the unlinkable noun phrase
problem: Given a noun phrase that does not link
into Wikipedia, return whether it is an entity, as well
its fine-grained semantic types. Deciding if a non-
Wikipedia noun phrase is an entity is challenging
because many of them are not entities (e.g., ?Some
people,? ?an addition? and ?nearly half?). Predict-
1The same problem occurs with Freebase, which is also
missing the same Table 1 entities.
893
ing semantic types is a challenge because of the di-
versity of entity types in general text. In our experi-
ments, we utilized the Freebase type system, which
contains over 1,000 semantic types.
The first part of this paper proposes a novel
method for detecting entities by observing that enti-
ties often have different usage-over-time character-
istics than non-entities. Evaluation shows that our
method achieves 24% relative accuracy gain over
a NER baseline. The second part of this paper
shows how instance-to-instance class propagation
(Kozareva et al2011) can be adapted and scaled to
semantically type general noun-phrase entities using
types from linked entities, by leveraging over one
million different possible textual relations.
Contributions of our research include:
? We motivate and introduce the unlinkable noun
phrase problem, which extends previous work
in entity linking.
? We propose a novel method for discriminating
entities from arbitrary noun phrases, utilizing
features derived from Google Books ngrams.
? We adapt and scale instance-to-instance class
propagation in order to associate types with
non-Wikipedia entities.
? We implement and evaluate our methods, em-
pirically verifying improvement over appropri-
ate baselines.
2 Background
In this section we provide an overview of entity link-
ing, how we entity link our data set, and describe
how our problem and approach differ from related
areas such as NER and Web extraction.
2.1 Entity Linking
Given text, the task of entity linking (Bunescu
and Pas?ca, 2006; Cucerzan, 2007; Milne and Wit-
ten, 2008; Kulkarni et al2009) is to identify the
Wikipedia entities within the text, and mark them
with which Wikipedia entity they correspond to. En-
tity linking elevates us from plain text into mean-
ingful entities that have properties, semantic types,
and relationships with each other. Other entity cata-
logs can be used in place of Wikipedia, especially in
domain-specific contexts, but general purpose link-
ing systems all use Wikipedia because of its broad
general coverage, and to leverage its article texts and
link structure during the linking process.
A problem we observed when using entity link-
ing systems is that despite containing over 3 million
entities, Wikipedia does not cover a significant num-
ber of entities. This occurs with entities that are not
prominent enough to have their own dedicated arti-
cle and with entities that are very new. For exam-
ple, Facebook has over 600 million users, and each
of them could be considered an entity. The REVERB
extractor (Fader et al2011) on the ClueWeb09 Web
corpus found over 1.4 billion noun phrases partic-
ipating in textual relationships, and a sizable por-
tion of these noun phrases are entities. While re-
cent research has used NIL features to determine
whether they are being asked to link an entity not in
Wikipedia (Dredze et al2010; Ploch, 2011), there
has been no research on whether given noun phrases
that are unlinkable (for not being in Wikipedia) are
entities, and how to make them usable if they are.
Our goal is to address this problem of learning
whether non-Wikipedia noun phrases are entities,
and assigning semantic types to them to make them
useful. We begin with a corpus of 15 million ?(noun
phrase subject, textual relation, noun phrase object)?
assertions from the Web that were extracted by RE-
VERB (Fader et al2011).2 REVERB already filters
out relative pronouns, WHO-adverbs, and existential
?there? noun phrases that do not make meaningful
arguments. We then employ standard entity linking
techniques including string matching, prominence
priors (Fader et al2009), and context matching
(Bunescu and Pas?ca, 2006) to link the noun phrase
subjects into Wikipedia.
In this manner, we were able to entity link the
noun phrase subject of 9,699,967 extractions, while
the remaining 5,028,301 extractions had no matches
(mostly due to no close string matches). There were
1,401,713 distinct noun phrase subjects in the 5 mil-
lion extractions that had no matches. These are the
unlinkable noun phrases we will study here.
2.2 Named Entity Recognition
Named Entity Recognition (NER) is the task of
identifying named entities in text. A key difference
between our final goals and NER is that in the con-
2available at http://reverb.cs.washington.edu
894
text of entity linking and Wikipedia, there are many
more entities than just the named entities. For ex-
ample, ?apple juice? and ?television? are Wikipedia
entities (with Wikipedia articles), but are not tradi-
tional named entities. Still, as named entities do
comprise a sizable portion of our unlinkable noun
phrases, we compare against a NER baseline in our
entity detection step.
Fine-grained NER (Sekine and Nobata, 2004; Lee
et al2007) has studied scaling NER to up to 200 se-
mantic types. This differs from our semantic typing
of unlinked entities because our approach assumes
access to corpora-level relationships between a large
set of linked entities (with semantic types) and the
unlinked entities. As a result we are able to propa-
gate 1,339 Freebase semantic types from the linked
entities to the unlinked entities, which is substan-
tially more types than fine-grained NER.
2.3 Extracting Entity Sets
There is a line of research in using Web extraction
(Etzioni et al2005) and entity set expansion (Pantel
et al2009) to extract lists of typed entities from the
Web (e.g., a list of every city). Our problem instead
focuses on determining whether any individual noun
phrase is an entity, and what semantic types it holds.
Given a noun phrase representing a person name, we
return that this is a person entity even if it is not in a
list of people names harvested from the Web.
3 Architecture
Our goal is: given (1) a large set of linked assertions
L and (2) a large set of unlinked assertions U , for
each unlinkable noun phrase subject n ? U , predict:
(1) whether n is an entity, and if so, then (2) the set
of Freebase semantic types for n. For L we use the
9.7 million assertions whose subject argument we
were able to link in Section 2.1, and for U we use
the 5 million assertions that we could not link.
We divide the system into two components. The
first component (described in Section 4) takes any
unlinkable noun phrase and outputs whether it is an
entity. All n ? U classified as entities are placed in
a set E. The second component (described in Sec-
tion 5) uses L and U to predict the semantic types
for each entity e ? E.
Figure 1: Usage over time in Google books for the noun
phrase ?Prices quoted? (e.g., from ?Prices quoted are for
2 adults?) which is not an entity.
Figure 2: Usage over time for the unlinkable noun phrase
?Soluble fibre,? which is an entity. The best fit line has
steeper slope compared to Figure 1.
4 Detecting Unlinkable Entities
This first task takes in any unlinkable noun phrase
and outputs whether it is an entity. There is a long
history of discussion in analytic philosophy litera-
ture on the question of what exists (e.g., (Quine,
1948)). We adopt a more pragmatic view, defin-
ing an ?entity? as a noun phrase that could have a
Wikipedia-style article if there were no notability or
newness considerations, and which would have se-
mantic types. We are interested in entities that could
help populate an entity store. ?EMNLP 2012? is an
example of an entity, while ?The method? and ?12
cats? are examples of noun phrases that are not en-
tities. This is challenging because at a surface level,
many entities and non-entities look similar: ?Sex
and the City? is an entity, while ?John and David?
is not. ?Eminem? is an entity, while ?Youd? (a typo
from ?You?d?) is not.
We address this task by training a classifier with
features primarily derived from a timestamped cor-
pus. An intuition here is that when considering
only unlinkable noun phrases, usage patterns across
895
Figure 3: Plot of R2 vs Slope for the usage over time of a collection of noun phrases selected for illustrative purposes.
Many of the non-entities occur at lower Slope and higher R2, while the entities often have higher slope and/or lower
R2. ?Bluetooth technology? actually has even higher slope, but was adjusted left to fit in this figure.
time often differ for entities and non-entities. Noun
phrase entities that are observed in text going back
hundreds of years (e.g., ?Europe?) almost all have
their own Wikipedia entries, so in unlinkable noun
phrase space, the remaining noun phrases that are
observed in text going back hundreds of years tend
to be all the textual references and expressions that
are not entities. We plan to use this signal to help
separate the entities from the non-entities.
4.1 Classifier Features
We use the Google Books ngram corpus (Michel
et al2010), which contains timestamped usage
of 1-grams through 5-grams in millions of digi-
tized books for each year from 1500 to 2007.3 We
use ngram match count values from case-insensitive
matching. To avoid sparsity anomalies we observed
in years before 1740, we use the data from 1740 on-
ward. While it has not been used for our task before,
this corpus is a rich resource that enables reason-
ing about knowledge (Evans and Foster, 2011) and
3available at http://books.google.com/ngrams/datasets
understanding semantic changes of words over time
(Wijaya and Yeniterzi, 2011). Talukdar et al2012)
recently used it to effectively temporally scope rela-
tional facts.
Our first feature is the slope of the least-squares
best fit line for usage over time. For example, if a
term appears 25 times in books in 1950, 30 times in
1951, ..., 100 times in 2007, then we compute the
straight line that best fits {(1950, 25), (1951, 31), ...,
(2007, 100)}, and examine the slope. We have ob-
served cases of non-entity noun phrases (e.g., Fig-
ure 1) having lower slopes than entity noun phrases
(e.g., Figure 2). Note that we do not normalize
match counts by yearly total frequency, but we do
normalize counts for each term to range from 0 to 1
(setting the max count for each term to 1) to avoid
bias from entity prominence. To capture the current
usage, in cases where there exists a ? 5 year gap in
usage of a term we only use the data after the gap.
Another feature is the R2 fit of the best fit line.
Higher R2 indicates that the data is closer to a
straight line. Figure 3 plots R2 vs Slope values
896
Figure 4: UsageSinceYear of example unlinked terms.
for some sample noun phrases. We observed that
along with their lower Slope, the non-entities often
also had higher R2, indicating that their usage does
not vary wildly from year to year. This contrasts
with certain entities (e.g., ?FY 99? for ?Fiscal Year
1999?) whose usage sometimes varied sharply from
year to year based on their prominence in those spe-
cific years.
A third feature is UsageSinceYear, which finds the
year from when a term last started continually being
used. For example, a UsageSinceYear value of 1920
would indicate that the term was used in books ev-
ery year from 1920 through 2007. Figure 4 shows
examples of where various terms fall along this di-
mension.
From the books ngram corpus, we also calcu-
late features for: PercentProperCaps - the percent-
age of case-insensitive matches for the term where
all words began with a capital letter, PercentExact-
Match - the percentage of case-insensitive matches
for the term that matched the capitalization in the
assertion exactly, and Frequency - the total number
of case-insensitive occurrences of the term in the
book ngrams data, summed across all years, which
reflects prominence. Last, we also include a sim-
ple numeric feature to detect the presence of leading
numeric words (e.g., ?5? in ?5 days? or ?Three? in
?Three choices?).
4.2 Evaluation
From the corpus of 15 million REVERB assertions,
there were 1.4 million unlinked noun phrases includ-
ing 17% unigrams, 51% bigrams, 21% trigrams, and
11% 4-grams or longer. Bigrams comprise over half
the noun phrases and the books bigram data is a self-
contained download that is easier to obtain and store
system correctly classified
Majority class baseline 50.4%
Named Entity Recognition 63.3%
Slope feature only 61.1%
PUF feature combination 69.1%
ALL features 78.4%
Table 2: Our classifier using all features (ALL) outper-
forms majority class and NER baselines.
than the full books ngram corpus, so we focus on
bigrams in our evaluation. In a random sample of
unlinked bigrams, we found that 73% were present
in the books ngram data (65% exact match, 8% case-
insensitive match only), while 27% were not (these
were mostly entities or errors with non-alphabetic
characters). Coverage is a greater issue with longer
ngrams (e.g., there are many more possible 5-grams
than bigrams, so any individual 5-gram is less likely
to reach the minimum threshold to be included in the
books data), but as mentioned earlier, only 11% of
unlinkable noun phrases were 4-grams or longer.
We randomly sampled 250 unlinked bigrams that
had books ngram data, and asked 2 annotators to la-
bel each as ?entity,? ?non-entity,? or ?unclear.? Our
goal is to separate noun phrases that are clearly en-
tities (e.g., ?prune juice?) from those that are clearly
not entities (e.g., ?prices quoted?), rather than to de-
bate phrases that may be in some entity store defi-
nitions but not others, so we asked the annotators to
choose ?unclear? when there was any doubt. There
were 151 bigrams that both annotators believed to
be very clear labels, including 69 that both annota-
tors labeled as entities, 70 that both annotators la-
beled as non-entities, and 12 with label disagree-
ment. Cohen?s kappa was 0.84, indicating excellent
agreement. Our experiment is now to separate the
69 clear entities from the 70 clear non-entities.
For experiment baselines we use the majority
class baseline MAJ, as well as a Named Entity
Recognition baseline NER. For NER we used the
Illinois Named Entity Tagger (Ratinov and Roth,
2009) on the highest setting (that achieved 90.5 F1
score on the CoNLL03 shared task). NER expects a
sentence, so we use the longest assertion in the cor-
pus that the noun phrase was observed in. We eval-
uate several combinations of our features to test dif-
897
ferent aspects of our system: Slope uses only Slope,
PUF uses PercentProperCaps + UsageSinceYear +
Frequency, and ALL uses all features. We evaluate
using the WEKA J48 Decision Tree on default set-
tings, with leave-one-out cross validation.
Table 2 shows the results. MAJ correctly classifies
50.4% of instances, NER correctly classifies 63.3%
and ALL correctly classifies 78.4%.
4.3 Analysis
Overall, 78.4% correctly classified instances is fairly
strong performance on this task. By using the de-
scribed features, our classifier was able to detect and
filter many of the non-entity noun phrases in this
scenario. Compared to the 63.3% of NER, it is an
absolute gain of 15.1%, a relative gain of 24%, and a
reduction in error of 41.1% (from 36.7% to 21.6%).
Student?s t-test at 95% confidence verified that the
difference was significant.
We found that while low Slope (especially with
higher R2) often indicated non-entity, there were nu-
merous cases where higher Slope did not necessarily
indicate entity. For example, the noun phrase ?sev-
eral websites? has fairly sharp slope, but still does
not denote a clear entity. In these cases, the addi-
tion of other features can serve as additional useful
signal. One error from ALL is the term ?Analyst esti-
mates,? which the annotators labeled as a non-entity,
but which occasionally appears in text (especially ti-
tles) as ?Analyst Estimates,? and is a relatively re-
cent phrase. NER misses entities such as ?synthetic
cubism? and ?hunter orange? that occur in our data
but are not traditional named entities. We observed
that while none of our features achieves over 70%
accuracy by themselves, they perform well in con-
junction with each other.
5 Propagating Semantic Types
This second task uses a set of linked assertions L and
set of unlinked assertions U to predict the semantic
types for each entity e ? E. If the previous step
output that ?Sun Microsystems? is likely to be an
entity, then the goal of this step is to further predict
that it has the Freebase types such as organization
and software developer.
From L we use the set of linked entities and the
textual relations they occur with. For example, L
might contain that the entity Microsoft links to a par-
ticular Wikipedia article, and also that it occurs with
textual relations such as ?has already announced?
and ?has released updates for.? For each Wikipedia-
linked entity in L, we further look up its exact set
of Freebase types.4 From U we obtain the set of
textual relations that each e ? E is in the domain
of. We now have a large set of class-labeled in-
stances (all entities in L), a large set of unlabeled
instances (E), and a method to connect the unla-
beled instances with the class-labeled instances (via
any shared textual relations), so we cast this task
as an instance-to-instance class propagation problem
(Kozareva et al2011) for propagating class labels
from labeled to unlabeled instances.
We build on the recent work of Kozareva et al
(2011), and adapt their approach to leverage the
scale and resources of our scenario. While they use
only one type of edge between instances, namely
shared presence in the high precision DAP pattern
(Hovy et al2009), our final system uses 1.3 mil-
lion textual relations from |L ? U |, corresponding
to 1.3 million potential edge types. Their evaluation
involved only 20 semantic classes, while we use all
1,339 Freebase types covered by our entities in L.
There is a rich history of other approaches for
predicting semantic types. (Talukdar et al2008)
and (Talukdar and Pereira, 2010) model relation-
ships between instances and classes, but our un-
linked entities do not come with any class informa-
tion. Pattern-based approaches (Pas?ca, 2004; Pantel
and Ravichandran, 2004) are popular, but (Kozareva
et al2011) notes that they ?are constraint to the in-
formation matched by the pattern and often suffer
from recall,? meaning that they do not cover many
instances. Classifiers have also been trained for fine-
grained semantic typing, but for noticeably fewer
types than we work with. (Rahman and Ng, 2010)
studied hierarchical and collective classification us-
ing 92 types, and FIGER (Ling and Weld, 2012) re-
cently used an adapted perceptron for multi-class
multi-label classification into 112 types.
5.1 Algorithm
Given an entity e, our algorithm involves: (1) find-
ing the textual relations that e is in the domain of, (2)
4data available at http://download.freebase.com/wex
898
Figure 5: This example illustrates the set of Freebase type predictions for the noun phrase ?Sun Microsystems.? We
predict the semantic type of a noun phrase by: (1) finding the textual relations it is in the domain of, (2) finding linked
entities that are also in the domain of those textual relations, and (3) observing their semantic types.
finding linked entities that are also in the domain of
those textual relations, and then (3) predicting types
by observing the types of those linked entities. Fig-
ure 5 illustrates how we would predict the semantic
types of the noun phrase ?Sun Microsystems.?
Find Relations: Obtain the set R of all textual re-
lations in U that e is in the domain of. For example,
if U contains the assertion ?(Sun Microsystems, has
released a minor update to, Java 1.4.2),? then the tex-
tual relation ?has released a minor update to? should
be added to R when typing ?Sun Microsystems.?
Find Similar Entities: Find the linked entities in
L that are in the domain of the most relations in
R. In our example, entities such as ?Microsoft? and
?Apple Inc.? have the greatest overlap in textual re-
lations because they are most often in the domain
of the same textual relations, e.g., (?Microsoft, has
released a minor update to, Windows Live Essen-
tials?). Create a set S of the entities that share the
most textual relations. We found keeping 10 similar
entities (|S| = 10) is generally enough to predict the
original entity?s types in the final step.
Predict Types: Return the most frequent Freebase
types of the entities in S as the prediction. To
avoid penalizing very small types, if there are n in-
stances of semantic class C in S, then we rank C us-
ing a type score T (n,C, S) = max(n/|S|, n/|C|),
which we found to perform better than T (n,C, S) =
avg(n/|S|, n/|C|). For ?Sun Microsystems,? busi-
ness operation was the top predicted type because
all entities in S were observed to include business
operation type.
5.2 Edge Validity
This algorithm will only be effective if entities that
share textual relation strings are more likely to be
of the same semantic types. To verify this, we sam-
pled 30,000 linked entities from L that had at least
30 textual relations each, and associated each with
their 30 most frequent relations. From the 900 mil-
lion possible entity pairs, we then randomly sample
500 entity pairs that shared exactly k out of 30 rela-
tions, for each k from 0 to 15. At each k we then use
our sampled pairs to estimate the probability that any
two entities sharing exactly k relations (out of their
30 possible) will share at least one type.
The results are shown in Figure 6. We found that
entities sharing more textual relations were in fact
more likely to have semantic types in common. Two
entities that shared exactly 0 of 30 textual relations
were only 11% likely to share a semantic type, while
two entities that shared exactly 10 of 30 relations
were 80% likely to share a semantic type. This vali-
dates our use of textual relations as a signal-bearing
edge in instance-to-instance class propagation.
5.3 Weighting Textual Relations
The algorithm as currently described treats all tex-
tual relations equally, when in reality some are
stronger signal to entity type than others. For exam-
ple, two entities in the domain of the ?came with?
relation often will not share semantic types, but two
entities in the domain of the ?autographed? relation
will almost always share a type. To capture this intu-
ition, we define relation weight w(r) as the observed
probability (among the linked entities) that two en-
899
Figure 6: Entities that share more textual relations are
more likely to have semantic types in common.
tities will share a Freebase type if they both occur
in the domain of r. If D(r) = all entities observed
in the domain of relation r and T (e) = all Freebase
types listed for entity e, then weight w(r) of a tex-
tual relation string r is:
w(r) =
?
e1,e2?D(r), e1 6=e2
I(e1, e2)
|D(r)| ? (|D(r)| ? 1)
I(e1, e2) =
{
1, if |T (e1) ? T (e2)| > 0
0, otherwise.
Table 3 shows examples of high weight relations,
and Table 4 shows low weight relations. We now
modify the Find Similar Entities step such that if
a linked entity shares a set of relations Q with the
entity being typed, then it receives a score which
considers all shared relations q ? Q but uses the
high weight relations more. On a development set
we found that a score of
?
q?Q 10
4?w(q) was effec-
tive, as higher weight signifies much stronger signal.
This score then determines which entities to place in
S.
5.4 Evaluation
The goal of the evaluation is to judge how well our
method can predict the Freebase semantic types of
entities in our scenario. Our linked entities cov-
ered 1,339 Freebase types, including many interest-
ing types such as computer operating system, reli-
gious text, airline and baseball team. Human judges
would have trouble manually annotating new enti-
ties with all these types because there are too many
to keep in mind and understand the characteristics
?is a highway in?
?is a university located in?
?became the president of?
?turned down the role of?
?has an embassy in?
Table 3: Example relations found to have high weight.
?comes with?
?is a generic term for?
?works best on?
?can be made from?
?is almost identical to?
Table 4: Example relations found to have low weight.
of. Instead, we automatically generate testing data
by sampling entities from L, and then test on abil-
ity to recover the actual Freebase types (which we
know).
We sample a HEAD set of distinct 500 Freebase
entities (drawn randomly from our set of linked ex-
tractions), and a TAIL set of 500 entities (drawn ran-
domly from our set of linked entities). An entity
that occurs in many extractions is more likely to be
in HEAD than TAIL. Our sampling also picks only
entities that occur with at least 10 relations, which
is appropriate for the Web scenario where more in-
stances can always be queried for.
For baselines we use random baseline BRandom
and a frequency baseline BFrequency which always
returns types in order of their frequency among
all linked entities (e.g., always person, then loca-
tion, etc). We evaluate our system without rela-
tion weighting (SNoWeight) and also with relation
weighting (SWeighted). For SWeighted we leave all
the test set entities out when calculating global re-
lation weights. Our metrics are Precision at 1 and
F1 score. Precision at 1 measures how often the top
returned type is a correct type, and is useful for ap-
plications that want one type per entity. F1 mea-
sures how well the method recovers the full set of
Freebase types (for each test case we graph preci-
sion/recall and take the max F1), and is useful for
applications such as typed question answering.
Table 5 shows the results. BRandom performs
poorly because there are so many semantic types,
and very few of them are correct for each test
case. BFrequency performs slightly better on TAIL
than HEAD because TAIL contains more entities of
the most frequent types. SNoWeight performance
900
HEAD TAIL
Prec@1 F1 Prec@1 F1
BRandom 0.008 0.028 0.004 0.023
BFrequency 0.244 0.302 0.298 0.322
SNoWeight 0.542? 0.465? 0.510? 0.456?
SWeighted 0.610? 0.521? 0.598? 0.522?
Table 5: Evaluation on HEAD and TAIL, 500 elements each. ? indicates statistical significance over BFrequency, and
? over both BFrequency and SNoWeight. Significance is measured using the Student?s t-test at 95% confidence. The
top type predicted by our SWeighted method is correct about 60% of the time, while the top type predicted by the
BFrequency baseline is correct under 30% of the time.
is statistically significant above all baselines, and
SWeighted is statistically significant over SNoWeight
on both test sets and metrics.
5.5 Analysis
SWeighted was successful at recovering the correct
Freebase types of many entities. For example, it
finds that ?Atherosclerosis? is a medical risk fac-
tor by connecting it to ?obesity? and ?diabetes,? that
?Supernatural? is a TV program and a Netflix title
by connecting it to ?House? and ?30 Rock,? and that
?America West? is an aircraft owner and an airline
by connecting it to ?Etihad Airways? and ?China
Eastern Airlines.? While precision at 1 around 60%
may not be high enough yet for certain applications,
it is significantly better than competing approaches,
which are under 30%, and we hope that our values
can serve as a non-trivial baseline on this task for
future systems.
One example where SWeighted made some mis-
takes is fictional characters. Many fictional charac-
ters participate in a textual relations that make them
look like people (e.g., ?was born on?), but predicting
that they belong to people class is incorrect. Some
performance hit was also due to entity linking errors.
From an assertion like ?The Four Seasons is located
in Hamamatsu,? our entity linker (and other entity
linkers we tried) prefer linking ?The Four Seasons?
to Vivaldi?s music composition rather than the hotel
chain. We are then unable to recover music compo-
sition type from relations like ?is located in.? Our
algorithm relies on accurate entity linking in L, but
there is a precision/recall tradeoff to consider here
because it also benefits from higher coverage of en-
tities and relations in L.
As a general reference for performance of
state-of-the-art fine-grained entity classification, the
FIGER system (Ling and Weld, 2012) for classify-
ing into 112 types reported F1 scores ranging from
0.471 to 0.693 in their experiments. It is important to
note that these numbers are not directly comparable
to us because they used different data, different (and
fewer) types, and different evaluation methodology.
6 Discussion
This paper presented an approach for working with
non-Wikipedia entities in text. Consider the follow-
ing possibilities for a noun phrase in a text corpus:
Wikipedia Entity: (e.g., ?Computer Science,?
?South America,? ?apple juice?) - Entity linking
techniques can identify and type these.
Non-Wikipedia, Non-Entity: (e.g., ?strange
things,? ?Early studies,? ?A link?) - Our classifier
from Section 4 is able to filter these.
Non-Wikipedia, Entity: (e.g., ?Safflower oil,?
?prune juice,? ?Amazon UK?) - We identify these
as entities, then propagate semantic types to them.
Our technique finds that ?Safflower oil? occurs with
high weight relations such as ?is sometimes used to
treat? and ?can be substituted for,? making it similar
to linked entities such as ?Phentermine? and ?Dan-
delion,? and then correctly predicts semantic types
including food ingredient and medical treatment.
6.1 Typed Question Answering
From our set of 15 million assertions, we found and
typed many non-Wikipedia entities. In food while
Wikipedia has ?crab meat,? we find it is missing oth-
ers such as ?rabbit meat? and ?goat milk.? In job ti-
tles it has ?scientist? and ?lawyer,? but we find it is
missing ?PhD student,? ?fashion designer,? and oth-
ers. We find many of the people and employers not
901
prominent enough for Wikipedia.
One application of this research is to increase the
yield of applications such as Typed Question An-
swering (Buscaldi and Rosso, 2006). For example,
consider the query ?What computer game is a lot of
fun?? A search for assertions matching ?* is a lot
of fun? in the data yields around 1,000 results such
as ?camping,? ?David Sedaris? and ?Hawaii.? En-
tity linking allows us to identify just the computer
games in Wikipedia that match the query, such as
?Civilization.? However, around 400 query matches
could not be entity linked. Our noun-phrase clas-
sifier filters out non-entities such as ?actual play,?
?Just this? and ?Two kids.? After predicting types
for the matches that did not get filtered, we find ad-
ditional non-Wikipedia computer games that match
the query, including ?Cooking Dash,? ?Delicious
Deluxe? and ?Slingo Supreme.?
7 Future Work
An area we are continuing to improve the system
on is textual ambiguity. For example, an unlinkable
noun phrase might simultaneously be the name of a
film, a car, and a person. Instead of outputting that
the noun phrase holds all of those types, a stronger
output would be to realize that the noun phrase is
ambiguous, determine how many senses it has, and
determine which sense is being referred to in each
instance. We have ideas for how to detect ambiguous
entities using mutual exclusion (Carlson, 2010) and
functional relations. For example, if we predict that
a noun phrase has film and car types but we also
observe in our linked instances that these types are
mutually exclusive, then this is good evidence that
the noun phrase refers to multiple terms.
We also plan to continue improving our tech-
niques, as there is still plenty of room for improve-
ment on both subtasks. For detecting new entities,
we are interested in seeing if timestamped Twitter
data could be analyzed to increase both recall and
precision. For predicting semantic types, (Kozareva
et al2011) proposed additional techniques which
we have not fully explored. Also, we can incor-
porate additional signals such as shared term heads
when they are available, in order to help find terms
that are likely to share types. Last, we would like
to feed back our system output to improve system
performance. For example, non-entity noun phrases
that make it to the typing step might lead to particu-
lar predicted type distributions that indicate an error
occurred earlier in the process.
8 Conclusion
In this paper we showed that while entity linking
cannot link to entities outside of Wikipedia, once a
large text corpus has been entity linked, the presence
and content of the existing links can be leveraged to
help detect and semantically type the non-Wikipedia
entities as well. We designed techniques for de-
tecting whether unlinkable noun phrases are entities,
and if they are, then propagating semantic types to
them from the linked entities. In our evaluations, we
showed that our techniques achieve statistically sig-
nificant improvement over baseline methods.
Our research here takes initial steps toward a fu-
ture where the vast universe of entities that are not
prominent enough to include in manually-authored
knowledge bases is analyzed automatically instead
of being left behind.
Acknowledgements
We thank Stephen Soderland, Xiao Ling, and the
three anonymous reviewers for their helpful feed-
back on earlier drafts. This research was sup-
ported in part by NSF grant IIS-0803481, ONR grant
N00014-08-1-0431, and DARPA contract FA8750-
09-C-0179, and carried out at the University of
Washington?s Turing Center.
References
Razvan Bunescu and Marius Pas?ca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European
Chapter of the Association of Computational Linguis-
tics (EACL).
Davide Buscaldi and Paolo Rosso. 2006. Mining knowl-
edge from wikipedia for the question answering task.
In Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC).
Andrew Carlson. 2010. Coupled Semi-Supervised
Learning. Ph.D. thesis, Carnegie Mellon University.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP.
902
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation for
knowledge base population. In Proceedings of COL-
ING.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the Web: An
experimental study. In Artificial Intelligence.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine Reading. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI).
James A. Evans and Jacob G. Foster. 2011. Metaknowl-
edge. In Science.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling Wikipedia-based named entity disam-
biguation to arbitrary Web text. In IJCAI-09 Workshop
on User-contributed Knowledge and Artificial Intelli-
gence (WikiAI09).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
On-the-fly annotation of short text fragments (by
wikipedia entities). In Proceedings of CIKM.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff. 2009.
Toward completeness in concept extraction and classi-
fication. In Proceedings of EMNLP.
Zornitsa Kozareva, Konstantin Voevodski, and Shang-
Hua Teng. 2011. Class label enhancement via related
instances. In Proceedings of EMNLP.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
wikipedia entities in text. In Proceedings of KDD.
Changki Lee, Yi-Gyu Hwang, and Myung-Gil Jang.
2007. Fine-grained named entity recognition and rela-
tion extraction for question answering. In Proceedings
of SIGIR.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity
recognition. In Proceedings of the 26th Conference on
Artificial Intelligence (AAAI).
Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,
Adrian Veres, Matthew K. Gray, The Google Books
Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy,
Peter Norvig, Jon Orwant, and Steven Pinker. 2010.
Quantitative analysis of culture using millions of digi-
tized books. In Science.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17h ACM Inter-
national Conference on Information and Knowledge
Management (CIKM).
Marius Pas?ca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the ACM
International Conference on Information and Knowl-
edge Management (CIKM).
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings of
HLT-NAACL.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Danuta Ploch. 2011. Exploring entity relations for
named entity disambiguation. In Proceedings of the
Annual Meeting of the Association of Computational
Linguistics (ACL).
Willard Van Orman Quine. 1948. On what there is. In
Review of Metaphysics.
Altaf Rahman and Vincent Ng. 2010. Inducing fine-
grained semantic classes via hierarchical and collec-
tive classification. In Proceedings of COLING.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL).
Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-
son. 2011. Local and global algorithms for disam-
biguation to wikipedia. In Proceedings of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL).
Satoshi Sekine and Chikashi Nobata. 2004. Definition,
dictionaries and tagger for extended named entity hier-
archy. In Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation (LREC).
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceedings
of the Annual Meeting of the Association of Computa-
tional Linguistics (ACL).
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas?ca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of EMNLP.
Partha Pratim Talukdar, Derry Tanti Wijaya, and Tom
Mitchell. 2012. Coupled temporal scoping of rela-
tional facts. In Proceedings of WSDM.
Chi Wang, Kaushik Chakrabarti, Tao Cheng, and Surajit
Chaudhuri. 2012. Targeted disambiguation of ad-hoc,
homogeneous sets of named entities. In Proceedings
of the 21st International World Wide Web Conference
(WWW).
Derry Tanti Wijaya and Reyyan Yeniterzi. 2011. Un-
derstanding semantic changes of words over centuries.
In Workshop on Detecting and Exploiting Cultural Di-
versity on the Social Web.
903
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1721?1731,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Generating Coherent Event Schemas at Scale
Niranjan Balasubramanian, Stephen Soderland, Mausam, Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{niranjan,ssoderlan, mausam, etzioni}@cs.washington.edu
Abstract
Chambers and Jurafsky (2009) demonstrated
that event schemas can be automatically in-
duced from text corpora. However, our analy-
sis of their schemas identifies several weak-
nesses, e.g., some schemas lack a common
topic and distinct roles are incorrectly mixed
into a single actor. It is due in part to their
pair-wise representation that treats subject-
verb independently from verb-object. This of-
ten leads to subject-verb-object triples that are
not meaningful in the real-world.
We present a novel approach to inducing
open-domain event schemas that overcomes
these limitations. Our approach uses co-
occurrence statistics of semantically typed re-
lational triples, which we call Rel-grams (re-
lational n-grams). In a human evaluation, our
schemas outperform Chambers?s schemas by
wide margins on several evaluation criteria.
Both Rel-grams and event schemas are freely
available to the research community.
1 Introduction
Event schemas (also known as templates or frames)
have been widely used in information extraction.
An event schema is a set of actors (also known as
slots) that play different roles in an event, such as
the perpetrator, victim, and instrument in a bomb-
ing event. They provide essential guidance in ex-
tracting information related to events from free text
(Patwardhan and Riloff, 2009), and can also aid in
other NLP tasks, such as coreference (Irwin et al,
2011), summarization (Owczarzak and Dang, 2010),
and inference about temporal ordering and causality.
Actor Rel Actor
A1:<person> failed A2:test
A1:<person> was suspended for A3:<time period>
A1:<person> used A4:<substance, drug>
A1:<person> was suspended for A5:<game, activity>
A1:<person> was in A6:<location>
A1:<person> was suspended by A7:<org, person>
Actor Instances:
A1: {Murray, Morgan, Governor Bush, Martin, Nelson}
A2: {test}
A3: {season, year, week, month, night}
A4: {cocaine, drug, gasoline, vodka, sedative}
A5: {violation, game, abuse, misfeasance, riding}
A6: {desert, Simsbury, Albany, Damascus, Akron}
A7: {Fitch, NBA, Bud Selig, NFL, Gov Jeb Bush}
Table 1: An event schema produced by our system, rep-
resented as a set of (Actor,Rel, Actor) triples, and a set
of instances for each actor A1, A2, etc. For clarity we
show unstemmed verbs.
Until recently, all event schemas in use in NLP
were hand-engineered, e.g., the MUC templates and
ACE event relations (ARPA, 1991; ARPA, 1998;
Doddington et al, 2004). This led to technology that
could only focus on specific domains of interest and
has not been applicable more broadly.
The seminal work of Chambers and Jurafsky
(2009) showed that event schemas can also be in-
duced automatically from text corpora. Instead of
labeled roles these schemas have a set of relations
and actors that serve as arguments.1 Their system
is fully automatic, domain-independent, and scales
to large text corpora.
However, we identify several limitations in the
schemas produced by their system.2 Their schemas
1In the rest of this paper we use event schemas to refer to
these automatically induced schemas with actors and relations.
2Available at http://www.usna.edu/Users/cs/
nchamber/data/schemas/acl09
1721
Actor Rel Actor
A1 caused A2
A2 spread A1
A2 burned A1
- extinguished A1
A1 broke out -
- put out A1
Actor Instances:
A1: {fire, aids, infection, disease}
A2: {virus, bacteria, disease, urushiol, drug}
Table 2: An event schema from Chambers? system that
mixes the events of fire spreading and disease spreading.
often lack coherence: mixing unrelated events and
having actors whose entities do not play the same
role in the schema. Table 2 shows an event schema
from Chambers that mixes the events of fire spread-
ing and disease spreading.
Much of the incoherence of Chambers? schemas
can be traced to their representation that uses pairs
of elements from an assertion, thus, treating subject-
verb and verb-object separately. This often leads to
subject-verb-object triples that do not make sense in
the real world. For example, the assertions ?fire
caused virus? and ?bacteria burned AIDS? are im-
plicit in Table 2.
Another limitation in schemas Chambers released
is that they restrict schemas to two actors, which can
result in combining different actors. Table 4 shows
an example of combining perpeterators and victims
into a single actor.
1.1 Contributions
We present an event schema induction algorithm that
overcomes these weaknesses. Our basic represen-
tation is triples of the form (Arg1, Relation, Arg2),
extracted from a text corpus using Open Information
Extraction (Mausam et al, 2012). The use of triples
aids in agreement between subject and object of a
relation. The use of Open IE leads to more expres-
sive relation phrases (e.g., with prepositions). We
also assign semantic types to arguments, both to al-
leviate data sparsity and to produce coherent actors
for our schemas.
Table 1 shows an event schema generated by our
system. It has six relations and seven actors. The
schema makes several related assertions about a per-
son using a drug, failing a test, and getting sus-
pended. The main actors in the schema include the
person who failed the test, the drug used, and the
agent that suspended the person.
Our first step in creating event schemas is to tab-
ulate co-occurrence of tuples in a database that we
call Rel-grams (relational n-grams) (Sections 3, 5.1).
We then perform analysis on a graph induced from
the Rel-grams database and use this to create event
schemas (Section 4).
We compared our event schemas with those of
Chambers on several metrics including whether the
schema pertains to a coherent topic or event and
whether the actors play a coherent role in that event
(Section 5.2). Amazon Mechanical Turk workers
judged that our schemas have significantly better co-
herence ? 92% versus 82% have coherent topic and
81% versus 59% have coherent actors.
We release our open domain event schemas and
the Rel-grams database3 for further use by the NLP
community.
2 System Overview
Our approach to schema generation is based on the
idea that frequently co-occurring relations in text
capture relatedness of assertions about real-world
events. We begin by extracting a set of relational tu-
ples from a large text corpus and tabulate occurrence
of pairs of tuples in a database.
We then construct a graph from this database and
identify high-connectivity nodes (relational tuples)
in this graph as a starting point for constructing event
schemas. We use graph analysis to rank the tu-
ples and merge arguments to form the actors in the
schema.
3 Modeling Relational Co-occurrence
In order to tabulate pairwise occurences of relational
tuples we need a suitable relation-based represen-
tation. We now describe the extraction and rep-
resentation of relations, a database for storing co-
occurrence information, and our probabilistic model
for the co-occurrence. We call this model Rel-
grams, as it can be seen as a relational analog to the
n-grams language model.
3.1 Relations Extraction and Representation
We extract relational triples from each sentence in
a large corpus using the OLLIE Open IE system
3Available at http://relgrams.cs.washington.edu
1722
Tuples Table
Id Arg1 Rel Arg2 Count
... ... ... ... ...
13 bomb explode in <loc> 547
14 bomb explode in Baghdad 22
15 bomb explode in market 7
... ... ... ... ...
87 bomb kill <per> 173
... ... ... ... ...
92 <loc> be suburb of <loc> 1023
... ... ... ... ...
BigramCounts Table
T1 T2 Dist. Count E11 E12 E21 E22
... ... ... ... ... ... ... ...
13 87 1 27 25 0 0 0
13 87 2 35 33 0 0 0
... ... ... ... ... ... ... ...
13 87 10 62 59 0 0 0
87 13 1 6 0 0 0 0
... ... ... ... ... ... ... ...
92 13 1 12 0 0 12 0
... ... ... ... ... ... ... ...
Figure 1: Tables in the Rel-grams Database: Tuples maps tuples to unique identifiers, BigramCounts provides the
co-occurrence counts (Count) within various distances (Dist.), and four types of argument equality counts (E11-E22).
E11 is the number of times when T1.Arg1 = T2.Arg1, E12 is when T1.Arg1 = T2.Arg2 and so on.
(Mausam et al, 2012).4 This provides relational tu-
ples in the format (Arg1, Relation, Arg2) where each
tuple element is a phrase from the sentence. The
sentence ?He cited a new study that was released by
UCLA in 2008.? produces three tuples:
1. (He, cited, a new study)
2. (a new study, was released by, UCLA)
3. (a new study, was released in, 2008)
Relational triples provide a more specific repre-
sentation which is less ambiguous when compared
to (subj, verb) or (verb, obj) pairs. However, using
relational triples also increases sparsity. To reduce
sparsity and to improve generalization, we represent
the relation phrase by its stemmed head verb plus
any prepositions. The relation phrase may include
embedded nouns, in which case these are stemmed
as well. Moreover, tuple arguments are represented
as stemmed head nouns, and we also record seman-
tic types of the arguments.
We selected 29 semantic types from WordNet, ex-
amining the set of instances on a small development
set to ensure that the types are useful, but not overly
specific. The set of types are: person, organization,
location, time unit, number, amount, group, busi-
ness, executive, leader, effect, activity, game, sport,
device, equipment, structure, building, substance,
nutrient, drug, illness, organ, animal, bird, fish, art,
book, and publication.
To assign types to arguments, we apply Stanford
Named Entity Recognizer (Finkel et al, 2005)5, and
also look up the argument in WordNet 2.1 and record
4Available at: http://knowitall.github.io/
ollie/
5We used the system downloaded from: http://nlp.
stanford.edu/software/CRF-NER.shtml and used
the seven class CRF model distributed with it.
the first three senses if they map to our target se-
mantic types. We use regular expressions to recog-
nize dates and numeric expressions, and map per-
sonal pronouns to <person>. We associate all types
found by this mechanism with each argument. The
tuples in the example above are normalized to the
following:
1. (He, cite, study)
2. (He, cite, <activity>)
3. (<person>, cite, study)
4. (<person>, cite, <activity>)
5. (study, be release by, UCLA)
6. (study, be release by, <organization>)
7. (study, be release in, 2008)
8. (study, be release in, <time unit>)
9. (<activity>, be release by, UCLA)
...
In our preliminary experiments, we found that us-
ing normalized relation strings and semantic classes
for arguments results in a ten-fold increase in the
number of Rel-grams with a minimum support.
3.2 Co-occurrence Tabulation
We construct a database to hold co-occurrence
statistics for pairs of tuples found in each document.
Figure 1 shows examples for the types of statistics
contained in the database. The database consists
of two tables: 1) Tuples ? Maps each tuple to a
unique identifier and tabulates tuple counts. 2) Bi-
gramCounts ? Stores the directional co-occurrence
frequency, a count for tuple T followed by T ? at a
distance of k, and tabulates the number of times the
same argument was present in the pair of tuples.
Equality Constraints: Along with the co-
occurrence counts, we record the equality of argu-
ments in Rel-grams pairs. We assert an argument
1723
Table 3: Given a source tuple, the Rel-grams language
model estimates the probability of encountering other re-
lational tuples in a document. For clarity, we show the
unstemmed version.
Top tuples related to
(<person>, convicted of, murder)
1. (<person>, convicted in, <time unit>)
2. (<person>, sentenced to, death)
3. (<person>, sentenced to, year)
4. (<person>, convicted in, <location>)
5. (<person>, sentenced to, life)
6. (<person>, convicted in, <person>)
7. (<person>, convicted after, trial)
8. (<person>, sent to, prison)
pair is equal if they are from the same token se-
quence in the source sentence or one argument is a
co-referent mention of the other. We use the Stan-
ford Co-reference system (Lee et al, 2013)6 to de-
tect co-referring mentions. There are four possible
equalities depending on the specific pair of argu-
ments in the tuples are the same, shown as E11, E12,
E21 and E22 in Figure 1. For example, the E21 col-
umn has counts for the number of times the Arg2 of
T1 was determined to be the same as the Arg1 of T2.
Implementation and Query Language: We pop-
ulated the Rel-grams database using OLLIE extrac-
tions from a set of 1.8 Million New York Times arti-
cles drawn from the Gigaword corpus. The database
consisted of approximately 320K tuples that have
frequency ? 3 and 1.1M entries in the bigram table.
The Rel-grams database allows for powerful
querying using SQL. For example, Table 3 shows the
most frequent rel-grams associated with the query
tuple (<person>, convicted of, murder).
3.3 Rel-grams Language Model
From the tabulated co-occurrence statistics, we esti-
mate bi-gram conditional probabilities of tuples that
occur within a window of k tuples from each other.
Formally, we use Pk(T ?|T ) to denote the conditional
probability that T ? follows T within a window of k
tuples. To discount estimates from low-frequency
tuples, we use a ?-smoothed estimate:
6Available for download at: http://nlp.stanford.
edu/software/dcoref.shtml
Pk(T
?|T ) =
#(T, T ?, k) + ?
?
T ???V
#(T, T ??, k) + ? ? |V |
(1)
where, #(T, T ?, k) is the number of times T ? fol-
lows T within a window of k tuples. k = 1 in-
dicates adjacent tuples in the document. |V | is the
number of unique tuples in the corpus. For experi-
ments in this paper, we set ? to 0.05.
Co-occurrence within a small window is usu-
ally more reliable but is also sparse, whereas co-
occurrence within larger windows addresses sparsity
but may lead to topic drift. To leverage the bene-
fits of different window sizes, we also define a met-
ric with a weighted average of window sizes from 1
to 10, where the weight decays as window size in-
creases. For example, with ? set to 0.5 in equation
2, a window of k+1 has half the weight of a window
of k.
P (T ?|T ) =
?10
k=1 ?
kPk(T ?|T )
?10
k=1 ?
k
(2)
We believe that Rel-grams is a valuable source
of common-sense knowledge and may be useful for
several downstream tasks such as improving infor-
mation extractors, inference of implicit information,
etc. We assess its usefulness in the context of gener-
ating event schemas.
4 Schema Generation
We now use Rel-grams to identify relations and ac-
tors pertaining to a particular event. Our schema
generation consists of three steps. First, we build a
relation graph of tuples (G) using connections iden-
tified by Rel-grams. Second, we identify a set of
seed tuples as starting points for schemas. We use
graph analysis to find the tuples most related to each
seed. Finally, we merge the arguments in these tu-
ples to create actors and output the final schema.
Next we describe each of these steps in detail.
4.1 Rel-graph construction
We define a Rel-graph as an undirected weighted
graph G = (V,E), whose vertices (V ) are relation
tuples with edges (E), where an edge between ver-
tices T and T ? is weighted by the symmetric condi-
tional probability SCP (T, T ?) defined as
1724
SCP (T, T ?) = P (T |T ?)? P (T ?|T ) (3)
Both conditional probabilities are computed in
Equation 2. Figure 2 shows a portion of a Rel-graph
where the thickness of the edge indicates symmetric
conditional probability.
(bomb, explode at, <location>) 
(bomb, explode on,          <time_unit>) (bomb, kill,      <person>) 
(bomb, wound,       <person>) (<person>, plant,          bomb) 
(<organization>, claim, responsibility) 
Figure 2: Part of a Rel-graph showing tuples strongly
associated with (bomb, explode at, <location>). Undi-
rected edges are weighted by symmetric conditional
probability with line thickness indicating weight.
4.2 Finding Related Tuples
Our goal is to find closely related tuples that per-
tain to an event or topic. First, we locate high-
connectivity nodes in the Rel-graph to use as seeds.
We sort nodes by the sum of their top 25 edge
weights7 and take the top portion of this list after
filtering out redundant views of the same relation.
For each seed (Q), we find related tuples by ex-
tracting the sub-graph (GQ) from Q?s neighbors
(within two hops from Q) in the Rel-graph. Graph
analysis can detect the strongly connected nodes
within this sub-graph, representing tuples that fre-
quently co-occur in the context of the seed tuple.
Page rank is a well-known graph analysis algo-
rithm that uses graph connectivity to identify impor-
tant nodes within a graph (Brin and Page, 1998). We
are interested in connectivity within a subgraph with
respect to a designated query node (the seed). Con-
nection to a query node can help minimize concept
drift and ensure that the selected tuples are closely
related to the main topic of the sub-graph.
7Limiting to the top 25 edges avoids overly general tuples
that occur in many topics, which tend to have a large number of
weak edges.
In this work, we adapt the Personalized PageR-
ank algorithm (Haveliwala, 2002). The personalized
version of PageRank returns ranks of various nodes
with respect to a given query node and hence is more
appropriate for our task than the basic PageRank al-
gorithm. Within the subgraph GQ for a given seed
Q, we compute a solution to the following set of
PageRank Equations:
PRQ(T )
= (1? d) + d
X
T ?
SCP (T, T ?)PRQ(T
?) if T = Q
= d
X
T ?
SCP (T, T ?)PRQ(T
?) otherwise
Here PRQ(T ) denotes the page rank of a tuple T
personalized for the query tuple Q. It is a sum of
all its neighbors? page ranks, weighted by the edge
weights; d is the damping probability, which we set
to be 0.85 in our implementation.
The solution is computed iteratively by initializ-
ing the page rank of Q to 1 and all others to 0, then
recomputing page rank values until they converge to
within a small . This computation remains scalable,
since we restrict it to subgraphs a small number of
hops away from the query node. This is a standard
practice to handle large graphs (Agirre and Soroa,
2009; Mausam et al, 2010).
4.3 Creating Actors and Relations
We take the top n tuples from GQ according to
their Page rank scores. From each tuple T :
(Arg1, Rel, Arg2) in GQ, we record two actors
(A1, A2) corresponding to Arg1 and Arg2, and add
Rel to the list of relations that they participate in.
Then, we merge actors in two steps. First, we col-
lect the equality constraints for the tuples in GQ. If
the arguments corresponding to any pair of actors
have a non-zero equality constraint then we merge
them. Second, we merge actors that perform simi-
lar actions. A1 and A2 are merged if they are con-
nected to the same actor A3 through the same rela-
tion. For example, A1 and A2 in (A1:lawsuit, file by,
A3:company) and (A2:suit, file by, A3:company),
will be merged into a single actor. To avoid merging
distinct actors, we use a small list of rules that spec-
ify the semantic type pairs that cannot be merged
(e.g., location-date). Also, we do not merge two ac-
tors, if it can result in a relation where the same actor
1725
System A1 Rel A2
Relgrams {bomb, missile, grenade, device} explode in {city, Bubqua, neighborhood}
{bomb, missile, grenade, device} explode kill {people, civilian, lawmaker, owner, soldier}
{bomb, missile, grenade, device} explode on {Feb., Fri., Tues., Sun., Sept.}
{bomb, missile, grenade, device} explode wound {civilian, person, people, soldier, officer}
{bomb, missile, grenade, device} explode in {Feb., Beirut Monday, Sept., Aug.}
{bomb, missile, grenade, device} explode injure {woman, people, immigrant, policeman}
Chambers {bomb, explosion, blast, bomber, mine} explode {soldier, child, civilian, bomber, palestinian}
{soldier, child, civilian, bomber, palestinian} set off {bomb, explosion, blast, bomber, mine, bombing}
{bomb, explosion, blast, bomber, mine} kill {soldier, child, civilian, bomber, palestinian}
{soldier, child, civilian, bomber, palestinian} detonate {bomb, explosion, blast, bomber, mine, bombing}
{bomb, explosion, blast, bomber, mine} injure {soldier, child, civilian, bomber, palestinian}
{soldier, child, civilian, bomber, palestinian} plant {bomb, explosion, blast, bomber, mine, bombing}
Relgrams {Carey, John Anthony Volpe, Chavez, She } veto {legislation, bill, law, measure, version}
{legislation, bill, law, measure, version} be sign by {Carey, John Anthony Volpe, Chavez, She }
{legislation, bill, law, measure, version} be pass by {State Senate, State Assembly, House, Senate, Parliament}
{Carey, John Anthony Volpe, Chavez, She } sign into {law}
{Carey, John Anthony Volpe, Chavez, She } to sign {bill}
{Carey, John Anthony Volpe, Chavez, She } be governor of {Massachusetts, state, South Carolina, Texas, California}
Chambers {clinton, bush, bill, president, house} oppose {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} sign {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} approve {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house veto {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} support {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} pass {bill, measure, legislation, plan, law}
Table 4: ?Bombing? and ?legislation? schema examples from Rel-grams and Chambers represented as a set of
(A1, Rel, A2) tuples, where the schema provides a set of instances for each actor A1 and A2. Relations and argu-
ments are in the stemmed form, e.g., ?explode kill? refers to ?exploded killing?. Instances in bold produce tuples that
are not valid in the real world.
is both the Arg1 and Arg2.
Finally, we generate an ordered list of tuples using
the final set of actors and their relations. The out-
put tuples are sorted by the average page rank of the
original tuples, thereby reflecting their importance
within the sub-graph GQ.
5 Evaluation
We present experiments to explore two main ques-
tions: How well do Rel-grams capture real world
knowledge, and what is the quality of event schemas
built using Rel-grams.
5.1 Evaluating Rel-grams
What sort of common-sense knowledge is encap-
sulated in Rel-grams? How often does it indicate
an implication between a pair of statements, and
how often does it indicate a common real-world
event or topic? To answer these questions, we con-
ducted an experiment to identify a subset of our Rel-
grams database with high precision for two forms of
common-sense knowledge:
? Implication: The Rel-grams express an im-
plication from T to T? or from T? to T, a
bi-directional form of the Recognizing Tex-
tual Entailment (RTE) guidelines (Dagan et al,
2005).
? Common Topic: Is there an underlying com-
mon topic or event to which both T and T? are
relevant?
We also evaluated whether both T and T? are valid
tuples that are well-formed and make sense in the
real world, a necessary pre-condition for either im-
plication or common topic.
We are particularly interested in the highest pre-
cision portion of our Rel-grams database. The
database has 1.1M entries with support of at least
three instances for each tuple. To find the highest
precision subset of these, we identified tuples that
have at least 25 Rel-grams, giving us 12,600 seed
tuples with a total of over 280K Rel-grams. Finally,
we sorted this subset by the total symmetrical con-
ditional probability of the top 25 Rel-grams for each
seed tuple.
We tagged a sample of this 280K set of Rel-grams
for valid tuples, implication between T and T?, and
common topic. We found that in the top 10% of this
set, 87% of the seed tuples were valid and 74% of the
Rel-grams had both tuples valid. Of the Rel-grams
1726
System Id A1 Rel A2
Relgrams R1 bomb explode in city
bomb explode kill people
bomb explode on Fri.
... ... ...
Chambers C1 blast explode child
child detonate blast
child plant bomb
... ... ...
Table 5: A grounded instantiation of the schemas from
Table 4, where each actor is represented as a randomly
selected instance.
with both tuples valid, 83% expressed an implication
between the tuples, and 90% had a common topic.
There were several reasons for invalid tuples ?
parsing errors; binary projections of inherently n-ary
relations, for example (<person>, put, <person>);
head-noun only representation omitting essential in-
formation; and incorrect semantic types, primarily
due to NER tagging errors.
While the Rel-grams suffer from noise in the tu-
ple validity, there is clearly strong signal in the data
about common topic and implication between tuples
in the Rel-grams. As we demonstrate in the follow-
ing section, an end task can use graph analysis tech-
niques to amplify this strong signal, producing high-
quality relational schemas.
5.2 Schemas Evaluation
In our schema evaluation, we are interested in
assessing how well the schemas correspond to
common-sense knowledge about real world events.
To this end, we focus on three measures, topical co-
herence, tuple validity, and actor coherence.
A good schema must be topically coherent, i.e.,
the relations and actors should relate to some real
world topic or event. The tuples that comprise a
schema should be valid assertions that make sense
in the real world. Finally, each actor in the schema
should belong to a cohesive set that plays a consis-
tent role in the relations. Since there are no good
automated ways to make such judgments, we per-
form a human evaluation using workers from Ama-
zon?s Mechanical Turk (AMT).
We compare Rel-grams schemas against the state-
of-the-art narrative schemas released by Cham-
bers (Chambers and Jurafsky, 2009).8 Chambers?
8Available at http://www.usna.edu/Users/cs/
System Id A1 Rel A2
Relgrams R11 bomb explode in city
missile explode in city
grenade explode in city
... ... ...
Relgrams R21 missile explode in city
missile explode in neighborhood
missile explode in front
... ... ...
Table 6: A schema instantiation used to test for actor co-
herence. Each of the top instances for A1 or A2 is pre-
sented, holding the relation and the other actor fixed.
schemas are less expressive than ours ? they do not
associate types with actors and each schema has a
constant pre-specified number of relations. For a
fair comparison we use a similarly expressive ver-
sion of our schemas that strips off argument types
and has the same number of relations per schema
(six) as their highest quality output set.
5.2.1 Evaluation Design
We created two tasks for AMT annotators. The
first task tests the coherence and validity of rela-
tions in a schema and the second does the same
for the schema actors. In order to make the tasks
understandable to unskilled AMT workers, we fol-
lowed the accepted practice of presenting them with
grounded instances of the schemas (Wang et al,
2013), e.g., instantiating a schema with a specific ar-
gument instead of showing the various possibilities
for an actor.
First, we collect the information in schemas as a
set of tuples: S = {T1, T2, ? ? ? , Tn}, where each tu-
ple is of the form T : (X,Rel, Y ), which conveys
a relationship Rel between actors X and Y . Each
actor is represented by its highest frequency exam-
ples (instances). Table 4 shows examples of schemas
from Chambers and Rel-grams represented in this
format. Then, we create grounded tuples by ran-
domly sampling from top instances for each actor.
Task I: Topical Coherence To test whether the re-
lations in a schema form a coherent topic or event,
we presented the AMT annotators with a schema as
a set of grounded tuples, showing each relation in
the schema, but randomly selecting one of the top 5
instances from each actor. We generated five such
nchamber/data/schemas/acl09
1727
Figure 3: (a) Has Topic: Percentage of schema instanti-
ations with a coherent topic. (b) Valid Tuples: Percent-
age of grounded statements that assert valid real-world
relations. (c) Valid + On Topic: Percentage of grounded
statements where 1) the instantiation has a coherent topic,
2) the tuple is valid and 3) the relation belongs to the
common topic. All differences are statistically significant
with a p-value < 0.01.
instantiations for each schema. An example instan-
tiation is shown in Table 5.
We ask three kinds of questions on each grounded
schema: (1) is each of the grounded tuples valid (i.e.
meaningful in the real world); (2) do the majority of
relations form a coherent topic; and (3) does each
tuple belong to the common topic. Similar to pre-
vious AMT studies we get judgments from multiple
(five) annotators on each task and use the majority
labels (Snow et al, 2008).
Our instructions specified that the annotators
should ignore grammar and focus on whether a tuple
may be interpreted as a real world statement. For ex-
ample, the first tuple in R1 in Table 5 is a valid state-
ment ? ?a bomb exploded in a city?, but the tuples
in C1 ?a blast exploded a child?, ?a child detonated
a blast?, and ?a child planted a blast? don?t make
sense.
Task II: Actor Coherence To test whether the in-
stances of an actor form a coherent set, we held the
relation and one actor fixed and presented the AMT
annotators with the top 5 instances for the other ac-
tor. The first example R11 in Table 6 holds the
relation ?explode in? fixed, and A2 is grounded to
the randomly selected instance ?city?. We present
grounded tuples by varying A1 and ask annotators to
judge whether these instances form a coherent topic
and whether each instance belongs to that common
topic. As with Task I, we create five random instan-
tiations for each schema.
Figure 4: Actor Coherence: Has Role bars compare the
percentage of tuples where the tested actors have a co-
herent role. Fits Role compares the percentage of top
instances that fit the specified role for the tested actors.
All differences are statistically significant with a p-value
< 0.01.
5.2.2 Results
We obtained a test set of 100 schemas per system
by randomly sampling from the top 500 schemas
from each system. We evaluate this test set using
Task I and II as described above. For both tasks we
obtained ratings from five turkers and use the major-
ity labels as the final annotation.
Does the schema belong to a single topic? The
Has Topic bars in Figure 3 show results for schema
coherence. Rel-grams has a higher proportion of
schemas with a coherent topic, 91% compared to
82% for Chambers?. This is a 53% reduction in in-
coherent schemas.
Do tuples assert valid real-world relations? The
Valid Tuples bars in Figure 3 compare the percent-
age of valid grounded tuples in the schema instan-
tiations. A tuple was labeled valid if a majority of
the annotators labeled it to be meaningful in the real
world. Here we see a dramatic difference ? Rel-
grams have 92% valid tuples, compared with Cham-
bers? 61%.
What proportion of tuples belong? The Valid +
On Topic bars in Figure 3 compare the percentage
of tuples that are both valid and on topic, i.e., fits
the main topic of the schema. Tuples from schema
instantiations that did not have a coherent topic were
labeled incorrect.
Rel-grams have a higher proportion of valid tu-
ples belonging to a common topic, 82% compared to
1728
58% for Chambers? schemas, a 56% error reduction.
This is the strictest of the experiments described thus
far ? 1) the schema must have a topic, 2) the tuple
must be valid, and 3) the tuple must belong to the
topic.
Do actors represent a coherent set of argu-
ments? We evaluated schema actors from the top
25 schemas in Chambers? and Rel-grams schemas,
using grounded instances such as those in Table 6.
Figure 4 compares the percentage of tuples where
the actors play a coherent role (Has Role), and the
percentage of instances that fit that role for the actor
(Fits Role). Rel-grams has much higher actor co-
herence than Chambers?, with 97% judged to have a
topic compared to 81%, and 81% of instances fitting
the common role compared with Chambers? 59%.
5.2.3 Error Analysis
The errors in both our schemas and those of
Chambers are primarily due to mismatched actors
and from extraction errors, although Chambers?
schemas have a larger number of actor mismatch er-
rors and the cause of the errors is different for each
system.
Examining the data published by Chambers, the
main source of invalid tuples are mismatch of sub-
ject and object for a given relation, which accounts
for 80% of the invalid tuples. We hypothesize that
this is due to the pair-wise representation that treats
subject-verb and verb-object separately, causing in-
consistent s-v-o tuples. An example is (boiler, light,
candle) where (boiler, light) and (light, candle) are
well-formed, yet the entire tuple is not. In addition,
43% of the invalid tuples seem to be from errors by
the dependency parser.
Our schemas also suffer from mismatched actors,
despite the semantic typing of the actors ? we found
a mismatch in 56% of the invalid tuples (5% of
all tuples). A general type such as <person> or
<organization> may still have an instance that does
not play the same role as other instances. For exam-
ple a relation (A1, graduated from, A2) has A2 that
is mostly school names, but also includes ?church?
which leads to an invalid tuple.
Extraction errors account for 47% of the invalid
tuples in our schemas, primarily errors that truncate
an n-ary relation as a binary tuple. For example, the
sentence ?Mr. Diehl spends more time ... than the
commissioner? is misanalysed by the Open IE ex-
tractor as (Mr. Diehl, spend than, commissioner).
6 Related Work
Prior work by Chambers and Jurafsky (2008;
2009; 2010) showed that event sequences (narrative
chains) mined from text can be used to induce event
schemas in a domain-independent fashion. How-
ever, our manual evaluation of their output showed
key limitations which may limit applicability.
As pointed out earlier, a major weakness in
Chambers? approach is the pair-wise representation
of subject-verb and verb-object. Also, their released
a set of schemas are limited to two actors, although
this number can be increased by setting a chain split-
ting parameter.
Chambers and Jurafsky (2011) extended schema
generation to learn domain-specific event templates
and associated extractors. In work parallel to ours,
Cheung et al (2013), developed a probabilistic so-
lution for template generation. However, their ap-
proach requires performing joint probability estima-
tion using EM, which can limit scaling to large cor-
pora.
In this work we developed an Open IE based
solution to generate schemas. Following prior
work (Balasubramanian et al, 2012), we use Open
IE triples for modeling relation co-occurrence. We
extend the triple representation with semantic types
for arguments to alleviate sparisty and to improve
coherence. We developed a page rank based schema
induction algorithm which results in more coherent
schemas with several actors. Unlike Chambers? ap-
proach this method does not require explicit param-
eter tuning for controlling the number of actors.
While our event schemas are close to being tem-
plates (because of associated types, and actor clus-
tering), they do not have associated extractors. Our
future work will focus on building extractors for
these. It will also be interesting to compare with
Cheung?s system on smaller focused corpora.
Defining representations for events is a topic of
active interest (Fokkens et al, 2013). In this work,
we use a simpler representation, defining event
schemas as a set of actors with associated types and
a set of roles they play.
1729
7 Conclusions
We present a system for inducing event schemas
from text corpora based on Rel-grams, a language
model derived from co-occurrence statistics of re-
lational triples (Arg1, Relation, Arg2) extracted by
a state-of-the-art Open IE system. By using triples
rather than a pair-wise representation of subject-verb
and verb-object, we achieve more coherent schemas
than Chambers and Jurafsky (2009). In particular,
our schemas have higher topic coherence (92% com-
pared to Chambers? 82%; make a higher percentage
of valid assertions (94% compared with 61%); and
have greater actor coherence (81% compared with
59%).
Our schemas are also more expressive than those
published by Chambers ? we have semantic typing
for the actors, we are not limited to two actors per
schema, and our relation phrases include preposi-
tions and are thus more precise and have higher cov-
erage of actors involved in the event.
Our future plans are to build upon our event
schemas to create an open-domain event extractor.
This will extend each induced schema to have asso-
ciated extractors. These extractors will operate on a
document and instantiate an instance of the schema.
We have created a Rel-grams database with 1.1M
entries and a set of over 2K event schemas from a
corpus of 1.8M New York Times articles. Both are
freely available to the research community9 and may
prove useful for a wide range of NLP applications.
Acknowledgments
We thank the anonymous reviewers, Tony Fader, and
Janara Christensen for their valuable feedback. This
paper was supported by Office of Naval Research
(ONR) grant number N00014-11-1-0294, Army Re-
search Office (ARO) grant number W911NF-13-
1-0246, Intelligence Advanced Research Projects
Activity (IARPA) via Air Force Research Lab-
oratory (AFRL) contract number FA8650-10-C-
7058, and Defense Advanced Research Projects
Agency (DARPA) via AFRL contract number AFRL
FA8750-13-2-0019. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes notwithstanding any copyright
annotation thereon. The views and conclusions con-
tained herein are those of the authors and should
9available at http://relgrams.cs.washington.edu
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of ONR, ARO, IARPA, AFRL, or the U.S.
Government.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In 12th Con-
ference of the European Chapter of the Association for
Computational Linguistics (EACL), pages 33?41.
ARPA. 1991. Proc. 3rd Message Understanding Conf.
Morgan Kaufmann.
ARPA. 1998. Proc. 7th Message Understanding Conf.
Morgan Kaufmann.
Niranjan Balasubramanian, Stephen Soderland, Oren Et-
zioni, et al 2012. Rel-grams: a probabilistic model
of relations in text. In Proceedings of the Joint Work-
shop on Automatic Knowledge Base Construction and
Web-scale Knowledge Extraction, pages 101?105. As-
sociation for Computational Linguistics.
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
Networks, 30(1-7):107?117.
N. Chambers and D. Jurafsky. 2008. Unsupervised
learning of narrative event chains. In Proceedings of
ACL-08: HLT.
N. Chambers and D. Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
Proceedings of ACL.
N. Chambers and D. Jurafsky. 2010. A database of nar-
rative schemas. In Proceedings of LREC.
N. Chambers and D. Jurafsky. 2011. Template-based
information extraction without the templates. In Pro-
ceedings of ACL.
J. Cheung, H. Poon, and L. Vandervende. 2013. Prob-
abilistic frame induction. In Proceedings of NAACL
HLT.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, , and R. Weischedel. 2004. The auto-
matic content extraction (ACE) program-tasks, data,
and evaluation. In Procs. of LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363?370.
Association for Computational Linguistics.
1730
Antske Fokkens, Marieke van Erp, Piek Vossen, Sara
Tonelli, Willem Robert van Hage, BV SynerScope,
Luciano Serafini, Rachele Sprugnoli, and Jesper Hoek-
sema. 2013. Gaf: A grounded annotation framework
for events. NAACL HLT 2013, page 11.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW, pages 517?526.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 86?92. Associ-
ation for Computational Linguistics.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, (Just Accepted):1?54.
Mausam, Stephen Soderland, Oren Etzioni, Daniel Weld,
Kobi Reiter, Michael Skinner, Marcus Sammer, and
Jeff Bilmes. 2010. Panlingual lexical translation via
probabilistic inference. Artificial Intelligence Journal
(AIJ).
Mausam, Michael Schmitz, Robert Bart, Stephen Soder-
land, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of EMNLP.
K. Owczarzak and H.T. Dang. 2010. Overview of the tac
2010 summarization track.
S. Patwardhan and E. Riloff. 2009. A unified model of
phrasal and sentential evidence for information extrac-
tion. In Proceedings of EMNLP 2009.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254?
263. Association for Computational Linguistics.
A. Wang, C.D.V. Hoang, and M-Y. Kan. 2013. Perspec-
tives on crowdsourcing annotations for Natural Lan-
guage Processing. Language Resources and Evalua-
tion, 47:9?31.
1731
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523?533,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning to Solve Arithmetic Word Problems with Verb Categorization
Mohammad Javad Hosseini
1
, Hannaneh Hajishirzi
1
, Oren Etzioni
2
, and Nate Kushman
3
1
{hosseini, hannaneh}@washington.edu,
2
OrenE@allenai.org,
3
nkushman@csail.mit.edu
1
University of Washington,
2
Allen Institute for AI,
3
Massachusetts Institute of Technology
Abstract
This paper presents a novel approach to
learning to solve simple arithmetic word
problems. Our system, ARIS, analyzes
each of the sentences in the problem state-
ment to identify the relevant variables and
their values. ARIS then maps this infor-
mation into an equation that represents
the problem, and enables its (trivial) so-
lution as shown in Figure 1. The pa-
per analyzes the arithmetic-word problems
?genre?, identifying seven categories of
verbs used in such problems. ARIS learns
to categorize verbs with 81.2% accuracy,
and is able to solve 77.7% of the problems
in a corpus of standard primary school test
questions. We report the first learning re-
sults on this task without reliance on pre-
defined templates and make our data pub-
licly available.
1
1 Introduction
Designing algorithms to automatically solve math
and science problems is a long-standing AI chal-
lenge (Feigenbaum and Feldman, 1963). For NLP,
mathematical word problems are particularly at-
tractive because the text is concise and relatively
straightforward, while the semantics reduces to
simple equations.
Arithmetic word problems begin by describing
a partial world state, followed by simple updates
or elaborations and end with a quantitative ques-
tion. For a child, the language understanding part
is trivial, but the reasoning may be challenging;
for our system, the opposite is true. ARIS needs to
1
Our data is available at https://www.cs.
washington.edu/nlp/arithmetic.
Arithmetic word Problem
Liz had 9 black kittens. She gave some of her kittens to
Joan. Joan now has 11 kittens. Liz has 5 kittens left and 3
have spots. How many kittens did Joan get?
State Transitions1	
Liz	

N: 9	

E: Kitten	

A: Black	

Liz gave some of her kittens to Joan.	

s2	
 Liz	

N: 9-L1	
E: Kitten	

A: Black	

Joan	

N:  J0+L1	
E: Kitten	

A: Black	

give	

Equation: 9? x = 5
Solution: x = 4 kittens
Figure 1: Example problem and solution.
make sense of multiple sentences, as shown in Fig-
ure 2, without a priori restrictions on the syntax or
vocabulary used to describe the problem. Figure
1 shows an example where ARIS is asked to infer
how many kittens Joan received based on facts and
constraints expressed in the text, and represented
by the state diagram and corresponding equation.
While the equation is trivial, the text could have
involved assembling toy aircraft, collecting coins,
eating cookies, or just about any activity involving
changes in the quantities of discrete objects.
This paper investigates the task of learning to
solve such problems by mapping the verbs in the
problem text into categories that describe their im-
pact on the world state. While the verbs category
is crucial (e.g., what happens if ?give? is replaced
by ?receive? in Figure 1?), some elements of the
problem are irrelevant. For instance, the fact that
three kittens have spots is immaterial to the solu-
tion. Thus, ARIS has to determine what informa-
tion is relevant to solving the problem.
To abstract from the problem text, ARIS maps
the text to a state representation which consists of
523
a set of entities, their containers, attributes, quan-
tities, and relations. A problem text is split into
fragments where each fragment corresponds to an
observation or an update of the quantity of an en-
tity in one or two containers. For example in Fig-
ure 1, the sentence ?Liz has 5 kittens left and 3
have spots? has two fragments of ?Liz has 5 kit-
tens left? and ?3 have spots?.
The verb in each sentence is associated with one
or two containers, and ARIS has to classify each
verb in a sentence into one of seven categories
that describe the impact of the verb on the con-
tainers (Table 1). ARIS learns this classifier based
on training data as described in section 4.2.
To evaluate ARIS, we compiled a corpus of
about 400 arithmetic (addition and subtraction)
word problems and utilized cross validation to
both train ARIS and evaluate its performance
over this corpus. We compare its performance
to the template-based learning method developed
independently and concurrently by Kushman et
al. (2014). We find that our approach is much
more robust to domain diversity between the train-
ing and test sets.
Our contributions are three-fold: (a) We present
ARIS, a novel, fully automated method that learns
to solve arithmetic word problems; (b) We intro-
duce a method to automatically categorize verbs
for sentences from simple, easy-to-obtain train-
ing data; our results refine verb senses in Word-
Net (Miller, 1995) for arithmetic word problems;
(c) We introduce a corpus of arithmetic word prob-
lems, and report on a series of experiments show-
ing high efficacy in solving addition and subtrac-
tion problems based on verb categorization.
2 Related Work
Understanding semantics of a natural language
text has been the focus of many researchers in nat-
ural language processing (NLP). Recent work fo-
cus on learning to align text with meaning repre-
sentations in specific, controlled domains. A few
methods (Zettlemoyer and Collins, 2005; Ge and
Mooney, 2006) use an expensive supervision in
the form of manually annotated formal representa-
tions for every sentence in the training data. More
recent work (Eisenstein et al., 2009; Kate and
Mooney, 2007; Goldwasser and Roth, 2011; Poon
and Domingos, 2009; Goldwasser et al., 2011;
Kushman and Barzilay, 2013) reduce the amount
of required supervision in mapping sentences to
meaning representations while taking advantage
of special properties of the domains. Our method,
on the other hand, requires small, easy-to-obtain
training data in the form of verb categories that
are shared among many different problem types.
Our work is also closely related to the grounded
language acquisition research (Snyder and Barzi-
lay, 2007; Branavan et al., 2009; Branavan et al.,
2012; Vogel and Jurafsky, 2010; Chen et al., 2010;
Hajishirzi et al., 2011; Chambers and Jurafsky,
2009; Liang et al., 2009; Bordes et al., 2010)
where the goal is to align a text into underlying en-
tities and events of an environment. These meth-
ods interact with an environment to obtain super-
vision from the real events and entities in the envi-
ronment. Our method, on the other hand, grounds
the problem into world state transitions by learn-
ing to predict verb categories in sentences. In addi-
tion, our method combines the representations of
individual sentences into a coherent whole to form
the equations. This is in contrast with the previous
work that study each sentence in isolation from the
other sentences.
Previous work on studying math word and logic
problems uses manually aligned meaning repre-
sentations or domain knowledge where the seman-
tics for all the words is provided (Lev, 2007; Lev
et al., 2004). Most recently, Kushman et al. (2014)
introduced an algorithm that learns to align al-
gebra problems to equations through the use of
templates. This method applies to broad range of
math problems, including multiplication, division,
and simultaneous equations, while ARIS only han-
dles arithmetic problems (addition and subtrac-
tion). However, our empirical results show that
for the problems it handles, ARIS is much more
robust to diversity in the problem types between
the training and test data.
3 Arithmetic Problem Representation
We address solving arithmetic word problems that
include addition and subtraction. A problem text
is split into fragments where each fragment is rep-
resented as a transition between two world states
in which the quantities of entities are updated or
observed (Figure 2). We refer to these fragments
as sentences. We represent the world state as a tu-
ple ?E,C,R? consisting of entities E, containers
C, and relations R among entities, containers, at-
tributes, and quantities.
Entities: An entity is a mention in the text corre-
524
N: W0-13 	
E: tree	
A: walnut	

Liz had 9 black kittens. She gave some of her kittens to Joan. Joan has now 11 kittens. Liz has 5 kitten left and 3 has spots. How many kittens did Joan get?	

Liz had 9 	
black kittens	

s0	

s1	
Liz	

N: 9	
E: Kitten	
A: Black	
 She gave some of her kittens to Joan	

s2	
 Liz	

N: 9-L1	
E: Kitten	
A: Black	

Joan	

N:  J0+L1	
E: Kitten	
A: Black	
	

Joan has now 11 kittens	

 Liz has 5 kitten left	
 And 3 has spots	

Liz	

N: 9-L1	
E: Kitten	
A: Black	

Joan	

N: 11	
E: Kitten	
A: Black	

s3	
 Liz	

N: 5	
E: Kitten	
A: Black	

Joan	

N: 11	
E: Kitten	
A: Black	

s4	

Liz	

N: 5	
E: Kitten	
A: Black	

Joan	

N: 11	
E: Kitten	
A: Black	
unknown	

N:3	
E: Kitten	

s5	

There are 42 walnut trees and 12 orange trees currently in the park. Park workers cut down 13 walnut trees that were damaged. How many walnut trees will be in the park when the workers are finished?	

There are 42 walnut trees and 12 orange trees currently in the park. 	

s0	

s1	
Park	

N: 42	
E: tree	
A: walnut	
 Park workers cut down 13 walnut trees that were damaged	

N: 12	
E: tree	
A: orange	

s2	
 Park	
 N: 42-13 	
E: tree	
A: walnut	

N: 12	
E: tree	
A: orange	

Workers	

Figure 2: A figure sketching different steps of our method ? a sequence of states.
sponding to an object whose quantity is observed
or is changing throughout the problem. For in-
stance, kitten and tree are entities in Fig-
ure 2. In addition, every entity has attributes that
modify the entity. For instance, black is an at-
tribute of kittens, and walnut is an attribute
of tree (more details on attributes in section 4.1).
Relations describing attributes are invariant to the
state changes. For instance kittens stay black
throughout the problem of Figure 1.
Containers: A container is a mention in the
text representing a set of entities. For instance,
Liz, Joan, park, and workers are containers
in Figure 2. Containers usually correspond to the
person possessing entities or a location contain-
ing entities. For example, in the sentence ?There
are 43 blue marbles in the basket. John found 32
marbles.?, basket and John are containers of
marbles.
Quantities: Containers include entities with their
corresponding quantities in a particular world
state. Quantities can be known numbers (e.g. 9),
unknown variables (e.g. L
1
), or numerical expres-
sions over unknown quantities and numbers (e.g.
9?L
1
). For instance, in state 2 of Figure 2, the nu-
merical expression corresponding to Liz is 9?L
1
and corresponding to Joan is J
0
+ L
1
, where J
0
is a variable representing the number of kittens
that Joan has started with.
Hereinafter, we will refer to a generic entity as
e, container as c, number as num, attribute as a.
We represent the relation between a container, an
entity, and a number in the form of a quantity ex-
Category Example
Observation There were 28 bales of hay in the barn.
Positive Joan went to 4 football games this year.
Negative John lost 3 of the violet balloons.
Positive
Transfer
Mike?s dad borrowed 7 nickels from
Mike.
Negative
Transfer
Jason placed 131 erasers in the drawer.
Construct Karen added 1/4 of a cup of walnuts to a
batch of trail mix.
Destroy The rabbits ate 4 of Dan?s potatoes.
Table 1: Examples for different verb categories in sen-
tences. Entities are underlined; containers are italic, and
verbs are bolded.
pression N(c,e). Figure 2 shows the quantity
relations in different world states.
State transitions: Sentences depict progression
of the world state (Figure 2) in the form of ob-
servations of updates of quantities. We assume
that every sentence w consists of a verb v, an en-
tity e, a quantity num (might be unknown), one
or two containers c
1
, c
2
, and attributes a. The
presence of the second container, c
2
, will be dic-
tated by the category of the verb, as we discuss
below. Sentences abstract transitions (s
t
? s
t+1
)
between states in the form of an algebraic opera-
tion of addition or subtraction. For every sentence,
we model the state transition according to the verb
category and containers in the sentence. There are
three verb categories for sentences with one con-
tainer: Observation: the quantity is initialized in
the container, Positive: the quantity is increased
in the container, and Negative: the quantity is de-
creased in the container. Moreover, there are four
categories for sentences with two containers: Pos-
525
itive transfer: the quantity is transferred from the
second container to the first one, Negative trans-
fer: the quantity is transferred from the first con-
tainer to the second one, Construct: the quantity
is increased for both containers, and Destroy: the
quantity is decreased for both containers.
Figure 2 shows how the state transitions are
determined by the verb categories. The sen-
tence ?Liz has 9 black kittens? initializes the
quantity of kittens in the container Liz
to 9. In addition, the sentence ?She gave
some of her kittens to Joan.? shows the
negative transfer of L
1
kittens from Liz to
Joan represented as N(Liz,kitten)=9-L
1
and N(Joan,kitten)=J
0
+ L
1
.
Given a math word problem, ARIS grounds the
world state into entities (e.g., kitten), contain-
ers (e.g., Liz), attributes (e.g., black), and quan-
tities (e.g., 9) (Section 4.1). In addition, ARIS
learns state transitions by classifying verb cate-
gories in sentences (Section 4.2). Finally, from the
world state and transitions, it generates an arith-
metic equation which can be solved to generate the
numeric answer to the word problem.
4 Our Method
In this section we describe how ARIS maps an
arithmetic word problem into an equation (Fig-
ure 2). ARIS consists of three main steps (Fig-
ure 3): (1) grounding the problem into entities and
containers, (2) training a model to classify verb
categories in sentences, and (3) solving the prob-
lem by updating the world states with the learned
verb categories and forming equations.
4.1 Grounding into Entities and Containers
ARIS automatically identifies entities, attributes,
containers, and quantities corresponding to every
sentence fragment (details in Figure 3 step 1). For
every problem, this module returns a sequence of
sentence fragments ?w
1
, . . . , w
T
, w
x
?where every
w
t
consists of a verb v
t
, an entity e
t
, its quantity
num
t
, its attributes a
t
, and up to two containers
c
t
1
, c
t
2
. w
x
corresponds to the question sentence
inquiring about an unknown entity. ARIS applies
the Stanford dependency parser, named entity rec-
ognizer and coreference resolution system to the
problem text (de Marneffe et al., 2006; Finkel et
al., 2005; Raghunathan et al., 2010). It uses the
predicted coreference relationships to replace pro-
nouns (including possessive pronouns) with their
coreferenent links. The named entity recognition
output is used to identify numbers and people.
Entities: Entities are references to some object
whose quantity is observed or changing through-
out the problem. So to determine the set of
entities, we define h as the set of noun types
which have a dependent number (in the depen-
dency parse) somewhere in the problem text. The
set of entities is then defined as all noun phrases
which are headed by a noun type in h. For in-
stance kitten in the first sentence of Figure 1
is an entity because it is modified by the number
9, while kitten in the second sentence of Fig-
ure 1 is an entity because kitten was modified
by a number in the first sentence. Every number
in the text is associated with one entity. Num-
bers which are dependents of a noun are associ-
ated with its entity. Bare numbers (not dependent
on a noun) are associated with the previous entity
in the text. The entity in the last sentence is identi-
fied as the question entity e
x
. Finally, ARIS splits
the problem text into T + 1 sentence fragments
?w
1
, . . . w
T
, w
x
? such that each fragment contains
a single entity and it?s containers. For simplicity
we refer to these fragments as a sentences.
Containers: Each entity is associated with one
or two container noun phrases using the algorithm
described in in Figure 3 step 1c. As we saw earlier
with numbers, arithmetic problems often include
sentences with missing information. For example
in Figure 2, the second container in the the sen-
tence ?Park workers had to cut down 13 walnut
trees that were damaged.? is not explicitly men-
tioned. To handle this missing information, we
use the circumscription assumption (McCarthy,
1980). The circumscription assumption formal-
izes the commonsense assumption that things are
as expected unless otherwise specified. In this set-
ting, we assume that the set of containers are fixed
in a problem. Thus if the container(s) for a given
entity cannot be identified they are set to the con-
tainer(s) for the previous entity with the same head
word. For example in Figure 2 we know from the
previous sentence that trees were in the park.
Therefore, we assume that the unmentioned con-
tainer is the park.
Attributes: ARIS selects attributes A as modifiers
for every entity from the dependency parser (de-
tails in Figure 3 step 1a). For example black is
an attribute of the entity kitten and is an ad-
jective modifier in the parser. These attributes are
526
1. Grounding into entities and containers: for every problem p in dataset (Section 4.1)
(a) ?e
1
, . . . , e
T
, e
x
?
p
? extract all entities and the question entity
i. Extract all numbers and noun phrases (NP).
ii. h ? all noun types which appear with a number as a dependant (in the dependency parse tree) somewhere
in the problem text.
iii. e
t
? all NPs which are headed by a noun type in h.
iv. num
t
? the dependant number of e
t
if one exists. Bare numbers (not directly dependant on any noun
phrase) are associated with the previous entity in the text. All other num
t
are set to unknown.
v. e
x
? the last identified entity.
vi. a
t
? adjective and noun modifiers of e
t
. Update implicit attributes using the previously observed attributes.
vii. v
t
? the verb with the shortest path to e
t
in the dependency parse tree.
(b) ?w
1
, . . . , w
T
, w
x
?
p
? split the problem text into fragments based on the entities and verbs
(c) ?c
t
1
, c
t
2
, . . . , c
T
1
, c
T
2
, c
x
?
p
? the list of containers for each entity
i. c
t
1
? the subject of w
t
.
If w
t
contains There is/are, c
t
1
is the first adverb of place to the verb.
ii. c
t
2
? An NP that is direct object of the verb. If not found, c
t
2
is the object of the first adverbial phrase of
the verb.
iii. Circumscription assumption: When c
t
1
or c
t
2
are not found, they are set to the previous containers.
2. Training for sentence categorization (Section 4.2)
(a) instances
1
, instances
2
? ?
(b) for every sentence w
t
? ?w
1
, . . . , w
T
, w
x
?
p
in the training set:
i. features
t
? extract features (similarity based, WordNet based, structural) (Section 4.2.1)
ii. l
t
1
, l
t
2
? determine labels for containers c
t
1
and c
t
2
based on the verb category of w
t
.
iii. append ?features
t
, l
t,1
?, ?features
t
, l
t,2
? to instances
1
, instances
2
.
(c) M
1
,M
2
? train two SVMs for instances
1
, instances
2
3. Solving: for every problem p in the test set (Section 4.3)
(a) Identifying verb categories in sentences
i. for every sentence w
t
? ?w
1
, . . . , w
T
, w
x
?
p
:
A. features
t
? extract features (similarity based, WordNet based, structural).
B. l
t
1
, l
t
2
? classify w
t
for both containers c
t
1
and c
t
2
using models M
1
,M
2
.
(b) State progression: Form ?s
0
, . . . , s
T
? (Section 4.3.1)
i. s
0
? null.
ii. for t ? ?1, . . . , T ?: s
t
? progress(s
t?1
, w
t
).
A. if e
t
= e
x
and a
t
= a
x
:
if w
t
is an observation: N
t
(c
t
1
, e
t
) = num
t
.
else: update N
t
(c
t
1
, e
t
) and N
t
(c
t
2
, e
t
) given verb categories l
t
1
, l
t
2
.
B. copy N
t?1
(c, e) to N
t
(c, e) for all other (c, e) pairs.
(c) Forming equations and solution (Section 4.3.2)
i. Mark each w
t
that matches with w
x
if:
a) c
t
1
matches with c
x
and verb categories are equal or verbs are similar.
b) c
t
2
matches with c
x
and the verbs are in opposite categories.
ii. x? the unknown quantity if w
x
matches with a sentence introducing an unknown number
iii. If the question asks about an unknown variable x or a start variable (w
x
contains ?begin? or ?start?):
For some container c, find two states s
t
(quantity expression contains x) and s
t+1
(quantity is a known
number). Then, form an equation for x: N
t
(c, e
x
) = N
t+1
(c, e
x
).
iv. else: form equation as x = N
t
(c
x
, e
x
).
v. Solve the equation and return the absolute value of x.
Figure 3: ARIS: a method for solving arithmetic word problems.
used to prune the irrelevant information in pro-
gressing world states.
Arithmetic problems usually include sentences
with no attributes for the entities. For example,
the attribute black has not been explicitly men-
tioned for the kitten in the second sentence. In
particular, ARIS updates an implicit attribute using
the previously observed attribute. For example, in
?Joan went to 4 football games this year. She went
to 9 games last year.?, ARIS assigns football as
an attribute of the game in both sentences.
4.2 Training for Verb Categories
This step involves training a model to identify verb
categories for sentences. This entails predicting
one label (increasing, decreasing) for each (verb,
container) pair in the sentence. Each possible set-
ting of these binary labels corresponds to one of
the seven verb categories discussed earlier. For ex-
ample, if c
1
is increasing and c
2
is decreasing this
is a positive transfer verb.
Our dataset includes word problems from dif-
ferent domains (more details in Section 5.2). Each
verb in our dataset is labeled with one of the 7 cat-
527
egories from Table 1.
For training, we compile a list of sentences from
all the problems in the dataset and split sentences
into training and test sets in two settings. In the
first setting no instance from the same domain
appears in the training and test sets in order to
study the robustness of our method to new prob-
lem types. In the second setting no verb is re-
peated in the training and test sets in order to study
how well our method predicts categories of unseen
verbs.
For every sentence w
t
in the problems, we build
two data instances, (w
t
, c
1
) and (w
t
, c
2
), where c
1
and c
2
are containers extracted from the sentence.
For every instance in the training data, we assign
training labels using the verb categories of the sen-
tences instead of labeling every sentence individu-
ally. The verb can be increasing or decreasing cor-
responding to every container in the sentence. For
positive (negative) and construction (destruction)
verbs, both instances are labeled positive (nega-
tive). For transfer positive (negative) verbs, the
first instance is labeled positive (negative) and the
second instance is labeled negative (positive). For
observation verbs, both instances are labeled pos-
itive. We assume that the observation verbs are
known (total of 5 verbs). Finally, we train Support
Vector Machines given the extracted features and
training labels explained above (Figure 3 step 2).
In the following, we describe the features used for
training.
4.2.1 Features
There are three sets of features: similarity based,
Wordnet-based, and structural features. The first
two sets of features focus on the verb and the third
set focuses on the dependency structure of the sen-
tence. All of our features are unlexicalized. This
allows ARIS to handle verbs in the test questions
which are completely different from those seen in
the training data.
Similarity-based Features: For every instance
(w, c), the feature vector includes similarity be-
tween the verb of the sentence w and a list of seed
verbs. The list of seed verbs is automatically se-
lected from a set V containing the 2000 most com-
mon English verbs using `
1
regularized feature se-
lection technique. We select a small set of seed
verbs to avoid dominating the other feature types
(structural and WordNet-based features).
The goal is to automatically select verbs from
V that are most discriminative for each of the 7
verb categories in Table 1. We define 7 classifi-
cation tasks: ?Is a verb a member of each cate-
gory?? Then, we select the three most represen-
tative verbs for each category. To do so, we ran-
domly select a set of 65 verbs V
l
, from all the verbs
in our dataset (118 in total) and manually anno-
tate the verb categories. For every classification
task, the feature vector X includes the similarity
scores (Equation 1) between the verb v and all the
verbs in the V . We train an `
1
regularized regres-
sion model (Park and Hastie, 2007) over the fea-
ture vector X to learn each category individually.
The number of original (similarity based) features
in X is relatively large, but `
1
regularization pro-
vides a sparse weight vector. ARIS then selects the
three most common verbs (without replacement)
among the features (verbs) with non-zero weights.
This accounts for 21 total seed verbs to be used for
the main classification task. We find that in prac-
tice using this selection technique leads to better
performance than using either all the verbs in V or
using just the 65 randomly selected verbs.
Our method computes the similarity between
two verbs v
1
and v
2
from the similarity between all
the senses (from WordNet) of these verbs (Equa-
tion 1). We compute the similarity between two
senses using linear similarity (Lin, 1998). The
similarity between two synsets sv
1
and sv
2
are pe-
nalized according to the order of each sense for the
corresponding verb. Intuitively, if a synset appears
earlier in the set of synsets of a verb, it is more
likely to be considered as the correct meaning.
Therefore, later occurrences of a synset should re-
sult in reduced similarity scores. The similarity
between two verbs v
1
and v
2
is the maximum sim-
ilarity between two synsets of the verbs:
sim(v
1
, v
2
) = max
sv:synsets(v)
lin-sim(sv
1
, sv
2
)
log(p
1
+ p
2
)
(1)
where sv
1
, sv
2
are two synsets, p
1
, p
2
are the posi-
tion of each synset match, and lin-sim is the linear
similarity. Our experiments show better perfor-
mance using linear similarity compared to other
common similarity metrics (e.g., WordNet path
similarity and Resnik similarity (Resnik, 1995)).
WordNet-based Features: We use WordNet
verb categories in the feature vector. For each
part of speech in WordNet, the synsets are or-
ganized into different categories. There are
15 categories for verbs. Some examples in-
528
clude ?verb.communication?, ?verb.possession?,
and ?verb.creation?. In addition, WordNet in-
cludes the frequency measure f
c
sv
indicating how
often the sense sv has appeared in a reference cor-
pus. For each category i, we define the feature f
i
as the ratio of the frequency of the sense sv
i
over
the total frequency of the verb i.e., f
i
= f
c
sv
i
/f
c
v
.
Structural Features: For structural features, we
use the dependency relations between the verb and
the sentence elements since they can be a good
proxy of the sentence structure. ARIS uses a bi-
nary vector including 35 dependency relations be-
tween the verb and other elements. For example,
in the sentence ?Joan picked 2 apples from the ap-
ple tree?, the dependency between (?picked? and
?tree?) and (?picked? and ?apples?) are depicted as
?prep-from? and ?dobj? relations in the dependency
parser, respectively. In addition, we include the
length of the path in the dependency parse from
the entity to the verb.
4.3 Solving the Problem
So far, ARIS grounds every problem into entities,
containers, and attributes, and learns verb cate-
gories in sentences. Solving the problem consists
of two main steps: (1) progressing states based on
verb categories in sentences and (2) forming the
equation.
4.3.1 State Progression with Verb Categories
This step (Figure 3 step 3b) involves forming
states ?s
1
, . . . , s
T
? by updating quantities in every
container using learned verb categories (Figure 3
step 3a). ARIS initializes s
0
to an empty state. It
then iteratively updates the state s
t
by progressing
the state s
t?1
given the sentence w
t
with the verb
v, entity e, number num, and containers c
1
and c
2
.
For a given sentence t, ARIS attempts to match
e
t
and c
t
to entities and categories in s
t?1
. An
entity/category is matched if has the same head
word and same set of attributes as an existing en-
tity/category. If an entity or category cannot be
matching to one in s
t?1
, then a new one is created
in s
t
.
The progress subroutine prunes the irrelevant
sentences by checking if the entity e and its at-
tributes a agree with the question entity e
x
and its
attributes a
x
in the question. For example both
game entities agree with the question entity in the
problem ?Joan went to 4 football games this year.
She went to 9 games last year. How many football
games did Joan go??. The first entity has an ex-
plicit football attribute, and the second entity
has been assigned the same attribute (Section 4.1).
Even if the question asks about games without
mentioning football, the two sentences will
match the question. Note that the second sentence
would have not been matched if there was an ex-
plicit mention of the ?basketball game? in the sec-
ond sentence.
For the matched entities, ARIS initializes or up-
dates the values of the containers c
1
, c
2
in the state
s
t
. ARIS uses the learned verb categories in sen-
tences (Section 4.2) to update the values of con-
tainers. For an observation sentence w
t
, the value
of c
1
in the state s
t
is assigned to the observed
quantity num. For other sentence types, if the
container c does not match to a container the pre-
vious state, its value is initialized with a start vari-
able C
0
. For example, the container Joan is ini-
tialized with J
0
at the state s
1
(Figure 2). Other-
wise, the values of c
1
and c
2
are updated according
to the verb category in the sentence. For instance,
if the verb category in the sentence is a positive
transfer then N
t
(c
1
, e) = N
t?1
(c
1
, e)? num and
N
t
(c
2
, e) = N
t?1
(c
2
, e) + num where N
t
(c, e)
represents the quantity of e in the container c at
state s
t
(Figure 2).
4.3.2 Forming Equations and Solution
The question entity e
x
can match either to an en-
tity in the final state, or to some unknown gener-
ated during the state progression. Concretely, the
question sentence w
x
asks about the quantity x of
the entity e
x
in a container c
x
at a particular state
s
u
or a transition after the sentence w
u
(Figure 3
step 3c).
To determine if e
x
matches to an unknown vari-
able, we define a matching subroutine between
the question sentence w
x
and every sentence w
t
to check entities, containers, and verbs (Figure 3
step 3(c)i). We consider two cases. 1) When
w
x
contains the words ?begin?, or ?start?, the un-
known variable is about the initial value of an en-
tity, and it is set to the start variable of the con-
tainer c
x
(Figure 3 step 3(c)iii). For example, in
?Bob had balloons. He gave 9 to his friends. He
now has 4 balloons. How many balloons did he
have to start with??, the unknown variable is set to
the start variable B
0
. 2) When the question verb
is not one of the defined set of observation verbs,
ARIS attempts to match e
x
with an unknown in-
troduced by one of the state transitions (Figure 3
529
step 3(c)iii). For example, the second sentence
in Figure 1 introduces an unknown variable over
kittens. The matching subroutine matches this
entity with the question entity since the question
container, i.e. Joan, matches with the second
container and verb categories are complementary.
In order to solve for the unknown variable x,
ARIS searches through consecutive states s
t
and
s
t+1
, where in s
t
, the quantity of e
x
for a container
c is an expression over x, and in s
t+1
, the quan-
tity is a known number for a container matched
to c. It then forms an equation by comparing the
quantities for containers matched between the two
states. In the previous example, the equation will
be B
0
? 9 = 4 by comparing states s
2
and s
3
,
where the numerical expression over balloons
is B
0
?9 in the state s
2
, and the quantity is a known
number in the state s
3
.
When neither of the two above cases apply,
ARIS matches e
x
to an entity in the final state,
s
T
and returns its quantity, (Figure 3 step 3(c)iv).
In the football example of the previous sec-
tion, the equation will be x = N
t
(c
x
, e
x
), where
N
t
(c
x
, e
x
) is the quantity in the final state.
Finally, the equation will be solved for the un-
known variable x and the absolute value of the un-
known variable is returned.
5 Experiments
To experimentally evaluate our method we build
a dataset of arithmetic word problems along with
their correct solutions. We test our method on the
accuracy of solving arithmetic word problems and
identifying verb categories in sentences.
5.1 Experimental Setup
Datasets: We compiled three diverse datasets
MA1, MA2, IXL (Table 2) of Arithmetic word
problems on addition and subtraction for third,
fourth, and fifth graders. These datasets have sim-
ilar problem types, but have different characteris-
tics. Problem types include combinations of ad-
ditions, subtractions, one unknown equations, and
U.S. money word problems. Problems in MA2 in-
clude more irrelevant information compared to the
other two datasets, and IXL includes more infor-
mation gaps. In total, they include 395 problems,
13,632 words, 118 verbs, and 1,483 sentences.
Tasks and Baselines: We evaluate ARIS on two
tasks: 1) solving arithmetic word problems in the
three datasets and 2) classifying verb categories in
Source #Tests Avg.# Sentences
MA1 math-aids.com 134 3.5
IXL ixl.com 140 3.36
MA2 math-aids.com 121 4.48
Table 2: Properties of the datasets.
MA1 IXL MA2 Total
3-fold Cross validation
ARIS 83.6 75.0 74.4 77.7
ARIS
2
83.9 75.4
+
69.8
+
76.5
+
KAZB 89.6 51.1 51.2 64.0
Majority 45.5 71.4 23.7 48.9
Gold sentence categorization
Gold ARIS 94.0 77.1 81.0 84.0
Table 3: Accuracy of solving arithmetic word problems in
three datasets MA1, IXL, and MA2. This table compares
our method, ARIS, ARIS
2
with the state-of-the-art KAZB. All
methods are trained on two (out of three) datasets and tested
on the other one. ARIS
2
is trained when no verb is repeated
in the training and test sets. Gold ARIS uses gold verb cat-
egories. The improvement of ARIS (boldfaced) and ARIS
2
(denoted by
+
) are significant over KAZB and the majority
baseline with p < 0.05.
sentences. We use the percentage of correct an-
swers to the problems as the evaluation metric for
the first task and accuracy as the evaluation metric
for the second task. We use Weka?s SVM (Wit-
ten et al., 1999) with default parameters for clas-
sification which is trained with verb categories in
sentences (as described in Section 4.2).
For the first task, we compare ARIS with
KAZB (Kushman et al., 2014), majority baseline,
ARIS
2
, and Gold ARIS. KAZB requires training
data in the form of equation systems and numeri-
cal answers to the problems. The majority base-
line classifies every instance as increasing. In
ARIS
2
(a variant of ARIS) the system is trained in
a way that no verb is repeated in the training and
test sets. Gold ARIS uses the ground-truth sen-
tence categories instead of predicted ones. For the
second task, we compare ARIS with a baseline that
uses WordNet verb senses.
5.2 Results
We evaluate ARIS in solving arithmetic word
problems in the three datasets and then evaluate its
ability in classifying verb categories in sentences.
5.2.1 Solving Arithmetic Problems
Table 3 shows the accuracy of ARIS in solv-
ing problems in each dataset (when trained on
the other two datasets).Table 3 shows that ARIS
530
significantly outperforms KAZB and the major-
ity baseline. As expected, ARIS shows a larger
gain on the two more complex datasets MA2 and
IXL; our method shows promising results in deal-
ing with irrelevant information (dataset MA2) and
information gaps (dataset IXL). This is because
ARIS learns to classify verb categories in sen-
tences and does not require observing similar pat-
terns/templates in the training data. Therefore,
ARIS is more robust to differences between the
training and test datasets and can generalize across
different dataset types. As discussed in the ex-
perimental setup, the datasets have mathematically
similar problems, but differ in the natural language
properties such as in the sentence length and irrel-
evant information (Table 2).
Table 3 also shows that the sentence categoriza-
tion is performed with high accuracy even if the
problem types and also the verbs are different. In
particular, there are a total of 118 verbs among
which 64 verbs belong to MA datasets and 54 are
new to IXL. To further study this, we train our
method ARIS
2
in which no verb can be repeated
in the training and test sets. ARIS
2
still signifi-
cantly outperforms KAZB. In addition, we observe
only a slight change in accuracy between ARIS
and ARIS
2
.
To further understand our method, we study the
effect of verb categorization in sentences in solv-
ing problems. Table 3 shows the results of Gold
ARIS in solving arithmetic word problems with
gold sentence categorizations. In addition, com-
paring ARIS with Gold ARIS suggests that our
method is able to reliably identify verb categories
in sentences.
We also perform an experiment where we pool
all of the problems in the three datasets and
randomly choose 3 folds for the data (instead
of putting each original dataset into it?s own
fold). We compare our method with KAZBin
this scenario. In this setting, our method?s accu-
racy is 79.5% while KAZB?s accuracy is 81.8%.
As expected, our method?s performance has not
changed significantly from the previous setting,
while KAZB?s performance significantly improves
because of the reduced diversity between the train-
ing and test sets in this scenario.
5.2.2 Sentence Categorization
Table 4 compares accuracy scores of sentence
categorization for our method with different fea-
tures, a baseline that uses WordNet verb senses,
and the majority baseline that assigns every (verb,
container) pair as increasing. Similar to ARIS
2
,
we randomly split verbs into three equal folds
and assign the corresponding sentences to each
fold. No verb is shared between training and test
sets. We then directly evaluate the accuracy of
the SVM?s verb categorization (explained in Sec-
tion 4.2). This table shows that ARIS performs
well in classifying sentence categories even with
new verbs in the test set. This suggests that our
method can generalize well to predict verb cate-
gories for unseen verbs.
Table 4 also details the performance of four
variants of our method that ablate various features
of ARIS. The table shows that similarity, contex-
tual, and WordNet features are all important to
the performance of ARIS in verb categorization,
whereas the WordNet features are less important
for solving the problems. In addition, it shows that
similarity features play more important roles. We
also performed another experiment to study the ef-
fect of the proposed feature selection method for
similarity-based features. The accuracy of ARIS
in classifying sentence categories is 69.7% when
we use all the verbs in V in the similarity feature
vector. This shows that our feature selection algo-
rithm for selecting seed verbs is important towards
categorizing verbs.
Finally, Table 4 shows that our method signif-
icantly outperforms the baseline that only uses
WordNet verb sense. An interesting observation
is that the majority baseline in fact outperforms
WordNet verb senses in verb categorization, but
is significantly worse in solving arithmetic word
problems. In addition, we evaluate the accuracy
of predicting only verb categories by assigning the
verb label according to the majority of its labels
in the sentence categories. The accuracy of verb
categories is 78.2% confirming that ARIS is able
to successfully categorize verbs.
5.2.3 Error Analysis
We analyzed all 63 errors of Gold ARIS and
present our findings in Table 5. There are five ma-
jor classes of errors. In the first category, some in-
formation is not mentioned explicitly and should
be entailed. For example, ?washing cars? is the
source of ?making money?. Despite the improve-
ments that come from ARIS, a large portion of the
errors can still be attributed to irrelevant informa-
tion. For example, ?short? is not a ?toy?. The third
category refers to errors that require knowledge
531
Categorization Solution
ARIS 81.2
+
76.5
+
No similarity features 68.8 65.4
No WordNet features 75.3 78.0
+
No structural features 75.5 72.4
+
Baseline (WordNet) 67.8 68.4
Majority Baseline 73.4 48.9
Table 4: Ablation study and baseline comparisons: this ta-
ble reports the accuracy of verb categorization in sentences
and solutions for ARIS with ablating features. It also pro-
vides comparisons to WordNet and majority baselines. The
improvement of ARIS (boldfaced) and ablations denoted by
+
are statistically significant over the baselines (with p < 0.05)
for both tasks.
Error type Example
Entailment,
Implicit
Action (26%)
Last week Tom had $74. He washed cars
over the weekend and now has $86. How
much money did he make washing cars?
Irrelevant
Information
(19%)
Tom bought a skateboard for $9.46, and
spent $9.56 on marbles. Tom also spent
$14.50 on shorts. In total, how much did
Tom spend on toys?
Set Comple-
tion (13%)
Sara?s school played 12 games this year.
They won 4 games. How many games did
they lose?
Parsing
Issues (21%)
Sally had 27 Pokemon cards. Dan gave
her 41 new Pokemon cards. How many
Pokemon cards does Sally have now?
Others (21%) In March it rained 0.81 inches. It rained
0.35 inches less in April than in March.
How much did it rain in April?
Table 5: Examples of different error categories and relative
frequencies. The cause of error is bolded.
about set completions. For example, the ?played?
games can be split into ?win? and ?lost? games.
Finally, parsing and coreference mistakes are an-
other source of errors for ARIS.
6 Discussions and Conclusion
In this paper we introduce ARIS, a method for
solving arithmetic word problems. ARIS learns
to predict verb categories in sentences using syn-
tactic and (shallow) semantic features from small,
easy-to-obtain training data. ARIS grounds the
world state into entities, sets, quantities, attributes,
and their relations and takes advantage of the cir-
cumscription assumption and successfully fills in
the information gaps. Finally, ARIS makes use
of attributes and discards irrelevant information in
the problems. Together these provide a new rep-
resentation and a learning algorithm for solving
arithmetic word problems.
This paper is one step toward building a sys-
tem that can solve any math and logic word
problem. Our empirical evaluations show that
our method outperforms a template-based learn-
ing method (developed recently by Kushman et al.
(2014)) on solving addition and subtraction prob-
lems with diversity between the training and test
sets. In particular, our method generalizes bet-
ter to data from different domains because ARIS
only relies on learning verb categories which al-
leviates the need for equation templates for arith-
metic problems. In this paper, we have focused
on addition and subtraction problems. However,
KAZB can deal with more general types of prob-
lems such as multiplication, division, and simulta-
neous equations.
We have observed a complementary behavior
between our method and that of Kushman et al.
This suggests a hybrid approach that can bene-
fit from the strengths of both methods while be-
ing applicable to more general problems while ro-
bust to the errors specific to each. In addition, we
plan to focus on incrementally collecting domain
knowledge to deal with missing information gaps.
Another possible direction is to improve parsing
and coreference resolution.
Acknowledgments
The research was supported by the Allen Institute
for AI, and grants from the NSF (IIS-1352249)
and UW-RRF (65-2775). We thank Ben Hixon
and the anonymous reviewers for helpful com-
ments and the feedback on the work.
References
Antoine Bordes, Nicolas Usunier, and Jason Weston. 2010.
Label ranking under ambiguous supervision for learning
semantic correspondences. In Proc. International Confer-
ence on Machine Learning (ICML).
SRK Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina
Barzilay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proc. of the Annual Meeting of the
Association for Computational Linguistics and the Inter-
national Joint Conference on Natural Language Process-
ing of the AFNLP (ACL-AFNLP).
SRK Branavan, Nate Kushman, Tao Lei, and Regina Barzi-
lay. 2012. Learning high-level planning from text. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint Con-
ference on Natural Language Processing of the AFNLP
(ACL-AFNLP).
532
David Chen, Joohyun Kim, and Raymond Mooney. 2010.
Training a multilingual sportscaster: Using perceptual
context to learn language. Journal of Artificial Intelli-
gence Research, 37.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proc. Language
Resources and Evaluation Conference (LREC).
Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan
Roth. 2009. Reading to learn: Constructing features
from semantic abstracts. In Proc. Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP).
Edward A. Feigenbaum and Julian Feldman, editors. 1963.
Computers and Thought. McGraw Hill, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Ruifang Ge and Raymond J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Proc. of the Annual
Meeting of the Association for Computational Linguistics
(ACL).
Dan Goldwasser and Dan Roth. 2011. Learning from natural
instructions. In Proceedings of International Joint Con-
ference on Artificial Intelligence (IJCAI).
Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth.
2011. Confidence driven unsupervised semantic parsing.
In Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Hannaneh Hajishirzi, Julia Hockenmaier, Erik T. Mueller,
and Eyal Amir. 2011. Reasoning about robocup soccer
narratives. In Proc. Conference on Uncertainty in Artifi-
cial Intelligence (UAI).
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervision. In
Proc. Conference of the Association for the Advancement
of Artificial Intelligence (AAAI).
Nate Kushman and Regina Barzilay. 2013. Using seman-
tic unification to generate regular expressions from natu-
ral language. In Proceeding of the Annual Meeting of the
North American Chapter of the Association for Computa-
tional Linguistics.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina
Barzilay. 2014. Learning to automatically solve algebra
word problems. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Iddo Lev, Bill MacCartney, Christopher D. Manning, , and
Roger Levy. 2004. Solving logic puzzles: From robust
processing to precise semantics. In Workshop on Text
Meaning and Interpretation at Association for Computa-
tional Linguistics (ACL).
Iddo Lev. 2007. Packed Computation of Exact Meaning Rep-
resentations. Ph.D. thesis, CS, Stanford University.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learn-
ing semantic correspondences with less supervision. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint Con-
ference on Natural Language Processing of the AFNLP
(ACL-AFNLP).
Dekang Lin. 1998. An information-theoretic definition of
similarity. In Proc. International Conference on Machine
Learning (ICML).
John McCarthy. 1980. Circumscription?a form of non-
monotonic reasoning. Artificial Intelligence, 13.
George A Miller. 1995. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38.
Mee Young Park and Trevor Hastie. 2007. L1-regularization
path algorithm for generalized linear models. Journal of
the Royal Statistical Society: Series B (Statistical Method-
ology), 69.
Hoifung Poon and Pedro Domingos. 2009. Unsupervised se-
mantic parsing. In Proc. Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangara-
jan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky,
and Christopher Manning. 2010. A multi-pass sieve for
coreference resolution. In Proc. Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Philip Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In International joint
conference on Artificial intelligence (IJCAI).
Benjamin Snyder and Regina Barzilay. 2007. Database-text
alignment via structured multilabel classification. In Pro-
ceedings of International Joint Conference on Artificial
Intelligence (IJCAI).
Adam Vogel and Daniel Jurafsky. 2010. Learning to follow
navigational directions. In Proc. of the Annual Meeting of
the Association for Computational Linguistics (ACL).
Ian H Witten, Eibe Frank, Leonard E Trigg, Mark A Hall, Ge-
offrey Holmes, and Sally Jo Cunningham. 1999. Weka:
Practical machine learning tools and techniques with java
implementations.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proc. Confer-
ence on Uncertainty in Artificial Intelligence (UAI).
533
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 12?16,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Chinese Open Relation Extraction for Knowledge Acquisition Yuen-Hsien Tseng1, Lung-Hao Lee1,2, Shu-Yen Lin1, Bo-Shun Liao1,  Mei-Jun Liu1, Hsin-Hsi Chen2, Oren Etzioni3, Anthony Fader4   1Information Technology Center, National Taiwan Normal University  2Dept. of Computer Science and Information Engineering, National Taiwan University 3Allen Institute for Artificial Intelligence, Seattle, WA 4Dept. of Computer Science and Engineering, University of Washington  {samtseng, lhlee, sylin, skylock, meijun}@ntnu.edu.tw, hhchen@ntu.edu.tw, OrenE@allenai.org, afader@cs.washington.edu  Abstract 
This study presents the Chinese Open Relation Extraction (CORE) system that is able to extract entity-relation triples from Chinese free texts based on a series of NLP techniques, i.e., word segmentation, POS tagging, syntactic parsing, and extraction rules. We employ the proposed CORE techniques to extract more than 13 million entity-relations for an open domain question answering application. To our best knowledge, CORE is the first Chinese Open IE system for knowledge acquisition.  1 Introduction  Traditional Information Extraction (IE) involves human intervention of handcrafted rules or tagged examples as the input for machine learning to recognize the assertion of a particular relationship between two entities in texts (Riloff, 1996; Soderland, 1999). Although machine learning helps enumerate potential relation patterns for extraction, this approach is often limited to extracting the relation sets that are predefined. In addition, traditional IE has focused on satisfying pre-specified requests from small homogeneous corpora, leaving the question open whether it can scale up to massive and heterogeneous corpora such as the Web (Banko and Etzioni, 2008; Etzioni et al., 2008, 2011). Open IE, a new domain-independent knowledge discovery paradigm that extracts a diverse set of relations without requiring any relation-specific human inputs and a pre-specified vocabulary, is especially suited to 
massive text corpora, where target relations are unknown in advance. Several Open IE systems, such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), ReVerb (Fader et al., 2011), and OLLIE (Mausam et al., 2012) achieve promising performance in open relation extraction on English sentences. However, application of these systems poses challenges to those languages that are very different from English, such as Chinese, as grammatical functions in English and Chinese are realized in markedly different ways. It is not sure whether those techniques for English still work for Chinese. This issue motivates us to extend the state-of-the-art Open IE systems to extract relations from Chinese texts. The relatively rich morpho-syntactic marking system of English (e.g., verbal inflection, nominal case, clausal markers) makes the syntactic roles of many words detectable from their surface forms. A tensed verb in English, for example, generally indicates its main verb status of a clause. The pinning down of the main verb in a Chinese clause, on the other hand, must rely on other linguistic cues such as word context due to the lack of tense markers. In contrast to the syntax-oriented English language, Chinese is discourse-oriented and rich in ellipsis ? meaning is often construable in the absence of explicit linguistic devices such that many obligatory grammatical categories (e.g., pronouns and BE verbs) can be elided in Chinese.  For example, the three Chinese sentences ???????? (?Apples nutritious?), ????????? ? (?Apples are nutritious?), and ???????? 
12
(?Apples are rich in nutrition?) are semantically synonymous sentences, but the first one, which lacks an overt verb, is used far more often than the other two. Presumably, an adequate multilingual IE system must take into account those intrinsic differences between languages. This paper introduces the Chinese Open Relation Extraction (CORE) system, which utilizes a series of NLP techniques to extract relations embedded in Chinese sentences. Given a Chinese text as the input, CORE employs word segmentation, part-of-speech (POS) tagging, and syntactic parsing, to automatically annotate the Chinese sentences. Based on this rich information, the input sentences are chunked and the entity-relation triples are extracted. Our evaluation shows the effectiveness of CORE, and its deficiency as well. 2 Related Work TextRunner (Banko et al., 2007) was the first Open IE system, which trains a Na?ve Bayes classifier with POS and NP-chunk features to extract relationships between entities. The subsequent work showed that employing the classifiers capable of modeling the sequential information inherited in the texts, like linear-chain CRF (Banko and Etzioni, 2008) and Markov Logic Network (Zhu et al., 2009), can result in better extraction performance. The WOE system (Wu and Weld, 2010) adopted Wikipedia as the training source for their extractor. Experimental results indicated that parsed dependency features lead to further improvements over TextRunner.  ReVerb (Fader et al., 2011) introduced another approach by identifying first a verb-centered relational phrase that satisfies their pre-defined syntactic and lexical constraints, and then split the input sentence into an Argument-Verb-Argument triple. This approach involves only POS tagging for English and ?regular expression?-like matching. As such, it is suitable for large corpora, and likely to be applicable to Chinese.  
For multilingual open IE, Gamallo et al. (2012) adopts a rule-based dependency parser to extract relations represented in English, Spanish, Portuguese, and Galician. For each parsed sentence, they separate each verbal clause and then identify each one?s verb participants, including their functions: subject, direct object, attribute, and prepositional complements. A set of rules is then applied on the clause constituents to extract the target triples. For Chinese open IE, we adopt a similar general approach. The main differences are the processing steps specific to Chinese language. 3 Chinese Open Relation Extraction This section describes the components of CORE. Not requiring any predefined vocabulary, CORE?s sole input is a Chinese corpus and its output is an extracted set of relational tuples. The system consists of three key modules, i.e., word segmentation and POS tagging, syntactic parsing, and entity-relation triple extraction, which are introduced as follows: Chinese is generally written without word boundaries. As a result, prior to the implementation of most NLP tasks, texts must undergo automatic word segmentation. Automatic Chinese word segmenters are generally trained by an input lexicon and probability models. However, it usually suffers from the unknown word (i.e., the out-of-vocabulary, or OOV) problem. In CORE, a corpus-based learning method to merge the unknown words is adopted to tackle the OOV problem (Chen and Ma, 2002). This is followed by a reliable and cost-effective POS-tagging method to label the segmented words with part-of-speeches (Tsai and Chen, 2004). Take the Chinese sentence ?????????? (?Edison invented the light bulb?) for instance. It was segmented and tagged as follows: ???/Nb  ??/VC  ?/Di  ??/Na. Among these words, the translation of a foreign proper name ????? (?Edison?) is not likely to be included in a lexicon and therefore is extracted by the unknown word detection method. In this case, 
13
the special POS tag ?Di? is a tag to represent a verb?s tense when its character ??? follows immediately after its precedent verb. The complete set of part-of-speech tags is defined in the technical report (CKIP, 1993). In the above sentence, ?? ? could represent a complete different meaning if it is associate with other character, such as ???? meaning ?understand?. Therefore, ????????? ? (?Edison invented a cure?) would be segmented incorrectly once ?? ? is associated with its following character, instead of its precedent word. We adopt CKIP, the best-performing parser in the bakeoff of SIGHAN 2012 (Tseng et al., 2012), to do syntactic structure analysis. The CKIP solution re-estimates the context-dependent probability for Chinese parsing and improves the performance of probabilistic context-free grammar (Hsieh et al., 2012). For the example sentence above, ????/Nb? and ??? /Na? were annotated as two nominal phrases (i.e., ?NP?), and ???/VC  ?/Di? was annotated as a verbal phrase (i.e., ?VP?). CKIP parser also adopts dependency decision-making and example-based approaches to label the semantic role ?Head?, showing the status of a word or a phrase as the pivotal constituent of a sentence (You and Chen, 2004). CORE adopts the head-driven principle to identify the main relation in a given sentence (Huang et al., 2000). Firstly, a relation is defined by both the ?Head?-labeled verb and the other words in the syntactic chunk headed by the verb. Secondly, the noun phrases preceding/preceded by the relational chunk are regarded as the candidates of the head?s arguments. Finally, the entity-relation triple is identified in the form of (entity1, relation, entity2). Regarding the example sentence described above, the triple (???/Edison, ???/invented, ??/light bulb) is extracted by this approach. Figure 1 shows the parsed tree of a Chinese sentence for the relation extraction by CORE. The Chinese sentence ???????????
????????? (?Democrats on the House Budget Committee released a report on Monday?) is the manual translation of one of the English sentences evaluated by ReVerb (Fader et al., 2011). The first step of CORE involves word-segmentation and POS-tagging, thus returning eight word/POS pairs: ??/Nc, ??/Na, ???/Nc, ?/DE, ???/Nb, ???/Nd, ??/VE, ?? /Na. Next, ???? /Nd ?? /VE? is identified as the verbal phrase that heads the sentence. This verbal phrase is regarded as the center of a potential relation. The two noun phrases before and after the verbal phrase, i.e., the NP ??? ?? ??? ? ???? and NP ???? are regarded as the entities that complete the relation. A potential entity-relation-entity triple (i.e., ??????????? / ????? / ??, ?Democrats on the House Budget Committee / on Monday released / a report?) is extracted accordingly. This triple is chunked from its original sentence fully automatically. Finally, a filtering process, which retains ?Head?-labeled words only, can be applied to strain out from each component of this triple the most prominent word: ???? / ?? / ??? (?Democrats / released / report?). 
 Figure 1: The parsed tree of a Chinese sentence. 4 Experiments and Evaluation We adopted the same test set released by ReVerb for performance evaluation. The test set consists of 500 English sentences randomly sampled from the Web and were annotated using a pooling method. To obtain ?gold standard? relation triples in Chinese, the 500 test sentences were manually translated from English to Chinese by a 
14
trained native Chinese speaker and verified by another. Additionally, two other native Chinese speakers annotated the relation triples for each Chinese sentence. In total, 716 Chinese entity-relation triples with an agreement score of 0.79 between the two annotators were obtained and regarded as gold standard.  Performance evaluation of CORE was conducted based on: 1) exact match; and 2) relation-only match. For exact match, each component of the extracted triple must be identical with the gold standard. For relation-only match, the extracted triple is regarded as a correct case if an extracted relation agreed with the relation of the gold standard.  Without another Chinese Open IE system for performance comparison, we compared CORE with a modification of ReVerb system capable of handling Chinese sentences. The modification of ReVerb?s verb-driven regular expression matching was kept to a minimum to deal with language-specific processing. As such, ReVerb remains mostly the same as its English counterpart so that a bilingual (Chinese/English) Open IE system can be easily implemented. Table 1 shows the experimental results. Our CORE system obviously performs better than ReVerb when recall is considered for both exact and relation-only match. The results suggest that utilizing more sophisticated NLP techniques is effective to extract relations without any specific human intervention. In addition, there is a slight decrease in the precision of exact match for CORE. This reveals that ReVerb?s original syntactic and lexical constraints are also useful to identify the arguments and their relationship precisely. In summary, CORE achieved relatively promising F1 scores. These results imply that CORE method is more suitable for Chinese open relation extraction. 
Chinese Open IE Precision Recall F1 Exact Match ReVerb 0.5820 0.0987 0.1688 CORE 0.5579 0.3291 0.4140 Relation Only ReVerb 0.8361 0.1425 0.2435 CORE 0.8463 0.5000 0.6286 Table 1: Performance evaluation on Chinese Open IE. 
We also analyzed the errors made by the CORE model. Almost all the errors resulted from incorrect parsing. Enhancing the parsing effectiveness is most likely to improve the performance of CORE. The relatively low recall rate also indicates that CORE misses many types of relation expression. Ellipsis and flexibility in Chinese syntax are so difficult not only to fail the parser, but also the extraction attempts to bypass the parsing errors. To demonstrate the applicability of CORE, we implement a Chinese Question-Answering (QA) system based on two million news articles from 2002 to 2009 published by the United Daily News Group (udn.com/NEWS). CORE extracted more than 13 million unique entity-relation triples from this corpus. These extracted relations are useful for knowledge acquisition. Take the question ????????? ? (?What is originated from China??) as an example, the relation is automatically identified as ?? ? (?originate?) that heads the following entity ??? ? (?China?). Our open QA system then searched the triples and returned the first entity as the answers. In addition to the obvious answer ???? (?Chinese medicine?), which is usually considered as common-sense knowledge, we also obtained those that are less known, such as the traditional Japanese food ???? (?natto?) and the musical instrument ????? (?accordion?). 5 Conclusions This work demonstrates the feasibility of extracting relations from Chinese corpus without the input of any predefined vocabulary to IE systems. This work is the first to explore Chinese open relation extraction to our best knowledge.  Acknowledgments 
This research was partially supported by National Science Council, Taiwan under grant NSC102-2221-E-002-103-MY3, and the ?Aim for the Top University Project? of National Taiwan Normal University, sponsored by the Ministry of Education, Taiwan. 
15
References  Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. Proceedings of EMNLP?11, pages 1535-1545. Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao-Ming Gao, and Kuang-Yu Chen. 2000. Sinina Treebank: design criteria, annotation guidelines, and on-line interface. Proceedings of SIGHAN?00, pages 29-37. Chinese Knowledge Information Processing (CKIP) Group. 1993. Categorical analysis of Chinese. ACLCLP Technical Report # 93-05, Academia Sinica.  Fei Wu and Daniel S. Weld. 2010. Open information extraction using Wikipedia. Proceedings of ACL?10, pages 118-127. Jia-Ming You, and Keh-Jiann Chen. 2004. Automatic semantic role assignment for a tree structure.  In Proceedings of SIGHAN?04, pages 1-8. Jun Zhu, Zaiqing Nie, Xiaojiang Lium Bo Zhang, and Ji-Rong Wen. 2009. StatSnowball: a statistical approach to extracting entity relationships. In Proceedings of WWW?09, pages 101-110. Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word extraction for Chinese documents. In Proceedings of COLING?02, pages 169-175. Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. Proceedings of IJCAI?07, pages 2670-2676. 
Michele Banko, and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. Proceedings of ACL?08, pages 28-26.   Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam. 2011. Open information extraction: the second generation. In Proceedings of IJCAI?11, pages 3-10. Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open information extraction from the web. Communications of the ACM, 51(12):68-74. Pablo Gamallo, Marcos Garcia, and Santiago Fern?ndez-Lanza. 2012. Dependency-based open information extraction. In Proceedings of ROBUS-UNSUP?12, pages 10-18.  Elleen Riloff. 1996. Automatically constructing extraction patterns from untagged text. In Proceedings of AAAI?96, pages 1044-1049.  Stephen Soderland. 1999. Learning information extraction rules for semi-structured and free text.  Machine Learning, 34(1-3):233-272. Yu-Ming Hsieh, Ming-Hong Bai, Jason S. Chang, and Keh-Jiann Chen. 2012. Improving PCFG Chinese Parsing with Context-Dependent Probability Re-estimation. Proceedings of CLP?12, pages 216-221. Yu-Fang Tsai, and Keh-Jiann Chen. 2004. Reliable and cost-effective pos-tagging. International Journal of Computational Linguistics and Chinese Language Processing, 9(1):83-96. Yuen-Hsien Tseng, Lung-Hao Lee, and Liang-Chih Yu 2012. Traditional Chinese parsing evaluation at SIGHAN Bake-offs 2012. Proceedings of CLP?12, pages 199-205.   
16
Proceedings of NAACL-HLT 2013, pages 1163?1173,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Towards Coherent Multi-Document Summarization
Janara Christensen, Mausam, Stephen Soderland, Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{janara,mausam,soderlan,etzioni}@cs.washington.edu
Abstract
This paper presents G-FLOW, a novel system
for coherent extractive multi-document sum-
marization (MDS).1 Where previous work on
MDS considered sentence selection and or-
dering separately, G-FLOW introduces a joint
model for selection and ordering that balances
coherence and salience. G-FLOW?s core rep-
resentation is a graph that approximates the
discourse relations across sentences based on
indicators including discourse cues, deverbal
nouns, co-reference, and more. This graph en-
ables G-FLOW to estimate the coherence of a
candidate summary.
We evaluate G-FLOW on Mechanical Turk,
and find that it generates dramatically bet-
ter summaries than an extractive summarizer
based on a pipeline of state-of-the-art sentence
selection and reordering components, under-
scoring the value of our joint model.
1 Introduction
The goal of multi-document summarization (MDS)
is to produce high quality summaries of collections
of related documents. Most previous work in ex-
tractive MDS has studied the problems of sentence
selection (e.g., (Radev, 2004; Haghighi and Vander-
wende, 2009)) and sentence ordering (e.g., (Lapata,
2003; Barzilay and Lapata, 2008)) separately, but
we believe that a joint model is necessary to produce
coherent summaries. The intuition is simple: if the
sentences in a summary are first selected?without
regard to coherence?then a satisfactory ordering of
the selected sentences may not exist.
1System and data at http://knowitall.cs.washington.edu/gflow/
doc1: Bomb-
ing in
Jerusalem
doc1: Anger
from Israelis
doc1: Suspen-
sion of peace
accord due to
bombing
doc2: Hamas
claims respon-
sibility
doc5: Pales-
tinians con-
demn attack
doc4: Mubarak
urges peace
accord
doc5: Pales-
tinians urge
peace accord
doc3: Clinton
urges peace
accord
Figure 1: An example of a discourse graph covering a
bombing and its aftermath, indicating the source docu-
ment for each node. A coherent summary should begin
with the bombing and then describe the reactions. Sen-
tences are abbreviated for compactness.
An extractive summary is a subset of the sen-
tences in the input documents, ordered in some
way.2 Of course, most possible summaries are in-
coherent. Now, consider a directed graph where the
nodes are sentences in the collection, and each edge
represents a pairwise ordering constraint necessary
for a coherent summary (see Figure 1 for a sample
graph). By definition, any coherent summary must
obey the constraints in this graph.
Previous work has constructed similar graphs au-
tomatically for single document summarization and
manually for MDS (see Section 2). Our system,
G-FLOW extends this research in two important
ways. First, it tackles automatic graph construction
for MDS, which requires novel methods for identi-
fying inter-document edges (Section 3). It uses this
2We focus exclusively on extractive summaries, so we drop
the word ?extractive? henceforth.
1163
State-of-the-art MDS system G-FLOW
? The attack took place Tuesday near Cailaco in East Timor, a
former Portuguese colony, according to a statement issued by the
pro-independence Christian Democratic Union of East Timor.
? The United Nations does not recognize Indonesian claims to East
Timor.
? In a decision welcomed as a landmark by Portugal, European Union
leaders Saturday backed calls for a referendum to decide the fate of East
Timor, the former Portuguese colony occupied by Indonesia since 1975.
? Indonesia invaded East Timor in 1975 and annexed it the following
year.
? Bhichai Rattakul, deputy prime minister and president of the
Bangkok Asian Games Organizing Committee, asked the Foreign
Ministry to urge the Saudi government to reconsider withdrawing
its 105-strong team.
? The games will be a success.
? Thailand won host rights for the quadrennial games in 1995, but
setbacks in preparations led officials of the Olympic Council of Asia late
last year to threaten to move the games to another country.
? Thailand showed its nearly complete facilities for the Asian Games to
a tough jury Thursday - the heads of the organizing committees from the
43 nations competing in the December event.
Table 1: Pairs of sentences produced by a pipeline of a state-of-the-art sentence extractor (Lin and Bilmes, 2011) and
sentence orderer (Li et al, 2011a), and by G-FLOW.
graph to estimate coherence of a candidate summary.
Second, G-FLOW introduces a novel methodology
for joint sentence selection and ordering (Section 4).
It casts MDS as a constraint optimization problem
where salience and coherence are soft constraints,
and redundancy and summary length are hard con-
straints. Because this optimization problem is NP-
hard, G-FLOW uses local search to approximate it.
We report on a Mechanical Turk evaluation that
directly compares G-FLOW to state-of-the-art MDS
systems. Using DUC?04 as our test set, we com-
pare G-FLOW against a combination of an extractive
summarization system with state-of-the-art ROUGE
scores (Lin and Bilmes, 2011) followed by a state-
of-the-art sentence reordering scheme (Li et al,
2011a). We also compare G-FLOW to a combina-
tion of an extractive system with state-of-the-art co-
herence scores (Nobata and Sekine, 2004) followed
by the reordering system. In both cases participants
substantially preferred G-FLOW. Participants chose
G-FLOW 54% of the time when compared to Lin,
and chose Lin?s system 22% of the time. When com-
pared to Nobata, participants chose G-FLOW 60%
of the time, and chose Nobata only 20% of the time.
The remainder of the cases were judged equivalent.
A further analysis shows that G-FLOW?s sum-
maries are judged superior along several dimensions
suggested in the DUC?04 evaluation (including co-
herence, repetitive text, and referents). A compar-
ison against manually written, gold standard sum-
maries, reveals that while the gold standard sum-
maries are preferred in direct comparisons, G-FLOW
has nearly equivalent scores on almost all dimen-
sions suggested in the DUC?04 evaluation.
The paper makes the following contributions:
? We present G-FLOW, a novel MDS system that
jointly solves the sentence selection and order-
ing problems to produce coherent summaries.
? G-FLOW automatically constructs a domain-
independent graph of ordering constraints over
sentences in a document collection, based on
syntactic cues and redundancy across docu-
ments. This graph is the backbone for estimat-
ing the coherence of a summary.
? We perform human evaluation on blind test
sets and find that G-FLOW dramatically outper-
forms state-of-the-art MDS systems.
2 Related Work
Most existing research in multi-document summa-
rization (MDS) focuses on sentence selection for in-
creasing coverage and does not consider coherence
of the summary (Section 2.1). Although coherence
has been used in ordering of summary sentences
(Section 2.2), this work is limited by the quality of
summary sentences given as input. In contrast, G-
FLOW incorporates coherence in both selection and
ordering of summary sentences.
G-FLOW can be seen as an instance of discourse-
driven summarization (Section 2.3). There is prior
work in this area, but primarily for summarization of
single documents. There is some preliminary work
on the use of manually-created discourse models in
MDS. Our approach is fully automated.
2.1 Subset Selection in MDS
Most extractive summarization research aims to in-
crease the coverage of concepts and entities while
reducing redundancy. Approaches include the use of
maximum marginal relevance (Carbonell and Gold-
stein, 1998), centroid-based summarization (Sag-
gion and Gaizauskas, 2004; Radev et al, 2004), cov-
1164
ering weighted scores of concepts (Takamura and
Okumura, 2009; Qazvinian et al, 2010), formula-
tion as minimum dominating set problem (Shen and
Li, 2010), and use of submodularity in sentence se-
lection (Lin and Bilmes, 2011). Graph centrality has
also been used to estimate the salience of a sentence
(Erkan and Radev, 2004). Approaches to content
analysis include generative topic models (Haghighi
and Vanderwende, 2009; Celikyilmaz and Hakkani-
Tur, 2010; Li et al, 2011b), and discriminative mod-
els (Aker et al, 2010).
These approaches do not consider coherence as
one of the desiderata in sentence selection. More-
over, they do not attempt to organize the selected
sentences into an intelligible summary. They are
often evaluted by ROUGE (Lin, 2004), which is
coherence-insensitive. In practice, these approaches
often result in incoherent summaries.
2.2 Sentence Reordering
A parallel thread of research has investigated taking
a set of summary sentences as input and reordering
them to make the summary fluent. Various algo-
rithms use some combination of topic-relatedness,
chronology, precedence, succession, and entity co-
herence for reordering sentences (Barzilay et al,
2001; Okazaki et al, 2004; Barzilay and Lapata,
2008; Bollegala et al, 2010). Recent work has also
used event-based models (Zhang et al, 2010) and
context analysis (Li et al, 2011a).
The hypothesis in this research is that a pipelined
combination of subset selection and reordering will
produce high-quality summaries. Unfortunately,
this is not true in practice, because sentences are se-
lected primarily for coverage without regard to co-
herence. This methodology often leads to an inad-
vertent selection of a set of disconnected sentences,
which cannot be put together in a coherent sum-
mary, irrespective of how the succeeding algorithm
reorders them. In our evaluation, reordering had lim-
ited impact on the quality of the summaries.
2.3 Coherence Models and Summarization
Research on discourse analysis of documents pro-
vides a basis for modeling coherence in a docu-
ment. Several theories have been developed for
modeling discourse, e.g., Centering Theory, Rhetor-
ical Structure Theory (RST), Penn Discourse Tree-
Bank (Grosz and Sidner, 1986; Mann and Thomp-
son, 1988; Wolf and Gibson, 2005; Prasad et al,
2008). Numerous discourse-guided summariza-
tion algorithms have been developed (Marcu, 1997;
Mani, 2001; Taboada and Mann, 2006; Barzilay and
Elhadad, 1997; Louis et al, 2010). However, these
approaches have been applied to single document
summarization and not to MDS.
Discourse models have seen some application to
summary generation in MDS, for example, using a
detailed semantic representation of the source texts
(McKeown and Radev, 1995; Radev and McKe-
own, 1998). A multi-document extension of RST
is Cross-document Structure Theory (CST), which
has been applied to MDS (Zhang et al, 2002; Jorge
and Pardo, 2010). However, these systems require
a stronger input, such as a manual CST-annotation
of the set of documents. Our work can be seen as
an instance of summarization based on lightweight
CST. However, a key difference is that our proposed
algorithm is completely automated and does not re-
quire any additional human annotation. Addition-
ally, while incorporating coherence into selection,
this work does not attempt to order the sentences
coherently, while our approach performs joint selec-
tion and ordering.
Discourse models have also been used for evalu-
ating summary quality (Barzilay and Lapata, 2008;
Louis and Nenkova, 2009; Pitler et al, 2010). Fi-
nally, there is work on generating coherent sum-
maries in specific domains, such as scientific articles
(Saggion and Lapalme, 2002; Abu-Jbara and Radev,
2011) using domain-specific cues like citations. In
contrast, our work generates summaries without any
domain-specific knowledge. Other research has fo-
cused on identifying coherent threads of documents
rather than sentences (Shahaf and Guestrin, 2010).
3 Discourse Graph
As described in Section 1, our goal is to identify
pairwise ordering constraints over a set of input sen-
tences. These constraints specify a multi-document
discourse graph, which is used by G-FLOW to eval-
uate the coherence of a candidate summary.
In this graph G, each vertex is a sentence and an
edge from si to sj indicates that sj can be placed
right after si in a coherent summary. In other words,
the two share a discourse relationship. In the fol-
1165
lowing three sentences (from possibly different doc-
uments) there should be an edge from s1 to s2, but
not between s3 and the other sentences:
s1 Militants attacked a market in Jerusalem.
s2 Arafat condemned the bombing.
s3 The Wye River Accord was signed in Oct.
Discourse theories have proposed a variety of re-
lationships between sentences such as background
and interpretation. RST has 17 such relations (Mann
and Thompson, 1988) and PDTB has 16 (Prasad et
al., 2008). While we seek to identify pairs of sen-
tences that have a relationship, we do not attempt to
label the edges with the exact relation.
We use textual cues from the discourse literature
in combination with the redundancy inherent in re-
lated documents to generate edges. Because this
methodology is noisy, the graph used by G-FLOW is
an approximation, which we refer to as an approx-
imate discourse graph (ADG). We first describe the
construction of this graph, and then discuss the use
of the graph for summary generation (Section 4).
3.1 Deverbal Noun Reference
Often, the main description of an event is mentioned
in a verbal phrase and subsequent references use
deverbal nouns (nominalization of verbs) (e.g., ?at-
tacked? and ?the attack?). In this example, the noun
is derivationally related to the verb, but that is not al-
ways the case. For example, ?bombing? in s2 above
refers to ?attacked? in s1.
We identify verb-noun pairs with this relationship
as follows. First, we locate a set of candidate pairs
from WordNet: for each verb v, we determine po-
tential noun references n using a path length of up to
two in WordNet (moving from verb to noun is pos-
sible via WordNet?s ?derivationally related? links).
This set captures verb-noun pairs such as (?to at-
tack?, ?bombing?), but also includes generic pairs
such as (?to act?, ?attack?). To filter such errors
we score the candidate references. Our goal is to
emphasize common pairs and to deemphasize pairs
with common verbs or verbs that map to many
nouns. To this end, we score pairs by (c/p) ? (c/q),
where c is the number of times the pair (v, n) ap-
pears in adjacent sentences, p is the number of times
the verb appears, and q is the number of times that
v appears with a different noun. We generate these
statistics over a background corpus of 60,000 arti-
cles from the New York Times and Reuters, and
filter out candidate pairs scoring below a threshold
identified over a small training set.
We construct edges in the ADG between pairs of
sentences containing these verb to noun mappings.
To our knowledge, we are the first to use deverbal
nouns for summarization.
3.2 Event/Entity Continuation
Our second indicator is related to lexical chains
(Barzilay and Lapata, 2008). We add an edge in
the ADG from a sentence si to sj if they contain
the same event or entity and the timestamp of si is
less than or equal to the timestamp of sj (timestamps
generated with (Chang and Manning, 2012)).
3.3 Discourse Markers
We use 36 explicit discourse markers (e.g., ?but?,
?however?, ?moreover?) to identify edges between
two adjacent sentences of a document (Marcu and
Echihabi, 2002). This indicator lets us learn an edge
from s4 to s5 below:
s4 Arafat condemned the bombing.
s5 However, Netanyahu suspended peace talks.
3.4 Inferred Edges
We exploit the redundancy of information in MDS
documents to infer edges to related sentences. An
edge (s, s??) can be inferred if there is an existing
edge (s, s?) and s? and s?? express similar informa-
tion. As an example, the edge (s6, s7) can be in-
ferred based on edge (s4, s5):
s6 Arafat condemned the attack.
s7 Netanyahu has suspended the talks.
To infer edges we need an algorithm to identify
sentences expressing similar information. To iden-
tify these pairs, we extract Open Information Extrac-
tion (Banko et al, 2007) relational tuples for each
sentence, and we mark any pair of sentences with
an equivalent relational tuple as redundant (see Sec-
tion 4.3). The inferred edges allow us to propagate
within-document discourse information to sentences
from other documents.
3.5 Co-referent Mentions
A sentence sj will not be clearly understood in iso-
lation and may need another sentence si in its con-
text, if sj has a general reference (e.g., ?the presi-
1166
dent?) pointing to a specific entity or event in si (e.g.,
?President Bill Clinton?). We construct edges based
on coreference mentions, as predicted by Stanford?s
coreference system (Lee et al, 2011). We are able
to identify syntactic edge (s8, s9):
s8 Pres. Clinton expressed sympathy for Israel.
s9 He said the attack should not derail the deal.
3.6 Edge Weights
We weight each edge in the ADG by adding the
number of distinct indicators used to construct that
edge ? if sentences s and s? have an edge because
of a discourse marker and a deverbal reference, the
edge weight wG(s, s?) will be two. We also include
negative edges in the ADG. wG(s, s?) is negative if
s? contains a deverbal noun reference, a discourse
marker, or a co-reference mention that is not fulfilled
by s. For example, if s? contains a discourse marker,
and s is neither the sentence directly preceding s?
and there is no inferred discourse link between s and
s?, then we will add a negative edge wG(s, s?).
3.7 Preliminary Graph Evaluation
We evaluated the quality of the ADG used by G-
FLOW, which is important not only for its use in
MDS, but also because the ADG may be used for
other applications like topic tracking and decompos-
ing an event into sub-events. One author randomly
chose 750 edges and labeled an edge correct if the
pair of sentences did have a discourse relationship
between them and incorrect otherwise. 62% of the
edges accurately reflected a discourse relationship.
Our ADG has on average 31 edges per sentence for
a dataset in which each document cluster has on av-
erage 253 sentences. This evaluation includes only
the positive edges.
4 Summary Generation
We denote a candidate summary X to be a sequence
of sentences ?x1, x2, . . . , x|X|?. G-FLOW?s summa-
rization algorithm searches through the space of or-
dered summaries and scores each candidate sum-
mary along the dimensions of coherence (Section
4.1), salience (Section 4.2) and redundancy (Section
4.3). G-FLOW returns the summary that maximizes
a joint objective function (Section 4.4).
weight feature
-0.037 position in document
0.033 from first three sentences
-0.035 number of people mentions
0.111 contains money
0.038 sentence length > 20
0.137 length of sentence
0.109 #sentences verbs appear in (any form)
0.349 #sentences common nouns appear in
0.355 #sentences proper nouns appear in
Table 2: Linear regression features for salience.
4.1 Coherence
G-FLOW estimates coherence of a candidate sum-
mary via the ADG. We define coherence as the sum
of edge weights between successive summary sen-
tences. For disconnected sentence pairs, the edge
weight is zero.
Coh(X) =
?
i=1..|X|?1
wG+(xi, xi+1) + ?wG?(xi, xi+1)
wG+ represents positive edges and wG? represents
negative edge weights. ? is a tradeoff coefficient for
positive and negative weights, which is tuned using
the methodology described in Section 4.4.
4.2 Salience
Salience is the inherent value of each sentence to
the documents. We compute salience of a summary
(Sal(X)) as the sum of the saliences of individual
sentences (
?
i Sal(xi)).
To estimate salience of a sentence, G-FLOW uses
a linear regression classifier trained on ROUGE
scores over the DUC?03 dataset. The classifier uses
surface features designed to identify sentences that
cover important concepts. The complete list of fea-
tures and learned weights is in Table 2. The clas-
sifier finds a sentence more salient if it mentions
nouns or verbs that are present in more sentences
across the documents. The highest ranked features
are the last three ? number of other sentences that
mention a noun or a verb in the given sentence. We
use the same procedure as in deverbal nouns for de-
tecting verb mentions that appear as nouns in other
sentences (Section 3.1).
4.3 Redundancy
We also wish to avoid redundancy. G-FLOW first
processes each sentence with a state-of-the-art Open
Information extractor OLLIE (Mausam et al, 2012),
which converts a sentence into its component re-
lational tuples of the form (arg1, relational phrase,
1167
arg2).3 For example, it finds (Militants, bombed, a
marketplace) as a tuple from sentence s12.
Two sentences will express redundant information
if they both contain the same or synonymous com-
ponent fact(s). Unfortunately, detecting synonymy
even at relational tuple level is very hard. G-FLOW
approximates this synonymy by considering two re-
lational tuples synonymous if the relation phrases
contain verbs that are synonyms of each other, have
at least one synonymous argument, and are times-
tamped within a day of each other. Because the in-
put documents cover related events, these relatively
weak rules provide good performance. The same
algorithm is used for inferring edges for the ADG
(Section 3.4). This algorithm can detect that the fol-
lowing sentences express redundant information:
s12 Militants bombed a marketplace in Jerusalem.
s13 He alerted Arafat after assailants attacked the
busy streets of Mahane Yehuda.
4.4 Objective Function
The objective function needs to balance coherence,
salience and redundancy and also honor the given
budget, i.e., maximum summary lengthB. G-FLOW
treats redundancy and budget as hard constraints and
coherence and salience as soft. Coherence is neces-
sarily soft as the graph is approximate. While previ-
ous MDS systems specifically maximized coverage,
in preliminary experiments on a development set, we
found that adding a coverage term did not improve
G-FLOW?s performance. We optimize:
maximize: F (x) , Sal(X) + ?Coh(X)? ?|X|
s.t.
?
i=1..|X| len(xi) < B
?xi, xj ? X : redundant(xi, xj) = 0
Here len refers to the sentence length. We add |X|
term (the number of sentences in the summary) to
avoid picking many short sentences, which may in-
crease coherence and salience scores at the cost of
overall summary quality.
The parameters ?, ? and ? (see Section 4.1) are
tuned automatically using a grid search over a de-
velopment set as follows. We manually generate ex-
tractive summaries for each document cluster in our
development set (DUC?03) and choose the parame-
ter setting that minimizes |F (XG-FLOW) ? F (X?)|
3Available from http://ollie.cs.washington.edu
summed over all document clusters. F is the objec-
tive function, XG-FLOW is the summary produced by
G-FLOW and X? is the manual summary.
This constraint optimization problem is NP hard,
which can be shown by using a reduction of the
longest path problem. For this reason, G-FLOW uses
local search to reach an approximation of the opti-
mum. G-FLOW employs stochastic hill climbing
with random restarts as the base search algorithm.
At each step, the search either adds a sentence, re-
moves a sentence, replaces a sentence by another, or
reorders a pair of sentences. The initial summary for
random restarts is constructed as follows. We first
pick the highest salience sentence with no incoming
negative edges as the first sentence. The following
sentences are probabilistically added one at a time
based on the summary score up to that sentence. The
initial summary is complete when there are no possi-
ble sentences left to fit within the budget. Intuitively,
this heuristic chooses a good starting point by se-
lecting a first sentence that does not rely on context
and subsequent sentences that build a high scoring
summary. As with all local search algorithms, this
algorithm is highly scalable and can easily apply to
large collections of related documents, but does not
guarantee global optima.
5 Experiments
Because summaries are intended for human con-
sumption we focused on human evaluations. We
hired workers on Amazon Mechanical Turk (AMT)
to evaluate the summaries. Our evaluation addresses
the following questions: (1) how do G-FLOW sum-
maries compare against the state-of-the-art in MDS
(Section 5.2)? (2) what is G-FLOW?s performance
along important summarization dimensions such as
coherence and redundancy (Section 5.3)? (3) how
does G-FLOW perform on coverage as measured
by ROUGE (Section 5.3.1)? (4) how much do the
components of G-FLOW?s objective function con-
tribute to performance (Section 5.4)? (5) how do G-
FLOW?s summaries compare to human summaries?
5.1 Data and Systems
We evaluated the systems on the Task 2 DUC?04
multi-document summarization dataset. This dataset
consists of 50 clusters of related documents, each of
which contains 10 documents. Each cluster of doc-
1168
uments also includes four gold standard summaries
used for evaluation. As in the DUC?04 competition,
we allowed 665 bytes for each summary including
spaces and punctuation. We used DUC?03 as our
development set, which contains 30 document clus-
ters, again with approximately 10 documents each.
We compared G-FLOW against four systems. The
first is a recent MDS extractive summarizer, which
we choose for its state-of-the-art ROUGE scores
(Lin and Bilmes, 2011).4 The second is a pipeline
of Lin?s system followed by a reimplementation of
a state-of-the-art sentence reordering system (Li et
al., 2011a). We refer to these systems as LIN and
LIN-LI, respectively. This second baseline allows
us to quantify the advantage of using coherence as a
factor in both sentence extraction and ordering.
We also compare against the system that had the
highest coherence ratings at DUC?04 (Nobata and
Sekine, 2004), which we refer to as NOBATA. As
this system did not preform sentence ordering on its
output, we also compare against a pipeline of No-
bata?s system and the sentence reordering system.
We refer to this system as NOBATA-LI.
Lastly, to evaluate how well the system performs
against human generated summaries, we compare
against the gold standard summaries provided by
DUC.
5.2 Overall Summary Quality
Following (Haghighi and Vanderwende, 2009) and
(Celikyilmaz and Hakkani-Tur, 2010), to compare
overall summary quality, we asked AMT workers
to compare two candidate system summaries. The
workers first read a gold standard summary, fol-
lowed by the two system summaries, and were then
asked to choose the better summary from the pair.
The system summaries were shown in a random or-
der to remove any bias.
To ensure that workers provided high quality data
we added two quality checks. First, we restricted
to workers who have an overall approval rating of
over 95% on AMT. Second, we asked the workers
to briefly describe the main events of the summary.
We manually filtered out work where this descrip-
tion was incorrect.
4We thank Lin and Bilmes for providing us with their code.
Unfortunately, we were unable to obtain other recent MDS sys-
tems from their authors.
Six workers compared each pair of summaries.
We recorded the scores for each cluster, and report
three numbers: the percentages of clusters where a
system is more often preferred over the other and the
percentage where the two systems are tied. G-FLOW
is preferred almost three times as often as LIN:
G-FLOW Indifferent LIN
56% 24% 20%
Next, we compared G-FLOW and LIN-LI. Sen-
tence reordering improves performance, but G-
FLOW is still overwhelmingly preferred:
G-FLOW Indifferent LIN-LI
54% 24% 22%
These results suggest that incorporating coher-
ence in sentence extraction adds significant value to
a summarization system. In these experiments, LIN
and LIN-LI are preferred in some cases. We an-
alyzed those summaries more carefully, and found
that occasionally, G-FLOW will sacrifice a small
amount of coverage for coherence, resulting in lower
performance in those cases (see Section 5.3.1).
We also compared LIN and LIN-LI, and found
that reordering does not improve performance by
much.
LIN-LI Indifferent LIN
32% 38% 30%
While the scores presented above represent com-
parisons between G-FLOW and a summarization
system with state-of-the-art ROUGE scores, we
also compared against a summarization system with
state-of-the-art coherence scores ? the system with
the highest coherence scores from DUC?04, (No-
bata and Sekine, 2004). We found that G-FLOW was
again preferred:
G-FLOW Indifferent NOBATA
68% 10% 22%
Adding in sentence ordering again improved the
scores for the comparison system somewhat:
G-FLOW Indifferent NOBATA-LI
60% 20% 20%
While these scores show a significant improve-
ment over previous sytems, they do not convey how
well G-FLOW compares to the gold standard ? man-
ually generated summaries. As a final experiment,
we compared G-FLOW and a second, manually gen-
erated summary:
1169
G-FLOW Indifferent Gold
14% 18% 68%
While we were pleased that in 32% of the cases,
Turkers either preferred G-FLOW or were indiffer-
ent, there is clearly a lot of room for improvement
despite the gains reported over previous sytems.
5.3 Comparison along Summary Dimensions
A high quality summary needs to be good along sev-
eral dimensions. We asked AMT workers to rate
summaries using the quality questions enumerated
in DUC?04 evaluation scheme.5 These questions
concern: (1) coherence, (2) useless, confusing, or
repetitive text, (3) redundancy, (4) nouns, pronouns,
and personal names that are not well-specified (5)
entities rementioned in an overly explicit way, (6)
ungrammatical sentences, and (7) formatting errors.
We evaluated G-FLOW LIN-LI and NOBATA-LI
against the gold standard summaries, using the same
AMT scheme as in the previous section. To assess
automated performance with respect to the standards
set by human summaries, we also evaluated a (dif-
ferent) gold standard summary for each document
cluster, using the same Mechanical Turk scheme as
in the previous section. The 50 summaries produced
by each system were evaluated by four workers. The
results are shown in Figure 2.
G-FLOW was rated significantly better than LIN-
LI in all categories except ?Redundancy? and signif-
icant better than NOBATA-LI on ?Coherence? and
?Referents?. The ratings for ?Coherence?, ?Refer-
ents?, and ?OverlyExplicit? are not surprising given
G-FLOW?s focus on coherence. The results for
?UselessText? may also be due to G-FLOW?s focus
on coherence which ideally prevents it from getting
off topic. Lastly, G-FLOW may perform better on
?Grammatical? and ?Formatting? because it tends to
choose longer sentences than other systems, which
are less likely to be sentence segmentation errors.
There may also be some bleeding from one dimen-
sion to the other ? if a worker likes one summary she
may score it highly for many dimensions.
Finally, somewhat surprisingly, we find G-
FLOW?s performance to be nearly that of human
summaries. G-FLOW is rated statistically signifi-
cantly lower than the gold summaries on only ?Re-
5http://duc.nist.gov/duc2004/quality.questions.txt
System R F
NOBATA 30.44 34.36
Best system in DUC-04 38.28 37.94
Takamura and Okumura (2009) 38.50 -
LIN 39.35 38.90
G-FLOW 37.33 37.43
Gold Standard Summaries 40.03 40.03
Table 3: ROUGE-1 recall and F-measure results (%) on
DUC-04. Some values are missing because not all sys-
tems reported both F-measure and recall.
dundancy?. Given the results from the previous sec-
tion, G-FLOW is likely performing worse on cate-
gories not conveyed in these scores, such as Cover-
age, which we examine next.
5.3.1 Coverage Evaluation using ROUGE
Most recent research has focused on the ROUGE
evaluation, and thus implicitly on coverage of in-
formation in a summary. To estimate the coverage
of G-FLOW, we compared the systems on ROUGE
(Lin, 2004). We calculated ROUGE-1 scores for
G-FLOW, LIN, and NOBATA.6 As sentence order-
ing does not matter for ROUGE, we do not include
LIN-LI or NOBATA-LI in this evaluation. Because
our algorithm does not explicitly maximize coverage
while LIN does, we expected G-FLOW to perform
slightly worse than LIN.
The ROUGE-1 scores for G-FLOW, LIN, NO-
BATA and other recent MDS systems are listed in Ta-
ble 3. We also include the ROUGE-1 scores for the
gold summaries (compared to the other gold sum-
maries). G-FLOW has slightly lower scores than
LIN and the gold standard summaries, but much
higher scores than NOBATA. G-FLOW only scores
significantly lower than LIN and the gold standard
summaries.
We can conclude that good summaries have both
the characteristics listed in the quality dimensions,
and good coverage. The gold standard summaries
outperform G-FLOW on both ROUGE scores and
the quality dimension scores, and therefore, out-
perform G-FLOW on overall comparison. How-
ever, G-FLOW is preferred to LIN-LI in addition to
NOBATA-LI indicating that its quality scores out-
weigh its ROUGE scores in that comparison. An
improvement to G-FLOW may focus on increasing
6ROUGE version 1.5.5 with options: -a -c 95 -b 665 -m -n
4 -w 1.2
1170
Coherence UselessText Redundancy Referents OverlyExplicit Grammatical Formatting
Rat
ing
0
1
2
3
4
GoldG?FlowNobata?LiLin?Li
Figure 2: Ratings for the systems. 0 is the lowest possible score and 4 is the highest possible score. G-FLOW is rated
significantly higher than LIN-LI on all categories, except for ?Redundancy?, and significantly higher than NOBATA-LI
on ?Coherence? and ?Referents?. G-FLOW is only significantly lower than the gold standard on ?Redundancy?.
coverage while retaining strengths such as coher-
ence.
5.4 Ablation Experiments
In this ablation study, we evaluated the contribution
of the main components of G-FLOW ? coherence
and salience. The details of the experiments are the
same as in the experiment described in Section 5.2.
We first measured the importance of coherence in
summary generation. This system G-FLOW-SAL is
identical to the full system except that it does not
include the coherence term in the objective function
(see Section 4.4). The results show that coherence is
very important to G-FLOW?s performance:
G-FLOW Indifferent G-FLOW-SAL
54% 26% 20%
Similarly, we evaluated the contribution of
salience. This system G-FLOW-COH does not in-
clude the salience term in the objective function:
G-FLOW Indifferent G-FLOW-COH
60% 20% 20%
Without salience, the system produces readable,
but highly irrelevant summaries.
5.5 Agreement of Expert & AMT Workers
Because summary evaluation is a relatively complex
task, we compared AMT workers? annotations with
expert annotations from DUC?04. We randomly
selected ten summaries from each of the seven
DUC?04 annotators, and asked four Turk workers
to annotate them on the DUC?04 quality questions.
For each DUC?04 annotator, we selected all pairs
of summaries where one summary was judged more
than one point better than the other summary. We
compared whether the workers (voting as in Sec-
tion 5.2) likewise judged that summary better than
the second summary. We found that the annotations
agreed in 75% of cases. When we looked only at
pairs more than two points different, the agreement
was 80%. Thus, given the subjective nature of the
task, we feel reasonably confident that the AMT an-
notations are informative, and that the dramatic pref-
erence of G-FLOW over the baseline systems is due
to a substantial improvement in its summaries.
6 Conclusion
In this paper, we present G-FLOW, a multi-
document summarization system aimed at generat-
ing coherent summaries. While previous MDS sys-
tems have focused primarily on salience and cov-
erage but not coherence, G-FLOW generates an or-
dered summary by jointly optimizing coherence and
salience. G-FLOW estimates coherence by using
an approximate discourse graph, where each node
is a sentence from the input documents and each
edge represents a discourse relationship between
two sentences. Manual evaluations demonstrate that
G-FLOW generates substantially better summaries
than a pipeline of state-of-the-art sentence selec-
tion and reordering components. ROUGE scores,
which measure summary coverage, show that G-
FLOW sacrifices a small amount of coverage for
overall readability and coherence. Comparisons to
gold standard summaries show that G-FLOW must
improve in coverage to equal the quality of manu-
ally written summaries. We believe this research has
applications to other areas of summarization such as
update summarization and query based summariza-
tion, and we are interested in investigating these top-
ics in future work.
1171
Acknowledgements
We thank Luke Zettlemoyer, Lucy Vanderwende, Hal
Daume III, Pushpak Bhattacharyya, Chris Quirk, Erik
Frey, Tony Fader, Michael Schmitz, Alan Ritter, Melissa
Winstanley, and the three anonymous reviewers for help-
ful conversations and feedback on earlier drafts. We also
thank Lin and Bilmes for providing us with the code for
their system. This research was supported in part by NSF
grant IIS-0803481, ONR grant N00014-11-1-0294, and
DARPA contract FA8750-13-2-0019, and carried out at
the University of Washington?s Turing Center. This pa-
per was also supported in part by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via Air Force
Research Laboratory (AFRL) contract number FA8650-
10-C-7058. The U.S. Government is authorized to repro-
duce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon. The
views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily rep-
resenting the official policies or endorsements, either ex-
pressed or implied, of IARPA, AFRL, or the U.S. Gov-
ernment.
References
Amjad Abu-Jbara and Dragomir R. Radev. 2011. Coher-
ent citation-based summarization of scientific papers.
In Proceedings of ACL 2011, pages 500?509.
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010.
Multi-document summarization using A * search and
discriminative training. In Proceedings of EMNLP
2010.
Michele Banko, Michael Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI 2007, pages 68?74.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10?17.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Regina Barzilay, Noemie Elhadad, and Kathleen R McK-
eown. 2001. Sentence ordering in multidocument
summarization. In Proceedings of HLT 2001, pages
1?7.
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2010. A bottom-up approach to sentence
ordering for multi-document summarization. Informa-
tion Process Management, 46(1):89?109.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
SIGIR 1998, pages 335?336.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of ACL 2010, pages 815?824.
Angel Chang and Christopher Manning. 2012. SU-
TIME: A library for recognizing and normalizing time
expressions. In Proceedings of LREC 2012.
Gunes Erkan and Dragomir R Radev. 2004. LexRank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research,
22(1):457?479.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. Proceedings of NAACL 2009, pages 362?370.
Maria Lucia Castro Jorge and Thiago Alexan-
dre Salgueiro Pardo. 2010. Multi-Document
Summarization: Content Selection based on CST
Model (Cross-document Structure Theory). Ph.D.
thesis, Nu?cleo Interinstitucional de Lingu???stica
Computacional (NILC).
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL 2003, pages 545?552.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In CoNLL 2011
Shared Task.
Peifeng Li, Guangxi Deng, and Qiaoming Zhu. 2011a.
Using context inference to improve sentence ordering
for multi-document summarization. In Proceedings of
IJCNLP 2011, pages 1055?1061.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011b.
Generating aspect-oriented multi-document summa-
rization with event-aspect model. In Proceedings of
EMNLP 2011, pages 1137?1146.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL 2011, pages 510?520.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81.
Annie Louis and Ani Nenkova. 2009. Automatic sum-
mary evaluation without using human models. In Pro-
ceedings of EMNLP 2009, pages 306?314.
Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify im-
plicit discourse relations. In Proceedings of SIGDIAL
2010, pages 59?62.
1172
Inderjeet Mani. 2001. Automatic Summarization. John
Benjamins Publishing Co, Amsterdam/Philadelphia.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of ACL 2002, pages 368?375.
Daniel Marcu. 1997. From discourse structures to text
summaries. In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization, pages 82?88.
Mausam, Michael Schmitz, Robert Bart, Stephen Soder-
land, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of EMNLP
2012, pages 523?534.
Kathleen McKeown and Dragomir Radev. 1995. Gen-
erating summaries of multiple news articles. In Pro-
ceedings of SIGIR 1995, pages 74?82.
Chikashi Nobata and Satoshi Sekine. 2004. Crl/nyu
summarization system at duc-2004. In Proceedings of
DUC 2004.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka.
2004. Improving chronological sentence ordering by
precedence relation. In Proceedings of COLING 2004,
pages 750?756.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of ACL
2010, pages 544?554.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of LREC 2008.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
O?zgu?r. 2010. Citation summarization through
keyphrase extraction. In Proceedings of COLING
2010, pages 895?903.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):469?500.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40(6):919?938.
Dragomir R. Radev. 2004. LexRank: Graph-based lexi-
cal centrality as salience in text summarization. Jour-
nal of Artificial Intelligence Research, 22(1):457?479.
Horacio Saggion and Robert Gaizauskas. 2004. Multi-
document summarization by cluster/profile relevance
and redundancy removal. In Proceedings of DUC
2004.
Horacio Saggion and Guy Lapalme. 2002. Generating
indicative-informative summaries with sumUM. Com-
putational Linguistics, 28(4):497?526.
Dafna Shahaf and Carlos Guestrin. 2010. Connecting the
dots between news articles. In Proceedings of KDD
2010, pages 623?632.
Chao Shen and Tao Li. 2010. Multi-document summa-
rization via the minimum dominating set. In Proceed-
ings of Coling 2010, pages 984?992.
Maite Taboada and William C. Mann. 2006. Applica-
tions of rhetorical structure theory. Discourse Studies,
8(4):567?588.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proceedings of EACL 2009,
pages 781?789.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31(2):249?288.
Zhu Zhang, Sasha Blair-Goldensohn, and Dragomir R.
Radev. 2002. Towards CST-enhanced summarization.
In Proceedings of AAAI 2002, pages 439?445.
Renxian Zhang, Li Wenjie, and Lu Qin. 2010. Sen-
tence ordering with event-enriched semantics and two-
layered clustering for multi-document news summa-
rization. In Proceedings of COLING 2010, pages
1489?1497.
1173
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424?434,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Latent Dirichlet Allocation method for Selectional Preferences
Alan Ritter, Mausam and Oren Etzioni
Department of Computer Science and Engineering
Box 352350, University of Washington, Seattle, WA 98195, USA
{aritter,mausam,etzioni}@cs.washington.edu
Abstract
The computation of selectional prefer-
ences, the admissible argument values for
a relation, is a well-known NLP task with
broad applicability. We present LDA-SP,
which utilizes LinkLDA (Erosheva et al,
2004) to model selectional preferences.
By simultaneously inferring latent top-
ics and topic distributions over relations,
LDA-SP combines the benefits of pre-
vious approaches: like traditional class-
based approaches, it produces human-
interpretable classes describing each re-
lation?s preferences, but it is competitive
with non-class-based methods in predic-
tive power.
We compare LDA-SP to several state-of-
the-art methods achieving an 85% increase
in recall at 0.9 precision over mutual in-
formation (Erk, 2007). We also eval-
uate LDA-SP?s effectiveness at filtering
improper applications of inference rules,
where we show substantial improvement
over Pantel et al?s system (Pantel et al,
2007).
1 Introduction
Selectional Preferences encode the set of admissi-
ble argument values for a relation. For example,
locations are likely to appear in the second argu-
ment of the relation X is headquartered in Y and
companies or organizations in the first. A large,
high-quality database of preferences has the po-
tential to improve the performance of a wide range
of NLP tasks including semantic role labeling
(Gildea and Jurafsky, 2002), pronoun resolution
(Bergsma et al, 2008), textual inference (Pantel
et al, 2007), word-sense disambiguation (Resnik,
1997), and many more. Therefore, much atten-
tion has been focused on automatically computing
them based on a corpus of relation instances.
Resnik (1996) presented the earliest work in
this area, describing an information-theoretic ap-
proach that inferred selectional preferences based
on the WordNet hypernym hierarchy. Recent work
(Erk, 2007; Bergsma et al, 2008) has moved away
from generalization to known classes, instead
utilizing distributional similarity between nouns
to generalize beyond observed relation-argument
pairs. This avoids problems like WordNet?s poor
coverage of proper nouns and is shown to improve
performance. These methods, however, no longer
produce the generalized class for an argument.
In this paper we describe a novel approach to
computing selectional preferences by making use
of unsupervised topic models. Our approach is
able to combine benefits of both kinds of meth-
ods: it retains the generalization and human-
interpretability of class-based approaches and is
also competitive with the direct methods on pre-
dictive tasks.
Unsupervised topic models, such as latent
Dirichlet alocation (LDA) (Blei et al, 2003) and
its variants are characterized by a set of hidden
topics, which represent the underlying semantic
structure of a document collection. For our prob-
lem these topics offer an intuitive interpretation ?
they represent the (latent) set of classes that store
the preferences for the different relations. Thus,
topic models are a natural fit for modeling our re-
lation data.
In particular, our system, called LDA-SP, uses
LinkLDA (Erosheva et al, 2004), an extension of
LDA that simultaneously models two sets of dis-
tributions for each topic. These two sets represent
the two arguments for the relations. Thus, LDA-SP
is able to capture information about the pairs of
topics that commonly co-occur. This information
is very helpful in guiding inference.
We run LDA-SP to compute preferences on a
massive dataset of binary relations r(a1, a2) ex-
424
tracted from the Web by TEXTRUNNER (Banko
and Etzioni, 2008). Our experiments demon-
strate that LDA-SP significantly outperforms state
of the art approaches obtaining an 85% increase
in recall at precision 0.9 on the standard pseudo-
disambiguation task.
Additionally, because LDA-SP is based on a for-
mal probabilistic model, it has the advantage that
it can naturally be applied in many scenarios. For
example, we can obtain a better understanding of
similar relations (Table 1), filter out incorrect in-
ferences based on querying our model (Section
4.3), as well as produce a repository of class-based
preferences with a little manual effort as demon-
strated in Section 4.4. In all these cases we obtain
high quality results, for example, massively out-
performing Pantel et al?s approach in the textual
inference task.1
2 Previous Work
Previous work on selectional preferences can
be broken into four categories: class-based ap-
proaches (Resnik, 1996; Li and Abe, 1998; Clark
and Weir, 2002; Pantel et al, 2007), similarity
based approaches (Dagan et al, 1999; Erk, 2007),
discriminative (Bergsma et al, 2008), and genera-
tive probabilistic models (Rooth et al, 1999).
Class-based approaches, first proposed by
Resnik (1996), are the most studied of the four.
They make use of a pre-defined set of classes, ei-
ther manually produced (e.g. WordNet), or auto-
matically generated (Pantel, 2003). For each re-
lation, some measure of the overlap between the
classes and observed arguments is used to iden-
tify those that best describe the arguments. These
techniques produce a human-interpretable output,
but often suffer in quality due to an incoherent tax-
onomy, inability to map arguments to a class (poor
lexical coverage), and word sense ambiguity.
Because of these limitations researchers have
investigated non-class based approaches, which
attempt to directly classify a given noun-phrase
as plausible/implausible for a relation. Of these,
the similarity based approaches make use of a dis-
tributional similarity measure between arguments
and evaluate a heuristic scoring function:
Srel(arg)=
?
arg??Seen(rel)
sim(arg, arg?) ? wtrel(arg)
1Our repository of selectional preferences is available
at http://www.cs.washington.edu/research/
ldasp.
Erk (2007) showed the advantages of this ap-
proach over Resnik?s information-theoretic class-
based method on a pseudo-disambiguation evalu-
ation. These methods obtain better lexical cover-
age, but are unable to obtain any abstract represen-
tation of selectional preferences.
Our solution fits into the general category
of generative probabilistic models, which model
each relation/argument combination as being gen-
erated by a latent class variable. These classes
are automatically learned from the data. This re-
tains the class-based flavor of the problem, with-
out the knowledge limitations of the explicit class-
based approaches. Probably the closest to our
work is a model proposed by Rooth et al (1999),
in which each class corresponds to a multinomial
over relations and arguments and EM is used to
learn the parameters of the model. In contrast,
we use a LinkLDA framework in which each re-
lation is associated with a corresponding multi-
nomial distribution over classes, and each argu-
ment is drawn from a class-specific distribution
over words; LinkLDA captures co-occurrence of
classes in the two arguments. Additionally we
perform full Bayesian inference using collapsed
Gibbs sampling, in which parameters are inte-
grated out (Griffiths and Steyvers, 2004).
Recently, Bergsma et. al. (2008) proposed the
first discriminative approach to selectional prefer-
ences. Their insight that pseudo-negative exam-
ples could be used as training data allows the ap-
plication of an SVM classifier, which makes use of
many features in addition to the relation-argument
co-occurrence frequencies used by other meth-
ods. They automatically generated positive and
negative examples by selecting arguments having
high and low mutual information with the rela-
tion. Since it is a discriminative approach it is
amenable to feature engineering, but needs to be
retrained and tuned for each task. On the other
hand, generative models produce complete prob-
ability distributions of the data, and hence can be
integrated with other systems and tasks in a more
principled manner (see Sections 4.2.2 and 4.3.1).
Additionally, unlike LDA-SP Bergsma et al?s sys-
tem doesn?t produce human-interpretable topics.
Finally, we note that LDA-SP and Bergsma?s sys-
tem are potentially complimentary ? the output of
LDA-SP could be used to generate higher-quality
training data for Bergsma, potentially improving
their results.
425
Topic models such as LDA (Blei et al, 2003)
and its variants have recently begun to see use
in many NLP applications such as summarization
(Daume? III and Marcu, 2006), document align-
ment and segmentation (Chen et al, 2009), and
inferring class-attribute hierarchies (Reisinger and
Pasca, 2009). Our particular model, LinkLDA, has
been applied to a few NLP tasks such as simul-
taneously modeling the words appearing in blog
posts and users who will likely respond to them
(Yano et al, 2009), modeling topic-aligned arti-
cles in different languages (Mimno et al, 2009),
and word sense induction (Brody and Lapata,
2009).
Finally, we highlight two systems, developed
independently of our own, which apply LDA-style
models to similar tasks. O? Se?aghdha (2010) pro-
poses a series of LDA-style models for the task
of computing selectional preferences. This work
learns selectional preferences between the fol-
lowing grammatical relations: verb-object, noun-
noun, and adjective-noun. It also focuses on
jointly modeling the generation of both predicate
and argument, and evaluation is performed on a
set of human-plausibility judgments obtaining im-
pressive results against Keller and Lapata?s (2003)
Web hit-count based system. Van Durme and
Gildea (2009) proposed applying LDA to general
knowledge templates extracted using the KNEXT
system (Schubert and Tong, 2003). In contrast,
our work uses LinkLDA and focuses on modeling
multiple arguments of a relation (e.g., the subject
and direct object of a verb).
3 Topic Models for Selectional Prefs.
We present a series of topic models for the task of
computing selectional preferences. These models
vary in the amount of independence they assume
between a1 and a2. At one extreme is Indepen-
dentLDA, a model which assumes that both a1 and
a2 are generated completely independently. On
the other hand, JointLDA, the model at the other
extreme (Figure 1) assumes both arguments of a
specific extraction are generated based on a single
hidden variable z. LinkLDA (Figure 2) lies be-
tween these two extremes, and as demonstrated in
Section 4, it is the best model for our relation data.
We are given a set R of binary relations and a
corpus D = {r(a1, a2)} of extracted instances for
these relations. 2 Our task is to compute, for each
argument ai of each relation r, a set of usual ar-
gument values (noun phrases) that it takes. For
example, for the relation is headquartered in the
first argument set will include companies like Mi-
crosoft, Intel, General Motors and second argu-
ment will favor locations like New York, Califor-
nia, Seattle.
3.1 IndependentLDA
We first describe the straightforward application
of LDA to modeling our corpus of extracted rela-
tions. In this case two separate LDA models are
used to model a1 and a2 independently.
In the generative model for our data, each rela-
tion r has a corresponding multinomial over topics
?r, drawn from a Dirichlet. For each extraction, a
hidden topic z is first picked according to ?r, and
then the observed argument a is chosen according
to the multinomial ?z .
Readers familiar with topic modeling terminol-
ogy can understand our approach as follows: we
treat each relation as a document whose contents
consist of a bags of words corresponding to all the
noun phrases observed as arguments of the rela-
tion in our corpus. Formally, LDA generates each
argument in the corpus of relations as follows:
for each topic t = 1 . . . T do
Generate ?t according to symmetric Dirich-
let distribution Dir(?).
end for
for each relation r = 1 . . . |R| do
Generate ?r according to Dirichlet distribu-
tion Dir(?).
for each tuple i = 1 . . . Nr do
Generate zr,i from Multinomial(?r).
Generate the argument ar,i from multi-
nomial ?zr,i .
end for
end for
One weakness of IndependentLDA is that it
doesn?t jointly model a1 and a2 together. Clearly
this is undesirable, as information about which
topics one of the arguments favors can help inform
the topics chosen for the other. For example, class
pairs such as (team, game), (politician, political is-
sue) form much more plausible selectional prefer-
ences than, say, (team, political issue), (politician,
game).
2We focus on binary relations, though the techniques pre-
sented in the paper are easily extensible to n-ary relations.
426
3.2 JointLDA
As a more tightly coupled alternative, we first
propose JointLDA, whose graphical model is de-
picted in Figure 1. The key difference in JointLDA
(versus LDA) is that instead of one, it maintains
two sets of topics (latent distributions over words)
denoted by ? and ?, one for classes of each ar-
gument. A topic id k represents a pair of topics,
?k and ?k, that co-occur in the arguments of ex-
tracted relations. Common examples include (Per-
son, Location), (Politician, Political issue), etc.
The hidden variable z = k indicates that the noun
phrase for the first argument was drawn from the
multinomial ?k, and that the second argument was
drawn from ?k. The per-relation distribution ?r is
a multinomial over the topic ids and represents the
selectional preferences, both for arg1s and arg2s
of a relation r.
Although JointLDA has many desirable proper-
ties, it has some drawbacks as well. Most notably,
in JointLDA topics correspond to pairs of multi-
nomials (?k, ?k); this leads to a situation in which
multiple redundant distributions are needed to rep-
resent the same underlying semantic class. For
example consider the case where we we need to
represent the following selectional preferences for
our corpus of relations: (person, location), (per-
son, organization), and (person, crime). Because
JointLDA requires a separate pair of multinomials
for each topic, it is forced to use 3 separate multi-
nomials to represent the class person, rather than
learning a single distribution representing person
and choosing 3 different topics for a2. This results
in poor generalization because the data for a single
class is divided into multiple topics.
In order to address this problem while maintain-
ing the sharing of influence between a1 and a2, we
next present LinkLDA, which represents a com-
promise between IndependentLDA and JointLDA.
LinkLDA is more flexible than JointLDA, allow-
ing different topics to be chosen for a1, and a2,
however still models the generation of topics from
the same distribution for a given relation.
3.3 LinkLDA
Figure 2 illustrates the LinkLDA model in the
plate notation, which is analogous to the model
in (Erosheva et al, 2004). In particular note that
each ai is drawn from a different hidden topic zi,
however the zi?s are drawn from the same distri-
bution ?r for a given relation r. To facilitate learn-
?
a
1
a
2
?
|R|
N
?
?
1
?
T
?
2
z
Figure 1: JointLDA
?
z
1
z
2
a
1
a
2
?
|R|
N
?
?
1
?
T
?
2
Figure 2: LinkLDA
ing related topic pairs between arguments we em-
ploy a sparse prior over the per-relation topic dis-
tributions. Because a few topics are likely to be
assigned most of the probability mass for a given
relation it is more likely (although not necessary)
that the same topic number k will be drawn for
both arguments.
When comparing LinkLDA with JointLDA the
better model may not seem immediately clear. On
the one hand, JointLDA jointly models the gen-
eration of both arguments in an extracted tuple.
This allows one argument to help disambiguate
the other in the case of ambiguous relation strings.
LinkLDA, however, is more flexible; rather than
requiring both arguments to be generated from one
of |Z| possible pairs of multinomials (?z, ?z), Lin-
kLDA allows the arguments of a given extraction
to be generated from |Z|2 possible pairs. Thus,
instead of imposing a hard constraint that z1 =
z2 (as in JointLDA), LinkLDA simply assigns a
higher probability to states in which z1 = z2, be-
cause both hidden variables are drawn from the
same (sparse) distribution ?r. LinkLDA can thus
re-use argument classes, choosing different com-
binations of topics for the arguments if it fits the
data better. In Section 4 we show experimentally
that LinkLDA outperforms JointLDA (and Inde-
pendentLDA) by wide margins. We use LDA-SP
to refer to LinkLDA in all the experiments below.
3.4 Inference
For all the models we use collapsed Gibbs sam-
pling for inference in which each of the hid-
den variables (e.g., zr,i,1 and zr,i,2 in LinkLDA)
are sampled sequentially conditioned on a full-
assignment to all others, integrating out the param-
eters (Griffiths and Steyvers, 2004). This produces
robust parameter estimates, as it allows computa-
tion of expectations over the posterior distribution
427
as opposed to estimating maximum likelihood pa-
rameters. In addition, the integration allows the
use of sparse priors, which are typically more ap-
propriate for natural language data. In all exper-
iments we use hyperparameters ? = ?1 = ?2 =
0.1. We generated initial code for our samplers us-
ing the Hierarchical Bayes Compiler (Daume III,
2007).
3.5 Advantages of Topic Models
There are several advantages to using topic mod-
els for our task. First, they naturally model the
class-based nature of selectional preferences, but
don?t take a pre-defined set of classes as input.
Instead, they compute the classes automatically.
This leads to better lexical coverage since the is-
sue of matching a new argument to a known class
is side-stepped. Second, the models naturally han-
dle ambiguous arguments, as they are able to as-
sign different topics to the same phrase in different
contexts. Inference in these models is also scalable
? linear in both the size of the corpus as well as
the number of topics. In addition, there are several
scalability enhancements such as SparseLDA (Yao
et al, 2009), and an approximation of the Gibbs
Sampling procedure can be efficiently parallelized
(Newman et al, 2009). Finally we note that, once
a topic distribution has been learned over a set of
training relations, one can efficiently apply infer-
ence to unseen relations (Yao et al, 2009).
4 Experiments
We perform three main experiments to assess the
quality of the preferences obtained using topic
models. The first is a task-independent evaluation
using a pseudo-disambiguation experiment (Sec-
tion 4.2), which is a standard way to evaluate the
quality of selectional preferences (Rooth et al,
1999; Erk, 2007; Bergsma et al, 2008). We use
this experiment to compare the various topic mod-
els as well as the best model with the known state
of the art approaches to selectional preferences.
Secondly, we show significant improvements to
performance at an end-task of textual inference in
Section 4.3. Finally, we report on the quality of
a large database of Wordnet-based preferences ob-
tained after manually associating our topics with
Wordnet classes (Section 4.4).
4.1 Generalization Corpus
For all experiments we make use of a corpus
of r(a1, a2) tuples, which was automatically ex-
tracted by TEXTRUNNER (Banko and Etzioni,
2008) from 500 million Web pages.
To create a generalization corpus from this
large dataset. We first selected 3,000 relations
from the middle of the tail (we used the 2,000-
5,000 most frequent ones)3 and collected all in-
stances. To reduce sparsity, we discarded all tu-
ples containing an NP that occurred fewer than 50
times in the data. This resulted in a vocabulary of
about 32,000 noun phrases, and a set of about 2.4
million tuples in our generalization corpus.
We inferred topic-argument and relation-topic
multinomials (?, ?, and ?) on the generalization
corpus by taking 5 samples at a lag of 50 after
a burn in of 750 iterations. Using multiple sam-
ples introduces the risk of topic drift due to lack
of identifiability, however we found this to not be
a problem in practice. During development we
found that the topics tend to remain stable across
multiple samples after sufficient burn in, and mul-
tiple samples improved performance. Table 1 lists
sample topics and high ranked words for each (for
both arguments) as well as relations favoring those
topics.
4.2 Task Independent Evaluation
We first compare the three LDA-based approaches
to each other and two state of the art similarity
based systems (Erk, 2007) (using mutual informa-
tion and Jaccard similarity respectively). These
similarity measures were shown to outperform the
generative model of Rooth et al (1999), as well
as class-based methods such as Resnik?s. In this
pseudo-disambiguation experiment an observed
tuple is paired with a pseudo-negative, which
has both arguments randomly generated from the
whole vocabulary (according to the corpus-wide
distribution over arguments). The task is, for each
relation-argument pair, to determine whether it is
observed, or a random distractor.
4.2.1 Test Set
For this experiment we gathered a primary corpus
by first randomly selecting 100 high-frequency re-
lations not in the generalization corpus. For each
relation we collected all tuples containing argu-
ments in the vocabulary. We held out 500 ran-
domly selected tuples as the test set. For each tu-
3Many of the most frequent relations have very weak se-
lectional preferences, and thus provide little signal for infer-
ring meaningful topics. For example, the relations has and is
can take just about any arguments.
428
Topic t Arg1 Relations which assign
highest probability to t
Arg2
18 The residue - The mixture - The reaction
mixture - The solution - the mixture - the re-
action mixture - the residue - The reaction -
the solution - The filtrate - the reaction - The
product - The crude product - The pellet -
The organic layer - Thereto - This solution
- The resulting solution - Next - The organic
phase - The resulting mixture - C. )
was treated with, is
treated with, was
poured into, was
extracted with, was
purified by, was di-
luted with, was filtered
through, is disolved in,
is washed with
EtOAc - CH2Cl2 - H2O - CH.sub.2Cl.sub.2
- H.sub.2O - water - MeOH - NaHCO3 -
Et2O - NHCl - CHCl.sub.3 - NHCl - drop-
wise - CH2Cl.sub.2 - Celite - Et.sub.2O -
Cl.sub.2 - NaOH - AcOEt - CH2C12 - the
mixture - saturated NaHCO3 - SiO2 - H2O
- N hydrochloric acid - NHCl - preparative
HPLC - to0 C
151 the Court - The Court - the Supreme Court
- The Supreme Court - this Court - Court
- The US Supreme Court - the court - This
Court - the US Supreme Court - The court
- Supreme Court - Judge - the Court of Ap-
peals - A federal judge
will hear, ruled in, de-
cides, upholds, struck
down, overturned,
sided with, affirms
the case - the appeal - arguments - a case -
evidence - this case - the decision - the law
- testimony - the State - an interview - an
appeal - cases - the Court - that decision -
Congress - a decision - the complaint - oral
arguments - a law - the statute
211 President Bush - Bush - The President -
Clinton - the President - President Clinton
- President George W. Bush - Mr. Bush -
The Governor - the Governor - Romney -
McCain - The White House - President -
Schwarzenegger - Obama
hailed, vetoed, pro-
moted, will deliver,
favors, denounced,
defended
the bill - a bill - the decision - the war - the
idea - the plan - the move - the legislation -
legislation - the measure - the proposal - the
deal - this bill - a measure - the program -
the law - the resolution - efforts - the agree-
ment - gay marriage - the report - abortion
224 Google - Software - the CPU - Clicking -
Excel - the user - Firefox - System - The
CPU - Internet Explorer - the ability - Pro-
gram - users - Option - SQL Server - Code
- the OS - the BIOS
will display, to store, to
load, processes, cannot
find, invokes, to search
for, to delete
data - files - the data - the file - the URL -
information - the files - images - a URL - the
information - the IP address - the user - text
- the code - a file - the page - IP addresses -
PDF files - messages - pages - an IP address
Table 1: Example argument lists from the inferred topics. For each topic number t we list the most
probable values according to the multinomial distributions for each argument (?t and ?t). The middle
column reports a few relations whose inferred topic distributions ?r assign highest probability to t.
ple r(a1, a2) in the held-out set, we removed all
tuples in the training set containing either of the
rel-arg pairs, i.e., any tuple matching r(a1, ?) or
r(?, a2). Next we used collapsed Gibbs sampling
to infer a distribution over topics, ?r, for each of
the relations in the primary corpus (based solely
on tuples in the training set) using the topics from
the generalization corpus.
For each of the 500 observed tuples in the test-
set we generated a pseudo-negative tuple by ran-
domly sampling two noun phrases from the distri-
bution of NPs in both corpora.
4.2.2 Prediction
Our prediction system needs to determine whether
a specific relation-argument pair is admissible ac-
cording to the selectional preferences or is a ran-
dom distractor (D). Following previous work, we
perform this experiment independently for the two
relation-argument pairs (r, a1) and (r, a2).
We first compute the probability of observing
a1 for first argument of relation r given that it is
not a distractor, P (a1|r,?D), which we approx-
imate by its probability given an estimate of the
parameters inferred by our model, marginalizing
over hidden topics t. The analysis for the second
argument is similar.
P (a1|r,?D) ? PLDA(a1|r) =
TX
t=0
P (a1|t)P (t|r)
=
TX
t=0
?t(a1)?r(t)
A simple application of Bayes Rule gives the
probability that a particular argument is not a
distractor. Here the distractor-related proba-
bilities are independent of r, i.e., P (D|r) =
P (D), P (a1|D, r) = P (a1|D), etc. We estimate
P (a1|D) according to their frequency in the gen-
eralization corpus.
P (?D|r, a1) =
P (?D|r)P (a1|r,?D)
P (a1|r)
?
P (?D)PLDA(a1|r)
P (D)P (a1|D) + P (?D)PLDA(a1|r)
4.2.3 Results
Figure 3 plots the precision-recall curve for the
pseudo-disambiguation experiment comparing the
three different topic models. LDA-SP, which uses
LinkLDA, substantially outperforms both Inde-
pendentLDA and JointLDA.
Next, in figure 4, we compare LDA-SP with
mutual information and Jaccard similarities us-
ing both the generalization and primary corpus for
429
0.0 0.2 0.4 0.6 0.8 1.00
.4
0.6
0.8
1.0
recall
prec
ision
LDA?SPIndependentLDAJointLDA
Figure 3: Comparison of LDA-based approaches
on the pseudo-disambiguation task. LDA-SP (Lin-
kLDA) substantially outperforms the other mod-
els.
0.0 0.2 0.4 0.6 0.8 1.00
.4
0.6
0.8
1.0
recall
prec
ision
LDA?SPJaccardMutual Information
Figure 4: Comparison to similarity-based selec-
tional preference systems. LDA-SP obtains 85%
higher recall at precision 0.9.
computation of similarities. We find LDA-SP sig-
nificantly outperforms these methods. Its edge is
most noticed at high precisions; it obtains 85%
more recall at 0.9 precision compared to mutual
information. Overall LDA-SP obtains an 15% in-
crease in the area under precision-recall curve over
mutual information. All three systems? AUCs are
shown in Table 2; LDA-SP?s improvements over
both Jaccard and mutual information are highly
significant with a significance level less than 0.01
using a paired t-test.
In addition to a superior performance in se-
lectional preference evaluation LDA-SP also pro-
duces a set of coherent topics, which can be use-
ful in their own right. For instance, one could use
them for tasks such as set-expansion (Carlson et
al., 2010) or automatic thesaurus induction (Et-
LDA-SP MI-Sim Jaccard-Sim
AUC 0.833 0.727 0.711
Table 2: Area under the precision recall curve.
LDA-SP?s AUC is significantly higher than both
similarity-based methods according to a paired t-
test with a significance level below 0.01.
zioni et al, 2005; Kozareva et al, 2008).
4.3 End Task Evaluation
We now evaluate LDA-SP?s ability to improve per-
formance at an end-task. We choose the task of
improving textual entailment by learning selec-
tional preferences for inference rules and filtering
inferences that do not respect these. This applica-
tion of selectional preferences was introduced by
Pantel et. al. (2007). For now we stick to infer-
ence rules of the form r1(a1, a2) ? r2(a1, a2),
though our ideas are more generally applicable to
more complex rules. As an example, the rule (X
defeats Y) ? (X plays Y) holds when X and Y
are both sports teams, however fails to produce a
reasonable inference if X and Y are Britain and
Nazi Germany respectively.
4.3.1 Filtering Inferences
In order for an inference to be plausible, both re-
lations must have similar selectional preferences,
and further, the arguments must obey the selec-
tional preferences of both the antecedent r1 and
the consequent r2.4 Pantel et al (2007) made
use of these intuitions by producing a set of class-
based selectional preferences for each relation,
then filtering out any inferences where the argu-
ments were incompatible with the intersection of
these preferences. In contrast, we take a proba-
bilistic approach, evaluating the quality of a spe-
cific inference by measuring the probability that
the arguments in both the antecedent and the con-
sequent were drawn from the same hidden topic
in our model. Note that this probability captures
both the requirement that the antecedent and con-
sequent have similar selectional preferences, and
that the arguments from a particular instance of the
rule?s application match their overlap.
We use zri,j to denote the topic that generates
the jth argument of relation ri. The probability
that the two arguments a1, a2 were drawn from
the same hidden topic factorizes as follows due to
the conditional independences in our model:5
P (zr1,1 = zr2,1, zr1,2 = zr2,2|a1, a2) =
P (zr1,1 = zr2,1|a1)P (zr1,2 = zr2,2|a2)
4Similarity-based and discriminative methods are not ap-
plicable to this task as they offer no straightforward way
to compare the similarity between selectional preferences of
two relations.
5Note that all probabilities are conditioned on an estimate
of the parameters ?, ?, ? from our model, which are omitted
for compactness.
430
To compute each of these factors we simply
marginalize over the hidden topics:
P (zr1,j = zr2,j |aj) =
TX
t=1
P (zr1,j = t|aj)P (zr2,j = t|aj)
where P (z = t|a) can be computed using
Bayes rule. For example,
P (zr1,1 = t|a1) =
P (a1|zr1,1 = t)P (zr1,1 = t)
P (a1)
=
?t(a1)?r1(t)
P (a1)
4.3.2 Experimental Conditions
In order to evaluate LDA-SP?s ability to filter in-
ferences based on selectional preferences we need
a set of inference rules between the relations in
our corpus. We therefore mapped the DIRT In-
ference rules (Lin and Pantel, 2001), (which con-
sist of pairs of dependency paths) to TEXTRUN-
NER relations as follows. We first gathered all in-
stances in the generalization corpus, and for each
r(a1, a2) created a corresponding simple sentence
by concatenating the arguments with the relation
string between them. Each such simple sentence
was parsed using Minipar (Lin, 1998). From
the parses we extracted all dependency paths be-
tween nouns that contain only words present in
the TEXTRUNNER relation string. These depen-
dency paths were then matched against each pair
in the DIRT database, and all pairs of associated
relations were collected producing about 26,000
inference rules.
Following Pantel et al (2007) we randomly
sampled 100 inference rules. We then automati-
cally filtered out any rules which contained a nega-
tion, or for which the antecedent and consequent
contained a pair of antonyms found in WordNet
(this left us with 85 rules). For each rule we col-
lected 10 random instances of the antecedent, and
generated the consequent. We randomly sampled
300 of these inferences to hand-label.
4.3.3 Results
In figure 5 we compare the precision and recall of
LDA-SP against the top two performing systems
described by Pantel et al (ISP.IIM-? and ISP.JIM,
both using the CBC clusters (Pantel, 2003)). We
find that LDA-SP achieves both higher precision
and recall than ISP.IIM-?. It is also able to achieve
the high-precision point of ISP.JIM and can trade
precision to get a much larger recall.
0.0 0.2 0.4 0.6 0.8 1.00
.4
0.6
0.8
1.0
recall
prec
isio
n
X
O
 
XO
LDA?SPISP.JIMISP.IIM?OR
Figure 5: Precision and recall on the inference fil-
tering task.
Top 10 Inference Rules Ranked by LDA-SP
antecedent consequent KL-div
will begin at will start at 0.014999
shall review shall determine 0.129434
may increase may reduce 0.214841
walk from walk to 0.219471
consume absorb 0.240730
shall keep shall maintain 0.264299
shall pay to will notify 0.290555
may apply for may obtain 0.313916
copy download 0.316502
should pay must pay 0.371544
Bottom 10 Inference Rules Ranked by LDA-SP
antecedent consequent KL-div
lose to shall take 10.011848
should play could do 10.028904
could play get in 10.048857
will start at move to 10.060994
shall keep will spend 10.105493
should play get in 10.131299
shall pay to leave for 10.131364
shall keep return to 10.149797
shall keep could do 10.178032
shall maintain have spent 10.221618
Table 3: Top 10 and Bottom 10 ranked inference
rules ranked by LDA-SPafter automatically filter-
ing out negations and antonyms (using WordNet).
In addition we demonstrate LDA-SP?s abil-
ity to rank inference rules by measuring the
Kullback Leibler Divergence6 between the topic-
distributions of the antecedent and consequent, ?r1
and ?r2 respectively. Table 3 shows the top 10 and
bottom 10 rules out of the 26,000 ranked by KL
Divergence after automatically filtering antonyms
(using WordNet) and negations. For slight varia-
tions in rules (e.g., symmetric pairs) we mention
only one example to show more variety.
6KL-Divergence is an information-theoretic measure of
the similarity between two probability distributions, and de-
fined as follows: KL(P ||Q) =
P
x P (x) log
P (x)
Q(x) .
431
4.4 A Repository of Class-Based Preferences
Finally we explore LDA-SP?s ability to produce a
repository of human interpretable class-based se-
lectional preferences. As an example, for the re-
lation was born in, we would like to infer that
the plausible arguments include (person, location)
and (person, date).
Since we already have a set of topics, our
task reduces to mapping the inferred topics to an
equivalent class in a taxonomy (e.g., WordNet).
We experimented with automatic methods such
as Resnik?s, but found them to have all the same
problems as directly applying these approaches to
the SP task.7 Guided by the fact that we have a
relatively small number of topics (600 total, 300
for each argument) we simply chose to label them
manually. By labeling this small number of topics
we can infer class-based preferences for an arbi-
trary number of relations.
In particular, we applied a semi-automatic
scheme to map topics to WordNet. We first applied
Resnik?s approach to automatically shortlist a few
candidate WordNet classes for each topic. We then
manually picked the best class from the shortlist
that best represented the 20 top arguments for a
topic (similar to Table 1). We marked all incoher-
ent topics with a special symbol ?. This process
took one of the authors about 4 hours to complete.
To evaluate how well our topic-class associa-
tions carry over to unseen relations we used the
same random sample of 100 relations from the
pseudo-disambiguation experiment.8 For each ar-
gument of each relation we picked the top two top-
ics according to frequency in the 5 Gibbs samples.
We then discarded any topics which were labeled
with ?; this resulted in a set of 236 predictions. A
few examples are displayed in table 4.
We evaluated these classes and found the accu-
racy to be around 0.88. We contrast this with Pan-
tel?s repository,9 the only other released database
of selectional preferences to our knowledge. We
evaluated the same 100 relations from his website
and tagged the top 2 classes for each argument and
evaluated the accuracy to be roughly 0.55.
7Perhaps recent work on automatic coherence ranking
(Newman et al, 2010) and labeling (Mei et al, 2007) could
produce better results.
8Recall that these 100 were not part of the original 3,000
in the generalization corpus, and are, therefore, representative
of new ?unseen? relations.
9http://demo.patrickpantel.com/
Content/LexSem/paraphrase.htm
arg1 class relation arg2 class
politician#1 was running for leader#1
people#1 will love show#3
organization#1 has responded to accusation#2
administrative unit#1 has appointed administrator#3
Table 4: Class-based Selectional Preferences.
We emphasize that tagging a pair of class-based
preferences is a highly subjective task, so these re-
sults should be treated as preliminary. Still, these
early results are promising. We wish to undertake
a larger scale study soon.
5 Conclusions and Future Work
We have presented an application of topic mod-
eling to the problem of automatically computing
selectional preferences. Our method, LDA-SP,
learns a distribution over topics for each rela-
tion while simultaneously grouping related words
into these topics. This approach is capable of
producing human interpretable classes, however,
avoids the drawbacks of traditional class-based ap-
proaches (poor lexical coverage and ambiguity).
LDA-SP achieves state-of-the-art performance on
predictive tasks such as pseudo-disambiguation,
and filtering incorrect inferences.
Because LDA-SP generates a complete proba-
bilistic model for our relation data, its results are
easily applicable to many other tasks such as iden-
tifying similar relations, ranking inference rules,
etc. In the future, we wish to apply our model
to automatically discover new inference rules and
paraphrases.
Finally, our repository of selectional pref-
erences for 10,000 relations is available at
http://www.cs.washington.edu/
research/ldasp.
Acknowledgments
We would like to thank Tim Baldwin, Colin
Cherry, Jesse Davis, Elena Erosheva, Stephen
Soderland, Dan Weld, in addition to the anony-
mous reviewers for helpful comments on a previ-
ous draft. This research was supported in part by
NSF grant IIS-0803481, ONR grant N00014-08-
1-0431, DARPA contract FA8750-09-C-0179, a
National Defense Science and Engineering Grad-
uate (NDSEG) Fellowship 32 CFR 168a, and car-
ried out at the University of Washington?s Turing
Center.
432
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
ACL-08: HLT.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In EMNLP.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL, pages 103?111,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Andrew Carlson, Justin Betteridge, Richard C. Wang,
Estevam R. Hruschka Jr., and Tom M. Mitchell.
2010. Coupled semi-supervised learning for infor-
mation extraction. In WSDM 2010.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global models of document
structure using latent permutations. In NAACL.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Comput. Linguist.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. In Machine Learning.
Hal Daume? III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics.
Hal Daume III. 2007. hbc: Hierarchical bayes com-
piler. http://hal3.name/hbc.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. Proceedings of the National Academy of
Sciences of the United States of America.
Oren Etzioni, Michael Cafarella, Doug Downey,
Ana maria Popescu, Tal Shaked, Stephen Soderl,
Daniel S. Weld, and Alex Yates. 2005. Unsuper-
vised named-entity extraction from the web: An ex-
perimental study. Artificial Intelligence.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Comput. Linguist.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proc Natl Acad Sci U S A.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Comput.
Linguist.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL-08: HLT.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the mdl principle.
Comput. Linguist.
Dekang Lin and Patrick Pantel. 2001. Dirt-discovery
of inference rules from text. In KDD.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proc. Workshop on the Evaluation of
Parsing Systems.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In KDD.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. JMLR.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In NAACL-HLT.
Diarmuid O? Se?aghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard H. Hovy. 2007.
Isp: Learning inferential selectional preferences. In
HLT-NAACL.
Patrick Andre Pantel. 2003. Clustering by commit-
tee. Ph.D. thesis, University of Alberta, Edmonton,
Alta., Canada.
Joseph Reisinger and Marius Pasca. 2009. Latent vari-
able models of concept-attribute attachment. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
P. Resnik. 1996. Selectional constraints: an
information-theoretic model and its computational
realization. Cognition.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proc. of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why,
What, and How?
433
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
Proceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics.
Lenhart Schubert and Matthew Tong. 2003. Extract-
ing and evaluating general world knowledge from
the brown corpus. In In Proc. of the HLT-NAACL
Workshop on Text Meaning, pages 7?13.
Benjamin Van Durme and Daniel Gildea. 2009. Topic
models for corpus-centric knowledge generalization.
In Technical Report TR-946, Department of Com-
puter Science, University of Rochester, Rochester.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In NAACL.
L. Yao, D. Mimno, and A. Mccallum. 2009. Effi-
cient methods for topic model inference on stream-
ing document collections. In KDD.
434
Proceedings of the ACL 2010 Conference Short Papers, pages 286?290,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Extracting Sequences from the Web
Anthony Fader, Stephen Soderland, and Oren Etzioni
University of Washington, Seattle
{afader,soderlan,etzioni}@cs.washington.edu
Abstract
Classical Information Extraction (IE) sys-
tems fill slots in domain-specific frames.
This paper reports on SEQ, a novel
open IE system that leverages a domain-
independent frame to extract ordered se-
quences such as presidents of the United
States or the most common causes of death
in the U.S. SEQ leverages regularities
about sequences to extract a coherent set
of sequences from Web text. SEQ nearly
doubles the area under the precision-recall
curve compared to an extractor that does
not exploit these regularities.
1 Introduction
Classical IE systems fill slots in domain-specific
frames such as the time and location slots in sem-
inar announcements (Freitag, 2000) or the terror-
ist organization slot in news stories (Chieu et al,
2003). In contrast, open IE systems are domain-
independent, but extract ?flat? sets of assertions
that are not organized into frames and slots
(Sekine, 2006; Banko et al, 2007). This paper
reports on SEQ?an open IE system that leverages
a domain-independent frame to extract ordered se-
quences of objects from Web text. We show that
the novel, domain-independent sequence frame in
SEQ substantially boosts the precision and recall
of the system and yields coherent sequences fil-
tered from low-precision extractions (Table 1).
Sequence extraction is distinct from set expan-
sion (Etzioni et al, 2004; Wang and Cohen, 2007)
because sequences are ordered and because the ex-
traction process does not require seeds or HTML
lists as input.
The domain-independent sequence frame con-
sists of a sequence name s (e.g., presidents of the
United States), and a set of ordered pairs (x, k)
where x is a string naming a member of the se-
quence with name s, and k is an integer indicating
Most common cause of death in the United States:
1. heart disease, 2. cancer, 3. stroke, 4. COPD,
5. pneumonia, 6. cirrhosis, 7. AIDS, 8. chronic liver
disease, 9. sepsis, 10. suicide, 11. septic shock.
Largest tobacco company in the world:
1. Philip Morris, 2. BAT, 3. Japan Tobacco,
4. Imperial Tobacco, 5. Altadis.
Largest rodent in the world:
1. Capybara, 2. Beaver, 3. Patagonian Cavies. 4. Maras.
Sign of the zodiac:
1. Aries, 2. Taurus, 3. Gemini, 4. Cancer, 5. Leo,
6. Virgo, 7. Libra, 8. Scorpio, 9. Sagittarius,
10. Capricorn, 11. Aquarius, 12. Pisces, 13. Ophiuchus.
Table 1: Examples of sequences extracted by SEQ
from unstructured Web text.
its position (e.g., (Washington, 1) and (JFK, 35)).
The task of sequence extraction is to automatically
instantiate sequence frames given a corpus of un-
structured text.
By definition, sequences have two properties
that we can leverage in creating a sequence ex-
tractor: functionality and density. Functionality
means position k in a sequence is occupied by a
single real-world entity x. Density means that if
a value has been observed at position k then there
must exist values for all i < k, and possibly more
after it.
2 The SEQ System
Sequence extraction has two parts: identify-
ing possible extractions (x, k, s) from text, and
then classifying those extractions as either cor-
rect or incorrect. In the following section, we
describe a way to identify candidate extractions
from text using a set of lexico-syntactic patterns.
We then show that classifying extractions based
on sentence-level features and redundancy alone
yields low precision, which is improved by lever-
aging the functionality and density properties of
sequences as done in our SEQ system.
286
Pattern Example
the ORD the fifth
the RB ORD the very first
the JJS the best
the RB JJS the very best
the ORD JJS the third biggest
the RBS JJ the most popular
the ORD RBS JJ the second least likely
Table 2: The patterns used by SEQ to detect ordi-
nal phrases are noun phrases that begin with one
of the part-of-speech patterns listed above.
2.1 Generating Sequence Extractions
To obtain candidate sequence extractions (x, k, s)
from text, the SEQ system finds sentences in its
input corpus that contain an ordinal phrase (OP).
Table 2 lists the lexico-syntactic patterns SEQ uses
to detect ordinal phrases. The value of k is set to
the integer corresponding to the ordinal number in
the OP.1
Next, SEQ takes each sentence that contains an
ordinal phrase o, and finds candidate items of the
form (x, k) for the sequence with name s. SEQ
constrains x to be an NP that is disjoint from o, and
s to be an NP (which may have post-modifying
PPs or clauses) following the ordinal number in o.
For example, given the sentence ?With help
from his father, JFK was elected as the 35th Pres-
ident of the United States in 1960?, SEQ finds
the candidate sequences with names ?President?,
?President of the United States?, and ?President of
the United States in 1960?, each of which has can-
didate extractions (JFK, 35), (his father, 35), and
(help, 35). We use heuristics to filter out many of
the candidate values (e.g., no value should cross a
sentence-like boundary, and x should be at most
some distance from the OP).
This process of generating candidate ex-
tractions has high coverage, but low preci-
sion. The first step in identifying correct ex-
tractions is to compute a confidence measure
localConf(x, k, s|sentence), which measures
how likely (x, k, s) is given the sentence it came
from. We do this using domain-independent syn-
tactic features based on POS tags and the pattern-
based features ?x {is,are,was,were} the kth s? and
?the kth s {is,are,was,were} x?. The features are
then combined using a Naive Bayes classifier.
In addition to the local, sentence-based features,
1Sequences often use a superlative for the first item (k =
1) such as ?the deepest lake in Africa?, ?the second deepest
lake in Africa? (or ?the 2nd deepest ...?), etc.
we define the measure totalConf that takes into
account redundancy in an input corpus C. As
Downey et al observed (2005), extractions that
occur more frequently in multiple distinct sen-
tences are more likely to be correct.
totalConf(x, k, s|C) =
?
sentence?C
localConf(x, k, s|sentence) (1)
2.2 Challenges
The scores localConf and totalConf are not suffi-
cient to identify valid sequence extractions. They
tend to give high scores to extractions where the
sequence scope is too general or too specific. In
our running example, the sequence name ?Presi-
dent? is too general ? many countries and orga-
nizations have a president. The sequence name
?President of the United States in 1960? is too spe-
cific ? there were not multiple U.S. presidents in
1960.
These errors can be explained as violations of
functionality and density. The sequence with
name ?President? will have many distinct candi-
date extractions in its positions, which is a vio-
lation of functionality. The sequence with name
?President of the United States in 1960? will not
satisfy density, since it will have extractions for
only one position.
In the next section, we present the details of how
SEQ incorporates functionality and density into its
assessment of a candidate extraction.
Given an extraction (x, k, s), SEQ must clas-
sify it as either correct or incorrect. SEQ breaks
this problem down into two parts: (1) determining
whether s is a correct sequence name, and (2) de-
termining whether (x, k) is an item in s, assuming
s is correct.
A joint probabilistic model of these two deci-
sions would require a significant amount of la-
beled data. To get around this problem, we repre-
sent each (x, k, s) as a vector of features and train
two Naive Bayes classifiers: one for classifying s
and one for classifying (x, k). We then rank ex-
tractions by taking the product of the two classi-
fiers? confidence scores.
We now describe the features used in the two
classifiers and how the classifiers are trained.
Classifying Sequences To classify a sequence
name s, SEQ uses features to measure the func-
tionality and density of s. Functionality means
287
that a correct sequence with name s has one cor-
rect value x at each position k, possibly with ad-
ditional noise due to extraction errors and synony-
mous values of x. For a fixed sequence name s
and position k, we can weight each of the candi-
date x values in that position by their normalized
total confidence:
w(x|k, s, C) =
totalConf(x, k, s|C)
?
x
? totalConf(x?, k, s|C)
For overly general sequences, the distribution of
weights for a position will tend to be more flat,
since there are many equally-likely candidate x
values. To measure this property, we use a func-
tion analogous to information entropy:
H(k, s|C) = ?
?
x
w(x|k, s, C) log
2
w(x|k, s, C)
Sequences s that are too general will tend to have
high values of H(k, s|C) for many values of k.
We found that a good measure of the overall non-
functionality of s is the average value of H(k, s|C)
for k = 1, 2, 3, 4.
For a sequence name s that is too specific, we
would expect that there are only a few filled-in po-
sitions. We model the density of s with two met-
rics. The first is numFilledPos(s|C), the num-
ber of distinct values of k such that there is some
extraction (x, k) for s in the corpus. The second
is totalSeqConf(s|C), which is the sum of the
scores of most confident x in each position:
totalSeqConf(s|C) =
?
k
max
x
totalConf(x, k, s|C) (2)
The functionality and density features are com-
bined using a Naive Bayes classifier. To train the
classifier, we use a set of sequence names s labeled
as either correct or incorrect, which we describe in
Section 3.
Classifying Sequence Items To classify (x, k)
given s, SEQ uses two features: the total con-
fidence totalConf(x, k, s|C) and the same total
confidence normalized to sum to 1 over all x, hold-
ing k and s constant. To train the classifier, we use
a set of extractions (x, k, s) where s is known to
be a correct sequence name.
3 Experimental Results
This section reports on two experiments. First, we
measured how the density and functionality fea-
tures improve performance on the sequence name
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Both Feature Sets
Only Density
Only Functionality
Max localConf
Figure 1: Using density or functionality features
alone is effective in identifying correct sequence
names. Combining both types of features outper-
forms either by a statistically significant margin
(paired t-test, p < 0.05).
classification sub-task (Figure 1). Second, we
report on SEQ?s performance on the sequence-
extraction task (Figure 2).
To create a test set, we selected all sentences
containing ordinal phrases from Banko?s 500M
Web page corpus (2008). To enrich this set O,
we obtained additional sentences from Bing.com
as follows. For each sequence name s satis-
fying localConf(x, k, s|sentence) ? 0.5 for
some sentence in O, we queried Bing.com for
?the kth s? for k = 1, 2, . . . until no more hits
were returned.2 For each query, we downloaded
the search snippets and added them to our cor-
pus. This procedure resulted in making 95, 611
search engine queries. The final corpus contained
3, 716, 745 distinct sentences containing an OP.
Generating candidate extractions using the
method from Section 2.1 resulted in a set of over
40 million distinct extractions, the vast majority
of which are incorrect. To get a sample with
a significant number of correct extractions, we
filtered this set to include only extractions with
totalConf(x, k, s|C) ? 0.8 for some sentence,
resulting in a set of 2, 409, 211 extractions.
We then randomly sampled and manually la-
beled 2, 000 of these extractions for evaluation.
We did a Web search to verify the correctness of
the sequence name s and that x is the kth item in
the sequence. In some cases, the ordering rela-
tion of the sequence name was ambiguous (e.g.,
2We queried for both the numeric form of the ordinal and
the number spelled out (e.g ?the 2nd ...? and ?the second ...?).
We took up to 100 results per query.
288
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
SEQ
REDUND
LOCAL
Figure 2: SEQ outperforms the baseline systems,
increasing the area under the curve by 247% rela-
tive to LOCAL and by 90% relative to REDUND.
?largest state in the US? could refer to land area or
population), which could lead to merging two dis-
tinct sequences. In practice, we found that most
ordering relations were used in a consistent way
(e.g., ?largest city in? always means largest by
population) and only about 5% of the sequence
names in our sample have an ambiguous ordering
relation.
We compute precision-recall curves relative to
this random sample by changing a confidence
threshold. Precision is the percentage of correct
extractions above a threshold, while recall is the
percentage correct above a threshold divided by
the total number of correct extractions. Because
SEQ requires training data, we used 15-fold cross
validation on the labeled sample.
The functionality and density features boost
SEQ?s ability to correctly identify sequence
names. Figure 1 shows how well SEQ can iden-
tify correct sequence names using only functional-
ity, only density, and using functionality and den-
sity in concert. The baseline used is the maximum
value of localConf(x, k, s) over all (x, k). Both
the density features and the functionality features
are effective at this task, but using both types of
features resulted in a statistically significant im-
provement over using either type of feature in-
dividually (paired t-test of area under the curve,
p < 0.05).
We measure SEQ?s efficacy on the complete
sequence-extraction task by contrasting it with two
baseline systems. The first is LOCAL, which
ranks extractions by localConf .3 The second is
3If an extraction arises from multiple sentences, we use
REDUND, which ranks extractions by totalConf .
Figure 2 shows the precision-recall curves for each
system on the test data. The area under the curves
for SEQ, REDUND, and LOCAL are 0.59, 0.31,
and 0.17, respectively. The low precision and flat
curve for LOCAL suggests that localConf is not
informative for classifying extractions on its own.
REDUND outperformed LOCAL, especially at
the high-precision part of the curve. On the subset
of extractions with correct s, REDUND can iden-
tify x as the kth item with precision of 0.85 at re-
call 0.80. This is consistent with previous work on
redundancy-based extractors on the Web. How-
ever, REDUND still suffered from the problems
of over-specification and over-generalization de-
scribed in Section 2. SEQ reduces the negative ef-
fects of these problems by decreasing the scores
of sequence names that appear too general or too
specific.
4 Related Work
There has been extensive work in extracting lists
or sets of entities from the Web. These extrac-
tors rely on either (1) HTML features (Cohen
et al, 2002; Wang and Cohen, 2007) to extract
from structured text or (2) lexico-syntactic pat-
terns (Hearst, 1992; Etzioni et al, 2005) to ex-
tract from unstructured text. SEQ is most similar
to this second type of extractor, but additionally
leverages the sequence regularities of functionality
and density. These regularities allow the system to
overcome the poor performance of the purely syn-
tactic extractor LOCAL and the redundancy-based
extractor REDUND.
5 Conclusions
We have demonstrated that an extractor leveraging
sequence regularities can greatly outperform ex-
tractors without this knowledge. Identifying likely
sequence names and then filling in sequence items
proved to be an effective approach to sequence ex-
traction.
One line of future research is to investigate
other types of domain-independent frames that ex-
hibit useful regularities. Other examples include
events (with regularities about actor, location, and
time) and a generic organization-role frame (with
regularities about person, organization, and role
played).
the maximal localConf .
289
6 Acknowledgements
This research was supported in part by NSF
grant IIS-0803481, ONR grant N00014-08-1-
0431, DARPA contract FA8750-09-C-0179, and
an NSF Graduate Research Fellowship, and was
carried out at the University of Washington?s Tur-
ing Center.
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI, pages 2670?2676.
H. Chieu, H. Ng, and Y. Lee. 2003. Closing the
gap: Learning-based information extraction rival-
ing knowledge-engineering methods. In ACL, pages
216?223.
William W. Cohen, Matthew Hurst, and Lee S. Jensen.
2002. A flexible learning system for wrapping ta-
bles and lists in html documents. In In International
World Wide Web Conference, pages 232?241.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI, pages 1034?1041.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2004. Methods for domain-independent informa-
tion extraction from the Web: An experimental com-
parison. In Proceedings of the Nineteenth National
Conference on Artificial Intelligence (AAAI-2004),
pages 391?398.
Oren Etzioni, Michael Cafarella, Doug Downey,
Ana maria Popescu, Tal Shaked, Stephen Soderl,
Daniel S. Weld, and Er Yates. 2005. Unsupervised
named-entity extraction from the web: An experi-
mental study. Artificial Intelligence, 165:91?134.
D. Freitag. 2000. Machine learning for information
extraction in informal domains. Machine Learning,
39(2-3):169?202.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING, pages
539?545.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 731?738, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Richard C. Wang and William W. Cohen. 2007.
Language-independent set expansion of named enti-
ties using the web. In ICDM, pages 342?350. IEEE
Computer Society.
290
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1608?1618,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Paraphrase-Driven Learning for Open Question Answering
Anthony Fader Luke Zettlemoyer Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{afader, lsz, etzioni}@cs.washington.edu
Abstract
We study question answering as a ma-
chine learning problem, and induce a func-
tion that maps open-domain questions to
queries over a database of web extrac-
tions. Given a large, community-authored,
question-paraphrase corpus, we demon-
strate that it is possible to learn a se-
mantic lexicon and linear ranking func-
tion without manually annotating ques-
tions. Our approach automatically gener-
alizes a seed lexicon and includes a scal-
able, parallelized perceptron parameter es-
timation scheme. Experiments show that
our approach more than quadruples the re-
call of the seed lexicon, with only an 8%
loss in precision.
1 Introduction
Open-domain question answering (QA) is a long-
standing, unsolved problem. The central challenge
is to automate every step of QA system construc-
tion, including gathering large databases and an-
swering questions against these databases. While
there has been significant work on large-scale in-
formation extraction (IE) from unstructured text
(Banko et al, 2007; Hoffmann et al, 2010; Riedel
et al, 2010), the problem of answering questions
with the noisy knowledge bases that IE systems
produce has received less attention. In this paper,
we present an approach for learning to map ques-
tions to formal queries over a large, open-domain
database of extracted facts (Fader et al, 2011).
Our system learns from a large, noisy, question-
paraphrase corpus, where question clusters have
a common but unknown query, and can span
a diverse set of topics. Table 1 shows exam-
ple paraphrase clusters for a set of factual ques-
tions. Such data provides strong signal for learn-
ing about lexical variation, but there are a number
Who wrote the Winnie the Pooh books?
Who is the author of winnie the pooh?
What was the name of the authur of winnie the pooh?
Who wrote the series of books for Winnie the poo?
Who wrote the children?s storybook ?Winnie the Pooh??
Who is poohs creator?
What relieves a hangover?
What is the best cure for a hangover?
The best way to recover from a hangover?
Best remedy for a hangover?
What takes away a hangover?
How do you lose a hangover?
What helps hangover symptoms?
What are social networking sites used for?
Why do people use social networking sites worldwide?
Advantages of using social network sites?
Why do people use social networks a lot?
Why do people communicate on social networking sites?
What are the pros and cons of social networking sites?
How do you say Santa Claus in Sweden?
Say santa clause in sweden?
How do you say santa clause in swedish?
How do they say santa in Sweden?
In Sweden what is santa called?
Who is sweden santa?
Table 1: Examples of paraphrase clusters from the
WikiAnswers corpus. Within each cluster, there is
a wide range of syntactic and lexical variations.
of challenges. Given that the data is community-
authored, it will inevitably be incomplete, contain
incorrectly tagged paraphrases, non-factual ques-
tions, and other sources of noise.
Our core contribution is a new learning ap-
proach that scalably sifts through this para-
phrase noise, learning to answer a broad class
of factual questions. We focus on answer-
ing open-domain questions that can be answered
with single-relation queries, e.g. all of the para-
phrases of ?Who wrote Winnie the Pooh?? and
?What cures a hangover?? in Table 1. The
algorithm answers such questions by mapping
them to executable queries over a tuple store
containing relations such as authored(milne,
winnie-the-pooh) and treat(bloody-mary,
hangover-symptoms).
1608
The approach automatically induces lexical
structures, which are combined to build queries for
unseen questions. It learns lexical equivalences for
relations (e.g., wrote, authored, and creator), en-
tities (e.g., Winnie the Pooh or Pooh Bear), and
question templates (e.g., Who r the e books? and
Who is the r of e?). Crucially, the approach
does not require any explicit labeling of the ques-
tions in our paraphrase corpus. Instead, we use
16 seed question templates and string-matching to
find high-quality queries for a small subset of the
questions. The algorithm uses learned word align-
ments to aggressively generalize the seeds, pro-
ducing a large set of possible lexical equivalences.
We then learn a linear ranking model to filter the
learned lexical equivalences, keeping only those
that are likely to answer questions well in practice.
Experimental results on 18 million paraphrase
pairs gathered from WikiAnswers1 demonstrate
the effectiveness of the overall approach. We
performed an end-to-end evaluation against a
database of 15 million facts automatically ex-
tracted from general web text (Fader et al, 2011).
On known-answerable questions, the approach
achieved 42% recall, with 77% precision, more
than quadrupling the recall over a baseline system.
In sum, we make the following contributions:
? We introduce PARALEX, an end-to-end open-
domain question answering system.
? We describe scalable learning algorithms that
induce general question templates and lexical
variants of entities and relations. These algo-
rithms require no manual annotation and can
be applied to large, noisy databases of rela-
tional triples.
? We evaluate PARALEX on the end-task of an-
swering questions from WikiAnswers using a
database of web extractions, and show that it
outperforms baseline systems.
? We release our learned lexicon and
question-paraphrase dataset to the
research community, available at
http://openie.cs.washington.edu.
2 Related Work
Our work builds upon two major threads of re-
search in natural language processing: informa-
tion extraction (IE), and natural language inter-
faces to databases (NLIDB).
1http://wiki.answers.com/
Research in IE has been moving towards the
goal of extracting facts from large text corpora,
across many domains, with minimal supervision
(Mintz et al, 2009; Hoffmann et al, 2010; Riedel
et al, 2010; Hoffmann et al, 2011; Banko et al,
2007; Yao et al, 2012). While much progress
has been made in converting text into structured
knowledge, there has been little work on an-
swering natural language questions over these
databases. There has been some work on QA over
web text (Kwok et al, 2001; Brill et al, 2002), but
these systems do not operate over extracted rela-
tional data.
The NLIDB problem has been studied for
decades (Grosz et al, 1987; Katz, 1997). More
recently, researchers have created systems that
use machine learning techniques to automatically
construct question answering systems from data
(Zelle and Mooney, 1996; Popescu et al, 2004;
Zettlemoyer and Collins, 2005; Clarke et al, 2010;
Liang et al, 2011). These systems have the abil-
ity to handle questions with complex semantics
on small domain-specific databases like GeoQuery
(Tang and Mooney, 2001) or subsets of Freebase
(Cai and Yates, 2013), but have yet to scale to the
task of general, open-domain question answering.
In contrast, our system answers questions with
more limited semantics, but does so at a very large
scale in an open-domain manner. Some work has
been made towards more general databases like
DBpedia (Yahya et al, 2012; Unger et al, 2012),
but these systems rely on hand-written templates
for question interpretation.
The learning algorithms presented in this pa-
per are similar to algorithms used for paraphrase
extraction from sentence-aligned corpora (Barzi-
lay and McKeown, 2001; Barzilay and Lee, 2003;
Quirk et al, 2004; Bannard and Callison-Burch,
2005; Callison-Burch, 2008; Marton et al, 2009).
However, we use a paraphrase corpus for extract-
ing lexical items relating natural language patterns
to database concepts, as opposed to relationships
between pairs of natural language utterances.
3 Overview of the Approach
In this section, we give a high-level overview of
the rest of the paper.
Problem Our goal is to learn a function that will
map a natural language question x to a query z
over a database D. The database D is a collection
of assertions in the form r(e1, e2) where r is a bi-
1609
nary relation from a vocabulary R, and e1 and e2
are entities from a vocabulary E. We assume that
the elements of R and E are human-interpretable
strings like population or new-york. In our
experiments, R and E contain millions of en-
tries representing ambiguous and overlapping con-
cepts. The database is equipped with a simple in-
terface that accepts queries in the form r(?, e2) or
r(e1, ?). When executed, these queries return all
entities e that satisfy the given relationship. Thus,
our task is to find the query z that best captures the
semantics of the question x.
Model The question answering model includes a
lexicon and a linear ranking function. The lexicon
L associates natural language patterns to database
concepts, thereby defining the space of queries
that can be derived from the input question (see
Table 2). Lexical entries can pair strings with
database entities (nyc and new-york), strings with
database relations (big and population), or ques-
tion patterns with templated database queries (how
r is e? and r(?,e)). We describe this model in
more detail in Section 4.
Learning The learning algorithm induces a lex-
icon L and estimates the parameters ? of the
linear ranking function. We learn L by boot-
strapping from an initial seed lexicon L0 over a
corpus of question paraphrases C = {(x, x?) :
x? is a paraphrase of x}, like the examples in Ta-
ble 1. We estimate ? by using the initial lexicon to
automatically label queries in the paraphrase cor-
pus, as described in Section 5.2. The final result
is a scalable learning algorithm that requires no
manual annotation of questions.
Evaluation In Section 8, we evaluate our system
against various baselines on the end-task of ques-
tion answering against a large database of facts
extracted from the web. We use held-out known-
answerable questions from WikiAnswers as a test
set.
4 Question Answering Model
To answer questions, we must find the best query
for a given natural language question.
4.1 Lexicon and Derivations
To define the space of possible queries, PARALEX
uses a lexicon L that encodes mappings from nat-
ural language to database concepts (entities, rela-
tions, and queries). Each entry in L is a pair (p, d)
Entry Type NL Pattern DB Concept
Entity nyc new-york
Relation big population
Question (1-Arg.) how big is e population(?, e)
Question (2-Arg.) how r is e r(?, e)
Table 2: Example lexical entries.
where p is a pattern and d is an associated database
concept. Table 2 gives examples of the entry types
in L: entity, relation, and question patterns.
Entity patterns match a contiguous string of
words and are associated with some database en-
tity e ? E.
Relation patterns match a contiguous string of
words and are associated with a relation r ? R and
an argument ordering (e.g. the string child could
be modeled as either parent-of or child-of with
opposite argument ordering).
Question patterns match an entire question
string, with gaps that recursively match an en-
tity or relation patterns. Question patterns are as-
sociated with a templated database query, where
the values of the variables are determined by the
matched entity and relation patterns. A question
pattern may be 1-Argument, with a variable for
an entity pattern, or 2-Argument, with variables
for an entity pattern and a relation pattern. A 2-
argument question pattern may also invert the ar-
gument order of the matched relation pattern, e.g.
who r e? may have the opposite argument order
of who did e r?
The lexicon is used to generate a derivation y
from an input question x to a database query z.
For example, the entries in Table 2 can be used
to make the following derivation from the ques-
tion How big is nyc? to the query population(?,
new-york):
This derivation proceeds in two steps: first match-
ing a question form like How r is e? and then
mapping big to population and nyc to new-york.
Factoring the derivation this way allows the lexi-
cal entries for big and nyc to be reused in semanti-
1610
cally equivalent variants like nyc how big is it? or
approximately how big is nyc? This factorization
helps the system generalize to novel questions that
do not appear in the training set.
We model a derivation as a set of (pi, di) pairs,
where each pi matches a substring of x, the sub-
strings cover all words in x, and the database con-
cepts di compose to form z. Derivations are rooted
at either a 1-argument or 2-argument question en-
try and have entity or relation entries as leaves.
4.2 Linear Ranking Function
In general, multiple queries may be derived from a
single input question x using a lexicon L. Many of
these derivations may be incorrect due to noise in
L. Given a question x, we consider all derivations
y and score them with ? ??(x, y), where ?(x, y) is
a n-dimensional feature representation and ? is a
n-dimensional parameter vector. Let GEN(x;L)
be the set of all derivations y that can be generated
from x using L. The best derivation y?(x) accord-
ing to the model (?, L) is given by:
y?(x) = argmax
y?GEN(x;L)
? ? ?(x, y)
The best query z?(x) can be computed directly
from the derivation y?(x).
Computing the set GEN(x;L) involves finding
all 1-Argument and 2-Argument question patterns
that match x, and then enumerating all possible
database concepts that match entity and relation
strings. When the database and lexicon are large,
this becomes intractable. We prune GEN(x;L)
using the model parameters ? by only considering
the N -best question patterns that match x, before
additionally enumerating any relations or entities.
For the end-to-end QA task, we return a ranked
list of answers from the k highest scoring queries.
We score an answer a with the highest score of all
derivations that generate a query with answer a.
5 Learning
PARALEX uses a two-part learning algorithm; it
first induces an overly general lexicon (Section
5.1) and then learns to score derivations to increase
accuracy (Section 5.2). Both algorithms rely on an
initial seed lexicon, which we describe in Section
7.4.
5.1 Lexical Learning
The lexical learning algorithm constructs a lexi-
con L from a corpus of question paraphrases C =
{(x, x?) : x? is a paraphrase of x}, where we as-
sume that all paraphrased questions (x, x?) can be
answered with a single, initially unknown, query
(Table 1 shows example paraphrases). This as-
sumption allows the algorithm to generalize from
the initial seed lexicon L0, greatly increasing the
lexical coverage.
As an example, consider the paraphrase pair x
= What is the population of New York? and x? =
How big is NYC? Suppose x can be mapped to a
query under L0 using the following derivation y:
what is the r of e = r(?, e)
population = population
new york = new-york
We can induce new lexical items by aligning the
patterns used in y to substrings in x?. For example,
suppose we know that the words in (x, x?) align in
the following way:
Using this information, we can hypothesize that
how r is e, big, and nyc should have the same in-
terpretations as what is the r of e, population, and
new york, respectively, and create the new entries:
how r is e = r(?, e)
big = population
nyc = new-york
We call this procedure InduceLex(x, x?, y, A),
which takes a paraphrase pair (x, x?), a derivation
y of x, and a word alignment A, and returns a new
set of lexical entries. Before formally describing
InduceLex we need to introduce some definitions.
Let n and n? be the number of words in x and
x?. Let [k] denote the set of integers {1, . . . , k}.
A word alignment A between x and x? is a subset
of [n] ? [n?]. A phrase alignment is a pair of in-
dex sets (I, I ?) where I ? [n] and I ? ? [n?]. A
phrase alignment (I, I ?) is consistent with a word
alignment A if for all (i, i?) ? A, i ? I if and only
if i? ? I ?. In other words, a phrase alignment is
consistent with a word alignment if the words in
the phrases are aligned only with each other, and
not with any outside words.
We will now define InduceLex(x, x?, y, A) for
the case where the derivation y consists of a 2-
argument question entry (pq, dq), a relation entry
1611
function LEARNLEXICON
Inputs:
- A corpus C of paraphrases (x, x?). (Table 1)
- An initial lexicon L0 of (pattern, concept) pairs.
- A word alignment function WordAlign(x, x?).
(Section 6)
- Initial parameters ?0.
- A function GEN(x;L) that derives queries from
a question x using lexicon L. (Section 4)
- A function InduceLex(x, x?, y, A) that induces
new lexical items from the paraphrases (x, x?) us-
ing their word alignment A and a derivation y of
x. (Section 5.1)
Output: A learned lexicon L.
L = {}
for all x, x? ? C do
if GEN(x;L0) is not empty then
A?WordAlign(x, x?)
y? ? argmaxy?GEN(x;L0) ?0 ? ?(x, y)
L? L ? InduceLex(x, x?, y?, A)
return L
Figure 1: Our lexicon learning algorithm.
(pr, dr), and an entity entry (pe, de), as shown in
the example above.2 InduceLex returns the set of
all triples (p?q, dq), (p?r, dr), (p?e, de) such that for
all p?q, p?r, p?e such that
1. p?q, p?r, p?e are a partition of the words in x?.
2. The phrase pairs (pq, p?q), (pr, p?r), (pe, p?e)
are consistent with the word alignment A.
3. The p?r and p?e are contiguous spans of words
in x?.
Figure 1 shows the complete lexical learning al-
gorithm. In practice, for a given paraphrase pair
(x, x?) and alignment A, InduceLex will gener-
ate multiple sets of new lexical entries, resulting
in a lexicon with millions of entries. We use an
existing statistical word alignment algorithm for
WordAlign (see Section 6). In the next section,
we will introduce a scalable approach for learning
to score derivations to filter out lexical items that
generalize poorly.
5.2 Parameter Learning
Parameter learning is necessary for filtering out
derivations that use incorrect lexical entries like
new mexico = mexico, which arise from noise in
the paraphrases and noise in the word alignment.
2InduceLex has similar behavior for the other type of
derivation, which consists of a 1-argument question entry
(pq, dq) and an entity (pe, de).
We use the hidden variable structured perceptron
algorithm to learn ? from a list of (question x,
query z) training examples. We adopt the itera-
tive parameter mixing variation of the perceptron
(McDonald et al, 2010) to scale to a large number
of training examples.
Figure 2 shows the parameter learning algo-
rithm. The parameter learning algorithm operates
in two stages. First, we use the initial lexicon
L0 to automatically generate (question x, query z)
training examples from the paraphrase corpus C.
Then we feed the training examples into the learn-
ing algorithm, which estimates parameters for the
learned lexicon L.
Because the number of training examples is
large, we adopt a parallel perceptron approach.
We first randomly partition the training data T
into K equally-sized subsets T1, . . . , TK . We then
perform perceptron learning on each partition in
parallel. Finally, the learned weights from each
parallel run are aggregated by taking a uniformly
weighted average of each partition?s parameter
vector. This procedure is repeated for T iterations.
The training data consists of (question x, query
z) pairs, but our scoring model is over (question
x, derivation y) pairs, which are unobserved in
the training data. We use a hidden variable ver-
sion of the perceptron algorithm (Collins, 2002),
where the model parameters are updated using the
highest scoring derivation y? that will generate the
correct query z using the learned lexicon L.
6 Data
For our database D, we use the publicly avail-
able set of 15 million REVERB extractions (Fader
et al, 2011).3 The database consists of a set
of triples r(e1, e2) over a vocabulary of ap-
proximately 600K relations and 2M entities, ex-
tracted from the ClueWeb09 corpus.4 The RE-
VERB database contains a large cross-section of
general world-knowledge, and thus is a good
testbed for developing an open-domain QA sys-
tem. However, the extractions are noisy, unnor-
malized (e.g., the strings obama, barack-obama,
and president-obama all appear as distinct en-
tities), and ambiguous (e.g., the relation born-in
contains facts about both dates and locations).
3We used version 1.1, downloaded from http://
reverb.cs.washington.edu/.
4The full set of REVERB extractions from ClueWeb09
contains over six billion triples. We used the smaller subset
of triples to simplify our experiments.
1612
function LEARNPARAMETERS
Inputs:
- A corpus C of paraphrases (x, x?). (Table 1)
- An initial lexicon L0 of (pattern, db concept)
pairs.
- A learned lexiconL of (pattern, db concept) pairs.
- Initial parameters ?0.
- Number of perceptron epochs T .
- Number of training-data shards K.
- A function GEN(x;L) that derives queries from
a question x using lexicon L. (Section 4)
- A function PerceptronEpoch(T , ?, L) that runs
a single epoch of the hidden-variable structured
perceptron algorithm on training set T with initial
parameters ?, returning a new parameter vector
??. (Section 5.2)
Output: A learned parameter vector ?.
// Step 1: Generate Training Examples T
T = {}
for all x, x? ? C do
if GEN(x;L0) is not empty then
y? ? argmaxy?GEN(x;L0) ?0 ? ?(x, y)
z? ? query of y?
Add (x?, z?) to T
// Step 2: Learn Parameters from T
Randomly partition T into shards T1, . . . , TK
for t = 1 . . . T do
// Executed on k processors
?k,t = PerceptronEpoch(Tk, ?t?1, L)
// Average the weights
?t = 1K
?
k ?k,t
return ?T
Figure 2: Our parameter learning algorithm.
Our paraphrase corpus C was constructed from
the collaboratively edited QA site WikiAnswers.
WikiAnswers users can tag pairs of questions as
alternate wordings of each other. We harvested
a set of 18M of these question-paraphrase pairs,
with 2.4M distinct questions in the corpus.
To estimate the precision of the paraphrase cor-
pus, we randomly sampled a set of 100 pairs and
manually tagged them as ?paraphrase? or ?not-
paraphrase.? We found that 55% of the sampled
pairs are valid paraphrased. Most of the incorrect
paraphrases were questions that were related, but
not paraphrased e.g. How big is the biggest mall?
and Most expensive mall in the world?
We word-aligned each paraphrase pair using
the MGIZA++ implementation of IBM Model 4
(Och and Ney, 2000; Gao and Vogel, 2008). The
word-alignment algorithm was run in each direc-
tion (x, x?) and (x?, x) and then combined using
the grow-diag-final-and heuristic (Koehn et al,
2003).
7 Experimental Setup
We compare the following systems:
? PARALEX: the full system, using the lexical
learning and parameter learning algorithms
from Section 5.
? NoParam: PARALEX without the learned
parameters.
? InitOnly: PARALEX using only the initial
seed lexicon.
We evaluate the systems? performance on the end-
task of QA on WikiAnswers questions.
7.1 Test Set
A major challenge for evaluation is that the RE-
VERB database is incomplete. A system may cor-
rectly map a test question to a valid query, only
to return 0 results when executed against the in-
complete database. We factor out this source of
error by semi-automatically constructing a sample
of questions that are known to be answerable us-
ing the REVERB database, and thus allows for a
meaningful comparison on the task of question un-
derstanding.
To create the evaluation set, we identified ques-
tions x in a held out portion of the WikiAnswers
corpus such that (1) x can be mapped to some
query z using an initial lexicon (described in Sec-
tion 7.4), and (2) when z is executed against the
database, it returns at least one answer. We then
add x and all of its paraphrases as our evaluation
set. For example, the question What is the lan-
guage of Hong-Kong satisfies these requirements,
so we added these questions to the evaluation set:
What is the language of Hong-Kong?
What language do people in hong kong use?
How many languages are spoken in hong kong?
How many languages hong kong people use?
In Hong Kong what language is spoken?
Language of Hong-kong?
This methodology allows us to evaluate the sys-
tems? ability to handle syntactic and lexical varia-
tions of questions that should have the same an-
swers. We created 37 question clusters, result-
ing in a total of 698 questions. We removed all
of these questions and their paraphrases from the
training set. We also manually filtered out any in-
correct paraphrases that appeared in the test clus-
ters.
We then created a gold-standard set of (x, a, l)
triples, where x is a question, a is an answer, and l
1613
Question Pattern Database Query
who r e r(?, e)
what r e r(?, e)
who does e r r(e, ?)
what does e r r(e, ?)
what is the r of e r(?, e)
who is the r of e r(?, e)
what is r by e r(e, ?)
who is e?s r r(?, e)
what is e?s r r(?, e)
who is r by e r(e, ?)
when did e r r-in(e, ?)
when did e r r-on(e, ?)
when was e r r-in(e, ?)
when was e r r-on(e, ?)
where was e r r-in(e, ?)
where did e r r-in(e, ?)
Table 3: The question patterns used in the initial
lexicon L0.
is a label (correct or incorrect). To create the gold-
standard, we first ran each system on the evalua-
tion questions to generate (x, a) pairs. Then we
manually tagged each pair with a label l. This
resulted in a set of approximately 2, 000 human
judgments. If (x, a) was tagged with label l and x?
is a paraphrase of x, we automatically added the
labeling (x?, a, l), since questions in the same clus-
ter should have the same answer sets. This process
resulted in a gold standard set of approximately
48, 000 (x, a, l) triples.
7.2 Metrics
We use two types of metrics to score the systems.
The first metric measures the precision and recall
of each system?s highest ranked answer. Precision
is the fraction of predicted answers that are cor-
rect and recall is the fraction of questions where a
correct answer was predicted. The second metric
measures the accuracy of the entire ranked answer
set returned for a question. We compute the mean
average precision (MAP) of each systems? output,
which measures the average precision over all lev-
els of recall.
7.3 Features and Settings
The feature representation ?(x, y) consists of in-
dicator functions for each lexical entry (p, d) ? L
used in the derivation y. For parameter learning,
we use an initial weight vector ?0 = 0, use T = 20
F1 Precision Recall MAP
PARALEX 0.54 0.77 0.42 0.22
NoParam 0.30 0.53 0.20 0.08
InitOnly 0.18 0.84 0.10 0.04
Table 4: Performance on WikiAnswers questions
known to be answerable using REVERB.
F1 Precision Recall MAP
PARALEX 0.54 0.77 0.42 0.22
No 2-Arg. 0.40 0.86 0.26 0.12
No 1-Arg 0.35 0.81 0.22 0.11
No Relations 0.18 0.84 0.10 0.03
No Entity 0.36 0.55 0.27 0.15
Table 5: Ablation of the learned lexical items.
0.0 0.1 0.2 0.3 0.4 0.5
Recall
0.5
0.6
0.7
0.8
0.9
1.0
Pr
eci
sio
n
PARALEX
No 2-Arg.
Initial Lexicon
Figure 3: Precision-recall curves for PARALEX
with and without 2-argument question patterns.
iterations and shard the training data into K = 10
pieces. We limit each system to return the top 100
database queries for each test sentence. All input
words are lowercased and lemmatized.
7.4 Initial Lexicon
Both the lexical learning and parameter learning
algorithms rely on an initial seed lexicon L0. The
initial lexicon allows the learning algorithms to
bootstrap from the paraphrase corpus.
We construct L0 from a set of 16 hand-written
2-argument question patterns and the output of the
identity transformation on the entity and relation
strings in the database. Table 3 shows the question
patterns that were used in L0.
8 Results
Table 4 shows the performance of PARALEX on
the test questions. PARALEX outperforms the
baseline systems in terms of both F1 and MAP.
The lexicon-learning algorithm boosts the recall
by a factor of 4 over the initial lexicon, show-
ing the utility of the InduceLex algorithm. The
1614
String Learned Database Relations for String
get rid of treatment-for, cause, get-rid-of, cure-for, easiest-way-to-get-rid-of
word word-for, slang-term-for, definition-of, meaning-of, synonym-of
speak speak-language-in, language-speak-in, principal-language-of, dialect-of
useful main-use-of, purpose-of, importance-of, property-of, usefulness-of
String Learned Database Entities for String
smoking smoking, tobacco-smoking, cigarette, smoking-cigar, smoke, quit-smoking
radiation radiation, electromagnetic-radiation, nuclear-radiation
vancouver vancouver, vancouver-city, vancouver-island, vancouver-british-columbia
protein protein, protein-synthesis, plasma-protein, monomer, dna
Table 6: Examples of relation and entity synonyms learned from the WikiAnswers paraphrase corpus.
parameter-learning algorithm also results in a
large gain in both precision and recall: InduceLex
generates a noisy set of patterns, so selecting the
best query for a question is more challenging.
Table 5 shows an ablation of the different types
of lexical items learned by PARALEX. For each
row, we removed the learned lexical items from
each of the types described in Section 4, keeping
only the initial seed lexical items. The learned 2-
argument question templates significantly increase
the recall of the system. This increased recall
came at a cost, lowering precision from 0.86 to
0.77. Thresholding the query score allows us to
trade precision for recall, as shown in Figure 3.
Table 6 shows some examples of the learned en-
tity and relation synonyms.
The 2-argument question templates help PAR-
ALEX generalize over different variations of the
same question, like the test questions shown in
Table 7. For each question, PARALEX combines
a 2-argument question template (shown below the
questions) with the rules celebrate = holiday-of
and christians = christians to derive a full
query. Factoring the problem this way allows
PARALEX to reuse the same rules in different
syntactic configurations. Note that the imperfect
training data can lead to overly-specific templates
like what are the religious r of e, which can lower
accuracy.
9 Error Analysis
To understand how close we are to the goal of
open-domain QA, we ran PARALEX on an unre-
stricted sample of questions from WikiAnswers.
We used the same methodology as described in the
previous section, where PARALEX returns the top
answer for each question using REVERB.
We found that PARALEX performs significantly
worse on this dataset, with recall maxing out at ap-
Celebrations for Christians?
r for e?
Celebrations of Christians?
r of e?
What are some celebrations for Christians?
what are some r for e?
What are some celebrations of the Christians?
what are some r of e?
What are some of Christians celebrations?
what are some of e r?
What celebrations do Christians do?
what r do e do?
What did Christians celebrate?
what did e r?
What are the religious celebrations of Christians?
what are the religious r of e?
What celebration do Christians celebrate?
what r do e celebrate?
Table 7: Questions from the test set with 2-
argument question patterns that PARALEX used to
derive a correct query.
proximately 6% of the questions answered at pre-
cision 0.4. This is not surprising, since the test
questions are not restricted to topics covered by
the REVERB database, and may be too complex to
be answered by any database of relational triples.
We performed an error analysis on a sample
of 100 questions that were either incorrectly an-
swered or unanswered. We examined the can-
didate queries that PARALEX generated for each
question and tagged each query as correct (would
return a valid answer given a correct and com-
plete database) or incorrect. Because the input
questions are unrestricted, we also judged whether
the questions could be faithfully represented as a
r(?, e) or r(e, ?) query over the database vocabu-
lary. Table 8 shows the distribution of errors.
The largest source of error (36%) were on com-
1615
plex questions that could not be represented as a
query for various reasons. We categorized these
questions into groups. The largest group (14%)
were questions that need n-ary or higher-order
database relations, for example How long does
it take to drive from Sacramento to Cancun? or
What do cats and dogs have in common? Approx-
imately 13% of the questions were how-to ques-
tions like How do you make axes in minecraft?
whose answers are a sequence of steps, instead
of a database entity. Lastly, 9% of the questions
require database operators like joins, for example
When were Bobby Orr?s children born?
The second largest source of error (32%) were
questions that could be represented as a query, but
where PARALEX was unable to derive any cor-
rect queries. For example, the question Things
grown on Nigerian farms? was not mapped to
any queries, even though the REVERB database
contains the relation grown-in and the entity
nigeria. We found that 13% of the incorrect
questions were cases where the entity was not rec-
ognized, 12% were cases where the relation was
not recognized, and 6% were cases where both the
entity and relation were not recognized.
We found that 28% of the errors were cases
where PARALEX derived a query that we judged to
be correct, but returned no answers when executed
against the database. For example, given the ques-
tion How much can a dietician earn? PARALEX
derived the query salary-of(?, dietician) but
this returned no answers in the REVERB database.
Finally, approximately 4% of the questions in-
cluded typos or were judged to be inscrutable, for
example Barovier hiriacy of evidence based for
pressure sore?
Discussion Our experiments show that the learn-
ing algorithms described in Section 5 allow PAR-
ALEX to generalize beyond an initial lexicon and
answer questions with significantly higher accu-
racy. Our error analysis on an unrestricted set of
WikiAnswers questions shows that PARALEX is
still far from the goal of truly high-recall, open-
domain QA. We found that many questions asked
on WikiAnswers are either too complex to be
mapped to a simple relational query, or are not
covered by the REVERB database. Further, ap-
proximately one third of the missing recall is due
to entity and relation recognition errors.
Incorrectly Answered/Unanswered Questions
36% Complex Questions
Need n-ary or higher-order relations (14%)
Answer is a set of instructions (13%)
Need database operators e.g. joins (9%)
32% Entity or Relation Recognition Errors
Entity recognition errors (13%)
Relation recognition errors (12%)
Entity & relation recognition errors (7%)
28% Incomplete Database
Derived a correct query, but no answers
4% Typos/Inscrutable Questions
Table 8: Error distribution of PARALEX on an un-
restricted sample of questions from the WikiAn-
swers dataset.
10 Conclusion
We introduced a new learning approach that in-
duces a complete question-answering system from
a large corpus of noisy question-paraphrases. Us-
ing only a seed lexicon, the approach automat-
ically learns a lexicon and linear ranking func-
tion that demonstrated high accuracy on a held-out
evaluation set.
A number of open challenges remain. First,
precision could likely be improved by adding
new features to the ranking function. Second,
we would like to generalize the question under-
standing framework to produce more complex
queries, constructed within a compositional se-
mantic framework, but without sacrificing scala-
bility. Third, we would also like to extend the
system with other large databases like Freebase or
DBpedia. Lastly, we believe that it would be pos-
sible to leverage the user-provided answers from
WikiAnswers as a source of supervision.
Acknowledgments
This research was supported in part by ONR grant
N00014-11-1-0294, DARPA contract FA8750-09-
C-0179, a gift from Google, a gift from Vulcan
Inc., and carried out at the University of Washing-
ton?s Turing Center. We would like to thank Yoav
Artzi, Tom Kwiatkowski, Yuval Marton, Mausam,
Dan Weld, and the anonymous reviewers for their
helpful comments.
1616
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open Information Extraction from the Web. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting Paraphrases from a Parallel Corpus. In
Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics.
Eric Brill, Susan Dumais, and Michele Banko. 2002.
An Analysis of the AskMSR Question-Answering
System. In Proceedings of Empirical Methods in
Natural Language Processing.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexicon
Extension. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving Semantic Parsing from
the World?s Response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing.
Qin Gao and Stephan Vogel. 2008. Parallel Imple-
mentations of Word Alignment Tool. In Proc. of the
ACL 2008 Software Engineering, Testing, and Qual-
ity Assurance Workshop.
Barbara J. Grosz, Douglas E. Appelt, Paul A. Mar-
tin, and Fernando C. N. Pereira. 1987. TEAM:
An Experiment in the Design of Transportable
Natural-Language Interfaces. Artificial Intelligence,
32(2):173?243.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-Based Weak Supervision for Informa-
tion Extraction of Overlapping Relations. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics.
Boris Katz. 1997. Annotating the World Wide Web
using Natural Language. In RIAO, pages 136?159.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001.
Scaling Question Answering to the Web. ACM
Trans. Inf. Syst., 19(3):242?262.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning Dependency-Based Compositional Se-
mantics. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved Statistical Machine Trans-
lation Using Monolingually-Derived Paraphrases.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant Supervision for Relation Ex-
traction Without Labeled Data. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern
Natural Language Interfaces to Databases: Compos-
ing Statistical Parsing with Semantic Tractability. In
Proceedings of the Twentieth International Confer-
ence on Computational Linguistics.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Pro-
cessing.
1617
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling Relations and Their Mentions with-
out Labeled Text. In Proceedings of the 2010 Euro-
pean conference on Machine learning and Knowl-
edge Discovery in Databases.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing Multiple Clause Constructors in Inductive Logic
Programming for Semantic Parsing.
Christina Unger, Lorenz Bu?hmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-Based Question
Answering over RDF Data. In Proceedings of the
21st World Wide Web Conference 2012.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural Language Questions for
the Web of Data. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised Relation Discovery with Sense
Disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to Parse Database Queries Using Inductive Logic
Programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to Map Sentences to Logical Form: Struc-
tured Classification with Probabilistic Categorial
Grammars. In Proceedings of the 21st Conference
in Uncertainty in Artificial Intelligence.
1618
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 52?60,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Semantic Role Labeling for Open Information Extraction
Janara Christensen, Mausam, Stephen Soderland and Oren Etzioni
University of Washington, Seattle
Abstract
Open Information Extraction is a recent
paradigm for machine reading from arbitrary
text. In contrast to existing techniques, which
have used only shallow syntactic features, we
investigate the use of semantic features (se-
mantic roles) for the task of Open IE. We com-
pare TEXTRUNNER (Banko et al, 2007), a
state of the art open extractor, with our novel
extractor SRL-IE, which is based on UIUC?s
SRL system (Punyakanok et al, 2008). We
find that SRL-IE is robust to noisy heteroge-
neous Web data and outperforms TEXTRUN-
NER on extraction quality. On the other
hand, TEXTRUNNER performs over 2 orders
of magnitude faster and achieves good pre-
cision in high locality and high redundancy
extractions. These observations enable the
construction of hybrid extractors that output
higher quality results than TEXTRUNNER and
similar quality as SRL-IE in much less time.
1 Introduction
The grand challenge of Machine Reading (Etzioni
et al, 2006) requires, as a key step, a scalable
system for extracting information from large, het-
erogeneous, unstructured text. The traditional ap-
proaches to information extraction (e.g., (Soderland,
1999; Agichtein and Gravano, 2000)) do not oper-
ate at these scales, since they focus attention on a
well-defined small set of relations and require large
amounts of training data for each relation. The re-
cent Open Information Extraction paradigm (Banko
et al, 2007) attempts to overcome the knowledge
acquisition bottleneck with its relation-independent
nature and no manually annotated training data.
We are interested in the best possible technique
for Open IE. The TEXTRUNNER Open IE system
(Banko and Etzioni, 2008) employs only shallow
syntactic features in the extraction process. Avoid-
ing the expensive processing of deep syntactic anal-
ysis allowed TEXTRUNNER to process at Web scale.
In this paper, we explore the benefits of semantic
features and in particular, evaluate the application of
semantic role labeling (SRL) to Open IE.
SRL is a popular NLP task that has seen sig-
nificant progress over the last few years. The ad-
vent of hand-constructed semantic resources such as
Propbank and Framenet (Martha and Palmer, 2002;
Baker et al, 1998) have resulted in semantic role la-
belers achieving high in-domain precisions.
Our first observation is that semantically labeled
arguments in a sentence almost always correspond
to the arguments in Open IE extractions. Similarly,
the verbs often match up with Open IE relations.
These observations lead us to construct a new Open
IE extractor based on SRL. We use UIUC?s publicly
available SRL system (Punyakanok et al, 2008) that
is known to be competitive with the state of the art
and construct a novel Open IE extractor based on it
called SRL-IE.
We first need to evaluate SRL-IE?s effectiveness
in the context of large scale and heterogeneous input
data as found on the Web: because SRL uses deeper
analysis we expect SRL-IE to be much slower. Sec-
ond, SRL is trained on news corpora using a re-
source like Propbank, and so may face recall loss
due to out of vocabulary verbs and precision loss due
to different writing styles found on the Web.
In this paper we address several empirical ques-
52
tions. Can SRL-IE, our SRL based extractor,
achieve adequate precision/recall on the heteroge-
neous Web text? What factors influence the relative
performance of SRL-IE vs. that of TEXTRUNNER
(e.g., n-ary vs. binary extractions, redundancy, local-
ity, sentence length, out of vocabulary verbs, etc.)?
In terms of performance, what are the relative trade-
offs between the two? Finally, is it possible to design
a hybrid between the two systems to get the best of
both the worlds? Our results show that:
1. SRL-IE is surprisingly robust to noisy hetero-
geneous data and achieves high precision and
recall on the Open IE task on Web text.
2. SRL-IE outperforms TEXTRUNNER along di-
mensions such as recall and precision on com-
plex extractions (e.g., n-ary relations).
3. TEXTRUNNER is over 2 orders of magnitude
faster, and achieves good precision for extrac-
tions with high system confidence or high lo-
cality or when the same fact is extracted from
multiple sentences.
4. Hybrid extractors that use a combination of
SRL-IE and TEXTRUNNER get the best of
both worlds. Our hybrid extractors make effec-
tive use of available time and achieve a supe-
rior balance of precision-recall, better precision
compared to TEXTRUNNER, and better recall
compared to both TEXTRUNNER and SRL-IE.
2 Background
Open Information Extraction: The recently pop-
ular Open IE (Banko et al, 2007) is an extraction
paradigm where the system makes a single data-
driven pass over its corpus and extracts a large
set of relational tuples without requiring any hu-
man input. These tuples attempt to capture the
salient relationships expressed in each sentence. For
instance, for the sentence, ?McCain fought hard
against Obama, but finally lost the election? an
Open IE system would extract two tuples <McCain,
fought (hard) against, Obama>, and <McCain, lost,
the election>. These tuples can be binary or n-ary,
where the relationship is expressed between more
than 2 entities such as <Gates Foundation, invested
(arg) in, 1 billion dollars, high schools>.
TEXTRUNNER is a state-of-the-art Open IE sys-
tem that performs extraction in three key steps. (1)
A self-supervised learner that outputs a CRF based
classifier (that uses unlexicalized features) for ex-
tracting relationships. The self-supervised nature al-
leviates the need for hand-labeled training data and
unlexicalized features help scale to the multitudes of
relations found on the Web. (2) A single pass extrac-
tor that uses shallow syntactic techniques like part of
speech tagging, noun phrase chunking and then ap-
plies the CRF extractor to extract relationships ex-
pressed in natural language sentences. The use of
shallow features makes TEXTRUNNER highly effi-
cient. (3) A redundancy based assessor that re-ranks
these extractions based on a probabilistic model of
redundancy in text (Downey et al, 2005). This ex-
ploits the redundancy of information in Web text and
assigns higher confidence to extractions occurring
multiple times. All these components enable TEX-
TRUNNER to be a high performance, general, and
high quality extractor for heterogeneous Web text.
Semantic Role Labeling: SRL is a common NLP
task that consists of detecting semantic arguments
associated with a verb in a sentence and their classi-
fication into different roles (such as Agent, Patient,
Instrument, etc.). Given the sentence ?The pearls
I left to my son are fake? an SRL system would
conclude that for the verb ?leave?, ?I? is the agent,
?pearls? is the patient and ?son? is the benefactor.
Because not all roles feature in each verb the roles
are commonly divided into meta-roles (A0-A7) and
additional common classes such as location, time,
etc. Each Ai can represent a different role based
on the verb, though A0 and A1 most often refer to
agents and patients respectively. Availability of lexi-
cal resources such as Propbank (Martha and Palmer,
2002), which annotates text with meta-roles for each
argument, has enabled significant progress in SRL
systems over the last few years.
Recently, there have been many advances in SRL
(Toutanova et al, 2008; Johansson and Nugues,
2008; Coppola et al, 2009; Moschitti et al, 2008).
We use UIUC-SRL as our base SRL system (Pun-
yakanok et al, 2008). Our choice of the system is
guided by the fact that its code is freely available and
it is competitive with state of the art (it achieved the
highest F1 score on the CoNLL-2005 shared task).
UIUC-SRL operates in four key steps: pruning,
argument identification, argument classification and
53
inference. Pruning involves using a full parse tree
and heuristic rules to eliminate constituents that are
unlikely to be arguments. Argument identification
uses a classifier to identify constituents that are po-
tential arguments. In argument classification, an-
other classifier is used, this time to assign role labels
to the candidates identified in the previous stage. Ar-
gument information is not incorporated across argu-
ments until the inference stage, which uses an inte-
ger linear program to make global role predictions.
3 SRL-IE
Our key insight is that semantically labeled argu-
ments in a sentence almost always correspond to the
arguments in Open IE extractions. Thus, we can
convert the output of UIUC-SRL into an Open IE
extraction. We illustrate this conversion process via
an example.
Given the sentence, ?Eli Whitney created the cot-
ton gin in 1793,? TEXTRUNNER extracts two tuples,
one binary and one n-ary, as follows:
binary tuple:
arg0 Eli Whitney
rel created
arg1 the cotton gin
n-ary tuple:
arg0 Eli Whitney
rel created (arg) in
arg1 the cotton gin
arg2 1793
UIUC-SRL labels constituents of a sentence with
the role they play in regards to the verb in the sen-
tence. UIUC-SRL will extract:
A0 Eli Whitney
verb created
A1 the cotton gin
temporal in 1793
To convert UIUC-SRL output to Open IE format,
SRL-IE treats the verb (along with its modifiers and
negation, if present) as the relation. Moreover, it
assumes SRL?s role-labeled arguments as the Open
IE arguments related to the relation. The arguments
here consist of all entities labeled Ai, as well as any
entities that are marked Direction, Location, or Tem-
poral. We order the arguments in the same order as
they are in the sentence and with regard to the re-
lation (except for direction, location and temporal,
which cannot be arg0 of an Open IE extraction and
are placed at the end of argument list). As we are
interested in relations, we consider only extractions
that have at least two arguments.
In doing this conversion, we naturally ignore part
of the semantic information (such as distinctions be-
tween various Ai?s) that UIUC-SRL provides. In
this conversion process an SRL extraction that was
correct in the original format will never be changed
to an incorrect Open IE extraction. However, an in-
correctly labeled SRL extraction could still convert
to a correct Open IE extraction, if the arguments
were correctly identified but incorrectly labeled.
Because of the methodology that TEXTRUNNER
uses to extract relations, for n-ary extractions of the
form <arg0, rel, arg1, ..., argN>, TEXTRUNNER
often extracts sub-parts <arg0, rel, arg1>, <arg0,
rel, arg1, arg2>, ..., <arg0, rel, arg1, ..., argN-1>.
UIUC-SRL, however, extracts at most only one re-
lation for each verb in the sentence. For a fair com-
parison, we create additional subpart extractions for
each UIUC-SRL extraction using a similar policy.
4 Qualitative Comparison of Extractors
In order to understand SRL-IE better, we first com-
pare with TEXTRUNNER in a variety of scenarios,
such as sentences with lists, complex sentences, sen-
tences with out of vocabulary verbs, etc.
Argument boundaries: SRL-IE is lenient in de-
ciding what constitutes an argument and tends to
err on the side of including too much rather than
too little; TEXTRUNNER is much more conservative,
sometimes to the extent of omitting crucial informa-
tion, particularly post-modifying clauses and PPs.
For example, TEXTRUNNER extracts <Bunsen, in-
vented, a device> from the sentence ?Bunsen in-
vented a device called the Spectroscope?. SRL-IE
includes the entire phrase ?a device called the Spec-
troscope? as the second argument. Generally, the
longer arguments in SRL-IE are more informative
than TEXTRUNNER?s succinct ones. On the other
hand, TEXTRUNNER?s arguments normalize better
leading to an effective use of redundancy in ranking.
Lists: In sentences with a comma-separated lists of
nouns, SRL-IE creates one extraction and treats the
entire list as the argument, whereas TEXTRUNNER
separates them into several relations, one for each
item in the list.
Out of vocabulary verbs: While we expected
54
TEXTRUNNER to handle unknown verbs with lit-
tle difficulty due to its unlexicalized nature, SRL-
IE could have had severe trouble leading to a lim-
ited applicability in the context of Web text. How-
ever, contrary to our expectations, UIUC-SRL has
a graceful policy to handle new verbs by attempt-
ing to identify A0 (the agent) and A1 (the patient)
and leaving out the higher numbered ones. In prac-
tice, this is very effective ? SRL-IE recognizes the
verb and its two arguments correctly in ?Larry Page
googled his name and launched a new revolution.?
Part-of-speech ambiguity: Both SRL-IE and
TEXTRUNNER have difficulty when noun phrases
have an identical spelling with a verb. For example,
the word ?write? when used as a noun causes trouble
for both systems. In the sentence, ?Be sure the file
has write permission.? SRL-IE and TEXTRUNNER
both extract <the file, write, permission>.
Complex sentences: Because TEXTRUNNER only
uses shallow syntactic features it has a harder time
on sentences with complex structure. SRL-IE,
because of its deeper processing, can better handle
complex syntax and long-range dependencies, al-
though occasionally complex sentences will create
parsing errors causing difficulties for SRL-IE.
N-ary relations: Both extractors suffer significant
quality loss in n-ary extractions compared to binary.
A key problem is prepositional phrase attachment,
deciding whether the phrase associates with arg1 or
with the verb.
5 Experimental Results
In our quantitative evaluation we attempt to answer
two key questions: (1) what is the relative difference
in performance of SRL-IE and TEXTRUNNER on
precision, recall and computation time? And, (2)
what factors influence the relative performance of
the two systems? We explore the first question in
Section 5.2 and the second in Section 5.3.
5.1 Dataset
Our goal is to explore the behavior of TEXTRUN-
NER and SRL-IE on a large scale dataset containing
redundant information, since redundancy has been
shown to immensely benefit Web-based Open IE ex-
tractors. At the same time, the test set must be a
manageable size, due to SRL-IE?s relatively slow
processing time. We constructed a test set that ap-
proximates Web-scale distribution of extractions for
five target relations ? invent, graduate, study, write,
and develop.
We created our test set as follows. We queried a
corpus of 500M Web documents for a sample of sen-
tences with these verbs (or their inflected forms, e.g.,
invents, invented, etc.). We then ran TEXTRUNNER
and SRL-IE on those sentences to find 200 distinct
values of arg0 for each target relation, 100 from each
system. We searched for at most 100 sentences that
contain both the verb-form and arg0. This resulted
in a test set of an average of 6,000 sentences per re-
lation, for a total of 29,842 sentences. We use this
test set for all experiments in this paper.
In order to compute precision and recall on this
dataset, we tagged extractions by TEXTRUNNER
and by SRL-IE as correct or errors. A tuple is cor-
rect if the arguments have correct boundaries and
the relation accurately expresses the relationship be-
tween all of the arguments. Our definition of cor-
rect boundaries does not favor either system over
the other. For instance, while TEXTRUNNER ex-
tracts <Bunsen, invented, a device> from the sen-
tence ?Bunsen invented a device called the Spectro-
scope?, and SRL-IE includes the entire phrase ?a
device called the Spectroscope? as the second argu-
ment, both extractions would be marked as correct.
Determining the absolute recall in these experi-
ments is precluded by the amount of hand labeling
necessary and the ambiguity of such a task. Instead,
we compute pseudo-recall by taking the union of
correct tuples from both methods as denominator.1
5.2 Relative Performance
Table 1 shows the performance of TEXTRUNNER
and SRL-IE on this dataset. Since TEXTRUNNER
can output different points on the precision-recall
curve based on the confidence of the CRF we choose
the point that maximizes F1.
SRL-IE achieved much higher recall at substan-
tially higher precision. This was, however, at the
cost of a much larger processing time. For our
dataset, TEXTRUNNER took 6.3 minutes and SRL-
1Tuples from the two systems are considered equivalent if
for the relation and each argument, the extracted phrases are
equal or if one phrase is contained within the phrase extracted
by the other system.
55
TEXTRUNNER SRL-IE
P R F1 P R F1
Binary 51.9 27.2 35.7 64.4 85.9 73.7
N-ary 39.3 28.2 32.9 54.4 62.7 58.3
All 47.9 27.5 34.9 62.1 79.9 69.9
Time 6.3 minutes 52.1 hours
Table 1: SRL-IE outperforms TEXTRUNNER in both re-
call and precision, but has over 2.5 orders of magnitude
longer run time.
IE took 52.1 hours ? roughly 2.5 orders of magni-
tude longer. We ran our experiments on quad-core
2.8GHz processors with 4GB of memory.
It is important to note that our results for TEX-
TRUNNER are different from prior results (Banko,
2009). This is primarily due to a few operational
criteria (such as focusing on proper nouns, filtering
relatively infrequent extractions) identified in prior
work that resulted in much higher precision, proba-
bly at significant cost of recall.
5.3 Comparison under Different Conditions
Although SRL-IE has higher overall precision,
there are some conditions under which TEXTRUN-
NER has superior precision. We analyze the perfor-
mance of these two systems along three key dimen-
sions: system confidence, redundancy, and locality.
System Confidence: TEXTRUNNER?s CRF-based
extractor outputs a confidence score which can be
varied to explore different points in the precision-
recall space. Figure 1(a) and Figure 2(a) report the
results from ranking extractions by this confidence
value. For both binary and n-ary extractions the con-
fidence value improves TEXTRUNNER?s precision
and for binary the high precision end has approxi-
mately the same precision as SRL-IE. Because of
its use of an integer linear program, SRL-IE does
not associate confidence values with extractions and
is shown as a point in these figures.
Redundancy: In this experiment we use the re-
dundancy of extractions as a measure of confidence.
Here redundancy is the number of times a relation
has been extracted from unique sentences. We com-
pute redundancy over normalized extractions, ignor-
ing noun modifiers, adverbs, and verb inflection.
Figure 1(b) and Figure 2(b) display the results for
binary and n-ary extractions, ranked by redundancy.
We use a log scale on the x-axis since high redun-
dancy extractions account for less than 1% of the
recall. For binary extractions, redundancy improved
TEXTRUNNER?s precision significantly, but at a dra-
matic loss in recall. TEXTRUNNER achieved 0.8
precision with 0.001 recall at redundancy of 10 and
higher. For highly redundant information (common
concepts, etc.) TEXTRUNNER has higher precision
than SRL-IE and would be the algorithm of choice.
In n-ary relations for TEXTRUNNER and in binary
relations for SRL-IE, redundancy actually hurts
precision. These extractions tend to be so specific
that genuine redundancy is rare, and the highest fre-
quency extractions are often systematic errors. For
example, the most frequent SRL-IE extraction was
<nothing, write, home>.
Locality: Our experiments with TEXTRUNNER led
us to discover a new validation scheme for the ex-
tractions ? locality. We observed that TEXTRUN-
NER?s shallow features can identify relations more
reliably when the arguments are closer to each other
in the sentence. Figure 1(c) and Figure 2(c) report
the results from ranking extractions by the number
of tokens that separate the first and last arguments.
We find a clear correlation between locality and
precision of TEXTRUNNER, with precision 0.77 at
recall 0.18 for TEXTRUNNER where the distance is
4 tokens or less for binary extractions. For n-ary re-
lations, TEXTRUNNER can match SRL-IE?s preci-
sion of 0.54 at recall 0.13. SRL-IE remains largely
unaffected by locality, probably due to the parsing
used in SRL.
6 A TEXTRUNNER SRL-IE Hybrid
We now present two hybrid systems that combine
the strengths of TEXTRUNNER (fast processing time
and high precision on a subset of sentences) with the
strengths of SRL-IE (higher recall and better han-
dling of long-range dependencies). This is set in a
scenario where we have a limited budget on com-
putational time and we need a high performance ex-
tractor that utilizes the available time efficiently.
Our approach is to run TEXTRUNNER on all sen-
tences, and then determine the order in which to pro-
cess sentences with SRL-IE. We can increase preci-
sion by filtering out TEXTRUNNER extractions that
are expected to have low precision.
56
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
1e?04 1e?03 1e?02 1e?01 1e+00
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
Figure 1: Ranking mechanisms for binary relations. (a) The confidence specified by the CRF improves TEXTRUN-
NER?s precision. (b) For extractions with highest redundancy, TEXTRUNNER has higher precision than SRL-IE. Note
the log scale for the x-axis. (c) Ranking by the distance between arguments gives a large boost to TEXTRUNNER?s
precision.
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
1e?04 1e?03 1e?02 1e?01 1e+00
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
Figure 2: Ranking mechanisms for n-ary relations. (a) Ranking by confidence gives a slight boost to TEXTRUNNER?s
precision. (b) Redundancy helps SRL-IE, but not TEXTRUNNER. Note the log scale for the x-axis. (c) Ranking by
distance between arguments raises precision for TEXTRUNNER and SRL-IE.
A naive hybrid will run TEXTRUNNER over all
the sentences and use the remaining time to run
SRL-IE on a random subset of the sentences and
take the union of all extractions. We refer to this
version as RECALLHYBRID, since this does not lose
any extractions, achieving highest possible recall.
A second hybrid, which we call PRECHYBRID,
focuses on increasing the precision and uses the fil-
ter policy and an intelligent order of sentences for
extraction as described below.
Filter Policy for TEXTRUNNER Extractions: The
results from Figure 1 and Figure 2 show that TEX-
TRUNNER?s precision is low when the CRF confi-
dence in the extraction is low, when the redundancy
of the extraction is low, and when the arguments are
far apart. Thus, system confidence, redundancy, and
locality form the key factors for our filter policy: if
the confidence is less than 0.5 and the redundancy
is less than 2 or the distance between the arguments
in the sentence is greater than 5 (if the relation is
binary) or 8 (if the relation is n-ary) discard this tu-
ple. These thresholds were determined by a param-
eter search over a small dataset.
Order of Sentences for Extraction: An optimal
ordering policy would apply SRL-IE first to the sen-
tences where TEXTRUNNER has low precision and
leave the sentences that seem malformed (e.g., in-
complete sentences, two sentences spliced together)
for last. As we have seen, the distance between the
first and last argument is a good indicator for TEX-
TRUNNER precision. Moreover, a confidence value
of 0.0 by TEXTRUNNER?s CRF classifier is good ev-
idence that the sentence may be malformed and is
unlikely to contain a valid relation.
We rank sentences S in the following way, with
SRL-IE processing sentences from highest ranking
to lowest: if CRF.confidence = 0.0 then S.rank = 0,
else S.rank = average distance between pairs of ar-
guments for all tuples extracted by TEXTRUNNER
from S.
While this ranking system orders sentences ac-
cording to which sentence is likely to yield maxi-
mum new information, it misses the cost of compu-
tation. To account for computation time, we also
estimate the amount of time SRL-IE will take to
process each sentence using a linear model trained
on the sentence length. We then choose the sentence
57
that maximizes information gain divided by compu-
tation time.
6.1 Properties of Hybrid Extractors
The choice between the two hybrid systems is a
trade-off between recall and precision: RECALLHY-
BRID guarantees the best recall, since it does not lose
any extractions, while PRECHYBRID is designed to
maximize the early boost in precision. The evalua-
tion in the next section bears out these expectations.
6.2 Evaluation of Hybrid Extractors
Figure 3(a) and Figure 4(a) report the precision of
each system for binary and n-ary extractions mea-
sured against available computation time. PRECHY-
BRID starts at slightly higher precision due to our
filtering of potentially low quality extractions from
TEXTRUNNER. For binary this precision is even
better than SRL-IE?s. It gradually loses precision
until it reaches SRL-IE?s level. RECALLHYBRID
improves on TEXTRUNNER?s precision, albeit at a
much slower rate and remains worse than SRL-IE
and PRECHYBRID throughout.
The recall for binary and n-ary extractions are
shown in Figure 3(b) and Figure 4(b), again mea-
sured against available time. While PRECHYBRID
significantly improves on TEXTRUNNER?s recall, it
does lose recall compared to RECALLHYBRID, es-
pecially for n-ary extractions. PRECHYBRID also
shows a large initial drop in recall due to filtering.
Lastly, the gains in precision from PRECHYBRID
are offset by loss in recall that leaves the F1 mea-
sure essentially identical to that of RECALLHYBRID
(Figures 3(c),4(c)). However, for a fixed time bud-
get both hybrid F-measures are significantly bet-
ter than TEXTRUNNER and SRL-IE F-measures
demonstrating the power of the hybrid extractors.
Both methods reach a much higher F1 than TEX-
TRUNNER: a gain of over 0.15 in half SRL-IE?s
processing time and over 0.3 after the full process-
ing time. Both hybrids perform better than SRL-IE
given equal processing time.
We believe that most often constructing a higher
quality database of facts with a relatively lower
recall is more useful than vice-versa, making
PRECHYBRID to be of wider applicability than RE-
CALLHYBRID. Still the choice of the actual hybrid
extractor could change based on the task.
7 Related Work
Open information extraction is a relatively recent
paradigm and hence, has been studied by only a
small number of researchers. The most salient is
TEXTRUNNER, which also introduced the model
(Banko et al, 2007; Banko and Etzioni, 2008).
A version of KNEXT uses heuristic rules and syn-
tactic parses to convert a sentence into an unscoped
logical form (Van Durme and Schubert, 2008). This
work is more suitable for extracting common sense
knowledge as opposed to factual information.
Another Open IE system, Kylin (Weld et al,
2008), suggests automatically building an extractor
for each relation using self-supervised training, with
training data generated using Wikipedia infoboxes.
This work has the limitation that it can only extract
relations expressed in Wikipedia infoboxes.
A paradigm related to Open IE is Preemptive IE
(Shinyama and Sekine, 2006). While one goal of
Preemptive IE is to avoid relation-specificity, Pre-
emptive IE does not emphasize Web scalability,
which is essential to Open IE.
(Carlson et al, 2009) presents a semi-supervised
approach to information extraction on the Web. It
learns classifiers for different relations and couples
the training of those classifiers with ontology defin-
ing constraints. While we attempt to learn unknown
relations, it learns a pre-defined set of relations.
Another related system is WANDERLUST (Akbik
and Bro?, 2009). The authors of this system anno-
tated 10,000 sentences parsed with LinkGrammar,
resulting in 46 general linkpaths as patterns for rela-
tion extraction. With these patterns WANDERLUST
extracts binary relations from link grammar link-
ages. In contrast to our approaches, this requires a
large set of hand-labeled examples.
USP (Poon and Domingos, 2009) is based on
Markov Logic Networks and attempts to create a
full semantic parse in an unsupervised fashion. They
evaluate their work on biomedical text, so its appli-
cability to general Web text is not yet clear.
8 Discussion and Future Work
The Heavy Tail: It is well accepted that informa-
tion on the Web is distributed according to Zipf?s
58
0 10 20 30 40 50
0.
0
0.
2
0.
4
0.
6
Time (hours)
P
re
ci
si
on
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
R
ec
al
l
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
F?
m
ea
su
re
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
Figure 3: (a) Precision for binary extractions for PRECHYBRID starts higher than the precision of SRL-IE. (b) Recall
for binary extractions rises over time for both hybrid systems, with PRECHYBRID starting lower. (c) Hybrid extractors
obtain the best F-measure given a limited budget of computation time.
0 10 20 30 40 50
0.
0
0.
2
0.
4
0.
6
Time (hours)
P
re
ci
si
on
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
R
ec
al
l
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
F?
m
ea
su
re
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
Figure 4: (a) PRECHYBRID also gives a strong boost to precision for n-ary extractions. (b) Recall for n-ary extractions
for RECALLHYBRID starts substantially higher than PRECHYBRID and finally reaches much higher recall than SRL-
IE alone. (c) F-measure for n-ary extractions. The hybrid extractors outperform others.
Law (Downey et al, 2005), implying that there is a
heavy tail of facts that are mentioned only once or
twice. The prior work on Open IE ascribes prime
importance to redundancy based validation, which,
as our results show (Figures 1(b), 2(b)), captures a
very tiny fraction of the available information. We
believe that deeper processing of text is essential to
gather information from this heavy tail. Our SRL-
IE extractor is a viable algorithm for this task.
Understanding SRL Components: UIUC-SRL
as well as other SRL algorithms have different sub-
components ? parsing, argument classification, joint
inference, etc. We plan to study the effective con-
tribution of each of these components. Our hope is
to identify the most important subset, which yields
a similar quality at a much reduced computational
cost. Another alternative is to add the best perform-
ing component within TEXTRUNNER.
9 Conclusions
This paper investigates the use of semantic features,
in particular, semantic role labeling for the task of
open information extraction. We describe SRL-IE,
the first SRL based Open IE system. We empirically
compare the performance of SRL-IE with TEX-
TRUNNER, a state-of-the-art Open IE system and
find that on average SRL-IE has much higher re-
call and precision, however, TEXTRUNNER outper-
forms in precision for the case of highly redundant
or high locality extractions. Moreover, TEXTRUN-
NER is over 2 orders of magnitude faster.
These complimentary strengths help us design hy-
brid extractors that achieve better performance than
either system given a limited budget of computation
time. Overall, we provide evidence that, contrary to
belief in the Open IE literature (Banko and Etzioni,
2008), semantic approaches have a lot to offer for
the task of Open IE and the vision of machine read-
ing.
10 Acknowledgements
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750-09-C-0179, and carried
out at the University of Washington?s Turing Cen-
ter.
59
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM International Con-
ference on Digital Libraries.
Alan Akbik and Ju?gen Bro?. 2009. Wanderlust: Extract-
ing semantic relations from natural language text us-
ing dependency grammar patterns. In Proceedings of
the Workshop on Semantic Search (SemSearch 2009)
at the 18th International World Wide Web Conference
(WWW 2009).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI?07: Pro-
ceedings of the 20th international joint conference on
Artifical intelligence, pages 2670?2676.
Michele Banko. 2009. Open Information Extraction for
the Web. Ph.D. thesis, University of Washington.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka
Jr., and Tom M. Mitchell. 2009. Coupling semi-
supervised learning of categories and relations. In
Proceedings of the NAACL HLT 2009 Workskop on
Semi-supervised Learning for Natural Language Pro-
cessing.
Bonaventura Coppola, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Shallow semantic parsing
for spoken language understanding. In NAACL ?09:
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 85?88.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI ?05: Proceedings of the
20th international joint conference on Artifical intelli-
gence, pages 1034?1041.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine reading. In AAAI?06: proceedings of
the 21st national conference on Artificial intelligence,
pages 1517?1519.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role label-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 393?400.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In In Proceedings of LREC-
2002.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP ?09: Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1?10.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1-3):233?272.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Benjamin Van Durme and Lenhart Schubert. 2008. Open
knowledge extraction through compositional language
processing. In STEP ?08: Proceedings of the 2008
Conference on Semantics in Text Processing, pages
239?254.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2008.
Using wikipedia to bootstrap open information extrac-
tion. SIGMOD Rec., 37(4):62?68.
60
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	