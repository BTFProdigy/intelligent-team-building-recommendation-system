First Joint Conference on Lexical and Computational Semantics (*SEM), pages 487?492,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ANNLOR: A Na??ve Notation-system for Lexical Outputs Ranking
Anne-Laure Ligozat
LIMSI-CNRS/ENSIIE
rue John von Neumann
91400 Orsay, France
annlor@limsi.fr
Cyril Grouin
LIMSI-CNRS
rue John von Neumann
91400 Orsay, France
cyril.grouin@limsi.fr
Anne Garcia-Fernandez
CEA-LIST
NANO INNOV, Bt. 861
91191 Gif-sur-Yvette cedex, France
anne.garcia-fernandez@cea.fr
Delphine Bernhard
LiLPa, Universite? de Strasbourg
22 rue Rene? Descartes, BP 80010
67084 Strasbourg cedex, France
dbernhard@unistra.fr
Abstract
This paper presents the systems we developed
while participating in the first task (English
Lexical Simplification) of SemEval 2012. Our
first system relies on n-grams frequencies
computed from the Simple English Wikipedia
version, ranking each substitution term by de-
creasing frequency of use. We experimented
with several other systems, based on term fre-
quencies, or taking into account the context in
which each substitution term occurs. On the
evaluation corpus, we achieved a 0.465 score
with the first system.
1 Introduction
In this paper, we present the methods we used while
participating to the Lexical Simplification task at Se-
mEval 2012 (Specia et al, 2012). We experimented
with several methods:
? using word frequencies or other statistical fig-
ures from the BNC corpus, Google Books
NGrams, the Simple English Wikipedia, and
results from the Bing search engine (with/with-
out lemmatization);
? using association measures for a word and its
context based on language models (with/with-
out inflection);
? making a combination of previous methods
with SVMRank.
Depending on the results obtained on the training
corpus, we chose the methods that seemed to best fit
the data.
2 Task description
2.1 Presentation
The Lexical Simplification task aimed at determin-
ing the degree of simplicity of words. The inputs
given were a short text, in which a target word was
chosen, and several substitutes for the target word
that fit the context.
An example of a short text follows; the target
word is ?outdoor?, and other words of this text will
be considered as the context of this target word.
< i n s t a n c e i d =? 270 ?>
<c o n t e x t>With t h e growing demand f o r
t h e s e f i n e g a r de n f u r n i s h i n g s ,
t h e y found i t n e c e s s a r y t o d e d i c a t e
a p o r t i o n o f t h e i r b u s i n e s s t o
<head>o u t d o o r< / head> l i v i n g and
p a t i o f u r n i s h i n g s .< / c o n t e x t>
< / i n s t a n c e>
The substitutes given for this target word were
the following: ?alfresco;outside;open-air;outdoor;?.
The objective was to order these words by descend-
ing simplicity.
2.2 Corpora
Two corpora were provided: the trial corpus with
development examples, and the test corpus for eval-
uation.
In the trial corpus, a gold standard was also given.
For the previous example, it stated that the substi-
tutes had to be in the following order: ?outdoor
open-air outside, alfresco?, ?outdoor? being consid-
ered as the simplest substitute, and ?outside? and
?alfresco? being considered as the less simple ones.
487
Three baselines have been given by the organiz-
ers: the first one is a simple randomization of the
substitute list, the second one keeps the substitute
list as it is, and the third one (called ?simple fre-
quency?) relies on the use of the Google Web 1T
corpus.
3 Preprocessing
3.1 Corpus constitution
In order to use machine-learning based approaches,
we produced two sub-corpora respectively for the
training and evaluation stages from the trial corpus.
The training sub-corpus is used to develop and tune
the systems we produced while the evaluation sub-
corpus is used to evaluate the results of these sys-
tems.
For each set from the SemEval trial corpus, if the
set is composed of at least eight lexical elements be-
longing to the same morpho-syntactic category (e.g.,
a set with at least eight instances of ?bright? as an
adjective), we extracted three instances from this
set for the evaluation sub-corpus, the remaining in-
stances being part of the training sub-corpus. If the
set is composed of less than eight instances, all in-
stances are used in the training sub-corpus. We also
kept two complete sets of lexical elements for the
evaluation sub-corpus in order to test the robustness
of our methods on new lexical elements that have not
been studied yet. This distribution allows us to bene-
fit from a repartition between training and evaluation
sub-corpora where the instances ratio is of 66/33%.
3.2 Corpus cleaning
While studying the trial corpus, we noticed that the
texts were not always in plain text, and in particular
contained HTML entities. As some of our methods
used the context of target words, we decided to cre-
ate a cleaner version of the corpora. For the dash and
quote HTML entities (&#8211; &#8220; etc.), we
replaced each entity by its refering symbol. When
replacing the apostrophe HTML entity (&apos;), we
decided to link the abbreviated token with the previ-
ous one because all n-grams methods worked better
with abbreviated terms of one token-length (don?t)
than two token-length (do n?t) (see section 5).
3.3 Inflection
In some sentences, the target words are inflected, but
the substitutes are given in their lemmatized forms.
For example, one of the texts was the following :
<c o n t e x t>In f a c t , d u r i n g a t l e a s t s i x
d i s t i n c t p e r i o d s i n Army h i s t o r y
s i n c e World War I , l a c k o f t r u s t and
c o n f i d e n c e i n s e n i o r l e a d e r s c au se d
t h e so?c a l l e d b e s t and
<head>b r i g h t e s t< / head> t o l e a v e t h e
Army i n d r o v e s .< / c o n t e x t>
For this text and target word, the proposed sub-
stitutes were ?capable; most able; motivated; in-
telligent; bright; clever; sharp; promising?, and if
we want to test the simplicity of the words in con-
text, for example with a 2-words left context, we
will obtain unlikely phrases such as ?best and capa-
ble? (which should be ?best and most capable?). We
thus used several resources to get inflected forms of
words: we used the Lingua::EN::Conjugate and Lin-
gua::EN::Inflect Perl modules, which give inflected
forms of verbs and plural forms of nouns, as well as
the English dictionary of inflected forms DELA,1 to
validate the Perl modules outputs if necessary, and
get comparatives and superlatives of adjectives, and
a list of irregular English verbs, also to validate the
Perl modules outputs.
4 Simple English Wikipedia based system
Our best system, called ANNLOR-simple, is based
on Simple English Wikipedia frequencies. As the
challenge focused on substitutions performed by
non-native English speakers, we tried to use linguis-
tic resources that best fit this kind of data. In this
way, we made the hypothesis that training our sys-
tem on documents written by or written for non-
native English speakers would be useful.
The use of the Simple English version from
Wikipedia seems to be a good solution as it is tar-
geted at people who do not have English as their
mother tongue. Our hypothesis seems to be correct
due to the results we obtained. Morevover, the Sim-
ple English Wikipedia has been used previously in
work on automatic text simplification, e.g. (Zhu et
al., 2010).
1http://infolingu.univ-mlv.fr/
DonneesLinguistiques/Dictionnaires/
telechargement.html
488
First, we produced a plain text version of the Sim-
ple English Wikipedia. We downloaded the dump
dated February 27, 2012 and extracted the textual
contents using the wikipedia2text tool.2 The
final plaintext file contains approximately 10 million
words.
We extracted word n-grams (n ranging from 1 to
3) and their frequencies from this corpus thanks to
the Text-NSP Perl module 3 and its count.pl pro-
gram, which produces the list of n-grams of a docu-
ment, with their frequencies. Table 1 gives the num-
ber of n-grams produced.
Table 1: Number of distinct n-grams extracted from the
Simple English Wikipedia
n #n-grams
1 301,718
2 2,517,394
3 6,680,906
1 to 3 9,500,018
Some of these n-grams are invalid, and result
from problems when extracting plain text from
Wikipedia, such as ?27|ufc 1?, which corresponds
to wiki syntax. As we would not find these n-grams
in our substitution lists, we did not try to clean the
n-gram data.
Then, we ranked the possible substitutes of a lex-
ical item according to these frequencies, in descend-
ing order. For example, for the substitution list (in-
telligent, bright, clever, smart), the respective fre-
quencies in the Simple English Wikipedia are (206,
475, 141, 201), and the substitutes will be ranked in
descending frequencies: (bright, intelligent, smart,
clever).
Several tests were conducted, with varying pa-
rameters. We used the plain text version of the Sim-
ple English Wikipedia, but also tried to lemmatize it,
since substitutes are lemmatized. We used the Tree-
Tagger 4 (Schmid, 1994) and applied it on the whole
2See http://www.polishmywriting.com/
download/wikipedia2text\_rsm\_mods.tgz
and http://blog.afterthedeadline.com/
2009/12/04/generating-a-plain-text-corpus
-from-wikipedia
3http://search.cpan.org/?tpederse/
Text-NSP-1.25/lib/Text/NSP.pm
4http://www.ims.uni-stuttgart.de/
corpus, before counting n-grams. Moreover, since
bigrams and trigrams increase a lot, the size of n-
gram data, we evaluated their influence on results.
These tests are summed up in table 2.
Table 2: Results obtained with the Simple English
Wikipedia based system, on the trial and test corpora
reference lemmas score on score on
n-grams trial corpus test corpus
1-grams only no 0.333 ?
1 and 2-grams no 0.371 ?
1 to 3-grams no 0.381 0.465
1 to 3-grams yes 0.380 0.462
Simple Frequency
0.398 0.471
baseline
WLV-SHEF-SimpLex
(best system ? 0.496
@SemEval2012)
With unigrams only, 158 substitutes of the trial
corpus are absent of the reference dataset, 105 when
adding bigrams, and 91 when adding trigrams. Most
of the missing n-grams (when using 1 to 3-grams)
indeed seem to be very uncommon, such as ?undo-
mesticated? or ?telling untruths?.
The small difference between the lemmatized and
inflected versions of Wikipedia is due to two rea-
sons: some substitutes are found in the lemmatized
version because substitutes are given in the lemma-
tized form (for example ?abnormal growth? is only
present in its plural form ?abnormal growths? in
the inflected Wikipedia); and some other substitutes
are missing in the lemmatized version, mostly be-
cause of errors from the TreeTagger (for example
?be scared of? becomes ?be scare of?).
We kept the system that obtained the best scores
on the trial corpus, that is with 1 to 3-grams and non-
lemmatized n-grams, with a score of 0.381. This
system obtained a score of 0.465 on the evalua-
tion corpus, thus ranking second ex-aequo at the Se-
mEval evaluation.
projekte/corplex/TreeTagger/
489
5 Other frequency-based methods
We tried several other reference corpora, always
with the idea that the more frequent a word is, the
simpler it is. We used the BNC corpus,5 as well
as the Google Books NGrams.6 These NGrams
were calculated on the books digitized by Google,
and contain for each encountered n-gram, its num-
ber of occurrences for a given year. As the Google
Books NGrams are quite voluminous, we selected a
random year (2008), and kept only alphabetical n-
grams with potential hyphens, and used n-grams for
n ranging from 1 to 4. The dataset used contains
477,543,736 n-grams.
We also used the Microsoft Web N-gram Service
(more details on this service are given in the fol-
lowing section) to rank substitutes in descending or-
der. The results of these methods on the trial corpus
are given in table 3. The result of the simple fre-
quency baseline is also given: this baseline is also
frequency-based, but words are ranked according to
the number of hits found when querying the Google
Web 1T corpus with each substitute.
Table 3: Results obtained with frequency-based methods,
on the trial corpus
reference corpus score
BNC 0.347
Google Books NGrams 0.367
Microsoft NGrams 0.383
Simple Frequency baseline 0.398
This table shows that all frequency-based meth-
ods have lower scores than the Simple Frequency
baseline, although the score obtained with the Mi-
crosoft NGrams is quite close to the baseline. The
results from Microsoft Ngrams and the Simple En-
glish are very close. We decided to submit the Sim-
ple English Wikipedia-based system because it was
more different from the simple frequency baseline.
6 Contextual methods
We also wanted to use contextual information, since,
according to the contexts of the target word, dif-
ferent substitutes can be used, or ranked differ-
5http://www.natcorp.ox.ac.uk/
6http://books.google.com/ngrams/datasets
ently. In the following two examples, the same word
?film? is targetted, and the same substitutes are pro-
posed ?film;picture;movie;?; yet, in the gold stan-
dard, ?film? is placed before ?movie? in instance 19,
and after it in instance 15.
< i n s t a n c e i d =? 15 ?>
<c o n t e x t>Film Music L i t e r a t u r e
C y b e r p l a c e ? I n c l u d e s
<head>f i l m< / head> r e v i e w s , message
b o a r d s , c h a t room , and images
from v a r i o u s f i l m s .< / c o n t e x t>
< / i n s t a n c e>
( . . . )
< i n s t a n c e i d =? 19 ?>
<c o n t e x t>A f i n e s c o r e by George Fen ton
( THE CRUCIBLE ) and b e a u t i f u l
p h o t o g r a h y by Roger P r a t t add
g r e a t l y t o t h e e f f e c t i v e n e s s o f t h e
<head>f i l m< / head> .< / c o n t e x t>
< / i n s t a n c e>
Ranking substitutes thus depends on the context
of the target word. We implemented two systems
taking the context of target words into account.
6.1 Language model probabilities
The other system submitted (called ANNLOR-
lmbing) relies on language models, which was the
method used by the organizers in their Simple Fre-
quency baseline. While the organizers used Google
n-grams to rank terms to be substituted by decreas-
ing frequency of use, we used Microsoft Web n-
grams in the same way. Nevertheless, we also added
the contexts of each term to be substituted.
We used the Microsoft Web N-gram Service7 to
obtain joint probability for text units, and more
precisely its Python library.8 We used the bing-
body/apr10/ ) N-Gram model.
We considered a text unit composed of the lexi-
cal item and a contextual window of 4 words to the
left and 4 words to the right (words being separated
by spaces). For example, in the following sentence,
we tested ?He brings an incredibly rich and diverse
background that?, and the same unit with the tar-
get word replaced by substitutes, for example ?He
brings an incredibly lush and diverse background
that?.
7http://research.microsoft.com/en-us/
collaboration/focus/cs/web-ngram.aspx
8http://web-ngram.research.microsoft.
com/info/MicrosoftNgram-1.02.zip
490
< i n s t a n c e i d =? 118 ?>
<c o n t e x t>He b r i n g s an i n c r e d i b l y
<head> r i c h< / head> and d i v e r s e
background t h a t i n c l u d e s e v e r y t h i n g
from e x e c u t i v e c o a c h i n g , l e a r n i n g
&amp ; deve lopmen t and management
c o n s u l t i n g , t o s e n i o r o p e r a t i o n s
r o l e s , mixed wi th a m a s t e r s i n
o r g a n i z a t i o n a l
deve lopmen t .< / c o n t e x t>
< / i n s t a n c e>
We performed several tests, with different N-
Gram models, and different context sizes. Some of
these results for the trial corpus are given in table 4.
Table 4: Results obtained with Microsoft Web N-gram
Service, on the trial corpus
Size of left context Size of right context Score
0 3 0.362
3 0 0.358
2 2 0.365
3 3 0.358
4 4 0.370
For the evaluation, this system was our second
run, with the parameters that obtained the best scores
on the training corpus (contexts of 4 words to the
left and to the right). This method obtained a 0.370
score on the trial corpus and a 0.396 score on the test
corpus.9
7 Combination of methods
As each method seemed to have its own benefits, we
tried to combine them using SVMRank 10(Joachims,
2006). The output of each system is converted into
a feature file. For example, the output of the Simple
English Wikipedia based system begins with:
1 bright 475 1
1 intelligent 206 2
1 smart 201 3
1 clever 141 4
2 light 3241 1
2 clear 707 2
9This result is different from the official one, because an
incorrect file was submitted at the time.
10http://www.cs.cornell.edu/people/tj/
svm_light/svm_rank.html
2 bright 475 3
2 luminous 14 4
2 well-lit 0 5
The first column represent the instance id, the sec-
ond one the considered substitute, the third one the
feature (in this case, the frequency of the substitute
in the Simple English Wikipedia), and the last one,
the substitute rank according to this method. Then,
we combined these files to include all features (after
basic query-wise feature scaling). For example, the
training file begins with:
1 qid:1 2:-0.00461061395325929
3:0.0345010535723618
#intelligent
2 qid:1 2:-0.00485010755325339
3:-0.0213467053270483 #clever
3 qid:1 2:-0.00462903653787422
3:0.092640777900771 #smart
4 qid:1 2:-0.00361947890097599
3:0.0489145618699556 #bright
1 qid:4 2:-0.00461061395325929
3:0.0345010535723618
#intelligent
The first column gives the gold standard rank for
the substitute (in training phase), the second one the
instance id, and then feature ids and values for each
substitute. Default parameters were used.
We used the division of the trial corpus into a
training corpus and a development corpus. Table 5
gives some examples of scores obtained by combin-
ing two methods. The scores are not exactly those
presented earlier, since they correspond to a part of
the trial corpus only. Even though some improve-
ment can be obtained by this combination, it was
quite small, and so we did not use it for the evalua-
tion.
Table 5: Results obtained with combination of methods
with SVMRank, on the trial corpus
Simple English Microsoft
SVM
Wikipedia NGrams
0.352 0.352 0.354
491
8 Conclusion
In this paper, we present several systems developed
for the English Lexical Simplification task of Se-
mEval 2012. The best results are obtained using fre-
quencies from the Simple English Wikipedia. We
found the task quite hard to solve, since none of
our experiments significantly outperforms the Sim-
ple Frequency baseline. On the trial corpus, our
system based upon the Simple English Wikipedia
achieved a score of 0.381 (below the 0.399 base-
line score); on the test corpus, we achieved a score
of 0.465 with the Simple English Wikipedia system
while the baseline achieved a score of 0.471 score.
All our systems using contextual information did not
achieve high scores.
References
Thorsten Joachims. 2006. Training Linear SVMs in Lin-
ear Time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proc. of the Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012.
SemEval-2012 Task 1: English Lexical Simplification.
In Proc. of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), Montre?al, Canada.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A Monolingual Tree-based Translation Model
for Sentence Simplification. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING 2010), pages 1353?1361.
492
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 17?23
Manchester, August 2008
Human judgment as a parameter in evaluation campaigns
Jean-Baptiste Berthelin and Cyril Grouin and Martine Hurault-Plantet and Patrick Paroubek
LIMSI-CNRS
BP 133
F-91403 Orsay Cedex
firstname.lastname@limsi.fr
Abstract
The relevance of human judgment in an
evaluation campaign is illustrated here
through the DEFT text mining campaigns.
In a first step, testing a topic for a cam-
paign among a limited number of human
evaluators informs us about the feasibility
of a task. This information comes from the
results obtained by the judges, as well as
from their personal impressions after pass-
ing the test.
In a second step, results from individual
judges, as well as their pairwise matching,
are used in order to adjust the task (choice
of a marking scale for DEFT?07 and selec-
tion of topical categories for DEFT?08).
Finally, the mutual comparison of com-
petitors? results, at the end of the evalu-
ation campaign, confirms the choices we
made at its starting point, and provides
means to redefine the task when we shall
launch a future campaign based on the
same topic.
1 Introduction
For the past four years, the DEFT1 (De?fi Fouille
de Texte) campaigns have been aiming to evalu-
ate methods and software developed by several re-
search teams in French text mining, on a variety of
topics.
The different editions concerned, in this or-
der, the identification of speakers in political
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1See http://deft.limsi.fr/ for a presentation in
French.
speeches (2005), the topical segmentation of po-
litical, scientific and juridical corpora (2006), the
automatic affectation of opinion values to texts de-
veloping an argumented judgment (2007), and the
identification of the genre and topic of a docu-
ment (2008).
Human judgment was used during the prepara-
tion of the last two campaigns, to assess the dif-
ficulty of the task, and to see which parameters
could be modified. To do this, before the partic-
ipants start competing via their software, we put
human judges in front of versions of the task with
various sets of parameters. This allows us to adjust
the definition of the task according to which diffi-
culties were encountered, and how judges agree to-
gether. These human judges are in small number,
and belong to our team. However, results of the
campaign are automatically evaluated with refer-
ence to results attached to the corpus from the start.
This is because the evaluation of a campaign?s
results by human judges is expensive. For in-
stance, TREC2 international evaluation campaigns
are supported by the NIST institute and funded by
state agencies. In Europe, on the same domains,
the CLEF3 campaigns are funded by the Euro-
pean Commission, and in France, evaluation cam-
paigns are also funded by projects, such as Tech-
nolangue4. DEFT campaigns, however, are con-
ducted with small budgets. That means for us to
have selected corpora that contain the desired re-
sults. For instance, in a campaign for topical cat-
egorization, we must start with a topically tagged
corpus. By so doing, we also can, at the end of
a campaign, compare results from human judges
with results from competitors, using an identical
2http://trec.nist.gov
3http://www.clef-campaign.org
4http://www.technolangue.net
17
common reference.
In this paper, we describe experiments we per-
formed with human judgments when preparing
DEFT campaigns. We survey the various steps
in the preparation of the last two campaigns, and
we go through the detail of how human evalua-
tion, performed during these steps, led us to the
parametrization of these two campaigns. We also
present a comparative analysis of results found by
human judges and results submitted by competi-
tors in the challenge. We conclude about the rel-
evance of the human evaluation of a task, prior to
evaluating software dedicated to this task.
2 Parametrization of the campaign
We were competitors in the 2005 and 2006 edi-
tions, and became organisators for the 2007 and
2008 campaigns. For both challenges that we or-
ganized, we went through the classical steps of the
evaluation paradigm (Adda et al, 1999), to which
we systematically added a step of human test of the
task, in order to adjust those parameters that could
be modified. The steps, therefore, are following:
1. thinking about potential topics;
2. choice of a task and collection of corpora;
3. choice of measurements;
4. test of the task by human judges on an extract
of the corpus in order to precisely define its
parameters;
5. launching the task, recruiting participants;
6. testing period;
7. adjudication: possibility of complaints about
the results;
8. workshop that closes the campaign.
Whenever human judges have to evaluate the
results of participants in a campaign, the main
problems are about correctly defining the judging
criteria to be applied by judges, and that judges
be in sufficient number to vote on judging each
document. Hovy et al (2002) describe work to-
ward formalization of software evaluation method-
ology in NLP, developed in the EAGLES5 and
5http://www.ilc.cnr.it/EAGLES96/home.
html
ISLE6 projects. For cost-efficiency reasons, au-
tomatic evaluation is relevant, and its results have
sometimes been compared to results from human
judges. For instance, Eck and Hori (2005) com-
pare results of evaluation measurements used in
automatic translation with human judgments on
the same corpus. In (Burstein and Wolska, 2003),
the authors describe an experiment in the evalua-
tion of writing style and find a better agreement
between the automatic evaluation system and one
human judge, than between two human judges.
Returning to the DEFT campaign, once the task
is chosen, the corpora are collected, and evaluation
measurements are defined, there can remain some
necessity of adjusting parameters, according to the
expected difficulty of the task. This could be, for
instance, the level of granularity in a task of top-
ical segmentation, or which categories should be
relevant in a task of categorization. To get this ad-
justing done, we submit the task to human judges.
In 2007, the challenge was about the automatic
affectation of opinion values to texts developing an
argumented judgment (Grouin et al, 2007). We
collected opinion texts already tagged by an opin-
ion value, such as film reviews that, in addition
to a text giving the judgment of the critic on the
film, also feature a mark in the shape of a variable
number of stars. The adjustable parameter of the
task, therefore, is the scale of opinion values. The
task will be more or less difficult, according to the
range of this scale.
The 2008 campaign was about classifying a set
of documents by genre and topic (Hurault-Plantet
et al, 2008). The choice of genres and topics is
a crucial one. Some pairs of topics or genres are
more difficult to separate than other ones. We also
had to find different genres sharing a set of topical
categories, while corpora in French are not so very
abundant. So we selected two genres, encyclo-
pedia and daily newspaper, and about ten general
topical categories. The parameter we had to ad-
just was the set of categories to be matched against
each other.
3 Assessing the difficulty of a task
3.1 Calibration of an opinion value scale
In 2007, the challenge was about the automatic af-
fectation of opinion values to texts developing an
argumented judgment. In view of that, we col-
lected four corpora that covered various domains:
6http://www.ilc.cnr.it/EAGLES96/isle/
18
reviews of films and books, of video games and of
scientific papers, as well as parliamentary debates
about a draft law.
Each corpus had the interesting feature of com-
bining a mark or opinion with a descriptive text, as
the mark was used to sum up the judment in the
argumentative part of this text. Due to the diver-
sity of sources, we found as many marking scales
as involved copora:
? 2 values for parliamentary debates7 (the rep-
resentative who took part in the debate was
either in favour or in disfavour of the draft
law) ;
? 4 values for scientific paper reviews (accepted
as it stands ? accepted with minor changes
? accepted with major changes and second
overall reviewing ?rejected), based on a set of
criteria including interestingness, relevance
and originality of the paper?s content ;
? 5 values for film and book reviews8 (a mark
between 0 and 4, from bad to excellent) ;
? 20 values for video game reviews9 (a global
mark calculated from a set of advices about
various aspects of the game: graphics, playa-
bility, life span, sound track and scenario).
In order to, first, assess the feasibility of the task,
and to, secondly, define the scale of values to be
used in the evaluation campaign, we submitted hu-
man judges to several tests (Paek, 2001): they were
instructed to assign a mark on two kinds of scale, a
wide one with the original values, and a restricted
one with 2 or 3 values, depending on the corpus it
was applying to. The results from various judges
were evaluated in terms of precision and recall, and
matched to each other by way of the Kappa coeffi-
cient (Carletta, 1996) (Cohen, 1960).
We present hereunder the values of the ? coef-
ficient between pairs of human judges, and with
the reference, on the video game corpus. The wide
scale (Table 1) uses the original values (marks be-
tween 0 and 20), while the restricted scale (Ta-
ble 2) relies upon 3 values with following defini-
tions: class 0 for original marks between 0 and 10,
class 1 for marks between 11 and 14, and class 2
for marks between 15 and 20.
7http://www.assemblee-nationale.fr/12/
debats/
8http://www.avoir-alire.com
9http://www.jeuxvideo.com/etajvbis.htm
Judge Ref. 1 2 3
Ref. 0.17 0.12 0.07
1 0.17 0.03 0.05
2 0.12 0.03 0.07
3 0.07 0.05 0.07
Table 1: Video game corpus: wide scale, marks
from 0 to 20.
Judge Ref. 1 2 3
Ref. 0.74 0.79 0.69
1 0.74 0.74 0.54
2 0.79 0.74 0.69
3 0.69 0.54 0.69
Table 2: Video game corpus: restricted scale,
marks from 0 to 2.
Table 1 and 2 show that agreement between
judges varies widely when marking scales are
modified. Table 1 shows that there is an insuffi-
cient agreement among judges on the wide scale,
with ? coefficients lower than 0.20, while the
agreement between these same judges can be con-
sidered as good on the restricted scale, with ? co-
efficients between 0.54 and 0.79 (Table 2), the me-
dian being at 0.74.
In order to confirm the validity of the change
in scales, we used the ? to test how each judge
agreed with himself, between his two sets of re-
sults (Table 3). Therefore, we compared judg-
ments made by each judge using the initial value
scale and converted towards the restricted scale,
with judgments made by the same judge directly
using the restricted value scale. This measurement
shows the degree of correspondence between both
scales for each judge. Among the three judges who
took part in the test, the first and third one agree
well with themselves, while for the second one, the
agreement is only moderate.
Judge 1 2 3
1 0.74
2 0.46
3 0.70
Table 3: Video game corpus: agreement of each
judge with himself when scales change.
We did the same for a second corpus, of film re-
views. The test involved five judges, and the scale
19
change was smaller, since it was from five values
to three, and not from twenty to three. For this
scale change, we merged the two lowest values (0
and 1) into one (0), and the two highest ones (3
and 4) into one (2), and the middle value in the
wide scale (2) remained the intermediate one in
the restricted scale (1). This scale change was the
most relevant one, since, with 29.7% of the docu-
ments, the class of the middle mark (2) accounted
for almost one third of the corpus. However, the
two other groups of documents are less well bal-
anced. Indeed, the lowest mark concerns less doc-
uments than the highest one: 4.6% and 10.3% re-
spectively for the initial marks 0 and 1, while one
finds 39.8% and 15.6% of documents for the marks
3 and 4. Grouping the documents in only two
classes, by joining the middle class with the two
lowest ones, would have yielded a better balance
between classes, with 44.6% of documents for the
lower mark and 55.4% for the higher one, but that
would have been less meaningful.
Results from human judges are shown in the Ta-
bles 4 and 5 for both scales.
Judge Ref. 1 2 3 4 5
Ref. 0.10 0.29 0.39 0.46 0.47
1 0.10 0.37 0.49 0.48 0.35
2 0.29 0.37 0.36 0.30 0.43
3 0.39 0.49 0.36 0.49 0.54
4 0.46 0.48 0.30 0.49 0.60
5 0.47 0.35 0.43 0.54 0.60
Table 4: Film review corpus: wide scale, marks
from 0 to 4
Judge Ref. 1 2 3 4 5
Ref. 0.27 0.62 0.53 0.56 0.67
1 0.27 0.45 0.43 0.57 0.37
2 0.62 0.45 0.73 0.48 0.54
3 0.53 0.43 0.73 0.62 0.62
4 0.56 0.57 0.48 0.62 0.76
5 0.67 0.37 0.54 0.62 0.76
Table 5: Film review corpus: restricted scale,
marks from 0 to 2.
Agreements between human judges ranked from
bad to moderate for the wide scale (the five origi-
nal values in this corpus), while these agreements
rank from insufficient to good in the case of the
restricted scale with three values. We can see that
differences induced by the scale change are much
less important than with the video game corpus.
This agrees well with the scales being much closer
to each other.
By first performing a hand-made evaluation, and
secondly, matching between themselves the results
from the judges, we found a way to assess with
greater precision the difficulty of the evaluation
task we were about to launch. Concerning the
first two review corpora (films and books, video
games), we attached values good, average and bad
to the three selected classes. The scale for sci-
entific paper reviews was also restricted to three
classes for which following values were selected:
paper accepted as it stands or with minor edits, pa-
per accepted after major edits, paper rejected. Fi-
nally, since its original scale had only two values,
the corpus of parliamentary debates underwent no
change of scale.
3.2 Choice of a topical category set
In order to determine which topical categories
should be recognized in the 2008 task of classify-
ing documents by genre and topic, we performed a
manual evaluation of a sample of the corpus with 4
human judges. The sample included 30 Le Monde
papers for the journalistic genre, and 30 Wikipedia
entries for the encyclopedic genre. Only the title
and body of each article was kept in the sample,
and the tables were deleted. All marks of inclu-
sion in either corpus were also deleted (references
to Le Monde and Wikipedia tags).
The test ran this way: each article was put in a
separate file, and the evaluators had to identify the
genre and the topical category under which it was
published. All articles were included in one set,
which means evaluators had to choose, between all
categories and genres, which ones to match with
each document. This test was made with a first
selection of 8 categories, shared by both genres,
listed in Table 6.
Table 7 shows that results from human judges
in terms of precision and recall were excellent on
the identification of genre (F-scores between 0.94
and 1.00) and quite good on the identification of
categories (F-scores between 0.66 and 0.82).
We also proceeded to the pairwise matching of
results from human judges via the ? coefficient.
Results show an excellent agreement of judges
among themselves and with the reference for genre
identification (Table 8). The agreement is mod-
20
Le Monde Wikipedia
Notebook People
Economy Economy
France French Politics
International International Politics,
minus category
French Politic
Science Science
Society Society,
minus subcategories
Politics, People,
Sport, Media
Sport Sport
Television Television
Table 6: Correspondence between categories from
Le Monde and Wikipedia for the 8 categories in
the test.
Judge 1 2 3 4
Genres 1.00 0.98 0.97 0.94
Categories 0.79 0.77 0.82 0.66
Table 7: F-scores obtained by human judges on the
identification of genre and categories.
erate to good for categoy identification (Table 9).
These good results led us to keep the corpora as
they stood, since they appeared to constitute a
good reference for the defined task. However, we
made an exception for category Notebook (biogra-
phies of celebrities) which we discarded for two
reasons. First, it is more of a genre, namely, ?bi-
ography?, rather than a topical category. Secondly,
we found it rather difficult to assign a single cate-
gory to articles which could belong in two different
ones, as would be the case for the biography of a
sportsman, which would fall under both categories
Notebook et Sport.
Judge Re?f. 1 2 3 4
Re?f. 1.00 0.97 0.93 0.87
1 1.00 0.97 0.93 0.87
2 0.97 0.97 0.90 0.83
3 0.93 0.93 0.90 0.87
4 0.87 0.87 0.83 0.87
Table 8: ? coefficient between human judges and
the reference: Identification of genre.
Our task of genre and topic classification in-
Judge Re?f. 1 2 3 4
Re?f. 0.56 0.52 0.60 0.39
1 0.56 0.69 0.75 0.55
2 0.52 0.69 0.71 0.61
3 0.60 0.75 0.71 0.52
4 0.39 0.55 0.61 0.52
Table 9: ? coefficient between human judges and
the reference: Identification of categories.
cluded two subtasks, one being genre and topic
recognition for a first set of categories, the other
one being only topic recognition for a second set
of categories. Therefore, the corpus had to be di-
vided in two parts. In order to find which cate-
gories had to go into which subcorpus, we decided
to estimate, for each category, the difficulty of rec-
ognizing it. To do so, we calculated the precision
and recall of each evaluator for each category. This
measurement was obtained via a second evaluation
of human judges, with a wider set of categories (by
adding categories Art and Literature).
The ordering of categories by decreasing pre-
cision is following: Sport (1.00), International
(0.80), France (0.76), Literature (0.76), Art (0.74),
Television (0.71), Economy (0.58), Science (0.33),
Society (0.26). This means no document in the
Sport category was misclassified, and, contrari-
wise, categories Science and Society were the most
problematic ones.
The ordering by decreasing recall is slightly
different: International (0.87), Economy (0.80),
Sport (0.75), France (0.70), Art (0.62), Literature
(0.49), Television (0.46), Society (0.42), Science
(0.33). Hence, articles in the International cate-
gory were best identified. This ordering also con-
firms the difficulty felt by human judges concern-
ing the categories Society and Science.
We decided to distribute the categories for each
subtask according to a balance between easy and
diffucult ones in terms of human evaluation:
? Art, Economy, Sport, Television for the sub-
task with both genre and category recogni-
tion;
? France, International, Literature, Science,
Society for the subtask with only category
recognition. For this second subset, we put
together three categories which are topically
close (France, International and Society).
21
4 Human judgments and software
4.1 Confirming the difficulty of a task
The 2007 edition of DEFT highlighted two main
phenomena concerning the corpora involved in the
task.
First, each corpus yielded a different level of dif-
ficulty, and this gradation of difficulty among cor-
pora appeared both for human evaluators and com-
petitors in the challenge (Paroubek et al, 2007).
Judges Competitors
Debates 0.77/1.00 0.54/0.72
Game reviews 0.73/0.90 0.46/0.78
Film reviews 0.52/0.79 0.38/0.60
Paper reviews 0.41/0.58 0.40/0.57
Table 10: Minimal and maximal strict F-scores
between human evaluators and competitors in the
challenge, 2007 edition.
During human tests, judges mentioned the great
facility of finding about opinions expressed in the
corpus of parliamentary debate. Next came cor-
pora of video game reviews, and then of film and
book reviews, whose difficulty was considered av-
erage, and last, the corpus of scientific paper re-
views, which the judges perceived as particularly
difficult. This gradation of difficulty among cor-
pora was also found among competitors, following
the same ordering of three levels of difficulty.
Secondly, the difficulties met by human eval-
uators are also found in the case of competitors.
Upon finishing human tests, judges felt difficulties
in evaluating the corpus of scientific paper reviews,
yielding poor results. Now, the results of competi-
tors on the same corpus are quite as poor, occupy-
ing exactly the same value interval as for human
judges. Most competitors, by the way, obtained
their worst results on this corpus.
The alikeness of results between judges and
competitors reflects the complexity of the corpus:
when preparing the campaign, we observed that
reviews were quite short. Therefore, assigning a
value had to rely upon a small amount of data.
From that, we can derive a minimal size for docu-
ments to be used in this kind of evaluation. More-
over, a paper review can be seen as an aid for the
author, to be expressed as positively as possible,
even if it is also addressed to the Program Commit-
tee which has to accept or reject the paper. There-
fore, the mark could prove more negative than the
text of the review.
The case of comments about videogames is a
different one. Indeed, giving a global mark on a
scale of 20 is a difficult task. Therefore, this mark
comes most often from a sum of smaller marks
which rate either the whole document according
to various criteria, or parts of this document. In
our corpus, each reviewer rates the game accord-
ing to several criteria, namely, graphics, playa-
bility, life span, sound track and scenario, from
which a rather long text is produced, making the
judgment an easier task to perform. However, the
global mark differs from the sum of the smaller
ones from various criteria, hence the difficulty for
human judges to reckon this global mark on a scale
of 20.
4.2 Confirmation of the expected success of
competitors
Contrary to the 2007 edition, in which competi-
tors obtained results that confirmed those of human
judges, the 2008 edition gave them the opportunity
to reach a higher level than human evaluators.
While genre identification yielded no special
problem, either for human evaluators or for com-
petitors, and the results obtained by both groups
are similar, competitors reached better results than
human judges in topical categorization.
Concerning genre identification, strict F-scores
are situated between 0.94 and 1.00 for human
judges, and between 0.95 and 0.98 for the best
runs of competitors (each competitor was allowed
to submit up to three collections of results, only
the best one being used for the final ranking). As
for topical categorization, strict F-scores go from
0.66 to 0.82 for human evaluators, and from 0.84
to 0.89 for best runs from competitors.
The equivalence of results on genre identifica-
tion between judges and competitors can be ex-
plained by the fact that it was a simple, binary
choice (the newspaper Le Monde vs. Wikipedia).
Contrariwise, competitors obtained better re-
sults in topical categorization, since machines have
a stronger abstraction capacity than humans in
presence of the 9 topical categories we defined
(Art, Economy, France, International, Literature,
Science, Society, Sport and Television). However,
conditions were not quite similar, since human
judges had to pick a category among eight, and
not, like the automatic systems, a category within
two subsets of four and five categories. Indeed,
22
we dispatched the categories into two sets, by bal-
ancing categories that are easy or difficult for hu-
man evaluators. For the second set of categories,
we carefully put together three semantically close
ones, (France, International and Society, all three
of them being about political and societal con-
tents), to make the task more difficult. Although
the second set of categories seems more compli-
cated for human judges, half of the competitors ob-
tained better results in topical categorization of the
second set than of the first one.
5 Conclusion
The relevance of human judgment in an evaluation
campaign is present from the beginning to the end
of a campaign.
In a first step, testing a topic for a campaign
among a limited number of human evaluators al-
lows us to check the feasibility of a task. This
checking relies both on the results obtained by
judges (recall, precision, F-scores) and on their
personal impressions after passing the test.
In a second step, the study of both the results ob-
tained by the judges, and their pairwise matching
involving such a comparator as the ? coefficient
allows us to adjust the task (choice of a marking
scale for DEFT?07 and selection of topical cate-
gories for DEFT?08).
Finally, the mutual comparison of competitors?
results, at the end of the evaluation campaign, al-
lows us to validate the choices we made at its start-
ing point, and even to reposition the task when we
shall launch a future campaign based on the same
topic.
References
Adda, Gilles, Joseph Mariani, Patrick Paroubek, Mar-
tin Rajman, and Josette Lecomte. 1999. L?action
GRACE d?e?valuation de l?assignation des parties du
discours pour le franc?ais. Langues, 2(2):119?129,
juin.
Burstein, Jill and Magdalena Wolska. 2003. Toward
evaluation of writing style: Finding overly repetitive
word use in student essays. In 10th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, EACL?03, pages 35?42, Budapest,
Hungary, april.
Carletta, J. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistics. Computational Lin-
guistics, 2(22):249?254.
Cohen, Jacob. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, (20):37?46.
Eck, Matthias and Chiori Hori. 2005. Overview of
the iwslt 2005 evaluation campaign. In International
Workshop on Spoken Language Translation, pages
5?14, Pittsburg, PA.
Grouin, Cyril, Jean-Baptiste Berthelin, Sarra El Ayari,
Thomas Heitz, Martine Hurault-Plantet, Miche`le
Jardino, Zohra Khalis, and Michel Lastes. 2007.
Pre?sentation de DEFT?07 (D ?Efi Fouille de Textes).
In Actes de l?atelier de clo?ture du 3e`me D ?Efi
Fouille de Textes, pages 1?8, Grenoble. Association
Franc?aise d?Intelligence Artificielle.
Hovy, Eduard, Margaret King, and Andrei Popescu-
Belis. 2002. Principles of context-based machine
translation evaluation. Machine Translation.
Hurault-Plantet, Martine, Jean-Baptiste Berthelin,
Sarra El Ayari, Cyril Grouin, Patrick Paroubek, and
Sylvain Loiseau. 2008. Re?sultats de l?e?dition 2008
du D ?Efi Fouille de Textes. In Actes TALN?08, Avi-
gnon. Association pour le Traitement Automatique
des Langues.
Paek, Tim. 2001. Empirical Methods for Evaluat-
ing Dialog Systems. In Proceedings of the ACL
2001 Workshop on Evaluation Methodologies for
Language and Dialogue Systems, pages 3?10.
Paroubek, Patrick, Jean-Baptiste Berthelin, Sarra El
Ayari, Cyril Grouin, Thomas Heitz, Martine Hurault-
Plantet, Miche`le Jardino, Zohra Khalis, and Michel
Lastes. 2007. Re?sultats de l?e?dition 2007 du D ?Efi
Fouille de Textes. In Actes de l?atelier de clo?ture du
3e`me D ?Efi Fouille de Textes, pages 9?17, Grenoble.
Association Franc?aise d?Intelligence Artificielle.
23
Proceedings of the Fifth Law Workshop (LAW V), pages 92?100,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Proposal for an Extension of Traditional Named Entities:
From Guidelines to Evaluation, an Overview
Cyril Grouin?, Sophie Rosset?, Pierre Zweigenbaum?
Kar?n Fort?,? , Olivier Galibert?, Ludovic Quintard?
?LIMSI?CNRS, France ?INIST?CNRS, France ?LIPN, France ?LNE, France
{cyril.grouin,sophie.rosset,pierre.zweigenbaum}@limsi.fr
karen.fort@inist.fr, {olivier.galibert,ludovic.quintard}@lne.fr
Abstract
Within the framework of the construction of a
fact database, we defined guidelines to extract
named entities, using a taxonomy based on an
extension of the usual named entities defini-
tion. We thus defined new types of entities
with broader coverage including substantive-
based expressions. These extended named en-
tities are hierarchical (with types and compo-
nents) and compositional (with recursive type
inclusion and metonymy annotation). Human
annotators used these guidelines to annotate a
1.3M word broadcast news corpus in French.
This article presents the definition and novelty
of extended named entity annotation guide-
lines, the human annotation of a global corpus
and of a mini reference corpus, and the evalu-
ation of annotations through the computation
of inter-annotator agreements. Finally, we dis-
cuss our approach and the computed results,
and outline further work.
1 Introduction
Within the framework of the Quaero project?a mul-
timedia indexing project?we organized an evalu-
ation campaign on named entity extraction aiming
at building a fact database in the news domain, the
first step being to define what kind of entities are
needed. This campaign focused on broadcast news
corpora in French. While traditional named enti-
ties include three major classes (persons, locations
and organizations), we decided to extend the cov-
erage of our campaign to new types of entities and
to broaden their main parts-of-speech from proper
names to substantives, this extension being neces-
sary for ever-increasing knowledge extraction from
documents. We thus produced guidelines to specify
the way corpora had to be annotated, and launched
the annotation process.
In this paper, after covering related work (Sec-
tion 2), we describe the taxonomy we created (Sec-
tion 3) and the annotation process and results (Sec-
tion 4), including the corpora we gathered and the
tools we developed to facilitate annotation. We then
present inter-annotator agreement measures (Sec-
tion 5), outline limitations (Section 6) and conclude
on perspectives for further work (Section 7).
2 Related work
2.1 Named entity definitions
Named Entity recognition was first defined as recog-
nizing proper names (Coates-Stephens, 1992). Since
MUC-6 (Grishman and Sundheim, 1996; SAIC,
1998), named entities have been proper names
falling into three major classes: persons, locations
and organizations.
Proposals were made to sub-divide these entities
into finer-grained classes. The ?politicians? sub-
class was proposed for the ?person? class by (Fleis-
chman and Hovy, 2002) while the ?cities? subclass
was added to the ?location? class by (Fleischman,
2001; Lee and Lee, 2005).
The CONLL conference added a miscellaneous
type that includes proper names falling outside the
previous classes. Some classes have thus sometimes
been added, e.g. the ?product? class by (Bick, 2004;
Galliano et al, 2009).
92
Specific entities are proposed and handled in
some tasks: ?language? or ?shape? for question-
answering systems in specific domains (Rosset et
al., 2007), ?email address? or ?phone number? to
process electronic messages (Maynard et al, 2001).
Numeric types are also often described and used.
They include ?date?, ?time?, and ?amount? types
(?amount? generally covers money and percentage).
In specific domains, entities such as gene, protein,
are also handled (Ohta, 2002), and campaigns are or-
ganized for gene detection (Yeh et al, 2005). At the
same time, extensions of named entities have been
proposed: (Sekine, 2004) defined a complete hierar-
chy of named entities containing about 200 types.
2.2 Named Entities and Annotation
As for any other kind of annotation, some aspects are
known to lead to difficulties in obtaining coherence
in the manual annotation process (Ehrmann, 2008;
Fort et al, 2009). Three different classes of prob-
lems are distinguished: (1) selecting the correct cat-
egory in cases of ambiguity, where one entity can
fall into several classes, depending on the context
(?Paris? can be a town or a person name); (2) detect-
ing the boundaries (in a person designation, is only
the proper name to be annotated or the trigger ?Mr?
too?) and (3) annotating metonymies (?France? can
be a sports team, a country, etc.).
In the ACE Named Entity task (Doddington et al,
2004), a complex task, the obtained inter-annotator
agreement was 0.86 in 2002 and 0.88 in 2003. Some
tasks obtain better agreement. Desmet and Hoste
(2010) described the Named Entity annotation real-
ized within the Sonar project, where Named Entity
are clearly simpler. They follow the MUC Named
Entity definition with the subtypes as proposed
by ACE. The agreement computed over the Sonar
Dutch corpus ranges from 0.91 to 0.97 (kappa val-
ues) depending of the emphasized elements (span,
main type, subtype, etc.).
3 Taxonomy
3.1 Guidelines production
Having in mind the objective of building a fact
database through the extraction of named entities
from texts, we defined a richer taxonomy than those
used in other information extraction works.
Following (Bonneau-Maynard et al, 2005; Alex
et al, 2010), the annotation guidelines were first
written from December 2009 to May 2010 by three
researchers managing the manual annotation cam-
paign. During guidelines production, we evaluated
the feasibility of this specific annotation task and the
usefulness of the guidelines by annotating a small
part of the target corpus. Then, these guidelines
were delivered to the annotators. They consist of a
description of the objects to annotate, general anno-
tation rules and principles, and more than 250 pro-
totypical and real examples extracted from the cor-
pus (Rosset et al, 2010). Rules are important to set
the general way annotations must be produced. Ad-
ditionally, examples are essential for human annota-
tors to grasp the annotation rationale more easily.
Indeed, while producing the guidelines, we knew
that the given examples would never cover all possi-
ble cases because of the specificity of language and
of the ambiguity of formulations and situations de-
scribed in corpora, as shown in (Fort et al, 2009).
Nevertheless, guidelines examples must be consid-
ered as a way to understand the final objective of
the annotation work. Thanks to numerous meetings
from May to November 2010, we gathered feedback
from the annotators (four annotators plus one anno-
tation manager). This feedback allowed us to clarify
and extend the guidelines in several directions. The
guidelines are 72 pages long and consist of 3 major
parts: general description of the task and the prin-
ciples (25% of the overall document), presentation
of each type of named entity (57%), and a simpler
?cheat sheet? (18%).
3.2 Definition
We decided to use the three general types of
named entities: name (person, location, organi-
zation) as described in (Grishman and Sundheim,
1996; SAIC, 1998), time (date and duration), and
quantity (amount). We then included named entities
extensions proposed by (Sekine, 2004; Galliano et
al., 2009) (respectively products and functions) and
we extended the definition of named entities to ex-
pressions which are not composed of proper names
(e.g., phrases built around substantives). The ex-
tended named entities we defined are both hierar-
chical and compositional. For example, type pers
(person) is split into two subtypes, pers.ind (indi-
93
Person Function
pers.ind (individual
person)
pers.coll (group of
persons)
func.ind (individual
function)
func.coll (collectivity
of functions)
Location Product
administrative
(loc.adm.town,
loc.adm.reg,
loc.adm.nat,
loc.adm.sup)
physical
(loc.phys.geo,
loc.phys.hydro,
loc.phys.astro)
facilities
(loc.fac),
oronyms
(loc.oro),
address
(loc.add.phys,
loc.add.elec)
prod.object
(manufac-
tured object)
prod.serv
(transporta-
tion route)
prod.fin
(financial
products)
prod.doctr
(doctrine)
prod.rule
(law)
prod.soft
(software)
prod.art prod.media prod.award
Organization Time
org.adm (administra-
tion)
org.ent (services)
Amount
amount (with unit or general object), includ-
ing duration
time.date.abs
(absolute date),
time.date.rel (relative
date)
time.hour.abs
(absolute hour),
time.hour.rel (relative
hour)
Table 1: Types (in bold) and subtypes (in italic)
vidual person) and pers.coll (collective person), and
pers entities are composed of several components,
among which are name.first and name.last.
3.3 Hierarchy
We used two kinds of elements: types and compo-
nents. The types with their subtypes categorize a
named entity. While types and subtypes were used
before (ACE, 2000; Sekine, 2004; ACE, 2005; Gal-
liano et al, 2009), we consider that structuring the
contents of an entity (its components) is important
too. Components categorize the elements inside a
named entity.
Our taxonomy is composed of 7 main types
(person, function, location, product, organization,
amount and time) and 32 subtypes (Table 1). Types
and subtypes refer to the general category of a
named entity. They give general information about
the annotated expression. Almost each type is then
specified using subtypes that either mark an opposi-
tion between two major subtypes (individual person
vs. collective person), or add precisions (for exam-
ple for locations: administrative location, physical
location, etc.).
This two-level representation of named entities,
with types and components, constitutes a novel ap-
proach.
Types and subtypes To deal with the intrinsic am-
biguity of named entities, we defined two specific
transverse subtypes: 1. other for entities with a dif-
ferent subtype than those proposed in the taxon-
omy (for example, prod.other for games), and 2. un-
known when the annotator does not know which sub-
type to use.
Types and subtypes constitute the first level of an-
notation. They refer to a general segmentation of
the world into major categories. Within these cate-
gories, we defined a second level of annotation we
call components.
Components Components can be considered as
clues that help the annotator to produce an anno-
tation: either to determine the named entity type
(e.g. a first name is a clue for the pers.ind named
entity subtype), or to set the named entity bound-
aries (e.g. a given token is a clue for the named en-
tity, and is within its scope, while the next token is
not a clue and is outside its scope). Components are
second-level elements, and can never be used out-
side the scope of a type or subtype element. An en-
tity is thus composed of components that are of two
kinds: transverse components and specific compo-
nents (Table 2). Transverse components can be used
in several types of entities, whereas specific compo-
nents can only be used in one type of entity.
94
Transverse components
name (name of the entity), kind (hyperonym of the entity), qualifier (qualifying adjective), demonym
(inhabitant or ethnic group name), demonym.nickname (inhabitant or ethnic group nickname), val
(a number), unit (a unit), extractor (an element in a series), range-mark (range between two values),
time-modifier (a time modifier).
pers.ind loc.add.phys time.date.abs/rel amount
name.last, name.first,
name.middle, pseudonym,
name.nickname, title
address-number, po-box,
zip-code,
other-address-component
week, day, month, year,
century, millennium,
reference-era
object
prod.award
award-cat
Table 2: Transverse and specific components
3.4 Composition
Another original point in this work is the compo-
sitional nature of the annotations. Entities can be
compositional for three reasons: (i) a type contains a
component; (ii) a type includes another type, used as
a component; and (iii) in cases of metonymy. Dur-
ing the Ester II evaluation campaign, there was an
attempt to use compositionality in named entities for
two categories: persons and functions, where a per-
son entity could contain a function entity.
<pers.hum> <func.pol> pr?sident </func.pol>
<pers.hum> Chirac </pers.hum> </pers.hum>
Nevertheless, the Ester II evaluation did not take
this inclusion into account and only focused on
the encompassing annotation (<pers.hum> pr?sident
Chirac </pers.hum>). We drew our inspiration from
this experience, and allowed the annotators and the
systems to use compositionality in the annotations.
Cases of inclusion can be found in the function
type (Figure 1), where type func.ind, which spans
the whole expression, includes type org.adm, which
spans the single word ?budget?. In this case, we con-
sider that the designation of this function (?ministre
du budget?) includes both the kind (?ministre?) and
nouveau
qualifier
ministre
kind
du Budget
name
org.adm
func.ind
, Fran?ois
name.first
Baroin
name.last
pers.ind
Figure 1: Multi-level annotation of entity types (red tags)
and components (blue tags): new minister of budget ,
Fran?ois Baroin.
the name (?budget?) of the ministry, which itself is
typed as is relevant (org.adm). Recursive cases of
embedding can be found when a subtype includes
another named entity annotated with the same sub-
type (org.ent in Figure 2).
le collectif
kind
des associations
kind
des droits de l' Homme
name
prod.rule
au Sahara
name
loc.phys.geo
loc.adm.sup
org.ent
org.ent
Figure 2: Recursive embedding of the same subtype:
Collective of the Human Rights Organizations in Sahara.
Cases of metonymy include strict metonymy (a
term is substituted with another one in a relation
of contiguity) and antonomasia (a proper name is
used as a substantive or vice versa). In such cases,
the entity must be annotated with both types, first
(inside) with the intrinsic type of the entity, then
(outside) with the type that corresponds to the re-
sult of the metonymy. Basically, country names
correspond to ?national administrative? locations
(loc.adm.nat) but they can also designate the admin-
istration (org.adm) of the country (Figure 3).
depuis
time-modifier
plusieurs
val
mois
unit
amount
time.date.rel
, la Russie
name
loc.adm.nat
org.adm
Figure 3: Annotation with a metonymic use of country
?Russia? as its government: for several months , Russia...
95
3.5 Boundaries
Our definition of the scope of entities excludes rel-
ative clauses, subordinate clauses, and interpolated
clauses: the annotation of an entity must end before
these clauses. If an interpolated clause occurs inside
an entity, its annotation must be split. Moreover, two
distinct persons sharing the same last name must be
annotated as two separate entities (Figure 4); we in-
tend to use relations between entities to gather these
segments in the next step of the project.
depuis
utmi-oefrl
vifr-eua
il n.s,etui
utmi-oefrl
Rprveu
utmi-strl
vifr-eua
Figure 4: Separate (coordinated) named entities.
4 Annotation process
4.1 Corpus
We managed the annotation of a corpus of about one
hundred hours of transcribed speech from several
French-speaking radio stations in France and Mo-
rocco. Both news and entertainment shows were
transcribed, including dialogs, with speaker turns.1
Once annotated, the corpus was split into a de-
velopment corpus: one file from a French radio sta-
tion;2 a training corpus: 188 files from five French
stations3 and one Moroccan station;4 and a test cor-
pus: 18 files from two French stations already stud-
ied in the training corpus5 and from unseen sources,
both radio6 and television,7 in order to evaluate the
robustness of systems. These data have been used in
the 2011 Quaero named entity evaluation campaign.
1Potential named entities may be split across several seg-
ments or turns.
2News from France Culture.
3News from France Culture (refined language), France Info
(news with short news headlines), France Inter (generalist radio
station), Radio Classique (classical music and economic news),
RFI (international radio broadcast out of France).
4News from RTM (generalist French speaking radio).
5News from France Culture, news and entertainment from
France Inter.
6A popular entertainment show from Europe 1.
7News from Arte (public channel with art and culture),
France 2 (public generalist channel), and TF1 (private gener-
alist popular channel).
This corpus allows us to perform different evalua-
tions, depending of the knowledge the systems have
of the source (source seen in the training corpus vs.
unseen source), the kind of show (news vs. enter-
tainment), the language style (popular vs. refined),
and the type of media (radio vs. television).
4.2 Tools for annotators
To perform our test annotations (see Section 2.2),
we developed a very simple annotation tool as an in-
terface based on XEmacs. We provided the human
annotators with this tool and they decided to use it
for the campaign, despite the fact that it is very sim-
ple and that we told them about other, more generic,
annotation tools such as GATE8 or Glozz.9 This is
probably due to the fact that apart from being very
simple to install and use, it has interesting features.
The first feature is the insertion of annotations
using combinations of keyboard shortcuts based on
the initial of each type, subtype and component
name. For example, combination F2 key + initial
keys is used to annotate a subtype (pers.ind, etc.),
F3 + keys for a transverse component (name, kind,
etc.), F4 + keys for a specific component (name.first,
etc.), and F5 to delete the annotation selected with
the cursor (both opening and closing tags).
The second feature is boundary management: if
the annotator puts the cursor over the token to anno-
tate, the annotation tool will handle the boundaries
of this token; opening and closing tags will be in-
serted around the token.
However, it presents some limitations: tags are
inserted in the text (which makes visualization more
complex, especially for long sentences or in cases
of multiple annotations on the same entity), no per-
sonalization is offered (tags are of only one color),
and there is no function to express annotator uncer-
tainty (the user must choose among several possible
tags the one that fits the best;10 while producing the
guidelines, we did not consider it could be of inter-
est: as a consequence, no uncertainty management
was implemented). Therefore, this tool allows users
to insert tags rapidly into a text, but it offers no exter-
nal resources, as real annotation tools (e.g. GATE)
often do.
8http://gate.ac.uk/
9http://www.glozz.org/
10Uncertainty can be found in cases of lack of context.
96
These simplistic characteristics combined with a
fast learning curve allow the annotators to rapidly
annotate the corpora. Annotators were allowed not
to annotate the transverse component name (only if
it was the only component in the annotated phrase,
e.g. ?Russia? in Figure 3, blue tag) and to annotate
events, even though we do not focus on this type
of entity as of yet. We therefore also provided a
normalization tool which adds the transverse com-
ponent name in these instances, and which removes
event annotations.
4.3 Corpus annotation
Global annotation It took four human annotators
two months and a half to annotate the entire corpus
(10 man-month). These annotators were hired grad-
uate students (MS in linguistics). The overall corpus
was annotated in duplicate. Regular comparisons of
annotations were performed and allowed the anno-
tators to develop a methodology, which was subse-
quently used to annotate the remaining documents.
Mini reference corpus To evaluate the global an-
notation, we built a mini reference corpus by ran-
domly selecting 400 sentences from the training cor-
pus and distributing them into four files. These files
were annotated by four graduate human annotators
from two research institutes (Figure 5) with two hu-
mans per institute, in about 10 hours per annotator.
	








Figure 5: Creation of mini reference corpus and compu-
tation of inter-annotator agreement. Institute 1 = LIMSI?
CNRS, Institute 2 = INIST?CNRS
First, we merged the annotations of each file
within a given institute (1.5h per pair of annotators),
then merged the results across the two institutes
(2h). Finally, we merged the results with the anno-
tations of the hired annotators (8h). We thus spent
about 90 hours to annotate and merge annotations in
this mini reference corpus (0.75 man-month).
4.4 Annotation results
Our broadcast news corpus includes 1,291,225
tokens, among which there are 954,049 non-
punctuation tokens. Its annotation contains 113,885
named entities and 146,405 components (Table 3),
i.e. one entity per 8.4 non-punctuation tokens, and
one component per 6.5 non-punctuation tokens.
There is an average of 6 annotations per line.
PPPPPPPPInf.
Data
Training Test
# shows 188 18
# lines 43,289 5,637
# words 1,291,225 108,010
# entity types 113,885 5,523
# distinct types 41 32
# components 146,405 8,902
# distinct comp. 29 22
Table 3: Statistics on annotated corpora.
5 Inter-Annotator Agreement
5.1 Procedure
During the annotation campaign, we measured sev-
eral criteria on a regular basis: inter-annotator agree-
ment and disagreement. We used them to correct er-
roneous annotations, and mapped these corrections
to the original annotations. We also used these mea-
sures to give the annotators feedback on the en-
countered problems, discrepancies, and residual er-
rors. Whereas we performed these measurements all
along the annotation campaign, this paper focuses
on the final evaluation on the mini reference corpus.
5.2 Metrics
Because human annotation is an interpretation pro-
cess (Leech, 1997), there is no ?truth? to rely on. It
is therefore impossible to really evaluate the validity
of an annotation. All we can and should do is to eval-
uate its reliability, i.e. the consistency of the anno-
tation across annotators, which is achieved through
computation of the inter-annotator agreement (IAA).
97
The best way to compute it is to use one of
the Kappa family coefficients, namely Cohen?s
Kappa (Cohen, 1960) or Scott?s Pi (Scott, 1955),
also known as Carletta?s Kappa (Carletta, 1996),11
as they take chance into account (Artstein and Poe-
sio, 2008). However, these coefficients imply a
comparison with a ?random baseline? to establish
whether the correlation between annotations is sta-
tistically significant. This baseline depends on the
number of ?markables?, i.e. all the units that could
be annotated.
In the case of named entities, as in many others,
this ?random baseline? is known to be difficult?if
not impossible?to identify (Alex et al, 2010). We
wish to analyze this in more detail, to see how we
could actually compute these coefficients and what
information it would give us about the annotation.
Markables Annotators Both institutes
F = 0.84522 F = 0.91123
U1: n-grams
? = 0.84522 ? = 0.91123
pi = 0.81687 pi = 0.90258
U2: n-grams ? 6
? = 0.84519 ? = 0.91121
pi = 0.81685 pi = 0.90257
U3: NPs
? = 0.84458 ? = 0.91084
pi = 0.81628 pi = 0.90219
U4: Ester entities
? = 0.71300 ? = 0.82607
pi = 0.71210 pi = 0.82598
U5: Pooling
? = 0.71300 ? = 0.82607
pi = 0.71210 pi = 0.82598
Table 4: Inter-Annotator Agreements (? stands for Co-
hen?s Kappa, pi for Scott?s Pi, and F for F-measure). IAA
values were computed by taking as the reference the hired
annotators? annotation or that obtained by merging from
both institutes (see Figure 5).
In the present case, we could consider that, poten-
tially, all the noun phrases can be annotated (row U3
in Table 4, based on the PASSAGE campaign (Vil-
nat et al, 2010)). Of course, this is a wrong approx-
imation as named entities are not necessarily noun
phrases (e.g., ?? partir de l?automne prochain?, from
next autumn).
We could also consider all n-grams of tokens in
the corpus (row U1). However, it would be more
11For more details on terminology issues, we refer to the in-
troduction of (Artstein and Poesio, 2008).
relevant to limit their size. For a maximum size of
six, we get the results shown in row U2. All this, of
course, is artificial, as the named entity annotation
process is not random.
To obtain results that are closer to reality, we
could use numbers of named entities from previous
named entity annotation campaigns (row U4 based
on the Ester II campaign (Galliano et al, 2009)), but
as we consider here a largely extended version of
those, the results would again be far from reality.
Another solution is to consider as ?markables? all
the units annotated by at least one of the annotators
(row U5). In this particular case, units not annotated
by any of the annotators (i.e. silence) are overlooked.
The lowest IAA will be the one computed with
this last solution, while the highest IAA will be
equal to the F-measure (i.e. the measure computed
with all the markables as shown in row U1 in Ta-
ble 4). We notice that the first two solutions (U1
and U2 with n-grams) are not acceptable because
they are far from reality; even extended named en-
tities are sparse annotations, and just considering
all tokens as ?markables? is not suitable. The last
three ones seem to be more relevant because they
are based on an observed segmentation on similar
data. Still, the U3 solution (NPs) overrates the num-
ber of markables because not all noun phrases are
extended named entities. Although the U4 solution
(Ester entities) is based on the same corpus used for
a related task, it underrates the number of markables
because that task produced 16.3 times less annota-
tions. Finally the U5 solution (pooling) gives the
lower bound for the ? estimation which is an in-
teresting information but may easily undervalue the
quality of the annotation.
As (Hripcsak and Rothschild, 2005) showed, in
our case ? tends towards the F-measure when the
number of negative cases tends towards infinity. Our
results show that it is hard to build a justifiable hy-
pothesis on the number of markables which is larger
than the number of actually annotated entities while
keeping ? significantly under the F-measure. But
building no hypothesis leads to underestimating the
? value.
This reinforces the idea of using the F-measure
as the main inter-annotator agreement measure for
named entity annotation tasks.
98
6 Limitations
We used syntax to define some components (e.g. a
qualifier is an adjective) and to set the scope of en-
tities (e.g. stop at relative clauses). Nevertheless,
this syntactic definition cannot fit all named enti-
ties, which are mainly defined according to seman-
tics: the phrase ?dans les mois qui viennent? (?in
the coming months?) expresses an entity of type
time.date.rel where the relative clause ?qui vien-
nent? is part of the entity and contributes the time-
modifier component.
The distinction between some types of entities
may be fuzzy, especially for the organizations (is
the Social Security an administrative organization or
a company?) and for context-dependent annotations
(is lemonde.fr a URL, a media, or a company?). As a
consequence, some entity types might be converted
into specific components in a future revision, e.g. the
func type could become a component of the pers
type, where it would become a description of the
function itself instead of the person who performs
this function (Figure 6).
depuisptm
-its
oftrlits
vaienr
tn.pl,num
Rpeulits
depuisptm
oftr
vaienr
tn.pl,num
Rpeulits
Figure 6: Possible revision: current annotation (left),
transformation of func from entity to component (right).
7 Conclusion and perspectives
In this paper, we presented an extension of the tra-
ditional named entity categories to new types (func-
tions, civilizations) and new coverage (expressions
built over a substantive). We created guidelines
that were used by graduate annotators to annotate
a broadcast news corpus.
The organizers also annotated a small part of the
corpus to build a mini reference corpus. We evalu-
ated the human annotations with our mini-reference
corpus: the actual computed ? is between 0.71 et
0.85 which, given the complexity of the task, seems
to indicate a good annotation quality. Our results are
consistent with other studies (Dandapat et al, 2009)
in demonstrating that human annotators? training is
a key asset to produce quality annotations.
We also saw that guidelines are never fixed, but
evolve all along the annotation process due to feed-
back between annotators and organizers; the rela-
tionship between guidelines producers and human
annotators evolved from ?parent? to ?peer? (Akrich
and Boullier, 1991). This evolution was observed
during the annotation development, beyond our ex-
pectations. These data have been used for the 2011
Quaero Named Entity evaluation campaign.
Extensions and revisions are planned. Our first
goal is to add a new type of named entity for all
kinds of events; guidelines are being written and hu-
man annotation tests are ongoing. We noticed that
some subtypes are more difficult to disambiguate
than others, especially org.adm and org.ent (defi-
nition and examples in the guidelines are not clear
enough). We shall make decisions about this kind
of ambiguity, either by merging these subtypes or by
reorganizing the distinctions within the organization
type. We also plan to link the annotated entities us-
ing relations; further work is needed to define more
precisely the way we will perform these annotations.
Moreover, the taxonomy we defined was applied to
a broadcast news corpus, but we intend to use it in
other corpora. The annotation of an old press corpus
was performed according to the same process. Its
evaluation will start in the coming months.
Acknowledgments
We thank all the annotators who did such a great
work on this project, as well as Sabine Barreaux
(INIST?CNRS) for her work on the reference cor-
pus.
This work was partly realized as part of the
Quaero Programme, funded by Oseo, French State
agency for innovation and by the French ANR Etape
project.
References
ACE. 2000. Entity detection and tracking,
phase 1, ACE pilot study. Task definition.
http://www.nist.gov/speech/tests/ace/phase1/doc/summary-
v01.htm.
ACE. 2005. ACE (Automatic Con-
tent Extraction) English annotation guide-
lines for entities version 5.6.1 2005.05.23.
http://www.ldc.upenn.edu/Projects/ACE/docs/English-
Entities-Guidelines_v5.6.1.pdf.
99
Madeleine Akrich and Dominique Boullier. 1991. Le
mode d?emploi, gen?se, forme et usage. In Denis
Chevallier, editor, Savoir faire et pouvoir transmettre,
pages 113?131. ?d. de la MSH (collection Ethnologie
de la France, Cahier 6).
Beatrice Alex, Claire Grover, Rongzhou Shen, and Mijail
Kabadjov. 2010. Agile Corpus Annotation in Prac-
tice: An Overview of Manual and Automatic Annota-
tion of CVs. In Proc. of the Fourth Linguistic Annota-
tion Workshop, pages 29?37, Uppsala, Sweden. ACL.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4):555?596.
Eckhard Bick. 2004. A named entity recognizer for dan-
ish. In LREC?04.
H?l?ne Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Seman-
tic Annotation of the French Media Dialog Corpus. In
InterSpeech, Lisbon.
Jean Carletta. 1996. Assessing Agreement on Classifi-
cation Tasks: the Kappa Statistic. Computational Lin-
guistics, 22:249?254.
Sam Coates-Stephens. 1992. The analysis and acquisi-
tion of proper names for the understanding of free text.
Computers and the Humanities, 26:441?456.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37?46.
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex Linguistic An-
notation - No Easy Way Out! A Case from Bangla
and Hindi POS Labeling Tasks. In Proc. of the Third
Linguistic Annotation Workshop, Singapour. ACL.
Bart Desmet and V?ronique Hoste. 2010. Towards a
balanced named entity corpus for dutch. In LREC.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ACE) program tasks, data, and evaluation. In Proc. of
LREC.
Maud Ehrmann. 2008. Les entit?s nomm?es, de la lin-
guistique au TAL : statut th?orique et m?thodes de
d?sambigu?sation. Ph.D. thesis, Univ. Paris 7 Diderot.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proc. of
COLING, volume 1, pages 1?7. ACL.
Michael Fleischman. 2001. Automated subcategoriza-
tion of named entities. In Proc. of the ACL 2001 Stu-
dent Research Workshop, pages 25?30.
Kar?n Fort, Maud Ehrmann, and Adeline Nazarenko.
2009. Towards a Methodology for Named Entities An-
notation. In Proceeding of the 3rd ACL Linguistic An-
notation Workshop (LAW III), Singapore.
Sylvain Galliano, Guillaume Gravier, and Laura
Chaubard. 2009. The ESTER 2 evaluation campaign
for the rich transcription of French radio broadcasts.
In Proc of Interspeech 2009.
Ralph Grishman and Beth Sundheim. 1996. Message
Understanding Conference - 6: A brief history. In
Proc. of COLING, pages 466?471.
George Hripcsak and Adam S. Rothschild. 2005. Tech-
nical brief: Agreement, the f-measure, and reliability
in information retrieval. JAMIA, 12(3):296?298.
Seungwoo Lee and Gary Geunbae Lee. 2005. Heuris-
tic methods for reducing errors of geographic named
entities learned by bootstrapping. In IJCNLP, pages
658?669.
Geoffrey Leech. 1997. Introducing corpus annotation.
In Geoffrey Leech Roger Garside and Tony McEnery,
editors, Corpus annotation: Linguistic information
from computer text corpora, pages 1?18. Longman,
London.
Diana Maynard, Valentin Tablan, Cristian Ursu, Hamish
Cunningham, and Yorick Wilks. 2001. Named en-
tity recognition from diverse text types. In Recent Ad-
vances in NLP 2001 Conference, Tzigov Chark.
Tomoko Ohta. 2002. The genia corpus: An annotated
research abstract corpus in molecular biology domain.
In Proc. of HLTC, pages 73?77.
Sophie Rosset, Olivier Galibert, Gilles Adda, and Eric
Bilinski. 2007. The LIMSI participation to the QAst
track. In Working Notes for the CLEF 2007 Workshop,
Budapest, Hungary.
Sophie Rosset, Cyril Grouin, and Pierre Zweigenbaum.
2010. Entit?s nomm?es : guide d?annotation Quaero,
November. T3.2, presse ?crite et orale.
SAIC. 1998. Proceedings of the seventh message under-
standing conference (MUC-7).
William A Scott. 1955. Reliability of Content Analysis:
The Case of Nominal Scale Coding. Public Opinion
Quaterly, 19(3):321?325.
Satoshi Sekine. 2004. Definition, dictionaries and tagger
of extended named entity hierarchy. In Proc. of LREC.
Anne Vilnat, Patrick Paroubek, Eric Villemonte de la
Clergerie, Gil Francopoulo, and Marie-Laure Gu?not.
2010. Passage syntactic representation: a minimal
common ground for evaluation. In Proc. of LREC.
Alex Yeh, Alex Morgan, Marc Colosimo, and Lynette
Hirschman. 2005. BioCreAtIvE task 1A: gene men-
tion finding evaluation. BMC Bioinformatics, 6(1).
100
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 144?152,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Building A Contrasting Taxa Extractor for Relation Identification from
Assertions: BIOlogical Taxonomy & Ontology Phrase Extraction System
Cyril Grouin
LIMSI?CNRS, Orsay, France
cyril.grouin@limsi.fr
Abstract
In this paper, we present the methods
we used to extract bacteria and biotopes
names and then to identify the relation
between those entities while participating
to the BioNLP?13 Bacteria and Biotopes
Shared Task. We used machine-learning
based approaches for this task, namely
a CRF to extract bacteria and biotopes
names and a simple matching algorithm to
predict the relations. We achieved poor re-
sults: an SER of 0.66 in sub-task 1, and a
0.06 F-measure in both sub-tasks 2 and 3.
1 Introduction
The BioNLP?13 Bacteria and Biotopes shared task
aims at extracting bacteria names (bacterial taxa)
and biotopes names (bacteria habitats; geographi-
cal and organization entities). The task comprises
three sub-tasks (Bossy et al, 2012b).
? Sub-task 1 aims at extracting habitat names
and linking those names to the relevant con-
cept from the OntoBiotope ontology.
? Sub-task 2 aims at identifying relations be-
tween bacteria and habitats among two kinds
of relations (localization, part-of) based on
a ground truth corpus of bacteria and habitat
names. The ?localization? relation is the link
between a bacterium and the place where it
lives while the ?part-of? relation is the rela-
tion between hosts and host parts (bacteria)
(Bossy et al, 2012a).
? Sub-task 3 aims at extracting all bacteria and
biotopes names (including both habitat and
geographical names), and then identifying re-
lations between these concepts.
In this paper, we present the methods we de-
signed as first time participant to the BioNLP Bac-
teria Biotopes Shared Task.
2 Background
Scientific documents provide useful information
in many domains. Because processing those docu-
ments is time-consuming for a human, NLP tech-
niques have been designed to process a huge
amount of documents quickly. The microorgan-
isms ecology domain involves a lot of microorgan-
isms (bacteria, living and dead cells, etc.) and
habitats (food, medical, soil, water, hosts, etc.)
that have been described in details in the literature.
NLP techniques would facilitate the access to in-
formation from scientific texts and make it avail-
able for further studies.
Bacteria and biotopes identification has been
addressed for the first time during the BioNLP
2011 Bacteria Biotopes shared task (Bossy et
al., 2012a; Kim et al, 2011). This task con-
sisted in extracting bacteria location events from
texts among eight categories (Host, HostPart, Ge-
ographical, Environment, Food, Medical, Water
and Soil).
Three teams participated in this task. All sys-
tems followed the same process: in a first stage,
they detected bacteria names, detected and typed
locations; then, they used co-reference to link the
extracted entities; the last stage focused on the
event extraction.
Bjo?rne et al (2012) adapted an SVM-based
Named Entity Recognition system and used the
list of Prokaryotic Names with Standing in
Nomenclature. Nguyen and Tsuruoka (2011) used
a CRF-based system and used the NCBI web page
about the genomic BLAST. Ratkovic et al (2012)
designed an ad hoc rule-based system based on the
NCBI Taxonomy. The participants obtained poor
results (Table 1) which underlines the complexity
of this task.
144
Team R P F
Ratkovic et al (2012) 0.45 0.45 0.45
Nguyen and Tsuruoka (2011) 0.27 0.42 0.33
Bjo?rne et al (2012) 0.17 0.52 0.26
Table 1: Recall, Precision and F-measure at
BioNLP 2011 Bacteria and Biotopes Shared Task
3 Corpus
3.1 Presentation
The corpus comprises web pages about bacte-
rial species written for non-experts. Each text
consists of a description of individual bacterium
and groups of bacteria, in terms of first observa-
tion, characteristics, evolution and biotopes. Two
corpora have been released including both raw
textual documents and external reference annota-
tions. The training corpus contains 52 textual doc-
uments while the development corpus contains 26
documents. No tokenization has been performed
over the documents. In Table 2, we provide some
statistics on the annotations performed over both
corpora for each type of entity to be annotated
(bacteria, habitat, and geographical).
Corpus Training Development
# Documents 52 26
# Words 16,294 9,534
Avg # words/doc 313.3 366.7
# Bacteria 832 515
# Habitat 934 611
# Geographical 91 77
Table 2: Annotation statistics on both corpora
3.2 Corpus analysis
The bacteria names appear in the texts, either in
their longer form (Xanthomonas axonopodis pv.
citri), in a partial form (Xanthomonas) or in their
abbreviated form (Xac). The abbreviations are
case-sensitives since they follow the original form:
MmmSC is derived from M. mycoides ssp my-
coides SC.1 A few bacteria names can appear in
the text followed by a trigger word: Spirillum bac-
teria, but it will be abbreviated in the remainder of
the text, sometimes with a higher degree of speci-
ficity: S. volutans standing for Spirillum volutans.
1Mycoplasma mycoides subspecies mycoides Small
Colony in its longer form.
4 Methods
This year, the BioNLP organizers encouraged the
participants to use supporting resources in order
to reduce the time-investment in the challenge.
Those resources encompass sentence splitting, to-
kenization, syntactic parsing, and biological anno-
tations. Moreover, a specific ontology has been
released for the Bacteria Biotopes task.
We used some of the resources provided and
combined them with additional resources, in a
machine-learning framework we specifically de-
signed for this task.
4.1 Linguistic resources
4.1.1 The OntoBiotope Ontology
OntoBiotope2 is an ontology tailored for the
biotopes domain. The BioNLP-ST 2013 version
has been released in the OBO format. This ontol-
ogy integrates 1,756 concepts. Each concept has
been given a unique ID and is associated with ex-
act terms and related synonyms. The concept is
also defined in a ?is a? relation. The normaliza-
tion of the habitat names in the first sub-task must
be based on this ontology.
For example, the concept microorganism
(unique id MBTO:00001516) is a living organ-
ism which unique id is MBTO:00000297. For this
concept, microbe is an exact synonym while mi-
crobial is a related synonym (see Figure 1).
[Term]
id: MBTO:00001516
name: microorganism
exact synonym: ?microbe? [TyDI:23602]
related synonym: ?microbial? [TyDI:23603]
is a: MBTO:00000297 ! living organism
Figure 1: The concept microorganism in the On-
toBiotope ontology
4.1.2 The NCBI taxonomy
In order to help our system to identify the bacte-
ria names, we built a list of 357,387 bacteria taxa
based on the NCBI taxonomy database3 (Feder-
hen, 2012). This taxonomy describes a small part
(about 10%) of the living species on earth, based
on public sequence databases.
2http://bibliome.jouy.inra.fr/
MEM-OntoBiotope/OntoBiotope_BioNLP-ST13.
obo
3http://www.ncbi.nlm.nih.gov/taxonomy/
145
It includes twelve categories of information
from the biological domain (bacteria, inverte-
brates, mammals, phages, plants, primates, ro-
dents, synthetics, unassigned, viruses, vertebrates
and environmental samples).
We extracted from this taxonomy all names be-
longing to the Bacteria category, which represent
24.3% of the content. This output includes a few
variants of bacteria names (see Table 3).
tax id name txt name class
346 Xanthomonas citri (ex
Hasse 1915) Gabriel et al
1989
authority
346 Xanthomonas citri
scientific
name
346 Xanthomonas axonopodis
pv. citri
synonym
346 Xanthomonas campestris
(pv. citri)
synonym
346 Xanthomonas campestris
pv. Citri (A group)
synonym
Table 3: Bacteria names from the NCBI taxonomy
4.1.3 The Cocoa annotations
Cocoa is a WebAPI annotator tool for biological
text.4 We used the Cocoa annotations provided by
the organizers as part of the supporting resources.
These annotations emphasize 37 pre-defined cate-
gories. We noticed a few categories are often tied
with one of the three kinds of entities we have to
process:
? Bacteria: Cell, Chemical, Mutant Organism,
Organism, Protein, Unknown;
? Habitat: Body part, Cell, Cellu-
lar component, Chemical, Disease, Food,
Geometrical part, Habitat, Location,
Multi-tissue structure, Organism, Organ-
ism subdivision, Pathological formation,
Tissue;
? Geographical: Company, Habitat, Technique,
Unknown.
We believe these categories should be useful to
identify bacteria and biotopes entities in the texts,
and we used them as features in the CRF model
(see column #10 in Table 4).
4Compact cover annotator for biological noun phrases,
http://npjoint.com/annotate.php
4.2 System
4.2.1 Formalisms
Depending on the sub-task to process, we used two
distinct formalisms implemented in the Wapiti tool
(Lavergne et al, 2010) to build our models:
? Conditional Random Fields (CRF) (Lafferty
et al, 2001; Sutton and McCallum, 2006)
to identify bacteria and biotopes names (sub-
tasks 1 and 3).
? Maximum Entropy (MaxEnt) (Guiasu and
Shenitzer, 1985; Berger et al, 1996) to pro-
cess the relationships between entities (sub-
tasks 2 and 3).
4.2.2 Bacteria biotopes features set
We used several sets of features, including ?classi-
cal? internal features (columns #4 to #7 in Table 4:
typographic, digit, punctuation, length) and a few
semantic features. In table 4, we present a sam-
ple tabular file produced in order to train the CRF
model.
? Presence of the token in the NCBI taxonomy
(column #9);
? Presence of the token in the OntoBiotope on-
tology (column #8);
? Category of the token based on the Cocoa an-
notations (column #10);
? Unsupervised clusters (column #11) created
using Brown?s algorithm (Brown et al, 1992)
with Liang?s code5 (Liang, 2005).
Taxonomy feature. We noticed that 1,169 to-
kens out of 1,229 (95.1%) tokens we identified in
the NCBI taxonomy in both corpora correspond to
a Bacteria name in the reference (Table 5). This
characteristic should be useful to identify the bac-
teria names.
OntoBiotope feature. Regarding the presence
of the token in the OntoBiotope ontology, we no-
ticed that 1,487 tokens out of 1,906 (78.0%) from
both corpora correspond to a habitat name in the
reference (Table 6). The identification of habitat
names will benefit from this characteristic.
5http://www.cs.berkeley.edu/?pliang/
software/
146
1 2 3 4 5 6 7 8 9 10 11 12
33 8 Borrelia Mm O O 7 O NCBI Organism 11101010 B-Bacteria
42 7 afzelii mm O O 7 O NCBI Organism O I-Bacteria
49 1 . O Punct O 1 O O O 0010 O
51 4 This Mm O O 4 O O O 1001000 O
56 7 species mm O O 7 O O Organism1 100101100 O
64 3 was mm O O 3 O O O 0101000 O
68 8 isolated mm O O 7 O O O 1100100 O
77 4 from mm O O 4 O O O 011110110 O
82 1 a mm O O 1 O O O 1011000 O
84 4 skin mm O O 4 MBTO O Pathological 110111011 B-Habitat
formation
89 6 lesion mm O O 6 MBTO O Pathological 111101100 I-Habitat
formation
96 4 from mm O O 4 O O O 011110110 I-Habitat
101 1 a mm O O 1 O O O 1011000 I-Habitat
103 4 Lyme Mm O O 4 O O Disease 100010 I-Habitat
108 7 disease mm O O 7 O O Disease 110111101 I-Habitat
116 7 patient mm O O 7 MBTO O Organism2 1100110 I-Habitat
124 2 in mm O O 2 O O O 0111100 O
127 6 Europe Mm O O 6 MBTO O Habitat 111101101 B-Geographical
134 2 in mm O O 2 O O O 0111100 O
137 4 1993 O O Digit 4 O O O 111101101 O
141 1 . O Punct O 1 O O O 0010 O
Table 4: Tabular used for training the CRF model. Column 1: character offset; 2: length in characters;
3: token; 4: typographic features; 5: presence of punctuation; 6: presence of digit; 7: length in characters
(with a generic ?7? category for length higher than seven characters); 8: presence of the token in the
OntoBiotope ontology; 9: presence of the token in the NCBI taxonomy; 10: category of the token from
the Cocoa annotations; 11: cluster identifier; 12: expected answer
Reference annotation
Token in the NCBI
Present Absent
Bacteria 1,169 1,543
Geographical 0 276
Habitat 2 2,466
O (out of annotation) 58 25,060
Table 5: Correspondence between the reference
annotation and the token based on the presence of
the token in the NCBI taxonomy
4.2.3 Normalization with OntoBiotope
Habitat names normalization consisted in linking
the habitat names to the relevant concept in the
OntoBiotope ontology using an exact match of the
phrase to be normalized. This exact match is based
on both singular and plural forms of the phrase
to normalize, using a home-made function that in-
cludes regular and irregular plural forms. Never-
theless, we did not manage discontinuous entities.
Reference annotation
Token in OntoBiotope
Present Absent
Bacteria 1 2,711
Geographical 156 120
Habitat 1,487 981
O (out of annotation) 262 24,856
Table 6: Correspondence between the reference
annotation and the token based on the presence of
the token in the OntoBiotope ontology
4.2.4 Relationships approaches
Relationships features set. Our MaxEnt model
only relies on the kind of entities that can be linked
together:
? Bacterium and Localization (Habitat) for a
?localization? relation,
? Host and Part for a ?PartOf? relation (be-
tween two entities being of the same type).
147
For example, Bifidobacterium is a bacteria
name, human and human gastrointestinal tract are
two habitats (localizations). A ?localization? re-
lation can occur between Bifidobacterium and hu-
man while a ?PartOf? relation occurs between hu-
man and human gastrointestinal tract.
Basic approach. For the official submission, we
did not use this model because of the following
remaining problems: (i) a few relations we pro-
duced were not limited to the habitat category but
also involved the geographical category, (ii) we
did not manage the relations we produced in du-
plicate, and (iii) the weight our CRF system gave
to each relation was not relevant enough to be used
(for a relation involving A with B, C, and D, the
same weight was given in each relation).
All of those problems led us to process the re-
lations between entities using a too much simple
approach: we only considered if the relation be-
tween two entities from the test exists in the train-
ing corpus. This approach is not robust as it does
not consider unknown relations.
5 Results and Discussion
5.1 Identification of bacteria and biotopes
In this subsection, we present the results we
achieved on the development corpus (Table 2) to
identify bacteria and biotopes names without link-
ing those names to the concept in the OntoBiotope
ontology. We built the model on the training cor-
pus and applied it on the development corpus. The
evaluation has been done using the conlleval.pl
script6 (Tjong Kim Sang and Buchholz, 2000)
that has been created to evaluate the results in the
CoNLL-2000 Shared Task. We chose this script
because it takes as input a tabular file which is
commonly used in the machine-learning process.
Nevertheless, the script does not take into account
the offsets to evaluate the annotations, which is
the official way to evaluate the results. We give
in Table 7 the results we achieved. Those re-
sults show our system succeed to correctly iden-
tify the bacteria and biotopes names. Neverthe-
less, the biotopes names are more difficult to pro-
cess than the bacteria names. Similarly, Kolluru
et al (2011) achieved better results on the bacteria
category rather than on the habitat, confirming this
last category is more difficult to process.
6http://www.clips.ua.ac.be/conll2000/
chunking/
Category R P F
Bacteria 0.8794 0.9397 0.9085
Geographical 0.6533 0.7903 0.7153
Habitat 0.6951 0.8102 0.7482
Overall 0.7771 0.8715 0.8216
Table 7: Results on the bacteria biotopes identifi-
cation (development corpus)
There is still room for improvement, especially
in order to improve the recall in each category. We
plan to define some post-treatments so as to iden-
tify new entities and thus, increase the recall in
those three categories.
5.2 Official results
SER
Sub-task 1 0.66 4th/4
R P F
Sub-task 2 0.04 0.19 0.06 4th/4
Sub-task 3 0.04 0.12 0.06 2nd/2
Table 8: Official results and rank for LIMSI
5.2.1 Habitat entities normalization
General results. The first sub-task is evaluated
using the Slot Error Rate (Makhoul et al, 1999),
based on the exact boundaries of the entity to be
detected and the semantic similarity of the concept
from the ontology between reference and hypothe-
sis (Bossy et al, 2012b). This semantic similarity
is based on the ?is a? relation between two con-
cepts.
We achieved a 0.66 SER which places us 4th
out of four participants. Other participants ob-
tained SERs ranging from 0.46 to 0.49. Our sys-
tem achieved high precision (0.62) but low recall
(0.35). It produced two false positives and 144
false negatives. Out of 283 predicted habitats,
175.34 are correct. There was also a high number
of substitutions (187.66).
Correct entity, incorrect categorization. On
the entity boundaries evaluation, our system SER
(0.45) was similar to that of the other participants
(from 0.46 to 0.42). We achieved a 1.00 preci-
sion, a 0.56 recall and a 0.71 F-measure (the best
from all participants). Those results are consistent
with those we achieved on the development cor-
pus (Table 7) and confirm the benefit of using a
CRF-based system for entity detection.
148
While we correctly identified the habitat enti-
ties, the ontology categorization proved difficult:
we achieved an SER of 0.62 while other partic-
ipants obtained SERs ranging from 0.38 to 0.35.
For this task, we relied on exact match for map-
ping the concept to be categorized and the con-
cepts from the ontology, including both singular
and plural forms match. When no match was
found, because the categorization was mandatory,
we provided a default identifier?the first identi-
fier from the ontology?which is rarely correct.7
5.2.2 Relationships between entities
General results. The relation sub-task is evalu-
ated in terms of recall and precision for the pre-
dicted relations. On both second and third sub-
tasks, due to our too basic approach, we only
achieved a 0.06 F-measure. Obviously, because
considering only existing relations is not a robust
approach, the recall is very low (R=0.04). The
precision is not as high as we expected (P=0.19),
which indicates that if a relation exists in the train-
ing corpus for two entities, this relation does not
necessarily occur within the test for the two same
entities (two entities can occur in the same text
without any relation to be find between them). On
the second sub-task, other participants obtained
F-measures ranging from 0.42 to 0.27, while on
the third sub-task, the other participants obtained
a 0.14 F-measure, which underlines the difficulty
of the relation task.
Out of the two types of relation to be found,
this simple approach yielded better results for the
Localization relation (F=0.07) than for the PartOf
relation (F=0.02). While our results are probably
too bad to yield a definite conclusion, the results
of other participant also reflect a difference in per-
formance for relation Localization and PartOf.
Improvements. After fixing the technical prob-
lems we encountered, we plan to test other algo-
rithms such as SVM, which may be more adapted
for this kind of task.
6 Additional experiments
After the official submission, we carried out addi-
tional experiments.
7We gave the MBTO:00000001 identifier which is the id
for the concept ?gaz seep?.
6.1 Habitat entities normalization
6.1.1 Beyond exact match
The improvements we made on the habitat enti-
ties normalization are only based on the mapping
between the predicted concept and the ontology.
In our official submission, we only used an exact
match. We tried to produce a more flexible map-
ping in several ways.
First, we tried to normalize the mention gather-
ing all words from the mention into a single word.
Indeed, the concept ?rain forest? is not found in
the ontology while the concept ?rainforest? in one
word exists.
Second, we split the mention into single words
and tried matching based on the features listed be-
low, in order to manage the subsumption of con-
cepts.
? all words except the first one: ?savannah?
instead of ?brazilian savannah?,
? all words except the last one: ?glossina? in-
stead of ?glossina brevipalpis?,
? the last three words (we did not find example
in the corpus),
? the first three words: ?sugar cane fields? in-
stead of ?sugar cane fields freshly planted
with healthy plants?,
? the last two words: ?tsetse fly? instead of
?blood-sucking tsetse fly?,
? and the first two words: ?tuberculoid gran-
ulomas? instead of ?tuberculoid granulomas
with caseous lesions?.
If two parts of a mention can be mapped to two
concepts in the ontology, we added both concepts
in the output.
We also extended the coverage of the ontology
using the reference normalization from both train-
ing and development corpora, adding 316 entries
in the ontology. Those new concepts can be con-
sidered either as synonyms or as hyponyms:
? synonyms: ?root zone? is a synonym of ?rhi-
zosphere?. While only the second one occurs
in the ontology, we added the first concept
with the identifier from the second concept;
? hyponyms: ?bacteriologist? and ?entomol-
ogist? are both hyponyms of ?researcher?.
We gave the hypernym identifier to the hy-
ponym concepts.
149
At last, if no concept was found in the ontology,
instead of using the identifier of the first concept
in the ontology, we gave as a default identifier the
one of the more frequent concept in the corpora.8
This strategy improves system performance.
6.1.2 Results
The improvements we made allowed us to
achieved better results on the test corpus (table 9).
While on the official submission we achieved a
0.66 Slot Error Rate, we obtained a 0.53 SER
thanks to the improvements we made. This new
result does not lead us to obtain a better rank, but it
is closer to the ones the other participants achieved
(from 0.49 to 0.46).
Category
Official Additional
Evaluation Experiments
Substitution 187.66 121.99
Insertion 2 2
Deletion 144 144
Matches 175.34 241.01
Predicted 283 283
SER 0.66 0.53
Recall 0.35 0.48
Precision 0.62 0.85
F-measure 0.44 0.61
Table 9: Results on sub-task 1 on both the official
submission and the additional experiments
These improvements led us to obtain better re-
call, precision and F-measure. While our re-
call is still the lowest of all participants (0.48 vs.
[0.60;0.72]), our precision is the highest (0.85 vs.
[0.48;0.61]) and our F-measure is equal to the
highest one (0.61 vs. [0.57;0.61]).
6.2 Relationships between entities
6.2.1 Processing
On the relationships, as a first step, we fixed
the problems that prevented us to use the Max-
Ent model during the submission stage: (i) we
produced correct files for the algorithm, remov-
ing the geographical entities from our processing
accordingly with the guidelines, (ii) when deal-
ing with all possible combinations of entities that
can be linked together, we managed the relations
so as not to produce those relations in duplicate,
8The concept ?human? with identifier MBTO:00001402
is the more frequent concept in all corpora while the concept
?gaz seep? with identifier MBTO:00000001 was never used.
and (iii) we better managed the confidence score
given by the CRF on each relation.
6.2.2 Results
We produced new models on the training corpus
based on the following features: entities to be
linked, category of each entity, and whether a re-
lation between those entities exists in the training
corpus. We performed two evaluations of those
models: (i) on the development corpus, using the
official evaluation script, and (ii) on the test cor-
pus via the evaluation server.9 As presented in
Table 10, we achieved worse results (F=0.02 and
F=0.03) than our official submission (F=0.06) on
the test corpus.
#
Sub-task 2 Sub-task 3
Dev Test Test
1
R 0.18 0.11 0.06
P 0.49 0.01 0.01
F 0.26 0.02 0.01
2
R 0.58 0.02 0.02
P 0.77 0.16 0.33
F 0.66 0.03 0.04
Table 10: Results on sub-tasks 2 and 3 based on
the additional experiments (#1 and #2)
We also noticed that we achieved very poor re-
sults on the test corpus while the evaluation on the
development corpus provided promising results,
with a F-measure decreasing from 0.26 to 0.02 on
the first experiment, and from 0.66 to 0.04 on the
second one. The difference between the results
from both development and test corpora is hard to
understand. We have to perform additional anal-
yses on the outputs we produced to identify the
problem that occurred.
Moreover, we plan to use more contextual fea-
tures (specific words that indicate the relation, dis-
tance between two entities, presence of relative
pronouns, etc.) to improve the model. Indeed, in
relations between concepts, not only the concepts
must be studied but also the context in which they
occur as well as the linguistic features used in the
neighborhood of those concepts.
9The reference annotations from the test corpus will not
be released to the participants. Instead of those relations, an
evaluation server has been opened after the official evaluation
took place.
150
7 Conclusion
In this paper, we presented the methods we used
as first time participant to the BioNLP Bacteria
Biotopes Shared Task.
To detect bacteria and biotopes names, we used
a machine-learning approach based on CRFs. We
used several resources to build the model, among
them the NCBI taxonomy, the OntoBiotope on-
tology, the Cocoa annotations, and unsupervised
clusters created through Brown?s algorithm. The
normalization of the habitat names with the con-
cepts in the OntoBiotope ontology was performed
with a Perl script based on exact match of the en-
tity to be found, taking into account its plural form.
On this sub-task, we achieved a 0.66 Slot Error
Rate.
In order to process the relationships between en-
tities, our MaxEnt model was not ready for the of-
ficial submission. The simple approach we used
relies on the identification of the relation between
entities only if the relation exists in the training
corpus. This simple approach is not robust enough
to correctly process new data. On the relation sub-
tasks, due to the approach we used, we achieved a
0.06 F-measure.
On the first sub-task, we enhanced our habitat
entities normalization process, which led us to im-
prove our Slot Error Rate from 0.66 (official sub-
mission) to 0.53 (additional experiments).
On the relation detection, first, we plan to make
new tests with more features, including contextual
features. Second, we plan to test new algorithms,
such as SVM which seems to be relevant to pro-
cess relationships between entities.
Acknowledgments
This work has been done as part of the Quaero pro-
gram, funded by Oseo, French State Agency for
Innovation. I would like to thank the organizers
for their work and Aure?lie Ne?ve?ol for the proof-
read of this paper.
References
Adam L Berger, Stephen Della Pietra, and Vincent J
Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 shared task.
BMC Bioinformatics, 13(Suppl 11):S4.
Robert Bossy, Julien Jourde, Alain-Pierre Manine,
Philippe Veber, Erick Alphonse, Marteen van de
Guchte, Philippe Bessie`res, and Claire Ne?dellec.
2012a. BioNLP shared task ? the bacteria track.
BMC Bioinformatics, 13(Suppl 11):S3.
Robert Bossy, Claire Ne?dellec, and Julien Jourde,
2012b. Bacteria Biotope (BB) task at BioNLP
Shared Task 2013. Task proposal. INRA, Jouy-en-
Josas, France.
Peter F Brown, Vincent J Della Pietra, Peter V de
Souza, Jenifer C Lai, and Robert L Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?79.
Scott Federhen. 2012. The NCBI taxonomy database.
Nucleic Acids Res, 40(Database issue):D136?43.
Silviu Guiasu and Abe Shenitzer. 1985. The princi-
ple of maximum entropy. The Mathematical Intelli-
gence, 7(1).
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Juni?chi Tsujii. 2011.
Overview of BioNLP shared task 2011. In BioNLP
Shared Task 2011 Workshop Proc, pages 1?6, Port-
land, OR. ACL.
BalaKrishna Kolluru, Sirintra Nakjang, Robert P Hirt,
Anil Wipat, and Sophia Ananiadou. 2011. Auto-
matic extraction of microorganisms and their habi-
tats from free text using text mining workflows. J
Integr Bioinform, 8(2):184.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc of ICML.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. Proc of ACL,
pages 504?13, July.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, MIT.
John Makhoul, Francis Kubala, Richard Schwartz, and
Ralph Weischedel. 1999. Performance measures for
information extraction. In Proc. of DARPA Broad-
cast News Workshop, pages 249?52.
Nhung T. H. Nguyen and Yoshimasa Tsuruoka. 2011.
Extracting bacteria biotopes with semi-supervised
named entity recognition and coreference resolution.
In BioNLP Shared Task 2011 Workshop Proc, pages
94?101, Portland, OR. ACL.
Zorana Ratkovic, Wiktoria Golik, and Pierre Warnier.
2012. Event extraction of bacteria biotopes: a
knowledge-intensive NLP-based approach. BMC
Bioinformatics, 13(Suppl 11):S8.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
151
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared-task:
Chunking. In Proc of CoNLL-2000 and LLL-2000,
pages 127?32, Lisbon, Portugal.
152
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168?177,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Automatic Named Entity Pre-Annotation
for Out-of-Domain Human Annotation
Sophie Rosset?, Cyril Grouin?, Thomas Lavergne?,? , Mohamed Ben Jannet?,?,?,?
Je?re?my Leixa, Olivier Galibert? , Pierre Zweigenbaum?.
?LIMSI?CNRS ?Universite? Paris-Sud ?LNE
?LPP, Universite? Sorbonne Nouvelle ELDA
{rosset,grouin,lavergne,ben-jannet,pz}@limsi.fr
leixa@elda.org, olivier.galibert@lne.fr
Abstract
Automatic pre-annotation is often used to
improve human annotation speed and ac-
curacy. We address here out-of-domain
named entity annotation, and examine
whether automatic pre-annotation is still
beneficial in this setting. Our study de-
sign includes two different corpora, three
pre-annotation schemes linked to two an-
notation levels, both expert and novice an-
notators, a questionnaire-based subjective
assessment and a corpus-based quantita-
tive assessment. We observe that pre-
annotation helps in all cases, both for
speed and for accuracy, and that the sub-
jective assessment of the annotators does
not always match the actual benefits mea-
sured in the annotation outcome.
1 Introduction
Human corpus annotation is a difficult, time-
consuming, and hence costly process. This mo-
tivates research into methods which reduce this
cost (Leech, 1997). One such method consists of
automatically pre-annotating the corpus (Marcus
et al, 1993; Dandapat et al, 2009) using an ex-
isting system, e.g., a POS tagger, syntactic parser,
named entity recognizer, according to the task for
which the annotations aim to provide a gold stan-
dard. The pre-annotations are then corrected by
the human annotators. The underlying hypothe-
sis is that this should reduce annotation time while
possibly at the same time increasing annotation
completeness and consistency.
We study here corpus pre-annotation in a spe-
cific setting, out-of-domain named entity annota-
tion, in which we examine specific questions that
we present below. We produced corpora and an-
notation guidelines for named entities which are
both hierarchical and compositional (Grouin et al,
2011),1 and which we used in contrastive stud-
ies of news texts in French (Rosset et al, 2012).
We want to rely on the same named entity def-
initions for studies on two types of data we did
not cover: parliament debates (Europarl corpus)
and regional, contemporary written news (L?Est
Re?publicain), both in French. To help the annota-
tion process we could reuse our system (Dinarelli
and Rosset, 2011), but needed first to examine
whether a system trained on one type of text (our
first Broadcast News data) could be used to pro-
duce a useful pre-annotation for different types of
text (our two corpora).
We therefore set up the present study in which
we aim to answer the following questions linked
to this point and to related annotation issues:
? can a system trained on data from one spe-
cific domain be useful on data from another
domain in a pre-annotation task?
? does this pre-annotation help human annota-
tors or bias them?
? what importance can we give to the annota-
tors? subjective assessment of the usefulness
of the pre-annotation?
? can we observe differences in the use of pre-
annotation depending on the level of exper-
tise of human annotators?
Moreover, as the aforementioned annotation
scheme is based on two annotation levels (entities
and components), we want to answer these ques-
tions taking into account these two levels.
We first examine related work on pre-annotation
(Section 2), then present our corpora and annota-
tion task (Section 3). We describe and discuss ex-
periments in Section 4, and make subjective and
1Corpora, guidelines and tools are available through
ELRA under references ELRA-S0349 and ELRA-W0073.
168
quantitative observations in Sections 5 and 6. Fi-
nally, we conclude and present some perspectives
in Section 7.
2 Related Work
Facilitating human annotations has been the topic
of a large amount of research. Two different
approaches can be distinguished: active learn-
ing (Ringger et al, 2007; Settles et al, 2008) and
pre-annotation (Marcus et al, 1993; Dandapat et
al., 2009). Our work falls into the latter type.
Pre-annotation can be used in several ways. The
first is to provide annotations to be corrected by
human annotators (Fort and Sagot, 2010). A vari-
ant consists of merging multiple automatic anno-
tations before having them corrected by human
curators to produce a gold-standard (Rebholz-
Schuhmann et al, 2011). The second type con-
sists of providing clues to help human annotators
perform the annotation task (Mihaila et al, 2013).
This work addresses the first type, a single-
system pre-annotation with human correction. An
objective is to examine whether a system trained
on one type of text can be useful to pre-annotate
texts of a different type. Most previous studies
have been performed on well-behaved tasks such
as part-of-speech tagging on in-domain data, i.e.,
the model used for pre-annotating the target data
had been trained on similar data. For instance, Fort
and Sagot (2010) provide a precise evaluation of
the usefulness of pre-annotation and compare the
impact of different quality levels in POS taggers
on the Penn TreeBank corpus. They first trained
different models on the training part of the cor-
pus and applied them to the test corpus. The pre-
annotated test corpus was then corrected by hu-
mans. They reported gains in accuracy and inter-
annotator agreement. The study focused on the
minimal quality (accuracy threshold) of automatic
annotation that would prove useful for human an-
notation. They reported a gain for human annota-
tion when accuracy ranged from 66.5% to 81.6%.
On the contrary, for a semantic-frame annotation
task, Rehbein et al (2009) observed no significant
gain in quality and speed of annotation even when
using a state-of-the-art system.
Generally speaking, annotators find the pre-
annotation stage useful (Rehbein et al, 2009;
South et al, 2011; Huang et al, 2011). Anno-
tation managers consider that a bias may occur
depending on how much human annotators trust
the pre-annotation (Rehbein et al, 2009; Fort and
Sagot, 2010; South et al, 2011). In their frame-
semantic argument structure annotation, Rehbein
et al (2009) addressed a specific question consid-
ering a two-level annotation scheme: is the pre-
annotation of frame assignment (low-level anno-
tation) useful for annotating semantic roles (high-
level annotation)? Although for the low-level an-
notation task they observed a significant difference
in quality of final annotation, for the high-level
task they found no difference.
Most of these studies used a pre-annotation sys-
tem trained on the same kind of data as those
which were to be annotated manually. Neverthe-
less some system-oriented studies have focused on
the results obtained by systems trained on one type
of corpus and applied to another type of corpus,
e.g., for a Latin POS tagger (Poudat and Longre?e,
2009; Skj?rholt, 2011) or for a CoNLL named en-
tity tagger for German (Faruqui and Pado?, 2010)
for which the authors noticed noticed a reduc-
tion of the F-measure when going from in-domain
(newswire data, F=0.782 for their best system) to
out-of-domain (Europarl data, F=0.656).
One of our objectives is then to examine
whether a system trained on one type of text can
be useful to pre-annotate texts of a different type.
We set up experiments to study precisely the pos-
sible induced bias and whether the level of experi-
ence of the annotators would make a difference in
such a context. In this study, we used two different
kinds of corpora, which were both different from
the corpus used to train the pre-annotation system.
3 Task and corpus description
3.1 Task
In this work, we used the structured named entity
definition we proposed in a previous study (Grouin
et al, 2011): entities are both hierarchical (types
have subtypes) and compositional (types and com-
ponents are included in entities) as in Figure 1.
func.coll
org.ent
name
BEIde la
kind
analystes financiersles
Figure 1: Multi-level annotation of entity sub-
types (red tags) and components (blue tags): the
financial analysts of the EBI
169
This taxonomy of entity types is composed of
7 types (person, location, organization, amount,
time, production and function) and 32 sub-types
(individual person pers.ind vs. group of persons
pers.coll; administrative organization org.adm vs.
services org.ent; etc.). Types and subtypes consti-
tute the first level of annotation.
Within these categories, components are
second-level elements (kind, name, first.name,
etc.), and can never be used outside the scope of a
type or subtype element.
3.2 Corpora
Two French corpora were sampled from larger
ones:
Europarl: Prepared speech (Parliament
Debates?Europarl): 15,306 word extract;
Press: Local, contemporary written news (L?Est
Re?publicain): 11,146 word extract.
These corpora were automatically annotated us-
ing the system described in (Dinarelli and Rosset,
2011). This system relies on a Conditional Ran-
dom Field (CRF) model for the detection of com-
ponents and on a probabilistic context-free gram-
mar (PCFG) model for types and sub-types. These
models have been trained on Broadcast News data.
This system achieved a Slot Error Rate (Makhoul
et al, 1999) of 37.0% on Broadcast conversation
and 29.7% on Broadcast news, and ranked first in
the Quaero evaluation campaign (Galibert et al,
2011).
4 Experiments
In this section we present the protocol we designed
to study the usefulness of pre-annotation under
different conditions, and its overall results.
4.1 Protocol
We defined the following protocol, similar to the
one used in Rehbein et al (2009).
Corpora. Four versions of our two corpora were
prepared: (i) raw text, (ii) pre-annotation of
types, (iii) pre-annotation of components, and
(iv) full pre-annotation of both types and compo-
nents. Each of these four versions was split into
four quarters.
Annotators. Eight human annotators were in-
volved in this task. Among them, four are con-
sidered as expert annotators (they annotated cor-
pora in the previous years) while the four re-
maining ones are novice annotators (this was the
first time they annotated such corpora; they were
given training sessions before starting actual anno-
tation). We defined four pairs of annotators, where
each pair was composed of an expert and a novice
annotator.
Quarter allocation. We allocated each corpus
quarter in such a way that each pair of annotators
processed, in each corpus, material from each one
of the four pre-annotated versions (see Table 3).
The same allocation was made in both corpora.
4.2 Results
For each corpus part, a reference was built based
on a majority vote by confronting all annotations.
The resulting reference corpus is presented in Ta-
ble 1.
Corpus # comp. # types # entities # words
P
re
ss
Q1 481 310 791 3047
Q2 367 246 673 2628
Q3 495 327 822 2971
Q4 413 282 695 2600
E
ur
op
ar
l Q1 362 259 621 3926
Q2 309 221 530 3809
Q3 378 247 625 3604
Q4 413 299 712 3967
Table 1: General description of the reference an-
notations: number of components, types, entities
(the sum of components and types), and words
Table 2 presents the performance of the au-
tomatic pre-annotation system against the refer-
ence corpus. We used the well known F-measure
and in addition the Slot Error Rate as it allows
to weight different error classes (deletions, inser-
tions, type or frontier errors). Fort and Sagot
(2010) reported a gain in human annotation when
pre-annotation accuracy ranged from 66.5% to
81.6%. Given their results we can hope for a gain
in both accuracy and annotation time when using
pre-annotation.
Table 3 presents all results obtained by each an-
notators given each pre-annotation condition (raw,
components, types and full) in terms of precision,
recall and F-measure.
170
Corpus #
Raw text Components Types Full
R P F R P F R P F R P F
Press
Q1
0.874 0.777 0.823 0.876 0.741 0.803 0.824 0.870 0.846 0.852 0.800 0.825
0.810 0.766 0.788 0.815 0.777 0.796 0.645 0.724 0.683 0.844 0.785 0.813
Q2
0.765 0.796 0.780 0.870 0.773 0.819 0.822 0.801 0.812 0.917 0.773 0.839
0.558 0.654 0.602 0.826 0.775 0.800 0.815 0.777 0.795 0.816 0.752 0.783
Q3
0.835 0.715 0.771 0.888 0.809 0.847 0.884 0.796 0.837 0.887 0.859 0.873
0.792 0.689 0.736 0.904 0.780 0.837 0.876 0.771 0.820 0.780 0.827 0.803
Q4
0.802 0.757 0.779 0.845 0.876 0.860 0.900 0.702 0.789 0.914 0.840 0.876
0.794 0.727 0.759 0.696 0.715 0.705 0.812 0.701 0.752 0.802 0.757 0.779
Europarl
Q1
0.809 0.728 0.766 0.800 0.568 0.665 0.776 0.862 0.817 0.754 0.720 0.736
0.754 0.720 0.736 0.720 0.609 0.660 0.687 0.607 0.644 0.736 0.638 0.683
Q2
0.776 0.792 0.784 0.782 0.617 0.690 0.797 0.645 0.713 0.821 0.526 0.641
0.563 0.498 0.529 0.802 0.619 0.699 0.698 0.553 0.617 0.769 0.566 0.652
Q3
0.747 0.459 0.569 0.749 0.624 0.681 0.805 0.800 0.803 0.735 0.744 0.739
0.732 0.598 0.658 0.736 0.717 0.726 0.822 0.738 0.777 0.808 0.734 0.769
Q4
0.742 0.624 0.678 0.874 0.760 0.813 0.732 0.480 0.580 0.743 0.608 0.669
0.721 0.566 0.634 0.695 0.652 0.672 0.707 0.600 0.649 0.738 0.603 0.664
Table 3: Overall recall, precision and F-measure for each pair of annotators (blue: pair #1, ocre: pair
#2, green: pair #3, white: pair #4) on each corpus quarter (Q1, Q2, Q3, Q4), depending on the kind of
pre-annotation (raw text, only components, only types, full pre-annotation). Expert annotator is on the
upper line of each quarter, novice annotator is on the lower line. Boldface indicates the best F-measure
for each novice and expert annotator among all pre-annotation tasks in a given corpus quarter
Corpus
Components Types Full
F SER F SER F SER
P
re
ss
Q1 72.4 37.9 63.5 46.3 68.9 41.0
Q2 77.2 32.2 66.8 43.5 73.1 36.6
Q3 76.1 34.1 68.3 41.7 73.1 36.9
Q4 76.1 33.3 63.3 45.7 71.0 38.2
E
ur
op
ar
l Q1 61.9 49.9 57.5 55.4 60.1 52.2
Q2 61.2 51.3 54.6 54.3 58.5 52.5
Q3 61.6 50.1 53.3 55.7 58.2 52.2
Q4 57.1 57.0 48.1 59.7 53.3 58.1
Broad. 88.3 29.1 73.1 39.1 73.2 33.1
Table 2: F-measure and Slot Error Rate achieved
by the automatic system on each kind of annota-
tion and on in-domain broadcast data
We also computed inter-annotator agreement
(IAA) for each corpus considering two groups of
annotators, experts and novices. We consider that
the inter-annotator agreement is somewhere be-
tween the F-measure and the standard IAA con-
sidering as markables all the units annotated by at
least one of the annotators (Grouin et al, 2011).
We computed Scott?s Pi (Scott, 1955), and Co-
hen?s Kappa (Cohen, 1960). The former considers
one model for all annotators while the latter con-
siders one model per annotator. In our case, these
two values are almost the same, which means that
the proportions and kinds of annotations are very
similar across experts and novices. Figure 2 shows
the IAA (Cohen?s Kappa and F-measure) obtained
on the two corpora given the four pre-annotation
conditions (no pre-annotation, components, types,
and full pre-annotation). As we can see, IAA is
systematically higher for the Press corpus than
for the Europarl corpus, which can be linked
to the higher performance of the automatic pre-
annotation system on this corpus. We also can see
that pre-annotation always improves agreement
and that full pre-annotation yields the best result.
We observe that, as expected, pre-annotation leads
human annotators to obtain higher consistency.
5 Subjective assessment
An important piece of information in any anno-
tation campaign is the feelings of the annotators
about the task. This can give interesting clues
about the expected quality of their work and on the
usefulness of the pre-annotation step. We asked
the annotators a few questions concerning sev-
eral features of this project, such as the annotation
171
 0.5
 0.6
 0.7
 0.8
 0.9
 1
raw comp types full
Press: Cohen's kappaPress: F-measureEuroparl: Cohen's kappaEuroparl: F-measure
Figure 2: Cohen?s Kappa (red and blue) and F-
measure (green and pink) measuring agreement of
experts and novices on Press and Europarl corpora
in four pre-annotation conditions. Each measure
compares the concatenated annotations of the four
experts with the four novices.
manual, or how they assessed the benefits of pre-
annotation in the different corpora (Section 5.1).
Another important point is the experience of the
annotators, which we also examine in the light of
theirs answers to the questionnaire (Section 5.2).
5.1 Questionnaire
The questionnaire submitted to the annotators con-
tained 4 questions, dealing with their feedback on
the annotation process:
1. According to you, which level of pre-
annotation has been the most helpful during
the annotation process? Types, components,
or both?
2. To what extent would you say that pre-
annotation helped you in terms of precision
and speed? Did it produce many errors you
had to correct?
3. If you had to choose between the Europarl
corpus and the Press corpus, could you say
that one has been easier to annotate than the
other?
4. Concerning the annotation manual, are there
topics that you would like to change, or cor-
rect? In the same way, which named entities
caused you the most difficulties to deal with?
All 8 annotators answered these questions. We
summarize below what we found in their answers.
5.1.1 Level of pre-annotation
Most of the annotators preferred the corpora that
were pre-annotated with types only. The reason,
for the most part, is that a pre-annotation of types
allows the annotator to work faster on their files,
because guessing the components from the types
is easier than guessing types from components.2
Indeed, the different types of entities defined in
the manual always imply the same components,
be they specific (to one entity type) or transverse
(common to several entity types). On the contrary,
a transverse component, such as <kind>, can be
part of any type of named entity. The other rea-
son for this choice of pre-annotation concerns the
readability brought to the corpora. An annotation
with types only is easier to read than an annotation
with components, and less exhausting after many
hours of work on the texts.
5.1.2 Gain in precision and speed
What motivated the answers to the second ques-
tion mainly concerns the accuracy of the different
pre-annotation methods. While all of them pre-
sented errors that needed to be corrected, the pre-
annotation of types was the one that they felt pre-
sented the smaller number of errors. Thus, annota-
tors spent less time reviewing the corpora in search
of errors, compared to the other pre-annotated cor-
pora (with components, and with both types and
components), where more errors had to be spot-
ted and corrected. This search for incorrect pre-
annotations impacted the time spent on each cor-
pus. Indeed, most annotators declared that pre-
annotation with types was quicker to deal with
than other pre-annotation schemes.
5.1.3 Corpus differences
About one half of the annotators agreed that the
Europarl corpus had been more difficult to anno-
tate. Despite obvious differences in register, sen-
tence structure and vocabulary between the two
corpora, Europarl seemed more redundant and
complex than the other corpus. For instance, one
of the annotators declared:
The Europarl corpus is more difficult
to annotate in the sense that the exist-
ing types and components do not al-
ways match the realities found in the
corpus, either because their definitions
2This feeling is supported by results about ambiguity pre-
sented in Fort et al (2012).
172
cannot apply exactly, or because the re-
quired types and components are miss-
ing (mainly for frequencies: ?five times
per year?).
The other half of the annotators did not feel any
specific difficulties in annotating one corpus or the
other. According to them, both corpora are the
same in terms of register and sentence structure.
5.1.4 Improvements in guidelines
All of the annotators were unanimous in think-
ing that two points need to be modified in the
manual. First of all, the distinction between the
<org.adm> and <org.ent> subtypes is too diffi-
cult to apprehend, above all in the Europarl corpus
where these entities are too ambiguous to be anno-
tated correctly. Secondly, the distinction between
the <pers> and <func> types has also been diffi-
cult to deal with. The other remarks about poten-
tial changes mainly concerned the introduction of
explicit rules for frequencies, which are recurrent
in the Europarl corpus.
5.2 Experience
As mentioned earlier in Section 4.1, we will now
see if the differences in experience between an-
notators impacted their difficulty in annotating the
corpora. First of all, when we look at the answers
given to question 3, we notice that both novice and
expert annotators consider the Europarl corpus the
most difficult to annotate. Most of their answers
deal with the redundancy and the formal register
of the data. Moreover, as everyone answered in
question 4, both <func> and <org> entities have
to be modified to be easier to understand and to
use. This unanimous opinion about what needs
to be reviewed in the manual allows us to think
that the annotators? level of experience has a low
impact on their apprehension of the corpora, both
Europarl and Press. To confirm this, we can look
at the answers given to questions 1 and 2, as indi-
cated in the previous paragraph. As has been ex-
plained, every annotator correctly pointed at the
many errors found in pre-annotation, regardless
of their experience. Besides, the assessment of
the benefits of pre-annotation is the same for al-
most everyone, regardless of their experience too:
both novice and expert annotators agree that pre-
annotation with type adds efficiency and speed to
annotation.
To conclude, according to our observations
based on the questionnaire, we cannot assert that
there has been a difference between novice and ex-
pert annotators. Both groups agreed on the same
difficulties, pointed at the same errors, and crit-
icized the same entities, saying that their defini-
tions needed to be clarified.
6 Quantitative observations
In this section we provide results of quantitative
observations in order to support, or not, the anno-
tators? subjective assessment.
6.1 Corpus statistics
The annotators reported different feelings depend-
ing on the corpora. Some of them reported that
the Europarl corpus was more difficult to annotate,
with more complex sentence structures, or usage
of fewer proper nouns.
To explore these differences, we computed
some statistics over the two original, un-annotated
corpora (which are much larger than the samples
annotated in this experiment) as well as over the
original broadcast news corpus used to train the
pre-annotation system. Each of these corpora con-
tains several million words.
Table 4 reports simple statistics about sentences
in the three corpora. Based on these statistics,
while the Europarl (Euro) corpus is very similar to
the original Broadcast News (BN), the Press cor-
pus shows differences: sentences are 20% shorter,
with fewer but larger chunks, confirming the im-
pression of simpler, less convoluted sentences.
BN Press Euro
Mean sentence length 30.2 23.9 29.7
Mean chunk count 10.9 6.7 10.4
Mean chunk length 2.7 3.6 2.8
Table 4: Sentence summary of the three corpora
Looking more closely at the contents of these
sentences, Figure 3 summarizes the proportions of
grammatical word classes. The sentiment of ex-
tensive naming of entities in the Press corpus is
confirmed by the four times higher rate of proper
nouns. On the other hand, entities are more often
referred to using nouns with an optional adjective
in the Europarl corpus, leading to a more frequent
usage of the latter.
173
Figure 3: Frequency of word classes in the three
corpora (BN = Broadcast News, Est = Press, Euro
= Europarl). TOOL = grammatical words, PCT/NB
= punctuation and numbers, ADJ/ADV = adjectives
and adverbs, NAM = proper name, NOM = noun,
VER = verb.
6.2 Influence of pre-annotation on the
behaviour of annotators
As already mentioned, it is often reported that a
bias may occur depending on human confidence
in the pre-annotation (Fort and Sagot, 2010; Re-
hbein et al, 2009; South et al, 2011). An im-
portant unknown is always the influence of pre-
annotation on the behaviour of annotators, and at
which point pre-annotation induces more errors
than it helps. This may obviously depend on pre-
annotation quality. Table 5 summarizes the er-
ror rates of the automatic annotator in the stud-
ied data (Press + Europarl) and in comparison to
in-domain data. Insertions (Ins) are extra anno-
tations, deletions (Del) missing annotations, and
substitutions (Subs) are annotations that are incor-
rect in type, boundaries, or both. We can see that
Domain Pre-annotation Ins Del Subs
Components 4.4% 33.6% 7.8%
Out Types 7.0% 36.2% 12.7%
Full 5.5% 34.6% 9.7%
In Full 3.7% 23.4% 10.6%
Table 5: Pre-annotation errors and comparison
with in-domain (Broadcast News) data
going out-of-domain increased deletions, proba-
bly through a lack of knowledge of domain vo-
cabulary. But it did not influence the other error
rates significantly. It is also noticeable that dele-
tion is the type of error most produced by the sys-
tem, with every third entity missed. Automatic,
full pre-annotation of Press + Europarl obtains a
precision of 0.79 and a recall of 0.56.
Human annotator performance can then be mea-
sured over the same three error types (Table 6). We
Pre-annotation Ins Del Subs
Raw 8.9% 18.9% 12.8%
Components 5.9% 16.7% 11.3%
Types 7.1% 16.5% 12.0%
Full 7.1% 16.5% 10.1%
Table 6: Mean human annotation error levels for
each pre-annotation scheme
can see that annotation quality was systematically
improved by pre-annotation, with the best global
result obtained by full pre-annotation. In addition
there was no increase in deletions (had the human
stopped looking at the unannotated text) or inser-
tions (had the human always trusted the system) as
might have been feared. This may be a side effect
of the high deletion rate, making it obvious to the
human that the system was missing things. In any
case, the annotation was clearly beneficial in our
experiment with no ill effects seen in error rates
compared to the gold standard.
6.3 Is pre-annotation useful and to whom?
All annotators asserted that pre-annotation is use-
ful, specifically with types. In this section, we pro-
vide observations concerning variations in annota-
tion both in terms of accuracy (F-measure is used)
and duration.
Raw Comp. Types Full
Experts 0.748 0.786 0.778 0.791
Novices 0.682 0.737 0.721 0.742
Table 7: Mean F-measure of experts and novices,
for each pre-annotation scheme
Raw Comp. Types Full
Experts 109.0 52.5 64.0 39.13
Novices 151.7 135.5 117.9 103.88
Table 8: Mean duration (in minutes) of annotation
for experts and novices, for each pre-annotation
scheme (two corpus quarters)
Tables 7 and 8 confirm the hypothesis that auto-
matic pre-annotation helps annotators to annotate
174
faster and to be more efficient. All pre-annotation
levels (components, types and both) seem to be
helpful for both experts and novices. Experts
reached a higher accuracy (F=0.791) and they
were more than twice faster with components or
full pre-annotation. Similarly, novices performed
better when working on a full pre-annotation
(F=0.742) and reached a faster working time
(48mn less than with no pre-annotation). This last
observation contradicts the annotators? reported
experience: the annotators felt more comfortable
and faster with a types-only pre-annotation than
with full pre-annotation (see Section 5.1.2). The
results show that full pre-annotation was the best
choice for both quality and speed.
These results confirm that pre-annotation is use-
ful, even with a moderate level of performance of
the system. Does it help to annotate components
and types equally? To answer this question, we
computed the F-measure of novices and experts
for both components and types separately (see Fig-
ure 4).
 60
 65
 70
 75
 80
 85
 90
raw comp types full
types/novicescomponents/novicestypes/expertscomponents/experts
Figure 4: Mean F-measure on each pre-annotation
level for expert and novice annotators
For experts we can see that all pre-annotation
levels allow them to improve their performance on
both types and components. However for novices,
pre-annotation with types does not improve their
performance in labeling components. We also no-
tice that pre-annotation in both types and compo-
nents allows experts and novices to reach their best
performance for both types and components.
7 Conclusion and Perspectives
Conclusion. In this paper, we studied the inter-
est of a pre-annotation process for a complex an-
notation task with only an out-of-domain annota-
tion system available. We also designed our exper-
iments to check whether the level of experience of
the annotators made a difference in such a context.
The experiment produced in the end a high-quality
gold standard (8-way merge including 2 versions
without pre-annotation) which enabled us to mea-
sure quantitatively the performance of every pre-
annotation scheme.
We noticed that the pre-annotation system
proved relatively precise for such a complex task,
with 79% correct pre-annotations, but with a poor
recall at 56%. This may be a good operating point
for a pre-annotation system to reduce bias though.
In our quantitative experiments we found that
the fullest pre-annotation helped most, both in
terms of quality and annotation speed, even though
the quality of the pre-annotation system varied de-
pending on the annotation layer. This contradicted
the feelings of the annotators who thought that a
type-only pre-annotation was the most efficient.
This shows that in such a setting self-evaluation
cannot be trusted. On the other hand their remarks
about the problems in the annotation guide itself
seemed rather pertinent.
When it comes to experts vs. novices, we noted
that their behaviour and remarks were essentially
identical. Experts were both better and faster
at annotating, but had similar reactions to pre-
annotation and essentially the same feelings.
In conclusion, even with an out-of-domain sys-
tem, a pre-annotation step proves extremely useful
in both annotation speed and annotation quality,
and at least in our setting, with a reasonably pre-
cise system (at the expense of recall) no bias was
detectable. In addition, no matter what the anno-
tators feel, as long as precision is good enough,
the more pre-annotations the better. Pre-filtering
either of our two levels did not help.
Perspectives. Based upon this conclusion, we
plan to use automatic pre-annotation in further an-
notation work, beginning with the present corpora.
As a first use, we plan to propose a few changes
to the annotation principles in the guidelines we
used. To annotate existing corpora with these
changes, automatic pre-annotation will be useful.
As a second piece of future work, we plan to
annotate new corpora with the existing annotation
framework. We also plan to add new types of
named entities (e.g., events) to extend the anno-
tation of existing annotated corpora, using the pre-
annotation process to reduce the overall workload.
175
Acknowledgments
This work has been partially funded by OSEO un-
der the Quaero program and by the French ANR
VERA project.
References
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex linguistic an-
notation ? no easy way out! A case from Bangla and
Hindi POS labeling tasks. In Proc of 3rd Linguistic
Annotation Workshop (LAW-III), pages 10?18, Sun-
tec, Singapore, August. ACL.
Marco Dinarelli and Sophie Rosset. 2011. Models
cascade for tree-structured named entity detection.
In Proc of IJCNLP, pages 1269?1278, Chiang Mai,
Thailand.
Manaal Faruqui and Sebastian Pado?. 2010. Training
and evaluating a German named entity recognizer
with semantic generalization. In Proc of Konvens,
Saarbru?cken, Germany.
Kare?n Fort and Beno??t Sagot. 2010. Influence of pre-
annotation on POS-tagged corpus development. In
Proc of 4th Linguistic Annotation Workshop (LAW-
IV), pages 56?63, Uppsala, Sweden. ACL.
Kare?n Fort, Adeline Nazarenko, and Sophie Rosset.
2012. Modeling the complexity of manual anno-
tation tasks: a grid of analysis. In Proceedings of
COLING 2012, pages 895?910, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Olivier Galibert, Sophie Rosset, Cyril Grouin, Pierre
Zweigenbaum, and Ludovic Quintard. 2011. Struc-
tured and extended named entity evaluation in au-
tomatic speech transcriptions. In Proc of IJCNLP,
Chiang Mai, Thailand.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Kare?n Fort, Olivier Galibert, and Ludovic Quin-
tard. 2011. Proposal for an extension of tradi-
tional named entities: From guidelines to evalua-
tion, an overview. In Proc of 5th Linguistic Anno-
tation Workshop (LAW-V), pages 92?100, Portland,
OR. ACL.
Minlie Huang, Aure?lie Ne?ve?ol, and Zhiyong Lu.
2011. Recommending MeSH terms for annotating
biomedical articles. Journal of the American Medi-
cal Informatics Association, 18(5):660?7.
Geoffrey Leech. 1997. Introducing corpus annota-
tion. In Roger Garside, Geoffrey Leech, and Tony
McEnery, editors, Corpus annotation: Linguistic in-
formation from computer text corpora, pages 1?18.
Longman, London.
John Makhoul, Francis Kubala, Richard Schwartz, and
Ralph Weischedel. 1999. Performance measures for
information extraction. In Proc. of DARPA Broad-
cast News Workshop, pages 249?252.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn TreeBank. Com-
putational Linguistics, 19(2):313?330.
Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. Biocause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14:2.
Ce?line Poudat and Doninique Longre?e. 2009. Vari-
ations langagie`res et annotation morphosyntaxique
du latin classique. Traitement Automatique des
Langues, 50(2):129?148.
Dietrich Rebholz-Schuhmann, Antonio Jimeno, Chen
Li, Senay Kafkas, Ian Lewin, Ning Kang, Peter Cor-
bett, David Milward, Ekaterina Buyko, Elena Beiss-
wanger, Kerstin Hornbostel, Alexandre Kouznetsov,
Rene? Witte, Jonas B Laurila, Christopher JO Baker,
Cheng-Ju Kuo, Simone Clematide, Fabio Rinaldi,
Richa?rd Farkas, Gyo?rgy Mo?ra, Kazuo Hara, Laura I
Furlong, Michael Rautschka, Mariana Lara Neves,
Alberto Pascual-Montano, Qi Wei, Nigel Collier,
Md Faisal Mahbub Chowdhury, Alberto Lavelli,
Rafael Berlanga, Roser Morante, Vincent Van Asch,
Walter Daelemans, Jose? L Marina, Erik van Mulli-
gen, Jan Kors, and Udo Hahn. 2011. Assessment of
NER solutions against the first and second CALBC
silver standard corpus. J Biomed Semantics, 2.
Ines Rehbein, Josef Ruppenhofer, and Caroline
Sporleder. 2009. Assessing the benefits of partial
automatic pre-labeling for frame-semantic annota-
tion. In Proc of 3rd Linguistic Annotation Workshop
(LAW-III), pages 19?26, Suntec, Singapore. ACL.
Eric Ringger, Peter McClanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, Kevin
Seppi, and Deryle Lonsdale. 2007. Active learning
for part-of-speech tagging: Accelerating corpus an-
notation. In Proc of Linguistic Annotation Workshop
(LAW), pages 101?108. ACL.
Sophie Rosset, Cyril Grouin, Kare?n Fort, Olivier Gal-
ibert, Juliette Kahn, and Pierre Zweigenbaum. 2012.
Structured named entities in two distinct press cor-
pora: Contemporary broadcast news and old news-
papers. In Proc of 6th Linguistic Annotation Work-
shop (LAW-VI), pages 40?48, Jeju, South Korea.
ACL.
William A Scott. 1955. Reliability of content analysis:
The case of nominal scale coding. Public Opinion
Quaterly, 19(3):321?325.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proc
of the NIPS Workshop on Cost-Sensitive Learning.
176
Arne Skj?rholt. 2011. More, faster: Accelerated cor-
pus annotation with statistical taggers. Journal for
Language Technology and Computational Linguis-
tics, 26(2):151?163.
Brett R South, Shuying Shen, Robyn Barrus, Scott L
DuVall, O?zlem Uzuner, and Charlene Weir. 2011.
Qualitative analysis of workflow modifications used
to generate the reference standard for the 2010
i2b2/VA challenge. In Proc of AMIA.
177
LAW VIII - The 8th Linguistic Annotation Workshop, pages 54?58,
Dublin, Ireland, August 23-24 2014.
Optimizing annotation efforts to build reliable annotated corpora
for training statistical models
Cyril Grouin
1
Thomas Lavergne
1,2
Aur
?
elie N
?
ev
?
eol
1
1
LIMSI?CNRS, 91405 Orsay, France
2
Universit?e Paris Sud 11, 91400 Orsay, France
firstname.lastname@limsi.fr
Abstract
Creating high-quality manual annotations on text corpus is time-consuming and often requires the
work of experts. In order to explore methods for optimizing annotation efforts, we study three key
time burdens of the annotation process: (i) multiple annotations, (ii) consensus annotations, and
(iii) careful annotations. Through a series of experiments using a corpus of clinical documents
annotated for personally identifiable information written in French, we address each of these
aspects and draw conclusions on how to make the most of an annotation effort.
1 Introduction
Statistical and Machine Learning methods have become prevalent in Natural Language Processing (NLP)
over the past decades. These methods sucessfully address NLP tasks such as part-of-speech tagging or
named entity recognition by relying on large annotated text corpora. As a result, developping high-
quality annotated corpora representing natural language phenomena that can be processed by statistical
tools has become a major challenge for the scientific community. Several aspects of the annotation task
have been studied in order to ensure corpus quality and affordable cost. Inter-annotator agreement (IAA)
has been used as an indicator of annotation quality. Early work showed that the use of automatic pre-
annotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation
guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006).
Efforts have investigated methods to reduce the human workload while annotating corpora. In par-
ticular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most
benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double an-
notation and found that double annotation could be limited to carefully selected portions of a corpus.
They produced an algorithm that automatically selects portions of a corpus for double annotation. Their
approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and
maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic
pre-annotations was shown to increase annotation consistency and result in producing quality annotation
with a time gain over annotating raw data (Fort and Sagot, 2010; N?ev?eol et al., 2011; Rosset et al., 2013).
With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there
are ethic aspects to consider in addition to technical and monetary cost when using a microworking plat-
form for annotation. While selecting the adequate methods for computing IAA is important (Artstein
and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to
all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate
annotation confidence based on annotator modeling. Overall, past work shows that creating high-quality
manual annotations is time-consuming and often requires the work of experts. The time burden is dis-
tributed between the sheer creation of the annotations, the act of producing multiple annotations for the
same data and the subsequent analysis of multiple annotations to resolve conflicts, viz. the creation of a
consensus. Research has addressed methods for reducing the time burden associated to these annotation
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
54
activities (for example, adequate annotation tools such as automatic pre-annotations can reduce the time
burden of annotation creation) with the final goal of producing the highest quality of annotations.
In contrast, our hypothesis in this work is that annotations are being developed for the purpose of train-
ing a machine learning model. Therefore, our experiments consist in training a named entity recognizer
on a training set comprising annotations of varying quality to study the impact of training annotation
quality on model performance. In order to explore methods for optimizing annotation efforts for the de-
velopment of training corpora, we revisit the three key time burdens of the annotation process on textual
corpora: (i) careful annotations, (ii) multiple annotations, and (iii) consensus annotations. Through a
series of experiments using a corpus of French clinical documents annotated for personally identifiable
information (PHI), we address each of these aspects and draw conclusions on how to make the most of
an annotation effort.
2 Material and methods
2.1 Annotated corpus
Experiments were conducted with a corpus of clinical documents in French annotated for 10 categories of
PHI. The distribution of the categories over the corpus varies with some categories being more prevalent
than others. In addition, the performance of entity recognition for each type of PHI also varies (Grouin
and N?ev?eol, 2014). The datasets were split to obtain a training corpus (200 documents) and a test
corpus (100 documents). For all documents in the training corpus, three types of human annotations
are available: annotations performed independently by two human annotators and consensus annotations
obtained after adjudication to resolve conflicts between the two annotators. Inter-annotator agreement
on the training corpus was above 85% F-measure, which is considered high (Artstein and Poesio, 2008).
The distribution of annotations over all PHI categories on both corpora (train/dev) is: address
(188/100), zip code (197/97), date (1025/498), e-mail (119/57), hospital (448/208), identifier (135/76),
last name (1855/855), first name (1568/724), telephone (802/386) and city (450/217).
2.2 Automatic Annotation Methods
We directly applied the MEDINA rule-based de-identification tool (Grouin, 2013) to obtain baseline
automatic annotations. We used the CRF toolkit Wapiti (Lavergne et al., 2010) to train a series of models
on the various sets of annotations available for the training corpus.
Features set For each CRF experiment, we used the following set of features with a l1 regularization:
? Lexical features: unigram and bigram of tokens;
? Morphological features: (i) the token case (all in upper/lower case, combination of both), (ii) the
token is a digit, (iii) the token is a punctuation mark, (iv) the token belongs to a specific list (first
name, last name, city), (v) the token was not identified in a dictionary of inflected forms, (vi) the
token is a trigger word for specific categories (hospital, last name) ;
? Syntactic features: (i) the part-of-speech (POS) tag of the token, as provided by the Tree Tagger
tool (Schmid, 1994), (ii) the syntactic chunk the token belongs to, from a home made chunker
based upon the previouses POS tags;
? External features: (i) we created 320 classes of tokens using Liang?s implementation (Liang, 2005)
of the Brown clustering algorithm (Brown et al., 1992), (ii) the position of the token within the
document (begining, middle, end).
Design of experiments The models were built to assess three annotation time-saving strategies:
1. Careful annotation: (i) AR=based on automatic annotations from the rule-based system,
(ii) AR?H2=intersection of automatic annotations from the rule-based system with annotations
from annotator 2. This model captures a situation where the human annotator would quickly revise
the automatic annotations by removing errors: some annotations would be missing (average recall),
but the annotations present in the set would be correct (very high precision), (iii) ARH2=automatic
annotations from the rule-based system, with replacement of the three most difficult categories by
55
annotations from annotator 2. This model captures a situation where the human annotator would
focus on revising targeted categories, and (iv) ARHC=automatic annotations from the rule-based
system, with replacement of the three most difficult categories by consensus annotations;
2. Double annotation: (i) H1=annotations from annotator 1, (ii) H2=annotations from annotator 2,
(iii) H12=first half of the annotations from annotator 1, second half from annotator 2, and
(iv) H21=first half of the annotations from annotator 2, second half from annotator 1;
3. Consensus annotation: (i) H1?H2=all annotations from annotator 1 and 2 (concatenation without
adjudication), and (ii) HC=consensus annotations (after adjudication between annotator 1 and 2).
3 Results
Table 1 presents an overview of the global performance of each annotation run (H12 and H21 achieved
similar results) across all PHI categories in terms of precision, recall and F
1
-measure (Manning and
Sch?utze, 2000). Table 2 presents the detailed performance of each annotation run for individual PHI
categories in terms of F-measure.
Baseline AR AR?H2 ARH2* ARHC H1* H12 H1?H2 H2 HC
Precision .820 .868 .920 .942 .943 .959 .962 .969 .974 .974
Recall .806 .796 .763 .854 .854 .927 .934 .935 .936 .942
F-measure .813 .830 .834 .896 .896 .943 .948 .951 .955 .958
Table 1: Overall performance for all automatic PHI detection. A star indicates statistically significant
difference in F-measure over the previous model (Wilcoxon test, p<0.05)
Category Baseline AR AR?H2 ARH2 ARHC H1 H12 H1?H2 H2 HC
Address .648 .560 .000 .800 .800 .716 .744 .789 .795 .791
Zip code .950 .958 .947 .964 .958 .974 .984 .974 .984 .990
Date .958 .968 .962 .963 .967 .965 .963 .963 .959 .970
E-mail .937 .927 .927 .927 .927 1.000 1.000 1.000 1.000 1.000
Hospital .201 .248 .039 .856 .868 .789 .809 .856 .861 .867
Identifier .000 .000 .000 .762 .797 .870 .892 .823 .836 .876
Last name .816 .810 .834 .832 .828 .953 .957 .954 .961 .963
First name .849 .858 .900 .901 .902 .960 .956 .961 .965 .960
Telephone 1.000 .980 .978 .983 .980 .987 .994 .999 .999 1.000
City .869 .874 .883 .887 .887 .948 .972 .962 .965 .972
Table 2: Performance per PHI category (F-measure)
4 Discussion
4.1 Model performance
Overall, the task of automatic PHI recognition has been well studied and the rule-based tool provides a
strong baseline with .813 F-measure on the test set. Table 1 shows that there are three different types
of models, in terms of performance: the lower-performing category corresponds to models with no hu-
man input. The next category corresponds to models with some human input, and the higher-performing
models correspond to models with the most human input. This reflects the expectation that model per-
formance increases with training corpus quality. However, it also shows that, within the two categories
that include human input, there is no statistical difference in model performance with respect to the type
of human input. We observed that the model trained on annotations from the H2 human annotator per-
formed better (F=0.955) than the model trained on annotations from the H1 annotator (F=0.943). This
observation reflects the agreement of the annotators with consensus annotations, where H2 had higher
agreement than H1 (Grouin and N?ev?eol, 2014). This is also true at the category level: H2 achieved
56
higher agreement with the consensus compared to H1 on categories ?address? (F=0.985>0.767) and
?hospital? (F=0.947>0.806) but H2 had lower agreement with the consensus on the category ?identifier?
(F=0.840<0.933).
4.2 Error Analysis
The performance of CRF models depends on the size of the training corpus and the level of diversity of
the mentions. Error analysis on our test data shows that a few specific mentions are not tagged in the test
corpus, even though they occur in the training corpus. For example, some hospital names occur in the
clinical narratives either as acronyms or as full forms (e.g. ?GWH? for ?George Washington Hospital?
in transfer patient from GWH). The acronyms are overall much less prevalent than the full forms and
also happen to be difficult to identify for human annotators (depending on the context, a given acronym
could refer to either a medical procedure, a physician or a hospital). We observed that the only hospital
acronym present in the test corpus was not annotated by any of the CRF models. Nevertheless, only five
occurrences of this acronym were found in the training corpus which is not enough for the CRF to learn.
Other errors occur in recognizing sequences of doctors? names that appear without separators in sig-
natures lines at the end of documents (e.g. ?Jane BROWN John DOE Mary SMITH?). In our test set
we observed that models trained on automatic annotations correctly predicted the beginning of such se-
quences and then produced erroneous predictions for the rest of the sequence (models AR, AR?H2,
ARHC and ARH2). In contrast, models built on human annotations produced correct predictions on the
entire sequence (models H1, H12, H1?H2, H2 and HC). Similarly, for last names containing a nobiliary
particle, the models trained on automatic annotations only identified part of the last name as a PHI. We
also observed that spelling errors (e.g. ?Jhn DOE?) only resulted in correct predictions from the mod-
els trained on the human annotations. We did not find cases where the models built on the automatic
annotations performed better than the models built on the human annotations.
4.3 Annotation strategy
Table 1 indicates that for the purpose of training a machine learning entity recognizer, all types of human
input are equivalent. In practice, this means that double annotations or consensus annotations are not
necessary. The high inter-annotator agreement on our dataset may be a contributing factor for this finding.
Indeed, (Esuli et al., 2013) found that with low inter-annotator agreement, models are biased towards
the annotation style of the annotator who produced the training data. Therefore, we believe that inter-
annotator should be established on a small dataset before annotators work independently. Table 2 shows
that using human annotations for selected categories results in strong improvement of the performance
over these categories (?address?, ?hospital? and ?identifier? categories in ARHC and ARH2 vs. AR) with
little impact on the performance of the model on other categories. Therefore, careful human annotations
are not necessarily needed for the entire corpus. Targeting ?hard? categories for human annotations can
be a good time-saving strategy. While the difference between the models using some human input vs.
all human input is statistically significant, the performance gain is lower than between models without
human input and some human input. Using data with partial human input for training statistical models
can cut annotation cost.
5 Conclusion and future work
Herein we have shown that full double annotation of a corpus is not necessary for the purpose of training a
competitive CRF-based model. Our results suggest that a three-step annotation strategy can optimize the
annotation effort: (i) double annotate a small subset of the corpus to ensure human annotators understand
the guidelines; (ii) have annotators work independently on different sections of the corpus to obtain wide
coverage; and (iii) train a machine-learning based model on the human annotations and apply this model
on a new dataset.
In future work, we plan to re-iterate these experiments on a different type of entity recognition task
where inter-annotator agreement may be more difficult to achieve, and may vary more between categories
in order to investigate the influence of inter-annotator-agreement on our conclusions.
57
Acknowledgements
This work was supported by the French National Agency for Research under grant CABeRneT
1
ANR-
13-JS02-0009-01 and by the French National Agency for Medicines and Health Products Safety under
grant Vigi4MED
2
ANSM-2013-S-060.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4):555?596.
Peter F Brown, Vincent J Della Pietra, Peter V de Souza, Jenifer C Lai, and Robert L Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?79.
Dmitriy Dligach and Martha Palmer. 2011. Reducing the need for double annotation. In Proc of Linguistic
Annotation Workshop (LAW), pages 65?73. Association for Computational Linguistics.
Andrea Esuli, Diego Marcheggiani, and Fabrizio Sebastiani. 2013. An enhanced CRFs-based system for informa-
tion extraction from radiology reports. J Biomed Inform, 46(3):425?35, Jun.
Kar?en Fort and Beno??t Sagot. 2010. Influence of pre-annotation on POS-tagged corpus development. In Proc of
Linguistic Annotation Workshop (LAW), pages 56?63. Association for Computational Linguistics.
Kar?en Fort, Gilles Adda, and Kevin Bretonnel Cohen. 2011. Amazon Mechanical Turk: Gold Mine or Coal Mine?
Computational Linguistics, pages 413?420.
Cyril Grouin and Aur?elie N?ev?eol. 2014. De-identification of clinical notes in french: towards a protocol for
reference corpus development. J Biomed Inform.
Cyril Grouin. 2013. Anonymisation de documents cliniques : performances et limites des m?ethodes symboliques
et par apprentissage statistique. Ph.D. thesis, University Pierre et Marie Curie, Paris, France.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the
48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513. Association for
Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, MIT.
Chiristopher D. Manning and Hinrich Sch?utze. 2000. Foundations of Statistical Natural Language Processing.
MIT Press, Cambridge, Massachusetts.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
English: the Penn TreeBank. Computational Linguistics, 19(2):313?330.
Aur?elie N?ev?eol, Rezarta Islamaj Do?gan, and Zhiyong Lu. 2011. Semi-automatic semantic annotation of PubMed
queries: a study on quality, efficiency, satisfaction. J Biomed Inform, 44(2):310?8.
Sophie Rosset, Cyril Grouin, Thomas Lavergne, Mohamed Ben Jannet, J?er?emy Leixa, Olivier Galibert, and Pierre
Zweigenbaum. 2013. Automatic named entity pre-annotation for out-of-domain human annotation. In Proc of
Linguistic Annotation Workshop (LAW), pages 168?177. Association for Computational Linguistics.
Andrey Rzhetsky, Hagit Shatkay, and W John Wilbur. 2009. How to get the most out of your curation effort. PLoS
Comput Biol, 5(5):e1000391.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proc of International Confer-
ence on New Methods in Language.
Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proc of the
NIPS Workshop on Cost-Sensitive Learning.
W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay. 2006. New directions in biomedical text annotation:
definitions, guidelines and corpus construction. BMC Bioinformatics, 25(7):356.
1
CABeRneT: Compr?ehension Automatique de Textes Biom?edicaux pour la Recherche Translationnelle
2
Vigi4MED: Vigilance dans les forums sur les M?edicaments
58
