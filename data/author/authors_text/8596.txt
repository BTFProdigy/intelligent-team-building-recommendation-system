Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 678?687,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
The infinite HMM for unsupervised PoS tagging
Jurgen Van Gael
Department of Engineering
University of Cambridge
jv249@cam.ac.uk
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cl.cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk
Abstract
We extend previous work on fully unsu-
pervised part-of-speech tagging. Using
a non-parametric version of the HMM,
called the infinite HMM (iHMM), we ad-
dress the problem of choosing the number
of hidden states in unsupervised Markov
models for PoS tagging. We experi-
ment with two non-parametric priors, the
Dirichlet and Pitman-Yor processes, on the
Wall Street Journal dataset using a paral-
lelized implementation of an iHMM in-
ference algorithm. We evaluate the re-
sults with a variety of clustering evalua-
tion metrics and achieve equivalent or bet-
ter performances than previously reported.
Building on this promising result we eval-
uate the output of the unsupervised PoS
tagger as a direct replacement for the out-
put of a fully supervised PoS tagger for the
task of shallow parsing and compare the
two evaluations.
1 Introduction
Many Natural Language Processing (NLP) tasks
are commonly tackled using supervised learning
approaches. These learning methods rely on the
availability of labeled datasets which are usually
produced by expensive manual annotation. For
some tasks, we have the choice to use unsuper-
vised learning approaches. While they do not nec-
essarily achieve the same level of performance,
they are appealing as unlabeled data is usually
abundant. In particular, for the purpose of ex-
ploring new domains and languages, obtainining
labeled material can be prohibitively expensive
and unsupervised learning methods are a very at-
tractive choice. Recent work (Johnson, 2007;
Goldwater and Griffiths, 2007; Gao and Johnson,
2008) explored the task of part-of-speech tagging
(PoS) using unsupervised Hidden Markov Models
(HMMs) with encouraging results. PoS tagging is
a standard component in many linguistic process-
ing pipelines, so any improvement on its perfor-
mance is likely to impact a wide range of tasks.
It is important to point out that a completely
unsupervised learning method will discover the
statistics of a dataset according to a particular
model choice but these statistics might not cor-
respond exactly to our intuition about PoS tags.
Johnson (2007) and Gao & Johnson (2008) as-
sume that words are generated by a hidden Markov
model and find that the resulting states strongly
correlate with POS tags. Nonetheless, identifying
the HMM states with appropriate POS tags is hard.
Because many evaluation methods often require
POS tags (rather than HMM states) this identifica-
tion problem makes unsupervised systems difficult
to evaluate.
One potential solution is to add a small amount
of supervision as in Goldwater & Griffiths (2007)
who assume a dictionary of frequent words asso-
ciated with possible PoS tags extracted from a la-
beled corpus. Although this technique improves
performance, in this paper we explore the com-
pletely unsupervised approach. The reason for this
is that better unsupervised approaches provide us
with better starting points from which to explore
how and where to incorporate supervision.
In previous work on unsupervised PoS tagging
a main question was how to set the number of hid-
den states appropriately. Johnson (2007) reports
results for different numbers of hidden states but it
is unclear how to make this choice a priori, while
Goldwater & Griffiths (2007) leave this question
as future work.
It is not uncommon in statistical machine learn-
ing to distinguish between parameters of a model
and the capacity of a model. E.g. in a clustering
context, the choice for the number of clusters (ca-
pacity) and the parameters of each cluster are often
678
treated differently: the latter are estimated using
algorithms like EM, MCMC or Variational Bayes
while the former is chosen using common sense,
heuristics or in a Bayesian framework maybe us-
ing evidence maximization.
Non-parametric Bayesian methods are a class of
probability distributions which explicitly treat the
capacity of a model as ?just another parameter?.
Potential advantages are
? the model capacity can automatically adjust
to the amount of data: e.g. when clustering
a very small dataset, it is unlikely that many
fine grained clusters can be distinguished,
? inference can be more efficient: e.g. instead
of running full inference for different model
capacities and then choosing the best ca-
pacity (according to some choice of ?best?),
inference in non-parametric Bayesian meth-
ods integrates the capacity search in one al-
gorithm. This is particularly advantageous
when parameters other than capacity need to
be explored, since it reduces signifcantly the
number of experiments needed.
None of these potential advantages are guaranteed
and in this paper we investigate these two aspects
for the task of unsupervised PoS tagging.
The contributions in this paper extend previous
work on unsupervised PoS tagging in five ways.
First, we introduce the use of a non-parametric
version of the HMM, namely the infinite HMM
(iHMM) (Beal et al, 2002) for unsupervised PoS
tagging. This answers an open problem from
Goldwater & Griffiths (2007). Second, we care-
fully implemented a parallelized version of the
inference algorithms for the iHMM so we could
use it on the Wall Street Journal Penn Treebank
dataset. Third, we introduce a new variant of
the iHMM that builds on the Pitman-Yor process.
Fourth, we evaluate the results with a variety of
clustering evaluation methods and achieve equiv-
alent or better performances than previously re-
ported. Finally, building on this promising result
we use the output of the unsupervised PoS tagger
as a direct replacement for the output of a fully su-
pervised PoS tagger for the task of shallow pars-
ing. This evaluation enables us to assess the appli-
cability of an unsupervised PoS tagging method
and provides us with means of comparing its per-
formance against a supervised PoS tagger.
The rest of the paper is structured as follows:
in section 2 we introduce the iHMM as a non-
parametric version of the Bayesian HMM used
in previous work on unsupervised PoS tagging.
Then, in section 3 we describe some details of
our implementation of the iHMM. In section 4 we
present a variety of evaluation metrics to compare
our results with previous work. Finally, in sec-
tion 5 we report our experimental results. We con-
clude this paper with a discussion of ongoing work
and experiments.
2 The Infinite HMM
In this section, we describe a non-parametric hid-
den Markov model known as the infinite HMM
(iHMM) (Beal et al, 2002; Teh et al, 2006). As
we show below, this model is flexible in the num-
ber of hidden states which it can accomodate. In
other words, the capacity is an uncertain quantity
with an a priori infinite range that is a posteriori
inferred by the data. It is instructive to first re-
view the finite HMM and its Bayesian treatment:
for one, it is the model that has been used in previ-
ous work on unsupervised PoS tagging, secondly
it allows us to better understand the iHMM.
The Bayesian HMM A finite first-order HMM
consists of a hidden state sequence s =
(s
1
, s
2
, . . . , s
T
) and a corresponding observation
sequence y = (y
1
, y
2
, . . . , y
T
). Each state vari-
able s
t
can take on a finite number of states, say
1 . . .K. Transitions between states are governed
by Markov dynamics parameterized by the tran-
sition matrix pi, where pi
ij
= p(s
t
= j|s
t?1
=
i), while the initial state probabilities are pi
0i
=
p(s
1
= i). For each state s
t
? {1 . . .K} there
is a parameter ?
s
t
which parameterizes the obser-
vation likelihood for that state: y
t
|s
t
? F (?
s
t
).
Given the parameters {pi
0
,pi,?,K} of the HMM,
the joint distribution over hidden states s and ob-
servations y can be written (with s
0
= 0):
p(s,y|pi
0
,pi,?,K) =
T
?
t=1
p(s
t
|s
t?1
)p(y
t
|s
t
)
As Johnson (2007) clearly explained, training the
HMM with EM leads to poor results in PoS tag-
ging. However, we can easily treat the HMM in a
fully Bayesian way (MacKay, 1997) by introduc-
ing priors on the parameters of the HMM. With
no further prior knowledge, a typical prior for the
transition (and initial) probabilities are symmet-
ric Dirichlet distributions. This corresponds to our
679
belief that, a priori, each state is equally likely to
transition to every other state. Also, it is com-
monly known that the parameter of a Dirichlet
distribution controls how sparse its samples are.
In other words, by making the hyperprior on the
Dirichlet distribution for the rows of the transi-
tion matrix small, we can encode our belief that
any state (corresponding to a PoS tag in this ap-
plication context) will only be followed by a small
number of other states. As we explain below, we
will be able to include this desirable property in
the non-parametric model as well. Secondly, we
need to introduce a prior on the observation pa-
rameters ?
k
. Without any further prior knowl-
edge, a convenient choice here is another sym-
metric Dirichlet distribution with sparsity induc-
ing hyperprior. This encodes our belief that only
a subset of the words correspond to a particular
state.
The Infinite HMM A first na??ve way to obtain
a non-parametric HMM with an infinite number
of states might be to use symmetric Dirichlet pri-
ors over the transition probabilities with parameter
?/K and take K ? ?. This approach unfortu-
nately does not work: ?/K ? 0 when K ? ?
and hence the rows of the matrix will become ?in-
finitely sparse?. Since the sum of the entries must
sum to one, the rows of the transition matrix will
be zero everywhere and all its mass in a random
location. Unfortunately, this random location is
out of an infinite number of possible locations and
hence with probability 1 will be different for all
the rows. As a consequence, at each timestep the
HMM moves to a new state and will never revisit
old states. As we shall see shortly, we can fix this
by using a hierarchical Bayesian formalism where
the Dirichlet priors on the rows have a shared pa-
rameter.
Before moving on to the iHMM, let us look at
the finite HMM from a different perspective. The
finite HMM of length T with K hidden states can
be seen as a sequence of T finite mixture models.
The following equation illustrates this idea: con-
ditioned on the previous state s
t?1
, the marginal
probability of observation y
t
can be written as:
p(y
t
|s
t?1
= k) =
K
?
s
t
=1
p(s
t
|s
t?1
= k)p(y
t
|s
t
),
=
K
?
s
t
=1
pi
k,s
t
p(y
t
|?
s
t
). (1)
The variable s
t?1
= k specifies the mixing
weights pi
k,?
for the mixture distribution, while s
t
indexes the mixture component generating the ob-
servation y
t
. In other words, equation (1) says that
each row of the transition matrix pi specifies a dif-
ferent mixture distribution over the same set of K
mixture components ?.
Our second attempt to define a non-parametric
version of the hidden Markov model is to replace
the finite mixture by an infinite mixture. The
theory of Dirichlet process mixtures (Antoniak,
1974) tells us exactly how to do this. A draw
G ? DP (?,H) from a Dirichlet process (DP)
with base measure H and concentration parame-
ter ? ? 0 is a discrete distribution which can be
written as an infinite mixture of atoms
G(?) =
?
?
i=1
pi
i
?
?
i
(?)
where the ?
i
are i.i.d. draws from the base mea-
sure H , ?
?
i
(?) represents a point distribution at
?
i
and pi
i
= v
i
?
i?1
l=1
(1 ? v
l
) where each v
l
?
Beta(1, ?). The distribution over pi
i
is called a
stick breaking construction and is essentially an
infinite dimensional version of the Dirichlet dis-
tribution. We refer to Teh et al (2006) for more
details.
Switching back to the iHMM our next step is to
introduce a DP G
j
for each state j ? {1 ? ? ??};
we write G
j
(?) =
?
?
i=1
pi
j
i
?
?
j
i
(?). There is now
a parameter for each state j and each index i ?
{1, 2, ? ? ? ,?}. Next, we draw the datapoint at
timestep t given that the previous datapoint was in
state s
t?1
by drawing from DP G
s
t?1
. We first se-
lect a mixture component s
t
from the vector pi
s
t?1
,?
and then sample a datapoint y
t
? F (?
s
t?1
,s
t
) so
we get the following distribution for y
t
p(y
t
|?, s
t?1
) =
?
?
s
t
=1
pi
s
t?1
,s
t
p(y
t
|?
s
t?1
,s
t
).
This is almost the non-parametric equivalent of
equation (1) but there is a subtle difference: each
G
j
selects their own set of parameters ?
j
?
. This
is unfortunate as it means that the output distribu-
tion would not be the same for each state, it would
depend on which state we were moving to! Luck-
ily, we can easily fix this: by introducing an in-
termediate distribution G
0
? DP (?,H) and let
G
j
? DP (?,G
0
) we enforce that the i.i.d. draws
?
j
?
are draws from a discrete distribution (sinceG
0
680
is a draw from a Dirichlet process) and hence all
G
j
will share the same infinite set of atoms as cho-
sen byG
0
. Figure 1 illustrates the graphical model
for the iHMM.
The iHMM with Pitman-Yor Prior The
Dirichlet process described above defines a very
specific distribution over the number of states
in the iHMM. One particular generalization of
the Dirichlet process that has been studied in the
NLP literature before is the Pitman-Yor process.
Goldwater et al (2006) have shown that the
Pitman-Yor distribution can more accurately cap-
ture power-law like distributions that frequently
occur in natural language.
More specifically, a draw G ? PY (d, ?,H)
from a Pitman-Yor process (PY) with base mea-
sure H , discount parameter 0 ? d < 1 and con-
centration parameter ? > ?d is a discrete distri-
bution which can be written as an infinite mixture
of atoms
G(?) =
?
?
i=1
pi
i
?
?
i
(?)
where the ?
i
are i.i.d. draws from the base mea-
sure H , ?
?
i
(?) represents a point distribution at
?
i
and pi
i
= v
i
?
i?1
l=1
(1 ? v
l
) where each v
l
?
Beta(1?d, ?+ ld). Note the similarity to the DP:
in fact, the DP is a special case of PY with d = 0.
In our experiments, we constructed an iHMM
where the DP (?,H) base measure G
0
is re-
placed with its two parameter generalization
PY (d, ?,H). Because the Dirichlet and Pitman-
Yor processes only differ in the way pi is con-
structed, without loss of generality we will de-
scribe hyper-parameter choice and inference in the
context of the iHMM with Dirichlet process base
measure.
Hyperparameter Choice The description
above shows that there are 4 parameters which
we must specify: the base measure H , the
output distribution p(y
t
|?
s
t
), the discount
1
and
concentration
2
parameters d, ? for G
0
and the
concentration parameter ? for the DP?sG
j
. Just as
in the finite case, the base measure H is the prior
distribution on the parameter ? of p(y
t
|?
s
t
). We
chose to use a symmetric Dirichlet distribution
with parameter ? over the word types in our
corpus. Since we do not know the sparsity level ?
of the output distributions we decided to learn this
1
for Pitman-Yor base measure
2
for both Dirichlet and Pitman-Yor base measures
parameter from the data. We initially set a vague
Gamma prior over ? but soon realized that as we
expect hidden states in the iHMM to correspond
to PoS tags, it is unrealistic to expect each state
to have the same sparsity level. Hence we chose
a Dirichlet process as the prior for ?; this way
we end up with a small discrete set of sparsity
levels: e.g. we can learn that states corresponding
to verbs and nouns share one sparsity level
while states correpsonding to determiners have
their own (much sparser) sparsity level. For the
output distribution p(y
t
|?
s
t
) we chose a simple
multinomial distribution.
The hyperparameters d and ? mostly control the
number of states in the iHMM while - as we dis-
cussed above - ? controls the sparsity of the tran-
sition matrix. In the experiments below we report
both fixing the two parameters and learning them
by sampling (using vague Gamma hyperpriors).
Because of computational constraints, we chose to
use vague Bayesian priors for all hyperparameters
rather than run the whole experiment over a grid of
?reasonable? parameter settings and use the best
ones according to cross validation.
3 Inference
The Wall Street Journal part of the Penn Tree-
bank that was used for our experiments contains
about one million words. In the non-parametric
Bayesian literature not many algorithms have been
described that scale into this regime. In this sec-
tion we describe our parallel implementation of
the iHMM which can easily handle a dataset of
this scale.
There is a wealth of evidence (Scott, 2002; Gao
and Johnson, 2008) in the machine learning litera-
ture that Gibbs sampling for Markov models leads
to slow mixing times. Hence we decided our start-
ing point for inference needs to be based on dy-
namic programming. Because we didn?t have a
good idea for the number of states that we were go-
ing to end up with, we prefered the beam sampler
of Van Gael et al (2008) over a finite truncation
of the iHMM. Moreover, the beam sampler also
introduces a certain amount of sparsity in the dy-
namic program which can speed up computations
(potentially at the cost of slower mixing).
The beam sampler is a blocked Gibbs sampler
where we alternate between sampling the param-
eters (transition matrix, output parameters), the
state sequence and the hyperparameters. Sam-
681
k = 1 ? ? ?1
s0s1s2y1y2?k?k???H
Figure 1: The graphical model for the iHMM. The variable ? represents the mixture for the DP G
0
.
pling the transition matrix and output distribu-
tion parameters requires computing their sufficient
statistics and sampling from a Dirichlet distribu-
tion; we refer to the beam sampling paper for de-
tails. For the hyperparameters we use standard
Gibbs sampling. We briefly sketch the resam-
pling step for the state sequence for a single se-
quence of data (sentence of words). Running stan-
dard dynamic programming is prohibitive because
the state space of the iHMM is infinitely large.
The central idea of the beam sampler is to adap-
tively truncate the state space of the iHMM and
run dynamic programming. In order to truncate
the state space, we sample an auxilary variable u
t
for each word in the sequence from the distribu-
tion u
t
? Uniform(0, pi
s
t?1
s
t
) where pi represents
the transition matrix.
Intuitively, when we sample u
1:T
|s
1:T
accord-
ing to the distribution above, the only valid sam-
ples are those for which the u
t
are smaller than
the transition probabilities of the state sequence
s
1:T
. This means that when we sample s
1:T
|u
1:T
at a later point, it must be the case that the u
t
?s
are still smaller than the new transition probabil-
ities. This significantly reduces the set of valid
state sequences that we need to consider. More
specifically, Van Gael et al (2008) show that we
can compute p(s
t
|y
1:t
, u
1:t
) using the following
dynamic programming recursion p(s
t
|y
1:t
, u
1:t
) =
p(y
t
|s
t
)
?
s
t?1
:u
t
<pi
s
t?1
,s
t
p(s
t?1
|y
1:t?1
, u
1:t?1
).
The summation
?
s
t?1
:u
t
<pi
s
t?1
,s
t
ensures that this
computation remains finite. When we compute
p(s
t
|y
1:t
, u
1:t
) for t ? {1 ? ? ?T}, we can easily
sample s
T
and using Bayes rule backtrack sample
every other s
t
. It can be shown that this procedure
produces samples from the exact posterior.
Notice that the dynamic program only needs to
perform computation when u
t
< pi
s
t?1
,s
t
. A care-
ful implementation of the beam sampler consists
of preprocessing the transition matrix pi and sort-
ing its elements in descending order. We can then
iterate over the elements of the transition matrix
starting from the largest element and stop once
we reach the first element of the transition matrix
smaller than u
t
. In our experiments we found that
this optimization reduces the amount of computa-
tion per sentence by an order of magnitutde.
A second optimization which we introduced
is to use the map-reduce paradigm (Dean and
Ghemawat, 2004) to parallelize our computations.
More specifically, after we preprocess the transi-
tion matrix, the dynamic program computations
are independent for each sentence in the dataset.
This means we can perform each dynamic pro-
gram in parallel; in other words our ?map? con-
sists of running the dynamic program on one sen-
tence in the dataset. Next, we need to resample
the transition matrix and output distribution pa-
rameters. In order to do so we need to compute
their sufficient statistics: the number of transitions
from state to state and the number of emissions of
each word out of each state. Our ?reduce? func-
tion consists of computing the sufficient statistics
for each sentence and then aggregating the statis-
tics for the whole dataset. Our implementation
runs on a quad-core shared memory architecture
and we find an almost linear speedup going from
one to four cores.
4 Evaluation
Evaluating unsupervised PoS tagging is rather dif-
ficult mainly due to the fact that the output of such
682
systems are not actual PoS tags but state identi-
fiers. Therefore it is impossible to evaluate per-
formance against a manually annotated gold stan-
dard using accuracy. Recent work (Goldwater and
Griffiths, 2007; Johnson, 2007; Gao and Johnson,
2008) on this task explored a variety of method-
ologies to address this issue.
The most common approach followed in pre-
vious work is to evaluate unsupervised PoS tag-
ging as clustering against a gold standard using
the Variation of Information (VI) (Meil?a, 2007).
VI assesses homogeneity and completeness us-
ing the quantities H(C|K) (the conditional en-
tropy of the class distribution in the gold stan-
dard given the clustering) and H(K|C) (the con-
ditional entropy of clustering given the class dis-
tribution in the gold standard). However, as Gao
& Johnson (2008) point out, VI is biased to-
wards clusterings with a small number of clus-
ters. A different evaluation measure that uses
the same quantities but weighs them differently is
the V-measure (Rosenberg and Hirschberg, 2007),
which is defined in Equation 2 by setting the pa-
rameter ? to 1.
h = 1?
H(C|K)
H(C)
c = 1?
H(K|C)
H(K)
V
?
=
(1 + ?)hc
(?h) + c
(2)
Vlachos et al (2009) noted that V-measure favors
clusterings with a large number of clusters. Both
of these biases become crucial in our experiments,
since the number of clusters (states of the iHMM)
is not fixed in advance. Vlachos et al proposed a
variation of the V-measure, V-beta, that adjusts the
balance between homogeneity and completeness
using the parameter ? in Eq. 2.
It is worth mentioning that, unlike V-measure
and V-beta, VI scores are not normalized
and therefore they are difficult to interpret.
Meil?a (2007) presented two normalizations,
acknowledging the potential disadvantages
they have. The first one normalizes VI by
2 log(max(|K|, |C|)), which is inappropriate
when the number of clusters discovered |K|
changes between experiments. The second
normalization involves the quantity logN which
is appropriate when comparing different algo-
rithms on the same dataset (N is the number
of instances). However, this quantity depends
exclusively on the size of the dataset and hence if
the dataset is very large it can result in normalized
VI scores misleadingly close to 100%. This does
not affect rankings, i.e. a better VI score will also
be translated into a better normalized VI score. In
our experiments, we report results only with the
un-normalized VI scores, V-measure and V-beta.
All the evaluation measures mentioned so far
evaluate PoS tagging as a clustering task against
a manually annotated gold standard. While this
is reasonable, it still does not provide means of
assessing the performance in a way that would
allow comparisons with supervised methods that
output actual PoS tags. Even for the normalized
measures V-measure and V-beta, it is unclear how
their values relate to accuracy levels. Gao & John-
son (2008) partially addressed this issue by map-
ping states to PoS tags following two different
strategies, cross-validation accuracy, and greedy
1-to-1 mapping, which both have shortcomings.
We argue that since an unsupervised PoS tagger is
trained without taking any gold standard into ac-
count, it is not appropriate to evaluate against a
particular gold standard, or at least this should not
be the sole criterion. The fact that different authors
use different versions of the same gold standard to
evaluate similar experiments (e.g. Goldwater &
Griffiths (2007) versus Johnson (2007)) supports
this claim. Furthermore, PoS tagging is seldomly
a goal in itself, but it is a component in a linguistic
pipeline.
In order to address these issues, we perform an
extrinsic evaluation using a well-explored task that
involves PoS tags. While PoS tagging is consid-
ered a pre-processing step in many natural lan-
guage processing pipelines, the choice of task is
restricted by the lack of real PoS tags in the out-
put of our system. For our purposes we need a
task that relies on discriminating between PoS tags
rather than the PoS tag semantics themselves, in
other words, a task in which knowing whether a
word is tagged as noun instead of a verb is equiv-
alent to knowing it is tagged as state 1 instead of
state 2. Taking these considerations into account,
in Section 5 we experiment with shallow pars-
ing in the context of the CoNLL-2000 shared task
(Tjong Kim Sang and Buchholz, 2000) in which
very good performances were achieved using only
the words with their PoS tags. Our intuition is that
if the iHMM (or any unsupervised PoS tagging
683
method) has a reasonable level of performance, it
should improve on the performance of a system
that does not use PoS tags. Moreover, if the per-
formance is very good indeed, it should get close
to the performance of a system that uses real PoS
tags, provided either by human annotation or by a
good supervised system. Similar extrinsic evalu-
ation was performed by Biemann et al (2007). It
is of interest to compare the results between the
clustering evaluation and the extrinsic one.
A different approach in evaluating non-
parametric Bayesian models for NLP is state-
splitting (Finkel et al, 2007; Liang et al, 2007).
In this setting, the model is used in order to re-
fine existing annotation of the dataset. While this
approach can provide us with some insights and
interpretable results, the use of existing annotation
influences the output of the model. In this work,
we want to verify whether the output of the iHMM
(without any supervision) can be used instead of
that of a supervised system.
5 Experiments
In all our experiments, the Wall Street Journal
(WSJ) part of the Penn Treebank was used. As ex-
plained in Section 4, we evaluate the output of the
iHMM in two ways, as clustering with respect to a
gold standard and as direct replacement of the PoS
tags in the task of shallow parsing. In each experi-
ment, we obtain a sample from the iHMM over all
the sections of WSJ. The states for sections 15-18
and 20 of the WSJ (training and testing sets re-
spectvely in the CoNLL shared task) are used for
the evaluation based on shallow parsing, while the
remaining sections are used for evaluation against
the WSJ gold standard PoS tags using clustering
evaluation measures.
As described in Section 2 we performed three
runs with the iHMM: one run with DP prior and
fixed ?, ?, one with PY prior and fixed d, ?, ? and
one with DP prior but where we learn the hyper-
parameters ?, ? from the data. Our inference algo-
rithm uses 1000 burn-in iterations after which we
collect a sample every 1000 iterations. Our infer-
ence procedure is annealed during the first 1000
burnin and 2400 iterations by powering the likeli-
hood of the output distribution with a number that
smoothly increases from 0.4 to 1.0 over the 3400
first iterations. The numbers of iterations reported
in the remainder of the section refer to the itera-
tions after burn-in. We initialized the sampler by:
a) sampling the hyperparameters from the prior
where applicable, b) uniformly assign each word
one out of 20 iHMM states. For the DP run with
fixed parameters, we chose ? = 0.8 to encourage
some sparsity in the transition matrix and ? = 5.0
to allow for enough hidden states. For the PY run
with fixed parameters, we chose ? = 0.8 for simi-
lar reasons and d = 0.1 and ? = 1.0. We point out
that one weakness of MCMC methods is that they
are hard to test for convergence. We chose to run
the simulations until they became prohibitively ex-
pensive to obtain a new sample.
First, we present results using clustering eval-
uation measures which appear in the figures of
Table 1. The three runs exhibit different behav-
ior. The number of states reached by the iHMM
with fixed parameters using the DP prior stabilizes
close to 50 states, while for the experiment with
learnt hyperparameters the number of states grows
more rapidly, reaching 194 states after 8,000 iter-
ations. With the PY prior, the number of states
reached grows less rapidly reaching 90 states. All
runs achieve better performances with respect to
all the measures used as the number of iterations
grows. An exception is that VI scores tend to in-
crease (lower VI scores are better) when the num-
ber of states grows larger than the gold standard.
It is interesting to notice how the measures exhibit
different biases, in particular that VI penalizes the
larger numbers of states discovered in the DP run
with learnt parameters as well as the run with the
PY prior, compared to the more lenient scores pro-
vided by V-measure and V-beta. The latter though
assigns lower scores to the DP run with learnt pa-
rameters because it takes into account that the high
homogeneity is achieved using even more states.
Finally, the interpretability of these scores presents
some interest. For example, in the run with fixed
parameters using the DP prior, after burn-in VI
was 4.6, which corresponds to 76.65% normalized
VI score, while V-measure and V-beta were 12.7%
and 9% respectively. In 8,000 iterations after burn-
in, VI was 3.94 (80.3% when normalized), while
V-measure and V-beta were 53.3%, since the num-
ber of states was almost the same as the number of
unique PoS tags in the gold standard.
The closest experiment to ours is the one by
Gao & Johnson (2008) who run their Bayesian
HMM over the whole WSJ and evaluated against
the full gold standard, the only difference being
is that we exclude the CoNLL shared task sec-
684
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 200
 0  1  2  3  4  5  6  7  8
s
ta
te
s
DP-learnt
DP-fixed
PY-fixed
 0
 10
 20
 30
 40
 50
 60
 70
 0  1  2  3  4  5  6  7  8
h
o
m
o
ge
ne
ity
DP-learnt
DP-fixed
PY-fixed
 25
 30
 35
 40
 45
 50
 55
 60
 0  1  2  3  4  5  6  7  8
c
o
m
pl
et
en
es
s
DP-learnt
DP-fixed
PY-fixed
 3.6
 3.8
 4
 4.2
 4.4
 4.6
 4.8
 5
 5.2
 0  1  2  3  4  5  6  7  8
V
I
DP-learnt
DP-fixed
PY-fixed
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 0  1  2  3  4  5  6  7  8
V
-m
e
a
su
re
DP-learnt
DP-fixed
PY-fixed
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 0  1  2  3  4  5  6  7  8
V
-b
e
ta
DP-learnt
DP-fixed
PY-fixed
Table 1: Performance of the three iHMM runs according to clustering evaluation measures against num-
ber of iteretions (in thousands).
 93.2
 93.4
 93.6
 93.8
 94
 94.2
 94.4
 94.6
 0  1  2  3  4  5  6  7  8
a
c
c
u
ra
c
y
DP-learnt
DP-fixed
PY-fixed
 88.5
 89
 89.5
 90
 90.5
 91
 0  1  2  3  4  5  6  7  8
F
-s
co
re
DP-learnt
DP-fixed
PY-fixed
Table 2: Performance of the output of the three iHMM runs when used in shallow parsing against number
of iteretions (in thousands).
tions from our evaluation, which leaves us with 19
sections instead of 24. Their best VI score was
4.03886 which they achieved using the collapsed,
sentence-blocked Gibbs sampler with the number
of states fixed to 50. The VI score achieved by the
iHMM with fixed parameters using the PY prior
reaches 3.73, while using the DP prior VI reaches
4.32 with learnt parameters and 3.93 with fixed
685
parameters. These results, even if they are not
directly comparable, are on par with the state-of-
the-art, which encouraged us to proceed with the
extrinsic evaluation.
For the experiments with shallow parsing we
used the CRF++ toolkit
3
which has an efficient
implementation of the model introduced by Sha &
Pereira (2003) for this task. First we ran an experi-
ment using the words and the PoS tags provided in
the shared task data and the performances obtained
were 96.07% accuracy and 93.81% F-measure.
The PoS tags were produced using the Brill tag-
ger (Brill, 1994) which employs tranformation-
based learning and was trained using the WSJ cor-
pus. Then we ran an experiment removing the
PoS tags altogether, and the performances were
93.25% accuracy and 88.58% F-measure respec-
tively. This gave us some indication as to what the
contribution of the PoS tags is in the context of the
shallow parsing task at hand.
The experiments using the output of the iHMM
as PoS tags for shallow parsing are presented in
Table 2. The best performance achieved was
94.48% and 90.98% in accuracy and F-measure,
which is 1.23% and 2.4% better respectively than
just using words, but worse by 1.57% and 2.83%
compared to using the supervised PoS tagger out-
put. Given that the latter is trained on WSJ we be-
lieve that this is a good result. Interestingly, this
was obtained by using the last sample from the
iHMM run using the DP prior with learnt param-
eters which has worse overall clustering evalua-
tion scores, especially in terms of VI. This sample
though has the best homogeneity score (69.39%).
We believe that homogeneity is more important
than the overall clustering score due to the fact
that, in the application considered, it is probably
worse to assign tokens that belong to different PoS
tags to the same state, e.g. verb and adverbs, rather
than generate more than one state for the same
PoS. This is likely to be the case in tasks where
we are interested in distinguishing between PoS
tags rather than the actual tag itself. Also, clus-
tering evaluation measures tend to score leniently
consistent mixing of members of different classes
in the same cluster. However, such mixing results
in consistent noise when the clustering output be-
comes input to a machine learning method, which
is harder to deal with.
3
http://crfpp.sourceforge.net/
6 Conclusions - Future Work
In the context of shallow parsing we saw that the
performance of the iHMM does not match the
performance of a supervised PoS tagger but does
lead to a performance increase over a model us-
ing only words as features. Given that it was con-
structed without any need for human annotation,
we believe this is a good result. At the same time
though, it suggests that it is still some way from
being a direct drop-in replacement for a supervised
method. We argue that the extrinsic evaluation of
unsupervised PoS tagging performed in this paper
is quite informative as it allowed us to assess our
results in a more realistic context. In this work we
used shallow parsing for this, but we are consider-
ing other tasks in which we hope that PoS tagging
performance will be more crucial.
Our experiments also suggest that the number of
states in a Bayesian non-parametric model can be
rather unpredictable. On one hand, this is a strong
warning towards inference algorithms which per-
form finite truncation of non-parametric models.
On the other hand, the remarkable difference in
behavior between the DP with fixed and learned
priors suggests that more research is needed to-
wards understanding the influence of hyperparam-
eters in Bayesian non-parametric models.
We are currently experimenting with a semi-
supervised PoS tagger where we let the transi-
tion matrix of the iHMM depend on annotated
PoS tags. This model allows us to: a) use an-
notations whenever they are available and do un-
supervised learning otherwise; b) use the power
of non-parametric methods to possibly learn more
fine grained statistical structure than tag sets cre-
ated manually.
On the implementation side, it would be in-
teresting to see how our methods scale in a dis-
tributed map-reduce architecture where network
communication overhead becomes an issue.
Finally, the ultimate goal of our investigation is
to do unsupervised PoS tagging using web-scale
datasets. Although the WSJ corpus is reasonably
sized, our computational methods do not currently
scale to problems with one or two order of magni-
tude more data. We will need new breakthroughs
to unleash the full potential of unsupervised learn-
ing for NLP.
686
References
Charles E. Antoniak. 1974. Mixtures of dirichlet pro-
cesses with applications to bayesian nonparametric
problems. The Annals of Statistics, 2(6):1152?1174.
M. J. Beal, Z. Ghahramani, and C. E. Rasmussen.
2002. The infinite hidden markov model. Advances
in Neural Information Processing Systems, 14:577 ?
584.
Chris Biemann, Claudio Giuliano, and Alfio Gliozzo.
2007. Unsupervised part-of-speech tagging support-
ing supervised methods. In Proceedings of RANLP.
Eric Brill. 1994. Some advances in transformation-
based part of speech tagging. In National Confer-
ence on Artificial Intelligence, pages 722?727.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapre-
duce: Simplified data processing on large clusters.
In Sixth Symposium on Operating System Design
and Implementation.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2007. The infinite tree. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 272?279,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
J. Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahra-
mani. 2008. Beam sampling for the infinite hidden
markov model. In Proceedings of the 25th interna-
tional conference on Machine learning, volume 25,
Helsinki.
Jianfeng Gao and Mark Johnson. 2008. A compari-
son of bayesian estimators for unsupervised hidden
markov model pos taggers. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 344?352.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744?751, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
S. Goldwater, T. Griffiths, and M. Johnson. 2006. In-
terpolating between types and tokens by estimating
power-law generators. Advances in Neural Informa-
tion Processing Systems, 18.
Mark Johnson. 2007. Why Doesn?t EM Find Good
HMM POS-Taggers? In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 296?305.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007.
The infinite PCFG using hierarchical Dirichlet pro-
cesses. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
D. J. C. MacKay. 1997. Ensemble learning for hidden
Markov models. Technical report, Cavendish Labo-
ratory, University of Cambridge, 1997.
Marina Meil?a. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 410?420, Prague, Czech
Republic, June.
Steven L. Scott. 2002. Bayesian methods for hidden
markov models: Recursive computing in the 21st
century. Journal of the American Statistical Asso-
ciation, 97(457):337?351, March.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Human Lan-
guage Technology Conference and the 4th Meeting
of the North American Association for Computa-
tional Linguistics.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and D. M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Claire Cardie, Walter Daelemans,
Claire Nedellec, and Erik Tjong Kim Sang, editors,
Proceedings of the Fourth Conference on Computa-
tional Natural Language Learning, pages 127?132.
Lisbon, Portugal, September.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and Constrained
Dirichlet Process Mixture Models for Verb Cluster-
ing. In Proceedings of the EACL workshop on GEo-
metrical Models of Natural Language Semantics.
687
Active Annotation
Andreas Vlachos
William Gates Building
Computer Laboratory
University of Cambridge
av308@cl.cam.ac.uk
Abstract
This paper introduces a semi-supervised
learning framework for creating training
material, namely active annotation. The
main intuition is that an unsupervised
method is used to initially annotate imper-
fectly the data and then the errors made
are detected automatically and corrected
by a human annotator. We applied ac-
tive annotation to named entity recogni-
tion in the biomedical domain and encour-
aging results were obtained. The main
advantages over the popular active learn-
ing framework are that no seed annotated
data is needed and that the reusability of
the data is maintained. In addition to the
framework, an efficient uncertainty esti-
mation for Hidden Markov Models is pre-
sented.
1 Introduction
Training material is always an issue when applying
machine learning to deal with information extrac-
tion tasks. It is generally accepted that increasing
the amount of training data used improves perfor-
mance. However, training material comes at a cost,
since it requires annotation.
As a consequence, when adapting existing meth-
ods and techniques to a new domain, researchers and
users are faced with the problem of absence of an-
notated material that could be used for training. A
good example is the biomedical domain, which has
attracted the attention of the NLP community rel-
atively recently (Kim et al, 2004). Even though
there are plenty of biomedical texts, very little of it
is annotated, such as the GENIA corpus (Kim et al,
2003).
A very popular and well investigated framework
in order to cope with the lack of training mate-
rial is the active learning framework (Cohn et al,
1995; Seung et al, 1992). It has been applied
to various NLP/IE tasks, including named entity
recognition (Shen et al, 2004) and parse selec-
tion (Baldridge and Osborne, 2004) with rather im-
pressive results in reducing the amount of anno-
tated training data. However, some criticism of ac-
tive learning has been expressed recently, concern-
ing the reusability of the data (Baldridge and Os-
borne, 2004).
This paper presents a framework in order to deal
with the lack of training data for NLP tasks. The
intuition behind it is that annotated training data is
produced by applying an (imperfect) unsupervised
method, and then the errors inserted in the annota-
tion are detected automatically and reannotated by
a human annotator. The main difference compared
to active learning is that instead of selecting unla-
beled instances for annotation, possible erroneous
instances are selected for checking and correction
if they are indeed erroneous. We will refer to this
framework as ?active annotation? in the rest of the
paper.
The structure of this paper is as follows. In Sec-
tion 2 we describe the software and the dataset used.
Section 3 explores the effect of errors in the training
data and motivates the active annotation framework.
64
In Section 4 we describe the framework in detail,
while Section 5 presents a method for estimating un-
certainty for HMMs. Section 6 presents results from
applying the active annotation. Section 7 compares
the proposed framework to active learning and Sec-
tion 8 attempts an analysis of its performance. Fi-
nally, Section 9 suggests some future work.
2 Experimental setup
The data used in the experiments that follow are
taken from the BioNLP 2004 named entity recog-
nition shared task (Kim et al, 2004). The text pas-
sages have been annotated with five classes of en-
tities, ?DNA?, ?RNA?, ?protein?, ?cell type? and
?cell line?. In our experiments, following the ex-
ample of Dingare et al (2004), we simplified the an-
notation to one entity class, namely ?gene?, which
includes the DNA, RNA and protein classes. In or-
der to evaluate the performance on the task, we used
the evaluation script supplied with the data, which
computes the F-score (F1 = 2?Precision?RecallP recision+Recall ) for
each entity class. It must be noted that all tokens
of an entity must be recognized correctly in order to
count as a correct prediction. A partially recognized
entity counts both as a precision and recall error. In
all the experiments that follow, the official split of
the data in training and testing was maintained.
The named entity recognition system used in our
experiments is the open source NLP toolkit Ling-
pipe1. The named entity recognition module is
an HMM model using Witten-Bell smoothing. In
our experiments, using the data mentioned earlier it
achieved 70.06% F-score.
3 Effect of errors
Noise in the training data is a common issue in train-
ing machine learning for NLP tasks. It can have sig-
nificant effect on the performance, as it was pointed
out by Dingare et al (2004), where the performance
of the same system on the same task (named entity
recognition in the biomedical domain) was lower
when using noisier material. The effect of noise in
the data used to train machine learning algorithms
for NLP tasks has been explored by Osborne (2002),
using the task of shallow parsing as the case study
and a variety of learners. The impact of different
1http://www.alias-i.com/lingpipe/
types of noise was explored and learner specific ex-
tensions were proposed in order to deal with it.
In our experiments we explored the effect of noise
in training the selected named entity recognition sys-
tem, keeping in mind that we are going to use an
unsupervised method to create the training material.
The kind of noise we expect is mislabelled instances.
In order to simulate the behavior of a hypothetical
unsupervised method, we corrupted the training data
artificially using the following models:
? LowRecall: Change tokens labelled as entities
to non-entities. It must be noted that in this
model, due to the presence of multi-token en-
tities precision is reduced too, albeit less than
recall.
? LowRecall WholeEntities: Change the label-
ing of whole entities to non-entities. In this
model, precision is kept intact.
? LowPrecision: Change tokens labelled as non-
entities to entities.
? Random: Entities and non-entities are changed
randomly. It can be viewed alternatively as a
random tagger which labels the data with some
accuracy.
The level of noise inserted is adjusted by specify-
ing the probability with which a candidate label is
changed. In all the experiments in this paper, for a
particular model and level of noise, the corruption
of the dataset was repeated five times in order to
produce more reliable results. In practice, the be-
havior of an unsupervised method is likely to be a
mixture of the above models. However, given that
the method would tag the data with a certain per-
formance, we attempted through our experiments to
identify which of these (extreme) behaviors would
be less harmful. In Figure 1, we present graphs
showing the effect of noise inserted with the above
models. The experimental procedure was to add
noise to the training data according to a model, eval-
uate the performance of the hypothetical tagger that
produced it, train Lingpipe on the noisy training data
and evaluate the performance of the latter on the test
data. The process was repeated for various levels
of noise. In the top graph, the F-score achieved by
Lingpipe (F-ling) is plotted against the F-score of
65
the hypothetical tagger (F-tag), while in the bottom
graph the F-score achieved by Lingpipe (F-ling) is
plotted against the number of erroneous classifica-
tions made by the hypothetical tagger.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.2  0.4  0.6  0.8  1
F-
lin
g
F-tag
random
lowrecall
wholeentities
lowprecision
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  50  100 150 200 250 300 350 400 450 500
F-
lin
g
raw errors (in thousands)
random
lowrecall
wholeentities
lowprecision
Figure 1: F-score achieved by Lingpipe is plotted
against (a) the F-score of the hypothetical tagger in
the top graph and (b) the number of errors made by
the hypothetical tagger in the bottom graph.
A first observation is that limited noise does not
affect the performance significantly, a phenomenon
that can be attributed to the capacity of the machine
learning method to deal with noise. From the point
of view of correcting mistakes in the training data
this suggests that not all mistakes need to be cor-
rected. Another observation is that while the perfor-
mance for all the models follow similar curves when
plotted against the F-score of the hypothetical tag-
ger, the same does not hold when plotted against the
number of errors. While this can be attributed to the
unbalanced nature of the task (very few entity to-
kens compared to non-entities), it also suggests that
the raw number of errors in the training data is not
a good indicator for the performance obtained by
training on it. However, it represents the effort re-
quired to obtain the maximum performance from the
data by correcting it.
4 Active Annotation
In this section we present a detailed description of
the active annotation framework. Initially, we have
a pool of unlabeled data, D, whose instances are an-
notated using an unsupervised method u, which does
not need training data. As expected, a significant
amount of errors is inserted during this process. A
list L is created containing the tokens that have not
been checked by a human annotator. Then, a super-
vised learner s is used to train a model M on this
noisy training material. A query module q, which
uses the model created by s decides which instances
of D will be selected to be checked for errors by
a human annotator. The selected instances are re-
moved from L so that q does not select them again
in future. The learner s is then trained on this par-
tially corrected training data and the sequence is re-
peated from the point of applying the querying mod-
ule q. The algorithm written in pseudocode appears
in Figure 2.
Data D, unsupervised tagger u,
supervised learner s, query module q.
Initialization:
Apply u to D.
Create list of instances L.
Loop:
Using s train a model M on D.
Using q and M select a batch of instances B
to be checked.
Correct the instances of B in D.
Remove the instances of B from L.
Repeat until:
L is empty or annotator stops.
Figure 2: Active annotation algorithm
Comparing it with active learning, the similarities
are apparent. Both frameworks have a loop in which
a query module q, using a model produced by the
learner, selects instances to be presented to a human
annotator. The efficiency of active annotation can be
measured in two ways, both of them used in evalu-
ating active learning. The first is to measure the re-
duction in the checked instances needed in order to
achieve a certain level of performance. The second
is the increase in performance for a fixed number
of checked instances. Following the active learning
66
paradigm, a baseline for active annotation is random
selection of instances to be checked.
There are though some notable differences. Dur-
ing initialization, an unsupervised method u is re-
quired to provide an initial tagging on the data D.
This is an important restriction which is imposed
by the lack of any annotated data. Even under this
restriction, there are some options available, espe-
cially for tasks which have compiled resources. One
option is to use an unsupervised learning algorithm,
such the one presented by Collins & Singer (1999),
where a seed set of rules is used to bootstrap a rule-
based named entity recognizer. A different approach
could be the use of a dictionary-based tagger, as in
Morgan et al (2003). It must be noted that the unsu-
pervised method used to provide the initial tagging
does not need to generalize to any data (a common
problem for such methods), it only needs to perform
well on the data used during active annotation. Gen-
eralization on unseen data is an attribute we hope
that the supervised learning method s will have af-
ter training on the annotated material created with
active annotation.
The query module q is also different from the cor-
responding module in active learning. Instead of se-
lecting unlabeled informative instances to be anno-
tated and added to the training data, its purpose is
to identify likely errors in the imperfectly labelled
training data, so that they are checked and corrected
by the human annotator.
In order to perform error-detection, we chose
to adapt the approach of Nakagawa and Mat-
sumoto (2002) which resembles uncertainty based
sampling for active learning. According to their
paradigm, likely errors in the training data are in-
stances that are ?hard? for the classifier and incon-
sistent with the rest of the data. In our case, we used
the uncertainty of the classifier as the measure of the
?hardness? of an instance. As an indication of in-
consistency, we used the disagreement of the label
assigned by the classifier with the current label of the
instance. Intuitively, if the classifier disagrees with
the label of an instance used in its training, it indi-
cates that there have been other similar instances in
the training data that were labelled differently. Re-
turning to the description of active annotation, the
query module q ranks the instances in L first by their
inconsistency and then by decreasing uncertainty of
the classifier. As a result, instances that are inconsis-
tent with the rest of the data and hard for the classi-
fier are selected first, then those that are inconsistent
but easy for the classifier, then the consistent ones
but hard for the classifier and finally the consistent
and easy ones.
While this method of detecting errors resembles
uncertainty sampling, there are other approaches
that could have been used instead and they can be
very different. Sjo?bergh and Knutsson (2005) in-
serted artificial errors and trained a classifier to rec-
ognize them. Dickinson and Meuers (2003) pro-
posed methods based on n-grams occurring with dif-
ferent labellings in the corpus. Therefore, while it is
reasonable to expect some correlation between the
selections of active annotation and active learning
(hard instances are likely to be erroneously anno-
tated by the unsupervised tagger), the task of select-
ing hard instances is quite different from detecting
errors. The use of the disagreement between tag-
gers for selecting candidates for manual correction
is reminiscent of corrected co-training (Pierce and
Cardie, 2001). However, the main difference is cor-
rected co-training results in a manually annotated
corpus, while active annotation allows automatically
annotated instances to be kept.
5 HMM uncertainty estimation
In order to perform error detection according to the
previous section we need to obtain uncertainty es-
timations over each token from the named entity
recognition module of Lingpipe. For each token t
and possible label l, Lingpipe estimates the follow-
ing Hidden Markov Model from the training data:
P (t[n], l[n]|l[n ? 1], t[n ? 1], t[n ? 2]) (1)
When annotating a certain text passage, the tokens
are fixed and the joint probability of Equation 1 is
computed for each possible combination of labels.
From Bayes? rule, we obtain:
P (l[n]|t[n], l[n ? 1], t[n ? 1], t[n ? 2]) =
P (t[n], l[n]|l[n ? 1], t[n ? 1], t[n ? 2])
P (t[n]|l[n ? 1], t[n ? 1], t[n ? 2]) (2)
For fixed token sequence t[n], t[n ? 1], t[n ? 2]
and previous label (l[n ? 1]) the second term of the
67
left part of Equation 2 is a fixed value. Therefore,
under these conditions, we can write:
P (l[n]|t[n], l[n ? 1], t[n ? 1], t[n ? 2]) ?
P (t[n], l[n]|l[n ? 1], t[n ? 1], t[n ? 2]) (3)
From Equation 3 we obtain an approximation for
the conditional distribution of the current label (l[n])
conditioned on the previous label (l[n ? 1]) for a
fixed sequence of tokens. It must be stressed that
the later restriction is very important. The result-
ing distribution from Equation 3 cannot be com-
pared across different token sequences. However,
for the purpose of computing the uncertainty over
a fixed token sequence it is a reasonable approxi-
mation. One way to estimate the uncertainty of the
classifier is to calculate the conditional entropy of
this distribution. The conditional entropy for a dis-
tribution P (X|Y ) can be computed as:
H[X|Y ] =
?
y
P (Y = y)
?
x
logP (X = x|Y = y)
(4)
In our case, X is l[n] and Y is l[n ? 1]. Func-
tion 4 can be interpreted as the weighted sum of
the entropies of P (l[n]|l[n ? 1]) for each value
of l[n ? 1], in our case the weighted sum of en-
tropies of the distribution of the current label for
each possible previous label. The probabilities for
each tag (needed for P (l[n ? 1])) are not calcu-
lated directly from the model. P (l[n]) corresponds
to P (l[n]|t[n], t[n ? 1], t[n ? 2]), but since we are
considering a fixed token sequence, we approxi-
mate its distribution using the conditional proba-
bility P (l[n]|t[n], l[n ? 1], t[n ? 1], t[n ? 2]), by
marginalizing over l[n ? 1].
Again, it must be noted that the above calculations
are to be used in estimating uncertainty over a single
word. One property of the conditional entropy is that
it estimates the uncertainty of the predictions for the
current label given knowledge of the previous tag,
which is important in our application because we
need the uncertainty over each label independently
from the rest of the sequence. This is confirmed by
the theory, from which we know that for a condi-
tional distribution of X given Y the following equa-
tion holds, H[X|Y ] = H[X,Y ] ? H[Y ], where H
denotes the entropy.
A different way of obtaining uncertainty estima-
tions from HMMs in the framework of active learn-
ing has been presented in (Scheffer et al, 2001).
There, the uncertainty is estimated by the margin
between the two most likely predictions that would
result in a different current label, explicitly:
M = maxi,j {P (t[n] = i|t[n ? 1] = j)} ?
maxk,l,k 6=i {P (t[n] = k|t[n ? 1] = l)} (5)
Intuitively, the margin M is the difference be-
tween the two highest scored predictions that dis-
agree. The lower the margin, the higher the uncer-
tainty of the HMM on the token at question. A draw-
back of this method is that it doesn?t take into ac-
count the distribution of the previous label. It is pos-
sible that the two highest scored predictions are ob-
tained for two different previous labels. It may also
be the case that a highly scored label can be obtained
given a very improbable previous label. Finally, an
alternative that we did not explore in this work is
the Field Confidence Estimation (Culotta and Mc-
Callum, 2004), which allows the estimation of con-
fidence over sequences of tokens, instead of single-
ton tokens only. However, in this work confidence
estimation over singleton tokens is sufficient.
6 Experimental Results
In this section we present results from applying ac-
tive annotation to biomedical named entity recogni-
tion. Using the noise models described in Section 3,
we corrupted the training data and then using Ling-
pipe as the supervised learner we applied the algo-
rithm of Figure 2. The batch of tokens selected to be
checked in each round was 2000 tokens. As a base-
line for comparison we used random selection of to-
kens to be checked. The results for various noise
models and levels are presented in the graphs of Fig-
ure 3. In each of these graphs, the performance of
Lingpipe trained on the partially corrected material
(F-ling) is plotted against the number of checked in-
stances, under the label ?entropy?.
In all the experiments, active annotation signifi-
cantly outperformed random selection, with the ex-
ception of 50% Random, where the high level of
noise (the F-score of the hypothetical tagger that
provided the initial data was 0.1) affected the initial
68
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0  75  150 225 300 375 450 525
F-
lin
g
tokens_checked (in thousands)
Random 10%
random
margin
entropy
 0.675
 0.68
 0.685
 0.69
 0.695
 0.7
 0.705
 0  75  150 225 300 375 450 525
F-
lin
g
tokens_checked (in thousands)
LowRecall_WholeEntities 20%
random
margin
entropy
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0  75  150 225 300 375 450 525
F-
lin
g
tokens_checked (in thousands)
LowRecall 50%
random
margin
entropy
 0.6
 0.61
 0.62
 0.63
 0.64
 0.65
 0.66
 0.67
 0.68
 0.69
 0.7
 0.71
 0  75  150 225 300 375 450 525
F-
lin
g
tokens_checked (in thousands)
LowRecall 20%
random
uncertainty
entropy
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0  75  150 225 300 375 450 525
F-
lin
g
tokens_checked (in thousands)
LowPrecision 20%
random
uncertainty
entropy
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  75  150 225 300 375 450 525
F-
lin
g
tokens_checked (in thousands)
Random 50%
random
uncertainty
entropy
Figure 3: F-score achieved by Lingpipe is plotted
against the number of checked instances for various
models and levels of noise.
judgements of the query module on which instances
should be checked. After having checked some por-
tion of the dataset though, active annotation started
outperforming random selection. In the graphs for
the 10% Random, 20% LowRecall WholeEntities
and 50% LowRecall noise models, under the la-
bel ?margin?, appear the performance curves ob-
tained using the uncertainty estimation of Scheffer
et al (2001). Even though active annotation us-
ing this method performs better than random se-
lection, active annotation using conditional entropy
performs significantly better. These results provide
evidence of the theoretical advantages of conditional
entropy described earlier. We also ran experiments
using pure uncertainty based sampling (i.e. with-
out checking the consistency of the labels) on se-
lecting instances to be checked. The performance
curves appear under the label ?uncertainty? for the
20% LowRecall, 50% Random and 20% LowPreci-
sion noise models. The uncertainty was estimated
using the method described in Section 5. As ex-
pected, uncertainty based sampling performed rea-
sonably well, better than random selection but worse
than using labelling consistency, except for the ini-
tial stage of 20% LowPrecision.
7 Active Annotation versus Active
Learning
In order to compare active annotation to active learn-
ing, we run active learning experiments using the
same dataset and software. The paradigm employed
was uncertainty based sampling, using the uncer-
tainty estimation presented in Sec. 5. HMMs require
annotated sequences of tokens, therefore annotating
whole sentences seemed as the natural choice, as
in (Becker et al, 2005). While token-level selec-
tions could be used in combination with EM, (as in
(Scheffer et al, 2001)), constructing a corpus of in-
dividual tokens would result in a corpus that would
be very difficult to be reused, since it would be par-
tially labelled. We employed the two standard op-
tions of selecting sentences, selecting the sentences
with the highest average uncertainty over the tokens
or selecting the sentence containing the most uncer-
tain token. As cost metric we used the number of
tokens, which allows more straightforward compar-
ison with active annotation.
In Figure 4 (left graph), each active learning ex-
periment is started by selecting a random sentence as
seed data, repeating the seed selection 5 times. The
random selection is repeated 5 times for each seed
selection. As in (Becker et al, 2005), selecting the
sentences with the highest average uncertainty (ave)
performs better than selecting those with the most
uncertain token (max).
In the right graph, we compare the best active
learning method with active annotation. Apparently,
the performance of active annotation is highly de-
pendent on the performance of the unsupervised tag-
ger used to provide us with the initial annotation of
the data. In the graph, we include curves for two
of the noise models reported in the previous sec-
tion, LowRecall20% and LowRecall50% which cor-
respond to tagging performance of 0.66 / 0.69 / 0.67
and 0.33 / 0.43 / 0.37 respectively, in terms of Re-
call / Precision / F. We consider such tagging per-
formances feasible with a dictionary-based tagger,
since Morgan et al (2003) report performance of
69
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  75  150 225 300 375 450 525
F-
lin
g
tokens_checked (in thousands)
Active learning
ave
max
random
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  75  150 225 300 375 450 525
F-
lin
g
tokens_checked (in thousands)
AA vs AL
AA-20%
AA-50%
AL-ave
Figure 4: Left, comparison among various active
learning methods. Right, comparison of active
learning and active annotation.
0.88 / 0/78 / 83 with such a method.
These results demonstrate that active annotation,
given a reasonable starting point, can achieve reduc-
tions in the annotation cost comparable to those of
active learning. Furthermore, active annotation pro-
duces an actual corpus, albeit noisy. Active learn-
ing, as pointed out by Baldridge & Osborne (2004),
while it reduces the amount of training material
needed, it selects data that might not be useful to
train a different learner. In the active annotation
framework, it is likely to preserve correct instances
that might not be useful to the machine learning
method used to create it, but maybe beneficial to a
different method. Furthermore, producing an actual
corpus can be very important when adding new fea-
tures to the model. In the case of biomedical NER,
one could consider adding document-level features,
such as whether a token has been seen as part of a
gene name earlier in the document. With the cor-
pus constructed using active learning this is not fea-
sible, since it is unlikely that all the sentences of a
document are selected for annotation. Also, if one
intended to use the same corpus for a different task,
such as anaphora resolution, again the imperfectly
annotated corpus constructed using active annota-
tion can be used more efficiently than the partially
annotated one produced by active learning.
8 Selecting errors
In order to investigate further the behavior of ac-
tive annotation, we evaluated the performance of the
trained supervised method against the number of er-
rors corrected by the human annotator. The aim of
this experiment was to verify whether the improve-
ment in performance compared to random selection
is due to selecting ?informative? errors to correct, or
due to the efficiency of the error detection technique.
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0  5  10 15 20 25 30 35 40 45 50
F-
lin
g
errors corrected (in thousands)
random
entropy
reverse
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0  75  150 225 300 375 450 525e
rr
o
rs
 c
o
rr
e
ct
ed
 (in
 th
ou
sa
nd
s)
tokens checked (in thousands)
random
entropy
reverse
Figure 5: Left: F-score achieved by Lingpipe
is plotted against the number of corrected errors.
Right: Errors corrected plotted against the number
of checked tokens.
In Figure 5, we present such graphs for the 10%
Random noise model. Similar results were obtained
with different noise models. As can be observed on
the left graph, the errors corrected initially during
random selection are far more informative compared
to those corrected at the early stages of active anno-
tation (labelled ?entropy?). The explanation for this
is that using the error detection method described in
Section 4, the errors that are detected are those on
which the supervised method s disagrees with the
training material, which implies that even if such an
instance is indeed an error then it didn?t affect s.
Therefore, correcting such errors will not improve
the performance significantly. Informative errors are
those that s has learnt to reproduce with high cer-
tainty. However, such errors are hard to detect be-
cause similar attributes are exhibited usually by cor-
rectly labelled instances. This can be verified by the
curves labelled ?reverse? in the graphs of Figure 5,
in which the ranking of the instances to be selected
was reversed, so that instances where the supervised
method agrees confidently with the training material
70
are selected first. The fact that errors with high un-
certainty are less informative than those with low un-
certainty suggests that active annotation, while be-
ing related to active learning, it is sufficiently differ-
ent. The right graph suggests that the error-detection
performance during active annotation is much better
than that of random selection. Therefore, the per-
formance of active annotation could be improved by
preserving the high error-detection performance and
selecting more informative errors.
9 Future work
This paper described active annotation, a semi-
supervised learning framework that reduces the ef-
fort needed to create training material, which is
very important in adapting existing trainable meth-
ods to new domains. Future work should investi-
gate the applicability of the framework in a variety
of NLP/IE tasks and settings. We intend to apply this
framework to NER for biomedical literature from
the FlyBase project for which no annotated datasets
exist.
While we have used the number of instances
checked by a human annotator to measure the cost
of annotation, this might not be representative of the
actual cost. The task of checking and possibly cor-
recting instances differs from annotating them from
scratch. In this direction, experiments in realistic
conditions with human annotators should be carried
out. We also intend to explore the possibility of
grouping similar mistakes detected in a round of ac-
tive annotation, so that the human annotator can cor-
rect them with less effort. Finally, alternative error-
detection methods should be investigated.
Acknowledgments
The author was funded by BBSRC, grant number
38688. I would like to thank Ted Briscoe and Bob
Carpenter for their feedback and comments.
References
J. Baldridge and M. Osborne. 2004. Active learning and
the total cost of annotation. In Proceedings of EMNLP
2004, Barcelona, Spain.
M. Becker, B. Hachey, B. Alex, and C. Grover.
2005. Optimising selective sampling for bootstrap-
ping named entity recognition. In Proceedings of the
Workshop on Learning with Multiple Views, ICML.
D. A. Cohn, Z. Ghahramani, and M. I. Jordan. 1995.
Active learning with statistical models. In Advances
in Neural Information Processing Systems, volume 7.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In Proceedings of the
Joint SIGDAT Conference on EMNLP and VLC.
Aron Culotta and Andrew McCallum. 2004. Confidence
estimation for information extraction. In Proceedings
of HLT 2004, Boston, MA.
M. Dickinson andW. D. Meurers. 2003. Detecting errors
in part-of-speech annotation. In Proceedings of EACL
2003, pages 107?114, Budapest, Hungary.
S. Dingare, J. Finkel, M. Nissim, C. Manning, and
C. Grover. 2004. A system for identifying named en-
tities in biomedical text: How results from two evalua-
tions reflect on both the system and the evaluations. In
The 2004 BioLink meeting at ISMB.
J. D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Ge-
nia corpus - a semantically annotated corpus for bio-
textmining. In ISMB (Supplement of Bioinformatics).
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier,
editors. 2004. Proceedings of JNLPBA, Geneva.
A. Morgan, L. Hirschman, A. Yeh, and M. Colosimo.
2003. Gene name extraction using FlyBase resources.
In Proceedings of the ACL 2003 Workshop on NLP in
Biomedicine, pages 1?8.
T. Nakagawa and Y. Matsumoto. 2002. Detecting errors
in corpora using support vector machines. In Proceed-
ings of COLING 2002.
M. Osborne. 2002. Shallow parsing using noisy and
non-stationary training material. J. Mach. Learn. Res.,
2:695?719.
D. Pierce and C. Cardie. 2001. Limitations of co-training
for natural language learning from large datasets. In
Proceedings of EMNLP 2001, pages 1?9.
T. Scheffer, C. Decomain, and S. Wrobel. 2001. Ac-
tive hiddenMarkov models for information extraction.
Lecture Notes in Computer Science, 2189:309+.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of COLT 1992.
D. Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan. 2004.
Multi-criteria-based active learning for named entity
recongition. In Proceedings of ACL 2004, Barcelona.
J. Sjo?bergh and O. Knutsson. 2005. Faking errors to
avoid making errors: Machine learning for error de-
tection in writing. In Proceedings of RANLP 2005.
71
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 138?145,
New York City, June 2006. c?2006 Association for Computational Linguistics
Bootstrapping and Evaluating Named Entity Recognition in the Biomedical
Domain
Andreas Vlachos
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
av308@cl.cam.ac.uk
Caroline Gasperin
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
cvg20@cl.cam.ac.uk
Abstract
We demonstrate that bootstrapping a gene
name recognizer for FlyBase curation
from automatically annotated noisy text is
more effective than fully supervised train-
ing of the recognizer on more general
manually annotated biomedical text. We
present a new test set for this task based on
an annotation scheme which distinguishes
gene names from gene mentions, enabling
a more consistent annotation. Evaluating
our recognizer using this test set indicates
that performance on unseen genes is its
main weakness. We evaluate extensions
to the technique used to generate training
data designed to ameliorate this problem.
1 Introduction
The biomedical domain is of great interest to in-
formation extraction, due to the explosion in the
amount of available information. In order to deal
with this phenomenon, curated databases have been
created in order to assist researchers to keep up with
the knowledge published in their field (Hirschman et
al., 2002; Liu and Friedman, 2003). The existence
of such resources in combination with the need to
perform information extraction efficiently in order
to promote research in this domain, make it a very
interesting field to develop and evaluate information
extraction approaches.
Named entity recognition (NER) is one of the
most important tasks in information extraction. It
has been studied extensively in various domains,
including the newswire (Tjong Kim Sang and
De Meulder, 2003) domain and more recently the
biomedical domain (Blaschke et al, 2004; Kim et
al., 2004). These shared tasks aimed at evaluat-
ing fully supervised trainable systems. However,
the limited availability of annotated material in most
domains, including the biomedical, restricts the ap-
plication of such methods. In order to circum-
vent this obstacle several approaches have been pre-
sented, among them active learning (Shen et al,
2004) and rule-based systems encoding domain spe-
cific knowledge (Gaizauskas et al, 2003).
In this work we build on the idea of bootstrapping,
which has been applied by Collins & Singer (1999)
in the newsire domain and by Morgan et al (2004)
in the biomedical domain. This approach is based on
creating training material automatically using exist-
ing domain resources, which in turn is used to train
a supervised named entity recognizer.
The structure of this paper is the following. Sec-
tion 2 describes the construction of a new test set
to evaluate named entity recognition for Drosophila
fly genes. Section 3 compares bootstrapping to the
use of manually annotated material for training a su-
pervised method. An extension to the evaluation of
NER appear in Section 4. Based on this evaluation,
section 5 discusses ways of improving the perfor-
mance of a gene name recognizer bootstrapped on
FlyBase resources. Section 6 concludes the paper
and suggests some future work.
2 Building a test set
In this section we present a new test set created to
evaluate named entity recognition for Drosophila fly
genes. To our knowledge, there is only one other
test set built for this purpose, presented in Morgan et
138
al. (2004), which was annotated by two annotators.
The inter-annotator agreement achieved was 87% F-
score between the two annotators, which according
to the authors reflects the difficulty of the task.
Vlachos et al(2006) evaluated their system on
both versions of this test set and obtained signifi-
cantly different results. The disagreements between
the two versions were attributed to difficulties in ap-
plying the guidelines used for the annotation. There-
fore, they produced a version of this dataset resolv-
ing the differences between these two versions using
revised guidelines, partially based on those devel-
oped for ACE (2004). In this work, we applied these
guidelines to construct a new test set, which resulted
in their refinement and clarification.
The basic idea is that gene names (<gn>) are an-
notated in any position they are encountered in the
text, including cases where they are not referring to
the actual gene but they are used to refer to a differ-
ent entity. Names of gene families, reporter genes
and genes not belonging to Drosophila are tagged as
gene names too:
? the <gn>faf</gn> gene
? the <gn>Toll</gn> protein
? the <gn>string</gn>-<gn>LacZ</gn>
reporter genes
In addition, following the ACE guidelines, for
each gene name we annotate the shortest surround-
ing noun phrase. These noun phrases are classified
further into gene mentions (<gm>) and other men-
tions (<om>), depending on whether the mentions
refer to an actual gene or not respectively. Most of
the times, this distinction can be performed by look-
ing at the head noun of the noun phrase:
? <gm>the <gn>faf</gn> gene</gm>
? <om>the <gn>Reaper</gn> protein</om>
However, in many cases the noun phrase itself
is not sufficient to classify the mention, especially
when the mention consists of just the gene name, be-
cause it is quite common in the biomedical literature
to use a gene name to refer to a protein or to other
gene products. In order to classify such cases, the
annotators need to take into account the context in
which the mention appears. In the following exam-
ples, the word of the context that enables us to make
Morgan et al new dataset
abstracts 86 82
tokens 16779 15703
gene-names 1032 629
unique 347 326
gene-names
Table 1: Statistics of the datasets
the distinction between gene mentions (<gm>) and
other mentions is underlined:
? ... ectopic expression of
<gm><gn>hth</gn></gm> ...
? ... transcription of
<gm><gn>string</gn></gm> ...
? ... <om><gn>Rols7</gn></om> localizes ...
It is worth noticing as well that sometimes more
than one gene name may appear within the same
noun phrase. As the examples that follow demon-
strate, this enables us to annotate consistently cases
of coordination, which is another source of disagree-
ment (Dingare et al, 2004):
? <gm><gn>male-specific lethal-1</gn>,
<gn>-2</gn> and <gn>-3</gn> genes</gm>
The test set produced consists of the abstracts
from 82 articles curated by FlyBase1. We used the
tokenizer of RASP2 (Briscoe and Carroll, 2002) to
process the text, resulting in 15703 tokens. The size
and the characteristics of the dataset is comparable
with that of Morgan et al(2004) as it can be observed
from the statistics of Table 1, except for the num-
ber of non-unique gene-names. Apart from the dif-
ferent guidelines, another difference is that we used
the original text of the abstracts, without any post-
processing apart from the tokenization. The dataset
from Morgan et al (2004) had been stripped from
all punctuation characters, e.g. periods and commas.
Keeping the text intact renders this new dataset more
realistic and most importantly it allows the use of
tools that rely on this information, such as syntactic
parsers.
The annotation of gene names was performed
by a computational linguist and a FlyBase curator.
1www.flybase.net
2http://www.cogs.susx.ac.uk/lab/nlp/rasp/
139
We estimated the inter-annotator agreement in two
ways. First, we calculated the F-score achieved be-
tween them, which was 91%. Secondly, we used the
Kappa coefficient (Carletta, 1996), which has be-
come the standard evaluation metric and the score
obtained was 0.905. This high agreement score
can be attributed to the clarification of what gene
name should capture through the introduction of
gene mention and other mention. It must be men-
tioned that in the experiments that follow in the rest
of the paper, only the gene names were used to eval-
uate the performance of bootstrapping. The identifi-
cation and the classification of mentions is the sub-
ject of ongoing research.
The annotation of mentions presented greater dif-
ficulty, because computational linguists do not have
sufficient knowledge of biology in order to use the
context of the mentions whilst biologists are not
trained to identify noun phrases in text. In this ef-
fort, the boundaries of the mentions where defined
by the computational linguist and the classification
was performed by the curator. A more detailed de-
scription of the guidelines, as well as the corpus it-
self in IOB format are available for download3.
3 Bootstrapping NER
For the bootstrapping experiments presented in this
paper we employed the system developed by Vla-
chos et al (2006), which was an improvement of the
system of Morgan et al (2004). In brief, the ab-
stracts of all the articles curated by FlyBase were
retrieved and tokenized by RASP (Briscoe and Car-
roll, 2002). For each article, the gene names and
their synonyms that were recorded by the curators
were annotated automatically on its abstract using
longest-extent pattern matching. The pattern match-
ing is flexible in order to accommodate capitaliza-
tion and punctuation variations. This process re-
sulted in a large but noisy training set, consisting
of 2,923,199 tokens and containing 117,279 gene
names, 16,944 of which are unique. The abstracts
used in the test set presented in the previous section
were excluded. We used them though to evaluate the
performance of the training data generation process
and the results were 73.5% recall, 93% precision and
82.1% F-score.
3www.cl.cam.ac.uk/users/av308/Project Index/node5.html
Training Recall Precision F-score
std 75% 88.2% 81.1%
std-enhanced 76.2% 87.7% 81.5%
BioCreative 35.9% 37.4% 36.7%
Table 2: Results using Vlachos et al (2006) system
This material was used to train the HMM-based
NER module of the open-source toolkit LingPipe4.
The performance achieved on the corpus presented
in the previous section appears in Table 2 in the row
?std?. Following the improvements suggested by
Vlachos et al (2006), we also re-annotated as gene-
names the tokens that were annotated as such by the
data generation process more than 80% of the time
(row ?std-enhanced?), which slightly increased the
performance.
In order to assess the usefulness of this bootstrap-
ping method, we evaluated the performance of the
HMM-based tagger if we trained it on manually an-
notated data. For this purpose we used the anno-
tated data from BioCreative-2004 (Blaschke et al,
2004) task 1A. In that task, the participants were re-
quested to identify which terms in a biomedical re-
search article are gene and/or protein names, which
is roughly the same task as the one we are deal-
ing with in this paper. Therefore we would expect
that, even though the material used for the anno-
tation is not drawn from the exact domain of our
test data (FlyBase curated abstracts), it would still
be useful to train a system to identify gene names.
The results in Table 2 show that this is not the case.
Apart from the domain shift, the deterioration of the
performance could also be attributed to the differ-
ent guidelines used. However, given that the tasks
are roughly the same, it is a very important result
that manually annotated training material leads to
so poor performance, compared to the performance
achieved using automatically created training data.
This evidence suggests that manually created re-
sources, which are expensive, might not be useful
even in slightly different tasks than those they were
initially designed for. Moreover, it suggests that
the use of semi-supervised or unsupervised methods
for creating training material are alternatives worth-
exploring.
4http://www.alias-i.com/lingpipe/
140
4 Evaluating NER
The standard evaluation metric used for NER is the
F-score (Van Rijsbergen, 1979), which is the har-
monic average of Recall and Precision. It is very
successful and popular, because it penalizes systems
that underperform in any of these two aspects. Also,
it takes into consideration the existence multi-token
entities by rewarding systems able to identify the
entity boundaries correctly and penalizing them for
partial matches. In this section we suggest an exten-
sion to this evaluation, which we believe is mean-
ingful and informative for trainable NER systems.
Two are the main expectations from trainable sys-
tems. The first one is that they will be able to iden-
tify entities that they have encountered during their
training. This is not as easy as it might seem, be-
cause in many domains token(s) representing en-
tity names of a certain type can appear as common
words or representing an entity name of a different
type. Using examples from the biomedical domain,
?to? can be a gene name but it is also used as a prepo-
sition. Also gene names are commonly used as pro-
tein names, rendering the task of distinguishing be-
tween the two types non-trivial, even if examples of
those names exist in the training data. The second
expectation is that trainable systems should be able
to learn from the training data patterns that will al-
low it to generalize to unseen named entities. Im-
portant role in this aspect of the performance play
the features that are dependent on the context and
on observations on the tokens. The ability to gener-
alize to unseen named entities is very significant be-
cause it is unlikely that training material can cover
all possible names and moreover, in most domains,
new names appear regularly.
A common way to assess these two aspects is to
measure the performance on seen and unseen data
separately. It is straightforward to apply this in tasks
with token-based evaluation, such as part-of-speech
tagging (Curran and Clark, 2003). However, in the
case of NER, this is not entirely appropriate due
to the existence of multi-token entities. For exam-
ple, consider the case of the gene-name ?head inhi-
bition defective?, which consists of three common
words that are very likely to occur independently of
each other in a training set. If this gene name ap-
pears in the test set but not in the training set, with
a token-based evaluation its identification (or not)
would count towards the performance on seen to-
kens if the tokens appeared independently. More-
over, a system would be rewarded or penalized for
each of the tokens. One approach to circumvent
these problems and evaluate the performance of a
system on unseen named entities, is to replace all
the named entities of the test set with strings that
do not appear in the training data, as in Morgan et
al. (2004). There are two problems with this eval-
uation. Firstly, it alters the morphology of the un-
seen named entities, which is usually a source of
good features to recognize them. Secondly, it affects
the contexts in which the unseen named entities oc-
cur, which don?t have to be the same as that of seen
named entities.
In order to overcome these problems, we used the
following method. We partitioned the correct an-
swers and the recall errors according to whether the
named entity at question have been encountered in
the training data as a named entity at least once. The
precision errors are partitioned in seen and unseen
depending on whether the string that was incorrectly
annotated as a named entity by the system has been
encountered in the training data as a named entity
at least once. Following the standard F-score defi-
nition, partially recognized named entities count as
both precision and recall errors.
In examples from the biomedical domain, if ?to?
has been encountered at least once as a gene name in
the data but an occurrence of in the test dataset is er-
roneously tagged as a gene name, this will count as a
precision error on seen named entities. Similarly, if
?to? has never been encountered in the training data
as a gene name but an occurrence of it in the test
dataset is erroneously tagged as a common word,
this will count as a recall error on unseen named en-
tities. In a multi-token example, if ?head inhibition
defective? is a gene name in the test dataset and it
has been seen as such in the training data but the
NER system tagged (erroneously) ?head inhibition?
as a gene name (which is not the training data), then
this would result in a recall error on seen named en-
tities and a precision error on unseen named entities.
5 Improving performance
Using this extended evaluation we re-evaluated the
named entity recognition system of Vlachos et
141
Recall Precision F-score # entities
seen 95.9% 93.3% 94.5% 495
unseen 32.3% 63% 42.7% 134
overall 76.2% 87.7% 81.5% 629
Table 3: Extended evaluation
al. (2006) and Table 3 presents the results. The big
gap in the performance on seen and unseen named
entities can be attributed to the highly lexicalized
nature of the algorithm used. Tokens that have not
been seen in the training data are passed on to a mod-
ule that classifies them according to their morphol-
ogy, which given the variety of gene names and their
overlap with common words is unlikely to be suffi-
cient. Also, the limited window used by the tagger
(previous label and two previous tokens) does not
allow the capture of long-range contexts that could
improve the recognition of unseen gene names.
We believe that this evaluation allows fair com-
parison between the data generation process that
creating the training data and the HMM-based tag-
ger. This comparison should take into account the
performance of the latter only on seen named enti-
ties, since the former is applied only on those ab-
stracts for which lists of the genes mentioned have
been compiled manually by the curators. The re-
sult of this comparison is in favor of the HMM,
which achieves 94.5% F-score compared to 82.1%
of the data generation process, mainly due to the im-
proved recall (95.9% versus 73.5%). This is a very
encouraging result for bootstrapping techniques us-
ing noisy training material, because it demonstrates
that the trained classifier can deal efficiently with the
noise inserted.
From the analysis performed in this section, it
becomes obvious that the system is rather weak in
identifying unseen gene names. The latter contribute
31% of all the gene names in our test dataset, with
respect to the training data produced automatically
to train the HMM. Each of the following subsec-
tions describes different ideas employed to improve
the performance of our system. As our baseline,
we kept the version that uses the training data pro-
duced by re-annotating as gene names tokens that
appear as part of gene names more than 80% of
times. This version has resulted in the best perfor-
mance obtained so far.
Training Recall Precision F-score cover
bsl 76.2% 87.7% 81.5% 69%
sub 73.6% 83.6% 78.3% 69.6%
bsl+sub 82.2% 83.4% 82.8% 79%
Table 4: Results using substitution
5.1 Substitution
A first approach to improve the overall performance
is to increase the coverage of gene names in the
training data. We noticed that the training set
produced by the process described earlier contains
16944 unique gene names, while the dictionary of
all gene names from FlyBase contains 97227 entries.
This observation suggests that the dictionary is not
fully exploited. This is expected, since the dictio-
nary entries are obtained from the full papers while
the training data generation process is applied only
to their abstracts which are unlikely to contain all of
them.
In order to include all the dictionary entries in
the training material, we substituted in the training
dataset produced earlier each of the existing gene
names with entries from the dictionary. The pro-
cess was repeated until each of the dictionary entries
was included once in the training data. The assump-
tion that we take advantage of is that gene names
should appear in similar lexical contexts, even if the
resulting text is nonsensical from a biomedical per-
spective. For example, in a sentence containing the
phrase ?the sws mutant?, the immediate lexical con-
text could justify the presence of any gene name in
the place ?sws?, even though the whole sentence
would become untruthful and even incomprehensi-
ble. Although through this process we are bound
to repeat errors of the training data, we expect the
gains from the increased coverage to alleviate their
effect. The resulting corpus consisted of 4,062,439
tokens containing each of the 97227 gene names of
the dictionary once. Training the HMM-based tag-
ger with this data yielded 78.3% F-score (Table 4,
row ?sub?). 438 out of the 629 genes of the test set
were seen in the training data.
The drop in precision exemplifies the importance
of using naturally occurring training material. Also,
59 gene names that were annotated in the training
data due to the flexible pattern matching are not in-
142
Training Recall Precision F unseen
score F score
bsl 76.2% 87.7% 81.5% 42.7%
bsl-excl 80.8% 81.1% 81% 51.3%
Table 5: Results excluding sentences without enti-
ties
cluded anymore since they are not in the dictionary,
which explains the drop in recall. Given these ob-
servations, we trained HMM-based tagger on both
versions of the training data, which consisted of
5,527,024 tokens, 218,711 gene names, 106,235 of
which are unique. The resulting classifier had seen
in its training data 79% of the gene names in the
test set (497 out of 629) and it achieved 82.8% F-
score (row ?bsl+sub? in Table 4). It is worth point-
ing out that this improvement is not due to amelio-
rating the performance on unseen named entities but
due to including more of them in the training data,
therefore taking advantage of the high performance
on seen named entities (93.7%). Direct comparisons
between these three versions of the system on seen
and unseen gene names are not meaningful because
the separation in seen and seen gene names changes
with the the genes covered in the training set and
therefore we would be evaluating on different data.
5.2 Excluding sentences not containing entities
From the evaluation of the dictionary based tagger in
Section 3 we confirmed our initial expectation that
it achieves high precision and relatively low recall.
Therefore, we anticipate most mistakes in the train-
ing data to be unrecognized gene names (false neg-
atives). In an attempt to reduce them, we removed
from the training data sentences that did not contain
any annotated gene names. This process resulted
in keeping 63,872 from the original 111,810 sen-
tences. Apparently, such processing would remove
many correctly identified common words (true neg-
atives), but given that the latter are more frequent in
our data we expect it not to have significant impact.
The results appear in Table 5.
In this experiment, we can compare the perfor-
mances on unseen data because the gene names that
were included in the training data did not change.
As we expected, the F-score on unseen gene names
rose substantially, mainly due to the improvement in
recall (from 32.3% to 46.2%). The overall F-score
deteriorated, which is due to the drop in precision.
An error analysis showed that most of the precision
errors introduced were on tokens that can be part
of gene names as well as common words, which
suggests that removing from the training data sen-
tences without annotated entities, deprives the clas-
sifier from contexts that would help the resolution
of such cases. Still though, such an approach could
be of interest in cases where we expect a significant
amount of novel gene names.
5.3 Filtering contexts
The results of the previous two subsections sug-
gested that improvements can be achieved through
substitution and exclusion of sentences without en-
tities, attempting to include more gene names in the
training data and exclude false negatives from them.
However, the benefits from them were hampered be-
cause of the crude way these methods were applied,
resulting in repetition of mistakes as well as exclu-
sion of true negatives. Therefore, we tried to fil-
ter the contexts used for substitution and the sen-
tences that were excluded using the confidence of
the HMM based tagger.
In order to accomplish this, we used the ?std-
enhanced? version of the HMM based tagger to re-
annotate the training data that had been generated
automatically. From this process, we obtained a sec-
ond version of the training data which we expected
to be different from the original one by the data gen-
eration process, since the HMM based tagger should
behave differently. Indeed, the agreement between
the training data and its re-annotation by the HMM
based tagger was 96% F-score. We estimated the
entropy of the tagger for each token and for each
sentence we calculated the average entropy over all
its tokens. We expected that sentences less likely
to contain errors would be sentences on which the
two versions of the training data would agree and
in addition the HMM based tagger would annotate
with low entropy, an intuition similar to that of co-
training (Blum and Mitchell, 1998). Following this,
we removed from the dataset the sentences on which
the HMM-based tagger disagree with the annota-
tion of the data generation process, or it agreed with
but the average entropy of their tokens was above
a certain threshold. By setting this threshold at
143
Training Recall Precision F-score cover
filter 75.6% 85.8% 80.4% 65.5%
filter-sub 80.1% 81% 80.6% 69.6%
filter-sub 83.3% 82.8% 83% 79%
+bsl
Table 6: Results using filtering
0.01, we kept 72,534 from the original 111,810 sen-
tences, which contained 61798 gene names, 11,574
of which are unique. Using this dataset as training
data we achieved 80.4% F-score (row ?filter? in Ta-
ble 6). Even though this score is lower than our
baseline (81.5% F-score), this filtered dataset should
be more appropriate to apply substitution because it
would contain fewer errors.
Indeed, applying substitution to this dataset re-
sulted in better results, compared to applying it to
the original data. The performance of the HMM-
based tagger trained on it was 80.6% F-score (row
?filter-sub? in Table 6) compared to 78.3% (row
?sub? in Table 4). Since both training datasets
contain the same gene names (the ones contained
in the FlyBase dictionary), we can also compare
the performance on unseen data, which improved
from 46.7% to 48.6%. This improvement can be
attributed to the exclusion of some false negatives
from the training data, which improved the recall on
unseen data from 42.9% to 47.1%. Finally, we com-
bined the dataset produced with filtering and substi-
tution with the original dataset. Training the HMM-
based tagger on this dataset resulted in 83% F-score,
which is the best performance we obtained.
6 Conclusions - Future work
In this paper we demonstrated empirically the effi-
ciency of using automatically created training mate-
rial for the task of Drosophila gene name recogni-
tion by comparing it with the use of manually an-
notated material from the broader biomedical do-
main. For this purpose, a test dataset was created
using novel guidelines that allow more consistent
manual annotation. We also presented an informa-
tive evaluation of the bootstrapped NER system that
revealed that indicated its weakness in identifying
unseen gene names. Based on this result we ex-
plored ways to improve its performance. These in-
cluded taking fuller advantage of the dictionary of
gene names from FlyBase, as well as filtering out
likely mistakes from the training data using confi-
dence estimations from the HMM-based tagger.
Our results point out some interesting directions
for research. First of all, the efficiency of bootstrap-
ping calls for its application in other tasks for which
useful domain resources exist. As a complement
task to NER, the identification and classification of
the mentions surrounding the gene names should
be tackled, because it is of interest to the users of
biomedical IE systems to know not only the gene
names but also whether the text refers to the actual
gene or not. This could also be useful to anaphora
resolution systems. Future work for bootstrapping
NER in the biomedical domain should include ef-
forts to incorporate more sophisticated features that
would be able to capture more abstract contexts. In
order to evaluate such approaches though, we be-
lieve it is important to test them on full papers which
present greater variety of contexts in which gene
names appear.
Acknowledgments
The authors would like to thank Nikiforos Karama-
nis and the FlyBase curators Ruth Seal and Chi-
hiro Yamada for annotating the dataset and their ad-
vice in the guidelines. We would like also to thank
MITRE organization for making their data available
to us and in particular Alex Yeh for the BioCre-
ative data and Alex Morgan for providing us with
the dataset used in Morgan et al (2004). The authors
were funded by BBSRC grant 38688 and CAPES
award from the Brazilian Government.
References
ACE. 2004. Annotation guidelines for entity detection
and tracking (EDT).
Christian Blaschke, Lynette Hirschman, and Alexander
Yeh, editors. 2004. Proceedings of the BioCreative
Workshop, Granada, March.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT 1998.
E. J. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the
144
3rd International Conference on Language Resources
and Evaluation, pages 1499?1504.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In Proceedings of the
Joint SIGDAT Conference on EMNLP and VLC.
J. Curran and S. Clark. 2003. Investigating gis and
smoothing for maximum entropy taggers. In Pro-
ceedings of the 11th Annual Meeting of the European
Chapter of the Association for Computational Linguis-
tics.
S. Dingare, J. Finkel, M. Nissim, C. Manning, and
C. Grover. 2004. A system for identifying named en-
tities in biomedical text: How results from two evalua-
tions reflect on both the system and the evaluations. In
The 2004 BioLink meeting at ISMB.
R. Gaizauskas, G. Demetriou, P. J. Artymiuk, and P. Wil-
let. 2003. Protein structures and information ex-
traction from biological texts: The ?PASTA? system.
BioInformatics, 19(1):135?143.
L. Hirschman, J. C. Park, J. Tsujii, L. Wong, and C. H.
Wu. 2002. Accomplishments and challenges in
literature data mining for biology. Bioinformatics,
18(12):1553?1561.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier,
editors. 2004. Proceedings of JNLPBA, Geneva.
H. Liu and C. Friedman. 2003. Mining terminologi-
cal knowledge in large biomedical corpora. In Pacific
Symposium on Biocomputing, pages 415?426.
A. A. Morgan, L. Hirschman, M. Colosimo, A. S. Yeh,
and J. B. Colombe. 2004. Gene name identification
and normalization using a model organism database.
J. of Biomedical Informatics, 37(6):396?410.
D. Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan. 2004.
Multi-criteria-based active learning for named entity
recongition. In Proceedings of ACL 2004, Barcelona.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003, pages 142?147. Edmonton, Canada.
C. J. Van Rijsbergen. 1979. Information Retrieval, 2nd
edition. Dept. of Computer Science, University of
Glasgow.
A. Vlachos, C. Gasperin, I. Lewin, and T. Briscoe. 2006.
Bootstrapping the recognition and anaphoric linking of
named entities in drosophila articles. In Proceedings
of PSB 2006.
145
BioNLP 2007: Biological, translational, and clinical language processing, pages 199?200,
Prague, June 2007. c?2007 Association for Computational Linguistics
Evaluating and combining biomedical named entity recognition systems
Andreas Vlachos
William Gates Building
Computer Laboratory
University of Cambridge
av308@cl.cam.ac.uk
Abstract
This paper is concerned with the evaluation
of biomedical named entity recognition sys-
tems. We compare two such systems, one
based on a Hidden Markov Model and one
based on Conditional Random Fields and
syntactic parsing. In our experiments we
used automatically generated data as well
as manually annotated material, including
a new dataset which consists of biomedi-
cal full papers. Through our evaluation, we
assess the strengths and weaknesses of the
systems tested, as well as the datasets them-
selves in terms of the challenges they present
to the systems.
1 Introduction
The domain of biomedical text mining has become
of importance for the natural language processing
(NLP) community. While there is a lot of textual in-
formation available in the domain, either in the form
of publications or in model organism databases,
there is paucity in material annotated explicitly for
the purpose of developing NLP systems. Most of
the existing systems have been developed using data
from the newswire domain. Therefore, the biomedi-
cal domain is an appropriate platform to evaluate ex-
isting systems in terms of their portability and adapt-
ability. Also, it motivates the development of new
systems, as well as methods for developing systems
with these aspects in focus in addition to the perfor-
mance.
The biomedical named entity recognition (NER)
task in particular has attracted a lot of attention
from the community recently. There have been
three shared tasks (BioNLP/NLPBA 2004 (Kim et
al., 2004), BioCreative (Blaschke et al, 2004) and
BioCreative2 (Krallinger and Hirschman, 2007))
which involved some flavour of NER using manu-
ally annotated training material and fully supervised
machine learning methods. In parallel, there have
been successful efforts in bootstrapping NER sys-
tems using automatically generated training material
using domain resources (Morgan et al, 2004; Vla-
chos et al, 2006). These approaches have a signif-
icant appeal, since they don?t require manual anno-
tation of training material which is an expensive and
lengthy process.
Named entity recognition is an important task be-
cause it is a prerequisite to other more complex ones.
Examples include anaphora resolution (Gasperin,
2006) and gene normalization (Hirschman et al,
2005). An important point is that until now NER
systems have been evaluated on abstracts, or on sen-
tences selected from abstracts. However, NER sys-
tems will be applied to full papers, either on their
own or in order to support more complex tasks.
Full papers though are expected to present additional
challenges to the systems than the abstracts, so it is
important to evaluate on the former as well in or-
der to obtain a clearer picture of the systems and the
task (Ananiadou and McNaught, 2006).
In this paper, we compare two NER systems in
a variety of settings. Most notably, we use auto-
matically generated training data and we evaluate on
abstracts as well as a new dataset consisting of full
papers. To our knowledge, this is the first evalua-
tion of biomedical NER on full paper text instead of
199
abstracts. We assess the performance and the porta-
bility of the systems and using this evaluation we
combine them in order to take advantage of their
strengths.
2 Named entity recognition systems
This section presents the two biomedical named en-
tity recognition systems used in the experiments of
Section 4. Both systems have been used success-
fully for this task and are domain-independent, i.e.
they don?t use features or resources that are tailored
to the biomedical domain.
2.1 Hidden Markov Model
The first system used in our experiments was the
HMM-based (Rabiner, 1990) named entity recogni-
tion module of the open-source NLP toolkit Ling-
Pipe1. It is a hybrid first/second order HMM
model using Witten-Bell smoothing (Witten and
Bell, 1991). It estimates the following joint proba-
bility of the current token xt and label yt conditioned
on the previous label yt?1 and previous two tokens
xt?1 and xt?2:
P (xt, yt|yt?1, xt?1, xt?2) (1)
Tokens unseen in the training data are passed to
a morphological rule-based classifier which assigns
them to predefined classes according to their capital-
ization and whether they contain digits or punctua-
tion. In order to use these classes along with the or-
dinary tokens, during training a second pass over the
training data is performed in which tokens that ap-
pear fewer times than a given threshold are replaced
by their respective classes. In our experiments, this
threshold was set experimentally to 8. Vlachos et
al. (2006) employed this system and achieved good
results on bootstrapping biomedical named entity
recognition. They also note though that due to its re-
liance on seen tokens and the restricted way in which
unseen tokens are handled its performance is not as
good on unseen data.
1http://www.alias-i.com/lingpipe. The version used in the
experiments was 2.1.
2.2 Conditional Random Fields with Syntactic
Parsing
The second NER system we used in our experiments
was the system of Vlachos (2007) that participated
in the BioCreative2 Gene Mention task (Krallinger
and Hirschman, 2007). Its main components are the
Conditional Random Fields toolkit MALLET2 (Mc-
Callum, 2002) and the RASP syntactic parsing
toolkit3 (Briscoe et al, 2006), which are both pub-
licly available.
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) are undirected graphical models trained to
maximize the conditional probability of the output
sequence given the inputs, or, in the case of token-
based natural language processing tasks, the condi-
tional probability of the sequence of labels y given
a sequence of tokens x. Like HMMs, the number of
previous labels taken into account defines the order
of the CRF model. More formally:
P (y|x) = 1Z(x)exp{
T
?
t=1
K
?
k=1
?kfk(y, xt)} (2)
In the equation above, Z(x) is a normalization
factor computed over all possible label sequences,
fk is a feature function and ?k its respective weight.
y represents the labels taken into account as context
and it is defined by the order of the CRF. For a n-th
order model, y becomes yt, yt?1..., yt?n. It is also
worth noting that xt is the feature representation of
the token in position t, which can include features
extracted by taking the whole input sequence into
account, not just the token in question. The main
advantage is that as a conditionally-trained model
CRFs do not need to take into account dependen-
cies in input, which as a consequence, allows the use
of features dependent on each other. Compared to
HMMs, their main disadvantage is that during train-
ing, the computation time required is significantly
longer. The interested reader is referred to the de-
tailed tutorial of Sutton & McCallum (2006).
Vlachos (2007) used a second order CRF model
combined with a variety of features. These can
be divided into simple orthographic features and in
2http://mallet.cs.umass.edu/index.php/Main Page
3http://www.informatics.susx.ac.uk/research/nlp/rasp/
200
those extracted from the output of the syntactic pars-
ing toolkit. The former are extracted for every token
and they are rather common in the NER literature.
They include the token itself, whether it contains
digits, letters or punctuation, information about cap-
italization, prefixes and suffixes.
The second type of features are extracted from
the output of RASP for each sentence. The part-of-
speech (POS) tagger was parameterized to generate
multiple POS tags for each token in order to amelio-
rate unseen token errors. The syntactic parser uses
these sequences of POS tags to generate parses for
each sentence. The output is in the form of grammat-
ical relations (GRs), which specify the links between
the tokens in the sentence accoring to the syntactic
parser and they are encoded using the SciXML for-
mat (Copestake et al, 2006). From this output, for
each token the following features are extracted (if
possible):
? the lemma and the POS tag(s) associated with
the token
? the lemmas for the previous two and the fol-
lowing two tokens
? the lemmas of the verbs to which this token is
subject
? the lemmas of the verbs to which this token is
object
? the lemmas of the nouns to which this token
acts as modifier
? the lemmas of the modifiers of this token
Adding the features from the output of the syntac-
tic parser allows the incorporation of features from
a wider context than the two tokens before and after
captured by the lemmas, since GRs can link tokens
within a sentence independently of their proximity.
Also, they result in more specific features, since the
relation between two tokens is determined. The CRF
models in the experiments of Section 4 were trained
until convergence.
It must be mentioned that syntactic parsing is a
complicated task and therefore feature extraction on
its output is likely to introduce some noise. The
RASP syntactic parser is domain independent but
it has been developed using data from general En-
glish corpora mainly, so it is likely not to perform
as well in the biomedical domain. Nevertheless,
the results of the system in the BioCreative2 Gene
Mention task suggest that the use of syntactic pars-
ing features improve performance. Also, despite the
lack of domain-specific features, the system is com-
petitive with other systems, having performance in
the second quartile of the task. Finally, the BIOEW
scheme (Siefkes, 2006) was used to tag the tok-
enized corpora, under which the first token of a mul-
titoken mention is tagged as B, the last token as E,
the inner ones as I, single token mentions as W and
tokens outside an entity as O.
3 Corpora
In our experiments we used two corpora consisting
of abstracts and one consisting of full papers. One
of the abstracts corpora was automatically generated
while the other two were manually annotated. All
three were created using resources from FlyBase4
and they are publicly available5 .
The automatically generated corpus was created
in order to bootstrap a gene name recognizer in Vla-
chos & Gasperin (2006). The approach used was
introduced by Morgan et al(2004). In brief, the ab-
stracts of 16,609 articles curated by FlyBase were
retrieved and tokenized by RASP (Briscoe et al,
2006). For each article, the gene names and their
synonyms that were recorded by the curators were
annotated automatically in its abstract using longest-
extent pattern matching. The pattern matching is
flexible in order to accommodate capitalization and
punctuation variations. This process resulted in a
large but noisy dataset, consisting of 2,923,199 to-
kens and containing 117,279 gene names, 16,944 of
which are unique. The noise is due to two reasons
mainly. First, the lists constructed by the curators
for each paper are incomplete in two ways. They
don?t necessarily contain all the genes mentioned in
an abstract because not all genes are always curated
and also not all synonyms are recorded, thus result-
ing in false negatives. The other cause is the overlap
between gene names and common English words or
biomedical terms, which results in false positives for
4http://www.flybase.net/
5http://www.cl.cam.ac.uk/ nk304/Project Index/#resources
201
abstracts with such gene names.
The manually annotated corpus of abstracts was
described in Vlachos & Gasperin (2006). It con-
sists of 82 FlyBase abstracts that were annotated
by a computational linguist and a FlyBase curator.
The full paper corpus was described in Gasperin et
al. (2007). It consists of 5 publicly available full pa-
pers which were annotated by a computational lin-
guist and a FlyBase curator with named entities as
well as anaphoric relations in XML. To use it for
the gene name recognition experiments presented in
this paper, we converted it from XML to IOB format
keeping only the annotated gene names.
noisy golden full
abstracts abstracts papers
abstracts / 16,609 82 5
papers
sentences 111,820 600 1,220
tokens 2,923,199 15,703 34,383
gene names 117,279 629 2,057
unique 16,944 326 336
gene names
unique non- 60,943 3,018 4,113
gene tokens
Table 1: Statistics of the datasets
The gene names in both manually created cor-
pora were annotated using the guidelines presented
in Vlachos & Gasperin (2006). The main idea of
these guidelines is that gene names are annotated
anywhere they are encountered in the text, even
when they are used to refer to biomedical entities
other than the gene itself. The distinction between
the possible types of entities the gene name can re-
fer to is performed at the level of the shortest noun
phrase surrounding the gene name. This resulted in
improved inter-annotator agreement (Vlachos et al,
2006).
Statistics on all three corpora are presented in Ta-
ble 1. From the comparisons in this table, an in-
teresting observation is that the gene names in full
papers tend to be repeated more frequently than the
gene names in the manually annotated abstracts (6.1
compared to 1.9 times respectively). Also, the lat-
ter contain approximately 2 unique gene names ev-
ery 100 tokens while the full papers contain just 1.
This evidence suggests that annotating abstracts is
more likely to provide us with a greater variety of
gene names. Interestingly, the automatically anno-
tated abstracts contain only 0.6 unique gene names
every 100 tokens which hints at inclusion of false
negatives during the annotation.
Another observation is that, while the manually
annotated abstracts and full papers contain roughly
the same number of unique genes, the full papers
contain 36% more unique tokens that are not part
of a gene name (?unique non-gene tokens? in Ta-
ble 1). This suggests that the full papers contain a
greater variety of contexts, as well as negative ex-
amples, therefore presenting greater difficultiy to a
gene name recognizer.
4 Experiments
We ran experiments using the two NER systems and
the three datasets described in Sections 2 and 3.
In order to evaluate the performance of the sys-
tems, apart from the standard recall, precision and
F-score metrics, we measured the performance on
seen and unseen gene names independently, as sug-
gested by Vlachos & Gasperin (2006). In brief, the
gene names that are in the test set and the output
generated by the system are separated according to
whether they have been encountered in the training
data as gene names. Then, the standard recall, pre-
cision and F-score metrics are calculated for each of
these lists independently.
HMM CRF+RASP
Recall 75.68 63.43
overall Precision 89.14 90.89
F-score 81.86 74.72
Recall 94.48 76.32
seen Precision 93.62 95.4
genes F-score 94.05 84.80
Recall 33.51 34.54
unseen Precision 68.42 73.63
genes F-score 44.98 47.02
seen genes 435
unseen genes 194
Table 2: Results on training on noisy abstracts and
testing on manually annotated abstracts
202
HMM CRF+RASP
Recall 58.63 61.40
overall Precision 80.56 89.19
F-score 67.87 72.73
Recall 89.82 72.51
seen Precision 87.83 94.82
genes F-score 88.81 82.18
Recall 35.12 53.03
unseen Precision 69.48 84.05
genes F-score 46.66 65.03
seen genes 884
unseen genes 1173
Table 3: Results on training on noisy abstracts and
testing on full papers
Tables 2 and 3 report in detail the performance of
the two systems when trained on the noisy abstracts
and evaluated on the manually annotated abstracts
and full papers respectively. As it can be seen, the
performance of the HMM-based NER system is bet-
ter than that of CRF+RASP when evaluating on ab-
stracts and worse when evaluating on full papers
(81.86 vs 74.72 and 67.87 vs 72.73 respectively).
Further analysis of the performance of the two
systems on seen and unseen genes reveals that this
result is more likely to be due to the differences be-
tween the two evaluation datasets and in particular
the balance between seen and unseen genes with re-
spect to the training data used. In both evaluations,
the performance of the HMM-based NER system is
superior on seen genes while the CRF+RASP sys-
tem performs better on unseen genes. On the ab-
stracts corpus the performance on seen genes be-
comes more important since there are more seen
than unseen genes in the evaluation, while the op-
posite is the case for the full paper corpus.
The difference in the performance of the two sys-
tems is justified. The CRF+RASP system uses a
complex but more general representation of the con-
text based on the features extracted from the output
of syntactic parser, namely the lemmas, the part-of-
speech tags and the grammatical relationships, while
the HMM-based system uses a simple morphologi-
cal rule-based classifier. Also, the CRF+RASP sys-
tem takes the two previous labels into account, while
the HMM-based only the previous one. Therefore,
it is expected that the former has superior perfor-
mance on unseen genes. This difference between the
CRF+RASP and the HMM-based system is substan-
tially larger when evaluating on full papers (65.03
versus 46.66 respectively) than on abstracts (47.02
versus 44.98 respectively). This can be attributed
to the fact that the training data used is generated
from abstracts and when evaluating on full papers
the domain shift can be handled more efficiently by
the CRF+RASP system due to its more complex fea-
ture set.
However, the increased complexity of the
CRF+RASP system renders it more vulnerable to
noise. This is particularly important in these experi-
ments because we are aware that our training dataset
contains noise since it was automatically generated.
This noise is in addition to that from inaccurate syn-
tactic parsing employed, as explained in Section 2.2.
On the other hand, the simpler HMM-based sys-
tem is likely to perform better on seen genes, whose
recognition doesn?t require complex features.
We also ran experiments using the manually an-
notated corpus of abstracts as training data and eval-
uated on the full papers. The results in Table 4
confirmed the previous assessment, that the perfor-
mance of the CRF+RASP system is better on the un-
seen genes and that the HMM-based one is better on
seen genes. In this particular evaluation, the small
number of unique genes in the manually annotated
corpus of abstracts results in the majority of gene
names being unseen in the training data, which fa-
vors the CRF+RASP system.
It is important to note though that the perfor-
mances for both systems were substantially lower
than the ones achieved using the large and noisy
automatically generated corpus of abstracts. This
can be attributed to the fact that both systems have
better performance in recognizing seen gene names
rather than unseen ones. Given that the automati-
cally generated corpus required no manual annota-
tion and very little effort compared to the manually
annotated one, it is a strong argument for bootstrap-
ping techniques.
A known way of reducing the effect of noise in
sequential models such as CRFs is to reduce their
order. However, this limits the context taken into ac-
count, potentially harming the performance on un-
seen gene names. Keeping the same feature set, we
203
HMM CRF+RASP
Recall 52.65 49.88
overall Precision 46.56 72.77
F-score 49.42 59.19
Recall 96.49 47.37
seen Precision 58.51 55.1
genes F-score 72.85 50.94
Recall 51.4 49.95
unseen Precision 46.04 73.4
genes F-score 48.57 59.45
seen genes 57
unseen genes 2000
Table 4: Results on training on manually annotated
abstracts and testing on full papers
trained a first order CRF model on the noisy ab-
stracts corpus and we evaluated on the manually an-
notated abstracts and full papers. As expected, the
performance on the seen gene names improved but
deteriorated on the unseen ones. In particular, when
evaluating on abstracts the F-scores achieved were
93.22 and 38.1 respectively (compared to 84.8 and
47.02) and on full papers 86.64 and 59.86 (compared
to 82.18 and 65.03). The overall performance im-
proved substantially for the abstract where the seen
genes are the majority (74.72 to 80.69), but only
marginally for the more balanced full papers (72.73
to 72.89).
Ideally, we wouldn?t want to sacrifice the perfor-
mance on unseen genes of the CRF+RASP system
in order to deal with noise. While the large noisy
training dataset provides good coverage of the pos-
sible gene names, it is unlikely to contain every gene
name we would encounter, as well as all the possible
common English words which can become precision
errors. Therefore we attempted to combine the two
NER systems based on the evaluation presented ear-
lier. Since the HMM-based system is performing
very well on seen gene names, for each sentence we
check whether it has recognized any gene names un-
seen in the training data (potential unseen precision
errors) or if it considered as ordinary English words
any tokens not seen as such in the training data (po-
tential unseen recall errors). If either of these is true,
then we pass the sentence to the CRF+RASP sys-
tem, which has better performance on unseen gene
names.
Such a strategy is expected to trade some of the
performance of the seen gene names of the HMM-
based system for improved performance on the un-
seen gene names by using the predictions of the
CRF+RASP system. This occurs because in the
same sentence seen and unseen gene names may co-
exist and choosing the predictions of the latter sys-
tem could result in more errors on the seen gene
names. This strategy is likely to improve the per-
formance on datasets where there are more unseen
gene names and the difference in the performance
of the CRF+RASP on them is substantially better
than the HMM-based. Indeed, using this strategy we
achieved 73.95 overall F-score on the full paper cor-
pus which contains slightly more unseen gene names
(57% of the total gene names). For the corpus of
manually annotated abstracts the performance was
reduced to 80.21, which is expected since the major-
ity of gene names (69%) are seen in the training data.
and the performance of the CRF+RASP system on
the unseen data is better only by a small margin than
the HMM-based one (47.02 vs 44.98 in F-score re-
spectively).
5 Discussion - Related work
The experiments of the previous section are to our
knowledge the first to evaluate biomedical named
entity recognition on full papers. Furthermore, we
consider that using abstracts as the training mate-
rial for such evaluation is a very realistic scenario,
since abstracts are generally publicly available and
therefore easy to share and distribute with a trainable
system, while full papers on which they are usually
applied are not always available.
Differences between abstracts and full papers can
be important when deciding what kind of material to
annotate for a certain purpose. For example, if the
annotated material is going to be used as training
data and given that higher coverage of gene names
in the training data is beneficial, then it might be
preferable to annotate abstracts because they con-
tain greater variety of gene names which would re-
sult in higher coverage in the dataset. On the other
hand, full papers contain a greater variety of con-
texts which can be useful for training a system and
as mentioned earlier, they can be more appropriate
204
for evaluation.
It would be of interest to train NER systems on
training material generated from full papers. Con-
sidering the effort required in manual annotation
though, it would be difficult to obtain quantities of
such material large enough that would provide ade-
quate coverage of a variety of gene names. An alter-
native would be to generate it automatically. How-
ever, the approach employed to generate the noisy
abstracts corpus used in this paper is unlikely to pro-
vide us with material of adequate quality to train a
gene name recognizer. This is because more noise
is going to be introduced, since full papers are likely
to contain more gene names not recorded by the cu-
rators, as well as more common English words that
happen to overlap with the genes mentioned in the
paper.
The aim of this paper is not about deciding on
which of the two models is better but about how
the datasets used affect the evaluation and how to
combine the strengths of the models based on the
analysis performed. In this spirit, we didn?t attempt
any of the improvements discussed by Vlachos &
Gasperin (2006) because they were based on obser-
vations on the behavior of the HMM-based system.
From the analysis presented earlier, the CRF+RASP
system behaves differently and therefore it?s not cer-
tain that those strategies would be equally beneficial
to it.
As mentioned in the introduction, there has been
a lot of work on biomedical NER, either through
shared tasks or independent efforts. Of particular
interest is the work of Morgan et al(2004) who
bootstrapped an HMM-based gene name recognizer
using FlyBase resources and evaluate on abstracts.
Also of interest is the system presented by Set-
tles (2004) which used CRFs with rich feature sets
and suggested that one could use features from syn-
tactic parsing with this model given their flexibility.
Direct comparisons with these works are not possi-
ble since different datasets were used.
Finaly, combining models has been a successful
way of achieving good results, such as those of Flo-
rian et al (2003) who had the top performance in
the named entity recognition shared task of CoNLL
2003 (Tjong Kim Sang and De Meulder, 2003).
6 Conclusions- Future work
In this paper we compared two different named en-
tity recognition systems on abstracts and full pa-
per corpora using automatically generated training
data. We demonstrated how the datasets affect the
evaluation and how the two systems can be com-
bined. Also, our experiments showed that bootstrap-
ping using automatically annotated abstracts can be
efficient even when evaluating on full papers.
As future work, it would be of interest to de-
velop an efficient way to generate data automati-
cally from full papers which could improve the re-
sults further. An interesting approach would be to
combine dictionary-based matching with an exist-
ing NER system in order to reduce the noise. Also,
different ways of combining the two systems could
be explored. With constrained conditional random
fields (Kristjansson et al, 2004) the predictions of
the HMM on seen gene names could be added as
constraints to the inference performed by the CRF.
The good performance of bootstrapping gene
name recognizers using automatically created train-
ing data suggests that it is a realistic alternative to
fully supervised systems. The latter have benefited
from a series of shared tasks that, by providing a
testbed for evaluation, helped assessing and improv-
ing their performance. Given the variety of meth-
ods that are available for generating training data
efficiently automatically using extant domain re-
sources (Morgan et al, 2004) or semi-automatically
(active learning approaches like Shen et al (2004)
or systems using seed rules such as Mikheev et
al. (1999)), it would be of interest to have a shared
task in which the participants would have access to
evaluation data only and they would be invited to use
such methods to develop their systems.
References
Sophia Ananiadou and John McNaught, editors. 2006.
Text Mining in Biology and Biomedicine. Artech
House, Inc.
Christian Blaschke, Lynette Hirschman, and Alexander
Yeh, editors. 2004. Proceedings of the BioCreative
Workshop, Granada, March.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
205
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions.
Ann Copestake, Peter Corbett, Peter Murray-Rust,
CJ Rupp, Advaith Siddharthan, Simone Teufel, and
Ben Waldron. 2006. An architecture for language pro-
cessing for scientific texts. In Proceedings of the UK
e-Science All Hands Meeting 2006.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Walter Daelemans and Miles
Osborne, editors, Proceedings of CoNLL-2003, pages
168?171. Edmonton, Canada.
C. Gasperin, N. Karamanis, and R. Seal. 2007. Annota-
tion of anaphoric relations in biomedical full-text arti-
cles using a domain-relevant scheme. In Proceedings
of DAARC.
Caroline Gasperin. 2006. Semi-supervised anaphora res-
olution in biomedical texts. In Proceedings of BioNLP
in HLT-NAACL, pages 96?103.
Lynette Hirschman, Marc Colosimo, Alexander Morgan,
and Alexander Yeh. 2005. Overview of biocreative
task 1b: normalized gene lists. BMC Bioinformatics.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier,
editors. 2004. Proceedings of JNLPBA, Geneva.
Martin Krallinger and Lynette Hirschman, editors. 2007.
Proceedings of the Second BioCreative Challenge
Evaluation Workshop, Madrid, April.
Trausti Kristjansson, Aron Culotta, Paul Viola, and An-
drew McCallum. 2004. Interactive information ex-
traction with constrained conditional random fields.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of ICML 2001, pages 282?289.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
A. Mikheev, M. Moens, and C. Grover. 1999. Named
entity recognition without gazetteers.
A. A. Morgan, L. Hirschman, M. Colosimo, A. S. Yeh,
and J. B. Colombe. 2004. Gene name identification
and normalization using a model organism database.
J. of Biomedical Informatics, 37(6):396?410.
L. R. Rabiner. 1990. A tutorial on hidden markov mod-
els and selected apllications in speech recognition. In
A. Waibel and K.-F. Lee, editors, Readings in Speech
Recognition, pages 267?296. Kaufmann, San Mateo,
CA.
Burr Settles. 2004. Biomedical Named Entity Recog-
nition Using Conditional Random Fields and Novel
Feature Sets. Proceedings of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications.
D. Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan. 2004.
Multi-criteria-based active learning for named entity
recongition. In Proceedings of ACL 2004, Barcelona.
Christian Siefkes. 2006. A comparison of tagging strate-
gies for statistical information extraction. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Companion Volume: Short Papers,
pages 149?152, New York City, USA, June. Associ-
ation for Computational Linguistics.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003, pages 142?147. Edmonton, Canada.
A. Vlachos and C. Gasperin. 2006. Bootstrapping and
evaluating named entity recognition in the biomedical
domain. In Proceedings of BioNLP in HLT-NAACL,
pages 138?145.
A. Vlachos, C. Gasperin, I. Lewin, and T. Briscoe. 2006.
Bootstrapping the recognition and anaphoric linking of
named entities in drosophila articles. In Proceedings
of PSB 2006.
Andreas Vlachos. 2007. Tackling the BioCreative2
Gene Mention task with Conditional Random Fields
and Syntactic Parsing. In Proceedings of the Second
BioCreative Challenge Evaluation Workshop.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
206
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 74?82,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Unsupervised and Constrained Dirichlet Process Mixture Models for Verb
Clustering
Andreas Vlachos
Computer Laboratory
University of Cambridge
Cambridge CB3 0FD, UK
av308l@cl.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge CB3 0FD, UK
alk23@cl.cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
Cambridge CB2 1PZ, UK
zoubin@eng.cam.ac.uk
Abstract
In this work, we apply Dirichlet Process
Mixture Models (DPMMs) to a learning
task in natural language processing (NLP):
lexical-semantic verb clustering. We thor-
oughly evaluate a method of guiding DP-
MMs towards a particular clustering so-
lution using pairwise constraints. The
quantitative and qualitative evaluation per-
formed highlights the benefits of both
standard and constrained DPMMs com-
pared to previously used approaches. In
addition, it sheds light on the use of evalu-
ation measures and their practical applica-
tion.
1 Introduction
Bayesian non-parametric models have received a
lot of attention in the machine learning commu-
nity. These models have the attractive property
that the number of components used to model
the data is not fixed in advance but is actually
determined by the model and the data. This
property is particularly interesting for NLP where
many tasks are aimed at discovering novel, pre-
viously unknown information in corpora. Recent
work has applied Bayesian non-parametric mod-
els to anaphora resolution (Haghighi and Klein,
2007), lexical acquisition (Goldwater, 2007) and
language modeling (Teh, 2006) with good results.
Recently, Vlachos et al (2008) applied the ba-
sic models of this class, Dirichlet Process Mix-
ture Models (DPMMs) (Neal, 2000), to a typical
learning task in NLP: lexical-semantic verb clus-
tering. The task involves discovering classes of
verbs similar in terms of their syntactic-semantic
properties (e.g. MOTION class for travel, walk,
run, etc.). Such classes can provide important
support for other NLP tasks, such as word sense
disambiguation, parsing and semantic role label-
ing (Dang, 2004; Swier and Stevenson, 2004).
Although some fixed classifications are available
(e.g. VerbNet (Kipper-Schuler, 2005)) these are
not comprehensive and are inadequate for specific
domains (Korhonen et al, 2006b).
Unlike the clustering algorithms applied to this
task before, DPMMs do not require the number of
clusters as input. This is important because even
if the number of classes in a particular task was
known (e.g. in the context of a carefully controlled
experiment), a particular dataset may not contain
instances for all the classes. Moreover, each class
is not necessarily contained in one cluster exclu-
sively, since the target classes are defined manu-
ally without taking into account the feature rep-
resentation used. The fact that DPMMs do not
require the number of target clusters in advance,
renders them promising for the many NLP tasks
where clustering is used for learning purposes.
While the results of Vlachos et al (2008) are
promising, the use of a clustering approach which
discovers the number of clusters in data presents
a new challenge to existing evaluation measures.
In this work, we investigate optimal evaluation
for such approaches, using the dataset and the ba-
sic method of Vlachos et al as a starting point.
We review the applicability of existing evalua-
tion measures and propose a modified version of
the newly introduced V-measure (Rosenberg and
Hirschberg, 2007). We complement the quanti-
tative evaluation with thorough qualitative assess-
ment, for which we introduce a method to summa-
rize samples obtained from a clustering algorithm.
In preliminary work by Vlachos et al (2008),
a constrained version of DPMMs which takes ad-
vantage of must-link and cannot-link pairwise con-
straints was introduced. It was demonstrated how
such constraines can guide the clustering solution
towards some prior intuition or considerations rel-
evant to the specific NLP application in mind. We
explain the inference algorithm for the constrained
DPMM in greater detail and evaluate quantita-
74
tively the contribution of each constraint type of
independently, complementing it with qualitative
analysis. The latter demonstrates how the pairwise
constraints added affects instances beyond those
involved directly. Finally, we discuss how the un-
supervised and the constrained version of DPMMs
can be used in a real-world setup.
The results from our comprehensive evaluation
show that both versions of DPMMs are capable
of learning novel information not in the gold stan-
dard, and that the constrained version is more ac-
curate than a previous verb clustering approach
which requires setting the number of clusters in
advance and is therefore less realistic.
2 Unsupervised clustering with DPMMs
With DPMMs, as with other Bayesian non-
parametric models, the number of mixture compo-
nents is not fixed in advance, but is determined by
the model and the data. The parameters of each
component are generated by a Dirichlet Process
(DP) which can be seen as a distribution over the
parameters of other distributions. In turn, each in-
stance is generated by the chosen component given
the parameters defined in the previous step:
G|?,G0 ? DP (?,G0)
?i|G ? G (1)
xi|?i ? F (?i)
In Eq. 1, G0 and G are probability distributions
over the component parameters (?), and ? > 0 is
the concentration parameter which determines the
variance of the Dirichlet process. We can think
of G as a randomly drawn probability distribution
with meanG0. Intuitively, the larger ? is, the more
similar G will be to G0. Instance xi is generated
by distribution F , parameterized by ?i. The graph-
ical model is depicted in Figure 1.
The prior probability of assigning an instance
to a particular component is proportionate to the
number of instances already assigned to it (n?i,z).
In other words, DPMMs exhibit the ?rich get
richer? property. In addition, the probability that
a new cluster is created is dependent on the con-
centration parameter ?. A popular metaphor to de-
scribe DPMMs which exhibits an equivalent clus-
tering property is the Chinese Restaurant Process
(CRP). Customers (instances) arrive at a Chinese
restaurant which has an infinite number of tables
(components). Each customer sits at one of the ta-
bles that is either occupied or vacant with popular
tables attracting more customers.
Figure 1: Graphical representation of DPMMs.
In this work, the distribution used to model the
components is the multinomial and the prior used
is the Dirichlet distribution (F and G0 in Eq. 1).
The conjugacy between them allows for the ana-
lytic integration over the component parameters.
Following Neal (2000), the component assign-
ments zi are sampled using the following scheme:
P (zi = z|z?i, xi) ?
p(zi = z|z?i)DirM(xi|zi = z, x?i,z, ?) (2)
In Eq. 2DirM is the Dirichlet-Multinomial distri-
bution, ? are the parameters of the Dirichlet prior
G0 and x?i,z are the instances assigned already to
component z (none if we are sampling the prob-
ability of assignment to a new component). This
sampling scheme is possible due to the fact that the
instances in the model are exchangeable, i.e. the
order in which they are generated is not relevant.
In terms of the CRP metaphor, we consider each
instance xi as the last customer to arrive and he
chooses to sit together with other customers at an
existing table or to sit at a new table. Following
Navarro et al (2006) who used the same model to
analyze individual differences, we sample the con-
centration parameter ? using the inverse Gamma
distribution as a prior.
3 Evaluation measures
The evaluation of unsupervised clustering against
a gold standard is not straightforward because the
clusters found are not explicitly labelled. Formally
defined, an unsupervised clustering algorithm par-
titions a set of instances X = {xi|i = 1, ..., N}
into a set of clusters K = {kj |j = 1, ..., |K|}.
The standard approach to evaluate the quality of
the clusters is to use an external gold standard in
which the instances are partitioned into a set of
75
classes C = {cl|l = 1, ..., |C|}. Given this, the
goal is to find a partitioning of the instances K
that is as close as possible to the gold standard C.
Most work on verb clustering has used the F-
measure or the Rand Index (RI) (Rand, 1971)
for evaluation, which rely on counting pairwise
links between instances. However, Rosenberg and
Hirschberg (2007) pointed out that F-measure as-
sumes (the missing) mapping between cl and kj .
In practice, RI values concentrate in a small inter-
val near 100% (Meila?, 2007).
Rosenberg & Hirschberg (2007) proposed an
information-theoretic metric: V-measure. V-
measure is the harmonic mean of homogeneity
and completeness which evaluate the quality of the
clustering in a complementary way. Homogeneity
assesses the degree to which each cluster contains
instances from a single class of C. This is com-
puted as the conditional entropy of the class dis-
tribution of the gold standard given the clustering
discovered by the algorithm, H(C|K), normal-
ized by the entropy of the class distribution in the
gold standard, H(C). Completeness assesses the
degree to which each class is contained in a single
cluster. This is computed as the conditional en-
tropy of the cluster distribution discovered by the
algorithm given the class, H(K|C), normalized
by the entropy of the cluster distribution, H(K).
In both cases, we subtract the resulting ratios from
1 to associate higher scores with better solutions:
h = 1?
H(C|K)
H(C)
c = 1?
H(K|C)
H(K)
V? =
(1 + ?) ? h ? c
(? ? h) + c
(3)
The parameter ? in Eq. 3 regulates the balance
between homogeneity and completeness. Rosen-
berg & Hirschberg set it to 1 in order to obtain the
harmonic mean of these qualities. They also note
that V-measure favors clustering solutions with a
large number of clusters (large |K|), since such so-
lutions can achieve very high homogeneity while
maintaining reasonable completeness. This ef-
fect is more prominent when a dataset includes a
small number of instaces for gold standard classes.
While increasing |K| does not guarantee an in-
crease in V-measure (splitting homogeneous clus-
ters would reduce completeness without improv-
ing homogeneity), it is easier to achieve higher
scores when more clusters are produced.
Another relevant measure is the Variation of In-
formation (VI) (Meila?, 2007). Like V-measure,
it assesses homogeneity and completeness using
the quantitiesH(C|K) andH(K|C) respectively,
however it simply adds them up to obtain a final
result (higher scores are worse). It is also a metric,
i.e. VI scores can be added, subtracted, etc, since
the quantities involved are measured in bits. How-
ever, it can be observed that if |C| and |K| are very
different then the terms H(C|K) and H(K|C)
will not necessarily be in the same range. In par-
ticular, if |K|  |C| then H(K|C) (and V I) will
be low. In addition, VI scores are not normalized
and therefore their interpretation is difficult.
Both V-measure and VI have important advan-
tages over RI and F-measure: they do not assume
a mapping between classes and clusters and their
scores depend only on the relative sizes of the clus-
ters. However, V-measure and VI can be mislead-
ing if the number of clusters found (|K|) is sub-
stantially different than the number of gold stan-
dard classes (|C|). In order to ameliorate this, we
suggest to take advantage of the ? parameter in
Eq. 3 in order to balance homogeneity and com-
pleteness. More specifically, setting ? = |K|/|C|
assigns more weight to completeness than to ho-
mogeneity in case |K| > |C| since the former is
harder to achieve and the latter is easier when the
clustering solution has more clusters than the gold
standard has classes. The opposite occurs when
|K| < |C|. In case |K| = |C| the score is the
same as the original V-measure. Achieving 100%
score according to any of these measures requires
correct prediction of the number of clusters.
In this work, we evaluate our results using the
three measures described above (V-measure, VI,
V-beta). We complement this evaluation with
qualitative evaluation which assesses the poten-
tial of DPMMs to discover novel information that
might not be included in the gold standard.
4 Experiments
To perform lexical-semantic verb clustering we
used the dataset of Sun et al (2008). It contains
204 verbs belonging to 17 fine-grained classes in
Levin?s (1993) taxonomy so that each class con-
tains 12 verbs. The classes and their verbs were
selected randomly. The features for each verb are
its subcategorization frames (SCFs) and associ-
ated frequencies in corpus data, which capture the
76
DPMM Sun et al
no. of clusters 37.79 17
homogeneity 60.23% 57.57%
completeness 55.82% 60.19%
V-measure 57.94% 58.85%
V-beta 57.11% 58.85%
VI (bits) 3.5746 3.3598
Table 1: Clustering performances.
syntactic context in which the verb occurs. SCFs
were extracted from the publicly available VALEX
lexicon (Korhonen et al, 2006a). VALEX was ac-
quired automatically using a domain-independent
statistical parsing toolkit, RASP (Briscoe and Car-
roll, 2002), and a classifier which identifies verbal
SCFs. As a consequence, it includes some noise
due to standard text processing and parsing errors
and due to the subtlety of argument-adjunct dis-
tinction. In our experiments, we used the SCFs
obtained from VALEX1, parameterized for the
prepositional frame, which had the best perfor-
mance in the experiments of Sun et al (2008).
The feature sets based on verbal SCFs are very
sparse and the counts vary over a large range of
values. This can be problematic for generative
models like DPMMs, since a few dominant fea-
tures can mislead the model. To reduce the spar-
sity, we applied non-negative matrix factorization
(NMF) (Lin, 2007) which decomposes the dataset
in two dense matrices with non-negative values. It
has proven useful in a variety of tasks, e.g. infor-
mation retrieval (Xu et al, 2003) and image pro-
cessing (Lee and Seung, 1999).
We use a symmetric Dirichlet prior with param-
eters of 1 (? in Equation 2). The number of di-
mensions obtained using NMF was 35. We run
the Gibbs sampler 5 times, using 100 iterations for
burn-in and draw 20 samples from each run with
5 iterations lag between samples. Table 1 shows
the average performances. The DPMM discov-
ers 37.79 verb clusters on average with its perfor-
mance ranging between 53% and 58% depending
on the evaluation measure used. Homogeneity is
4.5% higher than completeness, which is expected
since the number of classes in the gold standard is
17. The fact that the DPMM discovers more than
twice the number of classes is reflected in the dif-
ference between the V-measure and V-beta, the lat-
ter being lower. In the same table, we show the re-
sults of Sun et al (2008), who used pairwise clus-
tering (PC) (Puzicha et al, 2000) which involves
determining the number of clusters in advance.
The performance of the DPMM is 1%-3% lower
than that of Sun et al As expected, the differ-
ence in V-measure is smaller since the DPMM
discovers a larger number of clusters, while for
VI it is larger. The slightly better performance
of PC can be attributed to two factors. First,
the (correct) number of clusters is given as in-
put to the PC algorithm and not discovered like
by the DPMM. Secondly, PC uses the similarities
between the instances to perform the clustering,
while the DPMM attempts to find the parameters
of the process that generated the data, which is a
different and typically a harder task. In addition,
the DPMM has two clear advantages which we il-
lustrate in the following sections: it can be used to
discover novel information and it can be modified
to incorporate intuitive human supervision.
5 Qualitative evaluation
The gold standard employed in this work (Sun et
al., 2008) is not fully accurate or comprehensive.
It classifies verbs according to their predominant
senses in the fairly small SemCor data. Individ-
ual classes are relatively coarse-grained in terms
of syntactic-semantic analysis1 and they capture
some of the meaning components only. In addi-
tion, the gold standard does not capture the se-
mantic relatedness of distinct classes. In fact, the
main goal of clustering is to improve such exist-
ing classifications with novel information and to
create classifications for new domains. We per-
formed qualitative analysis to investigate the ex-
tent to which the DPMM meets this goal.
We prepared the data for qualitative analysis as
follows: We represented each clustering sample
as a linking matrix between the instances of the
dataset and measured the frequency of each pair
of instances occurring in the same cluster. We
constructed a partial clustering of the instances
using only those links that occur with frequency
higher than a threshold prob link. Singleton clus-
ters were formed by considering instances that
are not linked with any other instances more fre-
quently than a threshold prob single. The lower
the prob link threshold, the larger the clusters will
be, since more instances get linked. Note that in-
cluding more links in the solution can either in-
1Many original Levin classes have been manually refined
in VerbNet.
77
crease the number of clusters when instances in-
volved were not linked otherwise, or decrease it
when linking instances that already belong to other
clusters. The higher the prob single threshold,
the more instances will end up as singletons. By
adjusting these two thresholds we can affect the
coverage of the analysis. This approach was cho-
sen because it enables to conduct qualitative analy-
sis of data relevant to most clustering samples and
irrespective of individual samples. It can also be
useful in order to use the output of the clustering
algorithm as a component in a pipeline which re-
quires a single result rather than multiple samples.
Using this method, we generated data sets for
qualitative analysis using 4 sets of values for
prob link and prob single, respectively: (99%,
1%), (95%, 5%), (90%, 10%) and (85%, 15%).
Table 1 shows the number of a) verbs, b) clusters
(2 or more instances) and c) singletons in each
resulting data set, along with the percentage and
size of the clusters which represent 1, 2, or mul-
tiple gold standard classes. As expected, higher
threshold values produce high precision clusters
for a smaller set of verbs (e.g. (99%,1%) pro-
duces 5 singletons and assigns 70 verbs to 20 clus-
ters, 55% of which represent a single gold stan-
dard class), while less extreme threshold values
yield higher recall clusters for a larger set of verbs
(e.g. (85%,15%) produces 10 singletons and as-
signs 140 verbs to 25 clusters, 20% of which con-
tain verbs from several gold standard classes).
We conducted the qualitative analysis by com-
paring the four data sets against the gold standard,
SCF distributions, and WordNet (Fellbaum, 1998)
senses for each test verb. We first analysed the
5-10 singletons in data sets and discovered that
while 3 of the verbs resist classification because
of syntactic idiosyncrasy (e.g. unite takes intransi-
tive SCFs with frequency higher than other mem-
bers of class 22.2), the majority of them (7) end
up in singletons for valid semantic reasons: taking
several frequent WordNet senses they are ?too pol-
ysemous? to be realistically clustered according to
their predominant sense (e.g. get and look).
We then examined the clusters, and discovered
that even in the data set created with the lowest
prob link threshold of 85%, almost half of the
?errors? are in fact novel semantic patterns discov-
ered by clustering. Many of these could be new
sub-classes of existing gold standard classes. For
example, looking at the 13 high accuracy clusters
which correspond to a single gold standard class
each, they only represent 9 gold standard classes
because as many as 4 classes been divided into
two clusters, suggesting that the gold standard is
too coarse-grained. Interestingly, each such sub-
division seems semantically justified (e.g. the 11.1
PUT verbs bury and immerse appear in a differ-
ent cluster than the semantically slightly different
place and situate).
In addition, the DPMM discovers semantically
similar gold standard classes. For example, in the
data set created with the prob link threshold of
99%, 6 of the clusters include members from 2
different gold standard classes. 2 occur due to
syntactic idiosyncrasy, but the majority (4) oc-
cur because of true semantic relatedness (e.g. the
clustering relates 22.2 AMALGAMATE and 36.1
CORRESPOND classes which share similar mean-
ing components). Similarly, in the data set pro-
duced by the prob link threshold of 85%, one
of the largest clusters includes 26 verbs from 5
gold standard classes. The majority of them be-
long to 3 classes which are related by the meaning
component of ?motion?: 43.1 LIGHT EMISSION,
47.3 MODES OF BEING INVOLVING MOTION, and
51.3.2 RUN verbs:
? class 22.2 AMALGAMATE: overlap
? class 36.1 CORRESPOND: banter, concur, dissent, hag-
gle
? class 43.1 LIGHT EMISSION: flare, flicker, gleam, glis-
ten, glow, shine, sparkle
? class 47.3 MODES OF BEING INVOLVING MOTION:
falter, flutter, quiver, swirl, wobble
? class 51.3.2 RUN: fly, gallop, glide, jog, march, stroll,
swim, travel, trot
Thus many of the singletons and the clusters
in the different outputs capture finer or coarser-
grained lexical-semantic differences than those
captured in the gold standard. It is encouraging
that this happens despite us focussing on a rela-
tively small set of 204 verbs and 17 classes only.
6 Constrained DPMMs
While the ability to discover novel information is
attractive in NLP, in many cases it is also desir-
able to influence the solution with respect to some
prior intuition or consideration relevant to the ap-
plication in mind. For example, while discover-
ing finer-grained classes than those included in the
gold standard is useful for some applications, oth-
ers may benefit from a coarser clustering or a clus-
tering that reveals a specific aspect of the dataset.
78
% and size of clusters containing
THR verbs clusters singletons 1 class 2 classes multiple classes
99%,1% 70 20 5 55% (3.0) 30% (2.8) 15% (4.5)
95%,5% 104 25 9 40% (3.7) 44% (2.8) 16% (6.8)
90%,10% 128 28 9 46% (3.4) 39% (2.5) 14% (11.0)
85%,15% 140 25 10 44% (3.7) 28% (3.3) 20% (13.0)
Table 2: An overview of the data sets generated for qualitative analysis
Preliminary work by Vlachos et al (2008) intro-
duced a constrained version of DPMMs that en-
ables human supervision to guide the clustering
solution when needed. We model the human su-
pervision as pairwise constraints over instances,
following Wagstaff & Cardie (2000): given a pair
of instances, they are either linked together (must-
link) or not (cannot-link). For example, charge
and run should form a must-link if the aim is
to cluster 51.3 MOTION verbs together, but they
should form a cannot-link if we are interested in
54.5 BILL verbs. In the discussion and the experi-
ments that follow, we assume that all links are con-
sistent with each other. This information can be
obtained by asking human experts to label links,
or by extracting it from extant lexical resources.
Specifying the relations between the instances re-
sults in a partial labeling of the instances. Such
labeling is likely to be re-usable, since relations
between the instances are likely to be useful for a
wider range of tasks which might not have identi-
cal labels but could still have similar relations.
In order to incorporate the constraints in the
DPMM, we modify the underlying generative pro-
cess to take them into account. In particular must-
linked instances are generated by the same com-
ponent and cannot-linked instances always by dif-
ferent ones. In terms of the CRP metaphor, cus-
tomers connected with must-links arrive at the
restaurant together and choose a table jointly, re-
specting their cannot-links with other customers.
They get seated at the same table successively one
after the other. Customers without must-links with
others choose tables avoiding their cannot-links.
In order to sample the component assignments
according to this model, we restrict the Gibbs sam-
pler to take them into account using the sampling
scheme of Fig. 2. First we identify linked-groups
of instances, taking into account transitivity2. We
then sample the component assignments only from
distributions that respect the links provided. More
2If A is linked to B and B to C, then A is linked to C.
specifically, for each instance that does not belong
to a linked-group, we restrict the sampler to choose
components that do not contain instances cannot-
linked with it. For instances in a linked-group, we
sample their assignment jointly, again taking into
account their cannot-links. This is performed by
adding each instance of the linked-group succes-
sively to the same component. In Fig. 2, Ci are the
cannot-links for instance(s) i, ` are the indices of
the instances in a linked-group, and z<i and x<i
are the assignments and the instances of a linked-
group that have been assigned to a component be-
fore instance i.
Input: data X , must-linksM, cannot-links C
linked groups = find linked groups(X ,M)
Initialize Z according toM, C
for i not in linked groups
for z = 1 to |Z|+ 1
if x?i,z ? Ci = ?
P (zi = z|z?i, xi) (Eq. 2)
else
P (zi = z|z?i, xi) = 0
Sample from P (zi)
for ` in linked groups
for z = 1 to |Z|+ 1
if x?`,z ? C` = ?
Set P (z` = z|z?`, x`) = 1
for i in `
P (z`= z|z?`, x`)? =
P (zi = z|z?`, x?`,z, z<i, x<i)
else
P (z` = z|z?`, x`) = 0
Sample from P (z`)
Figure 2: Gibbs sampler incorporating must-links
and cannot-links.
7 Experiments using constraints
To investigate the impact of pairwise constraints
on clustering by the DPMM, we conduct exper-
79
iments in which the links are sampled randomly
from the gold standard. The number of links var-
ied from 10 to 50 and the random choice was re-
peated 5 times without checking for redundancy
due to transitivity. All the other experimental set-
tings are identical to those in Section 4. Follow-
ing Wagstaff & Cardie (2000), in Table 3 we show
the impact of each link type independently (la-
beled ?must? and ?cannot? accordingly), as well
as when mixed in equal proportions (?mix?).
Adding randomly selected pairwise links is ben-
eficial. In particular, must-links improve the clus-
tering rapidly. Incorporating 50 must-links im-
proves the performance by 7-8% according to the
evaluation measures. In addition, it reduces the
average number of clusters by approximately 4.
The cannot-links are rather ineffective, which is
expected as the clustering discovered by the un-
supervised DPMM is more fine-grained than the
gold standard. For the same reason, it is more
likely that the randomly selected cannot-links are
already discovered by the DPMM and are thus re-
dundant. Wagstaff & Cardie also noted that the
impact of the two types of links tends to vary
across data sets. Nevertheless, a minor improve-
ment is observed in terms of homogeneity. The
balanced mix improves the performance, but less
rapidly than the must-links.
In order to assess how the links added help the
DPMM learn other links we use the Constrained
Rand Index (CRI), which is a modification of the
Rand Index that takes into account only the pair-
wise decisions that are not dictated by the con-
straints added (Wagstaff and Cardie, 2000; Klein
et al, 2002). We evaluate the constrained DPMM
with CRI (Table 3, bottom right graph) and our re-
sults show that the improvements obtained using
pairwise constraints are due to learning links be-
yond the ones enforced.
In a real-world setting, obtaining the mixed set
of links is equivalent to asking a human expert to
give examples of verbs that should be clustered to-
gether or not. Such information could be extracted
from a lexical resource (e.g. ontology). Alterna-
tively, the DPMM could be run without any con-
straints first and if a human expert judges the clus-
tering too coarse (or fine) then cannot-links (or
must-links) could help, since they can adapt the
clustering rapidly. When 20 randomly selected
must-links are integrated, the DPMM reaches or
exceeds the performance of PC used by Sun et
al. (2008) according to all the evaluation mea-
sures. We also argue that it is more realistic to
guide the clustering algorithm using pairwise con-
straints than by defining the number of clusters in
advance. Instead of using pairwise constraints to
affect the clustering solution, one could alter the
parameters for the Dirichlet prior G0 (Eq. 1) or
experiment with varying concentration parameter
values. However, it is difficult to predict in ad-
vance the exact effect such changes would have in
the solution discovered.
Finally, we conducted qualitative analysis of the
samples obtained constraining the DPMM with 10
randomly selected must-links. We first prepared
the data according to the method described in Sec-
tion 5, using prob link and prob single thresh-
olds of 99% and 1% respectively. This resulted in
26 clusters and one singleton for 79 verbs. Recall
that without constraining the DPMM these thresh-
olds produced 20 clusters and 5 singletons for 70
verbs. 49 verbs are shared in both outputs, while
the average cluster size is similar.
The resulting clusters are highly accurate. As
many as 16 (i.e. 62%) of them represent a sin-
gle gold standard class, 7 of which contain (only)
the pairs of must-linked verbs. Interestingly, only
11 out of 17 gold standard classes are exempli-
fied among the 16 clusters, with 5 classes sub-
divided into finer-grained classes. Each of these
sub-divisions seems semantically fully motivated
(e.g. 30.3 PEER verbs were subdivided so that
peep and peek were assigned to a different cluster
than the semantically different gaze, glance and
stare) and 4 of them can be directly attributed to
the use of must-links.
From the 6 clusters that contained members
from two different gold standard classes, the ma-
jority (5) make sense as well. 3 of these contain
members of must-link pairs together with verbs
from semantically related classes (e.g. 37.7 SAY
and 40.2 NONVERBAL EXPRESSION classes). 3 of
the clusters that contain members of several gold
standard classes include must-link pairs as well.
In two cases must-links have helped to bring to-
gether verbs which belong to the same class (e.g.
the members of the must-link pair broaden-freeze
which represent 45.4 CHANGE OF STATE class ap-
pear now in the same cluster with other class mem-
bers dampen, soften and sharpen). Thus, DP-
MMs prove useful in learning novel information
taking into account pairwise constraints. Only 4
80
 60
 61
 62
 63
 64
 65
 66
 67
 68
 0  10  20  30  40  50
Ho
m
og
en
eit
y
mix
must
cannot
 55
 56
 57
 58
 59
 60
 61
 62
 63
 0  10  20  30  40  50
Co
m
ple
te
ne
ss
mix
must
cannot
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 0  10  20  30  40  50
V-
m
ea
su
re
mix
must
cannot
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 0  10  20  30  40  50
V-
be
ta
mix
must
cannot
 2.9
 3
 3.1
 3.2
 3.3
 3.4
 3.5
 3.6
 3.7
 0  10  20  30  40  50
VI
mix
must
cannot
 90
 90.2
 90.4
 90.6
 90.8
 91
 91.2
 91.4
 91.6
 91.8
 0  10  20  30  40  50
CR
I
mix
must
cannot
Table 3: Performance of constrained DPMMs incorporating pairwise links.
(i.e. 15%) of the clusters in the output examined
are not meaningful (mostly due to the mismatch
between the syntax and semantics of verbs).
8 Related work
Previous work on unsupervised verb clustering
used algorithms that require the number of clus-
ters as input e.g. PC, Information Bottleneck (Ko-
rhonen et al, 2006b) and spectral clustering (Brew
and Schulte im Walde, 2002). In terms of apply-
ing non-parametric Bayesian approaches to NLP,
Haghighi and Klein (2007) evaluated the cluster-
ing properties of DPMMs by performing anaphora
resolution with good results.
There is a large body of work on semi-
supervised learning (SSL), but relatively little
work has been done on incorporating some form
of supervision in clustering. It is important to note
that the pairwise links used in this work consti-
tute a weak form of supervision since they cannot
be used to infer class labels which are required for
SSL. However, the opposite can be done. Wagstaff
& Cardie (2000) employed must-links and cannot-
links to constrain the COBWEB algorithm, while
Klein et al (2002) applied them to complete-link
hierarchical agglomerative clustering. The latter
also studied how the added links affect instances
not directly involved in them.
It can be argued that one could use clustering
algorithms that require the number of clusters to
be known in advance to discover interesting sub-
classes such as those discovered by the DPMMs.
However, this would normally require multiple
runs and manual inspection of the results, while
DPMMs discover them automatically. Apart from
the fact that fixing the number of clusters in ad-
vance restricts the discovery of novel information
in the data, such algorithms cannot take full ad-
vantage of the pairwise constraints, since the latter
are likely to change the number of clusters.
9 Conclusions - Future Work
In this work, following Vlachos et al (2008) we
explored the application of DPMMs to the task of
verb clustering. We modified V-measure (Rosen-
berg and Hirschberg, 2007) to deal more appro-
priately with the varying number of clusters dis-
covered by DPMMs and presented a method of
agregating the generated samples which allows for
qualitative evaluation. The quantitative and qual-
itative evaluation demonstrated that they achieve
performance comparable with that of previous
work and in addition discover novel information in
the data. Furthermore, we evaluated the incorpo-
ration of constraints to guide the DPMM obtaining
promising results and we discussed their applica-
tion in a real-world setup.
The results obtained encourage the application
of DPMMs and non-parametric Bayesian methods
to other NLP tasks. We plan to extend our ex-
periments to larger datasets and further domains.
While the improvements achieved using randomly
selected pairwise constraints were promising, an
active constraint selection scheme as in Klein et
al. (2002) could increase their impact. Finally,
an extrinsic evaluation of the clustering provided
by DPMMs in the context of an NLP application
would be informative on their practical potential.
81
Acknowledgments
We are grateful to Diarmuid O? Se?aghdha and Jur-
gen Van Gael for helpful discussions.
References
Chris Brew and Sabine Schulte im Walde. 2002. Spec-
tral Clustering for German Verbs. In Proceedings of
the 2002 Conference on Empirical Methods in Nat-
ural Language Processing, pages 117?124.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499?1504.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Sharon J. Goldwater. 2007. Nonparametric bayesian
models of lexical acquisition. Ph.D. thesis, Brown
University, Providence, RI, USA.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 848?855, Prague, Czech Republic.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Dan Klein, Sepandar Kamvar, and Chris Manning.
2002. From instance-level constraints to space-level
constraints: Making the most of prior knowledge in
data clustering. In Proceedings of the Nineteenth In-
ternational Conference on Machine Learning.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006a. A large subcategorization lexicon for natural
language processing applications. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2006b. Automatic classification of verbs in
biomedical texts. In Proceedings of the COLING-
ACL, pages 345?352.
Daniel D. Lee and Sebastian H. Seung. 1999. Learning
the parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788?791, October.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago.
Chih-Jen Lin. 2007. Projected gradient methods for
nonnegative matrix factorization. Neural Compua-
tion, 19(10):2756?2779.
Marina Meila?. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers,
and Michael D. Lee. 2006. Modeling individual dif-
ferences using dirichlet processes. Journal of Math-
ematical Psychology, 50(2):101?122, April.
Radford M. Neal. 2000. Markov Chain Sam-
pling Methods for Dirichlet Process Mixture Mod-
els. Journal of Computational and Graphical Statis-
tics, 9(2):249?265, June.
Jan Puzicha, Thomas Hofmann, and Joachim Buh-
mann. 2000. A theory of proximity based clus-
tering: Structure detection by optimization. Pattern
Recognition, 33(4):617?634.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical Association, 66(336):846?850.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP-
CoNLL, pages 410?420, Prague, Czech Republic.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the 9th International Conference
on Intelligent Text Processing and Computational
Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 95?102.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of COLING-ACL, pages 985?992, Syd-
ney, Australia.
Andreas Vlachos, Zoubin Ghahramani, and Anna Ko-
rhonen. 2008. Dirichlet process mixture models for
verb clustering. In Proceedings of the ICML work-
shop on Prior Knowledge for Text and Language.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning, pages 1103?1110, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Docu-
ment clustering based on non-negative matrix factor-
ization. In Proceedings of the 26th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in informaion retrieval, pages 267?273,
New York, NY, USA. ACM Press.
82
Proceedings of the Workshop on BioNLP: Shared Task, pages 37?40,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Biomedical Event Extraction without Training Data
Andreas Vlachos, Paula Buttery, Diarmuid O? Se?aghdha, Ted Briscoe
Computer Laboratory
University of Cambridge
Cambridge, UK
av308,pjb48,do242,ejb@cl.cam.ac.uk
Abstract
We describe our system for the BioNLP 2009
event detection task. It is designed to be as
domain-independent and unsupervised as pos-
sible. Nevertheless, the precisions achieved
for single theme event classes range from 75%
to 92%, while maintaining reasonable recall.
The overall F-scores achieved were 36.44%
and 30.80% on the development and the test
sets respectively.
1 Introduction
In this paper we describe the system built for the
BioNLP 2009 event detection and characterization
task (Task 1). The approach is based on the output
of a syntactic parser and standard linguistic process-
ing, augmented by rules acquired from the develop-
ment data. The key idea is that a trigger connected
with an appropriate argument along a path through
the syntactic dependency graph forms an event.
The goal we set for our approach was to avoid
using training data explicitly annotated for the task
and to preserve domain independence. While we
acknowledge the utility of supervision (in the form
of annotated data) and domain knowledge, we be-
lieve it is valuable to explore an unsupervised ap-
proach. Firstly, manually annotated data is ex-
pensive to create and the annotation process itself
is difficult and unavoidably results in inconsisten-
cies, even in well-explored tasks such as named en-
tity recognition (NER). Secondly, unsupervised ap-
proaches, even if they fail to reach the performance
of supervised ones, are likely to be informative in
identifying useful features for the latter. Thirdly, ex-
ploring the potential of such a system may highlight
what domain knowledge is useful and its potential
contribution to performance. Finally, preserving do-
main independence allows us to develop and evalu-
ate a system that could be used for similar tasks with
minimal adaptation.
The overall architecture of the system is as fol-
lows. Initiallly, event triggers are identified and la-
belled with event types using seed terms. Based on
the dependency output of the parser the triggers are
connected with candidate arguments using patterns
identified in the development data. Anaphoric can-
didate arguments are then resolved. Finally, the trig-
gers connected with appropriate arguments are post-
processed to generate the final set of events. Each
of these stages are described in detail in subsequent
sections, followed by experiments and discussion.
2 Trigger identification
We perform trigger identification using the assump-
tion that events are triggered in text either by verbal
or nominal prdicates (Cohen et al, 2008).
To build a dictionary of verbs and their associ-
ated event classes we use the triggers annotated in
the training data. We lemmatize and stem the trig-
gers with the morphology component of the RASP
toolkit (Briscoe et al, 2006)1 and the Porter stem-
mer2 respectively. We sort the trigger stem - event
class pairs found according to their frequency in
the training data and we keep only those pairs that
appear at least 10 times. The trigger stems are
then mapped to verbs. This excludes some rela-
tively common triggers, which will reduce recall,
but, given that we rely exclusively on the parser for
1http://www.cogs.susx.ac.uk/lab/nlp/rasp/
2http://www.tartarus.org/?martin/PorterStemmer
37
argument extraction, such triggers would be difficult
to handle. For verbs with more than one event class
we keep only the most frequent one.
We consider the assumption that each verb de-
notes a single event class to be a reasonable one
given the restricted task domain. It hinders us from
dealing with triggers denoting multiple event classes
but it simplifies the task so that we do not need anno-
tated data. While we use the training data triggers to
obtain the list of verbs and their corresponding event
types, we believe that such lists could be obtained by
clustering (Korhonen et al, 2008) with editing and
labelling by domain experts. This is the only use of
the training data we make in our system.
During testing, using the tokenized text provided,
we attempt to match each token with one of the
verbs associated with an event type. We perform
this by relaxing the matching successively, using the
token lemma, then stem, and finally allowing a par-
tial match in order to deal with particles (so that e.g.
co-transfect matches transfect). This process returns
single-token candidate triggers which, while they do
not reproduce the trigger annotation, are likely to be
adequate for event extraction. We overgenerate trig-
gers, since not all occurrences denote an event, ei-
ther because they are not connected with appropriate
arguments or because they are found in a non-event
denoting context, but we expect to filter these at the
argument extraction stage.
3 Argument extraction
Given a set of candidate triggers, we attempt to con-
nect them with appropriate arguments using the de-
pendency graph provided by a parser. In our ex-
periments we use the domain-independent unlexi-
calized RASP parser, which generates parses over
the part-of-speech (PoS) tags of the tokens generated
by an HMM-based tagger trained on balanced En-
glish text. While we expect that a parser adapted to
the biomedical domain may perform better, we want
to preserve the domain-independence of the system
and explore its potential.
The only adjustment we make is to change the
PoS tags of tokens that are part of a protein name
to proper names tags. We consider such an adjust-
ment domain-independent given that NER is avail-
able in many domains (Lewin, 2007). Following
Haghighi et al(2005), in order to ameliorate pars-
ing errors, we use the top-10 parses and return a
set of bilexical head-dependent grammatical rela-
tions (GRs) weighted according to the proportion
and probability of the top parses supporting that GR.
The GRs produced by the parser define directed
graphs between tokens in the sentence, and a partial
event is formed when a path that connects a trigger
with an appropriate argument is identified. GR paths
that are likely to generate events are selected using
the development data, which does not contradict the
goals of our approach because we do not require an-
notated training data. Development data is always
needed in order to build and test a system, and such
supervision could be provided by a human expert,
albeit not as easily as for the list of trigger verbs.
The set of GR paths identified follow:
VERB-TRIGGER ?subject? ARG
NOUN-TRIGGER ?iobj? PREP ?dobj? ARG
NOUN-TRIGGER ?modifier? ARG
TRIGGER ?modifier? PREP ?obj? ARG
TRIGGER ?passive subject? ARG
The final system uses three sets of GR paths:
one for Regulation events; one for Binding events;
and one for all other events. The difference be-
tween these sets is in the lexicalization of the link-
ing prepositions. For example, in Binding events
the linking preposition required lexicalization since
binds x to/with y denotes a correct event but not
binds x by y. Binding events also required additional
GR paths to capture constructions such as binding of
x to y. For Regulation events, the path set was fur-
ther augmented to differentiate between theme and
cause. When the lexicalized GR pattern sets yielded
no events we backed-off to the unlexicalized pattern
set, which is identical for all event types. In all GR
path sets, the trigger was unlexicalized and only re-
stricted by PoS tag.
4 Anaphora resolution
The events and arguments identified in the parsed
abstracts are post-processed in context to iden-
tify protein referents for event arguments that are
anaphoric (e.g., these proteins, its phosphorylation)
or too complex to be extracted directly from the
grammatical relations (phosphorylation of cellular
proteins , notably phospholipase C gamma 1). The
38
anaphoric linking is performed by a set of heuris-
tic rules manually designed to capture a number of
common cases observed in the development dataset.
A further phenomenon dealt with by rules is coref-
erence between events, for example in The expres-
sion of LAL-mRNA is induced. This induction is de-
pendent on. . . where the Induction event described
by the first sentence is the same as the theme of the
Regulation event in the second and should be given
the same event index. The development of the post-
processing rules favoured precision over recall, but
the low frequency of each case considered means
that some overfitting to the development data may
have been unavoidable.
5 Event post-processing
At the event post-processing stage, we form com-
plete events considering the trigger-argument pairs
produced at the argument extraction stage whose ar-
guments are resolved (possibly using anaphora res-
olution) either to a protein name or to a candidate
trigger. The latter are considered only for regula-
tion event triggers. Furthermore, regulation event
trigger-argument pairs are tagged either as theme or
cause at the argument extraction stage.
For each non-regulation trigger-argument pair, we
generate a single event with the argument marked as
theme. Given that we are dealing only with Task
1, this approach is expected to deal adequately with
all event types except Binding, which can have mul-
tiple themes. Regulation events are formed in the
following way. Given that the cause argument is
optional, we generate regulation events for trigger-
argument pairs whose argument is a protein name or
a trigger that has a formed event. Since regulation
events can have other regulation events as themes,
we repeat this process until no more events can be
formed. Occasionally, the use of multiple parses re-
sults in cycles between regulation triggers which are
resolved using the weighted GR scores. Then, we at-
tach any cause arguments that share the same trigger
with a formed regulation event.
In the analysis performed for trigger identification
in Section 2, we observed that certain verbs were
consistently annotated with two events (namely
overexpress and transfect), a non-regulation event
and a regulation event with the former event as its
theme. For candidate triggers that were recognized
due to such verbs, we treat them as non-regulation
events until the post-processing stage where we gen-
erate two events.
6 Experiments - Discussion
We expected that our approach would achieve high
precision but relatively low recall. The evaluation
of our final submissions on the development and test
data (Table 1) confirmed this to a large extent. For
the non-regulation event classes excluding Binding,
the precisions achieved range from 75% to 92% in
both development and test data, with the exception
of Transcription in the test data. Our approach ex-
tracts Binding events with a single theme, more suit-
ably evaluated by the Event Decomposition evalua-
tion mode in which a similar high precision/low re-
call trend is observed, albeit with lower scores.
Of particular interest are the event classes for
which a single trigger verb was identified, namely
Transcription, Protein catabolism and Phosphoryla-
tion, which makes it easier to identify the strengths
and weaknesses of our approach. For the Phos-
phorylation class, almost all the triggers that were
annotated in the training data can be captured us-
ing the verb phosporylate and as a result, the per-
formances achieved by our system are 70.59% and
60.63% F-score on the development and test data re-
spectively. The precision was approximately 78% in
both datasets, while recall was lower due to parser
errors and unresolved anaphoric references. For the
Protein catabolism class, degrade was identified as
the only trigger verb, resulting in similar high preci-
sion but relatively lower recall due to the higher lex-
ical variation of the triggers for this class. For the
Transcription class we considered only transcribe
as a trigger verb, but while the performance on the
development data is reasonable (55%), the perfor-
mance on the test data is substantially lower (20%).
Inspecting the event triggers in the training data re-
veals that some very common triggers for this class
either cannot be mapped to a verb (e.g., mrna) or are
commonly used as triggers for other event classes.
A notable case of the latter type is the verb express,
which, while mostly a Gene Expressions trigger, is
also annotated as Transcription more than 100 times
in the training data. Assuming that this is desirable,
39
Development Test
Event Class recall precision fscore recall precision fscore
Localization 45.28 92.31 60.76 25.86 90.00 40.18
Binding 12.50 24.41 16.53 12.68 31.88 18.14
Gene expression 52.25 80.79 63.46 45.57 75.81 56.92
Transcription 42.68 77.78 55.12 12.41 56.67 20.36
Protein catabolism 42.86 81.82 56.25 35.71 83.33 50.00
Phosphorylation 63.83 78.95 70.59 49.63 77.91 60.63
Event Total 39.03 65.97 49.05 33.16 68.15 44.61
Regulation 20.12 50.75 28.81 9.28 36.49 14.79
Positive regulation 16.86 48.83 25.06 11.39 38.49 17.58
Negative regulation 11.22 36.67 17.19 6.86 36.11 11.53
Regulation Total 16.29 47.06 24.21 9.98 37.76 15.79
Total 26.55 58.09 36.44 21.12 56.90 30.80
Binding (decomposed) 26.92 66.14 38.27 18.84 54.35 27.99
Table 1: Performance analysis on development and test data using Approximate Span/Partial Recursive Matching.
a more appropriate solution would need to take con-
text into account.
Our performance on the regulation events is sub-
stantially lower in both recall and precision. This
is expected, as they rely on the extraction of non-
regulation events. The variety of lexical triggers is
not causing the drop in performance though, since
our system performed reasonably well in the Gene
Expression and Localization classes which have
similar lexical variation. Rather it is due to the com-
bination of the lexical variation with the requirement
to make the distinction between the theme and op-
tional cause argument, which cannot be handled ap-
propriately by the small set of GR paths employed.
The contribution of anaphora resolution to our
system is limited as it relies on the argument ex-
traction stage which, apart from introducing noise,
is geared towards maintaining high precision. Over-
all, it contributes 22 additional events on the de-
velopment set, of which 14 out of 16 are correct
non-regulation events. Of the remaining 6 regula-
tion events only 2 were correct. Similar trends were
observed on the test data.
7 Conclusions - Future work
We described an almost unsupervised approach for
the BioNLP09 shared task on biomedical event ex-
traction which requires only a dictionary of verbs
and a set of argument extraction rules. Ignoring trig-
ger spans, the performance of the approach is parser-
dependent and while we used a domain-independent
parser in our experiments we also want to explore
the benefits of using an adapted one.
The main weakness of our approach is the han-
dling of events with multiple arguments and the dis-
tinctions between them, which are difficult to deal
with using simple unlexicalized rules. In our fu-
ture work we intend to explore semi-supervised ap-
proaches that allow us to acquire more complex
rules efficiently.
References
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL Interactive presentation ses-
sions, pages 77?80.
Kevin B. Cohen, Martha Palmer, and Lawrence Hunter.
2008. Nominalization and alternations in biomedical
language. PLoS ONE, 3(9).
Aria Haghighi, Kristina Toutanova, and Chris Manning.
2005. A Joint Model for Semantic Role Labeling. In
Proceedings of CoNLL-2005: Shared Task.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
2008. The choice of features for classification of verbs
in biomedical texts. In Proceedings of Coling.
Ian Lewin. 2007. BaseNPs that contain gene names:
domain specificity and genericity. In Proceedings of
the ACL workshop BioNLP: Biological, translational,
and clinical language processing, pages 163?170.
40
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1405?1410,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Dependency language models for sentence completion
Joseph Gubbins
Computer Laboratory
University of Cambridge
jsg52@cam.ac.uk
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cam.ac.uk
Abstract
Sentence completion is a challenging seman-
tic modeling task in which models must
choose the most appropriate word from a
given set to complete a sentence. Although
a variety of language models have been ap-
plied to this task in previous work, none of the
existing approaches incorporate syntactic in-
formation. In this paper we propose to tackle
this task using a pair of simple language mod-
els in which the probability of a sentence is
estimated as the probability of the lexicalisa-
tion of a given syntactic dependency tree. We
apply our approach to the Microsoft Research
Sentence Completion Challenge and show that
it improves on n-gram language models by 8.7
percentage points, achieving the highest accu-
racy reported to date apart from neural lan-
guage models that are more complex and ex-
pensive to train.
1 Introduction
The verbal reasoning sections of standardised tests
such as the Scholastic Aptitude Test (SAT) fea-
ture problems where a partially complete sentence
is given and the candidate must choose the word
or phrase from a list of options which completes
the sentence in a logically consistent way. Sen-
tence completion is a challenging semantic mod-
elling problem. Systematic approaches for solving
such problems require models that can judge the
global coherence of sentences. Such measures of
global coherence may prove to be useful in various
applications, including machine translation and nat-
ural language generation (Zweig and Burges, 2012).
Most approaches to sentence completion employ
language models which use a window of immedi-
ate context around the missing word and choose the
word that results in the completed sentence with the
highest probability (Zweig and Burges, 2012; Mnih
and Teh, 2012). However, such language models
may fail to identify sentences that are locally co-
herent but are improbable due to long-range syntac-
tic/semantic dependencies. Consider, for example,
completing the sentence
I saw a tiger which was really very ...
with either fierce or talkative. A language model
relying on up to five words of immediate context
would ignore the crucial dependency between the
missing word and the noun tiger.
In this paper we tackle sentence completion us-
ing language models based on dependency gram-
mar. These models are similar to standard n-gram
language models, but instead of using the linear or-
dering of the words in the sentence, they generate
words along paths in the dependency tree of the sen-
tence. Unlike other approaches incorporating syntax
into language models (e.g., Chelba et al, 1997), our
models are relatively easy to train and estimate, and
can exploit standard smoothing methods. We apply
them to the Microsoft Research Sentence Comple-
tion Challenge (Zweig and Burges, 2012) and show
an improvement of 8.7 points in accuracy over n-
gram models, giving the best results to date for any
method apart from the more computationally de-
manding neural language models.
1405
Figure 1: Dependency tree example
2 Unlabelled Dependency Language
Models
In dependency grammar, each word in a sentence is
associated with a node in a dependency tree (Figure
1). We define a dependency tree as a rooted, con-
nected, acyclic directed graph together with a map-
ping from the nodes of the tree to a set of gram-
matical relation labels R. We define a lexicalised
dependency tree as a dependency tree along with a
mapping from the vertices of the tree to a vocabulary
V .
We seek to model the probability distribution of
the lexicalisation of a given dependency tree. We
will use this as a language model; we neglect the
fact that a given lexicalised dependency tree can
correspond to more than one sentence due to vari-
ations in word order. Let ST be a lexicalised de-
pendency tree, where T is the unlexicalised tree and
let w1w2 . . . wm be an ordering of the words corre-
sponding to a breadth-first enumeration of the tree.
In order for this representation to be unique, when
we parse a sentence, we will use the unique breadth-
first ordering where the children of any node appear
in the same order as they did in the sentence. We
define w0 to be a special symbol denoting the root
of the tree. We denote the grammatical relation be-
tween wk and its parent by gk ? R.
We apply the chain rule to the words in the tree in
the order of this breadth-first enumeration:
P[ST |T ] =
m?
i=1
P[wi|(wk)
i?1
k=0, T ] (1)
Given a word wi, we define the ancestor sequence
A(w) to be the subsequence of (wk)
i?1
k=0 describ-
ing the path from the root node to the parent of
w, where each element of the sequence is the par-
ent of the next element. For example in Figure 1,
A(w8) = (w0, w1, w3). We make the following two
assumptions:
? that each word wi is conditionally independent
of the words outside of its ancestor sequence
(wk)
i?1
k=0?A(wi)
c, given the ancestor sequence
A(wi);
? that the words are independent of the labels
(gk)mk=1.
Using these assumptions, we can write the probabil-
ity as:
P[ST |T ] =
m?
i=1
P[wi|A(wi)] (2)
Given a training data corpus consisting of sen-
tences parsed into dependency trees, the maximum
likelihood estimator for the probability P[wi|A(wi)]
is given by the proportion of cases where the ances-
tor sequence A(wi) was followed by wi. Let C(?) be
the count of the number of observations of a pattern
in the corpus. We have
P?[wi|A(wi)] =
C((A(wi), wi))
?
w?V C((A(wi), w))
(3)
As is the case for n-gram language models, we can?t
hope to observe all possible sequences of words no
matter how big the corpus. To deal with this data
sparsity issue, we take inspiration from n-gram mod-
els and assume a Markov property of order (N ?1):
P[w|A(w)] = P[w|A(N?1)(w)] (4)
where A(N?1)(w) denotes the sequence of up to
(N ? 1) closest ancestors of w.
The maximum likelihood estimator for this prob-
ability is:
P?[wi|A(N?1)(wi)] =
C((A(N?1)(wi), wi))
?
w?V C((A
(N?1)(wi), w))
We have arrived at a model which is quite similar
to n-gram language models. The main difference
1406
is that each word in the tree can have several chil-
dren, while in the n-gram models it can only be fol-
lowed by one word. Thus the sum in the denomina-
tor above does not simplify to the count of the ances-
tor sequence in the way that it does for n-gram lan-
guage models. However, we can calculate and store
the denominators easily during training, so that we
do not need to sum over the vocabulary each time we
evaluate the estimator. We refer to this model as the
order N unlabelled dependency language model.
As is the case for n-gram language models, even
for low values of N, we will often encounter se-
quences (A(N?1)(w), w) which were not observed
in training. In order to avoid assigning zero prob-
ability to the entire sentence, we need to use a
smoothing method. We can use any of the smooth-
ing methods used for n-gram language models. For
simplicity, we use stupid backoff smoothing (Brants
et al, 2007).
3 Labelled Dependency Language Models
We assumed above that the words are generated in-
dependently from the grammatical relations. How-
ever, we are likely to ignore valuable information in
doing so. To illustrate this point, consider the fol-
lowing pair of sentences:
You ate an apple
nsubj
dobj
det
An apple ate you
det nsubj dobj
The dependency trees of the two sentences are
very similar, with only the grammatical relations be-
tween ate and its arguments differing. The unla-
belled dependency language model will assign the
same probability to both of the sentences as it ig-
nores the labels of grammatical relations. In order
to be able to distinguish between them, the nature
of the grammatical relations between the words in
the dependency tree needs to be incorporated in the
language model. We relax the assumption that the
words are independent of the labels of the parse tree,
assuming instead the each word is conditionally in-
dependent of the words and labels outside its ances-
tor path given the words and labels in its ancestor
path. We define G(wi) to be the sequence of gram-
matical relations between the successive elements of
(A(wi), wi). G(wi) is the sequence of grammatical
relations found on the path from the root node to
wi. For example, in Figure 1, G(w8) = (g1, g3, g8).
With our modified assumption we have:
P[ST |T ] =
m?
i=1
P[wi|A(wi), G(wi)] (5)
Once again we apply a Markov assumption.
Let G(N?1)(w) be the sequence of grammat-
ical relations between successive elements of
(A(N?1)(w), w). With an (N ? 1)th order Markov
assumption, we have:
P[ST |T ] =
m?
i=1
P[wi|A(N?1)(wi), G(N?1)(wi)]
The maximum likelihood estimator for the probabil-
ity is once again given by the ratio of the counts of
labelled paths. We refer to this model as the order
N labelled dependency language model.
4 Dataset and Implementation Details
We carried out experiments using the Microsoft
Research Sentence (MSR) Completion Challenge
(Zweig and Burges, 2012). This consists of a set
of 1,040 sentence completion problems taken from
five of the Sherlock Holmes novels by Arthur Co-
nan Doyle. Each problem consists of a sentence
in which one word has been removed and replaced
with a blank and a set of 5 candidate words to com-
plete the sentence. The task is to choose the can-
didate word which, when inserted into the blank,
gives the most probable complete sentence. The set
of candidates consists of the original word and 4
imposter words with similar distributional statistics.
Human judges were tasked with choosing imposter
words which would lead to grammatically correct
sentences and such that, with some thought, the cor-
rect answer should be unambiguous. The training
data set consists of 522 19th century novels from
Project Gutenberg. We parsed the training data us-
ing the Nivre arc-eager deterministic dependency
parsing algorithm (Nivre and Scholz, 2004) as im-
plemented in MaltParser (Nivre et al, 2006). We
trained order N labelled and unabelled dependency
1407
I saw a tiger which was really very
a. fierce
b. talkative
I saw a tiger which was really very fierce
ROOT
P[?fierce?] = P[saw|ROOT]? P[I|ROOT, saw]? P[tiger|ROOT, saw]? P[a|saw, tiger]? P[fierce|saw, tiger]
?P[which|tiger, fierce]? P[was|tiger, fierce]? P[really|tiger, fierce]? P[very|tiger, fierce]
PARSE
EVALUATE PROBABILITY
Figure 2: Procedure for evaluating sentence completion problems
N Unlab-SB Lab-SB Ngm-SB Ngm-KN
2 43.2% 43.0% 28.1% 27.8%
3 48.3% 49.8% 38.5% 38.4%
4 48.3% 50.0% 40.8% 41.1%
5 47.4% 49.9% 41.3% 40.8%
Table 1: Summary of results for Sentence Completion
language models for 2 ? N ? 5. Words which
occured fewer than 5 times were excluded from the
vocabulary. In order to have a baseline to compare
against, we also trained n-gram language models
with Kneser-Ney smoothing and stupid backoff us-
ing the Berkeley Language Modeling Toolkit (Pauls
and Klein, 2011).
To test a given language model, we calculated the
scores it assigned to each candidate sentence and
chose the completion with the highest score. For
the dependency language models we parsed the sen-
tence with each of the 5 possible completions and
calculated the probability in each case. Figure 2 il-
lustrates an example of this process for the order 3
unlabelled model.
5 Results
Table 1 summarises the results. Unlab-SB is the or-
der N unlabelled dependency language model with
Stupid Backoff, Lab-SB is the order N labelled
dependency language model with Stupid Backoff,
Ngm-SB is the n-gram language model with Stupid
Backoff and Ngm-KN is the interpolated Kneser-
Ney smoothed n-gram language model.
Both of the dependency language models outper-
fomed the n-gram language models by a substantial
Method Accuracy
n-grams (Various) 39% - 41%
Skip-grams (Mikolov) 48%
Unlabelled Dependency Model 48.3%
Average LSA (Zweig) 49%
Labelled Dependency Model 50.0%
Log-bilinear Neural LM (Mnih) 54.7%
Recurrent Neural LM (Mikolov) 55.4%
Table 2: Comparison against previous results
margin for all orders considered. The best result was
achieved by the order 4 labelled dependency model
which is 8.7 points in accuracy better than the best n-
gram model. Furthermore, the labelled dependency
models outperformed their unlabelled counterparts
for every order except 2.
Comparing against previous work (Table 2), the
performance of our n-gram baseline is slightly better
than the accuracy reported by other authors (Mnih
and Teh, 2012; Zweig et al, 2012) for models of this
type. The performance of the labelled dependency
language model is superior to the results reported
for any single model method, apart from those rely-
ing on neural language models (Mnih and Teh, 2012;
Mikolov et al, 2013) . However the superior perfor-
mance of neural networks comes at the cost of long
training times. The best result achieved in Zweig et
al. (2012) using a single method was 49% accuracy
with a method based on LSA. Mikolov et al (2013)
also reported accuracy of 48% for a method called
skip-grams, which uses a log-linear classifier to pre-
dict which words will appear close to each other in
sentences.
1408
6 Related Work and Discussion
The best-known language model based on depen-
dency parsing is that of Chelba et al (1997). This
model writes the probability in the familiar left-to-
right chain rule decomposition in the linear order
of the sentence, conditioning the probability of the
next word on the linear trigram context, as well as
some part of the dependency graph information re-
lating to the words on its left. The language mod-
els we propose are far simpler to train and compute.
A somewhat similar model to our unlabelled depen-
dency language model was proposed in Graham and
van Genabith (2010). However they seem to have
used different probability estimators which ignore
the fact that each node in the dependency tree can
have multiple children. Other research on syntac-
tic language modelling has focused on using phrase
structure grammars (Pauls and Klein, 2012; Char-
niak, 2001; Roark, 2001; Hall and Johnson, 2003).
The linear complexity of deterministic dependency
parsing makes dependency language models such as
ours more scalable than these approaches.
The most similar task to sentence completion is
lexical substitution (McCarthy and Navigli, 2007).
The main difference between them is that in the lat-
ter the word to be substituted provides a very im-
portant clue in choosing the right candidate, while
in sentence completion this is not available. An-
other related task is selectional preference modeling
(Se?aghdha, 2010; Ritter et al, 2010), where the aim
is to assess the plausibility of possible syntactic ar-
guments for a given word.
The dependency language models described in
this paper assign probabilities to full sentences. Lan-
guage models which require full sentences can be
used in automatic speech recognition (ASR) and ma-
chine translation (MT). The approach is to use a con-
ventional ASR or MT decoder to produce an N-best
list of the most likely candidate sentences and then
re-score these with the language model. This was
done by Chelba et al (1997) for ASR using a de-
pendency language model and by Pauls and Klein
(2011) for MT using a PSG-based syntactic lan-
guage model.
7 Conclusion
We have proposed a pair of language models which
are probabilistic models for the lexicalisation of a
given dependency tree. These models are simple
to train and evaluate and are scalable to large data
sets. We applied them to the Microsoft Research
Sentence Completion Challenge. They performed
substantially better than n-gram language models,
achieving the best result reported for any single
method except for the more expensive and complex
to train neural language models.
Acknowledgments
Andreas Vlachos is funded by the European
Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no.
270019 (SPACEBOOK project www.spacebook-
project.eu). The authors would like to thank Dr.
Stephen Clark for his helpful comments.
References
Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och,
and Jeffrey Dean. 2007. Large Language Mod-
els in Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 858?867. Association for
Computational Linguistics.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting on Association for Computational Linguis-
tics, pages 124?131. Association for Computational
Linguistics.
Ciprian Chelba, David Engle, Frederick Jelinek, Vic-
tor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry
Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-
cke, et al 1997. Structure and performance of a
dependency language model. In Proceedings of Eu-
rospeech, volume 5, pages 2775?2778.
Yvette Graham and Josef van Genabith. 2010. Deep syn-
tax language models and statistical machine transla-
tion. In Proceedings of the 4th Workshop on Syntax
and Structure in Statistical Translation, pages 118?
126. Coling 2010 Organizing Committee, August.
Keith Hall and Mark Johnson. 2003. Language mod-
eling using efficient best-first bottom-up parsing. In
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 507?512. IEEE.
1409
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Andriy Mnih and Yee W Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic language
models. In Proceedings of the 29th International Con-
ference on Machine Learning, pages 1751?1758.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
the 20th International Conference on Computational
Linguistics, page 64. Association for Computational
Linguistics.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC, volume 6, pages
2216?2219.
Adam Pauls and Dan Klein. 2011. Faster and Smaller
N-Gram Language Models. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 258?267. Association for Computational Lin-
guistics.
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers-Volume 1, pages
959?968. Association for Computational Linguistics.
Alan Ritter, Oren Etzioni, et al 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 424?434.
Association for Computational Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Diarmuid O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435?444. Association for Computa-
tional Linguistics.
Geoffrey Zweig and Chris JC Burges. 2012. A challenge
set for advancing language modeling. In Proceedings
of the NAACL-HLT 2012 Workshop: Will We Ever Re-
ally Replace the N-gram Model? On the Future of
Language Modeling for HLT, pages 29?36. Associa-
tion for Computational Linguistics.
Geoffrey Zweig, John C Platt, Christopher Meek,
Christopher JC Burges, Ainur Yessenalina, and Qiang
Liu. 2012. Computational approaches to sentence
completion. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 601?610. Association
for Computational Linguistics.
1410
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 47?52,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Parsing as Machine Translation
Jacob Andreas
Computer Laboratory
University of Cambridge
jda33@cam.ac.uk
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cam.ac.uk
Stephen Clark
Computer Laboratory
University of Cambridge
sc609@cam.ac.uk
Abstract
Semantic parsing is the problem of de-
riving a structured meaning representation
from a natural language utterance. Here
we approach it as a straightforward ma-
chine translation task, and demonstrate
that standard machine translation com-
ponents can be adapted into a semantic
parser. In experiments on the multilingual
GeoQuery corpus we find that our parser
is competitive with the state of the art,
and in some cases achieves higher accu-
racy than recently proposed purpose-built
systems. These results support the use of
machine translation methods as an infor-
mative baseline in semantic parsing evalu-
ations, and suggest that research in seman-
tic parsing could benefit from advances in
machine translation.
1 Introduction
Semantic parsing (SP) is the problem of trans-
forming a natural language (NL) utterance into
a machine-interpretable meaning representation
(MR). It is well-studied in NLP, and a wide va-
riety of methods have been proposed to tackle
it, e.g. rule-based (Popescu et al, 2003), super-
vised (Zelle, 1995), unsupervised (Goldwasser et
al., 2011), and response-based (Liang et al, 2011).
At least superficially, SP is simply a machine
translation (MT) task: we transform an NL ut-
terance in one language into a statement of an-
other (un-natural) meaning representation lan-
guage (MRL). Indeed, successful semantic parsers
often resemble MT systems in several impor-
tant respects, including the use of word align-
ment models as a starting point for rule extrac-
tion (Wong and Mooney, 2006; Kwiatkowski et
al., 2010) and the use of automata such as tree
transducers (Jones et al, 2012) to encode the re-
lationship between NL and MRL.
The key difference between the two tasks is that
in SP, the target language (the MRL) has very dif-
ferent properties to an NL. In particular, MRs must
conform strictly to a particular structure so that
they are machine-interpretable. Contrast this with
ordinary MT, where varying degrees of wrongness
are tolerated by human readers (and evaluation
metrics). To avoid producing malformed MRs, al-
most all of the existing research on SP has focused
on developing models with richer structure than
those commonly used for MT.
In this work we attempt to determine how ac-
curate a semantic parser we can build by treating
SP as a pure MT task, and describe pre- and post-
processing steps which allow structure to be pre-
served in the MT process.
Our contributions are as follows: We develop
a semantic parser using off-the-shelf MT compo-
nents, exploring phrase-based as well as hierarchi-
cal models. Experiments with four languages on
the popular GeoQuery corpus (Zelle, 1995) show
that our parser is competitve with the state-of-
the-art, in some cases achieving higher accuracy
than recently introduced purpose-built semantic
parsers. Our approach also appears to require
substantially less time to train than the two best-
performing semantic parsers. These results sup-
port the use of MT methods as an informative
baseline in SP evaluations and show that research
in SP could benefit from research advances in MT.
2 MT-based semantic parsing
The input is a corpus of NL utterances paired with
MRs. In order to learn a semantic parser using
MT we linearize the MRs, learn alignments be-
tween the MRL and the NL, extract translation
rules, and learn a language model for the MRL.
We also specify a decoding procedure that will re-
turn structured MRs for an utterance during pre-
diction.
47
states bordering Texas
state(next to(state(stateid(texas))))
? STEM & LINEARIZE
state border texa
state1 next to1 state1 stateid1 texas0
? ALIGN
state border texa
state1 next to1 state1 stateid1 texas0
? EXTRACT (PHRASE)
? state , state1 ?
? state border , state1 border1 ?
? texa , state1 stateid1 texas0 ?...
? EXTRACT (HIER)
[X] ? ?state , state1?
[X] ? ?state [X] texa ,
state1 [X] state1 stateid1 texas0?...
Figure 1: Illustration of preprocessing and rule ex-
traction.
Linearization We assume that the MRL is
variable-free (that is, the meaning representation
for each utterance is tree-shaped), noting that for-
malisms with variables, like the ?-calculus, can
be mapped onto variable-free logical forms with
combinatory logics (Curry et al, 1980).
In order to learn a semantic parser using MT
we begin by converting these MRs to a form more
similar to NL. To do so, we simply take a preorder
traversal of every functional form, and label every
function with the number of arguments it takes.
After translation, recovery of the function is easy:
if the arity of every function in the MRL is known,
then every traversal uniquely specifies its corre-
sponding tree. Using an example from GeoQuery,
given an input function of the form
answer(population(city(cityid(?seattle?, ?wa?))))
we produce a ?decorated? translation input of the
form
answer1 population1 city1 cityid2 seattle0 wa0
where each subscript indicates the symbol?s arity
(constants, including strings, are treated as zero-
argument functions). Explicit argument number
labeling serves two functions. Most importantly,
it eliminates any possible ambiguity from the tree
reconstruction which takes place during decod-
ing: given any sequence of decorated MRL to-
kens, we can always reconstruct the correspond-
ing tree structure (if one exists). Arity labeling ad-
ditionally allows functions with variable numbers
of arguments (e.g. cityid, which in some training
examples is unary) to align with different natural
language strings depending on context.
Alignment Following the linearization of the
MRs, we find alignments between the MR tokens
and the NL tokens using the IBM Model 4 (Brown
et al, 1993). Once the alignment algorithm is
run in both directions (NL to MRL, MRL to NL),
we symmetrize the resulting alignments to obtain
a consensus many-to-many alignment (Och and
Ney, 2000; Koehn et al, 2005).
Rule extraction From the many-to-many align-
ment we need to extract a translation rule ta-
ble, consisting of corresponding phrases in NL
and MRL. We consider a phrase-based transla-
tion model (Koehn et al, 2003) and a hierarchi-
cal translation model (Chiang, 2005). Rules for
the phrase-based model consist of pairs of aligned
source and target sequences, while hierarchical
rules are SCFG productions containing at most
two instances of a single nonterminal symbol.
Note that both extraction algorithms can learn
rules which a traditional tree-transducer-based ap-
proach cannot?for example the right hand side
[X] river1 all0 traverse1 [X]
corresponding to the pair of disconnected tree
fragments:
[X]

traverse

river

[X]
all
(where each X indicates a gap in the rule).
Language modeling In addition to translation
rules learned from a parallel corpus, MT systems
also rely on an n-gram language model for the tar-
get language, estimated from a (typically larger)
monolingual corpus. In the case of SP, such a
monolingual corpus is rarely available, and we in-
stead use the MRs available in the training data to
learn a language model of the MRL. This informa-
tion helps guide the decoder towards well-formed
48
structures; it encodes, for example, the preferences
of predicates of the MRL for certain arguments.
Prediction Given a new NL utterance, we need
to find the n best translations (i.e. sequences
of decorated MRL tokens) that maximize the
weighted sum of the translation score (the prob-
abilities of the translations according to the rule
translation table) and the language model score, a
process usually referred to as decoding. Standard
decoding procedures for MT produce an n-best list
of all possible translations, but here we need to
restrict ourselves to translations corresponding to
well-formed MRs. In principle this could be done
by re-writing the beam search algorithm used in
decoding to immediately discard malformed MRs;
for the experiments in this paper we simply filter
the regular n-best list until we find a well-formed
MR. This filtering can be done with time linear in
the length of the example by exploiting the argu-
ment label numbers introduced during lineariza-
tion. Finally, we insert the brackets according to
the tree structure specified by the argument num-
ber labels.
3 Experimental setup
Dataset We conduct experiments on the Geo-
Query data set. The corpus consists of a set of
880 natural-language questions about U.S. geog-
raphy in four languages (English, German, Greek
and Thai), and their representations in a variable-
free MRL that can be executed against a Prolog
database interface. Initial experimentation was
done using 10 fold cross-validation on the 600-
sentence development set and the final evaluation
on a held-out test set of 280 sentences. All seman-
tic parsers for GeoQuery we compare against also
makes use of NP lists (Jones et al, 2012), which
contain MRs for every noun phrase that appears in
the NL utterances of each language. In our exper-
iments, the NP list was included by appending all
entries as extra training sentences to the end of the
training corpus of each language with 50 times the
weight of regular training examples, to ensure that
they are learned as translation rules.
Evaluation for each utterance is performed by
executing both the predicted and the gold standard
MRs against the database and obtaining their re-
spective answers. An MR is correct if it obtains
the same answer as the gold standard MR, allow-
ing for a fair comparison between systems using
different learning paradigms. Following Jones et
al. (2012) we report accuracy, i.e. the percent-
age of NL questions with correct answers, and F1,
i.e. the harmonic mean of precision (percentage of
correct answers obtained).
Implementation In all experiments, we use the
IBM Model 4 implementation from the GIZA++
toolkit (Och and Ney, 2000) for alignment, and
the phrase-based and hierarchical models imple-
mented in the Moses toolkit (Koehn et al, 2007)
for rule extraction. The best symmetrization algo-
rithm, translation and language model weights for
each language are selected using cross-validation
on the development set. In the case of English and
German, we also found that stemming (Bird et al,
2009; Porter, 1980) was hepful in reducing data
sparsity.
4 Results
We first compare the results for the two translation
rule extraction models, phrase-based and hierar-
chical (?MT-phrase? and ?MT-hier? respectively
in Table 1). We find that the hierarchical model
performs better in all languages apart from Greek,
indicating that the long-range reorderings learned
by a hierarchical translation system are useful for
this task. These benefits are most pronounced in
the case of Thai, likely due to the the language?s
comparatively different word order.
We also present results for both models with-
out using the NP lists for training in Table 2. As
expected, the performances are almost uniformly
lower, but the parser still produces correct output
for the majority of examples.
As discussed above, one important modifica-
tion of the MT paradigm which allows us to pro-
duce structured output is the addition of structure-
checking to the beam search. It is not evident,
a priori, that this search procedure is guaran-
teed to find any well-formed outputs in reasonable
time; to test the effect of this extra requirement on
en de el th
MT-phrase 75.3 68.8 70.4 53.0
MT-phrase (-NP) 63.4 65.8 64.0 39.8
MT-hier 80.5 68.9 69.1 70.4
MT-hier (-NP) 62.5 69.9 62.9 62.1
Table 2: GeoQuery accuracies with and without
NPs. Rows with (-NP) did not use the NP list.
49
English [en] German [de] Greek [el] Thai [th]
Acc. F1 Acc. F1 Acc. F1 Acc. F1
WASP 71.1 77.7 65.7 74.9 70.7 78.6 71.4 75.0
UBL 82.1 82.1 75.0 75.0 73.6 73.7 66.4 66.4
tsVB 79.3 79.3 74.6 74.6 75.4 75.4 78.2 78.2
hybrid-tree 76.8 81.0 62.1 68.5 69.3 74.6 73.6 76.7
MT-phrase 75.3 75.8 68.8 70.8 70.4 73.0 53.0 54.4
MT-hier 80.5 81.8 68.9 71.8 69.1 72.3 70.4 70.7
Table 1: Accuracy and F1 scores for the multilingual GeoQuery test set. Results for other systems as
reported by Jones et al (2012).
the speed of SP, we investigate how many MRs
the decoder needs to generate before producing
one which is well-formed. In practice, increasing
search depth in the n-best list from 1 to 50 results
in a gain of no more than a percentage point or
two, and we conclude that our filtering method is
appropriate for the task.
We also compare the MT-based semantic
parsers to several recently published ones: WASP
(Wong and Mooney, 2006), which like the hier-
archical model described here learns a SCFG to
translate between NL and MRL; tsVB (Jones et
al., 2012), which uses variational Bayesian infer-
ence to learn weights for a tree transducer; UBL
(Kwiatkowski et al, 2010), which learns a CCG
lexicon with semantic annotations; and hybrid-
tree (Lu et al, 2008), which learns a synchronous
generative model over variable-free MRs and NL
strings.
In the results shown in Table 1 we observe that
on English GeoQuery data, the hierarchical trans-
lation model achieves scores competitive with the
state of the art, and in every language one of the
MT systems achieves accuracy at least as good as
a purpose-built semantic parser.
We conclude with an informal test of training
speeds. While differences in implementation and
factors like programming language choice make
a direct comparison of times necessarily impre-
cise, we note that the MT system takes less than
three minutes to train on the GeoQuery corpus,
while the publicly-available implementations of
tsVB and UBL require roughly twenty minutes and
five hours respectively on a 2.1 GHz CPU. So
in addition to competitive performance, the MT-
based parser also appears to be considerably more
efficient at training time than other parsers in the
literature.
5 Related Work
WASP, an early automatically-learned SP system,
was strongly influenced by MT techniques. Like
the present work, it uses GIZA++ alignments as
a starting point for the rule extraction procedure,
and algorithms reminiscent of those used in syn-
tactic MT to extract rules.
tsVB also uses a piece of standard MT ma-
chinery, specifically tree transducers, which have
been profitably employed for syntax-based ma-
chine translation (Maletti, 2010). In that work,
however, the usual MT parameter-estimation tech-
nique of simply counting the number of rule oc-
currences does not improve scores, and the au-
thors instead resort to a variational inference pro-
cedure to acquire rule weights. The present work
is also the first we are aware of which uses phrase-
based rather than tree-based machine translation
techniques to learn a semantic parser. hybrid-tree
(Lu et al, 2008) similarly describes a generative
model over derivations of MRL trees.
The remaining system discussed in this paper,
UBL (Kwiatkowski et al, 2010), leverages the fact
that the MRL does not simply encode trees, but
rather ?-calculus expressions. It employs resolu-
tion procedures specific to the ?-calculus such as
splitting and unification in order to generate rule
templates. Like other systems described, it uses
GIZA alignments for initialization. Other work
which generalizes from variable-free meaning rep-
resentations to ?-calculus expressions includes the
natural language generation procedure described
by Lu and Ng (2011).
UBL, like an MT system (and unlike most of the
other systems discussed in this section), extracts
rules at multiple levels of granularity by means of
this splitting and unification procedure. hybrid-
tree similarly benefits from the introduction of
50
multi-level rules composed from smaller rules, a
process similar to the one used for creating phrase
tables in a phrase-based MT system.
6 Discussion
Our results validate the hypothesis that it is possi-
ble to adapt an ordinary MT system into a work-
ing semantic parser. In spite of the compara-
tive simplicity of the approach, it achieves scores
comparable to (and sometimes better than) many
state-of-the-art systems. For this reason, we argue
for the use of a machine translation baseline as a
point of comparison for new methods. The results
also demonstrate the usefulness of two techniques
which are crucial for successful MT, but which are
not widely used in semantic parsing. The first is
the incorporation of a language model (or com-
parable long-distance structure-scoring model) to
assign scores to predicted parses independent of
the transformation model. The second is the
use of large, composed rules (rather than rules
which trigger on only one lexical item, or on tree
portions of limited depth (Lu et al, 2008)) in
order to ?memorize? frequently-occurring large-
scale structures.
7 Conclusions
We have presented a semantic parser which uses
techniques from machine translation to learn map-
pings from natural language to variable-free mean-
ing representations. The parser performs com-
parably to several recent purpose-built semantic
parsers on the GeoQuery dataset, while training
considerably faster than state-of-the-art systems.
Our experiments demonstrate the usefulness of
several techniques which might be broadly applied
to other semantic parsers, and provides an infor-
mative basis for future work.
Acknowledgments
Jacob Andreas is supported by a Churchill Schol-
arship. Andreas Vlachos is funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agree-
ment no. 270019 (SPACEBOOK project www.
spacebook-project.eu).
References
Steven Bird, Edward Loper, and Edward Klein.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Ann
Arbor, Michigan.
H.B. Curry, J.R. Hindley, and J.P. Seldin. 1980. To
H.B. Curry: Essays on Combinatory Logic, Lambda
Calculus, and Formalism. Academic Press.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1486?1495, Portland, Oregon.
Bevan K. Jones, Mark Johnson, and Sharon Goldwater.
2012. Semantic parsing with bayesian tree transduc-
ers. In Proceedings of the 50th Annual Meeting of
the Association of Computational Linguistics, pages
488?496, Jeju, Korea.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
48?54, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch-
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 177?180, Prague, Czech Republic.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1223?1233, Cambridge, Mas-
sachusetts.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
51
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590?599, Port-
land, Oregon.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?11, pages 1611?
1622. Association for Computational Linguistics.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke Zettle-
moyer. 2008. A generative model for parsing nat-
ural language to meaning representations. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 783?
792, Edinburgh, UK.
Andreas Maletti. 2010. Survey: Tree transducers
in machine translation. In Proceedings of the 2nd
Workshop on Non-Classical Models for Automata
and Applications, Jena, Germany.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440?447, Hong Kong,
China.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the 8th Inter-
national Conference on Intelligent User Interfaces,
pages 149?157, Santa Monica, CA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the 2006 Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 439?446, New York.
John M. Zelle. 1995. Using Inductive Logic Program-
ming to Automate the Construction of Natural Lan-
guage Parsers. Ph.D. thesis, Department of Com-
puter Sciences, The University of Texas at Austin.
52
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 1?9,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Two strong baselines for the BioNLP 2009 event extraction task
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cl.cam.ac.uk
Abstract
This paper presents two strong baselines
for the BioNLP 2009 shared task on event
extraction. First we re-implement a rule-
based approach which allows us to ex-
plore the task and the effect of domain-
adapted parsing on it. We then replace the
rule-based component with support vec-
tor machine classifiers and achieve perfor-
mance near the state-of-the-art without us-
ing any external resources. The good per-
formances achieved and the relative sim-
plicity of both approaches make them re-
producible baselines. We conclude with
suggestions for future work with respect to
the task representation.
1 Introduction
The term biomedical event extraction is used to re-
fer to tasks whose aim is the extraction of informa-
tion beyond the entity level. It commonly involves
recognizing actions and relations between one or
more entities. The recent BioNLP 2009 shared
task on event extraction (Kim et al, 2009) focused
on a number of relations of varying complexity in
which an event consisted of a trigger and one or
more arguments. It attracted 24 submissions and
provided a basis for system development. The per-
formances ranged from 16% to 52% in F-score.
In this paper we describe two strong baseline
approaches for the main task (described in Sec. 2)
with a focus on annotation costs and reproducibil-
ity. Both approaches rely on a dictionary of lem-
mas associated with event types (Sec. 3). First we
re-implement the rule-based approach of Vlachos
et al (2009) using resources provided in the shared
task. While it is unlikely to reach the perfor-
mance of approaches combining supervised ma-
chine learning, exploring its potential can high-
light what annotated data is useful and its poten-
tial contribution to performance. Also, given its
reliance on syntax, it allows us to assess the impor-
tance of syntactic parsing. Nevertheless, the per-
formance achieved (35.39% F-score) is competi-
tive with systems that used more annotated data
and/or other resources (Sec. 5).
Building on the error analysis of the rule-based
approach, we replace the rule-based component
with support vector machine (SVM) classifiers
trained on partial event annotation in the form of
trigger-argument associations (Sec. 6). The use
of a trainable classifier highlights issues concern-
ing the suitability of the annotated data as train-
ing material. Using a simple feature representa-
tion and no external resources, the performance
rises to 47.89% in F-score, which would have been
second best in the shared task (Sec. 7). The er-
ror analysis suggests that future work on event ex-
traction should look into different task representa-
tions which will allow more advanced models to
demonstrate their potential (Sec. 8). Both systems
shall become publically available.
2 Definition, datasets and resources
The BioNLP 2009 shared task focused on extrac-
tion of events involving proteins. Protein recogni-
tion was considered a given in order to focus the
research efforts on the novel aspects of the task.
Nine event types were defined in the main task,
which can be broadly classified in two classes.
Simple events, namely Gene expression, Tran-
scription, Protein catabolism, Phosphorylation,
Localization and Binding, which have proteins
as their Theme argument and Regulation events,
namely Positive regulation, Negative regulation
and (unspecified) Regulation which have an oblig-
atory Theme argument and an optional Cause ar-
gument which can be either a protein or another
event. Every event has a trigger which is a con-
tiguous textual string that can span over one or
more tokens, as well as a part of a token. Triggers
and arguments can be shared across events and
1
ID type trigger Theme Cause
E1 Neg reg suppressed E2
E2 Pos reg induced E3 gp41
E3 Gene exp production IL-10
Table 1: Shared task example annotation.
the same textual string can be a trigger for events
of different types. In an example demonstrating
the complexity of the task: ?. . . SQ 22536 sup-
pressed gp41-induced IL-10 production in mono-
cytes.? Participating systems, given the two pro-
teins (in bold), need to generate the three appro-
priately nested events of Table 1.
While event components can reside in different
sentences, we focus on events that are contained
in a single sentence. Participants were not pro-
vided with resources to develop anaphora resolu-
tion components and the anaphoric phenomena in-
volved were rather complex, as we observed in
Vlachos et al (2009). Extraction of events involv-
ing anaphoric relations inside a single sentence is
still possible but it is likely to require rather com-
plex patterns to be extracted.
The shared task involved three datasets, train-
ing, development and test, which consisted of 800,
150 and 260 abstracts respectively taken from the
GENIA event corpus. Their annotation was tai-
lored to the shared task definition. A resource
made available and used by the majority of the sys-
tems was the output of four syntactic parsers:
? Bikel?s (2004) re-implementation of Collins?
parsing model. This parser was trained on
newswire data exclusively.
? The re-ranking parser of Charniak & Johnson
adapted to the biomedical domain (McClosky
and Charniak, 2008). The in-domain, part-of-
speech (PoS) tagger was trained on the GENIA
corpus (Kim et al, 2003) and the self-training
of the re-ranking module used a part of the GE-
NIA treebank as development data.
? The C&C Combinatory Categorial Grammar
(CCG) parser adapted to the biomedical do-
main (Rimell and Clark, 2009). The PoS tag-
ger was trained on the GENIA corpus, while
1,000 sentences were annotated with lexical
categories and added to the training data of
the CCG supertagger and 600 sentences of the
BioInfer corpus (Pyysalo et al, 2007) were
used for parameter tuning.
? The GDep dependency parser trained for the
biomedical domain in the experiments of
Miyao et al (2008). This parser was trained
for the biomedical domain using the GENIA
treebank.
The native Penn TreeBank output of Bikel?s and
McClosky?s parser was converted to the Stanford
Dependency (SD) collapsed dependency format
(de Marneffe and Manning, 2008). The output of
the CCG parser was also converted to the same de-
pendency format, while the output of GDep was
provided in a different dependency format used
for the dependency parsing CoNLL 2007 shared
task. From the description above, it is clear that
the various parsers have different levels of adap-
tation to the biomedical domain. While it is diffi-
cult to assess quantitatively the actual annotation
effort involved, it is possible to make some com-
parisons. Bikel?s parser was not adapted to the
domain, therefore it would be the cheapest one to
deploy. McClosky and CCG used in-domain cor-
pora annotated with PoS tags for training, while
the latter using some additional annotation for lex-
ical categories. Furthermore, they were tuned us-
ing in-domain syntactic treebanks. Therefore, they
represent a more expensive option in terms of an-
notation cost. Finally, GDep was trained using
an in-domain treebanked corpus, thus representing
the alternative with the highest annotation cost.
3 Trigger extraction
We perform trigger identification using a dictio-
nary of lemmas associated with the event type they
indicate. The underlying assumption is that a par-
ticular lemma has the same semantic content in ev-
ery occurrence, which results in extracting all of
its occurrences as triggers of the same event type.
This is clearly an over-simplification, but the re-
stricted domain and the task definition alleviates
most of the problems caused. For each lemma in
the dictionary, we extract all its occurrences in the
text as triggers, therefore over-generating, since
not all occurrences denote a biomedical event.
This can be either because they are not connected
with appropriate arguments or because they are
used with a sense irrelevant to the task. Both is-
sues are being resolved at the argument identifi-
cation stage since superfluous triggers should not
receive arguments and not form events.
The one-sense-per-term assumption is further
challenged by the fact that occurrences of the same
2
term can denote events of different types. For ex-
ample, ?expression? is used as a trigger of four
different event types in the training data, namely
Gene expression, Transcription, Localization and
Positive regulation. While it can be argued that in
some cases this is due to annotation inconsisten-
cies, it is generally accepted that context can alter
the semantics of a token. In order to ameliorate
this problem, we define the concept of light trig-
gers in analogy analogy to light verbs. The latter
are verbs whose semantics are lost when occur-
ring in particular constructions, e.g. ?make? as in
?make mistakes?. In the shared task, some lem-
mas commonly associated with a particular event
type, when modified by a term associated with
a different event type, denote events of the type
of their modifier instead of their own. For ex-
ample, ?regulation? generally denotes Regulation
events, unless it has a modifier of a different event
type, e.g. ?positive?. In these cases, ?regulation?
becomes part of a multi-token Positive regulation
trigger (e.g. ?positive regulation?). However, if
the actual tokens are not adjacent, only ?regula-
tion? is annotated as a Positive regulation trigger,
which is due to the requirement that triggers are
contiguous textual strings. We refer to lemmas
exhibiting this behaviour as light triggers. Addi-
tionally, we observe that some lemmas triggered
events only when modified by another lemma as-
sociated with an event type. For example, ?ac-
tivity? when occurring without a modifier is not
considered a trigger of any event, however, when
modified by ?binding? then it becomes a Binding
event trigger. We refer to lemmas exhibiting this
behaviour as ultra-light triggers.1
In order to construct the dictionary of terms
with their associated event types we use the trig-
ger annotation from the training data, but we ar-
gue that such information could be obtained from
domain experts. First, we remove the triggers en-
countered only once in the data in order to avoid
processing non-indicative triggers. Then, we lem-
matize them with morpha (Minnen et al, 2001).
We remove prepositions and other stopwords from
multi-token triggers such as ?in response to? and
?have a prominent increase? in order to keep only
the terms denoting the event type. Then, using
the single-token triggers only, we associate each
lemma with its most common event type. In cases
1Kilicoglu and Bergler (2009) made similar observations
on the lemma ?activity? without formalizing them.
where a lemma consistently generates more that
one event trigger of different types (typically one
of the Simple event class and one of the Reg-
ulation class, we associate the lemma with all
the relevant event types. For example, ?overex-
press? consistently denotes Gene expression and
Positive regulation events. The last token of each
multi-token trigger becomes a light trigger. Fi-
nally, if a lemma is encountered as part of a multi-
token trigger of a different event type more of-
ten than with the event type associated with it as
a single-token trigger, then it becomes an ultra-
light trigger. We avoid stemming because suffixes
distinguish lemmas in an important way with re-
spect to the task. For example, ?activation? de-
notes Positive regulation events, while ?activity?
is an ultra-light trigger. We only keep lemmas as-
sociated at least four times with a particular event
type, since below that threshold the annotation was
rather inconsistent.
During testing, we attempt to match each token
with one of the lemmas associated with an event
type. We perform this by relaxing the matching
successively, using the token lemma first and if no
match is found allowing a partial match in order
to deal with particles (e.g. so that ?co-express?
matches ?express?). This process returns single-
token triggers, some of which are processed fur-
ther in case they are light or ultra-light using syn-
tactic dependencies in the following stage.
4 Rule-based argument identification
In this stage, we connect the triggers extracted
with appropriate arguments using rules defined
with the Stanford dependency (SD) scheme (de
Marneffe and Manning, 2008). We re-implement
the set of rules of Vlachos et al (2009) using the
syntactic parsing resources provided by the orga-
nizers for the development data. Rule-based sys-
tems need annotated data for tuning, but unlike
their supervised machine learning-based counter-
parts they do not learn parameters from it, thus re-
quiring less annotated data. We consider this to
be the main advantage of rule-based systems and
to demonstrate this point we explicitly avoid using
the training data provided. The rules define syn-
tactic dependency paths that connect tokens con-
taining triggers (trigger-tokens) with tokens con-
taining their arguments (arg-tokens). For multi-
token protein names, it is sufficient that a path
reaches any of its tokens. For Regulation event
3
class triggers we consider as arg-tokens not only
tokens containing (parts of) protein names but also
the trigger-tokens found in the same sentence. The
rules defined are the following:
? If a trigger-token is the governor of an arg-
token in subject relation (subj), then the latter
is identified as the Theme argument of the for-
mer, e.g. ?Stat1 expresses?. The only excep-
tion to this rule is that when the trigger denotes
Regulation class events and the nominal sub-
ject relation (nsubj) is observed, the arg-token
is identified as a Cause argument, e.g. ?gp41
induces?.
? If a trigger-token is the governor of an arg-
token in a prepositional relation, then the lat-
ter is identified as the Theme argument of the
former, e.g. ?expression of Stat1?.
? If a trigger-token is the governor of an arg-
token in modifier relation then the latter is
identified as the Theme argument of the for-
mer, e.g. ?Stat1 expression?. We restrict
the definition of the modifier relation to sub-
sume only the following relations: adjectival
modifier (amod), infinitival modifier (infmod),
participial modifier (partmod), adverbial mod-
ifier (advmod), relative clause modifier (rc-
mod), quantifier modifier (quantmod), tempo-
ral modifier (tmod) and noun compound mod-
ifier (nn) relations. This restriction is placed in
order to avoid matches irrelevant to the task.
? If a trigger-token is the governor of an arg-
token in object relation (obj) then the latter is
identified as the Theme argument, e.g. ?SQ
22536 suppressed gp41?.
? If a Regulation event class trigger and a pro-
tein name are found in the same token, then
the protein name is identified as the Cause ar-
gument, e.g. ?gp41-induced?.
A pre-processing step taken was to propagate
modifier and prepositional relations over tokens
that were co-ordinated or in an appositive relation.
This was necessary since the SD output provided
by the organizers is in the collapsed format, which
treats co-ordinated tokens asymmetrically without
propagating their dependencies.2
For each Simple or Binding trigger-argument
pair, we generate a single event with the argu-
2The organizers re-generated the dependencies in the
propagation format but we avoid using them in order to be
able to compare against the shared task participants.
ment marked as Theme. This approach is expected
to deal adequately with all event types except for
Binding, which can have multiple themes. We
generate Regulation events for trigger-argument
pairs whose argument is a protein name or a trig-
ger that has an already formed event. Since Reg-
ulation events can have other Regulation events as
Themes or Causes, we repeat this process until no
more events can be formed. Finally, at this stage
we generate the required Regulation class event
for triggers that consistently denote two events.
5 Rule-based system results
We report our results using the approximate span
matching/approximate recursive matching variant
of the evaluation. This variant allows for an event
to be considered extracted correctly if its trigger
is extracted with span within an one-token exten-
sion of the correct trigger span. Also in the case of
nested events, events below the top-level need only
their Theme argument to be correctly identified so
that the top-level event is considered correct. This
evaluation variant was used as the primary perfor-
mance criterion in the shared task.
We first compared the performances obtained
using the output of the different parsers pro-
vided by the organizers on the development data.
The best F-score was achieved using McClosky
(39.66%), followed by CCG (38.73%) and Bikel
(36.97%). As expected, the overall performance
correlates roughly with the adaptation cost in-
volved in the development of these parsers as de-
scribed in Section 2. Bikel, which is essentially
unadapted, has the worst performance overall, but
it would have been the cheapest to deploy. While
this can be viewed as a task-based parser compar-
ison, similar to the experiments of Miyao et al
(2008), one should be careful with the interpreta-
tion of the results. As pointed out by the authors,
this type of evaluation cannot substitute a parsing
evaluation against an appropriately annotated cor-
pus since in the context of a given task only some
aspects of parsing are likely to be relevant. Fur-
thermore, in our experiments we are are not us-
ing the native output of the parsers but its conver-
sion to the SD format. Therefore unavoidably we
evaluate the conversion as well as the parsing. For
this reason we avoided using the output of GDep
which was not provided in this format.
Examining the lists of false positives and false
negatives on the system using the McClosky
4
parser, we observe that the most common triggers
of events not extracted correctly had lemmas that
were included in the dictionary, such as ?binding?,
?expression?, ?induction? and ?activation?. This
suggests that most event extraction errors are due
to argument identification and that using a dictio-
nary for trigger extraction is sufficient, despite the
rather strong assumptions it is based upon. Dis-
abling the processing of light and ultra-light trig-
gers, the performance on the development data
drops to 39.28%, the main reason being the de-
creased recall in Binding events.
Based on the comparison performed on the de-
velopment data, we run our system using the Mc-
Closky parser on the test data (Table 2). The over-
all performance achieved (35.39%) is relatively
close to the one obtained on the development set
(4% lower). This is important since rule-based
systems are prone to overfitting their development
data due to the way they are built. Compared to the
performances achieved by the shared task partici-
pants, the system presented would be ranked sev-
enth in overall performance. We believe this is a
strong result, since it surpasses systems that used
supervised machine learning methods taking ad-
vantage of the development and the training data.
Restricting the comparison to rule-based systems,
it would have the second best performance out
of nine such systems, most of which used exter-
nal knowledge sources in order to improve their
performance. The best rule-based system (Kil-
icoglu and Bergler, 2009) had overall performance
of 44.62% in F-score, ranking third overall. The
main difference is that it used a much larger set
of lexicalized rules (27) which were extracted us-
ing the training data. Also, heuristics were em-
ployed in order to correct syntactic parsing errors
(Schuman and Bergler, 2006). While the benefits
from these additional processing steps are indis-
putable, they involved a lot of manual work, both
for rule construction as well as for the annotation
of the data used to extract the rules. We argue
that these performance benefits could be obtained
using machine learning methods aimed at ame-
liorating the argument identification stage. Com-
pared to the rule-based approach of Vlachos et
al. (2009), the performance is improved substan-
tially. The main difference between that system
and the one presented here is that the former uses
the domain-independent RASP parser (Briscoe et
al., 2006). While its performance was reasonable
(it was ranked 10th overall, 30.80% F-score), these
results lag behind those reported here. Note that
a direct comparison using the output of RASP is
not possible since the latter uses its own syntactic
dependency scheme and there is no lossless con-
version to the SD scheme.
Overall, the results of this section demonstrate
that the use of domain-adapted parsing is bene-
ficial to event extraction. This is not surprising
since the system presented depends heavily on the
parsing output. We argue that the annotation cost
of this adaptation is a good investment because,
unlike the task-specific training data, improved
syntactic parsing is likely to be useful for other
event extraction tasks, or even other IE tasks, e.g.
anaphora resolution. Therefore, we suggest that
domain-adaptation of syntactic parsing should be
considered first, especially in tasks that are heavily
dependent on it.
6 Improving argument identification
with partial annotation and support
vector machines
In this section, we present an approach to argu-
ment identification which attempts to overcome
the drawbacks of the rule-based approach. Fol-
lowing the trigger extraction stage, for each trigger
combined with each of its candidate arguments we
create a classification instance. The classification
task is to assign the correct argument type to the
instance. Therefore, we construct a binary clas-
sifier which determines whether a protein name
is the Theme argument of a Simple or a Binding
trigger (ThemePositive or ThemeNegative) and a
ternary classifier which determines whether a pro-
tein name or another trigger (and as consequence
its associated events) is the Theme or the Cause
argument of a Regulation trigger (RegThemePosi-
tive, RegCausePositive, RegNegative).
In order to acquire labeled instances for train-
ing, we decompose the gold standard (GS) events
into multiple events with single arguments. In
cases of events being arguments to Regulation
events, the former are replaced by their triggers.
We match the triggers extracted with those in-
cluded in the gold standard, ignoring the event
type annotation. Since we identify single-token
triggers, we replicate the approximate span match-
ing used in evaluation in order to achieve better
coverage. If the instance being considered has a
Simple or a Binding trigger, and if the pair is in-
5
Rules (MC) SVM (MC+CCG)
Event Type/Class recall precision F-score recall precision F-score
Gene expression 46.54 78.50 58.43 61.63 82.26 70.47
Transcription 26.28 28.57 27.38 29.93 62.12 40.39
Protein catabolism 28.57 100.00 44.44 42.86 85.71 57.14
Phosphorylation 65.19 82.24 72.73 78.52 91.38 84.46
Localization 32.18 88.89 47.26 40.80 95.95 57.26
Simple (total) 43.99 71.43 54.45 56.60 83.21 67.37
Binding 20.46 38.17 26.64 29.11 45.29 35.44
Regulation 15.81 23.47 18.89 23.71 39.20 29.55
Positive 21.16 33.02 25.79 37.03 43.65 40.07
Negative 17.15 29.41 21.67 30.34 40.35 34.64
Regulation (total) 19.30 30.47 23.63 33.15 42.32 37.18
Total 28.60 46.40 35.39 41.42 56.76 47.89
Table 2: Performance of the rule-based and the SVM-based systems on the test data. Each horizontal
corresponds to an event type or class. Binding events are not included in the Simple class aggregate
performance because they can have multiple Themes.
cluded in the GS then it is labeled as ThemePos-
itive, else it labeled as ThemeNegative. If the in-
stance being considered has a Regulation trigger
that has been matched with a GS trigger, and if
its argument is a protein name and their pair is in-
cluded in the GS then it is labeled according to
the latter (Reg{Theme/Cause}Positive), else, if not
found in the GS it is labeled as RegThemeNega-
tive. The same process is followed if the argument
is an event trigger which has been matched with
a GS trigger. We consider only Regulation trig-
gers that are matched in the GS in order to avoid
valid RegCausePositive instances being labeled as
RegNegative. Recall that the Cause argument is
optional, while the Theme is obligatory for Reg-
ulation events. This means that if an appropriate
Theme argument is not present, then it is possible
that a Cause argument that is present is not anno-
tated. Similarly, when considering event triggers
as arguments, we acquire labels only for instances
involving triggers that were annotated in the GS.
Since triggers without an appropriate Theme are
not annotated in the gold standard, it is possible
that a valid RegThemePositive or RegCausePosi-
tive is labeled as RegNegative instance not because
of the actual relation between the trigger and the
argument but because the argument did not have
an appropriate Theme present. In the example
mentioned in Sec. 2, if ?IL-10? was replaced by
?protein? then none of the events would be an-
notated. We argue that a human annotator would
produce these annotations implicitly, and that this
partial (with respect to the task definition) annota-
tion scheme allows the encoding of this informa-
tion in a more flexible way. Also, this is likely to
be a more efficient way to use the annotation time,
since annotators would be requested to annotate
pre-determined trigger-argument pairs instead of
searching for events from scratch, given only the
protein name annotation.
For training data generation we consider the
triggers extracted using the dictionary instead of
those in the GS. This process is certain to intro-
duce some noise as some triggers might be omit-
ted due to limited dictionary coverage. If the event
type determined by the dictionary is incorrect, this
is unlikely to affect the argument identification
process, since the latter is dependent on the lemma
of the trigger rather than its type. For example, the
Theme argument of the trigger ?expression? is un-
likely to depend on whether the event denoted is
Gene expression or Transcription.
The labeled instance acquisition process de-
scribed results in 9,699 binary and 10,541 ternary
labels compared to 6,607 triggers and 9,597 events
annotated in the training data provided. However,
it must be pointed out that in the shared task an-
notation scheme negative instances are annotated
implicitly, i.e. non-events are not annotated. If we
consider only the positive instances, then the anno-
tation scheme describeed results in 3,517 Theme-
Positive and 3,933 Reg{Theme/Cause}Positive in-
stances, which are simpler since they do not need
require textual span and event type specification.
6
For feature extraction, we find the shortest de-
pendency path connecting each trigger-argument
pair using Dijkstra?s algorithm. We allow paths
to follow either dependency direction by incor-
porating the direction in the dependency labels.
Apart from the dependency path, we extract as
features the trigger-token, the trigger event type
and the argument type (event type if the argument
is a trigger or Entity in case of protein names).
We filtered the training set considering only in-
stances in which the trigger was at a maximum dis-
tance of 4 dependencies away from the argument,
since longer paths were too sparse to be useful
in classifying unseen instances. At classification
time, we consider as {Theme/Reg}Negative any
instances in which the dependency path has not
been encountered in the training data, as well as
instances without a dependency path connecting
trigger and argument. This is necessary in order to
avoid instances being classified only on the basis
of the trigger-token and the argument type. Af-
ter the classifier has assigned labels to the trigger-
argument pairs, we construct events as described
in Sec. 4. In cases where it is unclear (to the classi-
fier) which is the trigger and which is the argument
in a given pair of Regulation event triggers the pro-
cess can result in cyclic dependencies. We resolve
them using the confidence of the classifier for each
decision by removing the least confident RegThe-
mePositive or RegCausePositive assignment.
7 SVM-based system results
In our experiments we used the LIBSVM toolkit
(Chang and Lin, 2001) which provides an imple-
mentation of Support Vector Machines with vari-
ous kernels and uses the one-against-one scheme
for multiclass problems. In all experiments, the
Gaussian kernel was used in order to capture po-
tential non-linear feature combinations, e.g. cases
where the combination of dependency path and
trigger-token would result in a different decision
rather than each of them independently. Prelimi-
nary experiments with the linear kernel confirmed
this expectation.
We focused on using the output of the two
domain-adapted parsers, namely CCG and Mc-
Closky. The reason for this is that, as argued in
Sec. 5, given the importance of syntactic parsing
to event extraction one should consider domain
adaptation of syntactic parsing before developing
task-specific training resources. We first compared
the performances obtained using the output of the
different parsers provided by the organizers us-
ing the development data. The main observation
is that, using either parser, the results are much
improved compared to those reported in Sec. 5,
by approximately eight percentage points in F-
score in either case (46.49% and 47.40% F-score
for CCG and McClosky respectively). Most of
the improvement is due to higher recall, suggest-
ing that the argument identification component is
able to learn patterns that are relevant to the task.
Overall, using the output of CCG results in higher
precision, while McClosky results in higher re-
call. These parsers have different theoretical foun-
dations, therefore they are expected to make dif-
ferent errors. In an effort to take advantage of
both parsers simultaneously, we combined them
by adding for each trigger-argument pair the de-
pendency paths extracted by both parsers. This
improved performance further to 49.35% F-score.
We then run the system combining the two
parsers on the test data, obtaining the results pre-
sented in Table 2. Overall, the system presented
would have the second best performance in the
shared task achieving 41.42%/56.76%/47.89% in
Recall/Precision/F-score. The top system (Bjorne
et al, 2009) achieved 46.73%/58.48%/51.95%
(R/P/F). It followed a machine learning approach
to trigger extraction which, while it is likely to
be responsible for the performance difference ob-
served when compared to the other participating
systems, requires explicit trigger annotation, thus
being more expensive. Furthermore, we argue that
the data provided by the organizers are not suit-
able to train a trigger extractor, since only triggers
participating in events are annotated, and semanti-
cally valid triggers without appropriate arguments
present are ignored. We hypothesize that this is
the reason the authors had to adjust the decisions
of their SVM classifiers.
The second best system (Buyko et al, 2009)
achieved 45.82%/47.52%/46.66% (R/P/F) using
many external knowledge sources such as the
Gene Ontology Annotation database, the Uni-
versal Protein Resourceand the Medical Subject
Headings thesaurus. While the use of these re-
sources and their successful usage is commend-
able, we believe it is important that the system
presented achieves comparable performance using
fewer resources.
Furthermore, joint inference models such as
7
Markov Logic Networks were applied to the
BioNLP 2009 event extraction shared task by
Riedel et al (2009) and were ranked fourth.
This result was improved upon recently by Poon
and Vanderwende (2010) who achieved 50% F-
score, 2.11 percentage points better than the re-
sult achieved in this work. Such models have great
potential for event extraction and we believe that
they can benefit from the insights presented here.
Finally, despite the fact that we used the same ex-
perimental setup as the shared task participants,
we do not consider our results are directly com-
parable to theirs since we did not work under the
same time constraints and we profited from their
experiences.
8 Discussion
Our error analysis on the output of the best system
on the development data discouraged us from pur-
suing further improvements. Echoing the observa-
tions of Buyko et al (2009), we found that anno-
tation inconsistency was affecting our results sig-
nificantly. In many cases the event triggers anno-
tated in the development data were rather mislead-
ing, e.g. ?negative? as a Gene expression event
trigger (abstract 8622883), ?increase the stabil-
ity? as a Positive regulation event trigger (abstract
8626752), ?disappearance? as a Binding event
trigger (abstract 10455128). Finally, some events
were ignored by the annotation, such as ?regula-
tion of thymidine kinase? (abstract 8622883).
An additional complication is that events that
are annotated due to anaphoric linking can have
a disproportionate effect on the scores. In an ex-
ample from abstract 9794375: ?CD3, CD2, and
CD28 are functionally distinct receptors on T lym-
phocytes. Engagement of any of these recep-
tors induces the rapid tyrosine phosphorylation
of a shared group of intracellular signaling pro-
teins, including Vav, Cbl, p85 phosphoinositide 3-
kinase, and the Src family kinases Lck and Fyn.?
Failing to recognize the anaphoric Binding events
involving proteins ?CD2? and ?CD28?, an other-
wise perfect system would receive two false nega-
tives for the Binding events, eight false negatives
for the missing Positive regulation events due to
the missing Causes and four false positives for the
incomplete Positive regulation events extracted.
Despite this criticism, we believe that the
BioNLP 2009 shared task on event extraction was
a big step forward for biomedical information ex-
traction and we are grateful to the organizers for
the effort and resources provided, without which
the research presented here would not have been
possible. The performances achieved in the main
Task1 ranged from 16% to 52% in F-score, sug-
gesting improvements in task definition, data an-
notation and participating systems compared to
previous community-wide efforts. Indicatively,
in the protein-protein interaction pair subtask of
BioCreative II (Krallinger et al, 2008) the anno-
tated datasets provided were produced by extract-
ing curation information from relevant databases.
This meant that there was no text-bound annota-
tion, thus making the application and evaluation
of existing NLP techniques difficult, resulting in
rather low performances. The best performance
achieved was 29% in F-score, while many of the
teams scored below 10%.
However, we believe that future work should
look at improving the annotation in order to be
able to assess the progress in the systems devel-
oped. In particular, we argue that we should move
towards a dependency-based representation, simi-
lar to the one introduced by Surdeanu et al (2008)
for joint syntactic parsing and semantic role label-
ing. Such representation can express the nested
nature of the events and evaluate the dependencies
between them directly. Furthermore, given the im-
portance of syntactic parsing via syntactic depen-
dencies to event extraction, it would be interesting
to see how performing these tasks jointly would
help improve the performance. A dependency-
based representation would also allow for non-
contiguous event components, as well as more
complex phenomena such as the light triggers dis-
cussed earlier.
9 Conclusions
In this paper we focused on the BioNLP 2009
shared task on event extraction. We developed
two systems, a rule-based one that does not re-
quire training data and a SVM-based one which
achieves near state-of-the-art performance. The
good performances achieved and their reliance on
shared task resources exclusively makes them re-
producible and strong baselines for future work.
Furthermore, we demonstrated the importance of
domain adaptation of syntactic parsing for event
extraction. Finally, based on our error analysis we
suggest future directions for event extraction with
respect to the task representation.
8
Acknowledgements
The author would like to thank Ted Briscoe, Mark
Craven and the three anonymous reviewers for
their feedback.
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP?09
Shared Task on Event Extraction, pages 10?18.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL Interactive presenta-
tion sessions, pages 77?80, Morristown, NJ, USA.
Association for Computational Linguistics.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed
dependency graphs. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 19?27.
Chih-Chung Chang and Chih-Jen Lin,
2001. LIBSVM: a library for support
vector machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In CrossParser ?08: Coling 2008: Pro-
ceedings of the Workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pages 1?8.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic
dependency based heuristics for biological event ex-
traction. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages
119?127.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. GENIA corpus - a semanti-
cally annotated corpus for bio-textmining. In Bioin-
formatics, volume 19, Suppl. 1, pages 180?182.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 1?9.
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, and Alfonso Valencia. 2008. Overview of
the protein-protein interaction annotation extraction
task of BioCreative II. Genome Biology, 9(Suppl
2):S4.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the 46th Annual Meeting of the Association of Com-
putational Linguistics: Human Language Technolo-
gies, pages 101?104.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of the 46th Annual Meeting of
the Association of Computational Linguistics: Hu-
man Language Technologies, pages 46?54.
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical
literature. In Proceedings of the North American
Chapter of the Association for Computational Lin-
guistics - Human Language Technologies 2010 con-
ference.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjorne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. BioInfer: a corpus for information
extraction in the biomedical domain. BMC Bioin-
formatics, 8(1):50+.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A Markov Logic Ap-
proach to Bio-Molecular Event Extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 41?49.
Laura Rimell and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852?865.
Jonathan Schuman and Sabine Bergler. 2006. Post-
nominal prepositional phrase attachment in pro-
teomics. In Proceedings of the Workshop on Linking
Natural Language Processing and Biology, pages
82?89.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 159?177.
Andreas Vlachos, Paula Buttery, Diarmuid
O? Se?aghdha, and Ted Briscoe. 2009. Biomedical
event extraction without training data. In Proceed-
ings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 37?40.
9
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 57?61,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Active Learning for Constrained Dirichlet Process Mixture Models
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cl.cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
ejb@cl.cam.ac.uk
Abstract
Recent work applied Dirichlet Process
Mixture Models to the task of verb cluster-
ing, incorporating supervision in the form
of must-links and cannot-links constraints
between instances. In this work, we intro-
duce an active learning approach for con-
straint selection employing uncertainty-
based sampling. We achieve substantial
improvements over random selection on
two datasets.
1 Introduction
Bayesian non-parametric mixture models have the
attractive property that the number of components
used to model the data is not fixed in advance but
is determined by the model and the data. This
property is particularly interesting for NLP where
many tasks are aimed at discovering novel in-
formation. Recent work has applied such mod-
els to various tasks with promising results, e.g.
Teh (2006) and Cohn et al (2009).
Vlachos et al (2009) applied the basic model
of this class, the Dirichlet Process Mixture Model
(DPMM), to lexical-semantic verb clustering with
encouraging results. The task involves discov-
ering classes of verbs similar in terms of their
syntactic-semantic properties (e.g. MOTION class
for travel, walk, run, etc.). Such classes can pro-
vide important support for other tasks, such as
word sense disambiguation, parsing and seman-
tic role labeling. (Dang, 2004; Swier and Steven-
son, 2004) Although some fixed classifications are
available these are not comprehensive and are in-
adequate for specific domains.
Furthermore, Vlachos et al (2009) used a con-
strained version of the DPMM in order to guide
clustering towards some prior intuition or consid-
erations relevant to the specific task at hand. This
supervision was modelled as pairwise constraints
between instances and it informs the model of re-
lations between them that cannot be recovered by
the model on the basis of the feature representa-
tion used. Like other forms of supervision, these
constraints require manual annotation and it is im-
portant to maximize the benefits obtained from it.
Therefore it is natural to consider active learning
(Settles, 2009) in order to focus the supervision on
clusterings on which the model is uncertain.
In this work, we propose a simple yet effec-
tive active learning method employing uncertainty
based sampling. The effectiveness of the AL
method is demonstrated on two datasets, one of
which has multiple gold standards.
2 Constrained DPMMs for clustering
In DPMMs, the parameters of each component are
generated by a Dirichlet Process (DP) which can
be seen as a distribution over distributions. Each
instance, represented by its features, is generated
by the component it is assigned to. The compo-
nents discovered correspond to the clusters. The
prior probability of assigning an instance to a par-
ticular component is proportionate to the number
of instances already assigned to it, in other words,
the DPMM exhibits the ?rich get richer? prop-
erty. A popular metaphor to describe the DPMM
which exhibits an equivalent clustering property
is the Chinese Restaurant Process (CRP). Cus-
tomers (instances) arrive at a Chinese restaurant
which has an infinite number of tables (compo-
nents). Each customer sits at one of the tables that
is either occupied or vacant with popular tables at-
tracting more customers.
Following Navarro et al (2006), parameter es-
timation is performed using Gibbs sampling by
sampling the assignment zi of each instance xi
given all the others z?i and the data X:
P (zi = z|z?i, X) ?
p(zi = z|z?i)P (xi|zi = z,X?i) (1)
57
In Eq. 1 p(zi = z|z?i) is the CRP prior and
P (xi|zi = z,X?i) is the distribution that gener-
ates instance xi given it has been assigned to com-
ponent z. This sampling scheme is possible be-
cause the assignments in the model are exchange-
able, i.e. their order is not relevant.
The constrained version of the DPMM uses
pairwise constraints over instances in order to
adapt the clustering discovered. Following
Wagstaff & Cardie (2000), a pair of instances is
either linked together (must-link) or not (cannot-
link). For example, charge and run should form a
must-link if the aim is to cluster MOTION verbs
together, but they should form a cannot-link if we
are interested in BILL verbs. All links are as-
sumed to be consistent with each other. In order
to incorporate the constraints in the DPMM, the
Gibbs sampling scheme is modified so that must-
linked instances are generated by the same compo-
nent and cannot-linked instances always by differ-
ent ones. Following Vlachos et al (2009), for each
instance that does not belong to a linked-group, the
sampler is restricted to choose components that do
not contain instances cannot-linked with it. For
instances in a linked-group, their assignment is
sampled jointly, again taking into account their
cannot-links. This is performed by adding each
instance of the linked-group successively to the
same component. In terms of the CRP metaphor,
customers connected with must-links arrive at the
restaurant and choose a table jointly, respecting
their cannot-links with other customers.
3 Active Constraint Selection
In active learning, the model selects the supervi-
sion to be provided by a human expert. In the con-
text of the DPMMs, the model chooses a pair of
instances for which a must-link or a cannot-link
must be provided. To select the pair, we employ
the simple but effective idea of uncertainty based
sampling. We consider the most informative link
as that on which the model is most uncertain, more
formally the link between instances l?ij that maxi-
mizes the following entropy:
l?ij = argmaxi,j
H(zi = zj) (2)
If we consider clustering as binary classification of
links into must-links and cannot-links, it is equiv-
alent to selecting the pair with the highest label
entropy. During the sampling process used for
parameter inference, component assignments vary
between samples and the components themselves
are not identifiable, i.e. one cannot match the com-
ponents of one sample with those of another. Fur-
thermore, the conditional assignments estimated
during Gibbs sampling (Eq. 1) they do not capture
the uncertainty of the assignments z?i on which
they condition. Therefore, we resort to generating
a set of samples from the (possibly constrained)
DPMM and pick the link on which these sam-
ples maximally disagree, i.e. we approximate the
distribution in Eq. 2 with the probability that in-
stances i, j are in the same cluster or not. Thus,
in a given set of samples the most uncertain link
would be the one between two instances which are
in the same cluster in exactly half of these sam-
ples. Using multiple samples allows us to take into
account the uncertainty in the assignments of the
other instances, as well as the varying number of
components.
Compared to standard pool-based AL, when
clustering with constraints the possible links be-
tween two instances (ignoring transitivity) are
C(N, 2) = N(N ? 1)/2 (N is the size of the
dataset) and there is an equal number of candi-
date queries to be considered, as opposed to N
queries in a supervised classification task. Another
interesting difference is that the the AL process
can be initiated without any supervision, since the
DPMM is unsupervised. On the other hand, in
the standard AL scenario a (usually small) labelled
seed set is used. Therefore, we rely exclusively on
the model and the features to guide the constraint
selection process. If the model combined with the
features is not appropriate for the task then the
constraints chosen are unlikely to be useful.
4 Datasets and Evaluation
In our experiments we used two verb clustering
datasets, one from general English (Sun et al,
2008) and one from the biomedical domain (Ko-
rhonen et al, 2006). In both datasets the fea-
tures for each verb are its subcategorization frames
(SCFs) which capture the syntactic context in
which it occurs. They were acquired automati-
cally using a domain-independent statistical pars-
ing toolkit, RASP (Briscoe and Carroll, 2002), and
a classifier which identifies verbal SCFs. As a
consequence, they include some noise due to stan-
dard text processing and parsing errors and due to
the subtlety of the argument-adjunct distinction.
The general English dataset contains 204 verbs
58
belonging to 17 fine-grained classes in Levin?s
(Levin, 1993) taxonomy so that each class con-
tains 12 verbs. The biomedical dataset consists of
193 medium to high frequency verbs from a cor-
pus of 2230 full-text articles from 3 biomedical
journals. A team of linguists and biologists cre-
ated a three-level gold standard with 16, 34 and
50 classes. Both datasets were pre-processed us-
ing non-negative matrix factorization (Lin, 2007)
which decomposes a large sparse matrix into two
dense matrices (of lower dimensionality) with
non-negative values. In all experiments 35 dimen-
sions were kept. Preliminary experiments with
different number of dimensions kept did not affect
the performance substantially.
We evaluate our results using three informa-
tion theoretic measures: Variation of Informa-
tion (Meila?, 2007), V-measure (Rosenberg and
Hirschberg, 2007) and V-beta (Vlachos et al,
2009). All three assess the two desirable proper-
ties that a clustering should have with respect to
a gold standard, homogeneity and completeness.
Homogeneity reflects the degree to which each
cluster contains instances from a single class and
is defined as the conditional entropy of the class
distribution of the gold standard given the clus-
tering. Completeness reflects the degree to which
each class is contained in a single cluster and is de-
fined as the conditional entropy of clustering given
the class distribution in the gold standard. V-beta
balances these properties explicitly by taking into
account the ratio of the number of cluster discov-
ered over the number of classes in the gold stan-
dard. While an ideal clustering should have both
properties, naively improving one of them can be
harmful for the other. Compared to the more com-
monly used F-measure (Fung et al, 2003), these
measures have the advantage that they do not as-
sume a mapping between clusters and classes.
5 Experiments
We performed experiments in order to assess the
effectiveness of the AL algorithm for the con-
strained DPMM comparing it to random selection.
In each AL round, we run the Gibbs sampler for
the (constrained) DPMM five times, using 100 it-
erations for burn-in, draw 20 samples from each
run with 5 iterations lag between samples and se-
lect the most uncertain link to be labeled. Fol-
lowing Navarro et al (2006), the concentration
parameter is inferred from the data using Gibbs
sampling. The performances were averaged across
the collected samples. Random selection was re-
peated three times. The three levels of the biomed-
ical gold standard were used independently and to-
gether with the general English dataset result in
four experimental setups.
The comparison between AL and random se-
lection for each dataset is shown in graphs 1(a)-
1(d) using V-beta, noting that the observations
made hold with all evaluation metrics used. Con-
straints selected via AL improve the performance
rapidly. Indicatively, the performance reached us-
ing 1000 randomly chosen constraints is obtained
using only 110 actively selected ones in the bio-50
dataset. AL performance levels out in later stages
with performance superior to the one achieved us-
ing random selection with the same number of
constraints. The poor performance of random se-
lection is expected, since the unsupervised DPMM
predicts more than 90% of the binary links cor-
rectly. Another interesting observation is that, dur-
ing AL, homogeneity increased faster than com-
pleteness (graphs 1(g) and 1(h)). This suggests
that the features used lead the model towards finer-
grained clusters, which is further confirmed by
the fact that the highest scores on the biomedical
dataset are achieved when comparing against the
finest-grained version of the gold standard. While
it is possible to choose constraints to the model
that would increase completeness with respect to
the gold standard, we argue that this would not al-
low us to obtain obtain insights on the model and
the features used.
We also noticed that the choice of batch size
has a significant effect on the learning rate of the
model. This phenomenon occurs in varying de-
grees in many applications of AL. Manual inspec-
tion of the links chosen at each round revealed that
batches often contained links involving the same
instances. This is expected due to transitivity: if
the link between instances A and B is uncertain
but the link between instances B and C is certain,
then the link between A and C will be uncertain
too. While reducing the batch size leads to bet-
ter learning rates, it requires estimating the model
more often. In order to ameliorate this issue, af-
ter obtaining the label of the most uncertain link,
we remove the samples that disagreed with it and
re-calculate the uncertainty of the remaining links
given the remaining samples. This is repeated un-
til the intended batch size is reached. Thus, we
59
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active
random
(a) bio-16
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active
random
(b) bio-34
 0.72
 0.76
 0.8
 0.84
 0.88
 0  50  100  150  200  250
V-
be
ta
links
active
random
(c) bio-50
 0.55
 0.6
 0.65
 0.7
 0.75
 0  50  100  150  200  250
V-
be
ta
links
active
random
(d) gen. English
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active10
batch
(e) bio-16
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active10
batch
(f) bio-34
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
hom
comp
(g) bio-50
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  50  100  150  200  250
V-
be
ta
links
hom
comp
(h) gen. English
Figure 1: (a)-(d): Constrained DPMM learning curves comparing random selection and AL. (e),(f):
Batch selection comparison. (g),(h): Homogeneity and completeness curves during AL.
avoid selecting links involving the same instance,
unless their uncertainty was not reduced by the
constraints added. A consideration that arises is
that by reducing the number of samples used for
uncertainty estimation, progressively we are left
with fewer samples to rank the remaining links.
Each labeled link reduces the number of samples
approximately by half since the most uncertain
link is likely to be a must-link in half the sam-
ples and a cannot-lnk in the remaining half. As
a result, for a batch with size |B| the uncertainty
of the last link will be estimated using |S|/2|B|?1
samples. A crude solution would be to generate
enough samples for the desired batch size. How-
ever, obtaining a very large number of samples can
be computationally expensive. Therefore, we set a
threshold for the minimum number of samples to
be used to estimate the link uncertainty and when
it is reached, more samples are generated using the
constraints selected. In graphs 1(e) and 1(f) we
demonstrate the effectiveness of the batch selec-
tion method proposed (labeled ?batch?) compared
to naive batch selection (labeled ?active10?).
6 Discussion and Future Work
We presented an AL method for constrained DP-
MMs employing uncertainty based sampling. We
applied it to two different verb clustering datasets
with 4 gold standards in total and obtained very
good results compared to random selection. The
idea, while explored in the context of verb cluster-
ing with the constrained DPMM, is likely to be ap-
plicable to other models that can incorporate must-
links and cannot-links in MCMC sampling.
Most literature on AL for NLP considers super-
vised methods for classification or sequential tag-
ging. However, AL for clustering is a relatively
under-explored area. Klein et al (2002) incorpo-
rated actively selected constraints in hierarchical
agglomerative clustering. Basu et al (2006) have
applied AL to obtain must-links and cannot-links
however, the clustering framework used requires
the number of clusters to be known in advance
which restricts counter-intuitively the clustering
solutions that are discovered. Moreover, semi-
supervised clustering is a form of semi-supervised
learning and in this light, our approach is related
to the work of Zhu et al (2003).
With respect to the practical application of the
AL method suggested, it is worth noting that in all
our experiments the constraints were obtained for
the respective gold standard of the dataset at ques-
tion and consequently they are all consistent with
each other. However, this assumption might not
hold in case human experts are employed for the
same purpose. In order to use such feedback in the
framework suggested, it is necessary to filter the
constraints provided in order to obtain a consistent
subset. To this end, it would be interesting to in-
vestigate the potential of using ?soft? constraints,
i.e. constraints that are provided with relative con-
fidence.
60
References
Sugato Basu, Mikhail Bilenko, Arindam Banerjee,
and Raymond J. Mooney. 2006. Probabilis-
tic semi-supervised clustering with constraints. In
O. Chapelle, B. Schoelkopf, and A. Zien, edi-
tors, Semi-Supervised Learning, pages 73?102. MIT
Press.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499?1504.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548?556.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA.
Benjamin C. M. Fung, Ke Wang, and Martin Ester.
2003. Hierarchical document clustering using fre-
quent itemsets. In Proceedings of SIAM Interna-
tional Conference on Data Mining, pages 59?70.
Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints to
space-level constraints: Making the most of prior
knowledge in data clustering. In Proceedings of the
Nineteenth International Conference on Machine
Learning, pages 307?314.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2006. Automatic classification of verbs in
biomedical texts. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 345?352.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago.
Chih-Jen Lin. 2007. Projected gradient methods for
nonnegative matrix factorization. Neural Compua-
tion, 19(10):2756?2779.
Marina Meila?. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers,
and Michael D. Lee. 2006. Modeling individual dif-
ferences using Dirichlet processes. Journal of Math-
ematical Psychology, 50(2):101?122, April.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 410?420.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, Univer-
sity of Wisconsin?Madison.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the 9th International Conference
on Intelligent Text Processing and Computational
Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 95?102.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 985?992, Sydney, Australia, July.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and Constrained
Dirichlet Process Mixture Models for Verb Cluster-
ing. In Proceedings of the EACL workshop on GEo-
metrical Models of Natural Language Semantics.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning, pages 1103?1110.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani.
2003. Combining Active Learning and Semi-
Supervised Learning Using Gaussian Fields and
Harmonic Functions. In ICML workshop on The
Continuum from Labeled to Unlabeled Data in Ma-
chine Learning and Data Mining, pages 58?65.
61
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 18?25,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Detecting Speculative Language Using Syntactic Dependencies
and Logistic Regression
Andreas Vlachos and Mark Craven
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
{vlachos,craven}@biostat.wisc.edu
Abstract
In this paper we describe our approach
to the CoNLL-2010 shared task on de-
tecting speculative language in biomedical
text. We treat the detection of sentences
containing uncertain information (Task1)
as a token classification task since the
existence or absence of cues determines
the sentence label. We distinguish words
that have speculative and non-speculative
meaning by employing syntactic features
as a proxy for their semantic content. In
order to identify the scope of each cue
(Task2), we learn a classifier that predicts
whether each token of a sentence belongs
to the scope of a given cue. The features
in the classifier are based on the syntactic
dependency path between the cue and the
token. In both tasks, we use a Bayesian
logistic regression classifier incorporating
a sparsity-enforcing Laplace prior. Over-
all, the performance achieved is 85.21%
F-score and 44.11% F-score in Task1 and
Task2, respectively.
1 Introduction
The term speculative language, also known as
hedging, refers to expressions of uncertainty over
statements. Recognition of such statements is im-
portant for higher-level applications. For exam-
ple, a multi-document summarization system can
assign different weights to speculative and non-
speculative statements when aggregating informa-
tion on a particular issue.
The CoNLL-2010 shared task (Farkas et al,
2010) formulates speculative language detection
as two subtasks. In the first subtask (Task1), sys-
tems need to determine whether a sentence con-
tains uncertain information or not. In the sec-
ond subtask (Task2), systems need to identify the
hedge cues and their scope in the sentence. Table 1
provides an example from the training data.
The participants are provided with data from
two domains: biomedical scientific literature (both
abstracts and full articles) and Wikipedia. We
choose to focus on the former. The training data
for this domain are nine full articles and 1,273 ab-
stracts from the BioScope corpus (Szarvas et al,
2008) and the test data are 15 full articles.
Our approach to speculative language detection
relies on syntactic parsing and machine learning.
We give a description of the techniques used in
Sections 2 and 3. We treat the detection of sen-
tences containing uncertain information (Task1) as
a token classification task in which we learn a clas-
sifier to predict whether a token is a cue or not. In
order to handle words that have speculative and
non-speculative meaning (e.g. ?indicating? in the
example of Table 1), we employ syntactic features
as a proxy for their semantic content (Section 4).
For scope identification (Task2), we learn a clas-
sifier that predicts whether each token of the sen-
tence belongs to the scope of a particular cue (Sec-
tion 6). The features used are based on the syntac-
tic dependency path between the cue and the to-
ken. We report results and perform error analysis
for both tasks, pointing out annotation issues that
could be ameliorated (Sections 5 and 7). Based on
our experience we suggest improvements on the
task definition taking into account work from the
broader field (Section 8).
2 Syntactic parsing for the biomedical
domain
The syntactic parser we chose for our experi-
ments is the C&C Combinatory Categorial Gram-
mar (CCG) parser adapted to the biomedical do-
main (Rimell and Clark, 2009). In this frame-
work, parsing is performed in three stages: part-
of-speech (PoS) tagging, CCG supertagging and
parse selection. The parse selection module de-
18
The Orthology and Combined modules both have states that achieve likelihood ratios above 400 (as
high as 1207 for the Orthology module and 613 for the Combined module), {indicating that both these
modules {can, on their own, predict some interacting protein pairs with a posterior odds ratio above 1}}.
Table 1: Sentence annotated as speculative with two cues (in boldface) and their scopes (in brackets).
rives the actual parse tree using the information
from the other two components. The intermediate
CCG supertagging stage assigns each token to a
lexical category which attempts to capture its syn-
tactic role in the sentence. Lexical categories con-
tain more information than PoS tags (mainly on
subcategorization) and they are more numerous,
thereby making their assignment a relatively dif-
ficult task. Therefore, the parse selection module
takes into account multiple predictions per token
which allows recovery from supertagging errors
while still reducing the ambiguity propagated. An
interesting aspect of this three-stage parsing ap-
proach is that, if the parse selection module fails to
construct a parse tree for the sentence (a common
issue when syntactic parsers are ported to new do-
mains), the lexical categories obtained by the su-
pertagger preserve some of the syntactic informa-
tion that would not be found in PoS tags.
The adaptation to the biomedical domain by
Rimell and Clark (2009) involved re-training the
PoS tagger and the CCG supertagger using in-
domain resources, while the parse selection com-
ponent was left intact. As recent work in the
BioNLP 2009 shared task has shown (Kim et al,
2009), domain-adapted parsing benefits informa-
tion extraction systems.
The native output of the C&C parser is con-
verted into the Stanford Dependency (SD) col-
lapsed dependency format (de Marneffe and Man-
ning, 2008). These dependencies define binary re-
lations between tokens and the labels of these re-
lations are obtained from a hierarchy. While the
conversion is unlikely to be perfect given that the
native C&C output follows a different formalism,
we made this choice because it allows for the use
of different parsers with minimal adaptation.
Finally, an important pre-processing step we
take is tokenization of the original text. Since the
PoS tagger is trained on the GENIA corpus which
follows the Penn TreeBank tokenization scheme,
we use the tokenization script provided by the tree-
bank.1
1http://www.cis.upenn.edu/?treebank/tokenization.html
3 Bayesian logistic regression
In both tasks, we use a Bayesian logistic regres-
sion classifier incorporating a sparsity-enforcing
Laplace prior (Genkin et al, 2006). Logistic re-
gression models are of the form:
p(y = +1|?, x) = exp(x?
T )
1 + exp(x?T ) (1)
where y ? {+1,?1} is a binary class label, x
is the feature vector representation of the instance
to be classified and ? is the feature weight vec-
tor which is learnt from the training data. Since
feature interactions are not directly represented,
the interactions that are expected to matter for the
task considered must be specified as additional
features. In Bayesian logistic regression, a prior
distribution on ? is used which encodes our prior
beliefs on the feature weights. In this work, we
use the Laplace prior which encourages the fea-
ture weight vector to be sparse, reflecting our be-
lief that most features will be irrelevant to the task.
4 Detecting sentences containing
speculation
In Task1, systems need to determine whether a
sentence contains uncertain information (labeled
uncertain) or not (labeled certain). A sentence is
uncertain if one or more of its tokens denote un-
certainty. Such tokens are labeled as cues and they
are provided by the organizers for training. If a
cue is a present, any other (potentially ?unhedg-
ing?) token becomes irrelevant to the task. There-
fore, we cast the task as a binary token classifi-
cation problem and determine the sentence label
from the token-level decisions.
Words used as speculative cues do not always
denote speculation. For example, in BioScope ?if?
and ?appear? are annotated as cues 4% and 83%
of the times they are encountered. In order to
gain better understanding of the task, we build a
dictionary-based cue extractor. First we extract all
the cues from the training data and use their lem-
mas, obtained using morpha (Minnen et al, 2001),
to tag tokens in the test data. We keep only single-
token cues in order to avoid non-indicative lem-
19
token=indicating lemma=indicate
PoS=VBG lemma+PoS=indicate+VBG
CCG=(S[ng]\NP)/S[em]
lemma+CCG=indicate+(S[ng]\NP)/S[em]
Table 2: Features extracted for the token ?indicat-
ing? from the Example in Table 1. CCG supertag
(S[ng]\NP)/S[em] denotes that ?indicating? ex-
pects an embedded clause (S[em]) to its right (in-
dicated by the forward slash /) and a noun phrase
(NP) to its left (indicated by the backward slash \)
to form a present participle (S[ng]).
mas entering the dictionary (e.g. ?that? in ?in-
dicate that?). Since the test data consist of full
articles only, we evaluate the performance of the
dictionary-based approach using four-fold cross-
validation on the nine full articles of the training
data with the abstracts added as training data in
every fold, but not used as test data. The recall
achieved is 98.07%, but F-score is lower (59.53%)
demonstrating that the single-token cues in the
training data provide adequate coverage, but low
precision. The restricted domain helps precision
as it precludes some word meanings from appear-
ing. For example ?might? is unlikely to be encoun-
tered as a noun in the biomedical domain. Never-
theless, in order to achieve better performance it
is important to further refine the cue identification
procedure.
Determining whether a token is used as a specu-
lative cue or not resembles supervised word sense
disambiguation. The main difference is that in-
stead of having an inventory of senses for each
word, we have two senses applicable to all words.
As in most word sense disambiguation tasks, the
classification of a word as cue or not is dependent
on the other words in the sentence, which we take
into account using syntax. The syntactic context
of words is a useful proxy to their semantics, as
shown in recent work on verb sense disambigua-
tion (Chen and Palmer, 2009). Furthermore, it is
easy to obtain syntactic information automatically
using a parser, even though there will be some
noise due to parsing errors. Similar intuitions were
exploited by Kilicoglu and Bergler (2008) in refin-
ing a dictionary of cues with syntactic rules.
In what follows, we present the features ex-
tracted for each token for our final system, along
with an example of their application in Table 2.
Where appropriate we give the relevant labels in
the Stanford Dependency (SD) scheme in paren-
theses for reproducibility:
? We extract the token itself and its lemma as
features.
? To handle cases where word senses are identi-
fiable by the PoS of a token (?might result? vs
?the might?), we combine the latter with the
lemma and add it as a feature.
? We combine the lemma with the CCG supertag
and add it as a feature in order to capture cases
where the hedging function of a word is de-
termined by its syntactic role in the sentence.
For example, ?indicating? in the example of
Table 1 is followed by a clausal complement (a
very reliable predictor of hedging function for
epistemic verbs), which is captured by its CCG
supertag. As explained in Section 2, this in-
formation can be recovered even in sentences
where the parser fails to produce a parse.
? Passive voice is commonly employed to limit
commitment to the statement made, therefore
we add it as a feature combined with the
lemma to verbs in that voice (nsubjpass).
? Modal auxiliaries are prime hedging devices
but they are also among the most ambiguous.
For example, ?can? is annotated as a cue in
16% of its occurrences and it is the fifth most
frequent cue in the full articles. To resolve
this ambiguity, we add as features the lemma
of the main verb the auxiliary is dependent on
(aux) as well as the lemmas of any dependents
of the main verb. Thus we can capture some
stereotypical speculative expressions in scien-
tific articles (e.g ?could be due?), while avoid-
ing false positives that are distinguished by the
use of first person plural pronoun and/or ref-
erence to objective enabling conditions (Kil-
icoglu and Bergler, 2008).
? Speculation can be expressed via negation of
a word expressing certainty (e.g. ?we do not
know?), therefore we add the lemma of the to-
ken prefixed with ?not? (neg).
? In order to capture stereotypical hedging ex-
pressions such as ?raises the question? and
?on the assumption? we add as features the di-
rect object of verbs combined with the lemma
of their object (dobj) and the preposition for
nouns in a prepositional relation (prep *).
? In order to capture the effect of adverbs on the
hedging function of verbs (e.g. ?theoretically
20
features Recall Precision F-score
tokens, lemmas 75.92 81.07 78.41
+PoS, CCG 78.23 83.71 80.88
+syntax 81.00 81.31 81.15
+combs 79.58 84.98 82.19
Table 3: Performance of various feature sets on
Task1 using cross-validation on full articles.
considered?) we add the lemma of the adverb
as a feature to the verb (advmod).
? To distinguish the probabilistic/numerical
sense from the hedging sense of adjectives
such as ?possible?, we add the lemma and the
number of the noun they modify as features
(amod), since plural number tends to be as-
sociated with the probabilistic/numerical sense
(e.g. ?all possible combinations?).
Finally, given that this stage is meant to identify
cues in order to recover their scopes in Task2, we
attempt to resolve multi-token cues in the train-
ing data into single-token ones. This agrees with
the minimal strategy for marking cues stated in the
corpus guidelines (Szarvas et al, 2008) and it sim-
plifies scope detection. Therefore, during train-
ing multi-token cues are resolved to their syntactic
head according to the dependency output, e.g. in
Table 1 ?indicate that? is restricted to ?indicate?
only. There were two cases in which this process
failed; the cues being ?cannot? (S3.167) and ?not
clear? (S3.269). We argue that the former is in-
consistently annotated (the sentence reads ?cannot
be defined. . . ? and it would have been resolved to
?defined?), while the latter is headed syntactically
by the verb ?be? which is preceding it.
5 Task1 results and error analysis
Initially we experiment using the full-articles part
of the training data only divided in four folds. The
reason for this choice is that the language of the
abstracts is relatively restricted and phenomena
that appear only in full papers could be obscured
by the abstracts, especially since the latter con-
sist of more sentences in total (11,871 vs. 2,670).
Such phenomena include language related to fig-
ures and descriptions of probabilistic models.
Each row in Table 3 is produced by adding
extra features to the feature set represented on
the row directly above it. First we consider us-
ing only the tokens and their lemmas as features
features Recall Precision F-score
tokens, lemmas 79.19 80.43 79.81
+PoS, CCG 81.12 85.22 83.12
+syntax 83.43 84.57 84.00
+combs 85.16 85.99 85.58
Table 4: Performance of various feature sets on
Task1 using cross-validation on full articles incor-
porating the abstracts as training data.
which amounts to a weighted dictionary but which
achieves reasonable performance. The inclusion
of PoS tags and CCG supertags improves perfor-
mance, whereas syntactic context increases recall
while decreasing precision slightly. This is due
to the fact that logistic regression does not rep-
resent feature interactions and the effect of these
features varies across words. For example, clausal
complements affect epistemic verbs but not other
words (?indicate? vs. ?state? in the example of
Table 1) and negation affects only words express-
ing certainty. In order to ameliorate this limitation
we add the lexicalized features described in Sec-
tion 4, for example the combination of the lemma
with the negation syntactic dependency. These ad-
ditional features improved precision from 81.31%
to 84.98%.
Finally, we add the abstracts to the training data
which improves recall but harms precision slightly
(Table 4) when only tokens and lemmas are used
as features. Nevertheless, we decided to keep them
as they have a positive effect for all other feature
representations.
A misinterpretation of the BioScope paper
(Szarvas et al, 2008) led us to believe that five of
the nine full articles in the training data were anno-
tated using the guidelines of Medlock and Briscoe
(2007). After the shared task, the organizers clar-
ified to us that all the full articles were annotated
using the BioScope guidelines. Due to our misin-
terpretation, we change our experimental setup to
cross-validate on the four full articles annotated in
BioScope only, considering the other five full ar-
ticles and the abstracts only as training data. We
keep this setup for the remainder of the paper.
We repeat the cross-validation experiments with
the full feature set and this new experimental setup
and report the results in Table 5. Using the same
feature set, we experiment with the Gaussian prior
instead of the sparsity-enforcing Laplace prior
which results in decreased precision and F-score,
21
Recall Precision F-score
cross-Laplace 80.33 84.21 82.23
cross-Gaussian 81.59 80.58 81.08
test 84.94 85.48 85.21
Table 5: Performance of the final system in Task1.
therefore confirming our intuition that most fea-
tures extracted are irrelevant to the task and should
have zero weight. Finally, we report our perfor-
mance on the test data using the Laplace prior.
6 Detecting the scope of the hedges
In Task2, the systems need to identify speculative
cues and their respective scopes. Since our system
for Task1 identifies cues, our discussion of Task2
focuses on identifying the scope of a given cue.
It is a non-trivial task, since scopes can be nested
and can span over a large number of tokens of the
sentence.
An initial approach explored was to associate
each cue with the token representing the syntactic
head of its scope and then to infer the scope us-
ing syntactic parsing. In order to achieve this, we
resolved the (almost always multi-token) scopes
to their syntactic heads and then built a classi-
fier whose features are based on syntactic depen-
dency paths. Multi-token scopes which were not
headed syntactically by a single token (according
to the parser) were discarded in order to obtain a
cleaner dataset for training. This phenomenon oc-
curs rather frequently, therefore reducing the train-
ing instances. At testing, the classifier identifies
the syntactic head of the scope for each cue and
we infer the scope from the syntactic parser?s out-
put. If more than one scope head is identified for
a particular cue, then the scopes are concatenated.
The performance of this approach turned out to
be very low, 10.34% in F-score. We identified two
principal reasons for this. First, relying on the syn-
tactic parser?s output to infer the scope is unavoid-
ably affected by parsing errors. Second, the scope
annotation takes into account semantics instead of
syntax. For example bibliographic references are
excluded based on their semantics.
In order to handle these issues, we developed an
approach that predicts whether each token of the
sentence belongs to the scope of a given cue. The
overall scope for that cue becomes the string en-
closed by the left- and right-most tokens predicted
to belong to the scope. The features used by the
classifier to predict whether a token belongs to the
scope of a particular cue are based on the short-
est syntactic dependency path connecting them,
which is found using Dijkstra?s algorithm. If no
such path is found (commonly due to parsing fail-
ure), then the token is classified as not belonging
to the scope of that cue. The features we use are
the following:
? The dependency path between the cue and the
token, combined with both their lemmas.
? According to the guidelines, different cues
have different preferences in having their
scopes extended to their left or to their right.
For example modal auxiliaries like ?can? in
Table 1 extend their scope to their right. There-
fore we add the dependency path feature de-
fined above refined by whether the token is on
the left or the right of the cue in question.
? We combine the dependency path and the lem-
mas of the cue and the token with their PoS
tags and CCG supertags, since these tags re-
fine the syntactic function of the tokens.
The features defined above are very sparse, espe-
cially when longer dependency paths are involved.
This can affect performance substantially, as the
scopes can be rather long, in many cases spanning
over the whole sentence. An unseen dependency
path between a cue and a token during testing re-
sults in the token being excluded from the scope
of that cue. In turn, this causes predicted scopes to
be shorter than they should be. We attempt to al-
leviate this sparsity in two stages. First, we make
the following simplifications to the labels of the
dependencies:
? Adjectival, noun compound, relative clause
and participial modifiers (amod, nn, rcmod,
partmod) are converted to generic modifiers
(mod).
? Passive auxiliary (auxpass) and copula (cop)
relations are converted to auxiliary relations
(aux).
? Clausal complement relations with inter-
nal/external subject (ccomp/xcomp) are con-
verted to complement relations (comp).
? All subject relations in passive or active voice
(nsubj, nsubjpass, csubj, csubjpass) are con-
verted to subjects (subj).
? Direct and indirect object relations (iobj, dobj)
are converted to objects (obj).
22
? We de-lexicalize conjunct (conj *) and prepo-
sitional modifier relations (prep *).
Second, we shorten the dependency paths:
? Since the SD collapsed dependencies format
treats conjunctions asymmetrically (conj), we
propagate the subject and object dependencies
of the head of the conjunction to the depen-
dent. We process appositional and abbrevi-
ation modifiers (appos, abbrev) in the same
way.
? Determiner and predeterminer relations (det,
predet) in the end of the dependency path are
removed, since the determiners (e.g. ?the?)
and predeterminers (e.g. ?both?) are included
in/excluded from the scope following their
syntactic heads.
? Consecutive modifier and dependent relations
(mod, dep) are replaced by a single relation of
the same type.
? Auxiliary relations (aux) that are not in the be-
ginning or the end of the path are removed.
Despite these simplifications, it is still possible
during testing to encounter dependency paths un-
seen in the training data. In order to ameliorate
this issue, we implement a backoff strategy that
progressively shortens the dependency path until
it matches a dependency path seen in the training
data. For example, if the path from a cue to a token
is subj-mod-mod and it has not been seen in the
training data, we test if subj-mod has been seen.
If it has, we consider it as the dependency path to
define the features described earlier. If not, we test
for subj in the same way. This strategy relies on
the assumption that tokens that are likely to be in-
cluded in/excluded from the scope following the
tokens they are syntactically dependent on. For
example, modifiers are likely to follow the token
being modified.
7 Task2 results and error analysis
In order to evaluate the performance of our ap-
proach, we performed four-fold cross-validation
on the four BioScope full articles, using the re-
maining full articles and the abstracts as training
data only. The performance achieved using the
features mentioned in Section 6 is 28.62% F-score,
while using the simplified dependency paths in-
stead of the path extracted from the parser?s out-
put improves it to 34.35% F-score. Applying the
back-off strategy for unseen dependency paths to
features Recall Precision F-score
standard 27.54 29.79 28.62
simplified 33.11 35.69 34.35
+backoff 34.10 36.75 35.37
+post 40.98 44.17 42.52
Table 6: Performance on Task2 using cross-
validation on BioScope full articles.
the simplified paths results in 35.37% F-score (Ta-
ble 6).
Our system predicts only single token cues.
This agrees in spirit with the minimal cue an-
notation strategy stated in the BioScope guide-
lines. The guidelines allow for multi-token cues,
referred to as complex keywords, which are de-
fined as cases where the tokens of a phrase cannot
express uncertainty independently. We argue that
this definition is rather vague, and combined with
the requirement for contiguity, results in cue in-
stances such as ?indicating that? (multiple occur-
rences), ?looks as? (S4.232) and ?address a num-
ber of questions? (S4.36) annotated as cues. It is
unclear why ?suggesting that? or ?appears that?
are not annotated as cues as well, or why ?that?
contributes to the semantic content of ?indicate?.
?that? does help determine the sense of ?indicate?,
but we argue that it should not be part of the cue as
it does not contribute to its semantic content. ?in-
dicate that? is the only consistent multi-token cue
pattern in the training data. Therefore, when our
system identifies as a cue a token with the lemma
?indicate?, if this token is followed by ?that?,
?that? is added to the cue. Given the annotation
difficulties multi-token cues present, it would be
useful during evaluation to relax cue matching in
the same way as in the BioNLP 2009 shared task,
i.e. considering as correct those cues predicted
within one token of the gold standard annotation.
As explained in Section 6, bibliographic ref-
erences are excluded from scopes and cannot be
recognized by means of syntactic parsing only.
Additionally, in some cases the XML formatting
does not preserve the parentheses and/or brack-
ets around numerical references. We employ two
post-processing steps to deal with these issues.
First, if the ultimate token of a scope happens to
be the penultimate token of the sentence and a
number, then it is removed from the scope. This
step can have a negative effect when the last to-
ken of the scope and penultimate token of the sen-
23
Recall Precision F-score
Cues cross 74.52 81.63 77.91test 74.50 81.85 78.00
Task2 cross 40.98 44.17 42.52test 42.40 45.96 44.11
Table 7: Performance on cue identification and
cue/scope identification in Task2.
tence happens to be a genuine number, as in Fig-
ure 1. In our experiments however, this heuristic
always increased performance. Second, if a scope
contains an opening parenthesis but not its clos-
ing one, then the scope is limited to the token im-
mediately before the opening one. Note that the
training data annotation allows for partial paren-
thetical statements to be included in scopes, as a
result of terminating scopes at bibliographic ref-
erences which are not the only tokens in a paren-
theses. For example, in S7.259: ?expressed (ED,
unpublished)? the scope is terminated after ?ED?.
These post-processing steps improved the perfor-
mance substantially to 42.52% F-score (Table 6).
The requirement for contiguous scope spans
which include their cue(s) is not treated appropri-
ately by our system, since we predict each token of
the scope independently. Combined with the fact
that the guidelines frequently define scopes to ex-
tend either to the left or to the right of the cue, an
approach based on sequential tagging and/or pre-
dicting boundaries could perform better. However,
as mentioned in the guidelines, the contiguity re-
quirement sometimes forced the inclusion of to-
kens that should have been excluded given the pur-
pose of the task.
Our final performance on the test data is 44.11%
in F-score (Table 7). This is higher than the one re-
ported in the official results (38.37%) because we
subsequently increased the coverage of the C&C
parser (parse failures resulted in 63 cues not re-
ceiving a scope), the addition of the back-off strat-
egy for unseen dependency paths and the clarifica-
tion on the inclusion of bibliographic references in
the scopes which resulted in improving the paren-
theses post-processing steps.
8 Related work
The shared task uses only full articles for testing
while both abstracts and full articles are used for
training. We argue that this represents a realistic
scenario for system developers since annotated re-
sources consist mainly of abstracts, while most in-
formation extraction systems are applied to full ar-
ticles. Also, the shared task aimed at detecting the
scope of speculation, while most previous work
(Light et al, 2004; Medlock and Briscoe, 2007;
Kilicoglu and Bergler, 2008) considered only clas-
sification of sentences, possibly due to the lack of
appropriately annotated resources.
The increasing interest in detecting speculative
language in scientific text resulted in a number of
guidelines. Compared to the most recent previous
definition by Medlock and Briscoe (2007), Bio-
Scope differs in the following ways:
? BioScope does not annotate anaphoric hedge
references.
? BioScope annotates indications of experimen-
tally observed non-universal behaviour.
? BioScope annotates statements of explicitly
proposed alternatives.
The first difference is due to the requirement that
the scope of the speculation be annotated, which
is not possible when it is present in a different sen-
tence. The other two differences follow from the
stated purpose which is the detection of sentences
containing uncertain information.
In related work, Hyland (1996) associates the
use of speculative language in scholarly publica-
tions with the purpose for which they are em-
ployed by the authors. In particular, he dis-
tinguishes content-oriented hedges from reader-
oriented ones. The former are used to calibrate
the strength of the claims made, while the latter
are employed in order to soften anticipated crit-
icism on behalf of the reader. Content-oriented
hedges are further distinguished as accuracy-
oriented ones, used to express uncertain claims
more accurately, and writer-oriented ones, used
to limit the commitment of the author(s) to the
claims. While the boundaries between these dis-
tinctions are not clear-cut and instances of hedging
can serve more than one of these purposes simulta-
neously, it is worth bearing them in mind while ap-
proaching the task. With respect to the shared task,
taking into account that hedging is used to ex-
press statements more accurately can help resolve
the ambiguity when annotating certain statements
about uncertainty. Such statements, which involve
words such as ?estimate?, ?possible?, ?predict?,
occur frequently in full articles.
Wilson (2008) analyzes speculation detection
24
inside a general framework for sentiment analysis
centered around the notion of private states (emo-
tions, thoughts, intentions, etc.) that are not open
to objective observation or verification. Specu-
lation is annotated with a spec-span/spec-target
scheme by answering the questions what the spec-
ulation is and what the speculation is about. With
respect to the BioScope guidelines, spec-span is
similar to what scope attempts to capture. spec-
span and spec-target do not need to be present
at the same time, which could help annotating
anaphoric cues.
9 Conclusions
This paper describes our approach to the CoNLL-
2010 shared task on speculative language detec-
tion using logistic regression and syntactic depen-
dencies. We achieved competitive performance on
sentence level uncertainty classification (Task1),
but not on scope identification (Task2). Motivated
by our error analysis we suggest refinements to the
task definition that could improve annotation.
Our approach to detecting speculation cues suc-
cessfully employed syntax as a proxy for the se-
mantic content of words. In addition, we demon-
strated that performance gains can be obtained by
choosing an appropriate prior for feature weights
in logistic regression. Finally, our performance in
scope detection was improved substantially by the
simplification scheme used to reduce the sparsity
of the dependency paths. It was devised using hu-
man judgment, but as information extraction sys-
tems become increasingly reliant on syntax and
each task is likely to need a different scheme, fu-
ture work should investigate how this could be
achieved using machine learning.
Acknowledgements
We would like to thank the organizers for provid-
ing the infrastructure and the data for the shared
task, Laura Rimell for her help with the C&C
parser and Marina Terkourafi for useful discus-
sions. The authors were funded by NIH/NLM
grant R01/LM07050.
References
Jinying Chen and Martha Palmer. 2009. Improving
English Verb Sense Disambiguation Performance
with Linguistically Motivated Features and Clear
Sense Distinction Boundaries. Language Resources
and Evaluation, 43(2):143?172.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In CrossParser ?08: Coling 2008: Pro-
ceedings of the Workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pages 1?8.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of CoNLL-2010 Shared Task, pages 1?12.
Alexander Genkin, David D. Lewis, and David Madi-
gan. 2006. Large-scale Bayesian Logistic Re-
gression for Text Classification. Technometrics,
49(3):291?304.
Ken Hyland. 1996. Writing Without Conviction?
Hedging in Science Research Articles. Applied Lin-
guistics, 17(4):433?454.
Halil Kilicoglu and Sabine Bergler. 2008. Recog-
nizing speculative language in biomedical research
articles: a linguistically motivated perspective. In
BioNLP ?08: Proceedings of the Workshop on Cur-
rent Trends in Biomedical Natural Language Pro-
cessing, pages 46?53.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 1?9.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The Language of Bioscience: Facts, Specu-
lations, and Statements In Between. In HLT-NAACL
2004 Workshop: BioLINK 2004, Linking Biological
Literature, Ontologies and Databases, pages 17?24.
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific
Literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Laura Rimell and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852?865.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In BioNLP ?08: Proceedings of
the Workshop on Current Trends in Biomedical Nat-
ural Language Processing, pages 38?45.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity
and Sentiment Analysis: Recognizing the Intensity,
Polarity, and Attitudes of Private States. Ph.D. the-
sis, University of Pittsburgh.
25
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 49?57,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Search-based Structured Prediction applied to Biomedical Event Extraction
Andreas Vlachos and Mark Craven
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
{vlachos,craven}@biostat.wisc.edu
Abstract
We develop an approach to biomedical event
extraction using a search-based structured pre-
diction framework, SEARN, which converts
the task into cost-sensitive classification tasks
whose models are learned jointly. We show
that SEARN improves on a simple yet strong
pipeline by 8.6 points in F-score on the
BioNLP 2009 shared task, while achieving the
best reported performance by a joint inference
method. Additionally, we consider the issue of
cost estimation during learning and present an
approach called focused costing that improves
improves efficiency and predictive accuracy.
1 Introduction
The term biomedical event extraction is used to re-
fer to the task of extracting descriptions of actions
and relations involving one or more entities from
the biomedical literature. The recent BioNLP 2009
shared task (BioNLP09ST) on event extraction (Kim
et al, 2009) focused on event types of varying com-
plexity. Each event consists of a trigger and one or
more arguments, the latter being proteins or other
events. Any token in a sentence can be a trigger for
one of the nine event types and, depending on their
associated event types, triggers are assigned appro-
priate arguments. Thus, the task can be viewed as
a structured prediction problem in which the output
for a given instance is a (possibly disconnected) di-
rected acyclic graph (not necessarily a tree) in which
vertices correspond to triggers or protein arguments,
and edges represent relations between them.
Despite being a structured prediction task, most of
the systems that have been applied to BioNLP09ST
to date are pipelines that decompose event extrac-
tion into a set of simpler classification tasks. Clas-
sifiers for these tasks are typically learned indepen-
dently, thereby ignoring event structure during train-
ing. Typically in such systems, the relationships
among these tasks are taken into account by incor-
porating post-processing rules that enforce certain
constraints when combining their predictions, and
by tuning classification thresholds to improve the ac-
curacy of joint predictions. Pipelines are appealing
as they are relatively easy to implement and they of-
ten achieve state-of-the-art performance (Bjorne et
al., 2009; Miwa et al, 2010).
Because of the nature of the output space, the task
is not amenable to sequential or grammar-based ap-
proaches (e.g. linear CRFs, HMMs, PCFGs) which
employ dynamic programming in order to do ef-
ficient inference. The only joint inference frame-
work that has been applied to BioNLP09ST to date
is Markov Logic Networks (MLNs) (Riedel et al,
2009; Poon and Vanderwende, 2010). However,
MLNs require task-dependent approximate infer-
ence and substantial computational resources in or-
der to achieve state-of-the-art performance.
In this work we explore an alternative joint in-
ference approach to biomedical event extraction us-
ing a search-based structured prediction framework,
SEARN (Daume? III et al, 2009). SEARN is an
algorithm that converts the problem of learning a
model for structured prediction into learning a set
of models for cost-sensitive classification (CSC).
CSC is a task in which each training instance has
a vector of misclassification costs associated with it,
thus rendering some mistakes on some instances to
be more expensive than others (Domingos, 1999).
Compared to a standard pipeline, SEARN is able to
49
achieve better performance because its models are
learned jointly. Thus, each of them is able to use fea-
tures representing the predictions made by the oth-
ers, while taking into account possible mistakes.
In this paper, we make the following contribu-
tions. Using the SEARN framework, we develop a
joint inference approach to biomedical event extrac-
tion. We evaluate our approach on the BioNLP09ST
dataset and show that SEARN improves on a simple
yet strong pipeline by 8.6 points in F-score, while
achieving the best reported performance on the task
by a joint inference method. Additionally, we con-
sider the issue of cost estimation and present an ap-
proach called focused costing that improves perfor-
mance. We believe that these contributions are likely
to be relevant to applications of SEARN to other
natural language processing tasks that involve struc-
tured prediction in complex output spaces.
2 BioNLP 2009 shared task description
BioNLP09ST focused on the extraction of events
involving proteins whose names are annotated in
advance. Each event has two types of arguments,
Theme and Cause, which correspond respectively to
the Agent and Patient roles in semantic role label-
ing (Gildea and Jurafsky, 2002). Nine event types
are defined which can be broadly classified in three
categories, namely Simple, Binding and Regulation.
Simple events include Gene expression, Transcrip-
tion, Protein catabolism, Phosphorylation, and Lo-
calization events. These have only one Theme ar-
gument which is a protein. Binding events have
one or more protein Themes. Finally, Regulation
events, which include Positive regulation, Nega-
tive regulation and Regulation, have one obligatory
Theme and one optional Cause, each of which can
be either a protein or another event. Each event has
a trigger which is a contiguous string that can span
over one or more tokens. Triggers and arguments
can be shared across events. In an example demon-
strating the complexity of the task, given the passage
?. . . SQ 22536 suppressed gp41-induced IL-10 pro-
duction in monocytes?, systems should extract the
three appropriately nested events listed in Fig. 1d.
Performance is measured using Recall, Precision
and F-score over complete events, i.e. the trigger,
the event type and the arguments all must be correct
in order to obtain a true positive. It is important to
note that if either the trigger, the type, or an argu-
ment of a predicted event is incorrect then this event
will result in one false positive and one false nega-
tive. In the example of Fig. 1, if ?suppressed? is rec-
ognized incorrectly as a Regulation trigger then it is
better to not assign a Theme to it so that we avoid
a false positive due to extracting an event with in-
correct type. Finally, the evaluation ignores triggers
that do not form events.
3 Event extraction decomposition
Figure 1 describes the event extraction decomposi-
tion that we use throughout the paper. We assume
that the sentences to be processed are parsed into
syntactic dependencies and lemmatized. Each stage
has its own module, which is either a learned classi-
fier (trigger recognition, Theme/Cause assignment)
or a rule-based component (event construction).
3.1 Trigger recognition
In trigger recognition the system decides whether a
token acts as a trigger for one of the nine event types
or not. Thus it is a 10-way classification task. We
only consider tokens that are tagged as nouns, verbs
or adjectives by the parser, as they cover the majority
of the triggers in the BioNLP09ST data. The main
features used in the classifier represent the lemma
of the token which is sufficient to predict the event
type correctly in most cases. In addition, we include
features that conjoin each lemma with its part-of-
speech tag. This allows us to handle words with
the same nominal and verbal form that have differ-
ent meanings, such as ?lead?. While the domain
restricts most lemmas to one event type, there are
some whose event type is determined by the context,
e.g. ?regulation? on its own denotes a Regulation
event but in ?positive regulation? it denotes a Posi-
tive regulation event instead. In order to capture this
phenomenon, we add as features the conjunction of
each lemma with the lemma of the tokens immedi-
ately surrounding it, as well as with the lemmas of
the tokens with which it has syntactic dependencies.
3.2 Theme and Cause assignment
In Theme assignment, we form an agenda of can-
didate trigger-argument pairs for all trigger-protein
combinations in the sentence and classify them as
50
SQ 22536 suppressed
Neg reg
gp41-induced
Pos reg
IL-10 production
Gene exp
(a) Trigger recognition
SQ 22536 suppressed
Neg reg
gp41-induced
Pos reg
IL-10 production
Gene exp
Theme
ThemeTheme
(b) Theme assignment
SQ 22536 suppressed
Neg reg
gp41-induced
Pos reg
IL-10 production
Gene exp
Theme
Theme
Cause
Theme
(c) Cause assignment
ID type Trigger Theme Cause
E1 Neg reg suppressed E2
E2 Pos reg induced E3 gp41
E3 Gene exp production IL-10
(d) Event construction
Figure 1: The stages of our event extraction decomposition. Protein names are shown in bold.
Themes or not. Whenever a trigger is predicted to be
associated with a Theme, we form candidate pairs
between all the Regulation triggers in the sentence
and that trigger as the argument, thus allowing the
prediction of nested events. Also, we remove candi-
date pairs that could result in directed cycles, as they
are not allowed by the task.
The features used to predict whether a trigger-
argument pair should be classified as a Theme are
extracted from the syntactic dependency path and
the textual string between them. In particular, we
extract the shortest unlexicalized dependency path
connecting each trigger-argument pair, allowing the
paths to follow either dependency direction. One set
of features represents these paths, and in addition,
we have sets of features representing each path con-
joined with the lemma, the PoS tag and the event
type of the trigger, the type of the argument and
the first and last lemmas in the dependency path.
The latter help by providing some mild lexicaliza-
tion. We also add features representing the textual
string between the trigger and the argument, com-
bined with the event type of the trigger. While not as
informative as dependency paths, such features help
in sentences where the parse is incorrect, as triggers
and their arguments tend to appear near each other.
In Cause assignment, we form an agenda of can-
didate trigger-argument pairs using only the Regu-
lation class triggers that were assigned at least one
Theme. These are combined with protein names and
other triggers that were assigned a Theme. We ex-
tract features as in Theme assignment, further fea-
tures representing the conjunction of the dependency
path of the candidate pair with the path(s) from the
trigger to its Theme(s).
3.3 Event construction
In event construction, we convert the predictions of
the previous stages into a set of legal events. If
a Binding trigger is assigned multiple Themes, we
choose to form either one event per Theme or one
event with multiple Themes. Following Bjorne et
al. (2009), we group the arguments of each Binding
trigger according to the first label in their syntac-
tic dependency path and generate events using the
cross-product of these groups. For example, assum-
ing the parse was correct and all the Themes recog-
nized, ?interactions of A and B with C? results in
two Binding events with two Themes each, A with
C, and B with C respectively. We add the exception
that if two Themes are in the same token (e.g. ?A/B
interactions?) or the lemma of the trigger is ?bind?
then they form one Binding event with two Themes.
4 Structured prediction with SEARN
SEARN (Daume? III et al, 2009) forms the struc-
tured output prediction for an instance s as a se-
quence of T multiclass predictions y?1:T made by a
hypothesis h. The latter consists of a set of classi-
fiers that are learned jointly. Each prediction y?t can
use features from s as well as from all the previous
predictions y?1:t?1. These predictions are referred to
51
as actions and we adopt this term in order to distin-
guish them from the structured output predictions.
The SEARN algorithm is presented in Alg. 1. It
initializes hypothesis h to the optimal policy pi (step
2) which predicts the optimal action in each step
t according to the gold standard. The optimal ac-
tion at step t is the one that minimizes the overall
loss over s assuming that all future actions y?t+1:T
are also made optimally. The loss function ` is de-
fined by the structured prediction task considered.
Each iteration begins by making predictions for all
instances s in the training data S (step 6). For each
s and each action y?t, a cost-sensitive classification
(CSC) example is generated (steps 8-12). The fea-
tures are extracted from s and the previous actions
y?1:t?1 (step 8). The cost for each possible action
yit is estimated by predicting the remaining actions
y?t+1:T in s using h (step 10) and evaluating the cost
incurred given that action (step 11). Using a CSC
learning algorithm, a new hypothesis is learned (step
13) which is combined with the current one accord-
ing to the interpolation parameter ?.
Algorithm 1 SEARN
1: Input: labeled instances S , optimal policy pi, CSC
learning algorithm CSCL, loss function `
2: current policy h = pi
3: while h depends significantly on pi do
4: Examples E = ?
5: for s in S do
6: Predict h(s) = y?1:T
7: for y?t in h(s) do
8: Extract features ?t = f(s, y?1:t?1)
9: for each possible action yit do
10: Predict y?t+1:T = h(s|y?1:t?1, yit)
11: Estimate cit = `(y?1:t?1, y
i
t, y?t+1:T )
12: Add (?t, ct) to E
13: Learn a hypothesis hnew = CSCL(E)
14: h = ?hnew + (1? ?)h
15: Output: policy h without pi
In each iteration, SEARN moves away from the
optimal policy and uses the learned hypotheses in-
stead when predicting (steps 6 and 10). Thus, each
hnew is adapted to the actions chosen by h instead
of those of the optimal policy. When the depen-
dence on the latter becomes insignificant, the algo-
rithm terminates and returns the weighted ensemble
of learned hypotheses without the optimal policy.
Note though that the estimation of the costs in step
11 is always performed using the gold standard.
The interpolation parameter ? determines how
fast SEARN moves away from the optimal policy
and as a result how many iterations will be needed to
minimize the dependence on it. Dependence in this
context refers to the probability of using the optimal
policy instead of the learned hypothesis in choos-
ing an action during prediction. In each iteration,
the features extracted ?t are progressively corrupted
with the actions chosen by the learned hypotheses
instead of those of the optimal policy.
Structural information under SEARN is incorpo-
rated in two ways. First, via the costs that are es-
timated using the loss over the instance rather than
isolated actions (e.g. in PoS tagging, the loss would
be the number of incorrect PoS tags predicted in
a sentence if a token is tagged as noun). Second,
via the features extracted from the previous actions
(y?1:t?1) (e.g. the PoS tag predicted for the previ-
ous token can be a feature). These types of features
are possible in a standard pipeline as well, but dur-
ing training they would have to be extracted using
the gold standard instead of the actual predictions
made by the learned hypotheses, as during testing.
Since the prediction for each instance (y?1:T in step
6) changes in every iteration, the structure features
used to predict the actions have to be extracted anew.
The extraction of features from previous actions
implies a search order. For some tasks, such as PoS
tagging, there is a natural left-to-right order in which
the tokens are treated, however for many tasks this
is not the case.
Finally, SEARN can be used to learn a pipeline of
independently trained classifiers. This is achieved
using only one iteration in which the cost for each
action is set to 0 if it follows from the gold standard
and to 1 otherwise. This adaptation allows for a fair
comparison between SEARN and a pipeline.
5 SEARN for biomedical event extraction
In this section we discuss how we learn the event
extraction decomposition described in Sec. 3 under
SEARN. Each instance is a sentence consisting of
the tokens, the protein names and the syntactic pars-
ing output. The hypothesis learned in each iteration
consists of a classifier for each stage of the pipeline,
52
excluding event construction which is rule-based.
Unlike PoS tagging, there is no natural ordering
of the actions in event extraction. Ideally, the ac-
tions predicted earlier should be less dependent on
structural features and/or easier so that they can in-
form the more structure dependent/harder ones. In
trigger recognition, we process the tokens from left
to right since modifiers appearing before nouns tend
to affect the meaning of the latter, e.g. ?binding ac-
tivity?. In Theme and Cause assignment, we predict
trigger-argument pairs in order of increasing depen-
dency path length, assuming that since dependency
paths are the main source of features at this stage and
shorter paths are less sparse, pairs containing shorter
ones should be more reliable to predict.
In addition to the features mentioned in Sec. 3,
SEARN allows us to extract and learn weights for
structural features for each action from the previous
ones. During trigger recognition, we add as features
the combination of the lemma of the current token
combined with the event type (if any) assigned to
the previous and the next token, as well as to the to-
kens that have syntactic dependencies with it. Dur-
ing Theme assignment, when considering a trigger-
argument pair, we add features based on whether it
forms an undirected cycle with previously predicted
Themes, whether the trigger has been assigned a pro-
tein as a Theme and the candidate Theme is an event
trigger (and the reverse) and whether the argument
has become the Theme of a trigger with the same
event type. We also add a feature indicating whether
the trigger has three Themes predicted already. Dur-
ing Cause assignment, we add features representing
whether the trigger has been assigned a protein as a
Cause and the candidate Cause is an event trigger.
The loss function ` sums the number of false pos-
itive and false negative events, which is the evalua-
tion measure of BioNLP09ST. The optimal policy is
derived from the gold standard and returns the ac-
tion that minimizes this loss over the sentence given
the previous actions and assuming that all future ac-
tions are optimal. In trigger recognition, it returns
either the event type for tokens that are triggers or a
?notrigger? label otherwise. In Theme assignment,
for a given trigger-argument pair the optimal policy
returns Theme only if the trigger is recognized cor-
rectly and the argument is indeed a Theme for that
trigger according to the gold standard. In case the ar-
gument is another event, we require that at least one
of its Themes to be recognized correctly as well. In
Cause assignment, the requirements are the same as
those for the Themes, but we also require that at least
one Theme of the trigger in the trigger-argument pair
to be considered correct. These additional checks
follow from the task definition, under which events
must have all their elements identified correctly.
5.1 Cost estimation
Cost estimation (steps 5-12 in Alg. 1) is crucial to
the successful application of SEARN. In order to
highlight its importance, consider the example of
Fig. 2 focusing on trigger recognition.
In the first iteration (Fig. 2a), the actions for the
sentence will be made using the optimal policy only,
thus replicating the gold standard. During costing,
if a token is not a trigger according to the gold stan-
dard (e.g. ?SQ?), then the cost for incorrectly pre-
dicting that it is a trigger is 0, as the optimal policy
will not assign Themes to a trigger with incorrect
event type. Such instances are ignored by the cost-
sensitive learner. If a token is a trigger according to
the gold standard, then the cost for not predicting it
as such or predicting its type incorrectly is equal to
the number of the events that are dependent on it, as
they will become false negatives. False positives are
avoided as we are using the optimal policy in this
iteration.
In the second iteration (Fig. 2b), the optimal pol-
icy is interpolated with the learned hypothesis, thus
some of the actions are likely to be incorrect. As-
sume that ?SQ? is incorrectly predicted to be a
Neg reg trigger and assigned a Theme. During cost-
ing, the action of labeling ?SQ? as Neg reg has a
cost of 1, as it would result in a false positive event.
Thus the learned hypothesis will be informed that it
should not label ?SQ? as a trigger as it would assign
Themes to it incorrectly and it is adapted to handle
its own mistakes. Similarly, the action of labeling
?production? as Neg reg in this iteration would in-
cur a cost of 6, as the learned hypothesis would as-
sign a Theme incorrectly, thus resulting in 3 false
negative and 3 false positive events. Therefore, the
learned hypothesis will be informed that assigning
the wrong event type to ?production? is worse than
not predicting a trigger.
By evaluating the cost of each action according to
53
SQ 22536 suppressed
Neg reg
gp41-induced
Pos reg
IL-10 production
Gene exp
Theme
Theme
Cause
Theme
token No Gene exp Pos reg Neg reg
SQ 0 0 0 0
suppressed 1 1 1 0
-induced 2 2 0 2
production 3 0 3 3
(a) First iteration (optimal policy only)
SQ
Neg reg
22536 suppressed
Neg reg
gp41-induced
Pos reg
IL-10 production
Neg reg
Theme
Theme
Cause
ThemeTheme
token No Gene exp Pos reg Neg reg
SQ 0 0 0 1
suppressed 1 1 1 0
-induced 2 2 0 2
production 3 0 3 6
(b) Second iteration (interpolation)
Figure 2: Prediction (top) and CSC examples for trigger recognition actions (bottom) in the first two SEARN
iterations. Each CSC example has its own vector of misclassification costs.
its effect on the prediction for the whole sentence,
we are able to take into account steps in the pre-
diction process that are not learned as actions. For
example, if the Binding event construction heuris-
tic described in Sec. 3.3 cannot produce the correct
events for a token that is a Binding trigger despite
the Themes being assigned correctly, then this will
increase the cost for tagging that trigger as Binding.
The interpolation between the optimal policy and
the learned hypothesis is stochastic, thus affecting
the cost estimates obtained. In order to obtain more
reliable estimates, one can average multiple sam-
ples for each action by repeating steps 10 and 11
of Alg. 1. However, the computational cost is effec-
tively multiplied by the number of samples.
In step 11 of Alg. 1, the cost of each action is esti-
mated over the whole sentence. While this allows us
to take structure into account, it can result in costs
being affected by a part of the output that is not re-
lated to that action. This is likely to occur in event
extraction, as sentences can often be long and con-
tain disconnected event components in their output
graphs. For this reason, we refine the cost estimation
of each action to take into account only the events
that are connected to it through either gold standard
or predicted events. For example, in Fig. 2 the cost
estimation for ?SQ? will ignore the predicted events
in the first iteration and the gold standard, while it
will take them into account in the second one. We
refer to this refinement as focused costing.
A different approach proposed by Daume? III et
al. (2009) is to assume that all actions following the
one we are costing are going to be optimal and use
the optimal policy to approximate the prediction of
the learned hypothesis in step 10 of Alg. 1. In tasks
where the learned hypothesis is accurate enough,
this has no performance loss and it is computation-
ally efficient as the optimal policy is deterministic.
However, in event extraction the learned hypothesis
is likely to make mistakes, thus the optimal policy
does not provide a good approximation for it.
5.2 CSC learning with passive-aggressive
algorithms
The SEARN framework requires a multiclass CSC
algorithm to learn how to predict actions. This algo-
rithm must be computationally fast during parameter
learning and prediction, as in every iteration we need
to learn a new hypothesis and to consider each pos-
sible action for each instance in order to construct
the cost-sensitive examples. Daume? III et al (2009)
showed that any binary classification algorithm can
be used to perform multiclass CSC by employing an
appropriate conversion between the tasks. The main
drawback of this approach is its reliance on multi-
ple subsamplings of the training data, which can be
inefficient for large datasets and many classes.
With these considerations in mind, we implement
a multiclass CSC learning algorithm using the gen-
eralization of the online passive-aggressive (PA) al-
gorithm for binary classification proposed by Cram-
mer et al (2006). For each training example xt,
the K-class linear classifier with K weight vectors
w(k)t makes a prediction y?t and suffers a loss `t. In
54
the case of multiclass CSC learning, each example
has its own cost vector ct. If the loss is 0 then the
weight vectors of the classifier are not updated (pas-
sive). Otherwise, the weight vectors are updated
minimally so that the prediction on example xt is
corrected (aggressive). The update takes into ac-
count the loss and the aggressiveness parameter C.
Crammer et al (2006) describe three variants to per-
form the updates which differ in how the learning
rate ?t is set. In our experiments we use the variant
named PA-II with prediction-based updates (Alg. 2).
Since we are operating in a batch learning setting
(i.e. we have access to all the training examples and
their order is not meaningful), we perform multiple
rounds over the training examples shuffling their or-
der, and average the weight vectors obtained.
Algorithm 2 Passive-aggressive CSC learning
1: Input: training examples X = x1 . . . xT , cost vec-
tors c1 . . . cT ? 0, rounds R, aggressiveness C
2: Initialize weights w(k)0 = (0, ..., 0)
3: for r = 1, ..., R do
4: Shuffle X
5: for xt ? X do
6: Predict y?t = argmaxk(w
(k)
t ? xt)
7: Receive cost vector ct ? 0
8: if c(y?t)t > 0 then
9: Suffer loss `t = w
(y?t)
t ?xt?w
(yt)
t ?xt+
?
c(y?t)t
10: Set learning rate ?t =
`t
||xt||2+ 12C
11: Update w(yt)t+1 = wt + ?txt
12: Update w(y?t)t+1 = wt ? ?txt
13: Average wavg = 1T?R
?T?R
i=0 wi
6 Experiments
BioNLP09ST comprises three datasets ? training,
development and test ? which consist of 800, 150
and 260 abstracts respectively. After the end
of the shared task, an on-line evaluation server
was activated in order to allow the evaluation on
the test data once per day, without allowing ac-
cess to the data itself. We report results using
Recall/Precision/F-score over complete events using
the approximate span matching/approximate recur-
sive matching variant which was the primary perfor-
mance criterion in BioNLP09ST. This variant counts
a predicted event as a true positive if its trigger is
extracted within a one-token extension of the gold-
standard trigger. Also, in the case of nested events,
those events below the top-level need their trigger,
event type and Theme but not their Cause to be cor-
rectly identified for the top-level event to be consid-
ered correct. The same event matching variant was
used in defining the loss as described in Sec. 5.
A pre-processing step we perform on the train-
ing data is to reduce the multi-token triggers in the
gold standard to their syntactic heads. This proce-
dure simplifies the task of assigning arguments to
triggers and, as the evaluation variant used allows
approximate trigger matching, it does not result in
a performance loss. For syntactic parsing, we use
the output of the BLLIP re-ranking parser adapted to
the biomedical domain by McClosky and Charniak
(2008), as provided by the shared task organizers
in the Stanford collapsed dependency format with
conjunct dependency propagation. Lemmatization
is performed using morpha (Minnen et al, 2001).
In all our experiments, for CSC learning with PA,
the C parameter is set by tuning on 10% of the train-
ing data and the number of rounds is fixed to 10. For
SEARN, we set the interpolation parameter ? to 0.3
and the number of iterations to 12. The costs for
each action are obtained by averaging three samples
as described in Sec. 5.1. ? and the number of sam-
ples are the only parameters that need tuning and we
use the development data for this purpose.
First we compare against a pipeline of indepen-
dently learned classifiers obtained as described in
Sec. 4 in order to assess the benefits of joint learning
under SEARN using focused costing. The results
shown in Table 1 demonstrate that SEARN obtains
better event extraction performance on both the de-
velopment and test sets by 7.7 and 8.6 F-score points
respectively. The pipeline baseline employed in our
experiments is a strong one: it would have ranked
fifth in BioNLP09ST and it is 20 F-score points bet-
ter than the baseline MLN employed by Poon and
Vanderwende (2010). Nevertheless, the indepen-
dently learned classifier for triggers misses almost
half of the event triggers, from which the subsequent
stages cannot recover. On the other hand, the trig-
ger classifier learned with SEARN overpredicts, but
since the Theme and Cause classifiers are learned
jointly with it they maintain relatively high precision
with substantially higher recall compared to their in-
55
pipeline SEARN focus SEARN default
R P F R P F R P F
triggerdev 53.0 61.1 56.8 81.8 34.2 48.2 84.9 12.0 21.0
Themedev 44.2 79.6 56.9 62.0 69.1 65.4 59.0 65.1 61.9
Causedev 18.1 59.2 27.8 30.6 45.0 36.4 31.9 45.5 37.5
Eventdev 35.8 68.9 47.1 50.8 59.5 54.8 47.4 54.3 50.6
Eventtest 30.8 67.4 42.2 44.5 59.1 50.8 41.3 53.6 46.6
Table 1: Recall / Precision / F-score on BioNLP09ST development and test data. Left-to-right: pipeline of
independently learned classifiers, SEARN with focused costing, SEARN with default costing.
dependently learned counterparts. The benefits of
SEARN are more pronounced in Regulation events
which are more complex. For these events, it im-
proves on the pipeline on both the development and
test sets by 11 and 14.2 F-score points respectively.
The focused costing approach we proposed con-
tributes to the success of SEARN. If we replace it
with the default costing approach which uses the
whole sentence, the F-score drops by 4.2 points on
both development and test datasets. The default
costing approach mainly affects the trigger recog-
nition stage, which takes place first. Trigger over-
prediction is more extreme in this case and renders
the Theme assignment stage harder to learn. While
the joint learning of the classifiers ameliorates this
issue and the event extraction performance is even-
tually higher than that of the pipeline, the use of fo-
cused costing improves the performance even fur-
ther. Note that trigger overprediction also makes
training slower, as it results in evaluating more ac-
tions for each sentence. Finally, using one instead
of three samples per action decreases the F-score by
1.3 points on the development data.
Compared with the MLN approaches applied to
BioNLP09ST, our predictive accuracy is better than
that of Poon and Vanderwende (2010) which is the
best joint inference performance to date and substan-
tially better than that of Riedel et al (2009) (50 and
44.4 in F-score respectively). Recently, McClosky
et al (2011) combined multiple decoders for a de-
pendency parser with a reranker, achieving 48.6 in
F-score. While they also extracted structure fea-
tures for Theme and Cause assignment, their model
is restricted to trees (ours can output directed acyclic
graphs) and their trigger recognizer is learned inde-
pendently.
When we train SEARN combining the training
and the development sets, we reach 52.3 in F-score,
which is better than the performance of the top
system in BioNLP09ST (51.95) by Bjorne et al
(2009) which was trained in the same way. The
best performance to date is reported by Miwa et al
(2010) (56.3 in F-score), who experimented with six
parsers, three dependency representations and vari-
ous combinations of these. They found that different
parser/dependency combinations provided the best
results on the development and test sets.
A direct comparison between learning frame-
works is difficult due to the differences in task de-
composition and feature extraction. In particular,
event extraction results depend substantially on the
quality of the syntactic parsing. For example, Poon
and Vanderwende (2010) heuristically correct the
syntactic parsing used and report that this improved
their performance by four F-score points.
7 Conclusions
We developed a joint inference approach to biomed-
ical event extraction using the SEARN framework
which converts a structured prediction task into a set
of CSC tasks whose models are learned jointly. Our
approach employs the PA algorithm for CSC learn-
ing and a focused cost estimation procedure which
improves the efficiency and accuracy of the standard
cost estimation method. Our approach provides the
best reported results for a joint inference method on
the BioNLP09ST task. With respect to the experi-
ments presented by Daume? III et al (2009), we em-
pirically demonstrate the gains of using SEARN on
a problem harder than sequential tagging.
Acknowledgments
The authors were funded by NIH/NLM grant R01 /
LM07050.
56
References
Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP 2009Work-
shop Companion Volume for Shared Task, pages 10?
18.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Hal Daume? III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75:297?325.
Pedro Domingos. 1999. Metacost: a general method for
making classifiers cost-sensitive. In Proceedings of
the 5th International Conference on Knowledge Dis-
covery and Data Mining, pages 155?164.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28:245?288.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the 46th Annual Meeting of the Association of Compu-
tational Linguistics: Human Language Technologies,
pages 101?104.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency pars-
ing. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating dependency repre-
sentation for event extraction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 779?787.
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of the Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 813?821.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A Markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
Shared Task, pages 41?49.
57
Proceedings of BioNLP Shared Task 2011 Workshop, pages 36?40,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Biomedical Event Extraction from Abstracts and Full Papers using
Search-based Structured Prediction
Andreas Vlachos and Mark Craven
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
{vlachos,craven}@biostat.wisc.edu
Abstract
In this paper we describe our approach to
the BioNLP 2011 shared task on biomedical
event extraction from abstracts and full pa-
pers. We employ a joint inference system de-
veloped using the search-based structured pre-
diction framework and show that it improves
on a pipeline using the same features and it is
better able to handle the domain shift from ab-
stracts to full papers. In addition, we report on
experiments using a simple domain adaptation
method.
1 Introduction
The term biomedical event extraction is used to re-
fer to the task of extracting descriptions of actions
and relations among one or more entities from the
biomedical literature. The BioNLP 2011 shared
task GENIA Task1 (BioNLP11ST-GE1) (Kim et al,
2011) focuses on extracting events from abstracts
and full papers. The inclusion of full papers in the
datasets is the only difference from Task1 of the
BioNLP 2009 shared task (BioNLP09ST1) (Kim et
al., 2009), which used the same task definition and
abstracts dataset. Each event consists of a trigger
and one or more arguments, the latter being proteins
or other events. The protein names are annotated in
advance and any token in a sentence can be a trig-
ger for one of the nine event types. In an exam-
ple demonstrating the complexity of the task, given
the passage ?. . . SQ 22536 suppressed gp41-induced
IL-10 production in monocytes?, systems should ex-
tract the three nested events shown in Fig. 1d.
In our submission, we use the event extraction
system of Vlachos and Craven (2011) which em-
ploys the search-based structured prediction frame-
work (SEARN) (Daume? III et al, 2009). SEARN
converts the problem of learning a model for struc-
tured prediction into learning a set of models for
cost-sensitive classification (CSC). In CSC, each
training instance has a vector of misclassification
costs associated with it, thus rendering some mis-
takes in some instances to be more expensive than
others. Compared to other structured prediction
frameworks such as Markov Logic Networks (Poon
and Vanderwende, 2010), SEARN provides high
modeling flexibility but it does not requiring task-
dependent approximate inference.
In this work, we show that SEARN is more accu-
rate than a pipeline using the same features and it is
better able to handle the domain shift from abstracts
to full papers. Furthermore, we report on exper-
iments with the simple domain adaptation method
proposed by Daume? III (2007), which creates a ver-
sion of each feature for each domain. While the re-
sults were mixed, this method improves our perfor-
mance on full papers of the test set, for which little
training data is available.
2 Event extraction decomposition
Figure 1 describes the event extraction decomposi-
tion that is used throughout the paper. Each stage has
its own module to perform the classification needed.
In trigger recognition the system decides whether
a token acts as a trigger for one of the nine event
types or not. We only consider tokens that are tagged
as nouns, verbs or adjectives by the parser, as they
36
SQ 22536 suppressed
Neg reg
gp41 -induced
Pos reg
IL-10 production
Gene exp
(a) Trigger recognition
SQ 22536 suppressed
Neg reg
gp41 -induced
Pos reg
IL-10 production
Gene exp
Theme
ThemeTheme
(b) Theme assignment
SQ 22536 suppressed
Neg reg
gp41 -induced
Pos reg
IL-10 production
Gene exp
Theme
Theme
Cause
Theme
(c) Cause assignment
ID type Trigger Theme Cause
E1 Neg reg suppressed E2
E2 Pos reg induced E3 gp41
E3 Gene exp production IL-10
(d) Event construction
Figure 1: The stages of our biomedical event extraction system.
cover the majority of the triggers in the data. The
main features used in the classifier represent the
lemma of the token which is sufficient to predict
the event type correctly in most cases. In addition,
we include features that conjoin each lemma with
its part-of-speech tag and its immediate lexical and
syntactic context, which allows us to handle words
that can represent different event types, e.g. ?activ-
ity? often denotes a Regulation event but in ?binding
activity? it denotes a Binding event instead.
In Theme assignment, we form an agenda of can-
didate trigger-argument pairs for all trigger-protein
combinations in the sentence and classify them as
Themes or not. Whenever a trigger is predicted to be
associated with a Theme, we form candidate pairs
between all the Regulation triggers in the sentence
and that trigger as the argument, thus allowing the
prediction of nested events. Also, we remove candi-
date pairs that could result in directed cycles, as they
are not allowed by the task. In Cause assignment,
we form an agenda of candidate trigger-argument
pairs and classify them as Causes or not. We form
pairs between Regulation class triggers that were as-
signed at least one Theme, and protein names and
other triggers that were assigned at least one Theme.
The features used in these two stages are extracted
from the syntactic dependency path and the textual
string between the trigger and the argument. We
extract the shortest unlexicalized dependency path
connecting each trigger-argument pair using Dijk-
stra?s algorithm, allowing the paths to follow either
dependency direction. One set of features represents
the shortest unlexicalized path between the pair and
in addition we have sets of features representing
each path conjoined with the lemma, the PoS tag and
the event type of the trigger, the type of the argument
and the first and last lemmas in the dependency path.
In the event construction stage, we convert the
predictions of the previous stages into events. If
a Binding trigger is assigned multiple Themes, we
choose to form either one event per Theme or one
event with multiple Themes. For this purpose, we
group the arguments of each nominal Binding trig-
ger according to the first label in their dependency
path and generate events using the cross-product of
these groups. For example, assuming the parse was
correct and all the Themes recognized, ?interactions
of A and B with C? results in two Binding events
with two Themes each, A with C, and B with C re-
spectively. We add the exceptions that if two Themes
are part of the same token (e.g. ?A/B interactions?),
or the trigger and one of the Themes are part of the
same token, or the lemma of the trigger is ?bind?
then they form one Binding event with two Themes.
3 Structured prediction with SEARN
SEARN (Daume? III et al, 2009) forms the struc-
tured output prediction of an instance s as a se-
quence of T multiclass predictions y?1:T made by a
hypothesis h. The latter is a weighted ensemble of
classifiers that are learned jointly. Each prediction y?t
can use features from s as well as from all the pre-
vious predictions y?1:t?1, thus taking structure into
37
account. These predictions are referred to as actions
and we adopt this term in order to distinguish them
from the structured output predictions.
The SEARN algorithm is presented in Alg. 1. In
each iteration, SEARN uses the current hypothesis
h to generate a CSC example for each action y?t cho-
sen to form the prediction for each labeled instance
s (steps 6-12). The cost associated with each action
is estimated using the gold standard according to a
loss function l which corresponds to the task eval-
uation metric (step 11). Using a CSC learning al-
gorithm, a new hypothesis hnew is learned (step 13)
which is combined with the current one according to
the interpolation parameter ? (step 14). h is initial-
ized to the optimal policy (step 2) which is derived
from the gold standard. In each iteration SEARN
?corrupts? the optimal policy with the learned hy-
potheses. Thus, each hnew is adapted to the actions
chosen by h instead of the optimal policy. The algo-
rithm terminates when the dependence on the latter
becomes insignificant.
Algorithm 1 SEARN
1: Input: labeled instances S , optimal policy pi, CSC
learning algorithm CSCL, loss function `
2: current policy h = pi
3: while h depends significantly on pi do
4: Examples E = ?
5: for s in S do
6: Predict h(s) = y?1 . . . y?T
7: for y?t in h(s) do
8: Extract features ?t = f(s, y?1:t?1)
9: for each possible action yit do
10: Predict y?t+1:T = h(s|y?1:t?1, yit)
11: Estimate cit = `(y?1:t?1, y
i
t, y?t+1:T )
12: Add (?t, ct) to E
13: Learn a classifier hnew = CSCL(E)
14: h = ?hnew + (1? ?)h
15: Output: hypothesis h without pi
4 Biomedical event extraction with
SEARN
In this section we describe how we learn the event
extraction decomposition described in Sec. 2 under
SEARN. Each instance is a sentence and the hypoth-
esis learned in each iteration consists of a classifier
for each stage of the pipeline, excluding event con-
struction which is rule-based.
SEARN allows us to extract structural features for
each action from the previous ones. During trig-
ger recognition, we add as features the combination
of the lemma of the current token combined with
the event type (if any) assigned to the previous and
the next token, as well as to the tokens that have
syntactic dependencies with it. During Theme as-
signment, when considering a trigger-argument pair,
we add features based on whether the pair forms an
undirected cycle with previously predicted Themes,
whether the trigger has been assigned a protein as a
Theme and the candidate Theme is an event trigger
(and the reverse), and whether the argument is the
Theme of a trigger with the same event type. We
also add a feature indicating whether the trigger has
three Themes predicted already. During Cause as-
signment, we add features representing whether the
trigger has been assigned a protein as a Cause and
the candidate Cause is an event trigger.
Since the features extracted for an action depend
on previous ones, we need to define a prediction or-
der for the actions. In trigger recognition, we pro-
cess the tokens from left to right since modifiers
appearing before nouns tend to affect the meaning
of the latter, e.g. ?binding activity?. In Theme
and Cause assignment, we predict trigger-argument
pairs in order of increasing dependency path length,
assuming that, since they are the main source of fea-
tures in these stages and shorter paths are less sparse,
pairs containing shorter ones should be predicted
more reliably. The loss function sums the number of
false positive and false negative events, which is the
evaluation measure of the shared task. The optimal
policy is derived from the gold standard and returns
the action that minimizes the loss over the sentence
given the previous actions and assuming that all fu-
ture actions are optimal.
In step 11 of Alg. 1, the cost of each action is esti-
mated over the whole sentence. While this allows us
to take structure into account, it can result in costs
being affected by a part of the output that is not re-
lated to that action. This is likely to occur in event
extraction, as sentences can often be long and con-
tain disconnected event components in their output
graphs. For this reason we use focused costing (Vla-
chos and Craven, 2011), in which the cost estimation
for an action takes into account only the part of the
output graph connected with that action.
38
pipeline (R/P/F) SEARN (R/P/F)
trigger 49.1 64.0 55.6 83.2 28.6 42.6
Theme 43.7 78.6 56.2 63.8 72.0 67.6
Cause 13.9 61.0 22.6 33.9 53.8 41.6
Event 31.7 70.1 43.6 45.8 60.51 52.1
Table 1: Results on the development dataset.
5 Experiments
In our experiments, we perform multiclass CSC
learning using our implementation of the on-
line passive-aggressive (PA) algorithm proposed by
Crammer et al (2006). The aggressiveness param-
eter and the number of rounds in parameter learn-
ing are set by tuning on 10% of the training data
and we use the variant named PA-II with prediction-
based updates. For SEARN, we set the interpolation
parameter ? to 0.3. For syntactic parsing, we use
the output of the parser of Charniak and Johnson
(2005) adapted to the biomedical domain by Mc-
Closky (2010), as provided by the shared task orga-
nizers in the Stanford collapsed dependencies with
conjunct dependency propagation (Stenetorp et al,
2011). Lemmatization is performed using morpha
(Minnen et al, 2001). No other knowledge sources
or tools are used.
In order to assess the benefits of joint learning un-
der SEARN, we compare it against a pipeline of in-
dependently learned classifiers using the same fea-
tures and task decomposition. Table 1 reports the
Recall/Precision/F-score achieved in each stage, as
well as the overall performance. SEARN obtains
better performance on the development set by 8.5
F-score points. This increase is larger than the 7.3
points reported in Vlachos and Craven (2011) on
the BioNLP09ST1 datasets which contain only ab-
stracts. This result suggests that the gains of joint
inference under SEARN are greater when learning
from the additional data from full papers. Note
that while the classifier learned with SEARN over-
predicts triggers, the Theme and Cause classifiers
maintain relatively high precision with substantially
higher recall as they are learned jointly with it.
As triggers that do not form events are ignored by
the evaluation, trigger overprediction without event
overprediction does not result in performance loss.
The results of our submission on the test
dataset using SEARN were 42.6/61.2/50.2
(Recall/Precision/F-score) which ranked sixth
in the shared task. In the Regulation events which
are considered harder due to nesting, our submis-
sion was ranked fourth. This demonstrates the
potential of SEARN for structured prediction, as the
performance on regulation events depends partly on
the performance on the simple ones on which our
submission was ranked eighth.
After the end of the shared task, we experimented
with the domain adaptation method proposed by
Daume? III (2007), which creates multiple versions
for each feature by conjoining it with the domain la-
bel of the instance it is extracted from (abstracts or
full papers). While this improved the performance
of the pipeline baseline by 0.3 F-score points, the
performance under SEARN dropped by 0.4 points
on the development data. Using the online service
provided by the organizers, we evaluated the perfor-
mance of the domain adapted SEARN-based system
on the test set and the overall performance improved
to 50.72 in F-score (would have ranked 5th). In
particular, domain adaptation improved the perfor-
mance on full papers by 1.22 points, thus reaching
51.22 in F-score. This version of the system would
have ranked 3rd overall and 1st in the Regulation
events in this part of the corpus. We hypothesize
that these mixed results are due to the sparse fea-
tures used in the stages of the event extraction de-
composition, which become even sparser using this
domain adaptation method, thus rendering the learn-
ing of appropriate weights for them harder.
6 Conclusions
We presented a joint inference approach to the
BioNLP11ST-GE1 task using SEARN which con-
verts a structured prediction task into a set of CSC
tasks whose models are learned jointly. Our results
demonstrate that SEARN achieves substantial per-
formance gains over a standard pipeline using the
same features.
Acknowledgments
The authors were funded by NIH/NLM grant R01
LM07050.
39
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Hal Daume? III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75:297?325.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
256?263.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of the Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 813?821.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task.
Andreas Vlachos and Mark Craven. 2011. Search-based
structured prediction applied to biomedical event ex-
traction. In Proceedings of the Fifteenth Conference
on Computational Natural Language Learning.
40
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 35?42,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Evaluating unsupervised learning for natural language processing tasks
Andreas Vlachos
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
vlachos@biostat.wisc.edu
Abstract
The development of unsupervised learning
methods for natural language processing tasks
has become an important and popular area
of research. The primary advantage of these
methods is that they do not require annotated
data to learn a model. However, this advan-
tage makes them difficult to evaluate against
a manually labeled gold standard. Using un-
supervised part-of-speech tagging as our case
study, we discuss the reasons that render this
evaluation paradigm unsuitable for the evalu-
ation of unsupervised learning methods. In-
stead, we argue that the rarely used in-context
evaluation is more appropriate and more infor-
mative, as it takes into account the way these
methods are likely to be applied. Finally, bear-
ing the issue of evaluation in mind, we pro-
pose directions for future work in unsuper-
vised natural language processing.
1 Introduction
The development of unsupervised learning methods
for natural language processing (NLP) tasks has be-
come an important and popular area of research. The
main attraction of these methods is that they can
learn a model using only unlabeled data. This is
an important advantage, as unlabeled text in digi-
tal form is in abundance, while labeled datasets are
usually expensive to construct. While methods such
as crowdsourcing (Snow et al, 2008) can help re-
duce this cost, in tasks for which specialist knowl-
edge is required, such as part-of-speech (PoS) tag-
ging or syntactic parsing, labeling datasets in this
fashion can be substantially harder.
Nevertheless, the advantage of requiring only un-
labeled data to learn a model renders the evaluation
of unsupervised learning methods to be more chal-
lenging than that of their supervised counterparts.
This is primarily because the output of unsupervised
methods does not contain labels that would be found
in a manually constructed gold standard. Simplisti-
cally expressed, no labels for model learning means
that there are no labels in the output. As a result, the
standard evaluation paradigm of comparing against
a gold standard using a performance measure such
as accuracy or F-score cannot be used, at least not
in the way it would be used in evaluating supervised
methods. Since methods are proposed or rejected
by researchers, and papers describing these methods
are assessed by their peers partly on the basis of such
results, the issue of evaluation is an important one.
Before we proceed, it is important to character-
ize the unsupervised learning methods we are con-
sidering, as the term unsupervised is used in mul-
tiple ways in the literature. In this work we focus
on methods that use only unlabeled data to learn a
model and do not involve any form of supervision at
any stage. Thus we exclude methods that use seeds
such as the dictionaries of PoS tags used by Ravi and
Knight (2009) and rules for producing labeled out-
put, e.g. those proposed by Teichert and Daume? III
(2009). We also exclude methods for which the data
used to learn a model does not contain any of the
labels we are learning to predict, but it does contain
other information that we use in the learning pro-
cess. For example, the multilingual PoS induction
approach of Das and Petrov (2011) assumes no su-
pervision for the language whose PoS tags are being
35
induced, but it assumes access to a labeled dataset of
a different language.
We begin by surveying recent work on unsuper-
vised PoS tagging, focusing on the issue of eval-
uation (Section 2). While PoS tagging is not the
only task for which unsupervised learning methods
are popular, its relative simplicity and the variety
of evaluation paradigms employed make it a use-
ful case study. Based on this survey, we show that
evaluation against a PoS tagging gold standard is
not only difficult, but it can be misleading as well.
The reason for this is that the unsupervised learn-
ing methods used, while they produce output that
correlates with PoS tags, perform a different task,
namely clustering-based word representation induc-
tion (Turian et al, 2010). Instead, we argue that
in-context evaluation is more appropriate and more
informative, as it takes into account the application
context in which these methods are intended to be
used (Section 3). Finally, bearing the issue of evalu-
ation in mind, we propose some directions for future
work in unsupervised learning for NLP (Section 4).
2 The case of unsupervised part-of-speech
tagging
PoS tagging is the task of assigning lexical cate-
gories such as noun or verb to tokens in a sentence.
It is commonly used either as an end-goal or as inter-
mediate processing stage for a downstream task such
as syntactic parsing. For languages with substan-
tial amounts of labeled data available such as En-
glish, the performance of supervised approaches has
reached very high levels.1 Thus, the research focus
has shifted to semisupervised and unsupervised ap-
proaches which would allow the processing of lan-
guages which do not have similar resources avail-
able.
At an abstract level, the unsupervised learning
methods applied to PoS tagging take as input tok-
enized unlabeled sentences, from which they learn
a model. These models are either hidden Markov
models (HMMs) (Clark, 2003; Goldwater and Grif-
fiths, 2007) or clustering models (Biemann, 2006;
Abend et al, 2010). During model learning, state
identifiers are assigned to the tokens (Figure 1a). In-
1According to the ACL wiki, state-of-the-art performance in
English is more than 97% per token accuracy.
dependently of the learning method and the model,
these identifiers are semantically void, i.e. they have
no linguistic meaning. Nevertheless, all the studies
conclude that there is a strong correlation between
the state identifiers assigned and the PoS tags in a
labeled gold standard (Figure 1b).
The most common way of assessing the level of
correlation achieved is the use clustering evaluation
measures. The latter operate on a confusion matrix
(Figure 1c), which is constructed by assuming that
each cluster consists of all the tokens assigned the
same state identifier. Intuitively, all clustering eval-
uation measures provide definitions for the two de-
sirable properties that a good clustering should pos-
sess with respect to a gold standard, homogeneity
and completeness. Homogeneity represents the de-
gree to which each cluster contains instances from a
single gold standard class, while completeness the
degree to which each gold standard class is con-
tained in a single cluster. Note that there tends to
be a trade-off between these two properties since,
increasing the number of clusters is likely to im-
prove homogeneity but worsen completeness and
vice-versa. Therefore, clustering evaluation mea-
sures need to balance appropriately between them.
Some authors proposed clustering evaluation
techniques that first induce the mapping from
state identifiers to gold standard tags automatically
and then use supervised measures to compare the
mapped output to the gold standard. For example,
Gao and Johnson (2008) proposed to induce a many-
to-one mapping of state identifiers to PoS tags from
one half of the corpus and evaluate on the second
half, which is referred to as cross-validation accu-
racy. However, such techniques evaluate the clus-
tering together with the induced mapping, thus the
quality of the latter influences the results obtained.
This can be misleading as unsupervised learning
methods for PoS tagging induce the clustering, but
not the mapping on which they are eventually eval-
uated.
In order to avoid the mapping induction step,
the use of information theoretic measures was pro-
posed instead. These include Variation of Informa-
tion (VI) (Meila?, 2007), V-measure (Rosenberg and
Hirschberg, 2007), and their respective variants NVI
(Reichart and Rappoport, 2009) and V-beta (Vla-
chos et al, 2009). Each of these measures exhibits
36
There are 70 children there .
1 2 3 4 1 5
(a) Unsupervised PoS tagger output
There are 70 children there .
EX VBP CD NNS RB .
(b) Gold standard
1 2 3 4 5
EX 1 0 0 0 0
VBP 0 1 0 0 0
CD 0 0 0 1 0
NNS 0 0 1 0 0
RB 1 0 0 0 0
. 0 0 0 0 1
(c) Confusion matrix
Figure 1: Unsupervised PoS tagging evaluation pipeline.
some kind of bias towards certain solutions though,
e.g. V-measure favors clusterings with large number
of clusters, while VI exhibits the opposite behavior.
While these biases might follow some reasonable in-
tuitions, unsurprisingly none is universally accepted
as the most appropriate.
In order to avoid these problems, Biemann et al
(2007) proposed to evaluate unsupervised PoS tag-
ging as a source of features for supervised learn-
ing approaches to NLP tasks, such as named entity
recognition and shallow parsing. The intuition be-
hind this extrinsic evaluation is that if a task relies
on discriminating between PoS labels rather than the
PoS labels semantics themselves, then the state iden-
tifiers obtained by an unsupervised method can be
used in the same way as PoS tags obtained from
a gold standard or a supervised system. In their
experiments they showed that the features obtained
from the unsupervised PoS tagger improve the per-
formance in all tasks, and in particular when little
training data is available.
Van Gael et al (2009) evaluated the output of dif-
ferent configurations of their unsupervised PoS tag-
ging approach both by comparing it against a gold
standard via clustering evaluation measures and by
using it as a source of features for shallow pars-
ing. Table 1 summarizes the results of their exper-
iments. In agreement with Biemann et al (2007),
they found that the features provided by the unsu-
pervised PoS tagger improved shallow parsing per-
formance. However, they observed that the cluster-
ing evaluation scores did not correlate with the re-
sults of this extrinsic evaluation. In other words,
better clustering evaluation scores did not always
result in better features for shallow parsing. Van
Gael et al noted that homogeneity correlated bet-
ter with shallow parsing performance, hypothesizing
it is probably worse to assign the same state identi-
fier to tokens that belong to different PoS tags, e.g.
verb and adverbs, rather than to generate more than
one state identifier for the same PoS. In the same
spirit, Christodoulopoulos et al (2010) used the out-
put of a number of unsupervised PoS tagging meth-
ods to extract seeds for the prototype-driven model
of Haghighi and Klein (2006). Like Van Gael et
al., they also found that better clustering evaluation
scores did not result in better seeds.
Given these results, as well as remembering that
unsupervised learning methods do not use any la-
bel information in model learning, one is entitled
to question whether it is reasonable to expect their
output to match a particular labeled gold standard.
Why not assume that the state identifiers obtained
correlate with named entity recognition tags or cat-
egorial grammar tags instead of PoS tags, tasks for
which sequential models are very common? Even
if the state identifiers induced correlate better with
PoS tags than with other kinds of annotation, eval-
uating them using a PoS tagging gold standard and
even naming the task unsupervised PoS tagging or
induction is probably misleading. We argue that
the task performed by the unsupervised PoS tag-
ging methods proposed is more accurately described
as clustering-based word representation induction
37
homogeneity completeness VI V-measure V-beta F-score accuracy
DP-learned 69.39 51.21 4.19 58.93 55.37 90.98 94.48
DP-fixed 51.80 54.84 3.94 53.27 52.88 89.99 93.89
PY-fixed 62.02 56.25 3.74 59.00 58.79 90.31 94.15
no PoS - - - - - 93.81 96.07
supervised PoS - - - - - 88.58 93.25
Table 1: Summary the results reported for the three configurations (DP-learned, DP-fixed, PY-fixed) of the
unsupervised PoS tagger of Van Gael et al (2009) and the two baselines (no PoS tags, supervised PoS tags).
Except for VI, higher scores mean better performance. The clustering evaluation scores (VI, V-measure, V-
beta) are obtained by comparing against a PoS gold standard, while F-score and accuracy scores are obtained
by extrinsinc evaluation using shallow parsing.
(Turian et al, 2010), and that this should be taken
into account in the evaluation. As further evidence
of the relation between the two tasks, note that some
of the unsupervised PoS tagging methods applied by
Christodoulopoulos et al (2010) were also used by
Turian et al (2010) for clustering-based word repre-
sentation induction.
3 In-context evaluation
All the papers on unsupervised PoS tagging men-
tioned in the previous section agree on the fact that
its evaluation, at least using clustering evaluation
measures, is difficult. This is an important problem
for other NLP tasks (e.g. anaphora resolution, word
sense induction) in which systems produce clusters
that need to be mapped to gold standard classes. In
their recent position paper, Guyon et al (2009) argue
that the problem lies in ignoring the context in which
clustering is performed. They distinguish between
two such contexts. The first one is the use of cluster-
ing as a pre-processing step for a downstream task,
in which the evaluation of the latter is used to eval-
uate the former. The second context is that of data
exploration in order to assist a human to analyze a
large dataset. In this case, performance might not be
as straightforward to assess, since it relies on many
external factors among which the human computer
interaction interface used is likely to be crucial. We
cumulatively refer to these evaluation paradigms as
in-context evaluation.
Returning to unsupervised PoS tagging and NLP,
the extrinsic evaluation of Biemann et al (2007) and
Van Gael et al (2009) falls under the pre-processing
paradigm. The approach of Christodoulopoulos et
al. (2010) falls between pre-processing and data ex-
ploration, as the clusters of tokens produced are
semi-automatically processed in order to produce
seeds which were then used by the prototype-driven
model of Haghighi and Klein (2006).2 In-context
evaluation can be used to assess the performance of
unsupervised learning methods for tasks other than
clustering-based word representation approaches.
For example, topic modeling (Blei et al, 2003) has
recently been used and evaluated in approaches to
learning models of selectional preferences (Ritter et
al., 2010; O? Se?aghdha, 2010).
The issues affecting the evaluation of unsuper-
vised learning methods are not restricted to PoS tag-
ging. Schwartz et al (2011) discussed similar issues
in the context of unsupervised dependency parsing.
Note that some of them arise due to the fact unsu-
pervised dependency parsing produces unlabeled di-
rected edges which are interpreted as denoting head-
dependent relations. However, there are linguistic
phenomena where unless the edges are labeled with
a specific interpretation, both directions could be
considered correct, e.g. the relation between modal
verb and main verb. Even though evaluation against
a syntactic parsing gold standard is useful, we argue
that in-context evaluation of the output of unsuper-
vised dependency parsers is likely to be more infor-
mative and more appropriate.
Despite the criticism against clustering evaluation
measures as well as other methods for comparing the
2Note that while evaluating in-context, these authors still re-
fer to the task performed as PoS tagging or induction and some
of their conclusions are drawn via comparisons against a PoS
tagging gold standard.
38
output of unsupervised learning methods against a
gold standard, we argue that they are still useful. The
various measures proposed, along with their inher-
ent biases and definitions of clustering quality, pro-
vide quantitative analysis of the behavior of unsu-
pervised learning methods by assessing correlations
between their output and a gold standard. This can
be very useful when developing such methods, as
their use is admittedly simpler than the in-context
evaluation paradigms discussed. However, they are
not as informative as in-context evaluation and they
should not be used to draw strong conclusions about
the usefulness of a method.
Acknowledging that the evaluation of unsuper-
vised learning for NLP is better performed in-
context instead of against a labeled gold standard
leads to the use of more appropriate experimen-
tal setups. Sometimes unsupervised learning meth-
ods are restricted to learning models using the un-
labeled gold standard against which they are evalu-
ated subsequently. Thus, they neither take full ad-
vantage of nor they demonstrate their main strength,
which is that they can use as much data as possi-
ble. Using the pre-processing paradigm, clustering-
based word representations induced from a large
unlabeled dataset would be evaluated according to
whether they improve the performance of the down-
stream task they are evaluated with, whose evalua-
tion is likely to be on a different dataset. This use of
clustering-based word representation is sometimes
referred to as semi-supervised learning and has been
shown to be effective in a variety of tasks, including
named entity recognition, shallow parsing and syn-
tactic dependency parsing (Koo et al, 2008; Turian
et al, 2010).
The use of large datasets would also help as-
sess the scalability of the unsupervised methods pro-
posed, as the amount of data that can be handled ef-
ficiently by an unsupervised method can be as im-
portant as the range of linguistic intuitions it can
capture. To examine this trade-off, it would be in-
formative to show performance curves with differ-
ent amounts of data, which should be straightfor-
ward to produce under the pre-processing evalua-
tion paradigm. An added benefit is that, as discussed
by Ben-Hur et al (2002), assessing clustering stabil-
ity using multiple runs and sub-samples of a dataset
can help establish whether a particular combination
of clustering algorithm and user-defined parameters
(including the number of clusters to be discovered)
is able to discover an appropriate clustering of the
dataset considered.
Avoiding comparisons against a labeled gold stan-
dard would also remove the temptation of adapting
it to the output of the unsupervised learning method.
For example, in unsupervised PoS tagging authors
sometimes simplify the gold standard by collapsing
the original 45 PoS tags of the Penn treebank to 17,
e.g. by removing the distinctions between different
noun tags. While such simplifications are linguisti-
cally plausible, they substitute one problem for an-
other, as methods are no longer penalized for miss-
ing some of the finer distinctions, but they are pe-
nalized for making them. Perhaps more importantly,
they result in fitting the gold standard to the output
of the method being evaluated, which is unlikely to
be informative.
Another related issue is that since unsupervised
learning methods do not need labeled data, it is a
tempting and common practice to learn a model and
report results on the same dataset, which usually
consists of all the labeled data available and which
is used to tune the parameters of the method evalu-
ated. This is equivalent to reporting results for su-
pervised learning methods on the development set,
while it is generally accepted that results on a sepa-
rate test set on which no parameter tuning is allowed
provide better performance estimates. The use of
the pre-processing evaluation paradigm with a su-
pervised learning approach for the downstream task
is likely to result in use the standard distinction be-
tween training, development and test set for the eval-
uation of unsupervised learning methods.
4 Directions for future work
While the previous sections have focused on why
unsupervised learning for NLP tasks is hard to eval-
uate, our intention is not to discourage further re-
search, but to encourage it. Unsupervised learning
can help exploit the large amounts of unlabeled text
that are available. For this purpose though we need
appropriate evaluation, and we argue that in-context
evaluation is likely to be more informative than the
evaluation against a gold standard.
A potential problem is that in-context evaluation
39
adds an extra layer in the experimental setup, either
in the form of a downstream task or of a human-
computer interaction study. This can make compar-
isons between methods harder as there are more ex-
perimental conditions to control for and discourage
researchers from adopting it. Therefore, it would
be useful to have a shared task that would provide
an experimental setup that can be re-used. Shared
tasks have been beneficial in cases where the exis-
tence of multiple datasets and task definitions hin-
dered progress and we would expect them to have a
similar effect on unsupervised learning methods.
As different application contexts are likely to ben-
efit from different solutions, this naturally leads to
the development of modeling approaches that are
adaptable, preferably in ways that enable experts to
incorporate their knowledge. This research direction
has already been pursued in clustering (Wagstaff and
Cardie, 2000; Basu et al, 2006) and more recently
in topic modeling (Blei and Mcauliffe, 2008; An-
drzejewski et al, 2011). We argue though that the
wider adoption of in-context evaluation will help as-
sess their performance and merits in a more infor-
mative way. An alternative approach to accommo-
date for the needs of different application contexts
is to induce multiple clusterings simultaneously for
the same dataset as proposed by Dasgupta and Ng
(2010) in the context of text classification. Such con-
siderations are particularly relevant to NLP applica-
tions as language exhibits ambiguity and polysemy,
which are rather difficult to capture in a context-
independent labeled gold standard.
If in-context evaluation must be avoided, it is ad-
visable to focus on tasks for which most applica-
tion contexts would agree on the clustering or latent
structure that must be discovered, such as the Web
People Search (Artiles et al, 2010) task on clus-
tering webpages about persons who share the same
name. Even in this case though, in-context evalua-
tion as pre-processing for an information extraction
system or as a visualization component in an inter-
face for exploring web pages is still likely to be in-
formative.
Finally, in this paper we considered methods
whose output consists of state identifiers which are
semantically void. However, obtaining meaningful
labels such as those found in a gold standard is a
useful and important goal in many NLP tasks. How-
ever, this purpose is better served by injecting ap-
propriate supervision to the model, instead of trying
to achieve it as an afterthought. Such approaches in-
clude the use of PoS dictionaries by sequential tag-
ging models (Haghighi and Klein, 2006; Ravi and
Knight, 2009), the use of labeled data from differ-
ent languages (Snyder et al, 2008; Das and Petrov,
2011) or the (possibly indirect) assignment of labels
to topics (Ramage et al, 2009; Zhu et al, 2009). Re-
search in unsupervised learning methods is likely to
benefit these partially supervised ones, as they both
seek to take advantage of unlabeled data. As the out-
put of such methods uses the same labels as those
found in the gold standard, they can be evaluated
against a labeled gold standard.
5 Conclusions
In this position paper, we discussed the issue of eval-
uation of unsupervised learning methods for NLP
tasks. Using PoS tagging as our case study, we ex-
amined recent attempts of evaluating unsupervised
approaches and showed that a lot of confusion is
caused due to evaluating their output against a la-
beled gold standard. Instead, we argue that it is
more appropriate to evaluate unsupervised meth-
ods in context, either as a pre-processing step for
a downstream task or as a tool for data exploration.
Following this, we proposed that future work should
focus on adapting to and evaluating unsupervised
learning methods in the context in which they are
intended to be used and that a shared task would fa-
cilitate research in this direction. Finally, we hope
that the adoption of in-context evaluation will result
in the development of improved unsupervised learn-
ing methods for NLP tasks, so that researchers and
practitioners can exploit the large amounts of textual
data available.
Acknowledgments
The author would like thank Mark Craven and Diar-
muid O? Se?aghdha for helpful comments and discus-
sions. The author was funded by NIH/NLM grant
R01 / LM07050.
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2010.
Improved unsupervised POS induction through pro-
40
totype discovery. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1298?1307.
David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorporat-
ing general domain knowledge into latent Dirichlet al
location using first-order logic. In Proceedings of the
22nd International Joint Conferences on Artificial In-
telligence.
Javier Artiles, Andrew Borthwick, Julio Gonzalo, Satoshi
Sekine, and Enrique Amigo?. 2010. WePS-3 evalu-
ation campaign: Overview of the web people search
clustering and attribute extraction tasks. In Proceed-
ings of the Conference on Multilingual and Multi-
modal Information Access Evaluation.
Sugato Basu, Mikhail Bilenko, Arindam Banerjee, and
Raymond J. Mooney. 2006. Probabilistic semi-
supervised clustering with constraints. In O. Chapelle,
B. Schoelkopf, and A. Zien, editors, Semi-Supervised
Learning, pages 73?102. MIT Press.
Asa Ben-Hur, Andre? Elisseeff, and Isabelle Guyon.
2002. A stability based method for discovering struc-
ture in clustered data. In Proceedings of the Pacific
Symposium on Biocomputing, pages 6?17.
Chris Biemann, Claudio Giuliano, and Alfio Gliozzo.
2007. Unsupervised part-of-speech tagging support-
ing supervised methods. In Proceedings of the In-
ternational Conference in Recent Advances in Natural
Language Processing.
Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics: Student Re-
search Workshop, pages 7?12.
David Blei and Jon Mcauliffe. 2008. Supervised topic
models. In Proceedings of the 22nd Annual Confer-
ence on Neural Information Processing Systems.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022, January.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: how far have we come? In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 575?584.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the 10th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 59?66.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies.
Sajib Dasgupta and Vincent Ng. 2010. Mining clustering
dimensions. In Proceedings of the 27th International
Conference on Machine Learning, pages 263?270.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
Bayesian estimators for unsupervised hidden Markov
model POS taggers. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 344?352.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 744?
751.
Isabelle Guyon, Ulrike Von Luxburg, and Robert C.
Williamson. 2009. Clustering: Science or art. In
NIPS 2009 Workshop on Clustering Theory.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 320?327.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion of Computational Linguistics: Human Language
Technologies, pages 595?603.
Marina Meila?. 2007. Comparing clusterings?an infor-
mation based distance. Journal of Multivariate Analy-
sis, 98(5):873?895.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435?444.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: a su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 248?256.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP,
pages 504?512.
Roi Reichart and Ari Rappoport. 2009. The NVI clus-
tering evaluation measure. In Proceedings of the
13th Conference on Computational Natural Language
Learning, pages 165?173.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional prefer-
ences. In Proceedings of the 48th Annual Meeting of
41
the Association for Computational Linguistics, pages
424?434.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 410?420.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing eval-
uation. In Proceedings of the 49th Annual Meeting of
the Association of Computational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing, pages 254?263.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2008. Unsupervised multilingual
learning for pos tagging. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1041?1050.
Adam R. Teichert and Hal Daume? III. 2009. Unsuper-
vised part of speech tagging without a lexicon. In
NIPS Workshop on Grammar Induction, Representa-
tion of Language and Language Learning.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 384?394.
Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahra-
mani. 2009. The infinite HMM for unsupervised
PoS tagging. In Proceedings of 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 678?687.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and Constrained Dirichlet
Process Mixture Models for Verb Clustering. In Pro-
ceedings of the EACL workshop on GEometrical Mod-
els of Natural Language Semantics.
Kiri Wagstaff and Claire Cardie. 2000. Clustering with
instance-level constraints. In Proceedings of the 17th
International Conference on Machine Learning, pages
1103?1110.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009.
MedLDA: maximum margin supervised topic models
for regression and classification. In Proceedings of
the 26th Annual International Conference on Machine
Learning, pages 1257?1264.
42
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 18?22,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Fact Checking: Task definition and dataset construction
Andreas Vlachos
Dept. of Computer Science
University College London
London, United Kingdom
a.vlachos@cs.ucl.ac.uk
Sebastian Riedel
Dept. of Computer Science
University College London
London, United Kingdom
s.riedel@ucl.ac.uk
Abstract
In this paper we introduce the task of fact
checking, i.e. the assessment of the truth-
fulness of a claim. The task is commonly
performed manually by journalists verify-
ing the claims made by public figures. Fur-
thermore, ordinary citizens need to assess
the truthfulness of the increasing volume
of statements they consume. Thus, de-
veloping fact checking systems is likely
to be of use to various members of soci-
ety. We first define the task and detail the
construction of a publicly available dataset
using statements fact-checked by journal-
ists available online. Then, we discuss
baseline approaches for the task and the
challenges that need to be addressed. Fi-
nally, we discuss how fact checking relates
to mainstream natural language processing
tasks and can stimulate further research.
1 Motivation
Fact checking is the task of assessing the truth-
fulness of claims made by public figures such
as politicians, pundits, etc. It is commonly per-
formed by journalists employed by news organisa-
tions in the process of news article creation. More
recently, institutes and websites dedicated to this
cause have emerged such as Full Fact
1
and Politi-
Fact
2
respectively. Figure 1 shows two examples
of fact checked statements, together with the ver-
dicts offered by the journalists.
Fact-checking is a time-consuming process. In
assessing the first claim in Figure 1 a journalist
would need to consult a variety of sources to find
1
http://fullfact.org
2
http://politifact.com
the average ?full-time earnings? for criminal bar-
risters. Fact checking websites commonly provide
the detailed analysis (not shown in the figure) per-
formed to support the verdict.
Automating the process of fact checking has re-
cently been discussed in the context of computa-
tional journalism (Cohen et al., 2011; Flew et al.,
2012). Inspired by the recent progress in natural
language processing, databases and information
retrieval, the vision is to provide journalists with
tools that would allow them to perform this task
automatically, or even render the articles ?live? by
updating them with most current data. This au-
tomation is further enabled by the increasing on-
line availability of datasets, survey results, and re-
ports in machine readable formats by various insti-
tutions, e.g. EUROSTAT releases detailed statis-
tics for all European economies.
3
Furthermore, ordinary citizens need to fact
check the information provided to them. This need
is intensified with the proliferation of social media
such as Twitter, since the dissemination of news
and information commonly circumvents the tra-
ditional news channels (Petrovic, 2013). In addi-
tion, the rise of citizen journalism (Goode, 2009)
suggests that often citizens become the sources
of information. Since the information provided
by them is not edited or curated, automated fact
checking would assist in avoiding the spreading
false information.
In this paper we define the task of fact-checking.
We then detail the construction of a dataset using
fact-checked statements available online. Finally,
we describe the challenges it poses and its relation
to current research in natural language processing.
3
http://epp.eurostat.ec.europa.eu/
portal/page/portal/eurostat/home
18
2 Task definition
We define fact-checking to be the assignment of a
truth value to a claim made in a particular con-
text. Thus it is natural to consider it as a bi-
nary classification task. However, it is often the
case that the statements are not completely true
or false. For example, the verdict for the third
claim in Figure 1 is MOSTLYTRUE because some
of the sources dispute it, while in the fourth exam-
ple the statistics can be manipulated to support or
disprove the claim as desired. Therefore it is bet-
ter to consider fact-checking as an ordinal classifi-
cation task (Frank and Hall, 2001), thus allowing
systems to capture the nuances of the task.
The verdict by itself, even if graded, needs to be
supported by an analysis (e.g., what is the systems
interpretation of the statement). However, given
the difficulty of carving out exactly what the cor-
rect analysis for a statement might be, we restrict
the task to be a prediction problem so that we can
evaluate performance automatically.
Context can be crucial in fact-checking. For ex-
ample, knowing that the fourth claim of Figure 1
is made by a UK politician is necessary in order
to assess it using data about this country. Fur-
thermore, time is also important since the vari-
ous comparisons usually refer to time-frames an-
chored at the time a claim is made.
The task is rather challenging. While some
claims such as the one about Crimea can be fact-
checked by extracting relations from WikiPedia,
the verdict often hinges on interpreting relatively
fine points, e.g. the last claim refers to a partic-
ular definition of income. Journalists also check
multiple sources in producing their verdicts, as in
the case of the third claim. Interestingly, they also
consider multiple interpretations of the data; e.g.
in the last claim is assessed as HALFTRUE since
different but reasonable interpretations of the same
data lead to different conclusions.
We consider all of the aspects mentioned (time,
speaker, multiple sources and interpretations) as
part of the task of fact checking. However, we
want to restrict the task to statements that can be
fact-checked objectively, which is not always true
for the statements assessed by journalists. There-
fore, we do not consider statements such as ?New
Labour promised social improvement but deliv-
ered a collapse in social mobility? to be part to
the task since there are no universal definitions of
?social improvement? and ?social mobility?.
4
4
http://blogs.channel4.com/factcheck/
factcheck-social-mobility-collapsed/
Claim (by Minister Shailesh Vara)
?The average criminal bar barrister working full-
time is earning some ?84,000.?
Verdict: FALSE (by Channel 4 Fact Check)
The figures the Ministry of Justice have stressed
this week seem decidedly dodgy. Even if you do
want to use the figures, once you take away the
many overheads self-employed advocates have to
pay you are left with a middling sum of money.
Claim (by U.S. Rep. Mike Rogers)
?Crimea was part of Russia until 1954, when it
was given to the Soviet Republic of the Ukraine.?
Verdict: TRUE (by Politifact)
Rogers said Crimea belonged to Russia until
1954, when Khrushchev gave the land to
Ukraine, then a Soviet republic.
Claim (by President Barack Obama)
?For the first time in over a decade, business
leaders around the world have declared that
China is no longer the world?s No. 1 place to
invest; America is.?
Verdict: MOSTLYTRUE (by Politifact)
The president is accurate by citing one particular
study, and that study did ask business leaders
what they thought about investing in the United
States. A broader look at other rankings doesn?t
make the United States seem like such a power-
house, even if it does still best China in some lists.
Claim (by Chancellor George Osborne)
?Real household disposable income is rising.?
Verdict: HALFTRUE (by Channel 4 Fact Check)
RHDI did grow in latest period we know about
(the second quarter of 2013), making Mr Osborne
arguably right to say that it is rising as we speak.
But over the last two quarters we know about,
income was down 0.1 per cent. If you want to
compare the latest four quarters of data with the
previous four, there was a fall in household
income, making the chancellor wrong. But if
you compare the latest full year of results, 2012,
with 2011, income is up and he?s right again.
Figure 1: Fact-checked statements.
19
3 Dataset construction
In order to construct a dataset to develop and eval-
uate approaches to fact checking, we first surveyed
popular fact checking websites. We decided to
consider statements from two of them, the fact
checking blog of Channel 4
5
and the Truth-O-
Meter from PolitiFact.
6
Both websites have large
archives of fact-checked statements (more than
1,000 statements each), they cover a wide range of
prevalent issues of U.K. and U.S. public life, and
they provide detailed verdicts with fine-grained la-
bels such as MOSTLYFALSE and HALFTRUE.
We examined recent fact-checks from each
website at the time of writing. For each state-
ment, apart from the statement itself, we recorded
the date it was made, the speaker, the label of
the verdict and the URL. As the two websites
use different labelling schemes, we aligned the la-
bels of the verdicts to a five-point scale: TRUE,
MOSTLYTRUE, HALFTRUE, MOSTLYFALSE and
FALSE. The speakers included, apart from pub-
lic figures, associations such as the American Bev-
erage Association, activists, even viral FaceBook
posts submitted by the public.
We then decided which of the statements should
be considered for the task proposed. As discussed
in the previous section we want to avoid state-
ments that cannot be assessed objectively. Follow-
ing this, we deemed unsuitable statements:
? assessing causal relations, e.g. whether a
statistic should be attributed to a particular law
? concerning the future, e.g. speculations involv-
ing oil prices
? not concerning facts, e.g. whether a politician
is supporting certain policies
For the statements that were considered suit-
able, we also collected the sources used by the
journalists in the analysis provided for the verdict.
Common sources include tables with statistics and
reports from governments, think tanks and other
organisations, available online. Automatic identi-
fication of the sources needed to fact check a state-
ment is an important stage in the process, which is
potentially useful in its own right in the context
of assisting journalists in a semi-automated fact-
checking approach Cohen et al. (2011). Some-
16444
5
http://blogs.channel4.com/factcheck/
6
http://www.politifact.com/
truth-o-meter/statements/
times the verdicts relied on data that were not
available online such personal communications;
statements whose verdict relied on such data were
also deemed unsuitable for the task.
As mentioned earlier, the verdicts on the web-
sites are accompanied by lengthy analyses. While
such analyses could be useful annotation for in-
termediate stages of the task ? e.g. we could use
it as supervision to learn how to combine the in-
formation extracted from the various sources into
a verdict ? we noticed that the language used in
them is indicative of the verdict.
7
Thus we decided
not to include them in the dataset, as it would en-
able tackling part of the task as sentiment analy-
sis. Out of the 221 fact-checked statements exam-
ined, we judged 106 as suitable. The dataset col-
lected including our suitability judgements is pub-
licly available
8
and we are working on extending
it so that it can support the development and the
automatic evaluation of fact checking approaches.
4 Baseline approaches
As discussed in Section 2, we consider fact check-
ing as an ordinal classification task. Thus, in the-
ory it would be possible to tackle it as a supervised
classification task using algorithms that learn from
statements annotated with the verdict labels. How-
ever this is unlikely to be successful, since state-
ments such as the ones verified by journalists do
not contain the world knowledge and the temporal
and spatial context needed for this purpose.
A different approach would be to match state-
ments to ones already fact-checked by journalists
and return the label in a K-nearest neighbour fash-
ion.
9
Thus the task is reduced to assessing the se-
mantic similarity between statements, which was
explored in a recent shared task (Agirre et al.,
2013). An obvious shortcoming of this approach
is that it cannot be applied to new claims that have
not been fact-checked, thus it can only be used to
detect repetitions and paraphrases of false claims.
A possible mechanism to extend the coverage of
such an approach to novel statements is to assume
that some large text collection is the source of all
true statements. For example, Wikipedia is likely
7
E.g. part of the analysis of the first claim in Figure 1
reads: ?the full-time figure has the handy effect of stripping
out the very lowest earners and bumping up the average?.
8
https://sites.google.com/site/
andreasvlachos/resources
9
The Truth-Teller by Washington Post (http://
truthteller.washingtonpost.com/) follows this
approach.
20
to contain a statement that would match the sec-
ond claim in Figure 1. However, it would still be
unable to tackle the other claims mentioned, since
they require calculations based on the data.
5 Discussion
The main drawback of the baseline approaches
mentioned (aside from their potential coverage) is
the lack of interpretability of their verdicts, also re-
ferred to as algorithmic accountability (Diakopou-
los, 2014). While it is possible for a natural lan-
guage processing expert to inspect aspects of the
prediction such as feature weights, this tends to
become harder as the approaches become more so-
phisticated. Ultimately, the user of a fact checking
system would trust a verdict only if it is accom-
panied by an analysis similar to the one provided
by the journalists. This desideratum is present in
other tasks such as the recently proposed science
test question answering (Clark et al., 2013).
Cohen et al. (2011) propose that fact checking
is about asking the right questions. These ques-
tions might be database queries, requests for in-
formation to be extracted from textual resources,
etc. For example, in checking the last claim in Fig-
ure 1 a critical reader would like to know what are
the possible interpretations of ?real household dis-
posable income? and what the calculations might
be for other reasonable time spans.
The manual fact checking process suggests an
approach that is more likely to give an inter-
pretable analysis and would decompose the task
into the following stages:
1. extract statements to be fact-checked
2. construct appropriate questions
3. obtain the answers from relevant sources
4. reach a verdict using these answers
The stages of this architecture can be mapped
to tasks well-explored in the natural language pro-
cessing community. Statement extraction could
be tackled as a sentence classification problem,
following approaches similar to those proposed
for speculation detection (Farkas et al., 2010) and
veridicality assessment (de Marneffe et al., 2012).
Furthermore, obtaining answers to questions from
databases is a task typically addressed in the con-
text of semantic parsing research, while obtaining
such answers from textual sources is usually con-
sidered in the context of information extraction.
Finally, the compilation of the answers into a ver-
dict could be considered as a form of logic-based
textual entailment (Bos and Markert, 2005).
However, the fact-checking stages described in-
clude a novel task, namely question construc-
tion for a given statement. This task is likely
to rely on semantic parsing of the statement fol-
lowed by restructuring of the logical form gener-
ated. Since question construction is a rather un-
common task, it is likely to require human supervi-
sion, which could possibly be obtained via crowd-
sourcing. Furthermore, the open-domain nature of
fact checking places greater demands on the estab-
lished tasks of information extraction and seman-
tic parsing. Thus, fact-checking is likely to stim-
ulate research in these tasks on methods that do
not require domain-specific supervision (Riedel et
al., 2013) and are able to adapt to new information
requests (Kwiatkowski et al., 2013).
Fact-checking is related to the tasks of textual
entailment (Dagan et al., 2006) and machine com-
prehension (Richardson et al., 2013), with the dif-
ference that the text which should be used to pre-
dict the entailment of the hypothesis or the correct
answer respectively is not provided in the input.
Instead, systems need to locate the sources needed
to predict the verdict label as part of the task. Fur-
thermore, by defining the task in the context of
real-world journalism we are able to obtain labeled
statements at no annotation cost, apart from the as-
sessment of their suitability for the task.
6 Conclusions
In this paper we introduced the task of fact check-
ing and detailed the construction of a dataset us-
ing statements fact-checked by journalists avail-
able online. In addition, we discussed baseline ap-
proaches that could be applied to perform the task
and the challenges that need to be addressed.
Apart from being a challenging testbed to stim-
ulate progress in natural language processing, re-
search in fact checking is likely to inhibit the in-
tentional or unintentional dissemination of false
information. Even an approach that would return
the sources related to a statement could be very
helpful to journalists as well as other critical read-
ers in a semi-automated fact checking approach.
Acknowledgments
The authors would like to thank the members of
the Machine Reading lab for useful discussions
21
and their help in compiling the dataset.
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity. In Proceedings of
the Second Joint Conference on Lexical and Compu-
tational Semantics and the Shared Task: Semantic
Textual Similarity, pages 32?43, Atlanta, GA.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of the 2005 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2005),
pages 628?635.
Peter Clark, Philip Harrison, and Niranjan Balasubra-
manian. 2013. A study of the knowledge base re-
quirements for passing an elementary science test.
In Proceedings of the 2013 Workshop on Automated
Knowledge Base Construction, pages 37?42.
Sarah Cohen, Chengkai Li, Jun Yang, and Cong Yu.
2011. Computational journalism: A call to arms to
database researchers. In Proceedings of the Confer-
ence on Innovative Data Systems Research, volume
2011, pages 148?151.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Proceedings of the First International
Conference on Machine Learning Challenges: Eval-
uating Predictive Uncertainty Visual Object Classi-
fication, and Recognizing Textual Entailment, pages
177?190.
Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did it happen?
the pragmatic complexity of veridicality assessment.
Computational Linguistics, 38(2):301?333, June.
Nick Diakopoulos. 2014. Algorithmic accountabil-
ity reporting: On the investigation of black boxes.
Technical report, Tow Center for Digital Journalism.
Richard Farkas, Veronika Vincze, Gyorgy Mora, Janos
Csirik, and Gyorgy Szarvas. 2010. The CoNLL
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the CoNLL 2010 Shared Task.
Terry Flew, Anna Daniel, and Christina L. Spurgeon.
2012. The promise of computational journalism.
In Proceedings of the Australian and New Zealand
Communication Association Conference, pages 1?
19.
Eibe Frank and Mark Hall. 2001. A simple approach
to ordinal classification. In Proceedings of the 12th
European Conference on Machine Learning, pages
145?156.
Luke Goode. 2009. Social news, citizen journalism
and democracy. New Media & Society, 11(8):1287?
1305.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545?1556, Seattle,
WA.
Sasa Petrovic. 2013. Real-time event detection in mas-
sive streams. Ph.D. thesis, School of Informatics,
University of Edinburgh.
Matthew Richardson, Christopher J.C. Burges, and
Erin Renshaw. 2013. MCTest: A challenge dataset
for the open-domain machine comprehension of
text. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 193?203, Seattle, WA.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin,
and Andrew McCallum. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, GA.
22
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 1?6,
Dublin, Ireland, August 23 2014.
Application-Driven Relation Extraction with Limited Distant Supervision
Andreas Vlachos
Computer Science Department
University College London
a.vlachos@cs.ucl.ac.uk
Stephen Clark
Computer Laboratory
University of Cambridge
sc609@cam.ac.uk
Abstract
Recent approaches to relation extraction following the distant supervision paradigm have focused
on exploiting large knowledge bases, from which they extract substantial amount of supervision.
However, for many relations in real-world applications, there are few instances available to seed
the relation extraction process, and appropriate named entity recognizers which are necessary for
pre-processing do not exist. To overcome this issue, we learn entity filters jointly with relation
extraction using imitation learning. We evaluate our approach on architect names and building
completion years, using only around 30 seed instances for each relation and show that the jointly
learned entity filters improved the performance by 30 and 7 points in average precision.
1 Introduction
In this paper we focus on relation extraction in the context of a real-world application. The application
is a dialog-based city tour guide, based in Edinburgh. One of the features of the system is its pro-active
nature, offering information which may be of interest to the user. In order to be pro-active in this way,
as well as answer users? questions, the system requires a large amount of knowledge about the city. Part
of that knowledge is stored in a database, which is time-consuming and difficult to populate manually.
Hence, we have explored the use of an automatic knowledge base population technique based on distant
supervision (Craven and Kumlien, 1999; Mintz et al., 2009).
The attraction of this approach is that the only input required is a list of seed instances of the relation in
question and a corpus of sentences expressing new instances of that relation. However, existing studies
typically assume a large seed set, whereas in our application such sets are often not readily available, e.g.
Mintz et al. (2009) reported using 7K-140K seed instances per relation as input. In this paper, the two
relations that we evaluate on are architect name and completion year of buildings. These were chosen
because they are highly relevant to our application, but also somewhat non-standard compared to the
existing literature; and crucially they do not come with a readily-available set of seed instances.
Furthermore, previous approaches typically assume named entity recognition (NER) as a pre-
processing step in order to construct the training and testing instances. However, since these tools are
not tailored to the relations of interest, they introduce spurious entity matches that are harmful to per-
formance as shown by Ling and Weld (2012) and Zhang et al. (2013). These authors ameliorated this
issue by learning fine-grained entity recognizers and filters using supervised learning. The labeled data
used was extracted from the anchor text of entity mentions annotated in Wikipedia, however this is not
possible for entities not annotated in this resource.
In this work, instead of relying on labeled data to construct entity filters, we learn them jointly with the
relation extraction component. For this purpose we use the imitation learning algorithm DAGGER (Ross
et al., 2011), which can handle the dependencies between actions taken in a sequence, and use supervision
for later actions to learn how to take actions earlier in the sequence. We evaluate our approach using
around 30 seed instances per relation and show that the jointly learned entity filters result in gains of 7
and 30 points in average precision for the completion year and the architect name relations respectively.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
relation keywords: building, architect
question answer
Advocates? Library William Playfair
Bute House Robert Adam
Dunstane House ?
Craigiehall ?
sentences
The Advocates? Library is currently located in a William Playfair-
designed building.
Bute House is unusual in Robert Adam?s design for Charlotte Square
in having a central front door.
Dunstane House in Edinburgh was built in 1852 to the design of
architect William Playfair.
The 16-room Dunstane House was originally built by the Ross family
as their private home in 1852.
Dunstane House was designed by famous architect William Playfair.
Craigiehall is a late-17th-century country house, which now serves as
the headquarters of the Second Division of the British Army.
label question candidate sentence
training instances
+ Advocates? Library William Playfair The Advocates? Library. . .
+ Bute House Robert Adam Bute House is unusual. . .
- Bute House Charlotte Square Bute House is unusual. . .
predicted instances
- Dunstane House Edinburgh Dunstane House in. . .
+ Dunstane House William Playfair Dunstane House in. . .
+ Dunstane House Ross The 16-room Dunstane. . .
+ Dunstane House William Playfair Dunstane House was. . .
- Craigiehall Second Division Craigiehall is a . . .
- Craigiehall British Army Craigiehall is a. . .
entity filter
relation extractor
question answer score
Dunstane House William Playfair 2
Ross 1
Craigiehall
WEB
DISTANT SUPERVISION
TRAIN
PREDICT
OUTPUT
Figure 1: The stages of our proposed approach applied to the architect name relation.
2 Approach overview
We will use the architect-building relation as an example to give an overview of our approach, as shown
in Figure 1. The input to the system is a list of buildings, where for some we know the architect (the
seeds), and the task is to find the architects for the remainder. One difference with the standard setup for
relation extraction using distant supervision is that we assume a list of historical buildings instead of a
tailored NER system. This is reasonable for the example, since such a list is relatively easy to acquire.
In order to create training data, queries containing words from the seeds are sent to a search engine.
Sentences from the returned pages are then processed to find examples which contain mentions of both
a building and the corresponding architect. Applying the distant supervision hypothesis, we assume that
such sentences are indeed expressing the desired relation, and these are positive examples. While such
data contains noise, it has been shown to be useful in practice (Yao et al., 2010; Hoffmann et al., 2011).
At test time the input is the name of a historical building. Now the web is searched to find example
sentences containing this name, and the classifier is applied to each sentence, returning either the name
of the architect, or none. Note that different sentences could provide evidence for different architects;
hence assuming only one architect for each building, a procedure is required to decide between the
possible answers (see Sec. 5).
3 Entity Filtering for Relation Extraction
Each relation extraction instance consists of a sentence containing a question entity (e.g. Bute House)
and a candidate answer (e.g. Robert Adam), and the task is to predict whether the answer and question
entity have the relation of interest. The standard approach is to learn a binary classifier (possibly as part
of a more complex model e.g. Hoffmann et al. (2011)) using features that describe each entity as well
as the lexico-syntactic relation between them in the sentence. These commonly include the lexicalized
dependency path from the question entity to the candidate answer, as well as the lemmas on this path. In
this setup, NER assists by filtering the instances generated to those that contain appropriate recognized
entities and by providing features for them.
However, since we do not assume NER in pre-processing, this task becomes harder in our setup,
since the candidate answers are very often inappropriate for the relation at question. A simple way
2
Algorithm 1: Learning with DAGGER
Input: training set S, loss `, CSC learner CSCL
Output: Learned policy H
N
1 CSC Examples E = ?
2 for i = 1 to N do
3 for s in S do
4 Predict y?
1:T
= H
i?1
(s)
5 for y?
t
in pi(s) do
6 Extract features ?
t
= f(s, y?
1:t?1
)
7 foreach possible action y
j
t
do
8 Predict y?
t+1:T
= H
i?1
(s; y?
1:t?1
, y
j
t
)
9 Assess c
j
t
= `(y?
1:t?1
, y
j
t
, y?
t+1:T
)
10 Add (?
t
, c
t
) to E
11 Learn H
i
= CSCL(E)
to incorporate NER-like information is to add the features that would have been used for NER to the
relation extraction features and learn a classifier as above. Such features are commonly extracted from
the candidate answer itself as well as its context. The former include the tokens of the answer, their
lemmas, whether the answer is capitalised, etc. The latter include the words and bigrams preceding
and following the answer, as well as syntactic dependencies between the words denoting the entity and
surrounding lemmas.
However, while these features are likely to be useful, they also render learning relation extraction
harder because they are not directly relevant to the task. For example, the features describing the first
training instance of Fig. 1 would include that the token Playfair is part of the candidate answer and that
the lemma design is part on the syntactic dependency path between the architect and the building, but
only the latter is crucial for the correct classification of this instance. Thus, including the NER features
about the candidate answer can be misleading, especially since they tend to be less sparse than the relation
extraction ones.
Therefore we split the prediction into two binary classification stages: the first stage predicts whether
the candidate answer is appropriate for the relation (entity filtering), and the second one whether the
sentence expresses the relation between the answer and the question entity (relation extraction). If the
prediction for the first stage is negative, then the second stage is not reached. However, we do not have
labels to train a classifier for the entity filtering stage since if an instance is negative this could be either
due to the candidate answer or to the relation expressed in the sentence. We discuss how we overcome
this issue using the algorithm DAGGER (Ross et al., 2011) next.
4 Imitation learning with DAGGER
Imitation learning algorithms such as DAGGER and SEARN (Daum?e III et al., 2009) have been applied
successfully to a variety of structured prediction tasks (Vlachos, 2012; He et al., 2013) due to their
flexibility in incorporating features. In this work we focus on the parameter-free version of DAGGER
and highlight its ability to handle missing labels in the training data. During training, DAGGER converts
the problem of learning how to predict sequences of actions into cost sensitive classification (CSC)
learning. The dependencies between the actions are learned by appropriate generation of CSC examples.
In our case, each instance is predicted by a sequence of two actions: an entity filtering action followed (if
positive) by a relation extraction action. The output is a learned policy, consisting of the binary classifiers
for entity filtering and relation extraction.
Following Alg. 1, in each iteration DAGGER generates training examples using the previous learned
policy H
i?1
to predict the instances (line 4). For each action taken, the cost for each possible action is
estimated by assuming that the action was taken; then the following actions for that instance are predicted
3
Recall-top Precision-top F-score-top Recall-all Precision-all F-score-all
Base 0.28 0.28 0.28 0.9 0.1 0.18
1stage 0.52 0.71 0.6 0.67 0.68 0.675
2stage 0.5 0.68 0.58 0.67 0.67 0.67
Base 0.0 0.0 0.0 0.62 0.002 0.004
1stage 0.15 0.26 0.19 0.23 0.17 0.2
2stage 0.26 0.65 0.37 0.3 0.55 0.39
Table 1: Test set results for the 3 systems on year completed (top) and architect name (bottom).
using H
i?1
(line 8); and the complete sequence of actions is compared against the correct output using
the loss function (line 9). Since the latter is only applied to complete sequences, it does not need to
decompose over individual actions. We define the loss to be 0 when the relation extraction stage is
correct and 1 otherwise. Therefore we do not need to know the labels for entity filtering, but we learn a
classifier for it so that the relation extraction predictions are correct. Finally, the CSC training examples
generated are added (line 10) and a new policy is learnt (line 11).
Since the losses are either 0 or 1, the CSC learning task is equivalent to ordinary classification learning.
To learn the binary classifiers for each stage we implemented the adaptive regularization of weights
(AROW) algorithm (Crammer et al., 2009) which scales to large datasets and handles sparse feature sets
by adjusting the learning rate for each feature. In the first iteration, we do not have a learned policy, thus
we assume a naive entity filter that accepts all candidate answers and a relation extractor that predicts the
correct label.
5 Experiments
The relations used for evaluation are building-architect and building-completion year, for the reasons
given in Sec. 1. For each of the 138 listed historical buildings in Wikipedia,
1
we found the correct
answers, resulting in 60 building-completion year and 68 building-architect pairs. We split the data into
two equal parts for training/development and testing. We then collected relevant web pages querying
the web as described in Sec. 2. The queries were submitted to Bing via its Search API and the top
300 results for each query were obtained. We downloaded the corresponding pages and extracted their
textual content with BoilerPipe (Kohlsch?utter et al., 2010). We then processed the texts using the Stanford
CoreNLP toolkit.
2
We tried to match the question entity with tokens in each of the sentences, allowing
for minor differences in tokenization, whitespace and capitalization. If a sentence contained the question
entity and a candidate answer, we parsed it using the Klein and Manning (2002) parser. The instances
generated were labeled using the distant supervision assumption, resulting in 974K and 4.5M labeled
instances for the completion year and the architect relation, respectively.
We ran experiments with three systems; the jointly learned entity filtering-relation extraction approach
using imitation learning (henceforth 2stage), the one-stage classification approach using the features for
both entity filtering and relation extraction (henceforth 1stage), and a baseline that for each question
entity returns all candidate answers for the relation ranked by the number of times they appeared with
the question entity and ignoring all other information (henceforth Base). Following four-fold cross-
validations experiment on the development data, we used 12 iterations for learning with DAGGER.
Each system returns a list of answers ranked according to the number of instances classified as positive
for that answer. We used two evaluation modes. The first considers only the top-ranked answer (top),
whereas the second considers all answers returned until either the correct one is found or they are ex-
hausted (all). In all we define recall as the number of correct answers over the total number of question
entities, and precision as the chance of finding the correct answer while traversing those returned.
Results by all models are reported for both relations in Table 1. A first observation is that the architect
name relation is substantially harder to extract since all models achieve worse scores than for the com-
pletion year relation. More specifically, Base achieves respectable scores in top mode in completion year
extraction, but it fails completely in architect name. This is due to the existence of many other names
1
http://en.wikipedia.org/wiki/Category:Listed_buildings_in_Edinburgh
2
http://nlp.stanford.edu/software/corenlp.shtml
4
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P
re
ci
si
on
-a
ll
Recall-all
1stage
2stage
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7
P
re
ci
si
on
-a
ll
Recall-all
1stage
2stage
Table 2: Test set precision-recall curves in all mode for year completed (left) and architect name (right).
that appear more frequently together with a building than that of its architect, while the completion year
is sometimes the number most frequently mentioned in the same sentence with the building. In addition,
Base achieves the maximum possible all recall by construction, since if there is a sentence containing the
correct answer for a question entity it will be returned. However this comes at a cost of low precision.
Both the machine-learned models improve upon Base substantially on both datasets, with the 2stage
model being substantially better in architect name extraction, especially in terms of precision. In comple-
tion year extraction the differences are smaller, with 1stage being slightly better. These small differences
are expected since recognizing completion years is much easier than recognizing architect names, thus
learning a separate entity filtering model for them is less likely to be useful. Nevertheless, inspecting
the weights learned by the 2stage model showed that some useful distinctions were learned, e.g. being
preceded by the word ?between? as in ?built between 1849 and 1852? renders a number less likely to be a
completion year. Finally, we examined the quality of the learned models further by generating precision-
recall curves for the all mode by adjusting the classification thresholds used by 1stage and 2stage. As
shown in the plots of Table 2, 2stage achieves higher precision than 1stage at most recall levels for both
relations, with the benefits being more pronounced in the architect name relation. Summarizing these
curves using average precision (Manning et al., 2008), the scores were 0.69 and 0.76 for the comple-
tion year, and 0.21 and 0.51 for the architect, for the 1stage and the 2stage models respectively, thus
confirming the usefulness of separating the entity filtering features from relation extraction.
6 Discussion
While all the buildings considered in our experiments have a dedicated Wikipedia page, only a few had
a sentence mentioning them together with the correct answer in that resource. Also, the architects who
were the correct answers did not always have a dedicated Wikipedia page. Even though combining
a search engine with distant supervision results in a highly imbalanced learning task, it increases the
potential coverage of our system. In this process we rely on the keywords used in the queries in order
to find pages containing the entities intended rather than synonymous ones, e.g. the keyword ?building?
helps avoid extracting sentences mentioning saints instead of churches. Nevertheless, building names
such as churches named after saints were often ambiguous resulting in false positives.
Bunescu and Mooney (2007) also used a small seed set and a search engine, but they collected sen-
tences via queries containing both the question and the answer entities, thus (unreallistically) assuming
knowledge of all the correct answers. Instead we rely on simple heuristics to identify candidate answers.
These heuristics are relation-dependent and different types of answers can be easily accommodated, e.g.
in completed year relation they are single-token numbers. Finally, the entity filters learned jointly with
relation extraction in our approach, while they perform a role similar to NER, they are learned so that
they help avoid relation extraction errors and not to replace an actual NER system.
7 Conclusions
Our application-based setting has placed novel demands on relation extraction system trained with distant
supervision, and in this paper we have shown that reasonable results can be obtained with only around
30 seed examples without requiring NER for pre-processing. Furthermore, we have demonstrated that
learning entity filters and relation extraction jointly improves performance.
5
Acknowledgements
The research reported was conducted while the first author was at the University of Cambridge and
funded by the European Community?s Seventh Framework Programme (FP7/2007-2013) under grant
agreement no. 270019 (SPACEBOOK project www.spacebook-project.eu).
References
Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal su-
pervision. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages
576?583.
Koby Crammer, Alex Kulesza, and Mark Dredze. 2009. Adaptive regularization of weight vectors. In Advances
in Neural Information Processing Systems 22, pages 414?422.
Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge-bases by extracting information from
text sources. In Proceedings of the 7th International Conference on Intelligent Systems for Molecular Biology,
pages 77?86.
Hal Daum?e III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning,
75:297?325.
He He, Hal Daum?e III, and Jason Eisner. 2013. Dynamic feature selection for dependency parsing. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455?1464, Seattle,
October.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics, pages 541?550.
Dan Klein and Chris Manning. 2002. Fast exact inference with a factored model for natural language parsing. In
Advances in Neural Information Processing Systems 15, pages 3?10.
Christian Kohlsch?utter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining, pages
441?450.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity recognition. In Proceedings of the 26th Conference on
Artificial Intelligence, pages 94?100.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. 2008. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP, pages 1003?1011.
St?ephane Ross, Geoffrey J. Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and structured
prediction to no-regret online learning. In 14th International Conference on Artificial Intelligence and Statistics,
pages 627?635.
Andreas Vlachos. 2012. An investigation of imitation learning algorithms for structured prediction. Journal of
Machine Learning Research Workshop and Conference Proceedings, Proceedings of the 10th European Work-
shop on Reinforcement Learning, 24:143?154.
Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extraction with-
out labelled data. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-
ing, pages 1013?1023.
Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zhifang Sui. 2013. Towards accurate
distant supervision for relational facts extraction. In Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers), pages 810?815, Sofia, Bulgaria, August. Association
for Computational Linguistics.
6
