Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 506?513, Prague, June 2007. c?2007 Association for Computational Linguistics
Methods to integrate a language model with semantic  
information for a word prediction component 
Tonio Wandmacher 
Laboratoire d?Informatique (LI) 
Universit? Fran?ois Rabelais de Tours 
3 place Jean-Jaur?s, 41000 Blois, France 
tonio.wandmacher@ 
univ-tours.fr 
Jean-Yves Antoine 
Laboratoire d?Informatique (LI) 
Universit? Fran?ois Rabelais de Tours 
3 place Jean-Jaur?s, 41000 Blois, France 
jean-yves.antoine@ 
univ-tours.fr 
 
Abstract 
Most current word prediction systems make 
use of n-gram language models (LM) to es-
timate the probability of the following word 
in a phrase. In the past years there have 
been many attempts to enrich such lan-
guage models with further syntactic or se-
mantic information. We want to explore the 
predictive powers of Latent Semantic 
Analysis (LSA), a method that has been 
shown to provide reliable information on 
long-distance semantic dependencies be-
tween words in a context. We present and 
evaluate here several methods that integrate 
LSA-based information with a standard 
language model: a semantic cache, partial 
reranking, and different forms of interpola-
tion. We found that all methods show sig-
nificant improvements, compared to the 4-
gram baseline, and most of them to a sim-
ple cache model as well. 
1 Introduction: NLP for AAC systems 
Augmented and Alternative Communication 
(AAC) is a field of research which concerns natural 
language processing as well as human-machine 
interaction, and which aims at restoring the com-
municative abilities of disabled people with severe 
speech and motion impairments. These people can 
be for instance cerebrally and physically handi-
capped persons or they suffer from a locked-in 
syndrome due to a cerebral apoplexy. Whatever the 
disease or impairment considered, oral communica-
tion is impossible for these persons who have in 
addition serious difficulties to control physically 
their environment. In particular, they are not able to 
use standard input devices of a computer. Most of 
the time, they can only handle a single switch de-
vice. As a result, communicating with an AAC sys-
tem consists of typing messages by means of a vir-
tual table of symbols (words, letters or icons) 
where the user successively selects the desired 
items. 
Basically, an AAC system, such as FASTY 
(Trost et al 2005) or SIBYLLE (Schadle et al 2004), 
consists of four components. At first, one finds a 
physical input interface connected to the computer. 
This device is adapted to the motion capacities of 
the user. When the latter must be restricted to a 
single switch (eye glimpse or breath detector, for 
instance), the control of the environment is reduced 
to a mere Yes/No command.  
Secondly, a virtual keyboard is displayed on 
screen. It allows the user to select successively the 
symbols that compose the intended message. In 
SIBYLLE, key selection is achieved by pointing let-
ters through a linear scan procedure: a cursor suc-
cessively highlights each key of the keyboard.  
The last two components are a text editor (to 
write e-mails or other documents) and a speech 
synthesis module, which is used in case of spoken 
communication. The latest version of SIBYLLE 
works for French and German, and it is usable with 
any Windows? application (text editor, web 
browser, mailer...), which means that the use of a 
specific editor is no longer necessary.  
The main weakness of AAC systems results from 
the slowness of message composition. On average, 
disabled people cannot type more than 1 to 5 words 
per minute; moreover, this task is very tiring. The 
use of NLP techniques to improve AAC systems is 
therefore of first importance. 
 
506
  
Figure 1: User interface of the SIBYLLE AAC system  
 
 
Two complementary approaches are possible to 
speed up communication. The first one aims at 
minimizing the duration of each item selection. 
Considering a linear scan procedure, one could for 
instance dynamically reorganize the keyboard in 
order to present the most probable symbols at first. 
The second strategy tries to minimize the number 
of keystrokes to be made. Here, the system tries to 
predict the words which are likely to occur just af-
ter those already typed. The predicted word is then 
either directly displayed after the end of the in-
serted text (a method referred to as ?word comple-
tion?, cf. Boissi?re and Dours, 1996), or a list of N-
best (typically 3 to 7) predictions is provided on the 
virtual keyboard. When one of these predictions 
corresponds to the intended word, it can be selected 
by the user. As can be seen in figure 1, the interface 
of the SIBYLLE system presents such a list of most 
probable words to the user. 
Several approaches can be used to carry out 
word prediction. Most of the commercial AAC sys-
tems make only use of a simple lexicon: in this ap-
proach, the context is not considered. 
On the other hand, stochastic language models 
can provide a list of word suggestions, depending 
on the n-1 (typically n = 3 or 4) last inserted words. 
It is obvious that such a model cannot take into ac-
count long-distance dependencies. There have been 
attempts to integrate part-of-speech information 
(Fazly and Hirst, 2003) or more complex syntactic 
models (Schadle et al 2004) to achieve a better 
prediction. In this paper, we will nevertheless limit 
our study to a standard 4-gram model as a baseline 
to make our results comparable. Our main aim is 
here to investigate the use of long-distance seman-
tic dependencies to dynamically adapt the predic-
tion to the current semantic context of communica-
tion. Similar work has been done by Li and Hirst 
(2005) and Matiasek and Baroni (2003), who ex-
ploit Pointwise Mutual Information (PMI; Church 
and Hanks, 1989). Trnka et al (2005) dynamically 
interpolate a high number of topic-oriented models 
in order to adapt their predictions to the current 
topic of the text or conversation. 
Classically, word predictors are evaluated by an 
objective metric called Keystroke Saving Rate 
(ksr): 
1001 ????
?
???
?
?=
a
p
n k
k
ksr  (1) 
 
with kp, ka being the number of keystrokes 
needed on the input device when typing a message 
with (kp) and without prediction (ka = number of 
characters in the text that has been entered, n = 
length of the prediction list, usually n = 5). As 
507
Trost et al (2005) and Trnka et al (2005), we as-
sume that one additional keystroke is required for 
the selection of a word from the list and that a 
space is automatically inserted afterwards. Note 
also that words, which have already occurred in the 
list, will not reappear after the next character has 
been inserted.  
The perplexity measure, which is frequently 
used to assess statistical language models, proved 
to be less accurate in this context. We still present 
perplexities as well in order to provide comparative 
results. 
2 Language modeling and semantics 
2.1 Statistical Language Models 
For about 10 to 15 years statistical language model-
ing has had a remarkable success in various NLP 
domains, for instance in speech recognition, ma-
chine translation, Part-of-Speech tagging, but also 
in word prediction systems. N-gram based lan-
guage models (LM) estimate the probability of oc-
currence for a word, given a string of n-1 preceding 
words. However, computers have only recently 
become powerful enough to estimate probabilities 
on a reasonable  amount of training data. More-
over, the larger n gets, the more important the prob-
lem of combinatorial explosion for the probability 
estimation becomes. A reasonable trade-off be-
tween performance and number of estimated events 
seems therefore to be an n of 3 to 5, including so-
phisticated techniques in order to estimate the 
probability of unseen events (smoothing methods). 
Whereas n-gram-like language models are al-
ready performing rather well in many applications, 
their capacities are also very limited in that they 
cannot exploit any deeper linguistic structure. 
Long-distance syntactic relationships are neglected 
as well as semantic or thematic constraints. 
In the past 15 years many attempts have been 
made to enrich language models with more com-
plex syntactic and semantic models, with varying 
success (cf. (Rosenfeld, 1996), (Goodman, 2002) 
or in a word prediction task: (Fazly and Hirst, 
2003), (Schadle, 2004), (Li and Hirst, 2005)). We 
want to explore here an approach based on Latent 
Semantic Analysis (Deerwester et al 1990). 
2.2 Latent Semantic Analysis 
Several works have suggested the use of Latent 
Semantic Analysis (LSA) in order to integrate se-
mantic similarity to a language model (cf. Belle-
garda, 1997; Coccaro and Jurafsky, 1998). LSA 
models semantic similarity based on co-occurrence 
distributions of words, and it has shown to be help-
ful in a variety of NLP tasks, but also in the domain 
of cognitive modeling (Landauer et al 1997). 
LSA is able to relate coherent contexts to spe-
cific content words, and it is good at predicting the 
occurrence of a content word in the presence of 
other thematically related terms. However, since it 
does not take word order into account (?bag-of-
words? model) it is very poor at predicting their 
actual position within the sentence, and it is com-
pletely useless for the prediction of function words. 
Therefore, some attempts have been made to inte-
grate the information coming from an LSA-based 
model with standard language models of the n-
gram type.  
In the LSA model (Deerwester et al 1990) a 
word wi is represented as a high-dimensional vec-
tor, derived by Singular Value Decomposition 
(SVD) from a term ? document (or a term ? term) 
co-occurrence matrix of a training corpus. In this 
framework, a context or history h (= w1, ... , wm) 
can be represented by the sum of the (already nor-
malized) vectors corresponding to the words it con-
tains (Landauer et al 1997):  
 
?
=
=
m
i
iwh
1
rr
 (2) 
 
This vector reflects the meaning of the preceding 
(already typed) section, and it has the same dimen-
sionality as the term vectors. It can thus be com-
pared to the term vectors by well-known similarity 
measures (scalar product, cosine).  
2.3 Transforming LSA similarities into prob-
abilities 
We make the assumption that an utterance or a 
text to be entered is usually semantically cohesive. 
We then expect all word vectors to be close to the 
current context vector, whose corresponding words 
belong to the semantic field of the context. This 
forms the basis for a simple probabilistic model of 
LSA: After calculating the cosine similarity for 
each word vector iw
r
 with the vector h
r
 of the cur-
rent context, we could use the normalized similari-
ties as probability values. This probability distribu-
tion however is usually rather flat (i.e. the dynamic 
508
range is low). For this reason a contrasting (or tem-
perature) factor ? is normally applied (cf. Coccaro 
and Jurafsky, 1998), which raises the cosine to 
some power (? is normally between 3 and 8). After 
normalization we obtain a probability distribution 
which can be used for prediction purposes. It is 
calculated as follows: 
 
( )
( )? ?
?
=
k
?
k
?
i
iLSA
hhw
hhwhwP
)(cos),cos(
)(cos),cos()(
min
min rrr
rrr
  (3) 
 
wi is a word in the vocabulary, h is the current con-
text (history) 
iw
r
and h
r
are their corresponding vec-
tors in the LSA space; cosmin( h
r
) returns the lowest 
cosine value measured for h
r
). The denominator 
then normalizes each similarity value to ensure that 
? =nk kLSA hwP 1),( . 
Let us illustrate the capacities of this model by 
giving a short example from the French version of 
our own LSA predictor: 
 
Context: ?Mon p?re ?tait professeur en math?matiques 
et je pense que ? 
 (?My dad has been a professor in mathemat-
ics and I think that ?) 
 
Rank Word P 
1. professeur (?professor?) 0.0117 
2. math?matiques (?mathematics?) 0.0109 
3. enseign? (participle of ?taught?) 0.0083 
4. enseignait (?taught?) 0.0053 
5. mathematicien (?mathematician?) 0.0049 
6. p?re (?father?) 0.0046 
7. math?matique (?mathematics?) 0.0045 
8. grand-p?re (?grand-father?) 0.0043 
9. sciences (?sciences?) 0.0036 
10. enseignant (?teacher?) 0.0032 
Example 1: Most probable words returned by the 
LSA model for the given context. 
 
As can be seen in example 1, all ten predicted 
words are semantically related to the context, they 
should therefore be given a high probability of oc-
currence. However, this example also shows the 
drawbacks of the LSA model: it totally neglects the 
presence of function words as well as the syntactic 
structure of the current phrase. We therefore need 
to find an appropriate way to integrate the informa-
tion coming from a standard n-gram model and the 
LSA approach. 
2.4 Density as a confidence measure 
Measuring relation quality in an LSA space, 
Wandmacher (2005) pointed out that the reliability 
of LSA relations varies strongly between terms. He 
also showed that the entropy of a term does not 
correlate with relation quality (i.e. number of se-
mantically related terms in an LSA-generated term 
cluster), but he found a medium correlation (Pear-
son coeff. = 0.56) between the number of semanti-
cally related terms and the average cosine similar-
ity of the m nearest neighbors (density). The closer 
the nearest neighbors of a term vector are, the more 
probable it is to find semantically related terms for 
the given word. In turn, terms having a high density 
are more likely to be semantically related to a given 
context (i.e. their specificity is higher). 
We define the density of a term wi as follows: 
 
  ?
=
?=
m
j
ijiim wNNw
m
wD
1
))(,cos(1)( rr  (4) 
 
In the following we will use this measure (with 
m=100) as a confidence metric to estimate the reli-
ability of a word being predicted by the LSA com-
ponent, since it showed to give slightly better re-
sults in our experiments than the entropy measure.  
3 Integrating semantic information 
In the following we present several different meth-
ods to integrate semantic information as it is pro-
vided by an LSA model into a standard LM. 
3.1 Semantic cache model 
Cache (or recency promotion) models have shown 
to bring slight but constant gains in language mod-
eling (Kuhn and De Mori, 1990). The underlying 
idea is that words that have already occurred in a 
text are more likely to occur another time. There-
fore their probability is raised by a constant or ex-
ponentially decaying factor, depending on the posi-
tion of the element in the cache. The idea of a de-
caying cache function is that the probability of re-
occurrence depends on the cosine similarity of the 
word in the cache and the word to be predicted. 
The highest probability of reoccurrence is usually 
after 15 to 20 words. 
Similar to Clarkson and Robinson (1997), we im-
plemented an exponentially decaying cache of 
length l (usually between 100 and 1000), using the 
509
following decay function for a word wi and its posi-
tion p in the cache. 
 
2)(5,0
),( ??
???
? ??
=
?
?p
id epwf  (5) 
 
? = ?/3 if p < ? and  ? = l/3 if p ? ?. The func-
tion returns 0 if wi is not in the cache, and it is 1 if 
p = ?. A typical graph for (5) can be seen in figure 
(2). 
 
 
Figure 2: Decay function with ?=20 and l=300. 
 
We extend this model by calculating for each ele-
ment having occurred in the context its m nearest 
LSA neighbors ( ),( ?wNN
occm
r
, using cosine simi-
larity), if their cosine lies above a threshold ?, and 
add them to the cache as well, right after the word 
that has occurred in the text (?Bring your friends?-
strategy). The size of the cache is adapted accord-
ingly (for ?, ? and l), depending on the number of 
neighbors added. This results in the following 
cache function: 
 
),(),()(
1 cos
pwfwwf?wP id
l
i
i
occicache ? ??=    (6) 
 
with l = size of the cache. ? is a constant  con-
trolling the influence of the component (usually ? ? 
0.1/l); wiocc is a word that has already recently oc-
curred in the context and is therefore added as a 
standard cache element, whereas wi is a nearest 
neighbor to wiocc. fcos(wiocc, wi) returns the cosine 
similarity between i
occ
w
r
 and iw
r
, with cos( i
occ
w
r
, iw
r ) 
> ? (Rem: wi with cos( ioccw
r
, iw
r ) ? ? have not been 
added to the cache).  Since cos( iw
r
, iw
r )=1, terms 
having actually occurred before will be given full 
weight, whereas all wi being only nearest LSA 
neighbors to wiocc will receive a weight correspond-
ing to their cosine similarity with wiocc , which is 
less than 1 (but larger than ?). 
fd(wi,p) is the decay factor for the current posi-
tion p of wi in the cache, calculated as shown in 
equation (5).  
3.2 Partial reranking 
The underlying idea of partial reranking is to re-
gard only the best n candidates from the basic lan-
guage model for the semantic model in order to 
prevent the LSA model from making totally im-
plausible (i.e. improbable) predictions. Words be-
ing improbable for a given context will be disre-
garded as well as words that do not occur in the 
semantic model (e.g. function words), because LSA 
is not able to give correct estimates for this group 
of words (here the base probability remains un-
changed). 
For the best n candidates their semantic probability 
is calculated and each of these words is assigned an 
additional value, after a fraction of its base prob-
ability has been subtracted (jackpot strategy). 
For a given context h we calculate the ordered set 
BESTn(h) = <w1, ? , wn>, so that P(w1|h) ? 
P(w2|h) ???P(wn|h) 
For each wi in BESTn(h) we then calculate its 
reranking probability as follows: 
 
)),(()(),cos()( iniiiRR whBestIwDhw?wP ???=
rr
 (7) 
 
? is a weighting constant controlling the overall 
influence of the reranking process, cos( iw
r
, iw
r ) re-
turns the cosine of the word?s vector and the cur-
rent context vector, D(wi) gives the confidence 
measure of wi and I is an indicator function being 
1, iff wi ?BEST(h), and 0 otherwise.  
3.3 Standard interpolation 
Interpolation is the standard way to integrate in-
formation from heterogeneous resources. While for 
a linear combination we simply add the weighted 
probabilities of two (or more) models, geometric 
interpolation multiplies the probabilities, which are 
weighted by an exponential coefficient (0??1?1): 
 
Linear Interpolation (LI): 
 
)()1()()(' 11 isibi wP?wP?wP ??+?=    (8) 
 
510
Geometric Interpolation (GI): 
 
?
=
?
?
?
?
= n
j
?
js
?
jb
?
is
?
ib
i
wPwP
wPwP
wP
1
)11(1
)11(1
)()(
)()()('     (9) 
 
The main difference between the two methods is 
that the latter takes the agreement of two models 
into account. Only if each of the single models as-
signs a high probability to a given event will the 
combined probability be assigned a high value. If 
one of the models assigns a high probability and 
the other does not the resulting probability will be 
lower. 
3.4 Confidence-weighted interpolation 
Whereas in standard settings the coefficients are 
stable for all probabilities, some approaches use 
confidence-weighted coefficients that are adapted 
for each probability. In order to integrate n-gram 
and LSA probabilities, Coccaro and Jurafsky 
(1998) proposed an entropy-related confidence 
measure for the LSA component, based on the ob-
servation that words that occur in many different 
contexts (i.e. have a high entropy), cannot well be 
predicted by LSA. We use here a density-based 
measure (cf. section 2.2), because we found it more 
reliable than entropy in preliminary tests. For inter-
polation purposes we calculate the coefficient of 
the LSA component as follows: 
 
)( ii wD?? ?= , iff D(wi) > 0; 0 otherwise (10) 
 
with ? being a weighting constant to control the 
influence of the LSA predictor. For all experi-
ments, we set ? to 0.4 (i.e. 0 ? ?i ? 0.4), which 
proved to be optimal in pre-tests. 
4 Results 
We calculated our baseline n-gram model on a 44 
million word corpus from the French daily Le 
Monde (1998-1999). Using the SRI toolkit (Stol-
cke, 2002)1 we computed a 4-gram LM over a con-
trolled 141,000 word vocabulary, using modified 
Kneser-Ney discounting (Goodman, 2001), and we 
applied Stolcke pruning (Stolcke, 1998) to reduce 
the model to a manageable size (? = 10-7). 
                                                 
1
 SRI Toolkit: www.speech.sri.com. 
The LSA space was calculated on a 100 million 
word corpus from Le Monde (1996 ? 2002). Using 
the Infomap toolkit2, we generated a term ? term 
co-occurrence matrix for an 80,000 word vocabu-
lary (matrix size = 80,000 ? 3,000), stopwords 
were excluded. After several pre-tests, we set the 
size of the co-occurrence window to ?100. The ma-
trix was then reduced by singular value decomposi-
tion to 150 columns, so that each word in the vo-
cabulary was represented by a vector of 150 di-
mensions, which was normalized to speed up simi-
larity calculations (the scalar product of two nor-
malized vectors equals the cosine of their angle).  
Our test corpus consisted of 8 sections from the 
French newspaper Humanit?, (January 1999, from 
5,378 to 8,750 words each), summing up to 58,457 
words. We then calculated for each test set the key-
stroke saving rate based on a 5-word list (ksr5) and 
perplexity for the following settings3: 
1. 4-gram LM only (baseline) 
2. 4-gram + decaying cache (l = 400) 
3. 4-gram + LSA using linear interpolation 
with ?LSA = 0.11 (LI). 
4. 4-gram + LSA using geometric interpola-
tion, with ?LSA = 0.07 (GI). 
5. 4-gram + LSA using linear interpolation 
and (density-based) confidence weighting 
(CWLI). 
6. 4-gram + LSA using geometric interpola-
tion and (density-based) confidence 
weighting (CWGI). 
7. 4-gram + partial reranking (n = 1000, ? = 
0.001) 
8. 4-gram + decaying semantic cache  
(l = 4000; m = 10; ? = 0.4, ? = 0.0001)  
Figures 3 and 4 display the overall results in terms 
of ksr and perplexity.  
                                                 
2
 Infomap Project: http://infomap-nlp.sourceforge.net/ 
3
 All parameter settings presented here are based on results of 
extended empirical pre-tests. We used held-out development 
data sets that have randomly been chosen from the Humanit? 
corpus.(8k to 10k words each). The parameters being pre-
sented here were optimal for our test sets. For reasons of sim-
plicity we did not use automatic optimization techniques such 
as the EM algorithm (cf. Jelinek, 1990). 
 
511
 Figure 3: Results (ksr5) for all methods tested. 
 
 
 
Figure 4: Results (perplexity) for all methods 
tested. 
 
Using the results of our 8 samples, we performed 
paired t tests for every method with the baseline as 
well as with the cache model. All gains for ksr 
turned out to be highly significant (sig. level < 
0.001), and apart from the results for CWLI, all 
perplexity reductions were significant as well (sig. 
level < 0.007), with respect to the cache results. We 
can therefore conclude that, with exception of 
CWLI, all methods tested have a beneficial effect, 
even when compared to a simple cache model. The 
highest gain in ksr (with respect to the baseline) 
was obtained for the confidence-weighted geo-
metric interpolation method (CWGI; +1.05%), the 
highest perplexity reduction was measured for GI 
as well as for CWGI (-9.3% for both). All other 
methods (apart from IWLI) gave rather similar re-
sults (+0.6 to +0.8% in ksr, and -6.8% to -7.7% in 
perplexity). 
We also calculated for all samples the correla-
tion between ksr and perplexity. We measured a 
Pearson coefficient of -0.683 (Sig. level < 0.0001).  
At first glance, these results may not seem over-
whelming, but we have to take into account that 
our ksr baseline of 57.9% is already rather high, 
and at such a level, additional gains become hard to 
achieve (cf. Lesher et al 2002). 
The fact that CWLI performed worse than even 
simple LI was not expected, but it can be explained 
by an inherent property of linear interpolation: If 
one of the models to be interpolated overestimates 
the probability for a word, the other cannot com-
pensate for it (even if it gives correct estimates), 
and the resulting probability will be too high. In 
our case, this happens when a word receives a high 
confidence value; its probability will then be over-
estimated by the LSA component. 
5 Conclusion and further work 
Adapting a statistical language model with seman-
tic information, stemming from a distributional 
analysis like LSA, has shown to be a non-trivial 
problem. Considering the task of word prediction 
in an AAC system, we tested different methods to 
integrate an n-gram LM with LSA: A semantic 
cache model, a partial reranking approach, and 
some variants of interpolation. 
We evaluated the methods using two different 
measures, the keystroke saving rate (ksr) and per-
plexity, and we found significant gains for all 
methods incorporating LSA information, compared 
to the baseline. In terms of ksr the most successful 
method was confidence-weighted geometric inter-
polation (CWGI; +1.05% in ksr); for perplexity, 
the greatest reduction was obtained for standard as 
well as for confidence-weighted geometric interpo-
lation (-9.3% for both). Partial reranking and the 
semantic cache gave very similar results, despite 
their rather different underlying approach.  
We could not provide here a comparison with 
other models that make use of distributional infor-
mation, like the trigger approach by Rosenfeld 
(1996), Matiasek and Baroni (2003) or the model 
presented by Li and Hirst (2005), based on Point-
wise Mutual Information (PMI). A comparison of 
these similarities with LSA remains to be done.  
Finally, an AAC system has not only the func-
tion of simple text entering but also of providing 
cognitive support to its user, whose communicative 
abilities might be totally depending on it. There-
fore, she or he might feel a strong improvement of 
the system, if it can provide semantically plausible 
predictions, even though the actual gain in ksr 
might be modest or even slightly decreasing. For 
this reason we will perform an extended qualitative 
512
analysis of the presented methods with persons 
who use our AAC system SIBYLLE.  This is one of 
the main aims of the recently started ESAC_IMC 
project. It is conducted at the Functional Reeduca-
tion and Rehabilitation Centre of Kerpape, Brit-
tany, where SIBYLLE is already used by 20 children 
suffering from traumatisms of the motor cortex. 
They appreciate the system not only for communi-
cation but also for language learning purposes. 
Moreover, we intend to make the word predictor 
of SIBYLLE publicly available (AFM Voltaire pro-
ject) in the not-too-distant future.  
Acknowledgements 
This research is partially founded by the UFA 
(Universit? Franco-Allemande) and the French 
foundations APRETREIMC (ESAC_IMC project) 
and AFM (VOLTAIRE project). We also want to 
thank the developers of the SRI and the Infomap 
toolkits for making their programs available. 
References 
Bellegarda, J. (1997): ?A Latent Semantic Analysis 
Framework for Large-Span Language Modeling?, 
Proceedings of the Eurospeech 97, Rhodes, Greece. 
Boissi?re Ph. and Dours D. (1996).  ?VITIPI : Versatile 
interpretation of text input by persons with impair-
ments?. Proceedings ICCHP'1996. Linz, Austria. 
Church, K. and Hanks, P. (1989). ?Word association 
norms, mutual information and lexicography?. Pro-
ceedings of ACL, pp. 76-83. 
Clarkson, P. R. and Robinson, A.J. (1997). ?Language 
Model Adaptation using Mixtures and an Exponen-
tially Decaying Cache?, in Proc. of the IEEE 
ICASSP-97, Munich. 
Coccaro, N. and Jurafsky, D. (1998). ?Towards better 
integration of semantic predictors in statistical lan-
guage modeling?, Proc. of the ICSLP-98, Sydney. 
Deerwester, S. C., Dumais, S., Landauer, T., Furnas, G. 
and Harshman, R. (1990). ?Indexing by Latent Se-
mantic Analysis?, JASIS  41(6), pp. 391-407. 
Fazly, A. and Hirst, G. (2003). ?Testing the efficacy of 
part-of-speech information in word completion?, 
Proceedings of the Workshop on Language Modeling 
for Text Entry Methods on EACL, Budapest. 
Goodman, J. (2001): ?A Bit of Progress in Language 
Modeling?, Extended Version Microsoft Research 
Technical Report MSR-TR-2001-72. 
Jelinek, F. (1990): ?Self-organized Language Models for 
Speech Recognition?, In: A. Waibel and K.-F. Lee 
(eds.), Readings in Speech Recognition, Morgan 
Kaufman Publishers, pp. 450-506. 
Kuhn, R. and De Mori, R. (1990). ?A Cache-Based 
Natural Language Model for Speech Reproduction?, 
IEEE Transactions on Pattern Analysis and Machine 
Intelligence, 12 (6), pp. 570-583. 
Landauer, T. K., Laham, D., Rehder, B. and Schreiner, 
M. E. (1997). ?How well can passage meaning be de-
rived without using word order? A comparison of 
LSA and humans?, Proceedings of the 19th annual 
meeting of the Cognitive Science Society, pp. 412-
417, Erlbaum Mawhwah, NJ. 
Lesher, G. W., Moulton, B. J, Higginbotham, D.J. and 
Alsofrom, B. (2002). ?Limits of human word predic-
tion performance?, Proceedings of the CSUN 2002. 
Li, J., Hirst, G. (2005). ?Semantic knowledge in a word 
completion task?, Proc. of the 7th Int. ACM Confer-
ence on Computers and Accessibility, Baltimore. 
Matiasek, H. and Baroni, M. (2003). ?Exploiting long 
distance collocational relations in predictive typing?, 
Proceedings of the EACL-03 Workshop on Language 
Modeling for Text Entry Methods, Budapest. 
Rosenfeld, R. (1996). ?A maximum entropy approach to 
adaptive statistical language modelling?, Computer 
Speech and Language, 10 (1), pp. 187-228. 
Schadle I., Antoine J.-Y., Le P?v?dic B., Poirier F. 
(2004).  ?Sibyl - AAC system using NLP tech-
niques?. Proc. ICCHP?2004, Paris, France. LNCS 
3118, Springer Verlag. 
Stolcke, A. (1998): ?Entropy-based pruning of backoff 
language models?. Proc.s of the DARPA Broadcast 
News Transcription and Understanding Workshop. 
Stolcke, A. (2002): ?SRILM - An Extensible Language 
Modeling Toolkit?, in Proc. of the Intl. Conference 
on Spoken Language Processing, Denver, Colorado. 
Trnka, K., Yarrington, D., McCoy, K. F. and Penning-
ton, C. (2006): ?Topic Modeling in Fringe Word Pre-
diction for AAC?, In Proceedings of the 2006 Inter-
national Conference on Intelligent User Interfaces, 
pp. 276 ? 278, Sydney, Australia. 
Trost, H., Matiasek, J. and Baroni, M. (2005): ?The 
Language Component of the FASTY Text Prediction 
System?, Applied Artificial Intelligence, 19 (8), pp. 
743-781. 
Wandmacher, T. (2005): ?How semantic is Latent Se-
mantic Analysis??, in Proceedings of 
TALN/RECITAL 2005, Dourdan, France, 6-10 june. 
513
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1388?1397,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Automatic Acquisition of the Argument-Predicate Relations
from a Frame-Annotated Corpus
Ekaterina Ovchinnikova
University of Osnabru?ck
eovchinn@uos.de
Theodore Alexandrov
University of Bremen
theodore@
math.uni-bremen.de
Tonio Wandmacher
University of Osnabru?ck
twandmac@uos.de
Abstract
This paper presents an approach to au-
tomatic acquisition of the argument-
predicate relations from a semantically
annotated corpus. We use SALSA, a
German newspaper corpus manually an-
notated with role-semantic information
based on frame semantics. Since the rel-
atively small size of SALSA does not al-
low to estimate the semantic relatedness
in the extracted argument-predicate pairs,
we use a larger corpus for ranking. Two
experiments have been performed in or-
der to evaluate the proposed approach.
In the first experiment we compare au-
tomatically extracted argument-predicate
relations with the gold standard formed
from associations provided by human sub-
jects. In the second experiment we cal-
culate correlation between automatic relat-
edness measure and human ranking of the
extracted relations.
1 Introduction
There are many debates in lexical semantics about
what kind of world knowledge actually belongs
to the meaning of a lexeme. Nowadays, it is
widely accepted that predicates impose selectional
restrictions on their arguments. For example, since
we know that the predicate to be hungry mainly
takes expressions describing animate beings as ar-
guments, we can correctly resolve the anaphora
in the following sentence: We gave the bananas
to the monkeys because they were hungry. There
exists also multiple linguistic evidence showing
that the semantics of arguments can help to pre-
dict implicit predicates. For example, the sentence
John finished the cigarette usually means John fin-
ished smoking the cigarette because the meaning
of the noun cigarette is strongly associated with
the smoking activity.
It has been claimed that information about pred-
icates associated with nouns can be helpful for
a variety of tasks in natural language processing
(NLP), see for example (Pustejovsky et al, 1993;
Voorhees, 1994). However, at present there exists
no corresponding lexical semantic resource. Sev-
eral approaches have been presented that aim at
creating a knowledge base containing noun-verb
relations. There are two main research paradigms
for developing such knowledge bases. The first
paradigm assumes manual development of the re-
source (Pustejovsky et al, 2006), while the sec-
ond one relies on automatic acquisition methods,
see for example (Cimiano and Wenderoth, 2007).
In this paper we propose a procedure for auto-
matic acquisition of argument-predicate relations
from a semantically annotated corpus. In line with
(Lapata and Lascarides, 2003) our approach is
based on the assumption that predicates are omit-
ted in a discourse when they are highly predictable
from the semantics of their arguments. We exploit
SALSA (Burchardt et al, 2006), a German news-
paper corpus manually annotated with FrameNet
frames based on frame semantics. Using a man-
ually annotated corpus for relation extraction has
one particular advantage compared to extraction
from plain text: the type of an argument-predicate
relation is already annotated; there is no need to
determine it by automatic means which are usu-
ally error-prone. However, the relatively small
size of SALSA does not allow to make relevant
predictions about the degree of semantic related-
ness in the extracted argument-predicate pairs, see
section 4. We therefore employ a considerably
larger unannotated corpus for weighting. The re-
sults are evaluated quantitatively against human
judgments obtained experimentally. The proposed
evaluation procedure is similar to that presented in
(Cimiano and Wenderoth, 2007). First, we create
a gold standard for 30 words from the argument
list and evaluate our approach with respect to this
1388
gold standard. Second, we provide results from
an evaluation in which test subjects are asked to
rate automatically extracted relations using a four-
point scale.
The paper is structured as follows: Section 2
describes some linguistic phenomena requiring in-
ferences of an implicit predicate from the seman-
tics of an explicitly given argument. In section 3
we give a short overview of the related work. Sec-
tions 4 discusses the SALSA corpus. Section 5 in-
troduces our approach. Finally, section 6 describes
an experimental evaluation of the presented ap-
proach and section 7 concludes the paper.
2 Implicit Predicates
In this section we discuss some linguistic phenom-
ena requiring inferences of an implicit predicate
from the semantics of an explicitly given argument
for their resolution. One of the most studied phe-
nomena that Pustejovsky (1991) has called logical
metonymy is illustrated by the examples (1a) and
(1b) below. In the case of logical metonymy an im-
plicit predicate is inferable from particular verb-
noun and adjective-noun pairs in a systematic way.
The verb anfangen ?to start? and the adjective kom-
pliziert ?complicated? in the mentioned examples
semantically select for an event, while the nouns
(Buch ?book? and Frage ?question? respectively)
have a different semantic type. However, the set
of the most probable implicit predicates is pre-
dictable from the semantics of the nouns. Thus,
(1a) plausibly means Als ich angefangen habe,
dieses Buch zu lesen/schreiben... ?When I have
started to read/write this book...? and (2a) plau-
sibly means eine Frage die kompliziert zu beant-
worten ist ?a question which is complicated to an-
swer?.
Example 1
(a) Als ich mit diesem Buch angefangen habe...
?When I have started this book...?
(b) eine komplizierte Frage
?a complicated question?
(c) Studentenfutter
?student food?
(d) Nachrichtenagentur Xinhua u?ber Beziehun-
gen beider Seiten der Taiwan-Strasse
?News agency Xinhua about relations of both
sides of the Taiwan Strait?
(e) Hans ist beredt
?Hans is eloquent?
As we can see from Example 1, besides logi-
cal metonymy there are other linguistic phenom-
ena requiring knowledge about predicates associ-
ated with an argument for their resolution. Exam-
ple (1c) contains a noun compound which can be
interpreted on basis of the meaning of the noun
Futter ?food?. In general, noun compounds can be
interpreted in many different ways depending on
the semantics of the constituencies: morning cof-
fee is a coffee which is drunk in the morning, brick
house is a house which is made of bricks etc. In
case of (1c) the relation via the predicate essen ?to
eat? taking Studenten ?students? as a subject and
Futter ?food? as an object seems to be the most
plausible one.
The phrase (1d) is a title of a newspaper ar-
ticle. As in the previous examples, a predicate
is left out in (1d). The meaning of the prepo-
sition u?ber ?about? can help to narrow down the
set of possible predicates, but still allows an in-
adequately large range of interpretations. How-
ever, the semantics of the noun Nachrichtenagen-
tur ?news agency? supports such interpretations as
berichten ?to report? or informieren ?to inform?.
Most of the literature discusses predicates infer-
able from nouns. However, other parts of speech
can support similar inferences. In example (1e) a
predicate is predictable on the basis of the mean-
ing of the adjective beredt ?eloquent?. The sen-
tence (1e) most plausibly means that Hans speaks
eloquently.
Example 1 shows that knowledge about pred-
icates associated with explicitly given arguments
can help to deal with several linguistic phenom-
ena. The cases when a predictable predicate is left
out are not rare in natural language. For example,
for logical metonymy a corpus study has shown
that the constructions like begin V NP occur rarely
if the verb V corresponds to a highly plausible in-
terpretation of begin NP (Briscoe et al, 1990).
3 Related Work
The most influential account of logical metonymy
is provided by Pustejovsky?s theory of the Gen-
erative Lexicon, GL (Pustejovsky, 1991). Ac-
cording to Pustejovsky the meaning of a noun in-
cludes a qualia structure representing ?the essen-
tial attributes of an object as defined by the lexi-
cal item?. Thus, the lexical meaning of the noun
book includes read and write as qualia roles. In
the framework of GL, Pustejovsky et al (2006)
1389
are manually developing the Brandeis Semantic
Ontology which is a large generative lexicon on-
tology and dictionary. There also exist several ap-
proaches to automatic acquisition of qualia struc-
tures from text corpora which aim at supporting
the time-consuming manual work. For example,
Pustejovsky et al (1993) use generalized syntac-
tic patterns for extracting qualia structures from a
partially parsed corpus. Cimiano and Wenderoth
(2007) suggest a pattern-based method for auto-
matic extraction of qualia structures from the Web.
The results of the human judgment experiment re-
ported in (Cimiano and Wenderoth, 2007) suggest
that the automatic acquisition of qualia structures
is a difficult task. Human test subjects have shown
a very low agreement (11,8% average agreement)
in providing qualia structures for given nouns.
Another line of research on inferring implicit
predicates concerns using information about col-
locations derived from corpora. For example,
Lapata and Lascarides (2003) resolve logical
metonymy on the basis of the distribution of para-
phrases like finish the cigarette ? finish smok-
ing the cigarette and easy problem ? problem
which is easy to solve in a corpus. This approach
shows promising results, but it is limited to logi-
cal metonymy. Similarly, Nastase et al (2006) use
grammatical collocations for defining semantic re-
lations between constituents in noun compounds.
In our study we aim at extracting intuitively
plausible argument-predicate relations from a se-
mantically annotated corpus. Using an annotated
corpus we avoid problems of defining types of
these relations by automatic means which are usu-
ally error-prone. We represent argument-predicate
relations in terms of FrameNet frames which al-
low for a fine-grained and grounded representation
supporting paraphrasing, see next sections. Our
approach is not restricted to nouns. We also con-
cern relations where argument positions are filled
by adjectives, adverbs or even verbs.
4 The SALSA Corpus
For relation extraction we have chosen the SALSA
corpus (Burchardt et al, 2006) developed at Saar-
land University. SALSA is a German corpus
manually annotated with role-semantic informa-
tion, based on the syntactically annotated TIGER
newspaper corpus (Brants et al, 2002). The
2006 SALSA release which we have used con-
tains about 20 000 annotated predicate instances.
The corpus is annotated with the set of FrameNet
frames.
The FrameNet, FN (Ruppenhofer et al, 2006),
lexical resource is based on frame semantics (Fill-
more, 1976), see http://framenet.icsi.berkeley.edu.
The lexical meaning of predicates in FN is ex-
pressed in terms of frames (approx. 800 frames)
which are supposed to describe prototypical sit-
uations spoken about in natural language. Every
frame contains a set of roles (or frame elements,
FEs) corresponding to the participants of the de-
scribed situation. Predicates with similar seman-
tics are assigned to the same frame, e.g. to give
and to hand over refer to the GIVING frame. Con-
sider a FN annotation for the sentence (2a) below.
In this annotation DONOR, RECIPIENT and THEME
are roles in the frame GIVING and John, Mary and
a book are fillers of these roles. The FN anno-
tation generalizes across near meaning-preserving
transformations, see (2b).
Example 2
(a) [John]DONOR [gave]GIVING
[Mary]RECIPIENT [a book]THEME.
(b) [John]DONOR [gave]GIVING [a
book]THEME [to Mary]RECIPIENT.
In FN information about syntactic realization
patterns of frame elements as well as information
about frequency of occurrences of these patterns in
corpora is provided. For example, the role DONOR
in the frame GIVING is most frequently filled by a
noun phrase in the subject position or by a prepo-
sitional phrase with the preposition by as the head
in the complement position.
The FN project originally aimed at developing a
frame-semantic lexicon for English. Later on FN
frames turned out to be to a large extent language
independent (Burchardt et al, 2006). In most of
the cases German predicates could be successfully
described by the FN frames. However, some of
the frames required adaptation to the German data,
e.g. new FEs were introduced. Since FN does not
cover all possible word senses, new frames needed
to be added for some of the predicates.
We have chosen the SALSA corpus for our
experiments because to our knowledge it is the
only freely available corpus which contains both
syntactic and role-semantic annotation. However,
we are aware that SALSA (approx. 700 000
tokens) is too small to compute a reliable co-
occurrence model for measuring plausibility of the
extracted argument-predicate relations, though it
1390
is relatively large for a manually annotated cor-
pus. As it was shown in (Bullinaria and Levy,
2007), co-occurrence-based approaches need very
large training corpora in order to reliably compute
semantic relatedness. The SALSA corpus, com-
prising less than 1 million tokens, is too small for
this purpose. Moreover, a considerable number of
predicates in SALSA appeared to be unannotated.
Some of the high frequency pairs, as for exam-
ple Bombe, explodieren ?bomb, to explode?, occur
in SALSA only once, just as occasional pairs like
Deutsche, entdecken ?German, to discover?. We
have tried to overcome the size problems by using
a larger unannotated corpus for recomputing the
rating of our resulting relations, see next section.
5 Automatic Acquisition of the
Argument-Predicate Relations
In line with (Lapata and Lascarides, 2003), our ap-
proach to extraction of argument-predicate (AP)
relations is based on two assumptions:
A1: If predicates are highly predictable from the
semantics of their arguments then they can be
omitted in a discourse;
A2: If a predicate frequently takes a word as an
argument then it is highly predictable from the se-
mantics of this word.
In the proposed experimental setting argument-
predicate relations are defined in terms of the
FrameNet frames. Thus, we aim at extracting
from SALSA tuples of the form ?Argument, ROLE,
FRAME, Predicate? such that the Argument plau-
sibly fills the ROLE in the FRAME evoked by the
Predicate. As already mentioned in section 3,
our approach is not restricted to nouns. We also
treat arguments expressed by other content parts
of speech. The proposed relation extraction pro-
cedure consists in
? finding for every content word which occurs
in the corpus a set of predicates taking this
word as an argument with a high probability;
? defining a relation between the word and ev-
ery predicate from this set by finding which
roles the noun fills in frames evoked by the
predicate;
? estimating the degree of the semantic relat-
edness in the extracted argument-predicate
pairs.
For example, analyzing the following sentence
[Fu?nf Oppositionelle]SUSPECT sind in Ebe-
biyin [von der Polizei]AUTHORITIES [festgenom-
men]ARREST worden.
?Five members of the opposition have been
arrested by the police in Ebebiyin.?
we aim at extracting the following tuples:
Argument Role Frame Predicate
Oppositionell SUSPECT ARREST festnehmen
Polizei AUTHORITIES ARREST festnehmen
Relation Extraction
In SALSA, every sentence is annotated with a set
of frames in such a way that for every frame its
FEs refer to some syntactic constituents in the sen-
tence. In order to extract argument-predicate rela-
tions from SALSA we need 1) to find a content
head for every constituent corresponding to a FE;
2) to resolve possibly existing anaphora. Since
SALSA is syntactically annotated, the first task
proved to be relatively easy.1 On the contrary,
anaphora resolution is well-known to be one of
most challenging NLP tasks. In our study, we
do not focus on it, and we treat only pronominal
anaphora using the following straightforward res-
olution algorithm: given a pronoun the first noun
which agrees in number and gender with the pro-
noun is supposed to be its antecedent. In order
to evaluate this resolution procedure we have in-
spected 100 anaphoric cases. In approximately
three fourths of the cases the anaphora were re-
solved correctly. Therefore, we have assigned a
confidence rate of 0,75 to the FE fillers resulting
from a resolved anaphora. In non-anaphoric cases
a confidence rate of 1 was assigned.
For every extracted tuple of the form
?Argument, ROLE, FRAME, Predicate? we
have summed up the corresponding confidence
rates. Finally, we have obtained around 30 000
tuples with confidence rates ranging from 0,75
to 88. It is not surprising that most of the argu-
ments appeared to be nouns, while most of the
predicates are expressed by verbs. Since SALSA
has been annotated manually, there are almost
no mistakes in defining types of the semantic
1We have excluded from the consideration foreign-
language expressions, while proper nouns were treated in the
usual way. For verb phrases with auxiliary or modal verbs as
heads the main verb was taken as a corresponding role filler.
1391
relations between arguments and predicates.2 For
several pairs, the semantic relation between an
argument and a predicate is ambiguous. Consider
the tuples extracted for the word pair Buch,
schreiben ?book, to write? which are given below.
While the first tuple corresponds to phrases like
ein Buch schreiben ?to write a book?, the second
one abstracts from the expressions like in einem
Buch schreiben ?to write in a book?.
Argument Role Frame Predicate
Buch TEXT TEXT CREATION schreiben
Buch MEDIUM STATEMENT schreiben
Additionally, ambiguity can arise because of the
annotation disagreements in SALSA. For exam-
ple, the pair (Haft, sitzen) ?imprisonment?, ?to sit?
in Table 1 was annotated in SALSA both with the
BEING LOCATED and with the POSTURE frames.
As mentioned in section 4, a considerable num-
ber of predicates in SALSA is not annotated se-
mantically. In order to find out how many relevant
AP-relations get lost if we consider only seman-
tically annotated predicates, we have additionally
extracted AP-pairs on the basis of the syntactic an-
notation only. The anaphora resolution procedure
as described above was again applied to the syn-
tactic argument heads. We have obtained around
56 500 pairs with confidence rates ranging from
0,75 to 71,50.3
As one could expect, being a newspaper corpus
SALSA appeared to be thematically unbalanced.
The most frequent argument-predicate relations
occurring in SALSA reflect common topics dis-
cussed in newspapers: economics (e.g. (Prozent,
steigen), ?percent?, ?to increase?), criminality (e.g.
(Haft, verurteilen) ?imprisonment?, ?to sentence?),
catastrophes (e.g. (Mensch, to?ten) ?human?, ?to
kill?) etc.
Ranking
As mentioned in section 4, the size of SALSA
does not allow to make relevant predictions about
the distribution of frames and role fillers. Only
2% of the relations occur in SALSA more then
3 times. In order to overcome this problem we
have developed a measure of semantic relatedness
between the extracted arguments and predicates
2Mistakes can arise only because of the annotation errors
and errors in the anaphora resolution procedure.
3The comparison of the results obtained by the extraction
procedure based on the semantic annotation with the results
of the procedure based on the syntactic annotation only is
provided in the next section.
which takes into account their co-occurrence in a
larger and more representative corpus. For com-
puting semantic relatedness we have used a lem-
matized newspaper corpus (Su?ddeutsche Zeitung,
SZ) of 145 million words. Given a tuple t with a
confidence rate c containing an argument a and a
predicate p, the relatedness measure rm of t was
computed as follows:
rm(t) = lsa(a, p) + c/max(c),
where the lsa(a, p) is based on Latent Semantic
Analysis, LSA (Deerwester et al, 1990). LSA is
a vector-based technique that has been shown to
give reliable estimates on semantic relatedness. It
makes use of distributional similarities of words
in text and constructs a semantic space (or word
space) in which every word of a given vocabulary
is represented as a vector. Such vectors can then
be compared to one another by the usual vector
similarity measures (e.g. cosine). We calculated
the LSA word space using the Infomap toolkit10
v. 0.8.6 (http://infomap-nlp.sourceforge.net). The
co-occurrence matrix (window size: 5 words)
comprised 80 000?3 000 terms and was reduced
by SVD to 300 dimensions. For the vector com-
parisons the cosine measure was applied. To those
words which did not occur in the analyzed SZ cor-
pus (approx. 3500 words) a lsa measure of 0 was
assigned. To provide a comparable contribution to
rm, the confidence rates c extracted from SALSA
are divided by the maximal confidence rate. The
rm function is a linear interpolation of the lsa and
the normalized c measure. As mentioned above,
the c measure is a discriminative factor for only
2% of the relations. For the remaining 98% the
normalized c values are small (0,003 or 0,002 or
0,001). Therefore, calculating the rm measure we
mainly rely on lsa, while normalized c actually
plays a role only for the relations frequently oc-
curring in SALSA. Table 1 contains the 5 most se-
mantically related predicates for an example argu-
ment.
6 Evaluation
Since the extracted argument-predicate relations
are intended to be used for inferring intuitively ob-
vious predicates,we evaluate to which extend they
correspond to human intuition.
1392
Table 1: Examples of the extracted argument-predicate relations
Argument Role Frame Predicate rm
Haft FINDING VERDICT verurteilen ?to sentence? 0,939
?imprisonment? LOCATION BEING LOCATED sitzen ?to sit? 0,237
LOCATION POSTURE sitzen ?to sit? 0,226
MESSAGE REQUEST fordern ?to demand? 0,153
BAD OUTCOME RUN RISK-FNSALSA drohen ?to threaten? 0,144
Gold Standard
Similar to (Cimiano and Wenderoth, 2007) we
provide a gold standard for 30 test arguments oc-
curring in the SALSA corpus. The test argu-
ments were selected randomly from the set of
those arguments that have more than one pred-
icate associated with them such that a value of
argument-predicate relatedness exceeds the aver-
age one. These words were nearly uniformly dis-
tributed among 20 participants of the experiment,
who were all non-linguists. We also ensured that
each word was treated by three different subjects.
For every word we asked our subjects to write be-
tween 5 and 10 short phrases that contain a pred-
icate taking the given word as an argument, e.g.
book ? to read a book. The participants were asked
to provide phrases instead of single predicates, be-
cause we wanted to control the syntactic and se-
mantic position of the arguments. The participants
received an instruction informally describing the
notion of predicate and what kind of phrases they
are supposed to come up with. Besides the task
description they were shown examples containing
appropriate and inappropriate phrases. Some of
the examples are given below.
Example 3
(a) Aktie ?stock? : Kauf der Aktien ?buying of
stocks?, Aktien kaufen ?to buy stocks?, Aktien an
der Bo?rse ?stocks on the bourse? (is inappropriate
because the word ?bourse? describes a place and
not an event)
(b) beredt ?eloquent?: beredt sprechen ?to
speak eloquently?, ein beredter Sprecher ?an elo-
quent speaker? (is inappropriate because the word
?speaker? describes a person and not an event)
The test was conducted via e-mail. In or-
der to compare the human associations with the
extracted AP-relations, we have manually anno-
tated the obtained phrases with SALSA frames.
The agreement for the described task for every
cue word was calculated as the averaged pairwise
agreement between the AP-relations delivered by
the three subjects, S
1
, S
2
and S
3
, as follows:
Agr =
|S
1
?S
2
|
|S
1
?S
2
|
+
|S
2
?S
3
|
|S
2
?S
3
|
+
|S
2
?S
3
|
|S
2
?S
3
|
3
.
Agreement results for every cue word are re-
ported in table 2. Second column of the table
contains gold standard predicates which were pro-
vided by all 3 participants treating the same word.4
Averaging over all words, we got a mean agree-
ment of 13%. Though this value seems to be low,
it is consistent with a mean agreement of 11,8%
for a similar task reported in (Cimiano and Wen-
deroth, 2007), see section 3. Cimiano and Wen-
deroth (2007) show that the lowest agreement is
yielded for more abstract words, while the agree-
ment for very concrete words is reasonable. We
could not make a similar observation, see table 2.
Comparison with the Gold Standard
In the first experiment we checked whether pred-
icates which people associate with the test argu-
ments can be automatically extracted by our pro-
cedure. For this aim we compared the gold stan-
dard with all automatically extracted argument-
predicate relations5 containing some of the 30 cue
words as follows. These relations were ranked ac-
cording to the relatedness measure described in
previous section. In line with (Cimiano and Wen-
deroth, 2007) we exploited an approach common
in information retrieval for estimating the qual-
ity of correspondence of a ranked output to a
gold standard, see (Baeza-Yates and Ribeiro-Neto,
1999).
Given some n automatically extracted relations
with the highest ranking we calculated a precision-
recall curve expressing precision and recall of our
procedure compared to the gold standard. The pre-
cision characterizes the procedure exactness, i.e.
how many redundant relations are retrieved. The
4The overall gold standard consists of 33 tuples.
5In order to evaluate the procedure extracting AP-
relations on the basis of the semantic annotation we com-
pared automatically extracted tuples to the gold standard tu-
ples. For the procedure using the syntactic annotation only
the AP-pairs were considered without regarding frames and
FEs.
1393
recall measures the completeness, i.e. how many
relations of the gold standard are extracted auto-
matically. For each point of the curve (which is
a pair (p, r) of values of precision p and recall r)
we calculated the F -measure as F = 2pr/(p+ r)
which is the harmonic mean between recall and
precision. The precision-recall curve is a set of
precision values for the prespecified recall levels
varying from 0 to 1 with a step 0,1. Then, to pro-
duce only one value evaluating the quality of the
ranked output compared to the gold standard, for
each precision-recall curve we calculated F
max
,
the maximal value of the F -measure achieved
for the points of this curve. F
max
expresses the
best trade-off between precision and recall for the
given ranked output. Finally, among all possible
n (numbers of the considered relations with the
highest ranking) we selected that one which pro-
vides the maximal F
max
value.
The resulting maximal F
max
values are 0,47 for
the procedure extracting AP-relations on the basis
of the semantic annotation and 0,41 for the pro-
cedure using the syntactic annotation only. We
compared these results with the baseline results
of maximal F
max
values produced for the output
with random ranking. The calculation of the base-
line was repeated 100 times, each time a new ran-
dom ranking was generated. The lowest baseline
results are 0,08/0,06 (semantic/syntactic annota-
tion), the highest are 0,18/0,14 and the medians
are 0,1/0,07. One can see that the results produced
using the relatedness measure (0,47/0,41) greatly
exceed the baseline. Based on this comparison we
conclude that the ranking done using the related-
ness measure brings a significant advantage. The
values of precision and recall for the reported max-
imal F
max
values are 0,5/0,33 (semantic/syntactic
annotation) and 0,45/0,54 respectively. This re-
sults show that half of the AP-relations from the
gold standard appeared to be in the list of the top-
ranked tuples extracted by the ?semantic? proce-
dure, while the size of this list (n = 28) was al-
most equal to the size of the gold standard (33).
The differences in performance between the ?se-
mantic? and ?syntactic? procedures could be ex-
plained by the fact that the ?syntactic? procedure
finds in the corpus more related predicates for ev-
ery argument than the ?semantic? one. Neverthe-
less, the ?semantic? procedure shows better per-
formance.
Next we investigated the results for each argu-
ment used in the gold standard separately in the
same way as described above. For each argument
the F
max
measure has been computed. Because
of the low agreement between the subjects ques-
tioned for the gold standard (see above), in these
calculations we considered all predicates reported
by our subjects. The calculated F
max
values are
reported in table 2 which shows a correlation be-
tween F
max
values calculated for the ?semantic?
and ?syntactic? procedures. However, there is
no correlation with human agreement. This issue
needs a further investigation, see section 7.
Human Judgments of the Relatedness
Following (Cimiano and Wenderoth, 2007), in or-
der to check whether the calculated relatedness
is reasonable according to human intuition, we
have performed another experiment. For each of
the 30 words selected for the gold standard we
selected the 5 top ranked predicates. Since for
some of the cue arguments only 3 predicates were
found in the corpus, the final test set contains only
138 argument-predicate tuples. From these tuples
we generated short grammatically correct phrases
structurally similar to those in example 3. These
phrases were uniformly distributed among 10 sub-
jects so that every phrase was evaluated by one
subject. The participants were asked to rate the
phrases with respect to their naturalness using a
scale from 0 to 3, whereby 0 means ?unnatural?,
1 ?possible?, 2 ?natural? and 3 ?totally natural and
self-evident?.
Further on we investigated the relationship be-
tween the human estimates and the relatedness
values obtained automatically. For this aim we
used the Spearman rank correlation coefficient.
Because of four-points scale used, the human
rankings are equal for many tuples which lead to
the so-called effect of ties. For this reason we
computed the correlation coefficient with a cor-
rection for ties. The coefficient value is 0,30 and
this correlation is statistically significant with p-
value 0,0006. Based on these results we conclude
that our relatedness measure is correlated with hu-
man judgments. Taking into account the subjec-
tive character of human ranking in terms of nat-
uralness, the achieved correlation values can be
considered as high.
1394
Table 2: Evaluation results for 30 gold standard cue words.
Cue word Shared predicates Agr Sem. F
max
Syn. F
max
Name ?name? haben ?to have? 14% 0,2 0,48
Urlaub ?vacation? fahren ?to go? 8% 0,13 0,16
Sprache ?language? sprechen ?to speak?, lernen ?to learn? 14% 0,4 0,3
Strafe ?fine? verurteilen ?to sentence? 11% 0,21 0,3
Stuhl ?chair? sitzen ?to sit? 14% 0,1 0,2
Bombe ?bomb? hochgehen ?to blow up? 14% 0,11 0,22
Blatt ?gazette?, ?page?, ?leaf? ? 2% 0 0
Flughafen ?airport? ankommen ?to arrive?, fahren ?to go? 21% 0,17 0,17
Gesetz ?low? ? 8% 0,17 0,38
Polizei ?police? rufen ?to call? 11% 0,22 0,23
Kompromiss ?compromise? schliessen ?to make? 15% 0,07 0,29
Fluggesellschaft ?airline? ? 3% 0,11 0,38
Antrag ?proposal?, ?application? stellen ?to introduce?, ablehnen ?to decline? 24% 0,43 0,42
Zeitung ?newspaper? lesen ?to read? 13% 0,17 0,09
Brief ?letter? verschicken ?to send?, schreiben ?to write? 19% 0,23 0,12
Flu?chtling ?refugee? aufnehmen ?to accept? 13% 0 0,07
Buch ?book? schreiben ?to write?, lesen ?to read? 15% 0,44 0,39
Za?hler ?counter? ablesen ? to read? 11% 0 0
Anzahl ?number? ? 3% 0,23 0,19
Prozent ?percent? ? 3% 0,48 0,21
Ziel ?goal? verfehlen ?to miss?, erreichen ?to reach? 20% 0,3 0,48
Schule ?school? schwa?nzen ?to miss?, gehen ?to go? 22% 0,13 0,23
Amt ?position?, ?department? bekleiden, innehaben ?to hold?, gehen ?to go? 20% 0 0,17
Frage ?question? beantworten ?to answer?, stellen ?to ask? 20% 0,15 0,37
Mensch ?human? sein ?to be? 16% 0,09 0,03
Zeuge ?witness? aussagen ?to testify?, sein ?to be? 22% 0,13 0,19
Thema ?theme? ? 7% 0,14 0,26
Preistra?ger ?prize winner? ? 5% 0,08 0,08
Initiative ?initiative? ergreifen ?to take? 17% 0,1 0,13
Wohnung ?flat? ? 7% 0,09 0,17
7 Conclusion and Discussion
In this paper we presented an approach to auto-
matic extraction of argument-predicate relations
from a frame-annotated corpus.6 In our approach
we aimed to combine the advantages offered by
annotated and unannotated lexical resources. Be-
sides extracting AP-pairs the proposed method al-
lows us to define types of semantic relations in
terms of FrameNet frames. The proposed proce-
dure is not restricted to arguments expressed by
nouns and treats also other content parts of speech.
The main goal of this paper was to show that
though manually annotated corpora usually have
a relatively small size, they can be successfully
exploited for the relation extraction. An obvious
limitation of the presented approach is that it is
bounded to manual annotations which are hard
to obtain. However, since semantic annotations
are useful for many different goals in linguistics
and NLP, the number of reliable annotated cor-
pora constantly grows.7 Moreover, recently sev-
6The complete list of the extracted AP-relations as well
as the results of the experiment will be available online at
http://www.ikw.uni-osnabrueck.de/?eovchinn/APrels/.
7At present FrameNet annotated corpora are
eral tools have been developed which perform role
annotation automatically, for example see (Erk
and Pado, 2006). Therefore we believe that ap-
proaches using semantic annotation are valid and
promising. In the future we plan to experiment
with large role-annotated corpora for English such
as PropBank (approx. 300 000 words, (Palmer
et al, 2005)) and the FrameNet-annotated corpus
provided by the FN project (more than 135 000
annotated sentences, (Ruppenhofer et al, 2006)).
Since these corpora do not contain syntactic anno-
tation, for extracting argument-predicate relations
we will need to parse annotated sentences.
There are several ways to improve the proposed
procedure. First, an implementation of a more
advanced anaphora resolution algorithm treating
pronominal as well as nominal anaphora should
significantly raise the precision/recall characteris-
tics. Second, splitting German compounds occur-
ring in the corpus should provide additional ev-
idence. We have treated such words as Kunde
?client? and Privatkunde ?private client? as differ-
ent lexemes, while they are strongly related se-
available for English, German and Spanish, see
http://framenet.icsi.berkeley.edu.
1395
mantically and information about predicates co-
occurring with the second word could probably
be used for describing the semantics of the first
one. Concerning relatedness measure, additional
corpus-based measures such as Web-based mea-
sures (Cimiano and Wenderoth, 2007) or measures
based on syntactic relations (Pustejovsky et al,
1993) could appear to be useful for improving the
ranking of the extracted relations.
The presented procedure was evaluated quanti-
tatively against human judgments obtained experi-
mentally. The participants of the experiment were
asked to provide short phrases containing given
cue words and predicates associated with these
words as well as to rate phrases generated from the
automatically extracted AP-relations. Concerning
the first experiment, the low human agreement has
shown that the proposed association task appeared
to be difficult for the subjects. Nevertheless, the
described learning procedure proved to extract in-
tuitively reasonable relations.
The evaluation strategy presented in this pa-
per on relies on the underlying assumptions (A1
and A2 in section 5) and is compatible with the
other approaches to relation extraction, cf. (Cimi-
ano and Wenderoth, 2007). However, it is plau-
sible that human responses in the context of pro-
viding associated predicates for target words will
differ from the responses in the experimental set-
tings where subjects are asked to infer implicit
predicates, e.g. to extend phrases containing im-
plicit predicates. In the future we plan to im-
plement a procedure making use of the extracted
AP-relations which would automatically extend
phrases containing implicit predicates. Then we
intend to compare output results of the procedure
with the human responses. Additionally, a study of
a possible correspondence between human agree-
ment on associated predicates and a semantic type
of an argument (e.g. concrete/abstract, natural
kind/artifact) should be performed on more test ar-
guments.
Potential Applications
As already mentioned in the literature, see for ex-
ample (Lapata and Lascarides, 2003), knowledge
about implicit predicates could be potentially use-
ful for a variety of NLP tasks such as language
generation, information extraction, question an-
swering or machine translation. Many applica-
tions of semantic relations in NLP are connected
to paraphrasing or query expansion, see for ex-
ample (Voorhees, 1994). Suppose that a search
engine or a question answering system receives
the query schnelle Bombe ?quick bomb?. Prob-
ably, in this case the user is interested in find-
ing information about bombs that explode quickly
rather then about bombs in general. Knowledge
about predicates associated with the noun Bombe
?bomb? could be used for predicting a set of prob-
able implicit predicates. For generation of the se-
mantically and syntactically correct paraphrases it
is sometimes not enough to guess the most prob-
able argument-predicate pairs. Information about
types of an argument-predicate relation could be
helpful, i.e. which semantic and syntactic posi-
tion does the argument fill in the argument struc-
ture of the predicate. For example, compare
eine Bombe explodiert schnell ?a bomb explodes
quickly? for schnelle Bombe with ein Buch schnell
lesen/schreiben ?to read/write a book quickly? for
schnelles Buch ?quick book?. In the first case the
argument Bombe fills the subject position, while
in the second case Buch fills the object posi-
tion. Since FrameNet contains information about
syntactic realization patterns for frame elements,
representation of argument-predicate relations in
terms of frames directly supports generation of se-
mantically and syntactically correct paraphrases.
The described procedure could also support
manual development of a lexical resource, provid-
ing evidence from corpora as well as the distribu-
tional information.
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. Addison Wesley,
Harlow, 1. aufl. edition.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Ted Briscoe, Ann Copestake, and Bran Boguraev.
1990. Enjoy the paper: Lexical semantics via lex-
icology. In Proceedings of the 13th International
Conference on Computational Linguistics, pages
42?47.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510?526.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado, and Manfred Pinkal.
1396
2006. The SALSA corpus: A German corpus re-
source for lexical semantics. In Proceedings of
LREC 2006, pages 969?974.
Philipp Cimiano and Johanna Wenderoth. 2007. Auto-
matic Acquisition of Ranked Qualia Structures from
the Web. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics,
pages 888?895.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. American
Society of Information Science, 41(6):391?407.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser -
a flexible toolbox for semantic role assignment. In
Proceedings of LREC 2006, Genoa, Italy.
Charles J. Fillmore. 1976. Frame semantics and the
nature of language. In Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, volume 280,
pages 20?32.
Mirella Lapata and Alex Lascarides. 2003. A Prob-
abilistic Account of Logical Metonymy. Computa-
tional Linguistics, 29(2):261?316.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina
Sokolova, and Stan Szpakowicz. 2006. Learning
noun-modifier semantic relations with corpus-based
and wordnet-based features. In Proceedings of the
AAAI 2006.
Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005. The Proposition Bank: A Corpus Annotated
with Semantic Roles. Computational Linguistics,
31(1):71?106.
James Pustejovsky, Peter Anick, and Sabine Bergler.
1993. Lexical semantic techniques for corpus anal-
ysis. Computational Linguistics, 19(2):331?358.
James Pustejovsky, Catherine Havasi, Roser Saur,
Patrick Hanks, Anna Rumshisky, Jessica Littman,
Jos Castao, and Marc Verhagen. 2006. Towards a
generative lexical resource: The Brandeis Semantic
Ontology. In Proceedings of the Fifth Language Re-
source and Evaluation Conference.
James Pustejovsky. 1991. The Generative Lexicon.
Computational Linguistics, 17(4):409?441.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended Theory and
Practice. International Computer Science Institute.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of the
17th annual international ACM SIGIR conference
on Research and development in information re-
trieval, pages 61?69.
1397
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358?364,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joint WMT Submission of the QUAERO Project
?Markus Freitag, ?Gregor Leusch, ?Joern Wuebker, ?Stephan Peitz, ?Hermann Ney,
?Teresa Herrmann, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Gilles Adda,?Josep Maria Crego,
?Bianka Buschbeck, ?Tonio Wandmacher, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2011 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems. Then RWTH system combination
combines these translations to a better one. In
this paper, we describe the single systems of
each group. Before we present the results of
the system combination, we give a short de-
scription of the RWTH Aachen system com-
bination approach.
1 Overview
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
1.1 Data Sets
For WMT 2011 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned on
the newstest2009 dev set. The newstest2008 dev set
was used to train the system combination parame-
ters. Finally the newstest2010 dev set was used to
compare the results of the different system combi-
nation approaches and settings.
2 Translation Systems
2.1 RWTH Aachen Single Systems
For the WMT 2011 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
bilingual corpus, the translation probabilities are es-
timated by relative frequencies. The standard feature
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. Parameters are optimized with the Downhill-
Simplex algorithm (Nelder and Mead, 1965) on the
word graph.
358
2.1.2 Hierarchical System
For the hierarchical setups described in this pa-
per, the open source Jane toolkit (Vilar et al, 2010)
is employed. Jane has been developed at RWTH
and implements the hierarchical approach as intro-
duced by Chiang (2007) with some state-of-the-art
extensions. In hierarchical phrase-based translation,
a weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The model weights are optimized
with standard MERT (Och, 2003) on 100-best lists.
2.1.3 Phrase Model Training
For some PBT systems a forced alignment pro-
cedure was applied to train the phrase translation
model as described in Wuebker et al (2010). A
modified version of the translation decoder is used
to produce a phrase alignment on the bilingual train-
ing data. The phrase translation probabilities are es-
timated from their relative frequencies in the phrase-
aligned training data. In addition to providing a sta-
tistically well-founded phrase model, this has the
benefit of producing smaller phrase tables and thus
allowing more rapid and less memory consuming
experiments with a better translation quality.
2.1.4 Final Systems
For the German?English task, RWTH conducted
experiments comparing the standard phrase extrac-
tion with the phrase training technique described in
Section 2.1.3. Further experiments included the use
of additional language model training data, rerank-
ing of n-best lists generated by the phrase-based sys-
tem, and different optimization criteria.
A considerable increase in translation quality can
be achieved by application of German compound
splitting (Koehn and Knight, 2003). In comparison
to standard heuristic phrase extraction techniques,
performing force alignment phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006),
sentence length model, a 6-gram LM and IBM-1 lex-
icon models in both normal and inverse direction.
These models are combined in a log-linear fashion
and the scaling factors are tuned in the same man-
ner as the baseline system (using TER?4BLEU on
newstest2009).
The final table includes two identical Jane sys-
tems which are optimized on different criteria. The
one optimized on TER?BLEU yields a much lower
TER.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogeneous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation. Op-
timization with regard to the BLEU score is done
using Minimum Error Rate Training as described
by Venugopal et al (2005). The translation model
is trained on the Europarl and News Commentary
Corpus and the phrase table is based on a GIZA++
Word Alignment. We use two 4-gram SRI language
models, one trained on the News Shuffle corpus and
one trained on the Gigaword corpus. Reordering is
performed based on continuous and non-continuous
POS rules to cover short and long-range reorder-
ings. The long-range reordering rules were also ap-
plied to the training corpus and phrase extraction
was performed on the resulting reordering lattices.
Part-of-speech tags are obtained using the TreeTag-
1http://hunspell.sourceforge.net/
359
ger (Schmid, 1994). In addition, the system applies
a bilingual language model to extend the context of
source language words available for translation. The
individual models are described briefly in the fol-
lowing.
2.2.3 POS-based Reordering Model
We use a reordering model that is based on parts-
of-speech (POS) and learn probabilistic rules from
the POS tags of the words in the training corpus and
the alignment information. In addition to continu-
ous reordering rules that model short-range reorder-
ing (Rottmann and Vogel, 2007), we apply non-
continuous rules to address long-range reorderings
as typical for German-English translation (Niehues
and Kolss, 2009). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are encoded
in a word lattice which is used as input to the de-
coder.
2.2.4 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract also phrase pairs for origi-
nally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths. To limit the number of extracted
phrase pairs, we extract a source phrase only once
per sentence, even if it is found in different paths and
we only use long-range reordering rules to generate
the lattices for the training corpus.
2.2.5 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder during the search pro-
cess. This segmentation into phrases leads to the
loss of context information at the phrase boundaries.
The language model can make use of more target
side context. To make also source language context
available we use a bilingual language model, an ad-
ditional language model in the phrase-based system
in which each token consist of a target word and all
source words it is aligned to. The bilingual tokens
enter the translation process as an additional target
factor.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
The LIMSI system is built with n-code2, an open
source statistical machine translation system based
on bilingual n-grams.
2.3.2 n-code Overview
In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training this
model requires to reorder source sentences so as to
match the target word order. This is performed by a
stochastic finite-state reordering model, which uses
part-of-speech information3 to generalize reordering
patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a weak
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
use in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 data as development
set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2http://www.limsi.fr/Individu/jmcrego/n-code
3Part-of-speech information for English and German is com-
puted using the TreeTagger.
360
2.3.3 Data Preprocessing
Based on previous experiments which have
demonstrated that better normalization tools provide
better BLEU scores (K. Papineni and Zhu, 2002),
all the English texts are tokenized and detokenized
with in-house text processing tools (De?chelotte et
al., 2008). For German, the standard tokenizer sup-
plied by evaluation organizers is used.
2.3.4 Target n-gram Language Models
The English language model is trained assuming
that the test set consists in a selection of news texts
dating from the end of 2010 to the beginning of
2011. This assumption is based on what was done
for the 2010 evaluation. Thus, a development cor-
pus is built in order to create a vocabulary and to
optimize the target language model.
Development Set and Vocabulary In order to
cover different period, two development sets are
used. The first one is newstest2008. However, this
corpus is two years older than the targeted time pe-
riod. Thus a second development corpus is gath-
ered by randomly sampling bunches of 5 consecu-
tive sentences from the provided news data of 2010
and 2011.
To estimate a LM, the English vocabulary is first
defined by including all tokens observed in the Eu-
roparl and news-commentary corpora. This vocabu-
lary is then expanded with all words that occur more
that 5 times in the French-English giga-corpus, and
with the most frequent proper names taken from the
monolingual news data of 2010 and 2011. This pro-
cedure results in a vocabulary around 500k words.
Language Model Training All the training data
allowed in the constrained task are divided into 9
sets based on dates on genres. On each set, a
standard 4-gram LM is estimated from the 500k
word vocabulary with in-house tools using abso-
lute discounting interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 are first linearly interpolated.
The associated coefficients are estimated so as to
minimize the perplexity evaluated on the dev2010-
2011. The resulting LM and the 2010-2011 LM are
finally interpolated with newstest2008 as develop-
ment data. This two steps interpolation aims to avoid
an overestimate of the weight associated to the 2010-
2011 LM.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
the SYSTRAN baseline system in combination with
a statistical post editing (SPE) component.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k ?
800k entries per language pair).
The basic setup of the SPE component is identi-
cal to the one described in (L. Dugast and Koehn,
2007). A statistical translation model is trained on
the rule-based translation of the source and the tar-
get side of the parallel corpus. This is done sepa-
rately for each parallel corpus. Language models are
trained on each target half of the parallel corpora and
also on additional in-domain corpora. Moreover, the
following measures ? limiting unwanted statistical
effects ? were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is signif-
icantly reduced. In addition, entity translation
is handled more reliably by the rule-based en-
gine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus (whose target is identical
to the source). This was added to the parallel
text in order to improve word alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
361
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained 15M
phrases from the news/europarl corpora, provided
as training data for WMT 2011. Weights for these
separate models were tuned by the MERT algorithm
provided in the Moses toolkit (P. Koehn et al, 2007),
using the provided news development set.
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical mod-
els is selected as consensus translation. A deeper
description will be also given in the WMT11 sys-
tem combination paper of RWTH Aachen Univer-
sity. For this task only the A2L framework has been
used.
4 Experiments
We tried different system combinations with differ-
ent sets of single systems and different optimiza-
tion criteria. As RWTH has two different transla-
tion systems, we put the output of both systems into
system combination. Although both systems have
the same preprocessing, their hypotheses differ. Fi-
nally, we added for both RWTH systems two addi-
tional hypotheses to the system combination. The
two hypotheses of Jane were optimized on differ-
ent criteria. The first hypothesis was optimized on
BLEU and the second one on TER?BLEU. The first
RWTH phrase-based hypothesis was generated with
force alignment, the second RWTH phrase-based
hypothesis is a reranked version of the first one as
described in 2.1.4. Compared to the other systems,
the system by SYSTRAN has a completely different
approach (see section 2.4). It is mainly based on a
rule-based system. For the German?English pair,
SYSTRAN achieves a lower BLEU score in each
test set compared to the other groups. But since the
SYSTRAN system is very different to the others, we
still obtain an improvement when we add it also to
system combination.
We obtain the best result from system combina-
tion of all seven systems, optimizing the parameters
on BLEU. This system was the system we submitted
to the WMT 2011 evaluation.
For each dev set we obtain an improvement com-
pared to the best single systems. For newstest2008
and newstest2009 we get an improvement of 0.5
points in BLEU and 1.8 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. For newstest2010 we get an improvement
of 1.8 points in BLEU and 2.7 points in TER com-
pared to the best single system of RWTH. The sys-
tem combination weights optimized for the best run
are listed in Table 2. We see that although the single
system of SYSTRAN has the lowest BLEU scores,
it gets the second highest system weight. This high
value shows the influence of a completely different
system. On the other hand, all RWTH systems are
very similar, because of their same preprocessing
and their small variations. Therefor the system com-
bination parameter of all four systems by themselves
are relatively small. The summarized ?RWTH ap-
proach? system weight, though, is again on par with
the other systems.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU
compared to each single system. If the system
combination implementation can handle enough sin-
gle systems we would recommend to add all single
systems to the system combination. Although the
single system of SYSTRAN has the lowest BLEU
scores and the RWTH single systems are similar we
achieved the best result in using all single systems.
362
newstest2008 newstest2009 newstest2010 description
BLEU TER BLEU TER BLEU TER
22.73 60.73 22.50 59.82 25.26 57.37 sc (all systems) BLEU opt
22.61 60.60 22.28 59.39 25.07 56.95 sc (all systems - (1)) TER?BLEU opt
22.50 60.41 22.52 59.61 25.23 57.40 sc (all systems) TER?BLEU opt
22.19 60.09 22.05 59.31 24.74 56.89 sc (all systems - (4)) TER?BLEU opt
22.21 60.71 21.89 59.95 24.72 57.58 sc (all systems - (4,7)) TER?BLEU opt
22.22 60.45 21.79 59.72 24.32 57.59 sc (all systems - (3,4)) TER?BLEU opt
22.27 60.60 21.75 59.92 24.35 57.64 sc (all systems - (3,4)) BLEU opt
22.10 62.59 22.01 61.64 23.34 60.35 (1) Karlsruhe Institute of Technology
21.41 62.77 21.12 61.91 23.44 60.06 (2) RWTH PBT (FA) rerank +GW
21.11 62.96 21.06 62.16 23.29 60.26 (3) RWTH PBT (FA)
21.47 63.89 21.00 63.33 22.93 61.71 (4) RWTH jane + GW BLEU opt
20.89 61.05 20.36 60.47 23.42 58.31 (5) RWTH jane + GW TER?BLEU opt
20.33 64.50 19.79 64.91 21.97 61.44 (6) Limsi-CNRS
17.06 69.48 17.52 67.34 18.68 66.37 (7) SYSTRAN Software
Table 1: All systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results are in
percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model. sc denotes
system combination.
system weight
Karlsruhe Institute of Technology 0.350
RWTH PBT (FA) rerank +GW 0.001
RWTH PBT (FA) 0.046
RWTH jane + GW BLEU opt 0.023
RWTH jane + GW TER?BLEU opt 0.034
Limsi-CNRS 0.219
SYSTRAN Software 0.328
Table 2: Optimized systems weights for each system of the best system combination result.
Acknowledgments
This work was achieved as part of the QUAERO
Programme, funded by OSEO, French State agency
for innovation.
References
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
S.F. Chen and J.T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
T. Ward K. Papineni, S. Roukos and W. Zhu. 2002. Bleu:
363
a method for automatic evaluation of machine transla-
tion. In ACL ?02: Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, pages
311?318. Association for Computational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
364
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185?192,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joint WMT 2013 Submission of the QUAERO Project
?Stephan Peitz, ?Saab Mansour, ?Matthias Huck, ?Markus Freitag, ?Hermann Ney,
?Eunah Cho, ?Teresa Herrmann, ?Mohammed Mediani, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Quoc Khanh Do,
?Bianka Buschbeck, ?Tonio Wandmacher
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint submis-
sion of the QUAERO project for the
German?English translation task of the
ACL 2013 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013).
The submission was a system combina-
tion of the output of four different transla-
tion systems provided by RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy (KIT), LIMSI-CNRS and SYSTRAN
Software, Inc. The translations were
joined using the RWTH?s system com-
bination approach. Experimental results
show improvements of up to 1.2 points in
BLEU and 1.2 points in TER compared to
the best single translation.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in ma-
chine translation is mainly assigned to the four
groups participating in this joint submission. The
aim of this submission was to show the quality of
a joint translation by combining the knowledge of
the four project partners. Each group develop and
maintain their own different machine translation
system. These single systems differ not only in
their general approach, but also in the preprocess-
ing of training and test data. To take advantage
of these differences of each translation system, we
combined all hypotheses of the different systems,
using the RWTH system combination approach.
This paper is structured as follows. First, the
different engines of all four groups are introduced.
In Section 3, the RWTH Aachen system combina-
tion approach is presented. Experiments with dif-
ferent system selections for system combination
are described in Section 4. This paper is concluded
in Section 5.
2 Translation Systems
For WMT 2013, each QUAERO partner trained
their systems on the parallel Europarl (EPPS),
News Commentary (NC) corpora and the web-
crawled corpus. All single systems were tuned on
the newstest2009 and newstest2010 development
set. The newstest2011 development set was used
to tune the system combination parameters. Fi-
nally, on newstest2012 the results of the different
system combination settings are compared. In this
Section, all four different translation engines are
presented.
2.1 RWTH Aachen Single System
For the WMT 2013 evaluation, RWTH utilized a
phrase-based decoder based on (Wuebker et al,
2012) which is part of RWTH?s open-source SMT
toolkit Jane 2.1 1. GIZA++ (Och and Ney, 2003)
was employed to train a word alignment, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
After phrase pair extraction from the word-
aligned parallel corpus, the translation probabil-
ities are estimated by relative frequencies. The
standard feature set alo includes an n-gram lan-
guage model, phrase-level IBM-1 and word-,
phrase- and distortion-penalties, which are com-
bined in log-linear fashion. Furthermore, we used
an additional reordering model as described in
(Galley and Manning, 2008). By this model six
1http://www-i6.informatik.rwth-aachen.
de/jane/
185
additional feature are added to the log-linear com-
bination. The model weights are optimized with
standard Mert (Och, 2003a) on 200-best lists. The
optimization criterion is BLEU.
2.1.1 Preprocessing
In order to reduce the source vocabulary size trans-
lation, the German text was preprocessed by split-
ting German compound words with the frequency-
based method described in (Koehn and Knight,
2003). To further reduce translation complexity
for the phrase-based approach, we performed the
long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.2 Translation Model
We applied filtering and weighting for domain-
adaptation similarly to (Mansour et al, 2011) and
(Mansour and Ney, 2012). For filtering the bilin-
gual data, a combination of LM and IBM Model
1 scores was used. In addition, we performed
weighted phrase extraction by using a combined
LM and IBM Model 1 weight.
2.1.3 Language Model
During decoding a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl,
the 109 French-English, UN and LDC Gigaword
Fourth Edition corpora.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
The training data was preprocessed prior to the
training. Symbols such as quotes, dashes and
apostrophes are normalized. Then the first words
of each sentence are smart-cased. For the Ger-
man part of the training corpus, the hunspell2 lex-
icon was used, in order to learn a mapping from
old German spelling to new German writing rules.
Compound-splitting was also performed as de-
scribed in Koehn and Knight (2003). We also re-
moved very long sentences, empty lines, and sen-
tences which show big mismatch on the length.
2.2.2 Filtering
The web-crawled corpus was filtered using an
SVM classifier as described in (Mediani et al,
2011). The lexica used in this filtering task were
obtained from Giza alignments trained on the
2http://hunspell.sourceforge.net/
cleaner corpora, EPPS and NC. Assuming that this
corpus is very noisy, we biased our classifier more
towards precision than recall. This was realized
by giving higher number of false examples (80%
of the training data).
This filtering technique ruled out more than
38% of the corpus (the unfiltered corpus contains
around 2.4M pairs, 0.9M of which were rejected
in the filtering task).
2.2.3 System Overview
The in-house phrase-based decoder (Vogel, 2003)
is used to perform decoding. Optimization with
regard to the BLEU score is done using Minimum
Error Rate Training (MERT) as described in Venu-
gopal et al (2005).
2.2.4 Reordering Model
We applied part-of-speech (POS) based reordering
using probabilistic continuous (Rottmann and Vo-
gel, 2007) and discontinuous (Niehues and Kolss,
2009) rules. This was learned using POS tags gen-
erated by the TreeTagger (Schmid, 1994) for short
and long range reorderings respectively.
In addition to this POS-based reordering, we
also used tree-based reordering rules. Syntactic
parse trees of the whole training corpus and the
word alignment between source and target lan-
guage are used to learn rules on how to reorder the
constituents in a German source sentence to make
it match the English target sentence word order
better (Herrmann et al, 2013). The training corpus
was parsed by the Stanford parser (Rafferty and
Manning, 2008). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are en-
coded in a word lattice which is used as input to
the decoder.
Moreover, our reordering model was extended
so that it could include the features of lexicalized
reordering model. The reordering probabilities for
each phrase pair are stored as well as the origi-
nal position of each word in the lattice. During
the decoding, the reordering origin of the words
is checked along with its probability added as an
additional score.
2.2.5 Translation Models
The translation model uses the parallel data of
EPPS, NC, and the filtered web-crawled data. As
word alignment, we used the Discriminative Word
Alignment (DWA) as shown in (Niehues and Vo-
186
gel, 2008). The phrase pairs were extracted using
different source word order suggested by the POS-
based reordering models presented previously as
described in (Niehues et al, 2009).
In order to extend the context of source lan-
guage words, we applied a bilingual language
model (Niehues et al, 2011). A Discriminative
Word Lexicon (DWL) introduced in (Mauser et
al., 2009) was extended so that it could take the
source context also into the account. For this,
we used a bag-of-ngrams instead of representing
the source sentence as a bag-of-words. Filtering
based on counts was then applied to the features
for higher order n-grams. In addition to this, the
training examples were created differently so that
we only used the words that occur in the n-best list
but not in the reference as negative example.
2.2.6 Language Models
We build separate language models and combined
them prior to decoding. As word-token based
language models, one language model is built on
EPPS, NC, and giga corpus, while another one is
built using crawled data. We combined the LMs
linearly by minimizing the perplexity on the de-
velopment data. As a bilingual language model we
used the EPPS, NC, and the web-crawled data and
combined them. Furthermore, we use a 5-gram
cluster-based language model with 1,000 word
clusters, which was trained on the EPPS and NC
corpus. The word clusters were created using the
MKCLS algorithm.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine transla-
tion system based on bilingual n-gram3. In this
approach, the translation model relies on a spe-
cific decomposition of the joint probability of a
sentence pair using the n-gram assumption: a sen-
tence pair is decomposed into a sequence of bilin-
gual units called tuples, defining a joint segmen-
tation of the source and target. In the approach of
(Marin?o et al, 2006), this segmentation is a by-
product of source reordering which ultimately de-
rives from initial word and phrase alignments.
2.3.2 An overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using
3http://ncode.limsi.fr/
a n-gram model of (source,target) pairs (Casacu-
berta and Vidal, 2004). Training this model re-
quires to reorder source sentences so as to match
the target word order. This is performed by
a stochastic finite-state reordering model, which
uses part-of-speech information4 to generalize re-
ordering patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized re-
ordering models (Tillmann, 2004) aiming at pre-
dicting the orientation of the next translation unit;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which compensate for the system prefer-
ence for short translations. The four lexicon mod-
els are similar to the ones use in a standard phrase
based system: two scores correspond to the rel-
ative frequencies of the tuples and two lexical
weights estimated from the automatically gener-
ated word alignments. The weights associated to
feature functions are optimally combined using a
discriminative training framework (Och, 2003b).
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the
tuple extraction process. The resulting reordering
hypotheses are passed to the decoder in the form
of word lattices (Crego and Mario, 2006).
2.3.3 Continuous space translation models
One critical issue with standard n-gram translation
models is that the elementary units are bilingual
pairs, which means that the underlying vocabu-
lary can be quite large, even for small translation
tasks. Unfortunately, the parallel data available to
train these models are typically order of magni-
tudes smaller than the corresponding monolingual
corpora used to train target language models. It is
very likely then, that such models should face se-
vere estimation problems. In such setting, using
neural network language model techniques seem
all the more appropriate. For this study, we fol-
low the recommendations of Le et al (2012), who
propose to factor the joint probability of a sen-
tence pair by decomposing tuples in two (source
and target) parts, and further each part in words.
This yields a word factored translation model that
4Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
187
can be estimated in a continuous space using the
SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the com-
putational cost of computing n-gram probabilities.
The solution used here was to resort to a two pass
approach: the first pass uses a conventional back-
off n-gram model to produce a k-best list; in the
second pass, the k-best list is reordered using the
probabilities of m-gram SOUL translation models.
In the following experiments, we used a fixed con-
text size for SOUL of m= 10, and used k = 300.
2.3.4 Corpora and data pre-processing
All the parallel data allowed in the constrained
task are pooled together to create a single par-
allel corpus. This corpus is word-aligned using
MGIZA++5 with default settings. For the English
monolingual training data, we used the same setup
as last year6 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we also took advantage of our in-
house text processing tools for the tokenization
and detokenization steps (Dchelotte et al, 2008)
and our system is built in ?true-case?. As Ger-
man is morphologically more complex than En-
glish, the default policy which consists in treat-
ing each word form independently is plagued with
data sparsity, which is detrimental both at training
and decoding time. Thus, the German side was
normalized using a specific pre-processing scheme
(described in (Allauzen et al, 2010; Durgar El-
Kahlout and Yvon, 2010)), which notably aims at
reducing the lexical redundancy by (i) normalizing
the orthography, (ii) neutralizing most inflections
and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
In the past few years, SYSTRAN has been focus-
ing on the introduction of statistical approaches
to its rule-based backbone, leading to Hybrid Ma-
chine Translation.
The technique of Statistical Post-Editing
(Dugast et al, 2007) is used to automatically edit
the output of the rule-based system. A Statistical
Post-Editing (SPE) module is generated from a
bilingual corpus. It is basically a translation mod-
ule by itself, however it is trained on rule-based
5http://geek.kyloo.net/software
6The fifth edition of the English Gigaword
(LDC2011T07) was not used.
translations and reference data. It applies correc-
tions and adaptations learned from a phrase-based
5-gram language model. Using this two-step
process will implicitly keep long distance re-
lations and other constraints determined by the
rule-based system while significantly improving
phrasal fluency. It has the advantage that quality
improvements can be achieved with very little
but targeted bilingual data, thus significantly
reducing training time and increasing translation
performance.
The basic setup of the SPE component is identi-
cal to the one described in (Dugast et al, 2007).
A statistical translation model is trained on the
rule-based translation of the source and the target
side of the parallel corpus. Language models are
trained on each target half of the parallel corpora
and also on additional in-domain corpora. More-
over, the following measures - limiting unwanted
statistical effects - were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is sig-
nificantly reduced. In addition, entity trans-
lation is handled more reliably by the rule-
based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the
reference translation) is used to produce an
additional parallel corpus (whose target is
identical to the source). This was added to the
parallel text in order to improve word align-
ment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side
are also discarded.
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained on 2M
phrases from the news/europarl and Common-
Crawl corpora, provided as training data for WMT
2013. Weights for these separate models were
tuned by the Mert algorithm provided in the Moses
toolkit (Koehn et al, 2007), using the provided
news development set.
188
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
3 RWTH Aachen System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses gener-
ated with different translation engines. First, a
word to word alignment for the given single sys-
tem hypotheses is produced. In a second step a
confusion network is constructed. Then, the hy-
pothesis with the highest probability is extracted
from this confusion network. For the alignment
procedure, each of the given single systems gen-
erates one confusion network with its own as pri-
mary system. To this primary system all other hy-
potheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1. The final network for one source sen-
tence is the union of all confusion networks gen-
erated from the different primary systems. That
allows the system combination to select the word
order from different system outputs.
Before performing system combination, each
translation output was normalized by tokenization
and lowercasing. The output of the combination
was then truecased based on the original truecased
output.
The model weights of the system combination
are optimized with standard Mert (Och, 2003a)
on 100-best lists. We add one voting feature for
each single system to the log-linear framework of
the system combination. The voting feature fires
for each word the single system agrees on. More-
over, a word penalty, a language model trained on
the input hypotheses, a binary feature which pe-
nalizes word deletions in the confusion network
and a primary feature which marks the system
which provides the word order are combined in
this log-linear model. The optimization criterion
is 4BLEU-TER.
4 Experimental Results
In this year?s experiments, we tried to improve the
result of the system combination further by com-
bining single systems tuned on different develop-
Table 1: Comparison of single systems tuned on
newstest2009 and newstest2010. The results are
reported on newstest2012.
single systems tuned on newstest2012
newstest BLEU TER
KIT 2009 24.6 58.4
2010 24.6 58.6
LIMSI 2009 22.5 61.5
2010 22.6 59.8
SYSTRAN 2009 20.9 63.3
2010 21.2 62.2
RWTH 2009 23.7 60.8
2010 24.4 58.8
ment sets. The idea is to achieve a more stable
performance in terms of translation quality, if the
single systems are not optimized on the same data
set. In Table 1, the results of each provided single
system tuned on newstest2009 and newstest2010
are shown. For RWTH, LIMSI and SYSTRAN,
it seems that the performance of the single system
depends on the chosen tuning set. However, the
translation quality of the single systems provided
by KIT is stable.
As initial approach and for the final submis-
sion, we grouped single systems with dissimilar
approaches. Thus, KIT (phrase-based SMT) and
SYSTRAN (rule-based MT) tuned their system on
newstest2010, while RWTH (phrase-based SMT)
and LIMSI (n-gram) optimized on newstest2009.
To compare the impact of this approach, all pos-
sible combinations were checked (Table 2). How-
ever, it seems that the translation quality can not be
improved by this approach. For the test set (new-
stest2012), BLEU is steady around 25.6 points.
Even if the single system with lowest BLEU are
combined (KIT 2010, LIMSI 2009, SYSTRAN
2010, RWTH 2009), the translation quality in
terms of BLEU is comparable with the combina-
tion of the best single systems (KIT 2009, LIMSI
2010, SYSTRAN 2010, RWTH 2010). However,
we could gain 1.0 point in TER.
Due to the fact, that for the final submission the
initial grouping was available only, we kept this
189
Table 2: Comparison of different system combination settings. For each possible combination of systems
tuned on different tuning sets, a system combination was set up, re-tuned on newstest2011 and evaluated
on newstest2012. The setting used for further experiments is set in boldface.
single systems system combinations
KIT LIMSI SYSTRAN RWTH newstest2011 newstest2012
tuned on newstest BLEU TER BLEU TER
2009 2009 2009 2009 24.6 58.0 25.6 56.8
2010 2010 2010 2010 24.2 58.1 25.6 57.7
2010 2009 2009 2009 24.5 57.9 25.7 57.4
2009 2010 2009 2009 24.4 58.3 25.7 57.0
2009 2009 2010 2009 24.5 57.9 25.6 57.0
2009 2009 2009 2010 24.5 58.0 25.6 56.8
2009 2010 2010 2010 24.1 57.5 25.4 56.4
2010 2009 2010 2010 24.3 57.6 25.6 56.9
2010 2010 2009 2010 24.2 58.0 25.6 57.3
2010 2010 2010 2009 24.3 57.9 25.5 57.6
2010 2010 2009 2009 24.4 58.1 25.6 57.5
2009 2009 2010 2010 24.4 57.8 25.5 56.6
2009 2010 2010 2009 24.4 58.2 25.5 57.0
2009 2010 2009 2010 24.2 57.8 25.5 56.8
2010 2009 2009 2010 24.4 57.9 25.6 57.4
2010 2009 2010 2009 24.4 57.7 25.6 57.4
Table 3: Results of the final submission (bold-
face) compared with best single system on new-
stest2012.
newstest2011 newstest2012
BLEU TER BLEU TER
best single 23.2 60.9 24.6 58.4
system comb. 24.4 57.7 25.6 57.4
+ IBM-1 24.6 58.1 25.6 57.6
+ bigLM 24.6 57.9 25.8 57.2
combination. To improve this baseline further, two
additional models were added. We applied lexi-
cal smoothing (IBM-1) and an additional language
model (bigLM) trained on the English side of the
parallel data and the News shuffle corpus. The re-
sults are presented in Table 3.
The baseline was slightly improved by 0.2
points in BLEU and TER. Note, this system com-
bination was the final submission.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, the partners of the QUAERO
project (Karlsruhe Institute of Technology, RWTH
Aachen University, LIMSI-CNRS and SYSTRAN
Software, Inc.) provided a joint submission. By
joining the output of four different translation sys-
tems with RWTH?s system combination, we re-
ported an improvement of up to 1.2 points in
BLEU and TER.
Combining systems optimized on different tun-
ing sets does not seem to improve the translation
quality. However, by adding additional model, the
baseline was slightly improved.
All in all, we conclude that the variability in
terms of BLEU does not influence the final result.
It seems that using different approaches of MT in
a system combination is more important (Freitag
et al, 2012).
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
190
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Josep M. Crego and Jose? B. Mario. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on systran?s rule-based trans-
lation system. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 220?223, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Daniel Dchelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Markus Freitag, Stephan Peitz, Matthias Huck, Her-
mann Ney, Teresa Herrmann, Jan Niehues, Alex
Waibel, Alexandre Allauzen, Gilles Adda, Bianka
Buschbeck, Josep Maria Crego, and Jean Senellart.
2012. Joint wmt 2012 submission of the quaero
project. In NAACL 2012 Seventh Workshop on Sta-
tistical Machine Translation, pages 322?329, Mon-
treal, Canada, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 847?855, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantine, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
pages 177?180, Prague, Czech Republic, June.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An Automatic Metric for MT Evaluation
with High Levels of Correlation with Human Judg-
ments. pages 228?231, Prague, Czech Republic,
June.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Saab Mansour and Hermann Ney. 2012. A sim-
ple and effective weighted phrase extraction for ma-
chine translation adaptation. In International Work-
shop on Spoken Language Translation, pages 193?
200, Hong Kong, December.
Sab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA,
December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
191
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation Systems for IWSLT
2011. In Proceedings of the Eighth Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003a. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sap-
poro, Japan, July.
Franz Josef Och. 2003b. Minimum error rate training
in statistical machine translation. In ACL ?03: Proc.
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Evelyne Tzoukermann and SusanEditors Arm-
strong, editors, Proceedings of the ACL SIGDAT-
Workshop, pages 47?50. Kluwer Academic Publish-
ers.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Int. Conf. on Spo-
ken Language Processing, volume 2, pages 901?
904, Denver, Colorado, USA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
192
