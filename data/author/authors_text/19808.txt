Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1533?1542, Dublin, Ireland, August 23-29 2014.
Empirical Analysis of Aggregation Methods
for Collective Annotation
Ciyang Qing, Ulle Endriss, Raquel Fern
?
andez and Justin Kruger
Institute for Logic, Language and Computation
University of Amsterdam
{qciyang | justin.g.kruger}@gmail.com
{ulle.endriss | raquel.fernandez}@uva.nl
Abstract
We investigate methods for aggregating the judgements of multiple individuals in a linguistic
annotation task into a collective judgement. We define several aggregators that take the relia-
bility of annotators into account and thus go beyond the commonly used majority vote, and we
empirically analyse their performance on new datasets of crowdsourced data.
1 Introduction
Human annotation of linguistic resources has become indispensable in computational linguistics, es-
pecially with regards to semantic and pragmatic information, which is yet beyond the reach of robust
automatic labelling. Most annotation campaigns involve a small group of trained annotators who may
not always agree on their judgements. The reliability of the annotation is typically assessed by quan-
tifying the level of inter-annotator agreement, while the final annotation to be released is consensuated
amongst experts. In recent years, however, crowdsourcing methods such Amazon?s Mechanical Turk
(AMT) have shaken up this scenario by making it possible to rapidly recruit large numbers of untrainned
annotators at a low cost. This offers great opportunities?in particular, if we consider that the community
of speakers is the highest authority regarding linguistic knowledge?but also creates several challenges:
amongst others, how to obtain good quality annotations from untrainned and unmonitored individuals,
and how to combine large numbers of possibly conflicting judgements into a single joint annotation. In
this paper we focus on the latter challenge. Our aim is to investigate and empirically test methods for
aggregating the judgements of large numbers of individuals in a linguistic annotation task conducted via
crowdsourcing into a collective judgement.
Most researchers who turn to crowdsourcing to collect data use majority voting to combine the par-
ticipants? responses (Sayeed et al., 2011; Zarcone and R?ud, 2012; Venhuizen et al., 2013). Although in
the limit it makes sense to take the judgement of the majority as reflecting the view of the community,
in practice we cannot reach out to the full population of speakers, which means that the possible biases
amongst the participants we manage to recruit may distort the outcome. Also, given the nature of crow-
sourcing (rewarding speed rather than quality), some participants may not respond truthfully according
to their intuitions as speakers. To address these issues, we propose aggregation methods that go beyond
majority voting by taking into account the reliability of individual annotators at the time of aggregation.
1
Our approach is related to existing work on analysing the quality of annotated data by examining, for
instance, (dis)agreement patterns amongst annotators (Bhardwaj et al., 2010; Peldszus and Stede, 2013;
Ramanath et al., 2013). However, while the main aim of this kind of studies is to gain insight into the
difficulty of an annotation task or into the feasibility of using untrainned annotators for particular tasks,
our focus is on exploiting patterns of judgements for the purpose of aggregation into a single collective
annotation?an aspect that has received far less attention in the literature.
We make the following contributions: (i) we make available two new datasets of judgements gathered
with AMT for two multi-category annotation tasks; (ii) we define several aggregation methods based, on
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
Other aspects can contribute to reduce the shortcomings of crowdsourcing at earlier stages, such as task design and anno-
tator recruiting constraints. However, here we specifically deal with improving quality at the time of aggregation.
1533
the one hand, on an approach by inspired by social choice theory (Endriss and Fern?andez, 2013; Kruger
et al., 2014), and on the other hand, on probabilistic generative models pioneered by Dawid and Skene
(1979); and (iii) we systematically evaluate the performance of the proposed methods on three different
annotation tasks.
2
The paper is structured as follows: In the next section, we introduce our aggregation methods. In
Section 3, we evaluate their performance on different datasets and analyse the results. We then examine
two further aspects: the impact of the number of annotators in Section 4 and the presence of highly
unreliable annotators in Section 5. We conclude in Section 6 with plans for future work.
2 Aggregation Methods
In this section, we define several methods for deriving a collective judgement in a linguistic annotation
task from a set of individual annotations. We focus on simple classification tasks where collecting these
individual annotations via a crowdsourcing platform is feasible.
2.1 Notation and Terminology
In our model, an annotation task consists of three finite sets: the items J , the categories K, and the
annotators N . Each annotator is asked to label some of the items with a category. A group annotation A
is an |N | ? |J | matrix, with a
ij
representing the category k ? K that annotator i ? N assigned to item
j ? J . Let N
j
denote the set of annotators who annotated item j (i.e., a
ij
is undefined if i 6? N
j
).
We want to aggregate the information contained in a group annotation into a single collective annota-
tion that assigns a category to each item. An aggregator is a function F that maps a group annotation A
into a collective annotation F (A), a vector of categories with dimensionality |J | labelling every item with
a category. The most widely used aggregator is the simple plurality rule (SPR)?known as simple major-
ity in the two-category case?which returns a collective annotation where each item j is labelled with the
category chosen most often for j by the group, i.e., SPR(A)
j
? argmax
k?K
|{i ? N
j
| a
ij
= k}|. Since
the SPR may lead to a tie, if we require a single category for each item, a tie-breaking method (such as
random tie-breaking) must be adopted. For the purposes of this paper, we assign the special category
?undecided? whenever an aggregator produces a tie (this is reasonable also in practice: we would not
want to commit to a randomly chosen category for an annotated linguistic resource).
2.2 Frequency-based Aggregation
In previous work, we introduced (Endriss and Fern?andez, 2013) and further refined (Kruger et al., 2014)
a framework for deriving a collective annotation inspired by social choice theory. They propose so-called
bias-correcting rules (BCR?s), which try to take the reliability of annotators into account by considering
the frequencies with which annotators choose certain categories. For example, if annotator i uses cate-
gory k very often, then this might be a sign that i is overusing k and we should give her votes for k less
weight. However, if k is also a frequent choice of the population of annotators at large, then this might
again temper that effect.
For a given group annotation A, define the individual frequency of annotator i choosing category k?
Freq
i
(k)?as the number of times i chooses k, divided by the total number of items she annotates. Define
the global frequency of k?Freq(k)?as the number of times k is chosen by someone, divided by the total
number of individual annotations. Thus, if Freq
i
(k) is high, particularly if Freq
i
(k) > Freq(k), we may
want to give a relatively low weight to any instance of annotator i choosing catgeory k.
Every BCR defines a family of weights w
ik
, specifying for each annotator i ? N and each category
k ? K how much weight to give to i?s choice of k:
F
w
(A)
j
? argmax
k?K
?
i?N
j
|a
ij
=k
w
ik
2
The new datasets and an implementation of our aggregation methods are available at http://www.illc.uva.nl/
Resources/CollectiveAnnotation/.
1534
Diff difference-based BCR w
ik
= 1 + Freq(k)? Freq
i
(k)
Com complement-based BCR w
ik
= 1 + 1/|K| ? Freq
i
(k)
Rat ratio-based BCR w
ik
= Freq(k)/Freq
i
(k)
Inv inverse-based BCR w
ik
= 1/Freq
i
(k)
Table 1: Weights used for canonical Bias Correcting Rules.
In case of a tie, we assign category ?undecided?. Table 1 defines the weights for four specific BCR?s.
Thus, for example, if an annotator uses k in 50% of the cases, while the general population only uses k
in 20% of all cases, then under Diff she has weight 0.7 whenever she chooses k. Note that Com and Inv
do not take global frequencies into account, while Diff and Rat do.
2.3 Agreement-based Aggregation
Suppose each item has a true (but unknown) category (its gold standard). We may view an annotator?s
judgement as a noisy signal of the gold standard. We now want to design an aggregator as a maximum
likelihood estimator for this ground truth. This approach has been pioneered by Dawid and Skene (1979).
Variants have been used for diverse purposes by, amongst others, Snow et al. (2008), Carpenter (2008),
Raykar et al. (2010), Ipeirotis et al. (2010), Li et al. (2013), and Passonneau and Carpenter (2013).
Let p(a
ij
= k | g
j
= k
?
), with k not necessarily distinct from k
?
, be the probability of agent i ? N
j
annotating item j with category k ? K, given that the gold standard category of j is k
?
? K. If we can
obtain estimates of these probabilities, then we can use them to calibrate the weights of the annotators.
The challenge, particularly for multi-category annotation tasks, is that the number of probabilities to
estimate is fairly large (in particular, it is quadratic in |K|). To be able to provide reasonable estimates,
we need a large amount of data from every individual annotator. But this precisely we do not have in
crowdsourcing: we have a lot of data, but it comes from many different annotators. We thus make two
simplifying assumptions, aimed at aggressively reducing the number of parameters to estimate:
3
(1) We assume that p(a
ij
=k
?
| g
j
=k
?
), i.e., annotator i?s probability of choosing the correct category,
does not depend on either j or k
?
. It only depends on i?s accuracy. Thus, we can abbreviate
acc
i
:= p(a
ij
=k
?
| g
j
=k
?
).
(2) We assume that when annotator i does not choose the correct category k
?
, then she is equally likely
to pick any of the wrong categories k 6= k
?
: p(a
ij
=k | g
j
=k
?
) =
1?acc
i
|K|?1
.
Assumption (1) is not uncommon (Li et al., 2013), but it clearly is a limiting assumption: accuracy not
depending on j means that we cannot model the fact that some items are more difficult to label correctly;
accuracy not depending on k means that we cannot model the fact that some categories are harder to
comprehend than others. Assumption (2) and its alternatives only come into play when there are more
than two categories; as large parts of the literature focus on the two-category case, this issue has received
less attention. One of the limitations of assumption (2) is that we cannot model that some categories may
?look similar? and are likely to get confused with each other.
On the positive side, in our simplified model we only have a single parameter to estimate for each
annotator, namely its accuracy acc
i
. Now suppose, hypothetically, we knew the acc
i
?s (which we do not
in practice). Which category should we pick for item j? To answer this question we need to consider
probabilities such as p(g
j
= k | A
j
), the probability that k is the true category for item j given our
observation of column A
j
. If we do not want to make any assumptions regarding possible priors for
either gold standards or annotation biases (i.e., if we opt for the default assumption of uniform priors),
then we can instead work with p(A
j
| g
j
= k). Specifically, we should choose k over k
?
if p(A
j
| g
j
=
k) > p(A
j
| g
j
=k
?
), i.e., if:
?
i|a
ij
=k
acc
i
?
i|a
ij
=k
?
1?acc
i
|K|?1
?
i|a
ij
6?{k,k
?
}
1?acc
i
|K|?1
>
?
i|a
ij
=k
?
acc
i
?
i|a
ij
=k
1?acc
i
|K|?1
?
i|a
ij
6?{k,k
?
}
1?acc
i
|K|?1
?
i|a
ij
=k
(|K|?1)?acc
i
1?acc
i
>
?
i|a
ij
=k
?
(|K|?1)?acc
i
1?acc
i
3
That is, we are trading generality of the model against estimation quality of its parameters (see also Section 3.4).
1535
Taking logarithms on both sides, we see that giving each annotator a weight of log
(|K|?1)?acc
i
1?acc
i
results in
an optimal aggregator. Let us call the corresponding aggregator the oracle rule Ora. Importantly, this
is not a practically useful rule, as in reality we do not know the acc
i
?s. As we shall see, however, it is a
useful benchmark, as it allows us to distinguish between loss in quality due to the simplicity of our model
and loss in quality accrued during estimation (given that Ora is perfect w.r.t. the latter dimension).
4
In practice, we need to estimate the acc
i
?s. We use a particularly simple method and estimate acc
i
as
i?s agreement agr
i
with the SPR, defined as follows:
5
agr
i
:=
|{j ? J | a
ij
= SPR(A)
j
}|+ 0.5
|{j ? J | i annotates j}|+ 1
We call the rule we obtain using this method, i.e., the rule giving weight log
(|K|?1)?agr
i
1?agr
i
to annotator i,
the agreement-based rule Agr. There are two natural refinements of Agr one might consider. First, we
could attempt to take priors regarding gold standards into account. If p(k) is the prior probability of
encountering (true) category k, then we get p(g
j
= k | A
j
) ? p(A
j
| g
j
= k) ? p(k). This corresponds
to adding log p(k) as an extra weight in favour of category k. We can estimate p(k) using either Freq(k)
or the SPR. The second possible refinement is to iterate the process used to estimate acc
i
, i.e., to use Agr
in place of SPR to compute better estimates agr
?
i
of acc
i
, and so forth. That is, we could use the EM
algorithm (Dawid and Skene, 1979) to estimate acc
i
. As we shall see, Agr outperforms both of these
refinements for the datasets considered in this paper.
3 Performance on Different Datasets
In this section, we evaluate the performance of our aggregation methods on three datasets from three
different categorical annotation tasks for which gold standard annotations are readily available. One of
these tasks?Recognising Textual Entailment?is a binary classification task and includes non-expert
annotations collected by Snow et al. (2008). The other two tasks?Preposition Sense Disambiguation
and Question Dialogue Acts?are multi-category tasks for which we have collected new crowdsourced
annotations for the purposes of the present study.
6
3.1 Recognising Textual Entailment (RTE)
This dataset is based on the task proposed by Dagan et al. (2006) in the PASCAL Recognizing Textual
Entailment (RTE) Challenge. The RTE task involves deciding whether the meaning of a sentence (the hy-
pothesis) can be inferred from a text. The original RTE1 Challenge testset consists of 800 text-hypothesis
pairs (e.g., T :?In central Antioquia two ranges of the Colombian Andeas meet?, H:?Antioquia is in
Colombia.?) with a gold standard annotation that classifies each of them as either true (1) or false (0),
depending on whether H can be inferred from T or not. The released expert annotation is perfectly
balanced, with 400 items annotated as 0 and 400 as 1.
Snow et al. (2008) used Amazon?s Mechanical Turk (AMT) to collect 10 non-expert annotations for
each of the 800 items. The annotation task included a total of 164 AMT workers who annotated between
20 items (124 annotators) and 800 items each (only one annotator). Amongst the non-expert annotations,
category 1 is slightly more frequent (? 57%) than category 0.
Table 2a shows the results of applying the aggregation rules (and the oracle rule) to this data. Here
(as later in Tables 2b and 2c), the first columns shows observed agreement (A) between the collective
annotation output by each rule and the gold standard.
7
The following columnss show precision and
recall for each category. We can see that all rules outperform the SPR.
8
Agr yields better results (93.3%)
4
Snow et al. (2008) used Dawid and Skene?s model to calibrate annotator judgements in terms of the gold standard. In
contrast, we only use Ora as a benchmark to get a better understanding of the limitations of our probabilistic model.
5
The smoothing terms (0.5 and 1) ensure that agr
i
will never be 0 or 1, i.e., log
(|K|?1)?agr
i
1?agr
i
is always well-defined.
6
For practical reasons, we have opted for evaluating our methods against a gold standard. However, we note that in linguistic
tasks, especially those concerning semantics and pragmatics, there may simply not be a ?true? category?a collective annotation
may be the closest we can get to representing the view of the community.
7
All aggregators assign category ?undecided? in case of a tie. Therefore, any ties are counted as instances of disagreement.
8
The SPR leads to 65 ties; the other rules lead to none.
1536
A 0 1
SPR 0.856 .96/.79 .91/.93
Com 0.916 .93/.90 .91/.93
Inv 0.893 .87/.92 .91/.87
Diff 0.915 .94/.88 .89/.95
Rat 0.908 .94/.88 .88/.94
Agr 0.933 .93/.93 .93/.94
[Ora] 0.941 .93/.96 .96/.93
(a) RTE
A 1 2 3
SPR 0.813 .89/.96 .82/.40 .82/.92
Com 0.820 .87/.95 .70/.46 .82/.92
Inv 0.807 .88/.95 .62/.51 .82/.85
Diff 0.833 .86/.96 .80/.46 .82/.93
Rat 0.840 .87/.96 .81/.49 .82/.93
Agr 0.827 .85/.98 .88/.40 .80/.93
[Ora] 0.833 .85/.98 .88/.43 .81/.93
(b) PSD
A 1 2 3 4
SPR 0.857 .86/.98 .87/1.0 .92/.75 .90/.42
Com 0.870 .87/.98 .87/1.0 .88/.77 .88/.49
Inv 0.877 .91/.91 .94/.98 .84/.77 .72/.73
Diff 0.867 .84/.98 .87/1.0 .89/.78 .91/.44
Rat 0.870 .84/.99 .87/1.0 .92/.77 .91/.47
Agr 0.867 .84/.99 .87/1.0 .92/.77 .91/.44
[Ora] 0.870 .85/.99 .87/1.0 .92/.77 .91/.47
(c) QDA
Table 2: Observed agreement with the gold standard and precision/recall per category for each task.
than any of the BCR?s in this case. For the SPR, category 1 has higher recall than precision, while the
opposite is the case for category 0. This is in line with the slightly higher frequency of category 1 in the
AMT annotations. The BCR?s should be able to correct for this bias and to some extent they do (note
the increase in category 0?s recall: 88% or higher for any of the BCR?s vs. 79% for the SPR). In this
dataset, the best-performing BCR is Com (91.6% agreement), keeping a good balance between precision
and recall for both categories. If we use the refinement of Agr with priors, then the observed agreement
drops slightly (to 92.9% if we estimate gold standard distributions using Freq(k), and to 93.1% if we use
the SPR). If we use the EM algorithm to estimate acc
i
, the system stabilises after six iterations and the
resulting rule also does slightly worse than Agr (93.0%).
3.2 Preposition Sense Disambiguation (PSD)
This annotation task is based on the dataset used in the SemEval 2007 task on word-sense disambigua-
tion of prepositions (Litkowski and Hargraves, 2007). The SemEval dataset consists of roughly 25,000
sentences each containing one of the 34 most common English prepositions. The gold standard annota-
tion was constructed by a single lexicographer who tagged each preposition instance with a sense from
the sense inventories given by the Oxford Dictionary of English (ODE).
For our non-expert data collection, we used the 150 sentences with the preposition among, which
according to ODE has four senses. We simplified the task by collapsing senses 3 and 4, as there is only
one item classified with sense 4 by the gold standard and that sense is closest to sense 3.
9
The annotation
task was conducted using AMT. We showed the workers the following sense definitions of among and
asked them to select the appropriate sense for each sentence:
(1) situated more or less centrally in relation to other things, e.g., ?There are flowers hidden among the roots of the trees.?
(2) being a member of a larger set, e.g., ?Snakes are among the animals most feared by man.?
(3) shared by some members of a group or community, e.g., ?Members of the government bickered among themselves.?
The distribution of categories according to the gold standard is 37.3%, 23.3%, and 39.3% for sense 1, 2,
and 3, respectively. The non-expert annotation task included 45 AMT workers who annotated between
15 items (26 annotators) and 150 items each (only one annotator; another annotated 135 items). Amongst
the AMT annotations, the relative frequency of the categories is 40.6%, 18.8%, and 40.6%, respectively.
The results are shown in Table 2b.
10
The rules with the highest agreement with the gold standard are
Diff (83.3%) and Rat (84%), i.e., the rules that take into account the global frequency of the categories.
Rat outperforms not only the other three BCR?s and the SPR (81.3%) but also Agr (82.7%) and Ora
(83.3%). Recall for sense 2 (the rarest category) is low across rules, although less so for the BCR?s,
which manage to correct slightly for the annotators? bias against this category.
11
9
The original ODE sense definitions for among can be found at http://tinyurl.com/ode-among.
10
The SPR leads to 6 ties; the other rules lead to none. The two refinements of Agr (priors and EM) do not affect the outcome.
11
After inspecting the data, we suspect that the gold standard overuses sense 2. For instance, in the folowing sentence among
is tagged with sense 2 although sense 1 seems more appropriate: ?[. . . ] like icebergs 90 per cent is under the water and that is
making them incredibly difficult to see among the waves.?
1537
3.3 Question Dialogue Acts (QDA)
The second dataset we collected is based on the Switchboard corpus (Godfrey et al., 1992). The cor-
pus includes a gold standard annotation prepared by trained annotators, labelling each utterance with a
dialogue act tag from the SWBD-DAMSL annotation scheme (Jurafsky et al., 1997).
For our crowdsourcing experiment, we restricted ourselves to four types of question dialogue acts:
Yes-No questions, Wh-questions, Declarative questions (including both declarative wh- and yes-no ques-
tions), and Rhetorical questions. We extracted 300 questions from the corpus, 35% of which were anno-
tated as Yes-No in the gold standard, 30% as Wh, 20% as Declarative, and 15% as Rhetorical. The AMT
workers were shown the following category definitions (here slightly simplified for space reasons):
(1) Yes-No: Questions with a standard form that could be answered with ?yes? or ?no? (?Is that the only pet that you have??)
(2) Wh: Questions with a standard form that ask for specific information using wh-words (?What kind of pet do you have??)
(3) Declarative: Questions with a statement-like form that nevertheless ask for an answer (?You have how many pets.?)
(4) Rhetorical: Questions that do not need to be answered. They can have the form of any of the question types above, but
they are asked only to make a point (?If I ever wanted to have a pet, how could I work??)
Each item consists of a short dialogue fragment showing three utterances before and after the question
to be annotated. The AMT workers were asked to classify the highlighted question with one of the four
question types above. Here is a sample item (with reduced context for space reasons):
A: I understand.
A: Where is home for you?
B: Originally, was born in Missouri.
A total of 63 AMT workers participated in the annotation task, annotating between 10 items (24 an-
notators) and 200 items each (only one annotator). Amongst these non-expert annotations, the relative
frequencies for category 1 to 4 are 36.6%, 34.1%, 18.4%, and 10.9%, respectively.
Table 2c shows the results of applying the aggregation rules to this data, plus the outcome of the oracle.
12
Inv yields the best result (87.7%), even outperforming Ora (87%). The annotators tend to overuse the
common categories (1 and 2), resulting in high recall but low precision. In contrast, the less frequent
categories (3 and 4) tend to be underused, resulting in high precision but low recall. Note how applying
Inv leads to particularly high recall for rhetorical questions (category 4). The price to pay is the drop in
precision for this category compared to the other rules. The dual effect is that precision for Yes-No (1)
and Wh (2) is higher with Inv than with the other rules, while recall is lower.
3.4 Comparative Analysis
First, let us compare Agr and Ora. The good performance of Agr suggests that our simple probabilistic
model is not too simplistic; the trade-off between loss in generality and gain in ability to estimate param-
eters mentioned in Section 2 appears to be appropriate. The fact that Ora outperforms Agr only slightly
suggests that the number of parameters in our model is sufficiently small to be estimated well using the
amount of data typically available in linguistic annotation taks conducted via crowdsourcing.
Second, the fact that Agr (modestly) outperforms its refinement using an estimated prior can be ex-
plained by the fact that, in our datasets, annotators tend to overuse frequent categories and underuse rare
categories. The reason why iterating the rule used to estimate accuracies did not improve performance
of Agr for our datasets is less clear, but may be related to the well-known fact that EM can get stuck in a
local optimum. The positive take-away message is that the simplest form of our agreement rule resulted
in the best performance (at least for our three datasets).
Third, the differences in performance between different BCR?s point at an interesting difference in
types of bias. Recall that Com and Inv judge the reliability of an annotator only in terms of her own
annotations and penalise frequent use of a category. Diff and Rat correct for this effect in case the global
frequency is high as well. This means that if a population of annotators has a shared bias against or in
favour of a category, then Diff and Rat cannot track this well. This explains the fact that Com outperforms
Diff and Inv outperforms Rat in the QDA data (see Table 2c): in this task many annotators appeared to
12
The SPR leads to 7 ties; the other rules to none. Once again, the observed agreement for Agr drops slightly for the two
refinements discussed (priors and EM).
1538
3 4 5 6 7 8 9 100.70
0.75
0.80
0.85
0.90
0.95
SPRComInvDiff
RatAgrOra
(a) RTE
3 4 5 6 7 8 9 100.74
0.76
0.78
0.80
0.82
0.84
SPRComInvDiff
RatAgrOra
(b) PSD
3 4 5 6 7 8 9 100.78
0.80
0.82
0.84
0.86
0.88
SPRComInvDiff
RatAgrOra
(c) QDA
Figure 1: Observed agreement with the gold standard (y-axis) for varying NAI (x-axis).
have difficulties recognising rhetorical questions, i.e., they had a shared bias against labelling an item as
Rhetorical. For a dataset with clear individual biases, on the other hand, we would expect Diff/Rat to
outperform Com/Inv. We do not have a clear case of such a phenomenon in the data analysed here. For
the PSD task, Diff/Rat do outperform Com/Inv (see Table 2b), but we believe that the explanation for this
finding is a different one: Arguably, the gold standard annotation overuses category 2 (see Footnote 11).
This means that high-quality annotators are seen as underusing it and get penalised by Com/Inv. For
Diff/Rat this effect is tempered by the fact that the population as a whole is underusing category 2
(relative to the questionable gold standard).
Finally, much can be learned from contraposing the frequency- and agreement-based approach. Sup-
pose the gold standard is uniformly distributed (as for RTE). Then the expected value of Freq
i
(k) is
1
|K|
,
i.e., it does not depend on acc
i
at all. Thus, the two approaches track entirely different parameters, yet
both achieve respectable results. This suggests that combining them might prove fruitful (see Section 5).
Certainly, an approach based on a richer probabilistic model would be able to track both kinds of parame-
ters, but as we had argued, this might be infeasible with the relatively small amount of data per annotator
we can collect through crowdsourcing. In some sense, what we have done with our rules is trying to
make up for the scarcity of data by exploiting our domain knowledge (e.g., regarding the relationships
between observed frequency and annotator reliability) to reduce the parameter space.
4 Impact of Number of Annotators
The cost and quality of an annotated linguistic resource created via crowdsourcing crucially depends on
the number of annotators that label each item. Having low numbers of coders will make the task more
affordable (in terms of time and money), but it will also make the aggregation process more vulnerable
to low-quality annotators. Snow et al. (2008) showed how the number of annotators per item (henceforth
NAI) influences the performance of the SPR. Here we further explore the impact of NAI on the quality
of the collective annotation obtained by different aggregation methods.
For each of the three datasets and each NAI n (3 6 n 6 9), we randomly resampled n annotations for
each set of items presented to a worker in one go (i.e., for each HIT in AMT terminology). This allowed
us to generate a subset of the original dataset with n annotators per item. We generated 1000 such random
subsets for each n, applied our aggregators to each subset (and also computed the oracle outcome). We
then calculated the average observed agreement with the gold standard. To test whether the differences
observed are statistically significant, we calculated the difference in performance between pairs of rules
on each subset and computed the 95% (one-sided) confidence intervals by using its distribution over the
1000 subsets. If the proportion of subsets on which this difference is strictly greater than 0 is higher than
95%, we consider the difference to be significant.
The results are shown in Figure 1. We can see that, as the NAI increases, the performance of the
rules generally improves (except for the oscillation of the SPR due to tie-breaking). This improvement
is greater when the NAI is small (from 3 to 5), which suggest that a minimum of 5 annotators per item is
1539
0 6
SPR 0.856 0.911
Com 0.916 0.930
Inv 0.893 0.933
Diff 0.915 0.928
Rat 0.908 0.926
Agr 0.933 0.929
[Ora] 0.941 0.944
(a) RTE
0 9
SPR 0.813 0.820
Com 0.820 0.840
Inv 0.807 0.840
Diff 0.833 0.820
Rat 0.840 0.833
Agr 0.827 0.827
[Ora] 0.833 0.827
(b) PSD
0 6
SPR 0.857 0.867
Com 0.870 0.883
Inv 0.877 0.903
Diff 0.867 0.873
Rat 0.870 0.877
Agr 0.867 0.867
[Ora] 0.870 0.883
(c) QDA
Table 3: Effect on observed agreement when removing 6 spammers in RTE, 9 in PSD, and 6 in QDA.
recommended. We can also observe that Agr has a robust performance on all datasets when the NAI is
between 5 and 7: its improvement over the SPR is statistically significant in all cases for the three tasks,
except on PSD when NAI is 7, in which case it is neither significantly better nor significantly worse
than the SPR. Note that in all datasets Agr only needs 6 or 7 annotators per item to achieve an accuracy
comparable to the SPR using 10 annotators per item.
The robustness of Agr with low NAI is not surprising, given that it already assigns low weights to
workers who consistently disagree with the majority. Discounting such problematic workers is partic-
ularly important when there are relatively few workers per item. But as the NAI increases, it becomes
more likely that random annotators will cancel each other out. It is then that we observe the greatest
advantage of using BCR?s. This can be seen in the plots for PSD and QDA with high NAI. In those
cases the improvement of the best performing BCR?s (Rat on PSD and Inv on QDA) over the other rules
approaches significance although does not reach the 95% threshold (e.g., on QDA when the NAI is 9,
Inv is strictly better than Agr for 93.4% of the subsets).
5 Removal of Low-Quality Annotators
Next we discuss how removing easily recognisable low-quality annotators (?spammers?) before aggre-
gation affects the quality of results. The BCR?s make the implicit assumption that annotators are sincere.
This can be problematic, given the nature of crowdsourcing, where it is not uncommon to encounter
workers giving random rather than truthful responses (Sheng et al., 2008; Raykar and Yu, 2012). BCR?s
are vulnerable to this phenomenon. Here we propose to combine the frequency- and agreement-based
approach by using the agreement rate of an annotator with the SPR outcome to identfy and remove
spammers prior to applying the frequency-based BCR?s.
We take spammers to be those annotators that annotate a large number of items (i.e., we have sufficient
evidence to judge) and that systematically deviate from the plurality outcome. In the specific context of
our datasets, we have implemented this idea by labelling as spammers those annotators who annotated
at least 20% of the total number of items and whose agreement rate with the SPR is below the median
agreement rate. This corresponds to 6 annotators in the RTE dataset, 9 in the PSD dataset, and 6 in the
QDA dataset. The effect of removing these low-quality annotators from the population can be seen in
Table 3 showing observed agreement of the different aggregation rules (and the oracle rule) with the gold
standard before and after spammer removal.
The results show that, with one exception, after removing spammers the performance of the BCR?s
improves significantly. The exception concerns Diff and Rat for the PSD dataset. Recall that the gold
standard for this dataset, arguably, overuses category 2 (see Footnote 11 and Section 3.4). That is, high-
quality annotators are (wrongly) judged to be underusing category 2. Before spammer removal, this effect
is tempered by the presence of a few annotators delivering ?random? annotations (thereby artificially
increasing the frequency of category 2). After spammer removal, this positive effect is diminished and
rules such as Diff and Rat suffer in performance. Con and Inv, on the other hand, can compensate for this
effect simply by giving very high weights to those (high-quality) annotators who still use the relatively
rare category 2. Also for RTE and QDA, amongst the BCR?s the rules not based on global frequencies,
i.e., Com and Inv, benefit most. Indeed, after spammer removal Com/Inv perform better than Diff/Rat
for all three datasets. Overall, Inv with spammer removal is our best-performing rule.
1540
Not surprisingly, Agr and Ora gain relatively little from spammer removal since, given our definition of
a spammer, the removed annotators already had very low weights to begin with. In fact, the performance
of these aggregation rules may even drop slightly after removing spammers (see Tables 3a and 3b).
6 Conclusions
We have argued that simply using the majority/plurality rule to aggregate individual linguistic judgments
in a crowdsourcing annotation task is far from optimal. Instead, we have proposed several methods that
weight the annotators? judgements by exploiting either the frequency with which they choose particular
categories or the degree to which they agree with the full population of annotators. We have tested
our methods on existing datasets and we have also created two new datasets. Our results show how
annotation tasks with different characteristics can benefit from different types of aggregation methods.
Our aggregation methods result in small but robust gains across datasets, both in terms of accuracy
achieved and in terms of the number of annotators required to obtain acceptable results.
Besides BCR?s, in our previous work we also proposed a greedy consensus rule, albeit only for the
two-category case (Endriss and Fern?andez, 2013) . This rule sequentially locks in simple majorities in
the order of relative majority strength, but along the way disregards annotators who disagree with too
many of those strong majorities. It performs well on the RTE dataset (almost as well as Agr). Intuitively
speaking, it can track item difficulty, by first settling the easy items (with clear majorities) and thereby
learning which annotators are most reliable to then have them decide on the harder items. Here we have
not included this rule as there is no single most natural way of generalising it to the multi-category case.
Arriving at such a generalisation in a principled manner is an important direction for future work.
It would also be interesting to get a clearer understanding of the links between methods for assessing
inter-annotator agreement (Artstein and Poesio, 2008) and methods of aggregation (i.e., methods that
may be applied to data of possibly rather poor inter-annotator agreement, as is the case for parts of our
datasets). A relevant observation in this context is that the notions of individual and global frequency at
the core of our BCR?s also play a role in agreement coefficients, namely to compute chance agreement:
pi (Scott, 1955) uses global frequencies and ? (Cohen, 1960) uses individual frequencies.
While the definition of Agr was motivated by a simple probabilistic model, the BCR?s were motivated
by rules of thumb regarding links between observed frequencies and reliability. We have noted before
that the BCR?s do not track the same phenomena as Agr; rather, they seem to complement each other, an
observation we have exploited explicitly when removing spammers before applying a BCR. Identifying
a suitable probabilistic model for our frequency-based BCR?s promises to be a fruitful future line of
research, as it would allow for a better comparison (and eventually integration) of the two approaches.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4):555?596.
Vikas Bhardwaj, Rebecca J Passonneau, Ansaf Salleb-Aouissi, and Nancy Ide. 2010. Anveshan: a framework for
analysis of multiple annotators? labeling behavior. In Proc. 4th Linguistic Annotation Workshop, pages 47?55.
ACL.
Bob Carpenter. 2008. Multilevel Bayesian Models of Categorical Data Annotation. Technical report, LingPipe.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement,
20(1):37?46.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge.
In Machine Learning Challenges, volume 3944 of LNCS, pages 177?190. Springer-Verlag.
Alexander P. Dawid and Allan M. Skene. 1979. Maximum likelihood estimation of observer error-rates using the
EM algorithm. Applied Statistics, 28(1):20?28.
Ulle Endriss and Raquel Fern?andez. 2013. Collective annotation of linguistic resources: Basic principles and
a formal model. In Proc. 51st Annual Meeting of the Association for Computational Linguistics (ACL-2013),
pages 539?549.
1541
John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. SWITCHBOARD: Telephone Speech Corpus
for Research and Devlopment. In Proc. IEEE Conference on Acoustics, Speech, and Signal Processing, pages
517?520.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang. 2010. Quality Management on Amazon Mechanical Turk.
In Proc. 2nd Human Computation Workshop (HCOMP-2010).
Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca. 1997. Switchboard SWBD-DAMSL shallow-discourse-
function-annotation coder?s manual, draft 13. Technical Report TR 97-02, Institute for Cognitive Science,
University of Colorado at Boulder.
Justin Kruger, Ulle Endriss, Raquel Fern?andez, and Ciyang Qing. 2014. Axiomatic analysis of aggregation meth-
ods for collective annotation. In Proc. 13th Int?l Conference on Autonomous Agents and Multiagent Systems
(AAMAS-2014), pages 1185?1192. IFAAMAS.
Hongwei Li, Bin Yu, and Dengyong Zhou. 2013. Error rate analysis of labeling by crowdsourcing. In Proc.
Machine Learning meets Crowdsourcing, Workshop at the Int?l Conference on Machine Learning (ICML-2013).
Kenneth C. Litkowski and Orin Hargraves. 2007. SemEval-2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proc. 4th Int?l Workshop on Semantic Evaluations (SemEval-2007).
Rebecca J. Passonneau and Bob Carpenter. 2013. The benefits of a model of annotation. In Proc. 7th Linguistic
Annotation Workshop, pages 187?195. ACL.
Andreas Peldszus and Manfred Stede. 2013. Ranking the annotators: An agreement study on argumentation
structure. In Proc. 7th Linguistic Annotation Workshop, pages 196?204. ACL.
Rohan Ramanath, Monojit Choudhury, Kalika Bali, and Rishiraj Saha Roy. 2013. Crowd prefers the middle path:
A new iaa metric for crowdsourcing reveals turker biases in query segmentation. Proc. 51st Annual Meeting of
the Association for Computational Linguistics (ACL-2013), pages 1713?1722.
Vikas Raykar and Shipeng Yu. 2012. Eliminating spammers and ranking annotators for crowdsourced labeling
tasks. Journal of Machine Learning Research, 13:491?518.
Vikas Raykar, Shipeng Yu, Linda Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda
Moy. 2010. Learning from crowds. Journal of Machine Learning Research, 11:1297?1322.
Asad Sayeed, Bryan Rusk, Martin Petrov, Hieu Nguyen, Timothy Meyer, and Amy Weinber. 2011. Crowdsourcing
syntactic relatedness judgements for opinion mining in the study of information technology adoption. In Proc.
Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH-2011).
William A. Scott. 1955. Reliability of content analysis: the case of nominal scale coding. Public Opinion
Quaterly, 19(3):321?325.
Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeirotis. 2008. Get another label? Improving data quality and
data mining using multiple, noisy labelers. In Proc. 14th ACM Int?l Conference on Knowledge Discovery and
Data Mining (KDD-208).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast?but is it good?
Evaluating non-expert annotations for natural language tasks. In Proc. Conference on Empirical Methods in
Natural Language Processing (EMNLP-2008), pages 254?263.
Noortje Venhuizen, Valerio Basile, Kilian Evang, and Johan Bos. 2013. Gamification for word sense labeling. In
Proc. 10th Int?l Conference on Computational Semantics (IWCS-2013), pages 397?403.
Alessandra Zarcone and Stefan R?ud. 2012. Logical metonymies and qualia structures: An annotated database of
logical metonymies for German. In Proc. Language Resources and Evaluation Conference (LREC-2012), pages
1799?1804.
1542
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 151?159,
Dublin, Ireland, August 23-24 2014.
Vagueness and Learning: A Type-Theoretic Approach
Raquel Fern
?
andez
Institute for Logic, Language
and Computation
University of Amsterdam
raquel.fernandez@uva.nl
Staffan Larsson
Department of Philosophy, Linguistics
and Theory of Science
University of Gothenburg
sl@ling.gu.se
Abstract
We present a formal account of the mean-
ing of vague scalar adjectives such as ?tall?
formulated in Type Theory with Records.
Our approach makes precise how percep-
tual information can be integrated into
the meaning representation of these pred-
icates; how an agent evaluates whether an
entity counts as tall; and how the proposed
semantics can be learned and dynamically
updated through experience.
1 Introduction
Traditional semantic theories such as those de-
scribed in Partee (1989) and Blackburn and
Bos (2005) offer precise accounts of the truth-
conditional content of linguistic expressions, but
do not deal with the connection between meaning,
perception and learning. One can argue, however,
that part of getting to know the meaning of lin-
guistic expressions consists in learning to identify
the individuals or the situations that the expres-
sions can describe. For many concrete words and
phrases, this identification relies on perceptual in-
formation. In this paper, we focus on characteris-
ing the meaning of vague scalar adjectives such
as ?tall?, ?dark?, or ?heavy?. We propose a for-
mal account that brings together notions from tra-
ditional formal semanticswith perceptual informa-
tion, which allows us to specify how a logic-based
interpretation function is determined and modified
dynamically by experience.
The need to integrate language and percep-
tion has been emphasised by researchers work-
ing on the generation and resolution of referring
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
expressions (Kelleher et al., 2005; Reiter et al.,
2005; Portet et al., 2009) and, perhaps even more
strongly, on the field of robotics, where ground-
ing language on perceptual information is critical
to allow artificial agents to autonomously acquire
and verify beliefs about the world (Siskind, 2001;
Steels, 2003; Roy, 2005; Skocaj et al., 2010).
Most of these approaches, however, do not build
on theories of formal semantics for natural lan-
guage. Here we choose to formalise our account
in a theoretical framework known as Type Theory
with Records (TTR), which has been shown to be
suitable for formalising classic semantic aspects
such as intensionality, quantification, and nega-
tion (Cooper, 2005a; Cooper, 2010; Cooper and
Ginzburg, 2011) as well as less standard phenom-
ena such as linguistic interaction (Ginzburg, 2012;
Purver et al., 2014), perception and action (Dob-
nik et al., 2013), and semantic coordination and
learning (Larsson, 2009). In this paper we use
TTR to put forward an account of the semantics of
vague scalar predicates like ?tall? that makes pre-
cise how perceptual information can be integrated
into their meaning representation; how an agent
evaluates whether an entity counts as tall; and how
the proposed semantics for these expressions can
be learned and dynamically updated through lan-
guage use.
We start by giving a brief overview of TTR and
explaining how it can be used for classifying en-
tities as being of particular types integrating per-
ceptual information. After that, in Section 3, we
describe the main properties of vague scalar pred-
icates. Section 4 presents a probabilistic TTR for-
malisation of the meaning of ?tall?, which captures
its context-dependence and its vague character. In
Section 5, we then offer an account of how that
meaning representation is acquired and updated
with experience. Finally, in Section 6 we discuss
related work, before concluding in Section 7.
151
2 Meaning as Classification in TTR
In this section we give a brief and hence inevitably
partial introduction to Type Theory with Records.
For more comprehensive introductions, we refer
the reader to Cooper (2005b) and Cooper (2012).
2.1 Type Theory with Records: Main Notions
As in any type theory, the most central notion in
TTR is that of a judgement that an object a is
of type T , written as a : T . In TTR judgements
are seen as fundamentally related to perception, in
the sense that perceiving inherently involves cate-
gorising what we perceive. Some common basic
types in TTR are Ind (the type of individuals) and
R
+
(the type of positive real numbers). All basic
types are members of a special type Type. Given
types T
1
and T
2
, we can create the function type
T
1
? T2 whose domain are objects of type T
1
and whose range are objects of type T
2
. Types
can also be constructed from predicates and ob-
jects P (a
1
, . . . , a
n
). Such types are called ptypes
and correspond roughly to propositions in first or-
der logic. In TTR, propositions are types of proofs,
where proofs can be a variety of things, from situ-
ations to sensor readings (more on this below).
Next, we introduce records and record types.
These are structured objects made up of pairs ?l, v?
of labels and values that are displayed in a matrix:
(1) a. A record type:
?
?
?
?
`
1
: T
1
`
2
: T
2
(`
1
)
. . .
`
n
: T
n
(`
1
, `
2
, . . . , `
n?1
)
?
?
?
?
b. A record: r =
?
?
?
?
?
`
1
= a
1
`
2
= a
2
. . .
`
n
= a
n
. . .
?
?
?
?
?
Record r in (1b) is of the record type in (1a) if
and only if a
1
: T
1
, a
2
: T
2
(a
1
), . . . , and a
n
:
T
n
(a
1
, a
2
, . . . , a
n?1
). Note that the record may
contain more fields but would still be of type (1a)
if the typing condition holds. Records and record
types can be nested so that the value of a label is
itself a record (or record type). We can use paths
within a record or record type to refer to specific
bits of structure: for instance, we can use r.`
2
to
refer to a
2
in (1b).
As can be seen in (1a), the labels `
1
, . . . `
n
in a
record type can be used elsewhere to refer to the
values associated with them. This is a common
way of constructing ptypes where the arguments
of a predicate are entities that have been intro-
duced before in the record type. A sample record
and record type are shown in (2).
(2)
?
?
x = a
c
man
= prf(man(a))
c
run
= prf(run(a))
?
?
:
?
?
x : Ind
c
man
: man(x)
c
run
: run(x)
?
?
In (2), a is an entity of type individual and prf(P )
is used as a placeholder for proofs of ptypes P .
In the record type above, the ptypes man(x) and
run(x) constructed from predicates are dependent
on x (introduced earlier in the record type).
2.2 Perceptual Meaning
Larsson (2013) proposes a system formalised in
TTR where some perceptual aspects of meaning
are represented using classifiers. For example, the
meaning of ?right? (as in ?to the right of ?) involves
a two-input perceptron classifier ?
right
(w, t, r),
specified by a weight vector w and a threshold
t, which takes as input a context r including an
object x and a position-sensor reading sr
pos
. The
sensor reading consists of a vector containing two
real numbers representing the space coordinates of
x. The classifier classifies x as either being to the
right on a plane or not.
1
(3) if r :
[
x : Ind
sr
pos
: RealVector
]
, then
?
right
(w, t, r) =
{
right(r.x) if (r.sr
pos
? w) > t
? right(r.x) otherwise
As output we get a record type containing either a
ptype right(x) or its negation, ? right(x). Larsson
(2013) proposes that readings from sensors may
count as proofs of such ptypes. A classifier can
be used for judging x as being of a particular type
on the grounds of perceptual information. A per-
ceptual proof for right(x) would thus include the
output from the position sensor that is directed to-
wards x. Here, this output would be the space co-
ordinates of x.
3 Vague Scalar Predicates
Scalar predicates such as ?tall?, ?long? and ?ex-
pensive?, also called ?relative gradable adjectives?
(Kennedy, 2007), are interpreted with respect to a
1
We are here assuming that we have a definition of dot
product for TTR vectors a:RealVector
n
and b:RealVector
n
such that a ? b = ?
n
i=1
a
i
b
i
= a
1
b
1
+ a
2
b
2
+ . . .+ a
n
b
n
. We
also implicitly assume that the weight vector and the sensor
reading vector have the same dimensionality.
152
scale, i.e., a dimension such as height, length, or
cost along which entities for which the relevant di-
mension is applicable can be ordered. This makes
scalar predicates compatible with degree morphol-
ogy, like comparative and superlative morphemes
(?taller than?, ?the longest?) and intensifier mor-
phemes such as ?very? or ?quite?. In this pa-
per, our focus is on the so-called positive form of
these adjectives (e.g. ?tall? as opposed to ?taller?
or ?tallest?).
A property that distinguishes the positive form
from the comparative and the superlative forms is
its context-dependance. To take a common exam-
ple: If Sue?s height is 180cm, she may be appro-
priately described as a tall woman, but probably
not as a tall basketball player. Thus, what counts
as tall can vary from context to context, with the
most relevant contextual parameter being a com-
parison class relative to which the adjective is in-
terpreted (e.g., the set of women, the set of bas-
ketball players, etc.). In addition to being context-
dependent, positive-form scalar predicates are also
vague, in the sense that they give rise to borderline
cases, i.e., entities for which it is unclear whether
the predicate holds or not.
Vagueness is certainly a property that affects
most natural language expressions, not only scalar
adjectives. However, scalar adjectives have a
relatively simple semantics (they are often uni-
dimensional) and thus constitute a perfect case-
study for investigating the properties and effects of
vagueness on language use. Gradable adjectives
have received a high amount of attention in the
formal semantics literature. It is common to dis-
tinguish between two main approaches to their se-
mantics: delineation-based and degree-based ap-
proaches. The delineation approach is associated
with the work of Klein (1980), who proposes that
gradable adjectives denote partial functions de-
pendent on a comparison class. They partition the
comparison class into three disjoint sets: a positive
extension, a negative extension, and an extension
gap (entities for which the predicate is neither true
nor false). In contrast, degree-based approaches
assume a measure function m mapping individu-
als x to degrees on a particular scale (degrees of
height, degrees of darkness, etc.) and a standard
of comparison or degree threshold ? (again, de-
pendent on a comparison class) such that x be-
longs to the adjective?s denotation if m(x) > ?
(Kamp, 1975; Pinkal, 1979; Pinkal, 1995; Barker,
2002; Kennedy and McNally, 2005; Kennedy,
2007; Solt, 2011; Lassiter, 2011).
We build on degree approaches but adopt a
perception-based perspective and take a step fur-
ther to formalise how the meaning of these pred-
icates can be learned and constantly updated
through language use.
4 A Perceptual Semantics for ?Tall?
To exemplify our approach, we will use the scalar
predicate ?tall? throughout.
4.1 Context-sensitivity
We first focus on capturing the context-
dependence of relative scalar predicates. For
this we define a type T
ctxt
as follows:
(4) T
ctxt
=
?
?
c : Type
x : c
h : R
+
?
?
The context (ctxt) of a scalar predicate like ?tall?
is a record of the type in (4), which includes: a
type c (typically a subtype of Ind) representing the
comparison class; an individual x within the com-
parison class (the argument of tall); a perceived
measure on the relevant scale(s), in this case the
perceived height h of x expressed as a positive real
number.
The context presupposes the acquisition of sen-
sory input from the environment. In particular, it
assumes that an agent using such a representation
is able to classify the entity in focus x as being
of type c and is able to use some height sensor to
obtain an estimate of x?s height (the value of h is
the sensor reading). We thus forgo the inclusion of
an abstract measure function in the representation.
In an artificial agent, this may be accomplished by
image processing software for detecting and mea-
suring objects in a digital image.
Besides the ctxt, we also assume a standard
threshold of tallness ?
tall
of the type given in (5).
?
tall
is a function from a type specifying a com-
parison class to a height value, which corresponds
to a tallness threshold for that comparison class.
(In Section 5 we will discuss how such a threshold
may be computed.)
(5) ?
tall
: Type? R
+
The meaning of ?tall? involves a classifier for tall-
ness, ?
tall
, of the following type:
(6) ?
tall
: (Type? R
+
, T
ctxt
)? Type
153
We define this classifier as a one-input perceptron
that compares the perceived height h of an indi-
vidual x to the relevant threshold ? determined by
a comparison class c. Thus, if ? : Type? R
+
and
r : T
ctxt
, then:
?
tall
(?, r) =
{
tall(r.x) if r.h > ?(r.c)
?tall(r.x) otherwise
Simplifying somewhat, we can represent the mea-
ning of ?tall?, tall, as a record specifying the type
of context (T
ctxt
) where an utterance of ?tall? can
be made, the parameter of the tallness classifier
(the threshold ?), and a function f which is applied
to the context to produce the content of ?tall?.
(7)
tall =
?
?
?
?
?
?
?
?
?
?
T
ctxt
=
?
?
c : Type
x : c
h : R
+
?
?
? = ?
tall
f = ?r : T
ctxt
.
[
sit = r
sit-type =
[
c
tall
: ?
tall
(?, r)
]
]
?
?
?
?
?
?
?
?
?
?
The output of the function f is an Austinian propo-
sition (Cooper, 2005b): a judgement that a situa-
tion (sit, represented as a record r of type T
ctxt
),
is of a particular type (specified in sit-type). In the
case of tall, the context of utterance (which instan-
tiates r) is judged to be of the type where there is
an individual x which is either tall or not tall, ac-
cording to the output of the classifier ?
tall
. The
context of utterance in the sit field will include the
height-sensor reading, which means that the sen-
sor reading is part of the proof of the sit-type indi-
cating that x is tall (or not, as the case may be).
Thus, to decide whether to refer to some indi-
vidual x as tall or to evaluate someone else?s utter-
ance describing x as tall, an agent applies the func-
tion tall.f to the current situation, represented as a
record r : T
ctxt
. As an example, let us consider a
situation that includes the context in (8), resulting
from observing John Smith as being 1.88 meters
tall (assuming this is our scale of tallness):
(8) ctxt =
?
?
c = Human
x = john smith
h = 1.88
?
?
Let us assume that given the comparison class
Human, ?
tall
(Human) = 1.87. In this case,
tall.f(ctxt) will compute as shown in (9). The re-
sulting Austinian proposition corresponds to the
agent?s judgement that the situation in sit is one
where John Smith counts as tall.
(9) ?r : T
ctxt
.
[
sit = r
sit-type =
[
c
tall
: ?
tall
(?
tall
, r)
]
]
(
?
?
c = Human
x = john smith
h = 1.88
?
?
) =
?
?
?
?
sit =
?
?
c = Human
x = john smith
h = 1.88
?
?
sit-type =
[
c
tall
: tall(john smith)
]
?
?
?
?
4.2 Vagueness
According to the above account, ?tall? has a
precise interpretation: given a degree of height
and a comparison class, the threshold sharply
determines whether tall applies or not. There
are several ways in which one can account for
vagueness?amongst others, by introducing per-
ceptual uncertainty (possibly inaccurate sensor
readings). Here, in line with Lassiter (2011), we
opt for substituting the precise threshold with a
noisy, probabilistic threshold. We consider the
threshold to be a normal random variable, which
can be represented by the parameters of its Gaus-
sian distribution, the mean ? and the standard de-
viation ? (the noise width).
2
To incorporate this modification into our ap-
proach, we update the tallness classifier ?
tall
we
had defined in (6) so that it now takes as parame-
ters ?
tall
and ?
tall
, both of them dependent on the
comparison class and hence of type Type? R
+
.
The output of the classifier is now a probability
rather than a ptype such as tall(x) or?tall(x). Be-
fore indicating how this probability is computed,
we give the type of the vague version of the clas-
sifier in (10) and the vague representation of the
meaning of ?tall? in (11).
(10)?
tall
: (Type?R
+
, Type?R
+
, T
ctxt
)? [0, 1]
(11)
tall =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
T
ctxt
=
?
?
c : Type
x : c
h : R
+
?
?
? = ?
tall
? = ?
tall
f = ?r : T
ctxt
.
?
?
sit = r
sit-type =
[
c
tall
: tall(r.x)
]
prob = ?
tall
(?, ?, r)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
2
Which noise function may be the most appropriate is an
empirical question we do not tackle in this paper. Our choice
of Gaussian noise follows Schmidt et al. (2009)?see Sec-
tion 5.1.
154
The output of the function tall.f is now a prob-
abilistic Austinian proposition (Cooper et al.,
2014). Like before, the proposition expresses a
judgement that a situation sit is of a particular
type. But here the judgement is probabilistic?it
encodes the belief of an agent concerning the like-
lihood that sit is of a type where x counts as tall.
Since we take the noisy threshold to be a normal
random variable, given a particular ? and ?, we
can calculate the probability that the height r.h of
individual r.x counts as tall as follows:
?
tall
(?, ?, r) =
1
2
[
1 + erf
(
r.h? ?(r.c)
?(r.c)
?
2
)]
Here erf is the error function, defined as
3
erf(x) =
2
?
pi
?
x
t=0
e
?t
2
dt
The error function defines a sigmoid shape (see
Figure 1), in line with the upward monotonicity
of ?tall?. The output of ?
tall
(?, ?, r) corresponds
to the probability that h will exceed the normal
random threshold with mean ? and deviation ?.
Figure 1: Plot of the error function.
Let us consider an example. Assume that we have
?
tall
(Human) = 1.87 and ?
tall
(Human) = 0.05
(see Section 5.1 below for justification of the latter
value). Let?s also assume the same ctxt as above
in (8). In this case, tall.f(ctxt) will compute as in
(12), given that
?
tall
(?
tall
, ?
tall
,
?
?
c=Human
x=john smith
h=1.88
?
?
) =
1
2
[
1 + erf
(
1.88? 1.87
0.05
?
2
)]
= 0.579
3
For an explanation of this standard definition, see http:
//en.wikipedia.org/wiki/Error_function,
which is the source of the graph in Figure 1.
(12) ?r : T
ctxt
.
?
?
sit = r
sit-type =
[
c
tall
: tall(r.x)
]
prob = ?
tall
(?
tall
, ?
tall
, r)
?
?
(
?
?
c = Human
x = john smith
h = 1.88
?
?
) =
?
?
?
?
?
?
sit =
?
?
c = Human
x = john smith
h = 1.88
?
?
sit-type =
[
c
tall
: tall(john smith)
]
prob = 0.579
?
?
?
?
?
?
This probability can now be used in further prob-
abilistic reasoning, to decide whether to refer to
an individual x as tall, or to evaluate someone
else?s utterance describing x is tall. For exam-
ple, an agent may map different probabilities to
different adjective qualifiers of tallness to yield
compositional phrases such as ?sort of tall?, ?quite
tall?, ?very tall?, ?extremely tall?, etc. The mean-
ings of these composed adjectival phrases could
specify probability ranges trained independently.
Compositionality for vague perceptual meanings,
and the interaction between compositionality and
learning, is an exciting area for future research.
4
5 Learning from Language Use
In this section we consider possibilities for com-
puting the noisy threshold we have introduced
in the previous section and discuss how such a
threshold and the probabilistic judgements it gives
rise to are updated with language use.
5.1 Computing the Noisy Threshold
We assume that agents keep track of judgements
made by other agents. More concretely, for a
vague scalar predicate like ?tall?, we assume that
an agent will have at its disposal a set of obser-
vations consisting of entities of a particular type
T (a comparison class such as Human) that have
been judged to be tall, together with their observed
heights. Judgements of tallness may vary across
individuals?indeed, such variation (both inter-
and intra-individual) is a hallmark of vague pred-
icates. We use ?
T
tall
to refer to the set of heights
of those entities x : T that have been considered
tall by some individual. From this agent-specific
set of observations, which is constantly updated as
the agent is exposed to new judgements by other
individuals, we want to compute a noisy threshold,
4
See Larsson (2013) for a sketch of compositionality for
perceptual meaning.
155
which the agent uses to make her own judgements
of tallness, as specified in (11).
Different functions can be used to compute ?
tall
and ?
tall
from ?
T
tall
. What constitutes an appro-
priate function is an empirical matter and what
the most suitable function is possibly varies across
predicates (what may apply to ?tall? may not be
suitable for ?dark? or ?expensive?, for example).
Hardly any work has been done on trying to iden-
tify how the threshold is computed from experi-
ence. A notable exception, however, is the work of
Schmidt et al. (2009), who collect judgements of
people asked to indicate which items are tall given
distributions of items of different heights. Schmidt
and colleagues then propose different probabilis-
tic models to account for the data and compare
their output to the human judgements. They ex-
plore two types of models: threshold-based mod-
els and category-based or cluster models. The best
performing models within these two types perform
equally well and the study does not identify any
advantages of one type over the other one. Since
we have chosen threshold models as our case-
study, we focus our attention on those here.
Each of the threshold models tested by Schmidt
et al. (2009) corresponds to a possible way of com-
puting the mean ?
tall
of a noisy threshold from a
set of observations. The best performing threshold
model in their study is the relative height by range
model, where (in our notation):
(13) relative height by range (RH-R): ?
tall
(T ) =
max(?
T
tall
)? k ? (max(?
T
tall
)?min(?
T
tall
))
Here max(?
T
tall
) and min(?
T
tall
) stand for the
maximum and the minimum height, respectively,
of the items that have been judged to be tall
by some individual. According to this threshold
model, any item within the top k% of the range
of heights that have been judged to be tall counts
as tall. The model includes two parameters, k and
a noise-width parameter that in our approach cor-
responds to ?
tall
. Schmidt et al. (2009) report
that the best fit of their data was obtained with
k = 29% and ?
tall
= 0.05.
5.2 Updating Vague Meanings
We now want to specify how the vague meaning
of ?tall? is updated as an agent is exposed to new
judgements via language use. Our setting so far
offers a straightforward solution to this: If a new
entity x : T with height h is referred to as tall, the
agent adds h to its set of observations ?
T
tall
and
recomputes ?
tall
(Human), for instance using RH-
R as defined in (13). If RH-H is used, ideally the
value of k and ?
tall
should be (re)estimated from
?
T
tall
. For the sake of simplicity, however, here
we will assume that these two parameters take the
values experimentally validated by Schmidt et al.
(2009) and are kept constant. An update to ?
tall
will take place if it is the case that h > max(?
T
tall
)
or h < min(?
T
tall
). This in turn will trigger un
update to the probability outputted by ?
tall
.
As an example, let us assume that our
initial set of observations is ?
Human
tall
=
{1.87, 1.92, 1, 90, 1.75, 1.80} (recall this corre-
sponds to the perceived heights of individuals
that have been described as tall by some agent).
This means that max(?
Human
tall
) = 1.92 and
min(?
Human
tall
) = 1.75. Hence, given (13):
(14) ?
tall
(Human) =
1.92? 0.29 ? (1.92? 1.75) = 1.87
Let?s assume we now make an observation where
a person of height 1.72 is judged to be tall. This
will mean that the set of observations is now
?
Human
tall
= {1.87, 1.92, 1, 90, 1.75, 1.80, 1.72}
and consequently min(?
Human
tall
) = 1.72, which
yields an updated mean of the noisy threshold:
(15) ?
tall
(Human) =
1.92? 0.29 ? (1.92? 1.72) = 1.862
If we were to re-evaluate John Smith?s tallness in
light of this observation, we would get a new prob-
ability 0.64 that he is tall (in contrast to the earlier
probability of 0.579 given in (12)).
5.3 Possible Extensions
The set of observations ?
Human
tall
can be derived
from a set of Austinian propositions correspond-
ing to instances where people have been judged
to be tall. To update from an Austinian proposi-
tion p we simply add p.sit.h to ?
tall
Human
and re-
compute ?
tall
(p.c). Note that we are here treating
these Austinian propositions as non-probabilistic.
This seems to make sense since an addressee does
not have direct access to the probability associated
with the judgement of the speaker. If we were to
take these probabilities into account (for instance,
the use of a hedge in ?sort of tall? may be used
to make inferences about such probabilities), and
if those probabilities are not always 1, we would
need a different way of computing ?
tall
than the
156
one specified so far.
Somewhat related to the point above, note that
in our approach we treat all judgements equally,
i.e., we do not distinguish between possible dif-
ferent levels of trustworthiness amongst speakers.
An agent who is told that an entity with height h
is tall adds that observation to its knowledge base
without questioning the reliability of the speaker.
This is clearly a simplification. For instance, there
is developmental evidence showing that children
are more sensitive to reliable speakers than to un-
reliable ones during language acquisition (Scofield
and Behrend, 2008).
6 Other Approaches
Within the literature in formal semantics, Las-
siter (2011) has put forward a proposal that ex-
tends in interesting ways earlier work by Barker
(2002) and shares some aspects with the account
we have presented here. Operating in a probabilis-
tic version of classical possible-worlds semantics,
Lassiter assumes a probability distribution over a
set of possible worlds and a probability distribu-
tion over a set of possible languages. Each pos-
sible language represents a precise interpretation
of a predicate like ?tall?: tall
1
= ?x.x?s height ?
5?6?; tall
2
= ?x.x?s height ? 5?7?; and so forth.
Lassiter thus treats ?metalinguistic belief? (repre-
senting an agent?s knowledge of the meaning of
words) in terms of probability distributions over
precise languages. Since each precise interpreta-
tion of ?tall? includes a given threshold, this can
be seen as defining a probability distribution over
possible thresholds, similarly to the noisy thresh-
old we have used in our account. Lassiter, how-
ever, is not concerned with learning.
Within the computational semantics literature,
DeVault and Stone (2004) describe an imple-
mented system in a drawing domain that is able to
interpret and execute instructions including vague
scalar predicates such as ?Make a small circle?.
Their approach makes use of degree-based seman-
tics, but does not take into account comparison
classes. This is possible in their drawing domain
since the kind of geometric figures it includes
(squares, rectangles, circles) do not have intrinsic
expected properties (size, length, etc). Their focus
is on modelling how the threshold for a predicate
such as ?small? is updated during an interaction
with the system given the local discourse context.
For instance, if the initial context just contains a
square, the size of that square is taken to be the
standard of comparison for the predicate ?small?.
The user?s utterance ?Make a small circle? is then
interpreted as asking for a circle of an arbitrary
size that is smaller than the square.
In our characterisation of the context-sensitivity
of vague gradable adjectives in Section 4.1, we
have focused on their dependence on general com-
parison classes corresponding to types of entities
(such as Human, Woman, etc) with expected prop-
erties such as height. Thus, in contrast to DeVault
and Stone (2004), who focus on the local context
of discourse, we have focused on what could be
called the global context (an agent?s experience re-
garding types of entities and their expected prop-
erties). How these two types of context interact
remains an open question, which we plan to ex-
plore in our future work (see Kyburg and Morreau
(2000), Kemp et al. (2007), and Fern?andez (2009)
for pointers in this direction).
7 Conclusions and future work
Traditional formal semantics theories postulate a
fixed, abstract interpretation function that medi-
ates between natural language expressions and the
world, but fall short of specifying how this func-
tion is determined or modified dynamically by
experience. In this paper we have presented a
characterisation of the semantics of vague scalar
predicates such as ?tall? that clarifies how their
context-dependent meaning and their vague char-
acter are connected with perceptual information,
and we have also shown how this low-level per-
ceptual information (here, real-valued readings
from a height sensor) connects to high level logical
semantics (ptypes) in a probabilistic framework.
In addition, we have put forward a proposal for
explaining how the meaning of vague scalar ad-
jectives like ?tall? is dynamically updated through
language use.
Tallness is a function of a single value (height),
and is in this sense a uni-dimensional pred-
icate. Indeed, most linguistic approaches to
vagueness focus on uni-dimensional predicates
such as ?tall?. However, many vague predicates
are multi-dimensional, including nouns for posi-
tions (?above?), shapes (?hexagonal?), and colours
(?green?), amongst many others. Together with
compositionality (mentioned at the end of Sec-
tion 4.2), generalisation of the present account to
multi-dimensional vague predicates is an interest-
ing area of future development.
157
Acknowledgements
The first author acknowledges the support of the
Netherlands Organisation for Scientific Research
(NWO) and thanks the Centre for Language Tech-
nology at the University of Gothenburg for gen-
erously funding research visits that led to the
work presented in this paper. The second au-
thor acknowledges the support of Vetenskapsr?adet,
project 2009-1569, Semantic analysis of interac-
tion and coordination in dialogue (SAICD); the
Department of Philosophy, Linguistics, and The-
ory of Science; and the Centre for Language Tech-
nology at the University of Gothenburg.
References
Chris Barker. 2002. The dynamics of vagueness. Lin-
guistics & Philosophy, 25(1):1?36.
Patrick Blackburn and Johan Bos. 2005. Represen-
tation and Inference for Natural Language: A First
Course in Computational Semantics. CSLI Publica-
tions.
Robin Cooper and Jonathan Ginzburg. 2011. Negation
in dialogue. In Proceedings of the 15th Workshop on
the Semantics and Pragmatics of Dialogue (SemDial
2011), Los Angeles (USA).
Robin Cooper, Simon Dobnik, Shalom Lappin, and
Staffan Larsson. 2014. A probabilistic rich type
theory for semantic interpretation. In Proceedings
of the EACL Workshop on Type Theory and Natural
Language Semantics (TTNLS).
Robin Cooper. 2005a. Austinian truth, attitudes and
type theory. Research on Language and Computa-
tion, 3(4):333?362, December.
Robin Cooper. 2005b. Austinian truth, attitudes and
type theory. Research on Language and Computa-
tion, 3:333?362.
Robin Cooper. 2010. Generalized quantifiers and clar-
ification content. In Pawe? ?upkowski and Matthew
Purver, editors, Aspects of Semantics and Pragmat-
ics of Dialogue. SemDial 2010, 14th Workshop on
the Semantics and Pragmatics of Dialogue, Pozna?n.
Polish Society for Cognitive Science.
Robin Cooper. 2012. Type theory and semantics in
flux. In Ruth Kempson, Nicholas Asher, and Tim
Fernando, editors, Handbook of the Philosophy of
Science, volume 14: Philosophy of Linguistics. El-
sevier BV. General editors: Dov M. Gabbay, Paul
Thagard and John Woods.
David DeVault and Matthew Stone. 2004. Interpret-
ing vague utterances in context. In Proceedings of
the 20th International Conference on Computational
Linguistics (COLING?04), pages 1247?1253.
Simon Dobnik, Robin Cooper, and Staffan Larsson.
2013. Modelling language, action, and perception
in type theory with records. In Constraint Solving
and Language Processing, Lecture Notes in Com-
puter Science, pages 70?91. Springer.
Raquel Fern?andez. 2009. Salience and feature vari-
ability in definite descriptions with positive-form
vague adjectives. In Workshop on the Production
of Referring Expressions: Bridging the gap between
computational and empirical approaches to refer-
ence (CogSci?09).
Jonathan Ginzburg. 2012. The Interactive Stance. Ox-
ford University Press.
Hans Kamp. 1975. Two theories of adjectives. In
E. Keenan, editor, Formal Semantics of Natural Lan-
guage, pages 123?155. Cambridge University Press.
John Kelleher, Fintan Costello, and Josef van Genabith.
2005. Dynamically structuring, updating and inter-
relating representations of visual and linguistic dis-
course context. Artificial Intelligence, 167(1):62?
102.
Charles Kemp, Amy Perfors, and Joshua B. Tenen-
baum. 2007. Learning overhypotheses with hier-
archical bayesian models. Developmental Science,
10(3):307?321.
Christopher Kennedy and Louise McNally. 2005.
Scale structure, degree modification, and the seman-
tics of gradable predicates. Language, pages 345?
381.
Christopher Kennedy. 2007. Vagueness and grammar:
The semantics of relative and absolute gradable ad-
jectives. Linguistics and Philosophy, 30(1):1?45.
Ewan Klein. 1980. A semantics for positive and
comparative adjectives. Linguistics and Philosophy,
4:1?45.
Alice Kyburg and Michael Morreau. 2000. Fitting
words: Vague language in context. Linguistics and
Philosophy, 23:577?597.
Staffan Larsson. 2009. Detecting and learning from
lexical innovation in dialogue: a ttr account. In
Proceedings of the 5th International Conference on
Generative Approaches to the Lexicon.
Staffan Larsson. 2013. Formal semantics for percep-
tual classification. Journal of Logic and Computa-
tion.
Dan Lassiter. 2011. Vagueness as probabilistic linguis-
tic knowledge. In R. Nowen, R. van Rooij, U. Sauer-
land, and H. C. Schmitz, editors, Vagueness in Com-
munication. Springer.
Barbara Partee. 1989. Possible worlds in model-
theoretic semantics: A linguistic perspective. In
S. Allen, editor, Possible Worlds in Humanities, Arts
and Sciences, pages 93?123. Walter de Gruyter.
158
Manfred Pinkal. 1979. Semantics from different
points of view. In R. B?aurle, U. Egli, and A. von
Stechow, editors, How to Refer with Vague Descrip-
tions, pages 32?50. Springer-Verlag.
Manfred Pinkal. 1995. Logic and lexicon: the seman-
tics of the indefinite, volume 56 of Studies in Lin-
guistics and Philosophy. Springer.
Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy
Sykes. 2009. Automatic generation of textual sum-
maries from neonatal intensive care data. Artificial
Intelligence, 173(7):789?816.
Matthew Purver, Julian Hough, and Eleni Gre-
goromichelaki. 2014. Dialogue and compound
contributions. In A. Stent and S. Bangalore, ed-
itors, Natural Language Generation in Interactive
Systems. Cambridge University Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167(1):137?169.
Deb Roy. 2005. Semiotic schemas: A framework for
grounding language in action and perception. Artifi-
cial Intelligence, 167(1):170?205.
L.A. Schmidt, N.D. Goodman, D. Barner, and J.B.
Tenenbaum. 2009. How tall is tall? composition-
ality, statistics, and gradable adjectives. In Proceed-
ings of the 31st annual conference of the cognitive
science society.
Jason Scofield and Douglas A Behrend. 2008. Learn-
ing words from reliable and unreliable speakers.
Cognitive Development, 23(2):278?290.
Jeffrey Mark Siskind. 2001. Grounding the lexical
semantics of verbs in visual perception using force
dynamics and event logic. Journal of Artificial In-
telligence Research, (15):31?90.
Danijel Skocaj, M Janicek, Matej Kristan, Geert-Jan M
Kruijff, Ale?s Leonardis, Pierre Lison, Alen Vrecko,
and Michael Zillich. 2010. A basic cognitive sys-
tem for interactive continuous learning of visual
concepts. In Proceeding of the Workshop on Inter-
active Communication for Autonomous Intelligent
Robots, pages 30?36.
Stephanie Solt. 2011. Notes on the comparison class.
In Vagueness in communication, pages 189?206.
Springer.
Luc Steels. 2003. Evolving grounded communication
for robots. Trends in cognitive sciences, 7(7):308?
312.
159
