Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 547?554, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Word Sense Disambiguation Using Sense Examples Automatically Acquired
from a Second Language
Xinglong Wang
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh
EH8 9LW, UK
xwang@inf.ed.ac.uk
John Carroll
Department of Informatics
University of Sussex
Falmer, Brighton
BN1 9QH, UK
johnca@sussex.ac.uk
Abstract
We present a novel almost-unsupervised
approach to the task of Word Sense Dis-
ambiguation (WSD). We build sense ex-
amples automatically, using large quanti-
ties of Chinese text, and English-Chinese
and Chinese-English bilingual dictionar-
ies, taking advantage of the observation
that mappings between words and mean-
ings are often different in typologically
distant languages. We train a classifier on
the sense examples and test it on a gold
standard English WSD dataset. The eval-
uation gives results that exceed previous
state-of-the-art results for comparable sys-
tems. We also demonstrate that a little
manual effort can improve the quality of
sense examples, as measured by WSD ac-
curacy. The performance of the classifier
on WSD also improves as the number of
training sense examples increases.
1 Introduction
The results of the recent Senseval-3 competition
(Mihalcea et al, 2004) have shown that supervised
WSD methods can yield up to 72.9% accuracy1
on words for which manually sense-tagged data are
available. However, supervised methods suffer from
the so-called knowledge acquisition bottleneck: they
need large quantities of high quality annotated data
1This figure refers to the highest accuracy achieved in the
Senseval-3 English Lexical Sample task with fine-grained scor-
ing.
to produce reliable results. Unfortunately, very
few sense-tagged corpora are available and manual
sense-tagging is extremely costly and labour inten-
sive. One way to tackle this problem is trying to
automate the sense-tagging process. For example,
Agirre et al (2001) proposed a method for building
topic signatures automatically, where a topic signa-
ture is a set of words, each associated with some
weight, that tend to co-occur with a certain concept.
Their system queries an Internet search engine with
monosemous synonyms of words that have multiple
senses in WordNet (Miller et al, 1990), and then ex-
tracts topic signatures by processing text snippets re-
turned by the search engine. They trained a classifier
on the topic signatures and evaluated it on a WSD
task, but the results were disappointing.
In recent years, WSD approaches that exploit
differences between languages have shown great
promise. Several trends are taking place simulta-
neously under this multilingual paradigm. A clas-
sic one is to acquire sense examples using bilin-
gual parallel texts (Gale et al, 1992; Resnik and
Yarowsky, 1997; Diab and Resnik, 2002; Ng et al,
2003): given a word-aligned parallel corpus, the dif-
ferent translations in a target language serve as the
?sense tags? of an ambiguous word in the source
language. For example, Ng et al (2003) acquired
sense examples using English-Chinese parallel cor-
pora, which were manually or automatically aligned
at sentence level and then word-aligned using soft-
ware. A manual selection of target translations was
then performed, grouping together senses that share
the same translation in Chinese. Finally, the occur-
rences of the word on the English side of the parallel
547
texts were considered to have been disambiguated
and ?sense tagged? by the appropriate Chinese trans-
lations. A classifier was trained on the extracted
sense examples and then evaluated on the nouns in
Senseval-2 English Lexical Sample dataset. The re-
sults appear good numerically, but since the sense
groups are not in the gold standard, comparison with
other Senseval-2 results is difficult. As discussed by
Ng et al, there are several problems with relying on
bilingual parallel corpora for data collection. First,
parallel corpora, especially accurately aligned par-
allel corpora are rare, although attempts have been
made to mine them from the Web (Resnik, 1999).
Second, it is often not possible to distinguish all
senses of a word in the source language, by merely
relying on parallel corpora, especially when the cor-
pora are relatively small. This is a common problem
for bilingual approaches: useful data for some words
cannot be collected because different senses of poly-
semous words in one language often translate to the
same word in the other. Using parallel corpora can
aggravate this problem, because even if a word sense
in the source language has a unique translation in the
target language, the translation may not occur in the
parallel corpora at all, due to the limited size of this
resource.
To alleviate these problems, researchers seek
other bilingual resources such as bilingual dictio-
naries, together with monolingual resources that can
be obtained easily. Dagan and Itai (1994) proposed
an approach to WSD using monolingual corpora, a
bilingual lexicon and a parser for the source lan-
guage. One of the problems of this method is that for
many languages, accurate parsers do not exist. With
a small amount of classified data and a large amount
of unclassified data in both the source and the tar-
get languages, Li and Li (2004) proposed bilingual
bootstrapping. This repeatedly constructs classifiers
in the two languages in parallel and boosts the per-
formance of the classifiers by classifying data in
each of the languages and by exchanging informa-
tion regarding the classified data between two lan-
guages. With a certain amount of manual work, they
reported promising results, but evaluated on rela-
tively small datasets.
In previous work, we proposed to use Chinese
monolingual corpora and Chinese-English bilin-
gual dictionaries to acquire sense examples (Wang,
2004)2. We evaluated the sense examples using a
vector space WSD model on a small dataset con-
taining words with binary senses, with promising
results. This approach does not rely on scarce re-
sources such as aligned parallel corpora or accurate
parsers.
This paper describes further progress based on our
proposal: we automatically build larger-scale sense
examples and then train a Na??ve Bayes classifier on
them. We have evaluated our system on the English
Lexical Sample Dataset from Senseval-2 and the re-
sults show conclusively that such sense examples
can be used successfully in a full-scale fine-grained
WSD task. We tried to analyse whether more sense
examples acquired this way would improve WSD
accuracy and also whether a little human effort on
sense mapping could further improve WSD perfor-
mance.
The reminder of the paper is organised as fol-
lows. Section 2 outlines the acquisition algorithm
for sense examples. Section 3 describes details of
building this resource and demonstrates our appli-
cation of sense examples to WSD. We also present
results and analysis in this section. Finally, we con-
clude in Section 4 and talk about future work.
2 Acquisition of Sense Examples
Following our previous proposal (Wang, 2004), we
automatically acquire English sense examples using
large quantities of Chinese text and English-Chinese
and Chinese-English dictionaries. The Chinese lan-
guage was chosen because it is a distant language
from English and the more distant two languages
are, the more likely that senses are lexicalised differ-
ently (Resnik and Yarowsky, 1999). The underlying
assumption of this approach is that in general each
sense of an ambiguous English word corresponds to
a distinct translation in Chinese. As shown in Fig-
ure 1, firstly, the system translates senses of an En-
glish word into Chinese words, using an English-
Chinese dictionary, and then retrieves text snippets
from a large amount of Chinese text, with the Chi-
nese translations as queries. Then, the Chinese text
snippets are segmented and then translated back to
English word by word, using a Chinese-English dic-
2Sense examples were referred to as ?topic signatures? in
that paper.
548
English ambiguous word w
Sense 1 of w Sense 2 of w
Chinese translation of
sense 2
Chinese translation of
sense 1
English-Chinese
Lexicon
Chinese text snippet 1
Chinese text snippet 2
... ...
Chinese
Search
Engine
Chinese-
English
Lexicon
Chinese text snippet 1
Chinese text snippet 2
... ...
{English sense example 1
for sense 1 of w}
{English sense example 2
for sense 1 of w}
... ...
{English sense example 1
for sense 2 of w}
{English sense example 2
for sense 2 of w}
... ...
Chinese
Segementation
Figure 1. Process of automatic acquisition of sense examples.
For simplicity, assume w has two senses.
tionary. In this way, for each sense, a set of sense
examples is produced. As an example, suppose one
wants to retrieve sense examples for the financial
sense of interest. One first looks up the Chinese
translations of this sense in an English-Chinese dic-
tionary, and finds that |E is the right Chinese
translation corresponding to this particular sense.
Then, the next stage is to automatically build a col-
lection of Chinese text snippets by either searching
in a large Chinese corpus or on the Web, using |
E as query. Since Chinese is a language written
without spaces between words, one needs to use a
segmentor to mark word boundaries before translat-
ing the snippets word by word back to English. The
result is a collection of sense examples for the finan-
cial sense of interest, each containing a bag of words
that tend to co-occur with that particular sense. For
example, {interest rate, bank, annual, economy, ...}
might be one of the sense examples extracted for the
financial sense of interest. Note that words in a sense
example are unordered.
Since this method acquires training data for WSD
systems from raw monolingual Chinese text, it
avoids the problem of the shortage of English sense-
tagged corpora, and also of the shortage of aligned
bilingual corpora. Also, if existing corpora are
not big enough, one can always harvest more text
from the Web. However, like all methods based
on the cross-language translation assumption men-
tioned above, there are potential problems. For ex-
ample, it is possible that a Chinese translation of an
English sense is also ambiguous, and thus the con-
tents of text snippets retrieved may be regarding a
concept other than the one we want. In general,
when the assumption does not hold, one could use
the glosses defined in a dictionary as queries to re-
trieve text snippets, as comprehensive bilingual dic-
tionaries tend to include translations to all senses of
a word, where multiword translations are used when
one-to-one translation is not possible. Alternatively,
a human annotator could map the senses and trans-
lations by hand. As we will describe later in this
paper, we chose the latter way in our experiments.
3 Experiments and Results
We firstly describe in detail how we prepared the
sense examples and then describe a large scale WSD
evaluation on the English Senseval-2 Lexical Sam-
ple dataset (Kilgarriff, 2001). The results show that
our system trained with the sense examples achieved
significantly better accuracy than comparable sys-
tems. We also show that when a little manual effort
was invested in mapping the English word senses
to Chinese monosemous translations, WSD perfor-
mance improves accordingly. Based on further ex-
periments on a standard binary WSD dataset, we
also show that the technique scales up satisfacto-
rily so that more sense examples help achieve better
WSD accuracy.
3.1 Building Sense Examples
Following the approach described in Section 2,
we built sense examples for the 44 words in the
Senseval-2 dataset3. These 44 words have 223
senses in total to disambiguate. The first step was
translating English senses to Chinese. We used the
Yahoo! Student English-Chinese On-line Dictio-
nary4, as well as a more comprehensive electronic
dictionary. This is because the Yahoo! dictionary is
designed for English learners, and its sense granu-
larity is rather coarse-grained. It is good enough for
words with fewer or coarse-grained senses. How-
3These 44 words cover all nouns and adjectives in the
Senseval-2 dataset, but exclude verbs. We discuss this point
in section 3.2.
4See: http://cn.yahoo.com/dictionary.
549
ever, the Senseval-2 Lexical Sample task5 uses
WordNet 1.7 as gold standard, which has very fine
sense distinctions and translation granularity in the
Yahoo! dictionary does not conform to this standard.
PowerWord 20026 was chosen as a supplementary
dictionary because it integrates several comprehen-
sive English-Chinese dictionaries in a single appli-
cation. For each sense of an English word entry, both
Yahoo! and PowerWord 2002 dictionaries list not
only Chinese translations but also English glosses,
which provides a bridge between WordNet synsets
and Chinese translations in the dictionaries. In de-
tail, to automatically find a Chinese translation for
sense s of an English word w, our system looks up
w in both dictionaries and determines whether w has
the same or greater number of senses as in Word-
Net. If it does, in one of the bilingual dictionaries,
we locate the English gloss g which has the max-
imum number of overlapping words with the gloss
for s in the WordNet synset. The Chinese transla-
tion associated with g is then selected. Although
this simple method successfully identified Chinese
translations for 23 out of the 44 words (52%), trans-
lations for the remaining word senses remain un-
known because the sense distinctions are different
between our bilingual dictionaries and WordNet. In
fact, unless an English-Chinese bilingual WordNet
becomes available, this problem is inevitable. For
our experiments, we solved the problem by manu-
ally looking up dictionaries and identifying transla-
tions. For each one of the 44 words, PowerWord
2002 provides more Chinese translations than the
number of its synsets in WordNet 1.7. Thus the an-
notator simply selects the Chinese translations that
he considers a best match to the corresponding En-
glish senses. This task took an hour for an annotator
who speaks both languages fluently.
It is possible that the Chinese translations are also
ambiguous, which can make the topic of a collection
of text snippets deviate from what is expected. For
example, the oral sense of mouth can be translated as
? or?n in Chinese. However, the first translation
5The task has two variations: one to disambiguate fine-
grained senses and the other to coarse-grained ones. We evalu-
ated our sense examples on the former variation, which is obvi-
ously more difficult.
6A commercial electronic dictionary application. We used
the free on-line version at: http://cb.kingsoft.com.
(?) is a single-character word and is highly ambigu-
ous: by combining with other characters, its mean-
ing varies. For example,??means ?an exit? or ?to
export?. On the other hand, the second translation
(?n) is monosemous and should be used. To as-
sess the influence of such ?ambiguous translations?,
we carried out experiments involving more human
labour to verify the translations. The same annotator
manually eliminated those highly ambiguous Chi-
nese translations and then replaced them with less
ambiguous or ideally monosemous Chinese trans-
lations. This process changed roughly half of the
translations and took about five hours. We compared
the basic system with this manually improved one.
The results are presented in section 3.2.
Using translations as queries, the sense examples
were automatically extracted from the Chinese Gi-
gaword Corpus (CGC), distributed by the LDC7,
which contains 2.7GB newswire text, of which
900MB are sourced from Xinhua News Agency of
Beijing, and 1.8GB are drawn from Central News
from Taiwan. A small percentage of words have
different meanings in these two Chinese dialects,
and since the Chinese-English dictionary (LDC
Mandarin-English Translation Lexicon Version 3.0)
we use later is compiled with Mandarin usages in
mind, we mainly retrieve data from Xinhua News.
We set a threshold of 100, and only when the amount
of snippets retrieved from Xinhua News is smaller
than 100, do we turn to Central News to collect more
data. Specifically, for 48 out of the 223 (22%) Chi-
nese queries, the system retrieved less than 100 in-
stances from Xinhua News so it extracted more data
from Central News. In theory, if the training data is
still not enough, one could always turn to other text
resources, such as the Web.
To decide the optimal length of text snippets to
retrieve, we carried out pilot experiments with two
length settings: 250 (? 110 English words) and
400 (? 175 English words) Chinese characters, and
found that more context words helped improve WSD
performance (results not shown). Therefore, we re-
trieve text snippets with a length of 400 characters.
We then segmented all text snippets, using an ap-
plication ICTCLAS8. After the segmentor marked
7Available at: http://www.ldc.upenn.edu/Catalog/
8See: http://mtgroup.ict.ac.cn/?zhp/ICTCLAS
550
all word boundaries, the system automatically trans-
lated the text snippets word by word using the elec-
tronic LDC Mandarin-English Translation Lexicon
3.0. As expected, the lexicon does not cover all
Chinese words. We simply discarded those Chi-
nese words that do not have an entry in this lexi-
con. We also discarded those Chinese words with
multiword English translations. Since the discarded
words can be informative, one direction of our re-
search in the future is to find an up-to-date wide cov-
erage dictionary, and to see how much difference it
will make. Finally, we filtered the sense examples
with a stop-word list, to ensure only content words
were included.
We ended up with 223 sets of sense examples
for all senses of the 44 nouns and adjectives in the
test dataset. Each sense example contains a set of
words that were translated from a Chinese text snip-
pet, whose content should closely relate to the En-
glish word sense in question. Words in a sense ex-
ample are unordered, because in this work we only
used bag-of-words information. Except for the very
small amount of manual work described above to
map WordNet glosses to those in English-Chinese
dictionaries, the whole process is automatic.
3.2 WSD Experiments on Senseval-2 Lexical
Sample dataset
The Senseval-2 English Lexical Sample Dataset
consists of manually sense-tagged training and test
instances for nouns, adjectives and verbs. We only
tested our system on nouns and adjectives because
verbs often have finer sense distinctions, which
would mean more manual work would need to be
done when mapping WordNet synsets to English-
Chinese dictionary glosses. This would involve us in
a rather different kind of enterprise since we would
have moved from an almost-unsupervised to a more
supervised setup.
We did not use the training data supplied with the
dataset. Instead, we train a classifier on our auto-
matically built sense examples and test it on the test
data provided. In theory, any machine learning clas-
sifier can be applied. We chose the Na??ve Bayes al-
gorithm with kernel estimation9 (John and Langley,
1995) which outperformed a few other classifiers in
9We used the implementation in the Weka machine learning
package, available at: http://www.cs.waikato.ac.nz/?ml/weka.
art-n(5)
authority-n(7)
bar-n(13)
blind-a(3)
bum-n(4)
chair-n(4)
channel-n(7)
child-n(4)
church-n(3)
circuit-n(6)
colourless-a(2)
cool-a(6)
day-n(9)
detention-n(2)
dyke-n(2)
facility-n(5)
faithful-a(3)
fatigue-n(4)
feeling-n(6)
fine-a(9)
fit-a(3)
free-a(8)
graceful-a(2)
green-a(7)
grip-n(7)
hearth-n(3)
holiday-n(2)
lady-n(3)
local-a(3)
material-n(5)
mouth-n(8)
nation-n(3)
natural-a(10)
nature-n(5)
oblique-a(2)
post-n(8)
restraint-n(6)
sense-n(5)
simple-a(7)
solemn-a(2)
spade-n(3)
stress-n(5)
vital-a(4)
yew-n(2)
Basic
39.8
21.5
44.7
74.5
64.4
80.0
32.4
56.3
59.4
48.8
66.7
50.9
32.4
60.6
82.8
27.1
66.7
77.3
50.0
34.3
44.8
37.3
70.0
53.2
35.3
48.5
64.5
71.7
38.5
47.8
38.3
39.5
14.6
27.7
72.4
34.7
17.4
25.9
49.3
73.1
67.6
45.0
41.0
82.8
 Sys B
MW
59.6
23.7
52.0
75.0
62.2
82.9
36.5
56.3
59.4
69.8
69.4
50.9
33.1
84.8
86.2
28.8
66.7
77.3
50.0
32.9
44.8
48.2
73.3
58.5
37.3
51.5
75.0
77.8
43.6
49.3
41.7
39.5
34.0
31.9
73.3
45.6
19.6
46.3
50.7
76.9
70.6
45.0
46.2
89.7
Lesk(U)
16.3
30.4
2.0
32.7
53.3
56.5
21.9
56.2
45.3
5.9
54.3
9.6
0
43.8
57.1
46.6
26.1
44.2
2.0
5.7
3.4
7.3
72.4
10.6
17.6
81.2
29.0
50.9
31.6
44.9
31.7
18.9
6.8
41.3
72.4
6.3
28.9
24.5
12.1
24.0
60.6
2.6
0
17.9
Word
46.1 52.0 24.6Avg.
Basic
29.9
20.7
41.1
74.5
60.0
81.2
31.5
56.3
53.1
48.2
45.7
26.9
32.2
62.5
85.7
20.7
69.6
76.7
11.7
8.6
44.8
29.3
58.6
53.2
35.3
46.9
64.5
69.2
36.8
39.1
38.3
35.1
14.6
23.9
72.4
34.7
6.7
20.7
45.5
64.0
66.7
37.5
42.1
21.4
 Sys A
MW
51.0
22.8
48.3
74.5
60.0
82.6
35.6
56.3
56.3
68.2
45.7
26.9
32.9
84.4
85.7
22.4
69.6
74.4
11.7
11.4
44.8
37.8
58.6
58.5
37.3
50.0
74.2
73.6
42.1
44.9
41.7
35.1
32.0
26.1
72.4
45.6
6.7
43.4
45.5
76.0
63.6
37.5
42.1
25.0
40.7 46.0
RB
16.3
10.0
3.3
40.0
15.6
23.2
12.3
18.8
29.7
10.6
42.9
13.5
7.6
43.8
28.6
13.8
21.7
25.6
9.8
7.1
31.0
15.9
62.1
21.3
19.6
31.2
38.7
28.3
26.3
10.1
11.7
21.6
6.8
15.2
44.8
10.1
11.1
24.5
13.6
32.0
18.2
12.8
21.1
57.1
Baselines & A Senseval-2 Entry
MFB
41.8
39.1
38.4
78.2
68.9
76.8
13.7
54.7
56.2
27.1
65.7
46.2
60.0
62.5
53.6
48.3
78.3
76.7
56.9
42.9
58.6
35.4
79.3
75.5
35.3
71.9
77.4
64.2
55.3
20.3
36.7
78.4
27.2
45.7
69.0
31.6
28.9
24.5
51.5
96.0
63.6
48.7
92.1
78.6
18.1 50.5
UNED
50.0
34.8
27.8
74.5
11.1
81.2
17.8
43.8
62.5
55.3
31.4
46.2
20.0
78.1
35.7
25.9
78.3
86.0
60.8
44.3
48.3
35.4
79.3
78.7
21.6
65.6
54.8
58.5
34.2
53.6
48.3
70.3
44.7
23.9
27.6
41.8
17.8
30.2
51.5
96.0
54.5
20.5
94.7
71.4
46.4
Table 1. WSD accuracy on words in the English Senseval-2
Lexical Sample dataset. The left most column shows words,
their POS tags and how many senses they have. ?Sys A? and
?Sys B? are our systems, and ?MW? denotes a multi-word de-
tection module was used in conjunction with the ?Basic? sys-
tem. For comparison, it also shows two baselines: ?RB? is the
random baseline and ?MFB? is the most-frequent-sense base-
line. ?UNED? is one of the best unsupervised participants
in the Senseval-2 competition and ?Lesk(U)? is the highest
unsupervised-baseline set in the workshop. All accuracies are
expressed as percentages.
our pilot experiments on other datasets (results not
shown). The average length of a sense example is
35 words, which is much shorter than the length of
the text snippets, which was set to 400 Chinese char-
acters (? 175 English words). This is because func-
tion words and words that are not listed in the LDC
Mandarin-English lexicon were eliminated. We did
not apply any weighting to the features because per-
formance went down in our pilot experiments when
we applied a TF.IDF weighting scheme (results not
shown). We also limited the maximum number of
551
training sense examples to 6000, for efficiency pur-
poses. We attempted to tag every test data instance,
so our coverage (on nouns and adjectives) is 100%.
To assess the influence of ambiguous Chinese
translations, we prepared two sets of training data.
As described in section 3.1: sense examples in the
first set were prepared without taking ambiguity in
Chinese text into consideration, while those in the
second set were prepared with a little more human
effort involved trying to reduce ambiguity by us-
ing less ambiguous translations. We call the system
trained on the first set ?Sys A? and the one trained
on the second ?Sys B?.
In this lexical sample task, multiwords are ex-
pected to be picked out by participating WSD sys-
tems. For example, the answer art collection should
be supplied when this multiword occurs in a test
instance. It would be judged wrong if one tagged
the art in art collection as the artworks sense, even
though one could argue that this was also a cor-
rect answer. To deal with multiwords, we imple-
mented a very simple detection module, which tries
to match multiword entries in WordNet to the am-
biguous word and its left and right neighbours. For
example, if the module finds art collection is an en-
try in WordNet, it tags all occurrences of this multi-
word in the test data, regardless of the prediction by
the classifier.
The results are shown in Table 1. Our ?Sys B?
system, with and without the multiword detection
module, outperformed ?Sys A?, which shows that
sense examples acquired with less ambiguous Chi-
nese translations contain less noise and therefore
boost WSD performance. For comparison, the ta-
ble also shows various baseline performance figures
and a system that participated in Senseval-210. Con-
sidering that the manual work involved in our ap-
proach is negligible compared with manual sense-
tagging, we classify our systems as unsupervised
and we should aim to beat the random baseline.
This all four of our systems do easily. We also eas-
ily beat another unsupervised baseline ? the Lesk
(1986) baseline, which disambiguates words using
WordNet definitions. The MFB baseline is actu-
ally a ?supervised? baseline, since an unsupervised
10Accuracies for each word and averages were calculated
by us, based on the information on Senseval-2 Website. See:
http://www.sle.sharp.co.uk/senseval2/.
system does not have such prior knowledge before-
hand. McCarthy et al (2004) argue that this is a
very tough baseline for an unsupervised WSD sys-
tem to beat. Our ?Sys B? with multiword detection
exceeds it. ?Sys B? also exceeds the performance
of UNED (Ferna?ndez-Amoro?s et al, 2001), which
was the second-best ranked11 unsupervised systems
in the Senseval-2 competition.
There are a number of factors that can influence
WSD performance. The distribution of training data
for senses is one. In our experiments, we used all
sense examples that we built for a sense (with an
upper bound of 6000). However, the distribution of
senses in English text often does not match the dis-
tribution of their corresponding Chinese translations
in Chinese text. For example, suppose an English
word w has two senses: s1 and s2, where s1 rarely
occurs in English text, whereas sense s2 is used fre-
quently. Also suppose s1?s Chinese translation is
much more frequently used than s2?s translation in
Chinese text. Thus, the distribution of the two senses
in English is different from that of the translations in
Chinese. As a result, the numbers of sense exam-
ples we would acquire for the two senses would be
distributed as if they were in Chinese text. A clas-
sifier trained on this data would then tend to predict
unseen test instances in favour of the wrong distribu-
tion. The word nation, for example, has three senses,
of which the country sense is used more frequently
in English. However, in Chinese, the country sense
and the people sense are almost equally distributed,
which might be the reason for its WSD accuracy be-
ing lower with our systems than most of the other
words. A possible way to alleviate this problem is to
select training sense examples according to an esti-
mated distribution in natural English text, which can
be done by analysing available sense-tagged corpora
with help of smoothing techniques, or with the un-
supervised approach of (McCarthy et al, 2004).
Cultural differences can cause difficulty in retriev-
ing sufficient training data. For example, transla-
tions of senses of church and hearth appear only in-
frequently in Chinese text. Thus, it is hard to build
sense examples for these words. Another problem,
11One system performed better but their answers were not
on the official Senseval-2 website so that we could not do the
comparison. Also, that system did not attempt to disambiguate
as many words as UNED and us.
552
as mentioned above, is that translations of English
senses can be ambiguous in Chinese. For exam-
ple, Chinese translations of the words vital, natu-
ral, local etc. are also ambiguous to some extent,
and this might be a reason for their low perfor-
mance. One way to solve this, as we described, is
to manually check the translations. Another auto-
matic way is that, before retrieving text snippets, we
could segment or even parse the Chinese corpora,
which should reduce the level of ambiguity and lead
to better sense examples.
3.3 Further WSD Experiments
One of the strengths of our approach is that training
data come cheaply and relatively easily. However,
the sense examples are acquired automatically and
they inevitably contain a certain amount of noise,
which may cause problems for the classifier. To as-
sess the relationship between accuracy and the size
of training data, we carried out a series of experi-
ments, feeding the classifier with different numbers
of sense examples as training data.
For these experiments, we used another standard
WSD dataset, the TWA dataset. This is a manu-
ally sense-tagged corpus (Mihalcea, 2003), which
contains 2-way sense-tagged text instances, drawn
from the British National Corpus, for 6 nouns. We
first built sense examples for all the 12 senses using
the approach described above, then trained the same
Na??ve Bayes algorithm (NB) on different numbers
of sense examples.
In detail, for all of the 6 words, we did the fol-
lowing: given a word wi, we randomly selected n
sense examples for each of its senses si, from the
total amount of sense examples built for si. Then
the NB algorithm was trained on the 2 ? n exam-
ples and tested on wi?s test instances in TWA. We
recorded the accuracy and repeated this process 200
times and calculated the mean and variance of the
200 accuracies. Then we assigned another value to
n and iterated the above process until n took all the
predefined values. In our experiments, n was taken
from {50, 100, 150, 200, 400, 600, 800, 1000, 1200}
for words motion, plant and tank and from {50, 100,
150, 200, 250, 300, 350} for bass, crane and palm,
because there were less sense example data available
for the latter three words. Finally, we used the t-test
(p = 0.05) on pairwise sets of means and variances
to see if improvements were statistically significant.
 0.93
 0.91
 0.89
 0.87
 0.85
 0  50  100  150  200  250  300  350  400bass
 0.78
 0.76
 0.74
 0.72
 0.7
 0.68
 0  50  100  150  200  250  300  350  400crane
 0.82
 0.79
 0.76
 0.73
 0.7
 0.67
 0.64
 0  200  400  600  800  1000  1200  1400motion
 0.77
 0.76
 0.75
 0.74
 0.73
 0.72
 0  50  100  150  200  250  300  350  400palm
 0.76
 0.74
 0.72
 0.7
 0.68
 0.66
 0.64
 0  200  400  600  800  1000  1200  1400plant
 0.74
 0.72
 0.7
 0.68
 0.66
 0.64
 0  200  400  600  800  1000  1200  1400tank
Figure 2. Accuracy scores with increasing number of training
sense examples. Each bar is a standard deviation.
The results are shown in Figure 212. 34 out of 42
t-scores are greater than the t-test critical values, so
we are fairly confident that the more training sense
examples used, the more accurate the NB classifier
becomes on this disambiguation task.
4 Conclusions and Future Work
We have presented WSD systems that use sense ex-
amples as training data. Sense examples are ac-
quired automatically from large quantities of Chi-
nese text, with the help of Chinese-English and
English-Chinese dictionaries. We have tested our
WSD systems on the English Senseval-2 Lexical
Sample dataset, and our best system outperformed
comparable state-of-the-art unsupervised systems.
Also, we found that increasing the number of the
sense examples significantly improved WSD perfor-
mance. Since sense examples can be obtained very
cheaply from any large Chinese text collection, in-
12These experiments showed that our systems outperformed
the most-frequent-sense baseline and Mihalcea?s unsupervised
system (2003).
553
cluding the Web, our approach is a way to tackle the
knowledge acquisition bottleneck.
There are a number of future directions that we
could investigate. Firstly, instead of using a bilin-
gual dictionary to translate Chinese text snippets
back to English, we could use machine translation
software. Secondly, we could try this approach on
other language pairs, Japanese-English, for exam-
ple. This is also a possible solution to the problem
that ambiguity may be preserved between Chinese
and English. In other words, when a Chinese transla-
tion of an English sense is still ambiguous, we could
try to collect sense examples using translation in a
third language, Japanese, for instance. Thirdly, it
would be interesting to try to tackle the problem of
Chinese WSD using sense examples built using En-
glish, the reverse process to the one described in this
paper.
Acknowledgements
This research was funded by EU IST-2001-34460
project MEANING: Developing Multilingual Web-
Scale Language Technologies.
References
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching WordNet concepts with topic
signatures. In Proceedings of the NAACL workshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations. Pittsburgh, USA.
Ido Dagan and Alon Itai. 1994. Word sense disam-
biguation using a second language monolingual cor-
pus. Computational Linguistics, 20(4):563?596.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of the 40th Anniversary Meeting of the
Association for Computational Linguistics (ACL-02).
Philadelphia, USA.
David Ferna?ndez-Amoro?s, Julio Gonzalo, and Felisa
Verdejo. 2001. The UNED systems at Senseval-
2. In Poceedings of Second International Wordshop
on Evaluating Word Sense Disambiguation Systems
(Senseval-2). Toulouse, France.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Using bilingual materials to de-
velop word sense disambiguation methods. In Pro-
ceedings of the International Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion, pages 101?112.
George H. John and Pat Langley. 1995. Estimating con-
tinuous distributions in Bayesian classifiers. In Pro-
ceedings of the Eleventh Conference on Uncertainty in
Artificial Intelligence, pages 338?345.
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In Proceedings of the Second International
Workshop on Evaluating Word Sense Disambiguation
Systems (SENSEVAL-2). Toulouse, France.
Michael E. Lesk. 1986. Automated sense disambigua-
tion using machine-readable dictionaries: how to tell a
pinecone from an ice cream cone. In Proceedings of
the SIGDOC Conference.
Hang Li and Cong Li. 2004. Word translation dis-
ambiguation using bilingual bootstrapping. Compu-
tational Linguistics, 20(4):563?596.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics. Barcelona, Spain.
Rada Mihalcea, Timothy Chklovski, and Adam Killgar-
iff. 2004. The Senseval-3 English lexical sample task.
In Proceedings of the Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text (Senseval-3).
Rada Mihalcea. 2003. The role of non-ambiguous words
in natural language disambiguation. In Proceedings of
the Conference on Recent Advances in Natural Lan-
guage Processing, RANLP 2003. Borovetz, Bulgaria.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: An on-line lexical database.
Journal of Lexicography, 3(4):235?244.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics.
Philip Resnik and David Yarowsky. 1997. A perspective
on word sense disambiguation methods and their eval-
uation. In Proceedings of the ACL SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, What
and How?, pages 79?86.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: New evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(2):113?133.
Philip Resnik. 1999. Mining the Web for bilingual text.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics.
Xinglong Wang. 2004. Automatic acquisition of En-
glish topic signatures based on a second language. In
Proceedings of the Student Research Workshop at ACL
2004. Barcelona, Spain.
554
Word Sense Disambiguation Using Automatically Translated
Sense Examples
Xinglong Wang
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh
EH8 9LW, UK
xwang@inf.ed.ac.uk
David Martinez
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
davidm@dcs.shef.ac.uk
Abstract
We present an unsupervised approach to
Word Sense Disambiguation (WSD). We
automatically acquire English sense exam-
ples using an English-Chinese bilingual
dictionary, Chinese monolingual corpora
and Chinese-English machine translation
software. We then train machine learn-
ing classifiers on these sense examples
and test them on two gold standard En-
glish WSD datasets, one for binary and
the other for fine-grained sense identifica-
tion. On binary disambiguation, perfor-
mance of our unsupervised system has ap-
proached that of the state-of-the-art super-
vised ones. On multi-way disambiguation,
it has achieved a very good result that is
competitive to other state-of-the-art unsu-
pervised systems. Given the fact that our
approach does not rely on manually anno-
tated resources, such as sense-tagged data
or parallel corpora, the results are very
promising.
1 Introduction
Results from recent Senseval workshops have
shown that supervised Word Sense Disambigua-
tion (WSD) systems tend to outperform their unsu-
pervised counterparts. However, supervised sys-
tems rely on large amounts of accurately sense-
annotated data to yield good results and such re-
sources are very costly to produce. It is difficult
for supervised WSD systems to perform well and
reliably on words that do not have enough sense-
tagged training data. This is the so-called knowl-
edge acquisition bottleneck.
To overcome this bottleneck, unsupervised
WSD approaches have been proposed. Among
them, systems under the multilingual paradigm
have shown great promise (Gale et al, 1992; Da-
gan and Itai, 1994; Diab and Resnik, 2002; Ng et
al., 2003; Li and Li, 2004; Chan and Ng, 2005;
Wang and Carroll, 2005). The underlying hy-
pothesis is that mappings between word forms
and meanings can be different from language to
language. Much work have been done on ex-
tracting sense examples from parallel corpora for
WSD. For example, Ng et al (2003) proposed
to train a classifier on sense examples acquired
from word-aligned English-Chinese parallel cor-
pora. They grouped senses that share the same
Chinese translation, and then the occurrences of
the word on the English side of the parallel corpora
were considered to have been disambiguated and
?sense tagged? by the appropriate Chinese trans-
lations. Their system was evaluated on the nouns
in Senseval-2 English lexical sample dataset, with
promising results. Their follow-up work (Chan
and Ng, 2005) has successfully scaled up the ap-
proach and achieved very good performance on
the Senseval-2 English all-word task.
Despite the promising results, there are prob-
lems with relying on parallel corpora. For exam-
ple, there is a lack of matching occurrences for
some Chinese translations to English senses. Thus
gathering training examples for them might be dif-
ficult, as reported in (Chan and Ng, 2005). Also,
parallel corpora themselves are rare resources and
not available for many language pairs.
Some researchers seek approaches using mono-
lingual resources in a second language and then
try to map the two languages using bilingual dic-
tionaries. For example, Dagan and Itai (1994) car-
ried out WSD experiments using monolingual cor-
pora, a bilingual lexicon and a parser for the source
language. One problem of this method is that
45
for many languages, accurate parsers do not exist.
Wang and Carroll (2005) proposed to use mono-
lingual corpora and bilingual dictionaries to auto-
matically acquire sense examples. Their system
was unsupervised and achieved very promising
results on the Senseval-2 lexical sample dataset.
Their system also has better portability, i.e., it runs
on any language pair as long as a bilingual dictio-
nary is available. However, sense examples ac-
quired using the dictionary-based word-by-word
translation can only provide ?bag-of-words? fea-
tures. Many other features useful for machine
learning (ML) algorithms, such as the ordering of
words, part-of-speech (POS), bigrams, etc., have
been lost. It could be more interesting to translate
Chinese text snippets using machine translation
(MT) software, which would provide richer con-
textual information that might be useful for WSD
learners. Although MT systems themselves are
expensive to build, once they are available, they
can be used repeatedly to automatically generate
as much data as we want. This is an advantage
over relying on other expensive resources such as
manually sense-tagged data and parallel copora,
which are limited in size and producing additional
data normally involves further costly investments.
We carried out experiments on acquiring sense
examples using both MT software and a bilingual
dictionary. When we had the two sets of sense ex-
amples ready, we trained a ML classifier on them
and then tested them on coarse-grained and fine-
grained gold standard WSD datasets, respectively.
We found that on both test datasets the classi-
fier using MT translated sense examples outper-
formed the one using those translated by a dictio-
nary, given the same amount of training examples
used on each word sense. This confirms our as-
sumption that a richer feature set, although from
a noisy data source, such as machine translated
text, might help ML algorithms. In addition, both
systems performed very well comparing to other
state-of-the-art WSD systems. As we expected,
our system is particularly good on coarse-grained
disambiguation. Being an unsupervised approach,
it achieved a performance competitive to state-of-
the-art supervised systems.
This paper is organised as follows: Section 2
revisits the process of acquiring sense examples
proposed in (Wang and Carroll, 2005) and then
describes our adapted approach. Section 3 out-
lines resources, the ML algorithm and evaluation
metrics that we used. Section 4 and Section 5 de-
tail experiments we carried out on gold standard
datasets. We also report our results and error anal-
ysis. Finally, Section 6 concludes the paper and
draws future directions.
2 Acquisition of Sense Examples
Wang and Carroll (2005) proposed an automatic
approach to acquire sense examples from large
amount of Chinese text and English-Chinese and
Chinese-English dictionaries. The acquisition pro-
cess is summarised as follows:
1. Translate an English ambiguous word   to Chinese,
using an English-Chinese lexicon. Given the assump-
tion that mappings between words and senses are dif-
ferent between English and Chinese, each sense   of
  maps to a distinct Chinese word. At the end of this
step, we have produced a set

, which consists of Chi-
nese words   	 
   
    
    , where   is the translation
corresponding to sense   of   , and  is the number of
senses that   has.
2. Query large Chinese corpora or/and a search engine us-
ing each element in

. For each   in

, we collect the
text snippets retrieved and construct a Chinese corpus.
3. Word-segment these Chinese text snippets.
4. Use an electronic Chinese-English lexicon to translate
the Chinese corpora constructed word by word to En-
glish.
This process can be completely automatic and
unsupervised. However, in order to compare
the performance against other WSD systems, one
needs to map senses in the bilingual dictionary to
those used by gold standard datasets, which are
often from WordNet (Fellbaum, 1998). This step
is inevitable unless we use senses in the bilingual
dictionary as gold standard. Fortunately, the map-
ping process only takes a very short time1, com-
paring to the effort that it would take to manually
sense annotate training examples. At the end of
the acquisition process, for each sense   of an am-
biguous word  , we have a large set of English
contexts. Note that a context is represented by a
bag of words only. We mimicked this process and
built a set of sense examples.
To obtain a richer set of features, we adapted the
above process and carried out another acquisition
experiment. When translating Chinese text snip-
pets to English in the 4th step, we used MT soft-
ware instead of a bilingual dictionary. The intu-
ition is that although machine translated text con-
tains noise, features like word ordering, POS tags
1A similar process took 15 minutes per noun as reported
in (Chan and Ng, 2005), and about an hour for 20 nouns as
reported in (Wang and Carroll, 2005).
46
English ambiguous word w
Sense 1 of w Sense 2 of w
Chinese translation of
sense 2
Chinese translation of
sense 1
English-Chinese
Lexicon
Chinese text snippet 1
Chinese text snippet 2
... ...
Search
Chinese
Corpora
Machine
Translation
Software
Chinese text snippet 1
Chinese text snippet 2
... ...
{English sense example 1
for sense 1 of w}
{English sense example 2
for sense 1 of w}
... ...
{English sense example 1
for sense 2 of w}
{English sense example 2
for sense 2 of w}
... ...
Figure 1:Adapted process of automatic acquisition of sense
examples. For simplicity, assume   has two senses.
and bigrams/trigrams may still be of some use for
ML classifiers. In this approach, the 3rd step can
be omitted, since MT software should be able to
take care of segmentation. Figure 1 illustrates our
adapted acquisition process.
As described above, we prepared two sets of
training examples for each English word sense
to disambiguate: one set was translated word-by-
word by looking up a bilingual dictionary, as pro-
posed in (Wang and Carroll, 2005), and the other
translated using MT software. In detail, we first
mapped senses of ambiguous words, as defined
in the gold-standard TWA (Mihalcea, 2003) and
Senseval-3 lexical sample (Mihalcea et al, 2004)
datasets (which we use for evaluation) onto their
corresponding Chinese translations. We did this
by looking up an English-Chinese dictionary Pow-
erWord 20022. This mapping process involved
human intervention, but it only took an annota-
tor (fluent speaker in both Chinese and English)
4 hours. Since some Chinese translations are
also ambiguous, which may affect WSD perfor-
mance, the annotator was asked to select the Chi-
nese words that are relatively unambiguous (or
ideally monosemous) in Chinese for the target
word senses, when it was possible. Sometimes
multiple senses of an English word can map to
the same Chinese word, according to the English-
Chinese dictionary. In such cases, the annotator
was advised to try to capture the subtle difference
between these English word senses and then to
2PowerWord is a commercial electronic dictio-
nary application. There is a free online version at:
http://cb.kingsoft.com.
select different Chinese translations for them, us-
ing his knowledge on the languages. Then, using
the translations as queries, we retrieved as many
text snippets as possible from the Chinese Giga-
word Corpus. For efficiency purposes, we ran-
domly chose maximumly     text snippets for
each sense, when acquiring data for nouns and
adjectives from Senseval-3 lexical sample dataset.
The length of the snippets was set to    Chinese
characters.
From here we prepared two sets of sense exam-
ples differently. For the approach of dictionary-
based translation, we segmented all text snippets,
using the application ICTCLAS3. After the seg-
mentor marked all word boundaries, the system
automatically translated the text snippets word by
word using the electronic LDC Mandarin-English
Translation Lexicon 3.0. All possible translations
of each word were included. As expected, the lex-
icon does not cover all Chinese words. We simply
discarded those Chinese words that do not have an
entry in this lexicon. We also discarded those Chi-
nese words with multiword English translations.
Finally we got a set of sense examples for each
sense. Note that a sense example produced here is
simply a bag of words without ordering.
We prepared the other set of sense examples by
translating text snippets with the MT software Sys-
tran  
 Standard, where each example contains
much richer features that potentially can be ex-
ploited by ML algorithms.
3 Experimental Settings
3.1 Training
We applied the Vector Space Model (VSM) algo-
rithm on the two different kinds of sense examples
(i.e., dictionary translated ones vs. MT software
translated ones), as it has been shown to perform
well with the features described below (Agirre and
Martinez, 2004a). In VSM, we represent each
context as a vector, where each feature has an 1
or 0 value to indicate its occurrence or absence.
For each sense in training, a centroid vector is ob-
tained, and these centroids are compared to the
vectors that represent test examples, by means of
the cosine similarity function. The closest centroid
assigns its sense to the test example.
For the sense examples translated by MT soft-
ware, we analysed the sentences using different
3See: http://mtgroup.ict.ac.cn/  zhp/ICTCLAS
47
tools and extracted relevant features. We ap-
plied stemming and POS tagging, using the fnTBL
toolkit (Ngai and Florian, 2001), as well as shal-
low parsing4. Then we extracted the following
types of topical and domain features5, which were
then fed to the VSM machine learner:
  Topical features: we extracted lemmas of the
content words in two windows around the tar-
get word: the whole context and a  4 word
window. We also obtained salient bigrams in
the context, with the methods and the soft-
ware described in (Pedersen, 2001). We in-
cluded another feature type, which match the
closest words (for each POS and in both
directions) to the target word (e.g. LEFT
NOUN ?dog? or LEFT VERB ?eat?).
  Domain features: The ?WordNet Domains?
resource was used to identify the most rel-
evant domains in the context. Following
the relevance formula presented in (Magnini
and Cavaglia?, 2000), we defined two feature
types: (1) the most relevant domain, and (2)
a list of domains above a threshold6.
For the dictionary-translated sense examples,
we simply used bags of words as features.
3.2 Evaluation
We evaluated our WSD classifier on both
coarse-grained and fine-grained datasets. For
coarse-grained WSD evaluation, we used TWA
dataset (Mihalcea, 2003), which is a binarily
sense-tagged corpus drawn from the British Na-
tional Corpus (BNC), for 6 nouns. For fine-
grained evaluation, we used Senseval-3 English
lexical sample dataset (Mihalcea et al, 2004),
which comprises 7,860 sense-tagged instances for
training and 3,944 for testing, on 57 words (nouns,
verbs and adjectives). The examples were mainly
drawn from BNC. WordNet      7 was used as
sense inventory for nouns and adjectives, and
Wordsmyth8 for verbs. We only evaluated our
WSD systems on nouns and adjectives.
4This software was kindly provided by David Yarowsky?s
group at Johns Hopkins University.
5Preliminary experiments using local features (bigrams
and trigrams) showed low performance, which was expected
because of noise in the automatically acquired data.
6This software was kindly provided by Gerard Escudero?s
group at Universitat Politecnica de Catalunya. The threshold
was set in previous work.
7http://wordnet.princeton.edu
8http://www.wordsmyth.net
We also used the SemCor corpus (Miller et al,
1993) for tuning our relative-threshold heuristic. It
contains a number of texts, mainly from the Brown
Corpus, comprising about 200,000 words, where
all content words have been manually tagged with
senses from WordNet.
Throughout the paper we will use the concepts
of precision and recall to measure the performance
of WSD systems, where precision refers to the ra-
tio of correct answers to the total number of an-
swers given by the system, and recall indicates the
ratio of correct answers to the total number of in-
stances. Our ML systems attempt every instance
and always give a unique answer, and hence preci-
sion equals to recall. When comparing with other
systems that participated in Senseval-3 in Table 7,
both recall and precision are shown. When POS
and overall averages are given, they are calculated
by micro-averaging the number of examples per
word.
4 Experiments on TWA dataset
First we trained a VSM classifier on the sense
examples translated with the Systran MT soft-
ware (we use notion ?MT-based approach? to re-
fer to this process), and then tested it on the TWA
test dataset. We tried two combinations of fea-
tures: one only used topical features and the other
used the whole feature set (i.e., topical and do-
main features). Table 1 summarises the sizes of
the training/test data, the Most Frequent Sense
(MFS) baseline and performances when apply-
ing the two different feature combinations. We
can see that best results were obtained when us-
ing all the features. It also shows that both our
systems achieved a significant improvement over
the MFS baseline. Therefore, in the subsequent
WSD experiments following the MT-based ap-
proach, we decided to use the entire feature set.
To compare the machine-translated sense exam-
ples with the ones translated word-by-word, we
then trained the same VSM classifier on the ex-
amples translated with a bilingual dictionary (we
use notion ?dictionary-based approach? to refer
to this process) and evaluated it on the same test
dataset. Table 2 shows results of the dictionary-
based approach and the MT-based approach. For
comparison, we include results from another sys-
tem (Mihalcea, 2003), which uses monosemous
relatives to automatically acquire sense examples.
The right-most column shows results of a 10-fold
48
Word Train ex. Test ex. MFS Topical All
bass 3,201 107 90.7 92.5 93.5
crane 3,656 95 74.7 84.2 83.2
motion 2,821 201 70.1 78.6 84.6
palm 1,220 201 71.1 82.6 85.1
plant 4,183 188 54.4 76.6 76.6
tank 3,898 201 62.7 79.1 77.1
Overall 18,979 993 70.6 81.1 82.5
Table 1:Recall(%) of the VSM classifier trained on the MT-
translated sense examples, with different sets of features. The
MFS baseline(%) and the number of training and test exam-
ples are also shown.
(Mihalcea, Dictionary- MT- Hand-
Word 2003) based based tagged
bass 92.5 91.6 93.5 90.7
crane 71.6 74.5 83.2 81.1
motion 75.6 72.6 84.6 93.0
palm 80.6 81.1 85.1 87.6
plant 69.1 51.6 76.6 87.2
tank 63.7 66.7 77.1 84.1
Overall 76.6 71.3 82.5 87.6
Table 2:Recall(%) on TWA dataset for 3 unsupervised sys-
tems and a supervised cross-validation on test data.
cross-validation on the TWA data, which indicates
the score that a supervised system would attain,
taking additional advantage that the examples for
training and test are drawn from the same corpus.
We can see that our MT-based approach has
achieved significantly better recall than the other
two automatic methods. Besides, the results of
our unsupervised system are approaching the per-
formance achieved with hand-tagged data. It is
worth mentioning that Mihalcea (2003) applied a
similar supervised cross-validation method on this
dataset that scored 83.35%, very close to our unsu-
pervised system9. Thus, we can conclude that the
MT-based system is able to reach the best perfor-
mance reported on this dataset for an unsupervised
system.
5 Experiments on Senseval-3
In this section we describe the experiments carried
out on the Senseval-3 lexical sample dataset. First,
we introduce a heuristic method to deal with the
problem of fine-grainedness of WordNet senses.
The remaining two subsections will be devoted
to the experiments of the baseline system and the
contribution of the heuristic to the final system.
9The main difference to our hand-tagged evaluation, apart
from the ML algorithm, is that we did not remove the bias
from the ?one sense per discourse? factor, as she did.
Remove Remove Sn.-Tk.
Threshold Senses Tokens ratio
4 7,669 (40.6) 11,154 (15.9) 2.55
5 9,759 (51.6) 15,516 (22.1) 2.34
6 11,341 (60.0) 18,827 (26.8) 2.24
7 12,569 (66.5) 21,775 (31.0) 2.14
8 13,553 (71.7) 24,224 (34.5) 2.08
9 14,376 (76.0) 27,332 (38.9) 1.95
10 14,914 (78.9) 29,418 (41.9) 1.88
Table 3:Sense filtering by relative-threshold on SemCor. For
each threshold the number of removed senses/tokens and am-
biguity are shown.
5.1 Unsupervised methods on fine-grained
senses
When applying unsupervised WSD algorithms to
fine-grained word senses, senses that rarely occur
in texts often cause problems, as these cases are
difficult to detect without relying on hand-tagged
data. This is why many WSD systems use sense-
tagged corpora such as SemCor to discard or pe-
nalise low-frequency senses.
For our work, we did not want to rely on hand-
tagged corpora, and we devised a method to detect
low-frequency senses and to remove them before
using our translation-based approach. The method
is based on the hypothesis that word senses that
have few close relatives (synonyms, hypernyms,
and hyponyms) tend to have low frequency in cor-
pora. We collected all the close relatives to the
target senses, according to WordNet, and then re-
moved all the senses that did not have a number of
relatives above a given threshold. We used this
method on nouns, as the WordNet hierarchy is
more developed for them.
First, we observed the effect of sense removal
in the SemCor corpus. For all the polysemous
nouns, we applied different thresholds (4-10 rel-
atives) and measured the percentage of senses and
SemCor tokens that were removed. Our goal was
to remove as many senses as we could, while keep-
ing as many tokens as possible. Table 3 shows
the results of the process on all        polysemous
nouns in SemCor for a total of 18,912 senses and
70,238 tokens. The average number of senses per
token initially is      .
For the lowest threshold (4) we can see that
we are able to remove a large number of senses
from consideration (40%), keeping 85% of the to-
kens in SemCor. Higher thresholds can remove
more senses, but it forces us to discard more valid
tokens. In Table 3, the best ratios are given by
lower thresholds, suggesting that conservative ap-
49
proaches would be better. However, we have to
take into account that unsupervised state-of-the-
art WSD methods on fine-grained senses perform
below 50% recall on this dataset10, and therefore
an approach that is more aggressive may be worth
trying.
We applied this heuristic method in our exper-
iments and decided to measure the effect of the
threshold parameter by relying on SemCor and the
Senseval-3 training data. Thus, we tested the MT-
based system for different threshold values, re-
moving the senses for consideration when the rel-
ative number was below the threshold. The results
of the experiments using this technique will be de-
scribed in Section 5.3.
5.2 Baseline system
We performed experiments on Senseval-3 test
data with both MT-based and dictionary-based ap-
proaches. We show the results for nouns and ad-
jectives in Table 4, together with the MFS base-
line (obtained from the Senseval-3 lexical sam-
ple training data). We can see that the results are
similar for nouns, while for adjectives the MT-
based system achieves significantly better recall.
Overall, the performance was much lower than our
previous 2-way disambiguation. The system also
ranks below the MFS baseline.
One of the main reasons for the low perfor-
mance was that senses with few examples in the
test data are over-represented in training. This is
because we trained the classifiers on equal num-
ber of maximumly 200 sense examples for every
sense, no matter how rarely a sense actually oc-
curs in real text. As we explained in the previ-
ous section, this problem could be alleviated for
nouns by using the relative-based heuristics. We
only implemented the MT-based approach for the
rest of the experiments, as it performed better than
the dictionary-based one.
5.3 Relative threshold
In this section we explored the contribution of
the relative-based threshold to the system. We
tested the system only on nouns. In order to
tune the threshold parameter, we first applied the
method on SemCor and the Senseval-3 training
data. We used hand-tagged corpora from two
different sources to see whether the method was
10Best score in Senseval-3 for nouns without SemCor
or hand-tagged data: 47.5% recall (figure obtained from
http://www.senseval.org).
Test Dictionary- MT-
Word Ex. MFS based based
Nouns 1807 54.23 40.07 40.73
Adjs 159 49.69 15.74 23.29
Overall 1966 53.86 38.10 39.32
Table 4:Averaged recall(%) for the dictionary-based and MT-
based methods in Senseval-3 lexical-sample data. The MFS
baseline(%) and the number of testing examples are also
shown.
Avg. test
Threshold ambiguity Senseval-3 SemCor
0 5.80 40.68 30.11
4 3.60 40.15 32.99
5 3.32 39.43 32.82
6 2.76 40.53 34.18
7 2.52 43.89 35.94
8 2.36 46.90 39.15
9 2.08 45.37 38.98
10 1.88 48.62 46.16
11 1.80 48.59 47.68
12 1.68 48.34 43.63
13 1.40 47.23 45.31
14 1.28 44.32 42.05
Table 5:Average ambiguity and recall(%) for the relative-
based threshold on Senseval-3 training data and SemCor (for
nouns only). Best results shown in bold.
generic enough to be applied on unseen test data.
Note also that we used this experiment to define a
general threshold for the heuristic, instead of opti-
mising it for different words. Once the threshold
is fixed, it will be used for all target words.
The results of the MT-based system applying
threshold values from 4 to 14 are given in Table 5.
We can see clearly that the algorithm benefits from
the heuristic, specially when ambiguity is reduced
to around 2 senses in average. Also observe that
the contribution of the threshold is quite similar
for SemCor and Senseval-3 training data. From
this table, we chose 11 as threshold value for the
test data, as it obtained the best performance on
SemCor.
Thus, we performed a single run of the algo-
rithm on the test data applying the chosen thresh-
old. The performance for all nouns is given in
Table 6. We can see that the recall has increased
significantly, and is now closer to the MFS base-
line, which is a very hard baseline for unsuper-
vised systems (McCarthy et al, 2004). Still, the
performance is significantly lower than the score
achieved by supervised systems, which can reach
above 72% recall (Mihalcea et al, 2004). Some of
the reasons for the gap are the following:
  The acquisition process: problems can arise
50
Word Test Ex. MFS Our System
argument 111 51.40 45.90
arm 133 82.00 85.70
atmosphere 81 66.70 35.80
audience 100 67.00 67.00
bank 132 67.40 67.40
degree 128 60.90 60.90
difference 114 40.40 40.40
difficulty 23 17.40 39.10
disc 100 38.00 27.00
image 74 36.50 17.60
interest 93 41.90 11.80
judgment 32 28.10 40.60
organization 56 73.20 19.60
paper 117 25.60 37.60
party 116 62.10 52.60
performance 87 26.40 26.40
plan 84 82.10 82.10
shelter 98 44.90 39.80
sort 96 65.60 65.60
source 32 65.60 65.60
Overall 1807 54.23 48.58
Table 6:Final results(%) for all nouns in Senseval-3 test data.
Together with the number of test examples and MFS base-
line(%).
from ambiguous Chinese words, and the ac-
quired examples can contain noise generated
by the MT software.
  Distribution of fine-grained senses: As we
have seen, it is difficult to detect rare senses
for unsupervised methods, while supervised
systems can simply rely on frequency of
senses.
  Lack of local context: Our system does
not benefit from local bigrams and trigrams,
which for supervised systems are one of the
best sources of knowledge.
5.4 Comparison with Senseval-3
unsupervised systems
Finally, we compared the performance of our
system with other unsupervised systems in the
Senseval-3 lexical-sample competition. We eval-
uated these systems for nouns, using the out-
puts provided by the organisation11 , and focusing
on the systems that are considered unsupervised.
However, we noticed that most of these systems
used the information of SemCor frequency, or
even Senseval-3 examples in their models. Thus,
we classified the systems depending on whether
they used SemCor frequencies (Sc), Senseval-3
examples (S-3), or did not (Unsup.). This is an
11http://www.senseval.org
System Type Prec. Recall
wsdiit S-3 67.96 67.96
Cymfony S-3 57.94 57.94
Prob0 S-3 55.01 54.13
clr04 Sc 48.86 48.75
upv-unige-CIAOSENSO Sc 53.95 48.70
MT-based Unsup. 48.58 48.58
duluth-senserelate Unsup. 47.48 47.48
DFA-Unsup-LS Sc 46.71 46.71
KUNLP.eng.ls Sc 45.10 45.10
DLSI-UA-ls-eng-nosu. Unsup. 20.01 16.05
Table 7:Comparison of unsupervised S3 systems for nouns
(sorted by recall(%)). Our system given in bold.
important distinction, as simply knowing the most
frequent sense in hand-tagged data is a big advan-
tage for unsupervised systems (applying the MFS
heuristic for nouns in Senseval-3 would achieve
54.2% precision, and 53.0% recall when using
SemCor). At this point, we would like to remark
that, unlike other systems using Semcor, we have
applied it to the minimum extent. Its only contri-
bution has been to indirectly set the threshold for
our general heuristic based on WordNet relatives.
We are exploring better ways to integrate the rela-
tive information in the model.
The results of the Senseval-3 systems are given
in Table 7. There are only 2 systems that do not re-
quire any hand-tagged data, and our method is able
to improve both when using the relative-threshold.
The best systems in Senseval-3 benefited from the
training examples from the training data, particu-
larly the top-scoring system, which is clearly su-
pervised. The 2nd ranked system requires 10%
of the training examples in Senseval-3 to map the
clusters that it discovers automatically, and the 3rd
simply applies the MFS heuristic.
The remaining systems introduce bias of the
SemCor distribution in their models, which clearly
helped their performance for each word. Our sys-
tem is able to obtain a similar performance to the
best of those systems without relying on hand-
tagged data. We also evaluated the systems on
the coarse-grained sense groups provided by the
Senseval-3 organisers. The results in Table 8 show
that our system is comparatively better on this
coarse-grained disambiguation task.
6 Conclusions and Future Work
We automatically acquired English sense exam-
ples for WSD using large Chinese corpora and MT
software. We compared our sense examples with
those reported in previous work (Wang and Car-
51
System Type Prec. Recall
wsdiit S-3 75.3 75.3
Cymfony S-3 66.6 66.6
Prob0 S-3 61.9 61.9
MT-based Unsup. 57.9 57.9
clr04 Sc. 57.6 57.6
duluth-senserelate Unsup. 56.1 56.1
KUNLP-eng-ls Sc. 55.6 55.6
upv-unige-CIAOSENSO- Sc. 61.3 55.3
DFA-Unsup-LS Sc. 54.5 54.5
DLSI-UA-ls-eng-nosu. Unsup. 27.6 27.6
Table 8:Coarse-grained evaluation of unsupervised S3 sys-
tems for nouns (sorted by recall(%)). Our system given in
bold.
roll, 2005), by training a ML classifier on them
and then testing the classifiers on both coarse-
grained and fine-grained English gold standard
datasets. On both datasets, our MT-based sense
examples outperformed dictionary-based ones. In
addition, evaluations show our unsupervised WSD
system is competitive to the state-of-the-art super-
vised systems on binary disambiguation, and un-
supervised systems on fine-grained disambigua-
tion.
In the future, we would like to combine our ap-
proach with other systems based on automatic ac-
quisition of sense examples that can provide lo-
cal context (Agirre and Martinez, 2004b). The
goal would be to construct a collection of exam-
ples automatically obtained from different sources
and to apply ML algorithms on them. Each exam-
ple would have a different weight depending on
the acquisition method used.
Regarding the influence of sense distribution
in the training data, we will explore the poten-
tial of using a weighting scheme on the ?relative
threshold? algorithm. Also, we would like to anal-
yse if automatically obtained information on sense
distribution (McCarthy et al, 2004) can improve
WSD performance. We may also try other MT
systems and possibly see if our WSD can in turn
help MT, which can be viewed as a bootstrapping
learning process. Another interesting direction is
automatically selecting the most informative sense
examples as training data for ML classifiers.
References
E. Agirre and D. Martinez. 2004a. The Basque Country Uni-
versity system: English and Basque tasks. In Proceedings
of the 3rd ACL workshop on the Evaluation of Systems
for the Semantic Analysis of Text (SENSEVAL), Barcelona,
Spain.
E. Agirre and D. Martinez. 2004b. Unsupervised wsd
based on automatically retrieved examples: The impor-
tance of bias. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing (EMNLP),
Barcelona, Spain.
Y. S. Chan and H. T. Ng. 2005. Scaling up word sense disam-
biguation via parallel texts. In Proceedings of the 20th Na-
tional Conference on Artificial Intelligence (AAAI 2005),
Pittsburgh, Pennsylvania, USA.
I. Dagan and A. Itai. 1994. Word sense disambiguation using
a second language monolingual corpus. Computational
Linguistics, 20(4):563?596.
M. Diab and P. Resnik. 2002. An unsupervised method for
word sense tagging using parallel corpora. In Proceedings
of the      Anniversary Meeting of the Association for
Computational Linguistics (ACL-02). Philadelphia, USA.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
W. A. Gale, K. W. Church, and D. Yarowsky. 1992. Us-
ing bilingual materials to develop word sense disambigua-
tion methods. In Proceedings of the International Con-
ference on Theoretical and Methodological Issues in Ma-
chine Translation, pages 101?112.
H. Li and C. Li. 2004. Word translation disambiguation us-
ing bilingual bootstrapping. Computational Linguistics,
20(4):563?596.
B. Magnini and G. Cavaglia?. 2000. Integrating subject field
codes into WordNet. In Proceedings of the Second Inter-
national LREC Conference, Athens, Greece.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding Predominant Word Senses in Untagged Text. In
Proceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Barcelona,
Spain.
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004. The
Senseval-3 English lexical sample task. In Proceedings of
the 3rd ACL workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
R. Mihalcea. 2003. The role of non-ambiguous words in
natural language disambiguation. In Proceedings of the
Conference on Recent Advances in Natural Language Pro-
cessing, RANLP.
G. A. Miller, C. Leacock, R. Tengi, and R. Bunker. 1993.
A Semantic Concordance. In Proceedings of the ARPA
Human Language Technology Workshop, pages 303?308,
Princeton, NJ, March. distributed as Human Language
Technology by San Mateo, CA: Morgan Kaufmann Pub-
lishers.
H. T. Ng, B. Wang, and Y. S. Chan. 2003. Exploiting parallel
texts for word sense disambiguation: an empirical study.
In Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics.
G. Ngai and R. Florian. 2001. Transformation-based learn-
ing in the fast lane. Proceedings of the Second Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 40-47, Pittsburgh,
PA, USA.
T. Pedersen. 2001. A decision tree of bigrams is an accu-
rate predictor of word sense. Proceedings of the Second
Meeting of the NAACL, Pittsburgh, PA.
X. Wang and J. Carroll. 2005. Word sense disambiguation
using sense examples automatically acquired from a sec-
ond language. In Proceedings of HLT/EMNLP, Vancou-
ver, Canada.
52
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 71?79,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Species Disambiguation for Biomedical Term Identification
Xinglong Wang and Michael Matthews
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
{xwang,mmatsews}@inf.ed.ac.uk
Abstract
An important task in information extraction
(IE) from biomedical articles is term iden-
tification (TI), which concerns linking en-
tity mentions (e.g., terms denoting proteins)
in text to unambiguous identifiers in stan-
dard databases (e.g., RefSeq). Previous work
on TI has focused on species-specific docu-
ments. However, biomedical documents, es-
pecially full-length articles, often talk about
entities across a number of species, in which
case resolving species ambiguity becomes an
indispensable part of TI. This paper de-
scribes our rule-based and machine-learning
based approaches to species disambiguation
and demonstrates that performance of TI can
be improved by over 20% if the correct species
are known. We also show that using the
species predicted by the automatic species tag-
gers can improve TI by a large margin.
1 Introduction
The exponential growth of the amount of scien-
tific literature in the fields of biomedicine and ge-
nomics has made it increasingly difficult for sci-
entists to keep up with the state of the art. The
TXM project (Alex et al, 2008a), a three-year project
which aims to produce software tools to aid cura-
tion of biomedical papers, targets this problem and
exploits natural language processing (NLP) technol-
ogy in an attempt to automatically extract enriched
protein-protein interactions (EPPI) and tissue expres-
sions (TE) from biomedical text.
A critical task in TXM is term identification (TI),
the task of grounding mentions of biomedical named
entities to identifiers in referent databases. TI can be
seen as an intermediate task that builds on the pre-
vious component in an information extraction (IE)
pipeline, i.e., named entity recognition (NER), and
provides crucial information as input to the more
complex module of relation extraction (RE). The
structure of the IE pipeline resembles a typical cu-
ration process by human biologists. For example,
when curating protein-protein interactions (PPIs), a
curator would first mark up the protein mentions in
text, and then identify the mentions by finding their
unique identifiers from standard protein databases
such as RefSeq,1 and finally curate pairs of IDs as
PPIs.
TI is a matching and disambiguation pro-
cess (Wang and Matthews, 2008), and a primary
source of ambiguity lies in the model organisms of
the terms. In curation tasks, one often needs to deal
with collections of articles that involve entities of a
large variety of species. For example, our collec-
tion of articles from PubMed and PubMed Central
involve over 100 model organisms. Also, it is of-
ten the case that more than one species appear in the
same document, especially when the document is a
full-length article. In our dataset, 74% of the arti-
cles concern more than one organism. In many stan-
dard databases, such as RefSeq and SwissProt, ho-
molog proteins in different species, which often con-
tain nearly identical synonym lists, are assigned dis-
tinct identifiers. This makes biomedical terms even
more polysemous and hence species disambiguation
becomes crucial to TI. For example, querying Ref-
Seq2 with the protein mention plk1 resulted in 98
1http://www.ncbi.nlm.nih.gov/RefSeq/
2The searches were carried out on November 5, 2007.
71
hits. By adding a species to the query, e.g. mouse,
one can significantly reduce the number of results to
two.
This paper describes our work on the task of
species disambiguation. We also report the perfor-
mance gain of a TI system from integration of vari-
ous automatic species taggers. The paper is organ-
ised as follows. Section 2 gives a brief overview of
related work. Section 3 presents our methodologies
for species disambiguation. Section 4 describes a
rule-based TI system that we developed in the TXM
project, and the evaluation metrics. This section also
reports the evaluation results of the TI system with
and without help from the species predicted by the
taggers. We finally conclude in Section 5.
2 Related Work
The most relevant work to ours are the Gene Nor-
malisation (GN) tasks (Morgan and Hirschman,
2007; Hirschman et al, 2004) in the BioCreAtIvE I
& II workshops (Hirschman et al, 2007; Hirschman
et al, 2005), which provided forums for exchang-
ing thoughts and methodologies on tackling the task
of TI. The data provided in the GN tasks, however,
were species-specific, which means that the lexicons
and datasets were concerned with single model or-
ganisms and thus species disambiguation was not
required. A few participating systems, however, in-
tegrated a filter to rule out entities with erroneous
species (Hanisch et al, 2005; Fluck et al, 2007),
which were reported to be helpful. Another differ-
ence between our task and the BioCreAtIvE GN ones
is that we carry out TI on entity level while GN on
document level.
It is worth mentioning that the protein-protein in-
teraction task (IPS) in BioCreAtIvE II has taken into
account species ambiguity. The IPS task resembles
the work-flow of manual curation of PPIs in articles
involving multiple species, and to accomplish the
task, one would require a full pipeline of IE systems,
including named entity recognition, term identifica-
tion and relation extraction. The best result for IPS
(Krallinger et al, 2007) was fairly low at 28.85%
F1, which reflects the difficulty of the task. Some
participants of IPS have reported (e.g., Grover et al,
2007) that resolving species ambiguity was one of
the biggest challenges. Our analysis of the IPS train-
ing data revealed that the interacting proteins in this
corpus belong to over 60 species, and only 56.27%
of them are human.
As noted in previous work (Krauthammer and Ne-
nadic, 2004; Chen et al, 2005; Krallinger et al,
2007; Wang, 2007), determining the correct species
for the protein mentions is a very important step to-
wards TI. However, as far as we know, there has
been little work in species disambiguation and in to
what extent resolving species ambiguity can help TI.
3 Species Disambiguation
3.1 Data and Ontology
The species tagger was developed on the ITI TXM
corpora (Alex et al, 2008b), which were produced
as part of the TXM project (Alex et al, 2008a). We
created two corpora in slightly different domains,
EPPI and TE. The EPPI corpus consists of 217 full-
text papers selected from PubMed and PubMed Cen-
tral and domain experts annotated all documents for
both protein entities and PPIs, as well as extra (en-
riched) information associated with the PPIs and nor-
malisations of the proteins to publicly available on-
tologies. The TE corpus consists of 230 full-text
papers, in which entities such as proteins, tissues,
genes and mRNAcDNAs were identified, and a new
tissue expression relation was marked up.
We used these corpora to develop a species tag-
ging system. As the biomedical entities in the
data were manually assigned with standard database
identifiers,3 it was straightforward to obtain their
species IDs through the mappings provided by En-
trezGene and RefSeq. In more detail, proteins, pro-
tein complexes, genes and mRNAcDNAs in both EPPI
and TE datasets were assigned with NCBI Taxon-
omy IDs (TaxIDs)4 denoting their species. The
EPPI and TE datasets have different distributions of
species. The entities in the EPPI data belong to
118 species with human being the most frequent
at 51.98%. In the TE data, the entities are across
67 species and mouse was the most frequent at
44.67%.5
To calculate the inter-annotator-agreement, about
40% of the documents were doubly annotated by
different annotators. The averaged F1 scores of
3In our data, genes are tagged with EntrezGene IDs, and
proteins and mRNAcDNAs with RefSeq IDs.
4http://www.ncbi.nlm.nih.gov/sites/
entrez?db=Taxonomy
5These figures were obtained from the training split of the
datasets.
72
EPPI devtest TE devtest
P R F1 P R F1
PreWd 81.88 1.87 3.65 91.49 1.63 3.21
PreWd + Spread 63.85 14.17 23.19 77.84 17.97 29.20
PreWd Sent 60.79 5.16 9.52 56.16 7.76 13.64
PreWd Sent + Spread 39.74 50.54 44.49 31.71 46.68 37.76
Prefix 98.98 3.07 5.96 77.93 2.97 5.72
PreWd + Prefix 91.95 4.95 9.40 82.27 4.62 8.75
PreWd + Prefix + Spread 68.46 17.49 27.87 77.77 21.26 33.39
Table 1: Results (%) of the rule-based species tagger.
species annotation on the doubly annotated EPPI and
TE datasets are 86.45% and 95.11%, respectively,
indicating that human annotators have high agree-
ment when assigning species to biomedical entities.
3.2 Detecting Species Words
Words referring to species, such as human, are im-
portant indicators of the species of the nearby enti-
ties. We have developed a rule-based program that
detects species words, which were used to help the
species identification systems described in the fol-
lowing sections.
The species word tagger is a lexical look-up
component which applies to tokenised text and
marks content words such as human, murine and
D. melanogaster with their corresponding species
TaxIDs. In addition, rules written in an lxtransduce
grammar6 are used to identify species prefixes (e.g.,
?h? for human, ?m? for mouse). For example, the
term mSos-1 would be assigned with a TaxID for
mouse. Note that a species ?word? may contain sev-
eral words, for example, ?E. coli?. Please see (Wang
and Grover, 2008) for more details on the species
word tagger.
3.3 Assigning Species to Entities
3.3.1 Rule-based Approach
It is intuitive that a species word that occurs near
an entity (e.g., ?mouse p53?) is a strong indicator of
its species. To assess this intuition, we developed a
set of five rules using heuristics and species words
detected by the species word tagger.
? PreWd: If the word preceding an entity is a species
word, assign the species indicated by that word to
the entity.
6See http://www.ltg.ed.ac.uk/software/
ltxml2 for details of the LT-XML 2 tools developed at the
LTG group at Edinburgh University.
? PreWd Sent: If a species word that occurs to the
left of an entity and in the same sentence, assign the
species indicated by that word to the entity.
? Prefix: If an entity has a species-indicating prefix,
e.g., mSos-1, then tag the species to that entity.
? Spread: Spread the species of an entity e to all en-
tities in the same document that have the same sur-
face form with e. This rule must be used in conjunc-
tion with the other rules.
? Majority Vote:7 Count the species words in a docu-
ment and assign as a weight to each species the pro-
portion of all species words in the document that
refer to the species.8 Tag all entities in the docu-
ment the species with the highest weight, defaulting
to human in the case of a tie.
Table 1 shows the results of species tagging when
the above rules were applied. As we can see, the pre-
cision of the systems that rely solely on the previous
species words or prefixes is very good but the recall
is low. The system that looks at the previous species
word in the same sentence does better as measured
by F1. In addition, spreading the species improves
both systems but the overall results are still not sat-
isfactory.
It is slightly counter-intuitive that using a rule
such as ?PreWd? did not achieve perfect precision.
Closer inspection revealed that most of the false pos-
itives were due to a few problematic guidelines in
the annotation process. For example,
? ?The amounts of human and mouse CD200R ...?,
where ?CD200R? was tagged as mouse (10090) by
the system but the gold-standard answer was human
(9606). This was due to the fact that the annotation
tool was not able to assign multiple correct species
7TheMajority Vote rule was used by default in the TI system,
which is described in Section 4.1.
8For example, if there are N species words in a document
and Nhuman are associated with human, the human species
weight is calculated as NhumanN .
73
BL EPPI TE Combined EPPI Model TE Model Combined Model
Model Model Model +Rules +Rules +Rules
EPPI devtest 60.56 73.03 58.67 72.28 74.24 59.67 73.77
TE devtest 30.22 67.15 69.82 67.20 67.53 70.14 67.47
Overall 48.88 70.77 62.96 70.33 71.66 63.70 71.34
Table 2: Accuracy (%) of the machine-learning based species tagger and the hybrid species tagger as tested on the
EPPI and TE devtest datasets. An ?Overall? score is the micro-average of a system?s accuracy on both datasets.
to a single entity.
? ?... wheat eIFiso4G ...?, where ?eIFiso4G? was
tagged as wheat (4565) but the annotator thought
it was Triticum (4564). In this case, TaxID 4565 is
a species under genus 4564, and arguably is also a
correct answer. Other similar cases include Xeno-
pus vs. Xenopus tropicalis, and Rattus vs. Rattus
norvegicus, etc. This is the main cause for the false
positives as our system always predicts species in-
stead of genus or TaxIDs of any other ranks, which
the annotators occasionally employed.
3.3.2 Machine Learning Approach
We split the EPPI and TE datasets into training
and development test (devtest) sets and developed
a machine-learning (ML) based species tagger. Us-
ing the training splits, we trained a maximum en-
tropy classifier9 using the following set of features,
with respect to each entity occurrence. The param-
eter n was empirically developed using the training
datasets.
? leftContext The n word lemmas to the left of the
entity, without position (n = 200).
? rightContext The n word lemmas to the right of the
entity, without position (n = 200).
? leftSpeciesIDs The n species IDs, located to the left
of the entity and assigned by the species word tag-
ger (n = 5).
? rightSpeciesIDs The n species IDs, located to the
right of the entity and assigned by the species word
tagger (n = 5).
? leftNouns The n nouns to the left of the entity (with
order and n = 2). This feature attempts to cap-
ture cases where a noun preceding an entity indi-
cates species, e.g., mouse protein p53.
? leftAdjs The n adjectives to the left of the entity
(with order and n = 2). This feature intends to
capture cases where an adjective preceding an en-
tity indicates species, e.g., murine protein p53.
9http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
? leftSpeciesWords The n species word forms, identi-
fied by the species word tagger, located to the left
of the entity (n = 5).
? rightSpeciesWords The n species word forms, iden-
tified by the species word tagger, located to the right
of the entity (n = 5).
? firstLetter The first character of the entity itself.
Sometimes the first letters of entities indicate their
species, e.g., hP53.
? documentSpeciesIDs All species IDs that occur in
the article in question.
? useStopWords If this feature is switched on then fil-
ter out the words that appear in a pre-compiled stop-
word list from the above features. The list consists
of frequent common English words such as prepo-
sitions (e.g., in).
? useStopPattern If this feature is switched on then fil-
ter out the words consisting only of digits and punc-
tuation characters.
The results of the ML species tagger are shown in
Table 2. We measure the performance in accuracy
instead of F1 because the ML based tagger assigns a
species tag to every entity occurrence, and therefore
precision is equal to recall. We tested four models
on the devtest portions of the EPPI and TE corpora:
? BL: a baseline system, which tags the devtest in-
stances using the most frequent species occurring
in the corresponding training dataset. For example,
human is the most frequent species in the EPPI train-
ing data, and therefore all entities in the EPPI devtest
dataset were tagged with human.
? EPPI Model: obtained by training the maxent clas-
sifier on the EPPI training data.
? TE Model: obtained by training the maxent classi-
fier on the TE training data.
? Combined Model: obtained by training the maxent
classifier on a joint dataset consisting of both the
EPPI and TE training corpora.
3.3.3 Hybrid Approach
As we have shown, rules ?PreWd? and ?Prefix?
achieved very good precision but low recall, which
74
suggests that when these rules were applicable, it is
highly likely that they would get the correct species.
Based on this observation, we combined the ML ap-
proach and the rule-based approach in such a way
that the rules ?PreWd? and ?Prefix? were applied on
top of ML and override predictions made by ML. In
other words, the rules act as a post-processor and
correct the decisions made by the ML when very
strong species indicators such as previous species
words or species prefixes are detected. This should
increase precision and at the same time keep recall
relatively intact. The hybrid systems were tested on
the same datasets and the results are shown in the
right 3 columns in Table 2.
We performed significance tests on the results in
Table 2. First, a Friedman test was used to deter-
mine whether the 7 sets of results10 were signifi-
cantly different, and then pairwise Wilcoxon Signed
Rank tests were employed to tell whether any sys-
tem performed significantly better than others. On
both datasets, the 6 machine-learning models signif-
icantly outperformed the baseline (p < 0.01). On
EPPI devtest dataset, the EPPI models (with or with-
out rules) and the Combined Models outperformed
the TE models (p < 0.05), while on TE dataset, the
TE models and the Combined Models outperformed
the EPPI models (p < 0.05). Also, applying the
post filtering rules did not significantly improve the
ML models, although it appears that adding the rules
consistently increase the accuracy by a small mar-
gin.
4 Term Identification
4.1 The TI system
The TI system is composed of a matcher which de-
termines a list of candidate identifiers and a ranker
that assigns a confidence value to each identifier
that is used to rank the candidates in order with the
most likely identifiers occurring first. The matcher is
based largely on the rule-based system described in
(Wang and Matthews, 2008), but has been put into a
more flexible framework that allows for defining and
customising the rules in a configuration file. In ad-
dition, the system has been expanded to perform TI
on additional entity types. The rules for each entity
were developed using the training data and a visuali-
10The Friedman test requires accuracy figures with respect to
each document in the datasets, which are not shown in Table 2.
sation system that compared the synonym list for the
target identifiers with the actual entity mentions and
provided visual feedback on the true positives and
false positives resulting from candidate rules sets.
Examples of some of the rules that can be incorpo-
rated into the system are listed below. A confidence
value is assigned to each of the rules using heuristics
and passed to the ranking system.
1. LowerCase: Convert the entity mention to lower-
case and look up the result in a lower case version
of the entity term database.
2. Norm: Normalise the mention11 and look up the re-
sult in a normalised version of the term database.
3. Prefix: Add and/or remove a set of prefixes from
the entity mention and look up the result in the en-
tity term database. The actual prefixes and whether
to add or remove them are specified in the configu-
ration file.
4. Suffix: Add and/or remove a set of suffixes from the
entity mention and look up the result in the entity
term database. The actual suffixes and whether to
add or remove them are specified in the configura-
tion file.
5. Porter: Compute the Porter stem of the entity men-
tion and looked up the synonym in a Porter stemmed
version of the entity term database.
The ranking system currently works by defining a
set of confidence indicators for each entity, comput-
ing the confidence for each indicator and then multi-
plying each individual confidence together to deter-
mine the overall identifier confidence. The follow-
ing indicators are currently used by the system.
1. Match: The confidence as determined by the
matcher.
2. Species: The confidence that the species of the iden-
tifier is the correct species.
3. Reference Count: Based on the number of liter-
ature references12 associated with each identifier.
The higher the reference count, the higher the con-
fidence.
11Normalising a string involves converting Greek characters
to English (e.g., ??alpha), converting to lowercase, changing
sequential indicators to integer numerals (e.g., i, a, alpha?1,
etc.) and removing all spaces and punctuation. For example,
rab1, rab-1, rab?, rab I are all normalised to rab1.
12The Reference Counts were obtained from EntrezGene and
RefSeq databases.
75
4. Primary Name: Based on a determination that the
entity mention is the primary name for the identi-
fier. This is based both on a name provided by the
lexicon and a name derived from the synonym list.
Among these, one of the most critical indicators is
the species confidence. By default, this confidence
is set to the weight assigned to the species by the
Majority Vote tagger (see Section 3.3.1). When the
species of an entity is tagged by an external species
tagger or by human annotators, the default confi-
dence can be overridden. This setting allows us to
integrate automatic species taggers, such as the ones
described in the previous section, for achieving bet-
ter TI performance. For example, suppose we want
to employ theHybrid species tagger. To compute the
species confidence, first the hybrid tagger is used to
predict the most likely species and the Majority Vote
tagger is run at the same time. If the species of an
identifier matches the species assigned by the hybrid
tagger, the species confidence is set to the weight
generated by the hybrid tagger. Otherwise, the con-
fidence is set to the weight generated by theMajority
Vote tagger.
To assess how much species ambiguity accounts
for the overall ambiguity in biomedical entities, we
estimated the averaged ambiguity rates for the pro-
tein entities in the TXM datasets, without and with
the species information. Suppose there are n unique
protein mentions in a dataset. First, we look up the
RefSeq database by exact match with every unique
protein mention mi, where i ? {0..n ? 1}, and
for each mi we retrieve two lists of identifiers: Li
and L?i, where Li consists of all identifiers and L
?
i
only contains the identifiers whose model organ-
ism matches the manually tagged species of the pro-
tein mention. The ambiguity rates without and with
species are computed by
Pn?1
i=0 |Li|
n and
Pn?1
i=0 |L
?
i|
n , re-
spectively. Table 3 shows the ambiguity rates on the
EPPI and TE datasets.
Protein Cnt ID Cnt Ambiguity
EPPI 6,955 184,633 26.55
EPPI species 6,955 17,357 2.50
TE 8,539 103,016 12.06
TE species 8539 12,705 1.49
Table 3: Ambiguity in protein entities, with and without
species information, in EPPI and TE datasets.
4.2 Experiments on TXM Data
To identify whether species disambiguation can im-
prove performance of TI, we ran the TI system on
the EPPI and TE data. As shown in Tables 4 and 5,
we tested the TI systems with or without help from
a number of species tagging systems, including:
? Baseline: Run TI without species tags.13
? Gold Species: Run TI with manually tagged species.
This is the upper-bound performance.
? Rule: Run TI with species predicted by the rule-
based species tagger.
? ML(human/mouse): Run TI with the species that oc-
curs most frequently in the training datasets (i.e.,
human for EPPI and mouse for TE).
? ML(EPPI): Run TI with species predicted by the ML
tagger trained on the EPPI training dataset.
? ML(EPPI)+Rule: Run TI with species predicted by
the hybrid system using both ML(EPPI) and the
rules.
? ML(TE): Run TI with species predicted by the ML
tagger trained on the TE training dataset.
? ML(TE)+Rule: Run TI with species predicted by the
hybrid system using both ML(TE) and the rules.
? ML(EPPI+TE): Run TI with species predicted by the
ML tagger trained on both EPPI and TE training data.
? ML(EPPI+TE)+Rule: Run TI with species predicted
by the hybrid system using both ML(EPPI+TE) and
the rules.
We score the systems using top n precision, where
n ? {1, 5, 10, 15, 20}. The argument for this evalua-
tion scheme is that if a TI system is not good enough
in predicting a single identifier correctly, a ?bag? of
IDs with the correct answer included would also be
helpful. The ?Avg. Rank? field denotes the averaged
position where the correct answer lies in, and the
lower the value is, the better the TI system performs.
For example, a TI system with an ?Avg. Rank? of 1
would be ideal, as it would always return the correct
ID at the top of the list. Note that in the TE data, not
only protein entities, but also genes, mRNAcDNA,
and GOMOPs14 were tagged.
On both datasets, using the gold standard species
much improved accuracy of TI (e.g., 19.2% on EPPI
13Note that the TI system already integrated a basic species
tagging system that uses the Majority Vote rule as described in
Section 3.3.1. Thus this is a fairly high ?baseline?.
14GOMOP is a tag that denotes an entity being either a gene,
or an mRNAcDNA, or a protein, which was used when the anno-
tator could not determine what type the entity in question was.
76
Method Prec@1 Prec@5 Prec@10 Prec@15 Prec@20 Avg. Rank
Baseline 54.31 73.45 76.44 77.90 78.51 5.82
Gold Species 73.52 79.36 80.75 80.75 80.99 1.62
Rule 54.99 73.72 76.45 77.91 78.52 5.79
ML(human) 65.66 76.36 78.82 79.78 80.03 2.58
ML(EPPI) 65.24 76.82 79.01 79.93 80.29 2.39
ML(EPPI)+Rule 65.88 77.09 79.04 79.94 80.30 2.36
ML(TE) 55.87 75.14 78.69 79.85 80.30 2.86
ML(TE)+Rule 56.54 75.47 78.70 79.86 80.31 2.83
ML(EPPI+TE) 64.55 76.48 78.53 79.83 80.38 2.49
ML(EPPI+TE)+Rule 65.03 76.62 78.55 79.84 80.39 2.46
Table 4: Results of TI on the EPPI dataset. All figures, except ?Avg. Rank?, are percentages. This evaluation was
carried out on protein entities only.
Method Prec@1 Prec@5 Prec@10 Prec@15 Prec@20 Avg. Rank
Baseline 63.24 76.20 77.30 77.94 78.25 1.72
Gold Species 71.82 78.03 78.34 78.40 78.41 1.29
Rule 63.45 76.21 77.30 77.95 78.25 1.72
ML(mouse) 58.76 75.40 77.25 77.92 78.24 1.90
ML(EPPI) 66.59 76.53 77.23 77.76 78.12 1.68
ML(EPPI)+Rule 66.85 76.54 77.24 77.76 78.12 1.67
ML(TE) 66.12 76.25 77.32 77.81 78.11 1.70
ML(TE)+Rule 66.37 76.25 77.32 77.81 78.11 1.70
ML(EPPI+TE) 65.78 76.14 77.28 77.84 78.12 1.71
ML(EPPI+TE)+Rule 66.03 76.14 77.29 77.84 78.12 1.70
Table 5: Results of TI on the TE dataset. All figures, except ?Avg. Rank?, are percentages. There are four entity types
in the TE data, i.e., protein, gene, mRNAcDNA and GOMOP. The evaluation was carried out on all entity types.
data). Also, automatically predicted species tags
were proven to be helpful. On the EPPI data, the
ML(EPPI)+Rule outperformed other systems. Note
that the species distribution in the devtest dataset is
strongly biased to human, which explains why the
ML(human) system performed nearly as well. How-
ever, defaulting to human was not guaranteed to suc-
ceed because one would not be able to know the
prior species in a collection of unseen documents.
Indeed, on the TE data, the system ML(mouse),
which uses the most frequent species in the training
data, i.e. mouse, as default, yielded poor results.
4.3 Experiments on BioCreAtIvE Data
To assess the portability of the species tagging ap-
proaches, an ?artificial? dataset was created by join-
ing the species-specific datasets from BioCreAtIvE
1 & 2 GN tasks to form a corpus consisting of four
species. In detail, four datasets were taken, three
from BioCreAtIvE 1 task 1B (i.e., fly, mouse and
yeast) and one from BioCreAtIvE 2 task GN (i.e., hu-
man). Assuming genes in each dataset are species-
specific,15 we can train/test ML models for species
disambiguation and apply them to help TI. This task
is more difficult than the original BioCreAtIvE GN
tasks due to the additional ambiguity caused by mul-
tiple model organisms.
We first carried out experiments on species dis-
ambiguation. In addition to the TXM (i.e., the sys-
tem uses ML(EPPI+TE)+Rule model) and the Major-
ity Vote taggers, we trained the species tagger on
a dataset comprising of the devtest sets from the
BioCreAtIvE I & II GN tasks. In more detail, we first
pre-processed the dataset and marked up gene enti-
ties with an NER system (Alex et al, 2007; Grover et
al., 2007).16 The entities were also tagged with the
15This assumption is not strictly true because each dataset
may contain genes of other species, and it would be hard to
assess how true it is as abstracts in the BioCreAtIvE GN datasets
are not normalised to an entity level.
16The NER system was trained on BioCreAtIvE II GM train-
ing and test datasets.
77
species as indicated by the source dataset where they
were drawn from, which were used as the ?Gold?
species. Using the same algorithm and feature set as
described in Section 3.3.2, a BC model was trained.
human fly mouse yeast
Majority Vote 82.35 78.43 71.69 85.12
BC model 70.23 89.24 75.41 87.64
TXM model 93.35 3.27 31.89 3.49
Table 6: Accuracy (%) of the species disambiguation
systems as tested on the BioCreAtIvE I & II test data.
The ?BC model? was trained on the BioCreAtIvE de-
vtest data, the ?TXM model? was trained on the TXM EPPI
and TE training data, and the ?Majority Vote? was the de-
fault species tagging system in the TI system (see Section
3.3.1).
As shown in Table 6, except on human, the TXM
model yielded very disappointing results, whereas
the BC model did well overall. This was because
the TXM model was trained on a dataset where fly
and yeast entities occur rarely with only 2% and 5%
of the training instances belonging to these species,
respectively, which again revealed the influence of
the bias introduced in the training material to the ML
models.
System Precision Recall F1
Gold 70.1 63.3 66.5
Majority Vote 46.7 56.3 51.0
TXM model 37.8 46.5 41.7
BC model 45.8 56.1 50.4
Table 7: Performance of TI with or without the automati-
cally predicted species on the joint BioCreAtIvE GN test
dataset.
Using the species disambiguation models, we car-
ried out TI experiments, using the same procedure
as we did on the TXM data. The results were ob-
tained using the official BioCreAtIvE GN scorers17
and are presented in Table 7. Performance of TI as-
sisted by all three species taggers were much behind
that of TI using the gold-standard species, which
shows species-tagging can potentially enhance TI
performance and there is much room for improving
17We tested the TI system on the four original BioCreAtIvE
GN datasets separately and the averaged performance was about
the median among the participating systems in the workshops.
We did not optimise the TXM TI system on BioCreAtIvE, as our
point here is to measure the TI performance with or without help
from the automatic predicted species.
the species disambiguation systems. On the other
hand, it was disappointing that the ?Majority Vote?
system, which did not use any external species tag-
ger, achieved the best results, while TI with the ?BC
model? tagger yielded slightly worse results and the
TXM model performed poorly.
# Species # of Docs % of Docs
1 96 26.20
2 121 32.79
3+ 153 41.19
Table 8: # of species per document in the TXM data.
One possible reason that the ?Majority Vote? tag-
ger yielded reasonably good result on the BioCre-
AtIvE dataset, but unsatisfactory result on the TXM
datasets was due to the difference in document
length in the two corpora: the BioCreAtIvE corpus
is comprised of abstracts and the TXM corpora con-
sist of only full-length articles. In abstracts, authors
are inclined to only talk about the main biomedical
entities described in the paper, whereas in full arti-
cles, they tend to describe a larger number of enti-
ties, possibly in multiple species, for the purposes of
describing related work or comparison. Recall that
the ?Majority Vote? rule outputs the species indicated
by the majority of the species words, which would
obviously perform better on abstracts, where more
likely only one species is described, than on full-
length articles. Table 8 shows the number of species
per document in the TXM data, where most docu-
ments (i.e., 74%) involve more than one species, in
which cases the ?Majority Vote? would not be able to
take obvious advantage.
5 Conclusions and Future Work
This paper presented a range of solutions to the task
of species disambiguation and evaluated their per-
formance on the ITI TXM corpus, and on a joint
dataset from BioCreAtIvE I & II GN tasks. We
showed that rule-based species tagging systems that
exploit heuristics, such as previous species words or
species prefix, can achieve very high precision but
low recall. ML species taggers, on the other hand,
can achieve good overall performance, under the
condition that the species distributions in training
and test datasets are not too distant. Our best per-
forming species tagger is a hybrid system that first
78
uses ML to predict species and then applies certain
rules to correct errors.
We also performed TI experiments with help from
species tags assigned by human annotators, or pre-
dicted by the automatic species taggers. On all
datasets, the gold-standard species tags improved TI
performance by a large margin: 19.21% on the EPPI
devtest set, 8.59% on the TE devtest set, and 23.4%
on the BioCreAtIvE GN test datasets, which clearly
shows that species information is indeed very impor-
tant for TI. On the EPPI and TE datasets, the species
predicted by the best-performing hybrid system im-
proved TI by 11.57% and 3.61%, respectively. On
the combined dataset from BioCreAtIvE GN tasks,
however, it did not work as well as expected.
In the future we plan to work on better ways to
integrate the machine learning approaches and the
rules. In particular, we would like to explore statis-
tical relational learning, which may provide ways to
integrate rules as constraints into machine learning
and may be able to alleviate the bias in the learnt
models.
Acknowledgements
The work was supported by the ITI Life Sciences
Text Mining programme.18
References
B. Alex, B. Haddow, and C. Grover. 2007. Recognising
nested named entities in biomedical text. In Proceed-
ings of BioNLP 2007, Prague, Czech Republic.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang.
2008a. Assisted curation: does text mining really
help? In Proceedings of PSB.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang.
2008b. The ITI TXM corpus: Tissue expression
and protein-protein interactions. In Proceedings of
the LREC Workshop on Building and Evaluating Re-
sources for Biomedical Text Mining, Morocco.
L. Chen, H. Liu, and C. Friedman. 2005. Gene name am-
biguity of eukaryotic nomenclatures. Bioinformatics,
21(2):248?256.
J. Fluck, H. Mevissen, H. Dach, M. Oster, and
M. Hofmann-Apitius. 2007. ProMiner: Recogni-
tion of human gene and protein names using regularly
18http://www.itilifesciences.com
updated disctionaries. In Proceedings of the Second
BioCreative Challenge Evaluation Workshop.
C. Grover, B. Haddow, E. Klein, M. Matthews, L. A.
Nielsen, R. Tobin, and X. Wang. 2007. Adapting a re-
lation extraction pipeline for the BioCreAtIvE II task.
In Proceedings of the BioCreAtIvE II Workshop 2007,
Madrid.
D. Hanisch, K. Fundel, H-T Mevissen, R Zimmer, and
J Fluck. 2005. ProMiner: Organism-specific pro-
tein name detection using approximate string match-
ing. BMC Bioinformatics, 6(Suppl 1):S14.
L. Hirschman, M. Colosimo, A. Morgan, J. Columbe, and
A. Yeh. 2004. Task 1B: Gene list task BioCreAtIve
workshop. In BioCreative: Critical Assessment for In-
formation Extraction in Biology.
L. Hirschman, A. Yeh, C. Blaschke, and A. Valencia.
2005. Overview of BioCreAtIvE: critical assessment
of information extraction for biology. BMC Bioinfor-
matics, 6(Suppl1):S1.
L. Hirschman, M. Krallinger, and A. Valencia, edi-
tors. 2007. Second BioCreative Challenge Evaluation
Workshop. Fundacio?n CNIO Carlos III, Madrid.
M. Krallinger, F. Leitner, and A. Valencia. 2007. Assess-
ment of the second BioCreative PPI task: Automatic
extraction of protein-protein interactions. In Proceed-
ings of the BioCreAtIvE II Workshop 2007, pages 41?
54, Madrid, Spain.
M. Krauthammer and G. Nenadic. 2004. Term iden-
tification in the biomedical literature. Journal of
Biomedical Informatics (Special Issue on Named En-
tity Recogntion in Biomedicine), 37(6):512?526.
A. A. Morgan and L. Hirschman. 2007. Overview of
BioCreative II gene normalisation. In Proceedings of
the BioCreAtIvE II Workshop, Madrid.
X. Wang and C. Grover. 2008. Learning the species of
biomedical named entities from annotated corpora. In
Proceedings LREC2008, Marrakech, Morocco.
X. Wang and M. Matthews. 2008. Comparing usabil-
ity of matching techniques for normalising biomedical
named entities. In Proceedings of PSB.
X. Wang. 2007. Rule-based protein term identification
with help from automatic species tagging. In Proceed-
ings of CICLING 2007, pages 288?298, Mexico City.
79
Automatic Acquisition of English Topic Signatures Based on
a Second Language
Xinglong Wang
Department of Informatics
University of Sussex
Brighton, BN1 9QH, UK
xw20@sussex.ac.uk
Abstract
We present a novel approach for auto-
matically acquiring English topic sig-
natures. Given a particular concept,
or word sense, a topic signature is a
set of words that tend to co-occur with
it. Topic signatures can be useful in a
number of Natural Language Process-
ing (NLP) applications, such as Word
Sense Disambiguation (WSD) and Text
Summarisation. Our method takes ad-
vantage of the different way in which
word senses are lexicalised in English
and Chinese, and also exploits the large
amount of Chinese text available in cor-
pora and on the Web. We evaluated the
topic signatures on a WSD task, where
we trained a second-order vector co-
occurrence algorithm on standard WSD
datasets, with promising results.
1 Introduction
Lexical knowledge is crucial for many NLP tasks.
Huge efforts and investments have been made to
build repositories with different types of knowl-
edge. Many of them have proved useful, such as
WordNet (Miller et al, 1990). However, in some
areas, such as WSD, manually created knowledge
bases seem never to satisfy the huge requirement
by supervised machine learning systems. This
is the so-called knowledge acquisition bottleneck.
As an alternative, automatic or semi-automatic ac-
quisition methods have been proposed to tackle
the bottleneck. For example, Agirre et al (2001)
tried to automatically extract topic signatures by
querying a search engine using monosemous syn-
onyms or other knowledge associated with a con-
cept defined in WordNet.
The Web provides further ways of overcoming
the bottleneck. Mihalcea et al (1999) presented
a method enabling automatic acquisition of sense-
tagged corpora, based on WordNet and an Inter-
net search engine. Chklovski and Mihalcea (2002)
presented another interesting proposal which turns
to Web users to produce sense-tagged corpora.
Another type of method, which exploits dif-
ferences between languages, has shown great
promise. For example, some work has been done
based on the assumption that mappings of words
and meanings are different in different languages.
Gale et al (1992) proposed a method which au-
tomatically produces sense-tagged data using par-
allel bilingual corpora. Diab and Resnik (2002)
presented an unsupervised method for WSD us-
ing the same type of resource. One problem with
relying on bilingual corpora for data collection is
that bilingual corpora are rare, and aligned bilin-
gual corpora are even rarer. Mining the Web for
bilingual text (Resnik, 1999) is not likely to pro-
vide sufficient quantities of high quality data. An-
other problem is that if two languages are closely
related, data for some words cannot be collected
because different senses of polysemous words in
one language often translate to the same word in
the other.
In this paper, we present a novel approach for
automatically acquiring topic signatures (see Ta-
ble 1 for an example of topic signatures), which
also adopts the cross-lingual paradigm. To solve
the problem of different senses not being distin-
guishable mentioned in the previous paragraph,
we chose a language very distant to English ?
Chinese, since the more distant two languages
are, the more likely that senses are lexicalised
differently (Resnik and Yarowsky, 1999). Be-
cause our approach only uses Chinese monolin-
gual text, we also avoid the problem of shortage
of aligned bilingual corpora. We build the topic
signatures by using Chinese-English and English-
Chinese bilingual lexicons and a large amount of
Chinese text, which can be collected either from
the Web or from Chinese corpora. Since topic sig-
natures are potentially good training data for WSD
algorithms, we set up a task to disambiguate 6
words using a WSD algorithm similar to Schu?tze?s
(1998) context-group discrimination. The results
show that our topic signatures are useful for WSD.
The remainder of the paper is organised as fol-
lows. Section 2 describes the process of acqui-
sition of the topic signatures. Section 3 demon-
strates the application of this resource on WSD,
and presents the results of our experiments. Sec-
tion 4 discusses factors that could affect the acqui-
sition process and then we conclude in Section 5.
2 Acquisition of Topic Signatures
A topic signature is defined as: TS =
{(t1, w1), ..., (ti, wi), ...}, where ti is a term
highly correlated to a target topic (or concept) with
association weight wi, which can be omitted. The
steps we perform to produce the topic signatures
are described below, and illustrated in Figure 1.
1. Translate an English ambiguous word w to Chinese,
using an English-Chinese lexicon. Given the assump-
tion we mentioned, each sense si of w maps to a dis-
tinct Chinese word1. At the end of this step, we have
produced a set C, which consists of Chinese words
{c1, c2, ..., cn}, where ci is the translation correspond-
ing to sense si of w, and n is the number of senses that
w has.
2. Query large Chinese corpora or/and a search engine
that supports Chinese using each element in C. Then,
for each ci in C, we collect the text snippets retrieved
and construct a Chinese corpus.
1It is also possible that the English sense maps to a set of
Chinese synonyms that realise the same concept.
English ambiguous word w
Sense 1 of w Sense 2 of w
Chinese translation of
sense 2
Chinese translation of
sense 1
English-Chinese
Lexicon
1. Chinese document 1
2. Chinese document 2
... ...
Chinese
Search
Engine
Chinese
segmentation
and POS
tagging;
Chinese-
English
Lexicon
1. Chinese document 1
2. Chinese document 2
... ...
1. {English topic signature 1}
2. {English topic signature 2}
... ...
1. {English topic signature 1}
2. {English topic signature 2}
... ...
Figure 1:Process of automatic acquisition of topic signatures.
For simplicity, we assume here that w has two senses.
3. Shallow process these Chinese corpora. Text segmen-
tation and POS tagging are done in this step.
4. Either use an electronic Chinese-English lexicon to
translate the Chinese corpora word by word to En-
glish, or use machine translation software to translate
the whole text. In our experiments, we did the former.
The complete process is automatic, and unsu-
pervised. At the end of this process, for each sense
si of an ambiguous word w, we have a large set
of English contexts. Each context is a topic sig-
nature, which represents topical information that
tends to co-occur with sense si. Note that an el-
ement in our topic signatures is not necessarily a
single English word. It can be a set of English
words which are translations of a Chinese word c.
For example, the component of a topic signature,
{vesture, clothing, clothes}, is translated from the
Chinese word??. Under the assumption that the
majority of c?s are unambiguous, which we dis-
cuss later, we refer to elements in a topic signature
as concepts in this paper.
Choosing an appropriate English-Chinese dic-
tionary is the first problem we faced. The one
we decided to use is the Yahoo! Student English-
Chinese On-line Dictionary2. As this dictionary
is designed for English learners, its sense gran-
ularity is far coarser-grained than that of Word-
Net. However, researchers argue that the granular-
ity of WordNet is too fine for many applications,
and some also proposed new evaluation standards.
For example, Resnik and Yarowsky (1999) sug-
2See: http://cn.yahoo.com/dictionary/
gested that for the purpose of WSD, the different
senses of a word could be determined by consid-
ering only sense distinctions that are lexicalised
cross-linguistically. Our approach is in accord
with their proposal, since bilingual dictionaries in-
terpret sense distinctions crossing two languages.
For efficiency purposes, we extract our topic
signatures mainly from the Mandarin portion of
the Chinese Gigaword Corpus (CGC), produced
by the LDC3, which contains 1.3GB of newswire
text drawn from Xinhua newspaper. Some Chi-
nese translations of English word senses could be
sparse, making it impossible to extract sufficient
training data simply relying on CGC. In this sit-
uation, we can turn to the large amount of Chi-
nese text on the Web. There are many good search
engines and on-line databases supporting the Chi-
nese language. After investigation, we chose Peo-
ple?s Daily On-line4, which is the website for Peo-
ple?s Daily, one of the most influential newspaper
in mainland China. It maintains a vast database
of news stories, available to search by the public.
Among other reasons, we chose this website be-
cause its articles have similar quality and cover-
age to those in the CGC, so that we could com-
bine texts from these two resources to get a larger
amount of topic signatures. Note that we can al-
ways turn to other sources on the Web to retrieve
even more data, if needed.
For Chinese text segmentation and POS tag-
ging5 we adopted the freely-available software
package ? ICTCLAS6. This system includes a
word segmenter, a POS tagger and an unknown-
word recogniser. The claimed precision of seg-
mentation is 97.58%, evaluated on a 1.2M word
portion of the People?s Daily Corpus.
To automatically translate the Chinese text back
to English, we used the electronic LDC Chinese-
English Translation Lexicon Version 3.0. An al-
ternative was to use machine translation software,
which would yield a rather different type of re-
source, but this is beyond the scope of this pa-
per. Then, we filtered the topic signatures with
3Available at: http://www.ldc.upenn.edu/Catalog/
4See: http://www.people.com.cn
5POS tagging can be omitted. We did it in our experi-
ments purely for convenience for error analysis in the future.
6See: http://mtgroup.ict.ac.cn/?zhp/ICTCLAS/index.html
a stop-word list, to ensure only content words are
included in our final results.
One might argue that, since many Chinese
words are also ambiguous, a Chinese word may
have more than one English translation and thus
translated concepts in topic signatures would still
be ambiguous. This happens for some Chinese
words, and will inevitably affect the performance
of our system to some extent. A practical solu-
tion is to expand the queries with different descrip-
tions associated with each sense of w, normally
provided in a bilingual dictionary, when retriev-
ing the Chinese text. To get an idea of the baseline
performance, we did not follow this solution in our
experiments.
1. rate; 2. bond; 3. payment; 4. market; 5. debt; 6. dollar;
7. bank; 8. year; 9. loan; 10. income; 11. company;
12. inflation; 13. reserve; 14. government; 15. economy;
16. stock; 17. fund; 18. week; 19. security; 20. level;
A
M
1. {bank}; 2. {loan}; 3. {company, firm, corporation};
4. {rate}; 5. {deposit}; 6. {income, revenue}; 7. {fund};
8. {bonus, divident}; 9. {investment}; 10. {market};
11. {tax, duty}; 12. {economy}; 13. {debt}; 14. {money};
15. {saving}; 16. {profit}; 17. {bond}; 18. {income, earning};
19. {share, stock}; 20. {finance, banking};
Topic signatures for the "financial" sense of "interest"
Table 1:A sample of our topic signatures. Signature M was
extracted from a manually-sense-tagged corpus and A was
produced by our algorithm. Words occurring in both A and
M are marked in bold.
The topic signatures we acquired contain rich
topical information. But they do not provide any
other types of linguistic knowledge. Since they
were created by word to word translation, syntac-
tic analysis of them is not possible. Even the dis-
tances between the target ambiguous word and its
context words are not reliable because of differ-
ences in word order between Chinese and English.
Table 1 lists two sets of topic signatures, each con-
taining the 20 most frequent nouns, ranked by oc-
currence count, that surround instances of the fi-
nancial sense of interest. One set was extracted
from a hand-tagged corpus (Bruce and Wiebe,
1994) and the other by our algorithm.
3 Application on WSD
To evaluate the usefulness of the topic signatures
acquired, we applied them in a WSD task. We
adopted an algorithm similar to Schu?tze?s (1998)
context-group discrimination, which determines a
word sense according to the semantic similarity
of contexts, computed using a second-order co-
occurrence vector model. In this section, we firstly
introduce our adaptation of this algorithm, and
then describe the disambiguation experiments on
6 words for which a gold standard is available.
3.1 Context-Group Discrimination
We chose the so-called context-group discrimina-
tion algorithm because it disambiguates instances
only relying on topical information, which hap-
pens to be what our topic signatures specialise
in7. The original context-group discrimination
is a disambiguation algorithm based on cluster-
ing. Words, contexts and senses are represented
in Word Space, a high-dimensional, real-valued
space in which closeness corresponds to semantic
similarity. Similarity in Word Space is based on
second-order co-occurrence: two tokens (or con-
texts) of the ambiguous word are assigned to the
same sense cluster if the words they co-occur with
themselves occur with similar words in a training
corpus. The number of sense clusters determines
sense granularity.
In our adaptation of this algorithm, we omitted
the clustering step, because our data has already
been sense classified according to the senses de-
fined in the English-Chinese dictionary. In other
words, our algorithm performs sense classifica-
tion by using a bilingual lexicon and the level
of sense granularity of the lexicon determines the
sense distinctions that our system can handle: a
finer-grained lexicon would enable our system to
identify finer-grained senses. Also, our adapta-
tion represents senses in Concept Space, in con-
trast to Word Space in the original algorithm. This
is because our topic signatures are not realised in
the form of words, but concepts. For example, a
topic signature may consist of {duty, tariff, cus-
toms duty}, which represents a concept of ?a gov-
ernment tax on imports or exports?.
A vector for concept c is derived from all the
close neighbours of c, where close neighbours re-
fer to all concepts that co-occur with c in a context
window. The size of the window is around 100
7Using our topic signatures as training data, other classi-
fication algorithms would also work on this WSD task.
words. The entry for concept c? in the vector for
c records the number of times that c? occurs close
to c in the corpus. It is this representational vector
space that we refer to as Concept Space.
In our experiments, we chose concepts that
serve as dimensions of Concept Space using a
frequency cut-off. We count the number of oc-
currences of any concepts that co-occur with the
ambiguous word within a context window. The
2, 500 most frequent concepts are chosen as the
dimensions of the space. Thus, the Concept Space
was formed by collecting a n-by-2, 500 matrix M ,
such that element mij records the number of times
that concept i and j co-occur in a window, where
n is the number of concept vectors that occur in
the corpus. Row l of matrix M represents concept
vector l.
We measure the similarity of two vectors by the
cosine score:
corr(~v, ~w) =
?N
i=1 ~vi ~wi??N
i=1 ~vi2
?N
i=1 ~wi2
where ~v and ~w are vectors and N is the dimen-
sion of the vector space. The more overlap there
is between the neighbours of the two words whose
vectors are compared, the higher the score.
Contexts are represented as context vectors in
Concept Space. A context vector is the sum of the
vectors of concepts that occur in a context win-
dow. If many of the concepts in a window have a
strong component for one of the topics, then the
sum of the vectors, the context vector, will also
have a strong component for the topic. Hence, the
context vector indicates the strength of different
topical or semantic components in a context.
Senses are represented as sense vectors in Con-
cept Space. A vector of sense si is the sum of the
vectors of contexts in which the ambiguous word
realises si. Since our topic signatures are classi-
fied naturally according to definitions in a bilin-
gual dictionary, calculation of the vector for sense
si is fairly straightforward: simply sum all the vec-
tors of the contexts associated with sense si.
After the training phase, we have obtained a
sense vector ~vi for each sense si of an ambiguous
word w. Then, we perform the following steps to
tag an occurrence t of w:
1. Compute the context vector ~c for t in Concept Space
by summing the vectors of the concepts in t?s context.
Since the basic units of the test data are words rather
than concepts, we have to convert all words in the test
data into concepts. A simple way to achieve this is to
replace a word v with all the concepts that contain v.
2. Compute the cosine scores between all sense vectors of
w and ~c, and then assign t to the sense si whose sense
vector ~sj is closest to ~c.
3.2 Experiments and Results
We tested our system on 6 nouns, as shown in Ta-
ble 2, which also shows information on the train-
ing and test data we used in the experiments. The
training sets for motion, plant and tank are topic
signatures extracted from the CGC; whereas those
for bass, crane and palm are obtained from both
CGC and the People?s Daily On-line. This is be-
cause the Chinese translation equivalents of senses
of the latter 3 words don?t occur frequently in
CGC, and we had to seek more data from the Web.
Where applicable, we also limited the training data
of each sense to a maximum of 6, 000 instances for
efficiency purposes.
76.6%
Precision
93.5%bass 1203 90.7%
'Supervised'
BaselineTestTrainingSenseWord
2. music
1. fish
825
418
97
10
crane 2301 74.7%2. machine
1. bird
1472
829
71
24
107
95
69.7%motion 9265 70.1%2. legal
1. physical
3265
6000
60
141 201
76.1%palm 1248 71.1%2. tree
1. hand
396
852
58
143 201
70.2%plant 12000 54.3%2. factory
1. living
6000
6000
102
86 188
70.1%tank 9346 62.7%2. vehicle
1. container
3346
6000
75
126 201
Table 2:Sizes of the training data and the test data, baseline
performance, and the results.
The test data is a binary sense-tagged corpus,
the TWA Sense Tagged Data Set, manually pro-
duced by Rada Mihalcea and Li Yang (Mihalcea,
2003), from text drawn from the British National
Corpus. We calculated a ?supervised? baseline
from the annotated data by assigning the most fre-
quent sense in the test data to all instances, al-
though it could be argued that the baseline for un-
supervised disambiguation should be computed by
randomly assigning one of the senses to instances
(e.g. it would be 50% for words with two senses).
According to our previous description, the
2, 500 most frequent concepts were selected as di-
mensions. The number of features in a Concept
Space depends on how many unique concepts ac-
tually occur in the training sets. Larger amounts
of training data tend to yield a larger set of fea-
tures. At the end of the training stage, for each
sense, a sense vector was produced. Then we lem-
matised the test data and extracted a set of context
vectors for all instances in the same way. For each
instance in the test data, the cosine scores between
its context vector and all possible sense vectors ac-
quired through training were calculated and com-
pared, and then the sense scoring the highest was
allocated to the instance.
The results of the experiments are also given
in Table 2 (last column). Using our topic sig-
natures, we obtained good results: the accuracy
for all words exceeds the supervised baseline, ex-
cept for motion which approaches it. The Chi-
nese translations for motion are also ambiguous,
which might be the reason that our WSD system
performed less well on this word. However, as
we mentioned, to avoid this problem, we could
have expanded motion?s Chinese translations, us-
ing their Chinese monosemous synonyms, when
we query the Chinese corpus or the Web. Consid-
ering our system is unsupervised, the results are
very promising. An indicative comparison might
be with the work of Mihalcea (2003), who with
a very different approach achieved similar perfor-
mance on the same test data.
4 Discussion
Although these results are promising, higher qual-
ity topic signatures would probably yield better re-
sults in our WSD experiments. There are a num-
ber of factors that could affect the acquisition pro-
cess, which determines the quality of this resource.
Firstly, since the translation was achieved by look-
ing up in a bilingual dictionary, the deficiencies
of the dictionary could cause problems. For ex-
ample, the LDC Chinese-English Lexicon we used
is not up to date, for example, lacking entries for
words such as?? (mobile phone),p? (the
Internet), etc. This defect makes our WSD algo-
rithm unable to use the possibly strong topical in-
formation contained in those words. Secondly, er-
rors generated during Chinese segmentation could
affect the distributions of words. For example, a
Chinese string ABC may be segmented as either
A+BC or AB + C; assuming the former is cor-
rect whereas AB + C was produced by the seg-
menter, distributions of words A, AB, BC, and C
are all affected accordingly. Other factors such as
cultural differences reflected in the different lan-
guages could also affect the results of this knowl-
edge acquisition process.
In our experiments, we adopted Chinese as a
source language to retrieve English topic signa-
tures. Nevertheless, our technique should also
work on other distant language pairs, as long
as there are existing bilingual lexicons and large
monolingual corpora for the languages used. For
example, one should be able to build French topic
signatures using Chinese text, or Spanish topic
signatures from Japanese text. In particular cases,
where one only cares about translation ambiguity,
this technique can work on any language pair.
5 Conclusion and Future Work
We presented a novel method for acquiring En-
glish topic signatures from large quantities of
Chinese text and English-Chinese and Chinese-
English bilingual dictionaries. The topic signa-
tures we acquired are a new type of resource,
which can be useful in a number of NLP applica-
tions. Experimental results have shown its appli-
cation to WSD is promising and the performance
is competitive with other unsupervised algorithms.
We intend to carry out more extensive evaluation
to further explore this new resource?s properties
and potential.
Acknowledgements
This research is funded by EU IST-2001-
34460 project MEANING: Developing Multilin-
gual Web-Scale Language Technologies, and by
the Department of Informatics at Sussex Univer-
sity. I am very grateful to Dr John Carroll, my
supervisor, for his continual help and encourage-
ment.
References
Eneko Agirre, Olatz Ansa, David Martinez, and Ed-
uard Hovy. 2001. Enriching WordNet concepts
with topic signatures. In Proceedings of the NAACL
workshop on WordNet and Other Lexical Resources:
Applications, Extensions and Customizations. Pitts-
burgh, USA.
Rebecca Bruce and Janyce Wiebe. 1994. Word-sense
disambiguation using decomposable models. In
Proceedings of the 32nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 139?
146.
Timothy Chklovski and Rada Mihalcea. 2002. Build-
ing a sense tagged corpus with open mind word ex-
pert. In Proceedings of the ACL 2002 Workshop on
?Word Sense Disambiguation Recent Successes and
Future Directions?. Philadelphia, USA.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Anniversary Meet-
ing of the Association for Computational Linguistics
(ACL-02). Philadelphia, USA.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Using bilingual materials to
develop word sense disambiguation methods. In
Proceedings of the International Conference on
Theoretical and Methodological Issues in Machine
Translation, pages 101?112.
Rada Mihalcea and Dan I. Moldovan. 1999. An auto-
matic method for generating sense tagged corpora.
In Proceedings of the 16th Conference of the Amer-
ican Association of Artificial Intelligence.
Rada Mihalcea. 2003. The role of non-ambiguous
words in natural language disambiguation. In Pro-
ceedings of the Conference on Recent Advances
in Natural Language Processing, RANLP 2003.
Borovetz, Bulgaria.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller.
1990. Introduction to WordNet: An on-line lexical
database. Journal of Lexicography, 3(4):235?244.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: New evalua-
tion methods for word sense disambiguation. Natu-
ral Language Engineering, 5(2):113?133.
Philip Resnik. 1999. Mining the Web for bilingual
text. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513?1522,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Classifying Relations for Biomedical Named Entity Disambiguation
Xinglong Wang
??
Jun?ichi Tsujii
???
Sophia Ananiadou
??
?
School of Computer Science, University of Manchester, UK
?
National Centre for Text Mining, UK
?
Department of Computer Science, University of Tokyo, Japan
{xinglong.wang,j.tsujii,sophia.ananiadou}@manchester.ac.uk
Abstract
Named entity disambiguation concerns
linking a potentially ambiguous mention
of named entity in text to an unambigu-
ous identifier in a standard database. One
approach to this task is supervised classifi-
cation. However, the availability of train-
ing data is often limited, and the avail-
able data sets tend to be imbalanced and,
in some cases, heterogeneous. We pro-
pose a new method that distinguishes a
named entity by finding the informative
keywords in its surrounding context, and
then trains a model to predict whether each
keyword indicates the semantic class of
the entity. While maintaining a compara-
ble performance to supervised classifica-
tion, this method avoids using expensive
manually annotated data for each new do-
main, and thus achieves better portability.
1 Introduction
While technology on named entity recognition
(NER) matures, many researchers in the field of
information extraction (IE) gradually shifted their
focus to more complex tasks such as named en-
tity disambiguation and relation extraction. Both
tasks are particularly important for biomedical text
mining, which concerns automatically extracting
facts from the exponentially growing biomedical
literature (Hunter and Cohen, 2006). One type of
facts is relations between biomedical named en-
tities, such as disease-drug relation, gene-disease
relation, protein-protein interaction (PPI), etc. To
automatically extract these facts, advanced natu-
ral language processing techniques such as parsing
have been adopted to analyse the syntactic and se-
mantic structure of text. The idea is that linguistic
structures between the interacting biological enti-
ties may have common characteristics that can be
exploited by similarity measures or machine learn-
ing algorithms. For example, Erkan et al (2007)
used the shortest path between two genes accord-
ing to edit distance in a dependency tree to de-
fine a kernel function for extracting gene interac-
tions. Miwa et al (2008) comparably evaluated a
number of kernels for incorporating syntactic fea-
tures, including the bag-of-word kernel, the subset
tree kernel (Moschitti, 2006) and the graph ker-
nel (Airola et al, 2008), and they concluded that
combining all kernels achieved better results than
using any individual one. Miyao et al (2008)
used syntactic paths as one of the features to train
a support vector machines (SVM) model for PPIs
and also discussed how different parsers and out-
put representations affected the end results.
Another crucial IE task is named entity disam-
biguation, which concerns grounding mentions of
named entities in text to unambiguous concepts as
defined in some standard dictionary or database.
For instance, given a search term Python, users
may like to see the results grouped into the fol-
lowing categories: a type of snake, a programming
language, or a film (Bunescu and Pas?ca, 2006).
One approach to such lexical disambiguation tasks
is supervised classification. However, such tech-
niques suffer from the knowledge acquisition bot-
tleneck, meaning that manually annotating train-
ing data is costly and can never satisfy the need by
the machine learning algorithms. In addition, su-
pervised techniques may not yield reliable results
when the distributions of the semantic classes are
different in the training and test datasets (Agirre
and Martinez, 2004; Koeling et al, 2005). For ex-
ample, on the task of word sense disambiguation,
a model trained on a dataset where the predom-
inant sense of the word star is ?heavenly body?,
may not work well on text mainly composed of
entertainment news. Such problems are also ma-
jor concerns when developing a system to disam-
biguate biomedical named entities (e.g., protein,
1513
gene, and disease), for which some researchers
rely on hand-crafted rules in addition to a small
amount of training data (Morgan and Hirschman,
2007; Hakenberg et al, 2008).
This paper proposes a new disambiguation
method that, instead of classifying each individual
occurrence of an entity, it classifies pair-wise re-
lations between the entity mention in question and
the ?cue words? in its adjacent context, where each
cue word is assumed to bear a semantic class. We
then select the cue word that has a positive rela-
tion with the entity, and pass its semantic tag to it.
While an individual entity mention may belong to
a large number of semantic classes, a relation can
only take one of two values: positive or negative,
hence transforming a complex multi-classification
problem into a less complicated binary classifica-
tion task. The remainder of the paper is organised
as follows: Section 2 proposes the disambigua-
tion method and Section 3 introduces the task of
disambiguating the model organisms of biomedi-
cal named entities. Section 4 describes in detail
our proposed method and also a number of base-
line systems for comparison purposes. Section 5
shows the evaluation results and discusses the ad-
vantages and drawback of our system, and we fi-
nally conclude in Section 6.
2 Disambiguation as Relation
Classification
The named entity disambiguation task is defined
as follows: given a mention of a named entity in
text, we automatically assign a semantic tag d to
it, where d ? D, and D is a pre-compiled dic-
tionary with |D| entries. When |D| is small, the
problem can be approached by supervised classi-
fication. For example, to determine whether an
occurrence of an entity is a protein, a gene or an
RNA, Hatzivassiloglou et al (2001) compared
performance of 3 supervised classification meth-
ods and reported results near the human agree-
ment rate. Nevertheless, when |D| is large (e.g.,
> 100), the performance of classification may de-
crease, especially when the distribution of d in
training dataset differs from that in the test set. In
other words, when |D| is large, named entity dis-
ambiguation becomes a multi-class classification
task on heterogeneous and imbalanced datasets,
which is challenging for a machine learning model
to learn to discriminate enough between the se-
mantic classes (Japkowicz, 2000).
We propose an alternative method for named
entity disambiguation. Intuitively, in the surround-
ing context of an ambiguous entity, one can of-
ten find ?cue words? that are informative indica-
tors of the entity?s semantic category. These cue
words are provided by authors to remind readers
the semantic identity of a named entity. For ex-
ample, in an article about protein p53, phrase ?hu-
man protein p53? may be mentioned, where both
human and protein contain semantic information
regarding p53: human indicates the model organ-
ism of p53, and protein suggests the type of this
entity. Such cue words may occur infrequently in
the training data, making it difficult for machine
learning classifiers to capture.
Our method exploits this observation. Given a
sentence, let E be the set of ?target? entities (e.g.,
p53) and W of the ?cue? words (e.g., human) that
co-occur in a sentence, we define a relation as a
pair r = ?e, w?, where e ? E and w ? W , and
r is a positive relation if e belongs to the semantic
class indicated by w, and is a negative one if not.
Then we can disambiguate e by accomplishing the
following steps: 1) identify W and build a set
of relations R = {?e, w
i
?|w
i
?W, i = 1, 2, .., n},
where n is the size of W ; and 2) classify every
r ? R and assign the semantic tag of w
j
to e such
that r
j
= ?e, w
j
? is positive. The first task can be
tackled by a dictionary lookup, or by an NER sys-
tem, if manually annotated data is available. The
second is essentially a binary relation classifica-
tion task, and in this work, we use an SVM model
exploiting bag-of-word and syntactic features.
3 Species Disambiguation
We show the performance of the proposed method
on a task of resolving one major source of am-
biguity in protein and gene entities: model or-
ganisms. Model organisms are species studied to
understand particular biological phenomena. Bi-
ological experiments are often conducted on one
species, with the expectation that the discover-
ies will provide insight into the workings of oth-
ers, including humans, which are more difficult
to study directly. From viruses, prokaryotes, to
plants and animals, there are dozens of organ-
isms commonly used in biological studies, such
as E. coli, Drosophila, Homo sapiens, and hun-
dreds more are frequently mentioned in biologi-
cal research papers. In biomedical articles, entities
of different species are commonly referred to us-
1514
ing the same name, causing great ambiguity. For
example, searching a protein sequence database,
RefSeq
1
with query ?tumor protein p53? resulted
in over 100 proteins, as the name is shared by
many organisms.
The importance of distinguishing model organ-
isms has been recognised by the community of
biomedical text mining. Chen et al (2005) col-
lected gene names from various source databases
and calculated intra- and inter-species ambigui-
ties. Overall, only 25 (0.02%) official symbols
were ambiguous within the organisms. However,
when official symbols from 21 organisms were
combined, the ambiguity increased substantially
to 21, 279 (14.2%) symbols. Hakenberg et al
(2008) showed that species disambiguation is one
of the most important steps for term normalisa-
tion and identification, which concerns automat-
ically associating mentions of biomedical enti-
ties in text to unique database identifiers (Mor-
gan et al, 2008). Also, the task of extracting
PPIs in the recent BioCreative Challenge II work-
shop (Hirschman et al, 2007) requires protein
pairs to be recognised and normalised, which in-
evitably involves species disambiguation.
More specifically, given a text, in which men-
tions of biomedical named entities are annotated,
a species disambiguation system automatically as-
signs a species identifier, as in a standard database
of model organisms, to every entity mention. The
types of biomedical named entities concerned in
this study are protein, gene, protein complex and
mRNA/cDNA, and we used identifiers from the
NCBI Taxonomy of model organisms.
2
The work
focuses on species disambiguation and assumes
that the entities are already identified. In practice,
an automated named entity recogniser (e.g., AB-
NER (Settles, 2005)) should be used before apply-
ing the systems.
4 Approaches
This section describes a number of approaches to
species disambiguation, highlighting the relation
classification method proposed in Section 2.
4.1 Heuristics Baselines
The cue words for species are words denoting
names of model organisms (e.g., mouse as in
1
http://www.ncbi.nlm.nih.gov/RefSeq
2
http://www.ncbi.nlm.nih.gov/sites/
entrez?db=taxonomy
phrase ?mouse p53?). Another clue is the pres-
ence of the species-indicating prefixes in gene and
protein names. For instance, prefix ?h? in en-
tity ?hSos-1? suggests that it is a human protein.
Throughout this paper, we refer to such cue words
(e.g., mouse, hSos-1) as ?species words?. Note
that a species ?word? may contain multiple tokens
(e.g., E. Coli).
We encoded this knowledge in a rule-based
species tagging system (Wang and Grover, 2008).
The system takes a 2-step approach. First, it marks
up species words in the document using a species-
word detection program,
3
which searches every
word in a dictionary of model organisms and as-
signs a species ID to the word if a match is found.
The dictionary was built using the NCBI taxon-
omy
4
and the UniProt controlled vocabulary of
species,
5
and in total it contains 420,224 species
words for 324,157 species IDs. When species
words are identified, we disambiguate an entity
mention using one of the following rules:
1. previous species word: If the word preceding an entity
is a species word, assign the species ID indicated by
that word to the entity.
2. species word in the same sentence: If a species word
and an entity appear in the same sentence, assign its
species ID to the entity. When more than one species
word co-occurs in the sentence, priority is given to the
species word to the entity?s left with the smallest dis-
tance. If all species words occur to the right of the en-
tity, take the nearest one.
3. majority vote: assign the most frequently occurring
species ID in the document to all entity mentions.
It is expected that the first rule would produce
good precision. However, it can only disam-
biguate the fraction of entities that happen to have
a species word to their immediate left. The second
rule relaxes the first by allowing an entity to take
the species indicated by its nearest species word
in the same sentence, which should increase recall
but decrease precision. Statistics from our dataset
(see Section 5.1) show that only 5.68% entities can
potentially be resolved by rule 1 and 22.16% by
rule 2, while the majority rule can tackle every en-
tity mention in the dataset.
3
The species word detector identifies the cue words and
was used in all the systems studied in this paper. We could
not properly evaluate the detector due to the lack of man-
ually annotated data. Its performance, however, would not
affect the comparative evaluation results, and improvement
to species word detection should increase the performance of
these disambiguation systems.
4
ftp://ftp.ncbi.nih.gov/pub/taxonomy/
5
http://www.expasy.ch/cgi-bin/speclist
1515
4.2 Supervised Classification
The disambiguation problem can be approached as
a classification task. Given an entity mention and
its surrounding context, a machine learning model
classifies the entity into one of the classes, where
each class corresponds to a species ID. We car-
ried out experiments with two classification meth-
ods: multi-class classification and one-class clas-
sification, where a maximum entropy model
6
was
used for the former and SVM-light
7
for the lat-
ter. In one-class classification, we trained a se-
ries of binary SVM classifiers, each constructing
a separating hyperplane that maximises the mar-
gin between the instances of one specific species
(i.e., the target class) and a set of randomly se-
lected instances of other species (i.e., the outlier
class). We used equal numbers of instances for
both classes in training. The following types of
features were used in both multi-class and one-
class experiments, where the values of n were
set empirically by cross-validation on the training
data:
? leftContext The n word lemmas to the left of the entity
(n = 200).
? rightContext The n word lemmas to the right of the
entity (n = 200).
? leftSpeciesIDs The n species IDs to the left of the entity
(with order, n = 5).
? rightSpeciesIDs The n species IDs to the right of the
entity (with order, n = 5).
? leftNouns The n nouns to the left of the entity (with
order, n = 2).
? leftAdjs The n adjectives to the left of the entity (with
order, n = 2).
? leftSpeciesWords The n species word forms to the left
of the entity (n = 5).
? rightSpeciesWords The n species word forms to the
right of the entity (n = 5).
? firstLetter The first character of the entity itself (e.g.,
?h? in hP53).
? documentSpeciesIDs All species IDs that occur in the
document in question.
? useStopWords filter out function words.
? useStopPattern filter out words consisting only of digits
and punctuation characters.
Feature selection was also carried out for the
one-class classification experiments. We com-
pared two feature selection methods that report-
edly work well on the task of text classification:
information gain (IG) (Yang and Pedersen, 1997)
6
http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html
7
http://svmlight.joachims.org/
The ARG1 ARG1 ARG2
ARG1 ARG2ARG1
Drosophila Kip3 isorthologue of Klp67A.
Figure 1: Predicate argument structure (PAS).
and Bi-Normal separation (BNS) (Forman, 2003).
IG measures the decrease in entropy when the
feature is given vs. absent, and is defined as:
IG(Y |X) = H(Y ) ? H(Y |X) where H(Y ) is
the uncertainty about the value of Y (i.e., Y ?s en-
tropy), and H(Y |X) is Y ?s conditional entropy
given X . The BNS is defined as: |F
?1
(x) ?
F
?1
(y)|, where F
?1
is the standard Normal distri-
bution?s inverse cumulative probability function,
namely, z-score; x is the ratio between the number
of positive cases containing the feature in ques-
tion, and the total number of positive cases; and y
is the ratio between the number of negative cases
containing the feature, and the total number of
negative cases.
We computed a weight for each feature and then
ranked the features according to their weight, with
respect to each feature selection method. The top
10% features were used in training. Given a test
instance, the one-class classification method first
counts the species words in the document that the
instance appears in, and then applies in sequence
the binary models of each occurring species, start-
ing from the most frequent one. For example, if
a document contains 5 occurrences of human and
3 mouse, we first apply the human species model
to judge whether an entity mention is of human
species, and only if not, the mouse model was ap-
plied. The most-frequent species in the document
was used as backup when none of the binary mod-
els gives positive answers.
4.3 Relation Classification
4.3.1 Overview
As for the proposed relation classification method,
in the training phase, we first selected the sen-
tences in which an entity mention and a species
word co-occur, and constructed pair-wise entity-
species relations. We then assigned each relation a
binary label: a relation is positive if the species ID
inferred from the species word matches the gold-
standard species annotation on the entity, and is
negative otherwise. For example, for the sentence
shown in Figure 1, where Drosophila is a species
word, and Kip3 and Klp67A are proteins, relation
?Kip3, Drosophila? is a negative instance and the
1516
pair ?Klp67A, Drosophila? is a positive one.
8
For each relation, a vector of features were ex-
tracted. We followed the PPI extraction method
described in (Miyao et al, 2008), where two types
of features were used for a SVM classifier. The
first was bag-of-word features, i.e., the words be-
fore, between and after the pair of entities, where
the words were lemmatised. We added an ad-
ditional feature of the distance between the en-
tity and the cue word. The other type was syn-
tactic features obtained from parsers. For bag-
of-word features, a linear kernel was used, and
for syntactic ones, a subset tree kernel (Mos-
chitti, 2006) was adopted. The syntactic features
were represented in a flat tree format. Figure 2
shows such a feature for the negative instance
?Kip3, Drosophila? from Figure 1. Note that all
species words (e.g., Drosophila) were normalised
to ?SPECIESWORD?, and entities (e.g., Kip3) to
?ENTITY?, which not only reduces the noise in
the feature set, but also makes the model more
species-generic. From the training dataset (see
Section 5.1), 25, 413 relations were extracted, of
which 63.3% were positive.
(ENJU(noun arg1(SPECIESWORD orthologue))
(prep arg12(of orthologue))
(prep arg12(of ENTITY)))
Figure 2: A syntactic feature obtained from the ENJU
parser.
To identify the species of an entity in unseen
text, we first parsed the sentence, and then listed
all pairs of species words and entities as relations.
Having extracted the bag-of-word and syntactic
features from the instance, the trained model was
applied to judge whether each species-entity rela-
tion was positive. The entity mention in a positive
relation would be tagged with the ID indicated by
the species word, while the mentions in negative
relations would be left untagged. The next section
describes in detail how we extracted the syntactic
features from text.
4.3.2 Syntactic Features
Given a sentence, a natural language parser au-
tomatically recognises its syntactic structure and
outputs a parse tree, in which nodes represent
words or syntactic constituents. A path between
8
Orthologues are genes/proteins in different species but
have similar sequences. In this example it implies that
Klp67A is a Drosophila protein but Kip3 is not.
Parser Input Output
C&C POS-tagged GR
ENJU POS-tagged PAS
ENJU-Genia POS-tagged PAS
Minipar Sentence-detected Minipar
RASP Tokenised GR
Stanford POS-tagged SD
Stanford-Genia POS-tagged SD
Table 1: Parsers and their input and output format
a pair of nodes can be interpreted as a syntactic re-
lation between sentence units, which was proved
useful to infer biological relations (e.g., Airola et
al., 2008; Miwa et al, 2008).
We experimented with the following parsers
(summarised in Table 1):
? Dependency parsers identify one word as the head
of a sentence and all other words are either a depen-
dent of that word, or else dependent on some other
word that connects to the headword through a sequence
of dependencies. We used Minipar (Lin, 1998) and
RASP (Briscoe et al, 2006) for the experiments;
? Constituent-structured parsers split a sentence into
syntactic constituents such as noun phrases or verb
phrases. We used the Stanford parser (Klein and Man-
ning, 2003), and also a variant of the Stanford parser
(i.e., Stanford-Genia), which was trained on the GE-
NIA treebank (Tateisi et al, 2005) for biomedical text;
? Deep parsers aim to compute in-depth syntactic and
semantic structures based on syntactic theories such as
HPSG (Pollard and Sag, 1994) and CCG (Steedman,
2000). We used the C&C parser (Clark and Curran,
2007), ENJU (Miyao and Tsujii, 2008), and a variant
of ENJU (Hara et al, 2007) adapted for the biomedical
domain (i.e., ENJU-Genia);
There were a number of practical issues to con-
sider when using parsers for this task. Firstly, be-
fore parsing, the text needs to be linguistically pre-
processed, and the quality of this process has a sig-
nificant impact on parsers? performance. The pre-
processing steps include sentence boundary detec-
tion, tokenisation and part-of-speech (POS) tag-
ging, all of which can be tricky especially when
applied to biomedical text (Grover et al, 2003).
To avoid the noise that can be introduced in the
pre-processing steps and to concentrate on evalu-
ating the performance of the parsers, we used the
same pre-processing tools (Alex et al, 2008a)
9
whenever possible. The middle column in Ta-
ble 1 shows how the input text was linguisti-
cally pre-processed with respect to each parser.
A POS-tagged text implies that it was also sen-
tence boundary detected and tokenised Except for
9
These particular tools were chosen because they were
adopted to pre-process the ITI-TXM dataset, which we used
in our study.
1517
RASP and Minipar, all parsers took POS-tagged
text as input. RASP requires POS tags and punctu-
ation labels that were derived from the CLAWS-7
tagset,
10
whereas our dataset uses POS labels from
the Penn Treebank tagset (Marcus et al, 1994).
As RASP does not recognise the Penn tagset, we
used its build-in POS tagger. Minipar, on the other
hand, does not support input of tokenised or POS-
tagged text, and therefore took split sentences as
input.
Secondly, the output representations of the
parsers are different and we preferred a format
that depicts relations between words instead of
syntactic constituents. In total, 4 representations
were used: grammatical relation (GR) (Briscoe et
al., 2006), Stanford typed dependency (SD) (de
Marneffe et al, 2006), Minipar?s own representa-
tion (Lin, 1998), and ENJU?s predicate-argument
structure (PAS). All the above representations de-
fine relations of words in triples, where a depen-
dency triple (i.e., GR, SD and Minipar) consists
of head, dependent and relation, and a PAS triple
contains predicate, argument, and relation. Fig-
ure 1 shows a sentence parsed by ENJU in PAS
representation. The right-most column in Table 1
lists the output representation of each parser. A
syntactic path between an entity and a species
word was represented by a sequence of triples,
each following the order of head-dependent or
predicate-argument. These paths were used as
syntactic features for the SVM classifier.
4.4 Spreading Strategies
Except for the majority vote rule, the approaches
described in Sections 4.1 and 4.3 were expected
to yield low recall, because they can only detect
intra-sentential relations, and therefore only be ap-
plied to the entities having at least one species
word appearing in the same sentence.
Since our aim is to disambiguate as many entity
mentions as possible, we would like to ?spread?
the decisions from the disambiguated mentions to
their ?relatives? in the same document. We define
an entity mention e? as another mention e?s rela-
tive under either of the following conditions: a)
if e? has the same surface form with e; or, b) if
e? is an abbreviation or an antecedent of e, where
abbreviation/antecedent pairs were detected using
the algorithm described in (Schwartz and Hearst,
10
http://ucrel.lancs.ac.uk/claws7tags.
html
2003). Given the set of disambiguated mentions,
we then ?spread? their species IDs to their rela-
tives in the same document. After this process, the
mentions that do not have any disambiguated rela-
tives would still be missed by the system. In such
cases, we used a ?default? species, as determined
by the rule of majority vote (see Section 4.1).
5 Evaluation
5.1 Data and Ontology
The species disambiguation experiments were
conducted using the ITI-TXM corpus (Alex et al,
2008b), a collection of full-length biomedical re-
search articles manually annotated with linguistic
and biomedical information for developing auto-
matic information extraction systems. The cor-
pus contains two datasets covering slightly dif-
ferent domains: enriched protein-protein interac-
tion (EPPI) and tissue expression (TE). When-
ever possible, protein, protein complex, gene, and
mRNA/cDNA entities were tagged with NCBI
Taxonomy IDs, denoting their species, and it was
the species annotation that this study used.
The EPPI and TE datasets have different distri-
butions of species. The entities in EPPI belong to
118 species with human being the most frequent at
51.98%. In TE, the entities are across 67 species
and mouse is the most frequent at 44.67%.
11
The
inter-annotator agreement of species annotation on
EPPI and TE are 86.45% and 95.11%, respectively.
The species disambiguation systems were de-
veloped on the training portions of the EPPI and
TE corpora, each containing 221 articles, and eval-
uated on a dataset combining the development
test (DEVTEST) datasets of EPPI and TE, contain-
ing 58 and 48 articles, respectively. The com-
bined training dataset contains 96, 992 entity men-
tions belonging to 138 model organisms, while the
DEVTEST dataset contains 23, 118 entities of 54
species. The diversity of model organisms in this
corpus highlights the fact that a primary consid-
eration when developing a species disambiguation
system is its ability to distinguish a wide range of
species with minimal additional manual effort.
5.2 Results
5.2.1 Evaluation Metrics
The evaluation was carried out on the DEVTEST
dataset, and the systems are compared using av-
11
These figures were obtained from the training split of the
datasets.
1518
micro-avg. macro-avg.
Maxent 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85
SVM 62.24 / 59.35 / 60.76 14.70 / 17.11 / 15.01
SVM (IG) 65.20 / 61.06 / 63.06 14.90 / 19.53 / 16.09
SVM (BNS) 43.61 / 42.63 / 43.11 11.99 / 10.05 / 9.34
Table 2: Evaluation results of the classification systems on
DEVTEST (precision/recall/F1-score, in %)
eraged precision, recall and F1 scores over all
species. In more detail, for each model organism
that appears in the DEVTEST dataset, we collect
two lists of entity mentions of that species: one
from the gold-standard DEVTEST dataset, and the
other from the output of a disambiguation system.
Then the list of system output is compared against
the gold-standard list to obtain precision, recall
and F1 score. For each system, the scores ob-
tained from all species are averaged using micro-
average and macro-average. The micro-average is
the mean of the summation of contingency metrics
for all model organisms, so that scores of the more
frequent species influence the mean more than
those of less frequent ones. The macro-average is
the mean of precision, recall, or F1 over all labels,
thus attributing equal weights to each species, and
measuring a system?s adaptability across different
model organisms.
5.2.2 Evaluation Results
First of all, Table 2 shows the results of the clas-
sification methods described in Section 4.2. The
multi-classification system using a maximum en-
tropy model (Maxent) yielded the highest overall
micro-averaged F1. Among the SVM-based sys-
tems, the one using IG feature selection achieved
better performance. In particular, it outperformed
the Maxent model in term of macro-averages. The
performance of the SVM model with BNS feature
selection is disappointing, perhaps because the oc-
currences of a feature in each instance are not nor-
mally distributed. As the Maxent system obtained
better results, it was used to compare with other
disambiguation systems.
Table 3 shows the results of a number of meth-
ods described in the previous sections. The meth-
ods are categorised into 4 groups: rule-based
baseline systems, a Maxent classification model,
relation-classification methods, and a hybrid sys-
tem. The difference between the relation classifi-
cation systems is the features adopted. Rel-Context
was trained on only bag-of-word and distance fea-
tures, whereas each other system also used syn-
tactic features provided by a specific parser. For
example, the Rel-RASP system identifies an entity?s
species by finding positive relations between the
entity and its neighbouring species words, using
features including bag-of-word, distance, and de-
pendency paths generated by RASP. The hybrid
system (Hbrd) ran the Rel-ENJU-Genia system on top
of the outcome of Maxent. When a conflict oc-
curs, the species ID is chosen by Rel-ENJU-Genia.
The idea is that the relation classification system
is more accurate than Maxent when it is applica-
ble, and hence would improve precision on dis-
ambiguating the species with few or no training
instances.
Without spreading (shown in the ?NO SPRD?
columns of Table 3), most of the rule-based and re-
lation classification systems only work on a subset
of DEVTEST, resulting in low recall: Rule-Sp works
on the small proportion of entities (5.68%) with a
preceding species word, while the other systems
only work on the collection of sentences contain-
ing at least one species word and one entity, which
covers 4.60% sentences and 22.16% entity men-
tions. Rule-Majority, Maxent, and Hbrd, on the other
hand, apply to all entity mentions, and therefore
they are only compared against the others when
spreading was applied.
The results shown in the ?NO SPRD? columns
can be viewed as a comparative evaluation of
the usefulness of the syntactic features supplied
by the parsers on this particular task. The rule-
based systems set high baselines: Rule-Sp pro-
duced good precision and Rule-SpSent achieved the
highest micro-averaged F1, thanks to its high
coverage, which is also an upperbound of recall
for the relation classification systems. Neverthe-
less, it is encouraging that the relation classifica-
tion systems obtained higher precision than Rule-
SpSent, which is important, considering the de-
cisions will be transfered to the untagged entity
mentions across the document. Indeed, as shown
in the SPRD columns in Table 3, most relation
classification systems outperformed the Rule-SpSent
baseline when spreading was used. The scores
of the systems using different parser outputs only
vary slightly. Rel-Context, on the other hand, sur-
passed others in terms of micro-averaged preci-
sion, while sacrificing micro-averaged recall and
macro-averaged scores.
Next, the SPRD columns in Table 3 show the re-
sults when the spreading rules were applied, which
1519
METHOD NO SPRD (micro-avg) NO SPRD (macro-avg) SPRD (micro-avg) SPRD (macro-avg)
Rule-Majority N/A N/A 66.14 / 61.99 / 64.00 16.76 / 21.75 / 18.08
Rule-Sp 88.96 / 5.02 / 9.51 33.77 / 8.55 / 10.18 66.96 / 63.41 / 65.13 28.25 / 30.65 / 27.00
Rule-SpSent 80.82 / 16.88 / 27.93 43.16 / 28.85 / 24.73 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10
Maxent N/A N/A 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85
Rel-Context 90.04 / 3.71 / 6.13 15.23 / 4.45 / 4.90 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10
Rel-C&C 82.79 / 16.14 / 27.02 43.97 / 29.56 / 25.60 66.59 / 63.64 / 65.08 32.29 / 33.20 / 29.14
Rel-ENJU 83.39 / 15.87 / 26.66 46.89 / 29.88 / 25.95 68.28 / 65.02 / 66.61 31.82 / 34.08 / 29.67
Rel-ENJU-Genia 83.54 / 15.74 / 26.49 44.13 / 29.93 / 25.78 68.91 / 65.45 / 67.13 32.00 / 34.87 / 30.21
Rel-Minipar 81.82 / 16.27 / 27.14 43.63 / 27.88 / 24.15 67.98 / 63.77 / 65.81 31.83 / 33.93 / 29.44
Rel-RASP 81.67 / 16.10 / 26.90 43.95 / 28.92 / 25.03 66.62 / 64.08 / 65.33 32.66 / 33.54 / 29.80
Rel-Stanford 82.75 / 16.10 / 26.95 44.05 / 29.49 / 25.92 66.81 / 63.81 / 65.28 32.67 / 33.03 / 29.45
Rel-Stanford-Genia 82.22 / 16.04 / 26.84 43.37 / 29.40 / 25.22 66.85 / 63.64 / 65.21 32.72 / 32.29 / 28.64
Hbrd N/A N/A 74.15 / 73.26 / 73.70 43.98 / 37.47 / 31.80
Table 3: Evaluation results of the species disambiguation systems on DEVTEST (precision/recall/F1-score, in %)
effectively improved recall (see Section 5.2.3
for discussion on statistical significance tests on
the results). The Maxent system achieved very
good micro-averaged precision, but low macro-
averaged scores. In fact, as shown in Table 4, Max-
ent can only disambiguate 7 species (out of a total
of 54) that have relatively large amount of train-
ing instances,
12
and failed completely on other
species. This suggests that Maxent may not be able
to generate good micro-averaged scores when ap-
plied to a dataset where the dominant species are
different from those in the training set. On the
other hand, the relation-classification approaches
have a clear advantage over Maxent as measured
by macro-averaged scores. As shown in Table 4,
Rel-ENJU-Genia worked well on most of the species,
displaying its good adaptability, while achieving
comparable micro-averaged F1 to Maxent. Over-
all, Hbrd, which combines the strengths of relation
classification and the Maxent classification model,
obtained the highest points as measured by every
metric.
5.2.3 Statistical Significance
To see whether our methods significantly im-
proved the baseline systems, we performed ran-
domisation tests (Noreen, 1989; Yeh, 2000) on
some of the results shown in Table 3. The in-
tuition of randomisation test is as follows: when
comparing two systems (e.g., A and B), we erase
the labels ?output of A? or ?output of B? from all
observations. The null hypothesis is that there is
no difference between A and B, and thus any re-
sponse produced by one of the systems could have
as likely come from the other. We shuffle these re-
12
The following 7 species occur most frequently in the
training set: H. sapiens (43.25%), M. musculus (27.05%),
R. norvegicus (5.35%), S. cerevisiae (3.98%), X. tropicalis
(3.56%), D. melanogaster (3.33%) and C. elegans (0.94%).
Species Name Pct Mxt Rel Hbrd
H. sapiens 50.13% 76.25 65.33 79.51
M. musculus 13.99% 66.41 58.29 68.27
X. tropicalis 7.35% 64.80 77.72 71.39
D. melanogaster 6.34% 93.17 78.46 95.15
S. cerevisiae 4.79% 90.12 83.32 87.68
R. norvegicus 2.97% 44.04 38.69 51.77
T. aestivum 2.62% 0.00 89.68 23.35
P. americana 2.27% 0.00 98.50 7.76
C. elegans 2.08% 96.83 95.88 97.50
H. herpesvirus 5 1.58% 0.00 54.46 4.27
R. virus 1.45% 0.00 28.54 6.45
H. spumaretrovirus 1.17% 0.00 99.37 2.49
... ... ... ... ...
Macro-average 9.85 30.21 31.80
Micro-average 70.48 67.13 73.70
Table 4: The micro-averaged F1 scores (%) of Maxent
(Mxt), Rel-ENJU-Genia with spreading (Rel), and Hbrd with
respect to each of the most frequent 12 species in DEVTEST.
sponses R times, reassign each response to A or
B and see how likely such a shuffle produces a
difference in the metric of interest that is at least
as large as the difference observed when using A
and B on the test data. Let r denote the number
of times that such a difference occurred, then as
R ? ?,
r+1
R+1
approaches the significance level.
In our case, the metrics tested were micro- and
macro-averaged precision, recall and F1.
Following this procedure, we tested whether the
improvements made by a relation classification
based system (i.e., Rel-ENJU-Genia with SPRD) and
the hybrid system (i.e., Hbrd) over the baseline sys-
tems were statistically significant. We carried out
approximate randomisation with 10,000 shuffles
and the test results are shown in Table 5. The nu-
merical figures in the cells are differences in pre-
cision, recall and F1 between a pair of systems.
The significance levels (i.e., p-values) are indi-
cated by superscript marks, whose correspond-
ing values are displayed in Table 6. For exam-
1520
Rule-Majority Rule-Sp Rule-SpSent Maxent
Rel
micro-avg 2.77
?
/3.46
?
/3.13
?
1.95
?
/2.04
?
/2.00
?
1.57
?
/2.22
?
/1.92
?
-1.57
?
/ -5.02
?
/ -3.35
?
macro-avg 15.24
?
/13.12
?
/12.13
?
3.75
a
/4.21
a
/3.20
a
9.35
?
/8.44
?
/7.10
?
21.92
?
/24.87
?
/20.35
?
Hbrd
micro-avg 8.01
?
/11.27
?
/9.70
?
7.19
?
/9.85
?
/8.57
?
6.81
?
/10.04
?
/8.49
?
3.67
?
/2.78
?
/2.82
b
macro-avg 27.22
?
/15.72
c
/13.72
d
15.73
?
/6.82
e
/4.80
f
21.33
?
/11.05
g
/ 8.70
h
33.91
i
/27.47
?
/21.95
?
Table 5: Results of paired randomisation tests on whether Rel-ENJU-Genia with SPRD (Rel) and Hbrd significantly im-
proved the baseline systems. The numerical figures in the cells show the differences between the two systems as measured by
precision/recall/F1 in percentage. The superscript marks indicate the significance levels and are explained in Table 6.
ple, the difference in micro-averaged precision be-
tween Rel-ENJU-Genia and Rule-Majority on the test
data was 2.77%, and in 10,000 approximate ran-
domisation trials, there was zero times
13
that Rel-
ENJU-Genia?s micro-averaged precision is greater
than Rule-Majority?s by at least 2.77% (p < 0.0001).
MARK VALUE MARK VALUE
* p < 0.0001 a p < 0.06
b p < 0.002 c p < 0.0003
d p < 0.0002 e p < 0.03
f p < 0.05 g p < 0.003
h p < 0.005 i p < 0.07
Table 6: p-values.
The test results confirmed that, the improve-
ments made by Hbrd are statistically significant
with at least 95% confidence as measured by all
metrics except for macro-averaged precision. The
relation classification approach achieved signifi-
cantly lower performance than Maxent in terms of
micro-averaged scores (hence the ?-? sign in the
corresponding cell in Table 5), but in all other
cases it can reject the null hypothesis with very
high confidence (i.e., p < 0.0001).
6 Conclusions and Future Work
This paper proposes a method that tackles a com-
plex disambiguation problem by breaking it into
two cascaded simpler tasks of cue word discov-
ery and binary relation classification. We evalu-
ated the method on the task of disambiguating the
model organisms of biomedical named entities,
along with a number of other approaches. As mea-
sured by micro-averaged F1 score, a supervised
classification approach (Maxent) yielded the second
best result. However, it can only disambiguate
a small number of species that have abundant
training instances. With spreading rules, a rela-
tion classification system (Rel-ENJU-Genia) trained
on word and syntactic features from ENJU-Genia
also obtained good micro-averaged F1, while sur-
13
The numbers of times are not shown in Table5 for
brevity.
passing Maxent significantly in terms of macro-
averaged scores. Combining these two systems
achieved the best overall performance. Neverthe-
less, we combined the two methods in a rather
crude way, leaving ample room for exploring bet-
ter strategies in the future.
One drawback of the relation classification sys-
tems is that they can not cover all entity mentions
but only the ones with informative keywords co-
occurring in the same sentence. We overcame the
drawback by using spreading rules. For some ap-
plications, however, it may be sufficient to make
predictions exclusively for cases where the sys-
tems are applicable. Also, the predictions with
high confidence can be used as seed training ma-
terial for automatically harvesting more training
data.
Acknowledgments
The work reported in this paper is funded by Pfizer
Ltd.. The UK National Centre for Text Mining is
funded by JISC. The ITI-TXM corpus used in the
experiments was developed at School of Informat-
ics, University of Edinburgh, in the TXM project,
which was funded by ITI Life Sciences, Scotland.
References
E. Agirre and D. Martinez. 2004. Unsupervised WSD based
on automatically retrieved examples: The importance of
bias. In Proceedings of EMNLP.
A. Airola, S. Pyysalo, J. Bj?orne, T. Pahikkala, F. Ginter, and
T. Salakoski. 2008. A graph kernel for protein-protein
interaction extraction. In Proceedings of BioNLP.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008a.
Assisted curation: does text mining really help? In Pro-
ceedings of the Pacific Symposium on Biocomputing.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008b.
The ITI TXM corpus: Tissue expression and protein-
protein interactions. In Proceedings of the Workshop on
Building and Evaluating Resources for Biomedical Text
Mining at LREC.
E. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the RASP system. In Proceedings of the COL-
ING/ACL Interactive Presentation Sessions.
1521
R. Bunescu and M. Pas?ca. 2006. Using encyclopedic knowl-
edge for named entity disambiguation. In Proceedings of
EACL.
L. Chen, H. Liu, and C. Friedman. 2005. Gene name
ambiguity of eukaryotic nomenclatures. Bioinformatics,
21(2):248?256.
S. Clark and J. R. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models. Com-
putational Linguistics, 33(4).
M-C de Marneffe, B. MacCartney, and C. D. Manning. 2006.
Generating typed dependency parses from phrase struc-
ture. In Proceedings of LREC.
G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semi-
supervised classification for extracting protein interaction
sentences using dependency parsing. In Proceedings of
the Joint Conference of EMNLP and CoNLL.
G. Forman. 2003. An extensive empirical study of feature se-
lection metrics for text classification. Journal of Machine
Learning Research, 3:1289?1305.
C. Grover, M. Lapata, and A. Ascarides. 2003. A compar-
ison of parsing technologies for the biomedical domain.
Natural Language Engineering, 1(1):1?38.
J. Hakenberg, C. Plake, R. Leaman, M. Schroeder, and
G. Gonzalez. 2008. Inter-species normalization of gene
mentions with GNAT. Bioinformatics, 24(16).
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating impact
of re-training a lexical disambiguation model on domain
adaptation of an HPSG parser. In Proceedings of the 10th
International Conference on Parsing Technology.
V. Hatzivassiloglou, PA Dubou?e, and A. Rzhetsky. 2001.
Disambiguating proteins, genes, and RNA in text: a ma-
chine learning approach. Bioinformatics, 17(Suppl 1).
L. Hirschman, M. Krallinger, J. Wilbur, and A. Valencia, ed-
itors. 2007. The BioCreative II - Critical Assessment
for Information Extraction in Biology Challenge, volume
9(Suppl 2). Genome Biology.
L. Hunter and K. B. Cohen. 2006. Biomedical language
processing: what?s beyond PubMed. Molecular Cell,
21(5):589?594.
N. Japkowicz. 2000. Learning from imbalanced data sets: a
comparison of various strategies. In Proceedings of AAAI
Workshop on Learning from Imbalanced Data Sets.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of ACL.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of HLT/EMNLP.
D. Lin. 1998. Dependency-based evaluation of MINIPAR.
In Proceedings of Workshop on the Evaluation of Parsing
Systems.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1994.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. Miwa, R. Satre, Y. Miyao, T. Ohta, and J. Tsujii. 2008.
Combining multiple layers of syntactic information for
protein-protein interaction extraction. In Proceedings of
SMBM.
Y. Miyao and J. Tsujii. 2008. Feature forest models for prob-
abilistic HPSG parsing. Computational Linguistics, 34(1).
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsujii.
2008. Task-oriented evaluation of syntactic parsers and
their representations. In Proceedings of ACL-08: HLT.
A. A. Morgan and L. Hirschman. 2007. Overview of
BioCreAtIvE II gene normalisation. In Proceedings of the
BioCreAtIvE II Workshop, Madrid.
A. A. Morgan, Z. Lu, X. Wang, A. M. Cohen, J. Fluck,
P. Ruch, A. Divoli, K. Fundel, R. Leaman, J. Haken-
berg, C. Sun, H. Liu, R. Torres, M. Krauthammer, W. W.
Lau, H. Liu, C. Hsu, M. Schuemie, K. B. Cohen, and
L. Hirschman. 2008. Overview of BioCreAtIvE II gene
normalization. Genome Biology, 9(Suppl 2).
A. Moschitti. 2006. Making tree kernels practical for natural
language learning. In Proceedings of EACL.
E. W. Noreen. 1989. Computer Intensive Methods for Test-
ing Hypothesis. John Wiley & Sons.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
A. S. Schwartz and M. A. Hearst. 2003. Identifying abbrevi-
ation definitions in biomedical text. In Proceedings of the
Pacific Symposium on Biocomputing.
B. Settles. 2005. ABNER: An open source tool for automat-
ically tagging genes, proteins, and other entity names in
text. Bioinformatics, 21(14):3191?3192.
M. Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005. Syn-
tax annotation for the GENIA corpus. In Proceedings of
IJCNLP.
X. Wang and C. Grover. 2008. Learning the species of
biomedical named entities from annotated corpora. In
Proceedings of LREC.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Proceedings of
ICML.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proceedings of COLING.
1522
Coling 2010: Poster Volume, pages 851?859,
Beijing, August 2010
Imbalanced Classification Using Dictionary-based Prototypes and
Hierarchical Decision Rules for Entity Sense Disambiguation
Tingting Mu
National Centre for Text Mining
University of Manchester
tingting.mu@man.ac.uk
Xinglong Wang
National Centre for Text Mining
University of Manchester
xinglong.wang@man.ac.uk
Jun?ichi Tsujii
Department of Computer Science
University of Tokyo
tsujii@is.s.u-tokyo.ac.jp
Sophia Ananiadou
National Centre for Text Mining
University of Manchester
Sophia.Ananiadou@man.ac.uk
Abstract
Entity sense disambiguation becomes dif-
ficult with few or even zero training in-
stances available, which is known as im-
balanced learning problem in machine
learning. To overcome the problem, we
create a new set of reliable training in-
stances from dictionary, called dictionary-
based prototypes. A hierarchical classifi-
cation system with a tree-like structure is
designed to learn from both the prototypes
and training instances, and three different
types of classifiers are employed. In addi-
tion, supervised dimensionality reduction
is conducted in a similarity-based space.
Experimental results show our system out-
performs three baseline systems by at least
8.3% as measured by macro F1 score.
1 Introduction
Ambiguities in terms and named entities are a
challenge for automatic information extraction
(IE) systems. The problem is particularly acute
for IE systems targeting the biomedical domain,
where unambigiously identifying terms is of fun-
damental importance. In biomedical text, a term
(or its abbreviation (Okazaki et al, 2010)) may
belong to a wide variety of semantic categories
(e.g., gene, disease, etc.). For example, ER may
denote protein estrogen receptor in one context,
but cell subunit endoplasmic reticulum in another,
not to mention it can also mean emergency room.
In addition, same terms (e.g., protein) may be-
long to many model organisms, due to the nomen-
clature of gene and gene products, where genes
in model organisms other than human are given,
whenever possible, the same names as their hu-
man orthologs (Wain et al, 2002). On the other
hand, public biological databases keep species-
specific records for the same protein or gene,
making species disambiguation an inevitable step
for assigning unique database identifiers to entity
names in text (Hakenberg et al, 2008; Krallinger
et al, 2008).
One way to entity disambiguation is classify-
ing an entity into pre-defined semantic categories,
based on its context (e.g., (Bunescu and Pas?ca,
2006)). Existing classifiers, such as maximum
entropy model, achieved satisfactory results on
the ?majority? classes with abundant training in-
stances, but failed on the ?minority? ones with few
or even zero training instances, i.e., the knowl-
edge acquisition bottleneck (Agirre and Martinez,
2004). Furthermore, it is often infeasible to cre-
ate enough training data for all existing semantic
classes. In addition, too many training instances
for certain majority classes lead to increased com-
putational complexity for training, and a biased
system ignoring the minority ones. These corre-
spond to two previously addressed difficulties in
imbalanced learning: ?... either (i) you have far
more data than your algorithms can deal with,
851
and you have to select a sample, or (ii) you have
no data at all and you have to go through an in-
volved process to create them? (Provost, 2000).
Given an entity disambiguation task with imbal-
anced data, this paper explores how to create more
informative training instances for minority classes
and how to improve the large-scale training for
majority classes.
Previous research has shown that words denot-
ing class information in the surrounding context of
an entity can be an informative indicator for dis-
ambiguation (Krallinger et al, 2008; Wang et al,
2010). Such words are refered to as ?cue words?
throughout this paper. For example, to disam-
biguate the type of an entity, that is, whether it
is a protein, gene, or RNA, looking at words such
as protein, gene and RNA are very helpful (Hatzi-
vassiloglou et al, 2001). Similarly, for the task
of species disambiguation (Wang et al, 2010),
the occurrence of mouse p53 strongly suggests
that p53 is a mouse protein. In many cases, cue
words are readily available in dictionaries. Thus,
for the minority classes, instead of creating arti-
ficial training instances by commonly used sam-
pling methods (Haibo and Garcia, 2009), we pro-
pose to create a new set of real training instances
by modelling cue words from a dictionary, called
dictionary-based prototypes. To learn from both
the original training instances and the dictionary-
based prototypes, a hierarchical classification sys-
tem with a tree-like structure is designed. Further-
more, to cope with the large number of features
representing each instance, supervised orthogo-
nal locality preserving projection (SOLPP) is con-
ducted for dimensionality reduction, by simulta-
neously preserving the intrinsic structures con-
structed from both the features and labels. A new
set of lower-dimensional embeddings with better
discriminating power is obtained and used as in-
put to the classifier. To cope with the large num-
ber of training instances in some majority classes,
we propose a committee machine scheme to ac-
celerate training speed without sacrificing classi-
fication accuracy. The proposed method is evalu-
ated on a species disambiguation task, and the em-
pirical results are encouraging, showing at least
8.3% improvement over three different baseline
systems.
2 Related Work
Construction of a classification model using su-
pervised learning algorithms is popular for entity
disambiguation. A number of researchers have
tackled entity disambiguation in general text us-
ing wikipedia as a resource to learn classifica-
tion models (Bunescu and Pas?ca, 2006). Hatzi-
vassiloglou et al (2001) studied disambiguating
proteins, genes, and RNA in text by training var-
ious classifiers using entities with class informa-
tion provided by adjacent cue words. Wang et
al. (2010) proposed a ?hybird? system for species
disambiguation, which heuristically combines re-
sults obtained from classifying the context, and
those from modeling relations between cue words
and entities. Although satisfactory performance
was reported, their system incurs higher computa-
tional cost due to syntactic parsing and the binary
relation classifier.
Many imbalanced learning techniques, as re-
viewed by Haibo and Garcia (2009), can also be
used to achieve the same purpose. However, to
our knowledge, there is little research in apply-
ing these machine learning (ML) techniques to en-
tity disambiguation. It is worth mentioning that
although these ML techniques can improve the
learning performance to some extent, they only
consider the information contained in the origi-
nal training instances. The created instances do
not add new information, but instead utilize the
original training information in a more sophisti-
cated way. This motivates us to pursue a differ-
ent method of creating new training instances by
using information from a related and easily ob-
tained source (e.g., a dictionary), similar to trans-
fer learning (Pan and Yang, 2009).
3 Task and Corpus
In this work, we develop an entity disambiguation
technique with the use of cue words, as well as a
general ML algorithm for imbalanced classifica-
tion using a set of newly created dictionary-based
prototypes. These prototypes are represented with
different features from those used by the original
training instances. The proposed method is eval-
uated on a species disambiguation task: given a
text, in which mentions of biomedical named en-
852
tities are annotated, we assign a species identi-
fier to every entity mention. The types of entities
studied in this work are genes and gene products
(e.g., proteins), and we use the NCBI Taxonomy1
(taxon) IDs as species tags and to build the proto-
types. Note that this paper focuses on the task of
species disambiguation and makes the assumption
that the named entities are already recognised.
Consider the following sentence as an exam-
ple: if one searches the proteins (i.e., the under-
lined term) in a protein database, he/she will find
they belong to many model organisms. However,
in this particular context, CD200R-CD4d3+4 is
human and mouse protein, while rCD4d3+4 is
a rat one.2 We call such a task of assigning
species identifiers to entities, according to context,
as species disambiguation.
The amounts of human and mouse
CD200R-CD4d3+4 and rCD4d3+4
protein on the microarray spots were
similar as visualized by the red fluo-
rescence of OX68 mAb recognising
the CD4 tag present in each of the
recombinant proteins.
The informative cue words (e.g., mouse) used
to help species disambiguation are called species
words. In this work, species words are defined as
any word that indicates a model organism and also
appears in the organism dictionaries we use. They
may have various parts-of-speech, and may also
contain multiple tokens (despite the name species
word). For example, ?human?, ?mice?, ?bovine?
and ?E. Coli? are all species words. We detect
these words by automatic dictionary lookup: a
word is annotated as a species word if it matches
an entry in a list of organism names. Each entry in
the list contains a species word and its correspond-
ing taxon ID, and the list is merged from two dic-
tionaries: the NCBI Taxonomy and the UniProt
controlled vocabulary of species.3 The NCBI por-
tion is a flattened NCBI Taxonomy (i.e., without
hierarchy) including only the identifiers of genus
and species ranks. In total, the merged list con-
1http://www.ncbi.nlm.nih.gov/sites/entrez?db= taxon-
omy
2Prefix ?r? in ?rCD4d3+4? indicates that it is a rat protein.
3http://www.expasy.ch/cgi-bin/speclist
tains 356,387 unique species words and 272,991
unique species IDs. The ambiguity in species
words is low: 3.86% of species words map to mul-
tiple IDs, and on average each word maps to 1.043
IDs.
The proposed method was evaluated on the
corpus developed in (Wang et al, 2010), con-
taining 6, 223 genes and gene products, each of
which was manually assigned with either a taxon
ID or an ?Other? tag, with human being the
most frequent at 50.30%. With the extracted
features and the species ID tagged by domain
experts, each occurrence of named entities can
be represented as a d-dimensional vector with
a label. Species disambiguation can be mod-
elled as a multi-classification task: Given n train-
ing instances {xi}ni=1, their n ? d feature ma-
trix X = [xij ] and n-dimensional label vector
y = [y1, y2, . . . , yn]T are used to train a clas-
sifier C(?), where xi = [xi1, xi2, . . . , xid]T , yi ?
{1, 2, . . . , c}, and c denotes the number of ex-
isting species in total. Given m different query
instances {x?i}mi=1, their m ? d feature matrix
X? = [x?ij ] are used as the input to the trained
classifier, so that their labels can be predicted by
{C(x?i)}mi=1.
We used relatively simple contextual features
because this work was focused on developing a
ML framework. In more detail, we used the fol-
lowing features: 1) 200 words surrounding the en-
tity in question; 2) two nouns and two adjectives
at the entity?s left and right; 3) 5 species words
at the entity?s left and right. In addition, function
words and words that consist of only digits and
punctuations are filtered out. The final numeri-
cal dataset consists of 6,227 instances, each rep-
resented by 16,851 binary features and belonging
to one of the 13 classes. The dataset is highly im-
balanced: among the 13 classes, the numbers of
instances in the four majority classes vary from
449 to 3,220, while no more than 20 instances are
contained in the eight minority classes (see Table
1).
853
4 Proposed Method
4.1 Dictionary-based Prototypes
For each existing species, we create a b-
dimensional binary vector, given as pi =
[pi1, pi2, . . . , pib]T , using b different species
words listed in the dictionary as features, which
is called dictionary-based prototype. The binary
value pij denotes whether the jth species word
belongs to the ith species in the dictionary. This
leads to a c ? b feature matrix P = [pij ] for c
species.
Considering that the species words preceding
and appearing in the same sentence as an en-
tity can be informative indicators for the possible
species of this entity, we create two morem?b bi-
nary feature matrices for the query instances with
the same b species words as features: X?1 = [x?(1)ij ]
and X?2 = [x?(2)ij ], where x?(1)ij denotes whether the
jth species word is the preceding word of the ith
entity, and x?(2)ij denotes whether the jth species
word appears in the same sentence as the ith en-
tity but is not preceding word. Thus, the similar-
ity between each query entity and existing species
can be simply evaluated by calculating the inner-
product between the entity instance and the cor-
responding prototype. This leads to the following
m? c similarity matrix S? = [s?ij ]:
S? = ?X?1PT + (1? ?)X?2PT , (1)
where 0 ? ? ? 1 is a user-defined parameter con-
trolling the degree of indicating reliability of the
preceding word and the same-sentence word. The
n?c similarity matrix S = [sij ] between the train-
ing instances and the species can be constructed in
exactly the same way. Based on empirical expe-
rience, the preceding word indicates the entity?s
species more accurately than the same-sentence
word. Thus, ? is preferred to be set as greater
than 0.5. The obtained similarity matrix will be
used in the nearest neighbour classifier (see Sec-
tion 4.2.1).
Both the original training instances X and the
newly created prototypes P are used to train the
proposed hierarchical classification system. Sub-
ject to the nature of the classifier employed, it is
convenient to construct one single feature matrix
Nearest Neighbor Classifier(IT1)
Minority Classes
Majority Classes
SOLPP-FLDA Classifier(IT2)
Small-scale Majority Classes
Large-scale Majority Classes
Committee Classifier(END)
Yes
No Yes
No
Output: Instances with predicted labels belonging to MI Output: Instances with predicted labels belonging to SMA
Output: Instances with predicted labels belonging to LMA
Note: Definition of the minority,majority, small-scale majority, large-scale majority classes, as well as theIF-THEN rule 1 (IT1) and IF-THEN rule2 (IT2) are provided in the paper.
Figure 1: Structure of the proposed hierarchical
classification system
instead of using X and P individually. Aiming at
keeping the same similarity values between each
entity instance and the species prototype, we con-
struct the following (n+c)?(d+b) feature matrix
for both the training instances and prototypes:
F =
[ X ?X1 + (1? ?)X2
0 P
]
, (2)
where X1 and X2 are constructed in the same way
as X?1 and X?2 but for training instances. Their cor-
responding label vector is l = [yT , 1, 2, . . . , c]T .
4.2 Hierarchical Classification
Multi-stage or hierarchical classification (Giusti
et al, 2002; Podolak, 2007; Kurzyn?ski, 1988)
is widely used in many complex multi-category
classification tasks. Existing research shows such
techniques can potentially achieve right trade-off
between accuracy and resource allocation (Giusti
et al, 2002; Podolak, 2007). Our proposed hier-
archical system has a tree-like structure with three
different types of classifier at nodes (see Figure 1).
Different classes are organized in a hierarchical
order to be classified based on the corresponding
numbers of available training instances. Letting
ni denote the number of training instances avail-
able in the ithe class excluding the created proto-
types, we categorize the classes as follows:
? Minority Classes (MI): Classes with less
training instances than the threshold: MI =
{i : nin < ?1, i ? {1, 2 . . . , c}}.
854
? Majority Classes (MA): Classes with more
training instances than the threshold: MA =
{i : nin ? ?1, i ? {1, 2 . . . , c}}.
? Small-scale Majority Classes (SMA): Ma-
jority Classes with less training instances
than the threshold: SMA = {i : nin <
?2, i ? MA}.
? Large-scale Majority Classes (LMA): Ma-
jority Classes with more training instances
than the threshold: LMA = {i : nin ?
?2, i ? MA}.
Here, 0 < ?1 < 1 and 0 < ?2 < 1 are size
thresholds set by users. We have MI ?MA = ?,
SMA ? LMA = ?, and SMA ? LMA = MA.
The tree-like hierarchical structure of our sys-
tem is determined by MI, MA, SMA, and LMA.
We propose two IF-THEN rules to control the sys-
tem: Given a query instance x?i, the level 1 clas-
sifier C1 is used to predict whether x?i belongs to
MA or a specific class in MI, which wer call IF-
THEN rule 1 (IT1). If x?i belongs to MA, the level
2 classifier C2 is used to predict whether x?i be-
longs to LMA or a specific class in SMA, called
IF-THEN rule 2 (IT2). If x?i belongs to LMA, the
level 3 classifier C3 finally predicts the specific
class in LMA x?i belongs to. We explain in the
following sections how the classifiers C1, C2, and
C3 work in detail.
4.2.1 Nearest Neighbour Classifier
The goal of the nearest neighbour classifier, de-
noted by C1, is to decide whether the nearest-
neighbour prototype of the query instance be-
longs to MI. The only used training instances are
our created dictionary-based prototypes {pi}ci=1
with the label vector [1, 2, . . . , c]T . The nearest-
neighbour prototype of the query instance x?i pos-
sesses the maximum similarity to x?i:
NN(x?i) = arg maxj=1, 2, ..., c s?ij , (3)
where s?ij is obtained by Eq. (1). Consequently,
the output of the classifier C1 is given as
C1(x?i) =
{
NN(x?i), If NN(x?i) ? MI,
0, Otherwise.
(4)
The IF-THEN rule 1 can then be expressed as
Action(IT1) =
{ Go to C2, If C1(x?i) = 0,
Stop, Otherwise.
4.2.2 SOLPP-FLDA Classifier
The goal of the SOLPP-FLDA classifier, de-
noted by C2, is to predict whether the query in-
stance belongs to LMA or a specific class in SMA.
In this classifier, the used training instances are
the original training entities and the dictionary-
based prototypes, both belonging to MA. The fea-
ture matrix F and the label vector l defined in Sec-
tion 4.1 are used, but with instances from MI re-
moved (we use n? to denote the number of remain-
ing training instances, and the same symbol F for
feature matrix). The used label vector l? to train C2
should be re-defined as l?i = li if li ? SMA, and 0
otherwise.
First, we propose to implement orthog-
onal locality preserving projection (OLPP)
(Kokiopoulou and Saad, 2007) in a supervised
manner, leading to SOLPP, to obtain a smaller set
of more powerful features for classification. Also,
we conduct SOLPP in a similarity-based feature
space computed from (d + 2b) original features
by employing dot-product based similarity, given
by FFT . As explained later, to compute the
new features from FFT instead of the original
features F achieves reduced computational cost.
An n??k projection matrix V = [vij ] is optimized
in this n-dimensional similarity-based feature
space. The optimal projections are obtained by
minimizing the weighted distances between the
lower-dimensional embeddings so that ?similar?
instances are mapped together in the projected
feature space. Mathematically, this leads to the
following constrained optimization problem:
min
V?Rn??k,
VT V=Ik?k
tr[VTFTF(D?W)FFTV], (5)
where W = [wij ] denotes the n ? n weight ma-
trix with wij defining the degree of ?closeness? or
?similarity? between the ith and jth instances, D
is a diagonal matrix with {di =?n?j=1wij}n?i=1 as
the diagonal elements.
Usually, the weight matrix W is defined by
an adjacency graph constructed from the original
855
data, e.g. for OLPP. One common way to define
the adjacency is by including the K-nearest neigh-
bors (KNN) of a given node to its adjacency list,
which is also called the KNN-graph (Kokiopoulou
and Saad, 2007). There are two common ways to
define the weight matrix: constant value, where
wij = 1 if the ith and jth samples are adjacent,
while wij = 0 otherwise, and Gaussian kernel.
We will denote in the rest of the paper such a
weight matrix computed only from the features
as WX . Ideally, if the features can accurately
describe all the discriminating characteristics, the
samples that are close or similar enough to each
other should have the same label vectors. How-
ever, when processing real dataset, what may hap-
pen is that, in the d-dimensional feature space,
the data points that are close to each other may
belong to different categories, while on the con-
trary, the data points that are in a distant to each
other may belong to the same category. In the k-
dimensional projected feature space obtained by
OLPP, one may have the same problem. Because
OLPP solves the constrained optimization prob-
lem in Eq. (5) using WX : if two instances are
close or similar to each other in the original fea-
ture space, they will be the same close or simi-
lar to each other in the projected space. To solve
this problem, we decide to modify the ?closeness?
or ?similarity? between instances in the projected
feature space by considering the label informa-
tion. The following computation of a supervised
weight matrix is used for our SOLPP:
W = (1? ?)WX + ?LLT , (6)
where 0 ? ? ? 1 is a user-defined parameter
controlling the tradeoff between the label-based
and feature-based neighborhood structures, and
L = [lij ] is an n? ? c binary label matrix with
lij = 1 if the ith instance belongs to the jth class,
and lij = 0 otherwise.
The optimal solution of Eq. (5) is the top
(k + 1)th eigenvectors of the n? ? n? symmetric
matrix FTF(D ? W)FFT , corresponding to the
k + 1 smallest eigenvalues, but with the top one
eigenvector removed, denoted by V?. It is worth
to mention that if the original feature matrix F is
used as the input of SOLPP, one needs to com-
pute the eigen-decomposition of the (d + b) ?
(d+ b) symmetric matrix FT (D?W)F. The cor-
responding computation complexity increases in
O((d + b)3), which is unacceptable in practical
when d + b  n?. The projected features for the
training instances are computed by
Z = FFTV?. (7)
Given a different set of m query instances with an
m? (d+ b) feature matrix,
F? = [X?, ?X?1 + (1? ?)X?2], (8)
their embeddings can be easily obtained by
Z? = F?F?TV?. (9)
Then, the projected feature matrix Z and label
vector l? are used to train a multi-class classifier.
By employing the one-against-all scheme, differ-
ent binary classifiers {C(2)i }i?SMA?{0} with label
space {+1, ?1} are trained. For the ith class
(i ? SMA?{0}), the training instances belonging
to it are labeled as positive, otherwise negative. In
each binary classifier C(2)i , a separating function
f (2)i (x) = xTw
(2)
i + b
(2)
i (10)
is constructed, of which the optimal values of the
weight vector w(2)i and bias b(2)i are computed us-
ing Fisher?s linear discriminant analysis (FLDA)
(Fisher, 1936; Mu, 2008). Finally, the output of
the classifier C2 can be obtained by assigning the
most confident class label to the query instance x?i,
with the confidence value indicated by the value of
separating function:
C2(x?i) = arg max
j?SMA?{0}
f (2)j (x?i). (11)
The IF-THEN rule 2 can then be expressed as
Action(IT2) =
{ Go to C3, If C2(x?i) = 0,
Stop, Otherwise.
4.2.3 Committee Classifier
The goal of the committee classifier, denoted
by C3, is to predict the specific class in LMA
the query instance belongs to. The used training
856
instances are entities and dictionary-based proto-
types only belonging to LMA. With the same one-
against-all scheme, there are large number of pos-
itive and negative training instances to train a bi-
nary classifier for a class in LMA. To accelerate
the training procedure without sacrificing the ac-
curacy, the following scheme is designed.
Letting ne denote the number of experts in
committee, all the training instances are averagely
divided into ne+1 groups each containing similar
numbers of training instances from the same class.
The instances in the ith and the (i+1)th groups are
used to train the ith expert classifier. This achieves
overlapped training instances between expert clas-
sifiers. The output value of C(3)i is not the class in-
dex as used in C2, but the value of the separating
function of the most confident class, denoted by
f (3)i . Different from the commonly used majority
voting rule, we only trust the most confident ex-
pert. Thus, the output of C3 for a query instance
x?i can be obtained by
C3(x?i) = arg maxj=1, 2, ..., ne f
(3)
j (x?i). (12)
By using C3, different expert classifiers can be
trained in parallel. The total training time is equal
to that of the slowest expert classifier. The more
expert classifiers are used, the faster the system is,
however, the less accurate the system may become
due to the decrease of used training instances for
each expert, especially the positive instances in
the case of imbalanced classification. This is also
the reason we do not apply the committee scheme
to SMA classes.
5 Experiments
5.1 System Evaluation and Baseline
We evaluate the proposed method using 5-fold
cross validation, with around 4,980 instances for
training, and 1,245 instances for test in each trial.
We compute the F1 score for each species, and
employ macro- and micro- average scheme to
compute performance for all species. Three base-
lines for comparison include:
? Baseline 1 (B1) : A maximum entropy
model trained with training data only.
? Baseline 1 (B2) : Combination of B1 and
the species dictionary using rules employed
in Wang et al (2010).
? Baseline 2 (B3): The ?hybrid? system com-
bining B1, the dictionary and a relation
model 4 using rules (Wang et al, 2010).
Our hierarchical classification system were imple-
mented in two ways:
? HC: Only the training data on its own is used
to train the system.
? HC/D: Both the training data and the
dictionary-based prototypes are used to train
the system.
5.2 Results and Analysis
The proposed system was implemented with ? =
0.8, ? = 0.8, ne = 4, and k = 1000. The species
9606, 10090, 7227, and 4932 were categorized as
LMA, the species 10116 as SMA, and the rest sep-
cies as MI. To compute the supervised weight ma-
trix, the percentage of the used KNN in the KNN-
graph was 0.6. Parameters were not fine tuned, but
set based on our empirical experience on previous
classification research. As shown in Table 1: HC
and B1 were trained with the same instances and
features, and HC outperformed B1 in both macro
and micro F1. Both HC and B1 obtained zero F1
scores for most minority species, showing that it is
nearly impossible to correctly label the query in-
stances of minority classes, due to lack of training
data. By learning from a related resource, HC/D,
B2, and B3 yielded better macro performance. In
particular, while HC/D and B2 learned from the
same dictionary and training data, HC/D outper-
formed B2 by 19.1% in macro and 2.5% in mi-
cro F1. B3 aimed at improving the macro perfor-
mance by employing computationally expensive
syntactic parsers and also by training an extra re-
lation classifier. With the same goal, HC/D inte-
grated the cue word information into the ML clas-
sifier in a more general way, and yielded an 8.3%
improvement over B3, as measured by macro-F1.
4This is an SVM model predicting relations between en-
tities and nearby species words with positive output indicates
species words bear the semantic label of entities.
857
Species Name Cat. No. HC HC/D B1 B2 B3
Homo sapiens (9606) LMA 3220 87.39 87.48 86.06 85.43 86.48
Mus musculus (10090) LMA 1709 79.99 79.98 79.59 80.00 80.41
Drosophila melanogaster (7227) LMA 641 86.62 86.35 87.96 87.02 87.37
Saccharomyces cerevisiae (4932) LMA 499 90.24 90.24 83.35 81.64 84.64
Rattus norvegicus (10116) SMA 50 55.07 69.23 48.42 64.41 59.41
Escherichia coli K-12 (83333) MI 18 0.00 0.00 0.00 0.00 0.00
Xenopus tropicalis (8364) MI 8 0.00 40.00 0.00 41.67 36.36
Caenorhabditis elegans (6239) MI 7 0.00 22.22 0.00 28.57 22.22
Oryctolagus cuniculus (9986) MI 3 0.00 0.00 0.00 20.00 0.00
Bos taurus (9913) MI 3 0.00 50.00 0.00 0.00 100.00
Arabidopsis thaliana (3702) MI 2 0.00 0.00 0.00 0.00 66.67
Arthropoda (6656) MI 1 0.00 100.00 0.00 50.00 0.00
Martes zibellina (36722) MI 1 0.00 50.00 0.00 28.57 0.00
Micro-average N/A N/A 85.03 85.13 83.59 83.04 83.80
Macro-average N/A N/A 30.72 51.96 29.42 43.64 47.97
Table 1: Performance is compared in F1 (%), where ?No.? denotes the number of training instances
and ?Cat.? denotes the category of species class as defined in Section 4.2.
6 Conclusions and Future Work
Disambiguating bio-entities presents a challenge
for traditional supervised learning methods, due
to the high number of semantic classes and lack of
training instances for some classes. We have pro-
posed a hierarchical framework for imbalanced
learning, and evaluated it on the species disam-
biguation task. Our method automatically builds
training instances for the minority or missing
classes from a cue word dictionary, under the as-
sumption that cue words in the surrounding con-
text of an entity strongly indicate its semantic cat-
egory. Compared with previous work (Wang
et al, 2010; Hatzivassiloglou et al, 2001), our
method provides a more general way to integrate
the cue word information into a ML framework
without using deep linguistic information.
Although the species disambiguation task is
specific to bio-text, the difficulties caused by im-
balanced frequency of different senses are com-
mon in real application of sense disambiguation.
The proposed technique can also be applied to
other domains, providing the availability of a cue
word dictionary that encodes semantic informa-
tion regarding the target semantic classes. Build-
ing such a dictionary from scratch can be chal-
lenging, but may be easier compared to manual
annotation. In addition, such dictionaries may al-
ready exist in specialised domains.
Acknowledgment
The authors would like to thank the biologists who
annotated the species corpus, and National Cen-
tre for Text Mining. Funding: Pfizer Ltd.; Joint
Information Systems Committee (to UK National
Centre for Text Mining)
References
Agirre, E. and D. Martinez. 2004. Unsupervised WSD
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of EMNLP.
Bunescu, R. and M. Pas?ca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL.
Fisher, R. A. 1936. The use of multiple measure-
ments in taxonomic problems. Annals of Eugenics,
7(2):179?188.
Giusti, N., F. Masulli, and A. Sperduti. 2002. Theoret-
ical and experimental analysis of a two-stage system
for classification. IEEE Trans. on Pattern Analysis
and Machine Intelligence, 24(7):893?904.
Haibo, H. and E. A. Garcia. 2009. Learning from
imbalanced data. IEEE Trans. on Knowledge and
Data Engineering, 21(9):1263?1284.
858
Hakenberg, J., C. Plake, R. Leaman, M. Schroeder, and
G. Gonzalez. 2008. Inter-species normalization of
gene mentions with GNAT. Bioinformatics, 24(16).
Hatzivassiloglou, V., PA Duboue?, and A. Rzhetsky.
2001. Disambiguating proteins, genes, and RNA in
text: a machine learning approach. Bioinformatics,
17(Suppl 1).
Kokiopoulou, E. and Y. Saad. 2007. Orthogonal
neighborhood preserving projections: A projection-
based dimensionality reduction technique. IEEE
Trans. on Pattern Analysis and Machine Intelli-
gence, 29(12):2143?2156.
Krallinger, M., A. Morgan, L. Smith, F. Leitner,
L. Tanabe, J. Wilbur, L. Hirschman, and A. Valen-
cia. 2008. Evaluation of text-mining systems for
biology: overview of the second biocreative com-
munity challenge. Genome Biology, 9(Suppl 2).
Kurzyn?ski, M. W. 1988. On the multistage bayes clas-
sifier. Pattern Recognition, 21(4):355?365.
Mu, T. 2008. Design of machine learning algorithms
with applications to breast cancer detection. Ph.D.
thesis, University of Liverpool.
Okazaki, N., S. Ananiadou, and J. Tsujii. 2010.
Building a high quality sense inventory for im-
proved abbreviation disambiguation. Bioinformat-
ics, doi:10.1093/bioinformatics/btq129.
Pan, S. J. and Q. Yang. 2009. A survey on transfer
learning. IEEE Trans. on Knowledge and Data En-
gineering.
Podolak, I. T. 2007. Hierarchical rules for a hierarchi-
cal classifier. Lecture Notes in Computer Science,
4431:749?757.
Provost, F. 2000. Machine learning from imbalanced
data sets 101. In Proc. of Learning from Imbalanced
Data Sets: Papers from the Am. Assoc. for Artificial
Intelligence Workshop. (Technical Report WS-00-
05).
Wain, H., E. Bruford, R. Lovering, M. Lush,
M. Wright, and S. Povey. 2002. Guidelines for
human gene nomenclature. Genomics, 79(4):464?
470.
Wang, X., J. Tsujii, and S. Ananiadou. 2010. Dis-
ambiguating the species of biomedical named enti-
ties using natural language parsers. Bioinformatics,
26(5):661667.
859
