Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 177?185,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Importance-Driven Turn-Bidding for Spoken Dialogue Systems
Ethan O. Selfridge and Peter A. Heeman
Center for Spoken Language Understanding
Oregon Health & Science University
20000 NW Walker Rd., Beaverton, OR, 97006
selfridg@ohsu.edu, heemanp@ohsu.edu
Abstract
Current turn-taking approaches for spoken
dialogue systems rely on the speaker re-
leasing the turn before the other can take it.
This reliance results in restricted interac-
tions that can lead to inefficient dialogues.
In this paper we present a model we re-
fer to as Importance-Driven Turn-Bidding
that treats turn-taking as a negotiative pro-
cess. Each conversant bids for the turn
based on the importance of the intended
utterance, and Reinforcement Learning is
used to indirectly learn this parameter. We
find that Importance-Driven Turn-Bidding
performs better than two current turn-
taking approaches in an artificial collabo-
rative slot-filling domain. The negotiative
nature of this model creates efficient dia-
logues, and supports the improvement of
mixed-initiative interaction.
1 Introduction
As spoken dialogue systems are designed to
perform ever more elaborate tasks, the need
for mixed-initiative interaction necessarily grows.
Mixed-initiative interaction, where agents (both
artificial and human) may freely contribute to
reach a solution efficiently, has long been a focus
of dialogue systems research (Allen et al, 1999;
Guinn, 1996). Simple slot-filling tasks might
not require the flexible environment that mixed-
initiative interaction brings but those of greater
complexity, such as collaborative task comple-
tion or long-term planning, certainly do (Fergu-
son et al, 1996). However, translating this interac-
tion into working systems has proved problematic
(Walker et al, 1997), in part to issues surround-
ing turn-taking: the transition from one speaker to
another.
Many computational turn-taking approaches
seek to minimize silence and utterance overlap
during transitions. This leads to the speaker con-
trolling the turn transition. For example, systems
using the Keep-Or-Release approach will not at-
tempt to take the turn unless it is sure the user
has released it. One problem with this approach
is that the system might have important informa-
tion to give but will be unable to get the turn.
The speaker-centric nature of current approaches
does not enable mixed-initiative interaction and
results in inefficient dialogues. Primarily, these
approaches have been motivated by smooth tran-
sitions reported in the human turn-taking studies
of Sacks et al (1974) among others.
Sacks et al also acknowledge the negotiative
nature of turn-taking, stating that the ?the turn as
unit is interactively determined?(p. 727). Other
studies have supported this, suggesting that hu-
mans negotiate the turn assignment through the
use of cues and that these cues are motivated by
the importance of what the conversant wishes to
contribute (Duncan and Niederehe, 1974; Yang
and Heeman, 2010; Schegloff, 2000). Given
this, any dialogue system hoping to interact with
humans efficiently and naturally should have a
negotiative and importance-driven quality to its
turn-taking protocol. We believe that, by focus-
ing on the rationale of human turn-taking be-
havior, a more effective turn-taking system may
be achieved. We propose the Importance-Driven
Turn-Bidding (IDTB) model where conversants
bid for the turn based on the importance of their
utterance. We use Reinforcement Learning to map
a given situation to the optimal utterance and bid-
ding behavior. By allowing conversants to bid for
the turn, the IDTB model enables negotiative turn-
taking and supports true mixed-initiative interac-
tion, and with it, greater dialogue efficiency.
We compare the IDTB model to current turn-
taking approaches. Using an artificial collab-
orative dialogue task, we show that the IDTB
model enables the system and user to complete
177
the task more efficiently than the other approaches.
Though artificial dialogues are not ideal, they al-
low us to test the validity of the IDTB model be-
fore embarking on costly and time-consuming hu-
man studies. Since our primary evaluation criteria
is model comparison, consistent user simulations
provide a constant needed for such measures and
increase the external validity of our results.
2 Current Turn-Taking Approaches
Current dialogue systems focus on the release-turn
as the most important aspect of turn-taking, in
which a listener will only take the turn after the
speaker has released it. The simplest of these ap-
proaches only allows a single utterance per turn,
after which the turn necessarily transitions to the
next speaker. This Single-Utterance (SU) model
has been extended to allow the speaker to keep the
turn for multiple utterances: the Keep-Or-Release
(KR) approach. Since the KR approach gives the
speaker sole control of the turn, it is overwhelm-
ingly speaker-centric, and so necessarily unnego-
tiative. This restriction is meant to encourage
smooth turn-transitions, and is inspired by the or-
der, smoothness, and predictability reported in hu-
man turn-taking studies (Duncan, 1972; Sacks et
al., 1974).
Systems using the KR approach differ on how
they detect the user?s release-turn. Turn releases
are commonly identified in two ways: either us-
ing a silence-threshold (Sutton et al, 1996), or
the predictive nature of turn endings (Sacks et al,
1974) and the cues associated with them (e.g. Gra-
vano and Hirschberg, 2009). Raux and Eskenazi
(2009) used decision theory with lexical cues to
predict appropriate places to take the turn. Simi-
larly, Jonsdottir, Thorisson, and Nivel (2008) used
Reinforcement Learning to reduce silences be-
tween turns and minimize overlap between utter-
ances by learning the specific turn-taking patterns
of individual speakers. Skantze and Schlangan
(2009) used incremental processing of speech and
prosodic turn-cues to reduce the reaction time of
the system, finding that that users rated this ap-
proach as more human-like than a baseline system.
In our view, systems built using the KR turn-
taking approach suffer from two deficits. First,
the speaker-centricity leads to inefficient dialogues
since the speaker may continue to hold the turn
even when the listener has vital information to
give. In addition, the lack of negotiation forces
the turn to necessarily transition to the listener af-
ter the speaker releases it. The possibility that the
dialogue may be better served if the listener does
not get the turn is not addressed by current ap-
proaches.
Barge-in, which generally refers to allowing
users to speak at any time (Stro?m and Seneff,
2000), has been the primary means to create a
more flexible turn-taking environment. Yet, since
barge-in recasts speaker-centric systems as user-
centric, the system?s contributions continue to be
limited. System barge-in has also been investi-
gated. Sato et al (2002) used decision trees to de-
termine whether the system should take the turn or
not when the user pauses. An incremental method
by DeVault, Sagae, and Traum (2009) found pos-
sible points that a system could interrupt without
loss of user meaning, but failed to supply a rea-
sonable model as to when to use such information.
Despite these advances, barge-in capable systems
lack a negotiative turn-taking method, and con-
tinue to be deficient for reasons similar to those
described above.
3 Importance-Driven Turn-Bidding
(IDTB)
We introduce the IDTB model to overcome the de-
ficiencies of current approaches. The IDTB model
has two foundational components: (1) The impor-
tance of speaking is the primary motivation behind
turn-taking behavior, and (2) conversants use turn-
cue strength to bid for the turn based on this impor-
tance. Importance may be broadly defined as how
well the utterance leads to some predetermined
conversational success, be it solely task comple-
tion or encompassing a myriad of social etiquette
components.
Importance-Driven Turn-Bidding is motivated
by empirical studies of human turn-conflict res-
olution. Yang and Heeman (2010) found an in-
crease of turn conflicts during tighter time con-
straints, which suggests that turn-taking is in-
fluenced by the importance of task completion.
Schlegoff (2000) proposed that persistent utter-
ance overlap was indicative of conversants hav-
ing a strong interest in holding the turn. Walker
and Whittaker (1990) show that people will inter-
rupt to remedy some understanding discrepancy,
which is certainly important to the conversation?s
success. People communicate the importance of
their utterance through turn-cues. Duncan and
178
Niederehe (1974) found that turn-cue strength was
the best predictor of who won the turn, and this
finding is consistent with the use of volume to win
turns found by Yang and Heeman (2010).
The IDTB model uses turn-cue strength to bid
for the turn based on the importance of the utter-
ance. Stronger turn-cues should be used when the
intended utterance is important to the overall suc-
cess of the dialogue, and weaker ones when it is
not. In the prototype described in Section 5, both
the system and user agents bid for the turn after ev-
ery utterance and the bids are conceptualized here
as utterance onset: conversants should be quick
to speak important utterances but slow with less
important ones. This is relatively consistent with
Yang and Heeman (2010). A mature version of
our work will use cues in addition to utterance on-
set, such as those recently detailed in Gravano and
Hirshberg (2009).1
A crucial element of our model is the judgment
and quantization of utterance importance. We use
Reinforcement Learning (RL) to determine impor-
tance by conceptualizing it as maximizing the re-
ward over an entire dialogue. Whatever actions
lead to a higher return may be thought of as more
important than ones that do not.2 By using RL to
learn both the utterance and bid behavior, the sys-
tem can find an optimal pairing between them, and
choose the best combination for a given conversa-
tional situation.
4 Information State Update and
Reinforcement Learning
We build our dialogue system using the Informa-
tion State Update approach (Larsson and Traum,
2000) and use Reinforcement Learning for action
selection (Sutton and Barto, 1998). The system
architecture consists of an Information State (IS)
that represents the agent?s knowledge and is up-
dated using a variety of rules. The IS also uses
rules to propose possible actions. A condensed
and compressed subset of the IS ? the Reinforce-
ment Learning State ? is used to learn which pro-
posed action to take (Heeman, 2007). It has been
shown that using RL to learn dialogue polices is
generally more effective than ?hand crafted? di-
1Our work (present and future) is distinct from some re-
cent work on user pauses (Sato et al, 2002) since we treat
turn-taking as an integral piece of dialogue success.
2We gain an inherent flexibility in using RL since the re-
ward can be computed by a wide array of components. This
is consistent with the broad definition of importance.
alogue policies since the learning algorithm may
capture environmental dynamics that are unat-
tended to by human designers (Levin et al, 2000).
Reinforcement Learning learns an optimal pol-
icy, a mapping between a state s and action a,
where performing a in s leads to the lowest ex-
pected cost for the dialogue (we use minimum
cost instead of maximum reward). An -greedy
search is used to estimate Q-scores, the expected
cost of some state?action pair, where the system
chooses a random action with  probability and the
argminaQ(s, a) action with 1- probability. For
Q-learning, a popular RL algorithm and the one
used here,  is commonly set at 0.2 (Sutton and
Barto, 1998). Q-learning updates Q(s, a) based
on the best action of the next state, given by the
following equation, with the step size parameter
? = 1/
?
N(s, a) where N(s, a) is the number of
times the s, a pair has been seen since the begin-
ning of training.
Q(st, at) = Q(st, at) + ?[costt+1
+ argminaQ(st+1, a)?Q(st, at)]
The state space should be formulated as a
Markov Decision Process (MDP) for Q-learning
to update Q-scores properly. An MDP relies on
a first-order Markov assumption in that the transi-
tion and reward probability from some st, at pair
is completely contained by that pair and is unaf-
fected by the history st?1at?1, st?2at?2, . . .. For
this assumption to be met, care is required when
deciding which features to include for learning.
The RL State features we use are described in the
following section.
5 Domain and Turn-Taking Models
In this section, we show how the IDTB ap-
proach can be implemented for a collaborative
slot filling domain. We also describe the Single-
Utterance and Keep-Or-Release domain imple-
mentations that we use for comparison.
5.1 Domain Task
We use a food ordering domain with two partici-
pants, the system and a user, and three slots: drink,
burger, and side. The system?s objective is to fill
all three slots with the available fillers as quickly
as possible. The user?s role is to specify its de-
sired filler for each slot, though that specific filler
may not be available. The user simulation, while
intended to be realistic, is not based on empirical
data. Rather, it is designed to provide a rich turn-
179
taking domain to evaluate the performance of dif-
ferent turn-taking designs. We consider this a col-
laborative slot-filling task since both conversants
must supply information to determine the intersec-
tion of available and desired fillers.
Users have two fillers for each slot.3 A user?s
top choice is either available, in which case we say
that the user has adequate filler knowledge, or their
second choice will be available, in which we say
it has inadequate filler knowledge. This assures
that at least one of the user?s filler is available.
Whether a user has adequate or inadequate filler
knowledge is probabilistically determined based
on user type, which will be described in Section
5.2.
Table 1: Agent speech acts
Agent Actions
System query slot, inform [yes/no],
inform avail. slot fillers,
inform filler not available, bye
User inform slot filler,
query filler availability
We model conversations at the speech act level,
shown in Table 1, and so do not model the actual
words that the user and system might say. Each
agent has an Information State that proposes possi-
ble actions. The IS is made up of a number of vari-
ables that model the environment and is slightly
different for the system and the user. Shared vari-
ables include QUD, a stack which manages the
questions under discussion; lastUtterance, the pre-
vious utterance, and slotList, a list of the slot
names. The major system specific IS variables
that are not included in the RL State are availSlot-
Fillers, the available fillers for each slot; and three
slotFiller variables that hold the fillers given by the
user. The major user specific IS variables are three
desiredSlotFiller variables that hold an ordered list
of fillers, and unvisitedSlots, a list of slots that the
user believes are unfilled.
The system has a variety of speech actions: in-
form [yes/no], to answer when the user has asked a
filler availability question; inform filler not avail-
able, to inform the user when they have specified
an unavailable filler; three query slot actions (one
for each slot), a query which asks the user for a
filler and is proposed if that specific slot is unfilled;
3We use two fillers so as to minimize the length of train-
ing. This can be increased without substantial effort.
three inform available slot fillers actions, which
lists the available fillers for that slot and is pro-
posed if that specific slot is unfilled or filled with
an unavailable filler; and bye, which is always pro-
posed.
The user has two actions. They can inform the
system of a desired slot filler, inform slot filler, or
query the availability of a slot?s top filler, query
filler availability. A user will always respond with
the same slot as a system query, but may change
slots entirely for all other situations. Additional
details on user action selection are given in Section
5.2.
Specific information is used to produce an in-
stantiated speech action, what we refer to as an
utterance. For example, the speech action inform
slot filler results in the utterance of ?inform drink
d1.? A sample dialogue fragment using the Single-
Utterance approach is shown in Table 2. Notice
that in Line 3 the system informs the user that
their first filler, d1, is unavailable. The user then
asks asks about the availability of its second drink
choice, d2 (Line 4), and upon receiving an affirma-
tive response (Line 5), informs the system of that
filler preference (Line 6).
Table 2: Single-Utterance dialogue
Spkr Speech Action Utterance
1 S: q. slot q. drink
2 U: i. slot filler i. drink d1
3 S: i. filler not avail i. not have d1
4 U: q. filler avail q. drink have d2
5 S: i. slot i. yes
6 U: i. slot filler i. drink d2
7 S: i. avail slot fillers i. burger have b1
Implementation in RL: The system uses RL to
learn which of the IS proposed actions to take. In
this domain we use a cost function based on dia-
logue length and the number of slots filled with an
available filler: C = Number of Utterances + 25 ?
unavailablyFilledSlots. In the present implemen-
tation the system?s bye utterance is costless. The
system chooses the action that minimizes the ex-
pected cost of the entire dialogue from the current
state.
The RL state for the speaker has seven vari-
ables:4 QUD-speaker, the stack of speakers who
have unresolved questions; Incorrect-Slot-Fillers,
4We experimented with a variety of RL States and this one
proved to be both small and effective.
180
a list of slot fillers (ordered chronologically on
when the user informed them) that are unavail-
able and have not been resolved; Last-Sys-Speech-
Action, the last speech action the system per-
formed; Given-Slot-Fillers, a list of slots that the
system has performed the inform available slot
filler action on; and three booleans variables, slot-
RL, that specify whether a slot has been filled cor-
rectly or not (e.g. Drink-RL).
5.2 User Types
We define three different types of users ? Experts,
Novices, and Intermediates. User types differ
probabilistically on two dimensions: slot knowl-
edge, and slot belief strength. We define experts to
have a 90 percent chance of having adequate filler
knowledge, intermediates a 50 percent chance,
and novices a 10 percent chance. These proba-
bilities are independent between slots. Slot belief
strength represents the user?s confidence that it has
adequate domain knowledge for the slot (i.e. the
top choice for that slot is available). It is either
a strong, warranted, or weak belief (Chu-Carroll
and Carberry, 1995). The intuition is that experts
should know when their top choice is available,
and novices should know that they do not know
the domain well.
Initial slot belief strength is dependent on user
type and whether their filler knowledge is ade-
quate (their initial top choice is available). Ex-
perts with adequate filler knowledge have a 70,
20, and 10 percent chance of having Strong, War-
ranted, and Weak beliefs respectfully. Similarly,
intermediates with adequate knowledge have a 50,
25, and 25 percent chance of the respective belief
strengths. When these user types have inadequate
filler knowledge the probabilities are reversed to
determine belief strength (e.g. Experts with inad-
equate domain knowledge for a slot have a 70%
chance of having a weak belief). Novice users al-
ways have a 10, 10, and 80 percent chance of the
respective belief strengths.
The user choses whether to use the query or
inform speech action based on the slot?s belief
strength. A strong belief will always result in an
inform, a warranted belief resulting in an inform
with p = 0.5, and weak belief will result in an in-
form with p = 0.25. If the user is informed of the
correct fillers by the system?s inform, that slot?s
belief strength is set to strong. If the user is in-
formed that a filler is not available, than that filler
is removed from the desired filler list and the belief
remains the same.5
5.3 Turn-Taking Models
We now discuss how turn-taking works for the
IDTB model and the two competing models that
we use to evaluate our approach. The system
chooses its turn action based on the RL state and
we add a boolean variable turn-action to the RL
State to indicate when the system is performing a
turn action or a speech action. The user uses belief
to choose its turn action.
Turn-Bidding: Agents bid for the turn at the
end of each utterance to determine who will speak
next. Each bid is represented as a value between 0
and 1, and the agent with the lower value (stronger
bid) wins the turn. This is consistent with the
use of utterance onset. There are 5 types of bids,
highest, high, middle, low, and lowest, which are
spread over a portion of the range as shown in Fig-
ure 1. The system uses RL to choose a bid and
a random number (uniform distribution) is gener-
ated from that bid?s range. The users? bids are de-
termined by their belief strength, which specifies
the mean of a Gaussian distribution, as shown in
Figure 1 (e.g Strong belief implies a ? = 0.35).
Computing bids in this fashion leads to, on av-
erage, users with strong beliefs bidding highest,
warranted beliefs bidding in the middle, and weak
beliefs bidding lowest. The use of the probabil-
ity distributions allows us to randomly decide ties
between system and user bids.
Figure 1: Bid Value Probability Distribution
Single-Utterance: The Single-Utterance (SU)
approach, as described in Section 2, has a rigid
5In this simple domain the next filler is guaranteed to be
available if the first is not. We do not model this with belief
strength since it is probably not representative of reality.
181
turn-taking mechanism. After a speaker makes a
single utterance the turn transitions to the listener.
Since the turn transitions after every utterance the
system must only choose appropriate utterances,
not turn-taking behavior. Similarly, user agents do
not have any turn-taking behavior and slot beliefs
are only used to choose between a query and an
inform.
Keep-Or-Release Model: The Keep-Or-
Release (KR) model, as described in Section
2, allows the speaker to either keep the turn to
make multiple utterances or release it. Taking the
same approach as English and Heeman (2005),
the system learns to keep or release the turn after
each utterance that it makes. We also use RL
to determine which conversant should begin the
dialogue. While the use of RL imparts some
importance onto the turn-taking behavior, it
is not influencing whether the system gets the
turn when it did not already have it. This is an
crucial distinction between KR and IDTB. IDTB
allows the conversants to negotiate the turn using
turn-bids motivated by importance, whereas in
KR only the speaker determines when the turn
can transition.
Users in the KR environment choose whether to
keep or release the turn similarly to bid decisions.6
After a user performs an utterance, it chooses the
slot that would be in the next utterance. A number,
k, is generated from a Gaussian distribution using
belief strength in the same manner as the IDTB
users? bids are chosen. If k ? 0.55 then the user
keeps the turn, otherwise it releases it.
5.4 Preliminary Turn-Bidding System
We described a preliminary turn-bidding system
in earlier work presented at a workshop (Selfridge
and Heeman, 2009). A major limitation was an
overly simplified user model. We used two user
types, expert and novice, who had fixed bids. Ex-
perts always bid high and had complete domain
knowledge, and the novices always bid low and
had incomplete domain knowledge. The system,
using all five bid types, was always able to out bid
and under bid the simulated users. Among other
things, this situation gives the system complete
control of the turn, which is at odds with the nego-
tiative nature of IDTB. The present contribution is
a more realistic and mature implementation.
6We experimented with a few different KR decision
strategies, and chose the one that performed the best.
6 Evaluation and Discussion
We now evaluate the IDTB approach by compar-
ing it against the two competing models: Single-
Utterance and Keep-Or-Release. The three turn-
taking approaches are trained and tested in four
user conditions: novice, intermediate, expert, and
combined. In the combined condition, one of the
three user types is randomly selected for each dia-
logue. We train ten policies for each condition and
turn-taking approach. Policies are trained using Q-
learning, and ?greedy search for 10000 epochs
(1 epoch = 100 dialogues, after which the Q-scores
are updated) with  = 0.2. Each policy is then
ran over 10000 test dialogues with no exploration
( = 0), and the mean dialogue cost for that pol-
icy is determined. The 10 separate policy values
are then averaged to create the mean policy cost.
The mean policy cost between the turn-taking ap-
proaches and user conditions are shown in Table 3.
Lower numbers are indicative of shorter dialogues,
since the system learns to successfully complete
the task in all cases.
Table 3: Mean Policy Cost for Model and User
condition7
Model Novice Int. Expert Combined
SU 7.61 7.09 6.43 7.05
KR 6.00 6.35 4.46 6.01
IDTB 6.09 5.77 4.35 5.52
Single User Conditions: Single user conditions
show how well each turn-taking approach can op-
timize its behavior for specific user populations
and handle slight differences found in those pop-
ulations. Table 3 shows that the mean policy cost
of the SU model is higher than the other two mod-
els which indicates longer dialogues on average.
Since the SU system must respond to every user
utterance and cannot learn a turn-taking strategy
to utilize user knowledge, the dialogues are neces-
sarily longer. For example, in the expert condition
the best possible dialogue for a SU interaction will
have a cost of five (three user utterances for each
slot, two system utterances in response). This cost
is in contrast to the best expert dialogue cost of
three (three user utterances) for KR and IDTB in-
teractions.
The IDTB turn-taking approach outperforms
the KR design in all single user conditions ex-
7SD between policies ? 0.04
182
cept for novice (6.09 vs. 6.00). In this condi-
tion, the KR system takes the turn first, informs
the available fillers for each slot, and then releases
the turn. The user can then inform its filler eas-
ily. The IDTB system attempts a similar dialogue
strategy by using highest bids but sometimes loses
the turn when users also bid highest. If the user
uses the turn to query or inform an unavailable
filler the dialogue grows longer. However, this is
quite rare as shown by small difference in perfor-
mance between the two models. In all other single
user conditions, the IDTB approach has shorter di-
alogues than the KR approach (5.77 and 4.35 vs.
6.35 and 4.46). A detailed explanation of IDTB?s
performance will be given in Section 6.1.
Combined User Condition: We next measure
performance on the combined condition that
mixes all three user types. This condition is more
realistic than the other three, as it better mimics
how a system will be used in actual practice. The
IDTB approach (mean policy cost = 5.52) outper-
forms the KR (mean policy cost = 6.01) and SU
(mean policy cost = 7.05) approaches. We also
observe that KR outperforms SU. These results
suggest that the more a turn-taking design can be
flexible and negotiative, the more efficient the dia-
logues can be.
Exploiting User bidding differences: It fol-
lows that IDTB?s performance stems from its ne-
gotiative turn transitions. These transitions are dis-
tinctly different than KR transitions in that there is
information inherent in the users bids. A user that
has a stronger belief strength is more likely to be
have a higher bid and inform an available filler.
Policy analysis shows that the IDTB system takes
advantage of this information by using moderate
bids ?neither highest nor lowest bids? to filter
users based on their turn behavior. The distribu-
tion of bids used over the ten learned policies is
shown in Table 4. The initial position refers to
the first bid of the dialogue; final position, the last
bid of the dialogue; and medial position, all other
bids. Notice that the system uses either the low or
mid bids as its initial policy and that 67.2% of di-
alogue medial bids are moderate. These distribu-
tions show that the system has learned to use the
entire bid range to filter the users, and is not seek-
ing to win or lose the turn outright. This behavior
is impossible in the KR approach.
Table 4: Bid percentages over ten policies in the
Combined User condition for IDTB
Position H-est High Mid Low L-est
Initial 0.0 0.0 70.0 30.0 0.0
Medial 20.5 19.4 24.5 23.3 12.3
Final 49.5 41.0 9.5 0.0 0.0
6.1 IDTB Performance:
In our domain, performance is measured by dia-
logue length and solution quality. However, since
solution quality never affects the dialogue cost for
a trained system, dialogue length is the only com-
ponent influencing the mean policy cost.
The primary cause of longer dialogues are un-
available filler inform and query (UFI?Q) utter-
ances by the user, which are easily identified.
These utterances lengthen the dialogue since the
system must inform the user of the available fillers
(the user would otherwise not know that the filler
was unavailable) and then the user must then in-
form the system of its second choice. The mean
number of UFI?Q utterance for each dialogue over
the ten learned policies are shown for all user con-
ditions in Table 5. Notice that these numbers are
inversely related to performance: the more UFI?
Q utterances, the worse the performance. For ex-
ample, in the combined condition the IDTB users
perform 0.38 UFI?Q utterances per dialogue (u/d)
compared to the 0.94 UFI?Q u/d for KR users.
While a KR user will release the turn if its planned
Table 5: Mean number of UFI?Q utterances over
policies
Model Novice Int. Expert Combined
KR 0.0 1.15 0.53 0.94
IDTB 0.1 0.33 0.39 0.38
utterance has a weak belief, it may select that weak
utterance when first getting the turn (either after a
system utterance or at the start of the dialogue).
This may lead to a UFI?Q utterance. The IDTB
system, however, will outbid the same user, result-
ing in a shorter dialogue. This situation is shown
in Tables 6 and 7. The dialogue is the same un-
til utterance 3, where the IDTB system wins the
turn with a mid bid over the user?s low bid. In the
KR environment however, the user gets the turn
and performs an unavailable filler inform, which
the system must react to. This is an instance of
the second deficiency of the KR approach, where
183
Table 6: Sample IDTB dialogue in Combined User
condition; Cost=6
Sys Usr Spkr Utt
1 low mid U: inform burger b1
2 h-est low S: inform burger have b3
3 mid low S: inform side have s1
4 mid h-est U: inform burger b3
5 mid high U: inform drink d1
6 l-est h-est U: inform side s1
7 high mid S: bye
Table 7: Sample KR dialogue in Combined User
condition; Cost=7
Agent Utt Turn-Action
1 U: inform burger b1 Release
2 S: inform burger have b3 Release
3 U: inform side s1 Keep
4 U: inform drink d1 Keep
5 U: inform burger b3 Release
6 S: inform side have s2 Release
7 U: inform side s2 Release
8 S: bye
the speaking system should not have released the
turn. The user has the same belief in both scenar-
ios, but the negotiative nature of IDTB enables a
shorter dialogues. In short, the IDTB system can
win the turn when it should have it, but the KR
system cannot.
A lesser cause of longer dialogues is an instance
of the first deficiency of the KR systems; the lis-
tening user cannot get the turn when it should have
it. Usually, this situation presents itself when the
user releases the turn, having randomly chosen the
weaker of the two unfilled slots. The system then
has the turn for more than one utterance, inform-
ing the available fillers for two slots. However,
the user already had a strong belief and available
top filler for one of those slots, and the system
has increased the dialogue length unnecessarily. In
the combined condition, the KR system produces
0.06 unnecessary informs per dialogue, whereas
the IDTB system produces 0.045 per dialogue.
The novice and intermediate conditions mirror this
(IDTB: 0.009, 0.076 ; KR: 0.019, 0.096 respect-
fully), but the expert condition does not (IDTB:
0.011, KR: 0.0014). In this case, the IDTB system
wins the turn initially using a low bid and informs
one of the strong slots, whereas the expert user ini-
tiates the dialogue for the KR environment and un-
necessary informs are rarer. In general, however,
the KR approach has more unnecessary informs
since the KR system can only infer that one of the
user?s beliefs was probably weak, otherwise the
user would not have released the turn. The IDTB
system handles this situation by using a high bid,
allowing the user to outbid the system as its con-
tribution is more important. In other words, the
IDTB user can win the turn when it should have it,
but the KR user cannot.
7 Conclusion
This paper presented the Importance-Driven Turn-
Bidding model of turn-taking. The IDTB model is
motivated by turn-conflict studies showing that the
interest in holding the turn influences conversant
turn-cues. A computational prototype using Re-
inforcement Learning to choose appropriate turn-
bids performs better than the standard KR and SU
approaches in an artificial collaborative dialogue
domain. In short, the Importance-Driven Turn-
Bidding model provides a negotiative turn-taking
framework that supports mixed-initiative interac-
tions.
In the previous section, we showed that the KR
approach is deficient for two reasons: the speak-
ing system might not keep the turn when it should
have, and might release the turn when it should
not have. This is driven by KR?s speaker-centric
nature; the speaker has no way of judging the
potential contribution of the listener. The IDTB
approach however, due to its negotiative quality,
does not have this problem.
Our performance differences arise from situa-
tions when the system is the speaker and the user
is the listener. The IDTB model also excels in the
opposite situation, when the system is the listener
and the user is the speaker, though our domain is
not sophisticated enough for this situation to oc-
cur. In the future we hope to develop a domain
with more realistic speech acts and a more diffi-
cult dialogue task that will, among other things,
highlight this situation. We also plan on imple-
menting a fully functional IDTB system, using an
incremental processing architecture that not only
detects, but generates, a wide array of turn-cues.
Acknowledgments
We gratefully acknowledge funding from the
National Science Foundation under grant IIS-
0713698.
184
References
J.E Allen, C.I. Guinn, and Horvitz E. 1999. Mixed-
initiative interaction. IEEE Intelligent Systems,
14(5):14?23.
Jennifer Chu-Carroll and Sandra Carberry. 1995. Re-
sponse generation in collaborative negotiation. In
Proceedings of the 33rd annual meeting on Asso-
ciation for Computational Linguistics, pages 136?
143, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can i finish? learning when to respond to incre-
mental interpretation results in interactive dialogue.
In Proceedings of the SIGDIAL 2009 Conference,
pages 11?20, London, UK, September. Association
for Computational Linguistics.
S.J. Duncan and G. Niederehe. 1974. On signalling
that it?s your turn to speak. Journal of Experimental
Social Psychology, 10:234?247.
S.J. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23:283?292.
M. English and Peter A. Heeman. 2005. Learning
mixed initiative dialog strategies by using reinforce-
ment learning on both conversants. In Proceedings
of HLT/EMNLP, pages 1011?1018.
G. Ferguson, J. Allen, and B. Miller. 1996. TRAINS-
95: Towards a mixed-initiative planning assistant.
In Proceedings of the Third Conference on Artificial
Intelligence Planning Systems (AIPS-96), pages 70?
77.
A. Gravano and J. Hirschberg. 2009. Turn-yielding
cues in task-oriented dialogue. In Proceedings of the
SIGDIAL 2009 Conference: The 10th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 253?261. Association for Compu-
tational Linguistics.
C.I. Guinn. 1996. Mechanisms for mixed-initiative
human-computer collaborative discourse. In Pro-
ceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 278?285. As-
sociation for Computational Linguistics.
P.A. Heeman. 2007. Combining reinforcement learn-
ing with information-state update rules. In Pro-
ceedings of the Annual Conference of the North
American Association for Computational Linguis-
tics, pages 268?275, Rochester, NY.
Gudny Ragna Jonsdottir, Kristinn R. Thorisson, and
Eric Nivel. 2008. Learning smooth, human-like
turntaking in realtime dialogue. In IVA ?08: Pro-
ceedings of the 8th international conference on In-
telligent Virtual Agents, pages 162?175, Berlin, Hei-
delberg. Springer-Verlag.
S. Larsson and D. Traum. 2000. Information state and
dialogue managment in the trindi dialogue move en-
gine toolkit. Natural Language Engineering, 6:323?
340.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1):11 ? 23.
A. Raux and M. Eskenazi. 2009. A finite-state turn-
taking model for spoken dialog systems. In Pro-
ceedings of HLT/NAACL, pages 629?637. Associa-
tion for Computational Linguistics.
H. Sacks, E.A. Schegloff, and G. Jefferson. 1974. A
simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
R. Sato, R. Higashinaka, M. Tamoto, M. Nakano, and
K. Aikawa. 2002. Learning decision trees to de-
termine turn-taking by spoken dialogue systems. In
ICSLP, pages 861?864, Denver, CO.
E.A. Schegloff. 2000). Overlapping talk and the orga-
nization of turn-taking for conversation. Language
in Society, 29:1 ? 63.
E. O. Selfridge and Peter A. Heeman. 2009. A bidding
approach to turn-taking. In 1st International Work-
shop on Spoken Dialogue Systems.
G. Skantze and D. Schlangen. 2009. Incremental di-
alogue processing in a micro-domain. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 745?753. Association for Computational Lin-
guistics.
N. Stro?m and S. Seneff. 2000. Intelligent barge-in in
conversational systems. In Sixth International Con-
ference on Spoken Language Processing. Citeseer.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
S. Sutton, D. Novick, R. Cole, P. Vermeulen, J. de Vil-
liers, J. Schalkwyk, and M. Fanty. 1996. Build-
ing 10,000 spoken-dialogue systems. In ICSLP,
Philadelphia, Oct.
M. Walker and S. Whittaker. 1990. Mixed initiative
in dialoge: an investigation into discourse segmen-
tation. In Proceedings of the 28th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 70?76.
M. Walker, D. Hindle, J. Fromer, G.D. Fabbrizio, and
C. Mestel. 1997. Evaluating competing agent
strategies for a voice email agent. In Fifth European
Conference on Speech Communication and Technol-
ogy.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue?. Computer Speech
Language, 24(2):175 ? 189.
185
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 249?252,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Autism and Interactional Aspects of Dialogue
Peter A. Heeman, Rebecca Lunsford, Ethan Selfridge, Lois Black, and Jan van Santen
Center for Spoken Language Understanding
Oregon Health & Science University
heemanp@ohsu.edu
Abstract
Little research has been done to explore
differences in the interactional aspects of
dialogue between children with Autis-
tic Spectrum Disorder (ASD) and those
with typical development (TD). Quantify-
ing the differences could aid in diagnosing
ASD, understanding its nature, and better
understanding the mechanisms of dialogue
processing. In this paper, we report on a
study of dialogues with children with ASD
and TD. We find that the two groups differ
substantially in how long they pause be-
fore speaking, and their use of fillers, ac-
knowledgments, and discourse markers.
1 Introduction
Autism Spectrum Disorders (ASD) form a group
of severe neuropsychiatric conditions whose fea-
tures can include impairments in reciprocal social
interaction and in communication (APA, 2000).
These impairments may take different forms,
ranging from individuals with little or no com-
munication to fully verbal individuals with fluent,
grammatically correct speech. In this latter verbal
group, shortcomings in communication have been
noted, including using and processing social cues
during conversations. This is no surprise, since
negotiating a conversation requires many abilities,
several of which are generally impaired in ASD,
such as generating appropriate prosody (Kanner,
1943) and ?theory of mind? (Baron-Cohen, 2000).
We make a distinction between transactional
and interactional aspects of dialogue (Brown and
Yule, 1983). The transactional aspect refers to
message content and interactional focuses on ex-
pressing social relations and personal attitudes.
In this paper, we focus on surface behaviors
that speakers use to help manage the interaction,
namely turn-taking, and the use of fillers, dis-
course markers, and acknowledgments. One ad-
vantage of these behaviors is that they do not re-
quire complete understanding of the dialogue, and
thus lend themselves to automatic analysis. In
addition, these behaviors are under the speaker?s
control and should be robust to what the other
speaker is doing. We hypothesize that just as in-
teractional aspects in general are affected in ASD,
so are these surface behaviors. However, to our
knowledge, little or no work has been done on this.
Investigating how the interactional aspects of
dialogue are affected in ASD serves several pur-
poses. First, it can help in the diagnostic process.
Currently, diagnosing ASD is subjective. Objec-
tive measures based on dialogue interaction could
improve the reliability of the diagnostic process.
Second, it can help us refine the behavioral phe-
notypes of ASD, which is critical for progress on
the basic science front. Third, it can help us re-
fine therapy for people with ASD to address di-
alogue interaction deficits. Fourth, understand-
ing what dialogue aspects are affected in high-
functioning verbal children with ASD can help de-
termine which aspects of dialogue are primarily
social in nature. For example, do speakers use
fillers to signal that there is a communication prob-
lem, or are fillers a symptom of it (cf. Clark and
Fox Tree, 2002)?
In this paper, we report on a study of interac-
tional aspects of dialogues between clinicians and
children with ASD. The dialogues were recorded
during administration of the Autism Diagnostic
Observation Schedule (Lord et al, 2000), which
is an instrument used to assist in diagnosing ASD.
We compare the performance of these children
with a group of children with typical development
(TD).
2 Data
The data used in this paper was collected dur-
ing administration of the ADOS on 22 TD chil-
dren and 26 with ASD, ranging in age from 4 to
8 years old. The children with ASD were high-
functioning and verbal. The speech of the clini-
cian and child was transcribed into utterance-like
units, with a start and an end time. Activities were
annotated in a separate tier. The transcriptions in-
cluded the punctuation marks ?.?, ?!?, and ??? to
mark syntactically and semantically complete sen-
249
tences, and ?>? to mark incomplete ones. As a sin-
gle audio channel was used, the timing of overlap-
ping speech was marked as best as possible. Each
child on average said 2221 words, 574 utterances,
and 316 turns.
3 Results
Pauses between Turns: We first examine how
long children wait before starting their turn. We
hypothesized that children with ASD would wait
longer on average to respond, either because they
are less aware of (a) the turn-taking cues, (b) the
social obligation to minimize inter-turn pauses, or
(c) they have a slower processing and response
times. For this analysis, we look at all turns in
which there is no overlap between the beginning of
the child?s turn and the clinician?s speech. Data is
available on 4412 pauses for the TD children and
5676 for the children with ASD. The grand means
of the children?s pauses are shown in Table 1 along
with the standard deviations. The TD children?s
average pause length is 0.876s. For the children
with ASD, it is 1.115s, 27.3% longer. This dif-
ference is significant, a-priori independent t-test
t=2.34 (df=39), p<.02 one-tailed.
TD ASD
all 0.876 (0.24) 1.115 (0.45)
after question 0.748 (0.25) 1.005 (0.40)
after non-question 1.076 (0.37) 1.329 (0.74)
Table 1: Pauses before new turns.
We also examine the pauses following ques-
tions by the clinician versus non-questions. Ques-
tions are interesting as they impose a social obli-
gation for the child to respond, and they have
strong prosodic cues at their ending. We identified
questions as utterances transcribed with a ques-
tion mark, which might include rhetorical ques-
tions. After a non-question (e.g., a statement), the
average pause is 1.076s for the TD children and
1.329s for children with ASD. This difference is
not statistically significant by independent t-test,
t<1.6, NS. After a question, the average pause
is 0.748s for the TD children and 1.005s for the
children with ASD, a significant difference by a-
priori independent t-test t=2.72 (df=42), p<.005
one-tailed. The ASD children on average take
34.4% longer to respond. Thus, after a question,
the difference between children with TD and ASD
is more pronounced.
Pauses by Activity: The ADOS includes hav-
ing the child engage in different activities. For
this research, we collapse the activities into three
types: converse is when there is no non-speech
task; describe is when the child is doing a men-
tal task, such as describing a picture; and play is
when the child is interacting with the clinician in
a play session. To better understand the difference
between questions and non-questions, we examine
the pauses in each activity (Table 2).
TD ASD
question non-ques. question non-ques.
converse 0.730 0.30 0.656 0.27 0.890 0.34 0.932 0.88
describe 0.853 0.44 0.879 0.37 1.056 0.51 1.282 1.21
play 0.720 0.34 1.825 0.78 1.289 1.51 1.887 1.37
Table 2: Pauses for each type of activity.
After a question, the TD children tend to re-
spond with similar pauses in each activity (the dif-
ferences in column 2 between activities are not
significant by pairwise paired t-test, all t?s<1.6,
NS). After a question, the child has a social obli-
gation to respond, and this does not seem to be
overridden by whether there is a separate task
they are involved in. Even after a non-question,
conversants have a social obligation to keep the
speaking floor occupied and so to minimize inter-
utterance pauses (Sacks et al, 1974). However, as
seen in the third column, the pauses are affected
by the type of activity, and the differences are
statistically significant by pairwise paired t-test,
(df=21), two-tailed: converse-describe t=2.24,
p<.04; describe-play t=5.68, p<.0001; converse-
play t=6.87, p<.0001. The biggest difference is
with play. Here, it seems that the conversants
physical interaction lessens the social obligation
of maintaining the speaking floor. These findings
are interesting for social-linguistics as it suggests
that the social obligations of turn-taking are al-
tered by the presence of a non-speech task.
We next compare the children with ASD to the
TD children. For the converse activity, we see that
the children with ASD take longer to respond, af-
ter questions and non-questions. The difference
after questions is significant by independent t-test,
t=1.74 (df=46) p<.05, one-tailed, whereas the dif-
ference after non-questions is marginal, t=1.47
(df=28) p<.08. This result could be explained by
the slower processing and response times associ-
ated with ASD.
Just as with the TD children, we see that after
a non-question, the children with ASD take longer
to respond when there is another task. The differ-
ences in pause lengths between converse and play
are significant, by paired t-test, t=2.89 (df=23)
250
p<.009, two-tailed. The difference between de-
scribe and play is marginal, t=2.03 (df=25) p<.06,
and there was no significant difference between
converse and describe, t<1, NS.
After a question, the children with ASD take
longer to respond when there is another task, espe-
cially for play, although the pairwise differences in
pause length between activities are not significant.
This suggests that the children with ASD become
distracted when there is another task, and so be-
come less sensitive to either the question prosody
or the social obligation of questions.
Fillers: We next examine the rate of fillers, at
the beginning of turns, beginning of utterances,
and in the middle of utterances. We look at these
contexts individually as fillers can serve different
roles, such as turn-taking, stalling for time or as
part of a disfluency, and their role is correlated
to their position in a turn. The rates are reported
in Table 3, along with the total number of fillers
within each category. Interestingly, the rate of ?uh?
between children with TD and ASD is similar for
all positions (independent t-test, all g?s<1, NS).
uh um
TD ASD TD ASD
turn init. 1.70% 112 1.84% 159 3.86% 243 1.65% 146
utt. init. 1.31% 43 1.20% 33 2.29% 73 0.52% 10
utt. medial 0.25% 103 0.31% 137 1.03% 492 0.21% 123
Table 3: Rate of fillers.
The more interesting finding, though, is in the
usage of ?um?. Children with ASD use it signifi-
cantly less than the TD children in every position,
from 1/2 the rate in turn-initial position to 1/5 in
utterance-medial position, independent two-tailed
t-test: turn initial t=2.74 (df=38), p<.01; utterance
initial t=2.53 (df=31), p<.02; and utterance me-
dial t=3.94 (df=24), p<.001.
TD ASD
converse 1.76% 569 0.56% 190
describe 1.15% 115 0.33% 31
play 0.96% 124 0.45% 58
Table 4: Use of ?um? by activity.
We also examined the overall usage of ?um? in
each activity (Table 4). The TD children use ?um?
more often in each activity than the children with
ASD, and the differences are statistically signif-
icant by independent two-tailed t-test: converse
t=3.62 (df=29), p<.002; describe t=2.83 (df=27),
p<.01; play t=2.42 (df=33), p<.03. This result
supports the robustness of the findings about ?um?.
Many researchers have speculated on the role
of ?um? and ?uh?. In recent work, Clark and Fox
Tree (2002) argued that they signal a delay, and
that ?um? signals more delay than ?uh?. They view
both as linguistic devices that are planned for, just
as any other word is. Our work suggests that ?um?
and ?uh? arise from different cognitive processes,
and that the process that accounts for ?uh? is not
affected by ASD, while the process for ?um? is.1
Acknowledgments: We next look at the rate of
acknowledgments: single word utterances that are
used to show agreement or understanding. Thus,
the use of acknowledgments requires awareness of
the other person?s desire to ensure mutual under-
standing. As the corpus did not have these words
explicitly marked, we identify a word as an ac-
knowledgment if it meets the following criteria:
(a) it is one of the words listed in Table 5 (based
on Heeman and Allen, 1999); (b) it is first in the
speaker?s turn; and (c) it does not follow a question
by the clinician. The TD children used acknowl-
edgments in 17.42% of their turns that did not fol-
low a question, while the children with ASD did
this only 13.39% of the time (Table 5), a statis-
tically significant difference by a-priori indepen-
dent t-test t=1.78 (df=46), p<.05 one-tailed.
TD ASD
total 17.42% 568 13.39% 459
yeah 7.49% 248 5.87% 215
no 2.78% 78 2.06% 63
mm-hmm 2.06% 75 1.07% 35
mm 0.99% 29 1.35% 42
ok 1.87% 65 0.83% 27
yes 0.92% 32 0.88% 32
right 0.14% 5 0.23% 8
hm 0.73% 21 0.69% 20
uh-huh 0.44% 15 0.42% 17
Table 5: Use of acknowledgments.
Discourse Markers: We next examine dis-
course markers, which are words such as ?well?
and ?oh? that express how the current utterance
relates to the discourse context (Schiffrin, 1987).
We classified a word as a discourse marker if it
was the first word in an utterance and is one of
the words in Table 6 (Heeman and Allen, 1999).
As shown in Table 6, the children with ASD use
discourse markers significantly less than the TD
children in both conditions by a-priori indepen-
dent, one-tailed t-test: turn-initial t=3.24 (df=43)
p<.002; utterance-initial t=4.01 (df=44) p<.0001.
1In Lunsford et al (2010) we investigate the rate and
length of pauses after ?uh? and ?um?. In addition, we veri-
fied the t-tests using Wilcoxon rank sum tests.
251
As can be seen, most of the difference is in the use
of ?and?. The data for the other discourse markers
was sparse, so we compared ?and? against all of the
others combined. The decreased usage of ?and?
in the ASD children is statistically significant
for both conditions by a-priori independent, one-
tailed t-test: turn-initial t=4.47 (df=30), p<.0001;
utterance-initial t=3.79 (df=43), p<.0002. There
is little difference in the use of all of the other dis-
course markers combined, and the difference is not
statistically significant.
Turn Initial Utterance Initial
TD ASD TD ASD
all 19.2% 1290 12.8% 1196 28.7% 2053 19.4% 1330
and 10.7% 731 5.0% 471 19.5% 1419 12.0% 844
then 0.6% 38 1.0% 89 1.5% 97 1.4% 79
but 2.1% 144 1.3% 113 3.6% 238 2.7% 194
well 2.2% 143 2.7% 271 1.1% 74 1.2% 79
oh 2.0% 135 1.8% 160 1.0% 67 1.3% 68
so 1.2% 75 0.7% 60 1.6% 129 0.7% 49
wait 0.2% 9 0.2% 21 0.2% 17 0.2% 15
actually 0.2% 15 0.1% 11 0.2% 12 0.0% 2
not and 8.5% 559 7.8% 725 9.2% 634 7.4% 486
Table 6: Use of discourse markers.
The use of ?and? is also lower in each activity
for the ASD children (Table 7), a significant dif-
ference by a-priori independent one-tailed t-test:
converse t=3.00 (df=41), p<.003; describe t=4.79
(df=38), p<.0001, play t=4.07 (df=30), p<.0002.
TD ASD
converse 13.36% 1139 7.95% 755
describe 21.77% 587 10.76% 339
play 12.97% 424 5.18% 221
Table 7: Use of ?and? in each activity.
One explanation for the decreased usage of
?and? and not the other discourse markers might
be that, of all the discourse markers, ?and? seems
to have the least meaning. It simply signifies
that there is some continuation between the new
speech and the previous context. This might make
it difficult for children with ASD to learn its use. A
second explanation is that the children with ASD
are using ?and? correctly, but simply do not pro-
duce as many utterances that are related to the pre-
vious context (cf. Bishop et al, 2000).
4 Conclusion
In this paper, we examined a number of interac-
tional aspects of dialogue in the speech of children
with ASD and TD. We found that children with
ASD have a lower rate of the filler ?um?, acknowl-
edgments, and the discourse marker ?and.? We also
found that in certain situations, they take longer to
respond. These deficits might prove useful for im-
proved diagnosis of ASD. We also found that chil-
dren with ASD have a lower rate of ?um? but not
of ?uh?, and that only the discourse marker ?and?
seems to be affected. This might prove useful for
both better understanding the nature of ASD as
well as better understanding the role of these phe-
nomena in dialogue. Although the results reported
in this work are preliminary, they do show the po-
tential of our approach. More work is needed to
ensure that our automatic identification of turn-
taking events, discourse markers, and acknowl-
edgments is correct and to explore alternate expla-
nations for the results that we observed.
Acknowledgments
Funding gratefully received from the National In-
stitute of Heath under grants IR21DC010239 and
5R01DC007129, and the National Science Foun-
dation under IIS-0713698. The views herein are
those of the authors and reflect the views neither
of the funding agencies.
References
American Psychiatric Association, Washington DC,
2000. Diagnostic and Statistical Manual of Mental
Disorders, 4th Edition, Text Revision (DSM-IV-TR).
S. Baron-Cohen. 2000. Theory of mind and autism:
A review. In L. M. Glidden, editor, International
Review of Research in Mental Retardation, volume
23: Autism, pages 170?184. Academic Press.
D. Bishop et al 2000. Conversational responsive-
ness in specific language impairment: Evidence of
disproportionate pragmatic difficulties in a subset
of children. Development and Psychopathology,
12(2):177?199.
G. Brown and G. Yule. 1983. Discourse Analysis.
Cambridge University Press.
H. Clark and J. Fox Tree. 2002. Using uh and um in
spontaneous speaking. Cognition, 8:73?111.
P. Heeman and J. Allen. 1999. Speech repairs, in-
tonational phrases and discourse markers: Model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?572.
L. Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
C. Lord et al 2000. The autism diagnostic observa-
tion schedule-generic: a standard measure of social
and communication deficits associted with the spec-
trum of autium. Journal of Austism Developmental
Disorders, 30(3):205?223, June.
R. Lunsford et al 2010. Autism and the use of fillers:
differences between ?um? and ?uh?. In 5th Workshop
on Disfluency in Spontaneous Speech, Tokyo.
H. Sacks, E. Schegloff, and G. Jefferson. 1974. A sim-
plest systematics for the organization of turn-taking
for conversation. Language, 50(4):696?735.
D. Schiffrin. 1987. Discourse Markers. Cambridge
University Press, New York.
252
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 110?119,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Stability and Accuracy in Incremental Speech Recognition
Ethan O. Selfridge?, Iker Arizmendi?, Peter A. Heeman?, and Jason D. Williams?
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR
?AT&T Labs ? Research, Shannon Laboratory, Florham Park, NJ
{selfridg,heemanp}@ohsu.edu {iker,jdw}@research.att.com
Abstract
Conventional speech recognition ap-
proaches usually wait until the user
has finished talking before returning a
recognition hypothesis. This results in
spoken dialogue systems that are unable
to react while the user is still speaking.
Incremental Speech Recognition (ISR),
where partial phrase results are returned
during user speech, has been used to
create more reactive systems. However,
ISR output is unstable and so prone to
revision as more speech is decoded. This
paper tackles the problem of stability
in ISR. We first present a method that
increases the stability and accuracy of
ISR output, without adding delay. Given
that some revisions are unavoidable,
we next present a pair of methods for
predicting the stability and accuracy of
ISR results. Taken together, we believe
these approaches give ISR more utility for
real spoken dialogue systems.
1 Introduction
Incremental Speech Recognition (ISR) enables a
spoken dialogue system (SDS) to react quicker
than when using conventional speech recogni-
tion approaches. Where conventional methods
only return a result after some indication of user
completion (for example, a short period of si-
lence), ISR returns partial phrase results while
the user is still speaking. Having access to a real-
time stream of user speech enables more natural
behavior by a SDS, and is a foundation for cre-
ating systems which take a more active role in
conversations.
Research by Fink et al(1998) and Skantze
& Schlangen (2009), among others, has demon-
strated the efficacy of ISR but has also drawn
attention to a significant obstacle to widespread
use: partial phrase results are generally unsta-
ble and so, as more speech is decoded, are prone
to revision. For example, the ISR component in
a bus information SDS may return the partial
?leaving from Hills?, where ?Hills? is a neigh-
borhood name. It may then return the revi-
sion ?leaving from Pittsburgh?, which the sys-
tem must handle gracefully. Given this propen-
sity to revise, a Stability Measure (SM) ? like-
lihood of a partial result remaining unchanged
compared to the final result ? is necessary for
optimal incremental system behavior. Further-
more, since a stable partial may still be inaccu-
rate, a Confidence Measure (CM) ? likelihood
of partial correctness ? is also necessary.
Effective ISR enables systems to participate in
more dynamic turn-taking. For instance, these
two measures would enable an SDS to identify
inaccurate recognition results while the user is
still speaking. The SDS could then interrupt
and prompt the user to start again. On the
other hand, ISR allows systems to handle pauses
gracefully. If the SDS recognizes that an utter-
ance is incomplete (though stable and accurate),
it could give the user more time to speak before
reacting.
We present two contributions specific to the
use of ISR. First, we characterize three ap-
proaches to ISR which make different trade-offs
between stability and the number of partials
generated. We then present a novel hybrid ap-
proach that combines their strengths to increase
110
stability without adding latency. However, even
with this method, some partial results are still
later revised. The second contribution of the
paper is to present a pair of methods which pre-
dict the stability and accuracy of each partial
result. These two measures are designed for use
in concert by dialogue systems, which must de-
cide whether to act on each partial result in real
time.
2 Background and Related Work
We now describe modern speech recognition
methodology, the production of partial phrase
results, and the advantages and deficiencies of
ISR. In this we seek only to provide a topical
foundation, and not a comprehensive review.
Most modern speech recognition engines use
Hidden-Markov Models and the Viterbi algo-
rithm to decode words from audio. Decod-
ing employs three models: an acoustic model,
which assigns probabilities to speech audio given
a phone; a lexicon, which specifies phone se-
quences for a word; and a language model, which
specifies the probability of a word sequence. The
aim of the decoding process is to find the N most
probable word sequences given the audio spoken
and these three models.
Two useful but different forms of language
models are commonly used in spoken dialogue
systems. A Rule-based Language Model (RLM)
specifies a list of valid sentences which may be
recognized, usually via expansion rules. By con-
trast, a Statistical Language Model (SLM) spec-
ifies a vocabulary of words, allowing arbitrary
sentences to be formed. Both models specify
probabilities over their respective sets ? RLMs
via whole-sentence probabilities, and SLMs via
probabilities of short word sequences called N-
grams. In an SLM, special word symbols are
used to represent the beginning and end of the
phrase, so the probability of beginning or ending
phrases with words can be modeled.
As speech frames are received, the recognizer
builds up a lattice which compactly describes the
probable sequences of words decoded from the
audio. In conventional turn-based speech recog-
nition, decoding continues until the user finishes
speaking. Once the user has finished, the engine
searches the lattice for the most probable word
sequence and returns this to the dialogue man-
ager. By contrast, in ISR the engine inspects
the lattice as it is being built, and returns partial
results to the dialogue manager as they become
available. A key issue for ISR is that partial
results may later be revised, because as more
speech is received and the lattice is extended, a
different path may become the most probable.
In other words, partial results are unstable in
the sense that they may later be revised. Note
that stability is not the same as accuracy: a par-
tial result may be accurate (correct so far) but
unstable, because it is later revised. Similarly, a
stable result may not be accurate.
In the literature, ISR has been proposed for
dialogue systems to enable them to engage in
more natural, human-like interactions. Stud-
ies have shown that incremental systems react
faster than non-incremental ones, and are well-
liked by users because of their naturalness (Aist
et al, 2007; Skantze and Schlangen, 2009). Aist
et al (2007) found that incremental speech
recognition yielded 20% faster task completion.
Moreover, adding ISR improved users? satisfac-
tion with the interaction; the authors attributed
this improvement to ?naturalness?: ?incremen-
tal systems are more like human-human con-
versation than their non-incremental counter-
parts.? Skantze & Schlangen (2009) observed a
similar trend, finding that an incremental sys-
tem was ?clearly preferred? since it ?was ex-
perienced as more pleasant and human-like?,
though it did not actually outperform the non-
incremental system in a number dictation task.
Some recent work has focused on incremen-
tal natural language understanding (NLU). De-
Vault et al (2009) showed that when using a
relatively small number of semantic possibili-
ties the correct interpretation could be predicted
by early incremental results. Schlangen et al
(2009) demonstrated that an incremental refer-
ence resolver could identify the correct reference
out of 12 more than 50% of the time. This
type of NLU can use context and other infor-
mation to be somewhat resilient to errors, and
word recognition inaccuracies may not yield a
111
change in understanding. In this paper we focus
on improving accuracy and stability at the word
level; we belief that improvements at the word
level are likely to improve performance at the
understanding level, although we do not evalu-
ate this here.
A number of researchers have described meth-
ods for evaluating and improving the stability of
ISR results (Baumann et al, 2009; Fink et al,
1998). Baumann, Atterer, & Schlangen spoke
directly to stability by comparing partial phrase
results against the ?final hypothesis produced
by the ASR?. They show that increasing the
amount of ?right context? ? the amount of
speech after the end of the putative partial result
? increases the stability of the partials. Fink et
al. (1998) also used a right context delay to de-
crease the word error rate of ISR results.
A key limitation of these past efforts to im-
prove stability is that adding right context nec-
essarily incurs delay, which degrades responsive-
ness and erodes the overall benefits of ISR. Fur-
thermore, past work has not addressed the prob-
lem of identifying which partials are likely to be
revised. In this paper, we tackle both of these
problems. We first present a method for im-
proving stability by considering features of the
lattice itself, without incurring the delay asso-
ciated with adding right context. Additionally,
since some partials will still be revised, we then
propose a method of scoring the stability of par-
tial speech recognition results.
3 Three approaches to ISR
We now describe three approaches to ISR: Ba-
sic, Terminal, and Immortal. Basic ISR simply
returns the most likely word sequence observed
after some number of speech frames has been de-
coded (in our case every 3 frames or 30ms). This
is the least restrictive approach, and we believe
is the method used by recent ISR research.
Terminal ISR, a more restrictive approach,
finds a partial result if the most likely path
through the (partially-decoded) lattice ends at
a terminal node in the language model. The in-
tuition is that if a partial result finishes a com-
plete phrase expected by the language model,
it is more likely to be stable. The meaning of
terminal is slightly different for rule-based lan-
guage models (RLMs) and statistical language
models (SLMs). For a rule-based grammar,
the terminal node is simply one that ends a
valid phrase (?Pittsburgh? in ?leaving from Pitts-
burgh?). For an SLM, a terminal node indicates
that the most likely successor state is the spe-
cial end-of-sentence symbol. In other words, in
an SLM Terminal partial result, the language
model assigns the highest probability to ending
the phrase.
A third method, Immortal ISR, is the most
restrictive method (Spohrer et al, 1980). If all
paths of the lattice come together into a node
? called an immortal node ? then the lattice
structure before that node will be unchanged by
any subsequent decoding. This structure guar-
antees that the best word sequence prior to an
immortal node is stable. Immortal ISR operates
identically for both RLMs and SLMs.1
To compare these approaches we evaluate
their performance. Utterances were extracted
from real calls to the Carnegie Mellon ?Lets
Go!? bus information system for Pittsburgh,
USA (Raux et al, 2005; Parent and Eskenazi,
2009). We chose this domain because this cor-
pus is publicly available, and this domain has
recently been used as a test bed for dialogue
systems (Black et al , 2010). The AT&T WAT-
SON speech recognition engine was used, modi-
fied to output partials as described above (Goffin
et al, 2005). We tested these three approaches
to ISR on three different recognition tasks. The
first two tasks used rule-based language models
(RLM), and the third used a statistical language
model (SLM).
The two rule-based language models were de-
veloped for AT&T ?Let?s Go? dialogue sys-
tem, prior to its deployment (Williams et al
, 2010). The first RLM (RLM1) consisted
1The choice of search beam size affects both accuracy
and the number of immortal nodes produced: a smaller
beams yields a sparser lattice with more immortal nodes
and lower accuracy; a larger beam yields a richer lattice
with fewer immortal nodes and higher accuracy. In this
work we used our recognizer?s default beam size, which
allows recognition to run in less than real time and yields
near-asymptotic accuracy for all experiments.
112
of street and neighborhood names, built from
the bus timetable database. The second RLM
(RLM2) consisted of just neighborhood names.
Utterances to test RLM1 and RLM2 were se-
lected from the corpus provided by Carnegie
Mellon to match the expected distribution of
speech at the dialogue states where RLM1 and
RLM2 would be used. RLM1 was evaluated on
a set of 7722 utterances, and RLM2 on 5411 ut-
terances. To simulate realistic use, both RLM
test sets were built so that 80% of utterances
are in-grammar, and 20% are out-of-grammar.
The SLM was a 3-gram trained on a set of 140K
utterances, and is tested on a set of 42620 ut-
terances.
In past work, Raux et al (2005) report word
error rates (WERs) of 60-68% on data from the
same dialogue system, though on a different set
of utterances. By comparison, our SLM yields
a WER of 35%, which gives us some confidence
that our overall recognition accuracy is compet-
itive, and that our results are relevant.
Table 1 provides a few statistics of the LMs
and test sets, including whole-utterance accu-
racy, computed using an exact string match.
Results are analyzed in two groups: All, where
all of the utterances are analyzed, and Multi-
Word (MW), where only utterances whose tran-
scribed speech (what was actually said) has
more than one word. Intuitively, these utter-
ances are where ISR would be most effective.
That said, ISR is beneficial for both short and
long utterances ? for example, ISR systems
can react faster to users regardless of utterance
length.
ISR was run using each of the three ap-
proaches (Basic, Terminal, Immortal) in each of
the three configurations (RLM1, RLM2, SLM).
The mean number of partials per utterance is
shown in Table 2. For all ISR methods, the more
flexible SLM produces more partials than the
RLMs. Also as expected, multi-word utterances
produce substantially more partials per utter-
ance than when looking at the entire utterance
set. The Basic approach produces nearly dou-
ble the number of partials than Terminal ISR
does, and Immortal ISR production highlights
its primary weakness: in many utterances, no
Table 1: Statistics for Recognition Tasks. In all ta-
bles, All refers to all utterances in a test set, and
MW refers to the subset of multi-word utterances in
a test set.
RLM1 RLM2 SLM
Num. Utts All 7722 5411 42620
Num. Utts MW 3213 1748 20396
Words/Utt All 1.7 1.5 2.3
Words/Utt MW 2.8 2.6 3.8
Utt. Acc. All. 50 % 60 % 62 %
Utt. Acc. MW 53 % 56 % 44 %
immortal nodes are found. Given this however,
immortal node occurrence is directly related to
the number of words, as indicted by the greater
number of immortal partials in multi-word ut-
terances.
Stability is assessed by comparing the partial
to the final recognition result. For simplicity, we
restrict our analysis to 1-Best hypotheses. If the
partial 1-Best hypothesis is a prefix (or full ex-
act match) of the final 1-Best hypothesis then it
is considered stable. For instance, if the partial
1-Best hypothesis is ?leaving from Forbes? then
it would be stable if the final 1-Best is ?leaving
from Forbes? or ?leaving from Forbes and Mur-
ray? but not if it is ?from Forbes and Murray? or
?leaving?. Accuracy is assessed similarly except
that the transcribed reference is used instead of
the final recognition result.
We report stability and accuracy in Table 3.
Immortal partials are excluded from stability
since they are guaranteed to be stable. The first
four rows report stability, and the second six
report accuracy. The results show that Termi-
nal Partials are relatively unstable, with 23%-
Table 2: Average Number of Partials per utterance
ISR Group RLM1 RLM2 SLM
Basic All 12.0 9.9 11.6MW 14.6 12.3 29.7
Terminal All 5.4 3.3 6.2MW 6.4 4.1 8.8
Immortal All 0.22 0.32 0.55MW 0.42 0.67 0.63
113
Table 3: Stability and Accuracy Percentages
ISR Group RLM1 RLM2 SLM
Stability
Basic All 10 % 11 % 7 %MW 14 % 15 % 9 %
Terminal All 23 % 31 % 37 %MW 20 % 28 % 36 %
Accuracy
Basic All 9 % 1 % 5 %MW 11 % 13 % 6 %
Terminal All 13 % 21 % 24 %MW 12 % 17 % 21 %
Immortal All 91 % 93 % 55 %MW 90 % 90 % 56 %
37% of partials being stable, and that their sta-
bility drops off when looking at multi-word ut-
terances. SLM stability seems to be somewhat
higher than that of the RLM. Basic partials
are even more unstable (about 10% of partials
are stable), with extremely low stability for the
SLM. Unlike Terminal ISR, their stability grows
when only multi-word utterances are analyzed,
though the maximum is still quite low.
The results also show that partials are always
less accurate than they are stable, indicating
that not all stable partials are accurate. Immor-
tal partials are rare, but when they are found,
they are much more accurate than Terminal or
Basic partials. The RLM accuracy is very high,
and we suspect that immortal nodes are corre-
lated with utterances which are easier to recog-
nize. Terminal ISR is far more accurate than
Basic ISR for all of the utterances, but its im-
provement declines for multi-word RLMs.
We have shown three types of ISR: Basic, Ter-
minal and Immortal ISR. While Basic and Ter-
minal ISR are both highly productive, Terminal
ISR is far more stable and accurate than Basic.
Furthermore, there are far more Basic partials
than Terminal partials, implying that the dia-
logue manager would have to handle more un-
stable and inaccurate partials more often. Given
this, Terminal ISR is a far better ?productive
ISR? than the Basic method. Taking produc-
tion and stability together, there is a double dis-
Table 4: Lattice-Aware ISR (LAISR) Example
1-best Partial Type
yew Terminal
sarah Terminal
baum Terminal
dallas Terminal
downtown Terminal
downtown Immortal
downtown pittsburgh Terminal
downtown pittsburgh Immortal
sociation between Terminal and Immortal ISR.
Terminal partials are over produced and rela-
tively unstable. Furthermore, they are even less
stable when the transcribed reference is greater
than one word. On the other hand, Immortal
partials are stable and quite accurate, but too
rare for use alone. By integrating the Immortal
Partials with the Terminal ones, we may be able
to increase the stability and accuracy overall.
4 Lattice-Aware ISR (LAISR)
We introduce Lattice-Aware ISR (LAISR ?
pronounced ?laser?), that integrates Terminal
and Immortal ISR by allowing both types of par-
tials to be found. The selection procedure works
by first checking for an Immortal partial. If one
is not found then it looks for a Terminal. Re-
dundant partials are returned when the partial
type changes. An example recognition is shown
in Table 4. Notice how the first four partials
are completely unstable. This is very common,
and suppressing this noise is one of the primary
benefits of using more right context. Basic ISR
has even more of this type of noise.
LAISR was evaluated on the three recogni-
tion tasks described above (see Table 5). The
first two rows show the average number of par-
tials per utterance for each task and utterance
group. Unsurprisingly, these numbers are quite
similar to Terminal ISR. The stability percent-
age of LAISR is shown in the second two rows.
For all the utterances, there appears to be a very
slight improvement when compared to Termi-
nal ISR in Table 3. The improvement increases
for MW utterances, with LAISR improving over
114
Table 5: Lattice-Aware ISR Stats
Partials per Utterance
RLM1 RLM2 SLM
All 5.6 3.5 6.7
MW 6.7 4.5 9.6
Stability Percentage
All 24 % 33 % 40 %
MW 24 % 35 % 41 %
Accuracy Percentage
All 15 % 23 % 26 %
MW 16 % 22 % 24 %
Terminal ISR by 4?7 percentage points. This
is primarily because there is a higher occur-
rence of Immortal partials as the utterance gets
longer. Accuracy is reported in the final two
rows. Like the previous ISR methods described,
the accuracy percentage is lower than the sta-
bility percentage. When compared to Terminal
ISR, LAISR accuracy is slightly higher, which
confirms the benefit of incorporating immortal
partials with their relatively high accuracy. To
be useful in practice, it is important to exam-
ine when in the utterance ISR results are be-
ing produced. For example, if most of the par-
tials are returned towards the end of utterances,
than ISR is of little value over standard turn-
based recognition. Figure 1 shows the percent
of partials returned from the start of speech to
the final partial for MW utterances using the
SLM. This figure shows that partials are re-
turned rather evenly over the duration of ut-
terances. For example, in the first 10% of dura-
tion of each utterance, about 10% of all partial
results are returned. Figure 1 also reports the
stability and accuracy of the partials returned.
These numbers grow as decoding progresses, but
shows that mid-utterance results do yield rea-
sonable accuracy: partials returned in the mid-
dle of utterances (50%-60% duration) have an
accuracy of near 30%, compared to final partials
47% percent.
For use in a real-time dialogue system, it is
also important to assess latency. Here we define
latency as the difference in (real-world) time be-
tween (1) when the recognizer receives the last
Figure 1: Percent of LAISR partials returned from
the start of detected speech to the final partial using
the SLM. The percentage of partials returned that
are stable/accurate are also shown.
frame of audio for a segment of speech, and (2)
when the partial that covers that segment of
speech is returned from the recognizer. Mea-
suring latencies of LAISR on each task, we find
that RLM1 has a median of 0.26 seconds and a
mean of 0.41s; RLM2 has a median of 0.60s and
a mean of 1.48s; and SLM has a median of 1.04s
and a mean of 2.10s. Since reducing latency
was not the focus on this work, no speed opti-
mizations have been made, and we believe that
straightforward optimization can reduce these
latencies. For example, on the SLM, simply
turning off N-Best processing reduces the me-
dian latency to 0.55s and the mean to 0.79s.
Human reaction time to speech is roughly 0.20
seconds (Fry, 1975), so even without optimiza-
tion the RLM latencies are not far off human
performance.
In sum, LAISR produces a steady stream
of partials with relatively low latency over the
course of recognition. LAISR has higher stabil-
ity and accuracy than Terminal ISR, but its par-
tials are still quite unstable and inaccurate. This
means that in practice, dialogue systems will
need to make important decisions about which
partials to use, and which to discard. This need
motivated us to devise techniques for predicting
when a partial is stable, and when it is accurate,
which we address next.
115
Table 6: Equal Error Rates: Significant improvements in bold. Basic at p < 0.016, Terminal at p < 0.002,
and LAISR at p < 0.00001
All Multi-Word
Stability Measure (SM) Equal Error Rate
RLM 1 RLM 2 SLM RLM 1 RLM 2 SLM
Basic WATSON Score 13.3 13.3 12.8 15.6 16.4 15.2Regression 10.7 11.3 12.3 13.2 15.2 15.1
Terminal WATSON Score 24.3 29.1 34.4 26.6 26.0 34.1Regression 19.7 26.5 26.5 23.0 24.3 24.7
LAISR WATSON Score 24.7 29.3 35.0 24.0 27.0 35.3Regression 19.2 25.6 25.0 18.4 23.3 22.7
Confidence Measure (CM) Equal Error Rate
Basic WATSON Score 11.3 11.7 9.9 14.1 14.0 11.6Regression 9.8 9.8 9.7 12.3 12.9 11.0
Terminal WATSON Score 15.1 21.1 30.6 15.7 17.4 29.3Regression 11.7 16.8 20.8 12.1 14.5 18.4
LAISR WATSON Score 15.8 21.8 32.3 18.4 19.5 31.8Regression 11.6 16.6 21.0 11.6 14.2 18.7
5 Stability and Confidence Measures
As seen in the previous section, partial speech
recognition results are often revised and inaccu-
rate. In order for a dialogue system to make
use of partial results, measures of both stability
and confidence are crucial. A Stability Measure
(SM) predicts whether the current partial is a
prefix or complete match of the final recogni-
tion result (regardless of whether the final result
is accurate). A Confidence Measure (CM) pre-
dicts whether the current partial is a prefix or
complete match of what the user actually said.
Both are useful in real systems: for example, if
a partial is likely stable but unlikely correct, the
system might interrupt the user and ask them
to start again.
We use logistic regression to learn separate
classifiers for SM and CM. Logistic regression is
appealing because it is well-calibrated, and has
shown good performance for whole-utterance
confidence measures (Williams and Balakrish-
nan, 2009). For this, we use the BXR pack-
age with default settings (Genkin et al, 2011).
For Terminal and Basic ISR we use 11 features:
the raw WATSON confidence score, the individ-
ual features which affect the confidence score,
the normalized cost, the normalized speech like-
lihood, the likelihoods of competing models,
the best path score of word confusion network
(WCN), the length of WCN, the worst probabil-
ity in the WCN, and the length of N-best list.
For LAISR, four additional features are used:
three binary indicators of whether the partial is
Terminal, Immortal or a Terminal following an
Immortal, and one which gives the percentage
of words in the hypothesis that are immortal.
We built stability and confidence measures for
Basic ISR, Terminal ISR, and LAISR. Each of
the three corpora (RLM1, RLM2, SLM) was di-
vided in half to form a train set and test set.
Regression models were trained on all utter-
ances in the train set. The resulting models were
then evaluated on both All and MW utterances.
As a baseline for both measures, we compare
to AT&T WATSON?s existing confidence score.
This score is used in numerous deployed com-
mercial applications, so we believe it is a fair
baseline. Although the existing confidence score
is designed to predict accuracy (not stability),
there is no other existing mechanism for pre-
dicting stability.
We first report ?equal error rate? for the mea-
sures (Table 6). Equal error rate (EER) is the
sum of false accepts and false rejects at the rejec-
116
Figure 2: True accept percentages for stability measure (a) and confidence measure (b), using a fixed false
accept rate of 5%. LAISR yields highest true accept rates, with p < 0.0001 in all cases.
(a) Stability measure (b) Confidence measure
tion threshold for which false accepts and false
rejects are equal. Equal error rate is a widely
used metric to evaluate the quality of scoring
models used for accept/reject decisions. A per-
fect scoring model would yield an EER of 0. For
statistical significance we use ?2 contingency ta-
bles with 1 degree of freedom. It is inappropri-
ate to compare EER across ISR methods, since
the total percentage of stable or accurate par-
tials significantly effects the EER. For example,
Basic ISR has relatively low EER, but this is
because it also has a relatively low number of
stable or accurate partials.
The top six rows of Table 6 show EER for the
Stability Measure (SM). The left three columns
show results on the entire test set (all utterances,
of any length). On the whole, the SM outper-
forms the WATSON confidence scores, and the
greatest improvement is a 10.0 point reduction
in EER for LAISR on the SLM task. The right
three columns show results on only multi-word
(MW) utterances. Performance is similar to the
entire test set, with a maximum EER reduction
of 12.6 percent. The SLM MW performance is
interesting, suggesting that it is easier to pre-
dict stability after at least one word has been
decoded, possibly due to higher probability of
immortal nodes occurring. This suggests there
would be benefit in combining our method with
past work that adds right-context, perhaps us-
ing more context early in the utterance. This
idea is left for future work.
The bottom six rows show results for the Con-
fidence Measure (CM). We see that that even
when comparing our CM against the WATSON
confidence scores, there is significant improve-
ment, with a maximum of 13.1 for LAISR in the
MW SLM task.
The consistent improvement shows that logis-
tic regression is an effective technique for learn-
ing confidence and stability measures. It is most
powerful when combined with LAISR, and only
slightly less so with Terminal. Furthermore,
though the gains are slight, it is also useful with
Basic ISR, which speaks to the generality of the
approach.
While equal error rate is useful for evaluating
discriminative ability, when building an actual
system a designer would be interested to know
how often the correct partial is accepted. To
evaluate this, we assumed a fixed false-accept
rate of 5%, and report the resulting percentage
of partials which are correctly accepted (true-
accepts). Results are shown in Figure 1. LAISR
accepts substantially more correct partials than
other methods, indicating that LAISR would be
more useful in practice. This result also shows
a synergy between LAISR and our regression-
based stability and confidence measures: not
only does LAISR improve the fraction of stable
117
and correct partials, but the regression is able
to identify them better than for Terminal ISR.
We believe this shows the usefulness of the ad-
ditional lattice features used by the regression
model built on LAISR results.
6 Discussion and Conclusion
The adoption of ISR is hindered by the num-
ber of revisions that most partials undergo. A
number of researchers have proposed the use of
right context to increase the stability of par-
tials. While this does increase stability, it mit-
igates the primary gain of ISR: getting a rela-
tively real-time stream of the user?s utterance.
We offer two methods to improve ISR function-
ality: the integration of low-occurring Immortal
partials with higher occurring Terminal partials
(LAISR), and the use of logistic regression to
learn stability and confidence measures.
We find that the integrative approach,
LAISR, outperforms Terminal ISR on three
recognition tasks for a bus timetable spoken dia-
logue system. When looking at utterances with
more than one word this difference becomes even
greater, and this performance increase is due to
the addition of immortal partials, which have
a higher occurrence in longer utterances. This
suggests that as dialogue systems are used to
process multi-phrasal utterances and have more
dynamic turn-taking interactions, immortal par-
tials will play an even larger roll in ISR and par-
tial stability will further improve.
The Stability and Confidence measures both
have lower Equal Error Rates than raw recog-
nition scores when classifying partials. The im-
provement is greatest for LAISR, which benefits
from additional features describing lattice struc-
ture. It also suggests that other incremental fea-
tures such as the length of right context could be
useful for predicting stability. The higher num-
ber of True Accept partials by LAISR indicates
that this method is more useful to a dialogue
manager than Basic or Terminal ISR. Even so,
for all ISR methods there are still more use-
ful stable partials than there are accurate ones.
This suggests that both of these measures are
important to the downstream dialogue manager.
For example, if the partial is predicted to be sta-
ble but not correct, than the agent could possi-
bly interrupt the user and ask them to begin
again.
There are a number of avenues for future
work. First, this paper has examined the word
level; however dialogue systems generally oper-
ate at the intention level. Not all changes at
the word level yield a change in the resulting
intention, so it would be interesting to apply
the confidence measure and stability measures
developed here to the (partial) intention level.
These measures could also be applied to later
stages of the pipeline ? for example, tracking
stability and confidence in the dialogue state re-
sulting from the current partial intention. Fea-
tures from the intention level and dialogue state
could be useful for these measures ? for instance,
indicating whether the current partial intention
is incompatible with the current dialogue state.
Another avenue for future work would be to
apply these techniques to non-dialogue real-time
ASR tasks, such as transcription of broadcast
news. Confidence and stability measures could
be used to determine whether/when/how to dis-
play recognized text to a viewer, or to inform
down-stream processes such as named entity ex-
traction or machine translation.
Of course, an important objective is to eval-
uate our Stability and Confidence Measures
with LAISR in an actual spoken dialogue sys-
tem. ISR completely restructures the conven-
tional turn-based dialogue manager, giving the
agent the opportunity to speak at any mo-
ment. The use of reinforcement learning to make
these turn-taking decisions has been shown in a
small simulated domain by Selfridge and Hee-
man (2010), and we believe this paper builds
a foundation for pursuing these ideas in a real
system.
Acknowledgments
Thanks to Vincent Goffin for help with this
work, and the anonymous reviewers for their
thoughtful suggestions and critique. We ac-
knowledge funding from the NSF under grant
IIS-0713698.
118
References
G. Aist, J. Allen, E. Campana, C. Gallo, S. Stoness,
Mary Swift, and Michael K. Tanenhaus. 2007. In-
cremental understanding in human-computer di-
alogue and experimental evidence for advantages
over nonincremental methods. In Proc. DECA-
LOG, pages 149?154.
T. Baumann, M. Atterer, and D. Schlangen. 2009.
Assessing and improving the performance of
speech recognition for incremental systems. In
Proc. NAACL: HLT, pages 380?388.
A. Black, S. Burger, B. Langner, G. Parent, and
M. Eskenazi, 2010. Spoken dialog challenge 2010,
In Proc. Workshop on Spoken Language Technolo-
gies (SLT), Spoken Dialog Challenge 2010 Special
Session.
David DeVault, Kenji Sagae, and David Traum.
2009. Can i finish? learning when to respond to
incremental interpretation results in interactive di-
alogue. In Proc. SIGdial 2009 Conference, pages
11?20,
G.A. Fink, C. Schillo, F. Kummert, and G. Sagerer.
1998. Incremental speech recognition for multi-
modal interfaces. In Industrial Electronics Soci-
ety, 1998. IECON?98 volume 4, pages 2012?2017.
D.B. Fry. 1975. Simple reaction-times to speech and
non-speech stimuli.. Cortex volume 11, number 4,
page 355.
A. Genkin, L. Shenzhi, D. Madigan, and DD.
Lewis. 2011. Bayesian logistic regression.
http://www.bayesianregression.org.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-
Tur, A. Ljolje, S. Parthasarathy, M. Rahim,
G. Riccardi, and M. Saraclar. 2005. The AT&T
WATSON speech recognizer. In Proc. of ICASSP,
pages 1033?1036.
G. Parent and M. Eskenazi. 2009. Toward Better
Crowdsourced Transcription: Transcription of a
year of the Let?s Go Bus Information System Data.
Proc. of Interspeech 2005, Lisbon, Portugal.
A. Raux, B. Langner, D. Bohus, A.W. Black, and
M. Eskenazi. 2005. Lets go public! taking a spo-
ken dialog system to the real world. In Proc. of
Interspeech 2005.
D. Schlangen, T. Baumann, and M. Atterer. 2009.
Incremental reference resolution: The task, met-
rics for evaluation, and a Bayesian filtering model
that is sensitive to disfluencies. In Proc. SIGdial,
pages 30?37.
E.O. Selfridge and P.A. Heeman. 2010. Importance-
Driven Turn-Bidding for spoken dialogue systems.
In Proc. of ACL 2010, pages 177?185.
G. Skantze and D. Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proc.
EACL 2009, pages 745?753
J.C. Spohrer, PF Brown, PH Hochschild, and
JK Baker. 1980. Partial traceback in continuous
speech recognition. In Proc. of the IEEE Interna-
tional Conference on Cybernetics and Society.
J.D. Williams, I. Arizmendi and A. Conkie.
2010. Demonstration of AT&T ?Let?s Go?: A
production-grade statistical spoken dialog system.
In Proc Demonstration Session at IEEE Workshop
on Spoken Language Technology
J.D. Williams and S. Balakrishnan. 2009. Estimat-
ing probability of correctness for ASR N-Best lists.
In Proc. of SIGdial 2009, pages 132?135.
119
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 113?117,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Temporal Simulator for Developing Turn-Taking Methods for
Spoken Dialogue Systems
Ethan O. Selfridge and Peter A. Heeman
Center for Spoken Language Understanding
Oregon Health & Science University
20000 NW Walker Rd., Beaverton, OR, 97006
selfridg@ohsu.edu, heemanp@ohsu.edu
Abstract
Developing sophisticated turn-taking behavior
is necessary for next-generation dialogue sys-
tems. However, incorporating real users into
the development cycle is expensive and cur-
rent simulation techniques are inadequate. As
a foundation for advancing turn-taking behav-
ior, we present a temporal simulator that mod-
els the interaction between the user and the
system, including speech, voice activity de-
tection, and incremental speech recognition.
We describe the details of the simulator and
demonstrate it on a sample domain.
1 Introduction and Background
Effective turn-taking is critical for successful
human-computer interaction. Recently, approaches
have been proposed to improve system turn-taking
behavior that use reinforcement learning (Jonsdot-
tir et al, 2008; Selfridge and Heeman, 2010), de-
cision theory (e.g., Raux and Eskenazi, 2009), and
hard-coded policies (e.g., Skantze and Schlangen ,
2009). Some of these methods model turn-taking
as content-free decisions (Jonsdottir et al, 2008;
Skantze and Schlangen, 2009), while others primar-
ily rely on dialogue context (Selfridge and Heeman,
2010) and lexical cues (e.g., Raux and Eskenazi,
2009). Turn-taking continues to be an area of ac-
tive research and its development is vital for next-
generation dialogue systems, especially as they al-
low for more mixed initiative interaction.
Researchers have turned to simulation since de-
veloping a dialogue system with real users is ex-
pensive, time consuming, and sometimes impossi-
ble. Some turn-taking simulations have been highly
stylized and only model utterance content, failing to
give a realistic model of timing (Selfridge and Hee-
man, 2010). Others have modeled a content-free
form of turn-taking and only attend to timing and
prosodic information (Jonsdottir et al, 2008; Bau-
mann, 2008; Padilha and Carletta, 2002). The for-
mer is insufficient for the training of deployable real-
time systems, and the latter neglect an important as-
pect of turn-taking: semantic information (Gravano
and Hirschberg, 2011).
The overall goal is to develop a simulation en-
vironment to train behavior policies that can be
transferred with minimal modifications to produc-
tion systems. This paper presents some first steps
towards this goal. We describe a temporal simula-
tor that models the timing and content of both user
and system speech, as well as that of incremental
speech recognition (ISR) and voice activity detec-
tion (VAD). We detail the overall temporal simulator
architecture, the design of the individual agents that
simulate dialogue, and an instantiation of a simple
domain. To demonstrate the utility of the simulator,
we implement multiple turn-taking polices and use it
to compare these policies under conditions of vary-
ing reaction time and speech recognition accuracy.
2 Temporal Simulation Framework
We now describe the details of the temporal sim-
ulator. Inspired by the Open Agent Architecture
(Martin et al, 1999), it is composed of a number
of agents, each running as a separate computer pro-
cess. We first describe the time keeping procedure
and then the overall agent communication structure.
113
Time Keeping: To provide a useful training envi-
ronment, the simulator must realistically model, and
run much faster than, ?real-time?. To do this, the
simulator keeps an internal clock that advances to
the next time slice when all agents have been run
for the current time slice. This structure allows the
simulator to run far faster than ?real-time? while sup-
porting realistic communication. This framework is
similar to the clock cycle described by Padilha et al
(2002).
Agent Communication: Agents use messages to
communicate. Messages have three components:
the addressee, the content and a time stamp. Time
stamps dictate when the content is to be processed
and must always be for a future, not the current, time
slice, as the alternative would imply instantaneous
communication and overly complicate the software
architecture. A central hub receives all messages
and passes them to the intended recipient agent at
the appropriate time. At every slice, each agent runs
two procedures: one that retrieves messages and one
that can send messages. If there are multiple mes-
sages intended for the same time slice, the agent
completely processes one before moving to the next.
3 Dialogue Simulator
We use the above temporal simulator to simulate di-
alogue. At present, we focus on dyadic interaction
and have three agents that are run in a strict order at
every time slice: User, ISR, and System. Time slices
are modeled as 10 millisecond (ms) increments, as
this is the time scale that speech recognizers run at.
In general, the User agent sends messages to the
ISR agent that sends messages to the System agent.
The System agent generally sends messages to both
the User agent and the ISR agent. The behavior of
all three agents rely on parameters (Table 1) that
may either be set by hand or estimated from data.
The User and System agents have near identical con-
struction, the primary difference being that the Sys-
tem can misunderstand User speech.
User and System Design: Agent speech is gov-
erned by a number of timing parameters. The Take-
Turn parameter specifies when the agent will begin
speaking the selected utterance. The agent gets the
first word of the utterance, sets the Word Length pa-
rameter, and ?begins? to speak by sending a speech
event message. The agent outputs the word after
the specified Word Length, and sets the Inter-Word
Pause parameter that governs when the next word
will begin. When the agent completes the utter-
ance, it waits until a future time slice to start an-
other (as governed by the Inter-Utterance Pause pa-
rameter). However, if the listening agent interrupts
mid-utterance, the speaking agent stops speaking
and will not complete the utterance. Any dialogue
agent architecture can be used, providing the input
and output fit with the above specifications.
ISR Design: The ISR agent works as both an In-
cremental Speech Recognizer and a VAD. We cur-
rently model uncertainty in recognition but not in
the VAD, though this is certainly a plausible and
worthwhile addition. When the ISR agent receives
the speech event from the User, it sets the VAD
Speech Start parameter that models lag in speech
detection, and the Speech End (no word) parameter
that models situations where the user starts speaking
but stops mid-word and produces an unrecognizable
sound. When the word is received from the User,
the Speech End (word) parameter is set and a par-
tial phrase result is generated based on the probabil-
ity that the word will be correctly recognized. This
probability is then used as the basis for a confidence
score that is packaged with the partial phrase result.
A Recognition Lag parameter governs the time be-
tween User speech and the output of partial phrase
results to the System. The form of ISR we model
recognizes words cumulatively (see Figure 1 for an
example) though the confidence score, at present, is
only for the newly recognized word. The recognizer
will continue to output partials from User words un-
til the User stops speaking or the System sends a
message to stop recognizing. One critical aspect of
ISR which we are not modeling is partial instability,
where partials are revised as recognition progresses.
Partial instability is an area of active research (e.g.
Baumann et al 2009) and, while revisions may cer-
tainly be modeled in the future, we chose not to for
simplicity?s sake. We feel that, at present, the Recog-
nition Lag parameter is sufficient to model the time
for a partial to become stable.
114
Table 1: Parameters and demonstration values (ms)
Conversant Agents
Inter-Word pause (Usr) ? = 200, ? = 100
Inter-Word pause (Sys) 100
Inter-Utt. pause ? = 1000, ? = 500
Word Length 400
Take-Turn (Usr) 500/200
Take-Turn (Sys) 750/100
ISR Agent
Recog. Acc. variable
Recog. Lag 300
VAD
Speech Start 100
Speech End (word) 200
Speech End (no word) 600
4 Simulation demonstration
We now demonstrate the utility of the temporal sim-
ulator by showing that it can be used to evaluate
different turn-taking strategies under conditions of
varying ASR accuracy. This is the first step before
using it to train policies for use in a live dialogue
system.
For this demonstration the conversant agents, the
System and User, are built according to the Infor-
mation State Update approach (Larsson and Traum,
2000), and perform an update for every message as-
sociated with the current time slice. The conver-
sant agents are identical except for individual rule
sets. Four types of rule sets are common across
conversant agents: UNDERSTANDING rules, that up-
date the IS using raw message content; DELIBERA-
TION rules, that update the IS by comparing new in-
formation to old; UTTERANCE rules, that select the
next utterance based on dialogue context; and TURN
Figure 1: Sample dialogue with timing information
rules, that select the time to begin the new utterance
by modifying the Take Turn parameter. Rule sets are
executed in this order with one exception. After the
UNDERSTANDING rules, the System agent has AC-
CEPTANCE rules that use confidence scores to decide
whether to understand the recognition or not.
Temporal Simulation Example: We constructed
a simple credit card domain, similar to Skantze and
Schlangen (2009), where the User says four utter-
ances of four digits each. The System must implic-
itly confirm every number and if it is correct, the
User continues.1 It can theoretically do this at any
time, immediately after the word is recognized, af-
ter an utterance, or after multiple utterances. If the
system says a wrong number the User interrupts the
System with a ?no? and begins the utterance again.
The System has a Non-Understanding (NU) confi-
dence score threshold set at 0.5. After an NU, the
System will not understand any more words and will
either confirm any digits recognized before the NU
or, if there are no words to confirm, will say an NU
utterance (?pardon??). The User says ?yes? to the
final, correct confirmation. To maintain simplicity,
?yes? and ?no? are always accurate. If this were not
the case, there would be a number of dialogues that
were not successful. The User takes the turn in two
ways. It either waits 500 ms after a System utterance
to speak or interrupts 200 ms after the System con-
firms an misrecognized word, which is in line with
human reaction time (Fry, 1975).
We implemented three different turn-taking
strategies: two Fixed and one Context-based. Us-
ing the Fixed strategy the System either uses a Slow
policy, waiting 750 ms after no user speech is de-
tected, or a Fast policy, waiting only 100 ms. The
Fast reaction time results in the System interrupt-
ing the User during an utterance when the inter-word
pause becomes longer than 200 ms. This is because
the VAD Speech End parameter is 100 ms and the
System is waiting for 100 ms of silence after Speech
End. The Slow reaction time results in far less in-
terruptions. The Context-based turn-taking strategy
uses the recognition score to choose its turn-taking
behavior. The motivation is that one would want
1Unlike an explicit confirmation (?I heard five. Is that
right??), an implicit confirm (?Ok, five?) does not necessitate
a strict ?yes? or ?no? response.
115
Figure 2: Mean Time and Interruption for different turn-taking polices and ASR accuracy conditions
to confirm low-confidence recognitions sooner than
those with high confidence. If any unconfirmed re-
sult has scores less than 0.8 then the System uses the
Fast reaction time to try to confirm or reject as soon
as possible. Alternatively, if the results all have high
confidences, it can wait until a longer user pause
(generally between utterances) by using the Slow re-
action time. All parameter values are shown in Table
1.
Figure 1 shows a dialogue fragment of a System
using the Context-based turn-taking policy. Num-
bers are used for the sake of brevity. The start of
a box surrounding a word corresponds to when the
Speech message was sent (from the User agent to the
ISR agent) and the end of the box to when the word
has been completed and recognition lag timer be-
gins. The point of the ISR box refers to the time slice
when the partial phrase result and score were sent to
the System. Note how after the third User word the
System interrupts to confirm the utterance, since the
confidence score of a previous word dropped below
0.8. Also note how the User interrupts the System
after it confirms a wrong number.
Comparing turn-taking policies: We evaluated
the three (two Fixed and one Context-based) turn-
taking policies in two conditions of ASR accuracy:
Low Error, where the probability of correctness was
95%; and High Error, where the probability of cor-
rectness was 75%. We compared the mean dialogue
time (left Figure 2) and the mean number of in-
terruptions per dialogue (right Figure 2). For dia-
logue time, we find that all turn-taking policies per-
form similarly in the Low Error condition. How-
ever, in the High Error condition the Slow reac-
tion time performs much worse since it cannot ad-
dress poor recognitions with the speed of the other
two. For interruption, the Fast and Context-driven
policies have far more than the Slow for the High
Error condition. However, in the Low Error con-
dition the Fast policy interrupts far more than the
Context-driven. Given that natural behavior is one
goal of turn-taking, interruption, while effective at
handling High Error rates, should be minimized.
The Context-based policy provides support for in-
terruption when it is needed (High Error Condition)
and reduces it when it is not (Low Error Condition).
The other policies are either unable to interrupt at all
(Slow), increasing the dialogue time, or due to a lack
the flexibility (Fast), interrupt constantly.
5 Conclusion
We take the first steps towards a simulation approach
that characterizes both the content of conversant
speech as well as its timing. The temporal simula-
tor models conversant utterances, ISR, and the VAD.
The simulator runs quickly (100 times faster than
real-time), and is simple and highly flexible. Us-
ing an example, we demonstrated that the simula-
tor can help understand the ramifications of differ-
ent turn-taking policies. We also highlighted both
the temporal nature of turn-taking ? interruptions,
reaction time, recognition lag...etc. ? and the con-
tent of utterances ? speech recognition errors, con-
fidence scores, and wrong confirmations. Plans for
future work include adding realistic prosodic mod-
eling and estimating model parameters from data.
Acknowledgments
We thank to the reviewers for their thoughtful sug-
gestions and critique. We acknowledge funding
from the NSF under grant IIS-0713698.
116
References
T. Baumann, M. Atterer, and D. Schlangen. 2009.
Assessing and improving the performance of speech
recognition for incremental systems. In Proc. NAACL:
HLT, pages 380?388.
T. Baumann. 2008. Simulating spoken dialogue with
a focus on realistic turn-taking. In Proc. of ESSLLI
Student Session.
D. B. Fry. 1975. Simple reaction-times to speech and
non-speech stimuli. Cortex, 11(4):355?360.
A. Gravano and J. Hirschberg. 2011. Turn-taking cues
in task-oriented dialogue. Computer Speech & Lan-
guage, 25(3):601?634.
G.R. Jonsdottir, K.R. Thorisson, and Eric Nivel. 2008.
Learning smooth, human-like turntaking in realtime
dialogue. In Proc. of IVA, pages 162?175.
S. Larsson and D. Traum. 2000. Information state and di-
alogue managment in the trindi dialogue move engine
toolkit. Natural Language Engineering, 6:323?340.
D.L. Martin, Adam J. Cheyer, and Douglas B. Moran.
1999. The open agent architecture: A framework for
building distributed software systems. Applied Ar-
tificial Intelligence: An International Journal, 13(1-
2):91?128.
E. Padilha and J. Carletta. 2002. A simulation of small
group discussion. In Proc. of EDILOG, pages 117?
124.
A. Raux and M. Eskenazi. 2009. A finite-state turn-
taking model for spoken dialog systems. In Proc. of
HLT/NAACL, pages 629?637.
G. Skantze and D. Schlangen . 2009. Incremental di-
alogue processing in a micro-domain. In Proc. of
EACL, pages 745?753.
E.O. Selfridge and P.A. Heeman. 2010. Importance-
Driven Turn-Bidding for spoken dialogue systems. In
Proc. of ACL, pages 177?185.
117
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 275?279,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Integrating Incremental Speech Recognition and POMDP-based Dialogue
Systems
Ethan O. Selfridge?, Iker Arizmendi?, Peter A. Heeman?, and Jason D. Williams1
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR, USA
?AT&T Labs ? Research, Shannon Laboratory, Florham Park, NJ, USA
1Microsoft Research, Redmond, WA, USA
{selfridg,heemanp}@ohsu.edu
iker@research.att.com jason.williams@microsoft.com
Abstract
The goal of this paper is to present a first
step toward integrating Incremental Speech
Recognition (ISR) and Partially-Observable
Markov Decision Process (POMDP) based di-
alogue systems. The former provides sup-
port for advanced turn-taking behavior while
the other increases the semantic accuracy of
speech recognition results. We present an In-
cremental Interaction Manager that supports
the use of ISR with strictly turn-based dia-
logue managers. We then show that using
a POMDP-based dialogue manager with ISR
substantially improves the semantic accuracy
of the incremental results.
1 Introduction and Background
This paper builds toward integrating two distinct
lines of research in spoken dialogue systems: in-
cremental speech recognition (ISR) for input, and
Partially Observable Markov Decision Processes
(POMDPs) for dialogue management.
On the one hand, ISR improves on whole-
utterance speech recognition by streaming results to
the dialogue manager (DM) in real time (Baumann
et al, 2009; Skantze and Schlangen, 2009). ISR
is attractive because it enables sophisticated system
behavior such as interruption and back-channeling.
However, ISR output is particularly error-prone, and
often requires a specialized dialogue manager to be
written (Bu? and Schlangen, 2011; Schlangen and
Skantze, 2009).
1Work done while at AT&T Labs - Research
On the other hand, POMDP-based dialogue man-
agers improve on traditional approaches by (in part)
tracking a distribution over many possible dialogue
states, rather than just one, thereby improving ro-
bustness to speech recognition errors (Williams and
Young, 2007; Thomson and Young, 2010; Young
et al, 2010). The overall aim of combining these
two lines of research is to improve the robustness of
error-prone ISR output.
To our knowledge only one study to date has com-
bined ISR and POMDPs. Lu et al (2011) show
how 1-best ISR hypotheses can be used within a sin-
gle dialogue turn. This work is different than the
present paper, where we use N-Best lists of ISR re-
sults across multiple turns of a dialogue.
Specifically, this paper makes two contributions.
First, as a foundation, we introduce an Incremental
Interaction Manager (IIM) that enables ISR to be
used within the traditional turn-based dialogue man-
agement framework. The IIM confers many, but not
all, of the benefits of ISR without requiring mod-
ification to a traditional dialogue manager. Thus,
in theory, any existing dialogue system architecture
could use ISR with the addition of an IIM. Second,
we show that pairing our IIM with a POMDP-based
dialogue manager yields a substantial improvement
in accuracy for incremental recognition results at the
dialogue level.
The paper is organized as follows. Section 2 de-
scribes the IIM, section 3 describes the POMDP in-
tegration, sections 4 and 5 describe experiments and
results, and section 6 concludes.
275
Table 1: Example IIM operation. P = partial ISR result; A = dialogue action.
Original Copied
ISR IIM DM state DM state DM Action
Prompt: ?Where are you leaving from??
yew Rej. P 0 0 -
ridge Acc. P / Rej. A 0 0 ?I?m sorry...?
mckee Acc. P / Acc. A 0 1 ?Ok, Mckee...?
mckeesport Acc. P / Acc. A 0 2 ?Ok, Mckeesport..?
mckeesport center Acc. P / Rej. A 0 2 ?Ok, Mckeesport..?
Prompt: ?Ok, Mckeesport. Where are you going to??
pitt Acc. P / Rej. A 2 4 ?I?m sorry...?
pittsburgh Acc. P / Acc. A 2 5 ?Ok, Pittsburgh...?
2 Incremental Interaction manager
The Incremental Interaction Manager (IIM) medi-
ates communication between the incremental speech
recognizer and the DM. The key idea is that the
IIM evaluates potential dialogue moves by apply-
ing ISR results to temporary instances of the DM.
The IIM copies the current state of the DM, pro-
vides the copied DM with a recognition result, and
inspects the action that the copied DM would take.2
If the action does not sufficiently advance the dia-
logue (such as re-asking the same question), the ac-
tion is rejected and the copied DM is discarded. If
the action advances the dialogue (such as asking for
or providing new information), then that action is
immediately executed.
The system should gracefully handle revisions
following a premature action execution, and a copy-
ing procedure is a viable solution for any DM. When
a revision is received, a second copy of the original
DM is made and the new ISR result is passed to that
second copy; if that second copy takes an action that
advances the dialogue and is different from the ac-
tion generated by the first copy, then the first action
is terminated, the first copy of the DM is discarded,
the second action is initiated, and the second copy
assumes the position of the first copy. Additional
revisions can be handled by following the same pro-
cedure. Terminating a speech action and immedi-
ately starting another can be jarring (?Say a city /
Ok, Boston...?), which can be mitigated by preced-
2If the DM design does not force a state transition following
a result then the DM supplies the the action without copying.
ing actions with either a sound or simple silence (at
the expense of some response delay). Once recog-
nition is complete, the copied DM is installed as the
new original DM.
Many ISR results can be discarded before passing
them to the DM. First, only incremental results that
could correspond to complete user utterance are con-
sidered: incomplete results are discarded and never
passed to the DM. In addition, ISR results are of-
ten unstable, and it is undesirable to proceed with
an ISR result if it will very likely be revised. Thus
each candidate ISR result is scored for stability (Sel-
fridge et al, 2011) and results with scores below a
manually-set threshold are discarded.
Table 1 shows an example of the recognizer, the
IIM, and the DM. For sake of clarity, stability scores
are not shown. The system asks ?Where are you
leaving from?? and the user answers ?Mckeesport
Center.? The IIM receives five ISR results (called
partials), rejecting the first, yew, because its stabil-
ity score is too low (not shown). With the second,
ridge, it copies the DM, passes ridge to the copy,
and discards the action of the copied DM (also dis-
carded) because it does not advance the dialogue. It
accepts and begins to execute the action generated
by the third partial, mckee. The fourth partial revises
the action, and the fifth action is rejected since it is
the same. The original DM is then discarded and the
copied DM state is installed in its place.
Overall, the IIM enables a turn-based DM to en-
joy many of the benefits of ISR ? in particular, the
ability to make turn-taking decisions with a com-
plete account of the dialogue history.
276
3 Integrating ISR with a POMDP-based
dialogue manager
A (traditional) dialogue manager based on a partially
observable Markov decision process (POMDP DM)
tracks a probability distribution over multiple hid-
den dialogue states called a belief state (Williams
and Young, 2007).3 As such, POMDP DMs read-
ily make use of the entire ASR N-Best list, even
for low-confidence results ? the confidence level of
each N-Best list item contributes proportionally to
the probability of its corresponding hidden state.
It is straightforward to integrate ISR and a
POMDP DM using the IIM. Each item on the N-
Best list of an incremental result is assigned a confi-
dence score (Williams and Balakrishnan, 2009) and
passed to the POMDP DM as if it were a complete
result, triggering a belief state update. Note that this
approach is not predicting future user speech from
partial results (DeVault et al, 2009; Lu et al, 2011),
but rather (tentatively) assuming that partial results
are complete.
The key benefit is that a belief state generated
from an incremental result incorporates all of the
contextual information available to the system from
the start of the dialogue until the moment of that
incremental result. By comparison, an isolated in-
cremental result includes only information from the
current utterance. If the probability models in the
POMDP are estimated properly, belief states should
be more accurate than isolated incremental results.
4 Experimental design
For our experiments we used a corpus of 1037 calls
from real users to a single dialogue system that pro-
vides bus timetable information for Pittsburgh, PA
(a subsequent version of Williams (2011)). This di-
alogue system opened by asking the caller to say a
bus route number or ?I don?t know?; if the system
had insufficient confidence following recognition, it
repeated the question. We extracted the first 3 re-
sponses to the system?s bus route question. Often
the system did not need to ask 3 times; our exper-
imental set contained 1037 calls with one or more
attempts, 586 calls with two or more attempts, and
3It also uses reinforcement learning to choose actions, al-
though in this paper we are not concerned with this aspect.
356 calls with three or more attempts. These utter-
ances were all transcribed, and tagged for the bus
route they contained, if any: 25% contained neither
a route nor ?I don?t know?.
We ran incremental speech recognition on each
utterance using Lattice-Aware Incremental Speech
Recognition (Selfridge et al, 2011) on the AT&T
WATSONSM speech recognizer (Goffin et al, 2005)
with the same rule-based language models used in
the production system. On average, there were
5.78, 5.44, and 5.11 incremental results per utter-
ance (plus an utterance-final result) for the first, sec-
ond, and third attempts. For each incremental result,
we noted its time stamp and interpretation: correct,
if the interpretation was present and correct, other-
wise incorrect. Each incremental result included an
N-Best list, from which we determined oracle accu-
racy: correct if the correct interpretation was present
anywhere on the most recent ISR N-Best list, other-
wise incorrect.
Each incremental result was then passed to the
IIM and POMDP DM. The models in the POMDP
DM were estimated using data collected from a dif-
ferent (earlier) time period. When an incremental
result updated the belief state, the top hypothesis
for the route was extracted from the belief state and
scored for correctness. For utterances in the first at-
tempt, the belief state was initialized to its prior; for
subsequent attempts, it incorporated all of the prior
(whole-turn) utterances. In other words, each at-
tempt was begun assuming the belief state had been
running up to that point.
5 Results and Discussion
We present results by showing instantaneous seman-
tic accuracy for the raw incremental result (base-
line), the top belief state, and oracle. Instantaneous
semantic accuracy is shown with respect to the per-
cent of the total recognition time the partial is rec-
ognized at. An utterance is incorrect if it has no in-
cremental result before a certain percentage.
We show 2 sets of plots. Figure 1 shows only in-
cremental recognition results and excludes the end-
of-utterance (phrase) results; Figure 2 shows incre-
mental recognition results and includes phrase re-
sults. It is useful to view these separately since the
phrase result, having access to all the speech, is sub-
277
Figure 1: Instantaneous semantic accuracy of incremental results, excluding phrase-final results
Figure 2: Instantaneous semantic accuracy of incremental and phrase-final results
stantially more accurate than the incremental results.
Figure 1 shows that the POMDP is more accu-
rate than the raw incremental result (excluding end-
of-phrase results). Its performance gain is minimal
in attempt 1 because the belief is informed only by
the prior. In attempt 2 and 3, the gain is larger
since the belief also benefits from the previous at-
tempts. Since the top POMDP result in subsequent
attempts is sometimes already correct (because it
incorporates past recognitions), the POMDP some-
times meets and occasionally exceeds the oracle dur-
ing the early portions of attempts 2 and 3.
Figure 2 shows that when end-of-phrase recog-
nition results are included, the benefit of the belief
state is limited to the initial portions of the second
and third turns. This is because the POMDP mod-
els are not fit well to the data: the models were
estimated from an earlier version of the system,
with a different user base and different functionality.
Identifying and eliminating this type of mismatch
is an important issue and has been studied before
(Williams, 2011).
Taken as a whole, we find that using belief track-
ing increases the accuracy of partials by over 8%
(absolute) in some cases. Even though the final
phrase results of the 1-best list are more accurate
than the belief state, the POMDP shows better ac-
curacy on the volatile incremental results. As com-
pared to the whole utterance results, incremental re-
sults have lower 1-best accuracy, yet high oracle ac-
curacy. This combination is a natural fit with the
POMDPs belief state, which considers the whole N-
Best list, effectively re-ranking it by synthesizing in-
formation from dialogue history priors.
6 Conclusion
This paper has taken a step toward integrating ISR
and POMDP-based dialogue systems. The Incre-
mental Interaction Manager (IIM) enables a tradi-
tional turn-based DM to make use of incremental
results and enjoy many their benefits. When this
IIM is paired with a POMDP DM, the interpreta-
tion accuracy of incremental results improves sub-
stantially. In the future we hope to build on this work
by incorporating Reinforcement Learning into turn-
taking and dialogue action decisions.
Acknowledgments
Thanks to Vincent Goffin for help with this work,
and to the anonymous reviewers for their comments
and critique. We acknowledge funding from the
NSF under grant IIS-0713698.
278
References
T. Baumann, M. Atterer, and D. Schlangen. 2009.
Assessing and improving the performance of speech
recognition for incremental systems. In Proc. NAACL:
HLT, pages 380?388. Association for Computational
Linguistics.
O. Bu? and D. Schlangen. 2011. Dium?an incremen-
tal dialogue manager that can produce self-corrections.
Proceedings of semdial.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can i finish? learning when to respond to incremental
interpretation results in interactive dialogue. In Pro-
ceedings of the SIGDIAL 2009 Conference, pages 11?
20, London, UK, September. Association for Compu-
tational Linguistics.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur,
A. Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi,
and M. Saraclar. 2005. The AT&T WATSON speech
recognizer. In Proceedings of ICASSP, pages 1033?
1036.
D. Lu, T. Nishimoto, and N. Minematsu. 2011. Decision
of response timing for incremental speech recogni-
tion with reinforcement learning. In Automatic Speech
Recognition and Understanding (ASRU), 2011 IEEE
Workshop on, pages 467?472. IEEE.
D. Schlangen and G. Skantze. 2009. A general, ab-
stract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 710?718. Association for Computational
Linguistics.
E.O. Selfridge, I. Arizmendi, P.A. Heeman, and J.D.
Williams. 2011. Stability and accuracy in incremen-
tal speech recognition. In Proceedings of the SIGdial
2011.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, pages
745?753. Association for Computational Linguistics.
B. Thomson and S. Young. 2010. Bayesian update of di-
alogue state: A pomdp framework for spoken dialogue
systems. Computer Speech & Language, 24(4):562?
588.
J.D. Williams and S. Balakrishnan. 2009. Estimating
probability of correctness for asr n-best lists. In Proc
SIGDIAL, London, United Kingdom.
J.D. Williams and S. Young. 2007. Partially observable
markov decision processes for spoken dialog systems.
Computer Speech & Language, 21(2):393?422.
J.D. Williams. 2011. An empirical evaluation of a sta-
tistical dialog system in public use. In Proceedings of
the SIGDIAL 2011 Conference, pages 130?141. Asso-
ciation for Computational Linguistics.
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
pomdp-based spoken dialogue management. Com-
puter Speech & Language, 24(2):150?174.
279
Proceedings of the SIGDIAL 2013 Conference, pages 384?393,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Continuously Predicting and Processing Barge-in
During a Live Spoken Dialogue Task
Ethan O. Selfridge?, Iker Arizmendi?, Peter A. Heeman?, and Jason D. Williams1
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR, USA
?AT&T Labs ? Research, Shannon Laboratory, Florham Park, NJ, USA
1Microsoft Research, Redmond, WA, USA
selfridg@ohsu.edu
Abstract
Barge-in enables the user to provide input
during system speech, facilitating a more
natural and efficient interaction. Stan-
dard methods generally focus on single-
stage barge-in detection, applying the di-
alogue policy irrespective of the barge-in
context. Unfortunately, this approach per-
forms poorly when used in challenging
environments. We propose and evaluate
a barge-in processing method that uses a
prediction strategy to continuously decide
whether to pause, continue, or resume the
prompt. This model has greater task suc-
cess and efficiency than the standard ap-
proach when evaluated in a public spoken
dialogue system.
Index Terms: spoken dialogue systems, barge-in
1 Introduction
Spoken dialogue systems (SDS) communicate
with users with spoken natural language; the op-
timal SDS being effective, efficient, and natural.
Allowing input during system speech, known as
barge-in, is one approach that designers use to
improve system performance. In the ideal use
case, the system detects user speech, switches off
the prompt, and then responds to the user?s utter-
ance. Dialogue efficiency improves, as the sys-
tem receives information prior to completing its
prompt, and the interaction becomes more natu-
ral, as the system demonstrates more human-like
turn-taking behavior. However, barge-in poses a
number of new challenges; the system must now
recognize and process input during its prompt that
may not be well-formed system directed speech.
This is a difficult task and standard barge-in ap-
proaches often stop the prompt for input that will
not be understood, subsequently initiating a clari-
fication sub-dialogue (?I?m sorry, I didn?t get that.
You can say...etc.?). This non-understood barge-in
(NUBI) could be from environmental noise, non-
system directed speech, poorly-formed system di-
rected speech, legitimate speech recognition diffi-
culties (such as acoustic model mismatch), or any
combination thereof.
This paper proposes and evaluates a barge-in
processing method that focuses on handling NU-
BIs. Our Prediction-based Barge-in Response
(PBR) model continuously predicts interpretation
success by applying adaptive thresholds to incre-
mental recognition results. In our view, predicting
whether the recognition will be understood has far
more utility than detecting whether the barge-in
is truly system directed speech as, for many do-
mains, we feel only understandable input has more
discourse importance than system speech. If the
input is predicted to be understood, the prompt is
paused. If it is predicted or found to be NUBI, the
prompt is resumed. Using this method, the sys-
tem may resume speaking before recognition is
complete and will never initiate a clarifying sub-
dialogue in response to a NUBI. The PBR model
was implemented in a public Lets Go! statistical
dialogue system (Raux et al, 2005), and we com-
pare it with a system using standard barge-in meth-
ods. We find the PBR model has a significantly
better task success rate and efficiency.
Table 1 illustrates the NUBI responses produced
by the standard barge-in (Baseline) and PBR mod-
els. After both prompts are paused, the standard
method initiates a clarifying sub-dialogue whereas
PBR resumes the prompt.
We first provide background on Incremental
Speech Recognition and describe the relevant re-
lated work on barge-in. We then detail the
Prediction-based Barge-in Response model?s op-
eration and motivation before presenting a whole-
call and component-wise analysis of the PBR
1Work done while at AT&T Labs - Research
384
Table 1: System response to Non-Understood Barge-In (NUBI)
Baseline Ok, sixty one <NUBI> Sorry, say a bus route like twenty eight x
PBR Ok, sixty one <NUBI> sixty one c. Where are you leaving from?
model. The paper concludes with a discussion of
our findings and implications for future SDS.
2 Background and Related Work
Incremental Speech Recognition: Incremental
Speech Recognition (ISR) provides the real-time
information critical to the PBR model?s continu-
ous predictions. ISR produces partial recognition
results (?partials?) until input ceases and the ?fi-
nal? recognition result is produced following some
silence. As partials have a tendency to be revised
as more audio is processed, stability measures are
used to predict whether a given partial hypothe-
sis will be present in the final recognition result
(McGraw and Gruenstein, 2012; Selfridge et al,
2011). Here, we use Lattice-Aware ISR, which
produces partials after a Voice Activity Detector
(VAD) indicates speech and limits them to be a
complete language model specified phrase or have
guaranteed stability (Selfridge et al, 2011).
Barge-In: Using the standard barge-in model,
the system stops the prompt if barge-in is detected
and applies the dialogue logic to the final recogni-
tion result. This approach assumes that the barge-
in context should not influence the dialogue pol-
icy, and most previous work on barge-in has fo-
cused on detection: distinguishing system directed
speech from other environmental sounds. Cur-
rently, these methods are either based on a VAD
(e.g. (Stro?m and Seneff, 2000)), ISR hypothe-
ses (Raux, 2008), or some combination (Rose and
Kim, 2003). Both approaches can lead to detection
errors: background speech will trigger the VAD,
and partial hypotheses are unreliable (Baumann et
al., 2009). To minimize this, many systems only
enable barge-in at certain points in the dialogue.
One challenge with the standard barge-in model
is that detection errors can initiate a clarifying sub-
dialogue to non-system directed input, as it is un-
likely that this input will be understood (Raux,
2008). Since this false barge-in, which in most
cases is background speech (e.g. the television), is
highly indicative of poor recognition performance
overall, the system?s errant clarifying response can
only further degrade user experience.
Strom and Seneff (2000) provide, to our knowl-
edge, the only mature work that proposed deviat-
ing from the dialogue policy when responding to
a barge-in recognition. Instead of initiating a clar-
ifying sub-dialogue, the system produced a filled-
pause disfluency (?umm?) and resumed the prompt
at the phrase boundary closest to the prompt?s sus-
pension point. However, this model only operated
at the final recognition level (as opposed the incre-
mental level) and, unfortunately, they provide no
evaluation of their approach. An explicit compar-
ison between the approaches described here and
the PBR model is found in Section 3.5.
3 Prediction-based Barge-in Response
The PBR model is characterized by three high-
level states: State 1 (Speaking Prediction), whose
goal is to pause the prompt if stability scores pre-
dict understanding; State 2 (Silent Prediction),
whose goal is to resume the prompt if stability
scores and the incremental recognition rate pre-
dict non-understanding; and State 3 (Completion),
which operates on the final recognition result, and
resumes the prompt unless the recognition is un-
derstood and the new speech act will advance the
dialogue. Here, we define ?advancing the dia-
logue? to be any speech act that does not start a
clarifying sub-dialogue indicating a NUBI. Tran-
sitions between State 1 and 2 are governed by
adaptive thresholds ? repeated resumptions sug-
gest the user is in a noisy environment, so each
resumption increases the threshold required to ad-
vance from State 1 to State 2 and decreases the
threshold required to advance from State 2 to State
1. A high-level comparison of the standard model
and our approach is shown in Figure 1; a complete
PBR state diagram is provided in the Appendix.
3.1 State 1: Speaking Prediction
In State 1, Speaking Prediction, the system is both
speaking and performing ISR. The system scores
each partial for stability, predicting the probability
that it will remain ?stable? ? i.e., will not be later
revised ? using a logistic regression model (Self-
ridge et al, 2011). This model uses a number of
features related to the recognizer?s generic confi-
dence score, the word confusion network, and lat-
tice characteristics. Table 2 shows partial results
385
Table 2: Background noise and User Speech ISR
Background Noise User Utterance
Partial Stab. Scr. Partial Stab. Scr.
one 0.134 six 0.396
two 0.193 sixty 0.542
six 0.127 fifty one 0.428
two 0.078 sixty one a 0.491
and stability scores for two example inputs: back-
ground noise on the left, and the user saying ?sixty
one a? on the right.
State 1 relies on the internal threshold param-
eter, T1. If a partial?s stability score falls below
T1, control remains in State 1 and the partial re-
sult is discarded. If a stability score meets T1, the
prompt is paused and control transitions to State 2.
T1 is initially set to 0 and is adapted as the dialogue
progresses. The adaptation procedure is described
below in Section 3.4. If a final recognition result
is received, control transitions directly to State 3.
Transitioning from State 1 to State 2 is only al-
lowed during the middle 80% of the prompt; oth-
erwise only transitions to State 3 are allowed.1
3.2 State 2: Silent Prediction
Upon entering State 2, Silent Prediction, the
prompt is paused and a timer is started. State 2 re-
quires continuous evidence (at least every T2 ms)
that the ISR is recognizing valid speech and each
time a partial result that meets T1 is received, the
timer is reset. If the timer reaches the time thresh-
old T2, the prompt is resumed and control returns
to State 1. T2 is initially set at 1.0 seconds and is
adapted as the dialogue progresses. Final recogni-
tion results trigger a transition to State 3.
The resumption prompt is constructed using the
temporal position of the VAD specified speech
start to find the percentage of the prompt that was
played up to that point. This percentage is then
reduced by 10% and used to create the resump-
tion prompt by finding the word that is closest to,
but not beyond, the modified percentage. White
space characters and punctuation are used to deter-
mine word boundaries for text-to-speech prompts,
whereas automatically generated word-alignments
are used for pre-recorded prompts.
1We hypothesized that people will rarely respond to the
current prompt during the first 10% of prompt time as over-
laps at the beginning of utterances are commonly initiative
conflicts (Yang and Heeman, 2010). Users may produce
early-onset utterances during the last 10% that should not
stop the prompt as it is not an ?intentional? barge-in.
Figure 1: The Standard Barge-in and PBR Models
3.3 State 3: Completion
State 3, Completion, is entered when a final recog-
nition result is received and determines whether
the current dialogue policy will advance the dia-
logue or not. Here, the PBR model relies on the
ability of the dialogue manager (DM) to produce a
speculative action without transitioning to the next
dialogue state. If the new action will not advance
the dialogue, it is discarded and the recognition
is NUBI. However, if it will advance the dialogue
then it is classified as an Understood Barge-In
(UBI). In the NUBI case, the system either contin-
ues speaking or resumes the current prompt (tran-
sitioning to State 1). In the UBI case, the system
initiates the new speech act after playing a short
reaction sound and the DM transitions to the next
dialogue state. This reaction sound precedes all
speech acts outside the barge-in context but is not
used for resumption or timeout prompts. Note that
by depending solely on the new speech act, our
model does not require access to the DM?s internal
understanding or confidence scoring components.
3.4 Threshold adjustments
States 1 and 2 contain parameters T1 and T2 that
are adapted to the user?s environment. T1 is the
stability threshold used in State 1 and State 2 that
controls how stable an utterance must be before
the prompt should be paused. In quiet environ-
ments ? where only the user?s speech produces
partial results ? a low threshold is desirable as
it enables near-immediate pauses in the prompt.
Conversely, noisy environments yield many spu-
rious partials that (in general) have much lower
stability scores, so a higher threshold is advan-
tageous. T2 is the timing threshold used to re-
sume the prompt during recognition in State 2. In
quiet environments, a higher threshold reduces the
chance that the system will resume its prompt dur-
ing a well-formed user speech. In noisy environ-
386
Figure 2: Example dialogue fragment of PBR Model
ments, a lower threshold allows the system to re-
sume quickly as the NUBI likelihood is greater.
Both T1 and T2 are dependent on the number of
system resumptions, as we view the action of re-
suming the prompt as an indication that the thresh-
old is not correct. With every resumption, the pa-
rameter R is incremented by 1 and, to account for
changing environments, R is decremented by 0.2
for every full prompt that is not paused until it
reaches 0. UsingR, T1 is computed by T1 = 0.17?
R, and T2 by T2 = argmax(0.1, 1? (0.1 ?R)).2
3.5 Method Discussion
The motivation behind the PBR model is both the-
oretical and practical. According to Selfridge and
Heeman (2010), turn-taking is best viewed as a
collaborative process where the turn assignment
should be determined by the importance of the
utterance. During barge-in, the system is speak-
ing and so should only yield the turn if the user?s
speech is more important than its own. For many
domains, we view non-understood input as less
important than the system?s prompt and so, in this
case, the system should not release the turn by
stopping the prompt and initiating a clarifying sub-
dialogue. On the practical side, there is a high
likelihood that non-advancing input is not system
directed, to which the system should neither con-
sume, in terms of belief state updating, nor re-
spond to, in terms of asking for clarification. In
the rare case of non-understood system directed
speech, the user can easily repeat their utterance.
Here, we note that in the event that the user is
backchanneling, the PBR model will behave cor-
rectly and not release the turn.
The PBR approach differs from standard barge-
in approaches in several respects. First, standard
barge-in stops the prompt (i.e., transitions from
State 1 to State 2) if either the VAD or the partial
hypothesis suggests that there is speech; our ap-
proach? using acoustic, language model, and lat-
tice features ? predicts whether the input is likely
to contain an interpretable recognition result. Sec-
2The threshold update values were determined empiri-
cally by the authors.
ond, standard barge-in uses a static threshold; our
approach uses dynamic thresholds that adapt to
the user?s acoustic environment. Parameter adjust-
ments are straightforward since our method auto-
matically classifies each barge-in as NUBI or UBI.
In practice, the prompt will be paused incorrectly
only a few times in a noisy environment, after
which the adaptive thresholds will prevent incor-
rect pauses at the expense of being less responsive
to true user speech. If the noise level decreases,
the thresholds will become more sensitive again,
enabling swifter responses. Finally, with the ex-
ception of Strom and Seneff, standard approaches
always discard the prompt; our approach can re-
sume the prompt if recognition is not understood
or is proceeding poorly, enabling the system to
resume speaking before recognition is complete.
Moreover, resumption yields a natural user expe-
rience as it often creates a repetition disfluency
(?Ok, sixty - sixty one c?), which are rarely no-
ticed by the listener (Martin and Strange, 1968).
An example dialogue fragment is shown in Fig-
ure 2, with the state transitions shown above. Note
the transition from State 2 to State 1, which is the
system resuming speech during recognition. This
recognition stream, produced by non-system di-
rected user speech, does not end until the user says
?repeat? for the last time.
4 Evaluation Results
The PBR model was evaluated during the Spoken
Dialog Challenge 2012-2013 in a live Lets Go!
bus information task. In this task, the public can
access bus schedule information during off hours
in Pittsburgh, PA via a telephonic interaction with
a dialogue system (Raux et al, 2005). The task
can be divided into five sub-tasks: route, origin,
destination, date/time, and bus schedules. The last
sub-task, bus schedules, provides information to
the user whereas the first four gather information.
We entered two systems using the same POMDP-
based DM (Williams, 2012). The first system, the
?Baseline?, used the standard barge-in model with
VAD barge-in detection and barge-in disabled in
387
Figure 3: Estimated success rate for the PBR and Baseline systems. Stars indicate p<0.018 with ?2 test.
a small number of dialogue states that appeared
problematic during initial testing. The second sys-
tem used the PBR model with an Incremental In-
teraction Manager (Selfridge et al, 2012) to pro-
duce speculative actions in State 3. The pub-
lic called both systems during the final weeks of
2011 and the start of 2012. The DM applied a lo-
gistic regression based confidence measure to de-
termine whether the recognition was understood.
Both systems used the AT&T WATSONSM speech
recognizer (Goffin et al, 2005) with the same
sub-task specific rule-based language models and
standard echo cancellation techniques. The beam
width was set to maximize accuracy while still
running faster than real-time. The PBR system
used a WATSON modification to output lattice-
aware partial results.
Call and barge-in statistics are shown in Table
3. Here, we define (potential) barge-in (some-
what imprecisely) as a full recognition that at
some point overlaps with the system prompt, as
determined by the call logs. We show the calls
with barge-in before the bus schedule sub-task was
reached (BI-BS) and the calls with barge-in during
any point of the call (BI All). Since the Baseline
system only enabled barge-in at specific points in
the dialogue, it has fewer instances of barge-in
(Total Barge-In) and fewer barge-in calls. Regret-
fully, due to logging issues with the PBR system,
recognition specific metrics such as Word Error
Rate and true/false barge-in rates are unavailable.
4.1 Estimated Success Rate
We begin by comparing the success rate and
efficiency between the Baseline and PBR sys-
Table 3: Baseline and PBR call/barge-in statistics.
Baseline PBR
Total Calls 1027 892
BI-BS 228 (23%) 345 (39%)
BI All 281 (27%) 483 (54%)
Total Barge-In 829 1388
tems. Since task success can be quite difficult to
measure, we use four increasingly stringent task
success definitions: Bus Times Reached (BTR),
where success is achieved if the call reaches the
bus schedule sub-task; List Navigation (List Nav.),
where success is achieved if the user says ??next?,
?previous?, or ?repeat? ? the intuition being that
if the user attempted to navigate the bus sched-
ule sub-task they were somewhat satisfied with
the system?s performance so far; and Immediate
Exit (BTR2Ex and ListNav2Ex), which further
constrains both of the previous definitions to only
calls that finish directly after the initial visit to the
bus times sub-task. Success rate for the defini-
tions were automatically computed (not manually
labeled). Figure 3 shows the success rate of the
PBR and Baseline systems for all four definitions
of success. It shows, from left to right, Barge-In,
No Barge-In (NBI), and All calls. Here we restrict
barge-in calls to those where barge-in occurred
prior to the bus schedule task being reached.
For the calls with barge-in, a ?2 test finds sig-
nificant differences between the PBR and Base-
line for all four task success definitions. However,
we also found significant differences in the NBI
calls. This was surprising since, when barge-in
is not triggered, both systems are ostensibly the
same. We speculate this could be due to the Base-
line?s barge-in enabling strategy: an environment
that triggers barge-in in the Baseline would always
trigger barge-in in the PBR model, whereas the
converse is not true as the Baseline only enabled
barge-in in some of the states. This means that
there is a potential mismatch when separating the
calls based on barge-in, and so the fairest compar-
ison is using All the calls. This is shown on the far
right of Figure 3. We find that, while the effect is
not as large, there are significant differences in the
success rate for the PBR model for the most and
least stringent success definition, and very strong
trends for the middle two definitions (p < 0.07 for
BTR2Ex and p < 0.054 for List Nav.). Taken as
a whole, we feel this offers compelling evidence
388
Figure 4: Seconds from beginning of dialogue to
reaching the Bus Schedule Information sub-task
that the PBR method is more effective: i.e. yields
higher task completion.
Next, we turn our attention to task efficiency.
For this, we report the amount of clock time from
the beginning of the call to when the Bus Schedule
sub-task was reached. Calls that do not reach this
sub-task are obviously excluded, and PBR times
are adjusted for the reaction sound (explained in
Section 3.3). Task efficiency is reported by cu-
mulative percentage in Figure 4. We find that,
while the NBI call times are nearly identical for
both systems, the PBR barge-in calls are much
faster than the Baseline calls. Here, we do not
feel the previously described mismatch is partic-
ularly problematic as all the calls reached the goal
state and the NBI are nearly identical. In fact, as
more NUBI should actually reduce efficiency, the
potential mismatch only strengthens the result.
Taken together, these results provide substantial
evidence that the PBR model is more effective and
more efficient than the Baseline. In order to ex-
plain PBR?s performance, we explore the effect of
prediction and resumption in isolation.
4.2 State 1: Speaking Prediction
State 1 is responsible for pausing the prompt, the
goal being to pause the prompt for UBI input and
not to pause the prompt for NUBI input. The
prompt is paused if a partial?s stability score meets
or exceeds the T1 threshold. We evaluate the ef-
ficacy of State 1 and T1 by analyzing the statis-
tics of NUBI/UBI input and Paused/Not Paused
(hereafter Continued) prompts. Since resuming
the prompt during recognition affects the recog-
nition outcome, we restrict our analysis to recog-
nitions that do not transition from State 2 back
to State 1. For comparison we show the overall
UBI/NUBI percentages for the Baseline and PBR
systems. This represents the recognition distri-
Table 4: Evaluation of T1, off-line PBR, and Base-
line VAD. For T1 we respectively (?-? split) show
the UBI/NUBI % that are Paused/Continued, the
Paused/Continued % that are UBI/NUBI, and the
percentage over all recognitions
T1 (%) VAD (%)
Paused Continued PBR BL
UBI 72-40-26 28-29-10 36 54
NUBI 61-60-39 39-71-25 64 46
bution for the live Baseline VAD detection and
off-line speculation for the PBR model. Recall
PBR does have VAD activation preceding partial
results and so the off-line PBR VAD shows how
the model would have behaved if it only used the
VAD for detection, as the Baseline does.
Table 4 provides a number of percentages, with
three micro-columns separated by dashes (?-?) for
T1. The first micro-column shows the percent-
age of UBI/NUBI that either Paused or Contin-
ued the prompt (sums to 100 horizontally). The
second micro-column shows the percentage of
Paused/Continued that are UBI/NUBI (sums to
100 vertically). The third micro-column shows
the percentage of each combination (e.g. UBI and
Paused) over all the barge-in recognitions. The
VAD columns show the percentage of UBI/NUBI
that (would) pause the prompt.
We first look at UBI/NUBI percentage that are
Paused/Continued (first micro-column): We find
that 72% of UBI are paused and 28% are Contin-
ued versus 61% of NUBI that are Paused with 39%
Continued. We now look at the Paused/Continued
percentage that are UBI/NUBI (second micro-
column): We find that 40% of Paused are UBI
and 60% are NUBI, whereas 29% of Continued
are UBI and 71% are NUBI. So, while T1 sus-
pends the prompt for the majority of NUBI (not
desirable, though expected since T1 starts at 0),
it has high precision when continuing the prompt.
This reduces the number of times that the prompt
is paused erroneously for NUBI while minimizing
incorrect (UBI) continues. This is clearly shown
by considering all of the recognitions (third micro-
column). We find that PBR erroneously paused
the prompt for 39% of recognitions, as opposed to
64% for the off-line PBR and 46% for the Base-
line. This came at the cost of reducing the number
of correct (UBI) pauses to 26% from 36% (off-line
PBR) and 54% (Baseline VAD).
The results show that the T1 threshold had
389
Figure 5: Secs from Speech Start to Final Result
modest success at discriminating UBI and NUBI;
while continuing the prompt had quite a high
precision for NUBI, the recall was substantially
lower. We note that, since erroneous pauses lead
to resumptions and erroneous continues still lead
to a new speech act, there is minimal cost to these
errors. Furthermore, in our view, reducing the per-
centage of recognitions that pause and resume the
prompt is more critical as these needlessly disrupt
the prompt. In this, T1 is clearly effective, reduc-
ing the percentage from 64% to 39%.
4.3 State 2: Silent Prediction
State 2 governs whether the prompt will remain
paused or be resumed during incremental recogni-
tion. This decision depends on the time parameter
T2, which should trigger resumptions for NUBIs.
Since the act of resuming the prompt during recog-
nition changes the outcome of the recognition, it
is impossible to evaluate how well T2 discrimi-
nated recognition results. However, we can evalu-
ate the effect of that resumption by comparing UBI
percentages between the PBR and Baseline sys-
tems. We first present evidence that T2 is most ac-
tive during longer recognitions, and then show that
longer Baseline recognitions have a lower UBI
percentage than longer PBR recognitions specif-
ically because of T2 resumptions. ?Recognitions?
refer to speech recognition results, with ?longer?
or ?shorter? referring to the clock time between
speech detection and the final recognition result.
We first report the PBR and Baseline response
and recognition time. We separate the PBR barge-
in recognitions into two groups: State 2?State 3,
where the system never transitions from State 2
to State 1, and State 2?State 1, where the sys-
tem resumes the prompt during recognition, tran-
sitioning from State 2 to State 1. The cumulative
percentages of the time from speech detection to
final recognition are shown in Figure 5. We find
that the State 2?State 3 recognitions are far faster
Figure 6: UBI % by minimum recognition time
than the Baseline recognitions, which in turn are
far faster than the State 2?State 1 recognitions.
The difference between PBR and Baseline recog-
nitions implies that T2 has greater activation dur-
ing longer recognitions. Given this, the overall
barge-in response time for PBR should be faster
than the Baseline (as the PBR system is resum-
ing where the Baseline is silent). Indeed this is
the case: the PBR system?s overall mean/median
response time is 1.58/1.53 seconds whereas Base-
line has a mean/median response time of 2.61/1.8
seconds.
The goal of T2 is for the system to resume when
recognition is proceeding poorly, and we have
shown that it is primarily being activated during
longer recognitions. If T2 is functioning properly,
recognition length should be inversely related to
recognition performance, and longer recognitions
should be less likely to be understood. Further-
more, if T2 resumption improves the user?s expe-
rience then longer PBR recognitions should per-
form better than Baseline recognitions of compa-
rable length. Figure 6 presents the UBI percent-
age by the minimum time for recognitions that
reach State 2. We find that, when all recogni-
tions are accounted for (0 second minimum), the
Baseline has a higher rate of UBI. However, as
recognition time increases the Baseline UBI per-
centage decreases (suggesting successful T2 func-
tioning) whereas the PBR UBI percentage actu-
ally increases. Since longer PBR recognitions are
dominated by T2 resumptions, we speculate this
improvement is driven by users repeating or initi-
ating new speech that leads to understanding suc-
cess, as the PBR system is responding where the
Baseline system is silent.
4.4 Resumption
The PBR model relies on resumption to recover
from poor recognitions, either produced in State 2
or State 3. Instead of a resumption, the Baseline
390
Figure 7: Sub-Task Abandonment Rate. NUBI is
different at p < 0.003
system initiates a clarifying sub-dialogue when a
barge-in recognition is not understood. We com-
pare these two behaviors using the call abandon-
ment rate ? the user hangs-up ? of sub-tasks
with and without NUBI. Here, we exclude the Bus
Schedule sub-task as it is the goal state.
Figure 7 shows the call abandonment rate for
sub-tasks that either have or do not have NUBI.
We find that there is a significant difference in
abandoned calls for NUBI sub-tasks between the
two systems (33% vs 48%, p < 0.003 using a ?2
test), but that there is no difference for the calls
that do not have NUBI (7.6% vs 8.4%). This re-
sult shows that prompt resumption is viewed far
more favorably by users than initiating a clarify-
ing sub-dialogue.
5 Discussion and Conclusion
The above results offer strong evidence that the
PBR model increases task success and efficiency,
and we found that all three states contribute to
the improved performance by creating a more ro-
bust, responsive, and natural interaction. T1 pre-
diction in State 1 reduced the number of spurious
prompt suspensions, T2 prediction in State 2 led to
improved understanding performance, and prompt
resumption (States 2 and 3) reduced the number of
abandoned calls.
An important feature of the Prediction-based
Barge-in Response model is that, while it lever-
ages incremental speech processing for barge-in
processing, it does not require an incremental di-
alogue manager to drive its behavior. Since the
model is also domain independent and does not
require access to internal dialogue manager com-
ponents, it can easily be incorporated into any ex-
isting dialogue system. However, one limitation of
the current model is that the prediction thresholds
are hand-crafted. We also believe that substan-
tial improvements can be made by explicitly at-
tempting to predict eventual understanding instead
of using the stability score and partial production
rate as a proxy. Furthermore, the PBR model does
not distinguish between the causes of the non-
understanding, specifically whether the input con-
tained in-domain user speech, out-of-domain user
speech, or background noise. This case is specifi-
cally applicable in domains where system and user
speech are in the same channel, such as interact-
ing via speaker phone. In this context, the system
should be able to initiate a clarifying sub-dialogue
and release the turn, as the system must be more
sensitive to the shared acoustic environment and
so its current prompt may be less important than
the user?s non-understood utterance.
The results challenge a potential assumption re-
garding barge-in: that barge-in indicates greater
user pro-activity and engagement with the task.
One of the striking findings was that dialogues
with barge-in are slower and less successful than
dialogues without barge-in. This suggests that,
for current systems, dialogues with barge-in are
more indicative of environmental difficulty than
user pro-activity. The superior performance of
the PBR model, which is explicitly resistant to
non-system directed speech, implies that domi-
nant barge-in models will have increasingly lim-
ited utility as spoken dialogue systems become
more prevalent and are used in increasingly dif-
ficult environments. Furthermore, within the con-
text of overall dialogue systems, the PBR model?s
performance emphasizes the importance of contin-
uous processing for future systems.
This paper has proposed and evaluated the
Prediction-based Barge-in Response model. This
model?s behavior is driven by continuously pre-
dicting whether a barge-in recognition will be un-
derstood successfully, and combines incremental
speech processing techniques with a prompt re-
sumption procedure. Using a live dialogue task
with real users, we evaluated this model against
the standard barge-in model and found that it led
to improved performance in both task success and
efficiency.
Acknowledgments
Many thanks to Vincent Goffin for help with this
work, and to the anonymous reviewers for their in-
sightful comments and critique. We acknowledge
funding from the NSF under grant IIS-0713698.
391
References
T. Baumann, M. Atterer, and D. Schlangen. 2009. As-
sessing and improving the performance of speech
recognition for incremental systems. In Proc.
NAACL: HLT, pages 380?388. Association for Com-
putational Linguistics.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur,
A. Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi,
and M. Saraclar. 2005. The AT&T WATSON
speech recognizer. In Proceedings of ICASSP, pages
1033?1036.
James G Martin and Winifred Strange. 1968. The per-
ception of hesitation in spontaneous speech. Percep-
tion & Psychophysics, 3(6):427?438.
Ian McGraw and Alexander Gruenstein. 2012. Es-
timating word-stability during incremental speech
recognition. In in Proc. of Interspeech 2012.
A. Raux, B. Langner, D. Bohus, A.W. Black, and
M. Eskenazi. 2005. Lets go public! taking a spo-
ken dialog system to the real world. In in Proc. of
Interspeech 2005.
A. Raux. 2008. Flexible Turn-Taking for Spoken Dia-
log Systems. Ph.D. thesis, CMU.
Richard C Rose and Hong Kook Kim. 2003. A
hybrid barge-in procedure for more reliable turn-
taking in human-machine dialog systems. In Auto-
matic Speech Recognition and Understanding, 2003.
ASRU?03. 2003 IEEE Workshop on, pages 198?203.
IEEE.
E.O. Selfridge and P.A. Heeman. 2010. Importance-
Driven Turn-Bidding for spoken dialogue systems.
In Proc. of ACL 2010, pages 177?185. Association
for Computational Linguistics.
E.O. Selfridge, I. Arizmendi, P.A. Heeman, and J.D.
Williams. 2011. Stability and accuracy in incre-
mental speech recognition. In Proceedings of the
SIGdial 2011.
E.O. Selfridge, I. Arizmendi, P.A. Heeman, and J.D.
Williams. 2012. Integrating incremental speech
recognition and pomdp-based dialogue systems. In
Proceedings of the SIGdial 2012.
Nikko Stro?m and Stephanie Seneff. 2000. Intelligent
barge-in in conversational systems. Procedings of
ICSLP.
Jason D Williams. 2012. A critical analysis of two sta-
tistical spoken dialog systems in public use. In Spo-
ken Language Technology Workshop (SLT), 2012
IEEE, pages 55?60. IEEE.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue?. Computer Speech
Language, 24(2):175 ? 189.
392
A Appendix
This diagram represents the possible operating positions the Prediction-based Barge-in Response
model can be in. If the prompt is complete, the PBR model applies the dialogue policy to the final
recognition result and initiates the on-policy speech act. If the prompt was finished without being paused
it decrements R. In the latter case (barge-in), it operates using the three states as described in Section 2.
When a partial is recognized the Stability Score is computed and compared to the T1 threshold parame-
ter. If the score is below T1 the partial is discarded. Otherwise, if the model is in State 1 (the prompt is
on) the prompt is paused, a timer is started, and control transitions to State 2. If the model is in State 2
the timer is restarted. After transitioning to State 2, control only returns to State 1 if the timer exceeds
T2. At this time, the prompt is resumed and the resumption parameter R is incremented. Control im-
mediately transitions to State 3 if a final recognition result is received. The result is evaluated by the
dialogue manager, and the new speech act is returned. If the speech act indicates the recognition was not
understood successfully, the system either resumes (if in State 1) or continues (if in State 2). In the case
of resumption, R is incremented. If the new speech act indicates understanding success, the new speech
is immediately produced.
393
Proceedings of the SIGDIAL 2014 Conference, pages 257?259,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
MVA: The Multimodal Virtual Assistant
Michael Johnston
1
, John Chen
1
, Patrick Ehlen
2
, Hyuckchul Jung
1
, Jay Lieske
2
, Aarthi Reddy
1
,
Ethan Selfridge
1
, Svetlana Stoyanchev
1
, Brant Vasilieff
2
, Jay Wilpon
1
AT&T Labs Research
1
, AT&T
2
{johnston,jchen,ehlen,hjung,jlieske,aarthi,
ethan,sveta,vasilieff,jgw}@research.att.com
Abstract
The Multimodal Virtual Assistant (MVA)
is an application that enables users to plan
an outing through an interactive multi-
modal dialog with a mobile device. MVA
demonstrates how a cloud-based multi-
modal language processing infrastructure
can support mobile multimodal interac-
tion. This demonstration will highlight in-
cremental recognition, multimodal speech
and gesture input, contextually-aware lan-
guage understanding, and the targeted
clarification of potentially incorrect seg-
ments within user input.
1 Introduction
With the recent launch of virtual assistant appli-
cations such as Siri, Google Now, S-Voice, and
Vlingo, spoken access to information and services
on mobile devices has become commonplace. The
Multimodal Virtual Assistant (MVA) project ex-
plores the application of multimodal dialog tech-
nology in the virtual assistant landscape. MVA de-
parts from the existing paradigm for dialog-based
mobile virtual assistants that display the unfold-
ing dialog as a chat display. Instead, the MVA
prototype situates the interaction directly within a
touch-based interface that combines a map with
visual information displays. Users can interact
using combinations of speech and gesture inputs,
and the interpretation of user commands depends
on both map and GUI display manipulation and
the physical location of the device.
MVA is a mobile application that allows users
to plan a day or evening out with friends using
natural language and gesture input. Users can
search and browse over multiple interconnected
domains, including music events, movie show-
ings, and places to eat. They can specify multi-
ple parameters in natural language, such as ?Jazz
concerts around San Francisco next Saturday?. As
users find interesting events and places, they can
be collected together into plans and shared with
others. The central components of the graph-
ical user interface are a dynamic map showing
business and event locations, and an information
display showing the current recognition, system
prompts, search result listing, or plans (Figure 1).
Figure 1: MVA User Interface
Spoken input begins when the user taps a micro-
phone button on the display. As the user speaks,
incremental speech recognition results appear. In
addition to enabling voice input, the microphone
button also activates the map as a drawing can-
vas, and enables the user to combine speech with
drawing in coordinated multimodal commands.
For example, a user might say, ?Movies playing
tonight in this area? while simultaneously outlin-
ing a relevant area on the map. Or a user may say,
?Restaurants? while drawing a line down a spe-
cific street. MVA determines the intent and dis-
ambiguates concepts in the input in order to re-
turn relevant results. MVA then responds to user
input multimodally, by updating the display and
using speech synthesis to summarize results, pro-
vide feedback, or make requests for clarification
and additional information.
257
2 Sample Interaction
In Figure 2 we present a sample of interaction
from MVA that illustrates some of its capabilities.
The user starts with a spoken natural language
query where they specify some constraints: the
type of music (jazz), location (San Francisco), and
time (tomorrow). The system gets low confidence
on the location, so it constructs a targeted clarifi-
cation for clarifying only that constraint. The user
repeats the location, and then the system searches
for events meeting the user?s constraints. The user
then reviews the results, and follows on with a
refinement: ?What about blues??. Even though
many parameters in this query are underspecified,
the system applies contextually-aware natural lan-
guage understanding and interprets this as ?Blues
concerts near San Francisco tomorrow?. After
selecting a concert, the user then searches for a
restaurant nearby. The location of the concert re-
mains salient. The user follows up with a mul-
timodal query combining speech and gesture to
search for similar restaurants in an adjoining area.
U: ?Jazz concerts near San Francisco tomorrow.?
S: ?Where did you want to see jazz tomorrow??
U: ?San Francisco.?
S: ?I found 20 jazz concerts in San
Francisco tomorrow.?
[Zooms map to San Francisco and displays
pins on map and list of results]
U: ?What about blues??
S: ?I found 20 blues concerts in
San Francisco tomorrow.?
U: [Clicks on a concert listing and adds it
to the plan]
U: ?Sushi restaurants near there.?
S: ?I found 10 sushi restaurants.?
U: ?What about here??
[Circles adjoining area on map]
S: ?I found 5 sushi restaurants in
the area you indicated?
Figure 2: Sample Interaction
3 System Architecture
Figure 3 shows the underlying multimodal assis-
tant architecture supporting the MVA app. The
user interacts with a native iOS client. When the
user taps the microphone icon, this initiates the
flow of audio interleaved with gesture and context
information streamed over a WebSocket connec-
tion to the platform.
This stream of interleaved data is handled at
the server by a multimodal natural language pro-
cessing pipeline. This fields incoming packets of
Figure 3: MVA Multimodal assistant Architecture
data from the client, demuxes the incoming data
stream, and sends audio, ink traces, and context
information to three modules that operate in par-
allel. The audio is processed using the AT&T
Watson
SM
speech recognition engine (Goffin et
al., 2005). Recognition is performed using a dy-
namic hierarchical language model (Gilbert et al.,
2011) that combines a statistical N-gram language
model with weighted sub-grammars. Ink traces
are classified into gestures using a linear classi-
fier. Speech recognition results serve as input to
two NLUmodules. A discriminative stochastic se-
quence tagger assigns tags to phrases within the
input, and then the overall string with tags is as-
signed by a statistical intent classifier to one of
a number of intents handled by the system e.g.
search(music event), refine(location).
The NLU results are passed along with gesture
recognition results and the GUI and device context
to a multimodal dialog manager. The contextual
resolution component determines if the input is a
query refinement or correction. In either case, it
retrieves the previous command from a user con-
text store and combines the new content with the
context through destructive unification (Ehlen and
Johnston, 2012). A location salience component
then applies to handle cases where a location is
not specified verbally. This component uses a su-
pervised classifier to select from among a series
of candidate locations, including the gesture (if
present), the current device location, or the current
map location (Ehlen and Johnston, 2010).
The resolved semantic interpretation of the ut-
terance is then passed to a Localized Error Detec-
tion (LED) module (Stoyanchev et al., 2012). The
LEDmodule contains two maximum entropy clas-
sifiers that independently predict whether a con-
258
cept is present in the input, and whether a con-
cept?s current interpretation is correct. These clas-
sifiers use word scores, segment length, confu-
sion networks and other recognition and context
features. The LED module uses these classifiers
to produce two probability distributions; one for
presence and one for correctness. These distri-
butions are then used by a Targeted Clarification
component (TC) to either accept the input as is,
reject all of the input, or ask a targeted clarifica-
tion question (Stoyanchev et al., 2013). These de-
cisions are currently made using thresholds tuned
manually based on an initial corpus of user inter-
action withMVA. In the targeted clarification case,
the input is passed to the natural language gen-
eration component for surface realization, and a
prompt is passed back to the client for playback
to the user. Critically, the TC component decides
what to attempt to add to the common ground
by explicit or implicit confirmation, and what to
explicitly query from the user; e.g. ?Where did
you want to see jazz concerts??. The TC com-
ponent also updates the context so that incoming
responses from the user can be interpreted with re-
spect to the context set up by the clarification.
Once a command is accepted by the multimodal
dialog manager, it is passed to the Semantic Ab-
straction Layer (SAL) for execution. The SAL in-
sulates natural language dialog capabilities from
the specifics of any underlying external APIs that
the system may use in order to respond to queries.
A general purpose time normalization component
projects relative time expressions like ?tomorrow
night? or ?next week? onto a reference timeframe
provided by the client context and estimates the
intended time interval. A general purpose location
resolution component maps from natural language
expressions of locations such as city names and
neighborhoods to specific geographic coordinates.
These functions are handled by SAL?rather than
relying on any time and location handling in the
underlying information APIs?to provide consis-
tency across application domains.
SAL also includes mechanisms for category
mapping; the NLU component tags a portion
of the utterance as a concept (e.g., a mu-
sic genre or a cuisine) and SAL leverages
this information to map a word sequence to
generic domain-independent ontological represen-
tations/categories that are reusable across different
backend APIs. Wrappers in SAL map from these
categories, time, and location values to the spe-
cific query language syntax and values for each
specific underlying API. In some cases, a single
natural language query to MVA may require mul-
tiple API calls to complete, and this is captured
in the wrapper. SAL also handles API format dif-
ferences by mapping all API responses into a uni-
fied format. This unified format is then passed to
our natural language generation component to be
augmented with prompts, display text, and instruc-
tions to the client for updating the GUI. This com-
bined specification of a multimodal presentation is
passed to the interaction manager and routed back
to the client to be presented to the user.
In addition to testing the capabilities of our mul-
timodal assistant platform, MVA is designed as a
testbed for running experiments with real users.
Among other topics, we are planning experiments
with MVA to evaluate methods of multimodal in-
formation presentation and natural language gen-
eration, error detection and error recovery.
Acknowledgements
Thanks to Mike Kai and to Deepak Talesra for
their work on the MVA project.
References
Patrick Ehlen and Michael Johnston. 2010. Location
grounding in multimodal local search. In Proceed-
ings of ICMI-MLMI, pages 32?39.
Patrick Ehlen and Michael Johnston. 2012. Multi-
modal dialogue in mobile local search. In Proceed-
ings of ICMI, pages 303?304.
Mazin Gilbert, Iker Arizmendi, Enrico Bocchieri, Dia-
mantino Caseiro, Vincent Goffin, Andrej Ljolje,
Mike Phillips, Chao Wang, and Jay G. Wilpon.
2011. Your mobile virtual assistant just got smarter!
In Proceedings of INTERSPEECH, pages 1101?
1104. ISCA.
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,
Dilek Hakkani-Tur, Andrej Ljolje, S. Parthasarathy,
Mazim Rahim, Giuseppe Riccardi, and Murat Sar-
aclar. 2005. The AT&T WATSON speech recog-
nizer. In Proceedings of ICASSP, pages 1033?1036,
Philadelphia, PA, USA.
Svetlana Stoyanchev, Philipp Salletmayer, Jingbo
Yang, and Julia Hirschberg. 2012. Localized de-
tection of speech recognition errors. In Proceedings
of SLT, pages 25?30.
Svetlana Stoyanchev, Alex Liu, and Julia Hirschberg.
2013. Modelling human clarification strategies. In
Proceedings of SIGDIAL 2013, pages 137?141.
259
