Annotation-Based Multimedia Summarization and Translation
Katashi Nagao
Dept. of Information Engineering
Nagoya University
and CREST, JST
Furo-cho, Chikusa-ku,
Nagoya 464-8603, Japan
nagao@nuie.nagoya-u.ac.jp
Shigeki Ohira
School of Science and
Engineering
Waseda University
3-4-1 Okubo, Shinjuku-ku,
Tokyo 169-8555, Japan
ohira@shirai.info.waseda.ac.jp
Mitsuhiro Yoneoka
Dept. of Computer Science
Tokyo Institute of Technology
2-12-1 O-okayama, Meguro-ku,
Tokyo 152-8552, Japan
yoneoka@img.cs.titech.ac.jp
Abstract
This paper presents techniques for multimedia
annotation and their application to video sum-
marization and translation. Our tool for anno-
tation allows users to easily create annotation
including voice transcripts, video scene descrip-
tions, and visual/auditory object descriptions.
The module for voice transcription is capable
of multilingual spoken language identification
and recognition. A video scene description con-
sists of semi-automatically detected keyframes
of each scene in a video clip and time codes of
scenes. A visual object description is created
by tracking and interactive naming of people
and objects in video scenes. The text data in
the multimedia annotation are syntactically and
semantically structured using linguistic annota-
tion. The proposed multimedia summarization
works upon a multimodal document that con-
sists of a video, keyframes of scenes, and tran-
scripts of the scenes. The multimedia transla-
tion automatically generates several versions of
multimedia content in different languages.
1 Introduction
Multimedia content such as digital video is be-
coming a prevalent information source. Since
the volume of such content is growing to huge
numbers of hours, summarization is required to
effectively browse video segments in a short time
without missing significant content. Annotating
multimedia content with semantic information
such as scene/segment structures and metadata
about visual/auditory objects is necessary for
advanced multimedia content services. Since
natural language text such as a voice transcript
is highly manageable, speech and natural lan-
guage processing techniques have an essential
role in our multimedia annotation.
We have developed techniques for semi-
automatic video annotation integrating a mul-
tilingual voice transcription method, some
video analysis methods, and an interactive
visual/auditory annotation method. The
video analysis methods include automatic color
change detection, characterization of frames,
and scene recognition using similarity between
frame attributes.
There are related approaches to video annota-
tion. For example, MPEG-7 is an effort within
the Moving Picture Experts Group (MPEG) of
ISO/IEC that is dealing with multimedia con-
tent description (MPEG, 2002). MPEG-7 can
describe indeces, notes, and so on, to retrieve
necessary parts of content speedily. However, it
takes a high cost to add these descriptions by
hands. The method of extracting them auto-
matically through the video/audio analysis is
vitally important. Our method can be inte-
grated into tools for authoring MPEG-7 data.
The linguistic description scheme, which will be
a part of the amendment to MPEG-7, should
play a major role in this integration.
Using such annotation data, we have also de-
veloped a system for advanced multimedia pro-
cessing such as video summarization and trans-
lation. Our video summary is not just a shorter
version of the original video clip, but an in-
teractive multimedia presentation that shows
keyframes of important scenes and their tran-
scripts in Web pages and allow users to interac-
tively modify summary. The video summariza-
tion is customizable according to users? favorite
size and keywords. When a user?s client device
is not capable of video playing, our system tran-
forms video to a document that is the same as
a Web document in HTML format.
The multimedia annotation can make deliv-
ery of multimedia content to different devices
very effective. Dissemination of multimedia
content will be facilitated by annotation on the
usage of the content in different purposes, client
devices, and so forth. Also, it provides object-
level description of multimedia content which
allows a higher granularity of retrieval and pre-
sentation in which individual regions, segments,
objects and events in image, audio and video
data can be differentially accessed depending on
publisher and user preferences, network band-
width and client capabilities.
2 Multimedia Annotation
Multimedia annotation is an extension of doc-
ument annotation such as GDA (Global Docu-
ment Annotation) (Hasida, 2002). Since natu-
ral language text is more tractable and mean-
ingful than binary data of visual (image and
moving picture) and auditory (sound and voice)
content, we associate text with multimedia con-
tent in several ways. Since most video clips
contain spoken narrations, our system converts
them into text and integrates them into video
annotation data. The text in the multimedia
annotation is linguistically annotated based on
GDA.
2.1 Multimedia Annotation Editor
We developed an authoring tool called multi-
media annotation editor capable of video scene
change detection, multilingual voice transcrip-
tion, syntactic and semantic analysis of tran-
scripts, and correlation of visual/auditory seg-
ments and text.
Figure 1: Multimedia Annotation Editor
An example screen of the editor is shown in
Figure 1. The editor screen consists of three
windows. One window (top) shows a video con-
tent, automatically detected keyframes in the
video, and an automatically generated voice
transcript. The second window (left bottom)
enables the user to edit the transcript and mod-
ify an automatically analyzed linguistic markup
structure. The third window (right bottom)
shows graphically a linguistic structure of the
selected sentence in the second window.
The editor is capable of basic natural lan-
guage processing and interactive disambigua-
tion. The user can modify the results of the
automatically analyzed multimedia and linguis-
tic (syntactic and semantic) structures.
2.2 Linguistic Annotation
Linguistic annotation has been used to make
digital documents machine-understandable, and
to develop content-based presentation, retrieval,
question-answering, summarization, and trans-
lation systems with much higher quality than
is currently available. We have employed the
GDA tagset as a basic framework to describe
linguistic and semantic features of documents.
The GDA tagset is based on XML (Extensible
Markup Language) (W3C, 2002), and designed
to be as compatible as possible with TEI (TEI,
2002), CES (CES, 2002), and EAGLES (EA-
GLES, 2002).
An example of a GDA-tagged sentence fol-
lows:
<su><np opr="agt" sem="time0">Time</np>
<v sem="fly1">flies</v>
<adp opr="eg"><ad sem="like0">like</ad>
<np>an <n sem="arrow0">arrow</n></np>
</adp>.</su>
The <su> element is a sentential unit. The
other tags above, <n>, <np>, <v>, <ad> and
<adp> mean noun, noun phrase, verb, adnoun
or adverb (including preposition and postposi-
tion), and adnominal or adverbial phrase, re-
spectively.
The opr attribute encodes a relationship
in which the current element stands with
respect to the element that it semantically
depends on. Its value denotes a binary
relation, which may be a thematic role
such as agent, patient, recipient, etc., or a
rhetorical relation such as cause, concession,
etc. For instance, in the above sentence,
<np opr="agt" sem="time0">Time</np>
depends on the second element
<v sem="fly1">flies</v>. opr="agt"
means that Time has the agent role with
respect to the event denoted by flies. The sem
attribute encodes a word sense.
Linguistic annotation is generated by auto-
matic morphological analysis, interactive sen-
tence parsing, and word sense disambiguation
by selecting the most appropriate item in the
domain ontology. Some research issues on lin-
guistic annotation are related to how the anno-
tation cost can be reduced within some feasible
levels. We have been developing some machine-
guided annotation interfaces to simplify the an-
notation work. Machine learning mechanisms
also contribute to reducing the cost because
they can gradually increase the accuracy of au-
tomatic annotation.
In principle, the tag set does not depend on
language, but as a first step we implemented a
semi-automatic tagging system for English and
Japanese.
2.3 Video Annotation
The linguistic annotation technique has an im-
portant role in multimedia annotation. Our
video annotation consists of creation of text
data related to video content, linguistic anno-
tation of the text data, automatic segmentation
of video, semi-automatic linking of video seg-
ments with corresponding text data, and inter-
active naming of people and objects in video
scenes.
To be more precise, video annotation is per-
formed through the following three steps.
First, for each video clip, the annotation sys-
tem creates the text corresponding to its con-
tent. We developed a method for creation of
voice transcripts using speech recognition en-
gines. It is called multilingual voice transcrip-
tion and described later.
Second, some video analysis techniques are
applied to characterization of visual segments
(i.e., scenes) and individual video frames. For
example, by detecting significant changes in the
color histogram of successive frames, frame se-
quences can be separated into scenes.
Also, by matching prepared templates to in-
dividual regions in the frame, the annotation
system identifies objects. The user can specify
significant objects in some scene in order to re-
duce the time to identify target objects and to
obtain a higher recognition accuracy. The user
can name objects in a frame simply by selecting
words in the corresponding text.
Third, the user relates video segments to text
segments such as paragraphs, sentences, and
phrases, based on scene structures and object-
name correspondences. The system helps the
user select appropriate segments by prioritiz-
ing them based on the number of the detected
objects, camera motion, and the representative
frames.
2.4 Multilingual Voice Transcription
The multimedia annotation editor first extracts
the audio data from a target video clip. Then,
the extracted audio data is divided into left and
right channels. If the average for the difference
of the audio signals of the two channels exceeds
a certain threshold, they are considerd different
and transfered to the multilingual speech identi-
fication and recognition module. The output of
the module is a structured transcript contain-
ing time codes, word sequences, and language
information. It is described in XML format as
shown in Figure 2.
<transcript lang="en" channel="l">
<w in="20.264000" out="20.663000">Web grabber </w>
<w in="20.663000" out="21.072000">is a </w>
<w in="21.072000" out="21.611000">very simple </w>
<w in="21.611000" out="22.180000">utility </w>
<w in="22.180000" out="22.778000">that is </w>
<w in="22.778000" out="23.856000">attached to </w>
<w in="23.856000" out="24.215000">Netscape </w>
<w in="24.215000" out="24.934000">as a pull down menu </w>
<w in="24.934000" out="25.153000">and </w>
<w in="25.153000" out="25.462000">allows you </w>
<w in="25.462000" out="25.802000">take </w>
<w in="25.802000" out="26.191000">your Web content </w>
<w in="26.191000" out="27.039000">whether it?s a </w>
<w in="27.039000" out="27.538000">MPEG file </w>
...
</transcript>
Figure 2: Transcript Data
Our multilingual video transcriptor automat-
ically generates transcripts with time codes and
provides their reusable data structure which al-
lows easy manual corretion. An example screen
of the mulitilingual voice transcriptor is shown
in Figure 3.
Left Channel
Right Channel
Figure 3: Multilingual Voice Transcriptor
2.4.1 Multilingual Speech Identification
and Recognition
The progress of speech recognition technol-
ogy makes it comparatively easy to transform
speech into text, but spoken language identi-
fication is needed for processing multilingual
speech, because speech recognition technology
assumes that the language used is known.
While researchers have been working on the
multilingual speech identification, few applica-
tions based on this technology has been actually
used except a telephony speech translation sys-
tem. In the case of the telephone translation
system, the information of the language used
is self-evident; at least, the speaker knows; so
there are little needs and advantages of develop-
ing a multilingual speech identification system.
On the other hand, speech data in video do
not always have the information about the lan-
guage used. Due to the recent progress of digital
broadcasting and the signal compression tech-
nology, the information about the language is
expected to accompany the content in the fu-
ture. But most of the data available now do
not have it, so a large amount of labor is needed
to identify the language. Therefore, the multi-
lingual speech identification has a large part to
play with unknown-language speech input.
A process of multilingual speech identification
is shown in Figure 4. Our method determines
the language of input speech using a simple dis-
criminant function based on relative scores ob-
tained from multiple speech recognizers working
in parallel (Ohira et al, 2001).
Figure 4: Configuration of Spoken Language
Identification Unit
Multiple speech recognition engines work si-
multaneously on the input speech. It is as-
sumed that each speech recognition engine has
the speaker independent model, and each recog-
nition output word has a score within a constant
range dependent on each engine.
When a speech comes, each recognition en-
gine outputs a word sequence with scores. The
discriminant unit calculates a value of a dis-
criminant function using the scores for every
language. The engine with the highest average
discriminant value is selected and the language
is determined by the engine, whose recognition
result is accepted as the transcript. If there is
no distinct difference between discriminant val-
ues, that is not higher than a certain threshold,
a judgment is entrusted to the user.
Our technique is simple, it uses the exist-
ing speech recognition engines tuned in each
language without a special model for language
identification and acoustic features.
Combining the voice transcription and the
video image analysis, our tool enables users to
create and edit video annotation data semi-
automatically. The entire process is as shown
in Figure 5.
Figure 5: Multilingual Video Data Analysis
Our system drastically reduces the overhead
on the user who analyzes and manages a large
collection of video content. Furthermore, it
makes conventional natural language processing
techniques applicable to multimedia processing.
2.5 Scene Detection and Visual Object
Tracking
As mentioned earlier, visual scene changes are
detected by searching for significant changes in
the color histogram of successive frames. Then,
frame sequences can be divided into scenes. The
scene description consists of time codes of the
start and end frames, a keyframe (image data
in JPEG format) filename, a scene title, and
some text representing topics. Additionally,
when the user specifies a particular object in a
frame by mouse-dragging a rectangular region,
an automatic object tracking is executed and
time codes and motion trails in the frame (series
of coordinates for interpolation of object move-
ment) are checked out. The user can name the
detected visual objects interactively. The visual
object description includes the object name, the
related URL, time codes and motion trails in the
frame.
Our multimedia annotation also contains de-
scriptions on auditory objects in video. The au-
ditory objects can be detected by acoustic anal-
ysis on the user specified sound sequence visual-
ized in waveform. An example scene description
in XML format is shown in Figure 6, and an ex-
ample object description in Figure 7.
3 Multimedia Summarization and
Translation
Based on multimedia annotation, we have de-
veloped a system for multimedia (especially,
<scene>
<seg in="0.066733" out="11.945279"
keyframe="0.187643"/>
<seg in="11.945279" out="14.447781"
keyframe="12.004385"/>
<seg in="14.447781" out="18.685352"
keyframe="14.447781"/>
...
</scene>
Figure 6: Scene Description
<object>
<vobj begin="1.668335" end="4.671338" name="David"
description="anchor" img="o0000.jpg"
link="http://...">
<area time="1.668335" top="82" left="34"
width="156" height="145"/>
<area ... />
</vobj>
...
</object>
Figure 7: Object Description
video) summarization and translation. One of
the main functions of the system is to gener-
ate an interactive HTML (HyperText Markup
Language) document from multimedia content
with annotation data for interactive multime-
dia presentation, which consists of an embedded
video player, hyperlinked keyframe images, and
linguistically-annotated transcripts. Our sum-
marization and translation techniques are ap-
plied to the generated document called a multi-
modal document.
There are some previous work on multime-
dia summarization such as Informedia (Smith
and Kanade, 1995) and CueVideo (Amir et al,
1999). They create a video summary based on
automatically extracted features in video such
as scene changes, speech, text and human faces
in frames, and closed captions. They can pro-
cess video data without annotations. However,
currently, the accuracy of their summarization
is not for practical use because of the failure of
automatic video analysis. Our approach to mul-
timedia summarization attains sufficient quality
for use if the data has enough semantic informa-
tion. As mentioned earlier, we have developed
a tool to help annotators to create multimedia
annotation data. Since our annotation data is
declarative, hence task-independent and versa-
tile, the annotations are worth creating if the
multimedia content will be frequently used in
different applications such as automatic editing
and information extraction.
3.1 Multimodal Document
Video transformation is an initial process
of multimedia summarization and translation.
The transformation module retrieves the anno-
tation data accumulated in an annotation repos-
itory (XML database) and extracts necessary
information to generate a multimodal docu-
ment. The multimodal document consists of an
embedded video window, keyframes of scenes,
and transcipts aligned with the scenes as shown
in Figure 8. The resulting document can be
summarized and translated by the modules ex-
plained later.
Figure 8: Multimodal Document
This operation is also beneficial for people
with devices without video playing capabil-
ity. In this case, the system creates a simpli-
fied version of multimodal document containing
only keyframe images of important scenes and
summarized transcripts related to the selected
scenes.
3.2 Video Summarization
The proposed video summarization is per-
formed as a by-product of text summariza-
tion. The text summarization is an appli-
cation of linguistic annotation. The method
is cohesion-based and employs spreading acti-
vation to calculate the importance values of
words and phrases in the document (Nagao and
Hasida, 1998).
Thus, the video summarization works in
terms of summarization of a transcript from
multimedia annotation data and extraction of
the video scene related to the summary. Since
a summarized transcript contains important
words and phrases, corresponding video se-
quences will produce a collection of significant
scenes in the video. The summarization results
in a revised version of multimodal documemt
that contains keyframe images and summa-
rized transcripts of selected important scenes.
Keyframes of less important scenes are shown
in a smaller size. An example screen of a sum-
marized multimodal document is shown in Fig-
ure 9.
Figure 9: Summarized Multimodal Document
The vertical time bar in the middle of the
screen of multimodal document represents scene
segments whose color indicates if the segment
is included in the summary or not. The
keyframe images are linked with their corre-
sponding scenes so that the user can see the
scene by just clicking its related image. The
user can also access information about objects
such as people in the keyframe by dragging a
rectangular region enclosing them. The infor-
mation appears in external windows. In the case
of auditory objects, the user can select them by
clicking any point in the time bar.
3.3 Video Translation
One type of our video translation is achieved
through the following procedure. First, tran-
scripts in the annotation data are translated
into different languages for the user choice, and
then, the results are shown as subtitles syn-
chronized with the video. The video transla-
tion module invokes an annotation-based text
translation mechanism. Text translation is also
greatly improved by using linguistic annotation
(Watanabe et al, 2002).
The other type of translation is performed in
terms of synchronization of video playing and
speech synthesis of the translation results. This
translation makes another-language version of
the original video clip. If comments, notes, or
keywords are included in the annotation data
on visual/auditory objects, then they are also
translated and shown on a popup window.
In the case of bilingual broadcasting, since
our annotation system generates transcripts in
every audio channel, multimodal documents can
be coming from both channels. The user can
easily select a favorite multimodal document
created from one of the channels. We have also
developed a mechanism to change the language
to play depending on the user profile that de-
scribes the user?s native language.
4 Concluding Remarks
We have developed a tool to create multime-
dia annotation data and a mechanism to ap-
ply such data to multimedia summarization and
translation. The main component of the anno-
tation tool is a multilingual voice transcriptor to
generate transcripts from multilingual speech in
video clips. The tool also extracts scene and ob-
ject information semi-automatically, describes
the data in XML format, and associates the
data with content.
We also presented some advanced applica-
tions on multimedia content based on annota-
tion. We have implemented video-to-document
transformation that generates interactive multi-
modal documents, video summarization using a
text summarization technique, and video trans-
lation.
Linguistic processing is an essential task in
those applications so that natural language
technologies are still very important in process-
ing multimedia content.
Our future work includes a more efficient and
flexible retrieval of multimedia content for re-
quests in spoken and written natural language.
The retrieval of spoken documents has also been
evaluated in a subtask ?SDR (Spoken Docu-
ment Retrieval) track? at TREC (Text RE-
trieval Conference) (TREC, 2002). Johnson
(Johnson, 2001) suggested from his group?s ex-
perience on TREC-9 that new challenges such
as use of non-lexical information derived di-
rectly from the audio and integration with video
data are significant works for the improvement
of retrieval performance and usefulness. We,
therefore, believe that our research has signifi-
cant impacts and potetials on the content tech-
nology.
References
A. Amir, S. Srinivasan, D. Ponceleon, and
D. Petkovic. 1999. CueVideo: Automated index-
ing of video for searching and browsing. In Pro-
ceedings of SIGIR?99.
CES. 2002. Corpus Encoding Standard.
http://www.cs.vassar.edu/CES/.
EAGLES. 2002. EAGLES online.
http://www.ilc.pi.cnr.it/EAGLES/home.html.
Koiti Hasida. 2002. Global Document Annotation.
http://i-content.org/GDA/.
S. E. Johnson. 2001. Spoken document retrieval for
TREC-9 at Cambridge University. In Proceedings
of Text REtrieval Conference (TREC-9).
MPEG. 2002. MPEG-7 context and objectives.
http://drogo.cselt.stet.it/mpeg/standards/mpeg-
7/mpeg-7.htm.
Katashi Nagao and Ko?iti Hasida. 1998. Automatic
text summarization based on the Global Doc-
ument Annotation. In Proceedings of the Sev-
enteenth International Conference on Computa-
tional Linguistics (COLING-98), pages 917?921.
Shigeki Ohira, Mitsuhiro Yoneoka, and Katashi Na-
gao. 2001. A multilingual video transcriptor and
annotation-based video transcoding. In Proceed-
ings of the Second International Workshop on
Content-Based Multimedia Indexing (CBMI-01).
Michael A. Smith and Takeo Kanade. 1995. Video
skimming for quick browsing based on audio and
image characterization. Technical Report CMU-
CS-95-186, School of Computer Science, Carnegie
Mellon University.
TEI. 2002. Text Encoding Initiative.
http://www.uic.edu/orgs/tei/.
TREC. 2002. Text REtrieval Conference home
page. http://trec.nist.gov/.
W3C. 2002. Extensible Markup Language (XML).
http://www.w3.org/XML/.
Hideo Watanabe, Katashi Nagao, Michael C. Mc-
Cord, and Arendse Bernth. 2002. An annotation
system for enhancing quality of natural language
processing. In Proceedings of the Nineteenth In-
ternational Conference on Computational Lin-
guistics (COLING-2002).
An Annotation System for Enhancing Quality of Natural Language
Processing
Hideo Watanabe*, Katashi Nagao**, Michael C. McCord*** and Arendse Bernth***
* IBM Research,
Tokyo Research Laboratory
1623-14 Shimotsuruma, Yamato,
Kanagawa 242-8502, Japan
hiwat@jp.ibm.com
** Dept. of Information Engineering
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya 464-8603, Japan
nagao@nuie.nagoya-u.ac.jp
*** IBM T. J. Watson
Research Center
Route 134, Yorktown Heights,
NY 10598, USA
mcmccord@us.ibm.com,
arendse@us.ibm.com
Abstract
Natural language processing (NLP) programs are
confronted with various diculties in processing
HTML and XML documents, and have the po-
tential to produce better results if linguistic infor-
mation is annotated in the source texts. We have
therefore developed the Linguistic Annotation Lan-
guage (or LAL), which is an XML-compliant tag set
for assisting natural language processing programs,
and NLP tools such as parsers and machine trans-
lation programs which can accept LAL-annotated
input. In addition, we have developed a LAL-
annotation editor which allows users to annotate
documents graphically without seeing tags. Fur-
ther, we have conducted an experiment to check
the translation quality improvement by using LAL
annotation.
1 Introduction
Recently there has been increasing interest in
applying natural language processing (NLP) sys-
tems, such as keyword extraction, automatic text
summarization, and machine translation, to Inter-
net documents. However, there are various ob-
stacles that make it dicult for them to produce
good results. It is true that NLP technologies are
not perfect, but some of the diculties result from
problems in HTML. Further, in general, if linguis-
tic information is added to source texts, it greatly
helps NLP programs to produce better results. In
what follows, we would like to show some examples
related to machine translation.
In general, it is very helpful for machine transla-
tion programs to know boundaries on many levels
(such as sentence, phrases, and words) and to know
word-to-word dependency relations. For instance,
in the following example, since \St." has two possi-
ble meanings, \street" and \saint," it is dicult to
determine whether the following example consists
of one or two sentences.
I went to Newark St. Paul lived there
two years ago.
As another example, the following sentence has
two interpretations; one interpretation is that what
he likes is people and the other interpretation is
that what he likes is accommodating.
He likes accommodating people.
If there are tags indicating the direct-object mod-
ier of the word \like," then the correct interpreta-
tion is possible. NLP may be able to resolve these
ambiguities eventually by using advanced context
processing techniques, but current NLP technology
generally needs a hint from the author for these
sorts of ambiguities.
Further, there are issues in HTML/XML. When
MT systems are applied to Web pages, most of the
errors are generated by the linguistic incomplete-
ness of MT technology, but some are generated by
problems in HTML and XML tag usage. For in-
stance, writers often use <br> tag to sentence ter-
mination. Sometimes writers intend that a <br>
tag should terminate the sentence (even without
terminating punctuation such as a period), and in
other cases writers intend <br> only as a format-
ting device. In the HTML <table> shown in Figure
1, the writer intends each line of a cell to express
one linguistic unit. The MT program cannot tell
whether each line is a unit for translation, or, in-
stead, the two lines form one unit. In this example,
some MT programs would try to produce a transla-
tion of a unit \NetVista Models ThinkPad News."
As shown in the above examples, NLP appli-
cations do not achieve their full potential, on ac-
count of problems unrelated to the essential NLP
processes. If tags expressing linguistic information
<table><tr><td>
<a href="...">NetVista Models</a><br>
<a href="...">ThinkPad News</a><br>
</td></tr></table>
Figure 1: An example of using hbri tags in a table
are inserted into source documents, they help NLP
programs recognize document and linguistic struc-
tures properly, allowing the programs to produce
much better results. At the same time, it is true
that NLP technologies are incomplete, but their de-
ciencies can sometimes be circumvented through
the use of such tags. Therefore, this paper proposes
a set of tags for helping NLP programs, called Lin-
guistic Annotation Language (or LAL).
2 Linguistic Annotation Language
LAL is an XML-compliant tag set and its XML
namespace prex is lal.
The LAL tag set is designed to be as simple as
possible for the following reasons: (1) A simple tag
set is easier for developers to check manually. (2)
An easy-to-use annotation tool is mandatory for
this annotation scheme. Simplicity is important
for making an easy-to-use annotation tool, since if
we use a feature-rich tag set, the user must check
many annotation items.
2.1 Basic Tags
The sentence tag s is used to delimit a sentence.
<lal:s>This is the first sentence.</lal:s>
<lal:s>This is the second sentence.</lal:s>
The attribute type="hdr" means that the sen-
tence is a title or header.
The word tag w is used to delimit a word. It
can have attributes for additional information such
as base form (lex), part-of-speech (pos), features
(ftrs), and sense (sense) of a word. The values of
these attributes are language-dependent, and are
not described in this paper because of space limi-
tations. The following example illustrates some of
these tags and attributes.
<lal:s>
<lal:w lex="this" pos="det">This</lal:w>
<lal:w lex="be" pos="verb" ftr="sg,3rd">
is</lal:w>
<lal:w lex="a" pos="det">a</lal:w>
<lal:w lex="pen" pos="noun" ftr="sg,count">
pen</lal:w>
</lal:s>
The dependency (or word-to-word modication)
relationship can be expressed by using the id and
mod attributes of a word tag; that is, a word can
have the ID value of its modiee in a mod attribute.
The ID value of a mod attribute must be an ID value
of a word tag or a segment tag. For instance, the
following example contains attributes showing that
the word \with" modies the word \saw," meaning
that \she" has a telescope.
She <lal:w id="w1" lex="see" pos="v"
sense="see1">saw</lal:w> a man
<lal:w mod="w1">with</lal:w>
a telescope.
The phrase (or segment) tag seg is used to spec-
ify a phrase scope on any level. In addition, you
can specify the syntactic category for a phrase by
using an optional attribute cat. The following ex-
ample species the scope of a noun phrase \a man
... a telescope," and it is a noun phrase. This also
implies that the prepositional phrase \with a tele-
scope" modies the noun phrase \a man."
She saw <lal:seg cat="np">a man with a
telescope</lal:seg>.
The attribute para="yes" means that the seg-
ment is a coordinated segment. The following ex-
ample shows that the word \software" and the word
\hardware" are coordinated.
This company deals with <lal:seg cat="np"
para="yes">software and hardware</lal:seg>
for networking.
The ref attribute has the ID value of the refer-
ent of the current word. This can be used to specify
a pronoun referent, for instance:
<lal:s>He bought <lal:seg id="w1">a
new car</lal:seg> yesterday.</lal:s>
<lal:s>She was very surprised to
learn that <lal:w ref="w1">it</lal:w>
was very expensive.</lal:s>
2.2 Expressing Multiple Parses
As mentioned earlier, since natural language con-
tains ambiguities, it is useful for LAL annotation
to have a mechanism for expressing syntactic am-
biguities.
We have introduced a parse identier (or PID)
in attribute values for distinguishing parses. An
attribute value which may be changed according
to parses can be allowed to be expressed as space-
separated multiple values, each of which consists of
a PID prex followed by a colon and an attribute
value.
<lal:s>
<lal:w id="1" mod="2">He</lal:w>
<lal:w id="2" mod="0">likes</lal:w>
<lal:w id="3" mod="p1:2 p2:4">
accommodating</lal:w>
<lal:w id="4" mod="p1:3 p2:2">people
</lal:w>.</lal:s>
This example shows that there are two interpre-
tations whose PIDs are p1 and p2, and that the p1
interpretation is \He likes people" and p2 is \He
likes accommodating."
3 LAL-Aware NLP Programs
We have modied certain NLP systems to be
LAL-aware. ESG [5, 6] is an English parsing sys-
tem developed by the IBM Watson Research Cen-
ter, and updated to accept and generate LAL-annotated
English. We have also developed a Japanese pars-
ing system with LAL output functionality. These
LAL-aware versions of parsers are used as a back-
end process to show users the system's default in-
terpretation for a given sentence in the LAL-annotation
editor described below.
Further, the English to German, French, Span-
ish, Italian and Portuguese translation engines [6,
7] and English to Japanese translation engine [9]
are modied to accept LAL-annotated English HTML
input.
1
4 The LAL-Annotation Editor
Since inserting tags into documents manually is
not generally an easy task for end users, it is impor-
tant to provide an easy-to-use GUI-based editing
environment. In developing such an environment,
we took into consideration the following points: (1)
Users should not have to see any tags. (2) Users
should not have to see internal representations ex-
pressing linguistic information. (3) Users should be
able to view and modify linguistic information such
as feature values, but only if they want to.
Considering these points, we have found that
most of the errors made by NLP programs result
from their failure to recognize the phrasal struc-
tures of sentences. Therefore, we have decided to
1
In addition, Watanabe [11] reported on an algorithm
for accelerating CFG-parsing by using LAL tag informa-
tion, and it is implemented in the above English-to-Japanese
translation engine.
show only a structural view of a sentence in the ini-
tial screen; other information is shown only if the
user requests it.
The important issue here is how to represent the
syntactic structure of a sentence to the user. NLP
programs normally deal with a linguistic structure
by means of a syntactic tree, but such a structure
is not necessarily easy for end users to understand.
For instance, Figure 2 shows the dependency struc-
ture of the English sentence \IBM announced a new
computer system for children with voice function."
This dependency structure is dicult for end users,
partly because a dependency tree does not keep the
surface word order, so that it is dicult to map it
to the original sentence quickly.
2
Therefore, an im-
portant property for the linguistic structural view
is that users can easily reconstruct the original sur-
face sentence string.
The next important issue is how easily a user
can understand the overall linguistic structure. If
a user is, at rst, presented with detailed linguistic
structure at the word level, then it is dicult to
grasp the important linguistic skeleton of a sen-
tence. Therefore, another necessary property is
to give users a view in which the overall sentence
structure is easily recognized.
Figure 2: An example of tree structure of an En-
glish sentence
With these requirements in mind, we have devel-
oped a GUI tool called the LAL Editor. To satisfy
the last requirement, this editor has two presenta-
tion modes: the reduced presentation view and the
expanded presentation view. In the reduced pre-
sentation view, a main verb and its modiers are
basic units for presenting dependencies, and they
are located on dierent lines, keeping the surface
order. Figure 3 shows an example of this reduced
presentation view. In this view, since dependen-
cies that are obvious for native speakers (e.g. \a"
and \computer" ) are not displayed explicitly, the
user can concentrate on dependencies between key
2
You must perform an inorder tree walk to reconstruct a
surface sentence string.
Figure 3: Screen Images of LAL Editor - Reduced
View
units (or phrases). If the user nds any depen-
dency errors in the reduced view, he or she can
enter the expanded view mode in which all words
are basic units for presenting dependencies. Fig-
ure 4 (a, b) shows examples of this expanded view.
In these views, to satisfy the former requirement,
dependencies between basic units are expressed by
using indentation. Therefore you can easily recon-
struct the surface sentence string by just looking at
words from top to bottom and from left to right,
and easily know dependencies of words by looking
at words located in the same column. For details
of the algorithm, see [12].
In Figure 3, you can easily grasp the overall
structure. In this case, since the dependencies be-
tween \for" and \announced," and \with" and \an-
nounced" are wrong, the user can change the mode
to the expanded view (as shown in Figure 4 (a)).
In this view, the user can change dependencies by
dragging a modier to the correct modiee using
a mouse. The corrected dependency structure is
shown in Figure 4 (b).
In addition, the LAL Editor has the capability of
testing translation by using LAL annotation. Fig-
ure 5 shows a window in which the top pane shows
the input sentence, the second pane shows the LAL-
annotation of the input, the third pane shows the
translation result using the LAL annotation, and
the fourth pane shows the default translation with-
out using the LAL annotation. The user can easily
check whether the current annotation can improve
translations.
5 Experiment
We have conducted a small experiment for eval-
uating LAL annotation to our English-to-Japanese
machine translation system[9]. We gathered about
60 sentences from Web pages in the computer do-
main, and added LAL annotation to these sen-
(a) Expanded View (before correction)
(b) Expanded View (after correction)
Figure 4: Screen Images of LAL Editor - Expanded
View
tences with the LAL annotation editor. In this
experiment, only word-to-word modications were
corrected. Due to severe parsing errors and glitches
of the annotation editor, 53 of the 60 sentences
were used in this experiment. The average sentence
length for this test set was 21 words. Two evalu-
ators assigned a quality evaluation ranging from 1
(worst) to 5 (best) for each translation, with and
without use of annotation.
Translation results for 18 sentences (about 34%)
were better for the annotated case than the non-
annotated case. These better sentences were 1.16
Figure 5: Translation test window of LAL Editor
points better (27% better in quality score). On
the other hand, 26 sentences (about 49%) were not
changed, and 9 sentences (about 17%) were worse.
The main reason why these 9 sentences were worse
was the structural mismatch between the output
of the LAL Editor and the expected structure of
EtoJ translation system, since the LAL Editor and
the EtoJ MT system use dierent parsing systems.
We have developed a structure conversion routine
from LAL editor output to EtoJ input, but it does
not yet cover all situations. This is the reason why
these 9 sentences become worse.
Note that this experiment only uses word-to-
word modication corrections, so there is room for
producing better translations if we use other types
of annotation such as part-of-speech, and word sense.
6 Discussion
There have been several eorts to dene tags
for describing language resources, such as TEI [10],
OpenTag [8], CES [1], EAGLES [2], GDA [3]. The
main focus of these eorts other than GDA has
been to share linguistic resources by expressing them
in a standard tag set, and therefore they dene very
detailed levels of tags for expressing linguistic de-
tails. GDA has almost the same purposes but it
has also dened a very complex tag set. This com-
plexity discourages people from using these tag sets
when writing documents, and it also becomes dif-
cult to make an annotation tool for these tags.
LAL is not opposed to these previous eorts, but
attempts to strike a useful balance between expres-
siveness and simplicity, so that annotation can be
used widely.
As mentioned in the discussion of the experi-
ment, there is an issue when the parsing system
of LAL editor and the parsing system of a NLP
tool which accepts the output of LAL editor are
dierent. As mentioned before, we used the ESG
parser for producing LAL-annotated English, and
Japanese-to-EnglishMT system for accepting LAL-
annotated English. Since these systems have been
independently developed based on dierent approaches
by dierent developers, we found there are some
structural dierences. For instance, given a prepo-
sitional phrase Prep N, ESG's head word of the
prepositional phrase is Prep, but EtoJ MT engine's
head is N. In most cases, we can make systematic
conversion routines for dierent structures. In fact,
for most of sentences whose translation is worse
when annotation is used, we can provide struc-
tural conversion routines for linguistic structures
included in them. The basic idea of LAL-awareness
for NLP tools is that an NLP tool uses LAL infor-
mation as much as possible, but if LAL information
produces a severe conict with the internal process-
ing, then such information should not be used. Our
EtoJ MT program was basically implemented this
way based on the algorithm described in [11], but
we seem to need more research on this issue.
7 Conclusion
In this paper, we have proposed an XML-compliant
tag set called Linguistic Annotation Language (or
LAL), which helps NLP programs perform their
tasks more correctly. LAL is designed to be as
simple as possible so that humans can use it with
minimal help from assisting tools. We have also de-
veloped a GUI-based LAL annotation editor, and
have shown in an experiment that use of LAL anno-
tation enhances translation quality. We hope that
wide acceptance of LAL will make it possible to use
more intelligent Internet tools and services.
References
[1] CES, \Corpus Encoding Standard (CES),"
(http://www.cs.vassar.edu/CES/)
[2] EAGLES, \Expert Advisory Group on Language Engi-
neering Standards,"
(http://www.ilc.pi.cnr.it/EAGLES/home.html)
[3] GDA, \Global Document Annotation,"
(http://www.etl.go.jp/etl/nl/gda/)
[4] Koichi Hashida, Katashi Nagao, et. al, \Progress
and Prospect of Global Document Annotation," (in
Japanese) Proc. of 4th Annual Meeting of the Asso-
ciation of Natural Language Processing, pp. 618{621,
1998
[5] McCord, M. C., \Slot Grammars," Computational Lin-
guistics, Vol. 6, pp. 31{43, 1980.
[6] McCord, M. C., \Slot Grammar: A System for Sim-
pler Construction of Practical Natural Language Gram-
mars," in (ed) R. Studer, Natural Language and Logic:
International Scientic Symposium, Lecture Notes in
Computer Science, pp. 118{145, Springer Verlag, 1990.
[7] McCord, M. C., and Bernth, A., \The LMT Transfor-
mational System," Proc. of Proceedings of AMTA-98,
pp. 344{355, 1998.
[8] OpenTag, \A Standard Extraction/Abstraction Text
Format for Translation and NLP Tools,"
(http://www.opentag.org/)
[9] Takeda, K., \Pattern-Based Machine Translation,"
Proc. of 16th COLING, Vol. 2, pp. 1155{1158, August
1996.
[10] TEI, \Text Encoding Initiative (TEI),"
(http://www.uic.edu:80/orgs/tei/)
[11] Watanabe, H., \A Method for Accelerating CFG-
Parsing by Using Dependency Information," Proc. of
18th COLING, 2000.
[12] Watanabe, H., Nagao, K., McCord, M. C., and Bernth,
A., \Improving Natural Language Processing by Lin-
guistic Document Annotation," Proc. of COLING 2000
Workshop for Semantic Annotation and Intelligent Con-
tent, pp. 20{27, 2000.
Interactive Paraphrasing
Based on Linguistic Annotation
Ryuichiro Higashinaka
Keio Research Institute at SFC
5322 Endo, Fujisawa-shi,
Kanagawa 252-8520, Japan
rh@sfc.keio.ac.jp
Katashi Nagao
Dept. of Information Engineering
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya 464-8603, Japan
nagao@nuie.nagoya-u.ac.jp
Abstract
We propose a method ?Interactive Paraphras-
ing? which enables users to interactively para-
phrase words in a document by their definitions,
making use of syntactic annotation and word
sense annotation. Syntactic annotation is used
for managing smooth integration of word sense
definitions into the original document, and word
sense annotation for retrieving the correct word
sense definition for a word in a document. In
this way, documents can be paraphrased so that
they fit into the original context, preserving the
semantics and improving the readability at the
same time. No extra layer (window) is necessary
for showing the word sense definition as in con-
ventional methods, and other natural language
processing techniques such as summarization,
translation, and voice synthesis can be easily
applied to the results.
1 Introduction
There is a large number of documents of great
diversity on the Web, which makes some of the
documents difficult to understand due to view-
ers? lack of background knowledge. In particu-
lar, if technical terms or jargon are contained in
the document, viewers who are unfamiliar with
them might not understand their correct mean-
ings.
When we encounter unknown words in a doc-
ument, for example scientific terms or proper
nouns, we usually look them up in dictionar-
ies or ask experts or friends for their mean-
ings. However, if there are lots of unfamiliar
words in a document or there are no experts
around, the work of looking the words up can
be very time consuming. To facilitate the effort,
we need (1) machine understandable online dic-
tionaries, (2) automated consultation of these
dictionaries, and (3) effective methods to show
the lookup results.
There is an application which consults online
dictionaries when the user clicks on a certain
word on a Web page, then shows the lookup re-
sults in a popped up window. In this case, the
application accesses its inner/online dictionaries
and the consultation process is automated using
the viewer?s mouse click as a cue. Popup win-
dows correspond to the display method. Other
related applications operate more or less in the
same way.
We encounter three big problems with the
conventional method.
First, due to the difficulty of word sense dis-
ambiguation, in the case of polysemic words, ap-
plications to date show all possible word sense
candidates for certain words, which forces the
viewer to choose the correct meaning.
Second, the popup window showing the
lookup results hides the area near the clicked
word, so that the user tends to lose the context
and has to reread the original document.
Third, since the document and the dictio-
nary lookup results are shown in different layers
(e.g., windows), other natural language process-
ing techniques such as summarization, transla-
tion, and voice synthesis cannot be easily ap-
plied to the results.
To cope with these problems, we realized a
systematic method to annotate words in a doc-
ument with word senses in such a way that
anyone (e.g., the author) can easily add word
sense information to a certain word using a user-
friendly annotating tool. This operation can be
considered as a creation of a link between a word
in the document and a node in a domain-specific
ontology.
The ?Interactive Paraphrasing? that we pro-
pose makes use of word sense annotation and
paraphrases words by embedding their word
sense definitions into the original document to
generate a new document.
Embedding occurs at the user?s initiative,
which means that the user decides when and
where to embed the definition. The generated
document can also be the target for another em-
bedding operation which can be iterated until
the document is understandable enough for the
user.
One of the examples of embedding a doc-
ument into another document is quotation.
Transcopyright (Nelson, 1997) proposes a way
for quoting hypertext documents.
However, quoting means importing other doc-
uments as they are. Our approach is to convert
other documents so that they fit into the orig-
inal context, preserving the semantics and im-
proving the readability at the same time.
As the result of embedding, there are no win-
dows hiding any part of the original text, which
makes the context easy to follow, and the new
document is ready to be used for further natural
language processing.
2 Example
In this section, we present how our system per-
forms using screenshots.
Figure 1 shows an example of a Web docu-
ment 1 after the automatic lookup of dictionary.
Words marked with a different remains back-
ground color have been successfully looked up.
Figure 1: Example of a web document showing
dictionary lookup results
The conventional method such as showing the
definition of a word in a popup window hides the
neighboring text. (Figure 2)
Figure 2: Example of a conventional method
popup window for showing the definition
1This text, slightly modified here, is from ?Internet
Agents: Spiders, Wanderers, Brokers, and Bots,? Fah-
Chun Cheong, New Riders Publishing, 1996.
Figure 3 shows the result of paraphrasing the
word ?agent.? It was successfully paraphrased
using its definition ?personal software assistants
with authority delegated from their users.? The
word ?deployed? was also paraphrased by the
definition ?to distribute systematically.? The
paraphrased area is marked by a different back-
ground color.
Figure 3: Example of the results after para-
phrasing ?agents? and ?deployed?
Figure 4 shows the result of paraphrasing the
word in the area already paraphrased. The word
?authority? was paraphrased by its definition
?power to make decisions.?
Figure 4: Example of incremental paraphrasing
3 Linguistic Annotation
Semantically embedding word sense definitions
into the original document without changing
the original context is much more difficult than
showing the definition in popup windows.
For example, replacing some word in a sen-
tence only with its word sense definition may
cause the original sentence to be grammatically
wrong or less cohesive.
This is due to the fact that the word sense def-
initions are usually incapable of simply replac-
ing original words because of their fixed forms.
For appropriately integrating the word sense
definition into the original context, we employ
syntactic annotation (described in the next sec-
tion) to both original documents and the word
sense definitions to let the machine know their
contexts.
Thus, we need two types of annotations for
Interactive Paraphrasing. One is the word sense
annotation to retrieve the correct word sense
definition for a particular word, and the other is
the syntactic annotation for managing smooth
integration of word sense definitions into the
original document.
In this paper, linguistic annotation covers
syntactic annotation and word sense annota-
tion.
3.1 Syntactic Annotation
Syntactic annotation is very useful to make on-
line documents more machine-understandable
on the basis of a new tag set, and to de-
velop content-based presentation, retrieval,
question-answering, summarization, and
translation systems with much higher qual-
ity than is currently available. The new
tag set was proposed by the GDA (Global
Document Annotation) project (Hasida,
http://www.etl.go.jp/etl/nl/gda/). It is based
on XML , and designed to be as compatible
as possible with TEI (The Text Encoding Ini-
tiative, http://www.uic.edu:80/orgs/tei/)
and CES (Corpus Encoding Standard,
http://www.cs.vassar.edu/CES/). It specifies
modifier-modifiee relations, anaphor-referent
relations, etc.
An example of a GDA-tagged sentence is as
follows:
 ?
<su><np rel="agt">Time</np>
<v>flies</v><adp rel="eg">
<ad>like</ad><np>an <n>arrow</n></np>
</adp>.</su>
? ?
The tag, <su>, refers to a sentential unit.
The other tags above, <n>, <np>, <v>, <ad> and
<adp> mean noun, noun phrase, verb, adnoun
or adverb (including preposition and postposi-
tion), and adnominal or adverbial phrase, re-
spectively.
Syntactic annotation is generated by auto-
matic morphological analysis and interactive
sentence parsing.
Some research issues concerning syntactic an-
notation are related to how the annotation cost
can be reduced within some feasible levels. We
have been developing some machine-guided an-
notation interfaces that conceal the complexity
of annotation. Machine learning mechanisms
also contribute to reducing the cost because
they can gradually increase the accuracy of au-
tomatic annotation.
3.2 Word Sense Annotation
In the computational linguistic field, word sense
disambiguation has been one of the biggest is-
sues. For example, to have a better translation
of documents, disambiguation of certain poly-
semic words is essential. Even if an estimation
of the word sense is achieved to some extent, in-
correct interpretation of certain words can lead
to irreparable misunderstanding.
To avoid this problem, we have been pro-
moting annotation of word sense for polysemic
words in the document, so that their word
senses can be machine-understandable.
For this purpose, we need a dictionary of con-
cepts, for which we use existing domain ontolo-
gies. An ontology is a set of descriptions of con-
cepts - such as things, events, and relations -
that are specified in some way (such as specific
natural language) in order to create an agreed-
upon vocabulary for exchanging information.
Annotating a word sense is therefore equal to
creating a link between a word in the document
and a concept in a certain domain ontology. We
have made a word sense annotating tool for this
purpose which has been integrated with the an-
notation editor described in the next section.
3.3 Annotation Editor
Our annotation editor, implemented as a Java
application, facilitates linguistic annotation of
the document. An example screen of our anno-
tation editor is shown in Figure 5.
Figure 5: Annotation editor
The left window of the editor shows the docu-
ment object structure of the HTML document.
The center window shows some text that was
selected on the Web browser as shown on the
right top of the figure. The selected area is auto-
matically assigned an XPointer (i.e., a location
identifier in the document) (World Wide Web
Consortium, http://www.w3.org/TR/xptr/).
The right bottom window shows the linguistic
structure of the sentence in the selected area. In
this window, the user can modify the results of
the automatically-analyzed sentence structure.
Using the editor, the user annotates text
with linguistic structure (syntactic and seman-
tic structure) and adds a comment to an ele-
ment in the document. The editor is capable of
basic natural language processing and interac-
tive disambiguation.
The tool also supports word sense annotation
as shown in Figure 6. The ontology viewer ap-
pears in the right middle of the figure. The user
can easily select a concept in the domain ontol-
ogy and assign a concept ID to a word in the
document as a word sense.
Figure 6: Annotation editor with ontology
viewer
4 Interactive Paraphrasing
Using the linguistic annotation (syntactic and
word sense annotation), Interactive Paraphras-
ing offers a way to paraphrase words in the doc-
ument on user demand.
4.1 Interactivity
One of the objectives of this research is to make
online documents more understandable by para-
phrasing unknown words using their word sense
definitions.
Users can interactively select words to para-
phrase by casual movements like mouse clicks.
The paraphrase history is stored for later use
such as profile-based paraphrasing (yet to be
developped) which automatically selects words
to paraphrase based on user?s knowledge.
The resulting sentence can also be a target
for the next paraphrase. By allowing incremen-
tal operation, users can interact with the doc-
ument until there are no paraphrasable words
in the document or the document has become
understandable enough.
Interactive Paraphrasing is divided into click
paraphrasing and region paraphrasing accord-
ing to user interaction type. The former para-
phrases a single word specified by mouse click,
and the latter, one or more paraphrasable words
in a specified region.
4.2 Paraphrasing Mechanism
As described in previous sections, the original
document and the word sense definitions are an-
notated with linguistic annotation, which means
they have graph structures. A word corresponds
to a node, a phrase or sentence to a subgraph.
Our paraphrasing is an operation that replaces
a node with a subgraph to create a new graph.
Linguistic operations are necessary for creating
a graph that correctly fits the original context.
We have made some simple rules (principles)
for replacing a node in the original document
with a node representing the word sense defini-
tion.
There are two types of rules for paraphrasing.
One is a ?global rule? which can be applied to
any pair of nodes, the other is a ?local rule?
which takes syntactic features into account.
Below is the description of paraphrasing rules
(principles) that we used this time. Org stands
for the node in the original document to be
paraphrased by Def which represents the word
sense definition node. Global rules are applied
first followed by local rules. Pairs to which rules
cannot be applied are left as they are.
- Global Rules -
1. If the word Org is included in Def , para-
phrasing is not performed to avoid the loop
of Org.
2. Ignore the area enclosed in parentheses in
Def . The area is usually used for making
Def an independent statement.
3. Avoid double negation, which increases the
complexity of the sentence.
4. To avoid redundancy, remove from Def the
same case-marked structure found both in
Org and Def .
5. Other phrases expressing contexts in Def
are ignored, since similar contexts are likely
to be in the original sentence already.
- Local Rules -
The left column shows the pair of linguistic
features 2 corresponding to Org and Def . (e.g.
N ? N signifies the rule to be applied between
nodes having noun features.)
2
N stands for the noun feature, V , AJ and AD for
verbal, adjective and adverbial features respectively.
N ?N Replace Org with Def agreeing in
number.
N ? V Nominalize Def and replace Org.
(e.g., explain ? the explanation of)
V ?N If there is a verbal phrase modify-
ing Def , conjugate Org using Def ?s
conjugation and replace Org.
V ? V Apply Org?s conjugation to Def
and replace Org.
AD ?N Replace Org with any adverbial
phrase modifying Def .
AJ ?N Replace Org with any adjective
phrase modifying Def .
4.3 Implementation
We have implemented a system to realize Inter-
active Paraphrasing. Figure 7 shows the basic
layout of the system. The proxy server in the
middle deals with user interactions, document
retrievals, and the consultation of online dictio-
naries.
Figure 7: System architecture
The paraphrasing process follows the steps
described below.
1. On a user?s request, the proxy server
retrieves a document through which it
searches for words with word sense anno-
tations. If found, the proxy server changes
their background color to notify the user of
the paraphrasable words.
2. The user specifies a word in the document
on the browser.
3. Receiving the word to be paraphrased, the
proxy server looks it up in online dictio-
naries using the concept ID assigned to the
word.
4. Using the retrieved word sense definition,
the proxy server attempts to integrate it
into the original document using linguistic
annotation attached to both the definition
and the original document.
5 Related Work
Recently there have been some activities to add
semantics to the Web (Nagao et al, 2001) (Se-
manticWeb.org, http://www.semanticweb.org/)
(Heflin and Hendler, 2000) enabling comput-
ers to better handle online documents. As
for paraphrasing rules concerning structured
data, Inui et al are developing Kura (Inui
et al, 2001) which is a Transfer-Based Lexico-
Structural Paraphrasing Engine.
6 Conclusion and Future Plans
We have described a method, ?Interactive Para-
phrasing?, which enables users to interactively
paraphrase words in a document by their defi-
nitions, making use of syntactic annotation and
word sense annotation.
By paraphrasing, no extra layer (window) is
necessary for showing the word sense definition
as in conventional methods, and other natural
language processing techniques such as summa-
rization, translation, and voice synthesis can be
easily applied to the results.
Our future plans include: reduction of
the annotation cost, realization of profile-based
paraphrasing using personal paraphrasing his-
tory, and retrieval of similar pages for semanti-
cally merging them using linguistic annotation.
References
Jeff Heflin and James Hendler. 2000. Semantic In-
teroperability on the Web. In Proceedings of Ex-
treme Markup Languages 2000. Graphic Commu-
nications Association, 2000. pp. 111-120.
Kentaro Inui, Tetsuro Takahashi, Tomoya Iwakura,
Ryu Iida, and Atsushi Fujita. 2001. KURA:
A Transfer-Based Lexico-Structural Paraphrasing
Engine. In Proceedings of the 6th Natural Lan-
guage Processing Pacific Rim Symposium, Work-
shop on Automatic Paraphrasing: Theories and
Applications.
Katashi Nagao, Yoshinari Shirai, and Kevin Squire.
2001. Semantic annotation and transcoding:
Making Web content more accessible. IEEE Mul-
tiMedia. Vol. 8, No. 2, pp. 69?81.
Theodor Holm Nelson. 1997. Transcopyright: Deal-
ing with the Dilemma of Digital Copyright.
Educom Review, Vol. 32, No. 1, pp. 32-35.
