A Probabilistic Answer Type Model
Christopher Pinchak
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada
pinchak@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA
lindek@google.com
Abstract
All questions are implicitly associated
with an expected answer type. Unlike
previous approaches that require a prede-
fined set of question types, we present
a method for dynamically constructing
a probability-based answer type model
for each different question. Our model
evaluates the appropriateness of a poten-
tial answer by the probability that it fits
into the question contexts. Evaluation
is performed against manual and semi-
automatic methods using a fixed set of an-
swer labels. Results show our approach to
be superior for those questions classified
as having a miscellaneous answer type.
1 Introduction
Given a question, people are usually able to form
an expectation about the type of the answer, even
if they do not know the actual answer. An accu-
rate expectation of the answer type makes it much
easier to select the answer from a sentence that
contains the query words. Consider the question
?What is the capital of Norway?? We would ex-
pect the answer to be a city and could filter out
most of the words in the following sentence:
The landed aristocracy was virtually crushed
by Hakon V, who reigned from 1299 to 1319,
and Oslo became the capital of Norway, re-
placing Bergen as the principal city of the
kingdom.
The goal of answer typing is to determine
whether a word?s semantic type is appropriate as
an answer for a question. Many previous ap-
proaches to answer typing, e.g., (Ittycheriah et al,
2001; Li and Roth, 2002; Krishnan et al, 2005),
employ a predefined set of answer types and use
supervised learning or manually constructed rules
to classify a question according to expected an-
swer type. A disadvantage of this approach is that
there will always be questions whose answers do
not belong to any of the predefined types.
Consider the question: ?What are tourist attrac-
tions in Reims?? The answer may be many things:
a church, a historic residence, a park, a famous
intersection, a statue, etc. A common method to
deal with this problem is to define a catch-all class.
This class, however, tends not to be as effective as
other answer types.
Another disadvantage of predefined answer
types is with regard to granularity. If the types
are too specific, they are more difficult to tag. If
they are too general, too many candidates may be
identified as having the appropriate type.
In contrast to previous approaches that use a su-
pervised classifier to categorize questions into a
predefined set of types, we propose an unsuper-
vised method to dynamically construct a proba-
bilistic answer type model for each question. Such
a model can be used to evaluate whether or not
a word fits into the question context. For exam-
ple, given the question ?What are tourist attrac-
tions in Reims??, we would expect the appropriate
answers to fit into the context ?X is a tourist attrac-
tion.? From a corpus, we can find the words that
appeared in this context, such as:
A-Ama Temple, Aborigine, addition, Anak
Krakatau, archipelago, area, baseball,
Bletchley Park, brewery, cabaret, Cairo,
Cape Town, capital, center, ...
Using the frequency counts of these words in
the context, we construct a probabilistic model
to compute P (in(w,?)|w), the probability for a
word w to occur in a set of contexts ?, given an
occurrence of w. The parameters in this model are
obtained from a large, automatically parsed, un-
labeled corpus. By asking whether a word would
occur in a particular context extracted from a ques-
393
tion, we avoid explicitly specifying a list of pos-
sible answer types. This has the added benefit
of being easily adapted to different domains and
corpora in which a list of explicit possible answer
types may be difficult to enumerate and/or identify
within the text.
The remainder of this paper is organized as fol-
lows. Section 2 discusses the work related to an-
swer typing. Section 3 discusses some of the key
concepts employed by our probabilistic model, in-
cluding word clusters and the contexts of a ques-
tion and a word. Section 4 presents our probabilis-
tic model for answer typing. Section 5 compares
the performance of our model with that of an or-
acle and a semi-automatic system performing the
same task. Finally, the concluding remarks in are
made in Section 6.
2 Related Work
Light et al (2001) performed an analysis of the
effect of multiple answer type occurrences in a
sentence. When multiple words of the same type
appear in a sentence, answer typing with fixed
types must assign each the same score. Light et
al. found that even with perfect answer sentence
identification, question typing, and semantic tag-
ging, a system could only achieve 59% accuracy
over the TREC-9 questions when using their set of
24 non-overlapping answer types. By computing
the probability of an answer candidate occurring
in the question contexts directly, we avoid having
multiple candidates with the same level of appro-
priateness as answers.
There have been a variety of approaches to de-
termine the answer types, which are also known
as Qtargets (Echihabi et al, 2003). Most previous
approaches classify the answer type of a question
as one of a set of predefined types.
Many systems construct the classification rules
manually (Cui et al, 2004; Greenwood, 2004;
Hermjakob, 2001). The rules are usually triggered
by the presence of certain words in the question.
For example, if a question contains ?author? then
the expected answer type is Person.
The number of answer types as well as the num-
ber of rules can vary a great deal. For example,
(Hermjakob, 2001) used 276 rules for 122 answer
types. Greenwood (2004), on the other hand, used
46 answer types with unspecified number of rules.
The classification rules can also be acquired
with supervised learning. Ittycheriah, et al (2001)
describe a maximum entropy based question clas-
sification scheme to classify each question as hav-
ing one of the MUC answer types. In a similar ex-
periment, Li & Roth (2002) train a question clas-
sifier based on a modified version of SNoW using
a richer set of answer types than Ittycheriah et al
The LCC system (Harabagiu et al, 2003) com-
bines fixed types with a novel loop-back strategy.
In the event that a question cannot be classified as
one of the fixed entity types or semantic concepts
derived from WordNet (Fellbaum, 1998), the an-
swer type model backs off to a logic prover that
uses axioms derived form WordNet, along with
logic rules, to justify phrases as answers. Thus, the
LCC system is able to avoid the use of a miscel-
laneous type that often exhibits poor performance.
However, the logic prover must have sufficient ev-
idence to link the question to the answer, and gen-
eral knowledge must be encoded as axioms into
the system. In contrast, our answer type model
derives all of its information automatically from
unannotated text.
Answer types are often used as filters. It was
noted in (Radev et al, 2002) that a wrong guess
about the answer type reduces the chance for the
system to answer the question correctly by as
much as 17 times. The approach presented here
is less brittle. Even if the correct candidate does
not have the highest likelihood according to the
model, it may still be selected when the answer
extraction module takes into account other factors
such as the proximity to the matched keywords.
Furthermore, a probabilistic model makes it eas-
ier to integrate the answer type scores with scores
computed by other components in a question an-
swering system in a principled fashion.
3 Resources
Before introducing our model, we first describe
the resources used in the model.
3.1 Word Clusters
Natural language data is extremely sparse. Word
clusters are a way of coping with data sparseness
by abstracting a given word to a class of related
words. Clusters, as used by our probabilistic an-
swer typing system, play a role similar to that of
named entity types. Many methods exist for clus-
tering, e.g., (Brown et al, 1990; Cutting et al,
1992; Pereira et al, 1993; Karypis et al, 1999).
We used the Clustering By Committee (CBC)
394
Table 1: Words and their clusters
Word Clusters
suite software, network, wireless, ...
rooms, bathrooms, restrooms, ...
meeting room, conference room, ...
ghost rabbit, squirrel, duck, elephant, frog, ...
goblins, ghosts, vampires, ghouls, ...
punk, reggae, folk, pop, hip-pop, ...
huge, larger, vast, significant, ...
coming-of-age, true-life, ...
clouds, cloud, fog, haze, mist, ...
algorithm (Pantel and Lin, 2002) on a 10 GB En-
glish text corpus to obtain 3607 clusters. The fol-
lowing is an example cluster generated by CBC:
tension, anger, anxiety, tensions, frustration,
resentment, uncertainty, confusion, conflict,
discontent, insecurity, controversy, unease,
bitterness, dispute, disagreement, nervous-
ness, sadness, despair, animosity, hostility,
outrage, discord, pessimism, anguish, ...
In the clustering generated by CBC, a word may
belong to multiple clusters. The clusters to which
a word belongs often represent the senses of the
word. Table 1 shows two example words and their
clusters.
3.2 Contexts
The context in which a word appears often im-
poses constraints on the semantic type of the word.
This basic idea has been exploited by many pro-
posals for distributional similarity and clustering,
e.g., (Church and Hanks, 1989; Lin, 1998; Pereira
et al, 1993).
Similar to Lin and Pantel (2001), we define
the contexts of a word to be the undirected paths
in dependency trees involving that word at either
the beginning or the end. The following diagram
shows an example dependency tree:
Which city hosted the 1988 Winter Olympics?
det subj
obj
NN
NN
det
The links in the tree represent dependency rela-
tionships. The direction of a link is from the head
to the modifier in the relationship. Labels associ-
ated with the links represent types of relations.
In a context, the word itself is replaced with a
variable X. We say a word is the filler of a context
if it replaces X. For example, the contexts for the
word ?Olympics? in the above sentence include
the following paths:
Context of ?Olympics? Explanation
X Winter
NN
Winter X
X 1988
NN
1988 X
X host
obj
host X
X host
obj
city
subj
city hosted X
In these paths, words are reduced to their root
forms and proper names are reduced to their entity
tags (we used MUC7 named entity tags).
Paths allow us to balance the specificity of con-
texts and the sparseness of data. Longer paths typ-
ically impose stricter constraints on the slot fillers.
However, they tend to have fewer occurrences,
making them more prone to errors arising from
data sparseness. We have restricted the path length
to two (involving at most three words) and require
the two ends of the path to be nouns.
We parsed the AQUAINT corpus (3GB) with
Minipar (Lin, 2001) and collected the frequency
counts of words appearing in various contexts.
Parsing and database construction is performed
off-line as the database is identical for all ques-
tions. We extracted 527,768 contexts that ap-
peared at least 25 times in the corpus. An example
context and its fillers are shown in Figure 1.
X host Olympics
subj obj
Africa 2 grant 1 readiness 2
AP 1 he 2 Rio de Janeiro 1
Argentina 1 homeland 3 Rome 1
Athens 16 IOC 1 Salt Lake City 2
Atlanta 3 Iran 2 school 1
Bangkok 1 Jakarta 1 S. Africa 1
. . . . . . . . .
decades 1 president 2 Zakopane 4
facility 1 Pusan 1
government 1 race 1
Figure 1: An example context and its fillers
3.2.1 Question Contexts
To build a probabilistic model for answer typ-
ing, we extract a set of contexts, called question
contexts, from a question. An answer is expected
to be a plausible filler of the question contexts.
Question contexts are extracted from a question
with two rules. First, if the wh-word in a ques-
tion has a trace in the parse tree, the question con-
texts are the contexts of the trace. For example, the
395
question ?What do most tourists visit in Reims??
is parsed as:
Whati do most tourists visit ei in Reims?
det
i
subjdet obj
in
The symbol ei is the trace of whati. Minipar
generates the trace to indicate that the word what
is the object of visit in the deep structure of the
sentence. The following question contexts are ex-
tracted from the above question:
Context Explanation
X visit tourist
obj subj
tourist visits X
X visit Reims
obj in
visit X in Reims
The second rule deals with situations where
the wh-word is a determiner, as in the question
?Which city hosted the 1988 Winter Olympics??
(the parse tree for which is shown in section 3.2).
In such cases, the question contexts consist of a
single context involving the noun that is modified
by the determiner. The context for the above sen-
tence is X city
subj
, corresponding to the sentence
?X is a city.? This context is used because the
question explicitly states that the desired answer is
a city. The context overrides the other contexts be-
cause the question explicitly states the desired an-
swer type. Experimental results have shown that
using this context in conjunction with other con-
texts extracted from the question produces lower
performance than using this context alone.
In the event that a context extracted from a ques-
tion is not found in the database, we shorten the
context in one of two ways. We start by replac-
ing the word at the end of the path with a wildcard
that matches any word. If this fails to yield en-
tries in the context database, we shorten the con-
text to length one and replace the end word with
automatically determined similar words instead of
a wildcard.
3.2.2 Candidate Contexts
Candidate contexts are very similar in form to
question contexts, save for one important differ-
ence. Candidate contexts are extracted from the
parse trees of the answer candidates rather than the
question. In natural language, some words may
be polysemous. For example, Washington may re-
fer to a person, a city, or a state. The occurrences
of Washington in ?Washington?s descendants? and
?suburban Washington? should not be given the
same score when the question is seeking a loca-
tion. Given that the sense of a word is largely de-
termined by its local context (Choueka and Lusig-
nan, 1985), candidate contexts allow the model to
take into account the candidate answers? senses
implicitly.
4 Probabilistic Model
The goal of an answer typing model is to evalu-
ate the appropriateness of a candidate word as an
answer to the question. If we assume that a set
of answer candidates is provided to our model by
some means (e.g., words comprising documents
extracted by an information retrieval engine), we
wish to compute the value P (in(w,?Q)|w). That
is, the appropriateness of a candidate answer w is
proportional to the probability that it will occur in
the question contexts ?Q extracted from the ques-
tion.
To mitigate data sparseness, we can introduce
a hidden variable C that represents the clusters to
which the candidate answer may belong. As a can-
didate may belong to multiple clusters, we obtain:
P (in(w,?Q)|w) =
X
C
P (in(w,?Q), C|w) (1)
=
X
C
P (C|w)P (in(w,?Q)|C,w) (2)
Given that a word appears, we assume that it has
the same probability to appear in a context as all
other words in the same cluster. Therefore:
P (in(w,?Q)|C,w) ? P (in(C,?Q)|C) (3)
We can now rewrite the equation in (2) as:
P (in(w,?Q)|w) ?
X
C
P (C|w)P (in(C,?Q)|C) (4)
This equation splits our model into two parts:
one models which clusters a word belongs to and
the other models how appropriate a cluster is to
the question contexts. When ?Q consists of multi-
ple contexts, we make the na??ve Bayes assumption
that each individual context ?Q ? ?Q is indepen-
dent of all other contexts given the cluster C.
P (in(w,?Q)|w) ?
X
C
P (C|w)
Y
?Q??Q
P (in(C, ?Q)|C) (5)
Equation (5) needs the parameters P (C|w) and
P (in(C, ?Q)|C), neither of which are directly
available from the context-filler database. We will
discuss the estimation of these parameters in Sec-
tion 4.2.
396
4.1 Using Candidate Contexts
The previous model assigns the same likelihood to
every instance of a given word. As we noted in
section 3.2.2, a word may be polysemous. To take
into account a word?s context, we can instead com-
pute P (in(w,?Q)|w, in(w,?w)), where ?w is the
set of contexts for the candidate word w in a re-
trieved passage.
By introducing word clusters as intermediate
variables as before and making a similar assump-
tion as in equation (3), we obtain:
P (in(w,?Q)|w, in(w,?w))
=
X
C
P (in(w,?Q), C|w, in(w,?w)) (6)
?
X
C
P (C|w, in(w,?w))P (in(C,?Q)|C) (7)
Like equation (4), equation (7) partitions the
model into two parts. Unlike P (C|w) in equation
(4), the probability of the cluster is now based on
the particular occurrence of the word in the candi-
date contexts. It can be estimated by:
P (C|w, in(w,?w))
= P (in(w,?w)|w,C)P (w,C)P (in(w,?w)|w)P (w)
(8)
?
Y
?w??w
P (in(w, ?w)|w,C)
Y
?w??w
P (in(w, ?w)|w)
? P (C|w) (9)
=
Y
?w??w
?
P (C|w, in(w, ?w))
P (C|w)
?
? P (C|w) (10)
4.2 Estimating Parameters
Our probabilistic model requires the parameters
P (C|w), P (C|w, in(w, ?)), and P (in(C, ?)|C),
wherew is a word,C is a cluster thatw belongs to,
and ? is a question or candidate context. This sec-
tion explains how these parameters are estimated
without using labeled data.
The context-filler database described in Sec-
tion 3.2 provides the joint and marginal fre-
quency counts of contexts and words (|in(?, w)|,
|in(?, ?)| and |in(w, ?)|). These counts al-
low us to compute the probabilities P (in(w, ?)),
P (in(w, ?)), and P (in(?, ?)). We can also com-
pute P (in(w, ?)|w), which is smoothed with add-
one smoothing (see equation (11) in Figure 2).
The estimation of P (C|w) presents a challenge.
We have no corpus from which we can directly
measure P (C|w) because word instances are not
labeled with their clusters.
P (in(w, ?)|w) = |in(w, ?)|+ P (in(?, ?))|in(w, ?)|+ 1 (11)
Pu(C|w) =
(
1
|{C?|w?C?}| if w ? C,
0 otherwise (12)
P (C|w) =
X
w??S(w)
sim(w,w?)? Pu(C|w?)
X
{C?|w?C?},
w??S(w)
sim(w,w?)? Pu(C?|w?)
(13)
P (in(C, ?)|C) =
X
w??C
P (C|w?)? |in(w?, ?)|+ P (in(?, ?))
X
w??C
P (C|w?)? |in(w?, ?)|+ 1
(14)
Figure 2: Probability estimation
We use the average weighted ?guesses? of the
top similar words of w to compute P (C|w) (see
equation 13). The intuition is that if w? and w
are similar words, P (C|w?) and P (C|w) tend
to have similar values. Since we do not know
P (C|w?) either, we substitute it with uniform dis-
tribution Pu(C|w?) as in equation (12) of Fig-
ure 2. Although Pu(C|w?) is a very crude guess,
the weighted average of a set of such guesses can
often be quite accurate.
The similarities between words are obtained as
a byproduct of the CBC algorithm. For each word,
we use S(w) to denote the top-n most similar
words (n=50 in our experiments) and sim(w,w?)
to denote the similarity between words w and w?.
The following is a sample similar word list for the
word suit:
S(suit) = {lawsuit 0.49, suits 0.47, com-
plaint 0.29, lawsuits 0.27, jacket 0.25, coun-
tersuit 0.24, counterclaim 0.24, pants 0.24,
trousers 0.22, shirt 0.21, slacks 0.21, case
0.21, pantsuit 0.21, shirts 0.20, sweater 0.20,
coat 0.20, ...}
The estimation for P (C|w, in(w, ?w)) is sim-
ilar to that of P (C|w) except that instead of all
w? ? S(w), we instead use {w?|w? ? S(w) ?
in(w?, ?w)}. By only looking at a particular con-
text ?w, we may obtain a different distribution over
C than P (C|w) specifies. In the event that the data
are too sparse to estimate P (C|w, in(w, ?w)), we
fall back to using P (C|w).
P (in(C, ?)|C) is computed in (14) by assum-
ing each instance of w contains a fractional in-
stance of C and the fractional count is P (C|w).
Again, add-one smoothing is used.
397
System Median % Top 1% Top 5% Top 10% Top 50%
Oracle 0.7% 89 (57%) 123 (79%) 131 (85%) 154 (99%)
Frequency 7.7% 31 (20%) 67 (44%) 86 (56%) 112 (73%)
Our model 1.2% 71 (46%) 106 (69%) 119 (77%) 146 (95%)
no cand. contexts 2.2% 58 (38%) 102 (66%) 113 (73%) 145 (94%)
ANNIE 4.0% 54 (35%) 79 (51%) 93 (60%) 123 (80%)
Table 2: Summary of Results
5 Experimental Setup & Results
We evaluate our answer typing system by using
it to filter the contents of documents retrieved by
the information retrieval portion of a question an-
swering system. Each answer candidate in the set
of documents is scored by the answer typing sys-
tem and the list is sorted in descending order of
score. We treat the system as a filter and observe
the proportion of candidates that must be accepted
by the filter so that at least one correct answer is
accepted. A model that allows a low percentage
of candidates to pass while still allowing at least
one correct answer through is favorable to a model
in which a high number of candidates must pass.
This represents an intrinsic rather than extrinsic
evaluation (Molla? and Hutchinson, 2003) that we
believe illustrates the usefulness of our model.
The evaluation data consist of 154 questions
from the TREC-2003 QA Track (Voorhees, 2003)
satisfying the following criteria, along with the top
10 documents returned for each question as iden-
tified by NIST using the PRISE1 search engine.
? the question begins with What, Which, or
Who. We restricted the evaluation such ques-
tions because our system is designed to deal
with questions whose answer types are often
semantically open-ended noun phrases.
? There exists entry for the question in the an-
swer patterns provided by Ken Litkowski2.
? One of the top-10 documents returned by
PRISE contains a correct answer.
We compare the performance of our prob-
abilistic model with that of two other sys-
tems. Both comparison systems make use of a
small, predefined set of manually-assigned MUC-
7 named-entity types (location, person, organiza-
tion, cardinal, percent, date, time, duration, mea-
sure, money) augmented with thing-name (proper
1www.itl.nist.gov/iad/894.02/works/papers/zp2/zp2.html
2trec.nist.gov/data/qa/2003 qadata/03QA.tasks/t12.pats.txt
names of inanimate objects) and miscellaneous
(a catch-all answer type of all other candidates).
Some examples of thing-name are Guinness Book
of World Records, Thriller, Mars Pathfinder, and
Grey Cup. Examples of miscellaneous answers are
copper, oil, red, and iris.
The differences in the comparison systems is
with respect to how entity types are assigned to the
words in the candidate documents. We make use
of the ANNIE (Maynard et al, 2002) named entity
recognition system, along with a manual assigned
?oracle? strategy, to assign types to candidate an-
swers. In each case, the score for a candidate is
either 1 if it is tagged as the same type as the ques-
tion or 0 otherwise. With this scoring scheme pro-
ducing a sorted list we can compute the probability
of the first correct answer appearing at rankR = k
as follows:
P (R = k) =
k?2
Y
i=0
?
t? c? i
t? i
?
c
t? k + 1 (15)
where t is the number of unique candidate answers
that are of the appropriate type and c is the number
of unique candidate answers that are correct.
Using the probabilities in equation (15), we
compute the expected rank, E(R), of the first cor-
rect answer of a given question in the system as:
E(R) =
t?c+1
X
k=1
kP (R = k) (16)
Answer candidates are the set of ANNIE-
identified tokens with stop words and punctuation
removed. This yields between 900 and 8000 can-
didates for each question, depending on the top 10
documents returned by PRISE. The oracle system
represents an upper bound on using the predefined
set of answer types. The ANNIE system repre-
sents a more realistic expectation of performance.
The median percentage of candidates that are
accepted by a filter over the questions of our eval-
uation data provides one measure of performance
and is preferred to the average because of the ef-
fect of large values on the average. In QA, a sys-
tem accepting 60% of the candidates is not signif-
icantly better or worse than one accepting 100%,
398
System Measure
Question Type
All Location Person Organization Thing-Name Misc Other
(154) (57) (17) (19) (17) (37) (7)
Our model
Median 1.2% 0.8% 2.0% 1.3% 3.7% 3.5% 12.2%
Top 1% 71 34 6 9 7 13 2
Top 5% 106 53 11 11 10 19 2
Top 10% 119 55 12 17 10 22 3
Top 50% 146 56 16 18 17 34 5
Oracle
Median 0.7% 0.4% 1.0% 0.3% 0.4% 16.0% 0.3%
Top 1% 89 44 8 16 14 1 6
Top 5% 123 57 17 19 17 6 7
Top 10% 131 57 17 19 17 14 7
Top 50% 154 57 17 19 17 37 7
ANNIE
Median 4.0% 0.6% 1.4% 6.1% 100% 16.7% 50.0%
Top 1% 54 39 5 7 0 0 3
Top 5% 79 53 12 9 0 2 3
Top 10% 93 54 13 11 0 12 3
Top 50% 123 56 16 15 5 28 3
Table 3: Detailed breakdown of performance
but the effect on average is quite high. Another
measure is to observe the number of questions
with at least one correct answer in the top N% for
various values of N . By examining the number of
correct answers found in the topN%we can better
understand what an effective cutoff would be.
The overall results of our comparison can be
found in Table 2. We have added the results of
a system that scores candidates based on their fre-
quency within the document as a comparison with
a simple, yet effective, strategy. The second col-
umn is the median percentage of where the highest
scored correct answer appears in the sorted candi-
date list. Low percentage values mean the answer
is usually found high in the sorted list. The re-
maining columns list the number of questions that
have a correct answer somewhere in the top N%
of their sorted lists. This is meant to show the ef-
fects of imposing a strict cutoff prior to running
the answer type model.
The oracle system performs best, as it bene-
fits from both manual question classification and
manual entity tagging. If entity assignment is
performed by an automatic system (as it is for
ANNIE), the performance drops noticeably. Our
probabilistic model performs better than ANNIE
and achieves approximately 2/3 of the perfor-
mance of the oracle system. Table 2 also shows
that the use of candidate contexts increases the
performance of our answer type model.
Table 3 shows the performance of the oracle
system, our model, and the ANNIE system broken
down by manually-assigned answer types. Due
to insufficient numbers of questions, the cardinal,
percent, time, duration, measure, and money types
are combined into an ?Other? category. When
compared with the oracle system, our model per-
forms worse overall for questions of all types ex-
cept for those seeking miscellaneous answers. For
miscellaneous questions, the oracle identifies all
tokens that do not belong to one of the other
known categories as possible answers. For all
questions of non-miscellaneous type, only a small
subset of the candidates are marked appropriate.
In particular, our model performs worse than the
oracle for questions seeking persons and thing-
names. Person questions often seek rare person
names, which occur in few contexts and are diffi-
cult to reliably cluster. Thing-name questions are
easy for a human to identify but difficult for au-
tomatic system to identify. Thing-names are a di-
verse category and are not strongly associated with
any identifying contexts.
Our model outperforms the ANNIE system in
general, and for questions seeking organizations,
thing-names, and miscellaneous targets in partic-
ular. ANNIE may have low coverage on organi-
zation names, resulting in reduced performance.
Like the oracle, ANNIE treats all candidates not
assigned one of the categories as appropriate for
miscellaneous questions. Because ANNIE cannot
identify thing-names, they are treated as miscella-
neous. ANNIE shows low performance on thing-
names because words incorrectly assigned types
are sorted to the bottom of the list for miscella-
neous and thing-name questions. If a correct an-
swer is incorrectly assigned a type it will be sorted
near the bottom, resulting in a poor score.
399
6 Conclusions
We have presented an unsupervised probabilistic
answer type model. Our model uses contexts de-
rived from the question and the candidate answer
to calculate the appropriateness of a candidate an-
swer. Statistics gathered from a large corpus of
text are used in the calculation, and the model is
constructed to exploit these statistics without be-
ing overly specific or overly general.
The method presented here avoids the use of an
explicit list of answer types. Explicit answer types
can exhibit poor performance, especially for those
questions not fitting one of the types. They must
also be redefined when either the domain or corpus
substantially changes. By avoiding their use, our
answer typing method may be easier to adapt to
different corpora and question answering domains
(such as bioinformatics).
In addition to operating as a stand-alone answer
typing component, our system can be combined
with other existing answer typing strategies, es-
pecially in situations in which a catch-all answer
type is used. Our experimental results show that
our probabilistic model outperforms the oracle and
a system using automatic named entity recognition
under such circumstances. The performance of
our model is better than that of the semi-automatic
system, which is a better indication of the expected
performance of a comparable real-world answer
typing system.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their helpful comments on improving
the paper. The first author is supported by the Nat-
ural Sciences and Engineering Research Council
of Canada, the Alberta Ingenuity Fund, and the Al-
berta Informatics Circle of Research Excellence.
References
P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai, and R.L.
Mercer. 1990. Class-based n-gram Models of Natural
Language. Computational Linguistics, 16(2):79?85.
Y. Choueka and S. Lusignan. 1985. Disambiguation by Short
Contexts. Computer and the Humanities, 19:147?157.
K. Church and P. Hanks. 1989. Word Association Norms,
Mutual Information, and Lexicography. In Proceedings
of ACL-89, pages 76?83, Vancouver, British Columbia,
Canada.
H. Cui, K. Li, R. Sun, T-S. Chua, and M-K. Kan. 2004. Na-
tional University of Singapore at the TREC-13 Question
Answering Main Task. In Notebook of TREC 2004, pages
34?42, Gaithersburg, Maryland.
D.R. Cutting, D. Karger, J. Pedersen, and J.W. Tukey. 1992.
Scatter/Gather: A Cluster-based Approach to Browsing
Large Document Collections. In Proceedings of SIGIR-
92, pages 318?329, Copenhagen, Denmark.
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. Melz,
and D. Ravichandran. 2003. Multiple-Engine Question
Answering in TextMap. In Proceedings of TREC 2003,
pages 772?781, Gaithersburg, Maryland.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, Massachusetts.
M.A. Greenwood. 2004. AnswerFinder: Question Answer-
ing from your Desktop. In Proceedings of the Seventh
Annual Colloquium for the UK Special Interest Group
for Computational Linguistics (CLUK ?04), University of
Birmingham, UK.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden,
J. Williams, and J. Bensley. 2003. Answer Mining by
Combining Extraction Techniques with Abductive Rea-
soning. In Proceedings of TREC 2003, pages 375?382,
Gaithersburg, Maryland.
U. Hermjakob. 2001. Parsing and Question Classification for
Question Answering. In Proceedings of the ACL Work-
shop on Open-Domain Question Answering, Toulouse,
France.
A. Ittycheriah, M. Franz, W-J. Zhu, and A. Ratnaparkhi.
2001. Question Answering Using Maximum Entropy
Components. In Proceedings of NAACL 2001, Pittsburgh,
Pennsylvania.
G. Karypis, E.-H. Han, and V. Kumar. 1999. Chameleon: A
Hierarchical Clustering Algorithm using Dynamic Model-
ing. IEEE Computer: Special Issue on Data Analysis and
Mining, 32(8):68?75.
V. Krishnan, S. Das, and S. Chakrabarti. 2005. Enhanced
Answer Type Inference from Questions using Sequential
Models. In Proceedings of HLT/EMNLP 2005, pages
315?322, Vancouver, British Columbia, Canada.
X. Li and D. Roth. 2002. Learning Question Classifiers.
In Proceedings of COLING 2002, pages 556?562, Taipei,
Taiwan.
M. Light, G. Mann, E. Riloff, and E. Breck. 2001. Analyses
for Elucidating Current Question Answering Technology.
Natural Language Engineering, 7(4):325?342.
D. Lin and P. Pantel. 2001. Discovery of Inference Rules
for Question Answering. Natural Language Engineering,
7(4):343?360.
D. Lin. 1998. Automatic Retrieval and Clustering of Similar
Words. In Proceedings of COLING-ACL 1998, Montreal,
Que?bec, Canada.
D. Lin. 2001. Language and Text Analysis Tools. In Pro-
ceedings of HLT 2001, pages 222?227, San Diego, Cali-
fornia.
D. Maynard, V. Tablan, H. Cunningham, C. Ursu, H. Sag-
gion, K. Bontcheva, and Y. Wilks. 2002. Architectural
Elements of Language Engineering Robustness. Natural
Language Engineering, 8(2/3):257?274.
D. Molla? and B. Hutchinson. 2003. Intrinsic versus Extrinsic
Evaluations of Parsing Systems. In Proceedings of EACL
Workshop on Evaluation Initiatives in Natural Language
Processing, pages 43?50, Budapest, Hungary.
P. Pantel and D. Lin. 2002. Document Clustering with Com-
mittees. In Proceedings of SIGIR 2002, pages 199?206,
Tampere, Finland.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional Clus-
tering of English Words. In Proceedings of ACL 1992,
pages 183?190.
D. Radev, W. Fan, H. Qi, H. Wu, and A. Grewal. 2002. Prob-
ablistic Question Answering on the Web. In Proceedings
of the Eleventh International World Wide Web Conference.
E.M. Voorhees. 2003. Overview of the TREC 2003 Ques-
tion Answering Track. In Proceedings of TREC 2003,
Gaithersburg, Maryland.
400
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 666?674,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Flexible Answer Typing with Discriminative Preference Ranking
Christopher Pinchak? Dekang Lin? Davood Rafiei?
?Department of Computing Science ?Google Inc.
University of Alberta 1600 Amphitheatre Parkway
Edmonton, Alberta, Canada Mountain View, CA, USA
{pinchak,drafiei}@cs.ualberta.ca lindek@google.com
Abstract
An important part of question answering
is ensuring a candidate answer is plausi-
ble as a response. We present a flexible
approach based on discriminative prefer-
ence ranking to determine which of a set
of candidate answers are appropriate. Dis-
criminative methods provide superior per-
formance while at the same time allow the
flexibility of adding new and diverse fea-
tures. Experimental results on a set of fo-
cused What ...? and Which ...? questions
show that our learned preference ranking
methods perform better than alternative
solutions to the task of answer typing. A
gain of almost 0.2 in MRR for both the
first appropriate and first correct answers
is observed along with an increase in pre-
cision over the entire range of recall.
1 Introduction
Question answering (QA) systems have received a
great deal of attention because they provide both
a natural means of querying via questions and be-
cause they return short, concise answers. These
two advantages simplify the task of finding in-
formation relevant to a topic of interest. Ques-
tions convey more than simply a natural language
query; an implicit expectation of answer type is
provided along with the question words. The dis-
covery and exploitation of this implicit expected
type is called answer typing.
We introduce an answer typing method that is
sufficiently flexible to use a wide variety of fea-
tures while at the same time providing a high level
of performance. Our answer typing method avoids
the use of pre-determined classes that are often
lacking for unanticipated answer types. Because
answer typing is only part of the QA task, a flexi-
ble answer typing model ensures that answer typ-
ing can be easily and usefully incorporated into a
complete QA system. A discriminative preference
ranking model with a preference for appropriate
answers is trained and applied to unseen ques-
tions. In terms of Mean Reciprocal Rank (MRR),
we observe improvements over existing systems of
around 0.2 both in terms of the correct answer and
in terms of appropriate responses. This increase
in MRR brings the performance of our model to
near the level of a full QA system on a subset of
questions, despite the fact that we rely on answer
typing features alone.
The amount of information given about the ex-
pected answer can vary by question. If the ques-
tion contains a question focus, which we define
to be the head noun following the wh-word such
as city in ?What city hosted the 1988 Winter
Olympics??, some of the typing information is ex-
plicitly stated. In this instance, the answer is re-
quired to be a city. However, there is often addi-
tional information available about the type. In our
example, the answer must plausibly host a Winter
Olympic Games. The focus, along with the ad-
ditional information, give strong clues about what
are appropriate as responses.
We define an appropriate candidate answer as
one that a user, who does not necessarily know
the correct answer, would identify as a plausible
answer to a given question. For most questions,
there exist plausible responses that are not correct
answers to the question. For our above question,
the city of Vancouver is plausible even though it
is not correct. For the purposes of this paper, we
assume correct answers are a subset of appropri-
ate candidates. Because answer typing is only in-
tended to be a component of a full QA system, we
rely on other components to help establish the true
correctness of a candidate answer.
The remainder of the paper is organized as fol-
lows. Section 2 presents the application of dis-
criminative preference rank learning to answer
typing. Section 3 introduces the models we use
666
for learning appropriate answer preferences. Sec-
tions 4 and 5 discuss our experiments and their re-
sults, respectively. Section 6 presents prior work
on answer typing and the use of discriminative
methods in QA. Finally, concluding remarks and
ideas for future work are presented in Section 7.
2 Preference Ranking
Preference ranking naturally lends itself to any
problem in which the relative ordering between
examples is more important than labels or values
assigned to those examples. The classic exam-
ple application of preference ranking (Joachims,
2002) is that of information retrieval results rank-
ing. Generally, information retrieval results are
presented in some ordering such that those higher
on the list are either more relevant to the query or
would be of greater interest to the user.
In a preference ranking task we have a set of
candidates c1, c2, ..., cn, and a ranking r such that
the relation ci <r cj holds if and only if can-
didate ci should be ranked higher than cj , for
1 ? i, j ? n and i 6= j. The ranking r can form
a total ordering, as in information retrieval, or a
partial ordering in which we have both ci ?r cj
and cj ?r ci. Partial orderings are useful for our
task of answer typing because they can be used to
specify candidates that are of an equivalent rank.
Given some ci <r cj , preference ranking only
considers the difference between the feature rep-
resentations of ci and cj (?(ci) and ?(cj), respec-
tively) as evidence. We want to learn some weight
vector ~w such that ~w ??(ci) > ~w ??(cj) holds for
all pairs ci and cj that have the relation ci <r cj . In
other words, we want ~w ? (?(ci)??(cj)) > 0 and
we can use some margin in the place of 0. In the
context of Support Vector Machines (Joachims,
2002), we are trying to minimize the function:
V (~w, ~?) =
1
2
~w ? ~w + C
?
?i,j (1)
subject to the constraints:
?(ci <r cj) : ~w ? (?(ci)? ?(cj)) ? 1? ?i,j (2)
?i, j : ?i,j ? 0 (3)
The margin incorporates slack variables ?i,j for
problems that are not linearly separable. This
ranking task is analogous to the SVM classi-
fication task on the pairwise difference vectors
(?(ci) ? ?(cj)), known as rank constraints. Un-
like classification, no explicit negative evidence is
required as ~w?(?(ci)??(cj)) = (?1)~w?(?(cj)?
?(ci)). It is also important to note that no rank
constraints are generated for candidates for which
no order relation exists under the ranking r.
Support Vector Machines (SVMs) have previ-
ously been used for preference ranking in the
context of information retrieval (Joachims, 2002).
We adopt the same framework for answer typing
by preference ranking. The SVMlight package
(Joachims, 1999) implements the preference rank-
ing of Joachims (2002) and is used here for learn-
ing answer types.
2.1 Application to Answer Typing
Assigning meaningful scores for answer typing is
a difficult task. For example, given the question
?What city hosted the 1988 Winter Olympics??
and the candidates New York, Calgary, and the
word blue, how can we identify New York and
Calgary as appropriate and the word blue as inap-
propriate? Scoring answer candidates is compli-
cated by the fact that a gold standard for appropri-
ateness scores does not exist. Therefore, we have
no a priori notion that New York is better than the
word blue by some amount v. Because of this, we
approach the problem of answer typing as one of
preference ranking in which the relative appropri-
ateness is more important than the absolute scores.
Preference ranking stands in contrast to classifi-
cation, in which a candidate is classified as appro-
priate or inappropriate depending on the values in
its feature representation. Unfortunately, simple
classification does not work well in the face of a
large imbalance in positive and negative examples.
In answer typing we typically have far more inap-
propriate candidates than appropriate candidates,
and this is especially true for the experiments de-
scribed in Section 4. This is indeed a problem for
our system, as neither re-weighting nor attempt-
ing to balance the set of examples with the use
of random negative examples were shown to give
better performance on development data. This is
not to say that some means of balancing the data
would not provide comparable or superior perfor-
mance, but rather that such a weighting or sam-
pling scheme is not obvious.
An additional benefit of preference ranking over
classification is that preference ranking models the
better-than relationship between candidates. Typ-
ically a set of candidate answers are all related to a
question in some way, and we wish to know which
667
of the candidates are better than others. In con-
trast, binary classification simply deals with the
is/is-not relationship and will have difficulty when
two responses with similar feature values are clas-
sified differently. With preference ranking, viola-
tions of some rank constraints will affect the re-
sulting order of candidates, but sufficient ordering
information may still be present to correctly iden-
tify appropriate candidates.
To apply preference ranking to answer typing,
we learn a model over a set of questions q1, ..., qn.
Each question qi has a list of appropriate candidate
answers a(i,1), ..., a(i,u) and a list of inappropriate
candidate answers b(i,1), ..., b(i,v). The partial or-
dering r is simply the set
?i, j, k : {a(i,j) <r b(i,k)} (4)
This means that rank constraints are only gen-
erated for candidate answers a(i,j) and b(i,k) for
question qi and not between candidates a(i,j) and
b(l,k) where i 6= l. For example, the candidate an-
swers for the question ?What city hosted the 1988
Winter Olympics?? are not compared with those
for ?What colour is the sky?? because our partial
ordering r does not attempt to rank candidates for
one question in relation to candidates for another.
Moreover, no rank constraints are generated be-
tween a(i,j) and a(i,k) nor b(i,j) and b(i,k) because
the partial ordering does not include orderings be-
tween two candidates of the same class. Given two
appropriate candidates to the question ?What city
hosted the 1988 Winter Olympics??, New York
and Calgary, rank constraints will not be created
for the pair (New York, Calgary).
3 Methods
We begin with the work of Pinchak and Lin (2006)
in which question contexts (dependency tree paths
involving the wh-word) are extracted from the
question and matched against those found in a cor-
pus of text. The basic idea is that words that are
appropriate as answers will appear in place of the
wh-word in these contexts when found in the cor-
pus. For example, the question ?What city hosted
the 1988 Winter Olympics?? will have as one of
the question contexts ?X hosted Olympics.? We
then consult a corpus to discover what replace-
ments for X were actually mentioned and smooth
the resulting distribution.
We use the model of Pinchak and Lin (2006)
to produce features for our discriminative model.
Table 1: Feature templates
Pattern Description
E(t, c)
Estimated count of term t
in context c
C(t, c)
Observed count of term t in
context c
?
t? C(t
?, c)
Count of all terms appearing
in context c
?
c? C(t, c
?)
Count of term t in all
contexts
S(t)
Count of the times t occurs
in the candidate list
These features are mostly based on question con-
texts, and are briefly summarized in Table 1. Fol-
lowing Pinchak and Lin (2006), all of our features
are derived from a limited corpus (AQUAINT);
large-scale text resources are not required for our
model to perform well. By restricting ourselves
to relatively small corpora, we believe that our ap-
proach will easily transfer to other domains or lan-
guages (provided parsing resources are available).
To address the sparseness of question contexts,
we remove lexical elements from question context
paths. This removal is performed after feature val-
ues are obtained for the fully lexicalized path; the
removal of lexical elements simply allows many
similar paths to share a single learned weight. For
example, the term Calgary in context X ? sub-
ject ? host ? object ? Olympics (X hosted
Olympics) is used to obtain a feature value v that
is assigned to a feature such as C(Calgary, X ?
subject ? ? ? object ? ?) = v. Removal of
lexical elements results in a space of 73 possible
question contexts. To facilitate learning, all counts
are log values and feature vectors are normalized
to unit length.
The estimated count of term t in context c,
E(t, c), is a component of the model of Pinchak
and Lin (2006) and is calculated according to:
E(t, c) =
?
?
Pr(?|t)C(?, c) (5)
Essentially, this equation computes an expected
count for term t in question c by observing how
likely t is to be part of a cluster ? (Pr(?|t)) and
then observing how often terms of cluster ? oc-
cur in context c (C(?, c)). Although the model
of Pinchak and Lin (2006) is significantly more
668
complex, we use their core idea of cluster-based
smoothing to decide how often a term t will oc-
cur in a context c, regardless of whether or not t
was actually observed in c within our corpus. The
Pinchak and Lin (2006) system is unable to as-
sign individual weights to different question con-
texts, even though not all question contexts are
equally important. For example, the Pinchak and
Lin (2006) model is forced to consider a question
focus context (such as ?X is a city?) to be of equal
importance to non-focus contexts (such as ?X host
Olympics?). However, we have observed that it is
more important that candidate X is a city than it
hosted an Olympics in this instance. Appropriate
answers are required to be cities even though not
all cities have hosted Olympics. We wish to ad-
dress this problem with the use of discriminative
methods.
The observed count features of term t in con-
text c, C(t, c), are included to allow for combina-
tion with the estimated values from the model of
Pinchak and Lin (2006). Because Pinchak and Lin
(2006) make use of cluster-based smoothing, er-
rors may occur. By including the observed counts
of term t in context c, we hope to allow for the
use of more accurate statistics whenever they are
available, and for the smoothed counts in cases for
which they are not.
Finally, we include the frequency of a term t in
the list of candidates, S(t). The idea here is that
the correct and/or appropriate answers are likely
to be repeated many times in a list of candidate
answers. Terms that are strongly associated with
the question and appear often in results are likely
to be what the question is looking for.
Both the C(t, c) and S(t) features are exten-
sions to the Pinchak and Lin (2006) model and can
be incorporated into the Pinchak and Lin (2006)
model with varying degrees of difficulty. The
value of S(t) in particular is highly dependent on
the means used to obtain the candidate list, and the
distribution of words over the candidate list is of-
ten very different from the distribution of words in
the corpus. Because this feature value comes from
a different source than our other features, it would
be difficult to use in a non-discriminative model.
Correct answers to our set of questions are
obtained from the TREC 2002-2006 results
(Voorhees, 2002). For appropriateness labels we
turn to human annotators. Two annotators were in-
structed to label a candidate as appropriate if that
candidate was believable as an answer, even if that
candidate was not correct. For a question such as
?What city hosted the 1988 Winter Olympics??,
all cities should be labeled as appropriate even
though only Calgary is correct. This task comes
with a moderate degree of difficulty, especially
when dealing with questions for which appropriate
answers are less obvious (such as ?What kind of a
community is a Kibbutz??). We observed an inter-
annotator (kappa) agreement of 0.73, which indi-
cates substantial agreement. This value of kappa
conveys the difficulty that even human annotators
have when trying to decide which candidates are
appropriate for a given question. Because of this
value of kappa, we adopt strict gold standard ap-
propriateness labels that are the intersection of the
two annotators? labels (i.e., a candidate is only ap-
propriate if both annotators label it as such, other-
wise it is inappropriate).
We introduce four different models for the rank-
ing of appropriate answers, each of which makes
use of appropriateness labels in different ways:
Correctness Model: Although appropriateness
and correctness are not equivalent, this model
deals with distinguishing correct from incorrect
candidates in the hopes that the resulting model
will be able to perform well on finding both cor-
rect and appropriate answers. For learning, cor-
rect answers are placed at a rank above that of
incorrect candidates, regardless of whether or not
those candidates are appropriate. This represents
the strictest definition of appropriateness and re-
quires no human annotation.
Appropriateness Model: The correctness model
assumes only correct answers are appropriate. In
reality, this is seldom the case. For example,
documents or snipppets returned for the question
?What country did Catherine the Great rule?? will
contain not only Russia (the correct answer), but
also Germany (the nationality of her parents) and
Poland (her modern-day birthplace). To better ad-
dress this overly strict definition of appropriate-
ness, we rank all candidates labeled as appropri-
ate above those labeled as inappropriate, without
regards to correctness. Because we want to learn
a model for appropriateness, training on appropri-
ateness rather than correctness information should
produce a model closer to what we desire.
Combined Model: Discriminative preference
ranking is not limited to only two ranks. We
combine the ideas of correctness and appropri-
669
ateness together to form a three-rank combined
model. This model places correct answers above
appropriate-but-incorrect candidates, which are
in turn placed above inappropriate-and-incorrect
candidates.
Reduced Model: Both the appropriateness model
and the combined model incorporate a large num-
ber of rank constraints. We can reduce the number
of rank constraints generated by simply remov-
ing all appropriate, but incorrect, candidates from
consideration and otherwise following the correct-
ness model. The main difference is that some ap-
propriate candidates are no longer assigned a low
rank. By removing appropriate, but incorrect, can-
didates from the generation of rank constraints, we
no longer rank correct answers above appropriate
candidates.
4 Experiments
To compare with the prior approach of Pinchak
and Lin (2006), we use a set of what and which
questions with question focus (questions with a
noun phrase following the wh-word). These are
a subset of the more general what, which, and who
questions dealt with by Pinchak and Lin (2006).
Although our model can accommodate a wide
range of what, which, when, and who questions,
the focused what and which questions are an easily
identifiable subclass that are rarely definitional or
otherwise complex in terms of the desired answer.
We take the set of focused what and which ques-
tions from TREC 2002-2006 (Voorhees, 2002)
comprising a total of 385 questions and performed
9-fold cross-validation, with one dedicated devel-
opment partition (the tenth partition). The devel-
opment partition was used to tune the regulariza-
tion parameter of the SVM used for testing.
Candidates are obtained by submitting the ques-
tion as-is to the Google search engine and chunk-
ing the top 20 snippets returned, resulting in an
average of 140 candidates per question. Google
snippets create a better confusion set than simply
random words for appropriate and inappropriate
candidates; many of the terms found in Google
snippets are related in some way to the question.
To ensure a correct answer is present (where pos-
sible), we append the list of correct answers to the
list of candidates.
As a measure of performance, we adopt Mean
Reciprocal Rank (MRR) for both correct and ap-
propriate answers, as well as precision-recall for
appropriate answers. MRR is useful as a mea-
sure of overall QA system performance (Voorhees,
2002), but is based only on the top correct or
appropriate answer encountered in a ranked list.
For this reason, we also show the precision-recall
curve to better understand how our models per-
form.
We compare our models with three alternative
approaches, the simplest of which is random. For
random, the candidate answers are randomly shuf-
fled and performance is averaged over a number
of runs (100). The snippet frequency approach
orders candidates based on their frequency of oc-
currence in the Google snippets, and is simply the
S(t) feature of our discriminative models in isola-
tion. We remove terms comprised solely of ques-
tion words from all approaches to prevent question
words (which tend to be very frequent in the snip-
pets) from being selected as answers. The last of
our alternative systems is an implementation of the
work of Pinchak and Lin (2006) in which the out-
put probabilities of their model are used to rank
candidates.
4.1 Results
Figures 1 and 2 show the MRR results and
precision-recall curve of our correctness model
against the alternative approaches. In comparison
to these alternative systems, we show two versions
of our correctness model. The first uses a linear
kernel and is able to outperform the alternative ap-
proaches. The second uses a radial basis function
(RBF) kernel and exhibits performance superior to
that of the linear kernel. This suggests a degree
of non-linearity present in the data that cannot be
captured by the linear kernel alone. Both the train-
ing and running times of the RBF kernel are con-
siderably larger than that of the linear kernel. The
accuracy gain of the RBF kernel must therefore be
weighed against the increased time required to use
the model.
Figures 3 and 4 give the MRR results and
precision-recall curves for our additional mod-
els in comparison with that of the correctness
model. Although losses in MRR and precision
are observed for both the appropriate and com-
bined model using the RBF kernel, the linear ker-
nel versions of these models show slight perfor-
mance gains.
670
Figure 1: MRR results for the correctness model
First Correct Answer First Appropriate Candidate
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Random
Snippet Frequency
Pinchak and Lin (2006)
Linear Kernel
RBF Kernel
5 Discussion of Results
The results of our correctness model, found in Fig-
ures 1 and 2 show considerable gains over our al-
ternative systems, including that of Pinchak and
Lin (2006). The Pinchak and Lin (2006) system
was specifically designed with answer typing in
mind, although it makes use of a brittle generative
model that does not account for ranking of answer
candidates nor for the variable strength of various
question contexts. These results show that our dis-
criminative preference ranking approach creates a
better model of both correctness and appropriate-
ness via weighting of contexts, preference rank
learning, and with the incorporation of additional
related features (Table 1). The last feature, snippet
frequency, is not particularly strong on its own, but
can be easily incorporated into our discriminative
model. The ability to add a wide variety of po-
tentially helpful features is one of the strengths of
discriminative techniques in general.
By moving away from simply correct answers
in the correctness model and incorporating labeled
appropriate examples in various ways, we are able
to further improve upon the performance of our
approach. Training on appropriateness labels in-
stead of correct answers results in a loss in MRR
for the first correct answer, but a gain in MRR for
the first appropriate candidate. Unfortunately, this
does not carry over to the entire range of precision
over recall. For the linear kernel, our three ad-
Figure 2: Precision-recall of appropriate candi-
dates under the correctness model
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
P
r
e
c
i
s
i
o
n
RBF Kernel
Linear Kernel
Pinchak & Lin (2006)
Snippet Frequency
Random
ditional models (appropriateness, combined, and
reduced) show consistent improvements over the
correctness model, but with the RBF kernel only
the reduced model produces a meaningful change.
The precision-recall curves of Figures 2 and 4
show remarkable consistency across the full range
of recall, despite the fact that candidates exist for
which feature values cannot easily be obtained.
Due to tagging and chunking errors, ill-formed
candidates may exist that are judged appropriate
by the annotators. For example, ?explorer Her-
nando Soto? is a candidate marked appropriate
by both annotators to the question ?What Span-
ish explorer discovered the Mississippi River??
However, our context database does not include
the phrase ?explorer Hernando Soto? meaning that
only a few features will have non-zero values. De-
spite these occasional problems, our models are
able to rank most correct and appropriate candi-
dates high in a ranked list.
Finally, we examine the effects of training set
size on MRR. The learning curve for a single par-
titioning under the correctness model is presented
in Figure 5. Although the model trained with
the RBF kernel exhibits some degree of instabil-
ity below 100 training questions, both the linear
and RBF models gain little benefit from additional
training questions beyond 100. This may be due
to the fact that the most common unlexicalized
question contexts have been observed in the first
671
Figure 3: MRR results (RBF kernel)
First Correct Answer First Appropriate Candidate
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Correctness Model
Appropriateness Model
Combined Model
Reduced Model
100 training examples and so therefore additional
questions simply repeat the same information. Re-
quiring only a relatively small number of training
examples means that an effective model can be
learned with relatively little input in the form of
question-answer pairs or annotated candidate lists.
6 Prior Work
The expected answer type can be captured in a
number of possible ways. By far the most com-
mon is the assignment of one or more prede-
fined types to a question during a question anal-
ysis phase. Although the vast majority of the ap-
proaches to answer type detection make use of
rules (either partly or wholly) (Harabagiu et al,
2005; Sun et al, 2005; Wu et al, 2005; Molla? and
Gardiner, 2004), a few notable learned methods
for answer type detection exist.
One of the first attempts at learning a model for
answer type detection was made by Ittycheriah et
al. (2000; 2001) who learn a maximum entropy
classifier over the Message Understanding Confer-
ence (MUC) types. Those same MUC types are
then assigned by a named-entity tagger to iden-
tify appropriate candidate answers. Because of the
potential for unanticipated types, Ittycheriah et al
(2000; 2001) include a Phrase type as a catch-all
class that is used when no other class is appropri-
ate. Although the classifier and named-entity tag-
ger are shown to be among the components with
Figure 4: Precision-recall of appropriate (RBF
kernel)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
P
r
e
c
i
s
i
o
n
Correctness Model
Appropriateness Model
Combined Model
Reduced Model
the lowest error rate in their QA system, it is not
clear how much benefit is obtained from using a
relatively coarse-grained set of classes.
The approach of Li and Roth (2002) is sim-
ilar in that it uses learning for answer type de-
tection. They make use of multi-class learning
with a Sparse Network of Winnows (SNoW) and
a two-layer class hierarchy comprising a total of
fifty possible answer types. These finer-grained
classes are of more use when computing a notion
of appropriateness, although one major drawback
is that no entity tagger is discussed that can iden-
tify these types in text. Li and Roth (2002) also
rely on a rigid set of classes and so run the risk of
encountering a new question of an unseen type.
Pinchak and Lin (2006) present an alternative in
which the probability of a term being appropriate
to a question is computed directly. Instead of as-
signing an answer type to a question, the question
is broken down into a number of possibly overlap-
ping contexts. A candidate is then evaluated as to
how likely it is to appear in these contexts. Un-
fortunately, Pinchak and Lin (2006) use a brittle
generative model when combining question con-
texts that assumes all contexts are equally impor-
tant. This assumption was dealt with by Pinchak
and Lin (2006) by discarding all non-focus con-
texts with a focus context is present, but this is not
an ideal solution.
Learning methods are abundant in QA research
672
Figure 5: Learning curve for MRR of the first cor-
rect answer under the correctness model
10 25 50 100 150 200 310
Training Set Size
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
RBF Kernel
Linear Kernel
Snippet Frequency
Pinchak & Lin (2006)
Random
and have been applied in a number of different
ways. Ittycheriah et al (2000) created an en-
tire QA system based on maximum entropy com-
ponents in addition to the question classifier dis-
cussed above. Ittycheriah et al (2000) were able
to obtain reasonable performance from learned
components alone, although future versions of the
system use non-learned components in addition to
learned components (Prager et al, 2003). The
JAVELIN I system (Nyberg et al, 2005) uses
a SVM during the answer/information extraction
phase. Although learning is applied in many QA
tasks, very few QA systems rely solely on learn-
ing. Compositional approaches, in which multiple
distinct QA techniques are combined, also show
promise for improving QA performance. Echihabi
et al (2003) use three separate answer extraction
agents and combine the output scores with a max-
imum entropy re-ranker.
Surdeanu et al (2008) explore preference rank-
ing for advice or ?how to? questions in which a
unique correct answer is preferred over all other
candidates. Their focus is on complex-answer
questions in addition to the use of a collection of
user-generated answers rather than answer typing.
However, their use of preference ranking mirrors
the techniques we describe here in which the rela-
tive difference between two candidates at different
ranks is more important than the individual candi-
dates.
7 Conclusions and Future Work
We have introduced a means of flexible answer
typing with discriminative preference rank learn-
ing. Although answer typing does not represent a
complete QA system, it is an important component
to ensure that those candidates selected as answers
are indeed appropriate to the question being asked.
By casting the problem of evaluating appropriate-
ness as one of preference ranking, we allow for
the learning of what differentiates an appropriate
candidate from an inappropriate one.
Experimental results on focused what and
which questions show that a discriminatively
trained preference rank model is able to outper-
form alternative approaches designed for the same
task. This increase in performance comes from
both the flexibility to easily combine a number of
weighted features and because comparisons only
need to be made between appropriate and inappro-
priate candidates. A preference ranking model can
be trained from a relatively small set of example
questions, meaning that only a small number of
question/answer pairs or annotated candidate lists
are required.
The power of an answer typing system lies
in its ability to identify, in terms of some given
query, appropriate candidates. Applying the flexi-
ble model described here to a domain other than
question answering could allow for a more fo-
cused set of results. One straight-forward appli-
cation is to apply our model to the process of in-
formation or document retrieval itself. Ensuring
that there are terms present in the document ap-
propriate to the query could allow for the intel-
ligent expansion of the query. In a related vein,
queries are occasionally comprised of natural lan-
guage text fragments that can be treated similarly
to questions. Rarely are users searching for sim-
ple mentions of the query in pages; we wish to
provide them with something more useful. Our
model achieves the goal of finding those appropri-
ate related concepts.
Acknowledgments
We would like to thank Debra Shiau for her as-
sistance annotating training and test data and the
anonymous reviewers for their insightful com-
ments. We would also like to thank the Alberta
Informatics Circle of Research Excellence and the
Alberta Ingenuity Fund for their support in devel-
oping this work.
673
References
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu,
E. Melz, and D. Ravichandran. 2003. Multiple-
Engine Question Answering in TextMap. In Pro-
ceedings of the Twelfth Text REtrieval Conference
(TREC-2003), Gaithersburg, Maryland.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden,
A. Hickl, and P. Wang. 2005. Employing Two
Question Answering Systems in TREC-2005. In
Proceedings of the Fourteenth Text REtrieval Con-
ference (TREC-2005), Gaithersburg, Maryland.
A. Ittycheriah, M. Franz, W-J. Zhu, A. Ratnaparkhi,
and R. Mammone. 2000. IBM?s Statistical Ques-
tion Answering System. In Proceedings of the 9th
Text REtrieval Conference (TREC-9), Gaithersburg,
Maryland.
A. Ittycheriah, M. Franz, and S. Roukos. 2001. IBM?s
Statistical Question Answering System ? TREC-10.
In Proceedings of the 10th Text REtrieval Confer-
ence (TREC-10), Gaithersburg, Maryland.
T. Joachims. 1999. Making Large-Scale SVM Learn-
ing Practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT-Press.
T. Joachims. 2002. Optimizing Search Engines Us-
ing Clickthrough Data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing (KDD). ACM.
X. Li and D. Roth. 2002. Learning Question Clas-
sifiers. In Proceedings of the International Confer-
ence on Computational Linguistics (COLING 2002),
pages 556?562.
D. Molla? and M. Gardiner. 2004. AnswerFinder -
Question Answering by Combining Lexical, Syntac-
tic and Semantic Information. In Proceedings of the
Australian Language Technology Workshop (ALTW
2004, pages 9?16, Sydney, December.
E. Nyberg, R. Frederking, T. Mitamura, M. Bilotti,
K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, L. Lita,
V. Pedro, and A. Schlaikjer. 2005. JAVELIN I and
II Systems at TREC 2005. In Proceedings of the
Fourteenth Text REtrieval Conference (TREC-2005),
Gaithersburg, Maryland.
C. Pinchak and D. Lin. 2006. A Probabilistic Answer
Type Model. In Proceedings of the Eleventh Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL 2006), Trento,
Italy, April.
J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Itty-
cheriah, and R. Mahindru. 2003. IBM?s PIQUANT
in TREC2003. In Proceedings of the Twelfth Text
REtrieval Conference (TREC-2003), Gaithersburg,
Maryland.
R. Sun, J. Jiang, Y.F. Tan, H. Cui, T-S. Chua, and M-Y.
Kan. 2005. Using Syntactic and Semantic Relation
Analysis in Question Answering. In Proceedings
of the Fourteenth Text REtrieval Conference (TREC-
2005), Gaithersburg, Maryland.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of the 46th Annual Meeting for
the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-08: HLT), pages
719?727, Columbus, Ohio, June. Association for
Computational Linguistics.
E.M. Voorhees. 2002. Overview of the TREC 2002
Question Answering Track. In Proceedings of
TREC 2002, Gaithersburg, Maryland.
M. Wu, M. Duan, S. Shaikh, S. Small, and T. Strza-
lkowski. 2005. ILQUA ? An IE-Driven Ques-
tion Answering System. In Proceedings of the
Fourteenth Text REtrieval Conference (TREC-2005),
Gaithersburg, Maryland.
674
Proceedings of NAACL HLT 2007, pages 516?523,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automatic Answer Typing for How-Questions
Christopher Pinchak and Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta, T6G 2E8, Canada
 
pinchak,bergsma  @cs.ualberta.ca
Abstract
We introduce an answer typing strategy
specific to quantifiable how questions. Us-
ing the web as a data source, we auto-
matically collect answer units appropri-
ate to a given how-question type. Exper-
imental results show answer typing with
these units outperforms traditional fixed-
category answer typing and other strate-
gies based on the occurrences of numeri-
cal entities in text.
1 Introduction
Question answering (QA) systems are emerging as
a viable means of obtaining specific information in
the face of large availability. Answer typing is an
important part of QA because it allows the system
to greatly reduce the number of potential answers,
using general knowledge of the answer form for a
specific question. For example, for what, where, and
who questions like ?What is the capital of Canada??,
answer typing can filter the phrases which might be
proposed as candidate answers, perhaps only identi-
fying those textual entities known to be cities.
We focus on answer typing for how-questions, a
subset of questions which have received little spe-
cific attention in the QA community. Rather than
seeking an open-ended noun or verb phrase, how-
questions often seek a numerical measurement ex-
pressed in terms of a certain kind of unit, as in the
following example:
Example 1: ?How heavy is a grizzly bear??
An answer typing system might expect answers to
include units like kilograms, pounds, or tons. Enti-
ties with inappropriate units, such as feet, meters, or
honey pots, would be excluded as candidate answers.
We specifically handle the subset of how-
questions that we call how-adjective questions; that
is, questions of the form ?How adjective...?? such
as Example 1. In particular, we do not address ?how
many? questions, which usually specify the units di-
rectly following many, nor ?how much? questions,
which generally seek a monetary value.
Hand-crafting a comprehensive list of units ap-
propriate to many different adjectives is time-
consuming and likely to miss important units. For
example, an annotator might miss gigabytes for a
measure of ?how large.? Instead of compiling a list
manually, we propose a means of automatically gen-
erating lists of appropriate units for a number of real-
world questions.
How-adjective questions represent a significant
portion of queries sent to search engines; of the
35 million queries in the AOL search query data
set (Pass et al, 2006), over 11,000 are of the form
?how adjective...? ? close to one in every three thou-
sand queries. Of those 11,000 queries, 152 different
adjectives are used, ranging from the expected ?how
old? and ?how far? to the obscure ?how orwellian.?
This high proportion of queries is especially strik-
ing given that search engines provide little sup-
port for answering how-adjective questions. Indeed,
most IR systems work by keyword matching. En-
tering Example 1 into a search engine returns doc-
uments discussing the grizzly?s ?heavy fur,? ?heavy,
shaggy coat? and ?heavy stout body.? When faced
516
with such results, a smart search engine user knows
to inject answer units into their query to refine their
search, perhaps querying ?grizzly pounds.? They
may also convert their adjective (heavy) to a related
concept (weight), for the query ?grizzly weight.?
Similarly, our approach discovers unit types by
first converting the adjective to a related concept, us-
ing information in a structured ontology. For exam-
ple, ?big? can be used to obtain ?size,? and ?tall? can
derive ?height.? We then use an online search engine
to automatically find units appropriate to the con-
cept, given the assumption that the concept is explic-
itly measured in terms of specific units, e.g., height
can be measured in feet, weight can be measured in
pounds, and size can be measured in gigabytes.
By automatically extracting units, we do not re-
quire a set of prior questions with associated an-
swers. Instead, we use actual questions as a source
of realistic adjectives only. This is important be-
cause while large sets of existing questions can be
obtained (Li and Roth, 2002), there are many fewer
questions with available answers.
Our experiments demonstrate that how-question-
specific unit lists consistently achieve higher answer
identification performance than fixed-type, general-
purpose answer typing (which propose all numeri-
cal entities as answer candidates). Furthermore, our
precomputed, automatically-generated unit lists are
shown to consistently achieve better performance
than baseline systems which derive unit lists at run-
time from documents relevant to the answer query,
even when such documents are gathered using per-
fect knowledge of the answer distribution.
The outline of the paper is as follows. In Section 2
we outline related work. In Section 3 we provide the
framework of our answer-typing model. Section 4
describes the implementation details of the model.
Section 5 describes our experimental methodology,
while Section 6 shows the benefits of using auto-
matic how-question answer-typing. We conclude
with possible directions of future research opened
by this novel problem formulation.
2 Previous Work
Answer typing is an important component of any
QA system, but varies greatly in the approach
taken (Prager et al, 2003; Harabagiu et al, 2005).
Basically, answer typing provides a means of filter-
ing answer candidates as either appropriate or in-
appropriate to the question. For example, Li and
Roth (2002) assign one of fifty possible types to a
question based on features present in the question.
Answer candidates can then be selected from text
by finding entities whose type matches that of the
input question. Similarly, Ittycheriah et al (2000)
assign one of the MUC named-entity types to each
input question. In these fixed-category approaches,
how-questions are assigned a fixed type in the same
manner as other questions. For how-questions, this
corresponds to a numerical type. However, retriev-
ing all numerical entities will provide lower answer
identification precision than a system that only pro-
vides those specified with the expected answer units.
Pinchak and Lin (2006) propose a dynamic an-
swer typing system which computes a unique score
for the appropriateness of any word to a particu-
lar question. Unfortunately, their question context-
mapping is limited to what, where, and who ques-
tions, and thus is not defined for how-questions.
Wu et al (2005) handle how-questions differently
than other questions. They use special hand-crafted
rules to assign a particular answer target during the
answer typing phase. In this way, they take advan-
tage of the structure inherent in how-questions rather
than just treating them as general queries. However,
manually hand-crafting types is costly, and would
have to be repeated if the system was moved to a
new language or a new query domain. Our auto-
matic approach does not suffer from this drawback.
Light et al (2001) showed that for a small fixed
set of answer types, multiple words tagged with
the same type will exist even with perfect passage
retrieval, sentence retrieval, and type assignment.
For example, Example 1 may be answered with a
sentence such as ?bears range in weight from the
smaller black bear at 400 pounds to the gigantic griz-
zly at over 1200 pounds? in which two answers have
appropriate units but only one of which is correct.
We provide results in Section 6 confirming the lim-
its of answer typing at narrowing answer focus, us-
ing varying levels of perfect information.
Our approach makes use of the web as a large
corpus of useful information. Exploiting the vast
amount of data on the web is part of a growing trend
in Natural Language Processing (Keller and Lapata,
517
2003). Indeed, many QA systems have been devel-
oped using the web (to varying degrees) to assist in
finding a correct answer (Brill et al, 2001; Cucerzan
and Agichtein, 2005; Radev et al, 2001), as the web
is the largest available corpus even if its information
can be difficult to harness. Rather than relying on
the web to find the answer to a question, we rely on it
as a source of information on appropriate units only.
Should the domain of the question answering system
change from general factoid questions, units may be
extracted from a smaller, domain-specific corpus.
3 Model Framework
The objective of our model is to create a list of rel-
evant units for an adjective that may be found in a
how-question. We wish to create these lists a pri-
ori and off-line so that they are applicable to future
questions. Although the model described here can
be applied on-line at the time of question answering,
the resources and time required make off-line gener-
ation of unit lists the preferred approach.
We wish to automatically learn a mapping  
 
in which

is a set of adjectives derived
from how-questions and
	
is a set of lists of units
associated with these adjectives. For example, an
element of this mapping might be:
high 


 
feet, meter, foot, inches, ... 


which assigns height measurements to ?how high?
questions. Inducing this mapping means establish-
ing a connection, or co-occurrence, between each
adjective  and its units 	 . In the following sub-
sections, we show how to establish this connection.
3.1 Using WordNet for Adjective Expansion
In common documents, such as news articles or
web pages, the co-occurrence of an adjective and
its units may be unlikely. For example, the co-
occurrence between ?heavy? and ?pounds? may
not be as prevalent as the co-occurrence between
?weight? and ?pounds.? We therefore propose us-
ing WordNet (Fellbaum, 1998) to expand the how-
adjective  to a set of related concepts the adjective
may be used to describe. We denote a related con-
cept of  as  . In the above example, ?heavy? can be
used to describe a ?weight.? Two useful WordNet re-
lations are the attribute relation, in which the adjec-
tive is an attribute of the concept, and in cases where
no attribute exists, the derivationally-related words.
?Heavy? is an attribute of ?weight? whereas the
derivationally-related form is ?heaviness,? a plausi-
ble but less useful concept. Next we describe how
the particular co-occurrence of the related concept 
and unit  is obtained.
3.2 Using Google to Obtain Counts
We selected the Google search engine as a source
of co-occurrence data due to the large number of in-
dexed documents from which co-occurrence counts
can be derived. To further enhance the quality of
co-occurrence data, we search on the specific phrase
?  is measured in? in which  is one of the related
concepts of  . This allows for the simultaneous dis-
covery of unknown units and the retrieval of their
co-occurrence counts.
Sentences in which the pattern occurs are parsed
using Minipar (Lin, 1998b) so that we can obtain
the word related to ?measured? via the preposi-
tional in relation. This allows us to handle senten-
tial constructions that may intervene between ?mea-
sured? and a meaningful unit. For each unit  that
is related to ?measured? via in, we increment the
co-occurrence count 