An Annotation Tool for Multimodal Dialogue Corpora  
using Global Document Annotation 
Kazunari ITO and Hiroaki SAITO 
Keio University 
Department of Science for Open and Environmental Systems 
3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan, 223-8522 
 {k_ito , hxs}@nak.ics.keio.ac.jp 
 
 
 
 
 
Abstract 
This paper reports a tool which assists the 
user in annotating a video corpus and en-
ables the user to search for a semantic or  
pragmatic structure in a GDA tagged cor-
pus. An XQL format is allowed for search 
patterns as well as a plain phrase. This 
tool is capable of generating a GDA time-
stamped corpus from a video file  manu-
ally. It will be publicly available for aca-
demic purposes. 
1 Introduction 
To achieve a natural communication environment 
between computers and the users, many interactive 
prototype systems that can talk with the user have 
been developed using multimodal information 
(face expressions, voice tones, gestures, etc .). 
Since multimodalness of these systems is manually 
built in, achieving free and effective communica-
tion or enhancing communication ablilities is not 
easy. Thus automatic learning from huge data is 
hoped for.  
Recently such various video data as TV dramas, 
news, and language teaching materials are avail-
able, from which natural interactiveness should be 
extracted. Such interactiveness from an intellectual 
content is also valuable for the fields of machine 
translation, information retrieval, handling ques-
tion responses, and knowledge discovery systems. 
GDA1 (Global Document Annotation), which is 
an XML tag set, adds information on syntax, se-
mantics, and pragmatics to texts (Hashida 1998). 
The texts with GDA organically corresponding to 
voice and a video will contribute to the basic re-
search into these technologies and promote the ap-
plication development. 
2 GDA tagged corpus 
This chapter explains the GDA tag set and a 
method which relates tagged data with the video 
image. 
 
2.1 GDA 
 
The GDA Initiative aims at having Internet authors  
annotate their electronic documents with a com-
mon standard tag set which allows machines to  
automatically recognize the semantic and prag-
matic structures of the documents. A huge amount 
of annotated data is expected to emerge, which 
should serve not just as tagged linguistic corpora 
but also as a worldwide, self-extending knowl-
edge-base mainly consisting of examples of how 
our knowledge manifests. It describes the meaning 
of sentence analysis (semantics and pragmatics) 
basically. It also describes information on the  sub-
ject role, the rhetoric relation, and correspondence. 
Figure 1 shows an example of the text ?????
???? (an ear is covered)? tagged with GDA. 
Note that GDA is totally language independent, 
although all the following examples include Japa-
nese texts. 
 
                                                               
1 http://www.i-content.org/GDA/tagset.html 
<q who="A"> 
<su syn=?fc? id="kakure"> 
<vp syn="f"> 
<n arg="X">??</n> 
<ad sem="obj">?</ad> 
<adp syn="f"> 
<v>???</v> 
<ad>?</ad> 
</adp> 
<v>?</v> 
</vp> 
</su> 
</q> 
 
Figure 1: A fragment of GDA corpus 
 
In Figure 1, <q> represents a word where that 
part is spoken by someone, and the value of who 
attribute shows who utters it. <su> indicates a sen-
tence, having no syntactic relation to other parts of 
the utterance. Attribute ?syn? means it is a syntax 
structure of the sentence, and ?fc? means a forward 
link dependency. <v> and <vp> elements mean a 
verb and a verb phrase, respectively. <n> element 
represents a noun or a noun phrase. <ad> and 
<adp> elements are an adverb, and a postpositional 
phrase. As these examples display, the GDA tag 
set has been determined to show syntactic structure 
effectively where a word is assumed to be a unit. 
 
2.2 Adding Time -information to GDA tagged 
corpora 
 
When you relate the video image file with its text 
file, it is widely used to embed the time  informa-
tion indicating when an utterance is spoken. We 
define the following two kinds of new formats to 
relate a video file with its GDA corpus file . 
 
1. btime and etime attributes: These attributes can 
be added to an arbitrary tag. Attribute btime shows 
the start time of an utterance. Attribute etime 
shows the finished time. The format is described as 
follows. 
<any btime=?utterance start time(sec)? 
etime=?utterance end time(sec)? > 
sentence 
</any> 
 
With these attributes, the voiceless section of an 
utterance can be precisely indicated and the over-
lap event is detected in a multi-speaker environ-
ment. 
2. tst (time stamp) tag: Tag ?tst? is an empty ele-
ment tag and described in the following format. 
 
<tst val = ?utterance start time 
(sec)? /> 
 
A tst tag can be inserted in an arbitrary place. The 
ending time of an utterance can be determined 
from the value of the next btime attribute or val 
attribute of the next tst tag.  Moreover, it is also 
possible that the tst tag has btime and etime attrib-
ute. Figure 2 shows an example when time infor-
mation is added to the GDA tagged corpus in 
Figure 1. You can see a man allocated ?A? speaks 
a word ??? (an ear)? from 60.81 to 61.03 sec, a 
word ?? (is)? from 61.10 to 61.90 sec. 
 
<q who=?A?> 
<su id=?kakure?> 
<vp syn=?f?> 
<n arg="X" btime=?60.815? 
etime="61.034?>?? 
</n> 
<ad sem=?obj?> 
?<tst val=?61.100?/></ad> 
<adp syn=?f?> 
<v>??? 
<tst val="61.907"/></v> 
<ad>?<tst val="62.192"/></ad> 
</adp> 
<v>?<tst val="62.383"/></v> 
</vp> 
</su> 
</q> 
 
Figure 2: An example of GDA corpus with time-
stamp 
 
These time annotation is often inserted after 
GDA tagging, because time information is inde-
pendent from the standard syntactic/pragmatic 
GDA. 
3 An annotation tool for multimodal dia-
logue corpus 
We have developed a multimodal annotation tool 
for a video corpus and its annotated data (JEITA 
2001). This tool runs on any platform that accom-
modates Java2 and Java Media  Framework (JMF2) 
ver.2.0 or higher. It also requires Java-based XML 
parser Xerces-J 3  and Java-based XQL engine 
                                                               
2 http://java.sun.com/products/java-media/jmf/ 
3 http://xml.apache.org/xerces-j/ 
GMD-IPSI XQL4. Moreover, users can easily ex-
tend functions by mounting plug-ins. The proto-
type is equipped with two different types of plug-
ins. One is ?XQL search plug-in? in which the user 
can search for various syntactic and semantic  pat-
terns in a GDA corpus. An XQL format query as 
well as a plain word is allowed for search patterns. 
Another is ?Annotation plug-in? which assists the 
annotator in generating a time-stamped GDA cor-
pus from a video file. 
 
4 Basic Functions 
A screenshot of basic composition is shown in 
Figure 3. The whole window is composed of sev-
eral internal windows.  
 
 
Figure 3: Screenshot of basic  composition 
 
The left internal window is ?time table window? 
which displays each sentence in a table format. 
When the user clicks one of the columns in the 
table, the video corresponding to the sentence is 
played. Rows of the table are highlighted consecu-
tively during that part of the video is being dis-
played. The top right is ?video image window?. It 
displays and plays the video image.  
5 Extended Functions 
This section explains the outline of two plug-ins 
first and usage of  these after that. 
 
5.1 XQL search plug-in 
 
                                                               
4 http://xml.darmstadt.gmd.de/xql/  
Figure 4 shows the screenshot when the XQL 
search plug-in is loaded, you can see new window 
appears at the bottom left of the tool. The user can 
type a query into the text field at the bottom of the 
new window.  
 
 
Figure 4: Screenshot when XQL search plug-in is 
loaded 
 
An acceptable query format is a plain text or a 
query text defined by XQL (XML Query Lan-
guage). XQL is a subset of query language XQuery 
that uses XML as a data  model, a recommendation 
by W3C. XQL has already been mounted on soft-
ware over many fields like Web browsers, docu-
ment archiving systems, XML middleware, Perl 
libraries, and a command line utility. XQL always 
returns a part of the document. In XQL, the hierar-
chy of the node is written by ?/?, an arbitrary hie r-
archy is written by ?//?, the attribute name by 
?@name?, the tag name by ?name? as it is. A regu-
lar expression and a conditional expression are also 
acceptable. For a exmple, ? //q[@who='A']? returns 
?q? node with the value of attribute name ?who? 
equals ?A?.  
Figure 5 shows search results. In Figure 5, five 
words [????(a hair), ????(a forelock), ?
??(a prominent forehead), ??(a face), ??(an 
ear)] are matched on the condition that the part of 
speech is noun and the value of the attribute ?arg? 
equals to ?X? (see the fourth line of Figure 2). 
Moreover, since the label ?X? is attached to the 
utterance "???(this person)" in this GDA file, 
this query becomes a meaning of extracting a noun 
phrase whose subject is ?this person?. The user can 
search for the subject even if an object is omitted 
in suited sentences.  
  
Figure 6:  Screenshot when Annoation plug-in is loaded 
 
A query of extracting a synonym of a certain 
word, only the utterance of a specific speaker to 
another, and a sentence that of the response for a 
certain utterance, etc. are possible. Semantic  or 
discourse structures are also extractable if such 
information is annotated in the file. Clicking any 
column of the table, the corresponding media sec-
tion is played like the ?time table Window?. 
 
 
Figure 5: Result by query [a noun and value of at-
tribute ?arg? equals ?X?]  
 
5.2 Annotation plug-in 
 
Creating a GDA file with timestamps record auto-
matically with less time and less labor is indispen-
sable to make large quantities of them and to 
spread them. From the accuracy of the current 
speech recognition technology, it is difficult to at-
tach timestamps record automatically and accu-
rately by taking synchronization of a video image. 
Annotation plug-in increases the efficiency of 
time-stamping a GDA file by visual operation.  
Figure 6 shows the screenshot when it is loaded. 
The window located on the lower part of Figure 
6 called ?Annotation board? which displays infor-
mation on a GDA file with a time-stamp visually. 
You can also see a horizontal axis which expresses 
the time on media, and two layers in the board. 
Upper layer displays utterances of ?Speaker A?, 
lower layer displays ?Speaker B? in this case. Rec-
tangles on one layer represent each speaker's utter-
ances according to the time series. The utterance 
itseft is displayed in the rectangle, color of which 
is different for each speaker. Length and the pos i-
tion indicate time information of the utterance. A 
person edits the annotation by operating two kinds 
of lines on the board. "Current line" shows a cur-
rent playback position on the media. "Base line" 
indicates the start or the end point of the time-
stamp when the utterance sentence is newly in-
serted. Various functions (change of line?s position, 
and deletion or insertion of an utterance) can be 
executed by mouse operation. When the annotator 
clicks on the board, the ?Current line? moves and a 
frame to which a video image corresponds is dis-
played. Thus, an annotator can attach the informa-
tion of the start time and the end time and utterance 
text itself manually. A prototype system which 
automatically converts the GDA file from raw text 
files with a morphological analysis and a parsing 
tool in addition to the original filter has already 
been proposed (Suzuki 2001). 
6 Current development 
The core functions of the tool are complete and 
stable. Still, there is much room for expansion. The 
following functions are being developed. 
 
6.1 User-friendly GUI-based search interface 
 
Needless to say, there is no guarantee that a clause 
or a sentence which agrees with an XQL query 
exists. Moreover the user has to know the XQL 
expression for search. We believe it necessary that 
a retrieval way by the top-down philosophy which 
narrows the candidates while presenting suited 
clauses sequentially. 
It is very difficult at present for an XQL to ex-
press dependency relations among the search con-
dition. Now, a query language of XML has been 
integrated into Xquery. Hence, we are scheduled to 
bulid an Xquery engine. A user-friendly GUI-
based search window for the retrieval which does 
not require an explicit XQL query is currently be-
ing developed.  
 
6.2 Uniting with other multimodal data 
 
There are many kinds of specifications to describe 
multimodal data. For example, J-ToBI(Venditti, 
1995) which describes prosodic information of 
voice, FACS(Ekman, etc. 1978) which annotates 
person's expression, etc. We are scheduled to de-
sign the specification to integrate these information 
into GDA in the XML format. As a result, the user 
will be able to present a condition, for example, of 
a word ?Truth? of doubt type or ?I see? of  hesita-
tion. 
It is also scheduled to relate visual information 
of video data with GDA. They can contain infor-
mation on motion, glance and the place of the ob-
ject in video image. A reverse-search which 
extracts a corresponding text from visual informa-
tion in a video image becomes available, too. 
 
6.3 Coordinated functions 
 
We intend to enhance a relation with other annota-
tion tools. Concretely, various formats of output 
files can be taken in this tool in XML formats. We 
shall define a DTD (data conversion definition) to 
enable export and import to/from other tools. A 
function of date exchange enables the user to en-
hance flexibility and accessbility of this tool. 
 
6.4 Retrieval for multiple files 
 
The user can retrieve only a single file in a local 
machine at present. This tool will cope with the 
client-server model that it requests retrieval de-
mand to the corpora database servers on a network, 
downloads only necessary files to the local ma-
chine and analyzes them. 
7 Related works 
Most of recent multilmodal annotation tool pro-
jects are almost Java-based, use XML for file  ex-
change and have an object-oriented design:    
MATE (Carletta, etc. 1999) is an annotation 
workbench that allows highly generic  specification 
via stylesheets that determine  shape and 
functionality of the user?s implemation. Speed and 
stability for the tool are both still problematic for 
real annotation. Also, the generic approach 
increases the initial effort to set up the tool since 
you basically have to write your own tool using the 
MATE style language.  
EUDICO (Brugman et al 2000) is an effort to 
allow multi-user annotation of a centrally localted 
corpus via a web-based interface. The tools that are 
available to work with the  multimodal corpus 
make it possible to analyze their content and to add 
new free defined annotations to them. A EUDICO 
client can choose a subset of the corpus data.  
Anvil (Kipp, 2001) is a generic video annota-
tion tool which allows frame-accurate, hierarchical 
multi-layered annotation with objects that contain 
attribute-value pairs. Layers and attributes are all 
user-defineable. A time-aligned view and some 
configuration options make coding work quite in-
tuitive.  
ATLAS project (Steven, etc. 2000) deals with 
all types of annotation and is theoretically based on 
the idea of annotation graphs where nodes repre-
sent time points and edges indicate annotation la-
bels. 
8 Conclusion 
We have reported an annotation tool for multimo-
dal dialogue corpora. This tool enables semantic 
and pragmatic search from a video data with anno-
tated texts in the GDA format. This tool is plat-
form-independent and equipped with easy-to-use 
interface. It will be of use to researchers dealing 
with multimodal dialogue for exploratory studies 
as well as annotation. Core functions are complete 
and various extension facilities are now being de-
veloped.  This prototype will be publicly available 
soon. 
9 Acknowledgements 
We wish to express our gratitute for the members 
of the committee on methods and standards of dia-
logic content in Japan Electronics and Information 
Technology Industries Association (JEITA). Espe-
cially we would like to thank Koichi Hashida for 
insightful advices. 
 
References 
Hasida, K. (1998) Intellectual contents of all-round 
based on GDA meaning modification. The transac-
tion of Japanese Society for Artificial Intelligence., 
Vol. 13, No.4, pp.528-535 (in Japanese). 
JEITA (2001) Servey Report about natural language 
processing system. pp.49-56(in Japanese). 
J. Suzuki and K. Hasida (2001) Proposal of answer ex-
traction system using GDA tag. The 7th annual meet-
ing of Language Processing Society (in Japanese). 
Ekman, P. and Friesen, W.(1978) Facial action coding 
system : a technique for the measurement of facia l-
movement, Consulting Psychologists Press, 1978 
 
J.J.Venditti,(1995) Japanese ToBI Labelling Guide lines. 
Ohio-State University,Columbus,U.S.A.,1995. 
Michael Kipp (2001) Anvil - A Generic Annotation 
Tool for Multimodal Dialogue, Proceedings of Eu-
rospeech 2001, pp.1367-1370. 
Carletta, J. and Isard, A (1999) The MATE Annotation 
Workbench, In Proceedings of the ACL Workshop, 
Towards Standards and Tools for Discourse Tagging., 
pp.11-17. 
H. Brugman, A. Russel, D. Broeder, and P.Wittenburg 
(2000) EUDICO. Annotation and Exploitation of 
Multi Media Corpora, Proceedings of LREC 2000 
Workshop. 
Steven Bird, David Day, John Garofolo, John Hender-
son, Christophe Laprun, and Mark Liberman (2000) 
ATLAS: A Flexible and Extensible Architecture for 
Linguistic Annotation, Proceedings of the Second In-
ternational Conference on Language Resource and 
Evaluation, pp.1699-1706. 
 
A New E-learning Paradigm through Annotating Operations
Hiroaki Saito, Kyoko Ohara, Kengo Sato, Kazunari Ito, Shinsuke Hizuka
Masaya Soga, Tomoya Nishino, Yuji Nomura, Hideaki Shirakawa, Hiroyuki Okamoto
Dept. of Computer Science, Keio University
3-14-1, Hiyoshi, Kohoku-ku, Yokohama, 223-8522, Japan
hxs@ics.keio.ac.jp
Abstract
This paper proposes a new e-learning paradigm
which enables the user to type in arbitrary sentences.
The current NLP technologies, however, are not ma-
tured enough to perform full-automatic semantic or
discourse analysis. Thus, we take a different ap-
proach; an instructor corrects the contents and its
correction is embedded into the contents in an XML
format called KeML. The key/mouse operations of
the user are also embedded as annotations. Thus,
the contents can be incomplete at the initial stage
and become solid gradually as being utilized. We
have implemented an e-learning system of group dis-
cussion for a foreign language class, which is demon-
strated at the workshop.
1 Introduction
Many old e-learning systems asked the user to click
the button or to type a unique answer. The domains
of the e-learning systems were limited by their ap-
proaches and their contents were fixed. This paper
introduces a new approach to expand the domain.
Namely, the user himself annotates the contents in
addition to the instructor.
Annotated contents can be used for further learn-
ing such as example-based learning and group learn-
ing. The burden of building e-learning contents are
so heavy that this ?annotated contents become other
contents? scheme is important for practical applica-
tions. Annotations are attached in an XML format.
This project can be considered as another applica-
tion of XML technologies, like MATE [1] [2] or Anvil
[3] to name a few. The principal difference is that
some annotations are implicitly attached and used
for NLP.
2 System Overview
Here we consider the debate discussion for a for-
eign language class as an example. This course was
originally taught in a regular classroom and through
an electronic chatting board supervised by a human
instructor. One student posts his/her thought in
English and others express positive or negative re-
sponses to it. Since this is a foreign language appre-
hension class, the instructor corrects the students?
English if necessary. Since students express various
opinions, we cannot prepare for them in advance.
The instructor was occupied with correcting syntac-
tic errors, therefore, the instructor could not thor-
oughly pay attention to the flow of the debate or to
whether students had appropriately expressed their
opinions.
In Figure 1 example debate submissions are shown
on the topic ?English should be taught at an elemen-
tary school.? #n indicates the submitted order and
P1, P2, ... stand for the identifier of the debaters.
(We will further explain Figure 1 later.)
Our system is designed for multi-user discussion,
not for self-learning. Thus, we divide the system
into the server and the client machines as shown in
Figure 2 (only one client machine is drawn in the
figure). The server machine manages the contents
and handles computationally heavy NLP, while each
client machine is responsible for user interface.
We have developed an e-learning system which of-
fers the process above. Here we describe five impor-
tant modules:
? Sentence analysis module: In this module the
input sentences are parsed and tagged syntac-
tically and semantically in the GDA (Global
Document Annotation) [4] format. We have
adopted the Charniak parser [5], which is cus-
tomized so that the head word is identified be-
cause the GDA tagging requires the attachment
direction between phrases. The GDA tagger
consults the WordNet [6] to find the root form
of a word. GDA tags can be utilized for such
further NLP as high quality summarization or
information retrieval [7].
? KWIC module: Novice English learners of-
ten make such mistakes in collocations and in
phrasal verbs. These word-usage mistakes can
be effectively resolved by looking at example
sentences rather than by consulting a regular
dictionary. This module presents the corpus
sentences in the KWIC (KeyWord in Context)
format in which the specified words are in-
cluded. Although any corpus will do for KWIC,
we have chosen the EDR English corpus [8]
which consists of 125,820 tagged sentences. Be-
cause the root form of each word is described as
a tag, conjugated forms are also searched.
? Annotation module: The instructor corrects the
wrong usages of the students? English. This op-
PROPOSITION
English should be taught as early as at an elementary school.
2. It is often said that language is most efficiently taught during his/her early age.
against early English education?
What kind of negative effect would the score-ism cause
#6   P3 (pro)
approval
refutation
refutation
supplement
correction
1. English is a global standard language and is indispensable for grownups.
might lower their total achievement.
I disagree with teaching English at an elementary school.
more effectively   tought   in a junior high school also.
Current curriculum of teaching English is not effective enough.
I agree that the method of teaching English should be improved.
However, if the similar improvement is performed against other subjects, early
English education has no bad effect to them.
taught
#1   P1 (pro)
#2   P2 (con)
#3   P3 (pro)
#4   P4 (con)
#5   P1 (pro)
I am for teaching English at an elementary school in the following points.
Teaching English relatively decreases the class hours of other subjects and
current education system of ?  score-ism   ?, not the starting age.
he/she is young, but what we have to do first is   to improve the
It is true that learning a foreign language is best effective when
question
approval
We can expect that children who become familiar with English earlier can be
Figure 1: Submission Statements and their Relations
NL resources
EDRcorpus
dictionary
Parser
Charniak
GDA tagger
KWIC
NLP modules
results
user interface
user
profile
DB
Retriever
Summarizer
with data
commands
DB
contents
document in KeML
client server
user
user operations
info
display
operations
Figure 2: System Architecture
Figure 3: A Snapshot of Interface Window
eration is recorded as annotation, not overwrit-
ing the originals. Preserving the originals is ef-
fective for education; it can prevent other stu-
dents from making the similar mistakes. When
the debater expresses his opinion against/for
someone else?s, that operation is also observed
and attached to the contents, which will be ex-
ploited by NLP.
? Interface module: This module enables the user
to type in sentences, specify what part he is
arguing about, express his/her attitude, etc, ef-
ficiently. This module displays the contents ef-
fectively according to the needs of the user with
the help of annotations. Our current interface
snapshot is shown in Figure 3.
? Debate Flow module
It is important to know the debate flow when
one expresses his/her opinion. Since the rela-
tions among statements are annotated, precise
analysis of the debate flow is possible.
In the following sections, the annotation module
is explained deeply.
3 Annotation by the Instructor and
Students
When a student expresses his/her opinion in re-
sponse to someone else?s, he can specify and denote
what part he is arguing about. This linkage is an-
notated by the user and recorded in the contents.
The corrections/comments by the instructor are also
stored in the learning contents as annotations. Ar-
rows in Figure 1 show the relation of statements,
where a dotted line expresses the linkage denoted by
the instructor, and solid lines mean that the debater
specified those relations.
4 The Tag Set for Debate
We have defined a tag set for annotating debates
in an XML format called KeML (Keio e-learning
Markup Language). Here we describe our tag set
along with how each tag is attached through opera-
tions by the instructor or students.
<debate> encloses the whole debate and is at-
tached when a new debate starts. No at-
tribute is allowed. Possible child-nodes are one
<proposition> and zero or more <statement>s.
<proposition> is attached when a new proposition
is submitted. Its mandatory attribute is ?id?
and whose value is always ?0?. Its child-node
is <su> of GDA. The instructor or students
should remark the proposition as pros or cons.
<statement> This tag is attached when a state-
ment to a proposition or other statements is
submitted by the instructor or students. Its
mandatory attributes are ?attitude? whose value
would be pro or con, ?person? whose value in-
dicates who submitted that statement, ?time?
which indicates when that statement was given,
and ?id? number (an integer). The values of the
first two attributes are given by the user explic-
itly, while those of the last two are filled by the
system. Its optional attributes are ?approval?,
?refutation?, ?supplement?, ?summary?, ?ques-
tion?, and ?answer? (some of those attributes ap-
pear in Figure 1). They are expressed as ?ap-
proval=target id? for example. Its child-node is
<su> of GDA.
Such tags below the <su> level as <np> or <v>
are attached by the parser according to the
GDA specifications. Every tag must have ?id?
attribute and its value is filled automatically by
the server.
Appendix shows the annotated contents of the de-
bate example in Figure 1.
5 Preserving Corrected Contents
In order that a novice student could observe mis-
takes by other students, our system preserves the
original contents and shows them effectively when
needed. While some mistakes are obvious, others
are not. Only the instructor can correct or comment
those errors and KeML offers two levels of correction
preservation. Obvious mistakes are stored as the
value of ?original? attribute; ?<np original=?tought?
..>taught</np>? for instance. Unobvious mistakes
are commented in the value of ?comment? attribute;
?<su comment=?This is a comment for this sen-
tence.? ....</su>? for example. When further cor-
rection is made against already corrected contents,
only the very first version is preserved. Our current
implementation allows the correction/comments un-
der <su> nodes.
6 Conclusions
We have implemented an e-learning system which fa-
cilitates group-discussion for second language learn-
ing. Plain texts become solid as being used because
of the embedded explicit and implicit annotations by
the instructor and students. Accumulated contents
will be a good resource for statistical analysis and
example-based learning.
References
[1] MATE Workbench Homepage:
http://www.cogsci.ed.ac.uk/? dmck/MateCode/
[2] MATE Homepape:
http://mate.nis.sdu.dk
[3] Anvil Homepage:
http://www.dfki.de/? kipp/anvil/
[4] The GDA Tag Set Homepage:
http://www.i-content.org/gda/
[5] Charniak, E. ?A Maximum-Entropy-Inspired
Parser?, NAACL 2000. (For software, see
http://www.cs.brown.edu/people/ec/)
[6] WordNet
http://www.cogsci.princeton.edu/? wn/
[7] Miyata, T. and Hasida, K. ?Information Retrieval
Based on Linguistic Structure? in Proceedings of
the Japanese-German Workshop on Natural Language
Processing, July 2003.
[8] EDR Electric Dictionary, EDR English Corpus
http://www2.crl.go.jp/kk/e416/EDR/
Appendix. Annotated contents (a head por-
tion)
<?xml version=?1.0? encoding=?UTF-8?
standalone=?yes??> <debate> <proposition
id=?0? time=?Sun Jun 13 22:08:30 JST
2004?> <su id=?0.1?> <segp id=?0.1.1?
mph=?ptb;NNP;;English;?>English</segp>
<v id=?0.1.2?> <v id=?0.1.2.1?
mph=?ptb;MD;;should;?>should</v> <vp id=?0.1.2.2?>
<v id=?0.1.2.2.1? mph=?ptb;VBN;;teach;?>taught</v>
<adp id=?0.1.2.2.2?> <ad id=?0.1.2.2.2.1?> <adp
id=?0.1.2.2.2.1.1? mph=?ptb;RB;;as;?>as</adp> <ad
id=?0.1.2.2.2.1.2? mph=?ptb;RB;;early;?>early</ad>
</ad> <adp id=?0.1.2.2.2.2?> <ad id=?0.1.2.2.2.2.1?
mph=?ptb;IN;;as;?>as</ad> <adp id=?0.1.2.2.2.2.2?>
<ad id=?0.1.2.2.2.2.2.1? mph=?ptb;IN;;at;?>at</ad>
<np id=?0.1.2.2.2.2.2.2?> <adp id=?0.1.2.2.2.2.2.2.1?
mph=?ptb;DT;;an;?>an</adp> <ajp id=?0.1.2.2.2.2.2.2.2?
mph=?ptb;JJ;;elementary;?>elementary</ajp> <n
id=?0.1.2.2.2.2.2.2.3? mph=?ptb;NN;;school;?>school</n>
</np> </adp> </adp> </adp> </vp> </v>.</su>
</proposition> <statement id=?1? attitude=?pro?
person=?P1? time=?Sun Jun 13 22:10:41 JST 2004?>
<su id=?1.1?> <np id=?1.1.1? mph=?ptb;PRP;;I;?
eq=?p1?>I</np> <v id=?1.1.2?> <v id=?1.1.2.1?
mph=?ptb;VBP;;be;?>am</v> <adp id=?1.1.2.2?>
<ad id=?1.1.2.2.1? mph=?ptb;IN;;for;?>for</ad>
<vp id=?1.1.2.2.2?> <v id=?1.1.2.2.2.1?> <v
id=?1.1.2.2.2.1.1? mph=?ptb;VBG;;teach;?>teaching</v>
<segp id=?1.1.2.2.2.1.2?
mph=?ptb;NNP;;English;?>English</segp> <adp
id=?1.1.2.2.2.1.3?> <ad id=?1.1.2.2.2.1.3.1?
mph=?ptb;IN;;at;?>at</ad> <np
id=?1.1.2.2.2.1.3.2?> <n id=?1.1.2.2.2.1.3.2.1?> <adp
id=?1.1.2.2.2.1.3.2.1.1? mph=?ptb;DT;;an;?>an</adp>
<ajp id=?1.1.2.2.2.1.3.2.1.2?
mph=?ptb;JJ;;elementary;?>elementary</ajp>
<n id=?1.1.2.2.2.1.3.2.1.3?
mph=?ptb;NN;;school;?>school</n>
</n> <adp id=?1.1.2.2.2.1.3.2.2?> <ad
id=?1.1.2.2.2.1.3.2.2.1? mph=?ptb;IN;;in;?>in</ad> <np
id=?1.1.2.2.2.1.3.2.2.2?> <adp id=?1.1.2.2.2.1.3.2.2.2.1?
mph=?ptb;DT;;the;?>the</adp> <vp
id=?1.1.2.2.2.1.3.2.2.2.2? mph=?ptb;VBG;;follow;?>follo
wing</vp> <n id=?1.1.2.2.2.1.3.2.2.2.3?
mph=?ptb;NNS;;point;?>points</n> </np>
</adp> </np> </adp> </v> </vp> </adp>
</v>.</su> <su id=?1.2?> <np id=?1.2.1?> <num
id=?1.2.1.1? mph=?ptb;CD;;1.;?>1.</num> <segp
id=?1.2.1.2? mph=?ptb;NNP;;English;?>English</segp>
</np> <v id=?1.2.2?> <v id=?1.2.2.1?> <v
id=?1.2.2.1.1? mph=?ptb;VBZ;;be;?>is</v>
<np id=?1.2.2.1.2?> <adp id=?1.2.2.1.2.1?
mph=?ptb;DT;;a;?>a</adp> <ajp id=?1.2.2.1.2.2?
mph=?ptb;JJ;;global;?>global</ajp> <ajp id=?1.2.2.1.2.3?
mph=?ptb;JJ;;standard;?>standard</ajp> <n
id=?1.2.2.1.2.4? mph=?ptb;NN;;language;?>language</n>
</np> </v> <segp id=?1.2.2.2?
mph=?ptb;CC;;and;?>and</segp> <vp id=?1.2.2.3?>
<v id=?1.2.2.3.1? mph=?ptb;VBZ;;be;?>is</v>
<ajp id=?1.2.2.3.2?> <aj id=?1.2.2.3.2.1?
mph=?ptb;JJ;;indispensable;?>indispensable</aj>
<adp id=?1.2.2.3.2.2?> <ad id=?1.2.2.3.2.2.1?
mph=?ptb;IN;;for;?>for</ad> <np id=?1.2.2.3.2.2.2?
mph=?ptb;NNS;;grownup;?>grownups</np> </adp>
</ajp> </vp> </v>.</su>
