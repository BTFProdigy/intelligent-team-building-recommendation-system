Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 1?12,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multi-document multilingual summarization corpus preparation, Part 1:
Arabic, English, Greek, Chinese, Romanian
Lei Li
BUPT, China
leili@bupt.edu.cn
Corina Forascu
RACAI, Romania
UAIC, Romania
corinfor@info.uaic.ro
Mahmoud El-Haj
Lancaster Univ., UK
m.el-haj@lancaster.ac.uk
George Giannakopoulos
NCSR Demokritos, Greece
SciFY NPC, Greece
ggianna@iit.demokritos.gr
Abstract
This document overviews the strategy, ef-
fort and aftermath of the MultiLing 2013
multilingual summarization data collec-
tion. We describe how the Data Contrib-
utors of MultiLing collected and gener-
ated a multilingual multi-document sum-
marization corpus on 10 different lan-
guages: Arabic, Chinese, Czech, English,
French, Greek, Hebrew, Hindi, Romanian
and Spanish. We discuss the rationale be-
hind the main decisions of the collection,
the methodology used to generate the mul-
tilingual corpus, as well as challenges and
problems faced per language. This paper
overviews the work on Arabic, Chinese,
English, Greek, and Romanian languages.
A second part, covering the remaining lan-
guages, is available as a distinct paper in
the MultiLing 2013 proceedings.
1 Introduction
Summarization has recently received the focus
of media attention (Cahan, 2013; Shih, 2013), due
to a set of corporate buy-outs related to summariza-
tion technology companies. This trend of applying
summarization is the result of a long research effort
related to summarization. Previously, especially
within the Text Analysis Conference (TAC) series
of workshops (Dang, 2005; Dang, 2006; Dang and
Owczarzak, 2008), multi-document summariza-
tion has covered aspects of summarization such
as update summarization, guided summarization
and cross-lingual summarization. In TAC 2011
the MultiLing Pilot (Giannakopoulos et al, 2011)
was introduced: a combined community effort to
present and promote multi-document summariza-
tion apporaches that are (fully or partly) language-
neutral. To support this effort an organizing com-
mittee across more than six countries was assigned
to create a multi-lingual corpus on news texts, cov-
ering seven different languages: Arabic, Czech,
English, French, Greek, Hebrew, Hindi.
The Pilot gave birth to an active community of
researchers, who provided the effort and know-
how to realize a continuation of the original ef-
fort: MultiLing 2013. The MultiLing 2013 Work-
shop, taking place within ACL 2013, built upon
the existing corpus of MultiLing 2011 to provide
additional languages and challenges for summa-
rization systems. This year 3 new languages were
added: Chinese, Romanian and Spanish. Further-
more, more texts were added to most existing cor-
pus languages (with the exception of French and
Hindi).
In the following paragraphs we first overview
theMultiLing tasks, for which the corpus was built
(Section 2). We then describe the rationale and
strategy applied for the corpus collection and cre-
ation (Section 3). We continue with special com-
ments for the English, Greek, Chinese and Roma-
nian languages (Section 4). Finally, we summarize
the findings at the end of this paper (Section 5). We
note that a second paper (Elhadad et al, 2013) de-
scribes the language-specific notes related to the
rest of the MultiLing 2013 language contributions
(Czech, Hebrew, Spanish).
2 The MultiLing tasks
There are two main tasks (and a single-
document multilingual summarization pilot de-
scribed in a separate paper) in MultiLing 2013:
Summarization Task This MultiLing task aims
to evaluate the application of (partially or
fully) language-independent summarization
algorithms on a variety of languages. Each
system participating in the task was called
to provide summaries for a range of differ-
ent languages, based on corresponding cor-
pora. In the MultiLing Pilot of 2011 the lan-
1
guages used were 7, while this year systems
were called to summarize texts in 10 differ-
ent languages: Arabic, Chinese, Czech, En-
glish, French, Greek, Hebrew, Hindi, Roma-
nian, Spanish. Participating systems were re-
quired to apply their methods to a minimum
of two languages.
The task was aiming at the real problem of
summarizing news topics, parts of which may
be described or may happen in different mo-
ments in time. We consider, similarly to Mul-
tiLing 2011(Giannakopoulos et al, 2011) that
news topics can be seen as event sequences:
Definition 1 An event sequence is a set of
atomic (self-sufficient) event descriptions, se-
quenced in time, that share main actors, lo-
cation of occurence or some other important
factor. Event sequences may refer to topics
such as a natural disaster, a crime investiga-
tion, a set of negotiations focused on a single
political issue, a sports event.
The summarization task requires to generate
a single, fluent, representative summary from
a set of documents describing an event se-
quence. The language of the document set
will be within the given range of 10 languages
and all documents in a set share the same lan-
guage. The output summary should be of the
same language as its source documents. The
output summary should be between 240 and
250 words.
Evaluation Task This task aims to examine how
well automated systems can evaluate sum-
maries from different languages. This task
takes as input the summaries generated from
automatic systems and humans in the Sum-
marization Task. The output should be a grad-
ing of the summaries. Ideally, we would want
the automatic evaluation to maximally corre-
late to human judgement.
The first task was aiming at the real problem of
summarizing news topics, parts of which may be
described or happen in different moments in time.
The implications of including multiple aspects of
the same event, as well as time relations at a vary-
ing level (from consequtive days to years), are still
difficult to tackle in a summarization context. Fur-
thermore, the requirement for multilingual appli-
cability of the methods, further accentuates the dif-
ficulty of the task.
The second task, summarization evaluation has
come to be a prominent research problem, based on
the difficulty of the summary evaluation process.
While commonly used methods build upon a few
human summaries to be able to judge automatic
summaries (e.g., (Lin, 2004; Hovy et al, 2005)),
there also exist works on fully automatic evalua-
tion of summaries, without human?model? sum-
maries (Louis and Nenkova, 2012; Saggion et al,
2010). The Text Analysis Conference has a sepa-
rate track, named AESOP (Dang and Owczarzak,
2009) aiming to test and evaluate different auto-
matic evaluation methods of summarization sys-
tems.
Given the tasks, a corpus needed to be gener-
ated, that would be able to:
? provide input texts in different languages to
summarization systems.
? provide model summaries in different lan-
guages as gold standard summaries, to also
allow for automatic evaluation using model-
dependent methods.
? provide human grades to automatic and hu-
man summaries in different languages, to
support the testing of summary evaluation
systems.
In the following section we show how these re-
quirements were met in MultiLing 2013.
3 Corpus collection and generation
The overall process of creating the corpus of
MultiLing 2013 was, similarly to MultiLing 2011,
based on a community effort. The main processes
consisting of the generation of the corpus are as
follows:
? Selection of a source corpus in a single lan-
guage (see Section 3.1).
? Translation of the source corpus to different
languages (see Section 3.2).
? Human summarization of corpus topics per
language (see Section 3.3).
? Evaluation of human summaries, as well as of
submitted system runs (see Section 3.4).
2
We should note here that the translation is meant
to provide a parallel corpus of texts across differ-
ent languages. The main ideas behind this first ap-
proach are that:
? the corpus will allow performing secondary
studies, related to the human summarization
effort in different languages. Having a paral-
lel corpus is such cases can prove critical, in
that it provides a common working base.
? we may be able to study topic-related
or domain-related summarization difficulty
across languages.
? the parallel corpus highlights language-
specific problems (such as ambiguity in word
meaning, named entity representation across
languages).
? the parallel corpus fixes the setting in which
methods can show their cross-language ap-
plicability. Examining significantly varying
results in different languages over a parallel
corpus offers some background on how to im-
prove existing methods and may highlight the
need for language-specific resources.
On the other hand, the significant organizational
and implementaion effort required for the transla-
tion (please see per language notes in the corre-
sponding sections) may lead to a comparable (vs.
parallel) corpus in future MultiLing endeavours.
Given the tasks at hand, the Contributors first
performed the selection of the texts that would be
used for the MultiLing tracks, as described below.
3.1 Selecting the corpus
To support the summarization task, we needed
a dataset of freely available news texts (to allow
reuse), covering news topics that would contain
event sequences. Based on the ? apparently good
? decisions of the MultiLing 2011 Pilot, we de-
termined that each event sequence in the corpus
should contain at least three distinct atomic events,
to imply an underlying story.
The dataset created was based on the WikiNews
site1, which covers a variety of news topics,
while allowing the reuse of the texts based on the
Creative Commons Licence. An example topic
with two sample texts derived from the original
WikiNews documents is provided in Figure 1. It
1See http://www.wikinews.org.
can be seen clearly that the event in the example
has significantly different aspects, since an earth-
quake caused a radiation leak, via a series of inter-
actions in the real world. Systems would normally
be expected to express both aspects of the event
with adequate information.
During the selection of the source texts, we
first gathered an English corpus of 15 topics (10
of which were already available from MultiLing
2011), each containing 10 texts. Wemade sure that
each topic contained at least one event sequence.
From the original HTML text we only kept unfor-
matted content text, without any images, tables or
links.
While choosing topics we made sure that there
existed topics:
? with varying time granularity. Some top-
ics happen within days (e.g., sports events),
while others within years (e.g., Iranian nu-
clear policy and international negotiations).
? covering various domains. There existed top-
ics related to international politics, sports,
natural disasters, political campaigns and
elections.
? with a varying number of apparent actors.
Some topics focus on specific individuals
(e.g., campaign of Barack Obama) while oth-
ers refer to numerous participants (e.g., para-
Olympics and participating athletes).
? with numeric aspects, that would change over
time. Such examples are natural disasters
(with the number of estimated victims, or
the estimated magnitude of earthquakes) and
sports events (number of medals per country).
? with an important time dimension. For ex-
ample during the Egyptian riots, the order of
events is non-trivial to determine from text.
Determining the order of events is also very
challenging while following multi-day sports
events. Ignoring the time dimension in such
topics is expected to worsen the performance
of summarization systems.
Given the English texts, we now needed to pro-
vide corresponding texts in all the languages used
in MultiLing. To this end, we organized a transla-
tion process, which is elaborated below.
3
Fukushima reactor suffers multiple fires, radiation leak confirmed
Tuesday, March 15, 2011
Fires broke out at the Fukushima Daiichi plant's No. 4 reactor in Japan on
Tuesday, according to the Tokyo Electric Power Company. The first fire caused
a leak of concentrated radioactive material, according to the Japanese prime
minister, Naoto Kan.
The first fire broke out at 9:40 a.m. local time on Tuesday, and was thought
to have been put out, but another fire was discovered early on Wednesday,
believed to have started because the earlier one had not been fully
extinguished.
In a televised statement, the prime minister told residents near the plant
that "I sincerely ask all citizens within the 20 km distance from the reactor
to leave this zone." He went on to say that "[t]he radiation level has risen
substantially. The risk that radiation will leak from now on has risen."
Kan warned residents to remain indoors and to shut windows and doors to avoid
radiation poisoning.
The French Embassy in Japan reports that the radiation will reach Tokyo in 10
hours, with current wind speeds.
Death toll rises from Japan quake
Sunday, March 13, 2011
The death toll from the earthquake and subsequent tsunami that hit Japan on
Friday has risen to more than a thousand, with many people still missing,
according to reports issued over the weekend.
While Japan's police says that only 637 are confirmed dead, media reports say
that over a thousand people have been killed, with several hundred bodies
still being transported. Thousands more are still unaccounted for; in the town
of Minamisanriku, Miyagi Prefecture alone, up to 10,000 people are missing.
Four trains that were on the coast have yet to be located.
In the aftermath of the disaster, evacuations of around 300,000 people have
taken place; more evacuations are likely in the wake of concerns over a
damaged nuclear power plant. According to Prime Minister Naoto Kan, around
3,000 people have been rescued thus far. 50,000 troops from the Japanese
military have been deployed to assist in rescue efforts.
The tsunami generated by the quake has destroyed communities along Japan's
Pacific coast, with up to 90% of the houses in some towns having been
destroyed; at least 3,400 structures have been destroyed in total. Fires have
also sprung up among the impacted areas.
Figure 1: Topic Sample (Japan Earthquake and Nuclear Threat)
4
3.2 Translating the corpus
The English texts selected in the selection step
were translated using a sentence-by-sentence ap-
proach to each of the other languages: Arabic, Chi-
nese, Czech, French, Greek, Hebrew, Hindi, Ro-
manian, Spanish. This year there was no support
for the Hindi and French languages, which still
contain 10 topics. Also the Chinese language cov-
ers 10 topics. All the remaining languages cover
15 topics.
During the translation process, the guidelines
were minimal:
Given the source language text A,
the translator is requested to translate
each sentence in A, into the target lan-
guage. Each target sentence should keep
the meaning from the source language.
Some additional, optional guidelines (provided
in the Appendix) were provided by the Romanian
language Contributors, proposing ways to react to
date formatting, name translations, etc.
During the translation process, the translators
were also asked to keep track of the time spent on
different stages of the process: first full reading of
the source document, translation and verification.
The whole set of translated documents together
with the original English document set will be re-
ferred to as the Source Document Set. Given the
creation process, the Source Document Set con-
tains a total of 1350 texts (vs. 700 from MultiLing
2011): 7 languages with 15 topics per language, 10
texts per topic for a total of 1050 texts; 3 languages
with 10 topics per language, 10 texts per topic for
a total of 300 texts.
This Source Document Set was provided to par-
ticipating systems as input for their summarization
systems. It was also provided to human summa-
rizers, so that they would provide human, model
summaries on each topic and each language. The
human summarization process is described in the
following section.
3.3 Summarizing topics
In the summarization step of the corpus creation
different summarizers were asked to generate one
summary per topic in each language. The follow-
ing guidelines were provided to help the summa-
rizers:
The summarizer will read the whole
set of texts at least once. Then, the sum-
marizer should compose a summary,
with a minimum size of 240 and a maxi-
mum size of 250 words. The summary
should be in the same language as the
texts in the set. The aim is to create a
summary that covers all the major points
of the document set (what is major is
left to summarizer discretion). The sum-
mary should be written using fluent, eas-
ily readable language. No formatting or
other markup should be included in the
text. The output summary should be a
self-sufficient, clearly written text, pro-
viding no other information than what is
included in the source documents.
After summarization, human evaluation was
performed. The evaluation covered human sum-
maries, but also summarization system submis-
sions. The details are provided in the following
paragraphs.
3.4 Evaluating the summaries
The evaluation of summaries was performed
both automatically and manually. The manual
evaluation was based on the Overall Responsive-
ness (Dang and Owczarzak, 2008) of a text, as de-
scribed below, and the automatic evaluation used
the ROUGE (Lin, 2004) and AutoSummENG-
MeMoG (Giannakopoulos et al, 2008; Gian-
nakopoulos and Karkaletsis, 2011) and NPowER
(Giannakopoulos and Karkaletsis, 2013) methods
to provide a grading of performance.
For the manual evaluation the human evaluators
were provided the following guidelines:
Each summary is to be assigned an
integer grade from 1 to 5, related to the
overall responsiveness of the summary.
We consider a text to be worth a 5, if
it appears to cover all the important as-
pects of the corresponding document set
using fluent, readable language. A text
should be assigned a 1, if it is either un-
readable, nonsensical, or contains only
trivial information from the document
set. We consider the content and the
quality of the language to be equally im-
portant in the grading.
As indicated in the task, the acceptable limits for
the word count of a summary were between 240
5
and 250 words2 (inclusive). In the case of Chi-
nese there was a problem determining the number
of words. Based on the model summaries gathered
we (arbitrarily) set the upper limit of length in bytes
of the UTF8-encoded summary files to 750 bytes.
4 Language specific notes
In the following paragraphs we provide
language-specific overviews related to the corpus
contribution effort. The aim of these overviews is
to provide a reusable pool of knowledge for future
similar efforts.
In this document we elaborate on Arabic, En-
glish, Greek, Chinese and Romanian languages. A
second document (Elhadad et al, 2013) elaborates
on the rest of the languages.
4.1 Arabic language
The preparation of the Arabic corpus for the
2013 MultiLing Summarization tasks was organ-
ised jointly by Lancaster University and the Uni-
versity of Essex in the United Kingdom. 20 people
participated in translating the English corpus into
Arabic, validating the translation and summarising
the set of related Arabic articles. The participants
are studying, or have finished a university degree
in an Arabic speaking country. The participants?
age ranged between 21 and 32 years old.
The participants translated the English dataset
into Arabic. For each translated article another
translator validated the translation and fixed any
errors. For each of the translated articles, three
manual summaries were created by three different
participants (human peers). Amid the summarisa-
tion process the participants evaluated the quality
of the generated summary by assigning a score be-
tween one (unreadable summary) and five (fluent
and readable summary). No self evaluation was
allowed.
The average time for reading the English news
articles by the Arabic native speaker participants
was 5.58minutes. The average time it took them to
translate these articles into Arabic was 42.18 min-
utes and to validate each of the translated Arabic
articles the participants took 5.25 minutes on aver-
age.
For the summarisation task the average time for
reading the set of related articles (10 articles per
2The count of words was provided by thewc -w linux com-
mand.
each set) was 34.44 minutes. The average time for
summarising each set was 25.41 minutes.
4.1.1 Problems and Challenges
Many difficulties arose during the creation of
the gold-standard summaries. Some are language-
dependent and relate to the complexity of the Ara-
bic language. This required a special attention to
be paid while creating the summaries.
One problem concerns the handling of month
names in Arabic. There are twoways of translating
month names into Arabic:
? using the Arabic transliteration of the
Aramic (Syriac) month names (e.g. ?May?,
?PA


K




@?, ?Ayyar?).
? using the Arabic transliteration of the
English month names (e.g. ?May?,
??K


A

??, ?Mayo?).
Some of the participants found it difficult to
translate sentences where they believe they contain
an ambiguous structure. For example: ?She said
Iranian security Chief Saeed Jalili had requested a
meeting in a telephone call?. The translators (who
are Native Arabic speakers) found it a bit hard to
choose between two translations:
? ?Saeed Jalili asked to schedule a telephone
meeting?
? ?Saeed Jalili phoned to request a meeting?.
Arabic sentence structure is highly complex and
therefore great attention must be paid when mov-
ing forward or pushing back phrases within a sen-
tence, as such shifts are likely to change the over-
all meaning. In addition, the use of passive voice,
metaphors and idioms in the original English text
has captured the translators attention, as the mean-
ing in such cases takes precedence over the literal
translation.
During the summarisation process, a sum-
mariser found that ordering a set of related articles
(discussing the same topic) in chronological order
simplifies the summarisation process.
Many participants found it difficult to meet the
250 summary word-limit as they believe 250 is not
enough to cover all the essential information de-
rived from a given set of documents.
Another problem concerns ?proper nouns? when
translating into Arabic. The Arabic electronic dis-
course would sometimes show two variants of one
6
English proper noun, as in the case with the name
?Francois Hollande?. Mostly in such cases, the
variant used in popular websites such as the Arabic
version Wikipedia was adopted.
Finally, there were many questions by the par-
ticipants on whether to create abstractive or extrac-
tive summaries.
4.2 Chinese language
Below we provide an overview of the organiza-
tional effort and comments on a variety of prob-
lems related to the preparation of the Chinese cor-
pus for MultiLing 2013.
4.2.1 Organization
First, the Chinese language team translated two
texts from English to Chinese together in order to
make an original unified example for each trans-
lator, including file format, title format, date for-
mat, named entity translation, etc. Second, we as-
signed different set of news texts as specific task
for each translator. For each news topic, we usu-
ally split the ten texts to two different translators at
least, so as to bring more thoughts from different
viewers and prepare enough for later discussion.
During the process of each translator, they were
asked to note any problems in a ?problem file?, in-
cluding the source English part and the target Chi-
nese part. Third, we summed up a big problem
file from each translator. After a series of discus-
sions, we classified the problems into different cat-
egories and solved some of the problems success-
fully. The remaining problems were noted down in
a detailed report to the organizer of the MultiLing
2013Workshop of ACL 2013, as a knowledge pool
for future efforts. Fourth, we performed the verifi-
cation task. During the process, we made sure that
for each text, the verifier was different from the
translator. Also each verifier was demanded to log
any problems. Fifth, we did another discussion for
new problems coming from the verification phase.
Some problems were solved; others were added to
the detailed report. Sixth, we generated the needed
result files and made sure that they were in the re-
quested format (e.g., UTF8, no-BOM, plain text
files for summaries).
For the process of summarization and human
evaluation, first, we assigned three summarizers,
each of which needed to read all the ten topics and
write a summary for each topic. Second, we as-
signed three evaluators, making sure that for each
summary, the evaluator was different from the
summarizer. Third, we made a discussion about
the process of summarization and evaluation. All
agreed that summarization and evaluation were
much easier than translation.
There were mainly two common problems. One
was about the summary length. So we set a uni-
fied method for length checking. The other prob-
lem was more complex, which was that there
were many different information in the original ten
texts, but the result summary was limited to 250
words, so it was very difficult to choose the most
important information. As a result, some infor-
mation could be lost in final summaries. At the
same time, we also found minor problems regard-
ing the translation, improved the translation files
and updated the detailed report about the problems
we faced.
4.2.2 Problems and proposed solutions
In fact, related problems mainly came up from
the task of translation. Most of them were com-
mon questions of the translators and language-
dependent problems that needed special care. Here
we only list the main categories of problems 3.
First, there were problems with the translation of
person names. There are several sub-problems
here:
? There are some person names which are not
so popular, we could not find a result, so
we finally keep the unknown English words
among Chinese words.
? There is no specific separator between first
name, middle name and family name in En-
glish, only normal space. But in Chinese, we
usually add a separator ?? ? between them.
?
? There is also some ambiguity in person name
to us, since we may be not quite familiar with
some specific knowledge of news related do-
main. ?
? There are also some person names which
seem to contain non-English characters.
These names are more difficult for us, so we
just keep most of them as the original format
in English news. ?
? There are some person names with only one
capitalized character and a dot in the middle
3A more detailed report has been submitted to the orga-
nizer of the Workshop.
7
part. It?s really difficult for us to find a cor-
responding Chinese translation for it, so we
just keep it as the original English format in
the Chinese translations and keep the original
English name in the following brackets.
Second, the translation for the English name of
some websites, companies, organizations, etc, can
cause problems. Since the full name may be too
long for news reports, most of them also have oc-
curred in corresponding simple format of abbre-
viation. Some of them are famous enough that
we have a popular Chinese translation for them,
while others are not so popular. So we decided
that for unknown ones, we just reserve the English
name, but for those known ones, we add the Chi-
nese translation and keep some of the English ab-
breviation.
Third, the translation of time expressions is non-
trivial. In English, the order usually used is: Week-
day, Month Day, Year. But according to Chinese
habit, we mention time usually in the following or-
der: Year Month Day, Weekday.
Fourth, translation of locations names may not
exist. There are many location names in these
news texts. We tried to find their Chinese transla-
tion from many resources, but there are still some
difficult ones left.
Fifth, there are someEnglishwords in the source
texts which seem to be unrelated to other sentences
in the news text (these may be text captions of pho-
tos in the source WikiNews articles). We just left
them as they were.
Sixth, there are some sentences which are diffi-
cult to understand clearly because the context and
structure are ambiguous. In these cases, we made
a Chinese translation which seems best to us.
The above problems conclude the Chinese lan-
guage contribution language-specific notes.
4.3 English and Greek languages
The effort related to the organization of the En-
glish and Greek languages was essentially equiva-
lent to the MultiLing 2011 pilot (Giannakopoulos
et al, 2011). This year 5 new topics were added
to the two languages. The effort for English was
reduced because no translation was needed. In the
following subsections we elaborate on the organi-
zation details and the problems faced during the
different subprocesses of the corpus creation.
4.3.1 Organization
A total of 7 people (being either MSc students,
or researchers, all with fluency in English and
Greek) were recruited for the two languages. An
initial meeting was held to provide the basic guide-
lines and discuss questions on the translation pro-
cess. Subsequently, e-mail communication and
periodic conferences were used to assign the next
tasks, related to summarization and evaluation.
For the purposes of meaningful assignment we
created and used an automatic assignment script,
that allows pre-allocating specific texts to workers
(for any of the required tasks), while it automati-
cally distributes work according to the availability
of workers. The script avoids assigning workers to
texts/tasks more than once.
In the evaluation process, we made sure
(through pre-assignments) that no human would
judge their own summary. It would have increased
efficiency, if we had ascertained that human sum-
marization would occur right after the translation
of the texts.
The average time for reading the English news
articles by the Greek native speaker participants
was around 8 minutes. The average time it took
them to translate these articles into Greek was
around 48 minutes on average (with a couple of
extreme cases exceeding 100 minutes, due to tech-
nical terminology, which was difficult to trans-
late). The summarization time of the new topics
in English was around 24 minutes per topic (plus
an average of 8 minutes allocated to reading the
source texts). For Greek the summary time was
around 50 minutes per topic (we note that the sum-
marizers? groups for English and Greek were only
minimally overlapping). In the Greek case, some
deeper search showed that a single summarizer
heavily biased the distribution of times to higher
values.
To follow the progress of tasks, a generic project
management tool was used. However, the tool
proved insufficient in the micro-planning of the ef-
fort (individual assignments tracking). It would
clearly make sense to use an ad-hoc designed sys-
tem for planning and implementation of the effort.
4.3.2 Problems and proposed solutions
The main problems identified by contributors
for Greek and English translation were related to
well-known translation problems: named entity
translation, date formatting, highly technical or
domain specific terminology, ambiguous terms in
8
the source text. Additional effort from translators
provided solutions to these problems according to
common practice in the translation domain.
The summarization effort indicated a few inter-
esting points. Even though summarizers have their
individual method for summarizing, some com-
mon practices and notes arise:
? A non-thorough glimpse of the source texts
helps determine the overall topic.
? Time ordering is important in several cases,
thus time ordering of the source texts is ap-
plied before the summarization process itself.
The process is non-trivial even for humans.
? An initial summary which may be longer than
the target size is created and several reductive
transformations are applied. The 250 word
limit proved critical and challenging, in that it
forced summarizers to carefully choose infor-
mation, essentially not covering the whole set
of information from the source documents.
? Syntactic compression and rewriting is the
last line of summarization, when it is obvious
that more compression is needed.
As related to the evaluation process, we noted
that there exists an inherent tendency for evalua-
tors to determine whether a human or a machine
performed the summarization. There were cases
where evaluators altered their grading, because
they inferred that not all texts were from humans
or not all were from machines. We had noted this
phenomenon also in MultiLing 2011. There are
several cases where the evaluator also tries to de-
termine the strategy of the system and, when one
understands the underlying strategy, this may bias
the grade. It would be interesting to evaluate this
bias in the future.
Some additional notes are related to problems
with the organization of the effort:
? A distributed work environment that would
help track the progress of individuals and
assignment of new tasks without significant
communication effort, would have been very
helpful.
? The assignment script was really critical in
facilitating the organization of the effort and
we plan to make it publicly available to allow
reuse.
Overall, the collection and generation of the cor-
pus was a very challenging effort, both in terms
of organization and individual questions arising.
However, next steps can build upon the lessons
learnt, if the effort is well documented and the doc-
uments are freely and openly shared.
4.4 Romanian language
AtMultiLing 2013, Romanian was addressed as
a language for the first time. Following the Call
for Contributors launched by the MultiLing orga-
nizers and based on the experience in the QA @
CLEF4 evaluation campaign (Pe?as et al, 2012),
we started the data collection process workingwith
a group of ten MSc students in Computational
Linguistics from our Faculty, later adding another
MSc student to the working group. Below we pro-
vide some notes on the translation and generation
of human summaries processes:
? The translation, including verification, of
the 150 WikiNews text documents from En-
glish into Romanian, was performed in a dis-
tributed context, theoretically based on an ar-
chitecture like the one described in (Alboaie
et al, 2003). Each student received one topic
(10 documents) to be translated, based on a
set of guidelines. We devised guidelines to
tackle any language-dependent problems that
need special care, and they were improved af-
ter each solution received from the students
and based also on their questions. The full
guidelines are provided in the Appendix of
this document.
We started with the following workflow: stu-
dent A receives 10 English documents to be
translated and summarized and sends the re-
sults to the organizer; another student, B, re-
ceives the English documents and the Roma-
nian translations (made by student A) and s/he
verifies the translations and prepares another
summary. Finally, another student, C, re-
ceives from the organizer the 10 Romanian
documents and s/he prepares the third sum-
mary of a given topic.
Since the task proved to be very time-
consuming for the students, all the last five
topics (the ones introduced this year) were
given to one student and then the translations
were verified by the organizer.
4See http://celct.fbk.eu/ResPubliQA/index.
php for more information.
9
? The generation of human summaries was per-
formed immediately after the translation. For
each topic, the aim was to create a summary
that covers all the major points of the topic
(what is major was left to summarizer?s dis-
cretion), being a self-sufficient, clearly writ-
ten text, providing no other information than
what is included in the source documents.
The students were given no specific recom-
mendations regarding the type of summary
they should produce, e.g. an abstract ver-
sus an extract (Mani and Maybury, 1999),
but they were specifically instructed to under-
stand the main aspects of summarization.
5 Conclusions and lessons learnt
The corpus generated throughout the MultiLing
corpus preparation provides a benchmark dataset
for multilingual summarization. It tries to cap-
tured interesting, representative events, covering
a variety of well-known news events around the
world. The recent corporate interest in summa-
rization, in conjunction with the ever-present in-
crease of information flow from the Web and in-
formation redundancy, show that having a scien-
tifically plausible set of evaluation tools for sys-
tems can help bring useful summarization systems
to a wide audience. MultiLing functions as a fo-
cus point for multilingual summarization research
and this document described the methods used to
create a commonly accepted multilingual, multi-
document summarization corpus.
Concerning thoughts on the future work of Mul-
tiLing, there are some points that have been raised
by Contributors that we reproduce in the following
sentences:
? In the translation phase, it would be useful to
have translators for different languages dis-
cuss directly about some difficult cases, such
as some ambiguous words, phrases and sen-
tences, especially when they are expressed in
some language-specific way.
? It would be very interesting to exploit the po-
tential of comparable corpora, and not only
of the parallel ones, especially if we consider
the multilingual setting of MultiLing 2013.
This means that the data should be collected
starting from a given topic and each language
contributor should find 10 documents on that
given topic in his/her language.
? Creating a collaborative platform for build-
ing and improving summarization corpora
could significantly facilitate the corpus build-
ing process for future efforts.
We remind the reader that a second paper (El-
hadad et al, 2013) addresses the problems and
challenges faced in the remaining languages ac-
tively contributed to in MultiLing 2013 (Czech,
Hebrew and Spanish), thus completing the lessons
learnt from theMultiLing 2013 contribution effort.
Extended technical reports recapitulating discus-
sions and findings from the MultiLing Workshop
will be available after the workshop at the Multi-
Ling Community website5, as an addenum to the
proceedings.
Acknowledgments
MultiLing is a community effort and this com-
munity is what keeps it alive and interesting. We
would like to thank Contributors for their organi-
zational effort, which made MultiLing possible in
so many languages and all volunteers, helpers and
researchers that helped realize individual steps of
the process. A more detailed reference of the con-
tributor teams can be found in Appendix A.
The MultiLing 2013 organization has been par-
tially supported by the NOMAD FP7 EU Project
(cf. http://www.nomad-project.eu).
References
[Alboaie et al2003] Lenuta Alboaie, Sabin C Buraga,
and S?nica Alboaie. 2003. tuBiG?a layered infras-
tructure to provide support for grid functionalities.
Omega, 2:3.
[Cahan2013] Adam Cahan. 2013. Yahoo! To Acquire
Summly http://yodel.yahoo.com/blogs/
general/yahoo-acquire-summly-13171.
html, March 25th.
[Dang and Owczarzak2008] H. T. Dang and
K. Owczarzak. 2008. Overview of the TAC
2008 update summarization task. In TAC 2008
Workshop - Notebook papers and results, pages
10?23, Maryland MD, USA, November.
[Dang and Owczarzak2009] Hoa Trang Dang and
K. Owczarzak. 2009. Overview of the tac 2009
summarization track, Nov.
[Dang2005] H. T. Dang. 2005. Overview of DUC
2005. In Proceedings of the Document Under-
standing Conf. Wksp. 2005 (DUC 2005) at the
5See http://multiling.iit.demokritos.gr/
pages/view/1256/proceedings-addenum)
10
Human Language Technology Conf./Conf. on Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP 2005).
[Dang2006] H. T. Dang. 2006. Overview of DUC
2006. In Proceedings of HLT-NAACL 2006.
[Elhadad et al2013] Michael Elhadad, Sabino
Miranda-Jim?nez, Josef Steinberger, and George
Giannakopoulos. 2013. Multi-document multi-
lingual summarization corpus preparation, part 2:
Czech, hebrew and spanish. In MultiLing 2013
Workshop in ACL 2013, Sofia, Bulgaria, August.
[Giannakopoulos and Karkaletsis2011] George Gi-
annakopoulos and Vangelis Karkaletsis. 2011.
Autosummeng and memog in evaluating guided
summaries. In TAC 2011 Workshop, Maryland MD,
USA, November.
[Giannakopoulos and Karkaletsis2013] George Gi-
annakopoulos and Vangelis Karkaletsis. 2013.
Summary evaluation: Together we stand npower-ed.
In Computational Linguistics and Intelligent Text
Processing, pages 436?450. Springer.
[Giannakopoulos et al2008] George Giannakopoulos,
Vangelis Karkaletsis, George Vouros, and Panagio-
tis Stamatopoulos. 2008. Summarization system
evaluation revisited: N-gram graphs. ACM Trans.
Speech Lang. Process., 5(3):1?39.
[Giannakopoulos et al2011] G. Giannakopoulos,
M. El-Haj, B. Favre, M. Litvak, J. Steinberger,
and V. Varma. 2011. TAC 2011 MultiLing pilot
overview. In TAC 2011 Workshop, Maryland MD,
USA, November.
[Hovy et al2005] E. Hovy, C. Y. Lin, L. Zhou, and
J. Fukumoto. 2005. Basic elements.
[Lin2004] C. Y. Lin. 2004. Rouge: A package for
automatic evaluation of summaries. Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004), pages 25?26.
[Louis and Nenkova2012] Annie Louis and Ani
Nenkova. 2012. Automatically assessing ma-
chine summary content without a gold standard.
Computational Linguistics, 39(2):267?300, Aug.
[Mani and Maybury1999] Inderjeet Mani and Mark T
Maybury. 1999. Advances in automatic text sum-
marization. the MIT Press.
[Pe?as et al2012] Anselmo Pe?as, Eduard H. Hovy,
Pamela Forner, ?lvaro Rodrigo, Richard F. E. Sut-
cliffe, Caroline Sporleder, Corina Forascu, Yassine
Benajiba, and Petya Osenova. 2012. Overview of
qa4mre at clef 2012: Question answering for ma-
chine reading evaluation. In CLEF (Online Working
Notes/Labs/Workshop).
[Saggion et al2010] H. Saggion, J. M. Torres-Moreno,
I. Cunha, and E. SanJuan. 2010. Multilingual sum-
marization evaluation without human models. In
Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, page 1059?
1067.
[Shih2013] Gerry Shih. 2013. Sound Famil-
iar? After Yahoo Buys Summly, Google
Buys News Summarization App Wavii
http://www.huffingtonpost.com/2013/
04/24/google-wavii_n_3143116.html, April
23rd.
[Tufis et al2004] Dan Tufis, DanCristea, and Sofia Sta-
mou. 2004. Balkanet: Aims, methods, results and
perspectives. a general overview. Romanian Journal
of Information science and technology, 7(1-2):9?43.
Appendix A: Contributor teams
Arabic language team
Team members Mahmoud El-Haj (Lancaster
University, UK); Ans Alghamdi, Maha
Althobaiti (Essex University, UK); Ahmad
Alharthi (King Saud University, Saudi
Arabia)
Contact e-mail m.el-haj@lancaster.ac.uk
Chinese language team
Team members Lei Li, Wei Heng, Jia Yu, Yu Liu,
Qian Li
Team affiliation Center for Intelligence Science
and Technology (CIST), School of Com-
puter Science,Beijing University of Posts and
Telecommunications,
Postal Address P.O.Box 310, Beijing University
of Posts and Telecommunications, Xitucheng
Road 10, Haidian District, Beijing, China
Contact e-mail leili@bupt.edu.cn
English and Greek languages team
Team members Zoe Angelou, Argyro
Mavridakis, Valentini Mellas, Efrosini
Zacharopoulou, George Kiomourtzis,
George Petasis, George Giannakopoulos
Team affiliation NCSR?Demokritos?
Postal Address Institute of Informatics and
Telecommunications, Patriarchou Grigoriou
and Neapoleos Str., Aghia Paraskevi Attikis,
Athens, Greece
Contact e-mail ggianna@iit.demokritos.gr
11
Romanian language team
Team members Corina Forascu, Raluca Moi-
seanu; Ana Maria Timofciuc, Alexandra
Cristea, Alexandrina Sbiera, Bogdan Puiu,
and Tudor Popoiu; other contributors to the
task were Monica Ancu?a, Romic? Iarca,
Claudiu Popa, and Cosmin Vl?du?u
Team affiliation UAIC, Romania
Contact e-mail corinfor@info.uaic.ro
Appendix B: Romanian guidelines
1. Translation equivalents belonging to the same
part of speech should be used. The Romanian
words should be as?closest?as possible to
their English equivalents: If the English word
has as equivalent a cognate in Romanian, this
one should be used. The Romanian wordnet6
(Tufis et al, 2004) should be used for prob-
lematic situations. If the English word doesn?
t have a Romanian cognate, then the transla-
tor should not try to paraphrase it. Example:
The English ?sporadic?will be translated
into?sporadic?, even though the translator
would be tempted to use instead?izolat?or
?rar?. It is not recommended to give trans-
lations such as ?mai pu?in?or ?mai rar?
.
2. English words should not be omitted and
words which are not in the original English
text should not be added because of stylistic
reasons. Example:?The Telegraph?will be
not translated when it refers to the newspa-
per and, moreover, the translators will not in-
troduce an explanation, like?cotidianul The
Telegraph?[English: The Telegraph newspa-
per].
3. The Romanian diacritics have to be used, in
UTF-8 encoding.
4. The translators must preserve as much as pos-
sible the tenses of the English verbs. Any dis-
agreement from the English tense is allowed
for linguistic reasons only (Romanian spe-
cific constructions), and not for stylistic ones.
5. The translators will preserve the format of
dates, times, numbers. For example, for the
issuing date of an article being ?March 25,
6See http://www.racai.ro/wnbrowser/.
2010?, the Romanian translation will be?25
martie 2010?and NOT ?Martie, 25, 2010?
OR?25 Martie, 2010?.
6. The format of the numbers should follow the
Romanian convention with respect to the dec-
imal separator, which is comma (,), and not
the period (.), like in English-speaking coun-
tries.
7. The unclear or unsure situations encountered
by the translators will be separately recorded
in a file, indicating the provenance of the doc-
ument, the ID used for the problematic sen-
tence and the commentaries/suggestions.
12
