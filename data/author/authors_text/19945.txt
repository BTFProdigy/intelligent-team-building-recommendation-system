Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2249?2259, Dublin, Ireland, August 23-29 2014.
Learning to Distinguish Hypernyms and Co-Hyponyms
Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller
Department of Informatics,
University of Sussex,
Brighton, UK
juliewe,D.Clarke,J.P.Reffin,davidw,billk@sussex.ac.uk
Abstract
This work is concerned with distinguishing different semantic relations which exist between
distributionally similar words. We compare a novel approach based on training a linear Support
Vector Machine on pairs of feature vectors with state-of-the-art methods based on distributional
similarity. We show that the new supervised approach does better even when there is minimal
information about the target words in the training data, giving a 15% reduction in error rate over
unsupervised approaches.
1 Introduction
Over recent years there has been much interest in the field of distributional semantics, drawing on the
distributional hypothesis: words that occur in similar contexts tend to have similar meanings (Harris,
1954). There is a large body of work on the use of different similarity measures (Lee, 1999; Weeds and
Weir, 2003; Curran, 2004) and many researchers have built thesauri (i.e. lists of ?nearest neighbours?)
automatically and applied them in a variety of applications, generally with a good deal of success.
In early research there was much interest in how these automatically generated thesauri compare with
human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000).
More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Dis-
tributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala
et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), pre-
dicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh,
2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), tax-
onomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013).
A primary focus of distributional semantics has been on identifying words which are similar to each
other. However, semantic similarity encompasses a variety of different lexico-semantic and topical re-
lations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix
of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related
words. A central problem here is that whilst most measures of distributional similarity are symmetric,
some of the important semantic relations are not. The hyponymy relation (and converse hypernymy)
which forms the ISA backbone of taxonomies and ontologies such as WordNet (Fellbaum, 1989), and
determines lexical entailment (Geffet and Dagan, 2005), is asymmetric. On the other hand, the co-
hyponymy relation which relates two words unrelated by hyponymy but sharing a (close) hypernym, is
symmetric, as are synonymy and antonymy. Table 1 shows the distributionally nearest neighbours of the
words cat, animal and dog. In the list for cat we can see 2 hypernyms and 13 co-hyponyms
1
.
1
We read cat in the sense domestic cat rather than big cat, hence tiger is a co-hyponym rather than hyponym
of cat.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2249
cat dog 0.32, animal 0.29, rabbit 0.27, bird 0.26, bear 0.26, monkey 0.26, mouse 0.25, pig 0.25,
snake 0.24, horse 0.24, rat 0.24, elephant 0.23, tiger 0.23, deer 0.23, creature 0.23
animal bird 0.36, fish 0.34, creature 0.33, dog 0.31, horse 0.30, insect 0.30, species 0.29, cat 0.29,
human 0.28, mammal, 0.28, cattle 0.27, snake 0.27, pig 0.26, rabbit 0.26, elephant 0.25
dog cat 0.32, animal 0.31, horse 0.29, bird 0.26, rabbit 0.26, pig 0.25, bear 0.26, man 0.25, fish
0.24, boy 0.24, creature 0.24, monkey 0.24, snake 0.24, mouse 0.24, rat 0.23
Table 1: Top 15 neighbours of cat, animal and dog generated using Lin?s similarity measure (Lin,
1998) considering all words and dependency features occurring 100 or more times in Wikipedia.
Distributional similarity is being deployed (e.g., Dinu and Thater (2012)) in situations where it can
be useful to be able to distinguish between these different relationships. Consider the following two
sentences.
The cat ran across the road. (1)
The animal ran across the road. (2)
Sentence 1 textually entails sentence 2, but sentence 2 does not textually entail sentence 1. The ability
to determine whether entailment holds between the sentences, and in which direction, depends on the
ability to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we
know which is the hyponym and which is the hypernym?
In applying distributional semantics to the problem of textual entailment, there is a need to generalise
lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations
is crucial if approaches to the composition of distributional representations of meaning that are currently
receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli,
2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual
entailment problem.
We formulate the challenge as follows: Consider a set of pairs of similar words ?A,B? where one of
three relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are
related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section
2, we discuss existing attempts to address this problem through the use of various directional measures
of distributional similarity.
This paper considers the effectiveness of various supervised approaches, and makes the following
contributions. First, we show that a SVM can distinguish the entailment and co-hyponymy relations,
achieving a significant reduction in error rate in comparison to existing state-of-the-art methods based
on the notion of distributional generality. Second, by comparing two different data sets, one built from
BLESS (Baroni and Lenci, 2011) and the other from WordNet (Fellbaum, 1989), we derive important
insights into the requirements of a valid evaluation of supervised approaches, and provide a data set
for further research in this area. Third, we show that when learning how to determine an ontological
relationship between a pair of similar words by means of the word?s distributional vectors, quite different
vector operations are useful when identifying different ontological relationships. In particular, using the
difference between the vectors for pairs of words is appropriate for the entailment task, whereas adding
the vectors works well for the co-hyponym task.
2 Related Work
Lee (1999) noted that the substitutability of one word for another was asymmetric and proposed the
alpha-skew divergence measure, an asymmetric version of the Kullback-Leibler divergence measure. She
found that this measure improved results in language modelling, when a word?s distribution is smoothed
using the distributions of its nearest neighbours.
Weeds et al. (2004) proposed a notion of distributional generality, observing that more general words
tend to occur in a larger variety of contexts than more specific words. For example, we would expect to be
able to replace any occurrence of cat with animal and so all of the contexts of cat must be plausible
2250
contexts for animal. However, not all of the contexts of animal would be plausible for cat, e.g.,
?the monstrous animal barked at the intruder?. Weeds et al. (2004) attempt to capture this asymmetry
by framing word similarity in terms of co-occurrence retrieval (Weeds and Weir, 2003), where precision
and recall are defined as:
P
ww
(u, v) =
?
f?F (u)?F (v)
I(u, f)
?
f?F (u)
I(u, f)
and R
ww
(u, v) =
?
f?F (u)?F (v)
I(v, f)
?
f?F (v)
I(v, f)
where I(n, f) is the pointwise mutual information (PMI) between noun n and feature f and F(n) is the
set of all features f for which I(n, f) > 0.
By comparing the precision and recall of one word?s retrieval of another word?s contexts, they were
able to successfully identify the direction of an entailment relation in 71% of pairs drawn from WordNet.
However, this was not significantly better than a baseline which proposed that the most frequent word
was the most general.
Clarke (2009) formalised the idea of distributional generality using a partially ordered vector space.
He also argued for using a variation of co-occurrence retrieval where precision and recall are defined as:
P
cl
(u, v) =
?
f?F (u)?F (v)
min(I(u, f), I(v, f))
?
f?F (u)
I(u, f)
and R
cl
(u, v) =
?
f?F (u)?F (v)
min(I(u, f), I(v, f))
?
f?F (v)
I(v, f)
Lenci and Benotto (2012) took the notion further and hypothesised that more general terms should
have high recall and low precision, which would thus make it possible to distinguish them from other
related terms such as synonyms and co-hyponyms. They proposed a variant of the Clarke (2009) measure
to identify hypernyms:
invCL(u, v) =
2
?
P
cl
(u, v) ? (1?R
cl
(u, v))
Evaluation on the BLESS data set (Baroni and Lenci, 2011), showed that this measure is better at distin-
guishing hypernyms from other relations than the measures of Weeds et al. (2004) and Clarke (2009).
Geffet and Dagan (2005) proposed an approach based on feature inclusion, which extends the rationale
of Weeds et al. (2004) to lexical entailment. Using data from the web they demonstrated a strong cor-
relation between complete inclusion of prominent features and lexical entailment. However, they were
unable to assess this using an off-line corpus due to data sparseness.
Szpektor and Dagan (2008) found that the P
ww
measure tends to promote relationships between infre-
quent words with narrow vectors (i.e. those with relatively few distinct context features). They proposed
using the geometric average of P
ww
and the symmetric similarity measure of Lin (1998) in order to
penalise low frequency words.
Kotlerman et al. (2010) apply the IR evaluation method of Average Precision to the problem of identi-
fying lexical inference and use the balancing approach of Szpektor and Dagan (2008) to demote similar-
ities for narrow feature vectors; their measure is called balAPinc. They show that all of the asymmetric
similarity measures previously proposed perform much better than symmetric similarity measures on
a directionality detection experiment, and that their method and that of Clarke (2009) outperform the
others with statistical significance. They also show that their measure is superior when used for term
expansion in an event detection task.
Baroni et al. (2012) investigate the relation between phrasal and lexical entailment, and demonstrate
that support vector machines can generalise entailment relations between quantifier phrases to entailment
involving unseen quantifiers. They compare the performance of their system with the balAPinc measure.
The Stanford WordNet project (Snow et al., 2004) expands the WordNet taxonomy by analysing large
corpora to find patterns that are indicative of hyponymy. For example, the pattern ?NP
X
and otherNP
Y
?
is an indication that NP
X
is a NP
Y
, i.e. that NP
X
is a hyponym of NP
Y
. They use machine learning
to identify other such patterns from known hyponym-hypernym pairs, and then use these patterns to find
new relations in the corpus. The transitivity relation of the taxonomy is enforced by searching only over
valid taxonomies and evaluating the likelihood of each taxonomy given the available evidence (Snow
2251
et al., 2006). The approach is similar to ours in providing a supervised method of learning semantic
relations, but relies on having features for occurrences of pairs of terms rather than just vectors for terms
themselves. Our approach is therefore more generally applicable to systems which compose distribu-
tional representations of meaning.
Most recently, Rei and Briscoe (2013) note that hyponyms are well suited for lexical substitution.
In their experiments with smoothing edge scores for parser lexicalisation, they find that a directional
similarity measure, WeightedCosine
2
, performs best. Also of note, Mikolov et al. (2013) propose a vector
offset method to capture syntactic and semantic regularities between word representations learnt by a
recurrent neural network language model. Yih et al. (2012) present a method for distinguishing synonyms
and antonyms by inducing polarity in a document-term matrix before applying Latent Semantic Analysis.
Santus et al. (2014) propose identifying hypernyms using a new measure based on entropy, SLQS, which
is based on the hypothesis that the most typical linguistic contexts of a hypernym are less informative
than the most typical linguistic contexts of its hyponyms. Evaluated on pairs extracted from the BLESS
dataset (Baroni and Lenci, 2011), this measure outperforms P
ww
at both discriminating hypernym test
pairs from other types of relation and at determining the direction of the entailment relation.
3 Methodology
The code used to perform our experiments has been open sourced, and is available online.
3
3.1 Vector Representations
Distributional information was collected for all of the nouns from Wikipedia provided they had oc-
curred 100 or more times. We used a Wikimedia dump of Wikipedia from June 2011 and extracted
text using wp2txt
4
. This was part-of-speech tagged, lemmatised and dependency parsed using the Malt
Parser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech
(nsubj, dobj, iobj, conj, amod, nnmod) and also occurring 100 or more times were ex-
tracted as features of the POS-tagged and lemmatised nouns. The value of each feature is the positive
point wise mutual information (PPMI) (Church and Hanks, 1989) between the noun and the feature. The
total number of noun vectors which can be harvested from Wikipedia with these parameters is 124, 345.
Our goal is to build classifiers that establish whether or not a given semantic relation, rel, holds be-
tween two similar words A and B. Support vector machines (SVMs), which are effective across a variety
of classification scenarios, learn a boundary between two classes from a set of positive and negative ex-
ample vectors. The two classes correspond to the relation rel holding or not holding. Here, however, we
do not start with a single vector, but with two distributional vectors v
A
and v
B
for the words A and B,
respectively. These vectors must be combined in some way to produce the SVM?s input, and a number
of ways were considered, defined in Table 2. Of these operations, the vector difference (used by svm-
DIFF and knnDIFF) and direct sum (used by svmCAT) are asymmetric, whereas the sum and pointwise
multiplication (used by svmADD and svmMULT) are symmetric.
We now motivate the use of each of these operations. First, we note that pointwise multiplication
(svmMULT) is intersective. Similar vectors will have a large intersection and it might be possible to
learn the features that nouns occurring in different semantic relations should share. However, it does
not retain any information about non-shared features and it is symmetric so it is difficult to see how it
would be possible to use it to distinguish hypernyms from hyponyms. Pointwise addition (svmADD)
effectively performs the union of the features, giving emphasis to the shared features. Whilst it does
retain information about the non-shared features, it is also symmetric, making it difficult again to see
how it would be useful in determining the direction of an entailment relation
Vector difference (as used in svmDIFF and knnDIFF), on the other hand, is asymmetric. Further,
we might expect a small difference vector (containing many zeroes) to be indicative of similar nouns.
Further, considering the majority sign of features in this difference vector might indicate the direction of
2
The details of this measure are unpublished.
3
https://github.com/SussexCompSem/learninghypernyms
4
https://github.com/yohasebe/wp2txt
2252
entailment. Using an SVM, we might expect to be able to effectively learn which of these features should
be ignored and which should be combined, to decide the correct direction of entailment in the majority
number of cases in our training data. However, note that if one uses vector difference it is impossible to
distinguish between the case where a feature occurred with both nouns (to the same extent) and the case
where a feature occurs with neither noun. Accordingly, a small difference vector may indicate that both
nouns do not occur in many distinct contexts. A possible solution to this problem is to use the direct
sum of the vectors (i.e., the concatenation of the two vectors) which retains all of the information from
the original vectors. Finally, we consider the use of the single vector corresponding to the second word
(svmSING) as a baseline. High performance by this operation would indicate that we can learn features
of words which tend to be hypernyms (or co-hyponyms) without any regard to the other word in the
putative relationship.
We also note that the behaviour of these methods may differ depending on the weighting used for vec-
tors. For example, PMI is the log of a ratio of probabilities and therefore one might expect vector addition
where vectors are weighted using PMI to correspond to multiplication where vectors are weighted using
frequency or probability. However, the use of positive PMI (where negative PMI scores are regarded
equal to zero), which is consistent with other work in this area, means that this correspondence is lost.
Because of the nature of our datasets, we were concerned that systems could learn information about
the taxonomy from the relations in the training data, without making use of information in the vectors
themselves. To investigate this, we constructed random vectors to be used in place of the vectors derived
from Wikipedia. The dimensionality of the random vectors was chosen to be 1000 since this substantially
exceeds the average number (398) of non-zero features in the Wikipedia vectors.
3.2 Classifiers
We constructed linear SVMs for each of the vector operations outlined in Section 3.1. We used linear
SVMs for speed and simplicity, since the point is to compare the different vector representations of
the pairings. For comparison, we also constructed a number of supervised, unsupervised, and weakly
supervised classifiers. These are listed in Table 2. For the linear SVMs and kNN classifier, we used the
scikit-learn implementations with default settings. For k nearest neighbours, we performed a parameter
search, using nested cross-validation, varying k between 1 and 50.
For weakly supervised approaches, we evaluated the measure on the training set, then found the best
threshold p on the training set that best divides the two classes using that measure. When classifying, we
determine that the relation holds if the value of the measure exceeds p.
svmDIFF A linear SVM trained on the vector difference v
B
? v
A
svmMULT A linear SVM trained on the pointwise product vector v
B
? v
A
svmADD A linear SVM trained on the vector sum v
B
+ v
A
svmCAT A linear SVM trained on the vector concatenation v
B
? v
A
svmSING A linear SVM trained on the vector v
B
knnDIFF k nearest neighbours (knn) trained on the vector difference v
B
? v
A
.1 < k < 50
widthdiff width(B) > width(A)? rel(A,B) where width(A) is number of non-zero features in A
singlewidth width(B) > p? rel(A,B)
cosineP sim
cos
(A,B) > p? rel(A,B) where sim
cos
(A,B) is cosine similarity using PPMI
linP sim
lin
(A,B) > p? rel(A,B) (Lin, 1998)
CRdiff P
ww
(A,B) > R
ww
(A,B)? rel(A,B) (Weeds et al., 2004)
clarkediff P
cl
(A,B) > R
cl
(A,B)? rel(A,B) (Clarke, 2009)
invCLP invCL(A,B) > p? rel(A,B) (Lenci and Benotto, 2012)
balAPincP balAPinc(A,B) > p? rel(A,B) (Kotlerman et al., 2010)
most freq The most frequent label in the training data is assigned to every test point.
Table 2: Implemented classifiers
2253
3.3 Data Sets
One of key the challenges of this work has been to construct a data set which accurately and validly tests
our hypotheses. All four of our datasets detailed below are available online
5
.
In order to test our hypotheses, a data set needs to be balanced in many respects in order to prevent the
supervised classifiers making use of artefacts of the data. This would not only make it unfair to compare
the supervised approaches with the unsupervised approaches, but also make it unlikely that our results
would be generalisable to other data. Here, we outline the requirements for the data sets, the importance
of which is demonstrated by our initial results for a data set which does not satisfy all of them.
There should be an equal number of positive and negative examples of a semantic relation. Thus,
random guessing or labelling with the most frequently seen label in the training data will yield 50%
accuracy and precision. An advantage of incorporating this requirement means that evaluation can be in
terms of simple accuracy (or error rate).
It should not be possible to do well simply by considering the distributional similarity of the terms.
Hence, the negative examples need to be pairs of equally similar words, but where the relationship under
consideration does not hold.
It should not be possible to do well by pre-supposing an entailment relation and guessing the direction.
For example, it has been shown (Weeds et al., 2004) that given a pair of entailing words selected from
WordNet, over 70% of the time the more frequent word is also the entailed word.
It should not be possible to do well using ontological information learnt about one or both of the
words from the training data that is not generalisable to their distributional representations. For example,
it should not be possible for the classifier simply to learn directly from the training pairs ?cat ISA
mammal? and ?mammal ISA animal? that ?cat ISA animal?. Furthermore, we must ensure that
a classifier cannot learn that a particular word is near the top of the ontological hierarchy, and, as a
result, do well by guessing that a particular pairing probably has an entailment relation. For example,
given many pairs such as ?cat ISA animal?, ?dog ISA animal?, a system which guessed ?rabbit
ISA animal? but not ?animal ISA rabbit? would do better than random guessing. Whilst both
of these types of information could be useful in a hybrid system, they do not require any distributional
information and therefore we would not be learning anything about the distributional features of animal
which make it likely to be a hypernym.
3.3.1 BLESS
We have constructed two data sets from BLESS (Baroni and Lenci, 2011) which is a collection of ex-
amples of hypernyms, co-hyponyms, meronyms and random unrelated words for each of 200 concrete,
largely monosemous nouns. We will refer to these 200 nouns as the BLESS concepts.
hyponym
BLESS
is a set of 1976 labelled pairs of nouns. For each BLESS concept, 80% of the hypernyms
were randomly selected to provide positive examples of entailment. The remaining hypernyms for the
given concept were reversed and taken with the same number of co-hyponyms, meronyms and random
words to form negative examples of entailment. A filter was applied to ensure that duplicate pairs were
not included (e.g., if ?cat,animal? is a positive pair then ?animal,cat? cannot be a negative pair).
cohyponym
BLESS
is a set of 5835 labelled pairs of nouns. For each BLESS concept, the co-hyponyms
were taken as positive examples of this relation. The same total number of (and split evenly between)
hypernyms, meronyms and random words was taken to form the negative examples. The order of 50%
of the pairs was reversed and again duplicate pairs were disallowed.
In both cases the pairs are labelled as positive or negative for the specified semantic relation and in
both cases there are equal (?1) numbers of positive and negative examples. For 99% of the generated
BLESS pairs, both nouns had associated vectors harvested from Wikipedia. If a noun does not have an
associated vector, the classifiers use a zero vector.
5
https://github.com/SussexCompSem/learninghypernyms
2254
3.3.2 WordNet
We constructed two data sets using WordNet. Whilst these data sets are similar in size to the BLESS
data sets they more adequately satisfy the requirements laid out above
6
. We constructed a list of all non-
rare, largely monosemous, single word terms in WordNet. To be considered non-rare, a word needed to
have occurred in SemCor at least once (i.e. frequency information is provided about it in the WordNet
package) and to have occurred in Wikipedia at least 100 times. To be considered largely monosemous,
the predominant sense of the word needed to account for over 50% of the occurrences in the SemCor
frequency information provided with WordNet. This led to a list of 7613 nouns.
hyponym
WN
is a set of 2564 labelled pairs of nouns constructed in the following way. Pairs ?A,B? were
found in the list of nouns where B is an ancestor of A (i.e., A lexically entails B). Each found pair is
added either as a positive or a negative in the ratio 2:1 provided that the reverse pairing has not already
been added and provided that each word has not previously been used in that position. Co-hyponym
pairs (i.e., words which share a direct hypernym) were also found within the list of nouns. Each found
pair is added to the data set (as a negative) provided the reverse pairing has not already been added, and
provided that neither word has already been seen in that position in a pairing (either in the entailment
pairs or the co-hyponym pairs). The same number of co-hyponym pairs as hypernym-hyponym negatives
is selected. This provides a balanced data set where half of the pairs are positive examples of entailment
and the other half are semantically similar but not entailing.
cohyponym
WN
is a set of 3771 labelled pairs of nouns. It was constructed in the same way as hyponym
WN
except the same number of co-hyponym pairs were selected as the total number of entailment pairs (in
either direction). These co-hyponym pairs were labelled as positive and the entailment pairs were labelled
as negative. Thus, this provides a balanced data set where half of the pairs are positive examples of co-
hyponyms and the other half, the negative examples, are entailment pairs (with direction unspecified)
In both these sets, the average path distance between entailment pairs is 1.64, whereas path distance
between co-hyponym pairs is 2.
3.4 Experimental Setup
Most of our experiments were carried out using an implementation of five-fold cross-validation using
each combination of data set, vector set and classifier. In this setup, the pairs are randomly partitioned
into five subsets, one subset is held out for testing whilst the classifiers are trained on the remaining four,
and this process is repeated using each subset as the test set.
In initial experiments with the BLESS datasets, the SVM classifiers were able to achieve classification
accuracy of over 95% for hyponym
BLESS
and over 90% for cohyponym
BLESS
. However, the results us-
ing random vectors were not significantly different from using the distributional vectors harvested from
Wikipedia. This indicated that the classifiers were learning ontological information implicit in the train-
ing data. In order to address this, when using the BLESS datasets, we removed any pair from the training
data if either word was present in the test data. In order to preserve a reasonable amount of training data,
we implemented this approach with ten-fold cross-validation. In all subsequent experiments, across all
datasets and classifiers, we found performance by the random vectors was no higher than 52%. This
indicates that the performance seen in Table 3 is due to learning from distributional features rather than
any ontological information implicit in the training set.
4 Results
In Table 3, we compare average accuracy for a number of different classifiers on each of two tasks,
distinguishing hyponyms and distinguishing co-hyponyms, on each of the two datasets.
Looking at the results for the hyponym
BLESS
data set, we can see that the SVM methods do generally
outperform the unsupervised methods. However, the best performing model is svmSING, suggesting
that, for this data set, it is best to try to learn the distributional features of more general terms, rather than
comparing the vector representations of the two terms under consideration.
6
Note that imposing these requirements on the BLESS data sets would lead to very small data sets, since information is only
provided for 200 nouns.
2255
dataset svmDIFF svmMULT svmADD svmCAT svmSING knnDIFF
hyponym
BLESS
0.74 0.56 0.66 0.68 0.75 0.54
cohyponym
BLESS
0.62 0.39 0.41 0.40 0.40 0.58
hyponym
WN
0.75 0.45 0.37 0.74 0.69 0.50
cohyponym
WN
0.37 0.60 0.68 0.64 0.58 0.50
dataset most freq cosineP linP widthdiff singlewidth CRdiff invCLP balAPincP
hyponym
BLESS
0.54 0.53 0.54 0.56 0.58 0.52 0.54 0.54
cohyponym
BLESS
0.61 0.79 0.78 - - - - -
hyponym
WN
0.50 0.53 0.52 0.70 0.65 0.70 0.66 0.53
cohyponym
WN
0.50 0.50 0.55 - - - - -
Table 3: Accuracy Figures for the data sets generated from BLESS and WordNet (standard errors <
0.02). For cohyponyms, results for measures designed to detect hyponymy have been omitted. We also
omit results of clarkediff as these were consistently the same or less than CRdiff.
On the corresponding co-hyponym task, using the cohyponym
BLESS
data set, we see the best performing
classifier is the cosine measure. The cosine measure is able to perform relatively well here because a
substantial proportion of the negative examples (25%) are random unrelated words which will have low
cosine scores. It is also consistent with earlier work (e.g., (Lenci and Benotto, 2012)) which suggests
that measures such as the cosine measure ?prefer? words in symmetric semantic relationships such as co-
hyponymy. The poor performance of the SVM methods here can perhaps be explained by the paucity of
the training data in this experimental set up with this data set. If, for example, our test concept is robin,
our approach requires that we will not have any training pairs containing robin, or any training pairs
containing any of the words to which robin is related in the test set. In a dataset as small as BLESS,
this requirement effectively removes all knowledge of the distributional features of words in the target
domain. Hence, the need for a larger dataset as we have extracted from WordNet.
Looking at the results for the hyponym
WN
data set, the directional SVM methods (svmDIFF and svm-
CAT) substantially outperform the symmetric SVM methods, and their performance is significantly better
(at the 0.01% level) than the unsupervised methods. Also of note is the substantial difference between
svmDIFF and knnDIFF. Both of these methods are trained on the differences of vectors. However, the
linear SVM outperforms kNN by 19?25%. This may suggest that the shape of the vector space inhabited
by the positive entailment pairs is particularly conducive for learning a linear SVM. Positive and negative
pairs are close together (as evidenced by the poor performance of kNN), but generally linearly separable.
Looking at the results for the cohyponym
WN
data set, it is clear that the unsupervised methods cannot
distinguish the co-hyponym pairs from the entailing pairs. The supervised SVM methods do substantially
better, with the best performance achieved by svmADD and svmCAT. Both of these methods essentially
retain information about all of the features of both words. svmMULT does much better than svmDIFF,
which suggests that the shared features are more indicative than the non-shared features for this task.
The reasonably high performance of svmSING on both data sets suggests that words which have co-
hyponyms in the data set tend to inhabit a somewhat different part of the feature space to words which
are included as entailed words in the data set. We hypothesise that there are specific features which more
general words tend to share (regardless of their topic) which makes it possible to identify more general
words from more specific words. This is completely consistent with very recent results using SLQS, a
new entropy-based measure (Santus et al., 2014). Here, the authors hypothesise that the most typical
contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms,
with some promising results. It would be plausible to hypothesise that svmSING is learning which nouns
typically have less informative contexts and are therefore likely to by hypernyms.
Given prior work, the performance of the balAPincP measure is lower than expected on the
hyponym
WN
dataset. Our task is slightly different to that of (Kotlerman et al., 2010), since we are deter-
mining the existence (or not) of hyponymy, rather than the direction of entailment for pairs where it is
known that a relationship exists. It could be that the measure is particularly suited to the latter task.
2256
5 Conclusions and Further Work
We have shown that it is possible to predict to a large extent whether or not there is a specific semantic
relation between two words given their distributional vectors, using a supervised approach based on
linear SVMs. The increase in accuracy over unsupervised methods is significant at the 0.01% level and
corresponds to a substantial absolute reduction in error rate (over 15%).
We have also shown that the choice of vector operation is significant. Whilst concatenating the vectors,
and therefore retaining all of the information from both vectors including direction, generally performs
well, we have also shown that different vector operations are useful in establishing different relationships.
In particular, the vector difference operation, which loses information about the original vectors, achieved
performance indistinguishable from concatenation on the entailment task, where the classifier is required
to distinguish hyponyms from other semantically related words including hypernyms. On the other
hand, the addition operation, which also loses information, outperformed concatenation by 4% (which
is statistically significant at the 0.01% level) on the coordinate task, where the classifier is required to
distinguish co-hyponyms from hyponyms and hypernyms. Hence the nature of the relationship one is
trying to establish between words determines the nature of the operation one should perform on their
associated vectors.
We have also shown that it is possible to outperform state-of-the-art unsupervised methods even when
a data set has been constructed without ontological information, and when target words have not previ-
ously been seen in that position of a relationship in the training data. Hence, we believe the supervised
methods are learning characteristics of the underlying feature space which are generalisable to new words
(inhabiting the same feature space).
In future work, we intend to apply this approach to the problem of labelling the distributional neigh-
bours found for a given word with specific semantic relations. We also plan to investigate the use of
bag-of-words (windowed) vectors instead of grammatical relations for this task.
Finally, we believe that the data sets constructed from WordNet, which we publish alongside this
paper, can be used as a useful benchmark in evaluating future advances in this area, both for supervised
and unsupervised methods.
Acknowledgements
This work was funded by UK EPSRC project EP/IO37458/1 ?A Unified Model of Compositional and
Distributional Compositional Semantics: Theory and Applications?.
References
Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
of the GEMS 2011 workshop on Geometric Models of Natural Language Semantics, EMNLP 2011.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word
level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 23?32. Association for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphs. In
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220?1229,
Uppsala, Sweden, July. Association for Computational Linguistics.
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz Kondrak. 2010. Predicting the semantic composi-
tionality of prefix verbs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 293?303, Cambridge, MA, October. Association for Computational Linguistics.
Danushka Bollegala, David Weir, and John Carroll. 2011. Using multiple sources to construct a sentiment sen-
sitive thesaurus for cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011).
2257
Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography.
In Proceedings of the 27th Annual Meeting on Association for Computational Linguistics, ACL ?89, pages
76?83, Stroudsburg, PA, USA. Association for Computational Linguistics.
Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the
Workshop of Geometric Models for Natural Language Semantics.
James Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.
Georgiana Dinu and Stefan Thater. 2012. Saarland: Vector-based models of semantic textual similarity. In
Proceedings of the First Joint Conference on Lexical and Computational Semantics.
Christaine Fellbaum, editor. 1989. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge,
Massachusetts.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy induction using hierarchical random graphs. In Proceed-
ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 466?476, Montr?eal, Canada, June.
Maayan Geffet and Ido Dagan. 2005. Lexical entailment and the distributional inclusion hypothesis. In Proceed-
ings of the 43rd meeting of the Association for Computational Liuguistics (ACL), pages 107?114.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete
sentence spaces for compositional distributional models of meaning. Proceedings of the 9th International Con-
ference on Computational Semantics (IWCS 2011), pages 125?134.
Zelig Harris. 1954. Distributional structure. Word, 10:146?162.
Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted
WSD: Finding a middle ground between supervision and unsupervision. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics, pages 1532?1541, Uppsala, Sweden, July.
Adam Kilgarriff and Colin Yallop. 2000. What?s in a thesaurus? In Proceedings of the 2nd International
Conference on Language Resources and Evaluation (LREC2000).
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional simi-
larity for lexical inference. Special Issue of Natural Language Engineering on Distributional Lexical Semantics,
4(16):359?389.
Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, pages 25?32, College Park, Maryland, USA, June.
Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceed-
ings of the First Joint Conference on Lexical and Computational Semantics (*Sem).
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International
Conference on Computational Linguistics (COLING 1998).
Tara McIntosh. 2010. Unsupervised discovery of negative categories in lexicon bootstrapping. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing, pages 356?365, Cambridge, MA,
October. Association for Computational Linguistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751, Atlanta, Georgia, June.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for
lexical expansion in knowledge-based word sense disambiguation. In Proceedings of COLING 2012, pages
1781?1796, Mumbai, India, December.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08:
HLT, pages 236?244, Columbus, Ohio, June. Association for Computational Linguistics.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the ACL workshop on
Incremental Parsing, pages 50?57.
2258
Marek Rei and Ted Briscoe. 2013. Parser lexicalisation through self-learning. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 391?400, Atlanta, Georgia, June. Association for Computational Linguistics.
Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte Im Walde. 2014. Chasing hypernyms in vector
spaces with entropy. In Proceedings of the 14th Conference of the European Chapter of the Association for
Computational Linguistics, pages 38?42, Gothenburg, Sweden, April.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym
discovery. Advances in Neural Information Processing Systems 17.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006. Semantic taxonomy induction from heterogenous evidence.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages 801?808. Association for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1201?1211.
Gy?orgy Szarvas, Chris Biemann, and Iryna Gurevych. 2013. Supervised all-words lexical substitution using
delexicalized features. In Proceedings of the 2013 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 1131?1141, Atlanta, Georgia, June.
Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Linguistics (Coling 2008), pages 849?856, Manchester, UK,
August.
Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,
pages 81?88.
Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.
In Proceedings of Coling 2004, pages 1015?1021, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Julie Weeds, David Weir, and Jeremy Reffin. 2014. Distributional composition using higher-order dependency
vectors. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality,
EACL 2014, Gothenburg, Sweden, April.
Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second
Symposium on Quantum Interaction, Oxford, UK, pages 1?8.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1212?1222, Jeju Island, Korea, July. Association for Computational Linguistics.
Chen Zhang and Joyce Chai. 2010. Towards conversation entailment: An empirical investigation. In Proceedings
of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756?766, Cambridge,
MA, October. Association for Computational Linguistics.
2259
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 115?119, Dublin, Ireland, August 23-29 2014.
Method51 for Mining Insight from Social Media Datasets
Simon Wibberley
University of Sussex
simon.wibberley
@sussex.ac.uk
Jeremy Reffin
University of Sussex
j.p.reffin
@sussex.ac.uk
David Weir
University of Sussex
d.j.weir
@sussex.ac.uk
Abstract
We present Method51, a social media analysis software platform with a set of accompanying
methodologies. We discuss a series of case studies illustrating the platform?s application, and
motivating our methodological proposals.
1 Introduction
Social scientists wish to apply language processing technology on social media datasets to answer soci-
ological questions. To that end, the technology should support methodologies that allow analysts to gain
valuable insight from the datasets under examination. In previous work we have argued for the impor-
tance of agility when dealing with social media datasets (Wibberley et al., 2013). In this paper we present
a series of case studies, carried out on Twitter, that illustrate the importance of that agile paradigm, and
how they have motivated the development of several additional methodologies, including ?Twitcident?,
?Patterns of Use?, and ?Russian Doll? analysis. First, we present Method51
1
, the technological counter-
part to our methodological paradigm.
2 Method51
Method51 uses active learning, coupled with a Na??ve Bayes model, to allow social scientists to construct
chains of linked, bespoke classifiers. The framework, initially an extension of DUALIST (Settles, 2011),
utilises an EM step to harness information from large amounts of unlabelled data, and allows the analyst
to expedite learning by specifying features that are highly indicative of a class. Using this approach,
a classifier can be trained within minutes (Settles, 2011). This enables analysts to evolve the way that
the data is being analysed without significant loss of effort. Method51 provides significant additional
functionality including collaborative gold standard and classifier construction, processing pipeline con-
struction, data collection and storage, data visualisation, various filtering and processing modules, and
time-based data selection. Figure 1a show the pipeline construction interface, which allows analysts to
knit together chains of bespoke classifiers.
Figure 1b shows the ?Coding? interface which is the primary point of contact between the analyst and
the data, where documents and features are labelled. Classifier evaluation statistics are also displayed, so
analysts can rapidly assess whether the data and technology are amenable to their analytical approach.
Method51 aims to put social science researchers at the centre of the data exploration process. Insight is
generated through the iterative interaction of the subject matter expert and the data itself.
Method51 combines two strands of existing work. The first body of work employs tailored automatic
data analysis, using supervised machine-learning approaches (Carvalho et al., 2011; Papacharissi and de
Fatima Oliveira, 2012; Meraz and Papacharissi, 2013; Hopkins and King, 2010; Burnap et al., 2013b).
A second body of work focusses on providing user interfaces that enable researchers to customise their
processing pipeline, based on the requirements of their investigation (Blessing et al., 2013; Black et al.,
2012; Burnap et al., 2013a).
1
Method51 has been released under an open source license, and is available at https://github.com/simonwibberley/method51
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
115
(a) Pipeline Construction Interface
(b) Coding interface
3 Case Studies
Over the past 18 months we have conducted a wide range of studies using Method51. Three illustrative
case studies are presented here in chronological order; each investigation highlighted new analytical
challenges and therefore motivated methodological and technological development.
3.1 EU Sentiment
In the first study, we examined the attitudes of EU citizens towards the Eurozone crisis of 2013 (Bartlett et
al., textitforthcoming). We set out with a preconceived structure and methodology. We tracked 6 topics,
referencing EU institutions or prominent people, and collected Tweets in 3 languages (English, French,
and German) to create 18 distinct streams of messages. For each stream, we constructed a bespoke
classification pipeline with a common analytical architecture of three successive layers: a classifier for
relevancy, a classifier to determine whether Tweets were attitudinal, and a classifier to determine the
polarity of the sentiment being expressed.
We found that, broadly, the data did not fit the pre-conceived analytical architecture neatly and that
classifier performance was a poor fit with human judgements, particularly for the attitudinal and senti-
ment layers. Table 1 illustrates the range of performance of classifiers measured against human-annotated
gold standard. Although relevancy classifiers performed adequately, the reliability of attitudinal and sen-
timent classifiers varies widely across streams.
Investigation revealed a number of underlying issues and prompted a series of methodological re-
sponses:
Need for flexible architecture We observed that each stream presented different challenges that could
only be appropriately addressed by a bespoke architecture tailored to the anatomy of the stream. For
example, relevancy classification was only sometimes required. The ?Euro? stream was targeted towards
conversation about the Euro currency, but required relevancy filtering as the query ?#euro? matched con-
versations regarding a wide variety of other topics such as sport competitions. Conversely, the ?Barroso?
stream, tracking messages regarding the president of the European Commission, Jos?e Manuel Barroso,
was of sufficiently high precision not to warrant relevancy filtering.
?Twitcident? analysis We observed that attitudes were typically only exposed when some event in the
world ?provoked? a burst of reactions that was related to the topic of interest. These response bursts,
which usually occur over a matter of hours to days, have been labelled ?Twitcidents?. We found that the
nature of the response ? and how this should be mapped onto the broader topic ? was unique to the
event itself. The ?Twitcident? analysis principle states that each event needs to be studied separately in
order to be correctly interpreted. Using a common classification architecture, or otherwise aggregating
116
results across Twitcidents, is likely to create a misleading picture of how opinions are being shaped by
events over time. As an example, a speech by the UK Prime Minister expressing a sceptical view of
the EU prompted many enthusiastic responses. Messages that commented positively on his speech were
contributing evidence of negative sentiment towards the EU. Clearly the reaction to that speech had to be
analysed separately in order to allow for this reversal of sentiment polarity.
Exploratory ?Patterns of Use? analysis The poor fit between the data and the imposed classification
architecture prompted us to adopt a new approach to constructing pipelines. The framework enables
analysts to ?fail fast?: engage actively with the data and explore how it is structured, before committing
to the pipeline framework. This is feasible because Method51 enables classifiers to be built quickly.
This ?Patterns of Use? analysis is inspired by Grounded Theory (Glaser et al., 1968), and mandates
that classification categories should arise from an unbiased examination of the available data. Categories
arise naturally from the analyst?s engagement with the data.
3.2 Father?s Day
Our initial ideas about ?Patterns of Use? analysis were explored further in the Father?s Day study. Our
aim here was to identify users likely to respond positively to a targeted advertisement for Father?s Day
gift ideas, that assessment being driven by the content of a Tweet sent by the user. We collected and
analysed Tweets mentioning Father?s Day in the days leading up to the event, and our first attempts at
analysing underlying patterns prompted a revision to our methodology.
?Russian Doll? approach We observed that at any stage the data tends to be dominated by one pattern of
usage, obscuring other underlying patterns. We developed a ?Russian Doll? approach, which mandates
that at each layer a classifier is built to unpack from the data Tweets that match this most prominent
pattern of usage. With this usage pattern stripped out, new structure is typically revealed in the remaining
data, which can in turn be unpacked using simple bespoke classifiers. Chained together, this pipeline of
classifiers removes successive different patterns of usage to expose underlying structure.
In this case, our a-priori expectation was that the stream would contain marketing Tweets, general
conversation about Father?s Day, and our target subset: people expressing uncertainty about what gift to
get. Our analysis revealed, however, a significant critical class of Tweets (and Tweeters) the presence of
which was unexpected ? a category for which targeted marketing Tweets would be wholly inappropriate.
The content of the Tweets matching this category (dubbed ?Sad/Distressed?) was negative, with Tweeters
venting sad or angry feelings towards absent fathers, generally though family breakdown or bereavement.
It was only through this careful patterns of use analysis that we identified this critical subgroup.
The final architecture consisted of two layers that dealt with existing marketing Tweets, a layer that
assessed the suitability of the Tweet as a potential target for a marketing campaign, and a final layer
that identified explicit requests for gift ideas. This pipeline showed good performance as measured by
F-scores against gold-standard annotated data (see Table 2). The unexpected ?Sad/Distressed? category
constituted a high proportion (10%) of all Tweets in the data set. Of the remaining tweets, 50% were
classified ?Marketing?, 36% were classified ?Miscellaneous? comments about Father?s Day, and 4% as
?Gift Idea Request?, our target group.
3.3 Mark Duggan
Our final case study illustrates an analysis conducted using the ?Twitcident? and ?Patterns of Use? tech-
niques. The aim was to dissect the online reaction to developments in the Mark Duggan inquest, an
enquiry into the shooting by police in London of a young black man. Over the course of about a month,
Tweets regarding Mark Duggan were collected with a view to analysing reactions as the case progressed.
The response showed the familiar ?Twitcident? pattern (see Figure 2). Twitcidents were examined indi-
vidually, culminating in an analysis of the large response on Twitter to the final verdict. Four specific
categories of response were identified and analysed. These were: (i) ?No Justice? ? where a Tweet
included accusations of institutional racism and/or claims that Duggan was unarmed; (ii) ?Justice? ?
that Duggan ?had it coming?, and/or that Duggan was armed; (iii) ?Riot? ? warning of possible rioting,
117
Range Split
Relevance 0.5 - 0.9 2-way
Attitudinal 0.3 - 0.9 2-way
Sentiment 0.1 - 0.8 3-way
Table 1: Range of F1-scores of EU Sentiment
Individual Marketing
F1 0.873 0.844
(a) Marketing level 1
Suitable Sad Marketing
F1 0.785 0.564 0.227
(c) Suitability
Individual Marketing
F1 0.834 0.490
(b) Marketing level 2
Request Other
F1 0.583 0.959
(d) Gift request
Table 2: F-scores of Russion Dolls Analysis
Justice No Justice Riot Watching
F1 0.636 0.842 0.737 0.451
Split 58% 17% 7% 18%
Table 3: Verdict classifier performance
making calls for calm; and (iv) ?Watching? ? people neutrally expressing interest in a case. Pipeline
performance was good as measured by F-scores against gold-standard (see Table 3)
This case further illustrates how patterns of response are specific to a particular situation, greatly
limiting the usefulness of pre-defined classifiers (e.g. for sentiment) in real-world investigations.
Figure 2: Mark Duggan ?Twitcidents?
4 Discussion
We will end with a discussion of how we have interpreted the developments in methodology described
above, and how that interpretation leads to an intuitive framework for further work.
We have presented the application of three distinct methodologies for mining insight from social me-
dia data. The insights gained from each case study are the result of three interdependent factors; (i) the
question that is being posed, (ii) the extend to which the answers to the question reside in the data, and
(iii) the extent to which the technology is capable of addressing the question given the data. We encapsu-
late this line of interpretation as ?Question, Data, and Technology?. By considering these factors analysts
can remain plastic about how the ?Data? and ?Technology? can mutually constrain and inform the ?Ques-
tion?. For example, if answers to the question are not represented in the data in a form the technology
can recognise, then the question should be revised. Conversely, the technology may reveal unexpected
characteristics in the data that contribute towards the analysts understanding of what the question should
be. Careful alignment between all three tend to result in valuable insights being discovered.
Crucial to this alignment is assessing the performance of the technology on the data, given the question
being posed. Method51 provides some functionality towards supporting this, such as accuracy and F-
scores. However, these measures indirectly indicate whether the classifier is behaving sensibly by virtue
118
of the gold standard evaluation data being an i.i.d sample of the unlabelled data.
The behaviour of classifiers on unlabelled data forms a crucial role in how the technology supports the
analyst in their investigation. Directly exposing that behaviour and developing an informed understand-
ing of the relationship between ?Question, Data, and Technology? would only expedite and contribute
towards reliable insights being discovered. Incorporating technology into Method51 that exposes how
unlabelled data are effected by analytical processes is an area for further work.
In general, we have found that considering the interaction between ?Question, Data, and Technology?
provides an intuitive framework for refining the focus of methodological and technological development
towards demonstrably useful innovations.
Acknowledgments
This work was supported by the ESRC National Centre for Research Methods grant DU/512589110.
We are grateful to our collaborators at the Centre for the Analysis of social media, Jamie Bartlett and
Carl Miller for valuable contributions to this work. We thank the anonymous reviewers for their helpful
comments. This work was partially supported by the Open Society Foundation.
References
J. Bartlett, Miller C, J. Reffin, D. Weir, and Wibberley S. forthcoming. Vox digitas.
http://www.demos.co.uk/publications.
W. Black, R. Procter, S. Gray, and S. Ananiadou. 2012. A data and analysis resource for an experiment in text
mining a collection of micro-blogs on a political topic. In LREC. ELRA.
A. Blessing, J. Sonntag, F. Kliche, U. Heid, J. Kuhn, and M. Stede. 2013. Towards a tool for interactive concept
building for large scale analysis in the humanities. In LaTeCH, Social Sciences, and Humanities. ACL.
P. Burnap, N. Avis, and O. Rana. 2013a. Making sense of self-reported socially significant data using computa-
tional methods. International Journal of Social Research Methodology.
P. Burnap, O. Rana, N. Avis, M. Williams, W. Housley, A. Edwards, J. Morgan, and L. Sloan. 2013b. Detect-
ing tension in online communities with computational twitter analysis. Technological Forecasting and Social
Change.
P. Carvalho, L. Sarmento, J. Teixeira, and M. Silva. 2011. Liars and saviors in a sentiment annotated corpus of
comments to political debates. In ACL: Human Language Technologies.
Barney G Glaser, Anselm L Strauss, and Elizabeth Strutzel. 1968. The discovery of grounded theory; strategies
for qualitative research. Nursing Research.
D. J. Hopkins and G. King. 2010. A method of automated nonparametric content analysis for social science.
American Journal of Political Science.
Sharon Meraz and Zizi Papacharissi. 2013. Networked gatekeeping and networked framing on #egypt. The
International Journal of Press/Politics, 18(2):138?166.
Zizi Papacharissi and Maria de Fatima Oliveira. 2012. Affective news and networked publics: the rhythms of
news storytelling on #egypt. Journal of Communication, 62(2):266?282.
B. Settles. 2011. Closing the loop: Fast, interactive semi-supervised annotation with queries on features and
instances. In Empirical Methods in Natural Language Processing.
Simon Wibberley, David Weir, and Jeremy Reffin. 2013. Language technology for agile social media science. In
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Human-
ities, pages 36?42, Sofia, Bulgaria, August. Association for Computational Linguistics.
119
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 36?42,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Language Technology for Agile Social Media Science
Simon Wibberley
Department of Informatics
University of Sussex
sw206@susx.ac.uk
Jeremy Reffin
Department of Informatics
University of Sussex
j.p.reffin@susx.ac.uk
David Weir
Department of Informatics
University of Sussex
davidw@susx.ac.uk
Abstract
We present an extension of the DUALIST
tool that enables social scientists to engage
directly with large Twitter datasets. Our
approach supports collaborative construc-
tion of classifiers and associated gold stan-
dard data sets. The tool can be used to
build classifier cascades that decomposes
tweet streams, and provide analysis of tar-
geted conversations. A central concern is
to provide an environment in which social
science researchers can rapidly develop an
informed sense of what the datasets look
like. The intent is that they develop, not
only an informed view as to how the data
could be fruitfully analysed, but also how
feasible it is to analyse it in that way.
1 Introduction
In recent years, automatic social media analysis
(SMA) has emerged, not only as a major focus
of attention within the academic NLP community,
but as an area that is of increasing interest to a va-
riety of business and public sectors organisations.
Among the many social media platforms in use to-
day, the one that has received the most attention is
Twitter, the second most popular social media net-
work in the world with over 400 million tweets
sent each day. The popularity of Twitter as a tar-
get of SMA derives from both the public nature
of tweets, and the availability of the Twitter API
which provides a variety of flexible methods for
scraping tweets from the live Twitter stream.
A plethora of social media monitoring plat-
forms now exist, that are mostly concerned with
providing product marketing oriented services1.
For example, brand monitoring services seek to
provide companies with an understanding of what
1http://wiki.kenburbary.com/social-media-monitoring-
wiki lists 230 Social Media Monitoring Solutions
is being said about their brands and products,
with language processing technology being used
to capture relevant comments or conversations and
apply some form of sentiment analysis (SA), in or-
der to derive insights into what is being said. This
paper forms part of a growing body of work that
is attempting to broaden the scope of SMA be-
yond the realm of product marketing, and into ar-
eas of concern to social scientists (Carvalho et al,
2011; Diakopoulos and Shamma, 2010; Gonzalez-
Bailon et al, 2010; Marchetti-Bowick and Cham-
bers, 2012; O?Connor et al, 2010; Tumasjan et al,
2011; Tumasjan et al, 2010).
Social media presents an enormous opportunity
for the social science research community, consti-
tuting a window into what large numbers of people
are talking. There are, however, significant obsta-
cles facing social scientists interested in making
use of big social media datasets, and it is important
for the NLP research community to gain a better
understanding as to how language technology can
support such explorations.
A key requirement, and the focus of this paper,
is agility: the social scientist needs to be able to
engage with the data in a way that supports an it-
erative process, homing in on a way of analysing
the data that is likely to produce valuable insight.
Given what is typically a rather broad topic as a
starting point, there is a need to see what issues re-
lated to that topic are being discussed and to what
extent. It can be important to get a feeling for the
kind of language being used in these discussions,
and there is a need to rapidly assess the accuracy
of the automated decision making. There is little
value in developing an analysis of the data on an
approach that relies on the technology making de-
cisions that are so nuanced that the method being
used is highly unreliable. As the answers to these
questions are being exposed, insights emerge from
the data, and it becomes possible for the social sci-
entist to progressively refine the topics that are be-
36
ing targetted, and ultimately create a way of au-
tomatically analysing the data that is likely to be
insightful.
Supporting this agile methodology presents se-
vere challenges from an NLP perspective, where
the predominant approaches use classifiers that
involve supervised machine learning. The need
for substantial quantities of training data, and
the detrimental impact on performance that re-
sults when applying them to ?out-of-domain? data
mean that exisiting approaches cannot support the
agility that is so important when social scientists
engage with big social media datasets.
We describe a tool being developed in collab-
oration with a team of social scientists to support
this agile methodology. We have built a frame-
work based on DUALIST, an active learning tool
for building classifiers (Settles, 2011; Settles and
Zhu, 2012). This framework provides a way for
a group of social scientists to collaboratively en-
gage with a stream of tweets, with a goal of con-
structing a chain (or cascade) of automatic docu-
ment classification layers that isolate and analyse
targeted conversions on Twitter. Section 4 dis-
cusses ways in which the design of our frame-
work is intended to support the agile methodol-
ogy mentioned above, with particular emphasis on
the value of DUALIST?s active learning approach,
and the crucial role of the collaborative gold stan-
dard and model building activities. Section 4.3
discusses additional data processing step that have
been introduced to increase the frameworks use-
fulness, and section 5 introduces some projects to
which the framework is being applied.
2 Related Work
Work that focuses on addressing sociological
questions with SMA broadly fall into one of three
categories.
? Approaches that employ automatic data analy-
sis without tailoring the analysis to the specifics of
the situation e.g. (Tumasjan et al, 2010; Tumas-
jan et al, 2011; O?Connor et al, 2010; Gonzalez-
Bailon et al, 2010; Sang and Bos, 2012; Bollen
et al, 2011). This body of research involves lit-
tle or no manual inspection of the data. An an-
alytical technique is selected a-priori, applied to
the SM stream, and the results from that analy-
sis are then aligned with a real-world phenomenon
in order to draw predictive or correlative conclu-
sions about social media. A typical approach is
to predict election outcomes by counting mentions
of political parties and/or politicians as ?votes? in
various ways. Further content analysis is then
overlaid, such as sentiment or mood anlysis, in
an attempt to improve performance. However the
generic language-analysis techniques that are ap-
plied lead to little or no gain, often causing ad-
justments to target question to something with less
strict assessment criteria, such as poll trend instead
of election outcome (Tumasjan et al, 2010; Tu-
masjan et al, 2011; O?Connor et al, 2010; Sang
and Bos, 2012). This research has been criticised
for applying out-of-domain techniques in a ?black
box? fashion, and questions have been raised as
to how sensitive the results are to parameters cho-
sen (Gayo-Avello, 2012; Jungherr et al, 2012).
? Approaches that employ manual analysis of
the data by researchers with a tailored analyti-
cal approach (Bermingham and Smeaton, 2011;
Castillo et al, 2011).This approach reflects tra-
ditional research methods in the social sciences.
Through manual annotation effort, researchers en-
gage closely with the data in a manual but in-
teractive fashion, and this effort enables them to
uncover patterns in the data and make inferences
as to how SM was being used in the context of
the sociocultural phenomena under investigation.
This research suffers form either being restricted
to fairly small datasets.
? Approaches that employ tailored automatic
data analysis, using a supervised machine-learning
approach(Carvalho et al, 2011; Papacharissi and
de Fatima Oliveira, 2012; Meraz and Papacharissi,
2013; Hopkins and King, 2010). This research in-
fers properties of the SM data using statistics from
their bespoke machine learning analysis. Mannual
annotation effort is required to train the classifiers
and is typically applied in a batch process at the
commencement of the investigation.
Our work aims to expand this last category, im-
proving the quality of research by capturing more
of the insight-provoking engagement with the data
seen in more traditional research.
3 DUALIST
Our approach is built around DUALIST (Settles,
2011; Settles and Zhu, 2012), an open-source
project designed to enable non-technical analysts
to build machine-learning classifiers by annotat-
ing documents with just a few minutes of effort.
37
In Section 4, we discuss various ways in which
we have extended DUALIST, including function-
ality allowing multiple annotators to work in par-
allel; incorporating functionality to create ?gold-
standard? test sets and measure inter-annotator
agreement; and supporting on-going performance
evaluation against the gold standard during the
process of building a classifier. DUALIST pro-
vides a graphical interface with which an annota-
tor is able to build a Na??ve Bayes? classifier given
a collection of unlabelled documents. During the
process of building a classifier, the annotator is
presented with a selection of documents (in our
case tweets) that he/she has an opportunity to la-
bel (with one of the class labels), and, for each
class, a selection of features (tokens) that the an-
notator has an opportunity to mark as being strong
features for that class.
Active learning is used to select both the docu-
ments and the features being presented for annota-
tion. Documents are selected on the basis of those
that the current model is most uncertain about
(as measured by posterior class entropy), and fea-
tures are selected for a given class on the basis
of those with highest information gain occurring
frequently with that class. After a batch of docu-
ments and features have been annotated, a revised
model is built using both the labelled data and the
current model?s predictions for the remaining un-
labelled data, through the use of the Expectation-
Maximization algorithm. This new model is then
used as the basis for selecting the set of documents
and features that will be presented to the annotator
for the next iteration of the model building pro-
cess. Full details can be found in Settles (2011).
The upshot of this is two-fold: not only can a
reasonable model be rapidly created, but the re-
searcher is exposed to an interesting non-uniform
sample of the training data. Examples that are rel-
atively easy for the model to classify, i.e. those
with low entropy, are ranked lower in the list of
unlabelled data awaiting annotation. The effect of
this is that the training process facilitates a form of
data exploration that exposes the user to the hard-
est border cases.
4 Extending DUALIST for Social Media
Science Research
This section describes ways in which we have ex-
tended DUALIST to provide an integrated data ex-
ploration tool for social scientists. As outlined in
the introduction, our vision is that a team of social
scientists will be able to use this tool to collabora-
tively work towards the construction of a cascade
of automatic document classification layers that
carve up an incoming Twitter data stream in order
to pick out one or more targeted ?conversations?,
and provide an analysis of what is being discussed
in each of these ?conversations?. In what follows,
we refer to the social scientists as the researchers
and the activity during which the researchers are
working towards delivering a useful classifier cas-
cade as data engagement.
4.1 Facilitating data engagement
When embarking on the process of building one
of the classifiers in the cascade, researchers bring
preconceptions as to the basis for the classifica-
tion. It is only when engaging with the data that
it becomes possible to develop an adequate clas-
sification policy. For example, when looking for
tweets that express some attitude about a targeted
issue, one needs a policy as to how a tweet that
shares a link to an opinion piece on that topic
without any further comment should be classified.
There are a number of ways in which we support
the classification policy development process.
? One of the impacts of the active learning ap-
proach adopted in DUALIST is that by presenting
tweets that the current model is most unsure of,
DUALIST will very rapidly expose issues around
how to make decisions on boundary cases.
? We have extended DUALIST to allow multi-
ple researchers to build a classifier concurrently.
In addition to reducing the time it takes to build
classifiers, this fosters a collaborative approach to
classification policy development.
? We have added functionality that allows for the
collaborative construction of gold standard data
sets. Not only does this provide feedback dur-
ing the model building process as to when perfor-
mance begins to plateau, but, as a gold standard
is being built, researchers are shown the current
inter-annotator agreement score, and are shown
examples of tweets where there is disagreement
among annotators. This constitutes yet another
way in which researchers are confronted with the
most problematic examples.
4.2 Building classifier cascades
Having considered issues that relate to the con-
struction of an individual classifier, we end this
38
section by briefly considering issues relating to
the classifier cascade. The Twitter API provides
basic boolean search functionality that is used to
scrape the Twitter stream, producing the input to
the cascade. A typical strategy is to select query
terms for the boolean search with a view to achiev-
ing a reasonably high recall of relevant tweets2.
An effective choice of query terms that actually
achieves this is one of the things that is not well
understood in advance, but which we expect to
emerge during the data engagement phase. Cap-
turing an input stream that contains a sufficiently
large proportion of interesting (relevant) tweets is
usually achieved at the expense of precision (the
proportion of tweets in the stream being scraped
that are relevant). As a result, the first task that is
typically undertaken during the data engagement
phase involves building a relevancy classifier, to
be deployed at the top of the classifier cascade,
that is designed to filter out irrelevant tweets from
the stream of tweets being scraped.
When building the relevancy classifier, the re-
searchers begin to see how well their preconcep-
tions match the reality of the data stream. It is only
through the process of building this classifier that
the researchers begin to get a feel for the compo-
sition of the relevant data stream. This drives the
researcher?s conception as to how best to divide
up the stream into useful sub-streams, and, as a
result, provides the first insights into an appropri-
ate cascade architecture. Our experience is that in
many cases, classifiers at upper levels of the cas-
cade are involved in decomposing data streams in
useful ways, and classifiers that are lower down
in the cascade are designed to measure some facet
(e.g. sentiment polarity) of the material on some
particular sub-stream.
4.3 Tools for Data Analysis
As social scientists are starting to engage with
real-world data using this framework, it has
emerged that certain patterns of downstream data
analysis are of particular use.
Time series analysis. For many social phenom-
ena, the timing and sequence of social media mes-
sages are of critical importance, particularly for a
platform such as Twitter. Our framework supports
tweet volume analysis across any time frame, al-
2In many cases it is very hard to estimate recall since there
is no way to estimate accurately the volume of relevant tweets
in the full Twitter stream.
lowing researchers to review changes over time
in any classifier?s input or output tweet flows
(classes). This extends the common approach of
sentiment tracking over time to tracking over time
any attitudinal (or other) response whose essen-
tial features can be captured by a classifier of this
kind. These class-volume-by-time-interval plots
can provide insight into how and when the stream
changes in response to external events.
Link analysis. It is becoming apparent that link
sharing (attaching a URL to a tweet, typically
pointing to a media story) is an important aspect of
how information propagates through social media,
particularly on Twitter. For example, the mean-
ing of a tweet can sometimes only be discerned by
inspecting the link to which it points. We are in-
troducing to the framework automatic expansion
of shortened URLs and the ability to inspect link
URL contents, allowing researchers to interpret
tweets more rapidly and accurately. A combina-
tion of link analysis with time series analysis is
also providing researchers with insights into how
mainstream media stories propagate through soci-
ety and shape opinion in the social media age.
Language use analysis. Once a classifier has
been initially established, the framework analyses
the language employed in the input tweets using
an information gain (IG) measure. High IG fea-
tures are those that have occurrence distributions
that closely align the document classification dis-
tributions; essentially they are highly indicative of
the class. This information is proving useful to so-
cial science researchers for three purposes. First,
it helps identify the words and phrases people em-
ploy to convey a particular attitude or opinion in
the domain of interest. Second, it can provide in-
formation on how the language employed shifts
over time, for example as new topics are intro-
duced or external events occur. Third, it can be
used to select candidate keywords with which to
augment the stream?s boolean scraper query. In
this last case, however, we need to augment the
analysis; many high IG terms make poor scraper
terms because they are poorly selective in the more
general case (i.e. outside of the context of the ex-
isting query-selected sample). We take a sample
using the candidate term alone with the search API
and estimate the relevancy precision of the scraped
tweet sample by passing the tweets through the
first-level relevancy classifier. The precision of the
39
new candidate term can be compared to the preci-
sion of existing terms and a decision made.
5 Applications and Extensions
The framework?s flexibility enables it to be applied
to any task that can be broken down into a series of
classification decisions, or indeed where this ap-
proach materially assists the social scientist in ad-
dressing the issue at hand. In order to explore its
application, our framework is being applied to a
variety of tasks:
Identifying patterns of usage. People use the
same language for different purposes; the frame-
work is proving to be a valuable tool for eluci-
dating these usage patterns and for isolating data
sets that illustrate these patterns. As an example,
the authors (in collaboration with a team of so-
cial scientists) are studying the differing ways in
which people employ ethnically and racially sensi-
tive language in conversations on-line. The frame-
work has helped to reveal and isolate a number of
distinct patterns of usage.
Tracking changes in opinion over time. Sen-
timent classifiers trained in one domain perform
poorly when applied to another domain, even
when the domains are apparently closely related
(Pang and Lee, 2008). Traditionally, this has
forced a choice between building bespoke clas-
sifiers (at significant cost), or using generic sen-
timent classifiers (which sacrifice performance).
The ability to rapidly construct sentiment classi-
fiers that are specifically tuned to the precise do-
main can significantly increase classifier perfor-
mance without imposing major additional costs.
Moving beyond sentiment, with these bespoke
classifiers it is in principle possible to track over
time any form of opinion that is reflected in lan-
guage. In a second study, the authors are (in col-
laboration with a team of social scientists) build-
ing cascades of bespoke classifiers to investigate
shifts in citizens? attitudes over time (as expressed
in social media) to a range of political and social
issues arising across the European Union.
Entity disambiguation. References to individ-
uals are often ambiguous. In the general case,
word sense disambiguation is most success-
fully performed by supervised-learning classifiers
(Ma`rquez et al, 2006), and the low cost of pro-
ducing classifiers using this framework makes this
approach practical for situations where we require
repeated high recall, high precision searches of
large data sets for a specific entity. As an example,
this approach is being employed in the EU attitu-
dinal survey study.
Repeated complex search. In situations where
a fixed but complex search needs to be performed
repeatedly over a relatively long period of time,
then a supervised-learning classifier can be ex-
pected both to produce the best results and to be
cost-effective in terms of the effort required to
train it. The authors have employed this approach
in a commercial environment (Lyra et al, 2012),
and the ability to train classifiers more quickly
with this framework reduces the cost still further
and makes this a practical approach in a wider
range of circumstances.
With regard to extension of the framework, we
have identified a number of avenues for expansion
and improvement that will significantly increase
its usefulness and applicability to real-world sce-
narios, and we have recently commenced an 18-
month research programme to formalise and ex-
tend the framework and its associated methodol-
ogy for use in social science research3.
Conclusions and Future Work
We describe an agile analysis framework built
around the DUALIST tool designed to support ef-
fective exploration of large twitter data sets by
social scientists. The functionality of DUAL-
IST has been extended to allow the scraping of
tweets through access to the Twitter API, collab-
orative construction of both gold standard data
sets and Na??ve Bayes? classifiers, an Information
Gain-based method for automatic discovery of
new search terms, and support for the construction
of classifier cascades. Further extensions currently
under development include grouping tweets into
threads conversations, and automatic clustering of
relevant tweets in order to discover subtopics un-
der discussion.
Acknowledgments
We are grateful to our collaborators at the Cen-
tre for the Analysis of social media, Jamie Bartlett
and Carl Miller for valuable contributions to this
work. We thank the anonymous reviewers for their
helpful comments. This work was partially sup-
ported by the Open Society Foundation.
3Towards a Social Media Science, funded by the UK
ESRC National Centre for Research Methods.
40
References
[Bermingham and Smeaton2011] Adam Bermingham
and Alan F Smeaton. 2011. On using Twitter to
monitor political sentiment and predict election
results. In Proceedings of the Workshop on Senti-
ment Analysis where AI meets Psychology (SAAIP),
IJCNLP 2011, pages 2?10.
[Bollen et al2011] Johan Bollen, Alberto Pepe, and
Huina Mao. 2011. Modeling public mood and emo-
tion: Twitter sentiment and socio-economic phe-
nomena. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media,
pages 450?453.
[Carvalho et al2011] Paula Carvalho, Lu??s Sarmento,
Jorge Teixeira, and Ma?rio J. Silva. 2011. Liars and
saviors in a sentiment annotated corpus of comments
to political debates. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers - Volume 2, pages 564?568, Stroudsburg, PA,
USA.
[Castillo et al2011] Carlos Castillo, Marcelo Mendoza,
and Barbara Poblete. 2011. Information credibility
on Twitter. In Proceedings of the 20th International
Conference on World wide web, pages 675?684.
[Diakopoulos and Shamma2010] Nicholas A Di-
akopoulos and David A Shamma. 2010. Charac-
terizing debate performance via aggregated Twitter
sentiment. In Proceedings of the 28th international
conference on Human factors in computing systems,
pages 1195?1198.
[Gayo-Avello2012] Daniel Gayo-Avello. 2012. I
wanted to predict elections with twitter and all i
got was this lousy paper a balanced survey on elec-
tion prediction using Twitter data. arXiv preprint
arXiv:1204.6441.
[Gonzalez-Bailon et al2010] Sandra Gonzalez-Bailon,
Rafael E Banchs, and Andreas Kaltenbrunner. 2010.
Emotional reactions and the pulse of public opin-
ion: Measuring the impact of political events on
the sentiment of online discussions. arXiv preprint
arXiv:1009.4019.
[Hopkins and King2010] Daniel J. Hopkins and Gary
King. 2010. A method of automated nonparametric
content analysis for social science. American Jour-
nal of Political Science, 54(1):229?247.
[Jungherr et al2012] Andreas Jungherr, Pascal Ju?rgens,
and Harald Schoen. 2012. Why the Pirate Party
won the German election of 2009 or the trouble
with predictions: A response to Tumasjan, Sprenger,
Sander, & Welpe. Social Science Computer Review,
30(2):229?234.
[Lyra et al2012] Matti Lyra, Daoud Clarke, Hamish
Morgan, Jeremy Reffin, and David Weir. 2012.
Challenges in applying machine learning to media
monitoring. In Proceedings of Thirty-second SGAI
International Conference on Artificial Intelligence
(AI-2012).
[Marchetti-Bowick and Chambers2012] Micol
Marchetti-Bowick and Nathanael Chambers.
2012. Learning for microblogs with distant su-
pervision: political forecasting with Twitter. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational
Linguistics, pages 603?612.
[Ma`rquez et al2006] Llu??s Ma`rquez, Gerard Escudero,
David Mart??nez, and German Rigau. 2006. Su-
pervised corpus-based methods for wsd. In Eneko
Agirre and Philip Edmonds, editors, Word Sense
Disambiguation, volume 33 of Text, Speech and
Language Technology, pages 167?216. Springer
Netherlands.
[Meraz and Papacharissi2013] Sharon Meraz and Zizi
Papacharissi. 2013. Networked gatekeeping and
networked framing on #egypt. The International
Journal of Press/Politics, 18(2):138?166.
[O?Connor et al2010] Brendan O?Connor, Ramnath
Balasubramanyan, Bryan R Routledge, and Noah A
Smith. 2010. From tweets to polls: Linking text
sentiment to public opinion time series. In Proceed-
ings of the International AAAI Conference on We-
blogs and Social Media, pages 122?129.
[Pang and Lee2008] Bo Pang and Lillian Lee. 2008.
Opinion mining and sentiment analysis. Founda-
tions and trends in Information Retrieval, 2(1-2):1?
135.
[Papacharissi and de Fatima Oliveira2012] Zizi Pa-
pacharissi and Maria de Fatima Oliveira. 2012.
Affective news and networked publics: the rhythms
of news storytelling on #egypt. Journal of Commu-
nication, 62(2):266?282.
[Sang and Bos2012] Erik Tjong Kim Sang and Johan
Bos. 2012. Predicting the 2011 dutch senate elec-
tion results with Twitter. Proceedings of the Euro-
pean Chapter of the Association for Computational
Linguistics 2012, page 53.
[Settles and Zhu2012] Burr Settles and Xiaojin Zhu.
2012. Behavioral factors in interactive training of
text classifiers. In Proceedings of the 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 563?567.
[Settles2011] Burr Settles. 2011. Closing the loop:
Fast, interactive semi-supervised annotation with
queries on features and instances. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 1467?1478.
[Tumasjan et al2010] Andranik Tumasjan, Timm O
Sprenger, Philipp G Sandner, and Isabell M Welpe.
2010. Predicting elections with Twitter: What 140
characters reveal about political sentiment. In Pro-
ceedings of the fourth international AAAI confer-
ence on weblogs and social media, pages 178?185.
41
[Tumasjan et al2011] Andranik Tumasjan, Timm O
Sprenger, Philipp G Sandner, and Isabell M Welpe.
2011. Election forecasts with Twitter how 140 char-
acters reflect the political landscape. Social Science
Computer Review, 29(4):402?418.
42
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 11?20,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Distributional Composition using Higher-Order Dependency Vectors
Julie Weeds, David Weir and Jeremy Reffin
Department of Informatics
University of Sussex
Brighton, BN1 9QH, UK
{J.E.Weeds, D.J.Weir, J.P.Reffin}@sussex.ac.uk
Abstract
This paper concerns how to apply compo-
sitional methods to vectors based on gram-
matical dependency relation vectors. We
demonstrate the potential of a novel ap-
proach which uses higher-order grammat-
ical dependency relations as features. We
apply the approach to adjective-noun com-
pounds with promising results in the pre-
diction of the vectors for (held-out) ob-
served phrases.
1 Introduction
Vector space models of semantics characterise the
meaning of a word in terms of distributional fea-
tures derived from word co-occurrences. The most
widely adopted basis for word co-occurrence is
proximity, i.e. that two words (or more generally
lexemes) are taken to co-occur when they occur
together within a certain sized window, or within
the same sentence, paragraph, or document. Lin
(1998), in contrast, took the syntactic relationship
between co-occurring words into account: the dis-
tributional features of a word are based on the
word?s grammatical dependents as found in a de-
pendency parsed corpus. For example, observing
that the word glass appears as the indirect object
of the verb fill, provides evidence that the word
glass has the distributional feature iobj:fill, where
iobj denotes the inverse indirect object grammati-
cal relation. The use of grammatical dependents as
word features has been exploited in the discovery
of tight semantic relations, such as synonymy and
hypernymy, where an evaluation against a gold
standard such as WordNet (Fellbaum, 1998) can
be made (Lin, 1998; Weeds and Weir, 2003; Cur-
ran, 2004).
Pado and Lapata (2007) took this further by
considering not just direct grammatical depen-
dents, but also including indirect dependents.
Thus, observing the sentence She filled her glass
slowly would provide evidence that the word glass
has the distributional feature iobj:advmod:slowly
where iobj:advmod captures the indirect depen-
dency relationship between glass and slowly in the
sentence.
Note that Pado and Lapata (2007) included
a basis mapping function that gave their frame-
work flexibility as to how to map paths such as
iobj:advmod:slowly onto the basis of the vector
space. Indeed, the instantiation of their framework
that they adopt in their experiments uses a ba-
sis mapping function that removes the dependency
path to leave just the word, so iobj:advmod:slowly
would be mapped to slowly.
In this paper, we are concerned with the prob-
lem of distributional semantic composition. We
show that the idea that the distributional seman-
tics of a word can be captured with higher-order
dependency relationships, provides the basis for
a simple approach to compositional distributional
semantics. While our approach is quite gen-
eral, dealing with arbitrarily high-order depen-
dency relationships, and the composition of ar-
bitrary phrases, in this paper we consider only
first and second order dependency relations, and
adjective-noun composition.
In Section 2, we illustrate our proposal by
showing how second order dependency relations
can play a role in computing the semantics of
adjective-noun composition. In Section 3 we de-
scribe a number of experiments that are intended
to evaluate the approach, with the results presented
in Section 4.
The basis for our evaluation follows Baroni and
11
Zamparelli (2010) and Guevara (2010). Typically,
compositional distributional semantic models can
be used to generate an (inferred) distributional
vector for a phrase from the (observed) distribu-
tional vectors of the phrase?s constituents. One
of the motivations for doing this is that the ob-
served distributional vectors for most phrases tend
to be very sparse, a consequence of the frequency
with which typical phrases occur in even large cor-
pora. However, there are phrases that occur suffi-
ciently frequently that a reasonable characterisa-
tion of their meaning can be captured with their
observed distributional vector. Such phrases can
be exploited in order to assess the quality of a
model of composition. This is achieved by mea-
suring the distributional similarity of the observed
and inferred distributional vectors for these high
frequency phrases.
The contributions of this paper are as follows.
We propose a novel approach to phrasal composi-
tion which uses higher order grammatical depen-
dency relations as features. We demonstrate its
potential in the context of adjective-noun compo-
sition by comparing (held-out) observed and in-
ferred phrasal vectors. Further, we compare dif-
ferent vector operations, different feature associa-
tion scores and investigate the effect of weighting
features before or after composition.
2 Composition with Higher-order
Dependencies
Consider the problem of adjective-noun compo-
sition. For example, what is the meaning of the
phrase small child? How does it relate to the
meanings of the lexemes small and child? Figure 1
shows a dependency analysis for the sentence The
very small wet child cried loudly. Tables 1 and
2 show the grammatical dependencies (with other
open-class words) for the lexemes small and child
which would be extracted from it.
the/D very/R small/J wet/J child/N cry/V loudly/R
amod
amod
advmod
det
nsubj advmod
Figure 1: Example Dependency Tree
From Table 1 we see what kinds of (higher-
order) dependency paths appear in the distribu-
tional features of adjectives such as small. Simi-
larly, Table 2 indicates this for nouns such as child.
1st-order advmod:very/R
amod:child
2nd-order amod:amod:wet/J
amod:nsubj:cry/V
3rd-order amod:nsubj:advmod:loudly/R
Table 1: Grammatical Dependencies of small
1st-order amod:wet/J
amod:small/J
nsubj:cry/V
2nd-order amod:advmod:very/R
nsubj:advmod:loudly/R
Table 2: Grammatical Dependencies of child
It is clear that with a conventional grammatical
dependency-based approach where only first or-
der dependencies for small and child would be
considered, there will be very little overlap be-
tween the features of nouns and adjectives because
quite different grammatical relations are used in
the two types of vectors, and correspondingly lex-
emes with different parts of speech appear at the
end of these paths.
However, as our example illustrates, it is possi-
ble to align the 2nd-order feature space of adjec-
tives with the 1st-order feature space of nouns. In
this example, we have evidence that children cry
and that small things cry. Consequently, in order
to compose an adjective with a noun, we would
want to align 2nd-order features of the adjective
with 1st-order features of the noun; this gives us a
prediction of the first order features of the noun in
the context of the adjective
1
.
This idea extends in a straightforward way be-
yond adjective-noun composition. For example, it
is possible to align the 3rd order features of ad-
jectives with 2nd order features of nouns, which is
something that would be useful if one wanted to
compose verbs with their arguments. These argu-
ments will include adjective-noun compounds and
therefore adjective-noun compounds require 2nd-
order features which can be aligned with the first
order features of the verbs. This is, however, not
1
Note that it would also be possible to align 2nd-order
features of the noun with 1st-order features of the adjective,
resulting in a prediction of the first order features of the ad-
jective in the context of the noun.
12
something that we will pursue further in this paper.
We now clarify how features vectors are aligned
and then composed. Suppose that the lexemes w
1
and w
2
which we wish to compose are connected
by relation r. Let w
1
be the head of the relation
and w
2
be the dependent. In our example, w
1
is
child, w
2
is small and r is amod. We first pro-
duce a reduced vector for w
2
which is designed
to lie in a comparable feature space as the vector
for w
1
. To do this we take the set of 2nd order
features of w
2
which start with the relation r? and
reduce them to first order features (by removing
the r? at the start of the path). So in our example,
we create a reduced vector for small where fea-
tures amod:nsubj:x for some token x are reduced
to nsubj:x, features amod:amod:x for some token
x are reduced to the feature amod:x, and features
amod:nsubj:advmod:x for some token x are re-
duced to nsubj:advmod:x. Once the vector for w
2
has been reduced, it can be composed with the vec-
tor for w
1
using standard vector operations.
In Section 3 we describe experiments that ex-
plore the effectiveness of this approach to distri-
butional composition by measuring the similarity
of composed vectors with observed vectors for a
set of frequently occurring adjective-noun pairs
(details given below). We evaluate a number of
instantiations of our approach, and in particular,
there are three aspects of the model where alter-
native solutions are available: the choice of which
vector composition operation to use; the choice of
how to weight dependency features; and the ques-
tion as to whether feature weighting should take
place before or after composition.
Vector composition operation. We consider
each of the following seven alternatives: pointwise
addition (add), pointwise multiplication (mult),
pointwise geometric mean
2
(gm), pointwise max-
imum (max), pointwise minimum (min), first ar-
gument (hd), second argument (dp). The latter
two operations simply return the first (respectively
second) of the input vectors.
Feature weighting. We consider three options.
Much work in this area has used positive pointwise
mutual information (PPMI) (Church and Hanks,
1989) to weight the features. However, PPMI is
known to over-emphasise low frequency events,
and as a result there has been a recent shift to-
wards using positive localised mutual information
2
The geometric mean of x and y is
?
(x ? y).
PPMI(x, y) =
{
I(x, y) if I(x, y) > 0
0 otherwise
where I(x, y) = log
P (x,y)
P (x).P (y)
PLMI(x, y) =
{
L(x, y) if L(x, y) > 0
0 otherwise
where L(x, y) = P (x, y).log(
P (x,y)
P (x).P (y)
PNPMI(x, y) =
{
N(x, y) if N(x, y) > 0
0 otherwise
where N(x, y) =
1
?log(P (y)
.log
P (x,y)
P (x).P (y)
Table 3: Feature Association Scores
(PLMI) (Scheible et al., 2013) and positive nor-
malised point wise mutual information (PNPMI)
(Bouma, 2009). For definitions, see Table 3.
Timing of feature weighting. We consider two
alternatives: we can weight features before com-
position so that the composition operation is ap-
plied to weighted vectors, or we can compose vec-
tors prior to feature weighting, in which case the
composition operation is applied to unweighted
vectors, and feature weighting is applied in the
context of making a similarity calculation. In other
work, the former order is often implied. For exam-
ple, Boleda et al. (2013) state that they use ?PMI
to weight the co-occurrence matrix?. However, if
we allow the second order, features which might
have a zero association score in the context of the
the individual lexemes, could be considered sig-
nificant in the context of the phrase.
3 Evaluation
Our experimental evaluation of the approach is
based on the assumption, which is commonly
made elsewhere, that where there is a reasonable
amount of corpus data available for a phrase, this
will generate a good estimate of the vector of the
phrase. It has been shown (Turney, 2012; Baroni
and Zamparelli, 2010) that such ?observed? vec-
tors are indeed reasonable for adjective-noun and
noun-noun compounds. Hence, in order to evalu-
ate the compositional models under consideration
here, we compare observed phrasal vectors with
inferred phrasal vectors, where the comparison is
made using the cosine measure. We note that it is
13
not possible to draw conclusions from the absolute
value of the cosine score since this would favour
models which always assign higher cosine scores.
Hence, we draw conclusions from the change in
cosine score with respect to a baseline within the
same model.
Methodology
For each noun and adjective which occur more
than a threshold number of times in a corpus, we
first extract conventional first order dependency
vectors. The features of these lexemes define the
semantic space, and feature probabilities (for use
in association scores) are calculated from this data.
Given a list of adjective-noun phrases, we ex-
tract first order vectors for the nouns and second
order vectors for the adjectives, which we refer to
as observed constituent vectors. We also extract
first order vectors for the nouns in the context of
the adjective, which we refer to as the observed
phrasal vector.
For each adjective-noun pair, we build bespoke
constituent vectors for the adjective and noun, in
which we remove all counts which arise from co-
occurrences with that specific adjective-noun pair.
It is these constituent vectors that are used as the
basis for inferring the vector for that particular
adjective-noun phrase.
Our rationale for this is as follows. Without this
modification, the observed constituent vectors will
contain co-occurrences which are due to the ob-
served adjective-noun vector co-occurrences. To
see why this is undesirable, suppose that one of the
adjective-noun phrases was small child. We take
the observed vector for small child to be what we
are calling the observed phrasal vector for child (in
the context of small). Suppose that when building
the observed phrasal vector, we observe the phrase
the small child cried. This will lead to a count for
the feature nsubj:cry in the observed phrasal vec-
tor for child.
But if we are not careful, this same phrase will
contribute to counts in the constituent vectors for
small and child, producing counts for the features
amod:nsubj:cry and nsubj:cry, in their respective
vectors. To see why these counts should not be in-
cluded when building the constituent vectors that
we compose to produce inferred vectors for the
adjective-noun phrase small child, consider the
case where all of the evidence for small things be-
ing things that can cry and children being things
that can crying comes from having observed the
phrase small children crying. Despite not having
learnt anything about the composition of small and
child in general, we would be able to infer the cry
feature for the phrase. An adequate model of com-
position should be able to infer this on the basis
that other small things have been seen to cry, and
that non-small children have been seen to cry.
Here, we compare the proposed approach,
based on higher order dependencies, with the
standard method of composing conventional first-
order dependency vectors. The vector operation,
hd provides a baseline for comparison which is
the same in both approaches. This baseline corre-
sponds to a composition model where the first or-
der dependencies of the phrase (i.e. the noun in the
context of the adjective) are taken to be the same
as the first order dependencies of the uncontextu-
alized noun. For example, if we have never seen
the phrase small child before, we would assume
that it means the same as the head word child.
We hypothesise that it is not possible to im-
prove on this baseline using traditional first-order
dependency relation vectors, since the vector for
the modifier does not contain features of the right
type, but that with the proposed approach, the in-
ferred vector for a phrase such as small child will
be closer than observed vector for child to the ob-
served vector for small child. We also ask the re-
lated question of whether our inferred vector for
small child is closer than the constituent vector for
small to the observed vector for small child. This
comparison is achieved through use of the vector
operation dp that ignores the vector for the head,
simply returning a first-order vector derived from
the dependent.
Experimental Settings
Our corpus is a mid-2011 dump of WikiPedia.
This has been part-of-speech tagged, lemmatised
and dependency parsed using the Malt Parser
(Nivre, 2004). All major grammatical dependency
relations involving open class parts of speech
(nsubj, dobj, iobj, conj, amod, advmod, nnmod)
have been extracted for all POS-tagged and lem-
matised nouns and adjectives occurring 100 or
more times. In past work with conventional de-
pendency relation vectors we found that using a
feature threshold of 100, weighting features with
PPMI and a cosine similarity score work well.
For experimental purposes, we have taken
14
spanish british african japanese
modern classical female natural
digital military medical musical
scientific free black white
heavy common small large
strong short long good
similar previous future original
former subsequent next possible
Table 4: Adjectives considered
32 of the most frequently occurring adjectives
(see Table 4). These adjectives include ones
which would generally be considered intersective
(e.g., female), subsective (e.g,, long) and non-
subsective/intensional (e.g., former) (Pustejovsky,
2013) . For all of these adjectives there are at least
100 adjective-noun phrases which occur at least
100 times in the corpus. We randomly selected
50 of the phrases for each adjective. Note that
our proposed method does not require any hyper
parameters to be set during training, nor does it
require a certain number of phrases per adjective.
For the purpose of these experiments we have a list
of 1600 adjective-noun phrases, all of which occur
at least 100 times in WikiPedia.
4 Results and Discussion
Tables 5 and 6 summarise the average cosines for
the proposed higher-order dependency approach
and the conventional first-order dependency ap-
proach, respectively. In each case, we consider
each combination of vector operation, feature as-
sociation score, and composition timing (i.e. be-
fore, or after, vector weighting).
Table 7 shows the average improvement over
the baseline (hd), for each combination of exper-
imental variables, when considering the proposed
higher-order dependency approach. Note that this
is an average of paired differences (and not the dif-
ference of the averages in Table 6). For brevity, we
omit the results for PNPMI here, since there do not
appear to be substantial differences between using
PPMI and PNPMI. To indicate statistical signifi-
cance, we show estimated standard errors in the
means. All differences are statistically significant
(under a paired t-test) except those marked ?.
From Table 5, we see that none of the com-
positional operations on conventional dependency
vectors are able to beat the baseline of selecting
the head vector (hd). This is independent of the
choice of association measure and the order in
which weighting and composition are carried out.
For the higher order dependency vectors (Tables
6 and 7), we note, in contrast, that some com-
positional operations produce large increases in
cosine score compared to the head vector alone
(hd). Table 7 examines the statistical significance
of these differences. We find that for the inter-
sective composition operations (mult, min, and
gm), performance is statistically superior to using
the head alone in all experimental conditions stud-
ied. By contrast, additive measures (add, max)
typically have no impact, or decrease performance
marginally relative to the head alone. An explana-
tion for these significant differences is that inter-
sective vector operations are able to encapsulate
the way that an adjective disambiguates and spe-
cialises the sense of the noun that it is modifying.
We also note that the alternative baseline, dp,
which estimates the features of a phrase to be the
aggregation of all things which are modified by
the adjective, performs significantly worse than
the standard baseline, hd, which estimates the fea-
tures of a phrase to be the features of the head
noun. This is consistent with the intuition that the
distributional vector for small child should more
similar to the vector for child than it is to the vec-
tor for the things that can be small.
Considering the different intersective opera-
tions, mult appears to be the best choice when
the feature association score is PPMI or PNPMI
and gm appears to be the best choice when the fea-
ture association score is PLMI.
Further, PLMI consistently gives all of the vec-
tor pairings higher cosine scores than PPMI. Since
PLMI assigns less weight to low frequency event
and more weight to high frequency events, this
suggests that all of the composition methods, in-
cluding the baseline (hd), do better at predicting
the high frequency co-occurrences. This is not sur-
prising as these will more likely have been seen
with the phrasal constituents in other contexts.
Our final observation, based on Table 6, is that
the best order in which to carry out weighting and
composition appears to depend on the choice of
feature association score. In general, it appears
better to weight the features and then compose
vectors. This is always true when using PNPMI
or PLMI. However, using PPMI, the highest per-
formance is achieved by composing the raw vec-
tors using multiplication and then weighing the
15
weight:compose compose:weight
PPMI PNPMI PLMI PPMI PNPMI PLMI
x? s x? s x? s x? s x? s x? s
add 0.12 (0.06) 0.13 (0.05) 0.15 (0.16) 0.11 (0.05) 0.12 (0.06) 0.22 (0.20)
max 0.12 (0.06) 0.13 (0.05) 0.15 (0.16) 0.11 (0.05) 0.12 (0.06) 0.22 (0.20)
mult 0.06 (0.05) 0.06 (0.06) 0.06 (0.11) 0.07 (0.05) 0.07 (0.12) 0.07 (0.05)
min 0.05 (0.05) 0.06 (0.05) 0.04 (0.09) 0.05 (0.04) 0.05 (0.04) 0.04 (0.08)
gm 0.06 (0.05) 0.06 (0.05) 0.07 (0.11) 0.05 (0.04) 0.06 (0.04) 0.08 (0.11)
hd 0.13 (0.07) 0.15 (0.07) 0.28 (0.22) 0.13 (0.07) 0.15 (0.07) 0.28 (0.22)
Table 5: Means and Standard Deviations for Cosines Between Observed and Predicted Vectors for Con-
ventional First-Order Dependency Based Approach.
weight:compose compose:weight
PPMI PNPMI PLMI PPMI PNPMI PLMI
x? s x? s x? s x? s x? s x? s
add 0.14 (0.06) 0.16 (0.06) 0.29 (0.21) 0.10 (0.04) 0.12 (0.05) 0.29 (0.22)
max 0.10 (0.04) 0.11 (0.04) 0.27 (0.21) 0.10 (0.04) 0.11 (0.04) 0.26 (0.21)
mult 0.30 (0.12) 0.33 (0.12) 0.40 (0.29) 0.34 (0.10) 0.32 (0.10) 0.32 (0.27)
min 0.26 (0.11) 0.27 (0.11) 0.40 (0.24) 0.24 (0.10) 0.25 (0.10) 0.37 (0.23)
gm 0.27 (0.11) 0.29 (0.11) 0.46 (0.20) 0.26 (0.10) 0.27 (0.10) 0.44 (0.22)
dp 0.10 (0.05) 0.10 (0.05) 0.20 (0.20) 0.10 (0.05) 0.10 (0.05) 0.20 (0.20)
hd 0.13 (0.07) 0.15 (0.07) 0.28 (0.22) 0.13 (0.07) 0.15 (0.07) 0.28 (0.22)
Table 6: Means and Standard Deviations for Cosines Between Observed and Predicted Vectors for Pro-
posed Higher-Order Dependency Based Approach
remaining features. This can be explained by
considering the recall and precision of the com-
posed vector?s prediction of the observed vec-
tor. If we compose using gm before weighting
vectors, we increase the recall of the prediction,
but decrease precision. Whether we use PPMI,
PNPMI or PLMI, recall of features increases from
88.8% to 99.5% and precision drops from 5.5% to
4.8%. If we compose using mult before weight-
ing vectors, contrary to expectation, recall de-
creases and precision increases. Whether we use
PPMI, PNPMI or PLMI, recall of features de-
creases from 88.8% to 59.4% but precision in-
creases from 5.5% to 18.9%. Hence, multiplica-
tion of the raw vectors is causing a lot of potential
shared features to be ?lost? when the weighting
is subsequently carried out (since multiplication
stretches out the value space). This leads to an
increase in cosines when PPMI is used for weight-
ing, and a decrease in cosines when PLMI is used.
Hence, it appears that the features being removed
by multiplying the raw vectors before weighting
must be low frequency co-occurrences, which are
not observed with the phrase.
5 Related Work
In this work, we bring together ideas from sev-
eral different strands of distributional semantics:
incorporating syntactic information into the distri-
butional representation of a lexeme; representing
phrasal meaning by creating distributional repre-
sentations through composition; and representing
word meaning in context by modifying the distri-
butional representation of a word.
The use of syntactic structure in distributional
representations is not new. Two of the earliest
proponents of distributional semantics, Lin (1998)
and Lee (1999) used features based on first order
dependency relations between words in their dis-
tributional representations. More recently, Pado
and Lapata (2007) propose a semantic space based
on dependency paths. This model outperformed
traditional word-based models which do not take
syntax into account in a synonymy relation detec-
tion task and a prevalent sense acquisition task.
The problem of representing phrasal meaning
has traditionally been tackled by taking vector rep-
resentations for words (Turney and Pantel, 2010)
and combining them using some function to pro-
16
weight:compose compose:weight
PPMI PLMI PPMI PLMI
x? s
x?
x? s
x?
x? s
x?
x? s
x?
add 0.01 (0.001) ?0.004 (0.003) -0.03 (0.001) ?0.006 (0.004)
max -0.03 (0.001) -0.01 (0.003) -0.04 (0.001) -0.02 (0.003)
mult 0.16 (0.002) 0.11 (0.006) 0.21 (0.002) 0.03 (0.006)
min 0.13 (0.001) 0.11 (0.007) 0.10 (0.001) 0.09 (0.007)
gm 0.14 (0.001) 0.18 (0.005) 0.12 (0.001) 0.16 (0.005)
dp -0.03 (0.002) -0.09 (0.007) -0.04 (0.002) -0.09 (0.007)
Table 7: Means and Standard Errors for Increases in Cosine with respect to the hd Baseline for Proposed
Higher-Order Dependency Based Approach. All differences statistically significant (under a paired t-
test) except those marked ?.
duce a data structure that represents the phrase
or sentence. Mitchell and Lapata (2008, 2010)
found that simple additive and multiplicative func-
tions applied to proximity-based vector represen-
tations were no less effective than more com-
plex functions when performance was assessed
against human similarity judgements of simple
paired phrases.
The simple functions evaluated by Mitchell and
Lapata (2008) are generally acknowledged to have
serious theoretical limitations in their treatment
of composition. How can a commutative func-
tion such as multiplication or addition provide dif-
ferent interpretations for different word orderings
such as window glass and glass window? The
majority of attempts to rectify this have offered
a more complex, non-commutative function ?
such as weighted addition ? or taken the view
that some or all words are no longer simple vec-
tors. For example, in the work of Baroni and
Zamparelli (2010) and Guevara (2010), an adjec-
tive is viewed as a modifying function and rep-
resented by a matrix. Coecke et al. (2011) and
Grefenstette et al. (2013) also incorporate the no-
tion of function application from formal seman-
tics. They derived function application from syn-
tactic structure, representing functions as tensors
and arguments as vectors. The MV-RNN model
of Socher et al. (2012) broadened the Baroni and
Zamparelli (2010) approach; all words, regardless
of part-of-speech, were modelled with both a vec-
tor and a matrix. This approach also shared fea-
tures with Coecke et al. (2011) in using syntax
to guide the order of phrasal composition. These
higher order structures are typically learnt or in-
duced using a supervised machine learning tech-
nique. For example, Baroni and Zamparelli (2010)
learnt their adjectival matrixes by performing re-
gression analysis over pairs of observed nouns and
adjective-noun phrases. As a consequence of the
computational expense of the machine learning
techniques involved, implementations of these ap-
proaches typically require a considerable amount
of dimensionality reduction.
A long-standing topic in distributional seman-
tics has been the modification of a canonical repre-
sentation of a lexeme?s meaning to reflect the con-
text in which it is found. Typically, a canonical
vector for a lexeme is estimated from all corpus
occurrences and the vector then modified to reflect
the instance context (Lund and Burgess, 1996;
Erk and Pad?o, 2008; Mitchell and Lapata, 2008;
Thater et al., 2009; Thater et al., 2010; Thater et
al., 2011; Van de Cruys et al., 2011; Erk, 2012).
As described in Mitchell and Lapata (2008, 2010),
lexeme vectors have typically been modified using
simple additive and multiplicative compositional
functions. Other approaches, however, share with
our proposal the use of syntax to drive modifica-
tion of the distributional representation (Erk and
Pad?o, 2008; Thater et al., 2009; Thater et al., 2010;
Thater et al., 2011). For example, in the SVS rep-
resentation of Erk and Pad?o (2008), a word was
represented by a set of vectors: one which en-
codes its lexical meaning in terms of distribution-
ally similar words
3
, and one which encodes the
selectional preferences of each grammatical rela-
tion it supports. A word?s meaning vector was up-
dated in the context of another word by combining
it with the appropriate selectional preferences vec-
3
These are referred to as second-order vectors using
the terminology of Grefenstette (1994) and Sch?utze (1998).
However, this refers to a second-order affinity between the
words and is not related to the use of grammatical depen-
dency relations.
17
tor of the contextualising word.
Turney (2012) offered a model of phrasal level
similarity which combines assessments of word-
level semantic relations. This work used two
different word-level distributional representations
to encapsulate two types of similarity. Distribu-
tional similarity calculated from proximity-based
features was used to estimate domain similarity
and distributional similarity calculated from syn-
tactic pattern based features is used to estimate
functional similarity. The similarity of a pair of
compound noun phrases was computed as a func-
tion of the similarities of the components. Cru-
cially different from other models of phrasal level
similarity, it does not attempt to derive modified
vectors for phrases or words in context.
6 Conclusions and Further Work
Vectors based on grammatical dependency rela-
tions are known to be useful in the discovery of
tight semantic relations, such as synonymy and
hypernymy, between lexemes (Lin, 1998; Weeds
and Weir, 2003; Curran, 2004). It would be use-
ful to be able to extend these methods to deter-
mine similarity between phrases (of potentially
different lengths). However, conventional ap-
proaches to composition, which have been ap-
plied to proximity-based vectors, cannot sensibly
be used on vectors that are based on grammatical
dependency relations.
In our approach, we consider the vector for a
phrase to be the vector for the head lexeme in
the context of the other phrasal constituents. Like
Pado and Lapata (2007), we extend the concept
of a grammatical dependency relation feature to
include dependency relation paths which incor-
porate higher-order dependencies between words.
We have shown how it is possible to align the de-
pendency path features for words of different syn-
tactic types, and thus produce composed vectors
which predict the features of one constituent in the
context of the other constituent.
In our experiments with AN compounds, we
have shown that these predicted vectors are closer
than the head constituent?s vector to the observed
phrasal vector. We have shown this is true even
when the observed phrase is in fact unobserved,
i.e. when its co-occurrences do not contribute to
the constituents? vectors. Consistent with work us-
ing proximity-based vectors, we have found that
intersective operations perform substantially bet-
ter than additive operations. This can be under-
stood by viewing the intersective operations as en-
capsulating the way that adjectives can specialise
the meaning of the nouns that they modify.
We have investigated the interaction between
the vector operation used for composition, the fea-
ture association score and the timing of applying
feature weights. We have found that multiplication
works best if using PPMI to weight features, but
that geometric mean is better if using the increas-
ingly popular PLMI weighting measure. Whilst
applying an intersective composition operation be-
fore applying feature weighting does allow more
features to be retained in the predicted vector (it
is possible to achieve 99.5% recall), in general,
this does not correspond with an increase in co-
sine scores. In general, the corresponding drop in
precision (i.e., the over-prediction of unobserved
features) causes the cosine to decrease. The one
exception to this is using multiplication with the
PPMI feature weighting score. Here we actually
see a drop in recall, and an increase in precision
due to the nature of multiplication and PPMI.
One assumption that has been made throughout
the work, is that the observed phrasal vector pro-
vides a good estimate of the distributional repre-
sentation of the phrase and, consequently, the best
composition method is the one which returns the
most similar prediction. However, in general, we
notice that while the recall of the compositional
methods is good, the precision is very low. Lack of
precision may be due to the prevalence of plausi-
ble, but unobserved, co-occurrences of the phrase.
Consequently, this introduces uncertainty into the
conclusions which can be drawn from a study such
as this. Further work is required to develop effec-
tive intrinsic and extrinsic evaluations of models
of composition.
A further interesting area of study is whether
distributional models that include higher-order
grammatical dependencies can tell us more about
the lexical semantics of a word than the conven-
tional first-order models, for example by distin-
guishing semantic relations such as synonymy,
antonymy, hypernymy and co-hyponymy.
Acknowledgements
This work was funded by UK EPSRC project
EP/IO37458/1 ?A Unified Model of Composi-
tional and Distributional Compositional Seman-
tics: Theory and Applications?.
18
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing.
Gemma Boleda, Marco Baroni, The Nghia Pham, and
Louise McNally. 2013. Intensionality was only al-
leged: On adjective-noun composition in distribu-
tional semantics. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013) ? Long Papers, pages 35?46, Pots-
dam, Germany, March. Association for Computa-
tional Linguistics.
Gerlof Bouma. 2009. Normalised (point wise) mu-
tual information in collocation extraction, from form
to meaning: Processing texts automatically. In Pro-
ceedings of the Biennial International Conference of
the German Society for Computational Linguistics
and Language Technology.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In Proceedings of the 27th Annual Meeting
on Association for Computational Linguistics, ACL
?89, pages 76?83, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2011. Mathematical foundations for a com-
positional distributed model of meaning. Linguistic
Analysis, 36(1-4):345?384.
James Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Katrin Erk and Sebastian Pad?o. 2008. A structured
vector space model for word meaning in context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897?906, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6(10):635?653.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for compo-
sitional distributional semantics. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
Gregory Grefenstette. 1994. Corpus-derived first, sec-
ond and third-order word affinities. In Proceedings
of Euralex 1994.
Emiliano Guevara. 2010. A Regression Model of
Adjective-Noun Compositionality in Distributional
Semantics. In Proceedings of the ACL GEMS Work-
shop, pages 33?37.
Lillian Lee. 1999. Measures of distributional simi-
larity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pages 25?32, College Park, Maryland, USA, June.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistings
(COLING 1998).
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
mentation, and Computers, 28:203?208.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio,
June. Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Proceedings of the ACL
Workshop on Incremental Parsing, pages 50?57.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
James Pustejovsky. 2013. Inference patterns with in-
tensional adjectives. In Proceedings of the IWCS
Workshop on Interoperable Semantic Annotation,
Potsdam,Germany, March. Association for Compu-
tational Linguistics.
Silke Scheible, Sabine Schulte im Walde, and Sylvia
Springorum. 2013. Uncovering distributional dif-
ferences between synonyms and antonyms in a word
space model. In Proceedings of the International
Joint Conference on Natural Language Processing,
pages 489?497, Nagoya, Japan.
Heinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44?47, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
19
Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 948?957,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Stefan Thater, Hagen Frstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP 2011).
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1012?1022, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 81?88, Sapporo,
Japan.
20
