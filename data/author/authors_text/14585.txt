Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 546?554,
Beijing, August 2010
Learning to Predict Readability using Diverse Linguistic Features
Rohit J. Kate1 Xiaoqiang Luo2 Siddharth Patwardhan2 Martin Franz2
Radu Florian2 Raymond J. Mooney1 Salim Roukos2 Chris Welty2
1Department of Computer Science
The University of Texas at Austin
{rjkate,mooney}@cs.utexas.edu
2IBM Watson Research Center
{xiaoluo,spatward,franzm,raduf,roukos,welty}@us.ibm.com
Abstract
In this paper we consider the problem of
building a system to predict readability
of natural-language documents. Our sys-
tem is trained using diverse features based
on syntax and language models which are
generally indicative of readability. The
experimental results on a dataset of docu-
ments from a mix of genres show that the
predictions of the learned system are more
accurate than the predictions of naive hu-
man judges when compared against the
predictions of linguistically-trained expert
human judges. The experiments also com-
pare the performances of different learn-
ing algorithms and different types of fea-
ture sets when used for predicting read-
ability.
1 Introduction
An important aspect of a document is whether it
is easily processed and understood by a human
reader as intended by its writer, this is termed
as the document?s readability. Readability in-
volves many aspects including grammaticality,
conciseness, clarity, and lack of ambiguity. Teach-
ers, journalists, editors, and other professionals
routinely make judgements on the readability of
documents. We explore the task of learning to
automatically judge the readability of natural-
language documents.
In a variety of applications it would be useful to
be able to automate readability judgements. For
example, the results of a web-search can be or-
dered taking into account the readability of the
retrieved documents thus improving user satisfac-
tion. Readability judgements can also be used
for automatically grading essays, selecting in-
structional reading materials, etc. If documents
are generated by machines, such as summariza-
tion or machine translation systems, then they are
prone to be less readable. In such cases, a read-
ability measure can be used to automatically fil-
ter out documents which have poor readability.
Even when the intended consumers of text are
machines, for example, information extraction or
knowledge extraction systems, a readability mea-
sure can be used to filter out documents of poor
readability so that the machine readers will not ex-
tract incorrect information because of ambiguity
or lack of clarity in the documents.
As part of the DARPA Machine Reading Pro-
gram (MRP), an evaluation was designed and con-
ducted for the task of rating documents for read-
ability. In this evaluation, 540 documents were
rated for readability by both experts and novice
human subjects. Systems were evaluated based on
whether they were able to match expert readabil-
ity ratings better than novice raters. Our system
learns to match expert readability ratings by em-
ploying regression over a set of diverse linguistic
features that were deemed potentially relevant to
readability. Our results demonstrate that a rich
combination of features from syntactic parsers,
language models, as well as lexical statistics all
contribute to accurately predicting expert human
readability judgements. We have also considered
the effect of different genres in predicting read-
ability and how the genre-specific language mod-
els can be exploited to improve the readability pre-
dictions.
546
2 Related Work
There is a significant amount of published work
on a related problem: predicting the reading diffi-
culty of documents, typically, as the school grade-
level of the reader from grade 1 to 12. Some early
methods measure simple characteristics of docu-
ments like average sentence length, average num-
ber of syllables per word, etc. and combine them
using a linear formula to predict the grade level of
a document, for example FOG (Gunning, 1952),
SMOG (McLaughlin, 1969) and Flesh-Kincaid
(Kincaid et al, 1975) metrics. These methods
do not take into account the content of the doc-
uments. Some later methods use pre-determined
lists of words to determine the grade level of a
document, for example the Lexile measure (Sten-
ner et al, 1988), the Fry Short Passage measure
(Fry, 1990) and the Revised Dale-Chall formula
(Chall and Dale, 1995). The word lists these
methods use may be thought of as very simple
language models. More recently, language mod-
els have been used for predicting the grade level
of documents. Si and Callan (2001) and Collins-
Thompson and Callan (2004) train unigram lan-
guage models to predict grade levels of docu-
ments. In addition to language models, Heilman
et al (2007) and Schwarm and Ostendorf (2005)
also use some syntactic features to estimate the
grade level of texts.
Pitler and Nenkova (2008) consider a differ-
ent task of predicting text quality for an educated
adult audience. Their system predicts readabil-
ity of texts from Wall Street Journal using lex-
ical, syntactic and discourse features. Kanungo
and Orr (2009) consider the task of predicting
readability of web summary snippets produced by
search engines. Using simple surface level fea-
tures like the number of characters and syllables
per word, capitalization, punctuation, ellipses etc.
they train a regression model to predict readability
values.
Our work differs from this previous research in
several ways. Firstly, the task we have consid-
ered is different, we predict the readability of gen-
eral documents, not their grade level. The doc-
uments in our data are also not from any single
domain, genre or reader group, which makes our
task more general. The data includes human writ-
ten as well as machine generated documents. The
task and the data has been set this way because it
is aimed at filtering out documents of poor quality
for later processing, like for extracting machine-
processable knowledge from them. Extracting
knowledge from openly found text, such as from
the internet, is becoming popular but the quality
of text found ?in the wild?, like found through
searching the internet, vary considerably in qual-
ity and genre. If the text is of poor readability then
it is likely to lead to extraction errors and more
problems downstream. If the readers are going
to be humans instead of machines, then also it is
best to filter out poorly written documents. Hence
identifying readability of general text documents
coming from various sources and genres is an im-
portant task. We are not aware of any other work
which has considered such a task.
Secondly, we note that all of the above ap-
proaches that use language models train a lan-
guage model for each difficulty level using the
training data for that level. However, since the
amount of training data annotated with levels
is limited, they can not train higher-order lan-
guage models, and most just use unigram models.
In contrast, we employ more powerful language
models trained on large quantities of generic text
(which is not from the training data for readabil-
ity) and use various features obtained from these
language models to predict readability. Thirdly,
we use a more sophisticated combination of lin-
guistic features derived from various syntactic
parsers and language models than any previous
work. We also present ablation results for differ-
ent sets of features. Fourthly, given that the doc-
uments in our data are not from a particular genre
but from a mix of genres, we also train genre-
specific language models and show that including
these as features improves readability predictions.
Finally, we also show comparison between var-
ious machine learning algorithms for predicting
readability, none of the previous work compared
learning algorithms.
3 Readability Data
The readability data was collected and re-
leased by LDC. The documents were collected
547
from the following diverse sources or genres:
newswire/newspaper text, weblogs, newsgroup
posts, manual transcripts, machine translation out-
put, closed-caption transcripts and Wikipedia arti-
cles. Documents for newswire, machine transla-
tion and closed captioned genres were collected
automatically by first forming a candidate pool
from a single collection stream and then randomly
selecting documents. Documents for weblogs,
newsgroups and manual transcripts were also col-
lected in the same way but were then reviewed
by humans to make sure they were not simply
spam articles or something objectionable. The
Wikipedia articles were collected manually, by
searching through a data archive or the live web,
using keyword and other search techniques. Note
that the information about genres of the docu-
ments is not available during testing and hence
was not used when training our readability model.
A total of 540 documents were collected in this
way which were uniformly distributed across the
seven genres. Each document was then judged
for its readability by eight expert human judges.
These expert judges are native English speakers
who are language professionals and who have
specialized training in linguistic analysis and an-
notation, including the machine translation post-
editing task. Each document was also judged for
its readability by six to ten naive human judges.
These non-expert (naive) judges are native En-
glish speakers who are not language professionals
(e.g. editors, writers, English teachers, linguistic
annotators, etc.) and have no specialized language
analysis or linguistic annotation training. Both ex-
pert and naive judges provided readability judg-
ments using a customized web interface and gave
a rating on a 5-point scale to indicate how readable
the passage is (where 1 is lowest and 5 is highest
readability) where readability is defined as a sub-
jective judgment of how easily a reader can extract
the information the writer or speaker intended to
convey.
4 Readability Model
We want to answer the question whether a
machine can accurately estimate readability as
judged by a human. Therefore, we built a
machine-learning system that predicts the read-
ability of documents by training on expert hu-
man judgements of readability. The evaluation
was then designed to compare how well machine
and naive human judges predict expert human
judgements. In order to make the machine?s pre-
dicted score comparable to a human judge?s score
(details about our evaluation metrics are in Sec-
tion 6.1), we also restricted the machine scores to
integers. Hence, the task is to predict an integer
score from 1 to 5 that measures the readability of
the document.
This task could be modeled as a multi-class
classification problem treating each integer score
as a separate class, as done in some of the previ-
ous work (Si and Callan, 2001; Collins-Thompson
and Callan, 2004). However, since the classes
are numerical and not unrelated (for example, the
score 2 is in between scores 1 and 3), we de-
cided to model the task as a regression problem
and then round the predicted score to obtain the
closest integer value. Preliminary results verified
that regression performed better than classifica-
tion. Heilman et al (2008) also found that it
is better to treat the readability scores as ordinal
than as nominal. We take the average of the ex-
pert judge scores for each document as its gold-
standard score. Regression was also used by Ka-
nungo and Orr (2009), although their evaluation
did not constrain machine scores to be integers.
We tested several regression algorithms avail-
able in the Weka1 machine learning package, and
in Section 6.2 we report results for several which
performed best. The next section describes the
numerically-valued features that we used as input
for regression.
5 Features for Predicting Readability
Good input features are critical to the success of
any regression algorithm. We used three main cat-
egories of features to predict readability: syntac-
tic features, language-model features, and lexical
features, as described below.
5.1 Features Based on Syntax
Many times, a document is found to be unreadable
due to unusual linguistic constructs or ungram-
1http://www.cs.waikato.ac.nz/ml/weka/
548
matical language that tend to manifest themselves
in the syntactic properties of the text. There-
fore, syntactic features have been previously used
(Bernth, 1997) to gauge the ?clarity? of written
text, with the goal of helping writers improve their
writing skills. Here too, we use several features
based on syntactic analyses. Syntactic analyses
are obtained from the Sundance shallow parser
(Riloff and Phillips, 2004) and from the English
Slot Grammar (ESG) (McCord, 1989).
Sundance features: The Sundance system is a
rule-based system that performs a shallow syntac-
tic analysis of text. We expect that this analysis
over readable text would be ?well-formed?, adher-
ing to grammatical rules of the English language.
Deviations from these rules can be indications of
unreadable text. We attempt to capture such de-
viations from grammatical rules through the fol-
lowing Sundance features computed for each text
document: proportion of sentences with no verb
phrases, average number of clauses per sentence,
average sentence length in tokens, average num-
ber of noun phrases per sentence, average number
of verb phrases per sentence, average number of
prepositional phrases per sentence, average num-
ber of phrases (all types) per sentence and average
number of phrases (all types) per clause.
ESG features: ESG uses slot grammar rules to
perform a deeper linguistic analysis of sentences
than the Sundance system. ESG may consider
several different interpretations of a sentence, be-
fore deciding to choose one over the other inter-
pretations. Sometimes ESG?s grammar rules fail
to produce a single complete interpretation of a
sentence, in which case it generates partial parses.
This typically happens in cases when sentences
are ungrammatical, and possibly, less readable.
Thus, we use the proportion of such incomplete
parses within a document as a readability feature.
In case of extremely short documents, this propor-
tion of incomplete parses can be misleading. To
account for such short documents, we introduce
a variation of the above incomplete parse feature,
by weighting it with a log factor as was done in
(Riloff, 1996; Thelen and Riloff, 2002).
We also experimented with some other syn-
tactic features such as average sentence parse
scores from Stanford parser and an in-house maxi-
mum entropy statistical parer, average constituent
scores etc., however, they slightly degraded the
performance in combination with the rest of the
features and hence we did not include them in
the final set. One possible explanation could be
that averaging diminishes the effect of low scores
caused by ungrammaticality.
5.2 Features Based on Language Models
A probabilistic language model provides a predic-
tion of how likely a given sentence was generated
by the same underlying process that generated a
corpus of training documents. In addition to a
general n-gram language model trained on a large
body of text, we also exploit language models
trained to recognize specific ?genres? of text. If a
document is translated by a machine, or casually
produced by humans for a weblog or newsgroup,
it exhibits a character that is distinct from docu-
ments that go through a dedicated editing process
(e.g., newswire and Wikipedia articles). Below
we describe features based on generic as well as
genre-specific language models.
Normalized document probability: One obvi-
ous proxy for readability is the score assigned to
a document by a generic language model (LM).
Since the language model is trained on well-
written English text, it penalizes documents de-
viating from the statistics collected from the LM
training documents. Due to variable document
lengths, we normalize the document-level LM
score by the number of words and compute the
normalized document probability NP (D) for a
document D as follows:
NP (D) =
(
P (D|M)
) 1
|D| , (1)
where M is a general-purpose language model
trained on clean English text, and |D| is the num-
ber of words in the document D.
Perplexities from genre-specific language mod-
els: The usefulness of LM-based features in
categorizing text (McCallum and Nigam, 1998;
Yang and Liu, 1999) and evaluating readability
(Collins-Thompson and Callan, 2004; Heilman
et al, 2007) has been investigated in previous
work. In our experiments, however, since doc-
uments were acquired through several different
channels, such as machine translation or web logs,
549
we also build models that try to predict the genre
of a document. Since the genre information for
many English documents is readily available, we
trained a series of genre-specific 5-gram LMs us-
ing the modified Kneser-Ney smoothing (Kneser
and Ney, 1995; Stanley and Goodman, 1996). Ta-
ble 1 contains a list of a base LM and genre-
specific LMs.
Given a document D consisting of tokenized
word sequence {wi : i = 1, 2, ? ? ? , |D|}, its per-
plexity L(D|Mj) with respect to a LM Mj is
computed as:
L(D|Mj) = e
(
? 1|D|
P|D|
i=1 logP (wi|hi;Mj)
)
, (2)
where |D| is the number of words in D and hi are
the history words for wi, and P (wi|hi;Mj) is the
probability Mj assigns to wi, when it follows the
history words hi.
Posterior perplexities from genre-specific lan-
guagemodels: While perplexities computed from
genre-specific LMs reflect the absolute probabil-
ity that a document was generated by a specific
model, a model?s relative probability compared to
other models may be a more useful feature. To this
end, we also compute the posterior perplexity de-
fined as follows. Let D be a document, {Mi}Gi=1
be G genre-specific LMs, and L(D|Mi) be the
perplexity of the document D with respect to Mi,
then the posterior perplexity, R(Mi|D), is de-
fined as:
R(Mi|D) =
L(D|Mi)?G
j=1 L(D|Mj)
. (3)
We use the term ?posterior? because if a uni-
form prior is adopted for {Mi}Gi=1,R(Mi|D) can
be interpreted as the posterior probability of the
genre LM Mi given the document D.
5.3 Lexical Features
The final set of features involve various lexical
statistics as described below.
Out-of-vocabulary (OOV) rates: We conjecture
that documents containing typographical errors
(e.g., for closed-caption and web log documents)
may receive low readability ratings. Therefore,
we compute the OOV rates of a document with re-
spect to the various LMs shown in Table 1. Since
modern LMs often have a very large vocabulary,
to get meaningful OOV rates, we truncate the vo-
cabularies to the top (i.e., most frequent) 3000
words. For the purpose of OOV computation, a
document D is treated as a sequence of tokenized
words {wi : i = 1, 2, ? ? ? , |D|}. Its OOV rate
with respect to a (truncated) vocabulary V is then:
OOV (D|V) =
?D
i=1 I(wi /? V)
|D| , (4)
where I(wi /? V) is an indicator function taking
value 1 if wi is not in V , and 0 otherwise.
Ratio of function words: A characteristic of doc-
uments generated by foreign speakers and ma-
chine translation is a failure to produce certain
function words, such as ?the,? or ?of.? So we pre-
define a small set of function words (mainly En-
glish articles and frequent prepositions) and com-
pute the ratio of function words over the total
number words in a document:
RF (D) =
?D
i=1 I(wi ? F)
|D| , (5)
where I(wi ? F) is 1 ifwi is in the set of function
words F , and 0 otherwise.
Ratio of pronouns: Many foreign languages that
are source languages of machine-translated docu-
ments are pronoun-drop languages, such as Ara-
bic, Chinese, and romance languages. We conjec-
ture that the pronoun ratio may be a good indica-
tor whether a document is translated by machine
or produced by humans, and for each document,
we first run a POS tagger, and then compute the
ratio of pronouns over the number of words in the
document:
RP (D) =
?D
i=1 I(POS(wi) ? P)
|D| , (6)
where I(POS(wi) ? F) is 1 if the POS tag of wi
is in the set of pronouns, P , and 0 otherwise.
Fraction of known words: This feature measures
the fraction of words in a document that occur
either in an English dictionary or a gazetteer of
names of people and locations.
6 Experiments
This section describes the evaluation methodol-
ogy and metrics and presents and discusses our
550
Genre Training Size(M tokens) Data Sources
base 5136.8 mostly LDC?s GigaWord set
NW 143.2 newswire subset of base
NG 218.6 newsgroup subset of base
WL 18.5 weblog subset of base
BC 1.6 broadcast conversation subset of base
BN 1.1 broadcast news subset of base
wikipedia 2264.6 Wikipedia text
CC 0.1 closed caption
ZhEn 79.6 output of Chinese to English Machine Translation
ArEn 126.8 output of Arabic to English Machine Translation
Table 1: Genre-specific LMs: the second column contains the number of tokens in LM training data (in million tokens).
experimental results. The results of the official
evaluation task are also reported.
6.1 Evaluation Metric
The evaluation process for the DARPAMRP read-
ability test was designed by the evaluation team
led by SAIC. In order to compare a machine?s
predicted readability score to those assigned by
the expert judges, the Pearson correlation coef-
ficient was computed. The mean of the expert-
judge scores was taken as the gold-standard score
for a document.
To determine whether the machine predicts
scores closer to the expert judges? scores than
what an average naive judge would predict, a
sampling distribution representing the underlying
novice performance was computed. This was ob-
tained by choosing a random naive judge for every
document, calculating the Pearson correlation co-
efficient with the expert gold-standard scores and
then repeating this procedure a sufficient number
of times (5000). The upper critical value was set
at 97.5% confidence, meaning that if the machine
performs better than the upper critical value then
we reject the null hypothesis that machine scores
and naive scores come from the same distribution
and conclude that the machine performs signifi-
cantly better than naive judges in matching the ex-
pert judges.
6.2 Results and Discussion
We evaluated our readability system on the dataset
of 390 documents which was released earlier dur-
ing the training phase of the evaluation task. We
Algorithm Correlation
Bagged Decision Trees 0.8173
Decision Trees 0.7260
Linear Regression 0.7984
SVM Regression 0.7915
Gaussian Process Regression 0.7562
Naive Judges
Upper Critical Value 0.7015
Distribution Mean 0.6517
Baselines
Uniform Random 0.0157
Proportional Random -0.0834
Table 2: Comparing different algorithms on the readability
task using 13-fold cross-validation on the 390 documents us-
ing all the features. Exceeding the upper critical value of the
naive judges? distribution indicates statistically significantly
better predictions than the naive judges.
used stratified 13-fold cross-validation in which
the documents from various genres in each fold
was distributed in roughly the same proportion as
in the overall dataset. We first conducted experi-
ments to test different regression algorithms using
all the available features. Next, we ablated various
feature sets to determine how much each feature
set was contributing to making accurate readabil-
ity judgements. These experiments are described
in the following subsections.
6.2.1 Regression Algorithms
We used several regression algorithms available
in theWeka machine learning package and Table 2
shows the results obtained. The default values
551
Feature Set Correlation
Lexical 0.5760
Syntactic 0.7010
Lexical + Syntactic 0.7274
Language Model based 0.7864
All 0.8173
Table 3: Comparison of different linguistic feature sets.
in Weka were used for all parameters, changing
these values did not show any improvement. We
used decision tree (reduced error pruning (Quin-
lan, 1987)) regression, decision tree regression
with bagging (Breiman, 1996), support vector re-
gression (Smola and Scholkopf, 1998) using poly-
nomial kernel of degree two,2 linear regression
and Gaussian process regression (Rasmussen and
Williams, 2006). The distribution mean and the
upper critical values of the correlation coefficient
distribution for the naive judges are also shown in
the table.
Since they are above the upper critical value, all
algorithms predicted expert readability scores sig-
nificantly more accurately than the naive judges.
Bagged decision trees performed slightly better
than other methods. As shown in the following
section, ablating features affects predictive accu-
racy much more than changing the regression al-
gorithm. Therefore, on this task, the choice of re-
gression algorithm was not very critical once good
readability features are used. We also tested two
simple baseline strategies: predicting a score uni-
formly at random, and predicting a score propor-
tional to its frequency in the training data. As
shown in the last two rows of Table 2, these base-
lines perform very poorly, verifying that predict-
ing readability on this dataset as evaluated by our
evaluation metric is not trivial.
6.2.2 Ablations with Feature Sets
We evaluated the contributions of different fea-
ture sets through ablation experiments. Bagged
decision-tree was used as the regression algorithm
in all of these experiments. First we compared
syntactic, lexical and language-model based fea-
tures as described in Section 5, and Table 3 shows
2Polynomial kernels with other degrees and RBF kernel
performed worse.
the results. The language-model feature set per-
forms the best, but performance improves when it
is combined with the remaining features. The lex-
ical feature set by itself performs the worst, even
below the naive distribution mean (shown in Ta-
ble 2); however, when combined with syntactic
features it performs well.
In our second ablation experiment, we com-
pared the performance of genre-independent and
genre-based features. Since the genre-based fea-
tures exploit knowledge of the genres of text used
in the MRP readability corpus, their utility is
somewhat tailored to this specific corpus. There-
fore, it is useful to evaluate the performance of the
system when genre information is not exploited.
Of the lexical features described in subsection 5.3,
the ratio of function words, ratio of pronoun words
and all of the out-of-vocabulary rates except for
the base language model are genre-based features.
Out of the language model features described in
the Subsection 5.2, all of the perplexities except
for the base language model and all of the poste-
rior perplexities3 are genre-based features. All of
the remaining features are genre-independent. Ta-
ble 4 shows the results comparing these two fea-
ture sets. The genre-based features do well by
themselves but the rest of the features help fur-
ther improve the performance. While the genre-
independent features by themselves do not exceed
the upper critical value of the naive judges? dis-
tribution, they are very close to it and still out-
perform its mean value. These results show that
for a dataset like ours, which is composed of a mix
of genres that themselves are indicative of read-
ability, features that help identify the genre of a
text improve performance significantly.4 For ap-
plications mentioned in the introduction and re-
lated work sections, such as filtering less readable
documents from web-search, many of the input
documents could come from some of the common
genres considered in our dataset.
In our final ablation experiment, we evaluated
3Base model for posterior perplexities is computed using
other genre-based LMs (equation 3) hence it can not be con-
sidered genre-independent.
4We note that none of the genre-based features were
trained on supervised readability data, but were trained on
readily-available large unannotated corpora as shown in Ta-
ble 1.
552
Feature Set Correlation
Genre-independent 0.6978
Genre-based 0.7749
All 0.8173
Table 4: Comparison of genre-independent and genre-
based feature sets.
Feature Set By itself Ablated
from All
Sundance features 0.5417 0.7993
ESG features 0.5841 0.8118
Perplexities 0.7092 0.8081
Posterior perplexities 0.7832 0.7439
Out-of-vocabulary rates 0.3574 0.8125
All 0.8173 -
Table 5: Ablations with some individual feature sets.
the contribution of various individual feature sets.
Table 5 shows that posterior perplexities perform
the strongest on their own, but without them, the
remaining features also do well. When used by
themselves, some feature sets perform below the
naive judges? distribution mean, however, remov-
ing them from the rest of the feature sets de-
grades the performance. This shows that no indi-
vidual feature set is critical for good performance
but each further improves the performance when
added to the rest of the feature sets.
6.3 Official Evaluation Results
An official evaluation was conducted by the eval-
uation team SAIC on behalf of DARPA in which
three teams participated including ours. The eval-
uation task required predicting the readability of
150 test documents using the 390 training docu-
ments. Besides the correlation metric, two addi-
tional metrics were used. One of them computed
for a document the difference between the aver-
age absolute difference of the naive judge scores
from the mean expert score and the absolute dif-
ference of the machine?s score from the mean ex-
pert score. This was then averaged over all the
documents. The other one was ?target hits? which
measured if the predicted score for a document
fell within the width of the lowest and the highest
expert scores for that document, and if so, com-
System Correl. Avg. Diff. Target Hits
Our (A) 0.8127 0.4844 0.4619
System B 0.6904 0.3916 0.4530
System C 0.8501 0.5177 0.4641
Upper CV 0.7423 0.0960 0.3713
Table 6: Results of the systems that participated in the
DARPA?s readability evaluation task. The three metrics used
were correlation, average absolute difference and target hits
measured against the expert readability scores. The upper
critical values are for the score distributions of naive judges.
puted a score inversely proportional to that width.
The final target hits score was then computed by
averaging it across all the documents. The upper
critical values for these metrics were computed in
a way analogous to that for the correlation met-
ric which was described before. Higher score is
better for all the three metrics. Table 6 shows the
results of the evaluation. Our system performed
favorably and always scored better than the up-
per critical value on each of the metrics. Its per-
formance was in between the performance of the
other two systems. The performances of the sys-
tems show that the correlation metric was the most
difficult of the three metrics.
7 Conclusions
Using regression over a diverse combination of
syntactic, lexical and language-model based fea-
tures, we built a system for predicting the read-
ability of natural-language documents. The sys-
tem accurately predicts readability as judged by
linguistically-trained expert human judges and
exceeds the accuracy of naive human judges.
Language-model based features were found to be
most useful for this task, but syntactic and lexical
features were also helpful. We also found that for
a corpus consisting of documents from a diverse
mix of genres, using features that are indicative
of the genre significantly improve the accuracy of
readability predictions. Such a system could be
used to filter out less readable documents for ma-
chine or human processing.
Acknowledgment
This research was funded by Air Force Contract
FA8750-09-C-0172 under the DARPA Machine
Reading Program.
553
References
Bernth, Arendse. 1997. Easyenglish: A tool for improv-
ing document quality. In Proceedings of the fifth con-
ference on Applied Natural Language Processing, pages
159?165, Washington DC, April.
Breiman, Leo. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
Chall, J.S. and E. Dale. 1995. Readability Revisited: The
New Dale-Chall Readability Formula. Brookline Books,
Cambridge, MA.
Collins-Thompson, Kevyn and James P. Callan. 2004. A
language modeling approach to predicting reading diffi-
culty. In Proc. of HLT-NAACL 2004, pages 193?200.
Fry, E. 1990. A readability formula for short passages. Jour-
nal of Reading, 33(8):594?597.
Gunning, R. 1952. The Technique of Clear Writing.
McGraw-Hill, Cambridge, MA.
Heilman, Michael, Kevyn Collins-Thompson, Jamie Callan,
and Maxine Eskenazi. 2007. Combining lexical and
grammatical features to improve readability measures for
first and second language texts. In Proc. of NAACL-HLT
2007, pages 460?467, Rochester, New York, April.
Heilman, Michael, Kevyn Collins-Thompson, and Maxine
Eskenazi. 2008. An analysis of statistical models and fea-
tures for reading difficulty prediction. In Proceedings of
the Third Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 71?79, Columbus,
Ohio, June. Association for Computational Linguistics.
Kanungo, Tapas and David Orr. 2009. Predicting the read-
ability of short web summaries. In Proc. of WSDM 2009,
pages 202?211, Barcelona, Spain, February.
Kincaid, J. P., R. P. Fishburne, R. L. Rogers, and B.S.
Chissom. 1975. Derivation of new readability formulas
for navy enlisted personnel. Technical Report Research
Branch Report 8-75, Millington, TN: Naval Air Station.
Kneser, Reinhard and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc. of
ICASSP-95, pages 181?184.
McCallum, Andrew and Kamal Nigam. 1998. A comparison
of event models for naive Bayes text classification. In Pa-
pers from the AAAI-98 Workshop on Text Categorization,
pages 41?48, Madison, WI, July.
McCord, Michael C. 1989. Slot grammar: A system for
simpler construction of practical natural language gram-
mars. In Proceedings of the International Symposium on
Natural Language and Logic, pages 118?145, May.
McLaughlin, G. H. 1969. Smog: Grading: A new readabil-
ity formula. Journal of Reading, 12:639?646.
Pitler, Emily and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proc. of EMNLP 2008, pages 186?195,
Waikiki,Honolulu,Hawaii, October.
Quinlan, J. R. 1987. Simplifying decision trees. Interna-
tional Journal of Man-Machine Studies, 27:221?234.
Rasmussen, Carl and Christopher Williams. 2006. Gaussian
Processes for Machine Leanring. MIT Press, Cambridge,
MA.
Riloff, E. and W. Phillips. 2004. An introduction to the Sun-
dance and Autoslog systems. Technical Report UUCS-
04-015, University of Utah School of Computing.
Riloff, Ellen. 1996. Automatically generating extraction
patterns from untagged text. In Proc. of 13th Natl. Conf.
on Artificial Intelligence (AAAI-96), pages 1044?1049,
Portland, OR.
Schwarm, Sarah E. andMari Ostendorf. 2005. Reading level
assessment using support vector machines and statistical
language models. In Proc. of ACL 2005, pages 523?530,
Ann Arbor, Michigan.
Si, Luo and James P. Callan. 2001. A statistical model for
scientific readability. In Proc. of CIKM 2001, pages 574?
576.
Smola, Alex J. and Bernhard Scholkopf. 1998. A tutorial
on support vector regression. Technical Report NC2-TR-
1998-030, NeuroCOLT2.
Stanley, Chen and Joshua Goodman. 1996. An empirical
study of smoothing techniques for language modeling. In
Proc. of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL-96), pages 310?318.
Stenner, A. J., I. Horabin, D. R. Smith, and M. Smith. 1988.
The Lexile Framework. Durham, NC: MetaMetrics.
Thelen, M. and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proc. of EMNLP 2002, Philadelphia, PA, July.
Yang, Yiming and Xin Liu. 1999. A re-examination of text
cateogrization methods. In Proc. of 22nd Intl. ACM SI-
GIR Conf. on Research and Development in Information
Retrieval, pages 42?48, Berkeley, CA.
554
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 185?193,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
When Did that Happen? ? Linking Events and Relations to Timestamps
Dirk Hovy*, James Fan, Alfio Gliozzo, Siddharth Patwardhan and Chris Welty
IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532
dirkh@isi.edu, {fanj,gliozzo,siddharth,welty}@us.ibm.com
Abstract
We present work on linking events and flu-
ents (i.e., relations that hold for certain
periods of time) to temporal information
in text, which is an important enabler for
many applications such as timelines and
reasoning. Previous research has mainly
focused on temporal links for events, and
we extend that work to include fluents
as well, presenting a common methodol-
ogy for linking both events and relations
to timestamps within the same sentence.
Our approach combines tree kernels with
classical feature-based learning to exploit
context and achieves competitive F1-scores
on event-time linking, and comparable F1-
scores for fluents. Our best systems achieve
F1-scores of 0.76 on events and 0.72 on flu-
ents.
1 Introduction
It is a long-standing goal of NLP to process natu-
ral language content in such a way that machines
can effectively reason over the entities, relations,
and events discussed within that content. The ap-
plications of such technology are numerous, in-
cluding intelligence gathering, business analytics,
healthcare, education, etc. Indeed, the promise
of machine reading is actively driving research in
this area (Etzioni et al 2007; Barker et al 2007;
Clark and Harrison, 2010; Strassel et al 2010).
Temporal information is a crucial aspect of this
task. For a machine to successfully understand
natural language text, it must be able to associate
time points and temporal durations with relations
and events it discovers in text.
?The first author conducted this research during an in-
ternship at IBM Research.
In this paper we present methods to estab-
lish links between events (e.g. ?bombing? or
?election?) or fluents (e.g. ?spouseOf? or ?em-
ployedBy?) and temporal expressions (e.g. ?last
Tuesday? and ?November 2008?). While previ-
ous research has mainly focused on temporal links
for events only, we deal with both events and flu-
ents with the same method. For example, consider
the sentence below
Before his death in October, Steve Jobs
led Apple for 15 years.
For a machine reading system processing this
sentence, we would expect it to link the fluent
CEO of (Steve Jobs, Apple) to time duration ?15
years?. Similarly we expect it to link the event
?death? to the time expression ?October?.
We do not take a strong ?ontological? position
on what events and fluents are, as part of our
task these distinctions are made a priori. In other
words, events and fluents are input to our tempo-
ral linking framework. In the remainder of this pa-
per, we also do not make a strong distinction be-
tween relations in general and fluents in particu-
lar, and use them interchangeably, since our focus
is only on the specific types of relations that rep-
resent fluents. While we only use binary relations
in this work, there is nothing in the framework
that would prevent the use of n-ary relations. Our
work focuses on accurately identifying temporal
links for eventual use in a machine reading con-
text.
In this paper, we describe a single approach that
applies to both fluents and events, using feature
engineering as well as tree kernels. We show that
we can achieve good results for both events and
fluents using the same feature space, and advocate
185
the versatility of our approach by achieving com-
petitive results on yet another similar task with a
different data set.
Our approach requires us to capture contextual
properties of text surrounding events, fluents and
time expressions that enable an automatic system
to detect temporal linking within our framework.
A common strategy for this is to follow standard
feature engineering methodology and manually
develop features for a machine learning model
from the lexical, syntactic and semantic analysis
of the text. A key contribution of our work in this
paper is to demonstrate a shallow tree-like repre-
sentation of the text that enables us to employ tree
kernel models, and more accurately detect tempo-
ral linking. The feature space represented by such
tree kernels is far larger than a manually engi-
neered feature space, and is capable of capturing
the contextual information required for temporal
linking.
The remainder of this paper goes into the de-
tails of our approach for temporal linking, and
presents empirical evidence for the effectiveness
of our approach. The contributions of this paper
can be summarized as follows:
1. We define a common methodology to link
events and fluents to timestamps.
2. We use tree kernels in combination with clas-
sical feature-based approaches to obtain sig-
nificant gains by exploiting context.
3. Empirical evidence illustrates that our
framework for temporal linking is very ef-
fective for the task, achieving an F1-score of
0.76 on events and 0.72 on fluents/relations,
as well as 0.65 for TempEval2, approaching
state-of-the-art.
2 Related Work
Most of the previous work on relation extraction
focuses on entity-entity relations, such as in the
ACE (Doddington et al 2004) tasks. Temporal
relations are part of this, but to a lesser extent.
The primary research effort in event temporality
has gone into ordering events with respect to one
another (e.g., Chambers and Jurafsky (2008)), and
detecting their typical durations (e.g., Pan et al
(2006)).
Recently, TempEval workshops have focused
on the temporal related issues in NLP. Some of
the TempEval tasks overlap with ours in many
ways. Our task is similar to task A and C of
TempEval-1 (Verhagen et al 2007) in the sense
that we attempt to identify temporal relation be-
tween events and time expressions or document
dates. However, we do not use a restricted set of
events, but focus primarily on a single temporal
relation tlink instead of named relations like BE-
FORE, AFTER or OVERLAP (although we show
that we can incorporate these as well). Part of our
task is similar to task C of TempEval-2 (Verha-
gen et al 2010), determining the temporal rela-
tion between an event and a time expression in
the same sentence. In this paper, we do apply our
system to TempEval-2 data and compare our per-
formance to the participating systems.
Our work is similar to that of Boguraev and
Ando (2005), whose research only deals with
temporal links between events and time expres-
sions (and does not consider relations at all). They
employ a sequence tagging model with manual
feature engineering for the task and achieved
state-of-the-art results on Timebank (Pustejovsky
et al 2003) data. Our task is slightly different be-
cause we include relations in the temporal linking,
and our use of tree kernels enables us to explore a
wider feature space very quickly.
Filatova and Hovy (2001) also explore tempo-
ral linking with events, but do not assume that
events and time stamps have been provided by an
external process. They used a heuristics-based ap-
proach to assign temporal expressions to events
(also relying on the proximity as a base case).
They report accuracy of the assignment for the
correctly classified events, the best being 82.29%.
Our best event system achieves an accuracy of
84.83%. These numbers are difficult to compare,
however, since accuracy does not efficiently cap-
ture the performance of a system on a task with so
many negative examples.
Mirroshandel et al(2011) describe the use of
syntactic tree kernels for event-time links. Their
results on TempEval are comparable to ours. In
contrast to them, we found, though, that syntactic
tree kernels alone do not perform as well as using
several flat tree representations.
3 Problem Definition
The task of linking events and relations to time
stamps can be defined as the following: given a set
of expressions denoting events or relation men-
186
tions in a document, and a set of time expressions
in the same document, find all instances of the
tlink relation between elements of the two input
sets. The existence of a tlink(e, t) means that e,
which is an event or a relation mention, occurs
within the temporal context specified by the time
expression t.
Thus, our task can be cast as a binary rela-
tion classification task: for each possible pair
of (event/relation, time) in a document, decide
whether there exists a link between the two, and
if so, express it in the data.
In addition, we make these assumptions about
the data:
1. There does not exist a timestamp for ev-
ery event/relation in a document. Although
events and relations typically have temporal
context, it may not be explicitly stated in a
document.
2. Every event/relation has at most one time ex-
pression associated with it. This is a simpli-
fying assumption, which in the case of rela-
tions we explore as future work.
3. Each temporal expression can be linked to
one or more events or relations. Since mul-
tiple events or relations may happen for a
given time, it is safe to assume that each tem-
poral expression can be linked to more than
one event/relation.
In general, the events/relations and their associ-
ated timestamps may occur within the same sen-
tence or may occur across different sentences. In
this paper, we focus on our effort and our evalua-
tion on the same sentence linking task.
In order to solve the problem of temporal link-
ing completely, however, it will be important to
also address the links that hold between entities
across sentences. We estimate, based on our data
set, that across sentence links account for 41% of
all correct event-time pairs in a document. For flu-
ents, the ratio is much higher, more than 80% of
the correct fluent-time links are across sentences.
One of the main obstacles for our approach in the
cross-sentence case is the very low ratio of posi-
tive to negative instances (3 : 100) in the set of all
pairs in a document. Most pairs are not linked to
one another.
4 Temporal Linking Framework
As previously mentioned, we approach the tem-
poral linking problem as a classification task. In
the framework of classification, we refer to each
pair of (event/relation, temporal expression) oc-
curring within a sentence as an instance. The goal
is to devise a classifier that separates positive (i.e.,
linked) instances from negative ones, i.e., pairs
where there is no link between the event/relation
and the temporal expression in question. The lat-
ter case is far more frequent, so we have an inher-
ent bias toward negative examples in our data.1
Note that the basis of the positive and nega-
tive links is the context around the target terms.
It is impossible even for humans to determine the
existence of a link based only on the two terms
without their context. For instance, given just two
words (e.g., ?said? and ?yesterday?) there is no
way to tell if it is a positive or a negative example.
We need the context to decide.
Therefore, we base our classification models on
contextual features drawn from lexical and syn-
tactic analyses of the text surrounding the target
terms. For this, we first define a feature-based
approach, then we improve it by using tree ker-
nels. These two subsections, plus the treatment
of fluent relations, are the main contributions of
this paper. In all of this work, we employ SVM
classifiers (Vapnik, 1995) for machine learning.
4.1 Feature Engineering
A manual analysis of development data provided
several intuitions about the kinds of features that
would be useful in this task. Based on this anal-
ysis and with inspiration from previous work (cf.
Boguraev and Ando (2005)) we established three
categories of features whose description follows.
Features describing events or relations. We
check whether the event or relation is phrasal, a
verb, or noun, whether it is present tense, past
tense, or progressive, the type assigned to the
event/relation by the UIMA type system used for
processing, and whether it includes certain trig-
ger words, such as reporting verbs (?said?, ?re-
ported?, etc.).
1Initially, we employed an instance filtering method to
address this, which proved to be ineffective and was subse-
quently left out.
187
Features describing temporal expressions.
We check for the presence of certain trigger words
(last, next, old, numbers, etc.) and the type of
the expression (DURATION, TIME, or DATE) as
specified by the UIMA type system.
Features describing context. We also in-
clude syntactic/structural features, such as testing
whether the relation/event dominates the temporal
expression, which one comes first in the sentence
order, and whether either of them is dominated
by a separate verb, preposition, ?that? (which of-
ten indicates a subordinate sentence) or counter-
factual nouns or verbs (which would negate the
temporal link).
It is not surprising that some of the most in-
formative features (event comes before tempo-
ral expression, time is syntactic child of event)
are strongly correlated with the baselines. Less
salient features include the test for certain words
indicating the event is a noun, a verb, and if so
which tense it has and whether it is a reporting
verb.
4.2 Tree Kernel Engineering
We expect that there exist certain patterns be-
tween the entities of a temporal link, which mani-
fest on several levels: some on the lexical level,
others expressed by certain sequences of POS
tags, NE labels, or other representations. Kernels
provide a principled way of expanding the number
of dimensions in which we search for a decision
boundary, and allow us to easily model local se-
quences and patterns in a natural way (Giuliano et
al., 2009). While it is possible to define a space
in which we find a decision boundary that sepa-
rates positive and negative instances with manu-
ally engineered features, these features can hardly
capture the notion of context as well as those ex-
plored by a tree kernel.
Tree Kernels are a family of kernel functions
developed to compute the similarity between tree
structures by counting the number of subtrees
they have in common. This generates a high-
dimensional feature space that can be handled ef-
ficiently using dynamic programming techniques
(Shawe-Taylor and Christianini, 2004). For our
purposes we used an implementation of the Sub-
tree and Subset Tree (SST) (Moschitti, 2006).
The advantages of using tree kernels are
two-fold: thanks to an existing implementation
(SVMlight with tree kernels, Moschitti (2004)), it
is faster and easier than traditional feature engi-
neering. The tree structure also allows us to use
different levels of representations (POS, lemma,
etc.) and combine their contributions, while at the
same time taking into account the ordering of la-
bels. We use POS, lemma, semantic type, and a
representation that replaces each word with a con-
catenation of its features (capitalization, count-
able, abstract/concrete noun, etc.).
We developed a shallow tree representation that
captures the context of the target terms, without
encoding too much structure (which may prevent
generalization). In essence, our tree structure in-
duces behavior somewhat similar to a string ker-
nel. In addition, we can model the tasks by pro-
viding specific markup on the generated tree. For
example, in our experiment we used the labels
EVENT (or equivalently RELATION) and TIME-
STAMP to mark our target terms. In order to re-
duce the complexity of this comparison, we focus
on the substring between event/relation and time
stamp and the rest of the tree structure is trun-
cated.
Figure 1 illustrates an example of the structure
described so far for both lemmas and POS tags
(note that the lowest level of the tree contains tok-
enized items, so their number can differ form the
actual words, as in ?attorney general?). Similar
trees are produced for each level of representa-
tions used, and for each instance (i.e., pair of time
expressions and event/relation). If a sentence con-
tains more than one event/relation, we create sep-
arate trees for each of them, which differ in the po-
sition of the EVENT/RELATION marks (at level
1 of the tree).
The tree kernel implicitly expands this struc-
ture into a number of substructures allowing us
to capture sequential patterns in the data. As we
will see, this step provides significant boosts to
the task performance.
Curiously, using a full-parse syntactic tree as
input representation did not help performance.
This is in line with our finding that syntactic re-
lations are less important than sequential patterns
(see also Section 5.2). Therefore we adopted the
?string kernel like? representation illustrated in
Figure 1.
188
Scores of supporters of detained Egyptian opposition leader Nur demonstrated outside the attorney general?s
office in Cairo last Saturday, demanding he be freed immediately.
BOW
TIME
TOK
saturday
TOK
last
TERM
TOK
cairo
TERM
TOK
in
TERM
TOK
office
TERM
TOK
attorney general
TERM
TOK
outside
EVENT
TOK
demonstrate
BOP
TIME
TOK
NNP
TOK
JJ
TERM
TOK
NNP
TERM
TOK
IN
TERM
TOK
NN
TERM
TOK
NNP
TERM
TOK
ADV
EVENT
TOK
VBD
Figure 1: Input Sentence and Tree Kernel Representations for Bag of Words (BOW) and POS tags (BOP)
5 Evaluation
We now apply our models to real world data, and
empirically demonstrate their effectiveness at the
task of temporal linking. In this section, we de-
scribe the data sets that were used for evaluation,
the baselines for comparison, parameter settings,
and the results of the experiments.
5.1 Benchmark
We evaluated our approach in 3 different tasks:
1. Linking Timestamps and Events in the IC
domain
2. Linking Timestamps and Relations in the IC
domain
3. Linking Events to Temporal Expressions
(TempEval-2, task C)
The first two data sets contained annotations
in the intelligence community (IC) domain, i.e.,
mainly news reports about terrorism. It com-
prised 169 documents. This dataset has been de-
veloped in the context of the machine reading pro-
gram (MRP) (Strassel et al 2010). In both cases
our goal is to develop a binary classifier to judge
whether the event (or relation) overlaps with the
time interval denoted by the timestamp. Success
of this classification can be measured by precision
and recall on annotated data.
We originally considered using accuracy as a
measure of performance, but this does not cor-
rectly reflect the true performance of the system:
given the skewed nature of the data (much smaller
number of positive examples), we could achieve a
high accuracy simply by classifying all instances
as negative, i.e., not assigning a time stamp at all.
We thus decided to report precision, recall and F1.
Unless stated otherwise, results were achieved via
10-fold cross-validation (10-CV).
The number of instances (i.e., pairs of event
and temporal expression) for each of the differ-
ent cases listed above was (in brackets the ratio of
positive to negative instances).
? events: 2046 (505 positive, 1541 negative)
? relations: 6526 (1847 positive, 4679 nega-
tive)
The size of the relation data set after filtering is
5511 (1847 positive, 3395 negative).
In order to increase the originally lower number
of event instances, we made use of the annotated
event-coreference as a sort of closure to add more
instances: if events A and B corefer, and there
is a link between A and time expression t, then
there is also a link between B and t. This was not
explicitly expressed in the data.
For the task at hand, we used gold standard
annotations for timestamps, events and relations.
The task was thus not the identification of these
objects (a necessary precursor and a difficult task
in itself), but the decision as to which events and
time expressions could and should be linked.
We also evaluated our system on TempEval-
2 (Verhagen et al 2010) for better comparison
189
to the state-of-the-art. TempEval-2 data included
the task of linking events to temporal expressions
(there called ?task C?), using several link types
(OVERLAP, BEFORE, AFTER, BEFORE-OR-
OVERLAP, OVERLAP-OR-AFTER). This is a
bit different from our settings as it required the
implementation of a multi-class classifier. There-
fore we trained three different binary classifiers
(using the same feature set) for the first three of
those types (for which there was sufficient train-
ing data) and we used a one-versus-all strategy to
distinguish positive from negative examples. The
output of the system is the category with the high-
est SVM decision score. Since we only use three
labels, we incur an error every time the gold la-
bel is something else. Note that this is stricter
than the evaluation in the actual task, which left
contestants with the option of skipping examples
their systems could not classify.
5.2 Baselines
Intuitively, one would expect temporal expres-
sions to be close to the event they denote, or even
syntactically related. In order to test this, we ap-
plied two baselines. In the first, each temporal ex-
pression was linked to the closest event (as mea-
sured in token distance). In the second, we at-
tached each temporal expression to its syntactic
head, if the head was an event. Results are re-
ported in Figure 2.
While these results are encouraging for our
task, it seems at first counter-intuitive that the
syntactic baseline does worse than the proximity-
based one. It does, however, reveal two facts:
events are not always synonymous with syntactic
units, and they are not always bound to tempo-
ral expressions through direct syntactic links. The
latter makes even more sense given that the links
can even occur across sentence boundaries. Pars-
ing quality could play a role, yet seems far fetched
to account for the difference.
More important than syntactic relations seem
to be sequential patterns on different levels, a fact
we exploit with the different tree representations
used (POS tags, NE types, etc.).
For relations, we only applied the closest-
relation baseline. Since relations consist of two or
more arguments that occur in different, often sep-
arated syntactic constituents, a syntactic approach
seems futile, especially given our experience with
events. Results are reported in Figure 3.
baseline comparison
Page 1
Precision Recall F1
0
20
40
60
80
100
35.0
63.0
45.048.0
88.0
62.063.0
75.4 68.376.6 76.5 76.2
Evaluation Measures Events
BL-parent BL-closest features +tree kernel
metric
%
Figure 2: Performance on events
System Accuracy
TRIOS 65%
this work 64.5%
JU-CSE, NCSU-indi
TRIPS, USFD2
all 63%
Table 1: Comparison to Best Systems in TempEval-2
5.3 Events
Figure 2 shows the improvements of the feature-
based approach over the two baseline, and the ad-
ditional gain obtained by using the tree kernel.
Both the features and tree kernels mainly improve
precision, while the tree kernel adds a small boost
in recall. It is remarkable, though, that the closest-
event baseline has a very high recall value. This
suggests that most of the links actually do occur
between items that are close to one another. For a
possible explanation for the low precision value,
see the error analysis (Section 5.5).
Using a two-tailed t-test, we compute the sig-
nificance in the difference between the F1-scores.
Both the feature-based and the tree kernel ap-
proach improvements are statistically significant
at p < 0.001 over the baseline scores.
Table 1 compares the performances of our sys-
tem to the state-of-the-art systems on TempEval-2
Data, task C, showing that our approach is very
competitive. The best systems there used sequen-
tial models. We attribute the competitive nature
of our results to the use of tree kernels, which en-
ables us to make use of contextual information.
5.4 Relations
In general, performance for relations is not as high
as for events (see Figure 3). The reason here is
two-fold: relations consist of two (or more) ele-
ments, which can be in various positions with re-
spect to one another and the temporal expression,
and each relation can be expressed in a number of
190
baseline comparison
Page 1
Precision Recall F1
0
10
20
30
40
50
60
70
80
90
100
35.0
24.0 29.0
63.1
80.6
70.470.8 74.0 72.2
Evaluation Metric Relations
BL-closest features +tree kernel
metric
%
Figure 3: Performance on relations/fluents
learning curves
Page 1
0 10 20 30 40 50 60 70 80 90 100
40
45
50
55
60
65
70
75
80
Learning Curves Relations
features w/ tree 
kernel
% of data
F1
 sc
or
e
Figure 4: Learning curves for relation models
different ways.
Again, we perform significance tests on the dif-
ference in F1 scores and find that our improve-
ments over the baseline are statistically significant
at p < 0.001. The improvement of the tree kernel
over the feature-based approach, however, are not
statistically significant at the same value.
The learning curve over parts of the training
data (exemplary shown here for relations, Figure
4)2 indicates that there is another advantage to us-
ing tree kernels: the approach can benefit from
more data. This is conceivably because it allows
the kernel to find more common subtrees in the
various representations the more examples it gets,
while the feature space rather finds more instances
that invalidate the expressiveness of features (i.e.,
it encounters positive and negative instances that
have very similar feature vectors). The curve sug-
gests that tree kernels could yield even better re-
sults with more data, while there is little to no ex-
pected gain using only features.
5.5 Error Analysis
Examining the misclassified examples in our data,
we find that both feature-based and tree-kernel
approaches struggle to correctly classify exam-
2The learning curve for events looks similar and is omit-
ted due to space constraints.
ples where time expression and event/relation are
immediately adjacent, but unrelated, as in ?the
man arrested last Tuesday told the police ...?,
where last Tuesday modifies arrested. It limits
the amount of context that is available to the tree
kernels, since we truncate the tree representations
to the words between those two elements. This
case closely resembles the problem we see in the
closest-event/relation baseline, which, as we have
seen, does not perform too well. In this case, the
incorrect event (?told?) is as close to the time ex-
pression as the correct one (?arrested?), resulting
in a false positive that affects precision. Features
capturing the order of the elements do not seem
help here, since the elements can be arranged in
any order (i.e., temporal expression before or af-
ter the event/relation). The only way to solve this
problem would be to include additional informa-
tion about whether a time expression is already
attached to another event/relation.
5.6 Ablations
To quantify the utility of each tree representation,
we also performed all-but-one ablation tests, i.e.,
left out each of the tree representations in turn, ran
10-fold cross-validation on the data and observed
the effect on F1. The larger the loss in F1, the
more informative the left-out-representation. We
performed ablations for both events and relations,
and found that the ranking of the representations
is the same for both.
In events and relations alike, leaving out POS
trees has the greatest effect on F1, followed by
the feature-bundle representation. Lemma and se-
mantic type representation have less of an impact.
We hypothesize that the former two capture un-
derlying regularities better by representing differ-
ent words with the same label. Lemmas in turn
are too numerous to form many recurring pat-
terns, and semantic type, while having a smaller
label alphabet, does not assign a label to every
word, thus creating a very sparse representation
that picks up more noise than signal.
In preliminary tests, we also used annotated
dependency trees as input to the tree kernel, but
found that performance improved when they were
left out. This is at odds with work that clearly
showed the value of syntactic tree kernels (Mir-
roshandel et al 2011). We identify two poten-
tial causes?either our setup was not capable of
correctly capturing and exploiting the information
191
from the dependency trees, or our formulation of
the task was not amenable to it. We did not inves-
tigate this further, but leave it to future work.
6 Conclusion and Future Work
We cast the problem of linking events and rela-
tions to temporal expressions as a classification
task using a combination of features and tree ker-
nels, with probabilistic type filtering. Our main
contributions are:
? We showed that within-sentence temporal
links for both events and relations can be ap-
proached with a common strategy.
? We developed flat tree representations and
showed that these produce considerable
gains, with significant improvements over
different baselines.
? We applied our technique without great ad-
justments to an existing data set and achieved
competitive results.
? Our best systems achieve F1 score of 0.76
on events and 0.72 on relations, and are ef-
fective at the task of temporal linking.
We developed the models as part of a machine
reading system and are currently evaluating it in
an end-to-end task.
Following tasks proposed in TempEval-2, we
plan to use our approach for across-sentence clas-
sification, as well as a similar model for linking
entities to the document creation date.
Acknowledgements
We would like to thank Alessandro Moschitti for
his help with the tree kernel setup, and the review-
ers who supplied us with very constructive feed-
back. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA
Machine Reading Program.
References
Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw,
James Fan, Noah Friedland, Michael Glass, Jerry
Hobbs, Eduard Hovy, David Israel, Doo Soon Kim,
Rutu Mulkar-Mehta, Sourabh Patwardhan, Bruce
Porter, Dan Tecuci, and Peter Yeh. 2007. Learn-
ing by reading: A prototype system, performance
baseline and lessons learned. In Proceedings of
the 22nd National Conference for Artificial Intelli-
gence, Vancouver, Canada, July.
Branimir Boguraev and Rie Kubota Ando. 2005.
Timeml-compliant text analysis for temporal rea-
soning. In Proceedings of IJCAI, volume 5, pages
997?1003. IJCAI.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. pages
789?797. Association for Computational Linguis-
tics.
Peter Clark and Phil Harrison. 2010. Machine read-
ing as a process of partial question-answering. In
Proceedings of the NAACL HLT Workshop on For-
malisms and Methodology for Learning by Reading,
Los Angeles, CA, June.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion program ? tasks, data and evaluation. In Pro-
ceedings of the LREC Conference, Canary Islands,
Spain, July.
Oren Etzioni, Michele Banko, and Michael Cafarella.
2007. Machine reading. In Proceedings of the
AAAI Spring Symposium Series, Stanford, CA,
March.
Elena Filatova and Eduard Hovy. 2001. Assigning
time-stamps to event-clauses. In Proceedings of
the workshop on Temporal and spatial information
processing, volume 13, pages 1?8. Association for
Computational Linguistics.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and
Carlo Strapparava. 2009. Kernel methods for min-
imally supervised wsd. Computational Linguistics,
35(4).
Seyed A. Mirroshandel, Mahdy Khayyamian, and
Gholamreza Ghassem-Sani. 2011. Syntactic tree
kernels for event-time temporal relation learning.
Human Language Technology. Challenges for Com-
puter Science and Linguistics, pages 213?223.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 335?es. Associa-
tion for Computational Linguistics.
Alessandro Moschitti. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of EACL, volume 6.
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs. 2006.
Learning event durations from event descriptions.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 393?400. Association for Computa-
tional Linguistics.
James Pustejovsky, Patrick Hanks, Roser Saur?, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
192
Ferro, and Marcia Lazo. 2003. The TIMEBANK
Corpus. In Proceedings of Corpus Linguistics
2003, pages 647?656.
John Shawe-Taylor and Nello Christianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA Machine Read-
ing Program-Encouraging Linguistic and Reason-
ing Research with a Series of Reading Tasks. In
Proceedings of LREC 2010.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York, NY.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Puste-
jovsky. 2007. Semeval-2007 task 15: Tempeval
temporal relation identification. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 75?80. Association for Computational
Linguistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62. Association for Computational Linguistics.
193
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 24?33,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Large Scale Relation Detection?
Chris Welty and James Fan and David Gondek and Andrew Schlaikjer
IBM Watson Research Center ? 19 Skyline Drive ? Hawthorne, NY 10532, USA
{welty, fanj, dgondek, ahschlai}@us.ibm.com
Abstract
We present a technique for reading sentences
and producing sets of hypothetical relations
that the sentence may be expressing. The
technique uses large amounts of instance-level
background knowledge about the relations in
order to gather statistics on the various ways
the relation may be expressed in language, and
was inspired by the observation that half of the
linguistic forms used to express relations oc-
cur very infrequently and are simply not con-
sidered by systems that use too few seed ex-
amples. Some very early experiments are pre-
sented that show promising results.
1 Introduction
We are building a system that learns to read in a new
domain by applying a novel combination of natural
language processing, machine learning, knowledge
representation and reasoning, information retrieval,
data mining, etc. techniques in an integrated way.
Central to our approach is the view that all parts of
the system should be able to interact during any level
of processing, rather than a pipeline view in which
certain parts of the system only take as input the re-
sults of other parts, and thus cannot influence those
results. In this paper we discuss a particular case
of that idea, using large knowledge bases hand in
hand with natural language processing to improve
the quality of relation detection. Ultimately we de-
fine reading as representing natural language text in
? Research supported in part by DARPA MRP Grant
FA8750-09-C0172
a way that integrates background knowledge and in-
ference, and thus are doing the relation detection
to better integrate text with pre-existing knowledge,
however that should not (and does not) prevent us
from using what knowledge we have to influence
that integration along the way.
2 Background
The most obvious points of interaction between NLP
and KR systems are named entity tagging and other
forms of type instance extraction. The second ma-
jor point of interaction is relation extraction, and
while there are many kinds of relations that may
be detected (e.g. syntactic relations such as modi-
fiers and verb subject/object, equivalence relations
like coreference or nicknames, event frame relations
such as participants, etc.), the kind of relations that
reading systems need to extract to support domain-
specific reasoning tasks are relations that are known
to be expressed in supporting knowledge-bases. We
call these relations semantic relations in this paper.
Compared to entity and type detection, extraction
of semantic relations is significantly harder. In our
work on bridging the NLP-KR gap, we have ob-
served several aspects of what makes this task dif-
ficult, which we discuss below.
2.1 Keep reading
Humans do not read and understand text by first rec-
ognizing named entities, giving them types, and then
finding a small fixed set of relations between them.
Rather, humans start with the first sentence and build
up a representation of what they read that expands
and is refined during reading. Furthermore, humans
24
do not ?populate databases? by reading; knowledge
is not only a product of reading, it is an integral part
of it. We require knowledge during reading in order
to understand what we read.
One of the central tenets of our machine reading
system is the notion that reading is not performed on
sentences in isolation. Often, problems in NLP can
be resolved by simply waiting for the next sentence,
or remembering the results from the previous, and
incorporating background or domain specific knowl-
edge. This includes parse ambiguity, coreference,
typing of named entities, etc. We call this the Keep
Reading principle.
Keep reading applies to relation extraction as
well. Most relation extraction systems are imple-
mented such that a single interpretation is forced
on a sentence, based only on features of the sen-
tence itself. In fact, this has been a shortcoming
of many NLP systems in the past. However, when
you apply the Keep Reading principle, multiple hy-
potheses from different parts of the NLP pipeline are
maintained, and decisions are deferred until there is
enough evidence to make a high confidence choice
between competing hypotheses. Knowledge, such
as those entities already known to participate in a
relation and how that relation was expressed, can
and should be part of that evidence. We will present
many examples of the principle in subsequent sec-
tions.
2.2 Expressing relations in language
Due to the flexibility and expressive power of nat-
ural language, a specific type of semantic relation
can usually be expressed in language in a myriad
of ways. In addition, semantic relations are of-
ten implied by the expression of other relations.
For example, all of the following sentences more
or less express the same relation between an actor
and a movie: (1) ?Elijah wood starred in Lord of
the Rings: The Fellowship of the Ring?, (2) ?Lord
of the Rings: The Fellowship of the Ring?s Elijah
Wood, ...?, and(3) ?Elijah Wood?s coming of age
was clearly his portrayal of the dedicated and noble
hobbit that led the eponymous fellowship from the
first episode of the Lord of the Rings trilogy.? No
human reader would have any trouble recognizing
the relation, but clearly this variability of expression
presents a major problem for machine reading sys-
tems.
To get an empirical sense of the variability of nat-
ural language used to express a relation, we stud-
ied a few semantic relations and found sentences
that expressed that relation, extracted simple pat-
terns to account for how the relation is expressed
between two arguments, mainly by removing the re-
lation arguments (e.g. ?Elijah Wood? and ?Lord of
the Rings: The Fellowship of the Ring? above) and
replacing them with variables. We then counted the
number of times each pattern was used to express
the relation, producing a recognizable very long tail
shown in Figure 1 for the top 50 patterns expressing
the acted-in-movie relation in 17k sentences. More
sophisticated pattern generalization (as discussed in
later sections) would significantly fatten the head,
bringing it closer to the traditional 50% of the area
under the curve, but no amount of generalization
will eliminate the tail. The patterns become increas-
ingly esoteric, such as ?The movie Death Becomes
Her features a brief sequence in which Bruce Willis
and Goldie Hawn?s characters plan Meryl Streep?s
character?s death by sending her car off of a cliff
on Mulholland Drive,? or ?The best known Hawk-
sian woman is probably Lauren Bacall, who iconi-
cally played the type opposite Humphrey Bogart in
To Have and Have Not and The Big Sleep.?
2.3 What relations matter
We do not consider relation extraction to be an end
in and of itself, but rather as a component in larger
systems that perform some task requiring interoper-
ation between language- and knowledge-based com-
ponents. Such larger tasks can include question
answering, medical diagnosis, intelligence analysis,
museum curation, etc. These tasks have evaluation
criteria that go beyond measuring relation extraction
results. The first step in applying relation detection
to these larger tasks is analysis to determine what
relations matter for the task and domain.
There are a number of manual and semi-automatic
ways to perform such analysis. Repeating the
theme of this paper, which is to use pre-existing
knowledge-bases as resources, we performed this
analysis using freebase and a set of 20k question-
answer pairs representing our task domain. For each
question, we formed tuples of each entity name in
the question (QNE) with the answer, and found all
25
 0
100
200
300
400
500
600
700
800
900
1000
Figure 1: Pattern frequency for acted-in-movie relation for 17k sentences.
 
0
10
20
30
40
50
60
70
80
Figure 2: Relative frequency for top 50 relations in 20K question-answer pairs.
the relations in the KB connecting the entities. We
kept a count for each relation of how often it con-
nected a QNE to an answer. Of course we don?t ac-
tually know for sure that the relation is the one being
asked, but the intuition is that if the amount of data
is big enough, you will have at least a ranked list of
which relations are the most frequent.
Figure 2 shows the ranking for the top 50 rela-
tions. Note that, even when restricted to the top 50
relations, the graph has no head, it is basically all
tail; The top 50 relations cover about 15% of the do-
main. In smaller, manual attempts to determine the
most frequent relations in our domain, we had a sim-
ilar result. What this means is that supporting even
the top 50 relations with perfect recall covers about
15% of the questions. It is possible, of course, to
narrow the domain and restrict the relations that can
be queried?this is what database systems do. For
reading, however, the results are the same. A read-
ing system requires the ability to recognize hundreds
of relations to have any significant impact on under-
standing.
2.4 Multi-relation learning on many seeds
The results shown in Figure 1 and Figure 2 con-
firmed much of the analysis and experiences we?d
had in the past trying to apply relation extraction in
the traditional way to natural language problems like
26
question answering, building concept graphs from
intelligence reports, semantic search, etc. Either by
training machine learning algorithms on manually
annotated data or by manually crafting finite-state
transducers, relation detection is faced by this two-
fold problem: the per-relation extraction hits a wall
around 50% recall, and each relation itself occurs
infrequently in the data.
This apparent futility of relation extraction led us
to rethink our approach. First of all, the very long
tail for relation patterns led us to consider how to
pick up the tail. We concluded that to do so would
require many more examples of the relation, but
where can we get them? In the world of linked-data,
huge instance-centered knowledge-bases are rapidly
growing and spreading on the semantic web1. Re-
sources like DBPedia, Freebase, IMDB, Geonames,
the Gene Ontology, etc., are making available RDF-
based data about a number of domains. These
sources of structured knowledge can provide a large
number of seed tuples for many different relations.
This is discussed further below.
Furthermore, the all-tail nature of relation cover-
age led us to consider performing relation extraction
on multiple relations at once. Some promising re-
sults on multi-relation learning have already been re-
ported in (Carlson et al, 2009), and the data sources
mentioned above give us many more than just the
handful of seed instances used in those experiments.
The idea of learning multiple relations at once also
fits with our keep reading principle - multiple rela-
tion hypotheses may be annotated between the same
arguments, with further evidence helping to disam-
biguate them.
3 Approach
One common approach to relation extraction is to
start with seed tuples and find sentences that con-
tain mentions of both elements of the tuple. From
each such sentence a pattern is generated using at
minimum universal generalization (replace the tuple
elements with variables), though adding any form of
generalization here can significantly improve recall.
Finally, evaluate the patterns by applying them to
text and evaluating the precision and recall of the tu-
ples extracted by the patterns. Our approach, called
1http://linkeddata.org/
Large Scale Relation Detection (LSRD), differs in
three important ways:
1. We start with a knowledge-base containing a
large number (thousands to millions) of tuples
encoding relation instances of various types.
Our hypothesis is that only a large number of
examples can possibly account for the long tail.
2. We do not learn one relation at a time, but
rather, associate a pattern with a set of relations
whose tuples appear in that pattern. Thus, when
a pattern is matched to a sentence during read-
ing, each relation in its set of associated rela-
tions is posited as a hypothetical interpretation
of the sentence, to be supported or refuted by
further reading.
3. We use the knowledge-base as an oracle to de-
termine negative examples of a relation. As
a result the technique is semi-supervised; it
requires no human intervention but does re-
quire reliable knowledge-bases as input?these
knowledge-bases are readily available today.
Many relation extraction techniques depend on a
prior step of named entity recognition (NER) and
typing, in order to identify potential arguments.
However, this limits recall to the recall of the NER
step. In our approach patterns can match on any
noun phrase, and typing of these NPs is simply an-
other form of evidence.
All this means our approach is not relation extrac-
tion per se, it typically does not make conclusions
about a relation in a sentence, but extracts hypothe-
ses to be resolved by other parts of our reading sys-
tem.
In the following sections, we elaborate on the
technique and some details of the current implemen-
tation.
3.1 Basic pipeline
The two principle inputs are a corpus and a
knowledge-base (KB). For the experiments below,
we used the English Gigaword corpus2 extended
with Wikipedia and other news sources, and IMDB,
DBPedia, and Freebase KBs, as shown. The intent is
2http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2003T05
27
to run against a web-scale corpus and larger linked-
data sets.
Input documents are sentence delimited, tok-
enized and parsed. The technique can benefit dra-
matically from coreference resolution, however in
the experiments shown, this was not present. For
each pair of proper names in a sentence, the names
are looked up in the KB, and if they are related,
a pattern is extracted from the sentence. At min-
imum, pattern extraction should replace the names
with variables. Depending on how patterns are ex-
tracted, one pattern may be extracted per sentence,
or one pattern may be extracted per pair of proper
names in the sentence. Each pattern is associated
with all the relations known in the KB between the
two proper names. If the pattern has been extracted
before, the two are merged by incrementing the as-
sociated relation counts. This phase, called pattern
induction, is repeated for the entire corpus, resulting
in a large set of patterns, each pattern associated with
relations. For each ?pattern, relation? pair, there is a
count of the number of times that pattern appeared
in the corpus with names that are in the relation ac-
cording to the KB.
The pattern induction phase results in positive
counts, i.e. the number of times a pattern appeared
in the corpus with named entities known to be re-
lated in the KB. However, the induction phase does
not exhaustively count the number of times each pat-
tern appears in the corpus, as a pattern may appear
with entities that are not known in the KB, or are not
known to be related. The second phase, called pat-
tern training, goes through the entire corpus again,
trying to match induced patterns to sentences, bind-
ing any noun phrase to the pattern variables. Some
attempt is made to resolve the noun phrase to some-
thing (most obviously, a name) that can be looked
up in the KB, and for each relation associated with
the pattern, if the two names are not in the relation
according to the KB, the negative count for that re-
lation in the matched pattern is incremented. The
result of the pattern training phase is an updated set
of ?pattern, relation? pairs with negative counts.
The following example illustrates the basic pro-
cessing. During induction, this sentence is encoun-
tered:
Tom Cruise and co-star Nicole Kidman
appeared together at the premier.
The proper names ?Tom Cruise? and ?Nicole Kid-
man? are recognized and looked up in the KB. We
find instances in the KB with those names, and the
following relations: coStar(Tom Cruise,
Nicole Kidman); marriedTo(Tom
Cruise, Nicole Kidman). We extract a
pattern p1: ?x and co-star ?y appeared
together at the premier in which all the
names have been replace by variables, and the
associations <p1, costar, 1, 0> and <p1,
marriedTo, 1, 0> with positive counts and
zero negative counts. Over the entire corpus, we?d
expect the pattern to appear a few times and end
up with final positive counts like <p1, coStar,
14, 0> and <p1, marriedTo, 2, 0>, in-
dicating the pattern p1 appeared 14 times in the
corpus between names known to participate in the
coStar relation, and twice between names known
to participate in the marriedTo relation. During
training, the following sentence is encountered that
matches p1:
Tom Hanks and co-star Daryl Hannah ap-
peared together at the premier.
The names ?Tom Hanks? and ?Daryl Hannah?
are looked up in the KB and in this case only
the relation coStar is found between them, so the
marriedTo association is updated with a negative
count: <p1, marriedTo, 2, -1>. Over the
entire corpus, we?d expect the counts to be some-
thing like <p1, costar, 14, -6> and <p1,
marriedTo, 2, -18>.
This is a very simple example and it is difficult to
see the value of the pattern training phase, as it may
appear the negative counts could be collected during
the induction phase. There are several reasons why
this is not so. First of all, since the first phase only
induces patterns between proper names that appear
and are related within the KB, a sentence in the cor-
pus matching the pattern would be missed if it did
not meet that criteria but was encountered before the
pattern was induced. Secondly, for reasons that are
beyond the scope of this paper, having to do with
our Keep Reading principle, the second phase does
slightly more general matching: note that it matches
noun phrases instead of proper nouns.
28
3.2 Candidate-instance matching
An obvious part of the process in both phases is
taking strings from text and matching them against
names or labels in the KB. We refer to the strings in
the sentences as candidate arguments or simply can-
didates, and refer to instances in the KB as entities
with associated attributes. For simplicity of discus-
sion we will assume all KBs are in RDF, and thus
all KB instances are nodes in a graph with unique
identifiers (URIs) and arcs connecting them to other
instances or primitive values (strings, numbers, etc.).
A set of specially designated arcs, called labels, con-
nect instances to strings that are understood to name
the instances. The reverse lookup of entity identi-
fiers via names referred to in the previous section
requires searching for the labels that match a string
found in a sentence and returning the instance iden-
tifier.
This step is so obvious it belies the difficultly of
the matching process and is often overlooked, how-
ever in our experiments we have found candidate-
instance matching to be a significant source of error.
Problems include having many instances with the
same or lexically similar names, slight variations in
spelling especially with non-English names, inflex-
ibility or inefficiency in string matching in KB im-
plementations, etc. In some of our sources, names
are also encoded as URLs. In the case of movie
and book titles-two of the domains we experimented
with-the titles seem almost as if they were designed
specifically to befuddle attempts to automatically
recognize them. Just about every English word is a
book or movie title, including ?It?, ?Them?, ?And?,
etc., many years are titles, and just about every num-
ber under 1000. Longer titles are difficult as well,
since simple lexical variations can prevent matching
from succeeding, e.g. the Shakespeare play, A Mid-
summer Night?s Dream appears often as Midsummer
Night?s Dream, A Midsummer Night Dream, and oc-
casionally, in context, just Dream. When titles are
not distinguished or delimited somehow, they can
confuse parsing which may fail to recognize them as
noun phrases. We eventually had to build dictionar-
ies of multi-word titles to help parsing, but of course
that was imperfect as well.
The problems go beyond the analogous ones in
coreference resolution as the sources and technology
themselves are different. The problems are severe
enough that the candidate-instance matching prob-
lem contributes the most, of all components in this
pipeline, to precision and recall failures. We have
observed recall drops of as much as 15% and preci-
sion drops of 10% due to candidate-instance match-
ing.
This problem has been studied somewhat in the
literature, especially in the area of database record
matching and coreference resolution (Michelson and
Knoblock, 2007), but the experiments presented be-
low use rudimentary solutions and would benefit
significantly from improvements; it is important to
acknowledge that the problem exists and is not as
trivial as it appears at first glance.
3.3 Pattern representation
The basic approach accommodates any pattern rep-
resentation, and in fact we can accommodate non
pattern-based learning approaches, such as CRFs, as
the primary hypothesis is principally concerned with
the number of seed examples (scaling up initial set
of examples is important). Thus far we have only
experimented with two pattern representations: sim-
ple lexical patterns in which the known arguments
are replaced in the sentence by variables (as shown
in the example above), and patterns based on the
spanning tree between the two arguments in a de-
pendency parse, again with the known arguments re-
placed by variables. In our initial design we down-
played the importance of the pattern representation
and especially generalization, with the belief that
very large scale would remove the need to general-
ize. However, our initial experiments suggest that
good pattern generalization would have a signifi-
cant impact on recall, without negative impact on
precision, which agrees with findings in the litera-
ture (Pantel and Pennacchiotti, 2006). Thus, these
early results only employ rudimentary pattern gen-
eralization techniques, though this is an area we in-
tend to improve. We discuss some more details of
the lack of generalization below.
4 Experiment
In this section we present a set of very early proof of
concept experiments performed using drastic simpli-
fications of the LSRD design. We began, in fact, by
29
Relation Prec Rec F1 Tuples Seeds
imdb:actedIn 46.3 45.8 0.46 9M 30K
frb:authorOf 23.4 27.5 0.25 2M 2M
imdb:directorOf 22.8 22.4 0.22 700K 700K
frb:parentOf 68.2 8.6 0.16 10K 10K
Table 1: Precision and recall vs. number of tuples used
for 4 freebase relations.
using single-relation experiments, despite the cen-
trality of multiple hypotheses to our reading system,
in order to facilitate evaluation and understanding of
the technique. Our main focus was to gather data
to support (or refute) the hypothesis that more re-
lation examples would matter during pattern induc-
tion, and that using the KB as an oracle for training
would work. Clearly, no KB is complete to begin
with, and candidate-instance matching errors drop
apparent coverage further, so we intended to explore
the degree to which the KB?s coverage of the relation
impacted performance. To accomplish this, we ex-
amined four relations with different coverage char-
acteristics in the KB.
4.1 Setup and results
The first relation we tried was the acted-in-show
relation from IMDB; for convenience we refer to
it as imdb:actedIn. An IMDB show is a movie,
TV episode, or series. This relation has over 9M
<actor, show> tuples, and its coverage was
complete as far as we were able to determine. How-
ever, the version we used did not have a lot of name
variations for actors. The second relation was the
author-of relation from Freebase (frb:authorOf ),
with roughly 2M <author, written-work>
tuples. The third relation was the director-of-
movie relation from IMDB (imdb:directorOf ), with
700k <director,movie> tuples. The fourth
relation was the parent-of relation from Free-
base (frb:parentOf ), with roughly 10K <parent,
child> tuples (mostly biblical and entertainment).
Results are shown in Table 1.
The imdb:actedIn experiment was performed on
the first version of the system that ran on 1 CPU and,
due to resource constraints, was not able to use more
than 30K seed tuples for the rule induction phase.
However, the full KB (9M relation instances) was
available for the training phase. With some man-
ual effort, we selected tuples (actor-movie pairs) of
popular actors and movies that we expected to ap-
pear most frequently in the corpus. In the other ex-
periments, the full tuple set was available for both
phases, but 2M tuples was the limit for the size of
the KB in the implementation. With these promising
preliminary results, we expect a full implementation
to accommodate up to 1B tuples or more.
The evaluation was performed in decreasing de-
grees of rigor. The imdb:actedIn experiment was run
against 20K sentences with roughly 1000 actor in
movie relations and checked by hand. For the other
three, the same sentences were used, but the ground
truth was generated in a semi-automatic way by re-
using the LSRD assumption that a sentence con-
taining tuples in the relation expresses the relation,
and then spot-checked manually. Thus the evalua-
tion for these three experiments favors the LSRD ap-
proach, though spot checking revealed it is the pre-
cision and not the recall that benefits most from this,
and all the recall problems in the ground truth (i.e.
sentences that did express the relation but were not
in the ground truth) were due to candidate-instance
matching problems. An additional idiosyncrasy in
the evaluation is that the sentences in the ground
truth were actually questions, in which one of the
arguments to the relation was the answer. Since
the patterns were induced and trained on statements,
there is a mismatch in style which also significantly
impacts recall. Thus the precision and recall num-
bers should not be taken as general performance, but
are useful only relative to each other.
4.2 Discussion
The results are promising, and we are continuing the
work with a scalable implementation. Overall, the
results seem to show a clear correlation between the
number of seed tuples and relation extraction recall.
However, the results do not as clearly support the
many examples hypothesis as it may seem. When
an actor and a film that actor starred in are men-
tioned in a sentence, it is very often the case that the
sentence expresses that relation. However, this was
less likely in the case of the parent-of relation, and
as we considered other relations, we found a wide
degree of variation. The borders relation between
two countries, for example, is on the other extreme
from actor-in-movie. Bordering nations often wage
30
war, trade, suspend relations, deport refugees, sup-
port, oppose, etc. each other, so finding the two na-
tions in a sentence together is not highly indicative
of one relation or another. The director-of-movie re-
lation was closer to acted-in-movie in this regard,
and author-of a bit below that. The obvious next step
to gather more data on the many examples hypoth-
esis is to run the experiments with one relation, in-
creasing the number of tuples with each experiment
and observing the change in precision and recall.
The recall results do not seem particularly strik-
ing, though these experiments do not include pat-
tern generalization (other than what a dependency
parse provides) or coreference, use a small corpus,
and poor candidate-instance matching. Further, as
noted above there were other idiosyncrasies in the
evaluation that make them only useful for relative
comparison, not as general results.
Many of the patterns induced, especially for
the acted-in-movie relation, were highly lexical,
using e.g. parenthesis or other punctuation to
signal the relation. For example, a common
pattern was actor-name (movie-name), or
movie-name: actor-name, e.g. ?Leonardo
DiCaprio (Titanic) was considering accepting the
role as Anakin Skywalker,? or ?Titanic: Leonardo
DiCaprio and Kate Blanchett steam up the silver
screen against the backdrop of the infamous disas-
ter.? Clearly patterns like this rely heavily on the
context and typing to work. In general the pattern
?x (?y) is not reliable for the actor-in-movie re-
lation unless you know ?x is an actor and ?y is a
movie. However, some patterns, like ?x appears
in the screen epic ?y is highly indicative
of the relation without the types at all - in fact it is
so high precision it could be used to infer the types
of ?x and ?y if they were not known. This seems
to fit extremely well in our larger reading system,
in which the pattern itself provides one form of evi-
dence to be combined with others, but was not a part
of our evaluation.
One of the most important things to general-
ize in the patterns we observed was dates. If
patterns like, actor-name appears in the
1994 screen epic movie-name could have
been generalized to actor-name appears in
the date screen epic movie-name, re-
call would have been boosted significantly. As it
stood in these experiments, everything but the argu-
ments had to match. Similarly, many relations often
appear in lists, and our patterns were not able to gen-
eralize that away. For example the sentence, ?Mark
Hamill appeared in Star Wars, Star Wars: The Em-
pire Strikes Back, and Star Wars: The Return of the
Jedi,? causes three patterns to be induced; in each,
one of the movies is replaced by a variable in the
pattern and the other two are required to be present.
Then of course all this needs to be combined, so that
the sentence, ?Indiana Jones and the Last Crusade is
a 1989 adventure film directed by Steven Spielberg
and starring Harrison Ford, Sean Connery, Denholm
Elliott and Julian Glover,? would generate a pattern
that would get the right arguments out of ?Titanic
is a 1997 epic film directed by James Cameron and
starring Leonardo DiCaprio, Kate Winslett, Kathy
Bates and Bill Paxon.? At the moment the former
sentence generates four patterns that require the di-
rector and dates to be exactly the same.
Some articles in the corpus were biographies
which were rich with relation content but also with
pervasive anaphora, name abbreviations, and other
coreference manifestations that severely hampered
induction and evaluation.
5 Related work
Early work in semi-supervised learning techniques
such as co-training and multi-view learning (Blum
and Mitchell, 1998) laid much of the ground work
for subsequent experiments in bootstrapped learn-
ing for various NLP tasks, including named entity
detection (Craven et al, 2000; Etzioni et al, 2005)
and document classification (Nigam et al, 2006).
This work?s pattern induction technique also repre-
sents a semi-supervised approach, here applied to
relation learning, and at face value is similar in mo-
tivation to many of the other reported experiments
in large scale relation learning (Banko and Etzioni,
2008; Yates and Etzioni, 2009; Carlson et al, 2009;
Carlson et al, 2010). However, previous techniques
generally rely on a small set of example relation in-
stances and/or patterns, whereas here we explicitly
require a larger source of relation instances for pat-
tern induction and training. This allows us to better
evaluate the precision of all learned patterns across
multiple relation types, as well as improve coverage
31
of the pattern space for any given relation.
Another fundamental aspect of our approach lies
in the fact that we attempt to learn many relations
simultaneously. Previously, (Whitelaw et al, 2008)
found that such a joint learning approach was use-
ful for large-scale named entity detection, and we
expect to see this result carry over to the relation ex-
traction task. (Carlson et al, 2010) also describes
relation learning in a multi-task learning framework,
and attempts to optimize various constraints posited
across all relation classes.
Examples of the use of negative evidence
for learning the strength of associations between
learned patterns and relation classes as proposed
here has not been reported in prior work to our
knowledge. A number of multi-class learning tech-
niques require negative examples in order to prop-
erly learn discriminative features of positive class
instances. To address this requirement, a number of
approaches have been suggested in the literature for
selection or generation of negative class instances.
For example, sampling from the positive instances
of other classes, randomly perturbing known pos-
itive instances, or breaking known semantic con-
straints of the positive class (e.g. positing multiple
state capitols for the same state). With this work,
we treat our existing RDF store as an oracle, and as-
sume it is sufficiently comprehensive that it allows
estimation of negative evidence for all target relation
classes simultaneously.
The first (induction) phase of LSRD is very simi-
lar to PORE (Wang et al, 2007) (Dolby et al, 2009;
Gabrilovich and Markovitch, 2007) and (Nguyen
et al, 2007), in which positive examples were ex-
tracted from Wikipedia infoboxes. These also bear
striking similarity to (Agichtein and Gravano, 2000),
and all suffer from a significantly smaller number of
seed examples. Indeed, its not using a database of
specific tuples that distinguishes LSRD, but that it
uses so many; the scale of the induction in LSRD
is designed to capture far less frequent patterns by
using significantly more seeds
In (Ramakrishnan et al, 2006) the same intu-
ition is captured that knowledge of the structure of
a database should be employed when trying to inter-
pret text, though again the three basic hypotheses of
LSRD are not supported.
In (Huang et al, 2004), a similar phenomenon to
what we observed with the acted-in-movie relation
was reported in which the chances of a protein in-
teraction relation being expressed in a sentence are
already quite high if two proteins are mentioned in
that sentence.
6 Conclusion
We have presented an approach for Large Scale Re-
lation Detection (LSRD) that is intended to be used
within a machine reading system as a source of hy-
pothetical interpretations of input sentences in natu-
ral language. The interpretations produced are se-
mantic relations between named arguments in the
sentences, and they are produced by using a large
knowledge source to generate many possible pat-
terns for expressing the relations known by that
source.
We have specifically targeted the technique at the
problem that the frequency of patterns occurring in
text that express a particular relation has a very long
tail (see Figure 1), and without enough seed exam-
ples the extremely infrequent expressions of the re-
lation will never be found and learned. Further, we
do not commit to any learning strategy at this stage
of processing, rather we simply produce counts, for
each relation, of how often a particular pattern pro-
duces tuples that are in that relation, and how of-
ten it doesn?t. These counts are simply used as ev-
idence for different possible interpretations, which
can be supported or refuted by other components in
the reading system, such as type detection.
We presented some very early results which while
promising are not conclusive. There were many
idiosyncrasies in the evaluation that made the re-
sults meaningful only with respect to other experi-
ments that were evaluated the same way. In addi-
tion, the evaluation was done at a component level,
as if the technique were a traditional relation extrac-
tion component, which ignores one of its primary
differentiators?that it produces sets of hypothetical
interpretations. Instead, the evaluation was done
only on the top hypothesis independent of other evi-
dence.
Despite these problems, the intuitions behind
LSRD still seem to us valid, and we are investing in a
truly large scale implementation that will overcome
the problems discussed here and can provide more
32
valid evidence to support or refute the hypotheses
LSRD is based on:
1. A large number of examples can account for the
long tail in relation expression;
2. Producing sets of hypothetical interpretations
of the sentence, to be supported or refuted by
further reading, works better than producing
one;
3. Using existing, large, linked-data knowledge-
bases as oracles can be effective in relation de-
tection.
References
[Agichtein and Gravano2000] E. Agichtein and L. Gra-
vano. 2000. Snowball: extracting relations from large
plain-text collections. In Proceedings of the 5th ACM
Conference on Digital Libraries, pages 85?94, San
Antonio, Texas, United States, June. ACM.
[Banko and Etzioni2008] Michele Banko and Oren Et-
zioni. 2008. The tradeoffs between open and tradi-
tional relation extraction. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics.
[Blum and Mitchell1998] A. Blum and T. Mitchell. 1998.
Combining labeled and unlabeled data with co-
training. In Proceedings of the 1998 Conference on
Computational Learning Theory.
[Carlson et al2009] A. Carlson, J. Betteridge, E. R. Hr-
uschka Jr., and T. M. Mitchell. 2009. Coupling semi-
supervised learning of categories and relations. In
Proceedings of the NAACL HLT 2009 Workshop on
Semi-supervised Learning for Natural Language Pro-
cessing.
[Carlson et al2010] A. Carlson, J. Betteridge, R. C.
Wang, E. R. Hruschka Jr., and T. M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the 3rd ACM International
Conference on Web Search and Data Mining.
[Craven et al2000] Mark Craven, Dan DiPasquo, Dayne
Freitag, Andrew McCallum, Tom Mitchell, Kamal
Nigam, and Sean Slattery. 2000. Learning to construct
knowledge bases from the World Wide Web. Artificial
Intelligence, 118(1?2):69?113.
[Dolby et al2009] Julian Dolby, Achille Fokoue, Aditya
Kalyanpur, Edith Schonberg, and Kavitha Srinivas.
2009. Extracting enterprise vocabularies using linked
open data. In Proceedings of the 8th International Se-
mantic Web Conference.
[Etzioni et al2005] Oren Etzioni, Michael Cafarella,
Doug Downey, Ana-Maria Popescu, Tal Shaked,
Stephen Soderland, Daniel S. Weld, and Alexander
Yates. 2005. Unsupervised named-entity extraction
from the web: An experimental study. Artificial Intel-
ligence, 165(1):91?134, June.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovich
and Shaul Markovitch. 2007. Computing seman-
tic relatedness using wikipedia-based explicit seman-
tic analysis. In IJCAI.
[Huang et al2004] Minlie Huang, Xiaoyan Zhu, Yu Hao,
Donald G. Payan, Kunbin Qu, and Ming Li. 2004.
Discovering patterns to extract protein-protein interac-
tions from full texts. Bioinformatics, 20(18).
[Michelson and Knoblock2007] Matthew Michelson and
Craig A. Knoblock. 2007. Mining heterogeneous
transformations for record linkage. In Proceedings of
the 6th International Workshop on Information Inte-
gration on the Web, pages 68?73.
[Nguyen et al2007] Dat P. Nguyen, Yutaka Matsuo, ,
and Mitsuru Ishizuka. 2007. Exploiting syntactic
and semantic information for relation extraction from
wikipedia. In IJCAI.
[Nigam et al2006] K. Nigam, A. McCallum, , and
T. Mitchell, 2006. Semi-Supervised Learning, chapter
Semi-Supervised Text Classification Using EM. MIT
Press.
[Pantel and Pennacchiotti2006] Patrick Pantel and Marco
Pennacchiotti. 2006. Espresso: Leveraging generic
patterns for automatically harvesting semantic rela-
tions. In Proceedings of the 21st international Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association For Computational
Linguistics, Sydney, Australia, July.
[Ramakrishnan et al2006] Cartic Ramakrishnan, Krys J.
Kochut, and Amit P. Sheth. 2006. A framework for
schema-driven relationship discovery from unstruc-
tured text. In ISWC.
[Wang et al2007] Gang Wang, Yong Yu, and Haiping
Zhu. 2007. PORE: Positive-only relation extraction
from wikipedia text. In ISWC.
[Whitelaw et al2008] C. Whitelaw, A. Kehlenbeck,
N. Petrovic, , and L. Ungar. 2008. Web-scale named
entity recognition. In Proceeding of the 17th ACM
Conference on information and Knowledge Manage-
ment, pages 123?132, Napa Valley, California, USA,
October. ACM.
[Yates and Etzioni2009] Alexander Yates and Oren Et-
zioni. 2009. Unsupervised methods for determining
object and relation synonyms on the web. Artificial
Intelligence, 34:255?296.
33
