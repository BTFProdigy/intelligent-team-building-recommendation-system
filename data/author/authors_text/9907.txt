Proceedings of the 12th Conference of the European Chapter of the ACL, pages 648?656,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Predicting Strong Associations on the Basis of Corpus Data
Yves Peirsman
Research Foundation ? Flanders &
QLVL, University of Leuven
Leuven, Belgium
yves.peirsman@arts.kuleuven.be
Dirk Geeraerts
QLVL, University of Leuven
Leuven, Belgium
dirk.geeraerts@arts.kuleuven.be
Abstract
Current approaches to the prediction of
associations rely on just one type of in-
formation, generally taking the form of
either word space models or collocation
measures. At the moment, it is an open
question how these approaches compare
to one another. In this paper, we will
investigate the performance of these two
types of models and that of a new ap-
proach based on compounding. The best
single predictor is the log-likelihood ratio,
followed closely by the document-based
word space model. We will show, how-
ever, that an ensemble method that com-
bines these two best approaches with the
compounding algorithm achieves an in-
crease in performance of almost 30% over
the current state of the art.
1 Introduction
Associations are words that immediately come to
mind when people hear or read a given cue word.
For instance, a word like pepper calls up salt,
and wave calls up sea. Aitchinson (2003) and
Schulte im Walde and Melinger (2005) show that
such associations can be motivated by a number
of factors, from semantic similarity to colloca-
tion. Current computational models of associa-
tion, however, tend to focus on one of these, by us-
ing either collocation measures (Michelbacher et
al., 2007) or word space models (Sahlgren, 2006;
Peirsman et al, 2008). To this day, two gen-
eral problems remain. First, the literature lacks
a comprehensive comparison between these gen-
eral types of models. Second, we are still looking
for an approach that combines several sources of
information, so as to correctly predict a larger va-
riety of associations.
Most computational models of semantic rela-
tions aim to model semantic similarity in particu-
lar (Landauer and Dumais, 1997; Lin, 1998; Pado?
and Lapata, 2007). In Natural Language Process-
ing, these models have applications in fields like
query expansion, thesaurus extraction, informa-
tion retrieval, etc. Similarly, in Cognitive Science,
such models have helped explain neural activa-
tion (Mitchell et al, 2008), sentence and discourse
comprehension (Burgess et al, 1998; Foltz, 1996;
Landauer and Dumais, 1997) and priming patterns
(Lowe and McDonald, 2000), to name just a few
examples. However, there are a number of appli-
cations and research fields that will surely bene-
fit from models that target the more general phe-
nomenon of association. For instance, automat-
ically predicted associations may prove useful in
models of information scent, which seek to ex-
plain the paths that users follow in their search
for relevant information on the web (Chi et al,
2001). After all, if the visitor of a web shop
clicks on music to find the prices of iPods, this
behaviour is motivated by an associative relation
different from similarity. Other possible applica-
tions lie in the field of models of text coherence
(Landauer and Dumais, 1997) and automated es-
say grading (Kakkonen et al, 2005). In addition,
all research in Cognitive Science that we have re-
ferred to above could benefit from computational
models of association in order to study the effects
of association in comparison to those of similarity.
Our article is structured as follows. In sec-
tion 2, we will discuss the phenomenon of asso-
ciation and introduce the variety of relations that
it is motivated by. Parallel to these relations, sec-
tion 3 presents the three basic types of approaches
that we use to predict strong associations. Sec-
tion 4 will first compare the results of these three
approaches, for a total of 43 models. Section 5
will then show how these results can be improved
by the combination of several models in an ensem-
ble. Finally, section 6 wraps up with conclusions
and an outlook for future research.
648
cue association
amfibie (?amphibian?) kikker (?frog?)
peper (?pepper?) zout (?salt?)
roodborstje (?robin?) vogel (?bird?)
granaat (?grenade?) oorlog (?war?)
helikopter (?helicopter?) vliegen (?to fly?)
werk (?job?) geld (?money?)
acteur (?actor?) film (?film?)
cello (?cello?) muziek (?music?)
kruk (?stool?) bar (?bar?)
Table 1: Examples of cues and their strongest as-
sociation.
2 Associations
There are several reasons why a word may be asso-
ciated to its cue. According to Aitchinson (2003),
the four major types of associations are, in or-
der of frequency, co-ordination (co-hyponyms like
pepper and salt), collocation (like salt and wa-
ter), superordination (insect as a hypernym of but-
terfly) and synonymy (like starved and hungry).
As a result, a computational model that is able to
predict associations accurately has to deal with a
wide range of semantic relations. Past systems,
however, generally use only one type of informa-
tion (Wettler et al, 2005; Sahlgren, 2006; Michel-
bacher et al, 2007; Peirsman et al, 2008; Wand-
macher et al, 2008), which suggests that they are
relatively restricted in the number of associations
they will find.
In this article, we will focus on a set of Dutch
cue words and their single strongest association,
collected from a large psycholinguistic experi-
ment. Table 1 gives a few examples of such cue?
association pairs. It illustrates the different types
of linguistic phenomena that an association may
be motivated by. The first three word pairs are
based on similarity. In this case, strong associ-
ations can be hyponyms (as in amphibian?frog),
co-hyponyms (as in pepper?salt) or hypernyms of
their cue (as in robin?bird). The next three pairs
represent semantic links where no relation of sim-
ilarity plays a role. Instead, the associations seem
to be motivated by a topical relation to their cue,
which is possibly reflected by their frequent co-
occurrence in a corpus. The final three word pairs
suggest that morphological factors might play a
role, too. Often, a cue and its association form
the building blocks of a compound, and it is possi-
ble that one part of a compound calls up the other.
The examples show that the process of compound-
ing can go in either direction: the compound may
consist of cue plus association (as in cellomuziek
?cello music?), or of association plus cue (as in
filmacteur ?film actor?). While it is not clear if it
is the compounds themselves that motivate the as-
sociation, or whether it is just the topical relation
between their two parts, they might still be able to
help identify strong associations.
3 Approaches
Motivated by the three types of cue?association
pairs that we identified in Table 1, we study three
sources of information (two types of distributional
information, and one type of morphological infor-
mation) that may provide corpus-based evidence
for strong associatedness: collocation measures,
word space models and compounding.
3.1 Collocation measures
Probably the most straightforward way to pre-
dict strong associations is to assume that a cue
and its strong association often co-occur in text.
As a result, we can use collocation measures
like point-wise mutual information (Church and
Hanks, 1989) or the log-likelihood ratio (Dunning,
1993) to predict the strong association for a given
cue. Point-wise mutual information (PMI) tells
us if two words w1 and w2 occur together more or
less often than expected on the basis of their indi-
vidual frequencies and the independence assump-
tion:
PMI(w1, w2) = log2
P (w1, w2)
P (w1) ? P (w2)
The log-likelihood ratio compares the like-
lihoods L of the independence hypothesis (i.e.,
p = P (w2|w1) = P (w2|?w1)) and the de-
pendence hypothesis (i.e., p1 = P (w2|w1) 6=
P (w2|?w1) = p2), under the assumption that the
words in a text are binomially distributed:
log ? = log
L(P (w2|w1); p) ? L(P (w2|?w1); p)
L(P (w2|w1); p1) ? L(P (w2|?w1); p2)
3.2 Word Space Models
A respectable proportion (in our data about 18%)
of the strong associations are motivated by se-
mantic similarity to their cue. They can be syn-
onyms, hyponyms, hypernyms, co-hyponyms or
649
antonyms. Collocation measures, however, are not
specifically targeted towards the discovery of se-
mantic similarity. Instead, they model similarity
mainly as a side effect of collocation. Therefore
we also investigated a large set of computational
models that were specifically developed for the
discovery of semantic similarity. These so-called
word space models or distributional models of lex-
ical semantics are motivated by the distributional
hypothesis, which claims that semantically simi-
lar words appear in similar contexts. As a result,
they model each word in terms of its contexts in
a corpus, as a so-called context vector. Distribu-
tional similarity is then operationalized as the sim-
ilarity between two such context vectors. These
models will thus look for possible associations by
searching words with a context vector similar to
the given cue.
Crucial in the implementation of word space
models is their definition of context. In the cur-
rent literature, there are basically three popular ap-
proaches. Document-based models use some sort
of textual entity as features (Landauer and Du-
mais, 1997; Sahlgren, 2006). Their context vec-
tors note what documents, paragraphs, articles or
similar stretches of text a target word appears in.
Without dimensionality reduction, in these mod-
els two words will be distributionally similar if
they often occur together in the same paragraph,
for instance. This approach still bears some simi-
larity to the collocation measures above, since it
relies on the direct co-occurrence of two words
in text. Second, syntax-based models focus on
the syntactic relationships in which a word takes
part (Lin, 1998). Here two words will be sim-
ilar when they often appear in the same syntac-
tic roles, like subject of fly. Third, word-
based models simply use as features the words
that appear in the context of the target, without
considering the syntactic relations between them.
Context is thus defined as the set of n words
around the target (Sahlgren, 2006). Obviously, the
choice of context size will again have a major in-
fluence on the behaviour of the model. Syntax-
based and word-based models differ from collo-
cation measures and document-based models in
that they do not search for words that co-occur
directly. Instead, they look for words that often
occur together with the same context words or
syntactic relations. Even though all these models
were originally developed to model semantic sim-
ilarity relations, syntax-based models have been
shown to favour such relations more than word-
based and document-based models, which might
capture more associative relationships (Sahlgren,
2006; Van der Plas, 2008).
3.3 Compounding
As we have argued before, one characteristic of
cues and their strong associations is that they can
sometimes be combined into a compound. There-
fore we developed a third approach which dis-
covers for every cue the words in the corpus that
in combination with it lead to an existing com-
pound. Since in Dutch compounds are generally
written as one word, this is relatively easy. We at-
tached each candidate association to the cue (both
in the combination cue+association and associ-
ation+cue), following a number of simple mor-
phological rules for compounding. We then de-
termined if any of these hypothetical compounds
occurred in the corpus. The possible associa-
tions that led to an observed compound were then
ranked according to the frequency of that com-
pound.1 Note that, for languages where com-
pounds are often spelled as two words, like En-
glish, our approach will have to recognize multi-
word units to deal with this issue.
3.4 Previous research
In previous research, most attention has gone out
to the first two of our models. Sahlgren (2006)
tries to find associations with word space mod-
els. He argues that document-based models are
better suited to the discovery of associations than
word-based ones. In addition, Sahlgren (2006) as
well as Peirsman et al (2008) show that in word-
based models, large context sizes are more effec-
tive than small ones. This supports Wandmacher
et al?s (2008) model of associations, which uses a
context size of 75 words to the left and right of the
target. However, Peirsman et al (2008) find that
word-based distributional models are clearly out-
performed by simple collocation measures, par-
ticularly the log-likelihood ratio. Such colloca-
tion measures are also used by Michelbacher et al
(2007) in their classification of asymmetric associ-
ations. They show the chi-square metric to be a ro-
bust classifier of associations as either symmetric
or asymmetric, while a measure based on condi-
tional probabilities is particularly suited to model
1If both compounds cue+association and association+cue
occurred in the corpus, their frequencies were summed.
650
ll
l l l l l l l l
2 4 6 8 10
2
5
10
20
50
10
0
context size
m
ed
ian
 ra
nk
 of
 m
os
t fr
eq
ue
nt 
ass
oci
atio
n l word?based no stoplistword?based stoplist
pmi statistic
log?likelihood statistic
compound?based
syntax?based
document?based
Figure 1: Median rank of the strong associations.
the magnitude of asymmetry. In a similar vein,
Wettler et al (2005) successfully predict associa-
tions on the basis of co-occurrence in text, in the
framework of associationist learning theory. De-
spite this wealth of systems, it is an open question
how their results compare to each other. More-
over, a model that combines several of these sys-
tems might outperform any basic approach.
4 Experiments
Our experiments were inspired by the association
prediction task at the ESSLLI-2008 workshop on
distributional models. We will first present this
precise setup and then go into the results and their
implications.
4.1 Setup
Our data was the Twente Nieuws Corpus (TwNC),
which contains 300 million words of Dutch news-
paper articles. This corpus was compiled at the
University of Twente and subsequently parsed by
the Alpino parser at the University of Gronin-
gen (van Noord, 2006). The newspaper arti-
cles in the corpus served as the contextual fea-
tures for the document-based system; the depen-
dency triples output by Alpino were used as in-
put for the syntax-based approach. These syntactic
features of the type subject of fly covered
eight syntactic relations ? subject, direct object,
prepositional complement, adverbial prepositional
phrase, adjective modification, PP postmodifica-
tion, apposition and coordination. Finally, the col-
location measures and word-based distributional
models took into account context sizes ranging
from one to ten words to the left and right of the
target.
Because of its many parameters, the precise im-
plementation of the word space models deserves a
bit more attention. In all cases, we used the con-
text vectors in their full dimensionality. While this
is somewhat of an exception in the literature, it
has been argued that the full dimensionality leads
to the best results for word-based models at least
(Bullinaria and Levy, 2007). For the syntax-based
and word-based approaches, we only took into ac-
count features that occurred at least two times to-
gether with the target. For the word-based models,
we experimented with the use of a stoplist, which
allowed us to exclude semantically ?empty? words
as features. The simple co-occurrence frequencies
in the context vectors were replaced by the point-
wise mutual information between the target and
the feature (Bullinaria and Levy, 2007; Van der
Plas, 2008). The similarity between two vectors
was operationalized as the cosine of the angle be-
651
similar related, not similar
models mean med rank1 mean med rank1
pmi context 10 16.4 4 23% 25.2 9 10%
log-likelihood ratio context 10 12.8 2 41% 18.0 3 31%
syntax-based 16.3 4 22% 61.9 70 2%
word-based context 10 stoplist 10.7 3 27% 36.9 17 12%
document-based 10.1 3 26% 20.2 4 26%
compounding 80.7 101 5% 51.9 26 12%
Table 2: Performance of the models on semantically similar cue-association pairs and related but not
similar pairs.
med = median; rank1 = number of associations at rank 1
tween them. This measure is more or less stan-
dard in the literature and leads to state-of-the-art
results (Schu?tze, 1998; Pado? and Lapata, 2007;
Bullinaria and Levy, 2007). While the cosine is a
symmetric measure, however, association strength
is asymmetric. For example, snelheid (?speed?)
triggered auto (?car?) no fewer than 55 times in
the experiment, whereas auto evoked snelheid a
mere 3 times. Like Michelbacher et al (2007), we
solve this problem by focusing not on the similar-
ity score itself, but on the rank of the association in
the list of nearest neighbours to the cue. We thus
expect that auto will have a much higher rank in
the list of nearest neighbours to snelheid than vice
versa.
Our Gold Standard was based on a large-scale
psycholinguistic experiment conducted at the Uni-
versity of Leuven (De Deyne and Storms, 2008).
In this experiment, participants were asked to list
three different associations for all cue words they
were presented with. Each of the 1425 cues was
given to at least 82 participants, resulting in a to-
tal of 381,909 responses. From this set, we took
only noun cues with a single strong association.
This means we found the most frequent associ-
ation to each cue, and only included the pair in
the test set if the association occurred at least 1.5
times more often than the second most frequent
one. This resulted in a final test set of 593 cue-
association pairs. Next we brought together all the
associations in a set of candidate associations, and
complemented it with 1000 random words from
the corpus with a frequency of at least 200. From
these candidate words, we had each model select
the 100 highest scoring ones (the nearest neigh-
bours). Performance was then expressed as the
median and mean rank of the strongest association
in this list. Associations absent from the list auto-
matically received a rank of 101. Thus, the lower
the rank, the better the performance of the system.
While there are obviously many more ways of as-
sembling a test set and scoring the several systems,
we found these all gave very similar results to the
ones reported here.
4.2 Results and discussion
The median ranks of the strong associations for all
models are plotted in Figure 1. The means show
the same pattern, but give a less clear indication of
the number of associations that were suggested in
the top n most likely candidates. The most suc-
cessful approach is the log-likelihood ratio (me-
dian 3 with a context size of 10, mean 16.6),
followed by the document-based model (median
4, mean 18.4) and point-wise mutual informa-
tion (median 7 with a context size of 10, mean
23.1). Next in line are the word-based distribu-
tional models with and without a stoplist (high-
est medians at 11 and 12, highest means at 30.9
and 33.3, respectively), and then the syntax-based
word space model (median 42, mean 51.1). The
worst performance is recorded for the compound-
ing approach (median 101, mean 56.7). Overall,
corpus-based approaches that rely on direct co-
occurrence thus seem most appropriate for the pre-
diction of strong associations to a cue. This is
probably a result of two factors. First, collocation
itself is an important motivation for human asso-
ciations (Aitchinson, 2003). Second, while col-
location approaches in themselves do not target
semantic similarity, semantically similar associa-
tions are often also collocates to their cues. This is
particularly the case for co-hyponyms, like pepper
and salt, which score very high both in terms of
collocation and in terms of similarity.
Let us discuss the results of all models in a bit
652
ll
l
cue frequency
Index
me
dian
 ran
k of
 stro
nge
st a
ssoc
iatio
n
high mid low1
2
5
10
20
50
100
l
l
l
association frequency
Index
me
dian
 ran
k of
 stro
nge
st a
ssoc
iatio
n
high mid low1
2
5
10
20
50
100 l pmi context 10log?likelihood context 10
syntax?based
word?based context 10 stoplistdocument?based
compounding
Figure 2: Performance of the models in three cue and association frequency bands.
more detail. A first factor of interest is the dif-
ference between associations that are similar to
their cue and those which are related but not simi-
lar. Most of our models show a crucial difference
in performance with respect to these two classes.
The most important results are given in Table 2.
The log-likelihood ratio gives the highest number
of associations at rank 1 for both classes. Par-
ticularly surprising is its strong performance with
respect to semantic similarity, since this relation
is only a side effect of collocation. In fact, the
log-likelihood ratio scores better at predicting se-
mantically similar associations than related but not
similar associations. Its performance moreover
lies relatively close to that of the word space mod-
els, which were specifically developed to model
semantic similarity. This underpins the observa-
tion that even associations that are semantically
similar to their cues are still highly motivated by
direct co-occurrence in text. Interestingly, only the
compounding approach has a clear preference for
associations that are related to their cue, but not
similar.
A second factor that influences the performance
of the models is frequency. In order to test its
precise impact, we split up the cues and their as-
sociations in three frequency bands of compara-
ble size. For the cues, we constructed a band
for words with a frequency of less than 500 in
the corpus (low), between 500 and 2,500 (mid)
and more than 2,500 (high). For the associations,
we had bands for words with a frequency of less
than 7,500 (low), between 7,500 and 20,000 (mid)
and more than 20,000 (high). Figure 2 shows
the performance of the most important models in
these frequency bands. With respect to cue fre-
quency, the word space models and compound-
ing approach suffer most from low frequencies
and hence, data sparseness. The log-likelihood
ratio is much more robust, while point-wise mu-
tual information even performs better with low-
frequency cues, although it does not yet reach
the performance of the document-based system
or the log-likelihood ratio. With respect to asso-
ciation frequency, the picture is different. Here
the word-based distributional models and PMI per-
form better with low-frequency associations. The
document-based approach is largely insensitive to
association frequency, while the log-likelihood ra-
tio suffers slightly from low frequencies. The per-
formance of the compounding approach decreases
most. What is particularly interesting about this
plot is that it points towards an important differ-
ence between the log-likelihood ratio and point-
wise mutual information. In its search for nearest
neighbours to a given cue word, the log-likelihood
ratio favours frequent words. This is an advanta-
geous feature in the prediction of strong associa-
tions, since people tend to give frequent words as
associations. PMI, like the syntax-based and word-
based models, lacks this characteristic. It therefore
fails to discover mid- and high-frequency associa-
tions in particular.
Finally, despite the similarity in results between
the log-likelihood ratio and the document-based
word space model, there exists substantial varia-
tion in the associations that they predict success-
fully. Table 3 gives an overview of the top ten as-
sociations that are predicted better by one model
than the other, according to the difference be-
653
model cue?association pairs
document-based model cue?billiards, amphibian?frog, fair?doughnut ball, sperm whale?sea,
map?trip, avocado?green, carnivore?meat, one-wheeler?circus,
wallet?money, pinecone?wood
log-likelihood ratio top?toy, oven?hot, sorbet?ice cream, rhubarb?sour, poppy?red,
knot?rope, pepper?red, strawberry?red, massage?oil, raspberry?red
Table 3: A comparison of the document-based model and the log-likelihood ratio on the basis of the
cue?target pairs with the largest difference in log ranks between the two approaches.
tween the models in the logarithm of the rank of
the association. The log-likelihood ratio seems
to be biased towards ?characteristics? of the tar-
get. For instance, it finds the strong associative
relation between poppy, pepper, strawberry, rasp-
berry and their shared colour red much better than
the document-based model, just like it finds the re-
latedness between oven and hot and rhubarb and
sour. The document-based model recovers more
associations that display a strong topical connec-
tion with their cue word. This is thanks to its re-
liance on direct co-occurrence within a large con-
text, which makes it less sensitive to semantic sim-
ilarity than word-based models. It also appears to
have less of a bias toward frequent words than the
log-likelihood ratio. Note, for instance, the pres-
ence of doughnut ball (or smoutebol in Dutch) as
the third nearest neighbour to fair, despite the fact
it occurs only once (!) in the corpus. This com-
plementarity between our two most successful ap-
proaches suggests that a combination of the two
may lead to even better results. We therefore in-
vestigated the benefits of a committee-based or en-
semble approach.
5 Ensemble-based prediction of strong
associations
Given the varied nature of cue?association rela-
tions, it could be beneficial to develop a model that
relies on more than one type of information. En-
semble methods have already proved their effec-
tiveness in the related area of automatic thesaurus
extraction (Curran, 2002), where semantic similar-
ity is the target relation. Curran (2002) explored
three ways of combining multiple ordered sets of
words: (1) mean, taking the mean rank of each
word over the ensemble; (2) harmonic, taking the
harmonic mean; (3) mixture, calculating the mean
similarity score for each word. We will study only
the first two of these approaches, as the different
metrics of our models cannot simply be combined
in a mean relatedness score. More particularly, we
will experiment with ensembles taking the (har-
monic) mean of the natural logarithm of the ranks,
since we found these to perform better than those
working with the original ranks.2
Table 4 compares the results of the most im-
portant ensembles with that of the single best ap-
proach, the log-likelihood ratio with a context size
of 10. By combining the two best approaches
from the previous section, the log-likelihood ra-
tio and the document-based model, we already
achieve a substantial increase in performance. The
mean rank of the association goes from 3 to 2,
the mean from 16.6 to 13.1 and the number of
strong associations with rank 1 climbs from 194
to 223. This is a statistically significant increase
(one-tailed paired Wilcoxon test, W = 30866,
p = .0002). Adding another word space model
to the ensemble, either a word-based or syntax-
based model, brings down performance. However,
the addition of the compound model does lead to a
clear gain in performance. This ensemble finds the
strongest association at a median rank of 2, and a
mean of 11.8. In total, 249 strong associations (out
of a total 593) are presented as the best candidate
by the model ? an increase of 28.4% compared
to the log-likelihood ratio. Hence, despite its poor
performance as a simple model, the compound-
based approach can still give useful information
about the strong association of a cue word when
combined with other models. Based on the origi-
nal ranks, the increase from the previous ensem-
ble is not statistically significant (W = 23929,
p = .31). If we consider differences at the start
of the neighbour list more important and compare
the logarithms of the ranks, however, the increase
becomes significant (W = 29787.5, p = 0.0008).
Its precise impact should thus further be investi-
gated.
2In the case of the harmonic mean, we actually take the
logarithm of rank+1, in order to avoid division by zero.
654
mean harmonic mean
systems med mean rank1 med mean rank1
loglik10 (baseline) 3 16.6 194
loglik10 + doc 2 13.1 223 3 13.4 211
loglik10 + doc + word10 3 13.8 182 3 14.2 187
loglik10 + doc + syn 3 14.4 179 4 14.7 184
loglik10 + doc + comp 2 11.8 249 2 12.2 221
Table 4: Results of ensemble methods.
loglik10 = log-likelihood ratio with context size 10;
doc = document-based model;
word10 = word-based model with context size 10 and a stoplist;
syn = syntax-based model;
comp = compound-based model;
med = median; rank1 = number of associations at rank 1
Let us finally take a look at the types of strong
associations that still tend to receive a low rank in
this ensemble system. The first group consists of
adjectives that refer to an inherent characteristic of
the cue word that is rarely mentioned in text. This
is the case for tennis ball?yellow, cheese?yellow,
grapefruit?bitter. The second type brings together
polysemous cues whose strongest association re-
lates to a different sense than that represented by
its corpus-based nearest neighbour. This applies
to Dutch kant, which is polysemous between side
and lace. Its strongest association, Bruges, is
clearly related to the latter meaning, but its corpus-
based neighbours ball and water suggest the for-
mer. The third type reflects human encyclopaedic
knowledge that is less central to the semantics of
the cue word. Examples are police?blue, love?red,
or triangle?maths. In many of these cases, it ap-
pears that the failure of the model to recover the
strong associations results from corpus limitations
rather than from the model itself.
6 Conclusions and future research
In this paper, we explored three types of basic ap-
proaches to the prediction of strong associations
to a given cue. Collocation measures like the log-
likelihood ratio simply recover those words that
strongly collocate with the cue. Word space mod-
els look for words that appear in similar contexts,
defined as documents, context words or syntac-
tic relations. The compounding approach, finally,
searches for words that combine with the target to
form a compound. The log-likelihood ratio with
a large context size emerged as the best predic-
tor of strong association, followed closely by the
document-based word space model. Moreover,
we showed that an ensemble method combining
the log-likelihood ratio, the document-based word
space model and the compounding approach, out-
performed any of the basic methods by almost
30%.
In a number of ways, this paper is only a first
step towards the successful modelling of cue?
association relations. First, the newspaper cor-
pus that served as our data has some restrictions,
particularly with respect to diversity of genres. It
would be interesting to investigate to what degree
a more general corpus ? a web corpus, for in-
stance ? would be able to accurately predict a
wider range of associations. Second, the mod-
els themselves might benefit from some additional
features. For instance, we are curious to find
out what the influence of dimensionality reduction
would be, particularly for document-based word
space models. Finally, we would like to extend
our test set from strong associations to more asso-
ciations for a given target, in order to investigate
how well the discussed models predict relative as-
sociation strength.
References
Jean Aitchinson. 2003. Words in the Mind. An Intro-
duction to the Mental Lexicon. Blackwell, Oxford.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
haviour Research Methods, 39:510?526.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: Words, sentences,
discourse. Discourse Processes, 25:211?257.
655
Ed H. Chi, Peter Pirolli, Kim Chen, and James Pitkow.
2001. Using information scent to model user infor-
mation needs and actions on the web. In Proceed-
ings of the ACM Conference on Human Factors and
Computing Systems (CHI 2001), pages 490?497.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information and lexicog-
raphy. In Proceedings of ACL-27, pages 76?83.
James R. Curran. 2002. Ensemble methods for au-
tomatic thesaurus extraction. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), pages 222?229.
Simon De Deyne and Gert Storms. 2008. Word asso-
ciations: Norms for 1,424 Dutch words in a contin-
uous task. Behaviour Research Methods, 40:198?
205.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19:61?74.
Peter W. Foltz. 1996. Latent Semantic Analysis for
text-based research. Behaviour Research Methods,
Instruments, and Computers, 29:197?202.
Tuomo Kakkonen, Niko Myller, Jari Timonen, and
Erkki Sutinen. 2005. Automatic essay grading with
probabilistic latent semantic analysis. In Proceed-
ings of the 2nd Workshop on Building Educational
Applications Using NLP, pages 29?36.
Thomas K. Landauer and Susan T. Dumais. 1997. A
solution to Plato?s problem: The Latent Semantic
Analysis theory of acquisition, induction and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL98, pages 768?774, Montreal, Canada.
Will Lowe and Scott McDonald. 2000. The di-
rect route: Mediated priming in semantic space.
In Proceedings of COGSCI 2000, pages 675?680.
Lawrence Erlbaum Associates.
Lukas Michelbacher, Stefan Evert, and Hinrich
Schu?tze. 2007. Asymmetric association measures.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP-07).
Tom M. Mitchell, Svetlana V. Shinkareva, An-
drew Carlson, Kai-Min Chang, Vicente L. Malva,
Robert A. Mason, and Marcel Adam Just. 2008.
Predicting human brain activity associated with the
meanings of nouns. Science, 320:1191?1195.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Yves Peirsman, Kris Heylen, and Dirk Geeraerts.
2008. Size matters. Tight and loose context defini-
tions in English word space models. In Proceedings
of the ESSLLI Workshop on Distributional Lexical
Semantics, pages 9?16.
Magnus Sahlgren. 2006. The Word-Space Model.
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations Between Words
in High-dimensional Vector Spaces. Ph.D. thesis,
Stockholm University, Stockholm, Sweden.
Sabine Schulte im Walde and Alissa Melinger. 2005.
Identifying semantic relations and functional prop-
erties of human verb associations. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 612?619.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Lonneke Van der Plas. 2008. Automatic Lexico-
Semantic Acquisition for Question Answering.
Ph.D. thesis, University of Groningen, Groningen,
The Netherlands.
Gertjan van Noord. 2006. At last parsing is now oper-
ational. In Piet Mertens, Ce?drick Fairon, Anne Dis-
ter, and Patrick Watrin, editors, Verbum Ex Machina.
Actes de la 13e Confe?rence sur le Traitement Au-
tomatique des Langues Naturelles (TALN), pages
20?42.
Tonio Wandmacher, Ekaterina Ovchinnikova, and
Theodore Alexandrov. 2008. Does Latent Seman-
tic Analysis reflect human associations? In Pro-
ceedings of the ESSLLI Workshop on Distributional
Lexical Semantics, pages 63?70.
Manfred Wettler, Reinhard Rapp, and Peter Sedlmeier.
2005. Free word associations correspond to contigu-
ities between words in texts. Journal of Quantitative
Linguistics, 12(2/3):111?122.
656
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 16?24,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Looking at word meaning.
An interactive visualization of Semantic Vector Spaces for Dutch synsets
Kris Heylen, Dirk Speelman and Dirk Geeraerts
QLVL, University of Leuven
Blijde-Inkomsstraat 21/3308, 3000 Leuven (Belgium)
{kris.heylen, dirk.speelman, dirk.geeraerts}@arts.kuleuven.be
Abstract
In statistical NLP, Semantic Vector Spaces
(SVS) are the standard technique for the
automatic modeling of lexical semantics.
However, it is largely unclear how these
black-box techniques exactly capture word
meaning. To explore the way an SVS struc-
tures the individual occurrences of words,
we use a non-parametric MDS solution of
a token-by-token similarity matrix. The
MDS solution is visualized in an interac-
tive plot with the Google Chart Tools. As
a case study, we look at the occurrences of
476 Dutch nouns grouped in 214 synsets.
1 Introduction
In the last twenty years, distributional models of
semantics have become the standard way of mod-
eling lexical semantics in statistical NLP. These
models, aka Semantic Vector Spaces (SVSs) or
Word Spaces, capture word meaning in terms
of frequency distributions of words over co-
occurring context words in a large corpus. The
basic assumption of the approach is that words
occurring in similar contexts will have a simi-
lar meaning. Speficic implementations of this
general idea have been developed for a wide va-
riety of computational linguistic tasks, includ-
ing Thesaurus extraction and Word Sense Dis-
ambiguation, Question answering and the model-
ing of human behavior in psycholinguistic experi-
ments (see Turney and Pantel (2010) for a general
overview of applications and speficic models). In
recent years, Semantic Vector Spaces have also
seen applications in more traditional domains of
linguistics, like diachronic lexical studies (Sagi et
al., 2009; Cook and Stevenson, 2010; Rohrdantz
et al, 2011) , or the study of lexical variation
(Peirsman et al, 2010). In this paper, we want to
show how Semantic Vector Spaces can further aid
the linguistic analysis of lexical semantics, pro-
vided that they are made accessible to lexicolo-
gists and lexicographers through a visualization
of their output.
Although all applications mentioned above as-
sume that distributional models can capture word
meaning to some extent, most of them use SVSs
only in an indirect, black-box way, without an-
alyzing which semantic properties and relations
actually manifest themselves in the models. This
is mainly a consequence of the task-based evalu-
ation paradigm prevalent in Computational Lin-
guistics: the researchers address a specific task
for which there is a pre-defined gold standard;
they implement a model with some new features,
that usually stem from a fairly intuitive, common-
sense reasoning of why some feature might bene-
fit the task at hand; the new model is then tested
against the gold standard data and there is an eval-
uation in terms of precision, recall and F-score.
In rare cases, there is also an error analysis that
leads to hypotheses about semantic characteristics
that are not yet properly modeled. Yet hardly ever,
there is in-depth analysis of which semantics the
tested model actually captures. Even though task-
based evaluation and shared test data sets are vital
to the objective comparison of computational ap-
proaches, they are, in our opinion, not sufficient
to assess whether the phenomenon of lexical se-
mantics is modeled adequately from a linguistic
perspective. This lack of linguistic insight into
the functioning of SVSs is also bemoaned in the
community itself. For example, Baroni and Lenci
(2011) say that ?To gain a real insight into the
16
abilities of DSMs (Distributional Semantic Mod-
els, A/N) to address lexical semantics, existing
benchmarks must be complemented with a more
intrinsically oriented approach, to perform direct
tests on the specific aspects of lexical knowledge
captured by the models?. They go on to present
their own lexical database that is similar to Word-
Net, but includes some additional semantic rela-
tions. They propose researchers test their model
against the database to find out which of the en-
coded relations it can detect. However, such an
analysis still boils down to checking whether a
model can replicate pre-defined structuralist se-
mantic relations, which themselves represent a
quite impoverished take on lexical semantics, at
least from a linguistic perspective. In this pa-
per, we want to argue that a more linguistically
adequate investigation of how SVSs capture lex-
ical semantics, should take a step back from the
evalution-against-gold-standard paradigm and do
a direct and unbiased analysis of the output of
SVS models. Such an analysis should compare
the SVS way of structuring semantics to the rich
descriptive and theoretic models of lexical se-
mantics that have been developed in Linguistics
proper (see Geeraerts (2010b) for an overview of
different research traditions). Such an in-depth,
manual analyis has to be done by skilled lexicolo-
gists and lexicographers. But would linguists, that
are traditionally seen as not very computation-
ally oriented, be interested in doing what many
Computational Linguists consider to be tedious
manual analysis? The answer, we think, is yes.
The last decade has seen a clear empirical turn
in Linguistics that has led linguists to embrace
advanced statistical analyses of large amounts of
corpus data to substantiate their theoretical hy-
potheses (see e.g. Geeraerts (2010a) and other
contributions in Glynn and Fischer (2010) on re-
search in semantics). SVSs would be an ideal
addition to those linguists? methodological reper-
toire. This creates the potential for a win-win sit-
uation: Computational linguists get an in-depth
evaluation of their models, while theoretical lin-
guists get a new tool for doing large scale empir-
ical analyses of word meaning. Of course, one
cannot just hand over a large matrix of word sim-
ilaties (the raw output of an SVS) and ask a lexi-
cologist what kind of semantics is ?in there?. In-
stead, a linguist needs an intuitive interface to ex-
plore the semantic structure captured by an SVS.
In this paper, we aim to present exactly that: an in-
teractive visualization of a Semantic Vector Space
Model that allows a lexicologist or lexicographer
to inspect how the model structures the uses of
words.
2 Token versus Type level
SVSs can model lexical semantics on two levels:
1. the type level: aggregating over all occur-
rences of a word, giving a representation of
a word?s general semantics.
2. the token level: representing the semantics
of each individual occurrence of a word.
The type-level models are mostly used to retrieve
semantic relations between words, e.g. synonyms
in the task of thesaurus extraction. Token-level
models are typically used to distinguish between
the different meanings within the uses of one
word, notably in the task of Word Sense Disam-
biguation or Word Sense Induction. Lexicological
studies on the other hand, typically combine both
perspectives: their scope is often defined on the
type level as the different words of a lexical field
or the set of near-synonyms referring to the same
concept, but they then go on to do a fine-grained
analysis on the token level of the uses of these
words to find out how the semantic space is pre-
cisely structured. In our study, we will also take
a concept-centered perspective and use as a start-
ing point the 218 sets of Dutch near-synonymous
nouns that Ruette et al (2012) generated with
their type-level SVS. For each synset, we then im-
plement our own token-level SVS to model the
individual occurrences of the nouns. The result-
ing token-by-token similarity matrix is then visu-
alized to show how the occurrences of the differ-
ent nouns are distributed over the semantic space
that is defined by the synset?s concept. Because
Dutch has two national varieties (Belgium and
the Netherlands) that show considerable lexical
variation, and because this is typically of inter-
est to lexicologists, we will also differentiate the
Netherlandic and Belgian tokens in our SVS mod-
els and their visualization.
The rest of this paper is structured as follows.
In the next section we present the corpus and
the near-synonym sets we used for our study.
Section 4 presents the token-level SVS imple-
mented for modeling the occurrences of the nouns
17
in the synsets. In section 5 we discuss the vi-
sualization of the SVS?s token-by-token similar-
ity matrices with Multi Dimensional Scaling and
the Google Visualization API. Finally, section 6
wraps up with conclusions and prospects for fu-
ture research.
3 Dutch corpus and synsets
The corpus for our study consists of Dutch news-
paper materials from 1999 to 2005. For Nether-
landic Dutch, we used the 500M words Twente
Nieuws Corpus (Ordelman, 2002)1, and for Bel-
gian Dutch, the Leuven Nieuws Corpus (aka Me-
diargus corpus, 1.3 million words2). The corpora
were automatically lemmatized, part-of-speech
tagged and syntactically parsed with the Alpino
parser (van Noord, 2006).
Ruette et al (2012) used the same corpora
for their semi-automatic generation of sets of
Dutch near-synonymous nouns. They used a so-
called dependency-based model (Pado? and Lap-
ata, 2007), which is a type-level SVS that models
the semantics of a target word as the weighted co-
occurrence frequencies with context words that
apear in a set of pre-defined dependency relations
with the target (a.o. adjectives that modify the
target noun, and verbs that have the target noun
as their subject). Ruette et al (2012) submitted
the output of their SVS to a clustering algorithm
known as Clustering by Committee (Pantel and
Lin, 2002). After some further manual cleaning,
this resulted in 218 synsets containing 476 nouns
in total. Table 1 gives some examples.
CONCEPT nouns in synset
INFRINGEMENT inbreuk, overtreding
GENOCIDE volkerenmoord, genocide
POLL peiling, opiniepeiling, rondvraag
MARIHUANA cannabis, marihuana
COUP staatsgreep, coup
MENINGITIS hersenvliesontsteking, meningitis
DEMONSTRATOR demonstrant, betoger
AIRPORT vliegveld, luchthaven
VICTORY zege, overwinning
HOMOSEXUAL homo, homoseksueel, homofiel
RELIGION religie, godsdienst
COMPUTER SCREEN computerschem, beeldscherm, monitor
Table 1: Dutch synsets (sample)
1Publication years 1999 up to 2002 of Algemeen Dag-
blad, NRC, Parool, Trouw and Volkskrant
2Publication years 1999 up to 2005 of De Morgen, De
Tijd, De Standaard, Het Laatste Nieuws, Het Nieuwsblad
and Het Belang van Limburg
4 Token-level SVS
Next, we wanted the model the individual oc-
currences of the nouns. The token-level SVS
we used is an adaptation the approach proposed
by Schu?tze (1998). He models the semantics
of a token as the frequency distribution over its
so-called second order co-occurrences. These
second-order co-occurrences are the type-level
context features of the (first-order) context words
co-occuring with the token. This way, a token?s
meaning is still modeled by the ?context? it oc-
curs in, but this context is now modeled itself by
combining the type vectors of the words in the
context. This higher order modeling is necessary
to avoid data-sparseness: any token only occurs
with a handful of other words and a first-order co-
occurrence vector would thus be too sparse to do
any meaningful vector comparison. Note that this
approach first needs to construct a type-level SVS
for the first-order context words that can then be
used to create a second-order token-vector.
In our study, we therefore first constructed a
type-level SVS for the 573,127 words in our cor-
pus with a frequency higher than 2. Since the fo-
cus of this study is visualization rather than find-
ing optimal SVS parameter settings, we chose set-
tings that proved optimal in our previous studies
(Peirsman et al, 2008; Heylen et al, 2008; Peirs-
man et al, 2010). For the context features of this
SVS, we used a bag-of-words approach with a
window of 4 to the left and right around the tar-
gets. The context feature set was restricted to the
5430 words, that were the among the 7000 most
frequent words in the corpus, (minus a stoplist of
34 high-frequent function words) AND that oc-
curred at least 50 times in both the Netherlandic
and Belgian part of the corpus. The latter was
done to make sure that Netherlandic and Belgian
type vectors were not dissimilar just because of
topical bias from proper names, place names or
words relating to local events. Raw co-occurrence
frequencies were weighted with Pointwise Mutual
Information and negative PMI?s were set to zero.
In a second step, we took a random sample of
100 Netherlandic and a 100 Belgian newspaper
issues from the corpus and extracted all occur-
rences of each of the 476 nouns in the synsets
described above. For each occurrence, we built
a token-vector by averaging over the type-vectors
of the words in a window of 5 words to the left
18
and right of the token. We experimented with two
averaging functions. In a first version, we fol-
lowed Schu?tze (1998) and just summed the type
vectors of a token?s context words, normalizing
by the number of context words for that token:
~owi =
?n
j?Cwi
~cj
n
where ~owi is the token vector for the i
th occur-
rence of noun w and Cwi is the set of n type
vectors ~cj for the context words in the window
around that ith occurrence of noun w. How-
ever, this summation means that each first order
context word has an equal weight in determining
the token vector. Yet, not all first-order context
words are equally informative for the meaning of
a token. In a sentence like ?While walking to
work, the teacher saw a dog barking and chasing
a cat?, bark and cat are much more indicative of
the meaning of dog than say teacher or work.
In a second, weighted version, we therefore in-
creased the contribution of these informative con-
text words by using the first-order context words?
PMI values with the noun in the synset. PMI can
be regarded as a measure for informativeness and
target-noun/context-word PMI-values were avail-
able anyway from our large type-level SVS. The
PMI of a noun w and a context word cj can now
be seen as a weight pmiwcj . In constructing the to-
ken vector ~owi for the ith occurrence of noun w ,
we now multiply the type vector ~cj of each con-
text word with the PMI weight pmiwcj , and then
normalize by the sum of the pmi-weights:
~owi =
?n
j?Cwi
pmiwcj ? ~cj
?n
j pmi
w
cj
The token vectors of all nouns from the same
synset were then combined in a token by second-
order-context-feature matrix. Note that this ma-
trix has the same dimensionality as the underlying
type-level SVS (5430). By calculating the cosine
between all pairs of token-vectors in the matrix,
we get the final token-by-token similarity matrix
for each of the 218 synsets 3.
3string operations on corpus text files were done with
Python 2.7. All matrix calculations were done in Matlab
R2009a for Linux
5 Visualization
The token-by-token similarity matrices reflect
how the different synonyms carve up the ?seman-
tic space? of the synset?s concept among them-
selves. However, this information is hard to grasp
from a large matrix of decimal figures. One pop-
ular way of visualizing a similarity matrix for
interpretative purposes is Multidimensional Scal-
ing (Cox and Cox, 2001). MDS tries to give an
optimal 2 or 3 dimensional representation of the
similarities (or distances) between objects in the
matrix. We applied Kruskal?s non-metric Multi-
dimensional Scaling to the all the token-by-token
similarity matrices using the isoMDS function in
the MASS package of R. Our visualisation soft-
ware package (see below) forced us to restrict our-
selves to a 2 dimensional MDS solution for now,
even tough stress levels were generally quite high
(0.25 to 0.45). Future implementation may use 3D
MDS solutions. Of course, other dimension re-
duction techniques than MDS exist: PCA is used
in Latent Semantic Analysis (Landauer and Du-
mais, 1997) and has been applied by Sagi et al
(2009) for modeling token semantics. Alterna-
tively, Latent Dirichlect Allocation (LDA) is at
the heart of Topic Models (Griffiths et al, 2007)
and was adapted by Brody and Lapata (2009) for
modeling token semantics. However, these tech-
niques all aim at bringing out a latent structure
that abstracts away from the ?raw? underlying
SVS similarities. Our aim, on the other hand,
is precisely to investigate how SVSs structure se-
mantics based on contextual distribution proper-
ties BEFORE additional latent structuring is ap-
plied. We therefore want a 2D representation of
the token similarity matrix that is as faithful as
possible and that is what MDS delivers 4.
In a next step we wanted to intergrate the 2
dimensional MDS plots with different types of
meta-data that might be of interest to the lexi-
cologist. Furthermore, we wanted the plots to
be interactive, so that a lexicologist can choose
which information to visualize in the plot. We
opted for the Motion Charts5 provided by Google
4Stress is a measure for that faithfulness. No such indi-
cation is directly available for LSA or LDA. However, we do
think LSA and LDA can be used to provide extra structure to
our visualizations, see section 6.
5To avoid dependence on commercial software, we also
made an implementation based on the plotting options of
R and the Python Image Library( https://perswww.
19
Chart Tools6, which allows to plot objects with
2D co-ordinates as color-codable and re-sizeable
bubbles in an interactive chart. If a time-
variable is present, the charts can be made dy-
namic to show the changing position of the ob-
jects in the plot over time7. We used the R-
package googleVis (Gesmann and Castillo,
2011), an interface between R and the Google
Visualisation API, to convert our R datamatri-
ces into Google Motion Charts. The interac-
tive charts, both those based on the weighted
and unweighted token-level SVSs, can be ex-
plored on our website ( https://perswww.
kuleuven.be/?u0038536/googleVis).
To illustrate the information that is avail-
able through this visualization, we discuss the
weighted chart for the concept COMPUTER
SCREEN (Figure 1 shows a screen cap, but we
strongly advise to look at the interactive version
on the website). In Dutch, this concept can be ref-
ered to with (at least) three near-synonyms, which
are color coded in the chart: beeldscherm (blue),
computerscherm (green) and monitor (yellow).
Each bubble in the chart is an occurrence (token)
of one these nouns. As Figure 2 shows, roling
over the bubbles makes the stretch of text visible
in which the noun occurs (These contexts are also
available in the lower right side bar). This usage-
in-context allows the lexicologist to interpret the
precise meaning of the occurrence of the noun.
The plot itself is a 2D representation of the seman-
tic distances between all tokens (as measured with
a token-level SVS) and reflects how the synonyms
are distributed over the ?semantic space?. As can
be expected with synonyms, they partially popu-
late the same area of the space (the right hand side
of the plot). Hovering over the bubbles and look-
ing at the contexts, we can see that they indeed
all refer to the concept COMPUTER SCREEN (See
example contexts 1 to 3 in Table 2). However, we
also see that a considerable part on the left hand
side of the plot shows no overlap and is only popu-
lated by tokens of monitor. Looking more closely
kuleuven.be/?u0038536/committees)
6(http://code.google.com/apis/chart/
interactive/docs/gallery/motionchart.
html)
7Since we worked with synchronic data, we did not
use this feature. However, Motion Charts have been used
by Hilpert (http://omnibus.uni-freiburg.de/
?mh608/motion.html) to visualize language change in
MDS plots of hand coded diachronic linguistic data.
at these occurrences, we see that they are instan-
tiations of another meaning of monitor, viz. ?su-
pervisor of youth leisure activities? (See example
context 4 in Table 2). Remember that our corpus
is stratified for Belgian and Netherlandic Dutch.
We can make this stratification visible by chang-
ing the color coding of the bubbles to COUNTRY
in the top right-hand drop-down menu. Figure 3
shows that the left-hand side, i.e. monitor-only
area of the plot, is also an all-Belgian area (hov-
ering over the BE value in the legend makes the
Belgian tokens in the plot flash). Changing the
color coding to WORDBYCOUNTRY makes this
even more clear. Indeed the youth leader mean-
ing of monitor is only familiar to speakers of Bel-
gian Dutch. Changing the color coding to the
variable NEWSPAPER shows that the youth leader
meaning is also typical for the popular, working
class newspapers Het Laatste Nieuws (LN) and
Het Nieuwsblad (NB) and is not prevelant in the
Belgian high-brow newspapers. In order to pro-
vide more structure to the plot, we also experi-
mented with including different K-means cluster-
ing solutions (from 2 up to 6 clusters) as color-
codable features, but these seem not very infor-
mative yet (but see section 6).
nr example context
1 De analisten houden met e?e?n oog de computerschermen
in de gaten
The analists keep one eye on the computer screen
2 Met een digitale camera... kan je je eigen foto op het
beeldscherm krijgen
With a digital camera, you can get your own photo on the
computer screen
3 Met een paar aanpassingen wordt het beeld op de moni-
toren nog completer
With a few adjustments, the image on the screen becomes
even more complete
4 Voor augustus zijn de speelpleinen nog op zoek naar mon-
itoren
For August, the playgrounds are still looking for supervi-
sors
Table 2: Contexts (shown in chart by mouse roll-over)
On the whole, the token-level SVS succeeds
fairly well in giving an interpretable semantic
structure to the tokens and the chart visualizes
this. However, SVSs are fully automatic ways of
modeling semantics and, not unexpectedly, some
tokens are out of place. For example, in the lower
left corner of the yellow cluster with monitor to-
kens referring to youth leader, there is also one
blue Netherlandic token of beeldscherm. Thanks
to the visualisation, such outliers can easily be
20
detected by the lexicologist who can then report
them to the computational linguist. The latter can
then try to come up with a model that gives a bet-
ter fit.
Finally, let us briefly look at the chart of another
concept, viz. COLLISION with its near-synonyms
aanrijding and botsing. Here, we expect the lit-
eral collissions (between cars), for which both
nouns can be used, to stand out form the figura-
tive ones (differences in opinion between people),
for which only botsing is apropriate in both vari-
eties of Dutch. Figure 4 indeed shows that the
right side of the chart is almost exclusively popu-
lated by botsing tokens. Looking at their contexts
reveals that they indeed overwhelmingly instan-
tiate the metaphorical meaning og collision. Yet
also here, there are some ?lost? aanrijding tokens
with a literal meaning and the visualization shows
that the current SVS implementation is not yet a
fully adequate model for capturing the words? se-
mantics.
6 General discussion
Although Vector Spaces have become the main-
stay of modeling lexical semantics in current sta-
tistical NLP, they are mostly used in a black box
way, and how exactly they capture word meaning
is not very clear. By visualizing their output, we
hope to have at least partially cracked open this
black box. Our aim is not just to make SVS out-
put easier to analyze for computer linguists. We
also want to make SVSs accessible for lexicolo-
gists and lexicographers with an interest in quanti-
tative, empirical data analysis. Such co-operation
brings mutual benefits: Computer linguists get ac-
cess to expert evaluation of their models. Lexicol-
ogists and lexicographers can use SVSs to iden-
tify preliminary semantic structure based on large
quantities of corpus data, instead of heaving to
sort through long lists of unstructured examples
of a word?s usage (the classical concordances). To
our knowledge, this paper is one of the first at-
tempts to visualize Semantic Vector Spaces and
make them accessible to a non-technical audi-
ence.
Of course, this is still largely work in progress
and a number of improvements and extensions are
still possible. First of all, the call-outs for the
bubbles in the Google Motion Charts were not
designed to contain large stretches of text. Cur-
rent corpus contexts are therefore to short to ana-
lyze the precise meaning of the tokens. One op-
tion would be to have pop-up windows with larger
contexts appear by clicking on the call-outs.
Secondly, we didn?t use the motion feature that
gave the charts its name. However, if we have
diachronic data, we could e.g. track the centroid
of a word?s tokens in the semantic space through
time and at the same time show the dispersion of
tokens around that centroid8.
Thirdly, in the current implementation, one im-
portant aspect of the black-box quality of SVSs
is not dealt with: it?s not clear which context
features cause tokens to be similar in the SVS
output, and, consequently, the interpreation of
the distances in the MDS plot remains quite ob-
scure. One option would be to use the cluster
solutions, that are already available as color cod-
able variables, and indicate the highest scoring
context features that the tokens in each cluster
have in common. Another option for bringing out
sense-distinguishing context words was proposed
by Rohrdantz et al (2011) who use Latent Dirich-
let Allocation to structure tokens. The loadings
on these latent topics could also be color-coded in
the chart.
Fourthly, we already indicated that two dimen-
sional MDS solutions have quite high stress val-
ues and a three dimensional solution would be
better to represent the token-by-token similari-
ties. This would require the 3D Charts, which are
not currently offered by the Google Chart Tools.
However both R and Matlab do have interactive
3D plotting functionality.
Finally, and most importantly, the plots cur-
rently do not allow any input from the user. If
we want the plots to be the starting point of an in-
depth semantic analysis, the lexicologist should
be able to annotate the occurrences with variables
of their own. For example, they might want to
code whether the occurrence refers to a laptop
screen, a desktop screen or cell phone screen, to
find out whether their is a finer-grained division of
labor among the synonyms. Additionally, an eval-
uation of the SVS?s performance might include
moving wrongly positioned tokens in the plot and
thus re-group tokens, based on the lexicologist?s
insights. Tracking these corrective movements
might then be valuable input for the computer lin-
guists to improve their models. Of course, this
8This is basically the approach of Sagi et al (2009) but
after LSA and without interactive visualization
21
goes well beyond our rather opportunistic use of
the Google Charts Tool.
References
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 1?10, Edinburgh, UK. Association for Com-
putational Linguistics.
Samuel Brody and Mirella Lapata. 2009. Bayesian
Word Sense Induction. In Proceedings of the 12th
Conference of the European Chapter of the ACL
(EACL 2009), pages 103?111, Athens, Greece. As-
sociation for Computational Linguistics.
Paul Cook and Suzanne Stevenson. 2010. Automat-
ically Identifying Changes in the Semantic Orien-
tation of Words. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC?10), pages 28?34, Valletta,
Malta. ELRA.
Trevor Cox and Michael Cox. 2001. Multidimen-
sional Scaling. Chapman & Hall, Boca Raton.
Dirk Geeraerts. 2010a. The doctor and the seman-
tician. In Dylan Glynn and Kerstin Fischer, edi-
tors, Quantitative Methods in Cognitive Semantics:
Corpus-Driven Approaches, pages 63?78. Mouton
de Gruyter, Berlin.
Dirk Geeraerts. 2010b. Theories of Lexical Semantics.
Oxford University Press, Oxford.
Markus Gesmann and Diego De Castillo. 2011. Using
the Google Visualisation API with R: googleVis-
0.2.4 Package Vignette.
Dylan Glynn and Kerstin Fischer. 2010. Quanti-
tative Methods in Cognitive Semantics: Corpus-
driven Approaches, volume 46. Mouton de Gruyter,
Berlin.
Thomas L. Griffiths, Mark Steyvers, and Joshua
Tenenbaum. 2007. Topics in Semantic Represen-
tation. Psychological Review, 114:211?244.
Kris Heylen, Yves Peirsman, Dirk Geeraerts, and Dirk
Speelman. 2008. Modelling Word Similarity. An
Evaluation of Automatic Synonymy Extraction Al-
gorithms. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC 2008),
pages 3243?3249, Marrakech, Morocco. ELRA.
Thomas K Landauer and Susan T Dumais. 1997. A
Solution to Plato?s Problem: The Latent Semantic
Analysis Theory of Acquisition, Induction and Rep-
resentation of Knowledge. Psychological Review,
104(2):240?411.
Roeland J F Ordelman. 2002. Twente Nieuws Cor-
pus (TwNC). Technical report, Parlevink Language
Techonology Group. University of Twente.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics,
33(2):161?199.
Patrick Pantel and Dekang Lin. 2002. Document clus-
tering with committees. In Proceedings of the 25th
annual international ACM SIGIR conference on Re-
search and development in information retrieval,
SIGIR ?02, pages 199?206, New York, NY, USA.
ACM.
Yves Peirsman, Kris Heylen, and Dirk Geeraerts.
2008. Size matters: tight and loose context defini-
tions in English word space models. In Proceedings
of the ESSLLI Workshop on Distributional Lexical
Semantics, pages 34?41, Hamburg, Germany. ESS-
LLI.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?490.
Christian Rohrdantz, Annette Hautli, Thomas Mayer,
Miriam Butt, Daniel A Keim, and Frans Plank.
2011. Towards Tracking Semantic Change by Vi-
sual Analytics. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
305?310, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Tom Ruette, Dirk Geeraerts, Yves Peirsman, and Dirk
Speelman. 2012. Semantic weighting mechanisms
in scalable lexical sociolectometry. In Benedikt
Szmrecsanyi and Bernhard Wa?lchli, editors, Aggre-
gating dialectology and typology: linguistic vari-
ation in text and speech, within and across lan-
guages. Mouton de Gruyter, Berlin.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic Density Analysis: Comparing Word
Meaning across Time and Phonetic Space. In Pro-
ceedings of the Workshop on Geometrical Mod-
els of Natural Language Semantics, pages 104?
111, Athens, Greece. Association for Computa-
tional Linguistics.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In Verbum Ex Machina. Actes de la
13e conference sur le traitement automatique des
langues naturelles (TALN06), pages 20?42, Leuven,
Belgium. Presses universitaires de Louvain.
22
Figure 1: Screencap of Motion Chart for COMPUTER SCREEN
Figure 2: token of beeldscherm with context
23
Figure 3: COMPUTER SCREEN tokens stratified by country
Figure 4: Screencap of Motion Chart for COLLISION
24
