Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1024?1033, Prague, June 2007. c?2007 Association for Computational Linguistics
A Topic Model for Word Sense Disambiguation
Jordan Boyd-Graber
Computer Science
Princeton University
Princeton, NJ 08540
jbg@princeton.edu
David Blei
Computer Science
Princeton University
Princeton, NJ 08540
blei@cs.princeton.edu
Xiaojin Zhu
Computer Science
University of Wisconsin
Madison, WI 53706
jerryzhu@cs.wisc.edu
Abstract
We develop latent Dirichlet alocation with
WORDNET (LDAWN), an unsupervised
probabilistic topic model that includes word
sense as a hidden variable. We develop a
probabilistic posterior inference algorithm
for simultaneously disambiguating a corpus
and learning the domains in which to con-
sider each word. Using the WORDNET hi-
erarchy, we embed the construction of Ab-
ney and Light (1999) in the topic model and
show that automatically learned domains
improve WSD accuracy compared to alter-
native contexts.
1 Introduction
Word sense disambiguation (WSD) is the task of
determining the meaning of an ambiguous word in
its context. It is an important problem in natural
language processing (NLP) because effective WSD
can improve systems for tasks such as information
retrieval, machine translation, and summarization.
In this paper, we develop latent Dirichlet aloca-
tion with WORDNET (LDAWN), a generative prob-
abilistic topic model for WSD where the sense of
the word is a hidden random variable that is inferred
from data.
There are two central advantages to this approach.
First, with LDAWN we automatically learn the con-
text in which a word is disambiguated. Rather
than disambiguating at the sentence-level or the
document-level, our model uses the other words that
share the same hidden topic across many documents.
Second, LDAWN is a fully-fledged generative
model. Generative models are modular and can be
easily combined and composed to form more com-
plicated models. (As a canonical example, the ubiq-
uitous hidden Markov model is a series of mixture
models chained together.) Thus, developing a gen-
erative model for WSD gives other generative NLP
algorithms a natural way to take advantage of the
hidden senses of words.
In general, topic models are statistical models of
text that posit a hidden space of topics in which the
corpus is embedded (Blei et al, 2003). Given a
corpus, posterior inference in topic models amounts
to automatically discovering the underlying themes
that permeate the collection. Topic models have re-
cently been applied to information retrieval (Wei and
Croft, 2006), text classification (Blei et al, 2003),
and dialogue segmentation (Purver et al, 2006).
While topic models capture the polysemous use
of words, they do not carry the explicit notion of
sense that is necessary for WSD. LDAWN extends
the topic modeling framework to include a hidden
meaning in the word generation process. In this
case, posterior inference discovers both the topics
of the corpus and the meanings assigned to each of
its words.
After introducing a disambiguation scheme based
on probabilistic walks over the WORDNET hierar-
chy (Section 2), we embed the WORDNET-WALK
in a topic model, where each topic is associated with
walks that prefer different neighborhoods of WORD-
NET (Section 2.1). Then, we describe a Gibbs sam-
pling algorithm for approximate posterior inference
that learns the senses and topics that best explain a
corpus (Section 3). Finally, we evaluate our system
on real-world WSD data, discuss the properties of
the topics and disambiguation accuracy results, and
draw connections to other WSD algorithms from the
research literature.
1024
1740
entity 1930
3122object
20846
15024
animal 1304946
1305277
artifact male
2354808 2354559
foalcolt
3042424
colt
4040311
revolver
Synset ID
Word
six-gun
six-shooter
0.00 0.25
0.58
0.00 0.04
0.02 0.010.16
0.05
0.04
0.690.00
0.00
0.381.000.42 0.00
0.000.57
1.00
0.38
0.07
Figure 1: The possible paths to reach the word ?colt?
in WORDNET. Dashed lines represent omitted links.
All words in the synset containing ?revolver? are
shown, but only one word from other synsets is
shown. Edge labels are probabilities of transitioning
from synset i to synset j. Note how this favors fre-
quent terms, such as ?revolver,? over ones like ?six-
shooter.?
2 Topic models and WordNet
The WORDNET-WALK is a probabilistic process of
word generation that is based on the hyponomy re-
lationship in WORDNET (Miller, 1990). WORD-
NET, a lexical resource designed by psychologists
and lexicographers to mimic the semantic organiza-
tion in the human mind, links ?synsets? (short for
synonym sets) with myriad connections. The spe-
cific relation we?re interested in, hyponomy, points
from general concepts to more specific ones and is
sometimes called the ?is-a? relationship.
As first described by Abney and Light (1999), we
imagine an agent who starts at synset [entity],
which points to every noun in WORDNET 2.1 by
some sequence of hyponomy relations, and then
chooses the next node in its random walk from the
hyponyms of its current position. The agent repeats
this process until it reaches a leaf node, which corre-
sponds to a single word (each of the synset?s words
are unique leaves of a synset in our construction).
For an example of all the paths that might gener-
ate the word ?colt? see Figure 1. The WORDNET-
WALK is parameterized by a set of distributions over
children for each synset s in WORDNET, ?s.
Symbol Meaning
K number of topics
?k,s multinomial probability vector over
the successors of synset s in topic k
S scalar that, when multiplied by ?s
gives the prior for ?k,s
?s normalized vector whose ith entry,
when multiplied by S, gives the prior
probability for going from s to i
?d multinomial probability vector over
the topics that generate document d
? prior for ?
z assignment of a word to a topic
? a path assignment through
WORDNET ending at a word.
?i,j one link in a path ? going from syn-
set i to synset j.
Table 1: A summary of the notation used in the pa-
per. Bold vectors correspond to collections of vari-
ables (i.e. zu refers to a topic of a single word, but
z1:D are the topics assignments of words in docu-
ment 1 through D).
2.1 A topic model for WSD
The WORDNET-WALK has two important proper-
ties. First, it describes a random process for word
generation. Thus, it is a distribution over words
and thus can be integrated into any generative model
of text, such as topic models. Second, the synset
that produces each word is a hidden random vari-
able. Given a word assumed to be generated by a
WORDNET-WALK, we can use posterior inference
to predict which synset produced the word.
These properties allow us to develop LDAWN,
which is a fusion of these WORDNET-WALKs and
latent Dirichlet alocation (LDA) (Blei et al, 2003),
a probabilistic model of documents that is an im-
provement to pLSI (Hofmann, 1999). LDA assumes
that there are K ?topics,? multinomial distributions
over words, which describe a collection. Each docu-
ment exhibits multiple topics, and each word in each
document is associated with one of them.
Although the term ?topic? evokes a collection of
ideas that share a common theme and although the
topics derived by LDA seem to possess semantic
coherence, there is no reason to believe this would
1025
be true of the most likely multinomial distributions
that could have created the corpus given the assumed
generative model. That semantically similar words
are likely to occur together is a byproduct of how
language is actually used.
In LDAWN, we replace the multinomial topic dis-
tributions with a WORDNET-WALK, as described
above. LDAWN assumes a corpus is generated by
the following process (for an overview of the nota-
tion used in this paper, see Table 1).
1. For each topic, k ? {1, . . . ,K}
(a) For each synset s, randomly choose transition prob-
abilities ?k,s ? Dir(S?s).
2. For each document d ? {1, . . . , D}
(a) Select a topic distribution ?d ? Dir(?)
(b) For each word n ? {1, . . . , Nd}
i. Select a topic z ? Mult(1, ?d)
ii. Create a path ?d,n starting with ?0 as the root
node.
iii. From children of ?i:
A. Choose the next node in the walk ?i+1 ?
Mult(1, ?z,?i)
B. If ?i+1 is a leaf node, generate the associ-
ated word. Otherwise, repeat.
Every element of this process, including the
synsets, is hidden except for the words of the doc-
uments. Thus, given a collection of documents, our
goal is to perform posterior inference, which is the
task of determining the conditional distribution of
the hidden variables given the observations. In the
case of LDAWN, the hidden variables are the param-
eters of the K WORDNET-WALKs, the topic assign-
ments of each word in the collection, and the synset
path of each word. In a sense, posterior inference
reverses the process described above.
Specifically, given a document collection w1:D,
the full posterior is
p(?1:K ,z1:D,?1:D,?1:D |w1:D, ?, S?) ?(?K
k=1 p(?k |S?)
?D
d=1 p(?d | ?)
?Nd
n=1 p(?d,n |?1:K)p(wd,n |?d,n)
)
, (1)
where the constant of proportionality is the marginal
likelihood of the observed data.
Note that by encoding the synset paths as a hid-
den variable, we have posed the WSD problem as
a question of posterior probabilistic inference. Fur-
ther note that we have developed an unsupervised
model. No labeled data is needed to disambiguate a
corpus. Learning the posterior distribution amounts
to simultaneously decomposing a corpus into topics
and its words into their synsets.
The intuition behind LDAWN is that the words
in a topic will have similar meanings and thus share
paths within WORDNET. For example, WORDNET
has two senses for the word ?colt;? one referring to a
young male horse and the other to a type of handgun
(see Figure 1).
Although we have no a priori way of know-
ing which of the two paths to favor for a
document, we assume that similar concepts
will also appear in the document. Documents
with unambiguous nouns such as ?six-shooter?
and ?smoothbore? would make paths that pass
through the synset [firearm, piece,
small-arm] more likely than those go-
ing through [animal, animate being,
beast, brute, creature, fauna]. In
practice, we hope to see a WORDNET-WALK that
looks like Figure 2, which points to the right sense
of cancer for a medical context.
LDAWN is a Bayesian framework, as each vari-
able has a prior distribution. In particular, the
Dirichlet prior for ?s, specified by a scaling factor
S and a normalized vector ?s fulfills two functions.
First, as the overall strength of S increases, we place
a greater emphasis on the prior. This is equivalent to
the need for balancing as noted by Abney and Light
(1999).
The other function that the Dirichlet prior serves
is to enable us to encode any information we have
about how we suspect the transitions to children
nodes will be distributed. For instance, we might ex-
pect that the words associated with a synset will be
produced in a way roughly similar to the token prob-
ability in a corpus. For example, even though ?meal?
might refer to both ground cereals or food eaten at
a single sitting and ?repast? exclusively to the lat-
ter, the synset [meal, repast, food eaten
at a single sitting] still prefers to transi-
tion to ?meal? over ?repast? given the overall corpus
counts (see Figure 1, which shows prior transition
probabilities for ?revolver?).
By setting ?s,i, the prior probability of transition-
ing from synset s to node i, proportional to the to-
tal number of observed tokens in the children of i,
1026
we introduce a probabilistic variation on informa-
tion content (Resnik, 1995). As in Resnik?s defini-
tion, this value for non-word nodes is equal to the
sum of all the frequencies of hyponym words. Un-
like Resnik, we do not divide frequency among all
senses of a word; each sense of a word contributes
its full frequency to ?.
3 Posterior Inference with Gibbs Sampling
As described above, the problem of WSD corre-
sponds to posterior inference: determining the prob-
ability distribution of the hidden variables given ob-
served words and then selecting the synsets of the
most likely paths as the correct sense. Directly com-
puting this posterior distribution, however, is not
tractable because of the difficulty of calculating the
normalizing constant in Equation 1.
To approximate the posterior, we use Gibbs sam-
pling, which has proven to be a successful approx-
imate inference technique for LDA (Griffiths and
Steyvers, 2004). In Gibbs sampling, like all Markov
chain Monte Carlo methods, we repeatedly sample
from aMarkov chain whose stationary distribution is
the posterior of interest (Robert and Casella, 2004).
Even though we don?t know the full posterior, the
samples can be used to form an empirical estimate
of the target distribution. In LDAWN, the samples
contain a configuration of the latent semantic states
of the system, revealing the hidden topics and paths
that likely led to the observed data.
Gibbs sampling reproduces the posterior distri-
bution by repeatedly sampling each hidden variable
conditioned on the current state of the other hidden
variables and observations. More precisely, the state
is given by a set of assignments where each word
is assigned to a path through one of K WORDNET-
WALK topics: uth word wu has a topic assignment
zu and a path assignment ?u. We use z?u and ??u
to represent the topic and path assignments of all
words except for u, respectively.
Sampling a new topic for the word wu requires
us to consider all of the paths that wu can take in
each topic and the topics of the other words in the
document u is in. The probability of wu taking on
topic i is proportional to
p(zu = i |z?u)
?
? p(? |??u)1[wu ? ?], (2)
which is the probability of selecting z from ?d times
the probability of a path generating wu from a path
in the ith WORDNET-WALK.
The first term, the topic probability of the uth
word, is based on the assignments to the K topics
for words other than u in this document,
p(zu = i|z?u) =
n(d)?u,i + ?i
?
j n
(d)
?u,j +
?K
j=1 ?j
, (3)
where n(d)?u,j is the number of words other than u in
topic j for the document d that u appears in.
The second term in Equation 2 is a sum over the
probabilities of every path that could have generated
the word wu. In practice, this sum can be com-
puted using a dynamic program for all nodes that
have unique parent (i.e. those that can?t be reached
by more than one path). Although the probability of
a path is specific to the topic, as the transition prob-
abilities for a synset are different across topics, we
will omit the topic index in the equation,
p(?u = ?|??u, ) =
?l?1
i=1 ?
?u
?i,?i+1
. (4)
3.1 Transition Probabilities
Computing the probability of a path requires us to
take a product over our estimate of the probability
from transitioning from i to j for all nodes i and j in
the path ?. The other path assignments within this
topic, however, play an important role in shaping the
transition probabilities.
From the perspective of a single node i, only paths
that pass through that node affect the probability of
u also passing through that node. It?s convenient to
have an explicit count of all of the paths that tran-
sition from i to j in this topic?s WORDNET-WALK,
so we use T?ui,j to represent all of the paths that go
from i to j in a topic other than the path currently
assigned to u.
Given the assignment of all other words to paths,
calculating the probability of transitioning from i to
j with word u requires us to consider the prior ? and
the observations Ti,j in our estimate of the expected
value of the probability of transitioning from i to j,
??ui,j =
T?ui,j + Si?i,j
Si +
?
k T
?u
i,k
. (5)
1027
As mentioned in Section 2.1, we paramaterize the
prior for synset i as a vector ?i, which sums to one,
and a scale parameter S.
The next step, once we?ve selected a topic, is to
select a path within that topic. This requires the
computation of the path probabilities as specified in
Equation 4 for all of the paths wu can take in the
sampled topic and then sampling from the path prob-
abilities.
The Gibbs sampler is essentially a randomized
hill climbing algorithm on the posterior likelihood as
a function of the configuration of hidden variables.
The numerator of Equation 1 is proportional to that
posterior and thus allows us to track the sampler?s
progress. We assess convergence to a local mode of
the posterior by monitoring this quantity.
4 Experiments
In this section, we describe the properties of the
topics induced by running the previously described
Gibbs sampling method on corpora and how these
topics improve WSD accuracy.
Of the two data sets used during the course of
our evaluation, the primary dataset was SEMCOR
(Miller et al, 1993), which is a subset of the Brown
corpus with many nouns manually labeled with the
correct WORDNET sense. The words in this dataset
are lemmatized, and multi-word expressions that are
present in WORDNET are identified. Only the words
in SEMCOR were used in the Gibbs sampling pro-
cedure; the synset assignments were only used for
assessing the accuracy of the final predictions.
We also used the British National Corpus, which
is not lemmatized and which does not have multi-
word expressions. The text was first run through
a lemmatizer, and then sequences of words which
matched a multi-word expression in WORDNET
were joined together into a single word. We took
nouns that appeared in SEMCOR twice or in the
BNC at least 25 times and used the BNC to com-
pute the information-content analog ? for individ-
ual nouns (For example, the probabilities in Figure 1
correspond to ?).
4.1 Topics
Like the topics created by structures such as LDA,
the topics in Table 2 coalesce around reasonable
themes. The word list was compiled by summing
over all of the possible leaves that could have gen-
erated each of the words and sorting the words by
decreasing probability. In the vast majority of cases,
a single synset?s high probability is responsible for
the words? positions on the list.
Reassuringly, many of the top senses for the
present words correspond to the most frequent sense
in SEMCOR. For example, in Topic 4, the senses for
?space? and ?function? correspond to the top senses
in SEMCOR, and while the top sense for ?set? corre-
sponds to ?an abstract collection of numbers or sym-
bols? rather than ?a group of the same kind that be-
long together and are so used,? it makes sense given
the math-based words in the topic. ?Point,? however,
corresponds to the sense used in the phrase ?I got to
the point of boiling the water,? which is neither the
top SEMCOR sense nor a sense which makes sense
given the other words in the topic.
While the topics presented in Table 2 resemble
the topics one would obtain through models like
LDA (Blei et al, 2003), they are not identical. Be-
cause of the lengthy process of Gibbs sampling, we
initially thought that using LDA assignments as an
initial state would converge faster than a random ini-
tial assignment. While this was the case, it con-
verged to a state that less probable than the randomly
initialized state and no better at sense disambigua-
tion (and sometimes worse). The topics presented
in 2 represent words both that co-occur together in
a corpus and co-occur on paths through WORDNET.
Because topics created through LDA only have the
first property, they usually do worse in terms of both
total probability and disambiguation accuracy (see
Figure 3).
Another interesting property of topics in LDAWN
is that, with higher levels of smoothing, words that
don?t appear in a corpus (or appear rarely) but are
in similar parts of WORDNET might have relatively
high probability in a topic. For example, ?maturity?
in topic two in Table 2 is sandwiched between ?foot?
and ?center,? both of which occur about five times
more than ?maturity.? This might improve LDA-
based information retrieval schemes (Wei and Croft,
2006) .
1028
1740
1930
0.23 0.76
3122 0.42
0.01
2236
0.10 0.00
0.00
0.00
0.00
7626
someone
0.00
9609711
0.00
9120316
1743824
0.00
cancer
7998922 genus0.04
0.04
8564599
star_sign
0.06
8565580
0.06
cancer
0.5
9100327
cancer
1
constellation
0.01
0.01
cancer
0.5
crab
0.5
13875408
0.58 0.19
14049094 14046733
tumor
0.97
14050958
0.00
malignancy
0.06
0.94
14051451
0.90
cancer
0.96
Synset ID
Transition Prob
Word
1957888
1.0
Figure 2: The possible paths to reach the word ?cancer? in WORDNET along with transition probabilities
from the medically-themed Topic 2 in Table 2, with the most probable path highlighted. The dashed lines
represent multiple links that have been consolidated, and synsets are represented by their offsets within
WORDNET 2.1. Some words for immediate hypernyms have also been included to give context. In all other
topics, the person, animal, or constellation senses were preferred.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7
president growth material point water plant music
party age object number house change film
city treatment color value road month work
election feed form function area worker life
administration day subject set city report time
official period part square land mercer world
office head self space home requirement group
bill portion picture polynomial farm bank audience
yesterday length artist operator spring farmer play
court level art component bridge production thing
meet foot patient corner pool medium style
police maturity communication direction site petitioner year
service center movement curve interest relationship show
Table 2: The most probable words from six randomly chosen WORDNET-walks from a thirty-two topic
model trained on the words in SEMCOR. These are summed over all of the possible synsets that generate
the words. However, the vast majority of the contributions come from a single synset.
1029
 
0.275 0.28
 
0.285 0.29
 
0.295 0.3
 
0.305
 
0
 
1000
 
2000
 
3000
 
4000
 
5000
 
6000
 
7000
 
8000
 
9000
 
10000
Accuracy
Iteratio
n
Unsee
ded
Seede
d with 
LDA
-
96000
-
94000
-
92000
-
90000
-
88000
-
86000
-
84000
-
82000
-
80000
 
0
 
1000
 
2000
 
3000
 
4000
 
5000
 
6000
 
7000
 
8000
 
9000
 
10000
Model Probability
Iteratio
n
Unsee
ded
Seede
d with 
LDA
Figure 3: Topics seeded with LDA initially have
a higher disambiguation accuracy, but are quickly
matched by unseeded topics. The probability for the
seeded topics starts lower and remains lower.
4.2 Topics and the Weight of the Prior
Because the Dirichlet smoothing factor in part
determines the topics, it also affects the disam-
biguation. Figure 4 shows the modal disambigua-
tion achieved for each of the settings of S =
{0.1, 1, 5, 10, 15, 20}. Each line is one setting of K
and each point on the line is a setting of S. Each
data point is a run for the Gibbs sampler for 10,000
iterations. The disambiguation, taken at the mode,
improved with moderate settings of S, which sug-
gests that the data are still sparse for many of the
walks, although the improvement vanishes if S dom-
inates with much larger values. This makes sense,
as each walk has over 100,000 parameters, there are
fewer than 100,000 words in SEMCOR, and each
 
0.24
 
0.26
 
0.28 0.3
 
0.32
 
0.34
 
0.36
 
0.38
S=20
S=15
S=10
S=5
S=1
S=0.1
Accuracy
Smoot
hing F
actor
64 top
ics
32 top
ics
16 top
ics 8 topic
s
4 topic
s
2 topic
s
1 topic Rando
m
Figure 4: Each line represents experiments with a set
number of topics and variable amounts of smooth-
ing on the SEMCOR corpus. The random baseline
is at the bottom of the graph, and adding topics im-
proves accuracy. As smoothing increases, the prior
(based on token frequency) becomes stronger. Ac-
curacy is the percentage of correctly disambiguated
polysemous words in SEMCOR at the mode.
word only serves as evidence to at most 19 parame-
ters (the length of the longest path in WORDNET).
Generally, a greater number of topics increased
the accuracy of the mode, but after around sixteen
topics, gains became much smaller. The effect of ?
is also related to the number of topics, as a value of S
for a very large number of topics might overwhelm
the observed data, while the same value of S might
be the perfect balance for a smaller number of topics.
For comparison, the method of using a WORDNET-
WALK applied to smaller contexts such as sentences
or documents achieves an accuracy of between 26%
and 30%, depending on the level of smoothing.
5 Error Analysis
This method works well in cases where the delin-
eation can be readily determined from the over-
all topic of the document. Words such as ?kid,?
?may,? ?shear,? ?coach,? ?incident,? ?fence,? ?bee,?
and (previously used as an example) ?colt? were
all perfectly disambiguated by this method. Figure
2 shows the WORDNET-WALK corresponding to a
medical topic that correctly disambiguates ?cancer.?
Problems arose, however, with highly frequent
1030
words, such as ?man? and ?time? that have many
senses and can occur in many types of documents.
For example, ?man? can be associated with many
possible meanings: island, game equipment, ser-
vant, husband, a specific mammal, etc.
Although we know that the ?adult male? sense
should be preferred, the alternative meanings will
also be likely if they can be assigned to a topic
that shares common paths in WORDNET; the doc-
uments contain, however, many other places, jobs,
and animals which are reasonable explanations (to
LDAWN) of how ?man? was generated. Unfortu-
nately, ?man? is such a ubiquitous term that top-
ics, which are derived from the frequency of words
within an entire document, are ultimately uninfor-
mative about its usage.
While mistakes on these highly frequent terms
significantly hurt our accuracy, errors associated
with less frequent terms reveal that WORDNET?s
structure is not easily transformed into a probabilis-
tic graph. For instance, there are two senses of
the word ?quarterback,? a player in American foot-
ball. One is position itself and the other is a per-
son playing that position. While one would expect
co-occurrence in sentences such as ?quarterback is a
easy position, so our quarterback is happy,? the paths
to both terms share only the root node, thus making
it highly unlikely a topic would cover both senses.
Because of WORDNET?s breadth, rare senses
also impact disambiguation. For example, the
metonymical use of ?door? to represent a whole
building as in the phrase ?girl next door? is un-
der the same parent as sixty other synsets contain-
ing ?bridge,? ?balcony,? ?body,? ?arch,? ?floor,? and
?corner.? Surrounded by such common terms that
are also likely to co-occur with the more conven-
tional meanings of door, this very rare sense be-
comes the preferred disambiguation of ?door.?
6 Related Work
Abney and Light?s initial probabilistic WSD ap-
proach (1999) was further developed into a Bayesian
network model by Ciaramita and Johnson (2000),
who likewise used the appearance of monosemous
terms close to ambiguous ones to ?explain away? the
usage of ambiguous terms in selectional restrictions.
We have adapted these approaches and put them into
the context of a topic model.
Recently, other approaches have created ad hoc
connections between synsets in WORDNET and then
considered walks through the newly created graph.
Given the difficulties of using existing connections
in WORDNET, Mihalcea (2005) proposed creating
links between adjacent synsets that might comprise
a sentence, initially setting weights to be equal to
the Lesk overlap between the pairs, and then using
the PageRank algorithm to determine the stationary
distribution over synsets.
6.1 Topics and Domains
Yarowsky was one of the first to contend that ?there
is one sense for discourse? (1992). This has lead
to the approaches like that of Magnini (Magnini et
al., 2001) that attempt to find the category of a text,
select the most appropriate synset, and then assign
the selected sense using domain annotation attached
to WORDNET.
LDAWN is different in that the categories are not
an a priori concept that must be painstakingly anno-
tated within WORDNET and require no augmenta-
tion of WORDNET. This technique could indeed be
used with any hierarchy. Our concepts are the ones
that best partition the space of documents and do the
best job of describing the distinctions of diction that
separate documents from different domains.
6.2 Similarity Measures
Our approach gives a probabilistic method of us-
ing information content (Resnik, 1995) as a start-
ing point that can be adjusted to cluster words in
a given topic together; this is similar to the Jiang-
Conrath similarity measure (1997), which has been
used in many applications in addition to disambigua-
tion. Patwardhan (2003) offers a broad evaluation of
similarity measures for WSD.
Our technique for combining the cues of topics
and distance in WORDNET is adjusted in a way sim-
ilar in spirit to Buitelaar and Sacaleanu (2001), but
we consider the appearance of a single term to be
evidence for not just that sense and its immediate
neighbors in the hyponomy tree but for all of the
sense?s children and ancestors.
Like McCarthy (2004), our unsupervised system
acquires a single predominant sense for a domain
based on a synthesis of information derived from a
1031
textual corpus, topics, and WORDNET-derived sim-
ilarity, a probabilistic information content measure.
By adding syntactic information from a thesaurus
derived from syntactic features (taken from Lin?s au-
tomatically generated thesaurus (1998)), McCarthy
achieved 48% accuracy in a similar evaluation on
SEMCOR; LDAWN is thus substantially less effec-
tive in disambiguation compared to state-of-the-art
methods. This suggests, however, that other meth-
ods might be improved by adding topics and that our
method might be improved by using more informa-
tion than word counts.
7 Conclusion and Future Work
The LDAWN model presented here makes two con-
tributions to research in automatic word sense dis-
ambiguation. First, we demonstrate a method for au-
tomatically partitioning a document into topics that
includes explicit semantic information. Second, we
show that, at least for one simple model of WSD,
embedding a document in probabilistic latent struc-
ture, i.e., a ?topic,? can improve WSD.
There are two avenues of research with LDAWN
that we will explore. First, the statistical nature of
this approach allows LDAWN to be used as a com-
ponent in larger models for other language tasks.
Other probabilistic models of language could in-
sert the ability to query synsets or paths of WORD-
NET. Similarly, any topic based information re-
trieval scheme could employ topics that include se-
mantically relevant (but perhaps unobserved) terms.
Incorporating this model in a larger syntactically-
aware model, which could benefit from the local
context as well as the document level context, is an
important component of future research.
Second, the results presented here show a marked
improvement in accuracy as more topics are added
to the baseline model, although the final result is not
comparable to state-of-the-art techniques. As most
errors were attributable to the hyponomy structure
of WORDNET, incorporating the novel use of topic
modeling presented here with a more mature unsu-
pervised WSD algorithm to replace the underlying
WORDNET-WALK could lead to advances in state-
of-the-art unsupervised WSD accuracy.
References
Steven Abney and Marc Light. 1999. Hiding a semantic
hierarchy in a markov model. In Proceedings of the
Workshop on Unsupervised Learning in Natural Lan-
guage Processing, pages 1?8.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Paul Buitelaar and Bogdan Sacaleanu. 2001. Ranking
and selecting synsets by domain relevance. In Pro-
ceedings of WordNet and Other Lexical Resources:
Applications, Extensions and Customizations. NAACL
2001. Association for Computational Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2000. Ex-
plaining away ambiguity: Learning verb selectional
preference with bayesian networks. In COLING-00,
pages 187?193.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In HLT
?91: Proceedings of the workshop on Speech and Nat-
ural Language, pages 233?237. Association for Com-
putational Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, pages 5228?5235.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. Proceedings of the Twenty-Second Annual
International SIGIR Conference.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference on
Research in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proc. 15th International Conf. on Ma-
chine Learning, pages 296?304.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,
and Alfio Gliozzo. 2001. Using domain information
for word sense disambiguation. In In Proceedings of
2nd International Workshop on Evaluating Word Sense
Disambiguation Systems, Toulouse, France.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In In 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 280?287.
Rada Mihalcea. 2005. Large vocabulary unsupervised
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the Joint Human Language Technology and Empirical
Methods in Natural Language Processing Conference,
pages 411?418.
1032
George Miller, Claudia Leacock, Randee Tengi, and Ross
Bunker. 1993. A semantic concordance. In 3rd
DARPA Workshop on Human Language Technology,
pages 303?308.
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using Measures of Semantic Related-
ness for Word Sense Disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics, pages
241?257.
Matthew Purver, Konrad Ko?rding, Thomas Griffiths, and
Joshua Tenenbaum. 2006. Unsupervised topic mod-
elling for multi-party spoken discourse. In Proceed-
ings of COLING-ACL.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Inter-
national Joint Conferences on Artificial Intelligence,
pages 448?453.
Christian Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer Texts in Statistics.
Springer-Verlag, New York, NY.
Xing Wei and Bruce Croft. 2006. LDA-based docu-
ment models for ad-hoc retrieval. In Proceedings of
the Twenty-Ninth Annual International SIGIR Confer-
ence.
1033
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 277?281,
Prague, June 2007. c?2007 Association for Computational Linguistics
PUTOP: Turning Predominant Senses into a Topic Model for Word Sense
Disambiguation
Jordan Boyd-Graber
Computer Science
Princeton University
Princeton, NJ 08540
jbg@princeton.edu
David Blei
Computer Science
Princeton University
Princeton, NJ 08540
blei@cs.princeton.edu
Abstract
We extend on McCarthy et al?s predom-
inant sense method to create an unsuper-
vised method of word sense disambiguation
that uses automatically derived topics us-
ing Latent Dirichlet alocation. Using topic-
specific synset similarity measures, we cre-
ate predictions for each word in each doc-
ument using only word frequency informa-
tion. It is hoped that this procedure can im-
prove upon the method for larger numbers
of topics by providing more relevant train-
ing corpora for the individual topics. This
method is evaluated on SemEval-2007 Task
1 and Task 17.
1 Generative Model of WSD
Word Sense Disambiguation (WSD) is the problem
of labeling text with the appropriate semantic labels
automatically. Although WSD is claimed to be an
essential step in information retrieval and machine
translation, it has not seen effective practical appli-
cation because the dearth of labeled data has pre-
vented the use of established supervised statistical
methods that have been successfully applied to other
natural language problems.
Unsupervised methods have been developed for
WSD, but despite modest success have not al-
ways been well understood statistically (Abney,
2004). Unsupervised methods are particularly ap-
pealing because they do not require expensive sense-
annotated data and can use the ever-increasing
amount of raw text freely available. This paper ex-
pands on an effective unsupervised method for WSD
and embeds it into a topic model, thus allowing an
algorithm trained on a single, monolithic corpora to
instead hand-pick relevant documents in choosing
a disambiguation. After developing this generative
statistical model, we present its performance on a
number of tasks.
1.1 The Intersection of Syntactic and Semantic
Similarity
McCarthy et al (2004) outlined a method for learn-
ing a word?s most-used sense given an untagged cor-
pus that ranks each sense wsi using a distributional
syntactic similarity ? and a WORDNET-derived se-
mantic similarity ?. This process for a word w uses
its distributional neighbors Nw, the possible senses
of not only the word in question, Sw, and also those
of the distributionally similar words, Snj . Thus,
P (wsi) =
?
nj?Nw
?(w, nj)
wnss(wsi, nj)
?
wsj?Sw
wnss(wsj , nj)
, (1)
where wnss(s, c) =
max
a?Sc
?(a, s). (2)
One can view finding the appropriate sense as a
search in two types of space. In determining how
good a particular synset wsi is, ? guides the search
in the semantic space and ? drives the search in the
syntactic space. We consider all of the words used
in syntactically similar contexts, which we call ?cor-
roborators,? and for each of them we find the closest
meaning to wsi using a measure of semantic sim-
ilarity ?, for instance a WORDNET-based similar-
ity measure such as Jiang-Conrath (1997). Each of
the neighboring words? contributions is weighted by
the syntactic probability, as provided by Lin?s distri-
butional similarity measure (1998), which rates two
words to be similar if they enter into similar syntac-
tic constructions.
277
Vw c
s
Figure 1: A reinterpretation of McCarthy et al?s pre-
dominant sense method as a generative model. Note
that this model has no notion of context; a synset is
assigned in an identical manner for all of the words
in a vocabulary.
One can think of this process as a generative
model, even though it was not originally posed in
such a manner. For each word w in the vocabulary,
we generate one of the neighbor corroborators ac-
cording to the Lin similarity, ?(c, w), between the
two words. We then generate a synset s for that
word proportional to the maximum semantic sim-
ilarity between s and any synset that contains the
corroborator c (see Figure 1).
Our aim in this paper is to extend the method of
McCarthy et al using topic models. It is hoped that
allowing the method to in effect ?choose? the con-
texts that it uses will improve its ability to disam-
biguate sentences.
1.2 Using Topic Models to Partition a
Document?s Words
Topic models like Latent Dirichlet alocation
(LDA) (Blei et al, 2003) assume a model of text
generation where each document has a multinomial
distribution over topics and each word comes from
one of these topics. In LDA, each topic is a multino-
mial distribution, and each document has a multino-
mial distribution over topics drawn from a Dirichlet
prior that selects the topic for each word in a docu-
ment. Previous work has shown that such a model
improves WSD over using a single corpus (Boyd-
Graber et al, 2007), and we use this insight to de-
velop an extension of McCarthy?s method for multi-
ple topics.
Although describing the statistical background
and motivations behind topic models are beyond the
scope of this paper, it suffices to note that the topics
induced from a corpus provide a statistical group-
ing of words that often occur together and a proba-
bilistic assignment of each word in a corpus to top-
ics. Thus, one topic might have terms like ?gov-
ernment,? ?president,? ?govern,? and ?regal,? while
another topic might have terms like ?finance,? ?high-
yield,? ?investor,? and ?market.? This paper assumes
that the machinery for learning these distributions
can, given a corpus and a specified number of top-
ics, return the topic distributions most likely to have
generated the corpus.
1.3 Defining the Model
While the original predominant senses method used
Lin?s thesaurus similarity method alone in generat-
ing the corroborator, we will also use the probability
of that word being part of the same topic as the word
to be disambiguated. Thus the process of choosing
the ?corroborator? is no longer identical for each
word; it is affected by its topic, which changes for
every document. This new generative process can
be thought of as a modified LDA system that, after
selecting the word generated by the topic, continues
on by generating a corroborator and a sense for the
original word:
For each document d ? {1 . . .D}:
1. Select a topic distribution ?d ? Dir(?)
2. For each word in the document n ? {1 . . . N}:
(a) Select a topic zn ? Mult(1, ?d)
(b) Select a word from that topic wn ? Mult(1, ?z)
(c) Select a ?corroborator? cn also proportional to how
important it is to the topic and its similarity to w
(d) Now, select a synset sn for that word based on a
distribution p(sn|wn, cn, zn)
The conditional dependencies for generating a
synset are shown in Figure 2. Our goal, like Mc-
Carthy et al?s, is to determine the most likely sense
for each word. This amounts to posterior inference,
which we address by marginalizing over the unob-
served variables (the topics and the corroborators),
where p(wsi) =
p(s|w) =
?
?
?
z
?
c
p(s|w, c, z)p(c|z, w)p(z|w, ?).
(3)
In order to fully specify this, we must determine the
distribution from which the corroborator is drawn
and the distribution from which the synset is drawn.
Ideally, we would want a distribution that for a
single topic would be identical to McCarthy et al?s
278
KD
N
? z
?
w c
s
Figure 2: Our generative model assumes that doc-
uments are divided into topics and that these topics
generate both the observed word and a ?corrobora-
tor,? a term similar in usage to the word. Next, a
sense that minimizes the semantic distance between
the corroborator and the word is generated.
method but would, as more topics are added, favor
corroborators in the same topic as the number of top-
ics increases. In McCarthy et al?s method, the prob-
ability of the corroborator given a word w is pro-
portional to the Lin similarity ?(w, c) between the
word and the corroborator. Here, the probability of
a corroborator c is
p(c|z, w) ?
?z,c
?0c
?(w, c), (4)
where ?z,c is the multinomial probability of word c
in the zth topic, and ?0c is the multinomial probabil-
ity of the word with a single topic (i.e. background
word probability).
Before, the corroborator was weighted simply
based on its syntactic similarity to the word w, now
we also weight that contribution by how important
(or unimportant) that word is to the topic that w has
been assigned to. This has the effect of increasing
the probability of words pertinent to the topic that
also have high syntactic similarity. Thus, whenever
the syntactic similarity captures polysemous usage,
we hope to be able to separate the different usages.
Note, however, that since for a single topic the ?
term cancels out and the procedure is equivalent to
McCarthy et al
We adapt the semantic similarity in much the
same way to make it topic specific. Because the
Jiang-Conrath similarity measure uses an underly-
ing term frequency to generate a similarity score, we
use the topic term frequency instead of the undivided
term frequency. Thus, the probability of a sense is
proportional to semantic similarity between it and
the closest sense among the senses of a corroborator
with respect to this topic-specific similarity (c.f. the
global similarity in Equation 2). The probability of
selecting a synset s given the corroborator c and a
topic z then becomes
p(s|w, c, z) ? max
s??S(c)
?z(s, s
?). (5)
This new dependence on the topic happens be-
cause we recompute the information content used by
Jiang-Conrath with the distribution over words im-
plied by each topic. We then use the similarity im-
plied by that similarity for ?z . Following the lead of
McCarthy, for notational ease, this becomes defined
as wnss in Equation 8.
1.4 Choosing a Synset
The problem of choosing a synset then is reduced to
finding the synset with the highest probability under
this model. The model is also designed so that the
task of learning the assignment of topics to words
and documents is not affected by this new machin-
ery for corroborators and senses that we?ve added
onto the model. Thus, we can use the variational in-
ference method described in (Blei et al, 2003) as a
foundation for the problem of synset inference.
Taking p(z|w) as a given (i.e. determined by run-
ning LDA on the corpus), the probability for a synset
s given a word w then becomes
p(s|w, z) =
?
z
?
c
p(s|w, c, z)p(c|z)p(z|w), (6)
whose terms have been described in the previous
section. With all of the normalization terms, we now
see that p(s|w, z) becomes
?
z
?
c
?z,c
?0c
?(w, c)
?
c?
?z,c
?0c
?(w, c?)
wnss(s, c, z)
?
s??Sw wnss(s
?, c, z)
.
(7)
and wnss(s, c, z) now becomes, for the zth topic,
max
a?S(c)
?z(a, s). (8)
Thus, we?ve now assigned a probability to each of
the possible senses a word can take in a document.
279
1.5 Intuition
For example, consider the word ?fly,? which has two
other words that have high syntactic similarity (in
our formulation, ?) with the terms ?fly ball? and ?in-
sect.? Both of these words would, given the seman-
tic similarity provided by WORDNET, point to a sin-
gle sense of ?fly;? one of them would give a higher
value, however, and thus all senses of the word ?fly?
would be assigned that sense. By separately weight-
ing these words by the topic frequencies, we would
hope to choose the sports sense in topics that have
a higher probability of the terms like ?foul ball,?
?pop fly,? and ?grounder? and the other sense in the
contexts where insect has a higher probability in the
topic.
2 Evaluations
This section describes three experiments to deter-
mine the effectiveness of this unsupervised system.
The first was used to help understand the system,
and the second two were part of the SemEval 2007
competition.
2.1 SemCor
As an initial evaluation, we learned LDA topics on
the British National corpus with paragraphs as the
underlying ?document? (this allowed for a more uni-
form document length). These documents were then
used to infer topic probabilities for each of the words
in SemCor (Miller et al, 1993), and the model de-
scribed in the previous section was run to determine
the most likely synset. The results of this procedure
are shown in Table 1. Accuracy is determined as the
percentage of words for which the most likely sense
was the one tagged in the corpus.
While the method does roughly recreate Mc-
Carthy et al?s result for a single topic, it only of-
fers a one percent improvement over McCarthy et
al. on five topics and then falls below McCarthy for
all greater numbers of topics tried. Thus, for all
subsequent experiments we used a five topic model
trained on the BNC.
2.2 SemEval-2007 Task 1: CLIR
Using IR metrics, this disambiguation scheme was
evaluated against another competing platform and
an algorithm provided by the Task 1 (Agirre et al,
Topics All Nouns
1 .393 .467
5 .397 .478
25 .387 .456
200 .359 .420
Table 1: Accuracy on disambiguating words in Sem-
Cor
Task PUTOP
Topic Expansion 0.30
Document Expansion 0.15
English Translation 0.17
SensEval 2 0.39
SensEval 3 0.33
Table 2: Performance results on Task 1
2007) organizers. Our system had the best results of
any expansion scheme considered (0.30) , although
none of the expansion schemes did better than us-
ing no expansion (0.36). Although our technique
also yielded a better score than the other competing
platform for cross-language queries (0.17), it did not
surpass the first sense-heuristic (0.26), but this is not
surprising given that our algorithm does not assume
the existence of such information. For an overview
of Task 1 results, see Table 2.
2.3 SemEval-2007 Task 17: All-Words
Task 17 (Pradhan et al, 2007) asked participants
to submit results as probability distributions over
senses. Because this is also the output of this algo-
rithm, we submitted the probabilities to the contest
before realizing that the distributions are very close
to uniform over all senses and thus yielded a pre-
cision of 0.12, very close to the random baseline.
Placing a point distribution on the argmax with our
original submission to the task, however, (consistent
with our methodology for evaluation on SemCor),
gives a precision of 0.39.
3 Conclusion
While the small improvement over the single topic
suggests that topic techniques might have traction
in determining the best sense, the addition is not ap-
preciable. In a way the failure of the technique is en-
280
couraging in that it affirms the original methodology
of McCarthy et al in finding a single predominant
sense for each word. While the syntactic similarity
measure indeed usually offers high values of similar-
ity for words related to a single sense of a word, the
similarity for words related to other senses, which
we had hoped to strengthen by using topic features,
are on par with words observed because of noise.
Thus, for a word like ?bank,? words like
?firm,? ?commercial bank,? ?company,? and ?finan-
cial institution? are the closest in terms of the syn-
tactic similarity, and this allows the financial senses
to be selected without any difficulty. Even if we had
corroborating words for another sense in some topic,
these words are absent from the syntactically simi-
lar words. If we want the meaning similar to that of
?riverbank,? the word with the most similar mean-
ing, ?side,? had a syntactic similarity on par with the
unrelated words ?individual? and ?group.? Thus, in-
terpretations other than the dominant sense as deter-
mined by the baseline method of McCarthy et al are
hard to find.
Because one topic is equivalent to McCarthy et
al.?s method, this means that we do no worse on
disambiguation. However, contrary to our hope, in-
creasing the number of topics does not lead to sig-
nificantly better sense predictions. This work has not
investigated using a topic-based procedure for deter-
mining the syntactic similarity, but we feel that this
extension could provide real improvement to the un-
supervised techniques that can make use of the co-
pious amounts of available unlabeled data.
References
Steven Abney. 2004. Understanding the yarowsky algo-
rithm. Comput. Linguist., 30(3):365?395.
Eneko Agirre, Oier Lopez de Lacalle, Arantxa Otegi,
German Rigau, and Piek Vossen. 2007. The Senseval-
2007 Task 1: Evaluating WSD on cross-language in-
formation retrieval. In Proceedings of SemEval-2007.
Association for Computational Linguistics.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
Jordan L. Boyd-Graber, David M. Blei, and Jerry Zhu.
2007. Probabalistic walks in semantic hierarchies as a
topic model for WSD. In Proc. EMNLP 2007.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference on
Research in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proc. 15th International Conf. on Ma-
chine Learning, pages 296?304. Morgan Kaufmann,
San Francisco, CA.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In In 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 280?287.
George Miller, Claudia Leacock, Randee Tengi, and Ross
Bunker. 1993. A semantic concordance. In 3rd
DARPA Workshop on Human Language Technology,
pages 303?308.
Sameer Pradhan, Martha Palmer, and Edward Loper.
2007. The Senseval-2007 Task 17: English fine-
grained all-words. In Proceedings of SemEval-2007.
Association for Computational Linguistics.
281
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 45?55,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Holistic Sentiment Analysis Across Languages:
Multilingual Supervised Latent Dirichlet Allocation
Jordan Boyd-Graber
UMD iSchool
and UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Department of Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
In this paper, we develop multilingual super-
vised latent Dirichlet alocation (MLSLDA),
a probabilistic generative model that allows
insights gleaned from one language?s data to
inform how the model captures properties of
other languages. MLSLDA accomplishes this
by jointly modeling two aspects of text: how
multilingual concepts are clustered into themat-
ically coherent topics and how topics associ-
ated with text connect to an observed regres-
sion variable (such as ratings on a sentiment
scale). Concepts are represented in a general
hierarchical framework that is flexible enough
to express semantic ontologies, dictionaries,
clustering constraints, and, as a special, degen-
erate case, conventional topic models. Both
the topics and the regression are discovered
via posterior inference from corpora. We show
MLSLDA can build topics that are consistent
across languages, discover sensible bilingual
lexical correspondences, and leverage multilin-
gual corpora to better predict sentiment.
Sentiment analysis (Pang and Lee, 2008) offers
the promise of automatically discerning how people
feel about a product, person, organization, or issue
based on what they write online, which is potentially
of great value to businesses and other organizations.
However, the vast majority of sentiment resources
and algorithms are limited to a single language, usu-
ally English (Wilson, 2008; Baccianella and Sebas-
tiani, 2010). Since no single language captures a
majority of the content online, adopting such a lim-
ited approach in an increasingly global community
risks missing important details and trends that might
only be available when text in multiple languages is
taken into account.
Up to this point, multiple languages have been
addressed in sentiment analysis primarily by trans-
ferring knowledge from a resource-rich language to
a less rich language (Banea et al, 2008), or by ig-
noring differences in languages via translation into
English (Denecke, 2008). These approaches are lim-
ited to a view of sentiment that takes place through
an English-centric lens, and they ignore the poten-
tial to share information between languages. Ide-
ally, learning sentiment cues holistically, across lan-
guages, would result in a richer and more globally
consistent picture.
In this paper, we introduce Multilingual Super-
vised Latent Dirichlet Allocation (MLSLDA), a
model for sentiment analysis on a multilingual cor-
pus. MLSLDA discovers a consistent, unified picture
of sentiment across multiple languages by learning
?topics,? probabilistic partitions of the vocabulary
that are consistent in terms of both meaning and rel-
evance to observed sentiment. Our approach makes
few assumptions about available resources, requiring
neither parallel corpora nor machine translation.
The rest of the paper proceeds as follows. In Sec-
tion 1, we describe the probabilistic tools that we use
to create consistent topics bridging across languages
and the MLSLDA model. In Section 2, we present
the inference process. We discuss our set of seman-
tic bridges between languages in Section 3, and our
experiments in Section 4 demonstrate that this ap-
proach functions as an effective multilingual topic
model, discovers sentiment-biased topics, and uses
multilingual corpora to make better sentiment pre-
dictions across languages. Sections 5 and 6 discuss
related research and discusses future work, respec-
tively.
45
1 Predictions from Multilingual Topics
As its name suggests, MLSLDA is an extension of
Latent Dirichlet alocation (LDA) (Blei et al, 2003),
a modeling approach that takes a corpus of unan-
notated documents as input and produces two out-
puts, a set of ?topics? and assignments of documents
to topics. Both the topics and the assignments are
probabilistic: a topic is represented as a probability
distribution over words in the corpus, and each doc-
ument is assigned a probability distribution over all
the topics. Topic models built on the foundations of
LDA are appealing for sentiment analysis because
the learned topics can cluster together sentiment-
bearing words, and because topic distributions are a
parsimonious way to represent a document.1
LDA has been used to discover latent structure
in text (e.g. for discourse segmentation (Purver et
al., 2006) and authorship (Rosen-Zvi et al, 2004)).
MLSLDA extends the approach by ensuring that this
latent structure ? the underlying topics ? is consis-
tent across languages. We discuss multilingual topic
modeling in Section 1.1, and in Section 1.2 we show
how this enables supervised regression regardless of
a document?s language.
1.1 Capturing Semantic Correlations
Topic models posit a straightforward generative pro-
cess that creates an observed corpus. For each docu-
ment d, some distribution ?d over unobserved topics
is chosen. Then, for each word position in the doc-
ument, a topic z is selected. Finally, the word for
that position is generated by selecting from the topic
indexed by z. (Recall that in LDA, a ?topic? is a
distribution over words).
In monolingual topic models, the topic distribution
is usually drawn from a Dirichlet distribution. Us-
ing Dirichlet distributions makes it easy to specify
sparse priors, and it also simplifies posterior infer-
ence because Dirichlet distributions are conjugate
to multinomial distributions. However, drawing top-
ics from Dirichlet distributions will not suffice if
our vocabulary includes multiple languages. If we
are working with English, German, and Chinese at
the same time, a Dirichlet prior has no way to fa-
vor distributions z such that p(good|z), p(gut|z), and
1The latter property has also made LDA popular for infor-
mation retrieval (Wei and Croft, 2006)).
p(ha?o|z) all tend to be high at the same time, or low
at the same time. More generally, the structure of our
model must encourage topics to be consistent across
languages, and Dirichlet distributions cannot encode
correlations between elements.
One possible solution to this problem is to use the
multivariate normal distribution, which can produce
correlated multinomials (Blei and Lafferty, 2005),
in place of the Dirichlet distribution. This has been
done successfully in multilingual settings (Cohen
and Smith, 2009). However, such models complicate
inference by not being conjugate.
Instead, we appeal to tree-based extensions of the
Dirichlet distribution, which has been used to induce
correlation in semantic ontologies (Boyd-Graber et
al., 2007) and to encode clustering constraints (An-
drzejewski et al, 2009). The key idea in this ap-
proach is to assume the vocabularies of all languages
are organized according to some shared semantic
structure that can be represented as a tree. For con-
creteness in this section, we will use WordNet (Miller,
1990) as the representation of this multilingual se-
mantic bridge, since it is well known, offers conve-
nient and intuitive terminology, and demonstrates the
full flexibility of our approach. However, the model
we describe generalizes to any tree-structured rep-
resentation of multilingual knowledge; we discuss
some alternatives in Section 3.
WordNet organizes a vocabulary into a rooted, di-
rected acyclic graph of nodes called synsets, short for
?synonym sets.? A synset is a child of another synset
if it satisfies a hyponomy relationship; each child ?is
a? more specific instantiation of its parent concept
(thus, hyponomy is often called an ?isa? relationship).
For example, a ?dog? is a ?canine? is an ?animal? is
a ?living thing,? etc. As an approximation, it is not
unreasonable to assume that WordNet?s structure of
meaning is language independent, i.e. the concept
encoded by a synset can be realized using terms in
different languages that share the same meaning. In
practice, this organization has been used to create
many alignments of international WordNets to the
original English WordNet (Ordan and Wintner, 2007;
Sagot and Fis?er, 2008; Isahara et al, 2008).
Using the structure of WordNet, we can now de-
scribe a generative process that produces a distribu-
tion over a multilingual vocabulary, which encour-
ages correlations between words with similar mean-
46
ings regardless of what language each word is in.
For each synset h, we create a multilingual word
distribution for that synset as follows:
1. Draw transition probabilities ?h ? Dir (?h)
2. Draw stop probabilities ?h ? Dir (?h)
3. For each language l, draw emission probabilities for
that synset ?h,l ? Dir (pih,l).
For conciseness in the rest of the paper, we will refer
to this generative process as multilingual Dirichlet
hierarchy, or MULTDIRHIER(? ,?,pi).2 Each ob-
served token can be viewed as the end result of a
sequence of visited synsets ?. At each node in the
tree, the path can end at node i with probability ?i,1,
or it can continue to a child synset with probability
?i,0. If the path continues to another child synset, it
visits child j with probability ?i,j . If the path ends at
a synset, it generates word k with probability ?i,l,k.3
The probability of a word being emitted from a path
with visited synsets r and final synset h in language
l is therefore
p(w, ? = r, h|l,?,?,?) =
?
?
?
(i,j)?r
?i,j?i,0
?
? (1? ?h,1)?h,l,w. (1)
Note that the stop probability ?h is independent of
language, but the emission ?h,l is dependent on the
language. This is done to prevent the following sce-
nario: while synset A is highly probable in a topic
and words in language 1 attached to that synset have
high probability, words in language 2 have low prob-
ability. If this could happen for many synsets in
a topic, an entire language would be effectively si-
lenced, which would lead to inconsistent topics (e.g.
2Variables ?h, pih,l, and ?h are hyperparameters. Their mean
is fixed, but their magnitude is sampled during inference (i.e.
?h,iP
k ?h,k
is constant, but ?h,i is not). For the bushier bridges,
(e.g. dictionary and flat), their mean is uniform. For GermaNet,
we took frequencies from two balanced corpora of German and
English: the British National Corpus (University of Oxford,
2006) and the Kern Corpus of the Digitales Wo?rterbuch der
Deutschen Sprache des 20. Jahrhunderts project (Geyken, 2007).
We took these frequencies and propagated them through the
multilingual hierarchy, following LDAWN?s (Boyd-Graber et
al., 2007) formulation of information content (Resnik, 1995) as
a Bayesian prior. The variance of the priors was initialized to be
1.0, but could be sampled during inference.
3Note that the language and word are taken as given, but the
path through the semantic hierarchy is a latent random variable.
Topic 1 is about baseball in English and about travel
in German). Separating path from emission helps
ensure that topics are consistent across languages.
Having defined topic distributions in a way that can
preserve cross-language correspondences, we now
use this distribution within a larger model that can
discover cross-language patterns of use that predict
sentiment.
1.2 The MLSLDA Model
We will view sentiment analysis as a regression prob-
lem: given an input document, we want to predict
a real-valued observation y that represents the senti-
ment of a document. Specifically, we build on super-
vised latent Dirichlet alocation (SLDA, (Blei and
McAuliffe, 2007)), which makes predictions based
on the topics expressed in a document; this can be
thought of projecting the words in a document to low
dimensional space of dimension equal to the number
of topics. Blei et al showed that using this latent
topic structure can offer improved predictions over re-
gressions based on words alone, and the approach fits
well with our current goals, since word-level cues are
unlikely to be identical across languages. In addition
to text, SLDA has been successfully applied to other
domains such as social networks (Chang and Blei,
2009) and image classification (Wang et al, 2009).
The key innovation in this paper is to extend SLDA
by creating topics that are globally consistent across
languages, using the bridging approach above.
We express our model in the form of a probabilis-
tic generative latent-variable model that generates
documents in multiple languages and assigns a real-
valued score to each document. The score comes
from a normal distribution whose sum is the dot prod-
uct between a regression parameter ? that encodes
the influence of each topic on the observation and
a variance ?2. With this model in hand, we use sta-
tistical inference to determine the distribution over
latent variables that, given the model, best explains
observed data.
The generative model is as follows:
1. For each topic i = 1 . . .K, draw a topic distribution
{?i,?i,?i} from MULTDIRHIER(? ,?,pi).
2. For each document d = 1 . . .M with language ld:
(a) Choose a distribution over topics ?d ?
Dir (?).
47
(b) For each word in the document n = 1 . . . Nd,
choose a topic assignment zd,n ? Mult (?d)
and a path ?d,n ending at word wd,n according
to Equation 1 using {?zd,n ,?zd,n ,?zd,n}.
3. Choose a response variable from y ?
Norm
(
?>z?, ?2
)
, where z?d ? 1N
?N
n=1 zd,n.
Crucially, note that the topics are not indepen-
dent of the sentiment task; the regression encourages
terms with similar effects on the observation y to
be in the same topic. The consistency of topics de-
scribed above allows the same regression to be done
for the entire corpus regardless of the language of the
underlying document.
2 Inference
Finding the model parameters most likely to explain
the data is a problem of statistical inference. We em-
ploy stochastic EM (Diebolt and Ip, 1996), using a
Gibbs sampler for the E-step to assign words to paths
and topics. After randomly initializing the topics,
we alternate between sampling the topic and path
of a word (zd,n, ?d,n) and finding the regression pa-
rameters ? that maximize the likelihood. We jointly
sample the topic and path conditioning on all of the
other path and document assignments in the corpus,
selecting a path and topic with probability
p(zn = k, ?n = r|z?n,??n, wn, ?, ?,?) =
p(yd|z, ?, ?)p(?n = r|zn = k,??n, wn, ? ,?,pi)
p(zn = k|z?n, ?). (2)
Each of these three terms reflects a different influence
on the topics from the vocabulary structure, the doc-
ument?s topics, and the response variable. In the next
paragraphs, we will expand each of them to derive
the full conditional topic distribution.
As discussed in Section 1.1, the structure of the
topic distribution encourages terms with the same
meaning to be in the same topic, even across lan-
guages. During inference, we marginalize over pos-
sible multinomial distributions ?, ?, and ?, using
the observed transitions from i to j in topic k; Tk,i,j ,
stop counts in synset i in topic k, Ok,i,0; continue
counts in synsets i in topic k, Ok,i,1; and emission
counts in synset i in language l in topic k, Fk,i,l. The
H
L
M
N
?
d
z
d,n
?
d,n
?
w
d,n
?
?
y
d
K
?
i,h
?
h
?
i,h
?
h
?
i,h,l
?
h,l
Multilingual Topics Text Documents Sentiment Prediction
Figure 1: Graphical model representing MLSLDA.
Shaded nodes represent observations, plates denote repli-
cation, and lines show probabilistic dependencies.
probability of taking a path r is then
p(?n = r|zn = k,??n) =
?
(i,j)?r
(
Bk,i,j + ?i,j
?
j? Bk,i,j? + ?i,j
Ok,i,1 + ?i
?
s?0,1Ok,i,s + ?i,s
)
? ?? ?
Transition
Ok,rend,0 + ?rend?
s?0,1Ok,rend,s + ?rend,s
Fk,rend,wn + pirend,l?
w? Frend,w? + pirend,w?
? ?? ?
Emission
.
(3)
Equation 3 reflects the multilingual aspect of this
model. The conditional topic distribution for
SLDA (Blei and McAuliffe, 2007) replaces this term
with the standard Multinomial-Dirichlet. However,
we believe this is the first published SLDA-style
model using MCMC inference, as prior work has
used variational inference (Blei and McAuliffe, 2007;
Chang and Blei, 2009; Wang et al, 2009).
Because the observed response variable depends
on the topic assignments of a document, the condi-
tional topic distribution is shifted toward topics that
explain the observed response. Topics that move the
predicted response y?d toward the true yd will be fa-
vored. We drop terms that are constant across all
48
topics for the effect of the response variable,
p(yd|z, ?, ?) ?
exp
[
1
?2
(
yd ?
?
k? Nd,k??k??
k? Nd,k?
)
?zk?
k? Nd,k?
]
? ?? ?
Other words? influence
exp
[
??2zk
2?2
?
k? N
2
d,k?
]
? ?? ?
This word?s influence
. (4)
The above equation represents the supervised aspect
of the model, which is inherited from SLDA.
Finally, there is the effect of the topics already
assigned to a document; the conditional distribution
favors topics already assigned in a document,
p(zn = k|z?n, ?) =
Td,k + ?k
?
k? Td,k? + ?k?
. (5)
This term represents the document focus of this
model; it is present in all Gibbs sampling inference
schemes for LDA (Griffiths and Steyvers, 2004).
Multiplying together Equations 3, 4, and 5 allows
us to sample a topic using the conditional distribution
from Equation 2, based on the topic and path of the
other words in all languages. After sampling the
path and topic for each word in a document, we then
find new regression parameters ? that maximize the
likelihood conditioned on the current state of the
sampler. This is simply a least squares regression
using the topic assignments z?d to predict yd.
Prediction on documents for which we don?t have
an observed yd is equivalent to marginalizing over
yd and sampling topics for the document from Equa-
tions 3 and 5. The prediction for yd is then the dot
product of ? and the empirical topic distribution z?d.
We initially optimized all hyperparameters using
slice sampling. However, we found that the regres-
sion variance ?2 was not stable. Optimizing ?2 seems
to balance between modeling the language in the doc-
uments and the prediction, and thus is sensitive to
documents? length. Given this sensitivity, we did
not optimize ?2 for our prediction experiments in
Section 4, but instead kept it fixed at 0.25. We leave
optimizing this variable, either through cross valida-
tion or adapting the model, to future work.
3 Bridges Across Languages
In Section 1.1, we described connections across lan-
guages as offered by semantic networks in a general
way, using WordNet as an example. In this section,
we provide more specifics, as well as alternative ways
of building semantic connections across languages.
Flat First, we can consider a degenerate mapping
that is nearly equivalent to running SLDA indepen-
dently across multiple languages, relating topics only
based on the impact on the response variable. Con-
sider a degenerate tree with only one node, with all
words in all languages associated with that node. This
is consistent with our model, but there is really no
shared semantic space, as all emitted words must
come from this degenerate ?synset? and the model
only represents the output distribution for this single
node.
WordNet We took the alignment of GermaNet to
WordNet 1.6 (Kunze and Lemnitzer, 2002) and re-
moved all synsets that were had no mapped German
words. Any German synsets that did not have English
translations had their words mapped to the lowest
extant English hypernym (e.g. ?beinbruch,? a bro-
ken leg, was mapped to ?fracture?). We stemmed
all words to account for inflected forms not being
present (Porter and Boulton, 1970). An example
of the paths for the German word ?wunsch? (wish,
request) is shown in Figure 2(a).
Dictionaries A dictionary can be viewed as a many
to many mapping, where each entry ei maps one
or more words in one language si to one or more
words ti in another language. Entries were taken
from an English-German dictionary (Richter, 2008)
a Chinese-English dictionary (Denisowski, 1997),
and a Chinese-German dictionary (Hefti, 2005). As
with WordNet, the words in entries for English and
German were stemmed to improve coverage. An
example for German is shown in Figure 2(b).
Algorithmic Connections In addition to hand-
curated connections across languages, one could also
consider automatic means of mapping across lan-
guages, such as using edit distance or local con-
text (Haghighi et al, 2008; Rapp, 1995) or us-
ing a lexical translation table obtained from paral-
lel text (Melamed, 1998). While we experimented
49
wish.n.04
wish wunsch
entity.n.01
entitiabstraction.n.06
cognition.n.01 event.n.01
event ereignis vorgang act.n.02
deed act handlung
speech_act.n.01
request.n.02
option.n.02
ask request anfrag wunsch
altern option choic option
preference.n.03
objekt
(a) GermaNet
dict.1
room gelass
root
dict.2
room raum platz
room zimm raum
dict.3
stub
(b) Dictionary
Figure 2: Two methods for constructing multilingual distributions over words. On the left, paths to the German word
?wunsch? in GermaNet are shown. On the right, paths to the English word ?room? are shown. Both English and German
words are shown; some internal nodes in GermaNet have been omitted for space (represented by dashed lines). Note
that different senses are denoted by different internal paths, and that internal paths are distinct from the per-language
expression.
with these techniques, constructing appropriate hier-
archies from these resources required many arbitrary
decisions about cutoffs and which words to include.
Thus, we do not consider them in this paper.
4 Experiments
We evaluate MLSLDA on three criteria: how well
it can discover consistent topics across languages
for matching parallel documents, how well it can
discover sentiment-correlated word lists from non-
aligned text, and how well it can predict sentiment.
4.1 Matching on Multilingual Topics
We took the 1996 documents from the Europarl cor-
pus (Koehn, 2005) using three bridges: GermaNet,
dictionary, and the uninformative flat matching.4 The
model is unaware that the translations of documents
in one language are present in the other language.
Note that this does not use the supervised framework
4For English and German documents in all experiments,
we removed stop words (Loper and Bird, 2002), stemmed
words (Porter and Boulton, 1970), and created a vocabulary
of the most frequent 5000 words per language (this vocabulary
limit was mostly done to ensure that the dictionary-based bridge
was of manageable size). Documents shorter than fifty content
words were excluded.
(as there is no associated response variable for Eu-
roparl documents); this experiment is to demonstrate
the effectiveness of the multilingual aspect of the
model. To test whether the topics learned by the
model are consistent across languages, we represent
each document using the probability distribution ?d
over topic assignments. Each ?d is a vector of length
K and is a language-independent representation of
the document.
For each document in one language, we computed
the Hellinger distance between it and all of the docu-
ments in the other language and sorted the documents
by decreasing distance. The translation of the docu-
ment is somewhere in that set; the higher the normal-
ized rank (the percentage of documents with a rank
lower than the translation of the document), the better
the underlying topic model connects languages.
We compare three bridges against what is to our
knowledge the only other topic model for unaligned
text, Multilingual Topics for Unaligned Text (Boyd-
Graber and Blei, 2009).5
5The bipartite matching was initialized with the dictionary
weights as specified by the Multilingual Topics for Unaligned
Text algorithm. The matching size was limited to 250 and the
bipartite matching was only updated on the initial iteration then
held fixed. This yielded results comparable to when the matching
50
Average Parallel Document Rank
Br
id
ge
Flat
GermaNet
MuTo
Dictionary
Flat
GermaNet
MuTo
Dictionary
Flat
GermaNet
MuTo
Dictionary
Flat
GermaNet
MuTo
Dictionary
0.0 0.2 0.4 0.6 0.8
5
25
50
75
Figure 3: Average rank of paired translation document
recovered from the multilingual topic model. Random
guessing would yield 0.5; MLSLDA with a dictionary
based matching performed best.
Figure 3 shows the results of this experiment. The
dictionary-based bridge had the best performance on
the task, ranking a large proportion of documents
(0.95) below the translated document once enough
topics were available. Although GermaNet is richer,
its coverage is incomplete; the dictionary structure
had a much larger vocabulary and could build a more
complete multilingual topics. Using comparable in-
put information, this more flexible model performed
better on the matching task than the existing multi-
lingual topic model available for unaligned text. The
degenerate flat bridge did no better than the baseline
of random guessing, as expected.
4.2 Qualitative Sentiment-Correlated Topics
One of the key tasks in sentiment analysis has been
the collection of lists of words that convey senti-
ment (Wilson, 2008; Riloff et al, 2003). These
resources are often created using or in reference
to resources like WordNet (Whitelaw et al, 2005;
Baccianella and Sebastiani, 2010). MLSLDA pro-
vides a method for extracting topical and sentiment-
correlated word lists from multilingual corpora. If
was updated more frequently.
a WordNet-like resource is used as the bridge, the
resulting topics are distributions over synsets, not just
over words.
As our demonstration corpus, we used the Amherst
Sentiment Corpus (Constant et al, 2009), as it has
documents in multiple languages (English, Chinese,
and German) with numerical assessments of senti-
ment (number of stars assigned to the review). We
segmented the Chinese text (Tseng et al, 2005) and
used a classifier trained on character n-grams to re-
move English-language documents that were mixed
in among the Chinese and German language reviews.
Figure 4 shows extracted topics from German-
English and German-Chinese corpora. MLSLDA
is able to distinguish sentiment-bearing topics from
content bearing topics. For example; in the German-
English corpus, ?food? and ?children? topics are
not associated with a consistent sentiment signal,
while ?religion? is associated with a more negative
sentiment. In contrast, in the German-Chinese cor-
pus, the ?religion/society? topic is more neutral, and
the gender-oriented topic is viewed more negatively.
Negative sentiment-bearing topics have reasonable
words such as ?pages,? ?ko?ng pa`? (Chinese for ?I?m
afraid that . . . ?) and ?tuo? (Chienese for ?discard?),
and positive sentiment-bearing topics have reason-
able words such as ?great,? ?good,? and ?juwel? (Ger-
man for ?jewel?).
The qualitative topics also betray some of the
weaknesses of the model. For example, in one of
the negative sentiment topics, the German word ?gut?
(good) is present. Because topics are distributions
over words, they can encode the presence of nega-
tions like ?kein? (no) and ?nicht? (not), but not collo-
cations like ?nicht gut.? More elaborate topic models
that can model local syntax and collocations (John-
son, 2010) provide options for addressing such prob-
lems.
We do not report the results for sentiment predic-
tion for this corpus because the baseline of predicting
a positive review is so strong; most algorithms do ex-
tremely well by always predicting a positive review,
ours included.
4.3 Sentiment Prediction
We gathered 330 film reviews from a German film
review site (Vetter et al, 2000) and combined them
with a much larger English film review corpus of over
51
0.0
-0.4
-0.8
-1.2 0.4
0.8
1.2
himmel
gedanken
glaube
unsere
kirche
wahrheit
god
us
religion
church
human
buch
roman
leser
seiten
geschichte
handlung
book
novel
reader
pages
books
tale
essen
di?t
verlieren
befinden
k?rper
rezepte
diet
food
eat
weight
eating
healthy
fat
buch
immer
leben
art
lesen
thema
autor
book
books
one
life
person
people
kind
kinder
eltern
baby
nacht
children
baby
child
parents
sleep
film
filme
episode
star
story
gibt
movie
film
episode
movies
scenes
separate
gives
gesellschaft
genau
?berzeugt
ergebnis
mittel
verlangen
great
good
business
all
one
companies
right
(a) German / English
-0.4-0.8-1.2-1.6 0.0 0.4 0.8
?? (god)?? (lord)? (both)?? (religion)?? (science)?? (community)
(god) gott(lord) herr(religion) religion(universe) all(world) welt(science) wissenschaft(medicine) medizin(society) gesellschaft
? (good)? (set)? (treasure)? (handsome)? (both)?? (story)? (small)
(good) gut(sentence) satz(two) zwei(story) story(treasure) schatz(attractive) attraktiv(elegant) elegant(gem) juwel
(book) buch(itself) sich(that) dass(much) viel(no) kein(good) gut(when) wenn
? (book) ?? ([I'm afraid that...])?  (myself)? (both)??? (mostly)?  (book)?? ([really isn't])? (discard)
(woman) frau(point) punkt(man) mann(equal) gleich(fast) schnell(female) weiblich(soon) bald
? (quick)? (a little)?? (woman)?? (man)? (female)? (male)?? (female)? (both)
?? (harry)? (belt)? (sky)? (both)? (section)??? (vampire)?? (strong)?? (last)
(harry) harry(volume) band(sky) himmel (universe) all(vampire) vampir(last) letzt(part) teil
(b) German / Chinese
Figure 4: Topics, along with associated regression coefficient ? from a learned 25-topic model on German-English (left)
and German-Chinese (right) documents. Notice that theme-related topics have regression parameter near zero, topics
discussing the number of pages have negative regression parameters, topics with ?good,? ?great,? ?ha?o? (good) and
?u?berzeugt? (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of ?gut?
(good) in one of the negative sentiment topics, showing the difficulty of learning collocations.
Train Test GermaNet Dictionary Flat
DE DE 73.8 24.8 92.2
EN DE 7.44 2.68 18.3
EN + DE DE 1.17 1.46 1.39
Table 1: Mean squared error on a film review corpus.
All results are on the same German test data, varying the
training data. Over-fitting prevents the model learning on
the German data alone; adding English data to the mix
allows the model to make better predictions.
5000 film reviews (Pang and Lee, 2005) to create a
multilingual film review corpus.6
The results for predicting sentiment in German
documents with 25 topics are presented in Table 1.
On a small monolingual corpus, prediction is very
poor. The model over-fits, especially when it has
the entire vocabulary to select from. The slightly
better performance using GermaNet and a dictionary
as topic priors can be viewed as basic feature selec-
tion, removing proper names from the vocabulary to
6We followed Pang and Lee?s method for creating a nu-
merical score between 0 and 1 from a star rating. We
then converted that to an integer by multiplying by 100;
this was done because initial data preprocessing assumed
integer values (although downstream processing did not as-
sume integer values). The German movie review corpus
is available at http://www.umiacs.umd.edu/?jbg/
static/downloads_and_media.html
prevent over-fitting.
One would expect that prediction improves with a
larger training set. For this model, such an improve-
ment is seen even when the training set includes no
documents in the target language. Note that even the
degenerate flat bridge across languages provides use-
ful information. After introducing English data, the
model learns to prefer smaller regression parameters
(this can be seen as a form of regularization).
Performance is best when a reasonably large cor-
pus is available including some data in the target
language. For each bridge, performance improves
dramatically, showing that MLSLDA is successfully
able to incorporate information learned from both
languages to build a single, coherent picture of how
sentiment is expressed in both languages. With the
GermaNet bridge, performance is better than both
the degenerate and dictionary based bridges, showing
that the model is sharing information both through
the multilingual topics and the regression parameters.
Performance on English prediction is comparable
to previously published results on this dataset (Blei
and McAuliffe, 2007); with enough data, a monolin-
gual model is no longer helped by adding additional
multilingual data.
52
5 Relationship to Previous Research
The advantages of MLSLDA reside largely in the
assumptions that it makes and does not make: docu-
ments need not be parallel, sentiment is a normally
distributed document-level property, words are ex-
changeable, and sentiment can be predicted as a re-
gression on a K-dimensional vector.
By not assuming parallel text, this approach can
be applied to a broad class of corpora. Other mul-
tilingual topic models require parallel text, either at
the document (Ni et al, 2009; Mimno et al, 2009)
or word-level (Kim and Khudanpur, 2004; Zhao and
Xing, 2006). Similarly, other multilingual sentiment
approaches also require parallel text, often supplied
via automatic translation; after the translated text
is available, either monolingual analysis (Denecke,
2008) or co-training is applied (Wan, 2009). In con-
trast, our approach requires fewer resources for a lan-
guage: a dictionary (or similar knowledge structure
relating words to nodes in a graph) and comparable
text, instead of parallel text or a machine translation
system.
Rather than viewing one language through the
lens of another language, MLSLDA views all lan-
guages through the lens of the topics present in a
document. This is a modeling decision with pros and
cons. It allows a language agnostic decision about
sentiment to be made, but it restricts the expressive-
ness of the model in terms of sentiment in two ways.
First, it throws away information important to sen-
timent analysis like syntactic constructions (Greene
and Resnik, 2009) and document structure (McDon-
ald et al, 2007) that may impact the sentiment rating.
Second, a single real number is not always sufficient
to capture the nuances of sentiment. Less critically,
assuming that sentiment is normally distributed is not
true of all real-world corpora; review corpora often
have a skew toward positive reviews. We standardize
responses by the mean and variance of the training
data to partially address this issue, but other response
distributions are possible, such as generalized linear
models (Blei and McAuliffe, 2007) and vector ma-
chines (Zhu et al, 2009), which would allow more
traditional classification predictions.
Other probabilistic models for sentiment classifi-
cation view sentiment as a word level feature. Some
models use sentiment word lists, either given or
learned from a corpus, as a prior to seed topics so
that they attract other sentiment bearing words (Mei
et al, 2007; Lin and He, 2009). Other approaches
view sentiment or perspective as a perturbation of
a log-linear topic model (Lin et al, 2008). Such
techniques could be combined with the multilingual
approach presented here by using distributions over
words that not only bridge different languages but
also encode additional information. For example, the
vocabulary hierarchies could be structured to encour-
age topics that encourage correlation among similar
sentiment-bearing words (e.g. clustering words asso-
ciated with price, size, etc.). Future work could also
more rigorously validate that the multilingual topics
discovered by MLSLDA are sentiment-bearing via
human judgments.
In contrast, MLSLDA draws on techniques that
view sentiment as a regression problem based on the
topics used in a document, as in supervised latent
Dirichlet alocation (SLDA) (Blei and McAuliffe,
2007) or in finer-grained parts of a document (Titov
and McDonald, 2008). Extending these models to
multilingual data would be more straightforward.
6 Conclusions
MLSLDA is a ?holistic? statistical model for multi-
lingual corpora that does not require parallel text
or expensive multilingual resources. It discovers
connections across languages that can recover la-
tent structure in parallel corpora, discover sentiment-
correlated word lists in multiple languages, and make
accurate predictions across languages that improve
with more multilingual data, as demonstrated in the
context of sentiment analysis.
More generally, MLSLDA provides a formalism
that can be used to incorporate the many insights of
topic modeling-driven sentiment analysis to multi-
lingual corpora by tying together word distributions
across languages. MLSLDA can also contribute to
the development of word list-based sentiment sys-
tems: the topics discovered by MLSLDA can serve
as a first-pass means of sentiment-based word lists
for languages that might lack annotated resources.
MLSLDA also can be viewed as a sentiment-
informed multilingual word sense disambiguation
(WSD) algorithm. When the multilingual bridge is an
explicit representation of sense such as WordNet, part
53
of the generative process is an explicit assignment
of every word to sense (the path latent variable ?);
this is discovered during inference. The dictionary-
based technique may be viewed as a disambiguation
via a transfer dictionary. How sentiment prediction
impacts the implicit WSD is left to future work.
Better capturing local syntax and meaningful col-
locations would also improve the model?s ability to
predict sentiment and model multilingual topics, as
would providing a better mechanism for represent-
ing words not included in our bridges. We intend to
develop such models as future work.
7 Acknowledgments
This research was funded in part by the Army Re-
search Laboratory through ARL Cooperative Agree-
ment W911NF-09-2-0072 and by the Office of the
Director of National Intelligence (ODNI), Intelli-
gence Advanced Research Projects Activity (IARPA),
through the Army Research Laboratory. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of ARL, IARPA, the ODNI, or the U.S. Govern-
ment. The authors thank the anonymous reviewers,
Jonathan Chang, Christiane Fellbaum, and Lawrence
Watts for helpful comments. The authors especially
thank Chris Potts for providing help in obtaining and
processing reviews.
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic mod-
eling via Dirichlet forest priors. In ICML.
Andrea Esuli Stefano Baccianella and Fabrizio Sebastiani.
2010. Sentiwordnet 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In LREC.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer
Hassan. 2008. Multilingual subjectivity analysis using
machine translation. In EMNLP.
David M. Blei and John D. Lafferty. 2005. Correlated
topic models. In NIPS.
David M. Blei and Jon D. McAuliffe. 2007. Supervised
topic models. In NIPS. MIT Press.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In UAI.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In EMNLP.
Jonathan Chang and David M. Blei. 2009. Relational
topic models for document networks. In AISTATS.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In NAACL.
Noah Constant, Christopher Davis, Christopher Potts, and
Florian Schwarz. 2009. The pragmatics of expressive
content: Evidence from large corpora. Sprache und
Datenverarbeitung, 33(1?2).
Kerstin Denecke. 2008. Using SentiWordNet for multilin-
gual sentiment analysis. In ICDEW 2008.
Paul Denisowski. 1997. CEDICT.
http://www.mdbg.net/chindict/.
Jean Diebolt and Eddie H.S. Ip, 1996. Markov Chain
Monte Carlo in Practice, chapter Stochastic EM:
method and application. Chapman and Hall, London.
Alexander Geyken. 2007. The DWDS corpus: A ref-
erence corpus for the German language of the 20th
century. In Idioms and Collocations: Corpus-based
Linguistic, Lexicographic Studies. Continuum Press.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
NAACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(Suppl 1):5228?5235.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and
Dan Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In ACL, Columbus, Ohio.
Jan Hefti. 2005. HanDeDict. http://chdw.de.
Hitoshi Isahara, Fransis Bond, Kiyotaka Uchimoto, Masao
Utiyama, and Kyoko Kanzaki. 2008. Development of
the Japanese WordNet. In LREC.
Mark Johnson. 2010. PCFGs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In ACL.
Woosung Kim and Sanjeev Khudanpur. 2004. Lexical
triggers and latent semantic analysis for cross-lingual
language model adaptation. TALIP, 3(2):94?112.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT Summit.
http://www.statmt.org/europarl/.
Claudia Kunze and Lothar Lemnitzer. 2002. Standardiz-
ing WordNets in a web-compliant format: The case of
GermaNet. In Workshop on Wordnets Structures and
Standardisation.
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In CIKM.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In ECML PKDD.
54
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching. ACL.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In ACL.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In WWW.
Ilya Dan Melamed. 1998. Empirical methods for exploit-
ing parallel texts. Ph.D. thesis, University of Pennsyl-
vania.
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264.
David Mimno, Hanna Wallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In EMNLP.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
WWW.
Noam Ordan and Shuly Wintner. 2007. Hebrew Word-
Net: a test case of aligning lexical databases across lan-
guages. International Journal of Translation, 19(1):39?
58.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In ACL.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Now Publishers Inc.
Martin Porter and Richard Boulton. 1970. Snowball
stemmer. http://snowball.tartarus.org/credits.php.
Matthew Purver, Konrad Ko?rding, Thomas L. Griffiths,
and Joshua Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In ACL.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In ACL, pages 320?322.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In IJCAI, pages
448?453.
Frank Richter. 2008. Dictionary nice grep. http://www-
user.tu-chemnitz.de/ fri/ding/.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern boot-
strapping. In NAACL.
Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model for
authors and documents. In UAI.
Beno??t Sagot and Darja Fis?er. 2008. Building a Free
French WordNet from Multilingual Resources. In On-
toLex.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization. In
ACL, pages 308?316.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Ju-
rafsky, and Christopher Manning. 2005. A conditional
random field word segmenter. In SIGHAN Workshop
on Chinese Language Processing.
University of Oxford. 2006. British Na-
tional Corpus. http://www.natcorp.ox.ac.uk/.
http://www.natcorp.ox.ac.uk/.
Tobias Vetter, Manfred Sauer, and Philipp Wallutat.
2000. Filmrezension.de: Online-magazin fu?r filmkritik.
http://www.filmrezension.de.
Xiaojun Wan. 2009. Co-training for cross-lingual senti-
ment classification. In ACL.
Chong Wang, David Blei, and Li Fei-Fei. 2009. Simulta-
neous image classification and annotation. In CVPR.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrieval. In SIGIR.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In CIKM.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polarity,
and Attitudes of Private States. Ph.D. thesis, University
of Pittsburgh.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In ACL.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. Medlda:
maximum margin supervised topic models for regres-
sion and classification. In ICML.
55
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284?292,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Modeling Perspective using Adaptor Grammars
Eric A. Hardisty
Department of Computer Science
and UMIACS
University of Maryland
College Park, MD
hardisty@cs.umd.edu
Jordan Boyd-Graber
UMD iSchool
and UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Department of Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
Strong indications of perspective can often
come from collocations of arbitrary length; for
example, someone writing get the government
out of my X is typically expressing a conserva-
tive rather than progressive viewpoint. How-
ever, going beyond unigram or bigram features
in perspective classification gives rise to prob-
lems of data sparsity. We address this prob-
lem using nonparametric Bayesian modeling,
specifically adaptor grammars (Johnson et al,
2006). We demonstrate that an adaptive na??ve
Bayes model captures multiword lexical usages
associated with perspective, and establishes a
new state-of-the-art for perspective classifica-
tion results using the Bitter Lemons corpus, a
collection of essays about mid-east issues from
Israeli and Palestinian points of view.
1 Introduction
Most work on the computational analysis of senti-
ment and perspective relies on lexical features. This
makes sense, since an author?s choice of words is
often used to express overt opinions (e.g. describing
healthcare reform as idiotic or wonderful) or to frame
a discussion in order to convey a perspective more
implicitly (e.g. using the term death tax instead of
estate tax). Moreover, it is easy and efficient to rep-
resent texts as collections of the words they contain,
in order to apply a well known arsenal of supervised
techniques (Laver et al, 2003; Mullen and Malouf,
2006; Yu et al, 2008).
At the same time, standard lexical features have
their limitations for this kind of analysis. Such fea-
tures are usually created by selecting some small
n-gram size in advance. Indeed, it is not uncommon
to see the feature space for sentiment analysis limited
to unigrams. However, important indicators of per-
spective can also be longer (get the government out
of my). Trying to capture these using standard ma-
chine learning approaches creates a problem, since
allowing n-grams as features for larger n gives rise
to problems of data sparsity.
In this paper, we employ nonparametric Bayesian
models (Orbanz and Teh, 2010) in order to address
this limitation. In contrast to parametric models, for
which a fixed number of parameters are specified in
advance, nonparametric models can ?grow? to the
size best suited to the observed data. In text analysis,
models of this type have been employed primarily
for unsupervised discovery of latent structure ? for
example, in topic modeling, when the true number of
topics is not known (Teh et al, 2006); in grammatical
inference, when the appropriate number of nontermi-
nal symbols is not known (Liang et al, 2007); and
in coreference resolution, when the number of enti-
ties in a given document is not specified in advance
(Haghighi and Klein, 2007). Here we use them for
supervised text classification.
Specifically, we use adaptor grammars (Johnson
et al, 2006), a formalism for nonparametric Bayesian
modeling that has recently proven useful in unsuper-
vised modeling of phonemes (Johnson, 2008), gram-
mar induction (Cohen et al, 2010), and named entity
structure learning (Johnson, 2010), to make super-
vised na??ve Bayes classification nonparametric in
order to improve perspective modeling. Intuitively,
na??ve Bayes associates each class or label with a
probability distribution over a fixed vocabulary. We
introduce adaptive na??ve Bayes (ANB), for which in
principle the vocabulary can grow as needed to in-
clude collocations of arbitrary length, as determined
284
by the properties of the dataset. We show that using
adaptive na??ve Bayes improves on state of the art
classification using the Bitter Lemons corpus (Lin
et al, 2006), a document collection that has been
used by a variety of authors to evaluate perspective
classification.
In Section 2, we review adaptor grammars, show
how na??ve Bayes can be expressed within the for-
malism, and describe how ? and how easily ? an
adaptive na??ve Bayes model can be created. Section 3
validates the approach via experimentation on the Bit-
ter Lemons corpus. In Section 4, we summarize the
contributions of the paper and discuss directions for
future work.
2 Adapting Na??ve Bayes to be Less Na??ve
In this work we apply the adaptor grammar formal-
ism introduced by Johnson, Griffiths, and Goldwa-
ter (Johnson et al, 2006). Adaptor grammars are a
generalization of probabilistic context free grammars
(PCFGs) that make it particularly easy to express non-
parametric Bayesian models of language simply and
readably using context free rules. Moreover, John-
son et al provide an inference procedure based on
Markov Chain Monte Carlo techniques that makes
parameter estimation straightforward for all models
that can be expressed using adaptor grammars.1 Vari-
ational inference for adaptor grammars has also been
recently introduced (Cohen et al, 2010).
Briefly, adaptor grammars allow nonterminals to
be rewritten to entire subtrees. In contrast, a non-
terminal in a PCFG rewrites only to a collection
of grammar symbols; their subsequent productions
are independent of each other. For instance, a tradi-
tional PCFG might learn probabilities for the rewrite
rule PP 7? P NP. In contrast, an adaptor gram-
mar can learn (or ?cache?) the production PP 7?
(P up)(NP(DET a)(N tree)). It does this by posit-
ing that the distribution over children for an adapted
non-terminal comes from a Pitman-Yor distribution.
A Pitman-Yor distribution (Pitman and Yor, 1997)
is a distribution over distributions. It has three pa-
rameters: the discount, a, such that 0 ? a < 1,
the strength, b, a real number such that ?a < b,
1And, better still, they provide code that
implements the inference algorithm; see
http://www.cog.brown.edu/ mj/Software.htm.
and a probability distribution G0 known as the base
distribution. Adaptor grammars allow distributions
over subtrees to come from a Pitman-Yor distribu-
tion with the PCFG?s original distribution over trees
as the base distribution. The generative process for
obtaining draws from a distribution drawn from a
Pitman-Yor distribution can be described by the ?Chi-
nese restaurant process? (CRP). We will use the CRP
to describe how to obtain a distribution over obser-
vations composed of sequences of n-grams, the key
to our model?s ability to capture perspective-bearing
n-grams.
Suppose that we have a base distribution ? that is
some distribution over all sequences of words (the
exact structure of such a distribution is unimportant;
such a distribution will be defined later in Table 1).
Suppose further we have a distribution ? drawn from
PY (a, b,?), and we wish to draw a series of obser-
vations w from ?. The CRP gives us a generative
process for doing those draws from ?, marginaliz-
ing out ?. Following the restaurant metaphor, we
imagine the ith customer in the series entering the
restaurant to take a seat at a table. The customer sits
by making a choice that determines the value of the
n-gram wi for that customer: she can either sit at an
existing table or start a new table of her own.2
If she sits at a new table j, that table is assigned
a draw yj from the base distribution, ?; note that,
since ? is a distribution over n-grams, yj is an n-
gram. The value of wi is therefore assigned to be yj ,
and yj becomes the sequence of words assigned to
that new table. On the other hand, if she sits at an
existing table, then wi simply takes the sequence of
words already associated with that table (assigned as
above when it was first occupied).
The probability of joining an existing table j,
with cj patrons already seated at table j, is
cj?a
c?+b
,
where c? is the number of patrons seated at all tables:
c? =
?
j? cj? . The probability of starting a new table
is b+t?ac?+b , where t is the number of tables presently
occupied.
Notice that ? is a distribution over the same space
as ?, but it can drastically shift the mass of the dis-
tribution, compared with ?, as more and more pa-
2Note that we are abusing notation by allowing wi to cor-
respond to a word sequence of length ? 1 rather than a single
word.
285
trons are seated at tables. However, there is always
a chance of drawing from the base distribution, and
therefore every word sequence can also always be
drawn from ?.
In the next section we will write a na??ve Bayes-like
generative process using PCFGs. We will then use
the PCFG distribution as the base distribution for a
Pitman-Yor distribution, adapting the na??ve Bayes
process to give us a distribution over n-grams, thus
learning new language substructures that are useful
for modeling the differences in perspective.
2.1 Classification Models as PCFGs
Na??ve Bayes is a venerable and popular mechanism
for text classification (Lewis, 1998). It posits that
there are K distinct categories of text ? each with a
distinct distribution over words ? and that every doc-
ument, represented as an exchangeable bag of words,
is drawn from one (and only one) of these distribu-
tions. Learning the per-category word distributions
and global prevalence of the classes is a problem of
posterior inference which can be approached using a
variety of inference techniques (Lowd and Domingos,
2005).
More formally, na??ve Bayes models can be ex-
pressed via the following generative process:3
1. Draw a global distribution over classes ? ?
Dir (?)
2. For each class i ? {1, . . . ,K}, draw a word
distribution ?i ? Dir (?)
3. For each document d ? {1, . . . ,M}:
(a) Draw a class assignment zd ? Mult (?)
(b) For each word position n ? {1, . . . , Nd,
draw wd,n ? Mult (?zd)
A variant of the na??ve Bayes generative process can
be expressed using the adaptor grammar formalism
(Table 1). The left hand side of each rule represents
a nonterminal which can be expanded, and the right
hand side represents the rewrite rule. The rightmost
indices show replication; for instance, there are |V |
rules that allow WORDi to rewrite to each word in the
3Here ? and ? are hyperparameters used to specify priors
for the class distribution and classes? word distributions, respec-
tively; ? is a symmetric K-dimensional vector where each ele-
ment is pi. Nd is the length of document d. Resnik and Hardisty
(2010) provide a tutorial introduction to the na??ve Bayes genera-
tive process and underlying concepts.
SENT 7? DOCd d = 1, . . . ,m
DOCd0.001 7? IDd WORDSi d = 1, . . . ,m;
i ? {1,K}
WORDSi 7? WORDSi WORDi i ? {1,K}
WORDSi 7? WORDi i ? {1,K}
WORDi 7? v v ? V ; i ? {1,K}
Table 1: A na??ve Bayes-inspired model expressed as a
PCFG.
vocabulary. One can assume a symmetric Dirichlet
prior of Dir (1?) over the production choices unless
otherwise specified ? as with the DOCd production
rule above, where a sparse prior is used.
Notice that the distribution over expansions for
WORDi corresponds directly to ?i in Figure 1(a).
There are, however, some differences between the
model that we have described above and the standard
na??ve Bayes model depicted in Figure 1(a). In par-
ticular, there is no longer a single choice of class per
document; each sentence is assigned a class. If the
distribution over per-sentence labels is sparse (as it
is above for DOCd), this will closely approximate
na??ve Bayes, since it will be very unlikely for the
sentences in a document to have different labels. A
non-sparse prior leads to behavior more like models
that allow parts of a document to express sentiment
or perspective differently.
2.2 Moving Beyond the Bag of Words
The na??ve Bayes generative distribution posits that
when writing a document, the author selects a distri-
bution of categories zd for the document from ?. The
author then generates words one at a time: each word
is selected independently from a flat multinomial
distribution ?zd over the vocabulary.
However, this is a very limited picture of how text
is related to underlying perspectives. Clearly words
are often connected with each other as collocations,
and, just as clearly, extending a flat vocabulary to
include bigram collocations does not suffice, since
sometimes relevant perspective-bearing phrases are
longer than two words. Consider phrases like health
care for all or government takeover of health care,
connected with progressive and conservative posi-
tions, respectively, during the national debate on
healthcare reform. Simply applying na??ve Bayes,
or any other model, to a bag of n-grams for high n is
286
KM
?
z
d
N
d
W
d,n
?
?
?
i
(a) Na??ve Bayes
K
M
?
z
d
N
d
W
d,n
a
?
?
i
b
?
?
(b) Adaptive Na??ve Bayes
Figure 1: A plate diagram for na??ve Bayes and adaptive na??ve Bayes. Nodes represent random variables and parameters;
shaded nodes represent observations; lines represent probabilistic dependencies; and the rectangular plates denote
replication.
going to lead to unworkable levels of data sparsity;
a model should be flexible enough to support both
unigrams and longer phrases as needed.
Following Johnson (2010), however, we can use
adaptor grammars to extend na??ve Bayes flexibly to
include richer structure like collocations when they
improve the model, and not including them when
they do not. This can be accomplished by introduc-
ing adapted nonterminal rules: in a revised genera-
tive process, the author can draw from Pitman-Yor
distribution whose base distribution is over word se-
quences of arbitrary length.4 Thus in a setting where,
say, K = 2, and our two classes are PROGRESSIVE
and CONSERVATIVE, the sequence health care for all
might be generated as a single unit for the progressive
perspective, but in the conservative perspective the
same sequence might be generated as three separate
draws: health care, for, all. Such a model is pre-
sented in Figure 1(b). Note the following differences
between Figures 1(a) and 1(b):
? zd selects which Pitman-Yor distribution to draw
from for document d.
? ?i is the distribution over n-grams that comes
from the Pitman-Yor distribution.
? Wd,n represents an n-gram draw from ?i
? a, b are the Pitman-Yor strength and discount
parameters.
? ? is the Pitman-Yor base distribution with ? as
its uniform hyperparameter.
4As defined above, the base distribution is that of the PCFG
production rule WORDSi. Although it has non-zero probability
of producing any sequence of words, it is biased toward shorter
word sequences.
Returning to the CRP metaphor discussed when we
introduced the Pitman-Yor distribution, there are two
restaurants, one for the PROGRESSIVE distribution
and one for the CONSERVATIVE distribution. Health
care for all has its own table in the PROGRESSIVE
restaurant, and enough people are sitting at it to make
it popular. There is no such table in the CONSERVA-
TIVE restaurant, so in order to generate those words,
the phrase health care for all would need to come
from a new table; however, it is more easily explained
by three customers sitting at three existing, popular
tables: health care, for, and all.
We follow the convention of Johnson (2010) by
writing adapted nonterminals as underlined. The
grammar for adaptive na??ve Bayes is shown in Ta-
ble 2. The adapted COLLOCi rule means that every
time we need to generate that nonterminal, we are
actually drawing from a distribution drawn from a
Pitman-Yor distribution. The distribution over the
possible yields of the WORDSi rule serves as the
base distribution.
Given this generative process for documents, we
can now use statistical inference to uncover the pos-
terior distribution over the latent variables, thus dis-
covering the tables and seating assignments of our
metaphorical restaurants that each cater to a specific
perspective filled with tables populated by words and
n-grams.
The model presented in Table 2 is the most straight-
forward way of extending na??ve Bayes to collocations.
For completeness, we also consider the alternative
of using a shared base distribution rather than dis-
tinguishing the base distributions of the two classes.
287
SENT 7? DOCd d = 1, . . . ,m
DOCd0.001 7? IDd SPANi d = 1, . . . ,m;
i ? {1,K}
SPANi 7? SPANi COLLOCi i ? {1,K}
SPANi 7? COLLOCi i ? {1,K}
COLLOCi 7? WORDSi i ? {1,K}
WORDSi 7? WORDSi WORDi i ? {1,K}
WORDSi 7? WORDi i ? {1,K}
WORDi 7? v v ? V ; i ? {1,K}
Table 2: An adaptive na??ve Bayes grammar. The
COLLOCi nonterminal?s distribution over yields is drawn
from a Pitman-Yor distribution rather than a Dirichlet over
production rules.
SENT 7? DOCd d = 1, . . . ,m
DOCd0.001 7? IDd SPANi d = 1, . . . ,m;
i ? {1,K}
SPANi 7? SPANi COLLOCi i ? {1,K}
SPANi 7? COLLOCi i ? {1,K}
COLLOCi 7? WORDS i ? {1,K}
WORDS 7? WORDS WORD
WORDS 7? WORD
WORD 7? v v ? V
Table 3: An adaptive na??ve Bayes grammar with a com-
mon base distribution for collocations. Note that, in con-
trast to Table 2, there are no subscripts on WORDS or
WORD.
Briefly, using a shared base distribution posits that
the two classes use similar word distributions, but
generate collocations unique to each class, whereas
using separate base distributions assumes that the
distribution of words is unique to each class.
3 Experiments
3.1 Corpus Description
We conducted our classification experiments on the
Bitter Lemons (BL) corpus, which is a collection of
297 essays averaging 700-800 words in length, on
various Middle East issues, written from both the
Israeli and Palestinian perspectives. The BL corpus
was compiled by Lin et al (2006) and is derived from
a website that invites weekly discussions on a topic
and publishes essays from two sets of authors each
week.5 Two of the authors are guests, one from each
perspective, and two essays are from the site?s regular
contributors, also one from each perspective, for a
5http://www.bitterlemons.org
K
M
?
z
d
N
d
W
d,n
a
?
?
i
b
?
?
Figure 2: An alternative adaptive na??ve Bayes with a com-
mon base distribution for both classes.
Training Set
Test Set
Corpus Filter
Grammar 
Generator
Corpus Filter
Vocabulary 
Generator
AG Classifier
Figure 3: Corpus preparation and experimental setup.
total of four essays on each topic per week. We chose
this corpus to allow us to directly compare our results
with Greene and Resnik?s (2009) Observable Proxies
for Underlying Semantics (OPUS) features and Lin
et al?s Latent Sentence Perspective Model (LSPM).
The classification goal for this corpus is to label each
document with the perspective of its author, either
Israeli or Palestinian.
Consistent with prior work, we prepared the corpus
by dividing it into two groups, one group containing
all of the essays written by the regular site contrib-
utors, which we call the Editor set, and one group
comprised of all the essays written by the guest con-
tributors, which we call the Guest set. Similar to the
above mentioned prior work, we perform classifica-
tion using one group as training data and the other as
test data and perform two folds of classification. The
overall experimental setup and corpus preparation
process is presented in Figure 3.
288
3.2 Experimental Setup
The vocabulary generator determines the vocabulary
used by a given experiment by converting the training
set to lower case, stemming with the Porter stemmer,
and filtering punctuation. We remove from the vocab-
ulary any words that appeared in only one document
regardless of frequency within that document, words
with frequencies lower than a threshold, and stop
words.6 The vocabulary is then passed to a grammar
generator and a corpus filter.
The grammar generator uses the vocabulary to gen-
erate the terminating rules of the grammar from the
ANB grammar presented in Tables 2 and 3. The cor-
pus filter takes in a set of documents and replaces all
words not in the vocabulary with ?out of vocabulary?
markers. This process ensures that in all experiments
the vocabulary is composed entirely of words from
the training set. After the groups have been filtered,
the group used as the test set has its labels removed.
The test and training set are then sent, along with the
grammar, into the adaptor grammar inference engine.
Each experiment ran for 3000 iterations. For the
runs where adaptation was used we set the initial
Pitman-Yor a and b parameters to 0.01 and 10 respec-
tively, then slice sample (Johnson and Goldwater,
2009).
We use the resulting sentence parses for classifi-
cation. By design of the grammar, each sentence?s
words will belong to one and only one distribution.
We identify that distribution from each of the test
set sentence parses and use it as the sentence level
classification for that particular sentence. We then
use majority rule on the individual sentence classifi-
cations in a document to obtain the document classifi-
cation. (In most cases the sentence-level assignments
are overwhelmingly dominated by one class.)
3.3 Results and Analysis
Table 4 gives the results and compares to prior
work. The support vector machine (SVM), NB-
B and LSPM results are taken directly from Lin
et al (2006). NB-B indicates na??ve Bayes with
full Bayesian inference. LSPM is the Latent
Sentence Perspective Model, also from Lin et
al. (2006). OPUS results are taken from Greene
6In these experiments, a frequency threshold of 4 was se-
lected prior to testing.
Training Set Test Set Classifier Accuracy
Guests Editors SVM 88.22
Guests Editors NB-B 93.46
Guests Editors LSPM 94.93
Guests Editors OPUS 97.64
Guests Editors ANB* 99.32
Guests Editors ANB Com 99.93
Guests Editors ANB Sep 99.87
Editors Guests SVM 81.48
Editors Guests NB-B 85.85
Editors Guests LSPM 86.99
Editors Guests OPUS 85.86
Editors Guests ANB* 84.98
Editors Guests ANB Com 82.76
Editors Guests ANB Sep 88.28
Table 4: Classification results. ANB* indicates the same
grammar as Adapted Na??ve Bayes, but with adaptation dis-
abled. Com and Sep refer to whether the base distribution
was common to both classes or separate.
and Resnik (2009). Briefly, OPUS features are gener-
ated from observable grammatical relations that come
from dependency parses of the corpus. Use of these
features provided the best classification accuracy for
this task prior to this work. ANB* refers to the gram-
mar from Table 2, but with adaptation disabled. The
reported accuracy values for ANB*, ANB with a
common base distribution (see Table 3), and ANB
with separate base distributions (see Table 2) are
the mean values from five separate sampling chains.
Bold face indicates statistical signficance (p < 0.05)
by unpaired t-test between the reported value and
ANB*.
Consistent with all prior work on this corpus we
found that the classification accuracy for training on
editors and testing on guests was lower than the other
direction since the larger number of editors in the
guest set alows for greater generalization. The dif-
ference between ANB* and ANB with a common
base distribution is not statistically significant. Also
of note is that the classification accuracy improves
for testing on Guests when the ANB grammar is al-
lowed to adapt and a separate base distribution is used
for the two classes (88.28% versus 84.98% without
adaptation).
Table 5 presents some data on adapted rules
289
Unique Unique Percent of Group
Class Group Unigrams Cached n-grams Cached Vocabulary Cached
Israeli Editors 2,292 19,614 77.62
Palestinian Editors 2,180 17,314 86.54
Israeli Guests 2,262 19,398 79.91
Palestinian Guests 2,005 16,946 74.94
Table 5: Counts of cached unigrams and n-grams for the two classes compared to the vocabulary sizes.
Israeli Palestinian
zionist dream american jew
zionist state achieve freedom
zionist movement palestinian freedom
american leadership support palestinian
american victory palestinian suffer
abandon violence palestinian territory
freedom (of the) press palestinian statehood
palestinian violence palestinian refugee
Table 6: Charged bigrams captured by the framework.
learned once inference is complete. The column
labeled unique unigrams cached indicates the num-
ber of unique unigrams that appear on the right hand
side of the adapted rules. Similarly, unique n-grams
cached indicates the number of unique n-grams that
appear on the right hand side of the adapted rules.
The rightmost column indicates the percentage of
terms from the group vocabulary that appear on the
right hand side of adapted rules as unigrams. Values
less than 100% indicate that the remaining vocabu-
lary terms are cached in n-grams. As the table shows,
a significant number of the rules learned during infer-
ence are n-grams of various sizes.
Inspection of the captured bigrams showed that
it captured sequences that a human might associate
with one perspective over the other. Table 6 lists just
a few of the more charged bigrams that were captured
in the adapted rules.
More specific caching information on the individ-
ual groups and classes is provided in Table 7. This
data clearly demonstrates that raw n-gram frequency
alone is not indicative of how many times an n-gram
is used as a cached rule. For example, consider the
bigram people go, which is used as a cached bigram
only three times, yet appears in the corpus 407 times.
Compare that with isra palestinian, which is cached
the same number of times but appears only 18 times
in the corpus. In other words, the sequence people go
is more easily explained by two sequential unigrams,
not a bigram. The ratio of cache use counts to raw
bigrams gives a measure of strength of collocation
between the terms of the n-gram. We conjecture that
the rareness of caching for n > 2 is a function of the
small corpus size. Also of note is the improvement in
performance of ANB* over NB-B when training on
guests, which we suspect is due to our use of sampled
versus fixed hyperparameters.
4 Conclusions
In this paper, we have applied adaptor grammars in
a supervised setting to model lexical properties of
text and improve document classification according
to perspective, by allowing nonparametric discovery
of collocations that aid in perspective classification.
The adaptive na??ve Bayes model improves on state
of the art supervised classification performance in
head-to-head comparisons with previous approaches.
Although there have been many investigations on
the efficacy of using multiword collocations in text
classification (Bekkerman and Allan, 2004), usually
such approaches depend on a preprocessing step such
as computing tf-idf or other measures of frequency
based on either word bigrams (Tan et al, 2002) or
character n-grams (Raskutti et al, 2001). In con-
trast, our approach combines phrase discovery with
the probabilistic model of the text. This allows for
the collocation selection and classification to be ex-
pressed in a single model, which can then be extended
later; it also is truly generative, as compared with fea-
ture induction and selection algorithms that either
under- or over-generate data.
There are a number of interesting directions in
which to take this research. As Johnson et al (2006)
argue, and as we have confirmed here, the adaptor
290
Guest Editor
Israeli Palestinian Israeli Palestinian
palestinian OOV 11 299 palestinian isra 6 178 palestinian OOV 8 254 OOV israel 7 198
OOV palestinian 7 405 OOV palestinian 6 405 OOV palestinian 7 319 OOV palestinian 6 319
isra OOV 6 178 palestinian OOV 5 29 OOV israel 7 123 OOV work 5 254
israel OOV 6 94 one OOV 4 25 OOV us 6 115 OOV agreement 5 75
sharon OOV 4 74 side OOV 3 21 OOV part 5 56 palestinian reform 4 49
polit OOV 4 143 polit OOV 3 299 israel OOV 5 81 palestinian OOV 4 81
OOV us 4 29 peopl go 3 407 attempt OOV 5 91 OOV isra 4 15
OOV state 4 37 palestinian govern 3 94 time OOV 4 63 one OOV 4 27
israel palestinian 4 52 palestinian accept 3 220 remain OOV 4 85 isra palestinian 4 17
even OOV 4 43 OOV state 3 150 OOV time 4 70 isra OOV 4 63
arafat OOV 4 41 OOV israel 3 18 OOV area 4 49 howev OOV 4 149
appear OOV 4 53 OOV end 3 20 OOV arafat 4 28 want OOV 3 36
total OOV 3 150 OOV act 3 105 isra OOV 4 8 us OOV 3 35
palestinian would 3 65 isra palestinian 3 18 would OOV 3 28 recent OOV 3 220
palestinian isra 3 35 israel OOV 3 198 use OOV 3 198 palestinian isra 3 115
Table 7: Most frequently used cached bigrams. The first colum in each section is the number of times that bigram was
used as a cached rule. The second column indicates the raw count of that bigram in the Guests or Editors group.
grammar formalism makes it quite easy to work with
latent variable models, in order to automatically dis-
cover structures in the data that have predictive value.
For example, it is easy to imagine a model where in
addition to a word distribution for each class, there
is also an additional shared ?neutral? distribution:
for each sentence, the words in that sentence can ei-
ther come from the class-specific content distribution
or the shared neutral distribution. This turns out to
be the Latent Sentence Perspective Model of Lin et
al. (2006), which is straightforward to encode using
the adaptor grammar formalism simply by introduc-
ing two new nonterminals to represent the neutral
distribution:
SENT 7? DOCd d = 1, . . . ,m
DOCd 7? IDd WORDSi d = 1, . . . ,m;
i ? {1,K}
DOCd 7? IDd NEUTS d = 1, . . . ,m;
WORDSi 7? WORDSi WORDi i ? {1,K}
WORDSi 7? WORDi i ? {1,K}
WORDi 7? v v ? V ; i ? {1,K}
NEUT 7? NEUTSi NEUTi
NEUT 7? NEUT
NEUT 7? v v ? V
Running this grammar did not produce improvements
consistent with those reported by Lin et al We plan to
investigate this further, and a natural follow-on would
be to experiment with adaptation for this variety of
latent structure, to produce an adapted LSPM-like
model analogous to adaptive na??ve Bayes.
Viewed in a larger context, computational classi-
fication of perspective is closely connected to social
scientists? study of framing, which Entman (1993)
characterizes as follows: ?To frame is to select some
aspects of a perceived reality and make them more
salient in a communicating text, in such a way as
to promote a particular problem definition, causal
interpretation, moral evaluation, and/or treatment rec-
ommendation for the item described.? Here and in
other work (e.g. (Laver et al, 2003; Mullen and Mal-
ouf, 2006; Yu et al, 2008; Monroe et al, 2008)),
it is clear that lexical evidence is one key to under-
standing how language is used to frame discussion
from one perspective or another; Resnik and Greene
(2009) have shown that syntactic choices can pro-
vide important evidence, as well. Another promising
direction for this work is the application of adaptor
grammar models as a way to capture both lexical and
grammatical aspects of framing in a unified model.
Acknowledgments
This research was funded in part by the Army Re-
search Laboratory through ARL Cooperative Agree-
ment W911NF-09-2-0072 and by the Office of the
Director of National Intelligence (ODNI), Intelli-
gence Advanced Research Projects Activity (IARPA),
through the Army Research Laboratory. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be con-
strued as representing the official views or policies
291
of ARL, IARPA, the ODNI, or the U.S. Government.
The authors thank Mark Johnson and the anonymous
reviewers for their helpful comments and discussions.
We are particularly grateful to Mark Johnson for mak-
ing his adaptor grammar code available.
References
R. Bekkerman and J. Allan. 2004. Using bigrams in text
categorization. Technical Report IR-408, Center of
Intelligent Information Retrieval, UMass Amherst.
S. B. Cohen, D. M. Blei, and N. A. Smith. 2010. Varia-
tional inference for adaptor grammars. In Conference
of the North American Chapter of the Association for
Computational Linguistics.
R.M. Entman. 1993. Framing: Toward Clarification of a
Fractured Paradigm. The Journal of Communication,
43(4):51?58.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 503?511.
Aria Haghighi and Dan Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 848?855,
Prague, Czech Republic, June.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Conference of the North American Chapter of
the Association for Computational Linguistics, pages
317?325, Boulder, Colorado, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2006. Adaptor grammars: A framework for
specifying compositional nonparametric Bayesian mod-
els. In Proceedings of Advances in Neural Information
Processing Systems.
Mark Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Proceedings of ACL-08: HLT, pages 398?
406, Columbus, Ohio, June.
Mark Johnson. 2010. PCFGs, topic models, adaptor gram-
mars and learning topical collocations and the structure
of proper names. In Proceedings of the Association for
Computational Linguistics, pages 1148?1157, Uppsala,
Sweden, July.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
pages 311?331.
David D. Lewis. 1998. Naive (bayes) at forty: The inde-
pendence assumption in information retrieval. In Claire
Ne?dellec and Ce?line Rouveirol, editors, Proceedings
of ECML-98, 10th European Conference on Machine
Learning, number 1398, pages 4?15, Chemnitz, DE.
Springer Verlag, Heidelberg, DE.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of Emperical Methods in
Natural Language Processing, pages 688?697.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexan-
der Hauptmann. 2006. Which side are you on? Identi-
fying perspectives at the document and sentence levels.
In Proceedings of the Conference on Natural Language
Learning (CoNLL).
Daniel Lowd and Pedro Domingos. 2005. Naive bayes
models for probability estimation. In ICML ?05: Pro-
ceedings of the 22nd international conference on Ma-
chine learning, pages 529?536, New York, NY, USA.
ACM.
Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn.
2008. Fightin? Words: Lexical Feature Selection and
Evaluation for Identifying the Content of Political Con-
flict. Political Analysis, Vol. 16, Issue 4, pp. 372-403,
2008.
Tony Mullen and Robert Malouf. 2006. A preliminary in-
vestigation into sentiment analysis of informal political
discourse. In AAAI Symposium on Computational Ap-
proaches to Analysing Weblogs (AAAI-CAAW), pages
159?162.
P. Orbanz and Y. W. Teh. 2010. Bayesian nonparamet-
ric models. In Encyclopedia of Machine Learning.
Springer.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordinator.
Annals of Probability, 25(2):855?900.
Bhavani Raskutti, Herman L. Ferra?, and Adam Kowal-
czyk. 2001. Second order features for maximising text
classification performance. In EMCL ?01: Proceedings
of the 12th European Conference on Machine Learning,
pages 419?430, London, UK. Springer-Verlag.
Philip Resnik and Eric Hardisty. 2010. Gibbs
sampling for the uninitiated. Technical Re-
port UMIACS-TR-2010-04, University of Maryland.
http://www.lib.umd.edu/drum/handle/1903/10058.
Chade-Meng Tan, Yuan-Fang Wang, and Chan-Do Lee.
2002. The use of bigrams to enhance text categoriza-
tion. Inf. Process. Manage., 38(4):529?546.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Association,
101(476):1566?1581.
B. Yu, S. Kaufmann, and D. Diermeier. 2008. Classify-
ing party affiliation from political speech. Journal of
Information Technology and Politics, 5(1):33?48.
292
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1290?1301, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Besting the Quiz Master:
Crowdsourcing Incremental Classification Games
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Brianna Satinoff, He He, and Hal Daume? III
Department of Computer Science
University of Maryland
{bsonrisa, hhe, hal}@cs.umd.edu
Abstract
Cost-sensitive classification, where the features
used in machine learning tasks have a cost, has
been explored as a means of balancing knowl-
edge against the expense of incrementally ob-
taining new features. We introduce a setting
where humans engage in classification with
incrementally revealed features: the collegiate
trivia circuit. By providing the community with
a web-based system to practice, we collected
tens of thousands of implicit word-by-word
ratings of how useful features are for eliciting
correct answers. Observing humans? classifi-
cation process, we improve the performance
of a state-of-the art classifier. We also use the
dataset to evaluate a system to compete in the
incremental classification task through a reduc-
tion of reinforcement learning to classification.
Our system learns when to answer a question,
performing better than baselines and most hu-
man players.
1 Introduction
A typical machine learning task takes as input a set
of features and learns a mapping from features to a
label. In such a setting, the objective is to minimize
the error of the mapping from features to labels. We
call this traditional setting, where all of the features
are consumed, rapacious machine learning.1
This not how humans approach the same task.
They do not exhaustively consider every feature. Af-
ter a certain point, a human has made a decision
and no longer needs additional features. Even in-
defatigable computers cannot always exhaustively
consider every feature. This is because the result
1Earlier drafts called this ?batch? machine learning, which
confused the distinction between batch and online learning. We
gladly adopt ?rapacious? to make this distinction clearer and
to cast traditional machine learning?that always examines all
features?as a resource hungry approach.
is time sensitive, such as in interactive systems, or
because processing time is limited by the sheer quan-
tity of data, as in sifting e-mail for spam (Pujara et
al., 2011). In such settings, often the best solution
is incremental: allow a decision to be made without
seeing all of an instance?s features. We discuss the
incremental classification framework in Section 2.
Our understanding of how humans conduct incre-
mental classification is limited. This is because com-
plicating an already difficult annotation task is often
an unwise tradeoff. Instead, we adapt a real world
setting where humans are already engaging (eagerly)
in incremental classification?trivia games?and de-
velop a cheap, easy method for capturing human
incremental classification judgments.
After qualitatively examining how humans con-
duct incremental classification (Section 3), we show
that knowledge of a human?s incremental classifi-
cation process improves state-of-the-art rapacious
classification (Section 4). Having established that
these data contain an interesting signal, we build
Bayesian models that, when embedded in a Markov
decision process, can engage in effective incremental
classification (Section 5), and develop new hierar-
chical models combining local and thematic content
to better capture the underlying content (Section 7).
Finally, we conclude in Section 8 and discuss exten-
sions to other problem areas.
2 Incremental Classification
In this section, we discuss previous approaches that
explore how much effort or resources a classifier
needs to come to a decision, a problem not as thor-
oughly examined as the question of whether the de-
cision is right or not.2 Incremental classification is
2When have an externally interrupted feature stream, the
setting is called ?any time? (Boddy and Dean, 1989; Horsch and
Poole, 1998). Like ?budgeted? algorithms (Wang et al 2010),
these are distinct but related problems.
1290
not equivalent to missing features, which have been
studied at training time (Cesa-Bianchi et al 2011),
test time (Saar-Tsechansky and Provost, 2007), and
in an online setting (Rostamizadeh et al 2011). In
contrast, incremental classification allows the learner
to decide whether to acquire additional features.
A common paradigm for incremental classification
is to view the problem as a Markov decision process
(MDP) (Zubek and Dietterich, 2002). The incremen-
tal classifier can either request an additional feature
or render a classification decision (Chai et al 2004;
Ji and Carin, 2007; Melville et al 2005), choosing
its actions to minimize a known cost function. Here,
we assume that the environment chooses a feature
in contrast to a learner, as in some active learning
settings (Settles, 2011). In Section 5, we use a MDP
to decide whether additional features need to be pro-
cessed in our application of incremental classification
to a trivia game.
2.1 Trivia as Incremental Classification
A real-life setting where humans classify documents
incrementally is quiz bowl, an academic competition
between schools in English-speaking countries; hun-
dreds of teams compete in dozens of tournaments
each year (Jennings, 2006). Note the distinction be-
tween quiz bowl and Jeopardy, a recent application
area (Ferrucci et al 2010). While Jeopardy also uses
signaling devices, these are only usable after a ques-
tion is completed (interrupting Jeopardy?s questions
would make for bad television). Thus, Jeopardy is
rapacious classification followed by a race to see?
among those who know the answer?who can punch
a button first. Moreover, buzzes before the question?s
end are penalized.
Two teams listen to the same question.3 In this
context, a question is a series of clues (features) re-
ferring to the same entity (for an example question,
see Figure 1). We assume a fixed feature ordering
for a test sequence (i.e., you cannot request specific
features). Teams interrupt the question at any point
by ?buzzing in?; if the answer is correct, the team
gets points and the next question is read. Otherwise,
the team loses points and the other team can answer.
3Called a ?starter? (UK) or ?tossup? (US) in the lingo, as it
often is followed by a ?bonus? given to the team that answers the
starter; here we only concern ourselves with tossups answerable
by both teams.
After losing a race for the Senate, this politician edited the Om-
aha World-Herald. This man resigned 3 from one of his posts
when the President sent a letter to Germany protesting the Lusi-
tania 3 sinking, and 3 he advocated 3 coining 3 silver at a 16
3 to 1 33 rate 3 compared to 3 gold. He was the 3 three-time
Democratic 3 Party 333 nominee for 3 President 3 but 333
lost to McKinley twice 33 and then Taft, although he served as
Secretary of State 33 under Woodrow Wilson, 3 and he later
argued 3 against Clarence Darrow 3 in the Scopes 33 Monkey
Trial. For ten points, name this 3 man who famously declared
that ?we shall not be crucified on a Cross of 3 Gold?. 3
Figure 1: Quiz bowl question on William Jennings Bryan,
a late nineteenth century American politician; obscure
clues are at the beginning while more accessible clues are
at the end. Words (excluding stop words) are shaded based
on the number of times the word triggered a buzz from any
player who answered the question (darker means more
buzzes; buzzes contribute to the shading of the previous
five words). Diamonds (3) indicate buzz positions.
The answers to quiz bowl questions are well-
known entities (e.g., scientific laws, people, battles,
books, characters, etc.), so the answer space is rel-
atively limited; there are no open-ended questions
of the form ?why is the sky blue?? However, there
are no multiple choice questions?as there are in
Who Wants to Be a Millionaire (Lam et al 2003)?
or structural constraints?as there are in crossword
puzzles (Littman et al 2002).
Now that we introduced the concepts of questions,
answers, and buzzes, we pause briefly to define them
more formally and explicitly connect to machine
learning. In the sequel, we will refer to: questions,
sequences of words (tokens) associated with a single
answer; features, inputs used for decisions (derived
from the tokens in a question); labels, a question?s
correct response; answers, the responses (either cor-
rect or incorrect) provided; and buzzes, positions in
a question where users halted the stream of features
and gave an answer.
Quiz bowl is not a typical problem domain for natu-
ral language processing; why should we care about it?
First, it is a real-world instance of incremental classi-
fication that happens hundreds of thousands of times
most weekends. Second, it is a classification problem
intricately intertwined with core computational lin-
guistics problems such as anaphora resolution, online
sentence processing, and semantic priming. Finally,
quiz bowl?s inherent fun makes it easy to acquire
human responses, as we describe in the next section.
1291
Number of Tokens Revealed
Ac
cu
rac
y
0.4
0.6
0.8
1.0
40 60 80 100
Total
500
1000
1500
2000
2500
3000
Figure 2: Users plotted based on accuracy vs. the number
of tokens?on average?the user took to give an answer.
Dot size and colour represent the total number of ques-
tions answered. Users that answered questions later in the
question had higher accuracy. However, there were users
that were able to answer questions relatively early without
sacrificing accuracy.
3 Getting a Buzz through Crowdsourcing
We built a corpus with 37,225 quiz bowl questions
with 25,498 distinct labels from 121 tournaments
written for tournaments between 1999 and 2010. We
created a webapp4 that simulates the experience of
playing quiz bowl. Text is incrementally revealed
(at a pace adjustable by the user) until users press
the space bar to ?buzz?. Users then answer, and the
webapp judges correctness using a string matching
algorithm. Players can override the automatic check
if the system mistakenly judged an answer incorrect.
Answers of previous users are displayed after answer-
ing a question; this enhances the sense of community
and keeps users honest (e.g., it?s okay to say that ?wj
bryan? is an acceptable answer for the label ?william
jennings bryan?, but ?asdf? is not). We did not see
examples of nonsense answers from malicious users;
in contrast, users were stricter than we expected, per-
haps because protesting required effort.
To collect a set of labels with many buzzes, we
focused on the 1186 labels with more than four dis-
tinct questions. Thus, we shuffled the labels into a
canonical order shown to all users (e.g., everyone
saw a question on ?Jonathan Swift? and then a ques-
tion on ?William Jennings Bryan?, but because these
labels have many questions the specific questions
4Play online or download the datasets at http://umiacs.
umd.edu/?jbg/qb.
Figure 3: A screenshot of the webapp used to collect data.
Users see a question revealed one word at a time. They
signal buzzes by clicking on the answer button and input
an answer.
were different for each user). Participants were ea-
ger to answer questions; over 7000 questions were
answered in the first day, and over 43000 questions
were answered in two weeks by 461 users.
To represent a ?buzz?, we define a function b(q, f)
(?b? for buzz) as the number of times that feature
f occurred in question q at most five tokens before
a user correctly buzzed on that question.5 Aggre-
gating buzzes across questions (summing over q)
shows different features useful for eliciting a buzz
(Figure 4(a)). Some features coarsely identify the
type of answer sought, e.g., ?author?, ?opera?, ?city?,
?war?, or ?god?. Other features are relational, con-
necting the answer to other clues, e.g., ?namesake?,
?defeated?, ?husband?, or ?wrote?. The set of buzzes
help narrow which words are important for matching
a question to its answer; for an example, see how
the word cloud for all of the buzzes on ?Wuthering
5This window was chosen qualitatively by examining the
patterns of buzzes; this is person-dependent, based on reading
comprehension, reaction time, and what reveal speed the user
chose. We leave explicitly modeling this for future work.
1292
(a) Buzzes over all Questions (b) Wuthering Heights Question Text (c) Buzzes on Wuthering Heights
Figure 4: Word clouds representing all words that were a part of a buzz (a), the original text appearing in seven questions
on the book ?Wuthering Heights? by Emily Bro?nte (b), and the buzzes of users on those questions (c). The buzzes
reflect what users remember about the work and is more focused than the complete question text.
Heights? (Figure 4(c)) is much more focused than the
word cloud for all of the words from the questions
with that label (Figure 4(b)).
4 Buzzes Reveal Useful Features
If we restrict ourselves to a finite set of labels, the
process of answering questions is a multiclass clas-
sification problem. In this section, we show that in-
formation gleaned from humans making a similar de-
cision can help improve rapacious machine learning
classification. This validates that our crowdsourcing
technique is gathering useful information.
We used a state-of-the-art maximum entropy clas-
sification model, MEGAM (Daume? III, 2004), that
accepts a per-class mean prior for feature weights
and applied MEGAM to the 200 most frequent labels
(11,663 questions, a third of the dataset). The prior
mean of the feature weight is a convenient, simple
way to incorporate human feature utility; apart from
the mean, all default options are used.
Specifying the prior requires us to specify a weight
for each pair of label and feature. The weight com-
bines buzz information (described in Section 3) and
tf-idf (Salton, 1968). The tf-idf value is computed by
treating the training set of questions with the same
label as a single document.
Buzzes and tf-idf information were combined into
the prior ? for label a and feature f as ?a,f =
[
?b(a, f) + ?I [b(a, f) > 0] + ?
]
tf-idf(a, f). (1)
We describe our weight strategies in increasing order
of human knowledge. If ?, ?, and ? are zero, this
is a na??ve zero prior. If ? only is nonzero, this is a
linear transformation of features? tf-idf. If only ?
is nonzero, this is a linear transformation of buzzed
Weighting ? ? ? Error
zero - - - 0.37
tf-idf - - 3.5 0.14
buzz-binary 7.1 - - 0.10
buzz-linear - 1.5 - 0.16
buzz-tier - 1.1 0.1 0.09
Table 1: Classification error of a rapacious classifier able
to draw on human incremental classification. The best
weighting scheme for each dataset is in bold. Missing
parameter values (-) mean that the parameter is fixed to
zero for that weighting scheme.
words? tf-idf weights. If only ? is non-zero, num-
ber of buzzes is now a linear multiplier of the tf-idf
weight (buzz-linear). Finally we allow unbuzzed
words to have a separate linear transformation if both
? and ? are non-zero (buzz-tier).
Grid search (width of 0.1) on development set error
was used to set parameters. Table 1 shows test error
for weighting schemes and demonstrates that adding
human information as a prior improves classification
error, leading to a 36% error reduction over tf-idf
alone. While not directly comparable (this classifier
is rapacious, not incremental, and has a predefined
answer space), the average user had an error rate of
16.7%.
5 Building an Incremental Classifier
In the previous section we improved rapacious classi-
fication using humans? incremental classification. A
more interesting problem is how to compete against
humans in incremental classification. While in the
previous section we used human data for a training
set, here we use human data as an evaluation set.
Doing so requires us to formulate an incremental rep-
resentation of the contents of questions and to learn
a strategy to decide when to buzz.
1293
Because this is the first machine learning algo-
rithm for quiz bowl, we attempt to provide reason-
able rapacious baselines and compare against our new
strategies. We believe that our attempts represent a
reasonable explanation of the problem space, but ad-
ditional improvements could improve performance,
as discussed in Section 8.
A common way to represent state-dependent strate-
gies is via a Markov decision process (MDP). The
most salient component of a MDP is the policy, i.e., a
mapping from the state space to an action. In our con-
text, a state is a sequence of (thus far revealed) tokens,
and the action is whether to buzz or not. To learn a
policy, we use a standard reinforcement learning tech-
nique (Langford and Zadrozny, 2005; Abbeel and
Ng, 2004; Syed et al 2008): given a representation
of the state space, learn a classifier that can map from
a state to an action. This is also a common paradigm
for other incremental tasks, e.g., shift-reduce pars-
ing (Nivre, 2008).
Given examples of the correct answer given a con-
figuration of the state space, we can learn a MDP
without explicitly representing the reward function.
In this section, we define our method of defining
actions and our representation of the state space.
5.1 Action Space
We assume that there are only two possible actions:
buzz now or wait. An alternative would be a more
expressive action space (e.g., an action for every pos-
sible answer). However, this conflates the question
of when to buzz with what to answer. Instead, we call
the distinct component that provides what to answer
the content model. We describe an initial content
model in Section 5.2, below, and improve the models
further in Section 7. For the moment, assume that
a content model maintains a posterior distribution
over labels and when needed can provide its best
guess (e.g., given the features seen, the best answer
is ?William Jennings Bryan?).
Given the action space, we need to specify where
examples of state space and action come from. In
the language of classification, we need to provide
(x, y) pairs to learn a mapping x 7? y. The clas-
sifier attempts to learn that action y is (?buzz?) in
all states where the content model gave a correct re-
sponse given state x. Negative examples (?wait?)
are applied to states where the content model gave
a wrong answer. Every token in our training set cor-
responds to a classification example; both states are
prevalent enough that we do not to explicitly need to
address class imbalance. This resembles approaches
that merge different classifiers (Riedel et al 2011) or
attempt to estimate confidence of models (Blatz et al
2004). However, here we use partial observations.
This is a simplification of the problem and corre-
sponds to a strategy of ?buzz as soon as you know the
answer?, ignoring all other factors. While reasonable,
this is not always optimal. For example, if you know
your opponent is unlikely to answer a question, it is
better to wait until you are more confident. Incorrect
answers might also help your opponent, e.g., by elim-
inating an incorrect answer. Moreover, strategies in a
game setting (rather than a single question) are more
complicated. For example, if a right answer is worth
+10 points and the penalty for an incorrect question
is ?5, then a team leading by 15 points on the last
question should never attempt to answer. Investigat-
ing such gameplay strategies would require a ?roll
out? of game states (Tesauro and Galperin, 1996) to
explore the efficacy of such strategies. While inter-
esting, we leave these issues to future work.
We also investigated learning a policy directly
from users? buzzes directly (Abbeel and Ng, 2004),
but this performed poorly because the content model
is incompatible with the players? abilities and the
high variation in players? ability and styles (compare
Figure 2).
5.2 State Space
Recall that our goal is to learn a classifier that maps
states to actions; above, we defined the action space
(the classifier?s output) but not the state space, the
classifier?s input. The straightforward parameteriza-
tion of the state space would be all of the words that
have been revealed. However, such a feature set is
very sparse.
We use three components to form the state space:
what information has been observed, what the content
model believes is the correct answer, how confident
the content model is, and whether the content model?s
confidence is changing. We describe each in more
detail below.
Text In addition to the obvious, sparse parameter-
ization that contains all of the features thus far ob-
1294
served, we also include the total number of tokens
revealed and whether the phrase ?for ten points? has
appeared.6
Guess An additional feature that we used to repre-
sent the state space is the current guess of the content
model; i.e., the argmax of the posterior.
Posterior The posterior feature (Pos for short) cap-
tures the shape of the posterior distribution: the prob-
ability of the current guess (the max of the poste-
rior), the difference between the top two probabilities
and the probabilities associated with the fifteen most
probable labels under the posterior.
Change As features are revealed, there is often
a rapid transition from a state of confusion?when
there are many candidates with no clear best choice?
to a state of clarity with the posterior pointing to only
one probable label. To capture when this happens,
we add a binary feature to reflect when the best guess
has changed when a single feature has been revealed.
Other Features We thought that other features
would be useful. While useful on their own, no
features that we tried were useful when the content
model?s posterior was also used as a feature. Fea-
tures that we attempted to use were: a logistic re-
gression model attempting to capture the probability
that any player would answer (Silver et al 2008), a
regression predicting how many individuals would
buzz in the next n words, the year the question was
written, the category of the question, etc.
5.3 Na??ve Content Model
The action space is only deciding when to answer,
having abdicated responsibility for what to answer.
So where does do the answers come from? We as-
sume that at any point we can ask ?what is the highest
probability label given my current feature observa-
tions?? We call the component of our model that
answers this question the content model.
Our first content model is a na??ve Bayes
model (Lewis, 1998) trained over a text collection.
This generative model assumes labels for questions
come from a multinomial distribution ? ? Dir(?)
6The phrase ?for ten points? (abbreviated FTP) appears in
all quiz bowl questions to signal the question?s last sentence or
clause. It is a signal to answer soon, as the final ?giveaway? clue
is next.
and assumes that label l has a word distribution
?l ? Dir(?). Each question n has a label zn and
its words are generated from ?zn . Given labeled ob-
servations, we use the maximum a posteriori (MAP)
estimate of ?l.
Why use a generative model when a discriminative
classifier could use a richer feature space? The most
important reason is that, by definition, it makes sense
to ask a generative model the probability of a label
given a partial observation; such a question is not
well-formed for discriminative models, which expect
a complete feature set. Another important consid-
eration is that generative models can predict future,
unrevealed features (Chai et al 2004); however, we
do not make use of that capability here.
In addition to providing our answers, the content
model also provides an additional, critically impor-
tant feature for our state space: its posterior (pos
for short) probability. With every revealed feature,
the content model updates its posterior distribution
over labels given that t tokens have been revealed in
question n,
p(zn |w1 . . . wt, ?,?). (2)
To train our na??ve Bayes model, we semi-
automatically associate labels with a Wikipedia page
(correcting mistakes manually) and then form the
MAP estimate of the class multinomial distribution
from the Wikipedia page?s text. We did this for the
1065 labels that had at least three human answers,
excluding ambiguous labels associated with multiple
concepts (e.g., ?europa?, ?steppenwolf?, ?georgia?,
?paris?, and ?v?).
Features were taken to be the 25,000 most frequent
tokens and bigrams7 that were not stop words; fea-
tures were extracted from the Wikipedia text in the
same manner as from the question tokens.8
After demonstrating our ability to learn an incre-
mental classifier using this simple content model, we
extend the content model to capture local context and
correlations between similar labels in Section 7.
7We used NLTK (Loper and Bird, 2002) to filter stop words
and we used a ?2 test to identify bigrams with that rejected the
null hypothesis at the 0.01 level.
8The Dirichlet scaling parameter ? was set to 10,000 given
our relatively large vocabulary (25,000) and to not penalize a
label?s posterior probability if there were unseen features; this
corresponds to a pseudocount of 0.4. ? was set to 1.0.
1295
6 Pitting the Algorithm Against Humans
With a state space and a policy, we now have all the
necessary ingredients to have our algorithm compete
against humans. Classification, which allows us to
learn a policy, was done using the default settings of
LIBLINEAR (Fan et al 2008). To determine where
the algorithm buzzes, we provide a sequence of state
spaces until the policy classifier determines that it is
time to buzz.
We simulate competition by taking the human an-
swers and buzzes as a given and ask our algorithm
(independently) to provide its decision on when to
buzz on a test set. We compare the two buzz positions.
If the algorithm buzzed earlier with the right answer,
we consider it to have ?won? the question; equiva-
lently, if the algorithm buzzed later, we consider it to
have ?lost? that question. Ties are rare (less than 1%
of cases), as the algorithm had significantly different
behavior from humans; in the case where there was a
tie, ties were broken in favor of the machine.
Because we have a large, diverse population an-
swering questions, we need aggregate measures of
human performance to get a comprehensive view of
algorithm performance. We use the following metrics
for each question in the test set:
? best: the earliest anyone buzzed correctly
? median: the first buzz after 50% of human buzzes
? mean: for each recorded buzz compute a reward and
we average over all rewards
We compare the algorithm against baseline strategies:
? rap The rapacious strategy waits until the end of the
question and answers the best answer possible.
? ftp Waiting until when ?for 10 points? is said, then
giving the best answer possible.
? indexn Waiting until the first feature after the nth to-
ken has been processed, then giving the best answer
possible. The indices were chosen as the quartiles
for question length (by convention, most questions
are of similar length).
We compare these baselines against policies that de-
cide when to buzz based on the state.
Recall that the non-oracle algorithms were un-
aware of the true reward function. To best simulate
conventional quiz bowl settings, a correct answer
was +10 and the incorrect answer was ?5. The full
payoff matrix for the computer is shown in Table 2.
Cases where the opponent buzzes first but is wrong
are equivalent to rapacious classification, as there is
no longer any incentive to answer early. Thus we
exclude such situations (Outcomes 3, 5, 6 in Table 2)
from the dataset to focus on the challenge of process-
ing clues incrementally.
Computer Human Payoff
1 first and wrong right ?15
2 ? first and correct ?10
3 first and wrong wrong ?5
4 first and correct ? +10
5 wrong first and wrong +5
6 right first and wrong +15
Table 2: Payoff matrix (from the computer?s perspective)
for when agents ?buzz? during a question. To focus on
incremental classification, we exclude instances where the
human interrupts with an incorrect answer, as after an
opponent eliminates themselves, the answering reduces to
rapacious classification.
Table 3 shows the algorithm did much better when
it had access to the posterior. While incremental
algorithms outperform rapacious baselines, they lose
to humans. Against the median and average players,
they lose between three and four points per question,
and nearly twice that against the best players.
Although the content model is simple, this poor
performance is not from the content model never
producing the correct answer. To see this, we also
computed the optimal actions that could be executed.
We called this strategy the oracle strategy; it was able
to consistently win against its opponents. Thus, while
the content model was able to come up with correct
answers often enough to on average win against oppo-
nents (even the best human players), we were unable
to consistently learn winning policies.
There are two ways to solve this problem: create
deeper, more nuanced policies (or the features that
feed into them) or refine content models that provide
the signal needed for our policies to make sound
decisions. We chose to refine the content model, as
we felt we had added all of the obvious features for
learning effective policies.
7 Expanding the Content Model
When we asked quiz bowlers how they answer ques-
tions, they said that they first determine the category
1296
Strategy Features Mean Best Median Index
Classify
text -8.72 -10.04 -6.50 40.36
+guess -5.71 -8.40 -3.95 66.02
+pos -4.13 -7.56 -2.70 67.97
+change -4.02 -7.41 -2.63 77.33
Oracle text 3.36 0.61 4.35 49.90
all -6.61 -9.03 -4.42 100.19
ftp -5.22 -8.62 -4.23 88.65
Rapacious index30 -7.89 -8.71 -6.41 32.23
Baseline index60 -5.16 -7.56 -3.71 61.90
index90 -5.02 -8.62 -3.50 87.13
Table 3: Performance of strategies against users. The
human scoring columns show the average points per ques-
tion (positive means winning on average, negative means
losing on average) that the algorithm would expect to ac-
cumulate per question versus each human amalgam metric.
The index column notes the average index of the token
when the strategy chose to buzz.
of a question, which substantially narrows the an-
swer space. Ideally, the content model should con-
duct the same calculus?if a question seems to be
about mathematics, all answers related with mathe-
matics should be more likely in the posterior. This
was consistent with our error analysis; many errors
were non-sensical (e.g., answering ?entropy? for ?Jo-
hannes Brahms?, when an answer such as ?Robert
Schumann?, another composer, would be better).
In addition, assuming independence between fea-
tures given a label causes us to ignore potentially
informative multiword expressions such as quota-
tions, titles, or dates. Adding a language model to
our content model allows us to capture some of these
phenomena.
To create a model that jointly models categories
and local context, we propose the following model:
1. Draw a distribution over labels ? ? Dir(?)
2. Draw a background distribution over words ?0 ?
Dir(?0~1)
(a) For each category c of questions, draw a distribution
over words ?c ? Dir(?1?0).
i. For each label l in category c, draw a distribu-
tion over words ?l,c ? Dir(?2?c)
A. For each type v, draw a bigram distribution
?l,c,v ? Dir(?3?l,c)
3. Draw a distribution over labels ? ? Dir(?).
4. For each question with category c and N words, draw
answer l ? Mult(?):
(a) Assume w0 ? START
(b) Draw wn ? Mult(?l,c,wn?1) for n ? {1 . . . N}
This creates a language model over categories, la-
bels, and observed words (we use ?words? loosely, as
bigrams replace some word pairs). By constructing
the word distributions using hierarchical distributions
based on domain and ngrams (a much simpler para-
metric version of more elaborate methods (Wood and
Teh, 2009)), we can share statistical strength across
related contexts. We assume that labels are (only)
associated with their majority category as seen in our
training data and that category assignments are ob-
served. All scaling parameters ? were set to 10,000,
? was 1.0, and the vocabulary was still 25,000.
We used the maximal seating assignment (Wallach,
2008) for propagating counts through the Dirichlet
hierarchy. Thus, if the word v appeared Bl,u,v times
in label l following a preceding word u, Sl,v times in
label l, Tc,v times in category c, andGv times in total,
we estimate the probability of a word v appearing
in label k, category t, and after word u as p(wn =
v | lab = l, cat = c, wn?1 = u;~?) =
Bl,u,v + ?3
Sl,v+?2
Tc,v+?1
Gv+?0/V
G?+?0
Tc,?+?2
Sl,?+?2
Bl,u,? + ?3
, (3)
where we use ? to represent marginalization, e.g.
Tc,? =
?
v? Tc,v? . As with na??ve Bayes, Bayes? rule
provides posterior label probabilities (Equation 2).
We compare the na??ve model with models that
capture more of the content in the text in Table 4;
these results also include intermediate models be-
tween na??ve Bayes and the full content model: ?cat?
(omit 2.a.i.A) and ?bigram? (omit 2.a). These models
perform much better than the na??ve Bayes models
seen in Table 3. They are about even against the
mean and median players and lose four points per
question against top players.
7.1 Qualitative Analysis
In this section, we explore what defects are prevent-
ing the model presented here from competing with
top players, exposing challenges in reinforcement
learning, interpreting pragmatic cues, and large data.
Three examples of failures of the model are in Fig-
ure 5. This model is the best performing model of
the previous section.
Too Slow The first example is a question on Mau-
rice Ravel, a French composer known for Bole?ro. The
question leads off with Ravel?s orchestral version of
1297
Strategy Model Mean Best Median Index
Classify
na??ve -4.02 -7.41 -2.63 77.33
cat -1.69 -5.22 0.12 67.97
bigram -3.80 -7.66 -2.51 78.69
bgrm+cat -0.86 -4.46 0.83 63.42
Oracle
naive 3.36 0.61 4.35 49.90
cat 4.48 1.64 5.47 47.88
bigram 3.58 0.87 4.61 49.34
bgrm+cat 4.67 1.99 5.74 46.49
Table 4: As in Table 3, performance of strategies against
users, but with enhanced content models. Modeling both
bigrams and label categories improves overall perfor-
mance.
Mussorgsky?s piano piece ?Pictures at an Exhibition?.
Based on that evidence, the algorithm considers ?Pic-
tures at an Exhibition? the most likely but does not
yet buzz. When it receives enough information to be
sure about the correct answer, over half of the players
had already buzzed. Correcting this problem would
require a more aggressive strategy, perhaps incorpo-
rating the identity of the opponent or estimating the
difficulty of the question.
Mislead by the Content Model The second ex-
ample is a question on Enrico Fermi, an Italian-
American physicist. The first clues are about mag-
netic fields near a Fermi surface, which causes the
content model to view ?magnetic field? as the most
likely answer. The question?s text, however, has
pragmatic cues ?this man? and ?this Italian? which
would have ruled out the abstract answer ?magnetic
field?. Correcting this would require a model that
jointly models content and bigrams (Hardisty et
al., 2010), has a coreference system as its content
model (Haghighi and Klein, 2007), or determines the
correct question type (Moldovan et al 2000).
Insufficient Data The third example is where our
approach had no chance. The question is a very diffi-
cult question about George Washington, America?s
first president. As a sign of its difficulty, only half
the players answered correctly, and only near the end
of the question. The question concerns lesser known
episodes from Washington?s life, including a mistress
caught in the elements. To the content model, of the
several hypotheses it considers, the closest match
it can find is ?Yasunari Kawabata?, who wrote the
novel Snow Country, whose plot matches some of
these keywords. To answer these types of question,
george washington
Tokens Revealed
0.0
0.2
0.4
0.6
0.8
0 10 20 30 40 50 60
maurice ravel
Tokens Revealed
0.0
0.2
0.4
0.6
0.8
1.0
0 10 20 30 40 50 60 70
enrico fermi
Tokens Revealed
0.0
0.2
0.4
0.6
0.8
1.0
0 20 40 60 80 100
charlemagne yasunari kawabata
generals
chill
mistress
language
magnetic field neutrinomagnetic field
magnetic
paradox
zero
this_man
this_italian
pictures
orchestrated
pictures at an exhibition
maurice ravel
this_french
composer
bolero
Prediction
answer
Observation
feature
Buzz
Posterior Opponent
Figure 5: Three questions where our algorithm performed
poorly. It gets ?Maurice Ravel? (top) right but only after
over half the humans had answered correctly (i.e., the
buzz?s hexagon appears when the cyan line is above 0.6);
on ?Enrico Fermi? (middle) it confuses the correct type
of answer (person vs. concept); on ?George Washington?
(bottom) it lacks information to answer correctly. Lines
represent the current estimate posterior probability of the
answer (red) and the proportion of opponents who have
answered the question correctly (cyan). The label of each
of the three questions is above each chart. Words are in
black with arrows and arrows, and the current argmax
answer is at the bottom of the graph in red. The buzz
location is the hexagon.
1298
the repository used to train the content model would
have to be orders of magnitude larger to be able to
link the disparate clues in the question to a consistent
target. The content model would also benefit from
weighting later (more informative) features higher.
7.2 Assumptions
We have made assumptions to solve a problem that
is subtly different that the game of quiz bowl that
a human would play. Some of these were simpli-
fying assumptions, such as our assumption that the
algorithm has a closed set of possible answers (Sec-
tion 5.3). Even with this advantage, the algorithm is
unable to compete with human players, who choose
answers from an unbounded set. On the other hand,
to focus on incremental classification, we idealized
our human opponents so that they never give incor-
rect answers (Section 6). This causes our estimates
of our performance to be lower than they would be
against real players.
8 Conclusion and Future Work
We make three contributions. First, we introduce a
new setting for exploring the problem of incremental
classification: trivia games. This problem is intrin-
sically interesting because of its varied topics and
competitive elements, has a great quantity of stan-
dardized, machine-readable data, and also has the
boon of being cheaply and easily annotated. We took
advantage of that ease and created a framework for
quickly and efficiently gathering examples of humans
doing incremental classification.
There are other potential uses for the dataset; the
progression of clues from obscure nuggets to could
help determine how ?known? a particular aspect of
an entity is (e.g., that William Jennings Bryant gave
the ?Cross of Gold? speech is better known his resig-
nation after the Lusitania sinking, Figure 1). Which
could be used in educational settings (Smith et al
2008) or summarization (Das and Martins, 2007).
The second contribution shows that humans? incre-
mental classification improves state-of-the-art rapa-
cious classification algorithms. While other frame-
works (Zaidan et al 2008) have been proposed to
incorporate user clues about features, the system de-
scribed here provides analogous features without the
need for explicit post-hoc reflection, has faster anno-
tation throughput, and is much cheaper.
The problem of answering quiz bowl questions is
itself a challenging task that combines issues from
language modeling, large data, coreference, and re-
inforcement learning. While we do not address all
of these problems, our third contribution is a sys-
tem that learns a policy in a MDP for incremental
classification even in very large state spaces; it can
successfully compete with skilled human players.
Incorporating richer content models is one of our
next steps. This would allow us to move beyond the
closed-set model and use a more general coreference
model (Haghighi and Klein, 2007) for identifying
answers and broader corpora for training. In addi-
tion, using larger corpora would allow us to have
more comprehensive doubly-hierarchical language
models (Wood and Teh, 2009). We are also inter-
ested in adding richer models of opponents to the
state space that would adaptively adjust strategies as
it learned more about the strengths and weaknesses
of its opponent (Waugh et al 2011).
Further afield, our presentation of sentences
closely resembles paradigms for cognitive experi-
ments in linguistics (Thibadeau et al 1982) but are
much cheaper to conduct. If online processing ef-
fects (Levy et al 2008; Levy, 2011) could be ob-
served in buzzing behavior; e.g., if a confusingly
worded phrase depresses buzzing probability, it could
help validate cognitively-inspired models of online
sentence processing.
Incremental classification is a natural problem,
both for humans and resource-limited machines.
While our data set is trivial (in a good sense), learn-
ing how humans process data and make decisions
in a cheap, easy crowdsourced application can help
us apply new algorithms to improve performance in
settings where features aren?t free, either because of
computational or annotation cost.
1299
Acknowledgments
We thank the many players who played our online
quiz bowl to provide our data (and hopefully had fun
doing so) and Carlo Angiuli, Arnav Moudgil, and
Jerry Vinokurov for providing access to quiz bowl
questions. This research was supported by NSF grant
#1018625. Jordan Boyd-Graber is also supported by
the Army Research Laboratory through ARL Cooper-
ative Agreement W911NF-09-2-0072. Any opinions,
findings, conclusions, or recommendations expressed
are the authors? and do not necessarily reflect those
of the sponsors.
References
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of International Conference of Machine
Learning.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confi-
dence estimation for machine translation. In Proceed-
ings of the Association for Computational Linguistics.
Mark Boddy and Thomas L. Dean. 1989. Solving time-
dependent planning problems. In International Joint
Conference on Artificial Intelligence, pages 979?984.
Morgan Kaufmann Publishers, August.
Nicolo` Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad
Shamir. 2011. Efficient learning with partially ob-
served attributes. Journal of Machine Learning Re-
search, 12:2857?2878.
Xiaoyong Chai, Lin Deng, Qiang Yang, and Charles X.
Ling. 2004. Test-cost sensitive naive bayes classi-
fication. In IEEE International Conference on Data
Mining.
Dipanjan Das and Andre Martins. 2007. A survey on
automatic text summarization. Engineering and Tech-
nology, 4:192?195.
Hal Daume? III. 2004. Notes on CG and LM-
BFGS optimization of logistic regression. Pa-
per available at http://pub.hal3.name/
?daume04cg-bfgs, implementation available at
http://hal3.name/megam/.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building Watson:
An Overview of the DeepQA Project. AI Magazine,
31(3).
Aria Haghighi and Dan Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the Association for Computational
Linguistics.
Eric Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Michael C. Horsch and David Poole. 1998. An anytime
algorithm for decision making under uncertainty. In
Proceedings of Uncertainty in Artificial Intelligence.
Ken Jennings. 2006. Brainiac: adventures in the curious,
competitive, compulsive world of trivia buffs. Villard.
Shihao Ji and Lawrence Carin. 2007. Cost-sensitive fea-
ture acquisition and classification. Pattern Recognition,
40:1474?1485, May.
Shyong K. Lam, David M. Pennock, Dan Cosley, and
Steve Lawrence. 2003. 1 billion pages = 1 million
dollars? mining the web to play ?who wants to be a mil-
lionaire??. In Proceedings of Uncertainty in Artificial
Intelligence.
John Langford and Bianca Zadrozny. 2005. Relating
reinforcement learning performance to classification
performance. In Proceedings of International Confer-
ence of Machine Learning.
Roger P. Levy, Florencia Reali, and Thomas L. Griffiths.
2008. Modeling the effects of memory on human on-
line sentence processing with particle filters. In Pro-
ceedings of Advances in Neural Information Processing
Systems.
Roger Levy. 2011. Integrating surprisal and uncertain-
input models in online sentence comprehension: formal
techniques and empirical results. In Proceedings of the
Association for Computational Linguistics.
David D. Lewis. 1998. Naive (Bayes) at forty: The inde-
pendence assumption in information retrieval. In Claire
Ne?dellec and Ce?line Rouveirol, editors, Proceedings
of European Conference of Machine Learning, number
1398.
Michael L. Littman, Greg A. Keim, and Noam Shazeer.
2002. A probabilistic approach to solving crossword
puzzles. Artif. Intell., 134(1-2):23?55, January.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching.
Prem Melville, Maytal Saar-Tsechansky, Foster Provost,
and Raymond J. Mooney. 2005. An expected utility
approach to active feature-value acquisition. In Inter-
national Conference on Data Mining, November.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
1300
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the Association for Computational Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34(4):513?553, December.
Jay Pujara, Hal Daume III, and Lise Getoor. 2011. Using
classifier cascades for scalable e-mail classification.
In Collaboration, Electronic Messaging, Anti-Abuse
and Spam Conference, ACM International Conference
Proceedings Series.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher D. Manning. 2011.
Model combination for event extraction in bionlp 2011.
In Proceedings of the BioNLP Workshop.
Afshin Rostamizadeh, Alekh Agarwal, and Peter L.
Bartlett. 2011. Learning with missing features. In
Proceedings of Uncertainty in Artificial Intelligence.
Maytal Saar-Tsechansky and Foster Provost. 2007. Han-
dling missing values when applying classification mod-
els. Journal of Machine Learning Research, 8:1623?
1657, December.
Gerard. Salton. 1968. Automatic Information Organiza-
tion and Retrieval. McGraw Hill Text.
Burr Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features
and instances. In Proceedings of Emperical Methods
in Natural Language Processing.
David Silver, Richard S. Sutton, and Martin Mu?ller. 2008.
Sample-based learning and search with permanent and
transient memories. In International Conference on
Machine Learning.
Noah A. Smith, Michael Heilman, and Rebecca Hwa.
2008. Question generation as a competitive under-
graduate course project. In Proceedings of the NSF
Workshop on the Question Generation Shared Task and
Evaluation Challenge.
Umar Syed, Michael Bowling, and Robert E. Schapire.
2008. Apprenticeship learning using linear program-
ming. In Proceedings of International Conference of
Machine Learning.
Gerald Tesauro and Gregory R. Galperin. 1996. On-line
policy improvement using monte-carlo search. In Pro-
ceedings of Advances in Neural Information Processing
Systems.
Robert Thibadeau, Marcel A. Just, and Patricia A. Carpen-
ter. 1982. A model of the time course and content of
reading. Cognitive Science, 6.
Hanna M Wallach. 2008. Structured Topic Models for
Language. Ph.D. thesis, University of Cambridge.
Lidan Wang, Donald Metzler, and Jimmy Lin. 2010.
Ranking Under Temporal Constraints. In Proceedings
of the ACM International Conference on Information
and Knowledge Management.
Kevin Waugh, Brian D. Ziebart, and J. Andrew Bagnell.
2011. Computational rationalization: The inverse equi-
librium problem. In Proceedings of International Con-
ference of Machine Learning.
F. Wood and Y. W. Teh. 2009. A hierarchical nonpara-
metric Bayesian approach to statistical language model
domain adaptation. In Proceedings of Artificial Intelli-
gence and Statistics.
Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2008.
Machine learning with annotator rationales to reduce
annotation cost. In Proceedings of the NIPS*2008
Workshop on Cost Sensitive Learning.
Valentina Bayer Zubek and Thomas G. Dietterich. 2002.
Pruning improves heuristic search for cost-sensitive
learning. In International Conference on Machine
Learning.
1301
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 633?644,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
A Neural Network for Factoid Question Answering over
Paragraphs
Mohit Iyyer
1
, Jordan Boyd-Graber
2
, Leonardo Claudino
1
,
Richard Socher
3
, Hal Daume? III
1
1
University of Maryland, Department of Computer Science and umiacs
2
University of Colorado, Department of Computer Science
3
Stanford University, Department of Computer Science
{miyyer,claudino,hal}@umiacs.umd.edu,
Jordan.Boyd.Graber@colorado.edu, richard@socher.org
Abstract
Text classification methods for tasks
like factoid question answering typi-
cally use manually defined string match-
ing rules or bag of words representa-
tions. These methods are ineffective
when question text contains very few
individual words (e.g., named entities)
that are indicative of the answer. We
introduce a recursive neural network
(rnn) model that can reason over such
input by modeling textual composition-
ality. We apply our model, qanta, to
a dataset of questions from a trivia
competition called quiz bowl. Unlike
previous rnn models, qanta learns
word and phrase-level representations
that combine across sentences to reason
about entities. The model outperforms
multiple baselines and, when combined
with information retrieval methods, ri-
vals the best human players.
1 Introduction
Deep neural networks have seen widespread
use in natural language processing tasks such
as parsing, language modeling, and sentiment
analysis (Bengio et al., 2003; Socher et al.,
2013a; Socher et al., 2013c). The vector spaces
learned by these models cluster words and
phrases together based on similarity. For exam-
ple, a neural network trained for a sentiment
analysis task such as restaurant review classifi-
cation might learn that ?tasty? and ?delicious?
should have similar representations since they
are synonymous adjectives.
These models have so far only seen success in
a limited range of text-based prediction tasks,
Later in its existence, this polity?s leader was chosen
by a group that included three bishops and six laymen,
up from the seven who traditionally made the decision.
Free imperial cities in this polity included Basel and
Speyer. Dissolved in 1806, its key events included the
Investiture Controversy and the Golden Bull of 1356.
Led by Charles V, Frederick Barbarossa, and Otto I,
for 10 points, name this polity, which ruled most of
what is now Germany through the Middle Ages and
rarely ruled its titular city.
Figure 1: An example quiz bowl question about
the Holy Roman Empire. The first sentence
contains no words or named entities that by
themselves are indicative of the answer, while
subsequent sentences contain more and more
obvious clues.
where inputs are typically a single sentence and
outputs are either continuous or a limited dis-
crete set. Neural networks have not yet shown
to be useful for tasks that require mapping
paragraph-length inputs to rich output spaces.
Consider factoid question answering: given
a description of an entity, identify the per-
son, place, or thing discussed. We describe a
task with high-quality mappings from natural
language text to entities in Section 2. This
task?quiz bowl?is a challenging natural lan-
guage problem with large amounts of diverse
and compositional data.
To answer quiz bowl questions, we develop
a dependency tree recursive neural network
in Section 3 and extend it to combine predic-
tions across sentences to produce a question
answering neural network with trans-sentential
averaging (qanta). We evaluate our model
against strong computer and human baselines
in Section 4 and conclude by examining the
latent space and model mistakes.
633
2 Matching Text to Entities: Quiz
Bowl
Every weekend, hundreds of high school and
college students play a game where they map
raw text to well-known entities. This is a trivia
competition called quiz bowl. Quiz bowl ques-
tions consist of four to six sentences and are
associated with factoid answers (e.g., history
questions ask players to identify specific battles,
presidents, or events). Every sentence in a quiz
bowl question is guaranteed to contain clues
that uniquely identify its answer, even without
the context of previous sentences. Players an-
swer at any time?ideally more quickly than
the opponent?and are rewarded for correct
answers.
Automatic approaches to quiz bowl based on
existing nlp techniques are doomed to failure.
Quiz bowl questions have a property called
pyramidality, which means that sentences early
in a question contain harder, more obscure
clues, while later sentences are ?giveaways?.
This design rewards players with deep knowl-
edge of a particular subject and thwarts bag
of words methods. Sometimes the first sen-
tence contains no named entities?answering
the question correctly requires an actual un-
derstanding of the sentence (Figure 1). Later
sentences, however, progressively reveal more
well-known and uniquely identifying terms.
Previous work answers quiz bowl ques-
tions using a bag of words (na??ve Bayes) ap-
proach (Boyd-Graber et al., 2012). These mod-
els fail on sentences like the first one in Figure 1,
a typical hard, initial clue. Recursive neural
networks (rnns), in contrast to simpler models,
can capture the compositional aspect of such
sentences (Hermann et al., 2013).
rnns require many redundant training exam-
ples to learn meaningful representations, which
in the quiz bowl setting means we need multiple
questions about the same answer. Fortunately,
hundreds of questions are produced during the
school year for quiz bowl competitions, yield-
ing many different examples of questions ask-
ing about any entity of note (see Section 4.1
for more details). Thus, we have built-in re-
dundancy (the number of ?askable? entities is
limited), but also built-in diversity, as difficult
clues cannot appear in every question without
becoming well-known.
3 Dependency-Tree Recursive
Neural Networks
To compute distributed representations for the
individual sentences within quiz bowl ques-
tions, we use a dependency-tree rnn (dt-rnn).
These representations are then aggregated and
fed into a multinomial logistic regression clas-
sifier, where class labels are the answers asso-
ciated with each question instance.
In previous work, Socher et al. (2014) use
dt-rnns to map text descriptions to images.
dt-rnns are robust to similar sentences with
slightly different syntax, which is ideal for our
problem since answers are often described by
many sentences that are similar in meaning
but different in structure. Our model improves
upon the existing dt-rnn model by jointly
learning answer and question representations
in the same vector space rather than learning
them separately.
3.1 Model Description
As in other rnn models, we begin by associ-
ating each word w in our vocabulary with a
vector representation x
w
? R
d
. These vectors
are stored as the columns of a d ? V dimen-
sional word embedding matrix W
e
, where V is
the size of the vocabulary. Our model takes
dependency parse trees of question sentences
(De Marneffe et al., 2006) and their correspond-
ing answers as input.
Each node n in the parse tree for a partic-
ular sentence is associated with a word w, a
word vector x
w
, and a hidden vector h
n
? R
d
of the same dimension as the word vectors. For
internal nodes, this vector is a phrase-level rep-
resentation, while at leaf nodes it is the word
vector x
w
mapped into the hidden space. Un-
like in constituency trees where all words reside
at the leaf level, internal nodes of dependency
trees are associated with words. Thus, the dt-
rnn has to combine the current node?s word
vector with its children?s hidden vectors to form
h
n
. This process continues recursively up to
the root, which represents the entire sentence.
We associate a separate d?d matrix W
r
with
each dependency relation r in our dataset and
learn these matrices during training.
1
Syntac-
tically untying these matrices improves com-
1
We had 46 unique dependency relations in our quiz
bowl dataset.
634
This city ?s economy depended on subjugated peasants called helots
ROOT
DET
POSSESSIVE
POSS
NSUBJ
PREP
POBJ
AMOD
VMOD
DOBJ
Figure 2: Dependency parse of a sentence from a question about Sparta.
positionality over the standard rnn model by
taking into account relation identity along with
tree structure. We include an additional d? d
matrix, W
v
, to incorporate the word vector x
w
at a node into the node vector h
n
.
Given a parse tree (Figure 2), we first com-
pute leaf representations. For example, the
hidden representation h
helots
is
h
helots
= f(W
v
? x
helots
+ b), (1)
where f is a non-linear activation function such
as tanh and b is a bias term. Once all leaves
are finished, we move to interior nodes with
already processed children. Continuing from
?helots? to its parent, ?called?, we compute
h
called
=f(W
DOBJ
? h
helots
+W
v
? x
called
+ b). (2)
We repeat this process up to the root, which is
h
depended
=f(W
NSUBJ
? h
economy
+W
PREP
? h
on
+W
v
? x
depended
+ b). (3)
The composition equation for any node n with
children K(n) and word vector x
w
is h
n
=
f(W
v
? x
w
+ b+
?
k?K(n)
W
R(n,k)
? h
k
), (4)
where R(n, k) is the dependency relation be-
tween node n and child node k.
3.2 Training
Our goal is to map questions to their corre-
sponding answer entities. Because there are
a limited number of possible answers, we can
view this as a multi-class classification task.
While a softmax layer over every node in the
tree could predict answers (Socher et al., 2011;
Iyyer et al., 2014), this method overlooks that
most answers are themselves words (features)
in other questions (e.g., a question on World
War II might mention the Battle of the Bulge
and vice versa). Thus, word vectors associated
with such answers can be trained in the same
vector space as question text,
2
enabling us to
model relationships between answers instead
of assuming incorrectly that all answers are
independent.
To take advantage of this observation, we
depart from Socher et al. (2014) by training
both the answers and questions jointly in a
single model, rather than training each sep-
arately and holding embeddings fixed during
dt-rnn training. This method cannot be ap-
plied to the multimodal text-to-image mapping
problem because text captions by definition are
made up of words and thus cannot include im-
ages; in our case, however, question text can
and frequently does include answer text.
Intuitively, we want to encourage the vectors
of question sentences to be near their correct
answers and far away from incorrect answers.
We accomplish this goal by using a contrastive
max-margin objective function described be-
low. While we are not interested in obtaining a
ranked list of answers,
3
we observe better per-
formance by adding the weighted approximate-
rank pairwise (warp) loss proposed in Weston
et al. (2011) to our objective function.
Given a sentence paired with its correct an-
swer c, we randomly select j incorrect answers
from the set of all incorrect answers and denote
this subset as Z. Since c is part of the vocab-
ulary, it has a vector x
c
? W
e
. An incorrect
answer z ? Z is also associated with a vector
x
z
?W
e
. We define S to be the set of all nodes
in the sentence?s dependency tree, where an
individual node s ? S is associated with the
2
Of course, questions never contain their own answer
as part of the text.
3
In quiz bowl, all wrong guesses are equally detri-
mental to a team?s score, no matter how ?close? a guess
is to the correct answer.
635
hidden vector h
s
. The error for the sentence is
C(S, ?) =
?
s?S
?
z?Z
L(rank(c, s, Z))max(0,
1? x
c
? h
s
+ x
z
? h
s
), (5)
where the function rank(c, s, Z) provides the
rank of correct answer c with respect to the
incorrect answers Z. We transform this rank
into a loss function
4
shown by Usunier et al.
(2009) to optimize the top of the ranked list,
L(r) =
r
?
i=1
1/i.
Since rank(c, s, Z) is expensive to compute,
we approximate it by randomly sampling K
incorrect answers until a violation is observed
(x
c
? h
s
< 1 + x
z
? h
s
) and set rank(c, s, Z) =
(|Z|?1)/K, as in previous work (Weston et al.,
2011; Hermann et al., 2014). The model mini-
mizes the sum of the error over all sentences T
normalized by the number of nodes N in the
training set,
J(?) =
1
N
?
t?T
C(t, ?). (6)
The parameters ? = (W
r?R
,W
v
,W
e
, b), where
R represents all dependency relations in the
data, are optimized using AdaGrad(Duchi et
al., 2011).
5
In Section 4 we compare perfor-
mance to an identical model (fixed-qanta)
that excludes answer vectors from W
e
and show
that training them as part of ? produces signif-
icantly better results.
The gradient of the objective function,
?C
??
=
1
N
?
t?T
?J(t)
??
, (7)
is computed using backpropagation through
structure (Goller and Kuchler, 1996).
3.3 From Sentences to Questions
The model we have just described considers
each sentence in a quiz bowl question indepen-
dently. However, previously-heard sentences
within the same question contain useful infor-
mation that we do not want our model to ignore.
4
Our experiments show that adding this loss term to
the objective function not only increases performance
but also speeds up convergence
5
We set the initial learning rate ? = 0.05 and reset
the squared gradient sum to zero every five epochs.
While past work on rnn models have been re-
stricted to the sentential and sub-sentential
levels, we show that sentence-level representa-
tions can be easily combined to generate useful
representations at the larger paragraph level.
The simplest and best
6
aggregation method
is just to average the representations of each
sentence seen so far in a particular question.
As we show in Section 4, this method is very
powerful and performs better than most of our
baselines. We call this averaged dt-rnn model
qanta: a question answering neural network
with trans-sentential averaging.
4 Experiments
We compare the performance of qanta against
multiple strong baselines on two datasets.
qanta outperforms all baselines trained only
on question text and improves an information
retrieval model trained on all of Wikipedia.
qanta requires that an input sentence de-
scribes an entity without mentioning that
entity, a constraint that is not followed by
Wikipedia sentences.
7
While ir methods can
operate over Wikipedia text with no issues,
we show that the representations learned by
qanta over just a dataset of question-answer
pairs can significantly improve the performance
of ir systems.
4.1 Datasets
We evaluate our algorithms on a corpus of over
100,000 question/answer pairs from two differ-
ent sources. First, we expand the dataset used
in Boyd-Graber et al. (2012) with publically-
available questions from quiz bowl tournaments
held after that work was published. This gives
us 46,842 questions in fourteen different cate-
gories. To this dataset we add 65,212 questions
from naqt, an organization that runs quiz
bowl tournaments and generously shared with
us all of their questions from 1998?2013.
6
We experimented with weighting earlier sentences
less than later ones in the average as well as learning an
additional RNN on top of the sentence-level representa-
tions. In the former case, we observed no improvements
over a uniform average, while in the latter case the
model overfit even with strong regularization.
7
We tried transforming Wikipedia sentences into
quiz bowl sentences by replacing answer mentions with
appropriate descriptors (e.g., ?Joseph Heller? with ?this
author?), but the resulting sentences suffered from a
variety of grammatical issues and did not help the final
result.
636
Because some categories contain substan-
tially fewer questions than others (e.g., astron-
omy has only 331 questions), we consider only
literature and history questions, as these two
categories account for more than 40% of the
corpus. This leaves us with 21,041 history ques-
tions and 22,956 literature questions.
4.1.1 Data Preparation
To make this problem feasible, we only consider
a limited set of the most popular quiz bowl an-
swers. Before we filter out uncommon answers,
we first need to map all raw answer strings to
a canonical set to get around formatting and
redundancy issues. Most quiz bowl answers are
written to provide as much information about
the entity as possible. For example, the follow-
ing is the raw answer text of a question on the
Chinese leader Sun Yat-sen: Sun Yat-sen; or
Sun Yixian; or Sun Wen; or Sun Deming; or
Nakayama Sho; or Nagao Takano. Quiz bowl
writers vary in how many alternate acceptable
answers they provide, which makes it tricky to
strip superfluous information from the answers
using rule-based approaches.
Instead, we use Whoosh,
8
an information re-
trieval library, to generate features in an active
learning classifier that matches existing answer
strings to Wikipedia titles. If we are unable
to find a match with a high enough confidence
score, we throw the question out of our dataset.
After this standardization process and manual
vetting of the resulting output, we can use the
Wikipedia page titles as training labels for the
dt-rnn and baseline models.
9
65.6% of answers only occur once or twice
in the corpus. We filter out all answers that
do not occur at least six times, which leaves
us with 451 history answers and 595 literature
answers that occur on average twelve times
in the corpus. These pruning steps result in
4,460 usable history questions and 5,685 liter-
ature questions. While ideally we would have
used all answers, our model benefits from many
training examples per answer to learn mean-
ingful representations; this issue can possibly
be addressed with techniques from zero shot
learning (Palatucci et al., 2009; Pasupat and
Liang, 2014), which we leave to future work.
8
https://pypi.python.org/pypi/Whoosh/
9
Code and non-naqt data available at http://cs.
umd.edu/
~
miyyer/qblearn.
We apply basic named entity recogni-
tion (ner) by replacing all occurrences of
answers in the question text with single
entities (e.g., Ernest Hemingway becomes
Ernest Hemingway). While we experimented
with more advanced ner systems to detect
non-answer entities, they could not handle
multi-word named entities like the book Love
in the Time of Cholera (title case) or battle
names (e.g., Battle of Midway). A simple
search/replace on all answers in our corpus
works better for multi-word entities.
The preprocessed data are split into folds
by tournament. We choose the past two na-
tional tournaments
10
as our test set as well
as questions previously answered by players in
Boyd-Graber et al. (2012) and assign all other
questions to train and dev sets. History results
are reported on a training set of 3,761 ques-
tions with 14,217 sentences and a test set of
699 questions with 2,768 sentences. Literature
results are reported on a training set of 4,777
questions with 17,972 sentences and a test set
of 908 questions with 3,577 sentences.
Finally, we initialize the word embedding
matrix W
e
with word2vec (Mikolov et al., 2013)
trained on the preprocessed question text in
our training set.
11
We use the hierarchical skip-
gram model setting with a window size of five
words.
4.2 Baselines
We pit qanta against two types of baselines:
bag of words models, which enable comparison
to a standard NLP baseline, and information
retrieval models, which allow us to compare
against traditional question answering tech-
niques.
BOW The bow baseline is a logistic regres-
sion classifier trained on binary unigram indi-
cators.
12
This simple discriminative model is
an improvement over the generative quiz bowl
answering model of Boyd-Graber et al. (2012).
10
The tournaments were selected because naqt does
not reuse any questions or clues within these tourna-
ments.
11
Out-of-vocabulary words from the test set are ini-
tialized randomly.
12
Raw word counts, frequencies, and TF-IDF
weighted features did not increase performance, nor
did adding bigrams to the feature set (possibly because
multi-word named entities are already collapsed into
single words).
637
BOW-DT The bow-dt baseline is identical
to bow except we augment the feature set with
dependency relation indicators. We include
this baseline to isolate the effects of the depen-
dency tree structure from our compositional
model.
IR-QB The ir-qb baseline maps questions to
answers using the state-of-the-art Whoosh ir
engine. The knowledge base for ir-qb consists
of ?pages? associated with each answer, where
each page is the union of training question text
for that answer. Given a partial question, the
text is first preprocessed using a query lan-
guage similar to that of Apache Lucene. This
processed query is then matched to pages uses
bm-25 term weighting, and the top-ranked page
is considered to be the model?s guess. We also
incorporate fuzzy queries to catch misspellings
and plurals and use Whoosh?s built-in query ex-
pansion functionality to add related keywords
to our queries. IR-WIKI The ir-wiki model
is identical to the ir-qb model except that each
?page? in its knowledge base also includes all
text from the associated answer?s Wikipedia
article. Since all other baselines and dt-rnn
models operate only on the question text, this
is not a valid comparison, but we offer it to
show that we can improve even this strong
model using qanta.
4.3 DT-RNN Configurations
For all dt-rnn models the vector dimension d
and the number of wrong answers per node j
is set to 100. All model parameters other than
W
e
are randomly initialized. The non-linearity
f is the normalized tanh function,
13
f(v) =
tanh(v)
?tanh(v)?
. (8)
qanta is our dt-rnn model with feature
averaging across previously-seen sentences in a
question. To obtain the final answer prediction
given a partial question, we first generate a
feature representation for each sentence within
that partial question. This representation is
computed by concatenating together the word
embeddings and hidden representations aver-
aged over all nodes in the tree as well as the
13
The standard tanh function produced heavy sat-
uration at higher levels of the trees, and corrective
weighting as in Socher et al. (2014) hurt our model
because named entities that occur as leaves are often
more important than non-terminal phrases.
root node?s hidden vector. Finally, we send
the average of all of the individual sentence fea-
tures
14
as input to a logistic regression classifier
for answer prediction.
fixed-qanta uses the same dt-rnn configu-
ration as qanta except the answer vectors are
kept constant as in the text-to-image model.
4.4 Human Comparison
Previous work provides human answers (Boyd-
Graber et al., 2012) for quiz bowl questions.
We use human records for 1,201 history guesses
and 1,715 literature guesses from twenty-two of
the quiz bowl players who answered the most
questions.
15
The standard scoring system for quiz bowl is
10 points for a correct guess and -5 points for
an incorrect guess. We use this metric to com-
pute a total score for each human. To obtain
the corresponding score for our model, we force
it to imitate each human?s guessing policy. For
example, Figure 3 shows a human answering
in the middle of the second sentence. Since our
model only considers sentence-level increments,
we compare the model?s prediction after the
first sentence to the human prediction, which
means our model is privy to less information
than humans.
The resulting distributions are shown in Fig-
ure 4?our model does better than the average
player on history questions, tying or defeat-
ing sixteen of the twenty-two players, but it
does worse on literature questions, where it
only ties or defeats eight players. The figure
indicates that literature questions are harder
than history questions for our model, which is
corroborated by the experimental results dis-
cussed in the next section.
5 Discussion
In this section, we examine why qanta im-
proves over our baselines by giving examples
of questions that are incorrectly classified by
all baselines but correctly classified by qanta.
We also take a close look at some sentences that
all models fail to answer correctly. Finally, we
visualize the answer space learned by qanta.
14
Initial experiments with L
2
regularization hurt per-
formance on a validation set.
15
Participants were skilled quiz bowl players and are
not representative of the general population.
638
History Literature
Model Pos 1 Pos 2 Full Pos 1 Pos 2 Full
bow 27.5 51.3 53.1 19.3 43.4 46.7
bow-dt 35.4 57.7 60.2 24.4 51.8 55.7
ir-qb 37.5 65.9 71.4 27.4 54.0 61.9
fixed-qanta 38.3 64.4 66.2 28.9 57.7 62.3
qanta 47.1 72.1 73.7 36.4 68.2 69.1
ir-wiki 53.7 76.6 77.5 41.8 74.0 73.3
qanta+ir-wiki 59.8 81.8 82.3 44.7 78.7 76.6
Table 1: Accuracy for history and literature at the first two sentence positions of each question
and the full question. The top half of the table compares models trained on questions only, while
the IR models in the bottom half have access to Wikipedia. qanta outperforms all baselines
that are restricted to just the question data, and it substantially improves an IR model with
access to Wikipedia despite being trained on much less data.
200
150
100
50
0
50
100
150
200
Sco
re D
iffe
ren
ce
History: Model vs. Human
Model losesModel wins 400300
200
100
0
100
200
Sco
re D
iffe
ren
ce
Literature: Model vs. Human
Model losesModel wins
Figure 4: Comparisons of qanta+ir-wiki to human quiz bowl players. Each bar represents an
individual human, and the bar height corresponds to the difference between the model score and
the human score. Bars are ordered by human skill. Red bars indicate that the human is winning,
while blue bars indicate that the model is winning. qanta+ir-wiki outperforms most humans
on history questions but fails to defeat the ?average? human on literature questions.
A minor character in this play can be summoned
by a bell that does not always work; that character
also doesn?t have eyelids. Near the end, a woman
who drowned her illegitimate child attempts to stab
another woman in the Second Empire-style
3
room
in which the entire play takes place. For 10 points,
Estelle and Ines are characters in which existentialist
play in which Garcin claims ?Hell is other people?,
written by Jean-Paul Sartre?
Figure 3: A question on the play ?No Exit?
with human buzz position marked as
3
. Since
the buzz occurs in the middle of the second
sentence, our model is only allowed to see the
first sentence.
5.1 Experimental Results
Table 1 shows that when bag of words and
information retrieval methods are restricted to
question data, they perform significantly worse
than qanta on early sentence positions. The
performance of bow-dt indicates that while
the dependency tree structure helps by itself,
the compositional distributed representations
learned by qanta are more useful. The signif-
icant improvement when we train answers as
part of our vocabulary (see Section 3.2) indi-
cates that our model uses answer occurrences
within question text to learn a more informa-
tive vector space.
The disparity between ir-qb and ir-wiki
indicates that the information retrieval models
need lots of external data to work well at all
sentence positions. ir-wiki performs better
than other models because Wikipedia contains
many more sentences that partially match spe-
cific words or phrases found in early clues than
the question training set. In particular, it is
impossible for all other models to answer clues
in the test set that have no semantically similar
639
or equivalent analogues in the training ques-
tion data. With that said, ir methods can
also operate over data that does not follow the
special constraints of quiz bowl questions (e.g.,
every sentence uniquely identifies the answer,
answers don?t appear in their corresponding
questions), which qanta cannot handle. By
combining qanta and ir-wiki, we are able to
leverage access to huge knowledge bases along
with deep compositional representations, giv-
ing us the best of both worlds.
5.2 Where the Attribute Space Helps
Answer Questions
We look closely at the first sentence from a
literature question about the author Thomas
Mann: ?He left unfinished a novel whose title
character forges his father?s signature to get
out of school and avoids the draft by feigning
desire to join?.
All baselines, including ir-wiki, are unable
to predict the correct answer given only this
sentence. However, qanta makes the correct
prediction. The sentence contains no named
entities, which makes it almost impossible for
bag of words or string matching algorithms to
predict correctly. Figure 6 shows that the plot
description associated with the ?novel? node
is strongly indicative of the answer. The five
highest-scored answers are all male authors,
16
which shows that our model is able to learn the
answer type without any hand-crafted rules.
Our next example, the first sentence in Ta-
ble 2, is from the first position of a question
on John Quincy Adams, which is correctly an-
swered by only qanta. The bag of words
model guesses Henry Clay, who was also a Sec-
retary of State in the nineteenth century and
helped John Quincy Adams get elected to the
presidency in a ?corrupt bargain?. However,
the model can reason that while Henry Clay
was active at the same time and involved in
the same political problems of the era, he did
not represent the Amistad slaves, nor did he
negotiate the Treaty of Ghent.
5.3 Where all Models Struggle
Quiz bowl questions are intentionally written to
make players work to get the answer, especially
at early sentence positions. Our model fails to
16
three of whom who also have well-known unfinished
novels
answer correctly more than half the time after
hearing only the first sentence. We examine
some examples to see if there are any patterns
to what makes a question ?hard? for machine
learning models.
Consider this question about the Italian ex-
plorer John Cabot: ?As a young man, this
native of Genoa disguised himself as a Muslim
to make a pilgrimage to Mecca?.
While it is obvious to human readers that
the man described in this sentence is not actu-
ally a Muslim, qanta has to accurately model
the verb disguised to make that inference. We
show the score plot of this sentence in Figure 7.
The model, after presumably seeing many in-
stances of muslim and mecca associated with
Mughal emperors, is unable to prevent this
information from propagating up to the root
node. On the bright side, our model is able to
learn that the question is expecting a human
answer rather than non-human entities like the
Umayyad Caliphate.
More examples of impressive answers by
qanta as well as incorrect guesses by all sys-
tems are shown in Table 2.
5.4 Examining the Attribute Space
Figure 5 shows a t-SNE visualization (Van der
Maaten and Hinton, 2008) of the 451 answers
in our history dataset. The vector space is
divided into six general clusters, and we focus
in particular on the us presidents. Zooming
in on this section reveals temporal clustering:
presidents who were in office during the same
timeframe occur closer together. This observa-
tion shows that qanta is capable of learning
attributes of entities during training.
6 Related Work
There are two threads of related work relevant
to this paper. First, we discuss previous ap-
plications of compositional vector models to
related NLP tasks. Then, we examine existing
work on factoid question-answering and review
the similarities and differences between these
tasks and the game of quiz bowl.
6.1 Recursive Neural Networks for
NLP
The principle of semantic composition states
that the meaning of a phrase can be derived
640
TSNE-1TSNE-2 Wars, rebellions, and battlesU.S. presidentsPrime ministersExplorers & emperorsPoliciesOthertammany_hall calvin_coolidgelollardy fourth_crusadesonghai_empire peace_of_westphaliainca_empireatahualpa charles_sumnerjohn_paul_jones wounded_knee_massacrehuldrych_zwingli darius_ibattle_of_ayacuchojohn_cabotghana ulysses_s._grant hartford_conventioncivilian_conservation_corpsroger_williams_(theologian)george_h._pendleton william_mckinleyvictoria_woodhullcredit_mobilier_of_america_scandal henry_cabot_lodge,_jr. mughal_empire john_marshallcultural_revolutionguadalcanallouisiana_purchasenight_of_the_long_kniveschandragupta_mauryasamuel_de_champlainthirty_years'_war compromise_of_1850battle_of_hastingsbattle_of_salamisakbar lewis_cassdawes_plan hernando_de_soto carthage joseph_mccarthymainesalvador_allende battle_of_gettysburgmikhail_gorbachev aaron_burrequal_rights_amendmentwar_of_the_spanish_successioncoxey's_army george_meadefourteen_pointsmapp_v._ohio sam_houston ming_ ynastyboxer_rebellionanti-masonic_partyporfirio_diaz treaty_of_portsmouththebes,_greece golden_hordefrancisco_i._madero hittitesjames_g._blaineschenck_v._united_states caligulawilliam_walker_(filibuster)henry_vii_of_ nglandkonrad_adenauerkellogg-briand_pact battle_of_cullodentreaty_of_brest-litovsk william_p nna._philip_randolphh nry_l._stimsonwhig_party_(united_states)caroline_affair clarence_darrowwhiskey_rebellionbattle_of_midwaybattle_of_lepantoadolf_eichmanngeorges_clemenceau battle_of_the_little_bighornpontiac_(person) black_hawk_warbattle_of_tannenbergclayton_antitrust_actprovisions_of_oxford battle_of_actiumsuez_crisis spartacusdorr_rebellion jay_treatytriangle_shirtwaist_factory_fire kamakura_shogunatejulius_nyerere frederick_douglasspierre_trudeaunagasaki suleiman_the_magnificentfalklands_war war_of_devolutioncharlemagnedaniel_boone edict_of_nantesharry_s._trumanshakapedro_alvares_cabralthomas_hart_benton_(politician)battle_of_the_coral_sea peterloo_massacrebattle_of_bosworth_fieldroger_b._taneybernardo_o'higginsneville_chamberlainhenry_hudson cyrus_the_great jane_addamsrough_ridersjames_a._garfieldnapoleon_iii missouri_compromisebattle_of_leyte_gulfambrose_burnsidetrent_affairmaria_theresawilliam_ewart_gladstone walter_mondalebarry_goldw terlouis_rielhideki_tojo marco_polobrian_mulroney truman_doctrineroald_amundsen tokugawa_shogunateeleanor_of_aquitaine louis_brandeisbattle_of_trentonkhmer_empirebenito_juarez battle_of_antietamwhiskey_ring otto_von_bismarckbook r_t._washingtonbattle_of_bannockburneugene_v._debs erie_canaljameson_raid green_mountain_boyshaymarket_affairfinlandfashoda_incident battle_of_shilohhannibal john_jayeaster_rising jamaicabrook_farm umayyad_caliph temuhammadfrancis_drakeclara_barton shays'_rebellion verdunhadrianvyacheslav_molotov oda_nobunagacanossasamuel_gompers battle_of_bunker_hi lbattle_of_plasseydavid_livingstonesol npericles tang_dynastyteutonic_knightssecond_vatican_councilalfred_dreyfushenry_the_navigatornelson_mandelapeasants'_revolt gaius_mariusgetulio_vargas horatio_gatesjohn_t._scopes league_of_nationsfirst_battle_of_bull_runalfred_the_greatleonid_brezhn cherokee long_marchemiliano_zapata james_monroewoodrow_wilsonvandals william_henry_harrisonbattle_of_puebla battle_of_zamajustinian_i thaddeus_stevenscecil_rhodeskwame_nkrumah diet_of_wormsgeorge_armstrong_custerbattle_of_agincourtseminole_wars shah_jahanamerigo_vespucci john_foster_dulleslester_b._pearson oregon_trail claudiuslateran_treatychester_a._arthuropium_wars treaty_of_utrechtknights_of_labor alexander_hamiltonplessy_v._ferguson hora e_greeleymary baker_eddyalex nder_kerensky jacquerie treaty_of_ghentb y_of_pigs_invasionantonio_lopez_de_santa_anna great_northern_warhenry_i_of_england council_of_trentchiang_kai-sheksamuel_j._tildenfidel_castro wilmot_proviso yu n_dynastybastille b njamin_harrisonwar_of_the_austrian_successioncrimean_warjohn_brown_(abolitionist)teapot_dome_scandal albert_b._fallmarcus_licinius_crassus earl_warrenwarren_g._harding gunpowder_plothomestead_strike samuel_adamsjohn_peter_zenger thomas_painefree_soil_partyst._barth lomew's_day_massacrearthur_wellesley,_1st_duke_of_wellingtoncharles_de_gaulleleon_trotsky hugh_capetal xander_h._stephens haile_selassieilliam_h._sewardrutherford_b._hayes safavid_dynastymuhammad_ali_jinnah kulturkampfmaximilien_de_robespierre hub rt_humphreyluddite hull_housephilip_ii_of_macedon guelphs_a d_ghibellines byzantine_empirealbigensian_crusade diocletianfort_ticonderoga parthian_empirecharles_martel william_jennings_bryanalexan er_ii_of_russiaferdinand_magellanstate_of_franklin ivan_the_terriblemartin_luther_(1953_film) millard_fillmorefrancisco_francoaethelred_th _unready ronald_reaganbenito_mussolini henry_claykitchen_cabinetblack_hole_of_calcuttaancient_corinth john_wilkes_b th john_tylerrobert_walpolehuey_long tokugawa_ieyasuthomas_nastnikita_khrushchev andrew_jacksonportug llabour_party_(uk) monroe_doctrine john_quincy_adamscongress_of_berlintecumsehjacques_cartier battle_ f_the_thamesspanis _civil_warethiopia fugitiv _slave_lawsjoh _a._macdonald council_of_chalcedonpancho_villa war_of_the_pacific george_ allacesusan_b._anthonymarcus_garvey grover_clevelandjohn_haygeorge_b._mcclellanoctober_manifestovitus_bering john_hanc ckwilliam_lloyd_garrisonplatt_amendmentmary,_queen_of_scotsfirst_triumviratefra cisc _vasquez_de_coronadomargaret_thatcher sherma _antitrust_acthanseatic_leaguehenry_morton_stanley ju y_revolutionstephen_a._douglas xyz_affair jimmy_ca terfrancisco_pizarrokublai_khanvasco_da_gama spartabattle_of_caporetto ostend_manifestomustafa_kemal_ataturk peter_the_greatgang_of_fourbattle_of_chance lorsvilledavid_lloyd_georgecardinal_mazarin embargo_act_of_1807 brigham_youngcharles_lindberghhudson's_bay_company attilaparis_commu e jefferson_davisamelia_earhart mali_empireadolf_hitler benedict_arnoldcamillo_benso,_count_of_cavour meiji_restorationblack_panther_party mark_antony f anklin_p ercemolly_maguires zachary_taylorhan_dynastyadlai_stevenson_iijames_k._polk douglas_macarthurboston_massacretoyotomi_hidey shigreenback_partysecond_boer_warthird_crusade j es_buchananjoh _she mangeorge_washingtonwars_of_the_rosesatlantic_charter eleanor_rooseveltcongress_ f_viennajohn_wycliffewinston_churchillmilio_aguinaldom guel_hidalgo_ costill second_bank_of_the_united_statescouncil_of constanceseneca_falls_convention first_crusade spiro_agnewtaiping_rebellionmao_zedong paul_von_hindenburgalbany_congressjawaharlal_nehru battle_of_blenheimethan_allenantonio_de_oliveira_salazar herbert_hooverpepin_the_shortindira_gandhi william_howard_taftthomas_jeffersonga a _abdel_nasser oliver_cromw llsalmon_p._chase battle_of_austerlitzbenjamin_disraeli gadsden purchasegirolamo_savonarola treaty_of_tordesillasbattle_of_marathon elizabeth_cady_stantonbattle_of_kings_mountainchristopher_colum uswilliam_the_conquerorbattle_of_trafalgar charles_evans_hughescleistheneswilliam_tecumseh_shermanmobutu_sese ek prague_spring babur peloponn sian_w rjacques_marquette neroparaguay hyksos artin_van_burenbonus rmycha les_stew rt_parnell edward_the_confessorbartolome _d ssalem_witch_trials battle of_ he_bulge john_ damsaginot_lin henry_cabot_logiuseppe_garib ldi daniel_websterjohn_c._calhoun treaty_of_waitangizebulon_pike genghis_khan
calvin_coolidgewilliam_mckinley
james_monroe
woodrow_wilson
william_henry_harrisonbenjamin_harrisonmillard_fillmoreronald_reagan
john_tyler andrew_jacksonjohn_quincy_adamsgrover_clevelandjimmy_carter
franklin_piercezachary_taylorjames_buchanangeorge_washington
herbert_hooverwilliam_howard_taft
thomas_jefferson
martin_van_buren
john_adams
Figure 5: t-SNE 2-D projections of 451 answer
vectors divided into six major clusters. The
blue cluster is predominantly populated by U.S.
presidents. The zoomed plot reveals temporal
clustering among the presidents based on the
years they spent in office.
from the meaning of the words that it con-
tains as well as the syntax that glues those
words together. Many computational models
of compositionality focus on learning vector
spaces (Zanzotto et al., 2010; Erk, 2012; Grefen-
stette et al., 2013; Yessenalina and Cardie,
2011). Recent approaches towards modeling
compositional vector spaces with neural net-
works have been successful, although simpler
functions have been proposed for short phrases
(Mitchell and Lapata, 2008).
Recursive neural networks have achieved
state-of-the-art performance in sentiment anal-
ysis and parsing (Socher et al., 2013c; Hermann
and Blunsom, 2013; Socher et al., 2013a). rnns
have not been previously used for learning at-
tribute spaces as we do here, although recursive
tensor networks were unsuccessfully applied to
a knowledge base completion task (Socher et
al., 2013b). More relevant to this work are the
dialogue analysis model proposed by Kalchbren-
ner & Blunsom (2013) and the paragraph vec-
tor model described in Le and Mikolov (2014),
both of which are able to generate distributed
representations of paragraphs. Here we present
a simpler approach where a single model is able
to learn complex sentence representations and
average them across paragraphs.
6.2 Factoid Question-Answering
Factoid question answering is often functionally
equivalent to information retrieval. Given a
knowledge base and a query, the goal is to
Thomas Mann
Joseph Conrad
Henrik Ibsen
Franz Kafka
Henry James
Figure 6: A question on the German novelist
Thomas Mann that contains no named entities,
along with the five top answers as scored by
qanta. Each cell in the heatmap corresponds
to the score (inner product) between a node
in the parse tree and the given answer, and
the dependency parse of the sentence is shown
on the left. All of our baselines, including ir-
wiki, are wrong, while qanta uses the plot
description to make a correct guess.
return the answer. Many approaches to this
problem rely on hand-crafted pattern matching
and answer-type classification to narrow down
the search space (Shen, 2007; Bilotti et al.,
2010; Wang, 2006). More recent factoid qa
systems incorporate the web and social media
into their retrieval systems (Bian et al., 2008).
In contrast to these approaches, we place the
burden of learning answer types and patterns
on the model.
7 Future Work
While we have shown that dt-rnns are effec-
tive models for quiz bowl question answering,
other factoid qa tasks are more challenging.
Questions like what does the aarp stand for?
from trec qa data require additional infras-
tructure. A more apt comparison would be to
IBM?s proprietary Watson system (Lally et al.,
2012) for Jeopardy, which is limited to single
sentences, or to models trained on Yago (Hof-
fart et al., 2013).
We would also like to fairly compare qanta
641
Akbar
Shah Jahan
Muhammad
Babur
Ghana
Figure 7: An extremely misleading question
about John Cabot, at least to computer models.
The words muslim and mecca lead to three
Mughal emperors in the top five guesses from
qanta; other models are similarly led awry.
with ir-wiki. A promising avenue for future
work would be to incorporate Wikipedia data
into qanta by transforming sentences to look
like quiz bowl questions (Wang et al., 2007) and
to select relevant sentences, as not every sen-
tence in a Wikipedia article directly describes
its subject. Syntax-specific annotation (Sayeed
et al., 2012) may help in this regard.
Finally, we could adapt the attribute space
learned by the dt-rnn to use information from
knowledge bases and to aid in knowledge base
completion. Having learned many facts about
entities that occur in question text, a dt-rnn
could add new facts to a knowledge base or
check existing relationships.
8 Conclusion
We present qanta, a dependency-tree recursive
neural network for factoid question answering
that outperforms bag of words and informa-
tion retrieval baselines. Our model improves
upon a contrastive max-margin objective func-
tion from previous work to dynamically update
answer vectors during training with a single
model. Finally, we show that sentence-level
representations can be easily and effectively
combined to generate paragraph-level represen-
Q he also successfully represented the amistad
slaves and negotiated the treaty of ghent and
the annexation of florida from spain during his
stint as secretary of state under james monroe
A john quincy adams, henry clay, andrew jack-
son
Q this work refers to people who fell on their
knees in hopeless cathedrals and who jumped
off the brooklyn bridge
A howl, the tempest, paradise lost
Q despite the fact that twenty six martyrs were
crucified here in the late sixteenth century it
remained the center of christianity in its coun-
try
A nagasaki, guadalcanal, ethiopia
Q this novel parodies freudianism in a chapter
about the protagonist ?s dream of holding a
live fish in his hands
A
billy budd, the ambassadors, all my sons
Q a contemporary of elizabeth i he came to power
two years before her and died two years later
A
grover cleveland, benjamin harrison, henry
cabot lodge
Table 2: Five example sentences occuring at
the first sentence position along with their top
three answers as scored by qanta; correct an-
swers are marked with blue and wrong answers
are marked with red. qanta gets the first
three correct, unlike all other baselines. The
last two questions are too difficult for all of
our models, requiring external knowledge (e.g.,
Freudianism) and temporal reasoning.
tations with more predictive power than those
of the individual sentences.
Acknowledgments
We thank the anonymous reviewers, Stephanie
Hwa, Bert Huang, and He He for their insight-
ful comments. We thank Sharad Vikram, R.
Hentzel, and the members of naqt for pro-
viding our data. This work was supported by
nsf Grant IIS-1320538. Boyd-Graber is also
supported by nsf Grant CCF-1018625. Any
opinions, findings, conclusions, or recommen-
dations expressed here are those of the authors
and do not necessarily reflect the view of the
sponsor.
642
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. JMLR.
Jiang Bian, Yandong Liu, Eugene Agichtein, and
Hongyuan Zha. 2008. Finding the right facts in
the crowd: factoid question answering over social
media. In WWW.
Matthew W. Bilotti, Jonathan Elsas, Jaime Carbonell,
and Eric Nyberg. 2010. Rank learning for factoid
question answering with linguistic and semantic con-
straints. In CIKM.
Jordan Boyd-Graber, Brianna Satinoff, He He, and
Hal Daume III. 2012. Besting the quiz master:
Crowdsourcing incremental classification games. In
EMNLP.
Marie-Catherine De Marneffe, Bill MacCartney, Christo-
pher D Manning, et al. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR, 999999:2121?
2159.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass.
Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Neural Networks,
1996., IEEE International Conference on, volume 1.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. CoRR.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In ACL.
Karl Moritz Hermann, Edward Grefenstette, and Phil
Blunsom. 2013. ?not not bad? is not ?bad?: A
distributional account of negation. Proceedings of the
ACL Workshop on Continuous Vector Space Models
and their Compositionality.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
ACL.
Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,
and Gerhard Weikum. 2013. Yago2: A spatially and
temporally enhanced knowledge base from wikipedia.
Artificial Intelligence, 194:28?61.
Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and
Philip Resnik. 2014. Political ideology detection
using recursive neural networks.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. Proceedings of the 2013 Workshop on
Continuous Vector Space Models and their Composi-
tionality.
Adam Lally, John M Prager, Michael C McCord,
BK Boguraev, Siddharth Patwardhan, James Fan,
Paul Fodor, and Jennifer Chu-Carroll. 2012. Ques-
tion analysis: How watson reads a clue. IBM Journal
of Research and Development.
Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL.
Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton,
and Tom M. Mitchell. 2009. Zero-shot learning with
semantic output codes. In NIPS.
P. Pasupat and P. Liang. 2014. Zero-shot entity extrac-
tion from web pages. In ACL.
Asad B Sayeed, Jordan Boyd-Graber, Bryan Rusk, and
Amy Weinberg. 2012. Grammatical structures for
word-level sentiment detection. In NAACL.
Dan Shen. 2007. Using semantic role to improve ques-
tion answering. In EMNLP.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Predict-
ing Sentiment Distributions. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing With Composi-
tional Vector Grammars. In ACL.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013b. Reasoning With Neural
Tensor Networks For Knowledge Base Completion.
In NIPS.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013c. Recursive deep models for
semantic compositionality over a sentiment treebank.
In EMNLP.
Richard Socher, Quoc V Le, Christopher D Manning,
and Andrew Y Ng. 2014. Grounded compositional
semantics for finding and describing images with
sentences. TACL.
Nicolas Usunier, David Buffoni, and Patrick Gallinari.
2009. Ranking with ordered weighted pairwise clas-
sification. In ICML.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. JMLR.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In EMNLP.
643
Mengqiu Wang. 2006. A survey of answer extraction
techniques in factoid question answering. Computa-
tional Linguistics, 1(1).
Jason Weston, Samy Bengio, and Nicolas Usunier. 2011.
Wsabie: Scaling up to large vocabulary image anno-
tation. In IJCAI.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In EMNLP.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In COLT.
644
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1342?1352,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Don?t Until the Final Verb Wait:
Reinforcement Learning for Simultaneous Machine Translation
Alvin C. Grissom II
and Jordan Boyd-Graber
Computer Science
University of Colorado
Boulder, CO
Alvin.Grissom@colorado.edu
Jordan.Boyd.Graber@colorado.edu
He He, John Morgan,
and Hal Daume? III
Computer Science and UMIACS
University of Maryland
College Park, MD
{hhe,jjm,hal}@cs.umd.edu
Abstract
We introduce a reinforcement learning-
based approach to simultaneous ma-
chine translation?producing a trans-
lation while receiving input words?
between languages with drastically dif-
ferent word orders: from verb-final lan-
guages (e.g., German) to verb-medial
languages (English). In traditional ma-
chine translation, a translator must
?wait? for source material to appear be-
fore translation begins. We remove this
bottleneck by predicting the final verb
in advance. We use reinforcement learn-
ing to learn when to trust predictions
about unseen, future portions of the
sentence. We also introduce an evalua-
tion metric to measure expeditiousness
and quality. We show that our new
translation model outperforms batch
and monotone translation strategies.
1 Introduction
We introduce a simultaneous machine transla-
tion (MT) system that predicts unseen verbs
and uses reinforcement learning to learn when
to trust these predictions and when to wait for
more input.
Simultaneous translation is producing a par-
tial translation of a sentence before the input
sentence is complete, and is often used in im-
portant diplomatic settings. One of the first
noted uses of human simultaneous interpreta-
tion was the Nuremberg trials after the Sec-
ond World War. Siegfried Ramler (2009), the
Austrian-American who organized the transla-
tion teams, describes the linguistic predictions
and circumlocutions that translators would use
to achieve a tradeoff between translation la-
tency and accuracy. The audio recording tech-
nology used by those interpreters sowed the
seeds of technology-assisted interpretation at
the United Nations (Gaiba, 1998).
Performing real-time translation is especially
difficult when information that comes early in
the target language (the language you?re trans-
lating to) comes late in the source language (the
language you?re translating from). A common
example is when translating from a verb-final
(sov) language (e.g., German or Japanese) to
a verb-medial (svo) language, (e.g., English).
In the example in Figure 1, for instance, the
main verb of the sentence (in bold) appears
at the end of the German sentence. An of-
fline (or ?batch?) translation system waits until
the end of the sentence before translating any-
thing. While this is a reasonable approach, it
has obvious limitations. Real-time, interactive
scenarios?such as online multilingual video
conferences or diplomatic meetings?require
comprehensible partial interpretations before
a sentence ends. Thus, a significant goal in
interpretation is to make the translation as
expeditious as possible.
We present three components for an sov-to-
svo simultaneous mt system: a reinforcement
learning framework that uses predictions to
create expeditious translations (Section 2), a
system to predict how a sentence will end (e.g.,
predicting the main verb; Section 4), and a met-
ric that balances quality and expeditiousness
(Section 3). We combine these components in
a framework that learns when to begin trans-
lating sections of a sentence (Section 5).
Section 6 combines this framework with a
1342
ich bin mit dem Zug nach Ulm gefahren
I am with the train to Ulm traveled
I (. . . . . . waiting. . . . . . ) traveled by train to Ulm
Figure 1: An example of translating from a
verb-final language to English. The verb, in
bold, appears at the end of the sentence, pre-
venting coherent translations until the final
source word is revealed.
translation system that produces simultaneous
translations. We show that our data-driven
system can successfully predict unseen parts
of the sentence, learn when to trust them, and
outperform strong baselines (Section 7).
While some prior research has approached
the problem of simultaneous translation?we re-
view these systems in more detail in Section 8?
no current model learns when to definitively
begin translating chunks of an incomplete sen-
tence. Finally, in Section 9, we discuss the
limitations of our system: it only uses the most
frequent source language verbs, it only applies
to sentences with a single main verb, and it
uses an idealized translation system. However,
these limitations are not insurmountable; we
describe how a more robust system can be as-
sembled from these components.
2 Decision Process for
Simultaneous Translation
Human interpreters learn strategies for their
profession with experience and practice. As
words in the source language are observed, a
translator?human or machine?must decide
whether and how to translate, while, for cer-
tain language pairs, simultaneously predicting
future words. We would like our system to do
the same. To this end, we model simultaneous
mt as a Markov decision process (mdp) and
use reinforcement learning to effectively com-
bine predicting, waiting, and translating into
a coherent strategy.
2.1 States: What is, what is to come
The state s
t
represents the current view of
the world given that we have seen t words of
a source language sentence.
1
The state con-
tains information both about what is known
and what is predicted based on what is known.
1
We use t to evoke a discrete version of time. We
only allow actions after observing a complete source
word.
To compare the system to a human transla-
tor in a decision-making process, the state is
akin to the translator?s cognitive state. At any
given time, we have knowledge (observations)
and beliefs (predictions) with varying degrees
of certainty: that is, the state contains the re-
vealed words x
1:t
of a sentence; the state also
contains predictions about the remainder of
the sentence: we predict the next word in the
sentence and the final verb.
More formally, we have a prediction at time
t of the next source language word that will
appear, n
(t)
t+1
, and for the final verb, v
(t)
. For
example, given the partial observation ?ich
bin mit dem?, the state might contain a pre-
diction that the next word, n
(t)
t+1
, will be ?Zug?
and that the final verb v
(t)
will be ?gefahren?.
We discuss the mechanics of next-word and
verb prediction further in Section 4; for now,
consider these black boxes which, after observ-
ing every new source word x
t
, make predictions
of future words in the source language. This
representation of the state allows for a richer set
of actions, described below, permitting simul-
taneous translations that outpace the source
language input
2
by predicting the future.
2.2 Actions: What our system can do
Given observed and hypothesized input, our
simultaneous translation system must decide
when to translate them. This is expressed
in the form of four actions: our system can
commit to a partial translation, predict the
next word and use it to update the transla-
tion, predict the verb and use it to update the
translation, or wait for more words.
We discuss each of these actions in turn be-
fore describing how they come together to in-
crementally translate an entire sentence:
Wait Waiting is the simplest action. It pro-
duces no output and allows the system to re-
ceive more input, biding its time, so that when
it does choose to translate, the translation is
based on more information.
Commit Committing produces translation
output: given the observed source sentence,
produce the best translation possible.
2
Throughout, ?input? refers to source language in-
put, and ?output? refers to target language translation.
1343
STOP
C
o
m
m
i
t
 Observation  Observation (prediction)  Observation  Observation 
 Observation 
1. Mit dem Zug 2. Mit dem Zug bin
ich
3. Mit dem Zug bin
ich nach
4. Mit dem Zug bin
 ich nach ? gefahren ...
 Observation (prediction) 
5. Mit dem Zug bin ich
nach Ulm
? gefahren ...
6. Mit dem Zug bin ich
nach Ulm gefahren.
Output: I traveled
by train
Output: I traveled
by train
to Ulm
Output: I traveled 
by train
to Ulm.
S
with the 
train
I am
with the 
train
by train
to Ulm
S
by train
Wait Wait
Predict
S
I traveled
by train
with the 
train
to
C
o
m
m
i
t
Wait
Fixed 
output
Commit
s
t
a
t
e
Figure 2: A simultaneous translation from source (German) to target (English). The agent
chooses to wait until after (3). At this point, it is sufficiently confident to predict the final verb
of the sentence (4). Given this additional information, it can now begin translating the sentence
into English, constraining future translations (5). As the rest of the sentence is revealed, the
system can translate the remainder of the sentence.
Next Word The next word action takes
a prediction of the next source word and pro-
duces an updated translation based on that
prediction, i.e., appending the predicted word
to the source sentence and translating the new
sentence.
Verb Our system can also predict the source
sentence?s final verb (the last word in the sen-
tence). When our system takes the verb ac-
tion, it uses its verb prediction to update the
translation using the prediction, by placing it
at the end of the source sentence.
We can recreate a traditional batch trans-
lation system (interpreted temporally) by a
sequence of wait actions until all input is ob-
served, followed by a commit to the complete
translation. Our system can commit to par-
tial translations if it is confident, but producing
a good translation early in the sentence often
depends on missing information.
2.3 Translation Process
Having described the state, its components,
and the possible actions at a state, we present
the process in its entirety. In Figure 2, after
each German word is received, the system ar-
rives at a new state, which consists of the source
input, target translation so far, and predictions
of the unseen words. The translation system
must then take an action given information
about the current state. The action will result
in receiving and translating more source words,
transitioning the system to the next state. In
the example, for the first few source-language
words, the translator lacks the confidence to
produce any output due to insufficient informa-
tion at the state. However, after State 3, the
state shows high confidence in the predicted
verb ?gefahren?. Combined with the German
input it has observed, the system is sufficiently
confident to act on that prediction to produce
English translation.
2.4 Consensus Translations
Three straightforward actions?commit, next
word, and verb?all produce translations.
These rely black box access to a translation
(discussed in detail in Section 6): that is, given
a source language sentence fragment, the trans-
lation system produces a target language sen-
tence fragment.
Because these actions can happen more than
once in a sentence, we must form a single con-
sensus translation from all of the translations
that we might have seen. If we have only one
translation or if translations are identical, form-
ing the consensus translation is trivial. But
how should we resolve conflicting translations?
Any time our system chooses an action that
1344
produces output, the observed input (plus extra
predictions in the case of next-word or verb),
is passed into the translation system. That
system then produces a complete translation
of its input fragment.
Any new words?i.e., words whose target
index is greater than the length of any previ-
ous translation?are appended to the previous
translation.
3
Table 1 shows an example of
forming these consensus translations.
Now that we have defined how states evolve
based on our system?s actions, we need to know
how to select which actions to take. Eventu-
ally, we will formalize this as a learned policy
(Section 5) that maps from states to actions.
First, however, we need to define a reward that
measures how ?good? an action is.
3 Objective: What is a good
simultaneous translation?
Good simultaneous translations must optimize
two objectives that are often at odds, i.e., pro-
ducing translations that are, in the end, accu-
rate, and producing them in pieces that are
presented expeditiously. While there are exist-
ing automated metrics for assessing translation
quality (Papineni et al., 2002; Banerjee and
Lavie, 2005; Snover et al., 2006), these must
be modified to find the necessary compromise
between translation quality and expeditious-
ness. That is, a good metric for simultaneous
translation must achieve a balance between
translating chunks early and translating accu-
rately. All else being equal, maximizing either
goal in isolation is trivial: for accurate transla-
tions, use a batch system and wait until the
sentence is complete, translating it all at once;
for a maximally expeditious translation, cre-
ate monotone translations, translating each
word as it appears, as in Tillmann et al. (1997)
and Pytlik and Yarowsky (2006). The former
is not simultaneous at all; the latter is mere
word-for-word replacement and results in awk-
ward, often unintelligible translations of distant
language pairs.
Once we have predictions, we have an ex-
panded array of possibilities, however. On one
extreme, we can imagine a psychic translator?
3
Using constrained decoding to enforce consistent
translation prefixes would complicate our method but
is an appealing extension.
one that can completely translate an imagined
sentence after one word is uttered?as an un-
obtainable system. On the other extreme is a
standard batch translator, which waits until
it has access to the utterer?s complete sentence
before translating anything.
Again, we argue that a system can improve
on this by predicting unseen parts of the sen-
tence to find a better tradeoff between these
conflicting goals. However, to evaluate and op-
timize such a system, we must measure where
a system falls on the continuum of accuracy
versus expeditiousness.
Consider partial translations in a two-
dimensional space, with time (quantized by
the number of source words seen) increasing
from left to right on the x axis and the bleu
score (including brevity penalty against the
reference length) on the y axis. At each point
in time, the system may add to the consensus
translation, changing the precision (Figure 3).
Like an roc curve, a good system will be high
and to the left, optimizing the area under the
curve: the ideal system would produce points
as high as possible immediately. A translation
which is, in the end, accurate, but which is less
expeditious, would accrue its score more slowly
but outperform a similarly expeditious system
which nevertheless translates poorly.
An idealized psychic system achieves this,
claiming all of the area under the curve, as it
would have a perfect translation instantly, hav-
ing no need of even waiting for future input.
4
A batch system has only a narrow (but tall)
sliver to the right, since it translates nothing
until all of the words are observed.
Formally, let Q be the score function for a
partial translation, x the sequentially revealed
source words x
1
, x
2
, . . . , x
T
from time step 1 to
T , and y the partial translations y
1
, y
2
, . . . , y
T
,
where T is the length of the source language
input. Each incremental translation y
t
has a
bleu-n score with respect to a reference r. We
apply the usual bleu brevity penalty to all the
incremental translations (initially empty) to
4
One could reasonably argue that this is not ideal:
a fluid conversation requires the prosody and timing
between source and target to match exactly. Thus, a
psychic system would provide too much information
too quickly, making information exchange unnatural.
However, we take the information-centric approach:
more information faster is better.
1345
Pos Input Intermediate Consensus
1
2 Er He
1
He
1
3 Er wurde
gestaltet
It
1
was
2
designed
3
He
1
was
2
designed
3
4 It
1
was
2
designed
3
He
1
was
2
designed
3
5 Er wurde
gestern
renoviert
It
1
was
2
renovated
3
yesterday
4
He
1
was
2
designed
3
yesterday
4
Table 1: How intermediate translations are combined into a consensus translation. Incorrect
translations (e.g., ?he? for an inanimate object in position 3) and incorrect predictions (e.g.,
incorrectly predicting the verb gestaltet in position 5) are kept in the consensus translation.
When no translation is made, the consensus translation remains static.
Er ist zum Laden gegangen
He went to 
the store
He
He to the
He to the store
Psychic
Monotone
He went 
to the 
store
Batch
Policy
Prediction
He
He went
He went to 
the store
He to the 
store went
He went 
to  the
Source Sentence
?
Figure 3: Comparison of lbleu (the area under
the curve given by Equation 1) for an impossi-
ble psychic system, a traditional batch system,
a monotone (German word order) system, and
our prediction-based system. By correctly pre-
dicting the verb ?gegangen? (to go), we achieve
a better overall translation more quickly.
obtain latency-bleu (lbleu),
Q(x,y) =
1
T
?
t
bleu(y
t
, r) (1)
+ T ? bleu(y
T
, r)
The lbleu score is a word-by-word inte-
gral across the input source sentence. As each
source word is observed, the system receives a
reward based on the bleu score of the partial
translation. lbleu, then, represents the sum of
these T rewards at each point in the sentence.
The score of a simultaneous translation is the
sum of the scores of all individual segments
that contribute to the overall translation.
We multiply the final bleu score by T to en-
sure good final translations in learned systems
to compensate for the implicit bias toward low
latency.
5
4 Predicting Verbs and Next
Words
The next and verb actions depend on predic-
tions of the sentence?s next word and final verb;
this section describes our process for predict-
ing verbs and next words given a partial source
language sentence.
The prediction of the next word in the source
language sentence is modeled with a left-to-
right language model. This is (na??vely) anal-
ogous to how a human translator might use
his own ?language model? to guess upcoming
words to gain some speed by completing, for
example, collocations before they are uttered.
We use a simple bigram language model for
next-word prediction. We use Heafield et al.
(2013).
For verb prediction, we use a generative
model that combines the prior probability of
a particular verb v, p(v), with the likelihood
of the source context at time t given that
verb (namely, p(x
1:t
| v)), as estimated by a
smoothed Kneser-Ney language model (Kneser
and Ney, 1995). We use Pauls and Klein
(2011). The prior probability p(v) is estimated
by simple relative frequency estimation. The
context, x
1:t
, consists of all words observed.
We model p(x
1:t
| v) with verb-specific n-gram
language models. The predicted verb v
(t)
at
time t is then:
arg max
v
p(v)
t
?
i=1
p(x
i
| v, x
i?n+1:i?1
) (2)
5
One could replace T with a parameter, ?, to bias
towards different kinds of simultaneous translations. As
? ??, we recover batch translation.
1346
where x
i?n+1:i?1
is the n?1-gram context. To
narrow the search space, we consider only the
100 most frequent final verbs, where a ?final
verb? is defined as the sentence-final sequence
of verbs and particles as detected by a German
part-of-speech tagger (Toutanova et al., 2003).
6
5 Learning a Policy
We have a framework (states and actions) for
simultaneous machine translation and a metric
for assessing simultaneous translations. We
now describe the use of reinforcement learning
to learn a policy, a mapping from states to
actions, to maximize lbleu reward.
We use imitation learning (Abbeel and Ng,
2004; Syed et al., 2008): given an optimal se-
quence of actions, learn a generalized policy
that maps states to actions. This can be viewed
as a cost-sensitive classification (Langford and
Zadrozny, 2005): a state is represented as a fea-
ture vector, the loss corresponds to the quality
of the action, and the output of the classifier is
the action that should be taken in that state.
In this section, we explain each of these com-
ponents: generating an optimal policy, repre-
senting states through features, and learning a
policy that can generalize to new sentences.
5.1 Optimal Policies
Because we will eventually learn policies via
a classifier, we must provide training exam-
ples to our classifier. These training exam-
ples come from an oracle policy pi
?
that
demonstrates the optimal sequence?i.e., with
maximal lbleu score?of actions for each se-
quence. Using dynamic programming, we can
determine such actions for a fixed translation
model.
7
From this oracle policy, we generate
training examples for a supervised classifier.
State s
t
is represented as a tuple of the ob-
served words x
1:t
, predicted verb v
(t)
, and the
predicted word n
(t)
t+1
. We represent the state to
a classifier as a feature vector ?(x
1:t
, n
(t)
t+1
, v
(t)
).
6
This has the obvious disadvantage of ignoring mor-
phology and occasionally creating duplicates of common
verbs that have may be associated with multiple parti-
cles; nevertheless, it provides a straightforward verb to
predict.
7
This is possible for the limited class of consensus
translation schemes discussed in Section 2.4.
5.2 Feature Representation
We want a feature representation that will al-
low a classifier to generalize beyond the specific
examples on which it is trained. We use sev-
eral general classes of features: features that
describe the input, features that describe the
possible translations, and features that describe
the quality of the predictions.
Input We include both a bag of words rep-
resentation of the input sentence as well as
the most recent word and bigram to model
word-specific effects. We also use a feature
that encodes the length of the source sentence.
Prediction We include the identity of the
predicted verb and next word as well as their re-
spective probabilities under the language mod-
els that generate the predictions. If the model
is confident in the prediction, the classifier can
learn to more so trust the predictions.
Translation In addition to the state, we in-
clude features derived from the possible actions
the system might take. This includes a bag of
words representation of the target sentence, the
score of the translation (decreasing the score is
undesirable), the score of the current consen-
sus translation, and the difference between the
current and potential translation scores.
5.3 Policy Learning
Our goal is to learn a classifier that can accu-
rately mimic the oracle?s choices on previously
unseen data. However, at test time, when we
run the learned policy classifier, the learned
policy?s state distribution may deviate from
the optimal policy?s state distribution due to
imperfect imitation, arriving in states not on
the oracle?s path. To address this, we use
searn (Daume? III et al., 2009), an iterative
imitation learning algorithm. We learn from
the optimal policy in the first iteration, as in
standard supervised learning; in the following
iterations, we run an interpolated policy
pi
k+1
= pi
k
+ (1? )pi
?
, (3)
with k as the iteration number and  the mixing
probability. We collect examples by asking
the policy to label states on its path. The
interpolated policy will execute the optimal
action with probability 1?  and the learned
1347
policy?s action with probability . In the first
iteration, we have pi
0
= pi
?
.
Mixing in the learned policy allows the
learned policy to slowly change from the oracle
policy. As it trains on these no-longer-perfect
state trajectories, the state distribution at test
time will be more consistent with the states
used in training.
searn learns the policy by training a cost-
sensitive classifier. Besides providing the opti-
mal action, the oracle must also assign a cost
to an action
C(a
t
,x) ? Q(x, pi
?
(x
t
))?Q(x, a
t
(x
t
)), (4)
where a
t
(x
t
) represents the translation outcome
of taking action a
t
. The cost is the regret of
not taking the optimal action.
6 Translation System
The focus of this work is to show that given an
effective batch translation system and predic-
tions, we can learn a policy that will turn this
into a simultaneous translation system. Thus,
to separate translation errors from policy er-
rors, we perform experiments with a nearly
optimal translation system we call an omni-
scient translator.
More realistic translation systems will nat-
urally lower the objective function, often in
ways that make it difficult to show that we can
effectively predict the verbs in verb-final source
languages. For instance, German to English
translation systems often drop the verb; thus,
predicting a verb that will be ignored by the
translation system will not be effective.
The omniscient translator translates a source
sentence correctly once it has been fed the ap-
propriate source words as input. There are
two edge cases: empty input yields an empty
output, while a complete, correct source sen-
tence returns the correct, complete translation.
Intermediate cases?where the input is either
incomplete or incorrect?require using an align-
ment. The omniscient translator assumes as
input a reference translation r, a partial source
language input x
1:t
and a corresponding partial
output y. In addition, the omniscient transla-
tor assumes access to an alignment between r
and x. In practice, we use the hmm aligner (Vo-
gel et al., 1996; Och and Ney, 2003).
We first consider incomplete but correct in-
puts. Let y = ?(x
1:t
) be the translator?s output
given a partial source input x
1:t
with transla-
tion y. Then, ?(x
1:t
) produces all target words
y
j
if there is a source word x
i
in the input
aligned to those words?i.e., (i, j) ? a
x,y
?and
all preceding target words can be translated.
(That translations must be contiguous is a nat-
ural requirement for human recipients of trans-
lations). In the case where y
j
is unaligned, the
closest aligned target word to y
j
that has a
corresponding alignment entry is aligned to x
i
;
then, if x
i
is present in the input, y
j
appears in
the output. Thus, our omniscient translation
system will always produce the correct output
given the correct input.
However, our learned policy can make wrong
predictions, which can produce partial trans-
lations y that do not match the reference.
In this event, an incorrect source word x?
i
produces incorrect target words y?
j
, for all
j : (i, j) ? a
x,y
. These y?
j
are sampled from
the ibm Model 1 lexical probability table mul-
tiplied by the source language model y?
j
?
Mult(?
x?
i
)p
LM
(x?).
8
Thus, even if we predict
the correct verb using a next word action, it
will be in the wrong position and thus gener-
ate a translation from the lexical probabilities.
Since translations based on Model 1 probabil-
ities are generally inaccurate, the omniscient
translator will do very well when given correct
input but will produce very poor translations
otherwise.
7 Experiments
In this section, we describe our experimental
framework and results from our experiments.
From aligned data, we derive an omniscient
translator. We use monolingual data in the
source language to train the verb predictor and
the next word predictor. From these features,
we compute an optimal policy from which we
train a learned policy.
7.1 Data sets
For translation model and policy training, we
use data from the German-English Parallel ?de-
news? corpus of radio broadcast news (Koehn,
2000), which we lower-cased and stripped of
8
If a policy chooses an incorrect unaligned word, it
has no effect on the output. Alignments are position-
specific, so ?wrong? refers to position and type.
1348
punctuation. A total of 48, 601 sentence pairs
are randomly selected for building our system.
Of these, we use 70% (34, 528 pairs) for training
word alignments.
For training the translation policy, we re-
strict ourselves to sentences that end with one
of the 100 most frequent verbs (see Section 4).
This results in a data set of 4401 training sen-
tences and 1832 test sentences from the de-news
data. We did this to narrow the search space
(from thousands of possible, but mostly very
infrequent, verbs).
We used 1 million words of news text from
the Leipzig Wortschatz (Quasthoff et al., 2006)
German corpus to train 5-gram language mod-
els to predict a verb from the 100 most frequent
verbs.
For next-word prediction, we use the 18, 345
most frequent German bigrams from the train-
ing set to provide a set of candidates in a lan-
guage model trained on the same set. We use
frequent bigrams to reduce the computational
cost of finding the completion probability of
the next word.
7.2 Training Policies
In each iteration of searn, we learn a
multi-class classifier to implement the pol-
icy. The specific learning algorithm we use
is arow (Crammer et al., 2013). In the com-
plete version of searn, the cost of each action
is calculated as the highest expected reward
starting at the current state minus the actual
roll-out reward. However, computing the full
roll-out reward is computationally very expen-
sive. We thus use a surrogate binary cost: if
the predicted action is the same as the opti-
mal action, the cost is 0; otherwise, the cost
is 1. We then run searn for five iterations.
Results on the development data indicate that
continuing for more iterations yields no benefit.
7.3 Policy Rewards on Test Set
In Figure 4, we show performance of the opti-
mal policy vis-a`-vis the learned policy, as well
as the two baseline policies: the batch policy
and the monotone policy. The x-axis is the
percentage of the source sentence seen by the
model, and the y-axis is a smoothed average of
the reward as a function of the percentage of
the sentence revealed. The monotone policy?s
performance is close to the optimal policy for
llllllllll l l l l l l l l
l
0.25
0.50
0.75
1.00
1.25
0.00 0.25 0.50 0.75 1.00% of Sentence
Sm
oo
the
d A
ve
ra
ge
l Batch Monotone Optimal Searn
Figure 4: The final reward of policies on Ger-
man data. Our policy outperforms all baselines
by the end of the sentence.
0
2500
5000
7500
10000
0
2500
5000
7500
10000
0
2500
5000
7500
10000
0
2500
5000
7500
10000
Batch
Monotone
Optimal
Searn
COMMIT WAIT NEXTWORD VERBAction
Ac
tio
n C
ou
nt
Policy Actions
Figure 5: Histogram of actions taken by the
policies.
the first half of the sentence, as German and
English have similar word order, though they
diverge toward the end. Our learned policy
outperforms the monotone policy toward the
end and of course outperforms the batch policy
throughout the sentence.
Figure 5 shows counts of actions taken by
each policy. The batch policy always commits
at the end. The monotone policy commits at
each position. Our learned policy has an ac-
tion distribution similar to that of the optimal
policy, but is slightly more cautious.
7.4 What Policies Do
Figure 6 shows a policy that, predicting incor-
rectly, still produces sensible output. The pol-
icy correctly intuits that the person discussed
1349
VE
R
B
federal minister of the 
environment angela merkel 
shown the draft of an 
ecopolitical program
bundesumweltministerin 
merkel hat den entwurf
bundesumweltministerin
INPUT OUTPUT
federal minister of the 
environment angela merkel
federal minister of the 
environment angela merkel 
shown the draft
Merkel
gezeigt
bundesumweltministerin 
merkel hat den entwurf 
eines umweltpolitischen 
programms vorgestellt
C
O
M
M
I
T
N
E
X
T
Figure 6: An imperfect execution of a learned
policy. Despite choosing the wrong verb
?gezeigt? (showed) instead of ?vorgestellt? (pre-
sented), the translation retains the meaning.
is Angela Merkel, who was the environmen-
tal minister at the time, but the policy uses
an incorrectly predicted verb. Because of our
poor translation model (Section 6), it renders
this word as ?shown?, which is a poor transla-
tion. However, it is still comprehensible, and
the overall policy is similar to what a human
would do: intuit the subject of the sentence
from early clues and use a more general verb
to stand in for a more specific one.
8 Related Work
Just as mt was revolutionized by statistical
learning, we suspect that simultaneous mt will
similarly benefit from this paradigm, both from
a systematic system for simultaneous transla-
tion and from a framework for learning how to
incorporate predictions.
Simultaneous translation has been
dominated by rule and parse-based ap-
proaches (Mima et al., 1998a; Ryu et al., 2006).
In contrast, although Verbmobil (Wahlster,
2000) performs incremental translation using a
statistical mt module, its incremental decision-
making module is rule-based. Other recent
approaches in speech-based systems focus on
waiting until a pause to translate (Sakamoto
et al., 2013) or using word alignments (Ryu
et al., 2012) between languages to determine
optimal translation units.
Unlike our work, which focuses on predic-
tion and learning, previous strategies for deal-
ing with sov-to-svo translation use rule-based
methods (Mima et al., 1998b) (for instance,
passivization) to buy time for the translator to
hear more information in a spoken context?or
use phrase table and reordering probabilities to
decide where to translate with less delay (Fu-
jita et al., 2013). Oda et al. (2014) is the
most similar to our work on the translation
side. They frame word segmentation as an
optimization problem, using a greedy search
and dynamic programming to find segmenta-
tion strategies that maximize an evaluation
measure. However, unlike our work, the direc-
tion of translation was from from svo to svo,
obviating the need for verb prediction. Simul-
taneous translation is more straightforward for
languages with compatible word orders, such
as English and Spanish (Fu?gen, 2008).
To our knowledge, the only attempt to
specifically predict verbs or any late-occurring
terms (Matsubara et al., 2000) uses pattern
matching on what would today be considered
a small data set to predict English verbs for
Japanese to English simultaneous mt.
Incorporating verb predictions into the trans-
lation process is a significant component of
our framework, though n-gram models strongly
prefer highly frequent verbs. Verb prediction
might be improved by applying the insights
from psycholinguistics. Ferreira (2000) argues
that verb lemmas are required early in sentence
production?prior to the first noun phrase
argument?and that multiple possible syntac-
tic hypotheses are maintained in parallel as the
sentence is produced. Schriefers et al. (1998)
argues that, in simple German sentences, non-
initial verbs do not need lemma planning at
all. Momma et al. (2014), investigating these
prior claims, argues that the abstract relation-
ship between the internal arguments and verbs
triggers selective verb planning.
9 Conclusion and Future Work
Creating an effective simultaneous translation
system for sov to svo languages requires not
only translating partial sentences, but also ef-
fectively predicting a sentence?s verb. Both
elements of the system require substantial re-
finement before they are usable in a real-world
system.
Replacing our idealized translation system
is the most challenging and most important
next step. Supporting multiple translation hy-
potheses and incremental decoding (Sankaran
1350
et al., 2010) would improve both the efficiency
and effectiveness of our system. Using data
from human translators (Shimizu et al., 2014)
could also add richer strategies for simultane-
ous translation: passive constructions, reorder-
ing, etc.
Verb prediction also can be substantially im-
proved both in its scope in the system and
how we predict verbs. Verb-final languages
also often place verbs at the end of clauses,
and also predicting these verbs would improve
simultaneous translation, enabling its effective
application to a wider range of sentences. In-
stead predicting an exact verb early (which is
very difficult), predicting a semantically close
or a more general verb might yield interpretable
translations.
A natural next step is expanding this work
to other languages, such as Japanese, which not
only has sov word order but also requires tok-
enization and morphological analysis, perhaps
requiring sub-word prediction.
Acknowledgments
We thank the anonymous reviewers, as well as
Yusuke Miyao, Naho Orita, Doug Oard, and
Sudha Rao for their insightful comments. This
work was supported by NSF Grant IIS-1320538.
Boyd-Graber is also partially supported by
NSF Grant CCF-1018625. Daume? III and He
are also partially supported by NSF Grant IIS-
0964681. Any opinions, findings, conclusions,
or recommendations expressed here are those
of the authors and do not necessarily reflect
the view of the sponsor.
References
Pieter Abbeel and Andrew Y. Ng. 2004. Appren-
ticeship learning via inverse reinforcement learn-
ing. In Proceedings of the International Confer-
ence of Machine Learning.
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evalua-
tion with improved correlation with human judg-
ments. In Proceedings of the Association for
Computational Linguistics. Association for Com-
putational Linguistics.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2013. Adaptive regularization of weight vectors.
Machine Learning, 91(2):155?187.
Hal Daume? III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Ma-
chine Learning Journal (MLJ).
Fernanda Ferreira. 2000. Syntax in language
production: An approach using tree-adjoining
grammars. Aspects of language production,
pages 291?330.
Christian Fu?gen. 2008. A system for simultane-
ous translation of lectures and speeches. Ph.D.
thesis, KIT-Bibliothek.
Tomoki Fujita, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2013.
Simple, lexicalized choice of translation timing
for simultaneous speech translation. INTER-
SPEECH.
Francesca Gaiba. 1998. The origins of simultane-
ous interpretation: The Nuremberg Trial. Uni-
versity of Ottawa Press.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable
modified Kneser-Ney language model estima-
tion. In Proceedings of the Association for Com-
putational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for n-gram language model-
ing. In Acoustics, Speech, and Signal Processing,
1995. ICASSP-95., 1995 International Confer-
ence on. IEEE.
Philipp Koehn. 2000. German-english parallel cor-
pus ?de-news?.
John Langford and Bianca Zadrozny. 2005. Relat-
ing reinforcement learning performance to clas-
sification performance. In Proceedings of the In-
ternational Conference of Machine Learning.
Shigeki Matsubara, Keiichi Iwashima, Nobuo
Kawaguchi, Katsuhiko Toyama, and Yasuyoshi
Inagaki. 2000. Simultaneous Japanese-English
interpretation based on early predition of En-
glish verb. In Symposium on Natural Language
Processing.
Hideki Mima, Hitoshi Iida, and Osamu Furuse.
1998a. Simultaneous interpretation utilizing
example-based incremental transfer. In Pro-
ceedings of the 17th international conference on
Computational linguistics-Volume 2, pages 855?
861. Association for Computational Linguistics.
Hideki Mima, Hitoshi Iida, and Osamu Furuse.
1998b. Simultaneous interpretation utilizing
example-based incremental transfer. In Proceed-
ings of the Association for Computational Lin-
guistics.
Shota Momma, Robert Slevc, and Colin Phillips.
2014. The timing of verb selection in english
active and passive sentences.
1351
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19?51.
Yusuke Oda, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2014. Op-
timizing segmentation strategies for simultane-
ous speech translation. In Proceedings of the As-
sociation for Computational Linguistics, June.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the Association for Computational
Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and
smaller n-gram language models. In Proceed-
ings of the Association for Computational Lin-
guistics.
Brock Pytlik and David Yarowsky. 2006. Machine
translation for languages lacking bitext via mul-
tilingual gloss transduction. In 5th Conference
of the Association for Machine Translation in
the Americas (AMTA), August.
Uwe Quasthoff, Matthias Richter, and Christian
Biemann. 2006. Corpus portal for search in
monolingual corpora. In International Language
Resources and Evaluation, pages 1799?1802.
Siegfried Ramler and Paul Berry. 2009. Nuremberg
and Beyond: The Memoirs of Siegfried Ramler
from 20th Century Europe to Hawai?i. Booklines
Hawaii Limited.
Koichiro Ryu, Shigeki Matsubara, and Yasuyoshi
Inagaki. 2006. Simultaneous english-japanese
spoken language translation based on incremen-
tal dependency parsing and transfer. In Proceed-
ings of the Association for Computational Lin-
guistics.
Koichiro Ryu, Shigeki Matsubara, and Yasuyoshi
Inagaki. 2012. Alignment-based translation
unit for simultaneous japanese-english spoken di-
alogue translation. In Innovations in Intelligent
Machines?2, pages 33?44. Springer.
Akiko Sakamoto, Nayuko Watanabe, Satoshi Ka-
matani, and Kazuo Sumita. 2013. Development
of a simultaneous interpretation system for face-
to-face services and its evaluation experiment in
real situation.
Baskaran Sankaran, Ajeet Grewal, and Anoop
Sarkar. 2010. Incremental decoding for phrase-
based statistical machine translation. In Pro-
ceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation.
H Schriefers, E Teruel, and RM Meinshausen.
1998. Producing simple sentences: Results from
picture?word interference experiments. Journal
of Memory and Language, 39(4):609?632.
Hiroaki Shimizu, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2014.
Collection of a simultaneous translation corpus
for comparative analysis. In International Lan-
guage Resources and Evaluation.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted hu-
man annotation. In In Proceedings of Associa-
tion for Machine Translation in the Americas.
Umar Syed, Michael Bowling, and Robert E.
Schapire. 2008. Apprenticeship learning using
linear programming. In Proceedings of the Inter-
national Conference of Machine Learning.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
and Alex Zubiaga. 1997. A dp-based search us-
ing monotone alignments in statistical transla-
tion. In Proceedings of the Association for Com-
putational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D
Manning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 173?180.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in
statistical translation. In Proceedings of the 16th
International Conference on Computational Lin-
guistics (COLING).
Wolfgang Wahlster. 2000. Verbmobil: foundations
of speech-to-speech translation. Springer.
1352
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1752?1757,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Sometimes Average is Best: The Importance of Averaging for Prediction
using MCMC Inference in Topic Modeling
Viet-An Nguyen
Computer Science
University of Maryland
College Park, MD
vietan@cs.umd.edu
Jordan Boyd-Graber
Computer Science
University of Colorado
Boulder, CO
jbg@boydgraber.org
Philip Resnik
Linguistics and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
Markov chain Monte Carlo (MCMC) approxi-
mates the posterior distribution of latent vari-
able models by generating many samples and
averaging over them. In practice, however, it
is often more convenient to cut corners, using
only a single sample or following a suboptimal
averaging strategy. We systematically study dif-
ferent strategies for averaging MCMC samples
and show empirically that averaging properly
leads to significant improvements in prediction.
1 Introduction
Probabilistic topic models are powerful methods to un-
cover hidden thematic structures in text by projecting
each document into a low dimensional space spanned
by a set of topics, each of which is a distribution over
words. Topic models such as latent Dirichlet alloca-
tion (Blei et al., 2003, LDA) and its extensions discover
these topics from text, which allows for effective ex-
ploration, analysis, and summarization of the otherwise
unstructured corpora (Blei, 2012; Blei, 2014).
In addition to exploratory data analysis, a typical goal
of topic models is prediction. Given a set of unanno-
tated training data, unsupervised topic models try to
learn good topics that can generalize to unseen text.
Supervised topic models jointly capture both the text
and associated metadata such as a continuous response
variable (Blei and McAuliffe, 2007; Zhu et al., 2009;
Nguyen et al., 2013), single label (Rosen-Zvi et al.,
2004; Lacoste-Julien et al., 2008; Wang et al., 2009)
or multiple labels (Ramage et al., 2009; Ramage et al.,
2011) to predict metadata from text.
Probabilistic topic modeling requires estimating the
posterior distribution. Exact computation of the poste-
rior is often intractable, which motivates approximate
inference techniques (Asuncion et al., 2009). One popu-
lar approach is Markov chain Monte Carlo (MCMC), a
class of inference algorithms to approximate the target
posterior distribution. To make prediction, MCMC al-
gorithms generate samples on training data to estimate
corpus-level latent variables, and use them to generate
samples to estimate document-level latent variables for
test data. The underlying theory requires averaging on
both training and test samples, but in practice it is often
convenient to cut corners: either skip averaging entirely
by using just the values of the last sample or use a single
training sample and average over test samples.
We systematically study non-averaging and averaging
strategies when performing predictions using MCMC in
topic modeling (Section 2). Using popular unsupervised
(LDA in Section 3) and supervised (SLDA in Section 4)
topic models via thorough experimentation, we show
empirically that cutting corners on averaging leads to
consistently poorer prediction.
2 Learning and Predicting with MCMC
While reviewing all of MCMC is beyond the scope of
this paper, we need to briefly review key concepts.
1
To
estimate a target density p(x) in a high-dimensional
space X , MCMC generates samples {x
t
}
T
t=1
while ex-
ploring X using the Markov assumption. Under this
assumption, sample x
t+1
depends on sample x
t
only,
forming a Markov chain, which allows the sampler to
spend more time in the most important regions of the
density. Two concepts control sample collection:
Burn-in B: Depending on the initial value of the
Markov chain, MCMC algorithms take time to reach
the target distribution. Thus, in practice, samples before
a burn-in period B are often discarded.
Sample-lag L: Averaging over samples to estimate
the target distribution requires i.i.d. samples. However,
future samples depend on the current samples (i.e., the
Markov assumption). To avoid autocorrelation, we dis-
card all but every L samples.
2.1 MCMC in Topic Modeling
As generative probabilistic models, topic models define
a joint distribution over latent variables and observable
evidence. In our setting, the latent variables consist of
corpus-level global variables g and document-level lo-
cal variables l; while the evidence consists of words w
and additional metadata y?the latter omitted in unsu-
pervised models.
During training, MCMC estimates the posterior
p(g, l
TR
|w
TR
,y
TR
) by generating a training Markov
chain of T
TR
samples.
2
Each training sample i pro-
vides a set of fully realized global latent variables
?
g(i),
which can generate test data. During test time, given a
1
For more details please refer to Neal (1993), Andrieu et
al. (2003), Resnik and Hardisty (2010).
2
We omit hyperparameters for clarity. We split data into
training (TR) and testing (TE) folds, and denote the training
iteration i and the testing iteration j within the corresponding
Markov chains.
1752
Training burn-in Btr Training lag LTR Training lag Ltr
Training period Ttr
T
e
s
t
b
u
r
n
-
i
n
B
te
T
e
s
t
p
e
r
i
o
d
T
te
T
e
s
t
l
a
g
L
te
1 2
3 4
2
4
2
4
3 4
3 4
4
4
4 4
1
2
3
4
Samples used in Single Final (SF)
Samples used in Single Average (SA)
Samples used in Multiple Final (MF)
Samples used in Multiple Average (MA)
Training chain
Single test chains
sample i in
training chain
(learned model)
test chain i
sample j in 
test chain i 
(prediction S(i,j))
Discarded samples during training
Discarded samples during test
Selected samples during training
Selected samples during test
Figure 1: Illustration of training and test chains in MCMC, showing samples used in four prediction strategies studied
in this paper: Single Final (SF), Single Average (SA), Multiple Final (MF), and Multiple Average (MA).
learned model from training sample i, we generate a test
Markov chain of T
TE
samples to estimate the local latent
variables p(l
TE
|w
TE
,
?
g(i)) of test data. Each sample
j of test chain i provides a fully estimated local latent
variables
?
l
TE
(i, j) to make a prediction.
Figure 1 shows an overview. To reduce the ef-
fects of unconverged and autocorrelated samples, dur-
ing training we use a burn-in period of B
TR
and a
sample-lag of L
TR
iterations. We use T
TR
= {i | i ?
(B
TR
, T
TR
] ? (i ? B
TR
) mod L
TR
= 0} to denote the
set of indices of the selected models. Similarly, B
TE
and L
TE
are the test burn-in and sample-lag. The
set of indices of selected samples in test chains is
T
TE
= {j | j ? (B
TE
, T
TE
] ? (j ?B
TE
) mod L
TE
= 0}.
2.2 Averaging Strategies
We use S(i, j) to denote the prediction obtained from
sample j of the test chain i. We now discuss different
strategies to obtain the final prediction:
? Single Final (SF) uses the last sample of last test
chain to obtain the predicted value,
S
SF
= S(T
TR
, T
TE
). (1)
? Single Average (SA) averages over multiple sam-
ples in the last test chain
S
SA
=
1
|T
TE
|
?
j?T
TE
S(T
TR
, j). (2)
This is a common averaging strategy in which we
obtain a point estimate of the global latent variables
at the end of the training chain. Then, a single test
chain is generated on the test data and multiple sam-
ples of this test chain are averaged to obtain the final
prediction (Chang, 2012; Singh et al., 2012; Jiang et
al., 2012; Zhu et al., 2014).
? Multiple Final (MF) averages over the last sam-
ples of multiple test chains from multiple models
S
MF
=
1
|T
TR
|
?
i?T
TR
S(i, T
TE
). (3)
? Multiple Average (MA) averages over all samples
of multiple test chains for distinct models,
S
MA
=
1
|T
TR
|
1
|T
TE
|
?
i?T
TR
?
j?T
TE
S(i, j), (4)
3 Unsupervised Topic Models
We evaluate the predictive performance of the unsu-
pervised topic model LDA using different averaging
strategies in Section 2.
LDA: Proposed by Blei et al. in 2003, LDA posits that
each document d is a multinomial distribution ?
d
over
K topics, each of which is a multinomial distribution
?
k
over the vocabulary. LDA?s generative process is:
1. For each topic k ? [1,K]
(a) Draw word distribution ?
k
? Dir(?)
2. For each document d ? [1, D]
(a) Draw topic distribution ?
d
? Dir(?)
(b) For each word n ? [1, N
d
]
i. Draw topic z
d,n
? Mult(?
d
)
ii. Draw word w
d,n
? Mult(?
z
d,n
)
In LDA, the global latent variables are topics {?
k
}
K
k=1
and the local latent variables for each document d are
topic proportions ?
d
.
Train: During training, we use collapsed Gibbs sam-
pling to assign each token in the training data with a
topic (Steyvers and Griffiths, 2006). The probability of
1753
assigning token n of training document d to topic k is
p(z
TR
d,n
= k | z
TR
?d,n
,w
TR
?d,n
, w
TR
d,n
= v) ?
N
?d,n
TR,d,k
+ ?
N
?d,n
TR,d,?
+K?
?
N
?d,n
TR,k,v
+ ?
N
?d,n
TR,k,?
+ V ?
, (5)
where N
TR,d,k
is the number of tokens in the training
document d assigned to topic k, and N
TR,k,v
is the num-
ber of times word type v assigned to topic k. Marginal
counts are denoted by ?, and
?d,n
denotes the count
excluding the assignment of token n in document d.
At each training iteration i, we estimate the distribu-
tion over words
?
?
k
(i) of topic k as
?
?
k,v
(i) =
N
TR,k,v
(i) + ?
N
TR,k,?
(i) + V ?
(6)
where the counts N
TR,k,v
(i) and N
TR,k,?
(i) are taken at
training iteration i.
Test: Because we lack explicit topic annotations for
these data (c.f. Nguyen et al. (2012)), we use perplexity?
a widely-used metric to measure the predictive power
of topic models on held-old documents. To compute
perplexity, we follow the estimating ? method (Wal-
lach et al., 2009, Section 5.1) and evenly split each test
document d into w
TE
1
d
and w
TE
2
d
. We first run Gibbs
sampling on w
TE
1
d
to estimate the topic proportion
?
?
TE
d
of test document d. The probability of assigning topic k
to token n inw
TE
1
d
is p(z
TE
1
d,n
= k | z
TE
1
?d,n
,w
TE
1
,
?
?(i)) ?
N
?d,n
TE
1
,d,k
+ ?
N
?d,n
TE
1
,d,?
+K?
?
?
?
k,w
TE
1
d,n
(i)
(7)
whereN
TE
1
,d,k
is the number of tokens inw
TE
1
d
assigned
to topic k. At each iteration j in test chain i, we can
estimate the topic proportion vector
?
?
TE
d
(i, j) for test
document d as
?
?
TE
d,k
(i, j) =
N
TE
1
,d,k
(i, j) + ?
N
TE
1
,d,?
(i, j) +K?
(8)
where both the counts N
TE
1
,d,k
(i, j) and N
TE
1
,d,?
(i, j)
are taken using sample j of test chain i.
Prediction: Given
?
?
TE
d
(i, j) and
?
?(i) at sample j
of test chain i, we compute the predicted likeli-
hood for each unseen token w
TE
2
d,n
as S(i, j) ?
p(w
TE
2
d,n
|
?
?
TE
d
(i, j),
?
?(i)) =
?
K
k=1
?
?
TE
d,k
(i, j) ?
?
?
k,w
TE
2
d,n
(i).
Using different strategies described in Section 2,
we obtain the final predicted likelihood for each un-
seen token p(w
TE
2
d,n
|
?
?
TE
d
,
?
?) and compute the perplex-
ity as exp
(
?(
?
d
?
n
log(p(w
TE
2
d,n
|
?
?
TE
d
,
?
?)))/N
TE
2
)
where N
TE
2
is the number of tokens in w
TE
2
.
Setup: We use three Internet review datasets in our
experiment. For all datasets, we preprocess by tokeniz-
ing, removing stopwords, stemming, adding bigrams to
l
l
l l l l l l l l
l
l
l l l l l l l l
l
l
l l l l l l l l
Restaurant Reviews
Movie Reviews
Hotel Reviews
1160
1200
1240
19502000
20502100
2150
750
775
800
600 700 800 900 1000
600 700 800 900 1000
600 700 800 900 1000Number of training iterations
Perp
lexity
lMultiple?Average Multiple?Final Single?Average Single?Final
Figure 2: Perplexity of LDA using different averaging
strategies with different number of training iterations
T
TR
. Perplexity generally decreases with additional
training iterations, but the drop is more pronounced
with multiple test chains.
the vocabulary, and we filter using TF-IDF to obtain a
vocabulary of 10,000 words.
3
The three datasets are:
? HOTEL: 240,060 reviews of hotels from TripAdvi-
sor (Wang et al., 2010).
? RESTAURANT: 25,459 reviews of restaurants from
Yelp (Jo and Oh, 2011).
? MOVIE: 5,006 reviews of movies from Rotten
Tomatoes (Pang and Lee, 2005)
We report cross-validated average performance over
five folds, and use K = 50 topics for all datasets. To
update the hyperparameters, we use slice sampling (Wal-
lach, 2008, p. 62).
4
Results: Figure 2 shows the perplexity of the four
averaging methods, computed with different number
of training iterations T
TR
. SA outperforms SF, showing
the benefits of averaging over multiple test samples
from a single test chain. However, both multiple chain
methods (MF and MA) significantly outperform these
two methods.
This result is consistent with Asuncion et al. (2009),
who run multiple training chains but a single test chain
for each training chain and average over them. This
is more costly since training chains are usually signif-
icantly longer than test chains. In addition, multiple
training chains are sensitive to their initialization.
3
To find bigrams, we begin with bigram candidates that
occur at least 10 times in the corpus and use a ?
2
test to filter
out those having a ?
2
value less than 5. We then treat selected
bigrams as single word types and add them to the vocabulary.
4
MCMC setup: T
TR
= 1, 000, B
TR
= 500, L
TR
= 50,
T
TE
= 100, B
TE
= 50 and L
TE
= 5.
1754
MSE
pR.squared
0.60
0.65
0.70
0.75
0.25
0.30
0.35
0.40
1000 2000 3000 4000 5000
1000 2000 3000 4000 5000Number of iterations
(a) Restaurant reviews MSE
pR.squared
9000
10000
11000
12000
13000
30000
31000
32000
33000
34000
1000 2000 3000 4000 5000
1000 2000 3000 4000 5000Number of iterations
(b) Movie reviews MSE
pR.squared
0.400
0.425
0.450
0.475
0.500
0.500
0.525
0.550
0.575
0.600
600 700 800 900 1000
600 700 800 900 1000Number of iterations
(c) Hotel reviews
Multiple AverageMultiple FinalSingle AverageSingle Final
Figure 3: Performance of SLDA using different averaging strategies computed at each training iteration.
4 Supervised Topic Models
We evaluate the performance of different prediction
methods using supervised latent Dirichlet allocation
(SLDA) (Blei and McAuliffe, 2007) for sentiment anal-
ysis: predicting review ratings given review text. Each
review text is the document w
d
and the metadata y
d
is
the associated rating.
SLDA: Going beyond LDA, SLDA captures the rela-
tionship between latent topics and metadata by mod-
eling each document?s continuous response variable
using a normal linear model, whose covariates are
the document?s empirical distribution of topics: y
d
?
N (?
T
?
z
d
, ?) where ? is the regression parameter vec-
tor and
?
z
d
is the empirical distribution over topics of
document d. The generative process of SLDA is:
1. For each topic k ? [1,K]
(a) Draw word distribution ?
k
? Dir(?)
(b) Draw parameter ?
k
? N (?, ?)
2. For each document d ? [1, D]
(a) Draw topic distribution ?
d
? Dir(?)
(b) For each word n ? [1, N
d
]
i. Draw topic z
d,n
? Mult(?
d
)
ii. Draw word w
d,n
? Mult(?
z
d,n
)
(c) Draw response y
d
? N (?
T
?
z
d
, ?) where
z?
d,k
=
1
N
d
?
N
d
n=1
I [z
d,n
= k]
where I [x] = 1 if x is true, and 0 otherwise.
In SLDA, in addition to the K multinomials {?
k
}
K
k=1
,
the global latent variables also contain the regression
parameter ?
k
for each topic k. The local latent variables
of SLDA resembles LDA?s: the topic proportion vector
?
d
for each document d.
Train: For posterior inference during training, follow-
ing Boyd-Graber and Resnik (2010), we use stochastic
EM, which alternates between (1) a Gibbs sampling
step to assign a topic to each token, and (2) optimizing
the regression parameters. The probability of assigning
topic k to token n in the training document d is
p(z
TR
d,n
= k | z
TR
?d,n
,w
TR
?d,n
, w
TR
d,n
= v) ?
N (y
d
;?
d,n
, ?) ?
N
?d,n
TR,d,k
+ ?
N
?d,n
TR,d,?
+K?
?
N
?d,n
TR,k,v
+ ?
N
?d,n
TR,k,?
+ V ?
(9)
where ?
d,n
= (
?
K
k
?
=1
?
k
?
N
?d,n
TR,d,k
?
+ ?
k
)/N
TR,d
is the
mean of the Gaussian generating y
d
if z
TR
d,n
= k. Here,
N
TR,d,k
is the number of times topic k is assigned to
tokens in the training document d;N
TR,k,v
is the number
of times word type v is assigned to topic k; ? represents
marginal counts and
?d,n
indicates counts excluding the
assignment of token n in document d.
We optimize the regression parameters ? using L-
BFGS (Liu and Nocedal, 1989) via the likelihood
L(?) = ?
1
2?
D?
d=1
(y
TR
d
??T ?zTR
d
)
2
?
1
2?
K?
k=1
(?
k
??)
2
(10)
At each iteration i in the training chain, the estimated
global latent variables include the a multinomial
?
?
k
(i)
and a regression parameter ??
k
(i) for each topic k.
Test: Like LDA, at test time we sample the topic as-
signments for all tokens in the test data
p(z
TE
d,n
= k | z
TE
?d,n
,w
TE
) ?
N
?d,n
TE,d,k
+ ?
N
?d,n
TE,d,?
+K?
?
?
?
k,w
TE
d,n
(11)
Prediction: The predicted value S(i, j) in this case is
the estimated value of the metadata review rating
S(i, j) ? y?
TE
d
(i, j) =
?
?(i)
T
z?
TE
d
(i, j), (12)
where the empirical topic distribution of test document d
is z?
TE
d,k
(i, j) ?
1
N
TE,d
?
N
TE,d
n=1
I
[
z
TE
d,n
(i, j) = k
]
.
1755
MSE
pR?squared
0.60
0.65
0.70
0.30
0.35
0.40
50 100 150 200
50 100 150 200Number of Topics
(a) Restaurant reviews MSE
pR?squared
0.60
0.70
0.80
0.90
0.00
0.10
0.20
0.30
0.40
40 60 80
40 60 80Number of Topics
(a) Restaurant reviews MSE
pR?squared
0.40
0.42
0.44
0.46
0.48
0.52
0.54
0.56
0.58
0.60
50 100 150 200
50 100 150 200Number of Topics
(a) Restaurant reviews
MLRSLDA?MASLDA?MFSLDA?SASLDA?SFSVR
Figure 4: Performance of SLDA using different averaging strategies computed at the final training iteration T
TR
,
compared with two baselines MLR and SVR. Methods using multiple test chains (MF and MA) perform as well as or
better than the two baselines, whereas methods using a single test chain (SF and SA) perform significantly worse.
Experimental setup: We use the same data as in Sec-
tion 3. For all datasets, the metadata are the review
rating, ranging from 1 to 5 stars, which is standard-
ized using z-normalization. We use two evaluation
metrics: mean squared error (MSE) and predictive R-
squared (Blei and McAuliffe, 2007).
For comparison, we consider two baselines: (1) multi-
ple linear regression (MLR), which models the metadata
as a linear function of the features, and (2) support vec-
tor regression (Joachims, 1999, SVR). Both baselines
use the normalized frequencies of unigrams and bigrams
as features. As in the unsupervised case, we report av-
erage performance over five cross-validated folds. For
all models, we use a development set to tune their pa-
rameter(s) and use the set of parameters that gives best
results on the development data at test.
5
Results: Figure 3 shows SLDA prediction results with
different averaging strategies, computed at different
training iterations.
6
Consistent with the unsupervised
results in Section 3, SA outperforms SF, but both are
outperformed significantly by the two methods using
multiple test chains (MF and MA).
We also compare the performance of the four pre-
diction methods obtained at the final iteration T
TR
of
the training chain with the two baselines. The results in
Figure 4 show that the two baselines (MLR and SVR) out-
perform significantly the SLDA using only a single test
5
For MLR we use a Gaussian prior N (0, 1/?) with ? =
a ? 10
b
where a ? [1, 9] and b ? [1, 4]; for SVR, we use
SVM
light
(Joachims, 1999) and vary C ? [1, 50], which
trades off between training error and margin; for SLDA, we fix
? = 10 and vary ? ? {0.1, 0.5, 1.0, 1.5, 2.0}, which trades
off between the likelihood of words and response variable.
6
MCMC setup: T
TR
= 5, 000 for RESTAURANT and
MOVIE and 1, 000 for HOTEL; for all datasets B
TR
= 500,
L
TR
= 50, T
TE
= 100, B
TE
= 20 and L
TE
= 5.
chains (SF and SA). Methods using multiple test chains
(MF and MA), on the other hand, match the baseline
7
(HOTEL) or do better (RESTAURANT and MOVIE).
5 Discussion and Conclusion
MCMC relies on averaging multiple samples to approxi-
mate target densities. When used for prediction, MCMC
needs to generate and average over both training sam-
ples to learn from training data and test samples to make
prediction. We have shown that simple averaging?not
more aggressive, ad hoc approximations like taking the
final sample (either training or test)?is not just a ques-
tion of theoretical aesthetics, but an important factor in
obtaining good prediction performance.
Compared with SVR and MLR baselines, SLDA using
multiple test chains (MF and MA) performs as well as
or better, while SLDA using a single test chain (SF and
SA) falters. This simple experimental setup choice can
determine whether a model improves over reasonable
baselines. In addition, better prediction with shorter
training is possible with multiple test chains. Thus, we
conclude that averaging using multiple chains produces
above-average results.
Acknowledgments
We thank Jonathan Chang, Ke Zhai and Mohit Iyyer for
helpful discussions, and thank the anonymous reviewers
for insightful comments. This research was supported
in part by NSF under grant #1211153 (Resnik) and
#1018625 (Boyd-Graber and Resnik). Any opinions,
findings, conclusions, or recommendations expressed
here are those of the authors and do not necessarily
reflect the view of the sponsor.
7
This gap is because SLDA has not converged after 1,000
training iterations (Figure 3).
1756
References
Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and
Michael I. Jordan. 2003. An introduction to MCMC for
machine learning. Machine Learning, 50(1-2):5?43.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference for
topic models. In UAI.
David M. Blei and Jon D. McAuliffe. 2007. Supervised topic
models. In NIPS.
David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent
Dirichlet allocation. JMLR, 3.
David M. Blei. 2012. Probabilistic topic models. Commun.
ACM, 55(4):77?84, April.
David M. Blei. 2014. Build, compute, critique, repeat: Data
analysis with latent variable models. Annual Review of
Statistics and Its Application, 1(1):203?232.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sen-
timent analysis across languages: Multilingual supervised
latent Dirichlet allocation. In EMNLP.
Jonathan Chang. 2012. lda: Collapsed Gibbs sampling meth-
ods for topic models. http://cran.r-project.
org/web/packages/lda/index.html. [Online;
accessed 02-June-2014].
Qixia Jiang, Jun Zhu, Maosong Sun, and Eric P. Xing. 2012.
Monte Carlo methods for maximum margin supervised
topic models. In NIPS.
Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment
unification model for online review analysis. In WSDM.
Thorsten Joachims. 1999. Making large-scale SVM learning
practical. In Advances in Kernel Methods - Support Vector
Learning, chapter 11. Cambridge, MA.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008.
DiscLDA: Discriminative learning for dimensionality re-
duction and classification. In NIPS.
D. Liu and J. Nocedal. 1989. On the limited memory BFGS
method for large scale optimization. Math. Prog.
Radford M. Neal. 1993. Probabilistic inference using Markov
chain Monte Carlo methods. Technical Report CRG-TR-
93-1, University of Toronto.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2012. SITS: A hierarchical nonparametric model using
speaker identity for topic segmentation in multiparty con-
versations. In ACL.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2013. Lexical and hierarchical topic regression. In Neural
Information Processing Systems.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect to
rating scales. In ACL.
Daniel Ramage, David Hall, Ramesh Nallapati, and Christo-
pher Manning. 2009. Labeled LDA: A supervised topic
model for credit attribution in multi-labeled corpora. In
EMNLP.
Daniel Ramage, Christopher D. Manning, and Susan Dumais.
2011. Partially labeled topic models for interpretable text
mining. In KDD, pages 457?465.
Philip Resnik and Eric Hardisty. 2010. Gibbs
sampling for the uninitiated. Technical Report
UMIACS-TR-2010-04, University of Maryland.
http://drum.lib.umd.edu//handle/1903/10058.
Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for authors
and documents. In UAI.
Sameer Singh, Michael Wick, and Andrew McCallum. 2012.
Monte Carlo MCMC: Efficient inference by approximate
sampling. In EMNLP, pages 1104?1113.
Mark Steyvers and Tom Griffiths. 2006. Probabilistic topic
models. In T. Landauer, D. Mcnamara, S. Dennis, and
W. Kintsch, editors, Latent Semantic Analysis: A Road to
Meaning. Laurence Erlbaum.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic models.
In Leon Bottou and Michael Littman, editors, ICML.
Hanna M Wallach. 2008. Structured Topic Models for Lan-
guage. Ph.D. thesis, University of Cambridge.
Chong Wang, David Blei, and Li Fei-Fei. 2009. Simultaneous
image classification and annotation. In CVPR.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. La-
tent aspect rating analysis on review text data: A rating
regression approach. In SIGKDD, pages 783?792.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. MedLDA:
maximum margin supervised topic models for regression
and classification. In ICML.
Jun Zhu, Ning Chen, Hugh Perkins, and Bo Zhang. 2014.
Gibbs max-margin topic models with data augmentation.
Journal of Machine Learning Research, 15:1073?1110.
1757
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 667?676,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Grammatical structures for word-level sentiment detection
Asad B. Sayeed
MMCI Cluster of Excellence
Saarland University
66123 Saarbru?cken, Germany
asayeed@coli.uni-sb.de
Jordan Boyd-Graber,
Bryan Rusk, Amy Weinberg
{iSchool / UMIACS, Dept. of CS, CASL}
University of Maryland
College Park, MD 20742 USA
{jbg@umiacs,brusk@,
aweinberg@casl}.umd.edu
Abstract
Existing work in fine-grained sentiment anal-
ysis focuses on sentences and phrases but ig-
nores the contribution of individual words and
their grammatical connections. This is because
of a lack of both (1) annotated data at the word
level and (2) algorithms that can leverage syn-
tactic information in a principled way. We ad-
dress the first need by annotating articles from
the information technology business press via
crowdsourcing to provide training and testing
data. To address the second need, we propose
a suffix-tree data structure to represent syntac-
tic relationships between opinion targets and
words in a sentence that are opinion-bearing.
We show that a factor graph derived from this
data structure acquires these relationships with
a small number of word-level features. We
demonstrate that our supervised model per-
forms better than baselines that ignore syntac-
tic features and constraints.
1 Introduction
The terms ?sentiment analysis? and ?opinion mining?
cover a wide body of research on and development of
systems that can automatically infer emotional states
from text (after Pang and Lee (2008) we use the two
names interchangeably). Sentiment analysis plays a
large role in business, politics, and is itself a vibrant
research area (Bollen et al, 2010).
Effective sentiment analysis for texts such as
newswire depends on the ability to extract who
(source) is saying what (target). Fine-grained sen-
timent analysis requires identifying the sources and
targets directly relevant to sentiment bearing expres-
sions (Ruppenhofer et al, 2008). For example, con-
sider the following sentence from a major informa-
tion technology (IT) business journal:
Lloyd Hession, chief security officer at BT
Radianz in New York, said that virtualiza-
tion also opens up a slew of potential net-
work access control issues.
There are three entities in the sentence that have the
capacity to express an opinion: Lloyd Hession, BT
Radianz, and New York. These are potential opinion
sources. There are also a number of mentioned con-
cepts that could serve as the topic of an opinion in
the sentence, or target. These include all the sources,
but also ?virtualization?, ?network access control?,
?network?, and so on.
The challenging task is to discriminate between
these mentions and choose the ones that are rele-
vant to the user. Furthermore, such a system must
also indicate the content of the opinion itself. This
means that we are actually searching for all triples
{source, target, opinion} in this sentence (Kim and
Hovy, 2006) and throughout each document in the
corpus. In this case, we want to identify that Lloyd
Hession is the source of an opinion, ?slew of network
issues,? about a target, virtualization. Providing such
fine-grained annotations would enrich information
extraction, question answering, and corpus explo-
ration applications by letting users see who is saying
what with what opinion (Wilson et al, 2005; Stoy-
anov and Cardie, 2006).
We motivate the need for a grammatically-focused
approach to fine-grained opinion mining and situate it
667
within the context of existing work in Section 2. We
propose a supervised technique for learning opinion-
target relations from dependency graphs in a way that
preserves syntactic coherence and semantic compo-
sitionality. In addition to being theoretically sound
? a lacuna identified in many sentiment systems1
? such approaches improve downstream sentiment
tasks (Moilanen and Pulman, 2007).
There are multiple types of downstream tasks that
potentially require the retrieval of {source, target,
opinion} relations on a sentence-by-sentence basis.
An increasingly significant application area is in the
use of large corpora in social science. This area of
research requires the exploration and aggregation of
data about the relationships between discourses, orga-
nizations, and people. For example, the IT business
press data that we use in this work belongs to a larger
research program (Tsui et al, 2009; Sayeed et al,
2010) of exploring industry opinion leadership. IT
business press text is one type of text in which many
entities and opinions can appear intermingled with
one another in a small amount of text.
Another application for fine-grained sentiment re-
lation retrieval of this type is paraphrasing, where
attribution of which opinion belongs to which entities
may be important for producing useful and accurate
output, since source and target identification errors
can change the entire meaning of an output text.
Unlike previous approaches that ignore syntax, we
use a sentence?s syntactic structure to build a proba-
bilistic model that encodes whether a word is opinion
bearing as a latent variable. We build a data structure
we call a ?syntactic relatedness trie? (Section 3) that
serves as the skeleton for a graphical model over the
sentiment relevance of words (Section 4). This ap-
proach allows us to learn features that predict opinion
bearing constructions from grammatical structures.
Because of a dearth of resources for this fine-grained
task, we also develop new crowdsourcing techniques
for labeling word-level, syntactically informed sen-
1Alm (2011) recently argued that work on sentiment anal-
ysis needs to de-emphasize the goal of building systems that
are ?high-performing? by traditional measures, because the field
risks sacrificing ?opportunities that may lead to a more thorough
understanding of language uses and users? in relation to subjec-
tive phenomena. The work we present in this paper therefore
focuses on extracting meaningful features as an investment in
future work that directly improves retrieval performance.
timent (Section 5). We use inference techniques to
uncover grammatical patterns that connect opinion-
expressing words and target entities (Section 6) per-
forming better than using syntactically uninformed
methods.
2 Background and existing work
We call opinion mining ?fine-grained? when it re-
trieves many different {source, target, opinion}
triples per document. This is particularly challenging
when there are multiple triples even within a sen-
tence. There is considerable work on identifying the
source of an opinion. However, it is much harder
to find obvious features that tell us whether ?virtual-
ization? is the target of an opinion. The most recent
target identification techniques use machine learning
to determine the presence of a target from known
opinionated language (Jakob and Gurevych, 2010).
Even when targets are identified we must decide if
an opinion is expressed, since not all target mentions
will necessarily be accompanied by opinion expres-
sions. Returning to the first example sentence, we
could say that the negative opinion about virtualiza-
tion is expressed by the words ?slew? and ?issues?.
A system that could automatically make this dis-
covery must draw on grammatical relationships be-
tween targets and the opinion bearing words. Parsers
reveal these relationships, but the relationships are
often indirect. The variability of language prevents
a complete enumeration of all intervening items that
make the relationships indirect, but examples include
negation and intensifiers, which change opinion, and
sentiment-neutral words, which fill syntactic or stylis-
tic needs. In this paper, we cope with the variability
of expression by using supervised machine learning
to generalize across observations and learn which fea-
tures best enable us to identify opinionated language.
Existing work in this area often uses semantic
frames and role labeling (Kim and Hovy, 2006; Choi
et al, 2006), but resources typically used in these
tasks (e.g. FrameNet) are not exhaustive. More gen-
eral approaches (Ruppenhofer et al, 2008) describe
semantic and discourse contexts of opinion sources
and targets cannot recognize them.
When techniques do identify targets via syntax,
they often only use grammar as a feature in an oth-
erwise syntax-agnostic model. Some work of this
668
nature merely identifies targets without providing the
syntactic evidence necessary to find domain-relevant
opinionated language (Jakob and Gurevych, 2010),
relying on lists of opinion keywords. There is also
work (Qiu et al, 2011) that uses predefined heuristics
over dependency parses to identify both targets and
opinion keywords but does not acquire new syntactic
heuristics. Other work (Nakagawa et al, 2010) is sim-
ilar to ours in that it uses factor graph modeling over
a dependency parse formalism, but it assumes that
opinionated language is known a priori and focuses
on polarity classification, while our work tackles the
more fundamental problem of identifying the opin-
ionated language itself.
Little work has been done to perform target and
opinion-expression extraction jointly, especially in a
way that extracts features for downstream processing.
This dearth persists despite evidence that such infor-
mation improves sentiment analysis (Moilanen and
Pulman, 2007).
An advantage of our proposed approach is that we
can use dependency paths in order to capture situa-
tions where the relations are non-compositional or
semantically motivated. In Section 5, we describe a
data set that has the additional property that opinion
is expressed in ways that require external pragmatic
knowledge of the domain. An advantage of arbi-
trary, non-local dependencies is that we can treat this
knowledge as part of the model we learn via long-
distance chains, which can capture pragmatics.
3 Syntactic relatedness tries
We now describe how we build the syntactic related-
ness trie (SRT) that forms the scaffolding for the prob-
abilistic models needed to identify sentiment-bearing
words via syntactic constraints extracted from a de-
pendency parse (Ku?bler et al, 2009).
We use the Stanford Parser (de Marneffe and Man-
ning, 2008) to produce a dependency graph and con-
sider the resulting undirected graph structure over
words. We construct a trie for each possible target
word in a sentence (it is possible for a sentence to
induce multiple tries if the sentence contains multi-
ple potential targets). Each trie encodes paths from
the possible target word to other words, and each
path represents a sequence of words connected by
undirected edges in the parse.
3.1 Encoding Dependencies in an SRT
SRTs enable us to encode the connections between
a single linguistic object of interest?in this appli-
cation, a possible target word?and a set of related
objects. SRTs are data structures consisting of nodes
and edges.
This description is very similar to the definition
of a dependency parse. The key difference is that
while a token only appears once as a node in a de-
pendency parse, an SRT can contain multiple nodes
that originate from the same token. This encodes the
possible connections between an opinion target and
opinion-conveying words.
The object of interest is the opinion target, defined
as the SRT root node (e.g. in Figure 1 ?policy? is a
known target, so it becomes the root of an SRT). Each
SRT edge corresponds to a grammatical relationship
between words and is labeled with that relationship.
We use the notation a
R
?? b to signify that node a has
the relationship (?role?) R with b. We say in this case
that node b is a descendent of node a with the role
R. The directed edges constitute a trie or suffix tree
that represents the fact that multiple paths may share
elements that all provide evidence for the relevance
of multiple leaves. 2
In the remainder of this section we describe the
necessary steps to create a training corpus for fine-
grained sentiment analysis. We provide an example
of how to create an SRT from a dependency parse and
then to attach latent variable assignments to an SRT
based on human annotations in a way that respects
syntactic constraints.
3.2 Using sentiment flow to label an SRT
Our goal is to discriminate between parts of the struc-
ture that are relevant to target-opinion word relations
and those that are not. We use the term sentiment
flow (shortened to ?flow? when space is an issue)
for relevant sentiment-bearing words in the SRT and
inert for the remainder of the sentence. We use the
term ?flow? because our invariant (section 3.3) con-
strains a sentiment flow in a SRT to be a contiguous
subgraph; this corresponds to linguistic intuitions
that, for example, in the sentence ?Linux with Wine
2The SRT will be used to create an undirected graphical
model; the notion of directedness refers to the traversal of paths
used to construct the SRT.
669
the dominant
role
the european climate protection
policy
has
benefits
our
economy
policy
policy
policy
protection
role
role
has
dominant
benefits
Dependency Parse
Paths for "policy" SRT
Figure 1: Dependency parse example. A dependency
parse (top) is used to generate a syntactic relatedness
trie for all possible targets of a sentiment-bearing
expression. For the target word ?policy?, there are a
number of paths (colors are consistent in paths to be
added to the SRT and in the dependency parse) that
connect it to other words; once extracted, these paths
will be inserted into a target-specific SRT.
is very usable?, {?Linux?, ?is?, ?very?} could not
be part of a sentiment flow without also including
{?usable?}.
Now that we have the structure of the model, we
need training data: sentences where sentiment bear-
ing words have been labeled. We describe how to go
from sentiment-labeled words to valid flows using
this sentence from the MPQA:
The dominant role of the European climate
protection policy has benefits for our econ-
omy.
In this sentence, the target word ?policy? is con-
nected to multiple sentiment-bearing words via paths
in the dependency parse (Figure 1). We can represent
these relationships using paths through the graph as
in Figure 2(a). (For clarity, we do not show some
paths.)
Suppose that an annotator decides that ?protec-
tion? and ?benefits? are directly expressing an opin-
ion about the policy, but ?dominant? is ambiguous (it
has some negative connotations). The nodes ?protec-
tion? and ?benefits? are a flow, and the ?dominant?
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
(a)
(b)
(c)
(d)
Figure 2: Labeled SRTs rooted on the target word
?policy?; green-filled nodes represent words that are
part of a sentiment flow and nodes with a red outline
represent inert nodes. (a) Initial labels for SRT (e.g.
as provided by annotators) (b) propagating labels to
yield a valid sentiment flow (c) a change of ?role? to
inert also renders its children inert (d) a change of
?dominant? to be part of a sentiment flow also causes
its parents to be part of a flow.
node is inert. However, there is considerable overlap
between the ?dominant? path and the ?benefits? path.
That is the motivation for combining them into a trie
structure and labeling them in such a way that the
path remains a flow until there is no path element that
leads to a flow leaf (Figure 2).
In other words, we want the path elements com-
mon to a flow path and an inert path to reinforce
sentiment flow. The transition from flow to inert is
learned by the classifier.
We enforce this requirement through the procedure
shown in Figure 2, which is equivalent to finding the
depth first search tree of the dependency graph and
applying the node-labeling scheme as above.
3.3 Invariant
Anything that follows a node with an inert label is
by definition not reachable from the root of the tree.
670
Consequently, any node that is part of a sentiment
flow that follows an inert node is not reachable along
a path and is actually inert itself. We specify this
directly as an invariant on the data structure:
Invariant: no node descending from a
node labeled inert can be labeled as a part
of a sentiment flow.
This specifies that flow labels spread out from the
root of the SRT. Our inference algorithm requires
that we be able to change the labels of nodes for
test data, thus we need to define invariant-respecting
operations for switching labels from flow to flow and
vice-versa. A flow label switched to inert will require
all the descendents of that particular node to switch
to inert as well as in figure 2(c). Similarly, an inert
label switched to flow will require all of the ancestors
of that node to switch to flow as in 2(d).
4 Encoding SRTs as a factor graph
In this section, we develop supervised machine learn-
ing tools to produce a labeled SRT from unlabeled,
held-out data in a single, unified model, without per-
mitting the sorts of inconsistencies that may be ad-
mitted by using a local classifier at each node.
4.1 Sampling labels
A factor graph (Kschischang et al, 1998) is a rep-
resentation of a joint probability distribution in the
form of a graph with two types of vertices: vari-
able vertices and factor vertices. Given a set of vari-
ables Z = {z1 . . . zn}, we connect them via factors
F = {f1 . . . fm}. Factors are functions that repre-
sent relationships, i.e. probabilistic dependencies,
among the variables; the product of all factors gives
the complete joint distribution p. Each factor fi can
take as input some corresponding subset of variables
Yi from Z. We can then write the relationship as
follows:
p(Z) ?
?m
k=1 fk(Yk)
Our goal is to discover the values for the variables
that best explain a dataset. While there are many
approaches for inference in statistical models, we
turn to MCMC methods (Neal, 1993) to discover the
underlying structure of the model. More specifically,
we seek a posterior distribution over latent variables
parent
node
child
1
child
2
child
3
h
g
f
Figure 3: Graphical model of SRT factors
that partition words in a sentence into flow and in-
ert groups; we estimate this posterior using Gibbs
sampling (Finkel et al, 2005).
The sampler requires an initial state that respects
the invariant. Our initial setting is produced by iterat-
ing through all labels in the SRT forest and randomly
setting them as either flow or inert with uniform
probability.
A Gibbs sampler samples new variable assign-
ments from the conditional distribution, treating the
variable assignments for all other variables fixed.
However, the assignment of a single node is highly
coupled with its neighbors, so a block sampler is used
to propose changes to groups nodes that respect the
flow labeling of the overall assignments. This was
implemented by changing the proposal distribution
used by the FACTORIE framework (McCallum et al,
2009).
We can thus represent a node and its contribution
to the overall score using the graph in Figure 3. This
graph contains the given node, its parent, and a vari-
able number of children. The factors that go into the
labeling decision for each node are thus constrained
to a small, computationally tractable space around
the given node. This graph contains three factors:
? g represents a function over features of the given
node itself, or ?node features.?
? f represents a function over a bigram of features
taken from the parent node and the given node,
or ?parent-node? features.
? h represents a function over a combination fea-
tures on the node and features of all its children,
or ?node-child? features.
We provide further details about these factors in the
next section.
671
In addition to the latent value associated with each
word, we associate each node with features derived
from the dependency parse: the word from the sen-
tence itself, the part-of-speech (POS) tag assigned
by the Stanford parser, and the label of the incoming
dependency edge. We treat the edge labels from the
original dependency parse as a feature of the node.
We can represent the set of possible observed lin-
guistic feature classes as the set of features ?. Fig-
ure 3 induces a scoring function with contributions
of each node to the score(label|node) =
?
???
(
f(parent?, node?|label)g(node?|label)
h(node?, child1?, . . . , childn?|label)
)
.
After assignments for the latent variables are sampled,
the weights for the factors (which when combined
create individual factors f that define the joint) must
be learned. This is accomplished via the sample-rank
algorithm (Wick et al, 2009).
5 Data source
Our goal is to identify opinion-bearing words and tar-
gets using supervised machine learning techniques.
Sentiment corpora with sub-sentential annotations,
such as the Multi-Perspective Question-Answering
(MPQA) corpus (Wilson and Wiebe, 2005) and the
J. D. Power and Associates (JDPA) blog post cor-
pus (Kessler et al, 2010), exist, but most of these
annotations are at a phrase level. Within a phrase,
however, some words may contribute more than oth-
ers to the statement of an opinion. We developed our
own annotations to discover such distinctions3. We
describe these briefly here; more information about
the development of the data source can be found in
Sayeed et al (2011).
5.1 Information technology business press
Our work is part of a larger collaboration with so-
cial scientists to study the diffusion of information
technology (IT) innovations through society by iden-
tifying opinion leaders and IT-relevant opinionated
language Rogers (2003). Thus, we focus on a col-
lection of articles from the IT professional maga-
zine, Information Week, from the years 1991 to 2008.
3To download the corpus, visit http://www.umiacs.
umd.edu/?asayeed/naacl12data/.
This consists of 33K articles including news bulletins
and opinion columns. Our IT concept target list (59
terms) comes from our application. Thus, we con-
struct a trie for each appearance of any of these possi-
ble target terms. We consider this list of target terms
to be complete, which allows us to focus on discover-
ing opinion-bearing text associated with these targets.
5.2 Crowdsourced annotation process
Our process for obtaining gold standard data involves
multiple levels of human annotation including on
crowdsourcing platforms Hsueh et al (2009).
There are 75K sentences with IT concept mentions,
only a minority of which express relevant opinions.
Hired undergraduate students searched a random se-
lection of these sentences and found 219 that contain
these opinions. We used cosine-similarity to rank the
remaining sentences against the 219.
We then needed to identify which of the words
contained an opinion. We excluded all words that
were common function words (e.g.,?the?, ?in?) but
left negations. We engineered tasks so that only
a randomly-selected five or six words appear high-
lighted for classification in order to limit annotator
boredom. We called this group a ?highlight group?.
The virtualization example would look like this:
Lloyd Hession, chief security officer at BT
Radianz in New York, said that virtual-
ization also opens up a slew of potential
network access control issues.
In the virtualization example, the worker would see
that virtualization is highlighted as the IT concept
target. Other words are highlighted as candidates that
the worker must classify as being opinion-relevant to
?virtualization?. Each highlight group corresponds to
a syntactic relatedness trie (Section 3).
A task was presented to a worker in the form of
a highlight group and some list boxes that represent
classes for the highlighted words: ?positive?, ?nega-
tive?, ?not opinion-relevant?, and ?ambiguous?. The
worker was required to drag each highlighted can-
didate word to exactly one of the boxes. As we are
not doing opinion polarity classification, the ?posi-
tive? and ?negative? boxes were intended as a form
of misdirection intended to avoid having the worker
consider what an opinion is; we treated this input as
a single ?opinion-relevant? category.
672
Three or more users annotated each highlight
group, and an aggregation scheme was applied af-
terwards: ?ambiguous? answers were rolled into ?not
opinion-relevant? and ties were dropped. Our qual-
ity control process involved filtering out workers
who performed poorly on a small subset of gold-
standard answers We annotated 30 evaluation units to
determine that our process retrieved opinion-relevant
words at 85% precision and 74% recall.
Annotators labeled 700 highlight groups for the
results in this paper. The total cost of this exercise
was approximately 250 USD, which includes the fees
charged by Amazon and CrowdFlower. These last
highlight groups were converted to SRTs and divided
into training and testing groups, 465 and 196 SRTs
respectively, with a small number lost to fatal errors
in the Stanford parser.
6 Experiments and discussion
During the training phase, we evaluate the quality
of a candidate labeling based on label accuracy. We
need to identify both flow nodes and inert nodes in
order to distinguish between relevant and irrelevant
subcomponents. We thus also employ precision and
recall as performance metrics.
An example of how this works can be seen by com-
paring figure 2(b) to figure 2(d), viewing the former
as the gold standard and the latter as a hypothetical
system output. If we run the evaluation over that
single SRT and treat flow as the positive class, we
find that 3 true positives, 1 false positive, 2 false neg-
atives, and no true negatives. There are 6 labels in
total. That yields 0.50 accuracy, 0.75 precision, 0.60
recall, and 0.67 F-measure.
We run every experiment (training a model and
testing on held-out data) 10 times and take the mean
average and range of all measures. F-measure is
calculated for each run and averaged post hoc.
6.1 Experiments
Our baseline system is the initial setting of the labels
for the sampler: uniform random assignment of flow
labels, respecting the invariant. This leads to a large
class imbalance in favor of inert as any switch to
inert converts all nodes downstream from the root to
convert to inert, while a switch to flow causes only
one ancestor branch to convert to flow.
Our next systems involve combinations of our SRT
factors with the observed linguistic features. All our
experiments include the factor g that pertains only to
the features of the node. Then we add factor f?the
parent-node ?bigram? features?and finally factor h,
the variable-length node-child features. We also ex-
periment with including and excluding combinations
of POS, role, and word features. We also explored
models that only made local decisions, ignoring the
consistency constraints over sentiment flows. Al-
though such models cannot be used in techniques
such as Nakagawa et al?s polarity classifier, they
function as a baseline and inform whether syntactic
constraints help performance.
We ran the inferencer for 200 iterations to train a
model with a particular factor-feature combination.
We use the learned model to predict the labels on
the held-out testing data by running the inference
algorithm (sampling labels only) for 50 iterations.
6.2 Discussion
We present a sampling of possible feature-factor com-
binations in table 1 in order to show trends in the
performance of the system.
Unsurprisingly, the invariant-respecting baseline
had very high precision but low recall. Simply includ-
ing the node-only g factor with all features increases
the recall while hurting precision. On removing word
features, recall increases without changing precision.
This suggests that some words in some SRTs are as-
sociated with flow labels in the training data, but not
as much in the testing data.
Including parent-node f features with the g fea-
tures yields higher precision and lower recall, sug-
gesting that parent-node word features support preci-
sion. Including all features on all factors (f , g, and h)
preserves most of the precision but improves recall.
Excluding h features increases recall slightly more
than it hurts precision. Excluding both word features
for all factors and role h features hurts all measures.
The accuracy measure, however, does show over-
all improvement with the inclusion of more feature-
factor combinations. In particular, the node-child h
factor does appear to have an effect on the perfor-
mance. The presence of some combinations of child
word, POS tags, and roles appear to provide some
indication of the flow labeling of some of the nodes.
The best models in terms of accuracy include all or
673
Experiment Features Invariant? Precision Recall F Accuracy
Baseline N/A
Yes 0.78 ? 0.05 0.06 ? 0.01 0.11 ? 0.02 0.51 ? 0.01
No 0.50 ? 0.00 0.49 ? 0.00 0.50 ? 0.00 0.50 ? 0.00
Node only
All
Yes 0.63 ? 0.10 0.34 ? 0.10 0.42 ? 0.07 0.54 ? 0.03
No 0.51 ? 0.00 0.88 ? 0.03 0.65 ? 0.01 0.51 ? 0.01
All but word
Yes 0.63 ? 0.16 0.40 ? 0.22 0.42 ? 0.19 0.53 ? 0.03
No 0.57 ? 0.04 0.56 ? 0.17 0.55 ? 0.07 0.55 ? 0.03
Parent, node
Parent: all but word
Yes 0.71 ? 0.06 0.21 ? 0.04 0.31 ? 0.05 0.55 ? 0.01
Node: all
All Yes 0.84 ? 0.07 0.11 ? 0.04 0.19 ? 0.06 0.53 ? 0.01
Full graph
Parent: all but word
Yes 0.59 ? 0.06 0.39 ? 0.11 0.46 ? 0.07 0.54 ? 0.03Node: all but word
Children: POS only
Parent: all
Yes 0.67 ? 0.05 0.39 ? 0.08 0.47 ? 0.06 0.59 ? 0.02Node: all
Children: all but word
All
Yes 0.70 ? 0.05 0.35 ? 0.08 0.46 ? 0.07 0.59 ? 0.02
No 0.70 ? 0.03 0.20 ? 0.05 0.36 ? 0.06 0.56 ? 0.01
Table 1: Performance using different feature combinations, including some without enforcing the invariant.
Mean averages and standard deviation for 10 runs.
almost all of the features.
Our non-invariant-respecting baseline unsurpris-
ingly was nearly 50% on all measures. Including the
node-only features dramatically increases recall, less
if we exclude word features. The word features ap-
pear to have an effect on recall just as in the invariant-
respecting case with node-only features. With all
features, precision is dramatically improved, but with
a large cost to recall. However, it underperforms
the equivalent invariant-respecting model in recall,
F-measure, and accuracy.
Though these invariant-violating models are un-
constrained in the way they label the graph, our
invariant-respecting models still outperform them.
A coherent path contains more information than an
incoherent one; it is important to find negating and
intensifying elements in context. Our SRT invariant
allows us to achieve better performance and will be
more useful to downstream tasks.
Finally, it appears that using more factors and lin-
guistic features promotes stability in performance
and decreases sensitivity to the initial setting.
6.3 Manual inspection
One pattern that prominently stood out in the testing
data with the full-graph model was the misclassifica-
tion of flow labels as inert in the vicinity of Stanford
dependency labels such as conj and. These kinds
of labels have high ?fertility?; the labels immediately
following them in the SRT could be a variety of types,
creating potential data sparsity issues.
This problem could be resolved by making some
features transparent to the learner. For example, if
node q has an incoming conj and dependency edge
label, then q?s parent could also be directly connected
to q?s children, as a conjunction should be linguisti-
cally transparent to the status of the children in the
sentiment flow.
There are many fewer incidents of inert labels be-
ing classified as flow. There are paths through an
SRT where a flow candidate word is the ancestor of
an inert candidate word from the set of crowdsourced
candidates. The model sometimes appears to ?over-
shoot? the flow candidate. Considering that recall is
already fairly low, attempts to address this problem
risks making the model too conservative. One poten-
tial solution is to prune or separate paths that contain
multiple flow candidates.
6.3.1 Paths found
We examined the labeling on the held-out testing
data of the best-performing model of the full graph
system with all linguistic features. For example, con-
sider the following highlight group:
But Microsoft?s informal approach may not be
enough as the number of blogs at the company
grows, especially since the line between ?personal?
Weblogs and those done as part of the job can be
hard to distinguish.
In this case, the Turkers decided that ?distinguish?
expressed a negative opinion about blogs, in the sense
674
that something that was difficult to distinguish was
a problem: the modifier ?hard? is what makes it
negative. The system found an entirely flow path that
connected these attributes into a single unit:
Blog:flow prepof????? number:flow nsubj????
grows:flow ccomp????? hard:flow xcomp?????
distinguish:flow
In this path, ?blog? and ?distinguish? are both con-
nected to one another by ?hard?, giving ?distinguish?
its negative spin. There are two non-local dependen-
cies in this example: xcomp, ccomp. Very often,
more than one unique path connects the concept to
the opinion candidate word.
7 Conclusions and future work
In this work, we have applied machine learning to
produce a robust modeling of syntactic structure for
an information extraction application. A solution to
the problem of modeling these structures requires the
development of new techniques that model complex
linguistic relationships in an application-dependent
way. We have shown that we can mine these relation-
ships without being overcome by the data-sparsity
issues that typically stymie learning over complex
linguistic structure.
The limitations on these techniques ultimately find
their root in the difficulty in modeling complex syn-
tactic structures that simultaneously exclude irrel-
evant portions of the structure while maintaining
connected relations. Our technique uses a structure-
labelling scheme that enforces connectedness. En-
forcing connected structure is not only necessary to
produce useful results but also to improve accuracy.
Further performance gains might be possible by en-
riching the feature set. For example, the POS tagset
used by the Stanford parser contains multiple verb
tags that represent different English tenses and num-
bers. For the purpose of sentiment relations, it is
possible that the differences between verb tags are
too small to matter and are causing data sparsity is-
sues. Thus, we could additional features that ?back
off? to general verb tags.
Acknowledgements
This paper is based upon work supported by the
US National Science Foundation under Grant IIS-
0729459. Additional support came from the Cluster
of Excellence ?Multimodal Computing and Innova-
tion?, Germany. Jordan Boyd-Graber is also sup-
ported by US National Science Foundation Grant
NSF grant #1018625 and the Army Research Labora-
tory through ARL Cooperative Agreement W911NF-
09-2-0072. Any opinions, findings, conclusions, or
recommendations expressed are the authors? and do
not necessarily reflect those of the sponsors.
References
Alm, C. O. (2011). Subjective natural language prob-
lems: Motivations, applications, characterizations,
and implications. In ACL (Short Papers).
Bollen, J., Mao, H., and Zeng, X.-J. (2010). Twit-
ter mood predicts the stock market. CoRR,
abs/1010.3003.
Choi, Y., Breck, E., and Cardie, C. (2006). Joint ex-
traction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
de Marneffe, M.-C. and Manning, C. D. (2008). The
stanford typed dependencies representation. In
CrossParser ?08: Coling 2008: Proceedings of
the workshop on Cross-Framework and Cross-
Domain Parser Evaluation, Morristown, NJ, USA.
Association for Computational Linguistics.
Finkel, J. R., Grenager, T., and Manning, C. (2005).
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Hsueh, P.-Y., Melville, P., and Sindhwani, V. (2009).
Data quality from crowdsourcing: a study of anno-
tation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learn-
ing for Natural Language Processing, HLT ?09,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jakob, N. and Gurevych, I. (2010). Extracting opin-
ion targets in a single and cross-domain setting
with conditional random fields. In EMNLP.
Kessler, J. S., Eckert, M., Clark, L., and Nicolov,
N. (2010). The 2010 ICWSM JDPA sentment
675
corpus for the automotive domain. In 4th Int?l
AAAI Conference on Weblogs and Social Media
Data Workshop Challenge (ICWSM-DWC 2010).
Kim, S.-M. and Hovy, E. (2006). Extracting opinions,
opinion holders, and topics expressed in online
news media text. In SST ?06: Proceedings of the
Workshop on Sentiment and Subjectivity in Text,
pages 1?8, Morristown, NJ, USA. Association for
Computational Linguistics.
Kschischang, F. R., Frey, B. J., and andrea Loeliger,
H. (1998). Factor graphs and the sum-product algo-
rithm. IEEE Transactions on Information Theory,
47:498?519.
Ku?bler, S., McDonald, R., and Nivre, J. (2009). De-
pendency parsing. Synthesis Lectures on Human
Language Technologies, 2(1).
McCallum, A., Schultz, K., and Singh, S. (2009).
Factorie: Probabilistic programming via impera-
tively defined factor graphs. In Neural Information
Processing Systems (NIPS).
Moilanen, K. and Pulman, S. (2007). Sentiment com-
position. In Proceedings of the Recent Advances in
Natural Language Processing International Con-
ference (RANLP-2007), Borovets, Bulgaria.
Nakagawa, T., Inui, K., and Kurohashi, S. (2010). De-
pendency tree-based sentiment classification using
crfs with hidden variables. In HLT-NAACL.
Neal, R. M. (1993). Probabilistic inference using
Markov chain Monte Carlo methods. Technical
Report CRG-TR-93-1, University of Toronto.
Pang, B. and Lee, L. (2008). Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2).
Qiu, G., Liu, B., Bu, J., and Chen, C. (2011). Opin-
ion word expansion and target extraction through
double propagation. Computational linguistics,
37(1):9?27.
Rogers, E. M. (2003). Diffusion of Innovations, 5th
Edition. Free Press.
Ruppenhofer, J., Somasundaran, S., and Wiebe, J.
(2008). Finding the sources and targets of sub-
jective expressions. In Calzolari, N., Choukri, K.,
Maegaard, B., Mariani, J., Odjik, J., Piperidis, S.,
and Tapias, D., editors, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco. European Lan-
guage Resources Association (ELRA).
Sayeed, A. B., Nguyen, H. C., Meyer, T. J., and
Weinberg, A. (2010). Expresses-an-opinion-about:
using corpus statistics in an information extraction
approach to opinion mining. In Proceedings of the
23rd International Conference on Computational
Linguistics, COLING ?10.
Sayeed, A. B., Rusk, B., Petrov, M., Nguyen, H. C.,
Meyer, T. J., and Weinberg, A. (2011). Crowd-
sourcing syntactic relatedness judgements for opin-
ion mining in the study of information technology
adoption. In Proceedings of the Association for
Computational Linguistics 2011 workshop on Lan-
guage Technology for Cultural Heritage, Social
Sciences, and the Humanities (LaTeCH). Associa-
tion for Computational Linguistics.
Stoyanov, V. and Cardie, C. (2006). Partially su-
pervised coreference resolution for opinion sum-
marization through structured rule learning. In
EMNLP ?06: Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 336?344, Morristown, NJ, USA.
Association for Computational Linguistics.
Tsui, C.-J., Wang, P., Fleischmann, K., Oard, D.,
and Sayeed, A. (2009). Understanding IT innova-
tions by computational analysis of discourse. In
International conference on information systems.
Wick, M., Rohanimanesh, K., Culotta, A., and Mccal-
lum, A. (2009). SampleRank: Learning preference
from atomic gradients. In NIPS WS on Advances
in Ranking.
Wilson, T. and Wiebe, J. (2005). Annotating attribu-
tions and private states. In CorpusAnno ?05: Pro-
ceedings of the Workshop on Frontiers in Corpus
Annotations II, Morristown, NJ, USA. Association
for Computational Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Rec-
ognizing contextual polarity in phrase-level senti-
ment analysis. In HLT/EMNLP.
676
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 36?39,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
Argviz: Interactive Visualization of Topic Dynamics in Multi-party
Conversations
Viet-An Nguyen
Dept. of Comp. Science
and UMIACS
University of Maryland
College Park, MD
vietan@cs.umd.edu
Yuening Hu
Dept. of Comp. Science
and UMIACS
University of Maryland
College Park, MD
ynhu@cs.umd.edu
Jordan Boyd-Graber
iSchool and
UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Department of Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
We introduce an efficient, interactive
framework?Argviz?for experts to analyze
the dynamic topical structure of multi-party
conversations. Users inject their needs,
expertise, and insights into models via iterative
topic refinement. The refined topics feed into a
segmentation model, whose outputs are shown
to users via multiple coordinated views.
1 Introduction
Uncovering the structure of conversations often re-
quires close reading by a human expert to be effective.
Political debates are an interesting example: political
scientists carefully analyze what gets said in debates
to explore how candidates shape the debate?s agenda
and frame issues or how answers subtly (or not so
subtly) shift the conversation by dodging the question
that was asked (Rogers and Norton, 2011).
Computational methods can contribute to the
analysis of topical dynamics, for example through
topic segmentation, dividing a conversation into
smaller, topically coherent segments (Purver, 2011);
or through identifying and summarizing the topics
under discussion (Blei et al, 2003; Blei, 2012). How-
ever, the topics uncovered by such methods can be
difficult for people to interpret (Chang et al, 2009),
and previous visualization frameworks for topic
models?e.g., ParallelTopics (Dou et al, 2011), Top-
icViz (Eisenstein et al, 2012), the Topical Guide,1 or
topic model visualization (Chaney and Blei, 2012)?
are not particularly well suited for linearly structured
conversations.
This paper describes Argviz, an integrated, inter-
active system for analyzing the topical dynamics of
1http://tg.byu.edu/
multi-party conversations. We bring together previ-
ous work on Interactive Topic Modeling (ITM) (Hu
et al, 2011), which allows users to efficiently inject
their needs, expertise, and insights into model build-
ing via iterative topic refinement, with Speaker Iden-
tity for Topic Segmentation (SITS) (Nguyen et al,
2012), a state-of-the-art model for topic segmenta-
tion and discovery of topic shifts in conversations.
Argviz?s interface allows users to quickly grasp the
topical flow of the conversation, discern when the
topic changes and by whom, and interactively visual-
ize the conversation?s details on demand.
2 System Overview
Our overall system consists of three steps: (1) data
preprocessing, (2) interactive topic modeling, and (3)
conversational topic segmentation and visualization.
Data preprocessing Preprocessing creates bags of
words that can be used by models. First, stopwords
and low frequency terms are removed from tokenized
text. This is then used as the data for topic modeling.
Interactive topic modeling The topic model-
ing process then discovers?through posterior
inference?the topics that best explain the conver-
sational turns. Each of the topics is a multinomial
distribution over words, which can be displayed to
users along with the association of turns (documents)
to these topics.
The result of topic modeling may be imperfect;
we give users an opportunity to refine and curate the
topics using Interactive Topic Modeling (ITM) (Hu
et al, 2011). The feedback from users is encoded
in the form of correlations: word types that should
co-occur in a topic or which should not. As these
correlations are incorporated into the model, the top-
ics learned by the model change and are presented
36
again to the user. The process repeats over multiple
iterations until the user is satisfied.
In addition, a simple but important part of the
interactive user experience is the ability for users to
label topics, i.e., to identify a ?congress? topic that
includes ?bill?, ?vote?, ?representative?, etc.
ITM is a web-based application with a HTML and
jQuery2 front end, connected via Ajax and JSON.
Topic segmentation After the user has built inter-
pretable topics, we use SITS?a hierarchical topic
model (Nguyen et al, 2012)?to jointly discover the
set of topics discussed in a given set of conversations
and how these topics change during each conversa-
tion. We use the output of ITM to initialize SITS3
with a high quality user-specific set of topics. The
outputs of SITS consist of (1) a set of topics, (2) a
distribution over topics for each turn, and (3) a proba-
bility associated with each turn indicating how likely
the topic of that turn has been shifted.
The outputs of SITS are displayed using Argviz
(Figure 2). Argviz is a web-based application, built
using Google Web Toolkit (GWT),4 which allows
users to visualize and manipulate SITS?s outputs en-
tirely in their browser after a single server request.
3 Argviz: Coordinated Conversational
Views
Given the limited screen of a web browser, Argviz
follows the multiple coordinated views approach
(Wang Baldonado et al, 2000; North and Shneider-
man, 2000) successfully used in Spotfire (Ahlberg,
1996), Improvise (Weaver, 2004), and SocialAc-
tion (Perer and Shneiderman, 2006). Argviz supports
three main coordinated views: transcript, overview
and topic.
Transcript occupies the prime real estate for a
close reading. It has a transcript panel and a speaker
panel. The transcript panel displays the original tran-
script. Each conversational turn is numbered and
color-coded by speaker. The color associated with
each speaker can be customized using the speaker
panel, which lists all the speakers.
2 http://jquery.com/
3Through per-word topic assignments
4 https://developers.google.com/web-toolkit/
Overview shows how topics gain and lose promi-
nence during the conversation. SITS?s outputs in-
clude a topic distribution and a topic shift probability
for each turn in the conversation. In Argviz, these are
represented using a heatmap and topic shift column.
In the heatmap, each turn-specific topic distribu-
tion is displayed by a heatmap row (Sopan et al,
2013). There is a cell for each topic, and the color
intensity of each cell is proportional to the probability
of the corresponding topic of a particular turn. Thus,
users can see the topical flow of the conversation
through the vertical change in cells? color intensities
as the conversation progresses. In addition, the topic
shift column shows the topic shift probability (in-
ferred by SITS) using color-coded bar charts, helping
users discern large topic changes in the conversation.
Each row is associated with a turn in the conversation;
clicking on one shifts the transcript view.
Topic displays the set of topics learned by SITS
(primed by ITM), with font-size proportional to the
words? topic probabilities. The selected topic panel
goes into more detail, with bar charts showing the
topic-word distribution. For example, in Figure 2, the
Foreign Affairs topic in panel E has high probability
words ?iraq?, ?afghanistan?, ?war?, etc. in panel F.
4 Demo: Detecting 2008 Debate Dodges
Visitors will have the opportunity to experiment with
the process of analyzing the topical dynamics of dif-
ferent multi-party conversations. Multiple datasets
will be preprocessed and set up for users to choose
and analyze. Examples of datasets that will be avail-
able include conversation transcripts from CNN?s
Crossfire program and debates from the 2008 and
2012 U.S. presidential campaigns. For this section,
we focus on examples from the 2008 campaign.
Interactive topic refinement After selecting a
dataset and a number of topics, the first thing a user
can do is to label topics. This will be used later in
Argviz and helps users build a mental model of what
the topics are. For instance, the user may rename the
second topic ?Foreign Policy?.
After inspecting the ?Foreign Policy? topic, the
user may notice the omission of Iran from the most
probable words in the topic. A user can remedy that
by adding the words ?Iran? and ?Iranians? into the
37
Figure 1: ITM user interface for refining a topic. Users can iteratively put words into different ?bins?, label topics, and
add new words to the topic. Users can also click on the provided links to show related turns for each topic in context.
Figure 2: The Argviz user interface consists of speaker panel (A), transcript panel (B), heatmap (C), topic shift column
(D), topic cloud panel (E), selected topic panel (F).
38
important words bin (Figure 1). Other bins include
ignored words for words that should be removed (e.g.,
?thing? and ?work? from this topic) from the topic
and trash (e.g., ?don?, which is a stop word).
The user can commit these changes by pressing the
Save changes button. The back end relearns given
the user?s feedback. Once users are satisfied with
the topic quality, they can click on the Finish button
to stop updating topics and start running the SITS
model, initialized using the final set of refined topics.
Visual analytic of conversations After SITS fin-
ishes (which takes just a few moments), users see the
dataset?s conversations in the Argviz interface. Fig-
ure 2 shows Argviz displaying the 2008 vice presiden-
tial debate between Senator Joe Biden and Governor
Sarah Palin, moderated by Gwen Ifill.
Users can start exploring the interface from any
of the views described in Section 3 to gain insight
about the conversation. For example, a user may
be interested in seeing how the ?Economy? is dis-
cussed in the debates. Clicking on a topic in the topic
cloud panel highlights that column in the heatmap.
The user can now see where the ?Economy? topic
is discussed in the debate. Next to the heatmap, the
topic shift column when debate participants changed
the topic. The red bar in turn 48 shows an interac-
tion where Governor Palin dodged a question on the
?bankruptcy bill? to discuss her ?record on energy?.
Clicking on this turn shows the interaction in the
transcript view, allowing a closer reading.
Users might also want to contrast the topics that
were discussed before and after the shift. This can
be easily done with the coordination between the
heatmap and the topic cloud panel. Clicking on a
cell in the heatmap will select the corresponding
topic to display in the selected topic panel. In our
example, the topic of the conversation was shifted
from ?Economy? to ?Energy? at turn 48.
5 Conclusion
Argviz is an efficient, interactive framework that al-
lows experts to analyze the dynamic topical structure
of multi-party conversations. We are engaged in col-
laborations with domain experts in political science
exploring the application of this framework to politi-
cal debates, and collaborators in social psychology
exploring the analysis of intra- and inter-cultural ne-
gotiation dialogues.
References
[Ahlberg, 1996] Ahlberg, C. (1996). Spotfire: an informa-
tion exploration environment. SIGMOD, 25(4):25?29.
[Blei, 2012] Blei, D. M. (2012). Probabilistic topic mod-
els. Communications of the ACM, 55(4):77?84.
[Blei et al, 2003] Blei, D. M., Ng, A., and Jordan, M.
(2003). Latent Dirichlet alocation. JMLR, 3.
[Chaney and Blei, 2012] Chaney, A. J.-B. and Blei, D. M.
(2012). Visualizing topic models. In ICWSM.
[Chang et al, 2009] Chang, J., Boyd-Graber, J., Wang, C.,
Gerrish, S., and Blei, D. M. (2009). Reading tea leaves:
How humans interpret topic models. In NIPS.
[Dou et al, 2011] Dou, W., Wang, X., Chang, R., and Rib-
arsky, W. (2011). ParallelTopics: A probabilistic ap-
proach to exploring document collections. In VAST.
[Eisenstein et al, 2012] Eisenstein, J., Chau, D. H., Kittur,
A., and Xing, E. (2012). TopicViz: interactive topic
exploration in document collections. In CHI.
[Hu et al, 2011] Hu, Y., Boyd-Graber, J., and Satinoff, B.
(2011). Interactive topic modeling. In ACL.
[Nguyen et al, 2012] Nguyen, V.-A., Boyd-Graber, J., and
Resnik, P. (2012). SITS: A hierarchical nonparametric
model using speaker identity for topic segmentation in
multiparty conversations. In ACL.
[North and Shneiderman, 2000] North, C. and Shneider-
man, B. (2000). Snap-together visualization: a user
interface for coordinating visualizations via relational
schemata. In AVI, pages 128?135.
[Perer and Shneiderman, 2006] Perer, A. and Shneider-
man, B. (2006). Balancing systematic and flexible
exploration of social networks. IEEE Transactions on
Visualization and Computer Graphics, 12(5):693?700.
[Purver, 2011] Purver, M. (2011). Topic segmentation. In
Spoken Language Understanding: Systems for Extract-
ing Semantic Information from Speech.
[Rogers and Norton, 2011] Rogers, T. and Norton, M. I.
(2011). The artful dodger: Answering the wrong ques-
tion the right way. Journal of Experimental Psychology:
Applied, 17(2):139?147.
[Sopan et al, 2013] Sopan, A., Freier, M., Taieb-Maimon,
M., Plaisant, C., Golbeck, J., and Shneiderman, B.
(2013). Exploring data distributions: Visual design
and evaluation. JHCI, 29(2):77?95.
[Wang Baldonado et al, 2000] Wang Baldonado, M. Q.,
Woodruff, A., and Kuchinsky, A. (2000). Guidelines
for using multiple views in information visualization.
In AVI, pages 110?119.
[Weaver, 2004] Weaver, C. (2004). Building highly-
coordinated visualizations in Improvise. In INFOVIS.
39
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 248?257,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Interactive Topic Modeling
Yuening Hu
Department of Computer Science
University of Maryland
ynhu@cs.umd.edu
Jordan Boyd-Graber
iSchool
University of Maryland
jbg@umiacs.umd.edu
Brianna Satinoff
Department of Computer Science
University of Maryland
bsonrisa@cs.umd.edu
Abstract
Topic models have been used extensively as a
tool for corpus exploration, and a cottage in-
dustry has developed to tweak topic models
to better encode human intuitions or to better
model data. However, creating such extensions
requires expertise in machine learning unavail-
able to potential end-users of topic modeling
software. In this work, we develop a frame-
work for allowing users to iteratively refine
the topics discovered by models such as la-
tent Dirichlet alocation (LDA) by adding con-
straints that enforce that sets of words must ap-
pear together in the same topic. We incorporate
these constraints interactively by selectively
removing elements in the state of a Markov
Chain used for inference; we investigate a va-
riety of methods for incorporating this infor-
mation and demonstrate that these interactively
added constraints improve topic usefulness for
simulated and actual user sessions.
1 Introduction
Probabilistic topic models, as exemplified by prob-
abilistic latent semantic indexing (Hofmann, 1999)
and latent Dirichlet alocation (LDA) (Blei et al,
2003) are unsupervised statistical techniques to dis-
cover the thematic topics that permeate a large cor-
pus of text documents. Topic models have had con-
siderable application beyond natural language pro-
cessing in computer vision (Rob et al, 2005), bi-
ology (Shringarpure and Xing, 2008), and psychol-
ogy (Landauer et al, 2006) in addition to their canon-
ical application to text.
For text, one of the few real-world applications
of topic models is corpus exploration. Unannotated,
noisy, and ever-growing corpora are the norm rather
than the exception, and topic models offer a way to
quickly get the gist a large corpus.1
1For examples, see Rexa http://rexa.info/, JSTOR
Contrary to the impression given by the tables
shown in topic modeling papers, topics discovered
by topic modeling don?t always make sense to os-
tensible end users. Part of the problem is that the
objective function of topic models doesn?t always cor-
relate with human judgements (Chang et al, 2009).
Another issue is that topic models ? with their bag-
of-words vision of the world ? simply lack the nec-
essary information to create the topics as end-users
expect.
There has been a thriving cottage industry adding
more and more information to topic models to cor-
rect these shortcomings; either by modeling perspec-
tive (Paul and Girju, 2010; Lin et al, 2006), syn-
tax (Wallach, 2006; Gruber et al, 2007), or author-
ship (Rosen-Zvi et al, 2004; Dietz et al, 2007). Sim-
ilarly, there has been an effort to inject human knowl-
edge into topic models (Boyd-Graber et al, 2007;
Andrzejewski et al, 2009; Petterson et al, 2010).
However, these are a priori fixes. They don?t help
a frustrated consumer of topic models staring at a
collection of topics that don?t make sense. In this
paper, we propose interactive topic modeling (ITM),
an in situ method for incorporating human knowl-
edge into topic models. In Section 2, we review prior
work on creating probabilistic models that incorpo-
rate human knowledge, which we extend in Section 3
to apply to ITM sessions. Section 4 discusses the
implementation of this process during the inference
process. Via a motivating example in Section 5, simu-
lated ITM sessions in Section 6, and a real interactive
test in Section 7, we demonstrate that our approach is
able to focus a user?s desires in a topic model, better
capture the key properties of a corpus, and capture
diverse interests from users on the web.
http://showcase.jstor.org/blei/, and the NIH
https://app.nihmaps.org/nih/.
248
2 Putting Knowledge in Topic Models
At a high level, topic models such as LDA take as
input a number of topics K and a corpus. As output,
a topic model discovers K distributions over words
? the namesake topics ? and associations between
documents and topics. In LDA both of these out-
puts are multinomial distributions; typically they are
presented to users in summary form by listing the
elements with highest probability. For an example
of topics discovered from a 20-topic model of New
York Times editorials, see Table 1.
When presented with poor topics learned from
data, users can offer a number of complaints:2
these documents should have similar topics but
don?t (Daume? III, 2009); this topic should have syn-
tactic coherence (Gruber et al, 2007; Boyd-Graber
and Blei, 2008); this topic doesn?t make any sense
at all (Newman et al, 2010); this topic shouldn?t be
associated with this document but is (Ramage et al,
2009); these words shouldn?t be the in same topic
but are (Andrzejewski et al, 2009); or these words
should be in the same topic but aren?t (Andrzejewski
et al, 2009).
Many of these complaints can be addressed by
using ?must-link? constraints on topics, retaining An-
drzejewski et als (2009) terminology borrowed from
the database literature. A ?must-link? constraint is a
group of words whose probability must be correlated
in the topic. For example, Figure 1 shows an example
constraint: {plant, factory}. After this constraint is
added, the probabilities of ?plant? and ?factory? in
each topic are likely to both be high or both be low.
It?s unlikely for ?plant? to have high probability in a
topic and ?factory? to have a low probability. In the
next section, we demonstrate how such constraints
can be built into a model and how they can even be
added while inference is underway.
In this paper, we view constraints as transitive; if
?plant? is in a constraint with ?factory? and ?factory?
is in a constraint with ?production,? then ?plant? is
in a constraint with ?production.? Making this as-
sumption can simplify inference slightly, which we
take advantage of in Section 3.1, but the real reason
for this assumption is because not doing so would
2Citations in this litany of complaints are offline solutions for
addressing the problem; the papers also give motivation why
such complaints might arise.
Constraints Prior Structure
{}
dogbark tree plant factory leash
?
?
?
?
?
?
{plant, factory}
dogbark tree
plant
factory
leash
?
? ?
?
2?
?
?
{plant, factory}
{dog, bark, leash}
dogbark
tree
plant factoryleash
?
?
?
?
2?
?
?
3?
Figure 1: How adding constraints (left) creates new topic
priors (right). The trees represent correlated distributions
(assuming ? >> ?). After the {plant, factory} constraint
is added, it is now highly unlikely for a topic drawn from
the distribution to have a high probability for ?plant? and
a low probability for ?factory? or vice versa. The bottom
panel adds an additional constraint, so now dog-related
words are also correlated. Notice that the two constraints
themselves are uncorrelated. It?s possible for both, either,
or none of ?bark? and ?plant? (for instance) to have high
probability in a topic.
introduce ambiguity over the path associated with an
observed token in the generative process. As long as
a word is either in a single constraint or in the general
vocabulary, there is only a single path. The details of
this issue are further discussed in Section 4.
3 Constraints Shape Topics
As discussed above, LDA views topics as distribu-
tions over words, and each document expresses an
admixture of these topics. For ?vanilla? LDA (no con-
straints), these are symmetric Dirichlet distributions.
A document is composed of a number of observed
words, which we call tokens to distinguish specific
observations from the more abstract word (type) as-
sociated with each token. Because LDA assumes
a document?s tokens are interchangeable, it treats
the document as a bag-of-words, ignoring potential
relations between words.
This problem with vanilla LDA can be solved by
encoding constraints, which will ?guide? different
words into the same topic. Constraints can be added
to vanilla LDA by replacing the multinomial distri-
bution over words for each topic with a collection of
249
tree-structured multinomial distributions drawn from
a prior as depicted in Figure 1. By encoding word
distributions as a tree, we can preserve conjugacy
and relatively simple inference while encouraging
correlations between related concepts (Boyd-Graber
et al, 2007; Andrzejewski et al, 2009; Boyd-Graber
and Resnik, 2010). Each topic has a top-level dis-
tribution over words and constraints, and each con-
straint in each topic has second-level distribution
over the words in the constraint. Critically, the per-
constraint distribution over words is engineered to be
non-sparse and close to uniform. The top level distri-
bution encodes which constraints (and unconstrained
words) to include; the lower-level distribution forces
the probabilities to be correlated for each of the con-
straints.
In LDA, a document?s token is produced in the
generative process by choosing a topic z and sam-
pling a word from the multinomial distribution ?z of
topic z. For a constrained topic, the process now can
take two steps. First, a first-level node in the tree is
selected from ?z . If that is an unconstrained word,
the word is emitted and the generative process for
that token is done. Otherwise, if the first level node
is constraint l, then choose a word to emit from the
constraint?s distribution over words piz,l.
More concretely, suppose for a corpus with M
documents we have a set of constraints ?. The prior
structure has B branches (one branch for each word
not in a constraint and one for each constraint). Then
the generative process for constrained LDA is:
1. For each topic i ? {1, . . .K}:
(a) draw a distribution over the B branches (words and
constraints) ?i ? Dir(~?), and
(b) for each constraint ?j ? ?, draw a distribution over
the words in the constraint pii,j ? Dir(?), where
pii,j is a distribution over the words in ?j
2. Then for each document d ? {1, . . .M}:
(a) first draw a distribution over topics ?d ? Dir(?),
(b) then for each token n ? {1, . . . Nd}:
i. choose a topic assignment zd,n ? Mult(?d),
and then
ii. choose either a constraint or word from
Mult(?zd,n):
A. if we chose a word, emit that word wd,n
B. otherwise if we chose a constraint index ld,n,
emit a word wd,n from the constraint?s dis-
tribution over words in topic zd,n: wd,n ?
Mult(pizd,n,ld,n).
In this model, ?, ?, and ? are Dirichlet hyperpa-
rameters set by the user; their role is explained below.
3.1 Gibbs Sampling for Topic Models
In topic modeling, collapsed Gibbs sampling (Grif-
fiths and Steyvers, 2004) is a standard procedure for
obtaining a Markov chain over the latent variables
in the model. Given certain technical conditions,
the stationary distribution of the Markov chain is
the posterior (Neal, 1993). Given M documents the
state of a Gibbs sampler for LDA consists of topic
assignments for each token in the corpus and is rep-
resented as Z = {z1,1 . . . z1,N1 , z2,1, . . . zM,NM }. In
each iteration, every token?s topic assignment zd,n
is resampled based on topic assignments for all the
tokens except for zd,n. (This subset of the state is
denoted Z?(d,n)). The sampling equation for zd,n is
p(zd,n = k|Z?(d,n), ?, ?) ?
Td,k + ?
Td,? +K?
Pk,wd,n + ?
Pk,? + V ?
(1)
where Td,k is the number of times topic k is used in
document d, Pk,wd,n is the number of times the type
wd,n is assigned to topic k, and ?, ? are the hyperpa-
rameters of the two Dirichlet distributions, and B is
the number of top-level branches (this is the vocab-
ulary size for vanilla LDA). When a dot replaces a
subscript of a count, it represents the marginal sum
over all possible topics or words, e.g. Td,? =
?
k Td,k.
The count statistics P and T provide summaries of
the state. Typically, these only change based on as-
signments of latent variables in the sampler; in Sec-
tion 4 we describe how changes in the model?s struc-
ture (in addition to the latent state) can be reflected
in these count statistics.
Contrasting with the above inference is the infer-
ence for a constrained model. (For a derivation, see
Boyd-Graber, Blei, and Zhu (2007) for the general
case or Andrzejewski, Zhu, and Craven (2009) for
the specific case of constraints.) In this case the
sampling equation for zd,n is changed to p(zd,n =
k|Z?(d,n), ?, ?, ?)
?
?
??
??
Td,k+?
Td,?+K?
Pk,wd,n+?
Pk,?+V ?
if ?l, wd,n 6? ?l
Td,k+?
Td,?+K?
Pk,l+Cl?
Pk,?+V ?
Wk,l,wd,n+?
Wk,l,?+Cl?
wd,n ? ?l
, (2)
where Pk,wd,n is the number of times the uncon-
strained word wd,n appears in topic k; Pk,l is the
250
number of times any word of constraint ?l appears in
topic k; Wk,l,wd,n is the number of times word wd,n
appears in constraint ?l in topic k; V is the vocabu-
lary size; Cl is the number of words in constraint ?l.
Note the differences between these two samplers for
constrained words; however, for unconstrained LDA
and for unconstrained words in constrained LDA, the
conditional probability is the same.
In order to make the constraints effective, we set
the constraint word-distribution hyperparameter ?
to be much larger than the hyperparameter for the
distribution over constraints and vocabulary ?. This
gives the constraints higher weight. Normally, esti-
mating hyperparameters is important for topic mod-
eling (Wallach et al, 2009). However, in ITM, sam-
pling hyperparameters often (but not always) undoes
the constraints (by making ? comparable to ?), so we
keep the hyperparameters fixed.
4 Interactively adding constraints
For a static model, inference in ITM is the same as
in previous models (Andrzejewski et al, 2009). In
this section, we detail how interactively changing
constraints can be accommodated in ITM, smoothly
transitioning from unconstrained LDA (n.b. Equa-
tion 1) to constrained LDA (n.b. Equation 2) with one
constraint, to constrained LDA with two constraints,
etc.
A central tool that we will use is the strategic unas-
signment of states, which we call ablation (distinct
from feature ablation in supervised learning). As
described in the previous section, a sampler stores
the topic assignment of each token. In the implemen-
tation of a Gibbs sampler, unassignment is done by
setting a token?s topic assignment to an invalid topic
(e.g. -1, as we use here) and decrementing any counts
associated with that word.
The constraints created by users implicitly signal
that words in constraints don?t belong in a given
topic. In other models, this input is sometimes used
to ?fix,? i.e. deterministically hold constant topic as-
signments (Ramage et al, 2009). Instead, we change
the underlying model, using the current topic assign-
ments as a starting position for a new Markov chain
with some states strategically unassigned. How much
of the existing topic assignments we use leads to four
different options, which are illustrated in Figure 2.
Previous New
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:-1, dog:-1, leash:-1 dog:-1]
[bark:-1, bark:-1, plant:-1, tree:-1]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:3]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:-1, dog:-1, leash:3 dog:-1]
[bark:-1, bark:-1, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:-1, dog:-1, leash:-1 dog:-1]
[bark:-1, bark:-1, plant:-1, tree:-1]
[tree:-1,play:-1,forest:-1,leash:-1]
None
Term
Doc
All
Figure 2: Four different strategies for state ablation after
the words ?dog? and ?bark? are added to the constraint
{?leash,? ?puppy?} to make the constraint {?dog,? ?bark,?
?leash,? ?puppy?}. The state is represented by showing the
current topic assignment after each word (e.g. ?leash? in
the first document has topic 3, while ?forest? in the third
document has topic 1). On the left are the assignments
before words were added to constraints, and on the right
are the ablated assignments. Unassigned words are given
the new topic assignment -1 and are highlighted in red.
All We could revoke all state assignments, essen-
tially starting the sampler from scratch. This does
not allow interactive refinement, as there is nothing
to enforce that the new topics will be in any way
consistent with the existing topics. Once the topic
assignments of all states are revoked, the counts for
T , P and W (as described in Section 3.1) will be
zero, retaining no information about the state the user
observed.
Doc Because topic models treat the document con-
text as exchangeable, a document is a natural context
for partial state ablation. Thus if a user adds a set of
words S to constraints, then we have reason to sus-
pect that all documents containing any one of S may
have incorrect topic assignments. This is reflected
in the state of the sampler by performing the UNAS-
SIGN (Algorithm 1) operation for each word in any
document containing a word added to a constraint.
Algorithm 1 UNASSIGN(d, n, wd,n, zd,n = k)
1: T : Td,k ? Td,k ? 1
2: If wd,n /? ?old,
P : Pk,wd,n ? Pk,wd,n ? 1
3: Else: suppose wd,n ? ?oldm ,
P : Pk,m ? Pk,m ? 1
W : Wk,m,wd,n ?Wk,m,wd,n ? 1
251
This is equivalent to the Gibbs2 sampler of Yao
et al (2009) for incorporating new documents in
a streaming context. Viewed in this light, a user
is using words to select documents that should be
treated as ?new? for this refined model.
Term Another option is to perform ablation only
on the topic assignments of tokens whose words have
added to a constraint. This applies the unassignment
operation (Algorithm 1) only to tokens whose corre-
sponding word appears in added constraints (i.e. a
subset of the Doc strategy). This makes it less likely
that other tokens in similar contexts will follow the
words explicitly included in the constraints to new
topic assignments.
None The final option is to move words into con-
straints but keep the topic assignments fixed. Thus,
P and W change, but not T , as described in Algo-
rithm 2.3 This is arguably the simplest option, and
in principle is sufficient, as the Markov chain should
find a stationary distribution regardless of the starting
position. In practice, however, this strategy is less
interactive, as users don?t feel that their constraints
are actually incorporated in the model, and inertia
can keep the chain from reflecting the constraints.
Algorithm 2 MOVE(d, n, wd,n, zd,n = k,?l)
1: If wd,n /? ?old,
P : Pk,wd,n ? Pk,wd,n ? 1, Pk,l ? Pk,l + 1
W : Wk,l,wd,n ?Wk,l,wd,n + 1
2: Else, suppose wd,n ? ?oldm ,
P : Pk,m ? Pk,m ? 1, Pk,l ? Pk,l + 1
W : Wk,m,wd,n ?Wk,m,wd,n ? 1
Wk,l,wd,n ?Wk,l,wd,n + 1
Regardless of what ablation scheme is used, after
the state of the Markov chain is altered, the next
step is to actually run inference forward, sampling
assignments for the unassigned tokens for the ?first?
time and changing the topic assignment of previously
assigned tokens. How many additional iterations are
3This assumes that there is only one possible path in the con-
straint tree that can generate a word; in other words, this as-
sumes that constraints are transitive, as discussed at the end of
Section 2. In the more general case, when words lack a unique
path in the constraint tree, an additional latent variable specifies
which possible paths in the constraint tree produced the word;
this would have to be sampled. All other updating strategies
are immune to this complication, as the assignments are left
unassigned.
required after adding constraints is a delicate tradeoff
between interactivity and effectiveness, which we
investigate further in the next sections.
5 Motivating Example
To examine the viability of ITM, we begin with a
qualitative demonstration that shows the potential
usefulness of ITM. For this task, we used a corpus
of about 2000 New York Times editorials from the
years 1987 to 1996. We started by finding 20 initial
topics with no constraints, as shown in Table 1 (left).
Notice that topics 1 and 20 both deal with Russia.
Topic 20 seems to be about the Soviet Union, with
topic 1 about the post-Soviet years. We wanted to
combine the two into a single topic, so we created a
constraint with all of the clearly Russian or Soviet
words (boris, communist, gorbachev, mikhail, russia,
russian, soviet, union, yeltsin ). Running inference
forward 100 iterations with the Doc ablation strat-
egy yields the topics in Table 1 (right). The two
Russia topics were combined into Topic 20. This
combination also pulled in other relevant words that
not near the top of either topic before: ?moscow?
and ?relations.? Topic 1 is now more about elections
in countries other than Russia. The other 18 topics
changed little.
While we combined the Russian topics, other re-
searchers analyzing large corpora might preserve the
Soviet vs. post-Soviet distinction but combine topics
about American government. ITM allows tuning for
specific tasks.
6 Simulation Experiment
Next, we consider a process for evaluating our ITM
using automatically derived constraints. These con-
straints are meant to simulate a user with a predefined
list of categories (e.g. reviewers for journal submis-
sions, e-mail folders, etc.). The categories grow more
and more specific during the session as the simulated
users add more constraint words.
To test the ability of ITM to discover relevant
subdivisions in a corpus, we use a dataset with pre-
defined, intrinsic labels and assess how well the dis-
covered latent topic structure can reproduce the cor-
pus?s inherent structure. Specifically, for a corpus
with M classes, we use the per-document topic dis-
tribution as a feature vector in a supervised classi-
252
Topic Words
1
election, yeltsin, russian, political, party, democratic, russia, presi-
dent, democracy, boris, country, south, years, month, government, vote,
since, leader, presidential, military
2
new, york, city, state, mayor, budget, giuliani, council, cuomo, gov,
plan, year, rudolph, dinkins, lead, need, governor, legislature, pataki,
david
3
nuclear, arms, weapon, defense, treaty, missile, world, unite, yet, soviet,
lead, secretary, would, control, korea, intelligence, test, nation, country,
testing
4
president, bush, administration, clinton, american, force, reagan, war,
unite, lead, economic, iraq, congress, america, iraqi, policy, aid, inter-
national, military, see
...
20
soviet, lead, gorbachev, union, west, mikhail, reform, change, europe,
leaders, poland, communist, know, old, right, human, washington,
western, bring, party
Topic Words
1
election, democratic, south, country, president, party, africa, lead, even,
democracy, leader, presidential, week, politics, minister, percent, voter,
last, month, years
2
new, york, city, state, mayor, budget, council, giuliani, gov, cuomo,
year, rudolph, dinkins, legislature, plan, david, governor, pataki, need,
cut
3 nuclear, arms, weapon, treaty, defense, war, missile, may, come, test,american, world, would, need, lead, get, join, yet, clinton, nation
4
president, administration, bush, clinton, war, unite, force, reagan, amer-
ican, america, make, nation, military, iraq, iraqi, troops, international,
country, yesterday, plan
...
20
soviet, union, economic, reform, yeltsin, russian, lead, russia, gor-
bachev, leaders, west, president, boris, moscow, europe, poland,
mikhail, communist, power, relations
Table 1: Five topics from a 20 topic topic model on the editorials from the New York times before adding a constraint
(left) and after (right). After the constraint was added, which encouraged Russian and Soviet terms to be in the same
topic, non-Russian terms gained increased prominence in Topic 1, and ?Moscow? (which was not part of the constraint)
appeared in Topic 20.
fier (Hall et al, 2009). The lower the classification
error rate, the better the model has captured the struc-
ture of the corpus.4
6.1 Generating automatic constraints
We used the 20 Newsgroups corpus, which contains
18846 documents divided into 20 constituent news-
groups. We use these newsgroups as ground-truth
labels.5
We simulate a user?s constraints by ranking words
in the training split by their information gain (IG).6
After ranking the top 200 words for each class
by IG, we delete words associated with multiple
labels to prevent constraints for different labels
from merging. The smallest class had 21 words
remaining after removing duplicates (due to high
4Our goal is to understand the phenomena of ITM, not classifica-
tion, so these classification results are well below state of the
art. However, adding interactively selected topics to the state
of the art features (tf-idf unigrams) gives a relative error reduc-
tion of 5.1%, while just adding topics from vanilla LDA gives
a relative error reduction of 1.1%. Both measurements were
obtained without tuning or weighting features, so presumably
better results are possible.
5http://people.csail.mit.edu/jrennie/20Newsgroups/
In preprocessing, we deleted short documents, leaving 15160
documents, including 9131 training documents and 6029 test
documents (default split). Tokenization, lemmatization, and
stopword removal was performed using the Natural Language
Toolkit (Loper and Bird, 2002). Topic modeling was performed
using the most frequent 5000 lemmas as the vocabulary.
6IG is computed by the Rainbow toolbox
http://www.cs.umass.edu/ mccallum/bow/rainbow/
overlaps of 125 words between ?talk.religion.misc?
and ?soc.religion.christian,? and 110 words between
?talk.religion.misc? and ?alt.atheism?), so the top 21
words for each class were the ingredients for our
simulated constraints. For example, for the class
?soc.religion.christian,? the 21 constraint words in-
clude ?catholic, scripture, resurrection, pope, sab-
bath, spiritual, pray, divine, doctrine, orthodox.? We
simulate a user?s ITM session by adding a word to
each of the 20 constraints until each of the constraints
has 21 words.
6.2 Simulation scheme
Starting with 100 base iterations, we perform suc-
cessive rounds of refinement. In each round a new
constraint is added corresponding to the newsgroup
labels. Next, we perform one of the strategies for
state ablation, add additional iterations of Gibbs sam-
pling, use the newly obtained topic distribution of
each document as the feature vector, and perform
classification on the test / train split. We do this for
21 rounds until each label has 21 constraint words.
The number of LDA topics is set to 20 to match the
number of newsgroups. The hyperparameters for all
experiments are ? = 0.1, ? = 0.01, and ? = 100.
At 100 iterations, the chain is clearly not con-
verged. However, we chose this number of iterations
because it more closely matches the likely use case as
users do not wait for convergence. Moreover, while
investigations showed that the patterns shown in Fig-
253
ure 4 were broadly consistent with larger numbers
of iterations, such configurations sometimes had too
much inertia to escape from local extrema. More iter-
ations make it harder for the constraints to influence
the topic assignment.
6.3 Investigating Ablation Strategies
First, we investigate which ablation strategy best al-
lows constraints to be incorporated. Figure 3 shows
the classification error of six different ablation strate-
gies based on the number of words in each constraint,
ranging from 0 to 21. Each is averaged over five dif-
ferent chains using 10 additional iterations of Gibbs
sampling per round (other numbers of iterations are
discussed in Section 6.4). The model runs forward 10
iterations after the first round, another 10 iterations
after the second round, etc. In general, as the number
of words per constraint increases, the error decreases
as models gain more information about the classes.
Strategy Null is the non-interactive baseline that
contains no constraints (vanilla LDA), but runs infer-
ence for a comparable number of rounds. All Initial
and All Full are non-interactive baselines with all
constraints known a priori. All Initial runs the model
for the only the initial number of iterations (100 it-
erations in this experiment), while All Full runs the
model for the total number of iterations added for the
interactive version. (That is, if there were 21 rounds
and each round of interactive modeling added 10 iter-
ations, All Full would have 210 iterations more than
All Initial).
While Null sees no constraints, it serves as an
upper baseline for the error rate (lower error being
better) but shows the effect of additional inference.
All Full is a lower baseline for the error rate since
it both sees the constraints at the beginning and also
runs for the maximum number of total iterations. All
Initial sees the constraints before the other ablation
techniques but it has fewer total iterations.
The Null strategy does not perform as well as
the interactive versions, especially with larger con-
straints. Both All Initial and All Full, however, show
a larger variance (as denoted by error bands around
the average trends) than the interactive schemes. This
can be viewed as akin to simulated annealing, as the
interactive search has more freedom to explore in
early rounds. As more constraint words are added
each round, the model is less free to explore.
Words per constraint
Error
0.380.40
0.420.44
0.460.48
0.50
0 5 10 15 20
StrategyAll FullAll InitialDocNoneNullTerm
Figure 3: Error rate (y-axis, lower is better) using different
ablation strategies as additional constraints are added (x-
axis). Null represents standard LDA, as the unconstrained
baseline. All Initial and All Full are non-interactive, con-
strained baselines. The results of None, Term, Doc are
more stable (as denoted by the error bars), and the error
rate is reduced gradually as more constraint words are
added.
The error rate of each interactive ablation strategy
is (as expected) between the lower and upper base-
lines. Generally, the constraints will influence not
only the topics of the constraint words, but also the
topics of the constraint words? context in the same
document. Doc ablation gives more freedom for the
constraints to overcome the inertia of the old topic
distribution and move towards a new one influenced
by the constraints.
6.4 How many iterations do users have to wait?
Figure 4 shows the effect of using different numbers
of Gibbs sampling iterations after changing a con-
straint. For each of the ablation strategies, we run
{10, 20, 30, 50, 100} additional Gibbs sampling iter-
ations. As expected, more iterations reduce error,
although improvements diminish beyond 100 itera-
tions. With more constraints, the impact of additional
iterations is lessened, as the model has more a priori
knowledge to draw upon.
For all numbers of additional iterations, while the
Null serves as the upper baseline on the error rate
in all cases, the Doc ablation clearly outperforms
the other ablation schemes, consistently yielding a
lower error rate. Thus, there is a benefit when the
model has a chance to relearn the document context
when constraints are added. The difference is even
larger with more iterations, suggesting Doc needs
more iterations to ?recover? from unassignment.
The luxury of having hundreds or thousands of
additional iterations for each constraint would be im-
254
Words per constraint
Err
or
0.40
0.42
0.44
0.46
0.48
0.50
 10
0 5 10 15 20
 20
0 5 10 15 20
 30
0 5 10 15 20
 50
0 5 10 15 20
100
0 5 10 15 20
Strategy
Doc
None
Null
Term
Figure 4: Classification accuracy by strategy and number of additional iterations. The Doc ablation strategy performs
best, suggesting that the document context is important for ablation constraints. While more iterations are better, there
is a tradeoff with interactivity.
practical. For even moderately sized datasets, even
one iteration per second can tax the patience of in-
dividuals who want to use the system interactively.
Based on these results and an ad hoc qualitative ex-
amination of the resulting topics, we found that 30
additional iterations of inference was acceptable; this
is used in later experiments.
7 Getting Humans in the Loop
To move beyond using simulated users adding the
same words regardless of what topics were discov-
ered by the model, we needed to expose the model
to human users. We solicited approximately 200
judgments from Mechanical Turk, a popular crowd-
sourcing platform that has been used to gather lin-
guistic annotations (Snow et al, 2008), measure topic
quality (Chang et al, 2009), and supplement tradi-
tional inference techniques for topic models (Chang,
2010). After presenting our interface for collecting
judgments, we examine the results from these ITM
sessions both quantitatively and qualitatively.
7.1 Interface for soliciting refinements
Figure 5 shows the interface used in the Mechanical
Turk tests. The left side of the screen shows the
current topics in a scrollable list, with the top 30
words displayed for each topic.
Users create constraints by clicking on words from
the topic word lists. The word lists use a color-coding
scheme to help the users keep track of which words
they are currently grouping into constraints. The right
side of the screen displays the existing constraints.
Users can click on icons to edit or delete each one.
The constraint currently being built is also shown.
Figure 5: Interface for Mechanical Turk experiments.
Users see the topics discovered by the model and select
words (by clicking on them) to build constraints to be
added to the model.
Clicking on a word will remove that word from the
current constraint.
As in Section 6, we can compute the classification
error for these users as they add words to constraints.
The best users, who seemed to understand the task
well, were able to decrease classification error. (Fig-
ure 6). The median user, however, had an error re-
duction indistinguishable from zero. Despite this, we
can examine the users? behavior to better understand
their goals and how they interact with the system.
7.2 Untrained users and ITM
Most of the large (10+ word) user-created constraints
corresponded to the themes of the individual news-
groups, which users were able to infer from the
discovered topics. Common constraint themes that
255
Round
Re
lat
ive
 E
rro
r
0.94
0.96
0.98
1.00
0 1 2 3 4
Best Session
 10 Topics 
 20 Topics 
 50 Topics 
 75 Topics 
Figure 6: The relative error rate (using round 0 as a base-
line) of the best Mechanical Turk user session for each of
the four numbers of topics. While the 10-topic model does
not provide enough flexibility to create good constraints,
the best users could clearly improve classification with
more topics.
matched specific newsgroups included religion, space
exploration, graphics, and encryption. Other com-
mon themes were broader than individual news-
groups (e.g. sports, government and computers). Oth-
ers matched sub-topics of a single newsgroup, such
as homosexuality, Israel or computer programming.
Some users created inscrutable constraints, like
(?better, people, right, take, things?) and (?fbi, let,
says?). They may have just clicked random words to
finish the task quickly. While subsequent users could
delete poor constraints, most chose not to. Because
we wanted to understand broader behavior we made
no effort to squelch such responses.
The two-word constraints illustrate an interesting
contrast. Some pairs are linked together in the corpus,
like (?jesus, christ?) and (?solar, sun?). With others,
like (?even, number?) and (?book, list?), the users
seem to be encouraging collocations to be in the
same topic. However, the collocations may not be in
any document in this corpus. Another user created a
constraint consisting of male first names. A topic did
emerge with these words, but the rest of the words
in that topic seemed random, as male first names are
not likely to co-occur in the same document.
Not all sensible constraints led to successful topic
changes. Many users grouped ?mac? and ?windows?
together, but they were almost never placed in the
same topic. The corpus includes separate newsgroups
for Macintosh and Windows hardware, and divergent
contexts of ?mac? and ?windows? overpowered the
prior distribution.
The constraint size ranged from one word to over
40. In general, the more words in the constraint,
the more likely it was to noticeably affect the topic
distribution. This observation makes sense given
our ablation method. A constraint with more words
will cause the topic assignments to be reset for more
documents.
8 Discussion
In this work, we introduced a means for end-users
to refine and improve the topics discovered by topic
models. ITM offers a paradigm for non-specialist
consumers of machine learning algorithms to refine
models to better reflect their interests and needs. We
demonstrated that even novice users are able to under-
stand and build constraints using a simple interface
and that their constraints can improve the model?s
ability to capture the latent structure of a corpus.
As presented here, the technique for incorporating
constraints is closely tied to inference with Gibbs
sampling. However, most inference techniques are
essentially optimization problems. As long as it is
possible to define a transition on the state space that
moves from one less-constrained model to another
more-constrained model, other inference procedures
can also be used.
We hope to engage these algorithms with more
sophisticated users than those on Mechanical Turk
to measure how these models can help them better
explore and understand large, uncurated data sets. As
we learn their needs, we can add more avenues for
interacting with topic models.
Acknowledgements
We would like to thank the anonymous reviewers, Ed-
mund Talley, Jonathan Chang, and Philip Resnik for
their helpful comments on drafts of this paper. This
work was supported by NSF grant #0705832. Jordan
Boyd-Graber is also supported by the Army Research
Laboratory through ARL Cooperative Agreement
W911NF-09-2-0072 and by NSF grant #1018625.
Any opinions, findings, conclusions, or recommenda-
tions expressed are the authors? and do not necessar-
ily reflect those of the sponsors.
256
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic mod-
eling via Dirichlet forest priors. In Proceedings of
International Conference of Machine Learning.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine Learn-
ing Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2008. Syntactic
topic models. In Proceedings of Advances in Neural
Information Processing Systems.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual su-
pervised latent Dirichlet alocation. In Proceedings of
Emperical Methods in Natural Language Processing.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean
Gerrish, and David M. Blei. 2009. Reading tea leaves:
How humans interpret topic models. In Neural Infor-
mation Processing Systems.
Jonathan Chang. 2010. Not-so-latent Dirichlet alocation:
Collapsed Gibbs sampling using human judgments. In
NAACL Workshop: Creating Speech and Language
Data With Amazon?ss Mechanical Turk.
Hal Daume? III. 2009. Markov random topic fields. In
Proceedings of Artificial Intelligence and Statistics.
Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In Pro-
ceedings of International Conference of Machine Learn-
ing.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. Proceedings of the National Academy
of Sciences, 101(Suppl 1):5228?5235.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic Markov models. In Artificial Intelligence
and Statistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1):10?18.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence.
Thomas K. Landauer, Danielle S. McNamara, Dennis S.
Marynick, and Walter Kintsch, editors. 2006. Proba-
bilistic Topic Models. Laurence Erlbaum.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexan-
der Hauptmann. 2006. Which side are you on? identi-
fying perspectives at the document and sentence levels.
In Proceedings of the Conference on Natural Language
Learning (CoNLL).
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching.
Radford M. Neal. 1993. Probabilistic inference using
Markov chain Monte Carlo methods. Technical Report
CRG-TR-93-1, University of Toronto.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Conference of the North American Chapter of
the Association for Computational Linguistics.
Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In Association for the Advancement of
Artificial Intelligence.
James Petterson, Smola Alex, Tiberio Caetano, Wray Bun-
tine, and Narayanamurthy Shravan. 2010. Word fea-
tures for latent Dirichlet alocation. In Neural Informa-
tion Processing Systems.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of Emperical Methods
in Natural Language Processing.
Fergus Rob, Li Fei-Fei, Perona Pietro, and Zisserman An-
drew. 2005. Learning object categories from Google?s
image search. In International Conference on Com-
puter Vision.
Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model for
authors and documents. In Proceedings of Uncertainty
in Artificial Intelligence.
Suyash Shringarpure and Eric P. Xing. 2008. mStruct:
a new admixture model for inference of population
structure in light of both genetic admixing and allele
mutations. In Proceedings of International Conference
of Machine Learning.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast?but is it good? Evalu-
ating non-expert annotations for natural language tasks.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Hanna Wallach, David Mimno, and Andrew McCallum.
2009. Rethinking LDA: Why priors matter. In Pro-
ceedings of Advances in Neural Information Processing
Systems.
Hanna M. Wallach. 2006. Topic modeling: Beyond bag-
of-words. In Proceedings of International Conference
of Machine Learning.
Limin Yao, David Mimno, and Andrew McCallum. 2009.
Efficient methods for topic model inference on stream-
ing document collections. In Knowledge Discovery and
Data Mining.
257
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 78?87,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
SITS: A Hierarchical Nonparametric Model using Speaker Identity for
Topic Segmentation in Multiparty Conversations
Viet-An Nguyen
Department of Computer Science
and UMIACS
University of Maryland
College Park, MD
vietan@cs.umd.edu
Jordan Boyd-Graber
iSchool
and UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Department of Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
One of the key tasks for analyzing conversa-
tional data is segmenting it into coherent topic
segments. However, most models of topic
segmentation ignore the social aspect of con-
versations, focusing only on the words used.
We introduce a hierarchical Bayesian nonpara-
metric model, Speaker Identity for Topic Seg-
mentation (SITS), that discovers (1) the top-
ics used in a conversation, (2) how these top-
ics are shared across conversations, (3) when
these topics shift, and (4) a person-specific
tendency to introduce new topics. We eval-
uate against current unsupervised segmenta-
tion models to show that including person-
specific information improves segmentation
performance on meeting corpora and on po-
litical debates. Moreover, we provide evidence
that SITS captures an individual?s tendency to
introduce new topics in political contexts, via
analysis of the 2008 US presidential debates
and the television program Crossfire.
1 Topic Segmentation as a Social Process
Conversation, interactive discussion between two or
more people, is one of the most essential and com-
mon forms of communication. Whether in an in-
formal situation or in more formal settings such as
a political debate or business meeting, a conversa-
tion is often not about just one thing: topics evolve
and are replaced as the conversation unfolds. Dis-
covering this hidden structure in conversations is a
key problem for conversational assistants (Tur et al,
2010) and tools that summarize (Murray et al, 2005)
and display (Ehlen et al, 2007) conversational data.
Topic segmentation also can illuminate individuals?
agendas (Boydstun et al, 2011), patterns of agree-
ment and disagreement (Hawes et al, 2009; Abbott
et al, 2011), and relationships among conversational
participants (Ireland et al, 2011).
One of the most natural ways to capture conversa-
tional structure is topic segmentation (Reynar, 1998;
Purver, 2011). Topic segmentation approaches range
from simple heuristic methods based on lexical simi-
larity (Morris and Hirst, 1991; Hearst, 1997) to more
intricate generative models and supervised meth-
ods (Georgescul et al, 2006; Purver et al, 2006;
Gruber et al, 2007; Eisenstein and Barzilay, 2008),
which have been shown to outperform the established
heuristics.
However, previous computational work on con-
versational structure, particularly in topic discovery
and topic segmentation, focuses primarily on con-
tent, ignoring the speakers. We argue that, because
conversation is a social process, we can understand
conversational phenomena better by explicitly model-
ing behaviors of conversational participants. In Sec-
tion 2, we incorporate participant identity in a new
model we call Speaker Identity for Topic Segmen-
tation (SITS), which discovers topical structure in
conversation while jointly incorporating a participant-
level social component. Specifically, we explicitly
model an individual?s tendency to introduce a topic.
After outlining inference in Section 3 and introducing
data in Section 4, we use SITS to improve state-of-
the-art-topic segmentation and topic identification
models in Section 5. In addition, in Section 6, we
also show that the per-speaker model is able to dis-
cover individuals who shape and influence the course
of a conversation. Finally, we discuss related work
and conclude the paper in Section 7.
2 Modeling Multiparty Discussions
Data Properties We are interested in turn-taking,
multiparty discussion. This is a broad category, in-
78
cluding political debates, business meetings, and on-
line chats. More formally, such datasets contain C
conversations. A conversation c has Tc turns, each of
which is a maximal uninterrupted utterance by one
speaker.1 In each turn t ? [1, Tc], a speaker ac,t utters
N words {wc,t,n}. Each word is from a vocabulary
of size V , and there are M distinct speakers.
Modeling Approaches The key insight of topic
segmentation is that segments evince lexical cohe-
sion (Galley et al, 2003; Olney and Cai, 2005).
Words within a segment will look more like their
neighbors than other words. This insight has been
used to tune supervised methods (Hsueh et al, 2006)
and inspire unsupervised models of lexical cohesion
using bags of words (Purver et al, 2006) and lan-
guage models (Eisenstein and Barzilay, 2008).
We too take the unsupervised statistical approach.
It requires few resources and is applicable in many
domains without extensive training. Like previ-
ous approaches, we consider each turn to be a bag
of words generated from an admixture of topics.
Topics?after the topic modeling literature (Blei and
Lafferty, 2009)?are multinomial distributions over
terms. These topics are part of a generative model
posited to have produced a corpus.
However, topic models alone cannot model the dy-
namics of a conversation. Topic models typically do
not model the temporal dynamics of individual docu-
ments, and those that do (Wang et al, 2008; Gerrish
and Blei, 2010) are designed for larger documents
and are not applicable here because they assume that
most topics appear in every time slice.
Instead, we endow each turn with a binary latent
variable lc,t, called the topic shift. This latent variable
signifies whether the speaker changed the topic of the
conversation. To capture the topic-controlling behav-
ior of the speakers across different conversations, we
further associate each speaker m with a latent topic
shift tendency, pim. Informally, this variable is in-
tended to capture the propensity of a speaker to effect
a topic shift. Formally, it represents the probability
that the speakerm will change the topic (distribution)
of a conversation.
We take a Bayesian nonparametric ap-
proach (Mu?ller and Quintana, 2004). Unlike
1Note the distinction with phonetic utterances, which by
definition are bounded by silence.
parametric models, which a priori fix the number of
topics, nonparametric models use a flexible number
of topics to better represent data. Nonparametric
distributions such as the Dirichlet process (Ferguson,
1973) share statistical strength among conversations
using a hierarchical model, such as the hierarchical
Dirichlet process (HDP) (Teh et al, 2006).
2.1 Generative Process
In this section, we develop SITS, a generative model
of multiparty discourse that jointly discovers topics
and speaker-specific topic shifts from an unannotated
corpus (Figure 1a). As in the hierarchical Dirichlet
process (Teh et al, 2006), we allow an unbounded
number of topics to be shared among the turns of the
corpus. Topics are drawn from a base distribution
H over multinomial distributions over the vocabu-
lary, a finite Dirichlet with symmetric prior ?. Unlike
the HDP, where every document (here, every turn)
draws a new multinomial distribution from a Dirich-
let process, the social and temporal dynamics of a
conversation, as specified by the binary topic shift
indicator lc,t, determine when new draws happen.
The full generative process is as follows:
1. For speaker m ? [1,M ], draw speaker shift probability
pim ? Beta(?)
2. Draw global probability measure G0 ? DP(?,H)
3. For each conversation c ? [1, C]
(a) Draw conversation distribution Gc ? DP(?0, G0)
(b) For each turn t ? [1, Tc] with speaker ac,t
i. If t = 1, set the topic shift lc,t = 1. Otherwise,
draw lc,t ? Bernoulli(piac,t).
ii. If lc,t = 1, draw Gc,t ? DP (?c, Gc). Other-
wise, set Gc,t ? Gc,t?1.
iii. For each word index n ? [1, Nc,t]
? Draw ?c,t,n ? Gc,t
? Draw wc,t,n ? Multinomial(?c,t,n)
The hierarchy of Dirichlet processes allows sta-
tistical strength to be shared across contexts; within
a conversation and across conversations. The per-
speaker topic shift tendency pim allows speaker iden-
tity to influence the evolution of topics.
To make notation concrete and aligned with the
topic segmentation, we introduce notation for seg-
ments in a conversation. A segment s of conver-
sation c is a sequence of turns [?, ? ?] such that
lc,? = lc,? ?+1 = 1 and lc,t = 0, ?t ? (?, ? ?]. When
lc,t = 0, Gc,t is the same as Gc,t?1 and all topics (i.e.
multinomial distributions over words) {?c,t,n} that
generate words in turn t and the topics {?c,t?1,n}
that generate words in turn t? 1 come from the same
79
pim ?
ac,2 ac,Tc
wc,1,n wc,2,n wc,Tc,n
?c,1,n ?c,2,n ?c,Tc,n
Gc,1 Gc,2 Gc,Tc
?c lc,2 lc,TcGc?0
G0
? H
C
M
Nc,1 Nc,2 Nc,Tc
(a)
?k?
? pim ?
ac,2 ac,Tc
wc,1,n
zc,1,n
?c,1
wc,2,n
zc,2,n
?c,2
lc,2
wc,Tc,n
zc,Tc,n
?c,Tc
lc,Tc
C
K
M
Nc,1 Nc,2 Nc,Tc
(b)
Figure 1: Graphical model representations of our proposed models: (a) the nonparametric version; (b) the
parametric version. Nodes represent random variables (shaded ones are observed), lines are probabilistic
dependencies. Plates represent repetition. The innermost plates are turns, grouped in conversations.
distribution. Thus all topics used in a segment s are
drawn from a single distribution, Gc,s,
Gc,s | lc,1, lc,2, ? ? ? , lc,Tc , ?c, Gc ? DP(?c, Gc) (1)
For notational convenience, Sc denotes the num-
ber of segments in conversation c, and st denotes
the segment index of turn t. We emphasize that all
segment-related notations are derived from the poste-
rior over the topic shifts l and not part of the model
itself.
Parametric Version SITS is a generalization of a
parametric model (Figure 1b) where each turn has
a multinomial distribution over K topics. In the
parametric case, the number of topics K is fixed.
Each topic, as before, is a multinomial distribution
?1 . . . ?K . In the parametric case, each turn t in con-
versation c has an explicit multinomial distribution
over K topics ?c,t, identical for turns within a seg-
ment. A new topic distribution ? is drawn from a
Dirichlet distribution parameterized by ? when the
topic shift indicator l is 1.
The parametric version does not share strength
within or across conversations, unlike SITS. When
applied on a single conversation without speaker iden-
tity (all speakers are identical) it is equivalent to
(Purver et al, 2006). In our experiments (Section 5),
we compare against both.
3 Inference
To find the latent variables that best explain observed
data, we use Gibbs sampling, a widely used Markov
chain Monte Carlo inference technique (Neal, 2000;
Resnik and Hardisty, 2010). The state space is latent
variables for topic indices assigned to all tokens z =
{zc,t,n} and topic shifts assigned to turns l = {lc,t}.
We marginalize over all other latent variables. Here,
we only present the conditional sampling equations;
for more details, see our supplement.2
3.1 Sampling Topic Assignments
To sample zc,t,n, the index of the shared topic as-
signed to token n of turn t in conversation c, we need
to sample the path assigning each word token to a
segment-specific topic, each segment-specific topic
to a conversational topic and each conversational
topic to a shared topic. For efficiency, we make use
of the minimal path assumption (Wallach, 2008) to
generate these assignments.3 Under the minimal path
assumption, an observation is assumed to have been
generated by using a new distribution if and only if
there is no existing distribution with the same value.
2 http://www.cs.umd.edu/?vietan/topicshift/appendix.pdf
3We also investigated using the maximal assumption and
fully sampling assignments. We found the minimal path assump-
tion worked as well as explicitly sampling seating assignments
and that the maximal path assumption worked less well.
80
We use Nc,s,k to denote the number of tokens in
segment s in conversation c assigned topic k; Nc,k
denotes the total number of segment-specific top-
ics in conversation c assigned topic k and Nk de-
notes the number of conversational topics assigned
topic k. TWk,w denotes the number of times the
shared topic k is assigned to word w in the vocab-
ulary. Marginal counts are represented with ? and
? represents all hyperparameters. The conditional
distribution for zc,t,n is P (zc,t,n = k | wc,t,n =
w, z?c,t,n,w?c,t,n, l, ?) ?
N?c,t,nc,st,k + ?c
N?c,t,nc,k +?0
N?c,t,nk +
?
K
N?c,t,n? +?
N?c,t,nc,? +?0
N?c,t,nc,st,? + ?c
?
?
?
?
?
?
TW?c,t,nk,w + ?
TW?c,t,nk,? + V ?
,
1
V
k new.
(2)
Here V is the size of the vocabulary, K is the current
number of shared topics and the superscript ?c,t,n
denotes counts without considering wc,t,n. In Equa-
tion 2, the first factor is proportional to the probability
of sampling a path according to the minimal path as-
sumption; the second factor is proportional to the
likelihood of observing w given the sampled topic.
Since an uninformed prior is used, when a new topic
is sampled, all tokens are equiprobable.
3.2 Sampling Topic Shifts
Sampling the topic shift variable lc,t requires us to
consider merging or splitting segments. We use kc,t
to denote the shared topic indices of all tokens in
turn t of conversation c; Sac,t,x to denote the num-
ber of times speaker ac,t is assigned the topic shift
with value x ? {0, 1}; Jxc,s to denote the number of
topics in segment s of conversation c if lc,t = x and
Nxc,s,j to denote the number of tokens assigned to the
segment-specific topic j when lc,t = x.4 Again, the
superscript ?c,t is used to denote exclusion of turn t
of conversation c in the corresponding counts.
Recall that the topic shift is a binary variable. We
use 0 to represent the case that the topic distribution
is identical to the previous turn. We sample this
assignment P (lc,t = 0 | l?c,t,w,k,a, ?) ?
S?c,tac,t,0 + ?
S?c,tac,t,? + 2?
?
?
J0c,stc
?J0c,st
j=1 (N
0
c,st,j ? 1)!
?N0c,st,?
x=1 (x? 1 + ?c)
. (3)
4Deterministically knowing the path assignments is the pri-
mary efficiency motivation for using the minimal path assump-
tion. The alternative is to explicitly sample the path assignments,
which is more complicated (for both notation and computation).
This option is spelled in full detail in the supplementary material.
In Equation 3, the first factor is proportional to the
probability of assigning a topic shift of value 0 to
speaker ac,t and the second factor is proportional to
the joint probability of all topics in segment st of
conversation c when lc,t = 0.
The other alternative is for the topic shift to be
1, which represents the introduction of a new distri-
bution over topics inside an existing segment. We
sample this as P (lc,t = 1 | l?c,t,w,k,a, ?) ?
S?c,tac,t,1 + ?
S?c,tac,t,? + 2?
?
?
?
?
J1c,(st?1)c
?J1c,(st?1)
j=1 (N
1
c,(st?1),j ? 1)!
?N1c,(st?1),?
x=1 (x? 1 + ?c)
?
J1c,stc
?J1c,st
j=1 (N
1
c,stj ? 1)!
?N1c,st,?
x=1 (x? 1 + ?c)
?
? . (4)
As above, the first factor in Equation 4 is propor-
tional to the probability of assigning a topic shift of
value 1 to speaker ac,t; the second factor in the big
bracket is proportional to the joint distribution of the
topics in segments st ? 1 and st. In this case lc,t = 1
means splitting the current segment, which results in
two joint probabilities for two segments.
4 Datasets
This section introduces the three corpora we use. We
preprocess the data to remove stopwords and remove
turns containing fewer than five tokens.
The ICSI Meeting Corpus: The ICSI Meeting
Corpus (Janin et al, 2003) is 75 transcribed meetings.
For evaluation, we used a standard set of reference
segmentations (Galley et al, 2003) of 25 meetings.
Segmentations are binary, i.e., each point of the doc-
ument is either a segment boundary or not, and on
average each meeting has 8 segment boundaries. Af-
ter preprocessing, there are 60 unique speakers and
the vocabulary contains 3346 non-stopword tokens.
The 2008 Presidential Election Debates Our sec-
ond dataset contains three annotated presidential de-
bates (Boydstun et al, 2011) between Barack Obama
and John McCain and a vice presidential debate be-
tween Joe Biden and Sarah Palin. Each turn is one
of two types: questions (Q) from the moderator or
responses (R) from a candidate. Each clause in a
turn is coded with a Question Topic (TQ) and a Re-
sponse Topic (TR). Thus, a turn has a list of TQ?s and
TR?s both of length equal to the number of clauses in
the turn. Topics are from the Policy Agendas Topics
81
Speaker Type Turn clauses TQ TR
Brokaw Q Sen. Obama, [. . . ] Are you saying [. . . ] that the American economy is going to get much worse
before it gets better and they ought to be prepared for that?
1 N/A
Obama R
No, I am confident about the American economy. 1 1
[. . . ] But most importantly, we?re going to have to help ordinary families be able to stay in their
homes, make sure that they can pay their bills [. . . ]
1 14
Brokaw Q Sen. McCain, in all candor, do you think the economy is going to get worse before it gets better? 1 N/A
McCain R
[. . . ] I think if we act effectively, if we stabilize the housing market?which I believe we can, 1 14
if we go out and buy up these bad loans, so that people can have a new mortgage at the new value
of their home
1 14
I think if we get rid of the cronyism and special interest influence in Washington so we can act
more effectively. [. . . ]
1 20
Table 1: Example turns from the annotated 2008 election debates. The topics (TQ and TR) are from the Policy
Agendas Topics Codebook which contains the following codes of topic: Macroeconomics (1), Housing &
Community Development (14), Government Operations (20).
Codebook, a manual inventory of 19 major topics
and 225 subtopics.5 Table 1 shows an example anno-
tation.
To get reference segmentations, we assign each
turn a real value from 0 to 1 indicating how much a
turn changes the topic. For a question-typed turn, the
score is the fraction of clause topics not appearing in
the previous turn; for response-typed turns, the score
is the fraction of clause topics that do not appear in
the corresponding question. This results in a set of
non-binary reference segmentations. For evaluation
metrics that require binary segmentations, we create
a binary segmentation by setting a turn as a segment
boundary if the computed score is 1. This threshold
is chosen to include only true segment boundaries.
CNN?s Crossfire Crossfire was a weekly U.S. tele-
vision ?talking heads? program engineered to incite
heated arguments (hence the name). Each episode
features two recurring hosts, two guests, and clips
from the week?s news. Our Crossfire dataset con-
tains 1134 transcribed episodes aired between 2000
and 2004.6 There are 2567 unique speakers. Unlike
the previous two datasets, Crossfire does not have
explicit topic segmentations, so we use it to explore
speaker-specific characteristics (Section 6).
5 Topic Segmentation Experiments
In this section, we examine how well SITS can repli-
cate annotations of when new topics are introduced.
5 http://www.policyagendas.org/page/topic-codebook
6 http://www.cs.umd.edu/?vietan/topicshift/crossfire.zip
We discuss metrics for evaluating an algorithm?s seg-
mentation against a gold annotation, describe our
experimental setup, and report those results.
Evaluation Metrics To evaluate segmentations,
we use Pk (Beeferman et al, 1999) and WindowDiff
(WD) (Pevzner and Hearst, 2002). Both metrics mea-
sure the probability that two points in a document
will be incorrectly separated by a segment boundary.
Both techniques consider all spans of length k in the
document and count whether the two endpoints of
the window are (im)properly segmented against the
gold segmentation.
However, these metrics have drawbacks. First,
they require both hypothesized and reference seg-
mentations to be binary. Many algorithms (e.g., prob-
abilistic approaches) give non-binary segmentations
where candidate boundaries have real-valued scores
(e.g., probability or confidence). Thus, evaluation
requires arbitrary thresholding to binarize soft scores.
To be fair, thresholds are set so the number of seg-
ments are equal to a predefined value (Purver et al,
2006; Galley et al, 2003).
To overcome these limitations, we also use Earth
Mover?s Distance (EMD) (Rubner et al, 2000), a
metric that measures the distance between two distri-
butions. The EMD is the minimal cost to transform
one distribution into the other. Each segmentation
can be considered a multi-dimensional distribution
where each candidate boundary is a dimension. In
EMD, a distance function across features allows par-
tial credit for ?near miss? segment boundaries. In
82
addition, because EMD operates on distributions, we
can compute the distance between non-binary hy-
pothesized segmentations with binary or real-valued
reference segmentations. We use the FastEMD im-
plementation (Pele and Werman, 2009).
Experimental Methods We applied the following
methods to discover topic segmentations in a docu-
ment:
? TextTiling (Hearst, 1997) is one of the earliest general-
purpose topic segmentation algorithms, sliding a fixed-
width window to detect major changes in lexical similarity.
? P-NoSpeaker-S: parametric version without speaker iden-
tity run on each conversation (Purver et al, 2006)
? P-NoSpeaker-M: parametric version without speaker
identity run on all conversations
? P-SITS: the parametric version of SITS with speaker iden-
tity run on all conversations
? NP-HMM: the HMM-based nonparametric model which
a single topic per turn. This model can be considered a
Sticky HDP-HMM (Fox et al, 2008) with speaker identity.
? NP-SITS: the nonparametric version of SITS with speaker
identity run on all conversations.
Parameter Settings and Implementations In our
experiment, all parameters of TextTiling are the
same as in (Hearst, 1997). For statistical models,
Gibbs sampling with 10 randomly initialized chains
is used. Initial hyperparameter values are sampled
from U(0, 1) to favor sparsity; statistics are collected
after 500 burn-in iterations with a lag of 25 itera-
tions over a total of 5000 iterations; and slice sam-
pling (Neal, 2003) optimizes hyperparameters.
Results and Analysis Table 2 shows the perfor-
mance of various models on the topic segmentation
problem, using the ICSI corpus and the 2008 debates.
Consistent with previous results, probabilistic
models outperform TextTiling. In addition, among
the probabilistic models, the models that had access
to speaker information consistently segment better
than those lacking such information, supporting our
assertion that there is benefit to modeling conversa-
tion as a social process. Furthermore, NP-SITS out-
performs NP-HMM in both experiments, suggesting
that using a distribution over topics to turns is bet-
ter than using a single topic. This is consistent with
parametric results reported in (Purver et al, 2006).
The contribution of speaker identity seems more
valuable in the debate setting. Debates are character-
ized by strong rewards for setting the agenda; dodg-
ing a question or moving the debate toward an oppo-
nent?s weakness can be useful strategies (Boydstun
et al, 2011). In contrast, meetings (particularly low-
stakes ICSI meetings) are characterized by pragmatic
rather than strategic topic shifts. Second, agenda-
setting roles are clearer in formal debates; a modera-
tor is tasked with setting the agenda and ensuring the
conversation does not wander too much.
The nonparametric model does best on the smaller
debate dataset. We suspect that an evaluation that
directly accessed the topic quality, either via predic-
tion (Teh et al, 2006) or interpretability (Chang et al,
2009) would favor the nonparametric model more.
6 Evaluating Topic Shift Tendency
In this section, we focus on the ability of SITS to
capture speaker-level attributes. Recall that SITS
associates with each speaker a topic shift tendency
pi that represents the probability of asserting a new
topic in the conversation. While topic segmentation
is a well studied problem, there are no established
quantitative measurements of an individual?s ability
to control a conversation. To evaluate whether the
tendency is capturing meaningful characteristics of
speakers, we compare our inferred tendencies against
insights from political science.
2008 Elections To obtain a posterior estimate of pi
(Figure 3) we create 10 chains with hyperparameters
sampled from the uniform distribution U(0, 1) and
averaged pi over 10 chains (as described in Section 5).
In these debates, Ifill is the moderator of the debate
between Biden and Palin; Brokaw, Lehrer and Schief-
fer are the three moderators of three debates between
Obama and McCain. Here ?Question? denotes ques-
tions from audiences in ?town hall? debate. The role
of this ?speaker? can be considered equivalent to the
debate moderator.
The topic shift tendencies of moderators are
much higher than for candidates. In the three de-
bates between Obama and McCain, the moderators?
Brokaw, Lehrer and Schieffer?have significantly
higher scores than both candidates. This is a useful
reality check, since in a debate the moderators are
the ones asking questions and literally controlling the
topical focus. Interestingly, in the vice-presidential
debate, the score of moderator Ifill is only slightly
higher than those of Palin and Biden; this is consis-
tent with media commentary characterizing her as a
83
Model EMD
Pk WindowDiff
k = 5 10 15 k = 5 10 15
IC
SI
D
at
as
et
TextTiling 2.507 .289 .388 .451 .318 .477 .561
P-NoSpeaker-S 1.949 .222 .283 .342 .269 .393 .485
P-NoSpeaker-M 1.935 .207 .279 .335 .253 .371 .468
P-SITS 1.807 .211 .251 .289 .256 .363 .434
NP-HMM 2.189 .232 .257 .263 .267 .377 .444
NP-SITS 2.126 .228 .253 .259 .262 .372 .440
D
eb
at
es
D
at
as
et TextTiling 2.821 .433 .548 .633 .534 .674 .760
P-NoSpeaker-S 2.822 .426 .543 .653 .482 .650 .756
P-NoSpeaker-M 2.712 .411 .522 .589 .479 .644 .745
P-SITS 2.269 .380 .405 .402 .482 .625 .719
NP-HMM 2.132 .362 .348 .323 .486 .629 .723
NP-SITS 1.813 .332 .269 .231 .470 .600 .692
Table 2: Results on the topic segmentation task.
Lower is better. The parameter k is the window
size of the metrics Pk and WindowDiff chosen to
replicate previous results.
0 0.1 0.2 0.3 0.4
IFILLBIDENPALINOBAMAMCCAINBROKAWLEHRERSCHIEFFERQUESTION
Table 3: Topic shift tendency pi of speakers in the
2008 Presidential Election Debates (larger means
greater tendency)
weak moderator.7 Similarly, the ?Question? speaker
had a relatively high variance, consistent with an
amalgamation of many distinct speakers.
These topic shift tendencies suggest that all can-
didates manage to succeed at some points in setting
and controlling the debate topics. Our model gives
Obama a slightly higher score than McCain, consis-
tent with social science claims (Boydstun et al, 2011)
that Obama had the lead in setting the agenda over
McCain. Table 4 shows of SITS-detected topic shifts.
Crossfire Crossfire, unlike the debates, has many
speakers. This allows us to examine more closely
what we can learn about speakers? topic shift ten-
dency. We verified that SITS can segment topics,
and assuming that changing the topic is useful for a
speaker, how can we characterize who does so effec-
tively? We examine the relationship between topic
shift tendency, social roles, and political ideology.
To focus on frequent speakers, we filter out speak-
ers with fewer than 30 turns. Most speakers have
relatively small pi, with the mode around 0.3. There
are, however, speakers with very high topic shift
tendencies. Table 5 shows the speakers having the
highest values according to SITS.
We find that there are three general patterns for
who influences the course of a conversation in Cross-
fire. First, there are structural ?speakers? the show
uses to frame and propose new topics. These are
7 http://harpers.org/archive/2008/10/hbc-90003659
audience questions, news clips (e.g. many of Gore?s
and Bush?s turns from 2000), and voice overs. That
SITS is able to recover these is reassuring. Second,
the stable of regular hosts receives high topic shift
tendencies, which is reasonable given their experi-
ence with the format and ostensible moderation roles
(in practice they also stoke lively discussion).
The remaining class is more interesting. The re-
maining non-hosts with high topic shift tendency are
relative moderates on the political spectrum:
? John Kasich, one of few Republicans to support the assault
weapons ban and now governor of Ohio, a swing state
? Christine Todd Whitman, former Republican governor of
New Jersey, a very Democratic state
? John McCain, who before 2008 was known as a ?maverick?
for working with Democrats (e.g. Russ Feingold)
This suggests that, despite Crossfire?s tendency to
create highly partisan debates, those who are able to
work across the political spectrum may best be able
to influence the topic under discussion in highly po-
larized contexts. Table 4 shows detected topic shifts
from these speakers; two of these examples (McCain
and Whitman) show disagreement of Republicans
with President Bush. In the other, Kasich is defend-
ing a Republican plan (school vouchers) popular with
traditional Democratic constituencies.
7 Related and Future Work
In the realm of statistical models, a number of tech-
niques incorporate social connections and identity to
explain content in social networks (Chang and Blei,
84
Previous turn Turn detected as shifting topic
D
eb
at
es
D
at
as
et
PALIN: Your question to him was whether he sup-
ported gay marriage and my answer is the same as
his and it is that I do not.
IFILL: Wonderful. You agree. On that note, let?s move to foreign policy. You
both have sons who are in Iraq or on their way to Iraq. You, Governor Palin,
have said that you would like to see a real clear plan for an exit strategy. [. . . ]
MCCAIN: I think that Joe Biden is qualified in
many respects. . . .
SCHIEFFER: [. . . ] Let?s talk about energy and climate control. Every president
since Nixon has said what both of you [. . . ]
IFILL: So, Governor, as vice president, there?s
nothing that you have promised [. . . ] that you
wouldn?t take off the table because of this finan-
cial crisis we?re in?
BIDEN: Again, let me?let?s talk about those tax breaks. [Obama] voted for an
energy bill because, for the first time, it had real support for alternative energy.
[. . . ] on eliminating the tax breaks for the oil companies, Barack Obama voted
to eliminate them. [. . . ]
C
ro
ss
fir
e
D
at
as
et
PRESS: But what do you say, governor, to Gov-
ernor Bush and [. . . ] your party who would let
politicians and not medical scientists decide what
drugs are distributed [. . . ]
WHITMAN: Well I disagree with them on this particular issues [. . . ] that?s
important to me that George Bush stands for education of our children [. . . ] I
care about tax policy, I care about the environment. I care about all the issues
where he has a proven record in Texas [. . . ]
WEXLER: [. . . ] They need a Medicare prescrip-
tion drug plan [. . . ] Talk about schools, [. . . ] Al
Gore has got a real plan. George Bush offers us
vouchers. Talk about the environment. [. . . ] Al
Gore is right on in terms of the majority of Ameri-
cans, but George Bush [. . . ]
KASICH: [. . . ] I want to talk about choice. [. . . ] George Bush believes that, if
schools fail, parents ought to have a right to get their kids out of those schools
and give them a chance and an opportunity for success. Gore says ?no way? [. . . ]
Social Security. George Bush says [. . . ] direct it the way federal employees do
[. . . ] Al Gore says ?No way? [. . . ] That?s real choice. That?s real bottom-up,
not a bureaucratic approach, the way we run this country.
PRESS: Senator, Senator Breaux mentioned that
it?s President Bush?s aim to start on education [. . . ]
[McCain] [. . . ] said he was going to do introduce
the legislation the first day of the first week of the
new administration. [. . . ]
MCCAIN: After one of closest elections in our nation?s history, there is one
thing the American people are unanimous about They want their government
back. We can do that by ridding politics of large, unregulated contributions that
give special interests a seat at the table while average Americans are stuck in the
back of the room.
Table 4: Example of turns designated as a topic shift by SITS. Turns were chosen with speakers to give
examples of those with high topic shift tendency pi.
Rank Speaker pi Rank Speaker pi
1 Announcer .884 10 Kasich .570
2 Male .876 11 Carville? .550
3 Question .755 12 Carlson? .550
4 G. W. Bush? .751 13 Begala? .545
5 Press? .651 14 Whitman .533
6 Female .650 15 McAuliffe .529
7 Gore? .650 16 Matalin? .527
8 Narrator .642 17 McCain .524
9 Novak? .587 18 Fleischer .522
Table 5: Top speakers by topic shift tendencies. We
mark hosts (?) and ?speakers? who often (but not al-
ways) appeared in clips (?). Apart from those groups,
speakers with the highest tendency were political
moderates.
2009) and scientific corpora (Rosen-Zvi et al, 2004).
However, these models ignore the temporal evolution
of content, treating documents as static.
Models that do investigate the evolution of topics
over time typically ignore the identify of the speaker.
For example: models having sticky topics over n-
grams (Johnson, 2010), sticky HDP-HMM (Fox et al,
2008); models that are an amalgam of sequential
models and topic models (Griffiths et al, 2005; Wal-
lach, 2006; Gruber et al, 2007; Ahmed and Xing,
2008; Boyd-Graber and Blei, 2008; Du et al, 2010);
or explicit models of time or other relevant features
as a distinct latent variable (Wang and McCallum,
2006; Eisenstein et al, 2010).
In contrast, SITS jointly models topic and individ-
uals? tendency to control a conversation. Not only
does SITS outperform other models using standard
computational linguistics baselines, but it also pro-
poses intriguing hypotheses for social scientists.
Associating each speaker with a scalar that mod-
els their tendency to change the topic does improve
performance on standard tasks, but it?s inadequate to
fully describe an individual. Modeling individuals?
perspective (Paul and Girju, 2010), ?side? (Thomas
et al, 2006), or personal preferences for topics (Grim-
mer, 2009) would enrich the model and better illumi-
nate the interaction of influence and topic.
Statistical analysis of political discourse can help
discover patterns that political scientists, who often
work via a ?close reading,? might otherwise miss.
We plan to work with social scientists to validate
our implicit hypothesis that our topic shift tendency
correlates well with intuitive measures of ?influence.?
85
Acknowledgements
This research was funded in part by the Army Re-
search Laboratory through ARL Cooperative Agree-
ment W911NF-09-2-0072 and by the Office of the
Director of National Intelligence (ODNI), Intelli-
gence Advanced Research Projects Activity (IARPA),
through the Army Research Laboratory. Jordan
Boyd-Graber and Philip Resnik are also supported
by US National Science Foundation Grant NSF grant
#1018625. Any opinions, findings, conclusions, or
recommendations expressed are the authors? and do
not necessarily reflect those of the sponsors.
References
[Abbott et al, 2011] Abbott, R., Walker, M., Anand, P.,
Fox Tree, J. E., Bowmani, R., and King, J. (2011). How
can you say such things?!?: Recognizing disagreement
in informal political argument. In Proceedings of the
Workshop on Language in Social Media (LSM 2011),
pages 2?11.
[Ahmed and Xing, 2008] Ahmed, A. and Xing, E. P.
(2008). Dynamic non-parametric mixture models and
the recurrent Chinese restaurant process: with applica-
tions to evolutionary clustering. In SDM, pages 219?
230.
[Beeferman et al, 1999] Beeferman, D., Berger, A., and
Lafferty, J. (1999). Statistical models for text segmen-
tation. Mach. Learn., 34:177?210.
[Blei and Lafferty, 2009] Blei, D. M. and Lafferty, J.
(2009). Text Mining: Theory and Applications, chapter
Topic Models. Taylor and Francis, London.
[Boyd-Graber and Blei, 2008] Boyd-Graber, J. and Blei,
D. M. (2008). Syntactic topic models. In Proceedings
of Advances in Neural Information Processing Systems.
[Boydstun et al, 2011] Boydstun, A. E., Phillips, C., and
Glazier, R. A. (2011). It?s the economy again, stupid:
Agenda control in the 2008 presidential debates. Forth-
coming.
[Chang and Blei, 2009] Chang, J. and Blei, D. M. (2009).
Relational topic models for document networks. In
Proceedings of Artificial Intelligence and Statistics.
[Chang et al, 2009] Chang, J., Boyd-Graber, J., Wang, C.,
Gerrish, S., and Blei, D. M. (2009). Reading tea leaves:
How humans interpret topic models. In Neural Infor-
mation Processing Systems.
[Du et al, 2010] Du, L., Buntine, W., and Jin, H. (2010).
Sequential latent dirichlet alocation: Discover underly-
ing topic structures within a document. In Data Mining
(ICDM), 2010 IEEE 10th International Conference on,
pages 148 ?157.
[Ehlen et al, 2007] Ehlen, P., Purver, M., and Niekrasz, J.
(2007). A meeting browser that learns. In In: Pro-
ceedings of the AAAI Spring Symposium on Interaction
Challenges for Intelligent Assistants.
[Eisenstein and Barzilay, 2008] Eisenstein, J. and Barzi-
lay, R. (2008). Bayesian unsupervised topic segmenta-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, Proceedings
of Emperical Methods in Natural Language Processing.
[Eisenstein et al, 2010] Eisenstein, J., O?Connor, B.,
Smith, N. A., and Xing, E. P. (2010). A latent variable
model for geographic lexical variation. In EMNLP?10,
pages 1277?1287.
[Ferguson, 1973] Ferguson, T. S. (1973). A Bayesian anal-
ysis of some nonparametric problems. The Annals of
Statistics, 1(2):209?230.
[Fox et al, 2008] Fox, E. B., Sudderth, E. B., Jordan, M. I.,
and Willsky, A. S. (2008). An hdp-hmm for systems
with state persistence. In Proceedings of International
Conference of Machine Learning.
[Galley et al, 2003] Galley, M., McKeown, K., Fosler-
Lussier, E., and Jing, H. (2003). Discourse segmenta-
tion of multi-party conversation. In Proceedings of the
Association for Computational Linguistics.
[Georgescul et al, 2006] Georgescul, M., Clark, A., and
Armstrong, S. (2006). Word distributions for thematic
segmentation in a support vector machine approach.
In Conference on Computational Natural Language
Learning.
[Gerrish and Blei, 2010] Gerrish, S. and Blei, D. M.
(2010). A language-based approach to measuring schol-
arly impact. In Proceedings of International Confer-
ence of Machine Learning.
[Griffiths et al, 2005] Griffiths, T. L., Steyvers, M., Blei,
D. M., and Tenenbaum, J. B. (2005). Integrating topics
and syntax. In Proceedings of Advances in Neural
Information Processing Systems.
[Grimmer, 2009] Grimmer, J. (2009). A Bayesian Hier-
archical Topic Model for Political Texts: Measuring
Expressed Agendas in Senate Press Releases. Political
Analysis, 18:1?35.
[Gruber et al, 2007] Gruber, A., Rosen-Zvi, M., and
Weiss, Y. (2007). Hidden topic Markov models. In
Artificial Intelligence and Statistics.
[Hawes et al, 2009] Hawes, T., Lin, J., and Resnik, P.
(2009). Elements of a computational model for multi-
party discourse: The turn-taking behavior of Supreme
Court justices. Journal of the American Society for In-
formation Science and Technology, 60(8):1607?1615.
[Hearst, 1997] Hearst, M. A. (1997). TextTiling: Segment-
ing text into multi-paragraph subtopic passages. Com-
putational Linguistics, 23(1):33?64.
86
[Hsueh et al, 2006] Hsueh, P.-y., Moore, J. D., and Renals,
S. (2006). Automatic segmentation of multiparty dia-
logue. In Proceedings of the European Chapter of the
Association for Computational Linguistics.
[Ireland et al, 2011] Ireland, M. E., Slatcher, R. B., East-
wick, P. W., Scissors, L. E., Finkel, E. J., and Pen-
nebaker, J. W. (2011). Language style matching pre-
dicts relationship initiation and stability. Psychological
Science, 22(1):39?44.
[Janin et al, 2003] Janin, A., Baron, D., Edwards, J., El-
lis, D., Gelbart, D., Morgan, N., Peskin, B., Pfau, T.,
Shriberg, E., Stolcke, A., and Wooters, C. (2003). The
ICSI meeting corpus. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing.
[Johnson, 2010] Johnson, M. (2010). PCFGs, topic mod-
els, adaptor grammars and learning topical collocations
and the structure of proper names. In Proceedings of
the Association for Computational Linguistics.
[Morris and Hirst, 1991] Morris, J. and Hirst, G. (1991).
Lexical cohesion computed by thesaural relations as
an indicator of the structure of text. Computational
Linguistics, 17:21?48.
[Mu?ller and Quintana, 2004] Mu?ller, P. and Quintana,
F. A. (2004). Nonparametric Bayesian data analysis.
Statistical Science, 19(1):95?110.
[Murray et al, 2005] Murray, G., Renals, S., and Carletta,
J. (2005). Extractive summarization of meeting record-
ings. In European Conference on Speech Communica-
tion and Technology.
[Neal, 2000] Neal, R. M. (2000). Markov chain sampling
methods for Dirichlet process mixture models. Journal
of Computational and Graphical Statistics, 9(2):249?
265.
[Neal, 2003] Neal, R. M. (2003). Slice sampling. Annals
of Statistics, 31:705?767.
[Olney and Cai, 2005] Olney, A. and Cai, Z. (2005). An
orthonormal basis for topic segmentation in tutorial di-
alogue. In Proceedings of the Human Language Tech-
nology Conference.
[Paul and Girju, 2010] Paul, M. and Girju, R. (2010). A
two-dimensional topic-aspect model for discovering
multi-faceted topics. In Association for the Advance-
ment of Artificial Intelligence.
[Pele and Werman, 2009] Pele, O. and Werman, M.
(2009). Fast and robust earth mover?s distances. In
International Conference on Computer Vision.
[Pevzner and Hearst, 2002] Pevzner, L. and Hearst, M. A.
(2002). A critique and improvement of an evaluation
metric for text segmentation. Computational Linguis-
tics, 28.
[Purver, 2011] Purver, M. (2011). Topic segmentation. In
Tur, G. and de Mori, R., editors, Spoken Language
Understanding: Systems for Extracting Semantic Infor-
mation from Speech, pages 291?317. Wiley.
[Purver et al, 2006] Purver, M., Ko?rding, K., Griffiths,
T. L., and Tenenbaum, J. (2006). Unsupervised topic
modelling for multi-party spoken discourse. In Pro-
ceedings of the Association for Computational Linguis-
tics.
[Resnik and Hardisty, 2010] Resnik, P. and Hardisty, E.
(2010). Gibbs sampling for the uninitiated. Technical
Report UMIACS-TR-2010-04, University of Maryland.
http://www.lib.umd.edu/drum/handle/1903/10058.
[Reynar, 1998] Reynar, J. C. (1998). Topic Segmentation:
Algorithms and Applications. PhD thesis, University of
Pennsylvania.
[Rosen-Zvi et al, 2004] Rosen-Zvi, M., Griffiths, T. L.,
Steyvers, M., and Smyth, P. (2004). The author-topic
model for authors and documents. In Proceedings of
Uncertainty in Artificial Intelligence.
[Rubner et al, 2000] Rubner, Y., Tomasi, C., and Guibas,
L. J. (2000). The earth mover?s distance as a metric
for image retrieval. International Journal of Computer
Vision, 40:99?121.
[Teh et al, 2006] Teh, Y. W., Jordan, M. I., Beal, M. J.,
and Blei, D. M. (2006). Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Association,
101(476):1566?1581.
[Thomas et al, 2006] Thomas, M., Pang, B., and Lee, L.
(2006). Get out the vote: Determining support or op-
position from Congressional floor-debate transcripts.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
[Tur et al, 2010] Tur, G., Stolcke, A., Voss, L., Peters, S.,
Hakkani-Tu?r, D., Dowding, J., Favre, B., Ferna?ndez,
R., Frampton, M., Frandsen, M., Frederickson, C., Gra-
ciarena, M., Kintzing, D., Leveque, K., Mason, S.,
Niekrasz, J., Purver, M., Riedhammer, K., Shriberg, E.,
Tien, J., Vergyri, D., and Yang, F. (2010). The CALO
meeting assistant system. Trans. Audio, Speech and
Lang. Proc., 18:1601?1611.
[Wallach, 2006] Wallach, H. M. (2006). Topic modeling:
Beyond bag-of-words. In Proceedings of International
Conference of Machine Learning.
[Wallach, 2008] Wallach, H. M. (2008). Structured Topic
Models for Language. PhD thesis, University of Cam-
bridge.
[Wang et al, 2008] Wang, C., Blei, D. M., and Heckerman,
D. (2008). Continuous time dynamic topic models. In
Proceedings of Uncertainty in Artificial Intelligence.
[Wang and McCallum, 2006] Wang, X. and McCallum, A.
(2006). Topics over time: a non-Markov continuous-
time model of topical trends. In Knowledge Discovery
and Data Mining, Knowledge Discovery and Data Min-
ing.
87
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 115?119,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Topic Models for Dynamic Translation Model Adaptation
Vladimir Eidelman
Computer Science
and UMIACS
University of Maryland
College Park, MD
vlad@umiacs.umd.edu
Jordan Boyd-Graber
iSchool
and UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
We propose an approach that biases machine
translation systems toward relevant transla-
tions based on topic-specific contexts, where
topics are induced in an unsupervised way
using topic models; this can be thought of
as inducing subcorpora for adaptation with-
out any human annotation. We use these topic
distributions to compute topic-dependent lex-
ical weighting probabilities and directly in-
corporate them into our translation model as
features. Conditioning lexical probabilities
on the topic biases translations toward topic-
relevant output, resulting in significant im-
provements of up to 1 BLEU and 3 TER on
Chinese to English translation over a strong
baseline.
1 Introduction
The performance of a statistical machine translation
(SMT) system on a translation task depends largely
on the suitability of the available parallel training
data. Domains (e.g., newswire vs. blogs) may vary
widely in their lexical choices and stylistic prefer-
ences, and what may be preferable in a general set-
ting, or in one domain, is not necessarily preferable
in another domain. Indeed, sometimes the domain
can change the meaning of a phrase entirely.
In a food related context, the Chinese sentence
????? ? (?fe?nsi? he?nduo??) would mean ?They
have a lot of vermicelli?; however, in an informal In-
ternet conversation, this sentence would mean ?They
have a lot of fans?. Without the broader context, it
is impossible to determine the correct translation in
otherwise identical sentences.
This problem has led to a substantial amount of
recent work in trying to bias, or adapt, the transla-
tion model (TM) toward particular domains of inter-
est (Axelrod et al, 2011; Foster et al, 2010; Snover
et al, 2008).1 The intuition behind TM adapta-
tion is to increase the likelihood of selecting rele-
vant phrases for translation. Matsoukas et al (2009)
introduced assigning a pair of binary features to
each training sentence, indicating sentences? genre
and collection as a way to capture domains. They
then learn a mapping from these features to sen-
tence weights, use the sentence weights to bias the
model probability estimates and subsequently learn
the model weights. As sentence weights were found
to be most beneficial for lexical weighting, Chiang
et al (2011) extends the same notion of condition-
ing on provenance (i.e., the origin of the text) by re-
moving the separate mapping step, directly optimiz-
ing the weight of the genre and collection features
by computing a separate word translation table for
each feature, estimated from only those sentences
that comprise that genre or collection.
The common thread throughout prior work is the
concept of a domain. A domain is typically a hard
constraint that is externally imposed and hand la-
beled, such as genre or corpus collection. For ex-
ample, a sentence either comes from newswire, or
weblog, but not both. However, this poses sev-
eral problems. First, since a sentence contributes its
counts only to the translation table for the source it
came from, many word pairs will be unobserved for
a given table. This sparsity requires smoothing. Sec-
ond, we may not know the (sub)corpora our training
1Language model adaptation is also prevalent but is not the
focus of this work.
115
data come from; and even if we do, ?subcorpus? may
not be the most useful notion of domain for better
translations.
We take a finer-grained, flexible, unsupervised ap-
proach for lexical weighting by domain. We induce
unsupervised domains from large corpora, and we
incorporate soft, probabilistic domain membership
into a translation model. Unsupervised modeling of
the training data produces naturally occurring sub-
corpora, generalizing beyond corpus and genre. De-
pending on the model used to select subcorpora, we
can bias our translation toward any arbitrary distinc-
tion. This reduces the problem to identifying what
automatically defined subsets of the training corpus
may be beneficial for translation.
In this work, we consider the underlying latent
topics of the documents (Blei et al, 2003). Topic
modeling has received some use in SMT, for in-
stance Bilingual LSA adaptation (Tam et al, 2007),
and the BiTAM model (Zhao and Xing, 2006),
which uses a bilingual topic model for learning
alignment. In our case, by building a topic distri-
bution for the source side of the training data, we
abstract the notion of domain to include automati-
cally derived subcorpora with probabilistic member-
ship. This topic model infers the topic distribution
of a test set and biases sentence translations to ap-
propriate topics. We accomplish this by introduc-
ing topic dependent lexical probabilities directly as
features in the translation model, and interpolating
them log-linearly with our other features, thus allow-
ing us to discriminatively optimize their weights on
an arbitrary objective function. Incorporating these
features into our hierarchical phrase-based transla-
tion system significantly improved translation per-
formance, by up to 1 BLEU and 3 TER over a strong
Chinese to English baseline.
2 Model Description
Lexical Weighting Lexical weighting features es-
timate the quality of a phrase pair by combining
the lexical translation probabilities of the words in
the phrase2 (Koehn et al, 2003). Lexical condi-
tional probabilities p(e|f) are obtained with maxi-
mum likelihood estimates from relative frequencies
2For hierarchical systems, these correspond to translation
rules.
c(f, e)/
?
e c(f, e) . Phrase pair probabilities p(e|f)
are computed from these as described in Koehn et
al. (2003).
Chiang et al (2011) showed that is it benefi-
cial to condition the lexical weighting features on
provenance by assigning each sentence pair a set
of features, fs(e|f), one for each domain s, which
compute a new word translation table ps(e|f) esti-
mated from only those sentences which belong to s:
cs(f, e)/
?
e cs(f, e) , where cs(?) is the number of
occurrences of the word pair in s.
Topic Modeling for MT We extend provenance
to cover a set of automatically generated topics zn.
Given a parallel training corpus T composed of doc-
uments di, we build a source side topic model over
T , which provides a topic distribution p(zn|di) for
zn = {1, . . . ,K} over each document, using Latent
Dirichlet Allocation (LDA) (Blei et al, 2003). Then,
we assign p(zn|di) to be the topic distribution for
every sentence xj ? di, thus enforcing topic sharing
across sentence pairs in the same document instead
of treating them as unrelated. Computing the topic
distribution over a document and assigning it to the
sentences serves to tie the sentences together in the
document context.
To obtain the lexical probability conditioned on
topic distribution, we first compute the expected
count ezn(e, f) of a word pair under topic zn:
ezn(e, f) =
?
di?T
p(zn|di)
?
xj?di
cj(e, f) (1)
where cj(?) denotes the number of occurrences of
the word pair in sentence xj , and then compute:
pzn(e|f) =
ezn(e, f)?
e ezn(e, f)
(2)
Thus, we will introduce 2?K new word trans-
lation tables, one for each pzn(e|f) and pzn(f |e),
and as many new corresponding features fzn(e|f),
fzn(f |e). The actual feature values we compute will
depend on the topic distribution of the document we
are translating. For a test document V , we infer
topic assignments on V , p(zn|V ), keeping the topics
found from T fixed. The feature value then becomes
fzn(e|f) = ? log
{
pzn(e|f) ? p(zn|V )
}
, a combi-
nation of the topic dependent lexical weight and the
116
topic distribution of the sentence from which we are
extracting the phrase. To optimize the weights of
these features we combine them in our linear model
with the other features when computing the model
score for each phrase pair3:
?
p
?php(e, f)
? ?? ?
unadapted features
+
?
zn
?znfzn(e|f)
? ?? ?
adapted features
(3)
Combining the topic conditioned word translation
table pzn(e|f) computed from the training corpus
with the topic distribution p(zn|V ) of the test sen-
tence being translated provides a probability on how
relevant that translation table is to the sentence. This
allows us to bias the translation toward the topic of
the sentence. For example, if topic k is dominant in
T , pk(e|f) may be quite large, but if p(k|V ) is very
small, then we should steer away from this phrase
pair and select a competing phrase pair which may
have a lower probability in T , but which is more rel-
evant to the test sentence at hand.
In many cases, document delineations may not be
readily available for the training corpus. Further-
more, a document may be too broad, covering too
many disparate topics, to effectively bias the weights
on a phrase level. For this case, we also propose a
local LDA model (LTM), which treats each sentence
as a separate document.
While Chiang et al (2011) has to explicitly
smooth the resulting ps(e|f), since many word pairs
will be unseen for a given domain s, we are already
performing an implicit form of smoothing (when
computing the expected counts), since each docu-
ment has a distribution over all topics, and therefore
we have some probability of observing each word
pair in every topic.
Feature Representation After obtaining the topic
conditional features, there are two ways to present
them to the model. They could answer the question
F1: What is the probability under topic 1, topic 2,
etc., or F2: What is the probability under the most
probable topic, second most, etc.
A model using F1 learns whether a specific topic
is useful for translation, i.e., feature f1 would be
f1 := pz=1(e|f) ? p(z = 1|V ). With F2, we
3The unadapted lexical weight p(e|f) is included in the
model features.
are learning how useful knowledge of the topic dis-
tribution is, i.e., f1 := p(argmaxzn (p(zn|V ))(e|f) ?
p(argmaxzn(p(zn|V ))|V ).
Using F1, if we restrict our topics to have a one-
to-one mapping with genre/collection4 we see that
our method fully recovers Chiang (2011).
F1 is appropriate for cross-domain adaptation
when we have advance knowledge that the distribu-
tion of the tuning data will match the test data, as in
Chiang (2011), where they tune and test on web. In
general, we may not know what our data will be, so
this will overfit the tuning set.
F2, however, is intuitively what we want, since
we do not want to bias our system toward a spe-
cific distribution, but rather learn to utilize informa-
tion from any topic distribution if it helps us cre-
ate topic relevant translations. F2 is useful for dy-
namic adaptation, where the adapted feature weight
changes based on the source sentence.
Thus, F2 is the approach we use in our work,
which allows us to tune our system weights toward
having topic information be useful, not toward a spe-
cific distribution.
3 Experiments
Setup To evaluate our approach, we performed ex-
periments on Chinese to English MT in two set-
tings. First, we use the FBIS corpus as our training
bitext. Since FBIS has document delineations, we
compare local topic modeling (LTM) with model-
ing at the document level (GTM). The second setting
uses the non-UN and non-HK Hansards portions of
the NIST training corpora with LTM only. Table 1
summarizes the data statistics. For both settings,
the data were lowercased, tokenized and aligned us-
ing GIZA++ (Och and Ney, 2003) to obtain bidi-
rectional alignments, which were symmetrized us-
ing the grow-diag-final-and method (Koehn
et al, 2003). The Chinese data were segmented us-
ing the Stanford segmenter. We trained a trigram
LM on the English side of the corpus with an addi-
tional 150M words randomly selected from the non-
NYT and non-LAT portions of the Gigaword v4 cor-
pus using modified Kneser-Ney smoothing (Chen
and Goodman, 1996). We used cdec (Dyer et al,
4By having as many topics as genres/collections and setting
p(zn|di) to 1 for every sentence in the collection and 0 to ev-
erything else.
117
Corpus Sentences Tokens
En Zh
FBIS 269K 10.3M 7.9M
NIST 1.6M 44.4M 40.4M
Table 1: Corpus statistics
2010) as our decoder, and tuned the parameters of
the system to optimize BLEU (Papineni et al, 2002)
on the NIST MT06 tuning corpus using the Mar-
gin Infused Relaxed Algorithm (MIRA) (Crammer
et al, 2006; Eidelman, 2012). Topic modeling was
performed with Mallet (Mccallum, 2002), a stan-
dard implementation of LDA, using a Chinese sto-
plist and setting the per-document Dirichlet parame-
ter ? = 0.01. This setting of was chosen to encour-
age sparse topic assignments, which make induced
subdomains consistent within a document.
Results Results for both settings are shown in Ta-
ble 2. GTM models the latent topics at the document
level, while LTM models each sentence as a separate
document. To evaluate the effect topic granularity
would have on translation, we varied the number of
latent topics in each model to be 5, 10, and 20. On
FBIS, we can see that both models achieve moderate
but consistent gains over the baseline on both BLEU
and TER. The best model, LTM-10, achieves a gain
of about 0.5 and 0.6 BLEU and 2 TER. Although the
performance on BLEU for both the 20 topic models
LTM-20 and GTM-20 is suboptimal, the TER im-
provement is better. Interestingly, the difference in
translation quality between capturing document co-
herence in GTM and modeling purely on the sen-
tence level is not substantial.5 In fact, the opposite
is true, with the LTM models achieving better per-
formance.6
On the NIST corpus, LTM-10 again achieves the
best gain of approximately 1 BLEU and up to 3 TER.
LTM performs on par with or better than GTM, and
provides significant gains even in the NIST data set-
ting, showing that this method can be effectively ap-
plied directly on the sentence level to large training
5An avenue of future work would condition the sentence
topic distribution on a document distribution over topics (Teh
et al, 2006).
6As an empirical validation of our earlier intuition regarding
feature representation, presenting the features in the form of F1
caused the performance to remain virtually unchanged from the
baseline model.
Model MT03 MT05
?BLEU ?TER ?BLEU ?TER
BL 28.72 65.96 27.71 67.58
GTM-5 28.95ns 65.45 27.98ns 67.38ns
GTM-10 29.22 64.47 28.19 66.15
GTM-20 29.19 63.41 28.00ns 64.89
LTM-5 29.23 64.57 28.19 66.30
LTM-10 29.29 63.98 28.18 65.56
LTM-20 29.09ns 63.57 27.90ns 65.17
Model MT03 MT05
?BLEU ?TER ?BLEU ?TER
BL 34.31 61.14 30.63 65.10
MERT 34.60 60.66 30.53 64.56
LTM-5 35.21 59.48 31.47 62.34
LTM-10 35.32 59.16 31.56 62.01
LTM-20 33.90ns 60.89ns 30.12ns 63.87
Table 2: Performance using FBIS training corpus (top)
and NIST corpus (bottom). Improvements are significant
at the p <0.05 level, except where indicated (ns).
corpora which have no document markings. De-
pending on the diversity of training corpus, a vary-
ing number of underlying topics may be appropriate.
However, in both settings, 10 topics performed best.
4 Discussion and Conclusion
Applying SMT to new domains requires techniques
to inform our algorithms how best to adapt. This pa-
per extended the usual notion of domains to finer-
grained topic distributions induced in an unsuper-
vised fashion. We show that incorporating lexi-
cal weighting features conditioned on soft domain
membership directly into our model is an effective
strategy for dynamically biasing SMT towards rele-
vant translations, as evidenced by significant perfor-
mance gains. This method presents several advan-
tages over existing approaches. We can construct
a topic model once on the training data, and use
it infer topics on any test set to adapt the transla-
tion model. We can also incorporate large quanti-
ties of additional data (whether parallel or not) in
the source language to infer better topics without re-
lying on collection or genre annotations. Multilin-
gual topic models (Boyd-Graber and Resnik, 2010)
would provide a technique to use data from multiple
languages to ensure consistent topics.
118
Acknowledgments
Vladimir Eidelman is supported by a National De-
fense Science and Engineering Graduate Fellow-
ship. This work was also supported in part by
NSF grant #1018625, ARL Cooperative Agree-
ment W911NF-09-2-0072, and by the BOLT and
GALE programs of the Defense Advanced Research
Projects Agency, Contracts HR0011-12-C-0015 and
HR0011-06-2-001, respectively. Any opinions, find-
ings, conclusions, or recommendations expressed
are the authors? and do not necessarily reflect those
of the sponsors.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of Emperical Methods in Natural
Language Processing.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent Dirichlet Allocation.
Journal of Machine Learning Research, 3:2003.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual su-
pervised latent Dirichlet alocation. In Proceedings of
Emperical Methods in Natural Language Processing.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the Association for Computational Linguistics, pages
310?318.
David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In Pro-
ceedings of the Human Language Technology Confer-
ence.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of Emperical Methods in Natural Language Process-
ing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, Stroudsburg, PA, USA.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of Em-
perical Methods in Natural Language Processing.
A. K. Mccallum. 2002. MALLET: A Machine Learning
for Language Toolkit.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. In
Computational Linguistics, volume 29(21), pages 19?
51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 311?
318.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of Emperical
Methods in Natural Language Processing.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, 21(4):187?207.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the Association for Computational Lin-
guistics.
119
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 275?279,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Efficient Tree-Based Topic Modeling
Yuening Hu
Department of Computer Science
University of Maryland, College Park
ynhu@cs.umd.edu
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland, College Park
jbg@umiacs.umd.edu
Abstract
Topic modeling with a tree-based prior has
been used for a variety of applications be-
cause it can encode correlations between words
that traditional topic modeling cannot. How-
ever, its expressive power comes at the cost
of more complicated inference. We extend
the SPARSELDA (Yao et al, 2009) inference
scheme for latent Dirichlet alocation (LDA)
to tree-based topic models. This sampling
scheme computes the exact conditional distri-
bution for Gibbs sampling much more quickly
than enumerating all possible latent variable
assignments. We further improve performance
by iteratively refining the sampling distribution
only when needed. Experiments show that the
proposed techniques dramatically improve the
computation time.
1 Introduction
Topic models, exemplified by latent Dirichlet aloca-
tion (LDA) (Blei et al, 2003), discover latent themes
present in text collections. ?Topics? discovered by
topic models are multinomial probability distribu-
tions over words that evince thematic coherence.
Topic models are used in computational biology, com-
puter vision, music, and, of course, text analysis.
One of LDA?s virtues is that it is a simple model
that assumes a symmetric Dirichlet prior over its
word distributions. Recent work argues for structured
distributions that constrain clusters (Andrzejewski et
al., 2009), span languages (Jagarlamudi and Daume?
III, 2010), or incorporate human feedback (Hu et al,
2011) to improve the quality and flexibility of topic
modeling. These models all use different tree-based
prior distributions (Section 2).
These approaches are appealing because they
preserve conjugacy, making inference using Gibbs
sampling (Heinrich, 2004) straightforward. While
straightforward, inference isn?t cheap. Particularly
for interactive settings (Hu et al, 2011), efficient
inference would improve perceived latency.
SPARSELDA (Yao et al, 2009) is an efficient
Gibbs sampling algorithm for LDA based on a refac-
torization of the conditional topic distribution (re-
viewed in Section 3). However, it is not directly
applicable to tree-based priors. In Section 4, we pro-
vide a factorization for tree-based models within a
broadly applicable inference framework that empiri-
cally improves the efficiency of inference (Section 5).
2 Topic Modeling with Tree-Based Priors
Trees are intuitive methods for encoding human
knowledge. Abney and Light (1999) used tree-
structured multinomials to model selectional restric-
tions, which was later put into a Bayesian context
for topic modeling (Boyd-Graber et al, 2007). In
both cases, the tree came from WordNet (Miller,
1990), but the tree could also come from domain
experts (Andrzejewski et al, 2009).
Organizing words in this way induces correlations
that are mathematically impossible to represent with
a symmetric Dirichlet prior. To see how correlations
can occur, consider the generative process. Start with
a rooted tree structure that contains internal nodes
and leaf nodes. This skeleton is a prior that generates
K topics. Like vanilla LDA, these topics are distribu-
tions over words. Unlike vanilla LDA, their structure
correlates words. Internal nodes have a distribution
pik,i over children, where pik,i comes from per-node
Dirichlet parameterized by ?i.1 Each leaf node is
associated with a word, and each word must appear
in at least (possibly more than) one leaf node.
To generate a word from topic k, start at the root.
Select a child x0 ? Mult(pik,ROOT), and traverse
the tree until reaching a leaf node. Then emit the
leaf?s associated word. This walk replaces the draw
from a topic?s multinomial distribution over words.
1Choosing these Dirichlet priors specifies the direction (i.e.,
positive or negative) and strength of correlations that appear.
275
The rest of the generative process for LDA remains
the same, with ?, the per-document topic multinomial,
and z, the topic assignment.
This tree structure encodes correlations. The closer
types are in the tree, the more correlated they are.
Because types can appear in multiple leaf nodes, this
encodes polysemy. The path that generates a token is
an additional latent variable we must sample.
Gibbs sampling is straightforward because the tree-
based prior maintains conjugacy (Andrzejewski et
al., 2009). We integrate the per-document topic dis-
tributions ? and the transition distributions pi. The
remaining latent variables are the topic assignment z
and path l, which we sample jointly:2
p(z = k, l = ?|Z?, L?, w) (1)
? (?k + nk|d)
?
(i?j)??
?i?j + ni?j|k
?
j? (?i?j? + ni?j?|k)
where nk|d is topic k?s count in the document d;
?k is topic k?s prior; Z? and L? are topic and path
assignments excluding wd,n; ?i?j is the prior for
edge i ? j, ni?j|t is the count of edge i ? j in
topic k; and j? denotes other children of node i.
The complexity of computing the sampling distri-
bution is O(KLS) for models with K topics, paths
at most L nodes long, and at most S paths per word
type. In contrast, for vanilla LDA the analogous
conditional sampling distribution requires O(K).
3 Efficient LDA
The SPARSELDA (Yao et al, 2009) scheme for
speeding inference begins by rearranging LDA?s sam-
pling equation into three terms:3
p(z = k|Z?, w) ? (?k + nk|d)
? + nw|k
?V + n?|k
(2)
?
?k?
?V + n?|k
? ?? ?
sLDA
+
nk|d?
?V + n?|k
? ?? ?
rLDA
+
(?k + nk|d)nw|k
?V + n?|k
? ?? ?
qLDA
Following their lead, we call these three terms
?buckets?. A bucket is the total probability mass
marginalizing over latent variable assignments (i.e.,
sLDA ?
?
k
?k?
?V+n?|k
, similarly for the other buck-
ets). The three buckets are a smoothing only bucket
2For clarity, we omit indicators that ensure ? ends at wd,n.
3To ease notation we drop the d,n subscript for z and w in
this and future equations.
sLDA, document topic bucket rLDA, and topic word
bucket qLDA (we use the ?LDA? subscript to contrast
with our method, for which we use the same bucket
names without subscripts).
Caching the buckets? total mass speeds the compu-
tation of the sampling distribution. Bucket sLDA is
shared by all tokens, and bucket rLDA is shared by a
document?s tokens. Both have simple constant time
updates. Bucket qLDA has to be computed specifi-
cally for each token, but only for the (typically) few
types with non-zero counts in a topic.
To sample from the conditional distribution, first
sample which bucket you need and then (and only
then) select a topic within that bucket. Because the
topic-term bucket qLDA often has the largest mass
and has few non-zero terms, this speeds inference.
4 Efficient Inference in Tree-Based Models
In this section, we extend the sampling techniques
for SPARSELDA to tree-based topic modeling. We
first factor Equation 1:
p(z = k, l = ?|Z?, L?, w) (3)
? (?k + nk|d)N
?1
k,?[S? +Ok,?].
Henceforth we call Nk,? the normalizer for path ?
in topic k, S? the smoothing factor for path ?, and
Ok,? the observation for path ? in topic k, which are
Nk,? =
?
(i?j)??
?
j?
(?i?j? + ni?j?|k)
S? =
?
(i?j)??
?i?j (4)
Ok,? =
?
(i?j)??
(?i?j + ni?j|k)?
?
(i?j)??
?i?j .
Equation 3 can be rearranged in the same way
as Equation 5, yielding buckets analogous to
SPARSELDA?s,
p(z = k,l = ?|Z?, L?, w) (5)
?
?kS?
Nk,?
? ?? ?
s
+
nk|dS?
Nk,?
? ?? ?
r
+
(?k + nk|d)Ok,?
Nk,?
? ?? ?
q
.
Buckets sum both topics and paths. The sampling
process is much the same as for SPARSELDA: select
which bucket and then select a topic / path combina-
tion within the bucket (for a slightly more complex
example, see Algorithm 1).
276
Recall that one of the benefits of SPARSELDA was
that s was shared across tokens. This is no longer
possible, as Nk,? is distinct for each path in tree-
based LDA. Moreover, Nk,? is coupled; changing
ni?j|k in one path changes the normalizers of all
cousin paths (paths that share some node i).
This negates the benefit of caching s, but we re-
cover some of the benefits by splitting the normalizer
to two parts: the ?root? normalizer from the root node
(shared by all paths) and the ?downstream? normal-
izer. We precompute which paths share downstream
normalizers; all paths are partitioned into cousin sets,
defined as sets for which changing the count of one
member of the set changes the downstream normal-
izer of other paths in the set. Thus, when updating
the counts for path l, we only recompute Nk,l? for all
l? in the cousin set.
SPARSELDA?s computation of q, the topic-word
bucket, benefits from topics with unobserved (i.e.,
zero count) types. In our case, any non-zero path, a
path with any non-zero edge, contributes.4 To quickly
determine whether a path contributes, we introduce
an edge-masked count (EMC) for each path. Higher
order bits encode whether edges have been observed
and lower order bits encode the number of times the
path has been observed. For example, if a path of
length three only has its first two edges observed, its
EMC is 11000000. If the same path were observed
seven times, its EMC is 11100111. With this formu-
lation we can ignore any paths with a zero EMC.
Efficient sampling with refined bucket While
caching the sampling equation as described in the
previous section improved the efficiency, the smooth-
ing only bucket s is small, but computing the asso-
ciated mass is costly because it requires us to con-
sider all topics and paths. This is not a problem
for SparseLDA because s is shared across all tokens.
However, we can achieve computational gains with
an upper bound on s,
s =
?
k,?
?k
?
(i?j)?? ?i?j
?
(i?j)??
?
j? (?i?j? + ni?j?|k)
?
?
k,?
?k
?
(i?j)?? ?i?j
?
(i?j)??
?
j? ?i?j?
= s?. (6)
A sampling algorithm can take advantage of this
by not explicitly calculating s. Instead, we use s?
4C.f. observed paths, where all edges are non-zero.
as proxy, and only compute the exact s if we hit the
bucket s? (Algorithm 1). Removing s? and always
computing s yields the first algorithm in Section 4.
Algorithm 1 SAMPLING WITH REFINED BUCKET
1: for word w in this document do
2: sample = rand() ?(s? + r + q)
3: if sample < s? then
4: compute s
5: sample = sample ?(s+ r + q)/(s? + r + q)
6: if sample < s then
7: return topic k and path ? sampled from s
8: sample ? = s
9: else
10: sample ? = s?
11: if sample < r then
12: return topic k and path ? sampled from r
13: sample ? = r
14: return topic k and path ? sampled from q
Sorting Thus far, we described techniques for ef-
ficiently computing buckets, but quickly sampling
assignments within a bucket is also important. Here
we propose two techniques to consider latent vari-
able assignments in decreasing order of probability
mass. By considering fewer possible assignments,
we can speed sampling at the cost of the overhead
of maintaining sorted data structures. We sort top-
ics? prominence within a document (SD) and sort the
topics and paths of a word (SW).
Sorting topics? prominence within a document
(SD) can improve sampling from r and q; when we
need to sample within a bucket, we consider paths in
decreasing order of nk|d.
Sorting path prominence for a word (SW) can im-
prove our ability to sample from q. The edge-masked
count (EMC), as described above, serves as a proxy
for the probability of a path and topic. If, when sam-
pling a topic and path from q, we sample based on
the decreasing EMC, which roughly correlates with
path probability.
5 Experiments
In this section, we compare the running time5 of our
sampling algorithm (FAST) and our algorithm with
the refined bucket (RB) against the unfactored Gibbs
sampler (NAI?VE) and examine the effect of sorting.
Our corpus has editorials from New York Times
5Mean of five chains on a 6-Core 2.8-GHz CPU, 16GB RAM
277
Number of Topics
T50 T100 T200 T500
NAIVE 5.700 12.655 29.200 71.223
FAST 4.935 9.222 17.559 40.691
FAST-RB 2.937 4.037 5.880 8.551
FAST-RB-SD 2.675 3.795 5.400 8.363
FAST-RB-SW 2.449 3.363 4.894 7.404
FAST-RB-SDW 2.225 3.241 4.672 7.424
Vocabulary Size
V5000 V10000 V20000 V30000
NAI?VE 4.815 12.351 28.783 51.088
FAST 2.897 9.063 20.460 38.119
FAST-RB 1.012 3.900 9.777 20.040
FAST-RB-SD 0.972 3.684 9.287 18.685
FAST-RB-SW 0.889 3.376 8.406 16.640
FAST-RB-SDW 0.828 3.113 7.777 15.397
Number of Correlations
C50 C100 C200 C500
NAI?VE 11.166 12.586 13.000 15.377
FAST 8.889 9.165 9.177 8.079
FAST-RB 3.995 4.078 3.858 3.156
FAST-RB-SD 3.660 3.795 3.593 3.065
FAST-RB-SW 3.272 3.363 3.308 2.787
FAST-RB-SDW 3.026 3.241 3.091 2.627
Table 1: The average running time per iteration (S) over
100 iterations, averaged over 5 seeds. Experiments begin
with 100 topics, 100 correlations, vocab size 10000 and
then vary one dimension: number of topics (top), vocabu-
lary size (middle), and number of correlations (bottom).
from 1987 to 1996.6 Since we are interested in vary-
ing vocabulary size, we rank types by average tf-idf
and choose the top V . WordNet 3.0 generates the cor-
relations between types. For each synset in WordNet,
we generate a subtree with all types in the synset?
that are also in our vocabulary?as leaves connected
to a common parent. This subtree?s common parent
is then attached to the root node.
We compared the FAST and FAST-RB against
NAI?VE (Table 1) on different numbers of topics, var-
ious vocabulary sizes and different numbers of cor-
relations. FAST is consistently faster than NAI?VE
and FAST-RB is consistently faster than FAST. Their
benefits are clearer as distributions become sparse
(e.g., the first iteration for FAST is slower than later
iterations). Gains accumulate as the topic number
increases, but decrease a little with the vocabulary
size. While both sorting strategies reduce time, sort-
ing topics and paths for a word (SW) helps more than
sorting topics in a document (SD), and combining the
613284 documents, 41554 types, and 2714634 tokens.
1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.42
4
6
8
10
12
14
16
Average number of senses per constraint word
Aver
age 
runn
ing ti
me p
er ite
ratio
n (S)
 
 NaiveFastFast?RB
Fast?RB?sDFast?RB?sWFast?RB?sDW
Figure 1: The average running time per iteration against
the average number of senses per correlated words.
two is (with one exception) better than either alone.
As more correlations are added, NAI?VE?s time in-
creases while that of FAST-RB decreases. This is be-
cause the number of non-zero paths for uncorrelated
words decreases as more correlations are added to the
model. Since our techniques save computation for
every zero path, the overall computation decreases
as correlations push uncorrelated words to a limited
number of topics (Figure 1). Qualitatively, when the
synset with ?king? and ?baron? is added to a model,
it is associated with ?drug, inmate, colombia, water-
front, baron? in a topic; when ?king? is correlated
with ?queen?, the associated topic has ?king, parade,
museum, queen, jackson? as its most probable words.
These represent reasonable disambiguations. In con-
trast to previous approaches, inference speeds up as
topics become more semantically coherent (Boyd-
Graber et al, 2007).
6 Conclusion
We demonstrated efficient inference techniques for
topic models with tree-based priors. These methods
scale well, allowing for faster exploration of models
that use semantics to encode correlations without sac-
rificing accuracy. Improved scalability for such algo-
rithms, especially in distributed environments (Smola
and Narayanamurthy, 2010), could improve applica-
tions such as cross-language information retrieval,
unsupervised word sense disambiguation, and knowl-
edge discovery via interactive topic modeling.
278
Acknowledgments
We would like to thank David Mimno and the anony-
mous reviewers for their helpful comments. This
work was supported by the Army Research Labora-
tory through ARL Cooperative Agreement W911NF-
09-2-0072. Any opinions or conclusions expressed
are the authors? and do not necessarily reflect those
of the sponsors.
References
Steven Abney and Marc Light. 1999. Hiding a seman-
tic hierarchy in a Markov model. In Proceedings of
the Workshop on Unsupervised Learning in Natural
Language Processing.
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic mod-
eling via Dirichlet forest priors. In Proceedings of
International Conference of Machine Learning.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine Learn-
ing Research, 3:993?1022.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Gregor Heinrich. 2004. Parameter estima-
tion for text analysis. Technical report.
http://www.arbylon.net/publications/text-est.pdf.
Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff.
2011. Interactive topic modeling. In Association for
Computational Linguistics.
Jagadeesh Jagarlamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned corpora. In
Proceedings of the European Conference on Informa-
tion Retrieval (ECIR).
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264.
Alexander J. Smola and Shravan Narayanamurthy. 2010.
An architecture for parallel topic models. International
Conference on Very Large Databases, 3.
Limin Yao, David Mimno, and Andrew McCallum. 2009.
Efficient methods for topic model inference on stream-
ing document collections. In Knowledge Discovery and
Data Mining.
279
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 359?369,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Anchors Regularized: Adding Robustness and Extensibility
to Scalable Topic-Modeling Algorithms
Thang Nguyen
iSchool and UMIACS,
University of Maryland
and National Library of Medicine,
National Institutes of Health
daithang@umiacs.umd.edu
Yuening Hu
Computer Science
University of Maryland
ynhu@cs.umd.edu
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Abstract
Spectral methods offer scalable alternatives
to Markov chain Monte Carlo and expec-
tation maximization. However, these new
methods lack the rich priors associated with
probabilistic models. We examine Arora et
al.?s anchor words algorithm for topic mod-
eling and develop new, regularized algo-
rithms that not only mathematically resem-
ble Gaussian and Dirichlet priors but also
improve the interpretability of topic models.
Our new regularization approaches make
these efficient algorithms more flexible; we
also show that these methods can be com-
bined with informed priors.
1 Introduction
Topic models are of practical and theoretical inter-
est. Practically, they have been used to understand
political perspective (Paul and Girju, 2010), im-
prove machine translation (Eidelman et al, 2012),
reveal literary trends (Jockers, 2013), and under-
stand scientific discourse (Hall et al, 2008). The-
oretically, their latent variable formulation has
served as a foundation for more robust models
of other linguistic phenomena (Brody and Lapata,
2009).
Modern topic models are formulated as a la-
tent variable model. Like hidden Markov mod-
els (Rabiner, 1989, HMM), each token comes from
one of K unknown distributions. Unlike a HMM,
topic models assume that each document is an ad-
mixture of these hidden components called topics.
Posterior inference discovers the hidden variables
that best explain a dataset. Typical solutions use
MCMC (Griffiths and Steyvers, 2004) or variational
EM (Blei et al, 2003), which can be viewed as local
optimization: searching for the latent variables that
maximize the data likelihood.
An exciting vein of new research provides
provable polynomial-time alternatives. These ap-
proaches provide solutions to hidden Markov mod-
els (Anandkumar et al, 2012), mixture mod-
els (Kannan et al, 2005), and latent variable gram-
mars (Cohen et al, 2013). The key insight is not to
directly optimize observation likelihood but to in-
stead discover latent variables that can reconstruct
statistics of the assumed generative model. Unlike
search-based methods, which can be caught in lo-
cal minima, these techniques are often guaranteed
to find global optima.
These general techniques can be improved by
making reasonable assumptions about the models.
For example, Arora et al (2012b)?s approach for in-
ference in topic models assume that each topic has
a unique ?anchor? word (thus, we call this approach
anchor). This approach is fast and effective; be-
cause it only uses word co-occurrence information,
it can scale to much larger datasets than MCMC or
EM alternatives. We review the anchor method in
Section 2.
Despite their advantages, these techniques are
not a panacea. They do not accommodate the
rich priors that modelers have come to expect.
Priors can improve performance (Wallach et al,
2009), provide domain adaptation (Daum?e III,
2007; Finkel and Manning, 2009), and guide mod-
els to reflect users? needs (Hu et al, 2013). In
Section 3, we regularize the anchor method to
trade-off the reconstruction fidelity with the penalty
terms that mimic Gaussian and Dirichlet priors.
Another shortcoming is that these models have
not been scrutinized using standard NLP evalua-
tions. Because these approaches emerged from
the theory community, anchor?s evaluations, when
present, typically use training reconstruction. In
Section 4, we show that our regularized models can
generalize to previously unseen data?as measured
by held-out likelihood (Blei et al, 2003)?and are
more interpretable (Chang et al, 2009; Newman
et al, 2010). We also show that our extension to
the anchor method enables new applications: for
359
K number of topics
V vocabulary size
M document frequency: minimum documents an an-
chor word candidate must appear in
Q word co-occurrence matrix
Q
i,j
= p(w
1
= i, w
2
= j)
?Q conditional distribution of Q
?
Q
i,j
= p(w
1
= j |w
2
= i)
?Q
i,?
row i of
?
Q
A topic matrix, of size V ?K
A
j,k
= p(w = j | z = k)
C anchor coefficient of size K ? V
C
j,k
= p(z = k |w = j)
S set of anchor word indexes {s
1
, . . . s
K
}
? regularization weight
Table 1: Notation used. Matrices are in bold
(Q,C), sets are in script S
example, using an informed priors to discover con-
cepts of interest.
Having shown that regularization does improve
performance, in Section 5 we explore why. We
discuss the trade-off of training data reconstruction
with sparsity and why regularized topics are more
interpretable.
2 Anchor Words: Scalable Topic Models
In this section, we briefly review the anchor
method and place it in the context of topic model
inference. Once we have established the anchor
objective function, in the next section we regularize
the objective function.
Rethinking Data: Word Co-occurrence Infer-
ence in topic models can be viewed as a black box:
given a set of documents, discover the topics that
best explain the data. The difference between an-
chor and conventional inference is that while con-
ventional methods take a collection of documents
as input, anchor takes word co-occurrence statis-
tics. Given a vocabulary of size V , we represent
this joint distribution asQ
i,j
= p(w
1
= i, w
2
= j),
each cell represents the probability of words appear-
ing together in a document.
Like other topic modeling algorithms, the output
of the anchor method is the topic word distribu-
tions A with size V ? K, where K is the total
number of topics desired, a parameter of the al-
gorithm. The k
th
column of A will be the topic
distribution over all words for topic k, and A
w,k
is
the probability of observing type w given topic k.
Anchors: Topic Representatives The anchor
method (Arora et al, 2012a) is based on the sepa-
rability assumption (Donoho and Stodden, 2003),
which assumes that each topic contains at least one
namesake ?anchor word? that has non-zero proba-
bility only in that topic. Intuitively, this means that
each topic has unique, specific word that, when
used, identifies that topic. For example, while
?run?, ?base?, ?fly?, and ?shortstop? are associated
with a topic about baseball, only ?shortstop? is un-
ambiguous, so it could serve as this topic?s anchor
word.
Let?s assume that we knew what the anchor
words were: a set S that indexes rows in Q. Now
consider the conditional distribution of word i,
the probability of the rest of the vocabulary given
an observation of word i; we represent this as
?
Q
i,?
,
as we can construct this by normalizing the rows of
Q. For an anchor word s
a
? S, this will look like
a topic;
?
Q
?shortstop?,?
will have high probability
for words associated with baseball.
The key insight of the anchor algorithm is that
the conditional distribution of polysemous non-
anchor words can be reconstructed as a linear com-
bination of the conditional distributions of anchor
words. For example,
?
Q
?fly?,?
could be recon-
structed by combining the anchor words ?insecta?,
?boeing?, and ?shortshop?. We represent the coeffi-
cients of this reconstruction as a matrix C, where
C
i,k
= p(z = k |w = i). Thus, for any word i,
?
Q
i,?
?
?
s
k
?S
C
i,k
?
Q
s
k
,?
. (1)
The coefficient matrix is not the usual output of a
topic modeling algorithm. The usual output is the
probability of a word given a topic. The coefficient
matrix C is the probability of a topic given a word.
We use Bayes rule to recover the topic distribution
p(w = i|z = k) ?
A
i,k
? p(z = k|w = i)p(w = i)
= C
i,k
?
j
?
Q
i,j
(2)
where p(w) is the normalizer of Q to obtain
?
Q
w,?
.
The geometric argument for finding the anchor
words is one of the key contributions of Arora et
al. (2012a) and is beyond the scope of this paper.
The algorithms in Section 3 use the anchor selec-
tion subroutine unchanged. The difference in our
approach is in how we discover the anchor coeffi-
cients C.
From Anchors to Topics After we have the an-
chor words, we need to find the coefficients that
360
best reconstruct the data
?
Q (Equation 1). Arora
et al (2012a) chose the C that minimizes the KL
divergence between
?
Q
i,?
and the reconstruction
based on the anchor word?s conditional word vec-
tors
?
s
k
?S
C
i,k
?
Q
s
k
,?
,
C
i,?
= argminC
i,?
D
KL
?
?
?
Q
i,?
||
?
s
k
?S
C
i,k
?
Q
s
k
,?
?
?
.
(3)
The anchor method is fast, as it only de-
pends on the size of the vocabulary once the co-
occurrence statistics Q are obtained. However, it
does not support rich priors for topic models, while
MCMC (Griffiths and Steyvers, 2004) and varia-
tional EM (Blei et al, 2003) methods can. This
prevents models from using priors to guide the
models to discover particular themes (Zhai et al,
2012), or to encourage sparsity in the models (Yao
et al, 2009). In the rest of this paper, we correct
this lacuna by adding regularization inspired by
Bayesian priors to the anchor algorithm.
3 Adding Regularization
In this section, we add regularizers to the anchor
objective (Equation 3). In this section, we briefly
review regularizers and then add two regularizers,
inspired by Gaussian (L
2
, Section 3.1) and Dirich-
let priors (Beta, Section 3.2), to the anchor objec-
tive function (Equation 3).
Regularization terms are ubiquitous. They typ-
ically appear as an additional term in an opti-
mization problem. Instead of optimizing a func-
tion just of the data x and parameters ?, f(x, ?),
one optimizes an objective function that includes
a regularizer that is only a function of parame-
ters: f(w, ?) + r(?). Regularizers are critical in
staid methods like linear regression (Ng, 2004),
in workhorse methods such as maximum entropy
modeling (Dud??k et al, 2004), and also in emerging
fields such as deep learning (Wager et al, 2013).
In addition to being useful, regularization terms
are appealing theoretically because they often corre-
spond to probabilistic interpretations of parameters.
For example, if we are seeking the MLE of a proba-
bilistic model parameterized by ?, p(x|?), adding
a regularization term r(?) =
?
L
i=1
?
2
i
corresponds
to adding a Gaussian prior
f(?
i
) =
1
?
2pi?
2
exp
{
?
?
2
i
2?
2
}
(4)
Corpus Train Dev Test Vocab
NIPS 1231 247 262 12182
20NEWS 11243 3760 3726 81604
NYT 9255 2012 1959 34940
Table 2: The number of documents in the train,
development, and test folds in our three datasets.
and maximizing log probability of the posterior
(ignoring constant terms) (Rennie, 2003).
3.1 L
2
Regularization
The simplest form of regularization we can add is
L
2
regularization. This is similar to assuming that
probability of a word given a topic comes from a
Gaussian distribution. While the distribution over
topics is typically Dirichlet, Dirichlet distributions
have been replaced by logistic normals in topic
modeling applications (Blei and Lafferty, 2005)
and for probabilistic grammars of language (Cohen
and Smith, 2009).
Augmenting the anchor objective with an L
2
penalty yields
C
i,?
=argmin
C
i,?
D
KL
?
?
?
Q
i,?
||
?
s
k
?S
C
i,k
?
Q
s
k
,?
?
?
+ ??C
i,?
? ?
i,?
?
2
2
, (5)
where regularization weight ? balances the impor-
tance of a high-fidelity reconstruction against the
regularization, which encourages the anchor coeffi-
cients to be close to the vector ?. When the mean
vector ? is zero, this encourages the topic coeffi-
cients to be zero. In Section 4.3, we use a non-zero
mean ? to encode an informed prior to encourage
topics to discover specific concepts.
3.2 Beta Regularization
The more common prior for topic models is a
Dirichlet prior (Minka, 2000). However, we cannot
apply this directly because the optimization is done
on a row-by-row basis of the anchor coefficient
matrix C, optimizing C for a fixed word w for and
all topics. If we want to model the probability of
a word, it must be the probability of word w in a
topic versus all other words.
Modeling this dichotomy (one versus all others
in a topic) is possible. The constructive definition
of the Dirichlet distribution (Sethuraman, 1994)
states that if one has a V -dimensional multinomial
? ? Dir(?
1
. . . ?
V
), then the marginal distribution
361
of ?
w
follows ?
w
? Beta(?
w
,
?
i 6=w
?
i
). This is
the tool we need to consider the distribution of a
single word?s probability.
This requires including the topic matrix as part
of the objective function. The topic matrix is a lin-
ear transformation of the coefficient matrix (Equa-
tion 2). The objective for beta regularization be-
comes
C
i,?
=argmin
C
i,?
D
KL
?
?
?
Q
i,?
||
?
s
k
?S
C
i,k
?
Q
s
k
,?
?
?
? ?
?
s
k
?S
log (Beta(A
i,k
; a, b)), (6)
where ? again balances reconstruction against the
regularization. To ensure the tractability of this
algorithm, we enforce a convex regularization func-
tion, which requires that a > 1 and b > 1. If we
enforce a uniform prior?E
Beta(a,b)
[A
i,k
] =
1
V
?
and that the mode of the distribution is also
1
V
,
1
this gives us the following parametric form for a
and b:
a =
x
V
+ 1, and b =
(V ? 1)x
V
+ 1 (7)
for real x greater than zero.
3.3 Initialization and Convergence
Equation 5 and Equation 6 are optimized using L-
BFGS gradient optimization (Galassi et al, 2003).
We initialize C randomly from Dir(?) with ? =
60
V
(Wallach et al, 2009). We update C after opti-
mizing all V rows. The newly updated C replaces
the old topic coefficients. We track how much
the topic coefficients C change between two con-
secutive iterations i and i + 1 and represent it as
?C ? ?C
i+1
?C
i
?
2
. We stop optimization when
?C ? ?. When ? = 0.1, the L
2
and unregularized
anchor algorithm converges after a single iteration,
while beta regularization typically converges after
fewer than ten iterations (Figure 4).
4 Regularization Improves Topic Models
In this section, we measure the performance of
our proposed regularized anchor word algorithms.
We will refer to specific algorithms in bold. For
example, the original anchor algorithm is an-
chor. Our L
2
regularized variant is anchor-L
2
,
1
For a, b < 1, the expected value is still the uniform
distribution but the mode lies at the boundaries of the simplex.
This corresponds to a sparse Dirichlet distribution, which our
optimization cannot at present model.
and our beta regularized variant is anchor-beta.
To provide conventional baselines, we also com-
pare our methods against topic models from varia-
tional inference (Blei et al, 2003, variational) and
MCMC (Griffiths and Steyvers, 2004; McCallum,
2002, MCMC).
We apply these inference strategies on three di-
verse corpora: scientific articles from the Neural
Information Processing Society (NIPS),
2
Internet
newsgroups postings (20NEWS),
3
and New York
Times editorials (Sandhaus, 2008, NYT). Statistics
for the datasets are summarized in Table 2. We split
each dataset into a training fold (70%), develop-
ment fold (15%), and a test fold (15%): the training
data are used to fit models; the development set are
used to select parameters (anchor thresholdM , doc-
ument prior ?, regularization weight ?); and final
results are reported on the test fold.
We use two evaluation measures, held-out likeli-
hood (Blei et al, 2003, HL) and topic interpretabil-
ity (Chang et al, 2009; Newman et al, 2010, TI).
Held-out likelihood measures how well the model
can reconstruct held-out documents that the model
has never seen before. This is the typical evaluation
for probabilistic models. Topic interpretability is a
more recent metric to capture how useful the topics
can be to human users attempting to make sense of
a large datasets.
Held-out likelihood cannot be computed with
existing anchor algorithms, so we use the topic
distributions learned from anchor as input to a ref-
erence variational inference implementation (Blei
et al, 2003) to compute HL. This requires an ad-
ditional parameter, the Dirichlet prior ? for the
per-document distribution over topics. We select ?
using grid search on the development set.
To compute TI and evaluate topic coherence,
we use normalized pairwise mutual informa-
tion (NPMI) (Lau et al, 2014) over topics? twenty
most probable words. Topic coherence is com-
puted against the NPMI of a reference corpus. For
coherence evaluations, we use both intrinsic and
extrinsic text collections to compute NPMI. Intrin-
sic coherence (TI-i) is computed on training and
development data at development time and on train-
ing and test data at test time. Extrinsic coherence
(TI-e) is computed from English Wikipedia articles,
with disjoint halves (1.1 million pages each) for
distinct development and testing TI-e evaluation.
2
http://cs.nyu.edu/
?
roweis/data.html
3
http://qwone.com/
?
jason/20Newsgroups/
362
ll l l l
l
l
l
l
l
l
l
l
l l
l
l
l
l l l
l l l l
?392?390
?388
?4720?4710
?4700?4690
?4680
?890.0?887.5
?885.0?882.5
20NEWS
NIPS
NYT
100 300 500 700 900Document Frequency M
HL 
Sco
re l l
l l l
l
l
l
l
l
l l l
l
l
l
l
l
l
l l
l l l l
0.020.03
0.040.05
0.060.07
0.0550.060
0.065
0.060.07
0.080.09
0.10
20NEWS
NIPS
NYT
100 300 500 700 900Document Frequency M
TI?
i Sc
ore
Figure 1: Grid search for document frequency M for our datasets with 20 topics (other configurations not
shown) on development data. The performance on both HL and TI score indicate that the unregularized
anchor algorithm is very sensitive to M . The M selected here is applied to subsequent models.
Topics l 20 40 60 80
Beta L2
l l l l l llllll l l l l lllll
l l l l l llllll l l l l lllll
l l l l l llllll
l l l l lllll
l l l l l llllll l l l l lllll
l l l l l llllll l l l l lllll
l l l l l llllll l l l l lllll
?410?405
?400?395
?390
?4800?4750
?4700?4650
?920?910
?900?890
?880
20NEWS
NIPS
NYT
00.01 0.1 0.5 1 00.01 0.1 0.5 1Regularization Weight ?
HL 
Sco
re
Topics l 20 40 60 80
Beta L2
l
l l l
l llllll l l
l l lll
l
l
l l
l l l llllll l l
l
l l
l
lll
l l
l l l llllll
l l l l lllll
l l l l l llllll l l l l lllll
l l l l l llllll
l l l l lllll
l l l l l llllll l l l l lllll
0.020.04
0.060.08
0.10
0.020.04
0.060.08
0.060.09
0.120.15
20NEWS
NIPS
NYT
0 0.01 0.1 0.5 1 0 0.01 0.1 0.5 1Regularization Weight ?
TI?
i Sc
ore
Figure 2: Selection of ? based on HL and TI scores on the development set. The value of ? = 0 is
equivalent to the original anchor algorithm; regularized versions find better solutions as the regularization
weight ? becomes non-zero.
4.1 Grid Search for Parameters on
Development Set
Anchor Threshold A good anchor word must
have a unique, specific context but also explain
other words well. A word that appears only once
will have a very specific cooccurence pattern but
will explain other words? coocurrence poorly be-
cause the observations are so sparse. As discussed
in Section 2, the anchor method uses document
frequency M as a threshold to only consider words
with robust counts.
Because all regularizations benefit equally
from higher-quality anchor words, we use cross-
validation to select the document frequency cut-
off M using the unregularized anchor algorithm.
Figure 1 shows the performance of anchor with
different M on our three datasets with 20 topics for
our two measures HL and TI-i.
Regularization Weight Once we select a cutoff
M for each combination of dataset, number of top-
ics K and a evaluation measure, we select a reg-
ularization weight ? on the development set. Fig-
ure 2 shows that beta regularization framework im-
proves topic interpretability TI-i on all datasets and
improved the held-out likelihood HL on 20NEWS.
The L
2
regularization also improves held-out like-
lihood HL for the 20NEWS corpus (Figure 2).
In the interests of space, we do not show the
figures for selecting M and ? using TI-e, which is
similar to TI-i: anchor-beta improves TI-e score on
all datasets, anchor-L
2
improves TI-e on 20NEWS
and NIPS with 20 topics and NYT with 40 topics.
4.2 Evaluating Regularization
With document frequency M and regularization
weight ? selected from the development set, we
363
compare the performance of those models on the
test set. We also compare with standard implemen-
tations of Latent Dirichlet Allocation: Blei?s LDAC
(variational) and Mallet (mcmc). We run 100 iter-
ations for LDAC and 5000 iterations for Mallet.
Each result is averaged over three random runs
and appears in Figure 3. The highly-tuned, widely-
used implementations uniformly have better held-
out likelihood than anchor-based methods, but the
much faster anchor methods are often comparable.
Within anchor-based methods, L
2
-regularization
offers comparable held-out likelihood as unregular-
ized anchor, while anchor-beta often has better
interpretability. Because of the mismatch between
the specialized vocabulary of NIPS and the general-
purpose language of Wikipedia, TI-e has a high
variance.
4.3 Informed Regularization
A frequent use of priors is to add information to a
model. This is not possible with the existing an-
chor method. An informed prior for topic models
seeds a topic with words that describe a topic of in-
terest. In a topic model, these seeds will serve as a
?magnet?, attracting similar words to the topic (Zhai
et al, 2012).
We can achieve a similar goal with anchor-L
2
.
Instead of encouraging anchor coefficients to be
zero in Equation 5, we can instead encourage word
probabilities to close to an arbitrary mean ?
i,k
.
This vector can reflect expert knowledge.
One example of a source of expert knowledge
is Linguistic Inquiry and Word Count (Pennebaker
and Francis, 1999, LIWC), a dictionary of key-
words related to sixty-eight psychological concepts
such as positive emotions, negative emotions, and
death. For example, it associates ?excessive, estate,
money, cheap, expensive, living, profit, live, rich,
income, poor, etc.? for the concept materialism.
We associate each anchor word with its closest
LIWC category based on the cooccurrence matrix
Q. This is computed by greedily finding the an-
chor word that has the highest cooccurrence score
for any LIWC category: we define the score of a
category to anchor word w
s
k
as
?
i
Q
s
k
,i
, where i
ranges over words in this category; we compute the
scores of all categories to all anchor words; then
we find the highest score and assign the category to
that anchor word; we greedily repeat this process
until all anchor words have a category.
Given these associations, we create a goal mean
?
i,k
. If there are L
i
anchor words associated with
LIWC word i, ?
i,k
=
1
L
i
if this keyword i is associ-
ated with anchor word w
s
k
and zero otherwise.
We apply anchor-L
2
with informed priors on
NYT with twenty topics and compared the topics
against the original topics from anchor. Table 3
shows that the topic with anchor word ?soviet?,
when combined with LIWC, draws in the new words
?bush? and ?nuclear?; reflecting the threats of force
during the cold war. For the topic with topic word
?arms?, when associated with the LIWC category
with the terms ?agree? and ?agreement?, draws
in ?clinton?, who represented a more conciliatory
foreign policy compared to his republican prede-
cessors.
5 Discussion
Having shown that regularization can improve the
anchor topic modeling algorithm, in this section
we discuss why these regularizations can improve
the model and the implications for practitioners.
Efficiency Efficiency is a function of the number
of iterations and the cost of each iteration. Both
anchor and anchor-L
2
require a single iteration,
although the latter?s iteration is slightly more ex-
pensive. For beta, as described in Section 3.2,
we update anchor coefficients C row by row, and
then repeat the process over several iterations until
it converges. However, it often converges within
ten iterations (Figure 4) on all three datasets: this
requires much fewer iterations than MCMC or vari-
ational inference, and the iterations are less expen-
sive. In addition, since we optimize each row C
i,?
independently, the algorithm can be easily paral-
lelized.
Sensitivity to Document Frequency While the
original anchor is sensitive to the document fre-
quency M (Figure 1), adding regularization makes
this less critical. Both anchor-L
2
and anchor-beta
are less sensitive to M than anchor.
To highlight this, we compare the topics of an-
chor and anchor-beta whenM = 100. As Table 4
shows, the words ?article?, ?write?, ?don? and
?doe? appear in most of anchor?s topics. While
anchor-L
2
also has some bad topics, it still can find
reasonable topics, demonstrating anchor-beta?s
greater robustness to suboptimal M .
L
2
(Sometimes) Improves Generalization As
Figure 2 shows, anchor-L
2
sometimes improves
held-out development likelihood for the smaller
364
Algorithm l anchor anchor?beta anchor?L2 MCMC variational
20NEWS
l l l l
l l l l
l
l l l
?410
?405
?400
?395
?390
0.03
0.04
0.05
0.06
0.07
0.06
0.08
0.10
HL
TI?e
TI?i
20 40 60 80
topic number
NIPS
l
l
l l
l
l l
l
l l l l
?4580
?4560
?4540
?4520
?4500
?4480
?4460
0.08
0.09
0.10
0.11
0.06
0.07
0.08
0.09
HL
TI?e
TI?i
20 40 60 80
topic number
NYT
l l
l l
l
l l l
l l l l
?880
?870
?860
0.07
0.08
0.09
0.08
0.10
0.12
0.14
HL
TI?e
TI?i
20 40 60 80
topic number
Figure 3: Comparing anchor-beta and anchor-L
2
against the original anchor and the traditional vari-
ational and MCMC on HL score and TI score. variational and mcmc provide the best held-out gener-
alization. anchor-beta sometimes gives the best TI score and is consistently better than anchor. The
specialized vocabulary of NIPS causes high variance for the extrinsic interpretability evaluation (TI-e).
Topic Shared Words Original (Top, green) vs. Informed L
2
(Bottom, orange)
soviet
american make president soviet union
war years
gorbachev moscow russian force economic world europe politi-
cal communist lead reform germany country
military state service washington bush army unite chief troops
officer nuclear time week
district
assembly board city county district
member state york
representative manhattan brooklyn queens election bronx council
island local incumbent housing municipal
people party group social republican year make years friend
vote compromise million
peace
american force government israel peace
political president state unite
washington
war military country minister leaders nation world palestinian
israeli election
offer justice aid deserve make bush years fair clinton hand
arms
arms bush congress force iraq make north
nuclear president state washington weapon
administration treaty missile defense war military korea
reagan
agree agreement american accept unite share clinton
years
trade
administration america american country
economic government make president state
trade unite washington
world market japan foreign china policy price political
business economy congress year years clinton bush
buy
Table 3: Examples of topic comparison between anchor and informed anchor-L
2
. A topic is labeled
with the anchor word for that topic. The bold words are the informed prior from LIWC. With an informed
prior, relevant words appear in the top words of a topic; this also draws in other related terms (red).
20NEWS corpus. However, the ? selected on devel-
opment data does not always improve test set per-
formance. This, in Figure 3, anchor-beta closely
tracks anchor. Thus, L
2
regularization does not
hurt generalization while imparting expressiveness
and robustness to parameter settings.
Beta Improves Interpretability Figure 3 shows
that anchor-beta improves topic interpretability
(TI) compared to unregularized anchor methods. In
this section, we try to understand why.
We first compare the topics from the original
anchor against anchor-beta to analyze the topics
qualitatively. Table 5 shows that beta regulariza-
tion promotes rarer words within a topic and de-
motes common words. For example, in the topic
about hockey with the anchor word game, ?run?
and ?good??ambiguous, polysemous words?in
the unregularized topic are replaced by ?playoff?
365
Topic anchor anchor-beta
frequently
article write don doe make time people good
file question
article write don doe make people time good
email file
debate
write article people make don doe god key gov-
ernment time
people make god article write don doe key
point government
wings game team write wings article win red play
hockey year
game team wings win red hockey play season
player fan
stats player team write game article stats year good
play doe
stats player season league baseball fan team in-
dividual playoff nhl
compile program file write email doe windows call prob-
lem run don
compile program code file ftp advance package
error windows sun
Table 4: Topics from anchor and anchor-beta with M = 100 on 20NEWS with 20 topics. Each topic is
identified with its associated anchor word. When M = 100, the topics of anchor suffer: the four colored
words appear in almost every topic. anchor-beta, in contrast, is less sensitive to suboptimal M .
l
l
l
l l l l l l l l l l l l l l l l l l0
10
20
30
40
0 5 10 15 20Iteration
?C
Dataset l 20NEWS NIPS NYT
Figure 4: Convergence of anchor coefficient C for
anchor-beta. ?C is the difference of current C
from theC at the previous iteration. C is converged
within ten iterations for all three datasets.
and ?trade? in the regularized topic. These words
are less ambiguous and more likely to make sense
to a consumer of topic models.
Figure 5 shows why this happens. Compared
to the unregularized topics from anchor, the beta
regularized topic steals from the rich and creates a
more uniform distribution. Thus, highly frequent
words do not as easily climb to the top of the distri-
bution, and the topics reflect topical, relevant words
rather than globally frequent terms.
6 Conclusion
A topic model is a popular tool for quickly get-
ting the gist of large corpora. However, running
such an analysis on these large corpora entail a
substantial computational cost. While techniques
such as anchor algorithms offer faster solutions, it
comes at the cost of the expressive priors common
in Bayesian formulations.
This paper introduces two different regulariza-
tions that offer users more interpretable models
and the ability to inject prior knowledge without
sacrificing the speed and generalizability of the
underlying approach. However, one sacrifice that
this approach does make is the beautiful theoretical
guarantees of previous work. An important piece
of future work is a theoretical understanding of
generalizability in extensible, regularized models.
Incorporating other regularizations could further
improve performance or unlock new applications.
Our regularizations do not explicitly encourage
sparsity; applying other regularizations such as L
1
could encourage true sparsity (Tibshirani, 1994),
and structured priors (Andrzejewski et al, 2009)
could efficiently incorporate constraints on topic
models.
These regularizations could improve spectral al-
gorithms for latent variables models, improving the
performance for other NLP tasks such as latent vari-
able PCFGs (Cohen et al, 2013) and HMMs (Anand-
kumar et al, 2012), combining the flexibility and
robustness offered by priors with the speed and
accuracy of new, scalable algorithms.
Acknowledgments
We would like to thank the anonymous reviewers,
Hal Daum?e III, Ke Wu, and Ke Zhai for their help-
ful comments. This work was supported by NSF
Grant IIS-1320538. Boyd-Graber is also supported
by NSF Grant CCF-1018625. Any opinions, find-
ings, conclusions, or recommendations expressed
here are those of the authors and do not necessarily
reflect the view of the sponsor.
366
computer drive game god power
?20
?15
?10
?5
0
?20
?15
?10
?5
0
anchor
anchor?beta
Rank of word in topic (topic shown by anchor word)
log
 p(
wo
rd
 | to
pic
)
Figure 5: How beta regularization influences the topic distribution. Each topic is identified with its
associated anchor word. Compared to the unregularized anchor method, anchor-beta steals probability
mass from the ?rich? and prefers a smoother distribution of probability mass. These words often tend to
be unimportant, polysemous words common across topics.
Topic Shared Words anchor (Top, green) vs. anchor-beta (Bottom, orange)
computer computer means science screen
system phone university problem doe work windows internet
software chip mac set fax technology information data
quote mhz pro processor ship remote print devices complex cpu
electrical transfer ray engineering serial reduce
power
power play period supply
ground light battery engine
car good make high problem work back turn control current
small time
circuit oil wire unit water heat hot ranger input total joe plug
god
god jesus christian bible faith church life christ belief
religion hell word lord truth love
people make things true doe
sin christianity atheist peace heaven
game
game team player play win fan hockey season baseball
red wings score division league goal leaf cup toronto
run good
playoff trade
drive
drive disk hard scsi controller card floppy ide mac bus
speed monitor switch apple cable internal port meg
problem work
ram pin
Table 5: Comparing topics?labeled by their anchor word?from anchor and anchor-beta. With beta
regularization, relevant words are promoted, while more general words are suppressed, improving topic
coherence.
References
Animashree Anandkumar, Daniel Hsu, and Sham M.
Kakade. 2012. A method of moments for mixture
models and hidden markov models. In Proceedings
of Conference on Learning Theory.
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In Proceedings
of the International Conference of Machine Learn-
ing.
Sanjeev Arora, Rong Ge, Yoni Halpern, David M.
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2012a. A practical algorithm
for topic modeling with provable guarantees. CoRR,
abs/1212.4777.
Sanjeev Arora, Rong Ge, and Ankur Moitra. 2012b.
Learning topic models - going beyond svd. CoRR,
abs/1204.1956.
David M. Blei and John D. Lafferty. 2005. Correlated
topic models. In Proceedings of Advances in Neural
Information Processing Systems.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine
Learning Research, 3.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the Euro-
pean Chapter of the Association for Computational
Linguistics, Athens, Greece.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Proceedings of Advances in Neural Information Pro-
cessing Systems.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying
in unsupervised grammar induction. In Conference
of the North American Chapter of the Association
for Computational Linguistics.
367
Shay Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2013. Experiments with
spectral learning of latent-variable PCFGs. In Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the Association for Com-
putational Linguistics.
David Donoho and Victoria Stodden. 2003. When
does non-negative matrix factorization give correct
decomposition into parts? page 2004. MIT Press.
Miroslav Dud??k, Steven J. Phillips, and Robert E.
Schapire. 2004. Performance guarantees for reg-
ularized maximum entropy density estimation. In
Proceedings of Conference on Learning Theory.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the Association
for Computational Linguistics.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Morristown, NJ,
USA.
Mark Galassi, Jim Davies, James Theiler, Brian Gough,
Gerard Jungman, Michael Booth, and Fabrice Rossi.
2003. Gnu Scientific Library: Reference Manual.
Network Theory Ltd.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl 1):5228?5235.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In Proceedings of Emperical Methods
in Natural Language Processing.
Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,
and Alison Smith. 2013. Interactive topic modeling.
Machine Learning Journal.
Matt L. Jockers. 2013. Macroanalysis: Digital Meth-
ods and Literary History. Topics in the Digital Hu-
manities. University of Illinois Press.
Ravindran Kannan, Hadi Salmasian, and Santosh Vem-
pala. 2005. The spectral method for general mixture
models. In Proceedings of Conference on Learning
Theory.
Ken Lang. 2007. 20 newsgroups data set.
Jey Han Lau, David Newman, and Timothy Baldwin.
2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality.
In Proceedings of the European Chapter of the Asso-
ciation for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Thomas P. Minka. 2000. Estimating a
dirichlet distribution. Technical report, Mi-
crosoft. http://research.microsoft.com/en-
us/um/people/minka/papers/dirichlet/.
David Newman, Jey Han Lau, Karl Grieser, and Timo-
thy Baldwin. 2010. Automatic evaluation of topic
coherence. In Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regu-
larization, and rotational invariance. In Proceedings
of the International Conference of Machine Learn-
ing.
Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering
multi-faceted topics. In Association for the Advance-
ment of Artificial Intelligence.
James W. Pennebaker and Martha E. Francis. 1999.
Linguistic Inquiry and Word Count. Lawrence Erl-
baum, 1 edition, August.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
286.
Jason Rennie. 2003. On l2-norm regularization and
the Gaussian prior.
Sam Roweis. 2002. NIPS 1-12 Dataset.
Evan Sandhaus. 2008. The New
York Times annotated corpus.
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2008T19.
Jayaram Sethuraman. 1994. A constructive definition
of Dirichlet priors. Statistica Sinica, 4:639?650.
Robert Tibshirani. 1994. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society, Series B, 58:267?288.
Stefan Wager, Sida Wang, and Percy Liang. 2013.
Dropout training as adaptive regularization. In Pro-
ceedings of Advances in Neural Information Pro-
cessing Systems, pages 351?359.
Hanna Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking LDA: Why priors matter.
In Proceedings of Advances in Neural Information
Processing Systems.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. In Knowledge
Discovery and Data Mining.
368
Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-
hamad Alkhouja. 2012. Mr. LDA: A flexible large
scale topic modeling package using variational infer-
ence in mapreduce. In Proceedings of World Wide
Web Conference.
369
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113?1122,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Political Ideology Detection Using Recursive Neural Networks
Mohit Iyyer
1
, Peter Enns
2
, Jordan Boyd-Graber
3,4
, Philip Resnik
2,4
1
Computer Science,
2
Linguistics,
3
iSchool, and
4
UMIACS
University of Maryland
{miyyer,peter,jbg}@umiacs.umd.edu, resnik@umd.edu
Abstract
An individual?s words often reveal their po-
litical ideology. Existing automated tech-
niques to identify ideology from text focus
on bags of words or wordlists, ignoring syn-
tax. Taking inspiration from recent work in
sentiment analysis that successfully models
the compositional aspect of language, we
apply a recursive neural network (RNN)
framework to the task of identifying the po-
litical position evinced by a sentence. To
show the importance of modeling subsen-
tential elements, we crowdsource political
annotations at a phrase and sentence level.
Our model outperforms existing models on
our newly annotated dataset and an existing
dataset.
1 Introduction
Many of the issues discussed by politicians and
the media are so nuanced that even word choice
entails choosing an ideological position. For ex-
ample, what liberals call the ?estate tax? conser-
vatives call the ?death tax?; there are no ideolog-
ically neutral alternatives (Lakoff, 2002). While
objectivity remains an important principle of jour-
nalistic professionalism, scholars and watchdog
groups claim that the media are biased (Groseclose
and Milyo, 2005; Gentzkow and Shapiro, 2010;
Niven, 2003), backing up their assertions by pub-
lishing examples of obviously biased articles on
their websites. Whether or not it reflects an under-
lying lack of objectivity, quantitative changes in the
popular framing of an issue over time?favoring
one ideologically-based position over another?can
have a substantial effect on the evolution of policy
(Dardis et al, 2008).
Manually identifying ideological bias in polit-
ical text, especially in the age of big data, is an
impractical and expensive process. Moreover, bias
They 
dubbed it 
the
death tax? ?
and created a 
big lie about
its adverse effects
on small 
businesses
Figure 1: An example of compositionality in ideo-
logical bias detection (red? conservative, blue?
liberal, gray? neutral) in which modifier phrases
and punctuation cause polarity switches at higher
levels of the parse tree.
may be localized to a small portion of a document,
undetectable by coarse-grained methods. In this pa-
per, we examine the problem of detecting ideologi-
cal bias on the sentence level. We say a sentence
contains ideological bias if its author?s political
position (here liberal or conservative, in the sense
of U.S. politics) is evident from the text.
Ideological bias is difficult to detect, even for
humans?the task relies not only on political
knowledge but also on the annotator?s ability to
pick up on subtle elements of language use. For
example, the sentence in Figure 1 includes phrases
typically associated with conservatives, such as
?small businesses? and ?death tax?. When we take
more of the structure into account, however, we
find that scare quotes and a negative propositional
attitude (a lie about X) yield an evident liberal bias.
Existing approaches toward bias detection have
not gone far beyond ?bag of words? classifiers, thus
ignoring richer linguistic context of this kind and
often operating at the level of whole documents.
In contrast, recent work in sentiment analysis has
used deep learning to discover compositional ef-
fects (Socher et al, 2011b; Socher et al, 2013b).
Building from those insights, we introduce a re-
cursive neural network (RNN) to detect ideological
bias on the sentence level. This model requires
1113
wb
 = changew
a
 = climate
w
d
 = so-called
p
c
 = climate change
p
e
 = so-called climate change
x
d
= x
c
=
x
e
=
x
a
= x
b
=
W
L
W
R
W
R
W
L
Figure 2: An example RNN for the phrase ?so-
called climate change?. Two d-dimensional word
vectors (here, d = 6) are composed to generate a
phrase vector of the same dimensionality, which
can then be recursively used to generate vectors at
higher-level nodes.
richer data than currently available, so we develop
a new political ideology dataset annotated at the
phrase level. With this new dataset we show that
RNNs not only label sentences well but also im-
prove further when given additional phrase-level
annotations. RNNs are quantitatively more effec-
tive than existing methods that use syntactic and
semantic features separately, and we also illustrate
how our model correctly identifies ideological bias
in complex syntactic constructions.
2 Recursive Neural Networks
Recursive neural networks (RNNs) are machine
learning models that capture syntactic and semantic
composition. They have achieved state-of-the-art
performance on a variety of sentence-level NLP
tasks, including sentiment analysis, paraphrase de-
tection, and parsing (Socher et al, 2011a; Hermann
and Blunsom, 2013). RNN models represent a shift
from previous research on ideological bias detec-
tion in that they do not rely on hand-made lexicons,
dictionaries, or rule sets. In this section, we de-
scribe a supervised RNN model for bias detection
and highlight differences from previous work in
training procedure and initialization.
2.1 Model Description
By taking into account the hierarchical nature of
language, RNNs can model semantic composition,
which is the principle that a phrase?s meaning is a
combination of the meaning of the words within
that phrase and the syntax that combines those
words. While semantic composition does not ap-
ply universally (e.g., sarcasm and idioms), most
language follows this principle. Since most ide-
ological bias becomes identifiable only at higher
levels of sentence trees (as verified by our annota-
tion, Figure 4), models relying primarily on word-
level distributional statistics are not desirable for
our problem.
The basic idea behind the standard RNN model
is that each word w in a sentence is associated
with a vector representation x
w
? R
d
. Based on a
parse tree, these words form phrases p (Figure 2).
Each of these phrases also has an associated vector
x
p
? R
d
of the same dimension as the word vectors.
These phrase vectors should represent the meaning
of the phrases composed of individual words. As
phrases themselves merge into complete sentences,
the underlying vector representation is trained to
retain the sentence?s whole meaning.
The challenge is to describe how vectors com-
bine to form complete representations. If two
words w
a
and w
b
merge to form phrase p, we posit
that the phrase-level vector is
x
p
= f(W
L
? x
a
+W
R
? x
b
+ b
1
), (1)
where W
L
and W
R
are d ? d left and right com-
position matrices shared across all nodes in the
tree, b
1
is a bias term, and f is a nonlinear activa-
tion function such as tanh. The word-level vectors
x
a
and x
b
come from a d ? V dimensional word
embedding matrix W
e
, where V is the size of the
vocabulary.
We are interested in learning representations that
can distinguish political polarities given labeled
data. If an element of this vector space, x
d
, repre-
sents a sentence with liberal bias, its vector should
be distinct from the vector x
r
of a conservative-
leaning sentence.
Supervised RNNs achieve this distinction by ap-
plying a regression that takes the node?s vector x
p
as input and produces a prediction y?
p
. This is a
softmax layer
y?
d
= softmax(W
cat
? x
p
+ b
2
), (2)
where the softmax function is
softmax(q) =
exp q
?
k
j=1
exp q
j
(3)
and W
cat
is a k ? d matrix for a dataset with k-
dimensional labels.
We want the predictions of the softmax layer to
match our annotated data; the discrepancy between
categorical predictions and annotations is measured
1114
through the cross-entropy loss. We optimize the
model parameters to minimize the cross-entropy
loss over all sentences in the corpus. The cross-
entropy loss of a single sentence is the sum over
the true labels y
i
in the sentence,
`(y?
s
) =
k
?
p=1
y
p
? log(y?
p
). (4)
This induces a supervised objective function
over all sentences: a regularized sum over all node
losses normalized by the number of nodes N in the
training set,
C =
1
N
N
?
i
`(pred
i
) +
?
2
???
2
. (5)
We use L-BFGS with parameter averag-
ing (Hashimoto et al, 2013) to optimize the model
parameters ? = (W
L
,W
R
,W
cat
,W
e
, b
1
, b
2
). The
gradient of the objective, shown in Eq. (6), is
computed using backpropagation through struc-
ture (Goller and Kuchler, 1996),
?C
??
=
1
N
N
?
i
?`(y?
i
)
??
+ ??. (6)
2.2 Initialization
When initializing our model, we have two choices:
we can initialize all of our parameters randomly or
provide the model some prior knowledge. As we
see in Section 4, these choices have a significant
effect on final performance.
Random The most straightforward choice is to
initialize the word embedding matrix W
e
and com-
position matrices W
L
and W
R
randomly such that
without any training, representations for words and
phrases are arbitrarily projected into the vector
space.
word2vec The other alternative is to initialize the
word embedding matrix W
e
with values that reflect
the meanings of the associated word types. This
improves the performance of RNN models over ran-
dom initializations (Collobert and Weston, 2008;
Socher et al, 2011a). We initialize our model with
300-dimensional word2vec toolkit vectors gener-
ated by a continuous skip-gram model trained on
around 100 billion words from the Google News
corpus (Mikolov et al, 2013).
The word2vec embeddings have linear relation-
ships (e.g., the closest vectors to the average of
?green? and ?energy? include phrases such as ?re-
newable energy?, ?eco-friendly?, and ?efficient
lightbulbs?). To preserve these relationships as
phrases are formed in our sentences, we initialize
our left and right composition matrices such that
parent vector p is computed by taking the average
of children a and b (W
L
= W
R
= 0.5I
d?d
). This
initialization of the composition matrices has pre-
viously been effective for parsing (Socher et al,
2013a).
3 Datasets
We performed initial experiments on a dataset of
Congressional debates that has annotations on the
author level for partisanship, not ideology. While
the two terms are highly correlated (e.g., a member
of the Republican party likely agrees with conserva-
tive stances on most issues), they are not identical.
For example, a moderate Republican might agree
with the liberal position on increased gun control
but take conservative positions on other issues. To
avoid conflating partisanship and ideology we cre-
ate a new dataset annotated for ideological bias on
the sentence and phrase level. In this section we
describe our initial dataset (Convote) and explain
the procedure we followed for creating our new
dataset (IBC).
1
3.1 Convote
The Convote dataset (Thomas et al, 2006) con-
sists of US Congressional floor debate transcripts
from 2005 in which all speakers have been labeled
with their political party (Democrat, Republican,
or independent). We propagate party labels down
from the speaker to all of their individual sentences
and map from party label to ideology label (Demo-
crat? liberal, Republican? conservative). This
is an expedient choice; in future work we plan to
make use of work in political science characteriz-
ing candidates? ideological positions empirically
based on their behavior (Carroll et al, 2009).
While the Convote dataset has seen widespread
use for document-level political classification, we
are unaware of similar efforts at the sentence level.
3.1.1 Biased Sentence Selection
The strong correlation between US political parties
and political ideologies (Democrats with liberal,
Republicans with conservative) lends confidence
that this dataset contains a rich mix of ideological
1
Available at http://cs.umd.edu/
?
miyyer/ibc
1115
statements. However, the raw Convote dataset con-
tains a low percentage of sentences with explicit
ideological bias.
2
We therefore use the features
in Yano et al (2010), which correlate with politi-
cal bias, to select sentences to annotate that have
a higher likelihood of containing bias. Their fea-
tures come from the Linguistic Inquiry and Word
Count lexicon (LIWC) (Pennebaker et al, 2001),
as well as from lists of ?sticky bigrams? (Brown
et al, 1992) strongly associated with one party or
another (e.g., ?illegal aliens? implies conservative,
?universal healthcare? implies liberal).
We first extract the subset of sentences that con-
tains any words in the LIWC categories of Negative
Emotion, Positive Emotion, Causation, Anger, and
Kill verbs.
3
After computing a list of the top 100
sticky bigrams for each category, ranked by log-
likelihood ratio, and selecting another subset from
the original data that included only sentences con-
taining at least one sticky bigram, we take the union
of the two subsets. Finally, we balance the resulting
dataset so that it contains an equal number of sen-
tences from Democrats and Republicans, leaving
us with a total of 7,816 sentences.
3.2 Ideological Books
In addition to Convote, we use the Ideologi-
cal Books Corpus (IBC) developed by Gross et
al. (2013). This is a collection of books and maga-
zine articles written between 2008 and 2012 by au-
thors with well-known political leanings. Each doc-
ument in the IBC has been manually labeled with
coarse-grained ideologies (right, left, and center) as
well as fine-grained ideologies (e.g., religious-right,
libertarian-right) by political science experts.
There are over a million sentences in the IBC,
most of which have no noticeable political bias.
Therefore we use the filtering procedure outlined
in Section 3.1.1 to obtain a subset of 55,932 sen-
tences. Compared to our final Convote dataset, an
even larger percentage of the IBC sentences exhibit
no noticeable political bias.
4
Because our goal
is to distinguish between liberal and conservative
2
Many sentences in Convote are variations on ?I think this
is a good/bad bill?, and there is also substantial parliamentary
boilerplate language.
3
While Kill verbs are not a category in LIWC, Yano et
al. (2010) adopted it from Greene and Resnik (2009) and
showed it to be a useful predictor of political bias. It includes
words such as ?slaughter? and ?starve?.
4
This difference can be mainly attributed to a historical
topics in the IBC (e.g., the Crusades, American Civil War).
In Convote, every sentence is part of a debate about 2005
political policy.
bias, instead of the more general task of classify-
ing sentences as ?neutral? or ?biased?, we filter
the dataset further using DUALIST (Settles, 2011),
an active learning tool, to reduce the proportion
of neutral sentences in our dataset. To train the
DUALIST classifier, we manually assigned class la-
bels of ?neutral? or ?biased? to 200 sentences, and
selected typical partisan unigrams to represent the
?biased? class. DUALIST labels 11,555 sentences as
politically biased, 5,434 of which come from con-
servative authors and 6,121 of which come from
liberal authors.
3.2.1 Annotating the IBC
For purposes of annotation, we define the task of
political ideology detection as identifying, if pos-
sible, the political position of a given sentence?s
author, where position is either liberal or conser-
vative.
5
We used the Crowdflower crowdsourcing
platform (crowdflower.com), which has previously
been used for subsentential sentiment annotation
(Sayeed et al, 2012), to obtain human annotations
of the filtered IBC dataset for political bias on both
the sentence and phrase level. While members of
the Crowdflower workforce are certainly not ex-
perts in political science, our simple task and the
ubiquity of political bias allows us to acquire useful
annotations.
Crowdflower Task First, we parse the filtered
IBC sentences using the Stanford constituency
parser (Socher et al, 2013a). Because of the ex-
pense of labeling every node in a sentence, we only
label one path in each sentence. The process for
selecting paths is as follows: first, if any paths
contain one of the top-ten partisan unigrams,
6
we
select the longest such path; otherwise, we select
the path with the most open class constituencies
(NP, VP, ADJP). The root node of a sentence is
always included in a path.
Our task is shown in Figure 3. Open class con-
stituencies are revealed to the worker incrementally,
starting with the NP, VP, or ADJP furthest from
the root and progressing up the tree. We choose
this design to prevent workers from changing their
lower-level phrase annotations after reading the full
sentence.
5
This is a simplification, as the ideological hierarchy in
IBC makes clear.
6
The words that the multinomial na??ve Bayes classifier
in DUALIST marked as highest probability given a polarity:
market, abortion, economy, rich, liberal, tea, economic, taxes,
gun, abortion
1116
Filtering the Workforce To ensure our anno-
tators have a basic understanding of US politics,
we restrict workers to US IP addresses and require
workers manually annotate one node from 60 dif-
ferent ?gold ? paths annotated by the authors. We
select these nodes such that the associated phrase is
either obviously biased or obviously neutral. Work-
ers must correctly annotate at least six of eight
gold paths before they are granted access to the full
task. In addition, workers must maintain 75% accu-
racy on gold paths that randomly appear alongside
normal paths. Gold paths dramatically improve
the quality of our workforce: 60% of contributors
passed the initial quiz (the 40% that failed were
barred from working on the task), while only 10%
of workers who passed the quiz were kicked out
for mislabeling subsequent gold paths.
Annotation Results Workers receive the
following instructions:
Each task on this page contains a set of
phrases from a single sentence. For each
phrase, decide whether or not the author fa-
vors a political position to the left (Liberal) or
right (Conservative) of center.
? If the phrase is indicative of a position to
the left of center, please choose Liberal.
? If the phrase is indicative of a position to
the right of center, please choose Conser-
vative.
? If you feel like the phrase indicates some
position to the left or right of the political
center, but you?re not sure which direc-
tion, please mark Not neutral, but I?m
unsure of which direction.
? If the phrase is not indicative of a posi-
tion to the left or right of center, please
mark Neutral.
We had workers annotate 7,000 randomly se-
lected paths from the filtered IBC dataset, with half
of the paths coming from conservative authors and
the other half from liberal authors, as annotated
by Gross et al (2013). Three workers annotated
each path in the dataset, and we paid $0.03 per
sentence. Since identifying political bias is a rela-
tively difficult and subjective task, we include all
sentences where at least two workers agree on a
label for the root node in our final dataset, except
when that label is ?Not neutral, but I?m unsure of
Figure 3: Example political ideology annotation
task showing incremental reveal of progressively
longer phrases.
which direction?. We only keep phrase-level an-
notations where at least two workers agree on the
label: 70.4% of all annotated nodes fit this defini-
tion of agreement. All unannotated nodes receive
the label of their closest annotated ancestor. Since
the root of each sentence is always annotated, this
strategy ensures that every node in the tree has a
label. Our final balanced IBC dataset consists of
3,412 sentences (4,062 before balancing and re-
moving neutral sentences) with a total of 13,640
annotated nodes. Of these sentences, 543 switch
polarity (liberal? conservative or vice versa) on
an annotated path.
While we initially wanted to incorporate neutral
labels into our model, we observed that lower-level
phrases are almost always neutral while full sen-
tences are much more likely to be biased (Figure 4).
Due to this discrepancy, the objective function in
Eq. (5) was minimized by making neutral predic-
tions for almost every node in the dataset.
4 Experiments
In this section we describe our experimental frame-
work. We discuss strong baselines that use lexi-
cal and syntactic information (including framing-
specific features from previous work) as well as
multiple RNN configurations. Each of these mod-
els have the same task: to predict sentence-level
ideology labels for sentences in a test set. To ac-
count for label imbalance, we subsample the data
so that there are an equal number of labels and
report accuracy over this balanced dataset.
1117
0 1 2 3 4 5 6 7 8 9 10Node Depth
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Lab
el P
rob
abil
ity
Label Probability vs. Node Depth
ConservativeLiberalNeutral / No Agreement
Figure 4: Proportion of liberal, conservative, and
neutral annotations with respect to node depth (dis-
tance from root). As we get farther from the root
of the tree, nodes are more likely to be neutral.
4.1 Baselines
? The RANDOM baseline chooses a label at ran-
dom from {liberal, conservative}.
? LR1, our most basic logistic regression base-
line, uses only bag of words (BoW) features.
? LR2 uses only BoW features. However, LR2
also includes phrase-level annotations as sep-
arate training instances.
7
? LR3 uses BoW features as well as syntac-
tic pseudo-word features from Greene &
Resnik (2009). These features from depen-
dency relations specify properties of verbs
(e.g., transitivity or nominalization).
8
? LR-(W2V) is a logistic regression model
trained on the average of the pretrained word
embeddings for each sentence (Section 2.2).
The LR-(W2V) baseline allows us to compare
against a strong lexical representation that encodes
syntactic and semantic information without the
RNN tree structure. (LR1, LR2) offer a compari-
son to simple bag of words models, while the LR3
baseline contrasts traditional syntactic features with
those learned by RNN models.
4.2 RNN Models
For RNN models, we generate a feature vector for
every node in the tree. Equation 1 allows us to
7
The Convote dataset was not annotated on the phrase
level, so we only provide a result for the IBC dataset.
8
We do not include phrase-level annotations in the LR3
feature set because the pseudo-word features can only be
computed from full sentence parses.
Model Convote IBC
RANDOM 50% 50%
LR1 64.7% 62.1%
LR2 ? 61.9%
LR3 66.9% 62.6%
LR-(W2V) 66.6% 63.7%
RNN1 69.4% 66.2%
RNN1-(W2V) 70.2% 67.1%
RNN2-(W2V) ? 69.3%
Table 1: Sentence-level bias detection accuracy.
The RNN framework, adding phrase-level data, and
initializing with word2vec all improve performance
over logistic regression baselines. The LR2 and
RNN2-(W2V) models were not trained on Convote
since it lacks phrase annotations.
percolate the representations to the root of the tree.
We generate the final instance representation by
concatenating the root vector and the average of
all other vectors (Socher et al, 2011b). We train
an L
2
-regularized logistic regression model over
these concatenated vectors to obtain final accuracy
numbers on the sentence level.
To analyze the effects of initialization and
phrase-level annotations, we report results for three
different RNN settings. All three models were im-
plemented as described in Section 2 with the non-
linearity f set to the normalized tanh function,
f(v) =
tanh(v)
?tanh(v)?
. (7)
We perform 10-fold cross-validation on the training
data to find the best RNN hyperparameters.
9
We report results for RNN models with the fol-
lowing configurations:
? RNN1 initializes all parameters randomly and
uses only sentence-level labels for training.
? RNN1-(W2V) uses the word2vec initialization
described in Section 2.2 but is also trained on
only sentence-level labels.
? RNN2-(W2V) is initialized using word2vec
embeddings and also includes annotated
phrase labels in its training. For this model,
we also introduce a hyperparameter ? that
weights the error at annotated nodes (1? ?)
higher than the error at unannotated nodes (?);
since we have more confidence in the anno-
tated labels, we want them to contribute more
towards the objective function.
9
[?
W
e
=1e-6, ?
W
=1e-4, ?
W
cat
=1e-3, ? = 0.3]
1118
For all RNN models, we set the word vector
dimension d to 300 to facilitate direct comparison
against the LR-(W2V) baseline.
10
5 Where Compositionality Helps Detect
Ideological Bias
In this section, we examine the RNN models to see
why they improve over our baselines. We also give
examples of sentences that are correctly classified
by our best RNN model but incorrectly classified by
all of the baselines. Finally, we investigate sentence
constructions that our model cannot handle and
offer possible explanations for these errors.
Experimental Results Table 1 shows the RNN
models outperforming the bag-of-words base-
lines as well as the word2vec baseline on both
datasets. The increased accuracy suggests that the
trained RNNs are capable of detecting bias polar-
ity switches at higher levels in parse trees. While
phrase-level annotations do not improve baseline
performance, the RNN model significantly bene-
fits from these annotations because the phrases are
themselves derived from nodes in the network struc-
ture. In particular, the phrase annotations allow our
best model to detect bias accurately in complex
sentences that the baseline models cannot handle.
Initializing the RNN W
e
matrix with word2vec
embeddings improves accuracy over randomly ini-
tialization by 1%. This is similar to improvements
from pretrained vectors from neural language mod-
els (Socher et al, 2011b).
We obtain better results on Convote than on IBC
with both bag-of-words and RNN models. This
result was unexpected since the Convote labels
are noisier than the annotated IBC labels; however,
there are three possible explanations for the discrep-
ancy. First, Convote has twice as many sentences
as IBC, and the extra training data might help the
model more than IBC?s better-quality labels. Sec-
ond, since the sentences in Convote were originally
spoken, they are almost half as short (21.3 words
per sentence) as those in the IBC (42.2 words per
sentence). Finally, some information is lost at ev-
ery propagation step, so RNNs are able to model
the shorter sentences in Convote more effectively
than the longer IBC sentences.
Qualitative Analysis As in previous work
(Socher et al, 2011b), we visualize the learned
10
Using smaller vector sizes (d ? {50, 100}, as in previous
work) does not significantly change accuracy.
vector space by listing the most probable n-grams
for each political affiliation in Table 2. As expected,
conservatives emphasize values such as freedom
and religion while disparaging excess government
spending and their liberal opposition. Meanwhile,
liberals inveigh against the gap between the rich
and the poor while expressing concern for minority
groups and the working class.
Our best model is able to accurately model the
compositional effects of bias in sentences with com-
plex syntactic structures. The first three sentences
in Figure 5 were correctly classified by our best
model (RNN2-(W2V)) and incorrectly classified by
all of the baselines. Figures 5A and C show tradi-
tional conservative phrases, ?free market ideology?
and ?huge amounts of taxpayer money?, that switch
polarities higher up in the tree when combined with
phrases such as ?made worse by? and ?saved by?.
Figure 5B shows an example of a bias polarity
switch in the opposite direction: the sentence neg-
atively portrays supporters of nationalized health
care, which our model picks up on.
Our model often makes errors when polarity
switches occur at nodes that are high up in the
tree. In Figure 5D, ?be used as an instrument to
achieve charitable or social ends? reflects a lib-
eral ideology, which the model predicts correctly.
However, our model is unable to detect the polarity
switch when this phrase is negated with ?should
not?. Since many different issues are discussed
in the IBC, it is likely that our dataset has too few
examples of some of these issues for the model to
adequately learn the appropriate ideological posi-
tions, and more training data would resolve many
of these errors.
6 Related Work
A growing NLP subfield detects private states such
as opinions, sentiment, and beliefs (Wilson et al,
2005; Pang and Lee, 2008) from text. In general,
work in this category tends to combine traditional
surface lexical modeling (e.g., bag-of-words) with
hand-designed syntactic features or lexicons. Here
we review the most salient literature related to the
present paper.
6.1 Automatic Ideology Detection
Most previous work on ideology detection ignores
the syntactic structure of the language in use in
favor of familiar bag-of-words representations for
1119
be used as an instrument to 
achieve charitable or social ends
should notthe law
X
X
X
nationalized health care
An entertainer once
said a sucker is born
every minute , and
surely this is the
case with
those who
support
made worse by
the implementing
Thus , the harsh
conditions for
farmers caused
by a number of
factors ,
, have created a 
continuing stream of 
people leaving the 
countryside and going 
to live in cities that do 
not have jobs for them .
of free-market
ideology
huge 
amounts of 
taxpayer 
money
saved byBut taxpayers 
do know
already that 
TARP was
designed in a 
way that
allowed
to continue to 
show the same 
arrogant traits 
that should have
destroyed their
companies .
the same 
corporations
who were
A B
C D
Figure 5: Predictions by RNN2-(W2V) on four sentences from the IBC. Node color is the true label (red
for conservative, blue for liberal), and an ?X? next to a node means the model?s prediction was wrong. In
A and C, the model accurately detects conservative-to-liberal polarity switches, while in B it correctly
predicts the liberal-to-conservative switch. In D, negation confuses our model.
the sake of simplicity. For example, Gentzkow
and Shapiro (2010) derive a ?slant index? to rate
the ideological leaning of newspapers. A newspa-
per?s slant index is governed by the frequency of
use of partisan collocations of 2-3 tokens. Simi-
larly, authors have relied on simple models of lan-
guage when leveraging inferred ideological posi-
tions. E.g., Gerrish and Blei (2011) predict the
voting patterns of Congress members based on bag-
of-words representations of bills and inferred polit-
ical leanings of those members.
Recently, Sim et al (2013) have proposed a
model to infer mixtures of ideological positions
in documents, applied to understanding the evolu-
tion of ideological rhetoric used by political can-
didates during the campaign cycle. They use an
HMM-based model, defining the states as a set
of fine-grained political ideologies, and rely on
a closed set of lexical bigram features associated
with each ideology, inferred from a manually la-
beled ideological books corpus. Although it takes
elements of discourse structure into account (cap-
turing the?burstiness? of ideological terminology
usage), their model explicitly ignores intrasenten-
tial contextual influences of the kind seen in Fig-
ure 1. Other approaches on the document level use
topic models to analyze bias in news articles, blogs,
and political speeches (Ahmed and Xing, 2010; Lin
et al, 2008; Nguyen et al, 2013).
6.2 Subjectivity Detection
Detecting subjective language, which conveys opin-
ion or speculation, is a related NLP problem. While
sentences lacking subjective language may con-
tain ideological bias (e.g., the topic of the sen-
tence), highly-opinionated sentences likely have
obvious ideological leanings. In addition, senti-
ment and subjectivity analysis offers methodolog-
ical approaches that can be applied to automatic
bias detection.
Wiebe et al (2004) show that low-frequency
words and some collocations are a good indica-
tors of subjectivity. More recently, Recasens et al
(2013) detect biased words in sentences using indi-
cator features for bias cues such as hedges and fac-
tive verbs in addition to standard bag-of-words and
part-of-speech features. They show that this type of
linguistic information dramatically improves per-
formance over several standard baselines.
Greene and Resnik (2009) also emphasize the
connection between syntactic and semantic rela-
tionships in their work on ?implicit sentiment?,
1120
n Most conservative n-grams Most liberal n-grams
1 Salt, Mexico, housework, speculated, consensus, lawyer,
pharmaceuticals, ruthless, deadly, Clinton, redistribution
rich, antipsychotic, malaria, biodiversity, richest, gene,
pesticides, desertification, Net, wealthiest, labor, fertil-
izer, nuclear, HIV
3 prize individual liberty, original liberal idiots, stock mar-
ket crash, God gives freedom, federal government inter-
ference, federal oppression nullification, respect individ-
ual liberty, Tea Party patriots, radical Sunni Islamists,
Obama stimulus programs
rich and poor,?corporate greed?, super rich pay, carrying
the rich, corporate interest groups, young women work-
ers, the very rich, for the rich, by the rich, soaking the
rich, getting rich often, great and rich, the working poor,
corporate income tax, the poor migrants
5 spending on popular government programs, bailouts and
unfunded government promises, North America from
external threats, government regulations place on busi-
nesses, strong Church of Christ convictions, radical Is-
lamism and other threats
the rich are really rich, effective forms of worker partic-
ipation, the pensions of the poor, tax cuts for the rich,
the ecological services of biodiversity, poor children and
pregnant women, vacation time for overtime pay
7 government intervention helped make the Depression
Great, by God in His image and likeness, producing
wealth instead of stunting capital creation, the tradi-
tional American values of limited government, trillions
of dollars to overseas oil producers, its troubled assets to
federal sugar daddies, Obama and his party as racialist
fanatics
African Americans and other disproportionately poor
groups; the growing gap between rich and poor; the
Bush tax cuts for the rich; public outrage at corporate
and societal greed; sexually transmitted diseases , most
notably AIDS; organize unions or fight for better condi-
tions, the biggest hope for health care reform
Table 2: Highest probability n-grams for conservative and liberal ideologies, as predicted by the RNN2-
(W2V) model.
which refers to sentiment carried by sentence struc-
ture and not word choice. They use syntactic depen-
dency relation features combined with lexical infor-
mation to achieve then state-of-the-art performance
on standard sentiment analysis datasets. However,
these syntactic features are only computed for a
thresholded list of domain-specific verbs. This
work extends their insight of modeling sentiment
as an interaction between syntax and semantics to
ideological bias.
Future Work There are a few obvious directions
in which this work can be expanded. First, we can
consider more nuanced political ideologies beyond
liberal and conservative. We show that it is pos-
sible to detect ideological bias given this binary
problem; however, a finer-grained study that also
includes neutral annotations may reveal more sub-
tle distinctions between ideologies. While acquir-
ing data with obscure political biases from the IBC
or Convote is unfeasible, we can apply a similar
analysis to social media (e.g., Twitter or Facebook
updates) to discover how many different ideologies
propagate in these networks.
Another direction is to implement more sophis-
ticated RNN models (along with more training
data) for bias detection. We attempted to apply
syntactically-untied RNNs (Socher et al, 2013a)
to our data with the idea that associating separate
matrices for phrasal categories would improve rep-
resentations at high-level nodes. While there were
too many parameters for this model to work well
here, other variations might prove successful, espe-
cially with more data. Finally, combining sentence-
level and document-level models might improve
bias detection at both levels.
7 Conclusion
In this paper we apply recursive neural networks
to political ideology detection, a problem where
previous work relies heavily on bag-of-words mod-
els and hand-designed lexica. We show that our
approach detects bias more accurately than existing
methods on two different datasets. In addition, we
describe an approach to crowdsourcing ideological
bias annotations. We use this approach to create a
new dataset from the IBC, which is labeled at both
the sentence and phrase level.
Acknowledgments
We thank the anonymous reviewers, Hal Daum?e,
Yuening Hu, Yasuhiro Takayama, and Jyothi Vinju-
mur for their insightful comments. We also want to
thank Justin Gross for providing the IBC and Asad
Sayeed for help with the Crowdflower task design,
as well as Richard Socher and Karl Moritz Her-
mann for assisting us with our model implemen-
tations. This work was supported by NSF Grant
CCF-1018625. Boyd-Graber is also supported by
NSF Grant IIS-1320538. Any opinions, findings,
conclusions, or recommendations expressed here
are those of the authors and do not necessarily re-
flect the view of the sponsor.
1121
References
Amr Ahmed and Eric P Xing. 2010. Staying informed: super-
vised and semi-supervised multi-view topical analysis of
ideological perspective. In EMNLP.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent
J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram
models of natural language. Comp. Ling., 18(4):467?479.
Royce Carroll, Jeffrey B Lewis, James Lo, Keith T Poole, and
Howard Rosenthal. 2009. Measuring bias and uncertainty
in dw-nominate ideal point estimates via the parametric
bootstrap. Political Analysis, 17(3):261?275.
Ronan Collobert and Jason Weston. 2008. A unified architec-
ture for natural language processing: Deep neural networks
with multitask learning. In ICML.
Frank E Dardis, Frank R Baumgartner, Amber E Boydstun,
Suzanna De Boef, and Fuyuan Shen. 2008. Media framing
of capital punishment and its impact on individuals? cogni-
tive responses. Mass Communication & Society, 11(2):115?
140.
Matthew Gentzkow and Jesse M Shapiro. 2010. What drives
media slant? evidence from us daily newspapers. Econo-
metrica, 78(1):35?71.
Sean Gerrish and David M Blei. 2011. Predicting legislative
roll calls from text. In ICML.
Christoph Goller and Andreas Kuchler. 1996. Learning task-
dependent distributed representations by backpropagation
through structure. In Neural Networks, 1996., IEEE Inter-
national Conference on, volume 1.
Stephan Greene and Philip Resnik. 2009. More than words:
Syntactic packaging and implicit sentiment. In NAACL.
Tim Groseclose and Jeffrey Milyo. 2005. A measure of media
bias. The Quarterly Journal of Economics, 120(4):1191?
1237.
Justin Gross, Brice Acree, Yanchuan Sim, and Noah A Smith.
2013. Testing the etch-a-sketch hypothesis: A compu-
tational analysis of mitt romney?s ideological makeover
during the 2012 primary vs. general elections. In APSA
2013 Annual Meeting Paper.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and
Takashi Chikayama. 2013. Simple customization of recur-
sive neural networks for semantic relation classification. In
EMNLP.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role of
Syntax in Vector Space Models of Compositional Seman-
tics. In ACL.
George Lakoff. 2002. Moral Politics: How Liberals and Con-
servatives Think, Second Edition. University of Chicago
Press.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann. 2008.
A joint topic and perspective model for ideological dis-
course. In Machine Learning and Knowledge Discovery in
Databases, pages 17?32. Springer.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
2013. Efficient estimation of word representations in vector
space. arXiv preprint arXiv:1301.3781.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2013. Lexical and hierarchical topic regression. In NIPS,
pages 1106?1114.
David Niven. 2003. Objective evidence on media bias: News-
paper coverage of congressional party switchers. Journal-
ism & Mass Communication Quarterly, 80(2):311?326.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment
analysis. Foundations and trends in information retrieval,
2(1-2).
James W. Pennebaker, Martha E. Francis, and Roger J. Booth.
2001. Linguistic inquiry and word count [computer soft-
ware]. Mahwah, NJ: Erlbaum Publishers.
Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan
Jurafsky. 2013. Linguistic models for analyzing and de-
tecting biased language.
Asad B Sayeed, Jordan Boyd-Graber, Bryan Rusk, and Amy
Weinberg. 2012. Grammatical structures for word-level
sentiment detection. In NAACL.
Burr Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features and
instances. In EMNLP.
Yanchuan Sim, Brice Acree, Justin H Gross, and Noah A
Smith. 2013. Measuring ideological proportions in politi-
cal speeches. In EMNLP.
Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y.
Ng, and Christopher D. Manning. 2011a. Dynamic Pool-
ing and Unfolding Recursive Autoencoders for Paraphrase
Detection. In NIPS.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y.
Ng, and Christopher D. Manning. 2011b. Semi-Supervised
Recursive Autoencoders for Predicting Sentiment Distribu-
tions. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning, and
Andrew Y. Ng. 2013a. Parsing With Compositional Vector
Grammars. In ACL.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,
Christopher D Manning, Andrew Y Ng, and Christopher
Potts. 2013b. Recursive deep models for semantic compo-
sitionality over a sentiment treebank. In EMNLP.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the
vote: Determining support or opposition from Congres-
sional floor-debate transcripts. In EMNLP.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell,
and Melanie Martin. 2004. Learning subjective language.
Comp. Ling., 30(3):277?308.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In EMNLP.
Tae Yano, Philip Resnik, and Noah A Smith. 2010. Shedding
(a thousand points of) light on biased language. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechanical
Turk, pages 152?158.
1122
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1166?1176,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Polylingual Tree-Based Topic Models for Translation Domain Adaptation
Yuening Hu
?
Computer Science
University of Maryland
ynhu@cs.umd.edu
Ke Zhai
?
Computer Science
University of Maryland
zhaike@cs.umd.edu
Vladimir Eidelman
FiscalNote Inc.
Washington DC
vlad@fiscalnote.com
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Abstract
Topic models, an unsupervised technique
for inferring translation domains improve
machine translation quality. However, pre-
vious work uses only the source language
and completely ignores the target language,
which can disambiguate domains. We pro-
pose new polylingual tree-based topic mod-
els to extract domain knowledge that con-
siders both source and target languages and
derive three different inference schemes.
We evaluate our model on a Chinese to En-
glish translation task and obtain up to 1.2
BLEU improvement over strong baselines.
1 Introduction
Probabilistic topic models (Blei and Lafferty,
2009), exemplified by latent Dirichlet aloca-
tion (Blei et al, 2003, LDA), are one of the most
popular statistical frameworks for navigating large
unannotated document collections. Topic models
discover?without any supervision?the primary
themes presented in a dataset: the namesake topics.
Topic models have two primary applications: to
aid human exploration of corpora (Chang et al,
2009) or serve as a low-dimensional representa-
tion for downstream applications. We focus on
the second application, which has been fruitful for
computer vision (Li Fei-Fei and Perona, 2005),
computational biology (Perina et al, 2010), and
information retrieval (Kataria et al, 2011).
In particular, we use topic models to aid statisti-
cal machine translation (Koehn, 2009, SMT). Mod-
ern machine translation systems use millions of
examples of translations to learn translation rules.
These systems work best when the training corpus
has consistent genre, register, and topic. Systems
that are robust to systematic variation in the train-
ing set are said to exhibit domain adaptation.
? indicates equal contributions.
As we review in Section 2, topic models are
a promising solution for automatically discover-
ing domains in machine translation corpora. How-
ever, past work either relies solely on monolingual
source-side models (Eidelman et al, 2012; Hasler
et al, 2012; Su et al, 2012), or limited modeling
of the target side (Xiao et al, 2012). In contrast,
machine translation uses inherently multilingual
data: an SMT system must translate a phrase or sen-
tence from a source language to a different target
language, so existing applications of topic mod-
els (Eidelman et al, 2012) are wilfully ignoring
available information on the target side that could
aid domain discovery.
This is not for a lack of multilingual topic mod-
els. Topic models bridge the chasm between lan-
guages using document connections (Mimno et
al., 2009), dictionaries (Boyd-Graber and Resnik,
2010), and word alignments (Zhao and Xing, 2006).
In Section 2, we review these models for discover-
ing topics in multilingual datasets and discuss how
they can improve SMT.
However, no models combine multiple bridges
between languages. In Section 3, we create a
model?the polylingual tree-based topic models
(ptLDA)?that uses information from both external
dictionaries and document alignments simultane-
ously. In Section 4, we derive both MCMC and
variational inference for this new topic model.
In Section 5, we evaluate our model on the task
of SMT using aligned datasets. We show that ptLDA
offers better domain adaptation than other topic
models for machine translation. Finally, in Sec-
tion 6, we show how these topic models improve
SMT with detailed examples.
2 Topic Models for Machine Translation
Before considering past approaches using topic
models to improve SMT, we briefly review lexical
weighting and domain adaptation for SMT.
1166
2.1 Statistical Machine Translation
Statistical machine translation casts machine trans-
lation as a probabilistic process (Koehn, 2009). For
a parallel corpus of aligned source and target sen-
tences (F , E), a phrase
?
f ? F is translated to a
phrase e? ? E according to a distribution p
w
(e?|
?
f).
One popular method to estimate the probability
p
w
(e?|
?
f) is via lexical weighting features.
Lexical Weighting In phrase-based SMT, lexi-
cal weighting features estimate the phrase pair
quality by combining lexical translation probabil-
ities of words in a phrase (Koehn et al, 2003).
Lexical conditional probabilities p(e|f) are maxi-
mum likelihood estimates from relative lexical fre-
quencies c(f, e)/
?
e
c(f, e) , where c(f, e) is the
count of observing lexical pair (f, e) in the train-
ing dataset. The phrase pair probabilities p
w
(e?|
?
f)
are the normalized product of lexical probabili-
ties of the aligned word pairs within that phrase
pair (Koehn et al, 2003). In Section 2.2, we create
topic-specific lexical weighting features.
Cross-Domain SMT A SMT system is usu-
ally trained on documents with the same genre
(e.g., sports, business) from a similar style (e.g.,
newswire, blog-posts). These are called domains.
Translations within one domain are better than
translations across domains since they vary dra-
matically in their word choices and style. A correct
translation in one domain may be inappropriate in
another domain. For example, ???? in a newspa-
per usually means ?underwater diving?. On social
media, it means a non-contributing ?lurker?.
Domain Adaptation for SMT Training a SMT
system using diverse data requires domain adap-
tation. Early efforts focus on building separate
models (Foster and Kuhn, 2007) and adding fea-
tures (Matsoukas et al, 2009) to model domain
information. Chiang et al (2011) combine these
approaches by directly optimizing genre and col-
lection features by computing separate translation
tables for each domain.
However, these approaches treat domains as
hand-labeled, constant, and known a priori. This
setup is at best expensive and at worst infeasible for
large data. Topic models provide a solution where
domains can be automatically induced from raw
data: treat each topic as a domain.
1
1
Henceforth we will use the term ?topic? and ?domain?
interchangeably: ?topic? to refer to the concept in topic models
and ?domain? to refer to SMT corpora.
2.2 Inducing Domains with Topic Models
Topic models take the number of topics K and a
collection of documents as input, where each docu-
ment is a bag of words. They output two distribu-
tions: a distribution over topics for each document
d; and a distribution over words for each topic. If
each topic defines a SMT domain, the document?s
topic distribution is a soft domain assignment for
that document.
Given the soft domain assignments, Eidelman et
al. (2012) extract lexical weighting features condi-
tioned on the topics, optimizing feature weights us-
ing the Margin Infused Relaxed Algorithm (Cram-
mer et al, 2006, MIRA). The topics come from
source documents only and create topic-specific
lexical weights from the per-document topic distri-
bution p(k | d). The lexical probability conditioned
on the topic is expected count e
k
(e, f) of a word
translation pair under topic k,
c?
k
(e, f) =
?
d
p(k|d)c
d
(e, f), (1)
where c
d
(?) is the number of occurrences of the
word pair in document d. The lexical probability
conditioned on topic k is the unsmoothed probabil-
ity estimate of those expected counts
p
w
(e|f ; k) =
c?
k
(e,f)?
e
c?
k
(e,f)
, (2)
from which we can compute the phrase pair proba-
bilities p
w
(e?|
?
f ; k) by multiplying the lexical prob-
abilities and normalizing as in Koehn et al (2003).
For a test document d, the document topic dis-
tribution p(k | d) is inferred based on the topics
learned from training data. The feature value of a
phrase pair (e?,
?
f) is
f
k
(e?|
?
f) = ? log
{
p
w
(e?|
?
f ; k) ? p(k|d)
}
, (3)
a combination of the topic dependent lexical weight
and the topic distribution of the document, from
which we extract the phrase. Eidelman et al (2012)
compute the resulting model score by combining
these features in a linear model with other standard
SMT features and optimizing the weights.
Conceptually, this approach is just reweighting
examples. The probability of a topic given a docu-
ment is never zero. Every translation observed in
the training set will contribute to p
k
(e|f); many of
the expected counts, however, will be less than one.
This obviates the explicit smoothing used in other
domain adaptation systems (Chiang et al, 2011).
1167
We adopt this framework in its entirety. Our
contribution are topics that capture multilingual
information and thus better capture the domains in
the parallel corpus.
2.3 Beyond Vanilla Topic Models
Eidelman et al (2012) ignore a wealth of infor-
mation that could improve topic models and help
machine translation. Namely, they only use mono-
lingual data from the source language, ignoring all
target-language data and available lexical semantic
resources between source and target languages.
Different complement each other to reduce ambi-
guity. For example, ???? in a Chinese document
can be either ?hobbyhorse? in a children?s topic,
or ?Trojan virus? in a technology topic. A short
Chinese context obscures the true topic. However,
these terms are unambiguous in English, revealing
the true topic.
While vanilla topic models (LDA) can only be
applied to monolingual data, there are a number
of topic models for parallel corpora: Zhao and
Xing (2006) assume aligned word pairs share same
topics; Mimno et al (2009) connect different lan-
guages through comparable documents. These
models take advantage of word or document align-
ment information and infer more robust topics from
the aligned dataset.
On the other hand, lexical information can in-
duce topics from multilingual corpora. For in-
stance, orthographic similarity connects words with
the same meaning in related languages (Boyd-
Graber and Blei, 2009), and dictionaries are a
more general source of information on which words
share meaning (Boyd-Graber and Resnik, 2010).
These two approaches are not mutually exclu-
sive, however; they reveal different connections
across languages. In the next section, we combine
these two approaches into a polylingual tree-based
topic model.
3 Polylingual Tree-based Topic Models
In this section, we bring existing tree-based topic
models (Boyd-Graber et al, 2007, tLDA) and
polylingual topic models (Mimno et al, 2009,
pLDA) together and create the polylingual tree-
based topic model (ptLDA) that incorporates both
word-level correlations and document-level align-
ment information.
Word-level Correlations Tree-based topic mod-
els incorporate the correlations between words by
encouraging words that appear together in a con-
cept to have similar probabilities given a topic.
These concepts can come from WordNet (Boyd-
Graber and Resnik, 2010), domain experts (An-
drzejewski et al, 2009), or user constrains (Hu et
al., 2013). When we gather concepts from bilin-
gual resources, these concepts can connect different
languages. For example, if a bilingual dictionary
defines ???? as ?computer?, we combine these
words in a concept.
We organize the vocabulary in a tree structure
based on these concepts (Figure 1): words in the
same concept share a common parent node, and
then that concept becomes one of many children of
the root node. Words that are not in any concept?
uncorrelated words?are directly connected to
the root node. We call this structure the tree prior.
When this tree serves as a prior for topic models,
words in the same concept are correlated in topics.
For example, if ???? has high probability in a
topic, so will ?computer?, since they share the same
parent node. With the tree priors, each topic is no
longer a distribution over word types, instead, it is a
distribution over paths, and each path is associated
with a word type. The same word could appear in
multiple paths, and each path represents a unique
sense of this word.
Document-level Alignments Lexical resources
connect languages and help guide the topics. How-
ever, these resources are sometimes brittle and may
not cover the whole vocabulary. Aligned document
pairs provide a more corpus-specific, flexible asso-
ciation across languages.
Polylingual topic models (Mimno et al, 2009)
assume that the aligned documents in different lan-
guages share the same topic distribution and each
language has a unique topic distribution over its
word types. This level of connection between lan-
guages is flexible: instead of requiring the exact
matching on words and sentences, only a coarse
document alignment is necessary, as long as the
documents discuss the same topics.
Combine Words and Documents We propose
polylingual tree-based topic models (ptLDA),
which connect information across different lan-
guages by incorporating both word correlation (as
in tLDA) and document alignment information (as
in pLDA). We initially assume a given tree struc-
ture, deferring the tree?s provenance to the end of
this section.
1168
Generative Process As in LDA, each word to-
ken is associated with a topic. However, tree-based
topic models introduce an additional step of select-
ing a concept in a topic responsible for generating
each word token. This is represented by a path y
d,n
through the topic?s tree.
The probability of a path in a topic depends on
the transition probabilities in a topic. Each concept
i in topic k has a distribution over its children nodes
is governed by a Dirichlet prior: pi
k,i
? Dir(?
i
).
Each path ends in a word (i.e., a leaf node) and
the probability of a path is the product of all of
the transitions between topics it traverses. Topics
have correlations over words because the Dirichlet
parameters can encode positive or negative correla-
tions (Andrzejewski et al, 2009).
With these correlated in topics in hand, the gen-
eration of documents are very similar to LDA. For
every document d, we first sample a distribution
over topics ?
d
from a Dirichlet prior Dir(?). For
every token in the documents, we first sample a
topic z
dn
from the multinomial distribution ?
d
, and
then sample a path y
dn
along the tree according to
the transition distributions specified by topic z
dn
.
Because every path y
dn
leads to a word w
dn
in lan-
guage l
dn
, we append the sampled word w
dn
to
document d
l
dn
. Aligned documents have words in
both languages; monolingual documents only have
words in a single language.
The full generative process is:
1: for topic k ? 1, ? ? ? ,K do
2: for each internal node n
i
do
3: draw a distribution pi
ki
? Dir(?
i
)
4: for document set d ? 1, ? ? ? , D do
5: draw a distribution ?
d
? Dir(?)
6: for each word in documents d do
7: choose a topic z
dn
? Mult(?
d
)
8: sample a path y
dn
with probability
?
(i,j)?y
dn
pi
z
dn
,i,j
9: y
dn
leads to word w
dn
in language l
dn
10: append token w
dn
to document d
l
dn
If we use a flat symmetric Dirichlet prior instead
of the tree prior, we recover pLDA; and if all docu-
ments are monolingual (i.e., with distinct distribu-
tions over topics ?), we recover tLDA. ptLDA con-
nects different languages on both the word level (us-
ing the word correlations) and the document level
(using the document alignments). We compare
these models? machine translation performance in
Section 5.
computer, 
market, ?
government, ??
science, ??
Dictionary: Vocabulary: English (0), Chinese (1)
computer

market ?
government
??
science
??
??scientific policy
0    scientific
0    policy
1    
1    ?
0    computer  
0    market
0    government
0    science
1    ??
1    ??
1    ??
Prior Tree:
 0  1
Figure 1: An example of constructing a prior tree
from a bilingual dictionary: word pairs with the
same meaning but in different languages are con-
cepts; we create a common parent node to group
words in a concept, and then connect to the root; un-
correlated words are connected to the root directly.
Each topic uses this tree structure as a prior.
Build Prior Tree Structures One remaining
question is the source of the word-level connections
across languages for the tree prior. We consider
two resources to build trees that correlate words
across languages. The first are a multilingual dic-
tionaries (dict), which match words with the same
meaning in different languages together. These re-
lations between words are used as the concepts in
the prior tree (Figure 1).
In addition, we extract the word alignments from
aligned sentences in a parallel corpus. The word
pairs define concepts for the prior tree (align). We
use both resources for our models (denoted as
ptLDA-dict and ptLDA-align) in our experiments
(Section 5) and show that they yield comparable
performance in SMT.
4 Inference
Inference of probabilistic models discovers the pos-
terior distribution over latent variables. For a col-
lection of D documents, each of which contains
N
d
number of words, the latent variables of ptLDA
are: transition distributions pi
ki
for every topic k
and internal node i in the prior tree structure; multi-
nomial distributions over topics ?
d
for every docu-
ment d; topic assignments z
dn
and path y
dn
for the
n
th
word w
dn
in document d. The joint distribution
of polylingual tree-based topic models is
p(w, z,y,?,pi;?, ?) =
?
k
?
i
p(pi
ki
|?
i
) (4)
?
?
d
p(?
d
|?) ?
?
d
?
n
p(z
dn
|?
d
)
?
?
d
?
n
(
p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)
)
.
Exact inference is intractable, so we turn to ap-
1169
proximate posterior inference to discover the latent
variables that best explain our data. Two widely
used approximation approaches are Markov chain
Monte Carlo (Neal, 2000, MCMC) and variational
Bayesian inference (Blei et al, 2003, VB). Both
frameworks produce good approximations of the
posterior mode (Asuncion et al, 2009). In addition,
Mimno et al (2012) propose hybrid inference that
takes advantage of parallelizable variational infer-
ence for global variables (Wolfe et al, 2008) while
enjoying the sparse, efficient updates for local vari-
ables (Neal, 1993). In the rest of this section, we
discuss all three methods in turn.
We explore multiple inference schemes because
while all of these methods optimize likelihood be-
cause they might give different results on the trans-
lation task.
4.1 Markov Chain Monte Carlo Inference
We use a collapsed Gibbs sampler for tree-based
topic models to sample the path y
dn
and topic as-
signment z
dn
for word w
dn
,
p(z
dn
= k, y
dn
= s|?z
dn
,?y
dn
,w;?,?)
? I [?(s) = w
dn
] ?
N
k|d
+?
?
k
?
(N
k
?
|d
+?)
?
?
i?j?s
N
i?j|k
+?
i?j?
j
?
(N
i?j
?
|k
+?
i?j
?
)
,
where ?(s) represents the word that path s leads
to, N
k|d
is the number of tokens assigned to topic k
in document d and N
i?j|k
is the number of times
edge i? j in the tree assigned to topic k, exclud-
ing the topic assignment z
dn
and its path y
dn
of
current token w
dn
. In practice, we sample the la-
tent variables using efficient sparse updates (Yao et
al., 2009; Hu and Boyd-Graber, 2012).
4.2 Variational Bayesian Inference
Variational Bayesian inference approximates the
posterior distribution with a simplified variational
distribution q over the latent variables: document
topic proportions ?, transition probabilities pi, topic
assignments z, and path assignments y.
Variational distributions typically assume a
mean-field distribution over these latent variables,
removing all dependencies between the latent vari-
ables. We follow this assumption for the transi-
tion probabilities q(pi |?) and the document topic
proportions q(? |?); both are variational Dirichlet
distributions. However, due to the tight coupling
between the path and topic variables, we must
model this joint distribution as one multinomial,
q(z,y |?). If word token w
dn
has K topics and
S paths, it has a K ? S length variational multino-
mial ?
dnks
, which represents the probability that
the word takes path s in topic k. The complete
variational distribution is
q(?,pi, z,y|?,?,?) =
?
d
q(?
d
|?
d
)? (5)
?
k
?
i
q(pi
ki
|?
ki
) ?
?
d
?
n
q(z
dn
, y
dn
|?
dn
).
Our goal is to find the variational distribution q
that is closest to the true posterior, as measured by
the Kullback-Leibler (KL) divergence between the
true posterior p and variational distribution q. This
induces an ?evidence lower bound? (ELBO, L) as a
function of a variational distribution q: L =
E
q
[log p(w, z,y,?,pi)]? E
q
[log q(?,pi, z,y)]
=
?
k
?
i
E
q
[log p(pi
ki
|?
i
)]
+
?
d
E
q
[log p(?
d
|?)]
+
?
d
?
n
E
q
[log p(z
dn
, y
dn
|?
d
,pi)p(w
dn
|y
dn
)]
+ H[q(?)] + H[q(pi)] + H[q(z,y)], (6)
where H[?] represents the entropy of a distribution.
Optimizing L using coordinate descent provides
the following updates:
?
dnkt
? exp{?(?
dk
)??(
?
k
?
dk
) (7)
+
?
i?j?s
(
?(?
k,i?j
)??(
?
j
?
?
k,i?j
?
)
)
};
?
dk
= ?
k
+
?
n
?
s??
?1
(w
dn
)
?
dnkt
; (8)
?
k,i?j
= ?
i?j
(9)
+
?
d
?
n
?
s??
?
(w
dn
)
?
dnkt
I [i? j ? s] ;
where ?
?
(w
dn
) is the set of all paths that lead to
wordw
dn
in the tree, and t represents one particular
path in this set. I [i? j ? s] is the indicator of
whether path s contains an edge from node i to j.
4.3 Hybrid Stochastic Inference
Given the complementary strengths of MCMC and
VB, and following hybrid inference proposed by
Mimno et al (2012), we also derive hybrid infer-
ence for ptLDA.
The transition distributions pi are treated identi-
cally as in variational inference. We posit a varia-
tional Dirichlet distribution ? and choose the one
that minimizes the KL divergence between the true
posterior and the variational distribution.
For topic z and path y, instead of variational
updates, we use a Gibbs sampler within a document.
We sample z
dn
and y
dn
conditioned on the topic
1170
and path assignments of all other document tokens,
based on the variational expectation of pi,
q(z
dn
= k, y
dn
= s|?z
dn
,?y
dn
;w) ? (10)
(?+
?
m 6=n
I [z
dm
= k])
? exp{E
q
[log p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)]}.
This equation embodies how this is a hybrid algo-
rithm: the first term resembles the Gibbs sampling
term encoding how much a document prefers a
topic, while the second term encodes the expecta-
tion under the variational distribution of how much
a path is preferred by this topic,
E
q
[log p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)] = I
[?(y
dn
)=w
dn
]
?
?
i?j?y
dn
E
q
[log ?
z
dn
,i?j
].
For every document, we sweep over all its to-
kens and resample their topic z
dn
and path y
dn
conditioned on all the other tokens? topic and path
assignments ?z
dn
and ?y
dn
. To avoid bias, we
discard the first B burn-in sweeps and take the
following M samples. We then use the empirical
average of these samples update the global varia-
tional parameter q(pi|?) based on how many times
we sampled these paths
?
k,i?j
=
1
M
?
d
?
n
?
s??
?1
(w
dn
)
(
I [i? j ? s]
? I [z
dn
= k, y
dn
= s]
)
+ ?
i?j
. (11)
For our experiments, we use the recommended set-
tingsB = 5 andM = 5 from Mimno et al (2012).
5 Experiments
We evaluate our new topic model, ptLDA, and exist-
ing topic models?LDA, pLDA, and tLDA?on their
ability to induce domains for machine translation
and the resulting performance of the translations
on standard machine translation metrics.
Dataset and SMT Pipeline We use the NIST MT
Chinese-English parallel corpus (NIST), excluding
non-UN and non-HK Hansards portions as our train-
ing dataset. It contains 1.6M sentence pairs, with
40.4M Chinese tokens and 44.4M English tokens.
We replicate the SMT pipeline of Eidelman et al
(2012): word segmentation (Tseng et al, 2005),
align (Och and Ney, 2003), and symmetrize (Koehn
et al, 2003) the data. We train a modified Kneser-
Ney trigram language model on English (Chen and
Goodman, 1996). We use CDEC (Dyer et al, 2010)
for decoding, and MIRA (Crammer et al, 2006)
for parameter training. To optimize SMT system,
we tune the parameters on NIST MT06, and report
results on three test sets: MT02, MT03 and MT05.
2
Topic Models Configuration We compare our
polylingual tree-based topic model (ptLDA) against
tree-based topic models (tLDA), polylingual topic
models (pLDA) and vanilla topic models (LDA).
3
We also examine different inference algorithms?
Gibbs sampling (gibbs), variational inference
(variational) and hybrid approach (variational-
hybrid)?on the effects of SMT performance. In
all experiments, we set the per-document Dirichlet
parameter ? = 0.01 and the number of topics to
10, as used in Eidelman et al (2012).
Resources for Prior Tree To build the tree for
tLDA and ptLDA, we extract the word correla-
tions from a Chinese-English bilingual dictio-
nary (Denisowski, 1997).
4
We filter the dictionary
using the NIST vocabulary, and keep entries map-
ping single Chinese and single English words. The
prior tree has about 1000 word pairs (dict).
We also extract the bidirectional word align-
ments between Chinese and English using
GIZA++ (Och and Ney, 2003). We then remove
the word pairs appearing more than 50K times or
fewer than 500 times and construct a second prior
tree with about 2500 word pairs (align).
We apply both trees to tLDA and ptLDA, denoted
as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDA-
align. However, tLDA-align and ptLDA-align do
worse than tLDA-dict and ptLDA-dict, so we omit
tLDA-align in the results.
Domain Adaptation using Topic Models We
examine the effectiveness of using topic models
for domain adaptation on standard SMT evalua-
tion metrics?BLEU (Papineni et al, 2002) and
TER (Snover et al, 2006). We report the results
on three different test sets (Figure 2), and all SMT
results are averaged over five runs.
We refer to the SMT model without domain adap-
tation as baseline.
5
LDA marginally improves ma-
chine translation (less than half a BLEU point).
2
The NIST datasets contain 878, 919, 1082 and 1664 sen-
tences for MT02, MT03, MT05 and MT06 respectively.
3
For Gibbs sampling, we use implementations available in
Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum,
2002) for LDA and pLDA.
4
This is a two-level tree structure. However, one could
build a more sophisticated tree prior with a hierarchical dictio-
nary such as multilingual WordNet.
5
Our replication of Eidelman et al (2012) yields slightly
higher baseline performance, but the trend is consistent.
1171
gibbs variational variational?hybrid
34.8 +0.3 +0.6 +0.4
+1.2 +0.5
35.1 +0.1 +0.3 +0.2 +0.7 +0.4
31.4 +0.4 +0.7 +0.4 +1 +0.4
34.8 +0.4 +0.5 +0.4 +0.8 +0.5
35.1
?0.1 +0.2 ?0.1 +0.2 +0.2
31.4 +0.3 +0.5 +0.3 +0.8 +0.4
34.8 +0.2 +0.4 +0.2 +0.7 +0.4
35.1
?0.1 ?0.1 ?0.1 +0.2 +0.2
31.4 +0.3 +0.3 +0.1 +0.6 +0.3
3132
3334
3536
37
3132
3334
3536
37
3132
3334
3536
37
mt02
mt03
mt05
BLE
U S
cor
e
model baseline LDA pLDA ptLDA?align ptLDA?dict tLDA?dict
gibbs variational variational?hybrid
61.9 ?0.1
?1 ?1.2
?2.5 ?1.1
60.1
?0.3
?0.9 ?0.8
?1.9 ?0.9
63.3
?0.9
?1.3 ?1.2
?2.6 ?1.1
61.9
?0.4
?1 ?0.6
?1.6 ?1.3
60.1
?0.2
?0.5 ?0.1
?1 ?0.7
63.3
?0.5
?1 ?0.4
?1.5 ?1.2
61.9
?0.3
?0.7 ?0.1
?1.6 ?0.9
60.1 0 ?0.2 +0.2
?1.1 ?0.5
63.3
?0.4
?0.7 ?0.1
?1.6 ?0.8
5658
6062
6466
5658
6062
6466
5658
6062
6466
mt02
mt03
mt05
TE
R S
cor
e
model baseline LDA pLDA ptLDA?align ptLDA?dict tLDA?dict
Figure 2: Machine translation performance for different models and inference algorithms against the
baseline, on BLEU (top, higher the better) and TER (bottom, lower the better) scores. Our proposed ptLDA
performs best. Results are averaged over 5 random runs. For model ptLDA-dict with different inference
schemes, the BLEU improvement on three test sets is mostly significant with p = 0.01, except the results
on MT03 using variational and variational-hybrid inferences.
Polylingual topic models pLDA and tree-based
topic models tLDA-dict are consistently better than
LDA, suggesting that incorporating additional bilin-
gual knowledge improves topic models. These im-
provements are not redundant: our new ptLDA-dict
model, which has aspects of both models yields the
best performance among these approaches?up to a
1.2 BLEU point gain (higher is better), and -2.6 TER
improvement (lower is better). The BLEU improve-
ment is significant (Koehn, 2004) at p = 0.01,
6
except on MT03 with variational and variational-
hybrid inference.
While ptLDA-align performs better than base-
line SMT and LDA, it is worse than ptLDA-dict,
possibly because of errors in the word alignments,
making the tree priors less effective.
Scalability While gibbs has better translation
scores than variational and variational-hybrid, it
is less scalable to larger datasets. With 1.6M NIST
6
Because we have multiple runs of each topic model (and
thus different translation models), we select the run closest to
the average BLEU for the translation significance test.
training sentences, gibbs takes nearly a week to
run 1000 iterations. In contrast, the parallelized
variational and variational-hybrid approaches,
which we implement in MapReduce (Dean and
Ghemawat, 2004; Wolfe et al, 2008; Zhai et al,
2012), take less than a day to converge.
6 Discussion
In this section, we qualitatively analyze the trans-
lation results and investigate how ptLDA and its
cousins improve SMT. We also discuss other ap-
proaches to improve unsupervised domain adapta-
tion for SMT.
6.1 How do Topic Models Help SMT?
We present two examples of how topic models can
improve SMT. The first example shows both LDA
and ptLDA improve the baseline. The second exam-
ple shows how LDA introduce biases that mislead
SMT and how ptLDA?s bilingual constraints correct
these mistakes.
Figure 3 shows a sentence about a company
1172
source ???????????
 , ????
reference
sony has already sold about 570,000 units of narrowband connection 
kits in north america at the price of about 39 us dollars and some 20 
compatible games .
baseline
LDA
ptLDA
? internet links set ...
? internet links kit ? 
? internet links kit ?  
? with about 20 of the game .
? , there are about 20 compatible games .
? , there are about 20 compatible games .
source ?  ... ? ???

LDA-Topic 0 (business)
ptLDA-Topic 0 (business)
reference
? connection kits ... ? some 20 compatible games .

	, ???

??(company), ??(China), ?(service), ?
(market), ?(technology), ?(industry), ??
(provide), (develop), ?(year), 
(product), 
?, ??(coorporate), ?, ??(manage), ?
(invest), (economy), ?(international), ?
(system), (bank)
??(company), ?(service), ?(market), ?
(technology), china, ?(industry), 

(product), market, company, technology, services, 
?(system), year, industry, products, business, 
(economy), information, ??(manage), ?
(invest), percent, ?	(internet), companies, world, 
system, ??(information), ?(increase), 
(device), service, (service)
Figure 3: Better SMT result using topic models for domain adaptation. Top row: the source sentence and
its reference translation. Middle row: the highlighted translations from different approaches. Bottom row:
the change of relevant translation probabilities after incorporating the domain knowledge from LDA and
ptLDA. Right: most-probable words of the topic the source sentence is assigned to under LDA (top) and
ptLDA (bottom). The Chinese translations are in parenthesis.
introducing new technology gadgets where both
LDA and ptLDA improve translations. The base-
line translates ???? to ?set? (red), and ???? to
?with? (blue), which do not capture the reference
meaning of a add-on device that works with com-
patible games. Both LDA and ptLDA assign this
sentence to a business domain, which makes the
translations probabilities shift toward correct trans-
lations: the probability of translating ???? to
?compatible? and the probability of translating ??
?? to ?kit? in the business domain are both signif-
icantly larger than without the domain knowledge;
and the probabilities of translating ???? to ?with?
and the probability of translating ?set? to ????
in the business domain decrease.
The second example (Figure 4) illustrates how
ptLDA offers further improvements over LDA. The
source sentence discusses foreign affairs. The
baseline correctly translates the word ???? to
?affect?. However, LDA?which only takes mono-
lingual information from the source language?
assigns this sentence to economic development.
This misleads SMT to lower the probability for
the correct translation ?affect?; it chooses ?impact?
instead. In contrast, ptLDA?which incorporates
bilingual constraints?successfully labels this sen-
tence as foreign affairs and produces a softer, more
nuanced translation that better matches the refer-
ence. The translation of ???? is very similar,
except in this case, both the baseline and LDA
produce the incorrect translation ?the commitment
of?. This is possible because the probabilities of
translating ???? to ?promised to? and translat-
ing ?promised to? to ???? (the correct transla-
tion, in both directions) increase when conditioned
on ptLDA?s correct topic but decrease when condi-
tioned on LDA?s incorrect topic.
6.2 Other Approaches
Other approaches have used topic models for ma-
chine translation. Xiao et al (2012) present a topic
similarity model based on LDA that produces a fea-
ture that weights grammar rules based on topic
compatibility. They also model the source and tar-
get side of rules and compare the target similarity
during decoding by projecting the target distribu-
tion into the source space. Hasler et al (2012)
use the source-side topic assignments from hidden
topic Markov models (Gruber et al, 2007, HTMM)
which models documents as a Markov chain and
assign one topic to the whole sentence, instead of
a mixture of topics. Su et al (2012) also apply
HTMM to monolingual data and apply the results to
machine translation. To our knowledge, however,
this is the first work to use multilingual topic mod-
els for domain adaptation in machine translation.
6.3 Improving Language Models
Topic models capture document-level properties
of language, but a critical component of machine
translation systems is the language model, which
provides local constraints and preferences. Do-
main adaptation for language models (Bellegarda,
2004; Wood and Teh, 2009) is an important avenue
for improving machine translation. Models that si-
multaneously discover global document themes as
well as local, contextual domain-specific informa-
1173
source
????, ?????????, ???????????Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 188?194,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Measuring Transitivity Using Untrained Annotators
Nitin Madnania,b Jordan Boyd-Grabera Philip Resnika,c
aInstitute for Advanced Computer Studies
bDepartment of Computer Science
cDepartment of Linguistics
University of Maryland, College Park
{nmadnani,jbg,resnik}@umiacs.umd.edu
Abstract
Hopper and Thompson (1980) defined a multi-axis
theory of transitivity that goes beyond simple syn-
tactic transitivity and captures how much ?action?
takes place in a sentence. Detecting these features
requires a deep understanding of lexical semantics
and real-world pragmatics. We propose two gen-
eral approaches for creating a corpus of sentences
labeled with respect to the Hopper-Thompson transi-
tivity schema using Amazon Mechanical Turk. Both
approaches assume no existing resources and incor-
porate all necessary annotation into a single system;
this is done to allow for future generalization to other
languages. The first task attempts to use language-
neutral videos to elicit human-composed sentences
with specified transitivity attributes. The second task
uses an iterative process to first label the actors and
objects in sentences and then annotate the sentences?
transitivity. We examine the success of these tech-
niques and perform a preliminary classification of
the transitivity of held-out data.
Hopper and Thompson (1980) created a multi-axis the-
ory of Transitivity1 that describes the volition of the sub-
ject, the affectedness of the object, and the duration of the
action. In short, this theory goes beyond the simple gram-
matical notion of transitivity (whether verbs take objects
? transitive ? or not ? intransitive) and captures how
much ?action? takes place in a sentence. Such notions of
Transitivity are not apparent from surface features alone;
identical syntactic constructions can have vastly different
Transitivity. This well-established linguistic theory, how-
ever, is not useful for real-world applications without a
Transitivity-annotated corpus.
Given such a substantive corpus, conventional machine
learning techniques could help determine the Transitivity
of verbs within sentences. Transitivity has been found to
play a role in what is called ?syntactic framing,? which
expresses implicit sentiment (Greene and Resnik, 2009).
1We use capital ?T? to differentiate from conventional syntactic tran-
sitivity throughout the paper.
In these contexts, the perspective or sentiment of the
writer is reflected in the constructions used to express
ideas. For example, a less Transitive construction might
be used to deflect responsibility (e.g. ?John was killed?
vs. ?Benjamin killed John?).
In the rest of this paper, we review the Hopper-
Thompson transitivity schema and propose two relatively
language-neutral methods to collect Transitivity ratings.
The first asks humans to generate sentences with de-
sired Transitivity characteristics. The second asks hu-
mans to rate sentences on dimensions from the Hopper-
Thompson schema. We then discuss the difficulties of
collecting such linguistically deep data and analyze the
available results. We then pilot an initial classifier on the
Hopper-Thompson dimensions.
1 Transitivity
Table 1 shows the subset of the Hopper-Thompson di-
mensions of Transitivity used in this study. We excluded
noun-specific aspects as we felt that these were well cov-
ered by existing natural language processing (NLP) ap-
proaches (e.g. whether the object / subject is person, ab-
stract entity, or abstract concept is handled well by exist-
ing named entity recognition systems) and also excluded
aspects which we felt had significant overlap with the
dimensions we were investigating (e.g. affirmation and
mode).
We also distinguished the original Hopper-Thompson
?Affectedness? aspect into separate ?Benefit? and
?Harm? components, as we suspect that these data will
be useful to other applications such as sentiment analy-
sis.
We believe that these dimensions of transitivity are
simple and intuitive enough that they can be understood
and labeled by the people on Amazon Mechanical Turk,
a web service. Amazon Mechanical Turk (MTurk) allows
individuals to post jobs on MTurk with a set fee that are
then performed by workers on the Internet. MTurk con-
nects workers to people with tasks and handles the coor-
dination problems of payment and transferring data.
188
Kinesis Sentences where movement happens are perceived to be more Transitive.?Sue jumped out of an airplane? vs. ?The corporation jumped to a silly conclusion.?
Punctuality Sentences where the action happens quickly are perceived to be more Transitive.?She touched her ID to the scanner to enter? vs. ?I was touched by how much she helped me.?
Mode Sentences with no doubt about whether the action happened are perceived to be more Transitive.?Bob was too busy to fix the drain? vs. ?Bob fixed the drain.?
Affectedness Sentences where the object is more affected by the action are perceived to be more Transitive.?The St. Bernard saved the climber? vs. ?Melanie looked at the model.?
Volition Sentences where the actor chose to perform the action are perceived to be more Transitive.?Paul jumped out of the bushes and startled his poor sister? vs. ?The picture startled George.?
Aspect Sentences where the action is done to completion are perceived to be more Transitive.?Walter is eating the hamburger? vs. ?Walter ate the pudding up.?
Table 1: The Hopper-Thompson dimensions of transitivity addressed in this paper. In experiments, ?Affectedness? was divided into
?Harm? and ?Benefit.?
2 Experiments
Our goal is to create experiments for MTurk that will pro-
duce a large set of sentences with known values of Tran-
sitivity. With both experiments, we design the tasks to
be as language independent as possible, thus not depend-
ing on language-specific preprocessing tools. This allows
the data collection approach to be replicated in other lan-
guages.
2.1 Elicitation
The first task is not corpus specific, and requires no
language-specific resources. We represent verbs using
videos (Ma and Cook, 2009). This also provides a form
of language independent sense disambiguation. We dis-
play videos illustrating verbs (Figure 1) and ask users on
MTurk to identify the action and give nouns that can do
the action and ? in a separate task ? the nouns that the
action can be done to. For quality control, Turkers must
match a previous Turker?s response for one of their an-
swers (a la the game show ?Family Feud?).
Figure 1: Stills from three videos depicting the verbs ?receive,?
?hear,? and ?help.?
We initially found that subjects had difficulty distin-
guishing what things could do the action (subjects) vs.
what things the action could be done to (objects). In or-
der to suggest the appropriate syntactic frame, we use
javascript to form their inputs into protosentences as they
typed. For example, if they identified an action as ?pick-
ing? and suggested ?fruit? as a possible object, the pro-
tosentence ?it is picking fruit? is displayed below their
input (Figure 2). This helped ensure consistent answers.
The subject and object tasks were done separately, and
for the object task, users were allowed to say that there
is nothing the action can be done to (for example, for an
intransitive verb).
Figure 2: A screenshot of a user completing a task to find ob-
jects of a particular verb, where the verb is represented by a
film. After the user has written a verb and a noun, a protosen-
tence is formed and shown to ensure that the user is using the
words in the appropriate roles.
These subjects and objects we collected were then used
as inputs for a second task. We showed workers videos
with potential subjects and objects and asked them to
create pairs of sentences with opposite Transitivity at-
tributes. For example, Write a sentence where the thing
to which the action is done benefits and Write a sentence
where the thing to which the action is done is not affected
by the action. For both sides of the Transitivity dimen-
sion, we allowed users to say that writing such a sentence
is impossible. We discuss the initial results of this task in
Section 3.
2.2 Annotation
Our second task?one of annotation?depends on having
a corpus available in the language of interest. For con-
189
creteness and availability, we use Wikipedia, a free mul-
tilingual encyclopedia. We extract a large pool of sen-
tences from Wikipedia containing verbs of interest. We
apply light preprocessing to remove long, unclear (e.g.
starting with a pronoun), or uniquely Wikipedian sen-
tences (e.g. very short sentences of the form ?See List
of Star Trek Characters?). We construct tasks, each for a
single verb, that ask users to identify the subject and ob-
ject for the verb in randomly selected sentences.2 Users
were prompted by an interactive javascript guide (Fig-
ure 3) that instructed them to click on the first word of the
subject (or object) and then to click on the last word that
made up the subject (or object). After they clicked, a text
box was automatically populated with their answer; this
decreased errors and made the tasks easier to finish. For
quality control, each HIT has a simple sentence where
subject and object were already determined by the au-
thors; the user must match the annotation on that sentence
for credit. We ended up rejecting less than one percent of
submitted hits.
Figure 3: A screenshot of the subject identification task. The
user has to click on the phrase that they believe is the subject.
Once objects and subjects have been identified, other
users rate the sentence?s Transitivity by answering the
following questions like, where $VERB represents the
verb of interest, $SUBJ is its subject and $OBJ is its ob-
ject3:
? Aspect. After reading this sentence, do you know
that $SUBJ is done $VERBing?
? Affirmation. From reading the sentence, how cer-
tain are you that $VERBing happened?
? Benefit. How much did $OBJ benefit?
? Harm. How much was $OBJ harmed?
? Kinesis. Did $SUBJ move?
? Punctuality. If you were to film $SUBJ?s act of
$VERBing in its entirety, how long would the movie
be?
? Volition. Did the $SUBJ make a conscious choice
to $VERB?
The answers were on a scale of 0 to 4 (higher num-
bers meant the sentence evinced more of the property in
2Our goal of language independence and the unreliable correspon-
dence between syntax and semantic roles precludes automatic labeling
of the subjects and objects.
3These questions were developed using Greene and Resnik?s (2009)
surveys as a foundation.
question), and each point in the scale had a description to
anchor raters and to ensure consistent results.
2.3 Rewards
Table 2 summarizes the rewards for the tasks used in
these experiments. Rewards were set at the minimal rate
that could attract sufficient interest from users. For the
?Video Elicitation? task, where users wrote sentences
with specified Transitivity properties, we also offered
bonuses for clever, clear sentences. However, this was
our least popular task, and we struggled to attract users.
3 Results and Discussion
3.1 Creative but Unusable Elicitation Results
We initially thought that we would have difficulty coax-
ing users to provide full sentences. This turned out not
to be the case. We had no difficulty getting (very imag-
inative) sentences, but the sentences were often incon-
sistent with the Transitivity aspects we are interested in.
This shows both the difficulty of writing concise instruc-
tions for non-experts and the differences between every-
day meanings of words and their meaning in linguistic
contexts.
For example, the ?volitional? elicitation task asked
people to create sentences where the subject made a con-
scious decision to perform the action. In the cases where
we asked users to create sentences where the subject did
not make a conscious decision to perform an action, al-
most all of the sentences created by users focused on sen-
tences where a person (rather than employ other tactics
such as using a less individuated subject, e.g. replacing
?Bob? with ?freedom?) was performing the action and
was coerced into doing the action. For example:
? Sellers often give gifts to their clients when they are
trying to make up for a wrongdoing.
? A man is forced to search for his money.
? The man, after protesting profusely, picked an exer-
cise class to attend
? The vegetarian Sherpa had to eat the pepperoni pizza
or he would surely have died.
While these data are likely still interesting for other pur-
poses, their biased distribution is unlikely to be useful for
helping identify whether an arbitrary sentence in a text
expresses the volitional Transitivity attribute. The users
prefer to have an animate agent that is compelled to take
the action rather than create sentences where the action
happens accidentally or is undertaken by an abstract or
inanimate actor.
Similarly, for the aspect dimension, many users simply
chose to represent actions that had not been completed
190
Task Questions / Hit Pay Repetition Tasks Total
Video Object 5 0.04 5 10 $2.00
Video Subject 5 0.04 5 10 $2.00
Corpus Object 10 0.03 5 50 $7.50
Corpus Subject 10 0.03 5 50 $7.50
Video Elicitation 5 0.10 2 70 $14.00
Corpus Annotation 7 0.03 3 400 $36.00
Total $69.00
Table 2: The reward structure for the tasks presented in this paper (not including bonuses or MTurk overhead). ?Video Subject? and
?Video Object? are where users were presented with a video and supplied the subjects and objects of the depicted actions. ?Corpus
Subject? and ?Corpus Object? are the tasks where users identified the subject and objects of sentences from Wikipedia. ?Video
Elicitation? refers to the task where users were asked to write sentences with specified Transitivity properties. ?Corpus Annotation?
is where users are presented with sentences with previously identified subjects and objects and must rate various dimensions of
Transitivity.
using the future tense. For the kinesis task, users dis-
played amazing creativity in inventing situations where
movement was correlated with the action. Unfortunately,
as before, these data are not useful in generating predic-
tive features for capturing the properties of Transitivity.
We hope to improve experiments and instructions to
better align everyday intuitions with the linguistic proper-
ties of interest. While we have found that extensive direc-
tions tend to discourage users, perhaps there are ways in-
crementally building or modifying sentences that would
allow us to elicit sentences with the desired Transitivity
properties. This is discussed further in the conclusion,
Section 4.
3.2 Annotation Task
For the annotation task, we observed that users often had
a hard time keeping their focus on the words in question
and not incorporating additional knowledge. For exam-
ple, for each of the following sentences:
? Bonosus dealt with the eastern cities so harshly that
his severity was remembered centuries later .
? On the way there, however, Joe and Jake pick an-
other fight .
? The Black Sea was a significant naval theatre of
World War I and saw both naval and land battles
during World War II .
? Bush claimed that Zubaydah gave information that
lead to al Shibh ?s capture .
some users said that the objects in bold were greatly
harmed, suggesting that users felt even abstract concepts
could be harmed in these sentences. A rigorous inter-
pretation of the affectedness dimension would argue that
these abstract concepts were incapable of being harmed.
We suspect that the negative associations (severity, fight,
battles, capture) present in this sentence are causing users
to make connections to harm, thus creating these ratings.
Similarly, world knowledge flavored other questions,
such as kinesis, where users were able to understand from
context that the person doing the action probably moved
at some point near the time of the event, even if move-
ment wasn?t a part of the act of, for example, ?calling? or
?loving.?
3.3 Quantitative Results
For the annotation task, we were able to get consistent
ratings of transitivity. Table 3 shows the proportion of
sentences where two or more annotators agreed on the
a Transitivity label of the sentences for that dimension.
All of the dimensions were significantly better than ran-
dom chance agreement (0.52); the best was harm, which
has an accessible, clear, and intuitive definition, and the
worst was kinesis, which was more ambiguous and prone
to disagreement among raters.
Dimension Sentences
with Agreement
HARM 0.87
AFFIRMATION 0.86
VOLITION 0.86
PUNCTUALITY 0.81
BENEFIT 0.81
ASPECT 0.80
KINESIS 0.70
Table 3: For each of the dimensions of transitivity, the propor-
tion of sentences where at least two of three raters agreed on the
label. Random chance agreement is 0.52.
Figure 4 shows a distribution for each of the Transitiv-
ity data on the Wikipedia corpus. These data are consis-
tent with what one would expect from random sentences
from an encyclopedic dataset; most of the sentences en-
191
Median Score
Co
un
t
0
50
100
150
200
250
0
50
100
150
200
250
AFFIRMATION
KINESIS
0 1 2 3 4
ASPECT
PUNCTUALITY
0 1 2 3 4
BENEFIT
VOLITIONALITY
0 1 2 3 4
HARM
0 1 2 3 4
Figure 4: Histograms of median scores from raters by Transitivity dimension. Higher values represent greater levels of Transitivity.
code truthful statements, most actions have been com-
pleted, most objects are not affected, most events are over
a long time span, and there is a bimodal distribution over
volition. One surprising result is that for kinesis there
is a fairly flat distribution. One would expect a larger
skew toward non-kinetic words. Qualitative analysis of
the data suggest that raters used real-world knowledge to
associate motion with the context of actions (even if mo-
tion is not a part of the action), and that raters were less
confident about their answers, prompting more hedging
and a flat distribution.
3.4 Predicting Transitivity
We also performed an set of initial experiments to investi-
gate our ability to predict Transitivity values for held out
data. We extracted three sets of features from the sen-
tences: lexical features, syntactic features, and features
derived from WordNet (Miller, 1990).
Lexical Features A feature was created for each word
in a sentence after being stemmed using the Porter stem-
mer (Porter, 1980).
Syntactic Features We parsed each sentence using the
Stanford Parser (Klein and Manning, 2003) and used
heuristics to identify cases where the main verb is tran-
sitive, where the subject is a nominalization (e.g. ?run-
ning?), or whether the sentence is passive. If any of these
constructions appear in the sentence, we generate a corre-
sponding feature. These represent features identified by
Greene and Resnik (2009).
WordNet Features For each word in the sentence, we
extracted all the possible senses for each word. If any
possible sense was a hyponym (i.e. an instance of) one
of: artifact, living thing, abstract entity, location, or food,
we added a feature corresponding to that top level synset.
For example, the string ?Lincoln? could be an instance
of both a location (Lincoln, Nebraska) and a living thing
(Abe Lincoln), so a feature was added for both the loca-
tion and living thing senses. In addition to these noun-
based features, features were added for each of the pos-
sible verb frames allowed by each of a word?s possible
senses (Fellbaum, 1998).
At first, we performed simple 5-way classification and
found that we could not beat the most frequent class base-
line for any dimension. We then decided to simplify the
classification task to make binary predictions of low-vs-
high instead of fine gradations along the particular di-
mension. To do this, we took all the rated sentences for
each of the seven dimensions and divided the ratings into
low (ratings of 0-1) and high (ratings of 2-4) values for
that dimension. Table 4 shows the results for these bi-
nary classification experiments using different classifiers.
All of the classification experiments were conducted us-
ing the Weka machine learning toolkit (Hall et al, 2009)
and used 10-fold stratified cross validation.
Successfully rating Transitivity requires knowledge
beyond individual tokens. For example, consider kine-
sis. Judging kinesis requires lexical semantics to realize
whether a certain actor is capable of movement, pragmat-
ics to determine if the described situation permits move-
ment, and differentiating literal and figurative movement.
One source of real-world knowledge is WordNet;
adding some initial features from WordNet appears to
help aid some of these classifications. For example, clas-
sifiers trained on the volitionality data were not able to
do better than the most frequent class baseline before the
addition of WordNet-based features. This is a reasonable
result, as WordNet features help the algorithm generalize
which actors are capable of making decisions.
192
Dimension Makeup
Classifier Accuracy
Baseline NB VP SVM-WN +WN -WN +WN -WN +WN
HARM 269/35 88.5 83.9 84.9 87.2 87.8 88.5 88.5
AFFIRMATION 380/20 95.0 92.5 92.0 94.3 95.0 95.0 95.0
VOLITION 209/98 68.1 66.4 69.4 67.1 73.3 68.1 68.1
PUNCTUALITY 158/149 51.5 59.6 61.2 57.0 59.6 51.5 51.5
BENEFIT 220/84 72.4 69.1 65.1 73.4 71.4 72.4 72.4
ASPECT 261/46 85.0 76.5 74.3 81.1 84.7 85.0 85.0
KINESIS 160/147 52.1 61.2 61.2 56.4 60.9 52.1 52.1
Table 4: The results of preliminary binary classification experiments for predicting various transitivity dimensions using different
classifiers such as Naive Bayes (NB), Voted Perceptron (VP) and Support Vector Machines (SVM). Classifier accuracies for two
sets of experiments are shown: without WordNet features (-WN) and with WordNet features (+WN). The baseline simply predicts
the most frequent class. For each dimension, the split between low Transitivity (rated 0-1) and high Transitivity (rated 2-4) is shown
under the ?Makeup? column. All reported accuracies are using 10-fold stratified cross validation.
4 Conclusion
We began with the goal of capturing a subtle linguistic
property for which annotated datasets were not available.
We created a annotated dataset of 400 sentences taken
from the real-word dataset Wikipedia annotated for seven
different Transitivity properties. Users were able to give
consistent answers, and we collected results in a man-
ner that is relatively language independent. Once we ex-
pand and improve this data collection scheme for English,
we hope to perform similar data collection in other lan-
guages. We have available the translated versions of the
questions used in this study for Arabic and German.
Our elicitation task was not as successful as we had
hoped. We learned that while we could form tasks using
everyday language that we thought captured these sub-
tle linguistic properties, we also had many unspoken as-
sumptions that the creative workers on MTurk did not
necessarily share. As we articulated these assumptions
in increasingly long instruction sets to workers, the sheer
size of the instructions began to intimidate and scare off
workers.
While it seems unlikely we can strike a balance that
will give us the answers we want with the elegant instruc-
tions that workers need to feel comfortable for the tasks
as we currently defined them, we hope to modify the task
to embed further linguistic assumptions. For example, we
hope to pilot another version of the elicitation task where
workers modify an existing sentence to change one Tran-
sitivity dimension. Instead of reading and understanding
a plodding discussion of potentially irrelevant details, the
user can simply see a list of sentence versions that are not
allowed.
Our initial classification results suggest that we do not
yet have enough data to always detect these Transitiv-
ity dimensions from unlabeled text or that our algorithms
are using features that do not impart enough information.
It is also possible that using another corpus might yield
greater variation in Transitivity that would aid classifica-
tion; Wikipedia by design attempts to keep a neutral tone
and eschews the highly charged prose that would contain
a great deal of Transitivity.
Another possibility is that, instead of just the Transi-
tivity ratings alone, tweaks to the data collection process
could also help guide classification algorithms (Zaidan et
al., 2008). Thus, instead of clicking on a single annota-
tion label in our current data collection process, Turkers
would click on a data label and the word that most helped
them make a decision.
Our attempts to predict Transitivity are not exhaus-
tive, and there are a number of reasonable algorithms
and resources which could also be applied to the prob-
lem; for example, one might expect semantic role label-
ing or sense disambiguation to possibly aid the prediction
of Transitivity. Determining which techniques are effec-
tive and the reasons why they are effective would aid not
just in predicting Transitivity, which we believe to be an
interesting problem, but also in understanding Transitiv-
ity.
Using services like MTurk allows us to tighten the loop
between data collection, data annotation, and machine
learning and better understand difficult problems. We
hope to refine the data collection process to provide more
consistent results on useful sentences, build classifiers,
and extract features that are able to discover the Transi-
tivity of unlabeled text. We believe that our efforts will
help cast an interesting aspect of theoretical linguistics
into a more pragmatic setting and make it accessible for
use in more practical problems like sentiment analysis.
References
C. Fellbaum, 1998. WordNet : An Electronic Lexi-
cal Database, chapter A semantic network of English
193
verbs. MIT Press, Cambridge, MA.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 503?
511.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
Paul J. Hopper and Sandra A. Thompson. 1980.
Transitivity in grammar and discourse. Language,
(56):251?299.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 423?430.
Xiaojuan Ma and Perry R. Cook. 2009. How well do
visual verbs work in daily communication for young
and old adults? In international conference on Human
factors in computing systems, pages 361?364.
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2008. Machine learning with annotator rationales
to reduce annotation cost. In Proceedings of the
NIPS*2008 Workshop on Cost Sensitive Learning,
Whistler, BC, December. 10 pages.
194
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 63?70,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Quantifying the role of discourse topicality
in speakers? choices of referring expressions
Naho Orita
Department of Linguistics
University of Maryland
naho@umd.edu
Eliana Vornov
Departments of Computer Science and Linguistics
University of Maryland
evornov@umd.edu
Naomi H. Feldman
Department of Linguistics
University of Maryland
nhf@umd.edu
Jordan Boyd-Graber
College of Information Studies and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Abstract
The salience of an entity in the discourse
is correlated with the type of referring ex-
pression that speakers use to refer to that
entity. Speakers tend to use pronouns to
refer to salient entities, whereas they use
lexical noun phrases to refer to less salient
entities. We propose a novel approach to
formalize the interaction between salience
and choices of referring expressions us-
ing topic modeling, focusing specifically
on the notion of topicality. We show that
topic models can capture the observation
that topical referents are more likely to be
pronominalized. This lends support to the-
ories of discourse salience that appeal to
latent topic representations and suggests
that topic models can capture aspects of
speakers? cognitive representations of en-
tities in the discourse.
1 Introduction
Speakers? choices of referring expressions (pro-
nouns, demonstratives, full names, and so on) have
been used as a tool to understand cognitive rep-
resentations of entities in a discourse. Many re-
searchers have proposed a correlation between the
type of a referring form and saliency (or accessi-
bility, prominence, focus) of the entity in the dis-
course (Chafe, 1976; Gundel et al., 1993; Bren-
nan, 1995; Ariel, 1990). Because a pronoun car-
ries less information compared to more specified
forms (e.g., she vs. Hillary Clinton), theories pre-
dict that speakers tend to use pronouns when they
think that a referent is sufficiently salient in the
discourse. When the referent is less salient, more
specified forms are used. In other words, the like-
lihood of pronominalization increases as referents
become more salient.
Topic modeling (Blei et al., 2003; Griffiths et
al., 2007) uses a probabilistic model that recovers
a latent topic representation from observed words
in a document. The model assumes that words ap-
pearing in documents have been generated from a
mixture of latent topics. These latent topics have
been argued to provide a coarse semantic repre-
sentation of documents and to be in close corre-
spondence with many aspects of human seman-
tic cognition (Griffiths et al., 2007). This previ-
ous work has focused on semantic relationships
among words and documents. While it is often
assumed that the topics extracted by topic models
correspond to the gist of a document, and although
topic models have been used to capture discourse-
level properties in some settings (Nguyen et al.,
2013), the ability of topic models to capture cogni-
tive aspects of speakers? discourse representations
has not yet been tested.
In this paper we use topic modeling to formal-
ize the idea of salience in the discourse. We fo-
cus specifically on the idea of topicality as a pre-
dictor of salience (Ariel, 1990; Arnold, 1998) and
ask whether the latent topics that are recovered by
topic models can predict speakers? choices of re-
ferring expressions. Simulations show that the ref-
erents of pronouns belong, on average, to higher
probability topics than the referents of full noun
phrases, indicating that topical referents are more
likely to be pronominalized. This suggests that
63
the information recovered by topic models is rele-
vant to speakers? choices of referring expressions
and that topic models can provide a useful tool for
quantifying speakers? representations of entities in
the discourse.
The structure of this paper is as follows. Sec-
tion 2 briefly reviews studies that look at the cor-
relation between saliency and choices of refer-
ring expression, focusing on topicality, and intro-
duces our approach to this problem. Section 3 de-
scribes a model that learns a latent topic distribu-
tion and formalizes the notion of topicality within
this framework. Section 4 describes the data we
used for our simulation. Section 5 shows simula-
tion results. Section 6 discusses implications and
future directions.
2 Saliency and referring expressions
Various factors have been proposed to influence
referent salience (Arnold, 1998; Arnold, 2010).
These factors include giveness (Chafe, 1976; Gun-
del et al., 1993), grammatical position (Bren-
nan, 1995; Stevenson et al., 1994), order of men-
tion (J?arvikivi et al., 2005; Kaiser and Trueswell,
2008), recency (Giv?on, 1983; Arnold, 1998), syn-
tactic focus and syntactic topic (Cowles et al.,
2007; Foraker and McElree, 2007; Walker et al.,
1994), parallelism (Chambers and Smyth, 1998;
Arnold, 1998), thematic role (Stevenson et al.,
1994; Arnold, 2001; Rohde et al., 2007), coher-
ence relation (Kehler, 2002; Rohde et al., 2007)
and topicality (Ariel, 1990; Arnold, 1998; Arnold,
1999). Psycholinguistic experiments (Arnold,
1998; Arnold, 2001; Kaiser, 2006) show that de-
termining the salient referent is a complex process
which is affected by various sources of informa-
tion, and that these multiple factors have different
strengths of influence.
Among the numerous factors influencing the
salience of a referent, this study focuses on top-
icality. In contrast to surface-level factors such
as grammatical position, order of mention, and re-
cency, the representation of topicality is latent and
requires inference. Because of this latent repre-
sentation, it has been challenging to investigate the
role of topicality in discourse.
Many researchers have observed that there is a
correlation between a linguistic category ?topic?
and referent salience and have suggested that top-
ical referents are more likely to be pronominal-
ized (Ariel, 1990; Dahl and Fraurud, 1996). How-
ever, Arnold (2010) points out that examining the
relation between topicality and choices of refer-
ring expressions is difficult for two reasons. First,
identifying the topic is known to be hard. Arnold
(2010) shows that it is hard to determine what the
topic is even in a simple sentence like Andy brews
beer (Is the topic Andy, beer, or brewing?). Sec-
ond, researchers have defined the notion of ?topic?
differently as follows.
? The topic is often defined as what the sen-
tence is about (Reinhart, 1981).
? The topic can be defined as prominent
characters such as the protagonist (Francik,
1985).
? The topic is often associated with old infor-
mation (Gundel et al., 1993).
? The subject position is considered to be a top-
ical position (Chafe, 1976).
? Repeated mentions are topical (Kameyama,
1994).
? Psycholinguistic experiments define a dis-
course topic as a referent that has already
been mentioned in the preceding discourse
as a pronoun/the topic of a cleft (Arnold,
1999) or realized in subject position (Cowles,
2003).
? Centering theory (Grosz et al., 1995; Bren-
nan, 1995) formalizes the topic as a
backward-looking center that is a single en-
tity mentioned in the last sentence and in the
most salient grammatical position (the gram-
matical subject is the most salient, and fol-
lowed by the object and oblique object).
? Giv?on (1983) suggests that all discourse enti-
ties are topical but that topicality is defined by
a gradient/continuous property. Giv?on shows
that three measures of topicality ? recency
(the distance between the referent and the
referring expression), persistence (how long
the referent would remain in the subsequent
discourse), and potential interference (how
many other potential referents of the refer-
ring expression there are in the preceding dis-
course) ? correlate with the types of reference
expressions. Note that these scales measure
topicality of the referring expression, but not
the referent per se.
The variation in the literature seems to de-
rive from three fundamental properties. First, as
Arnold (2010) pointed out, there is variation in the
64
linguistic unit that bears the topic. For example,
Reinhart (1981) defines each sentence as having
a single topic, whereas Giv?on (1983) defines each
entity as having a single topic. Second, there is a
variation in type of variable. For example, Giv?on
(1983) defines topicality as a continuous property,
whereas Centering seems to treat topicality as cat-
egorical based on the grammatical position of the
referent. Third, many studies define ?topic? as a
combination of surface linguistic factors such as
grammatical position and recency. When topical-
ity is defined in terms of meaning, as in Reinhart
(1981), we face difficulty in identifying what the
topic is, as summarized in Arnold (1998). None of
the existing definitions/measures seem to provide
a way to capture latent topic representations, and
this makes it challenging to investigate their role in
discourse representations. It is this idea of latent
topic representations that we aim to formalize.
Our study investigates whether topic modeling
(Blei et al., 2003; Griffiths et al., 2007) can be
used to formalize the relationship between topi-
cality and choices of referring expressions. Be-
cause of their structured representations, consist-
ing of a set of topics as well as information about
which words belong to those topics, topic models
are able to capture topicality by means of semantic
associations. For example, observing a word Clin-
ton increases the topicality of other words associ-
ated with the topic that Clinton belongs to, e.g.,
president, Washington and so on. In other words,
topic models can capture not only the salience of
referents within a document, but also the salience
of referents via the structured topic representation
learned from multiple texts.
We use topic modeling to verify the prevailing
hypothesis that topical referents are more likely to
be pronominalized than lexical nouns. Examin-
ing the relationship between topicality and refer-
ring expressions using topic modeling provides an
opportunity to test how well the representation re-
covered by topic models corresponds to the cogni-
tive representation of entities in a discourse. If we
can recover the observation that topical referents
are more likely to be pronominalized than more
specified forms, this could indicate that topic mod-
els can capture not only aspects of human seman-
tic cognition (Griffiths et al., 2007), but also as-
pects of a higher level of linguistic representation,
discourse.
3 Model
3.1 Recovering latent topics
We formalize topicality of referents using topic
modeling. Each document is represented as a
probability distribution over topics. Each topic is
represented as a probability distribution over pos-
sible referents in the corpus. In training our topic
model, we assume that all lexical nouns in the dis-
course are potential referents. The topic model is
trained only on lexical nouns, excluding all other
words. This ensures that the latent topics capture
information about which referents typically occur
together in documents.
1
Rather than pre-specifying a number of latent
topics, we use the hierarchical Dirichlet process
(Teh et al., 2006), which learns a number of topics
to flexibly represent input data. The summary of
the generative process is as follows.
1. Draw a global topic distribution
G
0
? DP(?,H) (where ? is a hyperparame-
ter and H is a base distribution).
2. For each document d ? {1, . . . , D} (where
D denotes the number of documents in the
corpus),
(a) draw a document-topic distribution
G
d
? DP(?
0
, G
0
) (where ?
0
is a hyper-
parameter).
(b) For each referent r ? {1, . . . , N
d
}
(where N
d
denotes the number of refer-
ents in document d),
i. draw a topic parameter ?
d,r
? G
d
.
ii. draw a word x
d,r
? Mult(?
d,r
).
This process generates a distribution over topics
for each document, a distribution over referents for
each topic, and a topic assignment for each refer-
ent. The distribution over topics for each docu-
ment represents what the topics of the document
are. The distribution over referents for each topic
represents what the topic is about. An illustra-
tion of this representation is in Table 3.1. Top-
ics and words that appear in the second and third
columns are ordered from highest to lowest. We
can represent topicality of the referents using this
1
Excluding pronouns from the training set introduces a
confound, because it artificially lowers the probability of the
topics corresponding to those pronouns. However, in this pa-
per our predicted effect goes in the opposite direction: we
predict that topics corresponding to the referents of pronouns
will have higher probability than those corresponding to the
referents of lexical nouns. Excluding pronouns thus makes us
less likely to find support for our hypothesis.
65
probabilistic latent topic representation, measur-
ing which topics have high probability and assum-
ing that referents associated with high probability
topics are likely to be topical in the discourse.
Word Top 3 topic IDs Associated words in the 1st topic
Clinton 5, 26, 61 president, meeting, peace,
Washington, talks
FBI 148, 73, 67 Leung, charges, Katrina,
documents, indictment
oil 91, 145, 140 Burmah, Iraq, SHV, coda,
pipeline
Table 1: Illustration of the topic distribution
Given this generative process, we can use
Bayesian inference to recover the latent topic dis-
tribution. We use the Gibbs sampling algorithm
in Teh et al. (2006) to estimate the conditional
distribution of the latent structure, the distribu-
tions over topics associated with each document,
and the distributions over words associated with
each topic. The state space consists of latent vari-
ables for topic assignments, which we refer to as
z = {z
d,r
}. In each iteration we compute the con-
ditional distribution p(z
d,r
|x, z
?d,r
, ?), where the
subscript ?d, r denotes counts without consider-
ing z
d,r
and ? denotes all hyperparameters. Recov-
ering these latent variables allows us to determine
what the topic of the referent is and how likely that
topic is in a particular document. We use the latent
topic and its probability to represent topicality.
3.2 A measure of topicality
Discourse theories predict that topical referents
are more likely to be pronominalized than more
specified expressions.
2
We can quantify the effect
of topicality on choices of referring expressions
by comparing the topicality of the referents of two
types of referring expressions, pronouns and lexi-
cal nouns. If topical words are more likely to be
pronominalized, then the topicality of the referents
of pronouns should be higher than the topicality of
the referents of lexical nouns.
Annotated coreference chains in the corpus, de-
scribed below, are used to determine the referent
of each referring expression. We look at the topic
assigned to each referent r in document d by the
topic model, z
d,r
. We take the log probability
2
Although theories make more fine-grained predictions
on the choices of referring expressions with respect to
saliency, e.g., a full name is used to refer to less salient entity
compared to a definite description (c.f. accessibility mark-
ing scale in Ariel 1990), we focus here on the coarse contrast
between pronouns and lexical nouns.
of this topic within the document, log p(z
d,r
|G
d
),
as a measure of the topicality of the referent.
We take the expectation over a uniform distri-
bution of referents, where the uniform distribu-
tions are denoted u(lex) and u(pro), to obtain
an estimate of the average topicality of the ref-
erents of lexical nouns, E
u(lex)
[log p(z
d,r
|G
d
)],
and the average topicality of the referents of pro-
nouns, E
u(pro)
[log p(z
d,r
|G
d
)], within each docu-
ment. The expectation for the referents of the pro-
nouns in a document is computed as
E
u(pro)
[log p(z
d,r
|G
d
)] =
N
d,pro
?
r=1
log p(z
d,r
|G
d
)
N
d,pro
(1)
where N
d,pro
denotes the number of pronouns in
a document d. Replacing N
d,pro
with N
d,lex
(the
number of lexical nouns in a document d) gives us
the expectation for the referents of lexical nouns.
To obtain a single measure for each document of
the extent to which our measure of topicality pre-
dicts speakers? choices of referring expressions,
we subtract the average topicality for the referents
of lexical nouns from the average topicality for the
referents of pronouns within the document to ob-
tain a log likelihood ratio q
d
,
q
d
= E
u(pro)
[log p(z
d,r
|G
d
)]?E
u(lex)
[log p(z
d,r
|G
d
)]
(2)
A value of q
d
greater than zero indicates that the
referents of pronouns are more likely to be topical
than the referents of lexical nouns.
4 Annotated coreference data
Our simulations use a training set of the Ontonotes
corpus (Pradhan et al., 2007), which consists of
news texts. We use these data because each entity
in the corpus has a coreference annotation. We use
the coreference annotations in our evaluation, de-
scribed above. The training set in the corpus con-
sists of 229 documents, which contain 3,648 sen-
tences and 79,060 word tokens. We extract only
lexical nouns (23,084 tokens) and pronouns (2,867
tokens) from the corpus as input to the model.
3
Some preprocessing is necessary before using
these data as input to a topic model. This necessity
arises because some entities in the corpus are rep-
resented as phrases, such as in (1a) and (1b) below,
3
In particular, we extracted words that are tagged as NN,
NNS, NNP, NNPS, and for pronouns as PRP, PRP$.
66
where numbers following each expression repre-
sent the entity ID that is assigned to this expression
in the annotated corpus. However, topic models
use bag-of-words representations and therefore as-
sign latent topic structure only to individual words,
and not to entire phrases. We preprocessed these
entities as in (2). This enabled us to attribute entity
IDs to individual words, rather than entire phrases,
allowing us to establish a correspondence between
these ID numbers and the latent topics recovered
by our model for the same words.
1. Before preprocessing
(a) a tradition in Betsy?s family: 352
(b) Betsy?s family: 348
(c) Betsy: 184
2. After preprocessing
(a) tradition: 352
(b) family: 348
(c) Betsy: 184
Annotated coreference chains in the corpus were
used to determine the referent of each pronoun
and lexical noun. The annotations group all re-
ferring expressions in a document that refer to the
same entity together into one coreference chain,
with the order of expressions in the chain corre-
sponding to the order in which they appear in the
document. We assume that the referent for each
pronoun and lexical noun appears in its corefer-
ence chain. We further assume that the referent
needs to be a lexical noun, and thus exclude all
pronouns from consideration as referents. If a lex-
ical noun does not have any other words before it
in the coreference chain, i.e., that noun is the first
or the only word in that coreference chain, we as-
sume that this noun refers to itself (the noun itself
is the referent). Otherwise, if a coreference chain
has multiple referents, we take its referent to be
the lexical noun that is before and closest to the
target word.
5 Results
To recover the latent topic distribution, we ran 5
independent Gibbs sampling chains for 1000 iter-
ations.
4
Hyperparameters ?, ?
0
, and ? were fixed
at 1.0, 1.0, and 0.01, respectively.
5
The model re-
4
We used a Python version of the hierarchical Dirichlet
process implemented by Ke Zhai (http://github.com/
kzhai/PyNPB/tree/master/src/hdp).
5
Parameter ? controls how likely a new topic is to be cre-
ated in the corpus. If the value of ? is high, more topics are
covered an average of 161 topics (range: 160?163
topics).
We computed the log likelihood ratio q
d
(Equa-
tion 2) for each document and took the average of
this value across documents for each chain. The
formula to compute this average is as follows.
For each chain g,
1. get the final sample s in g.
2. For each document d in the corpus,
i. compute q
d
based on s.
3. Compute the average of all q
d
in the cor-
pus.
The average log likelihood ratio in each chain con-
sistently shows values greater than zero across
the 5 chains. The average log likelihood ratio
across chains is 1.0625 with standard deviation
0.7329. As an example, in one chain, the aver-
age of the expected values for the referents of pro-
nouns across documents is?1.1849 with standard
deviation 0.8796. In the same chain, the average
of the expected values for the referents of lexical
nouns across documents is?2.2356 with standard
deviation 0.5009.
We used the median test
6
to evaluate whether
the two groups of the referents are different with
respect to the expected values of the log probabil-
ities of topics. The test shows a significant differ-
ence between two groups (p < 0.0001).
We also computed the probability density p(q)
from the log likelihood ratio q
d
for each docu-
ment using the final samples from each chain.
Graph 1 shows the probability density p(q) from
each chain. The peak after zero confirms the ob-
served effect.
Table 2 shows examples of target pronouns and
lexical nouns, their referents, and the topic as-
signed to each referent from a document. Table 3
shows the distribution over topics in the document
obtained from one chain. Topics in Table 3 are
ordered from highest to lowest. Only four topics
were present in this document. The list of referents
associated with each topic in Table 3 is recovered
from the topic distribution over referents. This list
shows what the topic is about.
discovered in the corpus. Parameter ?
0
controls the sparse-
ness of the distribution over topics in a document, and param-
eter ? controls the sparseness of the distribution over words
in a topic.
6
The median test compares medians to test group differ-
ences (Siegel, 1956).
67
Topic ID Assciated words Probability
1 Milosevic, Kostunica, Slobodan, president, Belgrade, Serbia, Vojislav, Yugoslavia, crimes, parliament 0.64
2 president, Clinton, meeting, peace, Washington, talks, visit, negotiators, region, . . . , Alabanians 0.16
3 people, years, U.S., president, time, government, today, country, world, way, year 0.16
4 government, minister, party, Barak, today, prime, east, parliament, leader, opposition, peace, leadership 0.04
Table 3: The document-topic distribution
0.0
0.2
0.4
?1 0 1 2 3q
prob
abilit
y den
sity p
(q) Gibbs chain IDchain.01chain.02chain.03chain.04chain.05
Figure 1: The probability density of p(q)
Target Referent Referent?s Topic ID
his Spilanovic 1
he Spilanovic 1
its Belgrade 1
Goran Minister 4
Albanians Albanians 2
Kosovo Kosovo 1
Table 2: Target words, their corresponding refer-
ents, and the assigned topics of the referents
The topics associated with the pronouns his,
he and its have the highest probability in the
document-topic distribution, as shown in Table 3.
In contrast, although the topic associated with
the word Kosovo has the highest probability in
the document-topic distribution, the topics asso-
ciated with nouns Goran and Albanians do not
have high probability in the document-topic dis-
tribution. This is an example from one document,
but this tendency is observed in most of the docu-
ments in the corpus.
These results indicate that the referents of pro-
nouns are more topical than the referents of lexi-
cal nouns using our measure of topicality derived
from the topic model. This suggests that our mea-
sure of topicality captures aspects of salience that
influence choices of referring expressions.
However, there is a possibility that the effect
we observed is simply derived from referent fre-
quencies and that topic modeling structure does
not play a role beyond this. Tily and Piantadosi
(2009) found that the frequency of referents has a
significant effect on predicting the upcoming ref-
erent. Although their finding is about comprehen-
der?s ability to predict the upcoming referent (not
the type of referring expression), we conducted
an additional analysis to rule out the possibility
that referent frequencies alone were driving our re-
sults.
In order to quantify the effect of referent fre-
quency on choices of referring expressions, we
computed the same log likelihood ratio q
d
with
referent probabilities. The probability of a refer-
ent in a document was computed as follows:
p(r
i
|doc
d
) =
C
d,r
i
C
d,?
(3)
where C
d,r
i
denotes the number of mentions that
refer to referent r
i
in document d and C
d,?
denotes
the total number of mentions in document d. We
can directly compute this value by using the anno-
tated coreference chains in the corpus.
The log likelihood ratio for this measure is
2.3562. The average of the expected values for
the referents of pronouns across documents is
?1.1993 with standard deviation 0.6812. The av-
erage of the expected values for the referents of
lexical nouns across documents is ?3.5556 with
standard deviation 0.9742. The median test shows
a significant difference between two groups. (p <
0.0001). These results indicate that the frequency
of a referent captures aspects of its salience that
influence choices of referring expressions, raising
the question of whether our latent topic represen-
tations capture something that simple referent fre-
quencies do not.
In order to examine to what extent the relation-
ship between topicality and referring expressions
captures information that goes beyond simple ref-
erent frequencies, we compare two logistic regres-
68
sion models.
7
Both models are built to predict
whether a referent will be a full noun phrase or a
pronoun. The first model incorporates only the log
probability of the referent as a predictor, whereas
the second includes both the log probability of the
referent and our topicality measure as predictors.
8
The null hypothesis is that removing our topi-
cality measure from the second model makes no
difference for predicting the types of referring ex-
pressions. Under this null hypothesis, twice the
difference in the log likelihoods between the two
models should follow a ?
2
(1) distribution. We
find a significant difference in likelihood between
these two models (?
2
(1) = 118.38, p < 0.0001),
indicating that the latent measure of topicality de-
rived from the topic model predicts aspects of lis-
teners? choices of referring expressions that are
not predicted by the probabilities of individual ref-
erents.
6 Discussion
In this study we formalized the correlation be-
tween topicality and choices of referring expres-
sions using a latent topic representation obtained
through topic modeling. Both quantitative and
qualitative results showed that according to this la-
tent topic representation, the referents of pronouns
are more likely to be topical than the referents of
lexical nouns. This suggests that topic models can
capture aspects of discourse representations that
are relevant to the selection of referring expres-
sions. We also showed that this latent topic repre-
sentation has an independent contribution beyond
simple referent frequency.
This study examined only two independent fac-
tors: topicality and referent frequency. However,
discourse studies suggest that the salience of a ref-
erent is determined by various sources of informa-
tion and multiple discourse factors with different
strengths of influence (Arnold, 2010). Our frame-
work could eventually form part of a more com-
plex model that explicitly formalizes the interac-
tion of information source and various discourse
factors. Having a formal model would help by al-
lowing us to test different hypotheses and develop
a firm theory regarding cognitive representations
of entities in the discourse.
7
Models were fit using glm in R. For the log-likelihood
ratio test, lrtest in R package epicalc was used.
8
We also ran a version of this comparison in which fre-
quency of mention was included as a predictor in both mod-
els, and obtained similar results.
One possibility for exploring the role of vari-
ous discourse factors in our framework is to use
recent advances in topic modeling. For example,
TagLDA (Zhu et al., 2006) includes part-of-speech
as part of the model, and syntactic topic models
(Boyd-Graber and Blei, 2008) incorporate syntac-
tic information. Whereas simulations in our study
only used nouns as input, it has been observed that
the thematic role of the entity influences referent
salience (Stevenson et al., 1994; Arnold, 2001;
Rohde et al., 2007). Using part-of-speech and syn-
tactic information together with the topic informa-
tion could help us approximate the influence of the
thematic role and allow us to simulate how this
factor interacts with latent topic information and
other factors.
It has been challenging to quantify the influence
of latent factors such as topicality, and the simula-
tions in this paper represent only a first step toward
capturing these challenging factors. The simula-
tions nevertheless provide an example of how for-
mal models can help us validate theories of the re-
lationship between speakers? discourse represen-
tations and the language they produce.
Acknowledgments
We thank Ke Zhai, Viet-An Nguyen, and four
anonymous reviewers for helpful comments and
discussion.
References
Mira Ariel. 1990. Accessing noun-phrase antecedents.
Routledge, London.
Jennifer Arnold. 1998. Reference form and discourse
patterns. Ph.D. thesis, Stanford University Stanford,
CA.
Jennifer Arnold. 1999. Marking salience: The simi-
larity of topic and focus. Unpublished manuscript,
University of Pennsylvania.
Jennifer Arnold. 2001. The effect of thematic roles
on pronoun use and frequency of reference continu-
ation. Discourse Processes, 31(2):137?162.
Jennifer Arnold. 2010. How speakers refer: the role of
accessibility. Language and Linguistics Compass,
4(4):187?203.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan L Boyd-Graber and David M Blei. 2008. Syn-
tactic topic models. In Neural Information Process-
ing Systems, pages 185?192.
69
Susan E Brennan. 1995. Centering attention in
discourse. Language and Cognitive Processes,
10(2):137?167.
Wallace Chafe. 1976. Givenness, contrastiveness, def-
initeness, subjects, topics, and point of view. In
C. N. Li, editor, Subject and Topic. Academic Press,
New York.
Craig G Chambers and Ron Smyth. 1998. Structural
parallelism and discourse coherence: A test of Cen-
tering theory. Journal of Memory and Language,
39(4):593?608.
H Wind Cowles, Matthew Walenski, and Robert Klu-
ender. 2007. Linguistic and cognitive prominence
in anaphor resolution: topic, contrastive focus and
pronouns. Topoi, 26(1):3?18.
Heidi Wind Cowles. 2003. Processing information
structure: Evidence from comprehension and pro-
duction. Ph.D. thesis, University of California, San
Diego.
Osten Dahl and Kari Fraurud. 1996. Animacy in gram-
mar and discourse. Pragmatics and Beyond New Se-
ries, pages 47?64.
Stephani Foraker and Brian McElree. 2007. The role
of prominence in pronoun resolution: Active ver-
sus passive representations. Journal of Memory and
Language, 56(3):357?383.
Ellen Palmer Francik. 1985. Referential choice and
focus of attention in narratives (discourse anaphora,
topic continuity, language production). Ph.D. thesis,
Stanford University.
Talmy Giv?on. 1983. Topic continuity in discourse: A
quantitative cross-language study, volume 3. John
Benjamins Publishing.
Thomas L Griffiths, Mark Steyvers, and Joshua B
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114(2):211.
Barbara J Grosz, Scott Weinstein, and Aravind K Joshi.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Jeanette K Gundel, Nancy Hedberg, and Ron
Zacharski. 1993. Cognitive status and the form of
referring expressions in discourse. Language, pages
274?307.
Juhani J?arvikivi, Roger PG van Gompel, Jukka Hy?on?a,
and Raymond Bertram. 2005. Ambiguous pro-
noun resolution contrasting the first-mention and
subject-preference accounts. Psychological Sci-
ence, 16(4):260?264.
Elsi Kaiser and John C Trueswell. 2008. Interpreting
pronouns and demonstratives in Finnish: Evidence
for a form-specific approach to reference resolution.
Language and Cognitive Processes, 23(5):709?748.
Elsi Kaiser. 2006. Effects of topic and focus on
salience. In Proceedings of Sinn und Bedeutung,
volume 10, pages 139?154. Citeseer.
Megumi Kameyama. 1994. Indefeasible semantics
and defeasible pragmatics. In CWI Report CS-
R9441 and SRI Technical Note 544. Citeseer.
Andrew Kehler. 2002. Coherence, reference, and the
theory of grammar. CSLI publications, Stanford,
CA.
Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah A Cai, Jennifer E Midberry, and Yuanxin
Wang. 2013. Modeling topic control to detect in-
fluence in conversations using nonparametric topic
models. Machine Learning, pages 1?41.
Sameer S Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified relational
semantic representation. International Journal of
Semantic Computing, 1(4):405?419.
Tanya Reinhart. 1981. Pragmatics and linguistics: An
analysis of sentence topics in pragmatics and philos-
ophy I. Philosophica, 27(1):53?94.
Hannah Rohde, Andrew Kehler, and Jeffrey L Elman.
2007. Pronoun interpretation as a side effect of dis-
course coherence. In Proceedings of the 29th An-
nual Conference of the Cognitive Science Society,
pages 617?622.
Sidney Siegel. 1956. Nonparametric statistics for the
behavioral sciences. McGraw-Hill.
Rosemary J Stevenson, Rosalind A Crawley, and David
Kleinman. 1994. Thematic roles, focus and the rep-
resentation of events. Language and Cognitive Pro-
cesses, 9(4):519?548.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101:1566?
1581.
Harry Tily and Steven Piantadosi. 2009. Refer effi-
ciently: Use less informative expressions for more
predictable meanings. In Proceedings of the work-
shop on the production of referring expressions:
Bridging the gap between computational and empir-
ical approaches to reference.
Marilyn Walker, Sharon Cote, and Masayo Iida. 1994.
Japanese discourse and the process of centering.
Computational Linguistics, 20(2):193?232.
Xiaojin Zhu, David Blei, and John Lafferty. 2006.
TagLDA: Bringing document structure knowledge
into topic models. Technical report, Technical Re-
port TR-1553, University of Wisconsin.
70
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 79?82,
Baltimore, Maryland, USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Concurrent Visualization of Relationships between Words and Topics in
Topic Models
Alison Smith
?
, Jason Chuang
?
, Yuening Hu
?
, Jordan Boyd-Graber
?
, Leah Findlater
?
?
University of Maryland, College Park, MD
?
University of Washington, Seattle, WA
amsmit@cs.umd.edu, jcchuang@cs.washington.edu, ynhu@cs.umd.edu, jbg@umiacs.umd.edu, leahkf@umd.edu
Abstract
Analysis tools based on topic models are
often used as a means to explore large
amounts of unstructured data. Users of-
ten reason about the correctness of a model
using relationships between words within
the topics or topics within the model. We
compute this useful contextual informa-
tion as term co-occurrence and topic co-
variance and overlay it on top of stan-
dard topic model output via an intuitive
interactive visualization. This is a work
in progress with the end goal to combine
the visual representation with interactions
and online learning, so the users can di-
rectly explore (a) why a model may not
align with their intuition and (b) modify
the model as needed.
1 Introduction
Topic modeling is a popular technique for analyz-
ing large text corpora. A user is unlikely to have
the time required to understand and exploit the raw
results of topic modeling for analysis of a corpus.
Therefore, an interesting and intuitive visualiza-
tion is required for a topic model to provide added
value. A common topic modeling technique is La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003),
which is an unsupervised algorithm for perform-
ing statistical topic modeling that uses a ?bag of
words? approach. The resulting topic model repre-
sents the corpus as an unrelated set of topics where
each topic is a probability distribution over words.
Experienced users who have worked with a text
corpus for an extended period of time often think
of the thematic relationships in the corpus in terms
of higher-level statistics such as (a) inter-topic cor-
relations or (b) word correlations. However, stan-
dard topic models do not explicitly provide such
contextual information to the users.
Existing tools based on topic models, such
as Topical Guide (Gardner et al., 2010), Top-
icViz (Eisenstein et al., 2012), and the topic vi-
sualization of (Chaney and Blei, 2012) support
topic-based corpus browsing and understanding.
Visualizations of this type typically represent stan-
dard topic models as a sea of word clouds; the in-
dividual topics within the model are presented as
an unordered set of word clouds ? or something
similar ? of the top words for the topic
1
where
word size is proportional to the probability of the
word for the topic. A primary issue with word
clouds is that they can hinder understanding (Har-
ris, 2011) due to the fact that they lack information
about the relationships between words. Addition-
ally, topic model visualizations that display topics
in a random layout can lead to a huge, inefficiently
organized search space, which is not always help-
ful in providing a quick corpus overview or assist-
ing the user to diagnose possible problems with
the model.
The authors of Correlated Topic Models (CTM)
(Lafferty and Blei, 2006) recognize the limitation
of existing topic models to directly model the cor-
relation between topics, and present an alterna-
tive algorithm, CTM, which models the correla-
tion between topics discovered for a corpus by us-
ing a more flexible distribution for the topic pro-
portions in the model. Topical n-gram models
(TNG) (Wang et al., 2007) discover phrases in
addition to topics. TNG is a probabilistic model
which assigns words and n-grams based on sur-
rounding context, instead of for all references in
the corpus. These models independently account
for the two limitations of statistical topic modeling
discussed in this paper by modifying the underly-
ing topic modeling algorithm. Our work aims to
provide a low-cost method for incorporating this
1
This varies, but typically is either the top 10 to 20 words
or the number of words which hold a specific portion of the
distribution weight.
79
information as well as visualizing it in an effec-
tive way. We compute summary statistics, term
co-occurrence and topic covariance, which can be
overlaid on top of any traditional topic model. As
a number of application-specific LDA implemen-
tations exist, we propose a meta-technique which
can be applied to any underlying algorithm.
We present a relationship-enriched visualiza-
tion to help users explore topic models through
word and topic correlations. We propose inter-
actions to support user understanding, validation,
and refinement of the models.
2 Group-in-a-box Layout for Visualizing
a Relationship-Enriched Topic Model
Existing topic model visualizations do not eas-
ily support displaying the relationships between
words in the topics and topics in the model. In-
stead, this requires a layout that supports intuitive
visualization of nested network graphs. A group-
in-a-box (GIB) layout (Rodrigues et al., 2011) is a
network graph visualization that is ideal for our
scenario as it is typically used for representing
clusters with emphasis on the edges within and
between clusters. The GIB layout visualizes sub-
graphs within a graph using a Treemap (Shneider-
man, 1998) space filling technique and layout al-
gorithms for optimizing the layout of sub-graphs
within the space, such that related sub-graphs are
placed together spatially. Figure 1 shows a sample
group-in-a-box visualization.
We use the GIB layout to visually separate top-
ics of the model as groups. We implement each
topic as a force-directed network graph (Fruchter-
man and Reingold, 1991) where the nodes of the
graph are the top words of the topic. An edge ex-
ists between two words in the network graph if
the value of the term co-occurrence for the word
pair is above a certain threshold,
2
and the edge is
weighted by this value. Similarly, the edges be-
tween the topic clusters represent the topic covari-
ance metric. Finally, the GIB layout optimizes the
visualization such that related topic clusters are
placed together spatially. The result is a topic visu-
alization where related words are clustered within
the topics and related topics are clustered within
the overall layout.
2
There are a variety of techniques for setting this thresh-
old; currently, we aim to display fewer, stronger relationships
to balance informativeness and complexity of the visualiza-
tion
Figure 1: A sample GIB layout from (Rodrigues
et al., 2011). The layout visualizes clusters dis-
tributed in a treemap structure where the partitions
are based on the size of the clusters.
3 Relationship Metrics
We compute the term and topic relationship in-
formation required by the GIB layout as term
co-occurrence and topic covariance, respectively.
Term co-occurrence is a corpus-level statistic that
can be computed independently from the LDA al-
gorithm. The results of the LDA algorithm are re-
quired to compute the topic covariance.
3.1 Corpus-Level Term Co-Occurrence
Prior work has shown that Pointwise Mutual
Information (PMI) is the most consistent scor-
ing method for evaluating topic model coher-
ence (Newman et al., 2010). PMI is a statistical
technique for measuring the association between
two observations. For our purposes, PMI is used
to measure the correlation between each term pair
within each topic on the document level
3
. The
PMI is calculated for every possible term pair in
the ingested data set using Equation 1. The visu-
alization uses only the PMI for the term pairs for
the top terms for each topic, which is a small sub-
set of the calculated PMI values. Computing the
PMI is trivial compared to the LDA calculation,
and computing the values for all pairs allows the
job to be run in parallel, as opposed to waiting for
the results of the LDA job to determine the top
term pairs.
PMI(x, y) = log
p(x, y)
p(x)p(y)
(1)
The PMI measure represents the probability of
observing x given y and vice-versa. PMI can be
3
We use document here, but the PMI can be computed at
various levels of granularity as required by the analyst intent.
80
positive or negative, where 0 represents indepen-
dence, and PMI is at its maximum when x and y
are perfectly associated.
3.2 Topic Covariance
To quantify the relationship between topics in the
model, we calculate the topic covariance metric
for each pair of topics. To do this, we use the
theta vector from the LDA output. The theta vec-
tor describes which topics are used for which doc-
uments in the model, where theta(d,i) represents
how much the ith topic is expressed in document
d. The equations for calculation the topic covari-
ance are shown below.
?
d
i
=
?
d
i
?
j
(?
d
j
)
(2)
?
i
=
1
D
?
d
(?
d
i
) (3)
?(i, j) =
1
D
?
d
(?
d
i
? ?
i
)(?
d
j
? ?
j
)) (4)
4 Visualization
The visualization represents the individual topics
as network graphs where nodes represent terms
and edges represent frequent term co-occurrence,
and the layout of the topics represents topic co-
variance. The most connected topic is placed in
the center of the layout, and the least connected
topics are placed at the corners. Figure 2 shows
the visualization for a topic model generated for
a 1,000 document NSF dataset. As demonstrated
in Figure 3, a user can hover over a topic to see
the related topics
4
. In this example, the user has
hovered over the {visualization, visual, interac-
tive} topic, which is related to {user, interfaces},
{human, computer, interaction}, {design, tools},
and {digital, data, web} among others. Unlike
other topical similarity measures, such as cosine
similarity or a count of shared words, the topic co-
variance represents topics which are typically dis-
cussed together in the same documents, helping
the user to discover semantically similar topics.
On the topic level, the size of the node in the
topic network graph represents the probability of
the word given the topic. By mapping word proba-
bility to the area of the nodes instead of the height
4
we consider topics related if the topic co-occurrence is
above a certain pre-defined threshold.
Figure 2: The visualization utilizes a group-in-a-
box-inspired layout to represent the topic model as
a nested network graph.
of words, the resulting visual encoding is not af-
fected by the length of the words, a well-known
issue with word cloud presentations that can visu-
ally bias longer terms. Furthermore, circles can
overlap without affecting a user?s ability to visu-
ally separate them, and lead to more compact and
less cluttered visual layout. Hovering over a word
node highlights the same word in other topics as
shown in Figure 4.
This visualization is an alternative interface
for Interactive Topic Modeling (ITM) (Hu et al.,
2013). ITM presents users with topics that can be
modified as appropriate. Our preliminary results
show that topics containing highly-weighted sub-
clusters may be candidates for splitting, whereas
positively correlated topics are likely to be good
topics, which do not need to be modified. In fu-
ture work, we intend to perform an evaluation to
show that this visualization enhances quality and
efficiency of the ITM process.
To support user interactions required by the
ITM algorithm, the visualization has an edit mode,
which is shown in Figure 5. Ongoing work in-
cludes developing appropriate visual operations to
support the following model-editing operations:
1. Adding words to a topic
2. Removing words from a topic
3. Requiring two words to be linked within a
topic (must link)
4. Requiring two words to be forced into sepa-
rate topics (cannot link)
5 Conclusion and Future Work
The visualization presented here provides a novel
way to explore topic models with incorporated
81
Figure 3: The user has hovered over the most-
central topic in the layout, which is the most con-
nected topic. The hovered topic is outlined, and
the topic name is highlighted in turquoise. The
topic names of the related topics are also high-
lighted.
Figure 4: The visualization where the user has
hovered over a word of interest. The same word
is highlighted turquoise in other topics.
Figure 5: The edit mode for the visualization.
From this mode, the user can add words, remove
words, or rename the topic.
term and topic correlation information. This is a
work in progress with the end goal to combine the
visual representation with interactive topic mod-
eling to allow users to explore (a) why a model
may not align with their intuition and (b) modify
the model as needed. We plan to deploy the tool
on real-world domain users to iteratively refine the
visualization and evaluate it in ecologically valid
settings.
References
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet
allocation. Machine Learning Journal, 3:993?1022.
Allison June-Barlow Chaney and David M Blei. 2012. Visualizing topic mod-
els. In ICWSM.
Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, and Eric Xing. 2012. Top-
icviz: interactive topic exploration in document collections. In CHI?12
Extended Abstracts, pages 2177?2182. ACM.
Thomas MJ Fruchterman and Edward M Reingold. 1991. Graph draw-
ing by force-directed placement. Software: Practice and experience,
21(11):1129?1164.
Matthew J Gardner, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric
Ringger, and Kevin Seppi. 2010. The topic browser: An interactive tool
for browsing topic models. In NIPS Workshop on Challenges of Data Vi-
sualization.
Jacon Harris. 2011. Word clouds considered harm-
ful. http://www.niemanlab.org/2011/10/
word-clouds-considered-harmful/.
Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith. 2013.
Interactive topic modeling. Machine Learning, pages 1?47.
JD Lafferty and MD Blei. 2006. Correlated topic models. In NIPS, Proceed-
ings of the 2005 conference, pages 147?155. Citeseer.
David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Au-
tomatic evaluation of topic coherence. In HLT, pages 100?108. ACL.
Eduarda Mendes Rodrigues, Natasa Milic-Frayling, Marc Smith, Ben Shnei-
derman, and Derek Hansen. 2011. Group-in-a-box layout for multi-
faceted analysis of communities. In ICSM, pages 354?361. IEEE.
Ben Shneiderman. 1998. Treemaps for space-constrained visualization of hi-
erarchies.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams:
Phrase and topic discovery, with an application to information retrieval. In
ICDM, pages 697?702. IEEE.
82
