Automatic Evaluation of Summaries Using Document Graphs 
 
Eugene Santos Jr., Ahmed A. Mohamed, and Qunhua Zhao 
Computer Science and Engineering Department 
University of Connecticut 
191 Auditorium Road, U-155, Storrs, CT 06269-3155 
{eugene, amohamed, qzhao}@engr.uconn.edu 
 
 
Abstract 
Summarization evaluation has been always a chal-
lenge to researchers in the document summariza-
tion field. Usually, human involvement is 
necessary to evaluate the quality of a summary. 
Here we present a new method for automatic  
evaluation of text summaries by using document 
graphs. Data from Document Understanding Con-
ference 2002 (DUC-2002) has been used in the 
experiment. We propose measuring the similarity 
between two summaries or between a summary 
and a document based on the concepts/entities and 
relations between them in the text.  
1 Introduction 
Document summarization has been the focus of 
many researchers for the last decade, due to the 
increase in on-line information and the need to 
find the most important information in a (set of) 
document(s). One of the biggest challenges in text 
summarization research is how to evaluate the 
quality of a summary or the performance of a 
summarization tool. There are different approa-
ches to evaluate overall quality of a summariza-
tion system. In general, there are two types of 
evaluation categories: intrinsic and extrinsic 
(Sparck-Jones and Galliers, 1996). Extrinsic ap-
proaches measure the quality of a summary based 
on how it affects certain tasks. In intrinsic approa-
ches, the quality of the summarization is evaluated 
based on analysis of the content of a summary 
itself. In both categories human involvement is 
used to judge the summarization outputs. The 
problem with having humans involved in evalua-
ting summaries is that we can not hire human jud-
ges every time we want to evaluate summaries 
(Mani and Maybury, 1999). In this paper, we dis-
cuss a new automated way to evaluate machine-
generated summaries without the need to have 
human judges being involved which decreases the 
cost of determining which summarization system 
is best. In our experiment, we used data from Do-
cument Understanding Conference 2002 (DUC-
2002). 
2 Related Work 
Researchers in the field of document summariza-
tion have been trying for many years to define a 
metric for evaluating the qua lity of a machine-
generated summary. Most of these attempts invol-
ve human interference, which make the process of 
evaluation expensive and time-consuming. We 
discuss some important work in the intrinsic cate-
gory. 
2.1 Sentence Precision-Recall Measure 
Sentence precision and recall have been widely 
used to evaluate the quality of a summarizer (Jing 
et al, 1998). Sentence precision measures the per-
cent of the summary that contains sentences mat-
ched with the model summary. Recall, on the 
other hand, measures the percent of sentences in 
the ideal summary that have been recalled in the 
summary. Even though sentence precision/recall 
factors can give us an idea about a summary?s 
quality, they are not the best metrics to evaluate a 
system?s quality. This is due to the fact that a 
small change in the output summary can dramati-
cally affect the quality of a summary (Jing et al, 
1998). For example, it is possible that a system 
will pick a sentence that does not match with a 
model sentence chosen by an assessor, but is 
equivalent to it in meaning. This, of course, will 
affect the score assigned to the system dramatica-
lly. It is also obvious that sentence precision/recall 
is only applicable to the summaries that are gene-
rated by sentence extraction, not abstraction (Ma-
ni, 2001).  
2.2 Content-Based Measure 
Content-based measure computes the similarity at 
the vocabulary level (Donaway, 2000 and Mani, 
2001). The evaluation is done by creating term 
frequency vectors for both the summary and the 
model summary, and measuring the cosine simila-
rity (Salton, 1988) between these two vectors. Of 
course, the higher the cosine similarity measure, 
the higher the quality of the summary is. Lin and 
Hovy (2002) used accumulative n-gram matching 
scores between model summaries and the summa-
ries to be evaluated as a performance indicator in 
multi-document summaries. They achieved their 
best results by giving more credit to longer n-
gram matches with the use of Porter stemmer.  
A problem raised in the evaluation approaches 
that use the cosine measure is that the summaries 
may use different key terms than those in the ori-
ginal documents or model summaries. Since term 
frequency is the base to score summaries, it is 
possible that a high quality summary will get a 
lower score if the terms used in the summary are 
not the same terms used in most of the document?s 
text. Donaway et al (2000) discussed using a 
common tool in information retrieval: latent se-
mantic indexing (LSI) (Deerwester et al, 1990) to 
address this problem. The use of LSI reduces the 
effect of near-synonymy problem on the similarity 
score. This is done by penalizing the summary 
less in the reduced dimension model when there 
are infrequent terms synonymous to frequent 
terms. LSI averages the weights of terms that co-
occur frequently with other mutual terms. For 
example, both ?bank? and ?financial institution? 
often occur with the term ?account? (Deerwester 
et al, 1990). Even though using LSI can be useful 
in some cases, it can produce unexpected results 
when the document contains terms that are not 
synonymous to each other, but, however, they co-
occur with other mutual terms. 
 
 
 
 
 
 
2.3 Document Graph 
2.3.1 Representing Content by Document 
Graph 
Current approaches in content-based summariza-
tion evaluation ignore the relations between the 
keywords that are expressed in the document. 
Here, we introduce our approach, which measures 
the similarity between two summaries or a sum-
mary and a document based on the relations (bet-
ween the keywords). In our approach, each 
document/summary is represented as a document 
graph (DG), which is a directed graph of con-
cepts/entities and the relations between them. A 
DG contains two kinds of nodes, concept/entity 
nodes and relation nodes. Currently, only two 
kinds of relations, ?isa? and ?related to?, are cap-
tured (Santos et al 2001) for simplicity. 
To generate a DG, a document/summary in 
plain text format is first tokenized into sentences; 
and then, each sentence is parsed using Link Par-
ser (Sleator and Temperley, 1993), and the noun 
phrases (NP) are extracted from the parsing re-
sults. The relations are generated based on three 
heuristic rules: 
? The NP-heuristic helps to set up the hierar-
chical relations. For example, from a noun 
phrase ?folk hero stature?, we generate re-
lations ?folk hero stature isa stature?, ?folk 
hero stature related to  folk hero?, and ?folk 
hero isa hero?. 
? The NP-PP-heuristic attaches all preposi-
tional phrases to adjacent noun phrases. For 
example, from ?workers at a coal mine?, 
we generate a relation, ?worker related to 
coal mine?. 
? The sentence-heuristic rela tes con-
cepts/entities contained in one sentence. 
The relations created by sentence-heuristic 
are then sensitive to verbs, since the inter-
val between two noun phrases usually con-
tains a verb. For example, from a sentence 
?Workers at a coal mine went on strike?, 
we generate a relation ?worker related to 
strike?.  Another example, from ?The usual 
cause of heart attacks is a blockage of the 
coronary arteries?, we generate ?heart at-
tack cause related to  coronary artery bloc-
kage?. Figure 1 shows a  example of a 
partial DG. 
 
 
 
Figure 1: A partial DG. 
 
 
2.3.2 Similarity Comparison between two 
Document Graphs 
The similarity of DG1 to DG2 is given by the 
equation:  
 
M
m
N
nDGDGSim
22
),( 21 +=  
 
which is modified from Montes-y-G?mez et al 
(2000). N is the number of concept/entity nodes in 
DG1, and M stands for number of relations in 
DG1; n is the number of matched concept/entity 
nodes in two DGs, and m is the number of mat-
ched relations. We say we find a matched relation 
in two different DGs, only when both of the two 
concept/entity nodes linked to the relation node 
are matched, and the relation node is also mat-
ched. Since we might compare two DGs that are 
significantly different in size (for example, DGs 
for an extract vs. its source document), we used 
the number of concept/entity nodes and relation 
nodes in the target DG as N and M, instead of the 
total number of nodes in both DGs. The target DG 
is the one for the extract in comparing an extract 
with its source text. Otherwise, the similarity will 
always be very low. Currently, we weight all the 
concepts/entities and relations equally. This can 
be fine tuned in the future.  
3 Data, and Experimental Design 
3.1 Data 
Because the data from DUC-2003 were short 
(~100 words per extract for multi-document task), 
we chose to use multi-document extracts from 
DUC-2002 (~200 words and ~400 words per ex-
tract for multi-document task) in our experiment. 
In this corpus, each of ten information analysts 
from the National Institute of Standards and 
Technology (NIST) chose one set of newswi-
re/paper articles in the following topics (Over and 
Liggett, 2002): 
? A single natural disaster event with docu-
ments created within at most a 7-day win-
dow 
? A single event of any type with documents 
created within at most a 7-day window  
? Multiple distinct events of the same type (no 
time limit) 
? Biographical (discuss a single person) 
Each assessor chose 2 more sets of articles so 
that we ended up with a total of 15 document sets 
of each type. Each set contains about 10 docu-
ments. All documents in a set are mainly about a 
specific ?concept.? 
A total of ten automatic summarizers participa-
ted to produce machine-generated summaries. 
Two extracts of different lengths, 200 and 400 
words, have been generated for each document-
set. 
3.2 Experimental Design 
A total of 10 different automatic summarization 
systems submitted their summaries to DUC. We 
obtained a ranking order of these 10 systems ba-
sed on sentence precision/recall by comparing the 
machine generated extracts to the human genera-
ted model summaries. The F-factor is calculated 
from the following equation (Rijsbergen, 1979): 
)(
2
RP
RPF
+
??=   
where P is the precision and R is the recall. We 
think this ranking order gives us some idea on 
how human judges think about the performance of 
different systems.  
For our evaluation based on DGs, we also 
calculated F-factors based on precision and recall, 
where P = Sim(DG1, DG2) and R = Sim(DG2, 
DG1). In the first experiment, we ranked the 10 
automatic summarization systems by comparing 
DGs generated from their outputs to the DGs gen-
erated from model summaries. In this case, DG1 is 
the machine generated extract and DG2 is the hu-
man generated extract. In the second experiment, 
we ranked the systems by comparing machine 
generated extracts to the original documents. In 
this case, DG1 is an extract and DG2 is the corre-
sponding  original document. Since the extracts 
were  generated  from multi-document  sets,  we  
  
 
 
used the average of the F-factors for ranking pur-
poses. 
4  Results 
The ranking orders obtained based on sentence 
precisions and recalls are shown in Tables 1 and 
2.  The results indicate that for sentence precision 
and recall, the ranking order for different summa-
rization systems is not affected by the summariza-
tion compression ratio.  The ranking results for 
200-word extracts and 400-word extracts are ex-
actly the same. 
Since the comparison is between the machine 
generated extracts and the human created model 
extracts, we believe that the rankings should rep-
resent the performance of 10 different automated 
summarization systems, to some degree. The ex-
periments using DGs instead of sentence matching 
give two very similar ranking orders (Spearman 
rank correlation coefficient [Myers and Well, 
1995] is 0.988) where only systems 24 and 19 are 
reversed in their ranks (Tables 1 and 2). The re-
sults show that when the evaluation is based on 
the comparison between machine generated ex-
tracts and the model extracts, our DG-based 
evaluation approach will provide roughly the 
same ranking results as the sentence precision and 
recall approach. Notice that the F-factors obtained  
 
 
 
 
by experiments using DGs are higher than those 
calculated based on sentence matching. This is 
because our DG-based evaluation approach com-
pares the two extracts at a more fine grained level 
than sentence matching does since we compare 
the similarity at the level of concepts/entities and 
their relations, not just whole sentences. The simi-
larity of the two extracts should actually be higher 
than the score obtained with sentence matching 
because there are sentences that are equivalent in 
meaning but not syntactically identical. 
Since we believe that the DGs captures the se-
mantic information content contained in the res-
pective documents, we rank the automatic 
summarization systems by comparing the DGs of 
their extract outputs against the DGs of the orig i-
nal documents. This approach does not need the 
model summaries, and hence no human involve-
ment is needed in the evaluation. The results are 
shown in Tables 3 and 4. As we can see, our ran-
kings are different from the ranking results based 
on comparison against the model extracts.  System 
28 has the largest change in rank in both 200-word 
and 400-word summaries.  It was ranked as the 
worst by our DG based approach instead of num-
ber 7 (10 is the best) by the approaches comparing 
to the model extracts.  We investigated the extract 
content of system 28 and found that many extracts 
System 
rank 
Sentence-
based 
Ranking 
Sentence-
based 
F-factor 
DG-
based 
Ranking 
DG-
based 
F-factor 
 
System 
rank 
Sentence-
based 
Ranking 
Sentence-
based 
F-factor 
DG-
based 
Ranking 
DG-
based 
F-factor 
1 
(worst) 22 0.000 22 0.122 
 1 
(worst) 22 0.000 22 0.181 
2 16 0.062 16 0.167  2 16 0.128 16 0.235 
3 31 0.081 31 0.180  3 25 0.147 25 0.256 
4 25 0.081 25 0.188  4 31 0.150 31 0.266 
5 29 0.090 29 0.200  5 29 0.155 20 0.273 
6 20 0.125 20 0.226  6 20 0.172 29 0.279 
7 28 0.138 28 0.255  7 28 0.197 28 0.316 
8 24 0.171 19 0.283  8 24 0.223 19 0.337 
9 19 0.184 24 0.283  9 19 0.224 24 0.355 
10 
(best) 21 0.188 21 0.308 
 10 
(best) 21 0.258 21 0.372 
Table 1: Model Summaries vs. machine-
generated summaries. Ranking results for 200 
words extracts 
 Table 2: Model Summaries vs. machine-
generated summaries. Ranking results for 400 
words extracts 
generated by system 28 included sentences that 
contain little information, e.g., author?s names, 
publishers, date of publication, etc. The following 
are sample extracts produced for document 120 by 
systems 28, 29 (the best ranked) and a human jud-
ge, at 200-words. 
 
[Extract for Document 120 by System 28]  
John Major, endorsed by Margaret Thatcher as 
the politician closest to her heart, was elected by 
the Conservative Party Tuesday night to succeed 
her as prime minister. 
Hong Kong WEN WEI PO 
By MICHAEL CASSELL and IVOR OWEN 
By MICHAEL THOMPSON-NOEL 
By DOMINIC LAWSON 
From Times Wire Services 
By WILLIAM TUOHY, TIMES STAFF WRITER 
From Associated Press 
 
[Extract for Document 120 by System 29] 
John Major, endorsed by Margaret Thatcher as 
the politician closest to her heart, was elected by 
the Conservative Party Tuesday night to succeed 
her as prime minister. 
Aides said Thatcher is "thrilled". 
Hurd also quickly conceded. 
ONE year ago tomorrow, Mr John Major surpri-
sed everyone but himself by winning the general 
election. 
It has even been suggested that the recording of 
the prime minister's conversation with Michael 
Brunson, ITN's political editor, in which Major 
used a variety of four-, six- and eight-letter words 
to communicate his lack of fondness for certain 
colleagues, may do him good. 
BFN 
[Colin Brown article: "Cabinet Allies Close Ranks 
But Bring 
Right-wing MPs confirmed the findings in an 
INDEPENDENT ON SUNDAY/NOP [National 
Opinion Poll] poll that Michael Heseltine was the 
favourite to replace Mr Major, if he is forced out. 
The Labour Party controls 90 local councils, whe-
reas the Conservatives only control 13, with a 
sharp contrast in strength between the two sides. 
If he did not see the similarity, that is still more 
revealing. 
 
[Extract for Document 120 by a human judge -- 
model extract] 
John Major, endorsed by Margaret Thatcher as 
the politician closest to her heart, was elected by 
the Conservative Party Tuesday night to succeed 
her as prime minister. 
While adopting a gentler tone on the contentious 
issue of Britain's involvement in Europe, he shares 
her opposition to a single European currency and 
shares her belief in tight restraint on government 
spending. 
FT 08 APR 93 / John Major's Year: Major's blue 
period - A year on from success at the polls, the 
prime minister's popularity has plunged. 
The past 12 months have been hijacked by inter-
nal party differences over Europe, by the debacle 
surrounding UK withdrawal from the exchange 
rates mechanism of the European Monetary Sys-
tem, and by a continuing, deep recession which 
has disappointed and alienated many traditional 
Tory supporters in business. 
Its Leader"] [Text] In local government elections 
across Britain yesterday, the Conservatives suffe-
red their worst defeat ever, losing control of 17 
regional councils and 444 seats. 
Even before all of the results were known, some 
Tories openly announced their determination to 
challenge John Major's position and remove him 
from office as early as possible. 
 
The extract generated by system 28 has 8 sen-
tences of which only one of them contained rele-
vant information. When comparing using sentence 
precision and recall, all three extracts only have 
one sentence match which is the first sentence. If 
we calculate the F-factors based on the model ex-
tract shown above, system 28 has a score of 0.143 
and system 29 has a lower score of 0.118.  After 
reading all three extracts, the extract generated by 
system 29 contains much more relevant informa-
tion than that generated by system 28. The mis-
sing information in system 28 is ---John Major 
and the Conservatives were losing the popularity 
in 1993, after John Major won the election one 
year ago,-- which should be the most important 
content in the extract. In our DG-based approach, 
the scores assigned to system 28 and 29 are 0.063 
and 0.100, respectively; which points out that sys-
tems 29 did a better job than system 28. 
  
200-word 400-word 
System F-factor System F-factor 
28 0.092 22 0.137 
22 0.101 28 0.141 
16 0.111 16 0.160 
20 0.115 25 0.163 
25 0.115 20 0.164 
21 0.122 31 0.165 
Model 0.124 Model 0.165 
31 0.124 21 0.167 
24 0.125 29 0.168 
19 0.129 19 0.168 
29 0.132 24 0.169 
Table 5: Average F-factors for the model sum-
maries and machine-generated summaries. 
Of the 59 submitted 200-word extracts by sys-
tem 28, 39 extracts suffer the problem of having 
less informative sentences. The number of such 
sentences is 103, where the total number of sen-
tences is 406 from all the extracts for system 28. 
On average, each extract contains 1.75 such sen-
tences, where each extract has 6.88 sentences. For 
the 400-words extracts, we found 54 extracts 
among the 59 submitted summaries also have this 
problem. The total number of such sentences  was 
206, and the total number of sentences was 802 
sentences. So,  about 3.49 sentences do not con-
tain much information, where the average length 
of each extract is 13.59 sentences. Thus, a large  
 
 
 
portion of each extract does not contribute to the 
do example, will not be considered a good sum-
mary, either on the criterion of summary coheren-
ce or summary informativeness, where coherence 
is how the summary reads and informativeness is 
how much information from the source is preser-
ved in the summary (Mani, 2001). 
From the results based on comparing extracts 
against original documents, we found that several 
systems perform very similarly, especially in the 
experiments with 400-word extracts (Table 4). 
The results show that except for systems 22 and 
28 which perform significantly worse, all other 
systems are very similar, from the point of view of 
informativeness. 
Finally, we generated DGs for the model extra-
cts and then compared them against their original 
documents. The average F-factors are calculated, 
which are listed in Table 5 along with the scores 
for different automatic summarization systems. 
Intuitively, a system provides extracts that contain 
more information than other systems will get a 
higher score. As we can see from the data, at 200-
words, the extracts generated by systems 21, 31, 
24, 19, and 29 contain roughly the same amount 
of information as those created by humans, while 
the other five systems performed worse than 
human judges. At 400-words, when the compres-
sion ratio of the extracts is decreased, more sys-
tems perform well; only systems 22 and 28 
System 
rank 
Sentence-
based 
Ranking 
Sentence-
based 
F-factor 
DG-
based 
Ranking 
DG-
based 
F-factor 
 System 
rank 
Sentence-
based 
Ranking 
Sentence-
based 
F-factor 
DG-
based 
Ranking 
DG-
based 
F-factor 
1 
(worst) 22 0.000 28 0.092 
 1 
(worst) 22 0.000 22 0.137 
2 16 0.062 22 0.101  2 16 0.128 28 0.141 
3 31 0.081 16 0.111  3 25 0.147 16 0.160 
4 25 0.081 20 0.115  4 31 0.150 25 0.163 
5 29 0.090 25 0.115  5 29 0.155 20 0.164 
6 20 0.125 21 0.122  6 20 0.172 31 0.165 
7 28 0.138 31 0.124  7 28 0.197 21 0.167 
8 24 0.171 24 0.125  8 24 0.223 29 0.168 
9 19 0.184 19 0.129  9 19 0.224 19 0.168 
10 
(best) 21 0.188 29 0.132 
 10 
(best) 21 0.258 24 0.169 
Table 3: Machine-generated summaries vs. 
source documents.  Ranking results for 200 
words extracts 
 Table 4: Machine-generated summaries vs. 
source documents. Ranking results for 400 
words extracts 
generated summaries that contain much less in-
formation than the model summaries. 
5 Discussion and Future Work 
In DUC 2002 data collection, 9 human judges 
were involved in creating model extracts; how-
ever, there are only 2 model extracts generated for 
each document set. The sentence precisions and 
recalls obtained from comparing the machine gen-
erated extracts and human generated model ex-
tracts are distributed along with raw data (DUC-
2002. http://www-nlpir.nist.gov/projects/duc), 
with the intent to use them in system performance 
comparison. Van Halteren (2002) argued that only 
two manually created extracts could not be used to 
form a sufficient basis for a good benchmark. To 
explore this issue, we obtained a ranking order for 
each human judge based on the extracts he/she 
generated.  The results showed that the ranking 
orders obtained from 9 different judges are actu-
ally similar to each other, with the average 
Spearman correlation efficient to be 0.901. From 
this point of view, if the ranking orders obtained 
by sentence precision and recall based on the 
model extracts could not form a good basis for a 
benchmark, it is because of its binary nature (Jing 
et al, 1998), not the lack of sufficient model ex-
tracts in DUC 2002 data.  
Van Halteren and Teufel (2003) proposed to 
evaluate summaries via factoids, a pseudo-
semantic representation based on atomic informa-
tion units. However, sufficient manually created 
model summaries are need; and factoids are also 
manually annotated. Donaway et al (2000) sug-
gested that it might be possible to use content-
based measures for summarization evaluation wit-
hout generating model summaries.  Here, we pre-
sented our approach to evaluate the summaries 
base on document graphs, which is generated au-
tomatically. It is not very surprising that different 
measures rank summaries differently. A similar 
observation has been reported previously (Radev, 
et al 2003). Our document graph approach on 
summarization evaluation is a new automatic way 
to evaluate machine-generated summaries, which 
measures the summaries from the point of view of 
informativeness. It has the potential to evaluate 
the quality of summaries, including extracts, abs-
tracts, and multi-document summaries, without 
human involvement. To improve the performance 
of our system and better represent the content of 
the summaries and source documents, we are 
working in several areas: 1) Improve the results of 
natural language processing to capture informa-
tion more accurately; 2) Incorporate a knowledge 
base, such as WordNet (Fellbaum, 1998), to ad-
dress the synonymy problem; and, 3) Use more 
heuristics in our relation extraction and genera-
tion. We are also going to extend our experiments 
by comparing our approach to content-based mea-
sure approaches, such as cosine similarity based 
on term frequencies and LSI approaches, in both 
extracts and abstracts. 
6 Acknowledgments 
This work was supported in part by the Advanced 
Research and Development Activity (ARDA) U.S. 
Government. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily 
reflect the views of the U. S. Government. 
References  
Scott Deerwester, Susan T. Dumais, George W. 
Furnas, Thomas K. Landauer, and Richard 
Harshman. 1990. Indexing by latent semantic 
analysis. Journal of the American Society for In-
formation Science, 41(6): 391-407. 
Robert L. Donaway, Kevin W. Drummey, and 
Laura A. Mather. 2000. A comparison of rank-
ings produced by summarization evaluation 
measures. In Proceedings of the Workshop on 
Automatic Summarization, pages 69-78. 
Christiane Fellbaum. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press. 
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, 
and Jaime Carbonell. 1999. Summarizing text 
documents: Sentence selection and evaluation 
metrics. In Proceedings the 24th Annual Interna-
tional ACM SIGIR Conference on Research and 
Development in Information Retrieval, pages 
121-128, ACM, New York.. 
Hans van Haltern. 2002. Writing style recognition 
and sentence extraction. DUC?02 Conference 
Proceedings. 
Hans Van Halteren and Simone Teufel, 2003. Ex-
amining the consensus between human summa-
ries: Initial experiments with factoid analysis. In 
HLT/NAACL-2003 Workshop on Automatic 
Summarization. 
Hongyan Jing, Kathleen McKeown, Regina Barzi-
lay, and Michael Elhadad. 1998. Summarization 
evaluation methods: experiments and analysis. 
In American Association for Artificial Intelli-
gence Spring Symposium Series, pages 60-68. 
Chen-Yew Lin. 2001. Summary evaluation 
environment. http://www.isi.edu/~cyl/SEE. 
Chin-Yew Lin and Eduard Hovy. 2002. Manual 
and automatic evaluation of summaries. In Pro-
ceedings of the Workshop on Automatic Summa-
rization post conference workshop of ACL-02, 
Philadelphia, PA, U.S.A., July 11-12 (DUC 
2002). 
Inderjeet Mani. 2001. Summarization evaluation: 
An overview. In Proceedings of the NTCIR 
Workshop 2 Meeting on Evaluation of Chinese 
and Japanese Text Retrieval and Test Summari-
zation, National Institute of Informatics. 
Inderjeet Mani and Mark T. Maybury. 1999. Ad-
vances in Automatic Text Summarization. The 
MIT Press. 
Manuel Montes-y-G?mez, Alexander Gelbukh, 
and Aurelio L?pez-L?pez. 2000. Comparison of 
conceptual graphs. In Proceedings of MICAI-
2000, 1st Mexican International Conference on 
Artificial Intelligence, Acapulco, Mexico. 
Jerome L. Myers and Arnold D. Well. 1995. Re-
search Design and Statistical Analysis, pages, 
488-490, Lawrence Erlbaum Associates, New 
Jersey  
Paul Over and Walter Liggett. 2002. Introduction 
to DUC-2002: An intrinsic evaluation of generic 
news text summarization systems. Document 
Understanding Conferences website 
(http://duc.nist.gov/) 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2001. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. 
Technical Report RC22176 (W0109-022), IBM 
Research Division, Thomas J. Watson Research 
Center. 
Dragomir R. Radev, Simone Teufel, Horacio Sag-
gion, Wai Lam, John Blitzer, Hong Qi, Arda 
Celebi, Danyu Liu, and Elliott Drabek. 2003. 
Evaluation challenges in large-scale document 
summarization. In Proceedings of the 41st An-
nual Meetring of the Association for Computa-
tional Linguistics, July 2003, pages 375-382. 
Keith van Rijsbergen. 1979. Information Re-
trieval. Second Edition Butterworths, London. 
Gerard Salton. 1988. Automatic Text Processing. 
Addison-Wesley Publishing Company. 
Eugene Santos Jr., Hien Nguyen, and Scott M. 
Brown. 2001. Kavanah: An active user interface 
Information Retrieval Agent Technology. Mae-
bashi, Japan, October 2001, pages 412-423. 
Danny Sleator and Davy Temperley. 1993. Pars-
ing English with a link grammar. In Proceed-
ings of the Third International Workshop on 
Parsing Technologies, pages 277-292. 
Karen Sparck-Jones and Julia R. Galliers. 1996. 
Evaluating Natural Language Processing Sys-
tems: An Analysis and Review (Lecture Notes in 
Artificial Inte lligence 1083). Springer-Verlag 
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 63?71,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Argument Formation in the Reasoning Process: Toward a Generic 
Model of Deception Detection 
 
 
Deqing Li and Eugene Santos, Jr. 
Thayer School of Engineering 
Dartmouth College 
Hanover, N.H., U.S.A 
{ Deqing.Li, Eugene.Santos.Jr }@Dartmouth.edu 
 
 
 
 
Abstract 
Research on deception detection has been 
mainly focused on two kinds of 
approaches. In one, people consider 
deception types and taxonomies, and use 
different counter strategies to detect and 
reverse deception. In the other, people 
search for verbal and non-verbal cues in 
the content of deceptive communication. 
However, general theories that study 
fundamental properties of deception 
which can be applied in computational 
models are still very rare. In this work, 
we propose a general model of deception 
detection guided by a fundamental 
principle in the formation of 
communicative deception. Experimental 
results using our model demonstrate that 
deception is distinguishable from 
unintentional misinformation. 
Introduction 
Conventional research on deception detection 
focuses on deception taxonomies and deception 
cues. Unfortunately, both of them neglect the fact 
that deception is rooted in the formation of 
arguments mainly because such formation is not 
directly observable. However, since the 
formation of arguments is where the 
implementation of deception starts, it is 
necessary to study it in depth.  
The act of deceiving involves two processes: 
the formation of deceptive arguments (the 
reasoning) and the communication of deception. 
The communication part is intuitive to 
understand and has been the focus of recent 
research efforts in deception detection. The 
reasoning part is a necessary component of 
deception because deceiving has been found to 
require a heavier cognitive load than telling the 
truth (Greene et. Al, 1985). The reasoning process 
involves generating and selecting arguments 
while the communication process involves 
wording and phrasing of the arguments. 
Deception detection in the process of 
communication is not ideal because firstly, it is 
easy to hide deceptive cues using careful 
wording and phrasing, and secondly, wording 
and phrasing of communication are mediated by 
the framing of the other party?s response (e.g. the 
answer to the question ?Did you go to class 
today?? always starts with ?Yes, I? or ?No, I?). 
On the other hand, it is hard to hide the intent of 
deception by distorting arguments formed in the 
reasoning process because it requires higher-
order deception that takes the other party?s intent 
and even the other party?s belief about the 
speaker?s intent into consideration. Higher-order 
deception demands much more cognitive load 
than first-order deception in order to retrieve the 
memory about the other party?s intent and 
leverage the original reasoning process behind it. 
Thus, the reasoning process provides more 
effective and reliable observations than the 
communication process. Moreover, it also guides 
and explains some observations in the 
communication process such as compellingness 
and level of detail of a story. 
We will illustrate the formation of deceptive 
arguments in the next section, according to 
which, we propose three hypotheses of the 
fundamental differences between deception and 
non-deception. In Section 3, we describe our 
model of detection and the data simulation 
process. Experiment setting and results are 
63
discusses in Section 4, followed by conclusions 
and future work in Section 5.  
1 Formation of Deceptive Argument 
The reasoning process can be regarded as 
inference based on the conditional relationship 
between arguments by assuming that human 
reasoning is akin to informal logic. Since 
deceivers intentionally reach the conclusion that 
they target at, we propose that the act of 
deceiving is to reason by supposing the truth of 
deceivers? targeted arguments, but the truth of 
the targeted arguments is not actually believed by 
the deceivers. For example, if a person is asked 
to lie about his attitude on abortion, he might 
raise arguments such as ?fetuses are human?, 
?god will punish anyone who aborts children? 
and ?children have the right to live?. He did not 
raise these arguments because he believed in 
them but because they support the false 
conclusion that he is against abortion. It is thus 
natural to imagine that the conclusion comes into 
deceivers? minds before the arguments. 
According to Levi (1996), ?The addition of the 
supposition to the agent?s state of full belief does 
not require jettisoning any convictions already 
fully believed. The result of this modification of 
the state of full belief by supposition is a new 
potential state of full belief containing the 
consequences of the supposition added and the 
initial state of full belief?, which means that the 
reasoning with a supposition is a regular 
reasoning with the addition of a piece of 
knowledge that has been assumed before the 
reasoning starts. It also follows that the reasoning 
with a supposition can be exactly the same as a 
regular reasoning in which the supposition in the 
former case is a true belief. That is to say, the 
reasoning in deception formation can be regarded 
to follow the same scheme as that in truth 
argumentation. However, even if deceiver and 
truth teller share the same reasoning scheme, 
their beliefs and processes of reasoning are 
different. In particular, if an opinion-based story 
is required from the speaker, truth tellers 
propagate beliefs from evidence, while deceivers 
adapt beliefs to suppositions. If an event-based 
story is required, truth tellers retrieve relevant 
memory which is based on past behavior and 
past behavior is based on past belief, which was 
propagated from past evidence, while deceivers 
suppose a part of the event and adapt his fantasy 
to the supposition. This fundamental difference 
in the reasoning of deceiver and truth teller is 
unavoidable due to the intentionality of 
deceivers. It provides reasoning a stable ground 
on which schemes of deception detection can be 
built. 
As we have discussed, the product of 
reasoning from truth teller and deceiver may be 
exactly the same. However it is hardly true in the 
real world because they do not share the same 
belief system that supports their reasoning. If in 
any case they do share the same belief system, 
they would reach the same conclusion without 
any deception and there would be no need to 
deceive. In order to mimic truth teller?s story, 
deceiver may manipulate his conclusion and 
distort other arguments to support the 
manipulated conclusion, but the supporting 
arguments are biased by his honest but untruthful 
belief system. Therefore, discrepancies in 
arguments that deceivers are reluctant to 
believe but truth tellers embrace can be 
expected. On the other hand, deception has been 
defined as ?a relationship between two 
statements? (Shibles, 1988), according to which, 
deception is a contradiction between belief and 
expression. A deceiver may lie about the polarity 
of belief as well as the strength or extent of belief 
as long as his belief expression deviates from his 
honest reasoning. The more manipulation he did 
to mimic the truth, the farther he deviates from 
himself. Therefore, discrepancies in 
arguments that are manipulated by deceivers 
can be expected. The above two discrepancies in 
deception have been popularly embraced by 
existing researchers (Mehrabian, 1972; Wiener & 
Mehrabian, 1968; Johnson & Raye, 1981, 
Markus, 1977). Our focus is to explain and 
measure them in terms of human reasoning, and 
argue that these two discrepancies follow our 
proposal that deceptive reasoning is reasoning 
with presupposition, due to which the 
discrepancies are the fundamental difference 
between deception and truth that produces other 
observable patterns. 
2 Hypotheses and Justification 
We have argued that the basic discrepancy in 
deceptive reasoning exists in inconsistency and 
untruthfulness. Inconsistency means that the 
arguments in the story contradict with what the 
speaker would believe. Untruthfulness means 
that the arguments in the story contradict with 
what an honest person would believe in order to 
reach the conclusion. On the other hand, 
inconsistency indicates that an honest person 
64
should behave as he always does, which requires 
some familiarity with the speaker, whereas 
untruthfulness indicates that an honest person 
should behave as a reasonable and convincing 
person, which requires some knowledge of the 
topic domain. Opinion change violates the 
former one but not the latter one as it changes the 
prior knowledge but still maintains truthfulness, 
and innovation violates the latter one but not the 
former one as innovation is convincing but not 
expectable. They do not violate both so they are 
not deceptions. However, these two elements are 
not the unique characteristics of deception 
because random manipulations without any 
purpose to deceive such as misinformation also 
show inconsistency and untruthfulness. 
Fortunately, deceivers can be distinguished by 
the manner they manipulate arguments. We 
propose the following hypotheses that can be 
expected in deceptive stories but not others. 
Firstly, explicit manipulations in deception 
continuously propagate to other arguments which 
become implicit manipulations. The purpose, of 
course, is to spread the manipulation to the 
conclusion. The propagation spreads to 
surrounding arguments and the influence of 
manipulation decreases as the propagation 
spreads farther away, which random 
manipulations do not exhibit. If one overlooks 
the abnormality of the explicit manipulations, the 
story would seem to flow smoothly from the 
arguments to the conclusion because the 
connection between the arguments is not broken. 
Inconsistency is particularly important when 
individual difference should be considered.  
Secondly, there is a correspondence between 
inconsistency and untruthfulness. Some 
inconsistencies were manipulated significantly 
because the deceiver wants to convince the 
listener of the argument and these arguments 
seem more reasonable to support the conclusion 
after manipulation. Therefore, the significant 
manipulations are often convincing, but there are 
also exceptions in which deceivers overly 
manipulate arguments that are usually ignored by 
truth tellers. We call these Type I incredibility:  
incredibility due to over-manipulation. The 
arguments that are not convincing usually can be 
found in the inconsistencies that were slightly 
manipulated or ignored by the deceiver because 
deceivers do not know that they are important 
supports to the conclusion but truth tellers never 
neglect these details. This is called Type II 
incredibility: incredibility due to ignorance. Type 
I and Type II incredibility are two examples of 
unconvincing arguments (According to DePaulo 
et. al (2003), liars tell less compelling tales than 
truth tellers), which can be quantitatively 
measured in the reasoning process. On the other 
hand, random manipulations do not show this 
correspondence between inconsistency and 
untruthfulness. Measuring untruthfulness is 
particularly effective in detecting deception from 
general population whom the detector is not 
familiar with.  
Thirdly, deceptions are intentional, which 
means the deceiver assumes the conclusion 
before inferring the whole story. Or in other 
words, deceivers fit the world to their mind, 
which is a necessary component of intentionality 
according to Humberstone (1992). They are 
convincers who reach arguments from 
conclusions, while others reach conclusions from 
arguments. According to the satisfaction of 
intention (Mele, 1992), an intention is "satisfied" 
only if behavior in which it issues is guided by 
the intention-embedded plan. Thus, deceivers 
choose the best behavior (argument in this case) 
that is guided (inferred in this case) by his desire 
(conclusion in this case), but not any behavior 
that can fulfill his desire. In particular, deceivers 
will choose the state of the argument in the story 
that is most effective compared with other states 
of the argument in reaching the conclusion of the 
story (e.g. the best state of whether ?an unborn 
baby is a life? towards the conclusion of 
supporting abortion is no). In deception, the 
inconsistent arguments are usually effective to 
the conclusion, while in random manipulation the 
inconsistent arguments are not.  
Inconsistency, untruthfulness, propagated 
manipulation and intentionality are the guiding 
concepts of our deception detection method, 
which is a general model independent of the 
domain knowledge. 
3 Methodology 
In this work, we will not only test the hypotheses 
proposed above, but also provide a 
computational model to identify the discrepancy 
in arguments that are manipulated by deceivers 
and the discrepancy in arguments that are not as 
convincing as truth tellers?.  
3.1 Computational Model of Deception 
Detection 
We propose a generic model to detect deception 
through the reasoning process without assuming 
human?s reasoning scheme. As shown in Figure 
65
1, the model is composed of two networks: 
Correlation Network and Consensus Network. 
Correlation Network connects each agent with 
agents who correlate with him in a specific 
argument. Neighbors in the Correlation Network 
represent acquaintances who can anticipate each 
other?s arguments. Consensus Network connects 
agents with similar conclusions. Neighbors in the 
Consensus Network represent people who agree 
with each other. We have pointed out that 
deception is deviation from one?s own subjective 
beliefs, but not deviation from the objective 
reality or from the public. Thus Correlation 
Network is essential in predicting an agent?s 
belief according to neighbors who can expect 
each other. This idea of measuring individual 
inconsistency has been discussed in our former 
work (Santos et. Al, 2010), which also provides 
details on the computation. The Consensus 
Network provides a sampled population of truth 
tellers who reach the same conclusion as the 
deceiver. If the deceiver told the truth, he should 
behave in no difference with the population. The 
untruthfulness of the deceiver can be evaluated 
by comparing the deceiver with the truth tellers. 
Functionality of the arguments can be revealed 
from the history data of the deceiver. By 
studying the history data, we can evaluate which 
arguments are effective to which from the 
perspective of the deceiver. 
 
 
Figure 1: Architecture of the model of deception 
detection 
 
3.2 Date Collection and Simulation 
To test the hypotheses we proposed, we simulate 
the reasoning process of a deceiver according to 
our assumption that deceivers pre-suppose 
conclusions before reasoning. The deceiver we 
simulate is a plaintiff in a lawsuit of a rape case 
shown in a popular Hong Kong TV episode. The 
case is described as following. A female 
celebrity coded as A claims that she was raped by 
an Indian young man coded as B. A claims that 
she keeps away from B because both her and her 
mother do not like the physical odor of Indians. 
A claims that B once joined her birthday party 
without any invitation and fed A drugs. B then 
conveyed A home and raped A. After A?s 
boyfriend arrived, A called police. However, the 
truth is that B is a fan of A and joined A?s party at 
A?s invitation. A lied about her aversion to 
Indians because she used to prostitute to Indians. 
Besides, B is new to the party club, so it is 
unlikely for him to obtain drugs there. A used 
drugs and enticed B to have sex with her. This 
artificial scenario is a simplification of a possible 
legal case, which provides realistic explanations 
compared with simulation data that simulate 
deception arbitrarily without considering the 
intent of deceiver. We did not use real cases or 
lab surveys because they either do not have the 
ground truth of the speaker?s truthfulness or lack 
sufficient information about the reasoning of the 
deceiver. Data that do have both ground truth and 
sufficient information such as military combat 
scenarios are mostly focused on behavioral 
deception instead of communicative deception. 
In addition, real cases may contain noisy data in 
which the communication content is mediated by 
factors other than reasoning. For the purpose of 
evaluating hypotheses about deceptive reasoning 
it is ideal to use clean data that only contains the 
semantic meaning of arguments. The evaluation 
of the hypotheses guides the development of our 
detection model, which we will apply to real data 
eventually.  
A?s belief system is represented by a Bayesian 
Network (BN) (Pearl, 1988). BNs have been 
used to simulate human reasoning processes for 
various purposes and have been shown to be 
consistent with the behavior of human (Tenenbau 
et. Al, 2006). A BN is a graphical structure in 
which a node represents a propositional 
argument and the conditional probability 
between nodes represent the conditional 
relationship between arguments. For example, 
the reasoning that B drives A home because B 
knows A?s address can be encoded in the 
conditional probability 
P(B_drive_A_home|B_know_A_s_adr)=0.9. In 
order to eliminate the variation due to wording, 
the semantics of the arguments instead of the 
phrases are encoded in the nodes. We designed a 
BN representing A?s belief system and also a BN 
66
representing the belief system of a true victim of 
the rape case according to the description of the 
scenario and some common sense. More 
specifically, we connect two arguments if their 
causal relationship is explicitly described by the 
deceiver or by the jury when they are analyzing 
the intent of the deceiver. The conditional 
probabilities between states of arguments are set 
as 0.7 to 0.99 according to the certainty of the 
speaker if they are explicitly described. As to the 
states that are not mentioned in the case, they are 
usually implied in or can be inferred from the 
scenario if their mutual exclusive states are 
described in the scenario, such as the probability 
of A_hate_Indian given that B?s relation with A?s 
mother is good and that A used to prostitute to 
Indians. Otherwise the mutual exclusive states 
are given the same or similar probabilities 
indicating that they are uncertain. To make sure 
that the discrepancies in deception are resulted 
from the manner of reasoning instead of from the 
inherent difference between the deceiver?s belief 
system and the true victim?s belief system, we 
minimize the difference between their belief 
systems. Specifically, we keep all their 
conditional probabilities the same by assuming 
that both are rational people with the same 
common sense. Only their prior probabilities of 
A?s experience as prostitute and whether B is 
new to the party or not are adjusted differently, 
because they are the essential truth in a true 
victim?s perspective. That is to say, those who do 
not like Indians could not prostitute to them, and 
to obtain drugs from the party club, B has to be a 
regular guest. However, as a result of sharing a 
similar belief system with the true victim, the 
deceiver?s story may become highly convincing. 
Although we expect it to be hard to detect the 
untruthfulness of the deceiver, the deceiver?s 
simulation is not unrealistic because some 
deceivers are consistently found to be more 
credible than others based on the research by 
Bond and Depaulo (2008). It is highly likely that 
a randomized BN with a perturbed copy can also 
serve our purposes, but again, building belief 
systems based on the intent of deception will 
provide more realistic data, more convincing 
results and more intuitive explanations. The BN 
of the deceiver is depicted in Figure 2. Its 
conditional probability tables are shown in 
Appendix A.  
The process of reasoning is represented by the 
process of inferencing, and the product of 
reasoning is represented by the inferred 
probabilities of the nodes. Computing posterior 
probabilities, P(A|E), is not feasible here since it 
does not consider the consistency over all 
variables. Consider the following example. 
Suppose 10 people join a lottery of which exactly 
one will win. By computing posterior 
probabilities, we obtain the result that no one will 
win because each of them wins with probability 
0.1. To retain the validity of the probability of 
each variable as well as the consistency over all 
variables, we propose the following inference. 
We first perform a belief revision and obtain the 
most probable world, which is the complete 
inference with the highest joint probability. Then 
for each variable, we compute its posterior 
probability given that all other variables are set 
as evidence with the same assignment as in the 
most probable world. By inferring the lottery 
example in this way, in each of its inferred world 
a different person wins with equal probability. 
Specifically, the probability of a person winning 
given all others not winning is 1, and the 
probability of a person winning given all but one 
winning is 0. As we proposed earlier, the 
reasoning process of the deceiver presupposes 
her target arguments, that is, she was raped, by 
adding the argument as an extra piece of 
evidence. The inference results of A in both 
deceptive and honest cases and those of a true 
victim are shown in Table 1. The arguments 
B_relation_with_A_s_mother=bad, 
B_drive_A_home=true, A_is_celebrity=true and 
A_s_boyfriend_catch_on_the_scene=true are set 
as evidence as suggested by the scenario. 
 
 
Figure 2: BN of the deceiver in the rape case 
 
People express attitudes as binary beliefs in 
communication if not as beliefs with fuzzy 
confidence, but not as degree of belief 
67
formulated by real-valued probabilities. To map 
degree of belief to binary beliefs, we need to 
know how much confidence is sufficient for a 
person to believe in an attitude. Or in other 
words, what is the probability threshold of 
something being true. Research has suggested 
that truth threshold varies by proposition and by 
individual, which means it is a subjective 
criterion (Ferreira, 2004). Since we use simulated 
data, we arbitrarily choose 0.66 as the threshold 
since it equally spaces the interval of an 
argument being true, unknown and false. Then 
the binary beliefs in the deceptive story and 
honest story of the deceiver and those in the true 
victim?s story would be the same as Table 2. To 
verify the inferred beliefs, we compare Table 2 
with the scenario. An argument is validated if it 
is in the same state as described in the scenario 
or in the unknown state given that it is ignored in 
the scenario. We verified that 13 out of the 16 
arguments in the deceptive story corresponds 
with what the deceiver claims, all of the 
arguments in the honest story corresponds with 
what is the truth. Although it is hard to verify the 
true victim?s story because we do not have its 
ground truth, we observe that all the arguments 
are reasonable and most are contrary to the 
deceiver?s honest story except the evidence. 
Arguments Dece
pt.  
Ho
nest  
True  
B_relation_with_As_mother=g
ood 0 0 0 
A_have_exp_of_prostitution=T 0.66 0.88 0.11 
A_hate_Indian=T 0.74 0.07 0.89 
A_is_nice_to_B=T 0.18 0.88 0.18 
B_relation_with_A=rape 0.98 0.16 0.96 
B_in_A_s_party_by=self 0.9 0.4 0.90 
B_knows_A_s_adr=T 0.95 0.95 0.95 
B_drive_A_home=T 1 1 1 
B_is_new_to_party=T 0.76 0.82 0.16 
A_have_drug_from=B 0.76 0.07 0.92 
sex_by=rape 0.93 0.08 0.98 
As_boyfriend_catch_on_the_sc
ene=T 1 1 1 
A_is_celebrity=T 1 1 1 
B_refuse_to_pay=T 0.8 0.85 0.50 
A_claim_being_raped=T 0.6 0.7 0.60 
cry_for_help=T 0.8 0.2 0.80 
 
Table 1: Inferred results of the deceiver?s deceptive 
story, her honest story and a true victim?s story 
   
The computation of the discrepancies assumes 
acquaintance of the deceiver, which requires 
sufficient number of history data and neighbors 
of the deceiver. To achieve it, we simulate 19 
agents by perturbing the deceiver?s BN and 
another 10 agents by perturbing the true victim?s 
BN. In total, we have 29 truth telling agents and 
1 deceiving agent. We simulate 100 runs of 
training data by inferring the network of each 
agent 100 times with different evidence at each 
run, and convert them to binary beliefs. Training 
data is assumed to contain no deception. This 
approach of inconsistency detection is borrowed 
from our past work (Santos et. Al, 2010).  
Arguments Dece
pt.  
Hone
st  
True  
B_relation_with_As_mother bad bad bad 
A_have_exp_of_prostitution unknn T F 
A_hate_Indian T F T 
A_is_nice_to_B F T F 
B_relation_with_A rape fan rape 
B_in_A_s_party_by self unknn self 
B_knows_A_s_adr T T T 
B_drive_A_home T T T 
B_is_new_to_party T T F 
A_have_drug_from B self B 
sex_by rape entice rape 
As_boyfriend_catch_on_the
_scene T T T 
A_is_celebrity T T T 
B_refuse_to_pay T T unknn 
A_claim_being_raped unknn T unknn 
cry_for_help T F T 
 
Table 2: Binary beliefs of the deceiver?s deceptive 
story, honest story and a true victim?s story 
4 Experiment and results  
To test the hypotheses, we compare the result of 
deceptive story with the result of misinformative 
story. A misinformative story is simulated by 
adding random error to the inferred results of the 
arguments. 
? Propagation of manipulation 
To calculate inconsistency we predict binary 
beliefs in the deceptive story using GroupLens 
(Resnick et. Al, 1994) based on stories of 
neighboring agents in the Correlation Network. 
We then compare the binary beliefs in the 
deceptive story with predicted binary beliefs to 
measure deviation of each argument due to 
inconsistency. We measure how many standard 
(std.) deviations the prediction error in deceptive 
story deviates from the prediction error in 
training data, and plot them according to their 
locations in the BN, as shown in Figure 3. The 
68
width of the links represents the sensitivity of 
each variable to its neighbors.  
We observe that the variables at the 
boundaries of the graph and not sensitive to 
neighbors (e.g. B_is_new_to_party) are ignored 
by the deceiver, while the variables in the center 
or sensitive to others (e.g. A_hate_Indian) are 
manipulated significantly. It demonstrates that 
manipulations propagate to closely related 
arguments. Unrelated arguments are probably 
considered as irrelevant or simply be ignored by 
the deceiver. On the other hand, if we compare 
deceptive story with honest story in Table 2, we 
obtain 9 arguments manipulated by the deceiver. 
Out of these 9 arguments, 8 are successfully 
identified as inconsistent by Figure 3 if we 
assume the suspicion threshold is 3 std. 
deviations. 
 
 
Figure 3: Inconsistency deviation of each variable 
 
? Correspondence between inconsistency 
and untruthfulness  
To compute untruthfulness, we calculate the 
deviation of the binary beliefs in the deceptive 
story from the population of truth teller?s stories 
who agrees with the deceiver in the Consensus 
Network. We then compare the deviation due to 
inconsistency with respect to the deceiver herself 
and that due to untruthfulness with respect to 
truth tellers. The result is shown in Table 3. 
The correlation between the deviation due to 
inconsistency and that due to untruthfulness is -
0.5186, which means that untruthfulness has a 
large negative correlation with inconsistency. It 
credits our hypothesis that significant 
manipulations are often convincing and 
unconvincing arguments usually can be found in 
slightly manipulated or ignored arguments. The 
only exception in the result is the argument 
B_knows_As_address, which is not manipulated 
but convincing. It is probably because the 
evidence B_drive_A_home enforced it to remain 
honest. Type I incredibility does not occur in this 
case, but type II incredibility appears in the 
argument B_is_new_to_party and 
B_refuse_to_pay. The deceiver ignored these 
arguments, which results in the incredibility of 
the story. The correlation between inconsistency 
and untruthfulness in misinformative stories 
ranges between 0.3128 and 0.9823, which 
demonstrates that the negative correction cannot 
be found in misinformative stories. If we 
compare the deceptive story and the true story in 
Table 2, we find out that 3 arguments in the 
deceptive story are unconvincing. By observing 
the untruthfulness in Table 3, we find out that 2 
of the 3 arguments are out of at least 1.44 std. 
deviations of the sample of true stories and all of 
them are out of at least 0.95 std. deviations. The 
small deviations indicate a high credibility of the 
deceiver, which is caused by the similarity 
between the belief systems of the deceiver and 
the true victim. 
Belief Incon. Untru. 
B_relation_with_As_mother=good N/A N/A 
A_have_exp_of_prostitution=T 3.48 0.95 
A_hate_Indian=T 3.48 0.28 
A_is_nice_to_B=T 3.31 0.28 
B_relation_with_A=rape 3.25 0 
B_in_A_s_party_by=self 3.39 0.28 
B_knows_A_s_adr=T 0.04 0 
B_drive_A_home=T N/A N/A 
B_is_new_to_party=T 0 1.59 
A_have_drug_from=B 2.93 0 
sex_by=rape 3.95 0 
As_boyfriend_catch_on_the_scene
=T N/A N/A 
A_is_celebrity=T N/A N/A 
B_refuse_to_pay=T 0.48 1.44 
A_claim_being_raped=T 4.63 0.41 
cry_for_help=T 3.37 0.41 
 
Table 3: Comparison of inconsistency and 
untruthfulness of the deceiver 
 
? Functionality  
Functionality means that the manipulated 
arguments are effective in reaching the goal and 
at the same time satisfies the evidence. In other 
words, we can expect the manipulated arguments 
from the goal and the evidence. The calculation 
69
of functionality is as following. For each 
inconsistent argument, we measure its correlation 
with other arguments in the past using training 
data. We then predict each argument?s binary 
belief based on the value of the conclusion and 
the evidence. If the predicted belief corresponds 
with the belief in the deceptive story, the variable 
is functional. We compare the results of 
deceptive story with those of misinformative 
story. In Table 4, all but one manipulated 
arguments in the deceptive story complies with 
the value expected by the conclusion and 
evidence, but none of the inconsistent arguments 
in misinformative stories does. Although the 
result shown in Table 5 comes from a random 
sample of misinformative story, we observed that 
most of the samples show the same functionality 
rate. Therefore, the functionality rate of 
deceptive story is 6/7, while the functionality rate 
of misinformative story is around 0/3. 
Arguments Pred.  Decept.  
A_have_exp_of_prostitution=T 0.24 0.5 
A_hate_Indian=T 0.85 1 
A_is_nice_to_B=T 0.07 0 
B_relation_with_A=rape 0.99 1 
B_in_A_s_party_by=self 1 1 
A_claim_being_raped=T 0.58 0.5 
cry_for_help=T 0.86 1 
 
Table 4: Functionality of the deceiver?s story 
 
Arguments Pred.  Misinfo.  
B_in_A_s_party_by=self 0.45 0 
B_knows_A_s_adr=T 0.90  0.5 
A_claim_being_raped=T 0.94 0.5 
 
Table 5: Functionality of a mininformative story 
5 Conclusion and future work 
We proposed in this work two fundamental 
discrepancies in deceptive communications: 
discrepancies in arguments that deceivers are 
reluctant to believe but truth tellers embrace and 
discrepancies in arguments that are manipulated 
by deceivers. The proposal follows the following 
three assumptions: The act of deceiving is 
composed of deceptive argument formation and 
argument communication; Deception is formed 
in the reasoning process rather than the 
communication process; Reasoning is interaction 
between arguments, and deceptive reasoning is 
reasoning with presupposition. Then we 
proposed three hypotheses in order to distinguish 
deception from unintentional misinformation: 
manipulations propagate smoothly through 
closely related arguments, inconsistency and 
untruthfulness are negatively correlated, and 
deceptive arguments are usually functional to 
deceiver?s goal and evidence. To evaluate and to 
measure these hypotheses from communication 
content, we designed a generic model of 
deception detection. In the model, agents are 
correlated with others to expect each other?s 
consistency in beliefs and consenting agents are 
compared with each other to evaluate the 
truthfulness of beliefs. Our experimental results 
credit the hypotheses. The main contribution of 
this work is not to follow or reject the path that 
linguistic cues have laid out, but to suggest a new 
direction in which deeper information about the 
intent of deceivers is carefully mined and 
analyzed based on their cognitive process.   
In the future, we will further develop the 
model by designing and implementing detection 
methods based on the hypotheses. Currently we 
use simulated data based on an artificial story, 
which is closer to a real legal case that provides 
concrete information about the reasoning of 
deceivers with minimum noise. In the future, we 
will apply the model to survey data that is 
commonly used in the area. Various natural 
language processing techniques can be utilized in 
the retrieval of the reasoning process. 
Specifically, Latent dirichlet alocation (Blei et. 
Al, 2002) can be used to categorize the sentences 
into topics (or arguments), sentiment analysis 
(Liu. 2010) can be used to extract the polarity of 
each argument, and various BN constructors such 
as PC algorithm (Spirtes et. Al, 1993) can be used 
to construct the belief systems. On the other 
hand, linguistic cues have been observed in past 
research (DePaulo et. al, 2003), but has not been 
defined or explained quantitatively. The study of 
the pattern of deceptive reasoning can ultimately 
provide guidance and explanations to existing 
observations in deception cueing. 
Acknowledgments 
This work was supported in part by grants from 
AFOSR, ONR, and DHS. 
References  
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent Dirichlet alocation. Journal of 
Machine Learning Research, Lafferty, John. 
Ed., 3 (4?5): 993?1022. 
Bella M. DePaulo, James J. Lindsay, Brian E. 
Malone, Laura Muhlenbruck, Kelly Charlton, and 
70
Harris Cooper. 2003. Cues to deception. 
Psychological Bulletin, 129(1): 74-118.  
Ulisses Ferreira. 2004. On the Foundations of 
Computing Science. Lecture Notes in Computer 
Science, 3002:46-65. 
John O. Greene, H. Dan O'hair, Micheal J. Cody, and 
Catherine Yen. 1985. Planning and Control of 
Behavior during Deception. Human 
Communication Research, 11:335-64. 
I. L. Humberstone. 1992. Direction of Fit. Mind, 
101(401): 59-84. 
Marcia K. Johnson and Carol L. Raye. 1981. Reality 
Monitoring. Psychological Bulletin, 88:67?85. 
Isaac Levi. 1996. For the Sake of the Argument. 
Cambridge University Press. New York, NY, USA. 
Bing Liu. 2010. Sentiment Analysis and Subjectivity. 
Handbook of Natural Language Processing Issue, 
1st ed., Taylor and Francis Group, Eds. CRC Press, 
1-38. 
Hazel Markus. 1977. Self-schemata and Processing 
Information about the Self. Journal of Personality 
and Social Psychology, 35:63?78.  
Albert Mehrabian. 1972. Nonverbal Communication. 
Aldine Atherton, Chicago, USA. 
Alfred R. Mele. 1992. Springs of Action: 
Understanding Intentional Behavior. Oxford 
University Press. New York, NY, USA. 
Judea Pearl. 1988. Probabilistic Reasoning in 
Intelligent Systems. Morgan Kaufmann Publishers, 
San Francisco, CA, USA. 
Paul Resnick, Neophytos Iacovou, Mitesh Suchak, 
Peter Bergstrom, and John Riedl. 1994. 
GroupLens: An Open Architecture for 
Collaborative Filtering of Netnews. Proc. of the 
Conference on Computer Supported Cooperative 
Work, 175-186. ACM Press, Chapel Hill, NC, 
USA.  
Eugene Santos, Jr. and Deqing Li. 2010. Deception 
Detection in Multi-Agent Systems. IEEE 
Transactions on Systems, Man, and Cybernetics: 
Part A, 40(2):224-235. 
Warren Shibles. 1988. A Revision of the Definition of 
Lying as an Untruth Told with Intent to Deceive. 
Argumentation, 2:99-115. 
Peter Spirtes, Clark N. Glymour, and Richard 
Scheines, 1993. Causation, Prediction, and Search. 
Springer-Verlag, New York, NY, USA. 
Morton Wiener and Albert Mehrabian. 1968. 
Language within Language: Immediacy, a Channel 
in Verbal Communication. Meredith Corporation, 
New York, NY, USA. 
71
