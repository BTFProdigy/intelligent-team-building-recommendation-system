Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 65?72,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The Linguistics of Readability: The Next Step for Word Processing  Neil Newbold Lee Gillam University of Surrey University of Surrey Guildford Guildford Surrey, GU2 7XH, UK Surrey, GU2 7XH, UK n.newbold@surrey.ac.uk l.gillam@surrey.ac.uk       Abstract 
In this paper, we present a new approach to writing tools that extends beyond the rudi-mentary spelling and grammar checking to the content of the writing itself.  Linguistic meth-ods have long been used to detect familiar lexical patterns in the text to aid automatic summarization and translation of documents.  We apply these methods to determine the quality of the text and implement new tech-niques for measuring readability and provid-ing feedback to authors on how to improve the quality of their documents.  We take an ex-tended view of readability that considers text cohesion, propositional density, and word fa-miliarity.  We provide simple feedback to the user detailing the most and least readable sen-tences, the sentences most densely packed with information and the most cohesive words in their document.  Commonly used verbose words and phrases in the text, as identified by The Plain English Campaign, can be replaced with user-selected replacements.  Our tech-niques were implemented as a free download extension to the Open Office word processor generating 6,500 downloads to date. 
1 Introduction Spell and grammar checking have become inherent tools in many modern word processors even if their results are not always deemed appropriate.  Work on writing tools has largely focused on improving these services, with superior grammar checkers being the emphasis of this work.  However, there has been little effort on providing a deeper analysis of the text, such as covering its semantic content 
and its potential success in conveying the authors intended message to the reader.  Research on read-ability aimed to provide an indication of the pro-portion of the population could understand the text but has been limited to simple checks of word and sentence length providing only some degree of feedback on where and why text is difficult to un-derstand.  Writing tools such as ?Stylewriter? scores documents based on average sentence length, number of passive verbs and overall style.  The style analysis uses an indexes check for a wide variety of common editorial issues like jargon, hy-phenation, sexist writing, clich?s, grammar, redun-dancies and troublesome words which are either abstract, complex, misused or overused.  However, their approach is based on a simple lookup of common writing patterns with no analysis of over-all message clarity.  More robust tools such as ?Coh-Metrix? (Graesser et al, 2004) deliver a sub-stantial analysis but can leave casual users con-fused with the quantity of numerical data produced. In this paper, we discuss how linguistic tech-niques have been deployed to measure largely ig-nored aspects of the text, which can benefit authors when writing texts.  We use automatic summariza-tion techniques to measure how cohesive or consis-tent the text is and parts of speech patterns to identify multi-word expressions, which indicate portions of text densely, packed with information.  We also deploy corpus linguistics methods to measure the familiarity of words in everyday use.  These techniques expand upon readability research to provide a series of tools for authors giving pointers to where their documents might confuse their intended audience. 
65
2 Background  In principle, readability measures identify some proportion of the population who could comforta-bly read a text.  Historically, readability research has focused primarily on producing a numeric evaluation of style of writing to associate textual content to a particular rating or the level of educa-tion of readers.  Readability research largely traces its origins to an initial study by Kitson (1921) who demonstrated tangible differences in sentence lengths and word lengths, measured in syllables, between two newspapers and two magazines.  Kit-son?s work led variously to the development of readability metrics, many of which are available in certain software applications.  Further discussion of these formulae can be found elsewhere (Dubay, 2004).  More recent considerations of readability account for reader factors, which consider certain abilities of the reader, and text factors, which con-sider the formulation of the text (Oakland and Lane, 2004).  Reader factors include the person?s ability to read fluently, level of prior subject knowledge, lexical knowledge or familiarity with the language, and motivation and engagement. Text factors account to some extent for current readability metrics, but also cover considerations of syntax, lexical selection, idea density, and cog-nitive load.  Oakland and Lane?s view of readabil-ity suggest that it may be possible to generically measure the difficulty of text as an artifact, but that ?text difficulty? necessitates consideration of each reader.  Our work elaborates that of Oakland and Lane in identifying difficulties in the apparently neat separation of the factors.  In this section, we propose a new framework for readability that builds on Oakland and Lane by making considera-tion of the relationship between text, reader, and author.  We explore, subsequently, how word proc-essors might use such a framework to help authors get across their intended messages. 2.1 Matching Text to Readers In writing a document, an author has to be mindful of the needs of his anticipated audience, particu-larly if they are to continue reading.  There must be some correlation across three principal aspects of a text: the nature and extent of its subject matter, its use of language, and its logical or narrative struc-ture.  The audience can be defined by their degree 
of interest in the subject, how much they already know about it, their reading ability, and their gen-eral intelligence.  If the author needs to learn more about a set of potential readers, standardized tests are available to measure levels of intelligence and reading skill, while interest and prior knowledge can be assessed by ad hoc surveys.  Two kinds of measure are suited to appraising text structure: logical coherence and propositional density.  By logical coherence, we mean the extent to which one statement is ordered according to a chain of reasoning, a sequence or chain of events, a hierarchy or a classificatory system.  By proposi-tional density we mean the closeness, measured by intervening words, between one crucial idea and the next.  The less coherently ordered are it?s the ideas and the greater their density, the larger the cognitive load on the reader.  If characteristics of the audience have been as-certained, the author must ensure that what he is writing is generally suitable for them.  A readabil-ity formula will produce a quick check on a given text for an author, and comparisons have been made amongst measures to correlate with specific human performances over largely disjoint sets of texts.  For our current considerations, we are inter-ested in providing more useful feedback to the author that a single numerical value.  A readability analysis should be able to provide hints to the author on how to improve their text.  However, this is not to say that existing measures are adequate, we propose other elements of text that can be measured instead of, or in addition to those exam-ined by the currently established readability formu-lae.  Our new framework for readability, describing the factors to be considered is presented in Fig. 1.  The matches needed for easy reading describes how an author can match their text to their target audience.  In the remainder of this sec-tion, we elaborate these factors.            
66
   Figure 1: Matches needed for easy reading 2.2 Language When matching text to reader, the author needs to consider the level of language and style of writing.  This can be described as the vocabulary familiarity and syntactic complexity of the writing.  These aspects are generally measured by the existing readability formulas as word length and sentence length.  Readability metrics generally determine the difficulty of a word by counting characters or syllables.  However, Oakland and Lane suggest that word difficulty can be determined by examin-ing whether the word is challenging, unusual or technical, and cite word familiarity as more effec-tive means of measuring word difficulty.  The process by which readers develop word familiarity is through their language acquisition and the de-velopment of their language capability.  Frequency plays an important role in building knowledge of a language so that it is sufficient to understand its written content.  Diessel (2007) showed that lin-guistic expressions stored in a person?s memory are reinforced by frequency so that the language user expects a particular word or word category to 
appear with a linguistic expression.  These linguis-tic expectations help comprehension. Frequency was also found to be fundamental in reading fluency as words are only analyzed when they cannot be read from memory as sight words.  A limited knowledge of words affects reading flu-ency as readers are likely to dwell over unfamiliar words or grammatical constructions.  This impedes the reader?s ability to construct an ongoing inter-pretation of the text.  The reading fluency of the reader is dependent on their familiarity with lan-guage.  When readers find text populated with un-familiar words it becomes harder for them to read.  This is especially prevalent in scientific or techni-cal documents where anyone unfamiliar with the terminology would find the document hard to un-derstand.  The terminological nature of specialized documents means that terms will appear with dis-proportionate frequently throughout the documents in contrast to what one would expect to encounter in everyday language.  Terminology extraction techniques exploit this relationship to identify terms.  We adapt this method by contrasting word frequency within documents with familiarity in general language.  We determine the difficulty of a word by its familiarity.  Vocabulary does not tend to exist in isolation.  The vocabulary may be well-defined, yet included in overly verbose sentences.  Consider these two sentences: 1. ?We endeavor to maintain the spinning of all the plates.? 2. ?We try to keep all the plates spinning.?  The first sentence uses passive voice, the second uses active voice.  Writing guidelines, such as those presented by the Plain English Campaign (1979), often recommend active voice wherever possible.  Active voice uses fewer words and helps readers build a mental representation of the text.  Existing readability formulae consider that long and complex sentences can confuse the reader and whilst we support this view, we consider that each individual sentence should be scored to allow the author to identify the particularly troublesome sec-tions of their text.  When matching text to reader, syntactic complexity should be examined not just for the entire document but whether each sentence is appropriate for the reading level.    
67
2.3 Subject To learn from text, a reader needs to associate the new information to their existing knowledge.  This task can be helped by the reader?s interest level.  Kintsch et al (1975) showed that we find stories easier to remember than technical texts because they are about human goals and actions, something to which we can all generally relate.  Scientific and technical texts require specific knowledge that is often uncommon, making the texts impenetrable to those outside the domain.  This suggests that read-ability is not merely an artifact of text with differ-ent readers having contrasting views of difficulty on the same piece of text.  Familiarity with certain words depends on experience: a difficult word for a novice is not always the same as a difficult word for an expert.  Reader characteristics such as moti-vation and knowledge may amplify or negate prob-lems with difficult text.  When matching text to reader, the author needs to consider the target audience and the extent of their knowledge. Many readability metrics do not make distinc-tions based on the background knowledge of the reader.  As discussed in relation to vocabulary, word familiarity can give a better indication of word difficulty than word length.  A longer word may only be difficult for a particular reader if un-familiar, and certain shorter words may even be more difficult to understand.  Consider a general reader confronted in text discussing a ?muon?: This short term would be rated as simple by current readability formulae.  However, a majority of peo-ple would be unfamiliar with this term, and only physicists are likely to know more about the term, its definition, and related items.  One way to meas-ure background knowledge would be to consider the extent of use of known terms in the text with direct consideration of previous documents within the reader?s experience. Entin and Klare (1985) showed that more read-able text is beneficial for those with less knowl-edge and interest.  In their study, students were presented with written material below their reading level.  When the reader?s interest was high, text below their grade level did not improve compre-hension.  However, when the reader?s interest was low their comprehension was improved by simpler text.  This suggests that more readable text im-proves comprehension for those less interested in the subject matter.  We consider the need to cap-
ture and analyze the user?s experience with prior documents as a proxy for reader knowledge and motivation.  Given a reading history for a user, we might next build their personalized vocabulary with frequency information, and therefore measure familiarity with words on an individual basis.  In the same way that an expert is familiar with the terminology of their subject, we can reflect the background knowledge required by a reader to in-terpret the text correctly.  When matching text to reader, word difficulty should be measured, if the information is available, on an individual basis. 2.4 Structure Well-written text requires a structure that readers can readily use to find the information they need and to understand it correctly.  Text can become confusing when information is inappropriately pre-sented.  Most sentences, when taken out of context, can become multiply ambiguous.  When we read text, we build a collection of the concepts de-scribed within it.  We identify these concepts with words and phrases using pragmatic, semantic, and syntactic features.  We build certain interpretations with these blocks of words that tend not to com-bine randomly or freely, but rather they keep pre-ferred company (Firth, 1957).  These collocations are evidence of preference for certain friends, and these friends may be kept at certain distances.  For example, words impose restrictions over syno-nyms, excluding some from their group of friends so that ?strong tea? may be acceptable, but ?power-ful tea? may not.  A reader unfamiliar with such constructions might not understand the precise meanings or variations.  In addition, individual words may not be particularly difficult but their combination may produce different meanings to the component words.  Collocation statistics may indicate compound nouns with specialized mean-ing but increased likelihood of misinterpretation.  Consider, for example, ?glass crack growth rate?: each word should be relatively easy to understand, but interpretations due to bracketing (Pustejovsky et al, 1994) might lead to interpretations of a ?crack growth rate? made of ?glass?, and an un-packing of semantics may be useful in removing ambiguities due to bracketing. Most researchers agree that collocations are se-quences of words that co-occur more often than by chance, with certain assumptions of randomness, 
68
and can be found using statistical measures of as-sociation.  Some linguists consider collocations are the building blocks of language, with the whole collocation being stronger than the sum of its parts.  They describe collocations as lexical items that represent uniquely identifiable concepts or seman-tic units.  Smadja (1993) elaborated criteria for a collocation, describing them as recurring and cohe-sive domain-dependent lexical structures such as ?stock market? and ?interest rate?, and suggested how components can imply collocations, for ex-ample ?United? produces an expectation of ?King-dom?, ?Nations?, or ?States?.  When frequently combined linguistic expressions develop into a processing unit, many of the linguistic elements are ignored and the whole chunk is compressed and treated as one semantic unit.  These units often develop into terms with multiword units represent-ing singular concepts.  This relates back to the as-sumed knowledge of the reader.  However, for readers unfamiliar with the terms, we have identi-fied two methods called ?Propositional Density? and ?Lexical Incoherence? for processing semantic units. When a significant amount of information is conveyed in a relatively small amount of text, the reader can become confused.  We identify this problem as ?Propositional Density?.  Although long collocations form semantic units that reduce con-ceptual complexity, problems occur when numer-ous semantic units are described within a short space of each other causing the reader to make numerous inferences.  The number of ideas ex-pressed in the text contributes to the work required of the reader to interpret the text correctly.  Pro-positional density may be measurable by examin-ing the quantity of objects within short distances of each other.  These objects can be labeled with sin-gle nouns or multi-word expressions.  By measur-ing the number of unique semantic units, we can approximate the workload required for processing or interpreting the text correctly. The second problem with text structure is called ?Lexical Incoherence? and occurs when writers present new information to the reader without making clear its relationship to previous informa-tion.  The writer assumes that they have provided enough information to allow readers to follow their arguments logically.  Repetition of concepts, terms, and other referents provides a structure for the reader to connect with.  It is through this repeti-
tion that a series of links can be made between the sentences.  There is a relationship here to work on lexical cohesion (Hoey, 1991).  If a large number of new, seemingly unrelated ideas are being intro-duced, low cohesion would be expected and meas-urable.  Efficiency can be increased here by using synonyms.  Semantic units can be referred to by a number of different labels and by identifying these different labels we can more accurately find the prominent ideas in the text. 3 Open Office Readability Report  To implement our new techniques for measuring readability, we used OpenOffice.org 3, which is the leading open-source office software suite.  As it can be downloaded and used free of charge, it has an already established user base and allows third-party developers to write extensions for their applications.  These extensions are made available to download for any OpenOffice.org user.  We cre-ated the readability report extension for ?Writer?, the open office word processor, to implement our readability techniques.  The extension generates 5 separate components devised from our framework for readability incorporating the matches for easy reading.  The components analyze the text factors in the framework to provide an indication of the corresponding reader factor.  The author can use this information to help match their text to their audience.  Each author and reader element is ad-dressed by a component as follows:  o Language -> Reading Level o Weirdness Measure o SimpleText SmartTags o Subject -> Interest and Knowledge o Not yet implemented o Structure -> Intelligence o Propositional Density o Lexical Coherence  The two language components address both the text features of vocabulary familiarity and syntac-tic complexity.  We have yet to implement a com-ponent to assess the subject of the text.  The separate components, which consist of either a generated report or text annotation through Smart-Tags, are detailed in the remainder of this section.   
69
3.1 Weirdness Measure The first generated report uses our new readability formula based on word frequency.  Unlike the es-tablished readability formulas, our measure can be applied to an individual sentence, allowing the re-port to highlight the most and least readable sen-tences in the document.  We use frequency information from the 100 million word tokens of the British National Corpus (BNC) to act as a ref-erence corpus.  The frequency counts for each word along with the number of words in the sen-tence are used to determine the sentence readabil-ity.  The score for the document can then be ascertained as an average value for each sentence.  We use log and other arbitrary values to bring the final number into a similar range as the other read-ability formulas. 
)
)1(
ln(1.2
?
+
?
SLGL
GLSL
SL
nf
nf
n
 (1) 
Eqn. 1 sentence-based familiarity 
f is word frequency, n is word count at sentence level (SL) or corpus level (GL).  Based on research by Stuart et al, (2004) con-cerning the frequency of use of apostrophes by children, contractions such as ?n?t? and ??s? were considered as separate words with their difficulty determined by their frequency count as per any other word.  This method for analyzing contrac-tions generated more effective results from the BNC. In addition, to the weirdness measure the report provided the scores from the established readabil-ity formulas, ?Flesch Easy Reading?, ?Flesch Kin-caid?, ?FOG?, ?SMOG? and ?ARI?.  To help authors understand the significance of the readability val-ues, a series of ratings were provided for each measure (inc. Weirdness), which grade a document as either ?Simple?, Easy?, ?Good?, ?Challenging? or ?Difficult? using a series of threshold values. 3.2 SimpleText SmartTags  SmartTags were developed in Open Office to high-light sections of documents and add contextual information.  The readability report extension uses SmartTags to highlight difficult words and phrases in the text, as identified by 
http://www.plainenglish.co.uk/.  The ?SimpleText SmartTags? provide suitable alternatives for these phrases which can be inserted automatically into the text.  The user can click on the SmartTags and select a possible alternative from a list of suitable replacements.  These SmartTags help authors avoid using common, verbose expressions that hinder the clarity of their writing.  Gillam and Newbold (2007) showed that plain English substi-tutions can lower readability scores. 3.3 Propositional Density  To measure the structure of the text, we use an analysis of propositional density.  This report analyses how many concepts and ideas are referred too in the text and was entitled the ?Brain Overload Report? to make it more accessible to the users.  An expert in a particular subject will often use spe-cific terms and jargon resulting in too much infor-mation being presented to the reader within a short space.  This can lead to learners become fatigued and confused.  The report measured the amount of single and compound nouns in comparison to the length of the sentence in which they occurred.  Sentences with contained a large amount of ?glue? words, such as ?the?, ?at?, etc. would score lower than sentences loaded with multi-word expres-sions.  The score for the document is determined as an average value for each sentence.  For user con-venience, we use some arbitrary values to bring the final number into a similar range as other readabil-ity formulas. 
)(3
2
cn
nu
n
?
+  (2) 
Eqn. 2 sentence-based propositional density 
u is the number of semantic units, n is sentence length, 
c is the number of collocated words.  Whilst this report was devised to measure the structure of the text, there is an also an element of subject which determines the assumed knowledge of the reader.  Scientific and technical texts often require specific knowledge represented in the text through frequent use of terminology.  The single and multi-word terms increase the propositional density of the document indicating that the text will be difficult for novices in the subject matter.  Texts intended for a general audience should score 
70
low on propositional density.  As with the previous report, the resulting score is graded as either ?Gen-eral?, ?Introductory?, ?Scholarly?, ?Technical? or ?Specialized? to help authors understand the impact their text will have on their intended audience.  For further feedback, the most frequent multi-word expressions are listed to show authors which expressions are contributing the most to their score.  Each expression is unpacked into its com-ponent expressions and a frequency count through-out the document is taken for each.  The most frequent component expression is then used as a basis for unpacking the full expression.  For exam-ple, ?current account balance? will be unpacked into either ?balance of current account? or ?account balance which is current?, depending on which of the component expressions, ?current account? and ?account balance? are more frequent.  If a suitable component expression is found, the full unpacked expression is suggested to the user as a possible way of rewriting the collocation.  The separating glue words are selected depending on the Part-Of-Speech tagging of the concluding phrase in the rewritten expression. 3.4 Cohesion Measure The ?Cohesion Report? uses techniques for auto-matic summarization to measure how easy a document is to follow.  It identifies the lexical words in each sentence and uses them to recognize sentence bonds.  Hoey (1991) described a sentence bond as two sentences sharing 3 or more lexical words.  The score for a document is determined by the number of sentence bonds against the total number of possible sentence bonds. 
)1( ?ss
b  (3) 
Eqn. 3 document-based lexical coherence 
b
is the number of sentence bonds, s is the number of sentences.  The sentence with the highest number of bonds is highlighted in the report as the most representa-tive of the document.  For further feedback, the report shows the words that are the strongest themes in the text.  These are lexical words that were used the most often to create sentence bonds.  By increasing the references to these themes the 
author can improve the cohesion of their text.  Authors need to pay particular attention to sen-tences with no sentence bonds are these are adding nothing to the coherence of the text.  These can be seen by using the detailed report described in the next section.  The cohesion measure is primarily useful for documents about a specific subject; fic-tional writing will often score low for cohesion.  As with previous reports, a document will be graded as either ?Creative?, ?Digressing?, ?Consis-tent?, ?Coherent?, or ?Fluent?. 3.5 Detailed Report  An option is provided for a detailed report that al-lows authors to view the readability score of each sentence in their document.  The report is dis-played in a spreadsheet and shows the results of each of the established readability measures and the new scores discussed in this paper.  The spreadsheet can be used to identify the most trou-blesome sentence in the document.  This is particu-larly useful for examining the results of the cohesion measure, as sentences that are not adding to the cohesion of a document can be easily identi-fied. 4 Conclusion  The weirdness measure correlates with the other readability formulas that have been shown to indi-cate the required reading age of the text, when ana-lyzing a large range of texts.  Our results show that frequency is a good indicator of word difficulty.  Table 1 shows a sample of texts, ordered by in-creasing difficulty, ranging from children?s books to technical reports and the correlation of the weirdness measure to the established formulas.  For sentences, containing relatively long but com-monly used words such as ?information? and ?busi-ness?, the score calculates more probable figures than the established readability formulas.  Certain children?s books (for example ?Jabberwocky?) contained made-up or nonsense words which caused the measure to the rate the texts as difficult.  It should be noted that the other readability formu-las rated these texts as simple.  We consider that these types of text would be confusing to non-native speakers of the language, with the effect of these words, which are unique to the document, being the same as terminology.   
71
 Text Weird Kincaid FOG SMOG ARI Lucky 8.54 3.80 5.26 6.74 2.75 The Abso-lutely True Diary 9.40 4.62 6.31 7.05 3.33 Coraline 9.41 5.08 7.24 8.05 4.41 Associated Press, Fed Revises 11.78 10.92 12.50 11.96 11.20 Bloomberg, U.S. Leading Indicators 11.76 11.28 13.33 12.60 11.51 USA Today, Greenspan predicts 12.07 11.36 13.25 11.21 12.27 Greenspan, to congressional committee 2005 12.75 14.32 16.29 14.60 14.55 Greenspan, speech 2005 12.52 15.69 17.80 15.70 16.18 Bernanke, speech 2008 13.29 16.28 17.97 15.58 17.72 Bernanke, report to con-gress 13.24 16.60 18.80 16.31 17.80 Correlation  0.98 0.98 0.96 0.99  Table 1: Correlation of weirdness measure with estab-lished readability formulas  Currently, we have not implemented a means to measure the user?s experience with prior docu-ments as a proxy for reader knowledge and motiva-tion.  In future, we would build a personalized vocabulary for each user with frequency informa-tion, and therefore measure familiarity with words on an individual basis.  At present the Open Office extension is more useful for writing general texts as opposed to specialized or technical documents.  Certain elements such as the weirdness measure and the SimpleText SmartTags become less useful with more expert texts and the prevailing use of terminology.  Other aspects such as propositional density and lexical coherence are of less use when analyzing children?s books.  This style of writing can score high for propositional density due to ex-travagant character names (e.g., ?The Mad Hatter? and ?Cheshire Cat?) increasing the number of compound nouns.  Lexical cohesion is also low for any fictional writing.  We are looking at improving the lexical cohe-sion measure with the consideration of synonyms.  Semantic units can be referred to by a number of different labels and by identifying these synonyms; 
we can more accurately identify the prominent ideas in the text.  It is the repetition of terms and their synonyms, along with other referents that provide a structure for the reader to connect with. The extension was made available on the Open Office website in July 2009.  In six months the ex-tension had received over 6,500 downloads indi-cating, along with positive user feedback that demand for word processors to go beyond simple spelling and grammar checking of text and provide more feedback is considerable. References  H. Diessel. 2007. Frequency effects in language acquisi-tion language use, and diachronic change.  New Ideas in Psychology, 25(2):108?127. W. H. DuBay. 2004. The Principles of Readability. Costa Mesa, CA: Impact Information. E. B. Entin, G. R. Klare. 1985. Relationships of meas-ures of interest, prior knowledge, and readability comprehension of expository passages. Advances in reading/language research, 3: 9?38. J. R. Firth. 1957. Papers in Linguistics: 1934-1951. London, Oxford University Press. L. Gillam, and N. Newbold. 2007. Quality assessment. Deliverable 1.3 of EU eContent project LIRICS, URL:http://lirics.loria.fr/doc_pub/T1.3Deliverable.final.2.pdf, last accessed 28 February 2010. A. C. Graesser, D. S. McNamara, M. M. Louwerse, and Z. Cai. 2004.  Coh-Metrix: Analysis of text on cohe-sion and language. Behavior Research Methods, In-struments, and Computers, 36:193?202. M. Hoey. 1991. Patterns of Lexis in Text. Oxford, OUP. W. Kintsch, E. Kozminsky, W. J. Streby, G. McKoon, and J.M. Keenan. 1975. Comprehension and recall as a function of content variables. Journal of Verbal Learning and Verbal Behavior, 14:196?214. H. D. Kitson. 1921. The mind of the buyer. New York, Macmillan. T. Oakland and H. B. Lane. 2004. Language, reading, and readability formulas: Implications for developing and adapting tests. International Journal of Testing, 4(3):239?252. The Plain English Campaign. Established 1979. URL:http://www.plainenglish.co.uk/, last accessed 28 February 2010. J. Pustejovsky, S. Bergler, P. Anick. 1994. Lexical se-mantic techniques for corpus analysis. Computa-tional Linguistics, 19(2):331?358. F. Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143?178. M. Stuart, M. Dixon, and J. Masterson. 2004. Use of apostrophes by six to nine year old children. Educa-tion Psychology, 24(3): 251?261. 
72
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 5?14,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
?I Don?t Know Where He is Not?: Does Deception Research yet offer a basis for Deception Detectives?   Anna Vartapetiance Lee Gillam Department of Computing Department of Computing Faculty of Engineering & Physical Sciences Faculty of Engineering & Physical Sciences University of Surrey University of Surrey a.vartapetiance@surrey.ac.uk  l.gillam@surrey.ac.uk        Abstract Suppose we wanted to create an intelligent machine that somehow drew its intelligence from large collections of text, possibly involving the processing of collections available on the Web such as Wikipedia. Does past research in deception offer a sufficiently robust basis upon which we might develop a means to filter out texts that are deceptive, either partially or entirely? Could we identify, for example, any deliberately deceptive edits to Wikipedia without consulting the edit history? In this paper, we offer a critical review of deception research. We suggest that there are a range of inconsistencies, contradictions, and other difficulties in recent deception research, and identify how we might begin to address deception research in a more systematic manner. 1 Introduction Deception exists in various forms, and there can be acceptance in society of deceits of various kinds - typically geared towards personal gain (self-deception) or protection from harm. Often termed ?white lies?, these differ significantly from those likely to be of a more harmful nature (?black lies?) ? and here we would include the misrepresentation of science and the misrepresentation as science; the latter is prevalent in, for example, the advertising of cosmetic products. It remains difficult to discern, however, whether the portrayal by an apparently trusted media outlet of a reported survey of 200 students? responses to a question of whether they thought they had hallucinated after drinking 
coffee fits the former or the latter when characterized by the BBC as ?'Visions link' to coffee intake?. Could we rely on existing deception research to enable us to distinguish amongst the presentation of such things on the Web? In this paper, we present a critical review of deception research, seeking to answer the questions outlined above. We first explain our preferred definition of deception, to disentangle deceptions from lies, and then clarify the impact that selection of a specific medium (text) has on the likely nature of deception. We then review the features that researchers tend to focus on as ?cues? that might be used for detecting deception, focus these down to a set as may be detectable in text, and then demonstrate that in treatments of such cues by leading deception researchers there are various inconsistencies, contradictions, and other difficulties. We further consider how we might begin to address these difficulties, such that a more systematic approach might emerge from this research, and what future work might emerge.  2 Defining Deception To understand deception, it is important to establish what we mean by it. Out of various definitions for deception, (e.g. Masip, Garrido & Herrero, 2004; Hall & Pritchard, 1996; Russow, 1986), we settle on Mahon (2007):  ?To intentionally cause another person to have or continue to have a false belief that is truly believed to be false by the person intentionally causing the false belief by bringing about evidence on the basis of which 
5
the other person has or continues to have that false belief.?  This particular definition leads with intent, which offers contrast with unintended actions as might lead to deception, and also allows us to distinguish from the ill-informed (e.g. believing the Earth is flat, the centre of the Universe, and so on). This also covers a deception occurring through a variety of actions or inactions. Some researchers equate lies with deceptions and have a tendency to use both terms interchangeably (Ekman; 1985; Vrij, 2000); we consider lies to be a specialized subgroup of deception and again highlight Mahon (2008):  ?? to make a believed-false statement with the intention that that statement be believed to be true?.  Hence, lies have an essentially narrow scope to specific false statements. For example, deliberately pointing the wrong way without saying which way to go would be a deception, but only becomes a lie through a speech act. Being ?very economical in his information? and hence concealing the truth leads to a deception but not a lie.  Given these differences between deception and lies, it then becomes interesting to see how actions and statements can be constructed in order to bring about such ?false beliefs?.  3 Structure and Media  Just like any other human interaction, deceptive behaviour can be divided into two main groups: planned and unplanned. In planned interactions, people have time to think, reflect and compare situations with past experiences. They know or have time to consider knowing the person who they interact with (DePaulo, 2003). In unplanned interactions, people are not necessarily aware of actions that will happen which might need to be controlled. They are not fully aware of the person they will interact with and cannot guarantee the outcomes. Planned deceits should be harder to detect simply because the deceivers have time to rehearse their words and behaviours in order to present the impression of being truthful, or at least being more compelling.  Moreover, the choice of medium for communication can force the type of interaction. Based on Hancock, Thom-Santelli & Ritchie, 
2004), deceptiveness in media relates to three main elements:  ? Synchronicity: to what extent the medium provides real-time communication. ? Distribution: whether the people who are communicating are in the same physical location or not. ? Recordability: to what extent the medium is automatically recordable. By knowing these, it is possible to argue that synchronicity and unplanned interactions are directly related, so media that are synchronous should be avoided for planned deceits as they give opportunity to discuss whilst deceivers might need time to rehearse their answers so will prefer asynchronous communication ? for example, email.  If we focus on running text as the medium for deception, then while synchronicity and distribution are variable, recordability is certain. This will mean that most of the deceits can be planned well in advance, which could well make their detection somewhat more challenging. On the other hand, social media tends to assume greater degrees of synchronicity and a notionally lower distribution, so deceptions in social media may be more prevalent, not least because there can be less opportunity for planning. The next question, then, is what might be detectable. This brings us to the notion of deception ?cues?. 4 Deception Detection Cues  Possibilities of being able to formulate human deception processes have encouraged experts in many fields such as psychology, sociology, criminology, philosophy and anthropology to study such behaviour and look for cues as might indicate it. Researchers have shown that telling a lie or being engaged in deceptive behaviour is mentally, emotionally and physically more challenging than being truthful (Miller & Stiff, 1993; Zuckerman et al, 1981; Vrij, Edward, & Bull, 2001). It is emotionally challenging because deceivers might experience Fear and Threat (of being caught), Guilt and Shame (of deceiving someone and of having their trust questioned) or even Duping Delight (joy of deceiving someone). It is mentally challenging as deceivers need to create a story that is believable and consistent and try to remember what they are saying just in case they are questioned later (Miller & Stiff, 1993; Vrij, 2000; Zuckerman et 
6
al., 1981; Cody, Marston & Foster, 1984; Vrij, Edward & Bull, 2001). It is physically challenging as deceivers usually attempt to control the physical signs of their deceptive behaviour (Buller & Burgoon 1996; Vrij & Mann, 2004). These attempts can give away a deception or a lie as it is not easy to hide nervousness and fear/guilt, remember lies in detail, and try to manage all of these to make an honest impression at the same time. These will result in behaviours which would be different from truthful actions, giving Cues of Deception.  In principle, almost any aspect of human existence that is involved in any action and behaviour may be carrying a cue to flag up deception; that can be eye movement, choice of words, arm positions or motions, and much more. One or many of these may be involved in a single communication, but some will be more specific to certain types of communication. For example, body language and eye movements are mainly considered in synchronous, non-distributed communication, while the structure of the sentence will be more apparent in recordable, distributed communication such as IMs and emails. Such cues can be readily grouped by the 3Vs:  Visual (Non-verbal): any physical behaviour; reactions, movements, etc in three main groups of Body Acts, Postures and Face. Vocal: elements that accompany verbal communication with two main features involved: Nature of voice (e.g. Tone/ Tension, Pitch) and Rhythm (e.g. number and the length of pauses). Verbal: anything said or written (e.g. wording and structure). However, it is important to note that the physical signs, the visual and vocal, cannot entirely be trusted since specific conditions may lead to similar effects. In certain circumstances, people will be naturally nervous or may feel fear simply because of a situation. For example, in an interview, and in particular in interviews with law enforcement officers, a cue to deception may be out of the normal for that interaction, whilst all parts of the interaction could indicate deception in contrast to everyday interactions (Navarro, 2008). With our interest in detecting deception in text, we focus towards Verbal and in particular Written. Here, the deceiver must make words and patterns of those words do the work, and there is some expectation that this leads to different word usage and language patterns from those that might be considered, somehow, normal.  
5 Verbal Deception Detection Cues  Three main types can be defined for verbal deception: ? Spoken (e.g. face-to-face, audio and video recordings) ? Written (e.g. blogs, emails, testimonies, academic articles) ? Transcripts of spoken (phonetic transcription, orthographic transcription) However, recordings of speech will retain vocal elements which may offer cues, and transcripts may offer surrogates for pauses and retain the speech disfluencies (?ums?, ?ahs?, ?like?, and so on). Written text, then, is possibly hardest to treat as the visual and vocal cues are missing in contrast to spoken and transcripts (Gupta & Skillicorn, 2006). Interestingly, this suggests that Web content could offer ready source material but with the significant challenge in terms of detecting deception in it as the deceiving authors of written content will have the opportunity to plan. Many researchers have investigated the lexical, syntactic, and meta-content features of verbal deception, classifying pattern changes into three main dimensions: (1) Quantity; (2) Quality; and (3) Overall impression. Quantity changes relate to the number of words being used. Quality change focuses on the difference between the word choices but still on a quantitative basis. However, Overall Impression is based on human judgment from deceivers? verbalizations including such elements as friendliness, sounding helpful, serious, uncertain, and so on (DePaulo et al 2003). We discard these cues due to reasons of subjective interpretation - judges (detectors) would need to be trained, and while something seems believable and helpful to one, it may not appear the same to others, and exploring inter-annotator agreement would become a distraction. We focus only on existing measurable cues that should be independent of a judge?s training and so could be used by both humans and machines.  For Quantity and Quality measurements there are various hypotheses, different lists of cues, and even different expected changes. We have focussed more on studies where ideas have gained traction through adoption (citation and derivative exploration) by others. For example, Pennebaker?s research has been adapted based on its style (word-by-word), accuracy and flexibility for both written and spoken text (e.g. Toma & 
7
Hancock, 2010; Little & Skillicorn, 2008; Gupta and Skillicorn, 2006; Newman et al 2003). 5.1 Generalized Cues  DePaulo et al (2003) developed a list of 158 visual, verbal and vocal deception cues, extracted from an analysis of 116 research papers between 1920 and 2001. From this list, we consider just 25 cues to relate to verbal and to be measurable, and these relate to just 10 research papers over that period. The cues include: Response length, Talking time, Cognitive complexity, Unique words, Generalising terms, Self-references, Mutual and group and other references, Word and phrase repetitions, Negative statements and complaints, and Extreme descriptions. As we will show, research since 2001 picks up on several of these cues, and we have been able to use DePaulo?s coding system to cross-reference subsequent papers for our own purposes.   5.2 Frequency-based Cues  A number of researchers appear to make use of Pennebaker?s Linguistic Inquiry and Word Count (LIWC) system to support their experiments and claims (e.g. Gupta & Skillicorn, 2006; Hancock et al, 2004; Keila & Skillicorn, 2005a, b, c). They mention that the cues defining deception according to Pennebaker involve: Self-references: Using first-person singular (e.g. me, I and my) shows speaker ownership of a specific statement or event. This offers a link between the reality and the speaker, and as deceivers haven?t experienced that link they will reduce the use of self-references. Negative words: Emotions such as guilt, shame and fear may be attributed to the deceivers? discomfort (DePaulo et al, 2003) and the effect of negative emotions on the pattern of language is believed to lead to an increase in the use of negative words. Cognitive complexity: As suggested earlier, cognitive complexity increases while deceiving. These effects become apparent in statements in various ways, which directly affects the structure of the text by changes in two main categories. (a) Exclusive words: Statements grounded in reality are more likely to highlight the details, including what happened and related reactions. Deceivers, lacking these details, use fewer exclusive words such as except, but, without and exclude. (b) Motion/action verbs: A decrease in exclusive words can result in an increase in action verbs (e.g. go, lead, walk) while trying to sound more 
assuring and convince others to take actions based on their words. Moreover, cognitively, it is easier to use simple and concrete actions in stating false stories compared to fake evaluations and retaining details.  However, we have so far found little evidence that Pennebaker has proposed cues for deception except for one research paper by Newman, Pennebaker, Bery & Richards (Newman et al, 2003). In that paper, the authors discuss cues previously offered by others (that relate to categories in LIWC) along with the reduction in the number of 3rd person pronouns, which contradicts previous studies such as Knapp et al, (1974). Subsequent authors have referenced such articles ambiguously, which may give the impression that LIWC itself offers the answer, for example, Hancock et al, (2004):   ?[LIWC] was used to create empirically derived statistical profiles of deceptive and truthful communications (Pennebaker et al, 2003),??  and Gupta & Skillicorn (2006):  ?Pennebaker et al have constructed a model (LIWC) (Newman et al, 2003; Pennebaker, Francis & Booth, 2001) for deception based on the frequencies of various classes of words.?  Whilst LIWC can offer analysis of data, when it comes to understanding the behaviours of cues as might indicate deception by ?increase? or ?decrease? in frequency, there is no clear baseline. So, to be able to detect any deception, work would first need to be done in order to (1) establish the frequency ranges for different elements within a specific collection, (2) set thresholds of deception per-collection and per cue, and then (3) manually verify those above and below the deception threshold. Relationship to some collection-specific average is unlikely to readily produce appropriate results. 5.3 Category-based Cues  Burgoon and colleagues have categorized deception cues. However, Burgoon and other researchers have, without much explanation that we can find, varied the number of categories and also reported cues in different categories in different research papers (Burgoon & Qin, 2006; Qin et al 2005; Qin, Burgoon & Nunamaker, 
8
2004; Zhou et al 2004; Zhou, Burgoon & Twitchell, 2003; Zhou et al 2003; Burgoon et al 2003). Indeed, they appear to add, delete, or otherwise emphasise different cues throughout their work. Neither the cues nor the threshold related to their deceptiveness appear stable. A set of cues that have been moved around categories is represented by Black cells in Table 1. Table 1 also shows, in gray, certain inconsistencies amongst these researchers: in Zhou et al (2004), the number of words, sentences and the 
emotiveness index show an increase in cases of deception, but in Burgoon et al (2003) and Zhou et al (2003) all three are shown to decrease.  Burgoon and colleagues are not alone in offering a categorization; Pennebakers? LIWC categories would be related, modulo terminological and category variation. However, indications of expected values for such cues remain elusive and we only have information that some may rise whilst others may fall. Cues (1) (2) (3) (4) (5) Word ** Q +** Q +** Q -** Q -** Q Sentence ** Q +** Q +** Q -** Q -** Q Modifiers -** U +** U ** Q ** Q -- -- First-person singular  -- -- -** V +** V -** V -- -- 2nd person pronouns -- -- -- -- -- -- 3rd person pronouns -- -- -** U ** V ** V -- -- Temporal details ** S -- -- -- -- Spatial details ** S +** S -- -- -** S -- -- Perceptual information -- -- +** S -- -- -** S -- -- Affective terms  ** A -- -- -- -- -- -- -** S Positive -- -- +** S +** A -** S -- -- Negative -- -- +** S +** A +** S -- -- Emotiveness index  -- -- +** E -- -- +** E -** S Lexical diversity  -** D -** D -** D -** D -- -- Redundancy  -** D -** D ** D +** D -- -- Passive voice ** V + ** V ** V + ** V -- -- Modal verbs -** U +** U ** V +** V -- -- Uncertainty +*** -- -** U ** V +** V -- -- Objectification -- -- -** V +** V ** V -- -- Typo errors -*** -- +** I +** I +** I -- -- Quantity = Q; Complexity = C; Specificity = S; Affect = A; Activation /Expressiveness = E; Diversity = D; Verbal non-immediacy = V; Informality = I; Uncertainty = U; Vocabulary Complexity = VC; Grammatical Complexity = GC;  (1) Qin et al 2005 (2) Zhou et al 2004 (3) Zhou, Burgoon & Twitchell, 2003 (4) Zhou et al 2003 (5) Burgoon et al 2003 Gray= inconsistency in expected results; in Black= inconsistency in categories  [**] included, [***] mentioned but not highlighted  Table 1: Contradictions in Cues and Expectation  5.4 Evaluating the Cues  Despite commonalities in what can be and is being studied amongst DePaulo, Pennebaker and Burgoon, it is apparent that there is not yet a clear set of cues with predefined expected values that could be used for detecting verbal deception. However, without clear descriptions of how to interpret results it is also possible that results could have been misreported. To address this, we undertook a number of small experiments ? mainly geared around repetition of previous reported experiments ? to try to understand the behaviour of deception cues. 
Our experiments involve analysis of the BBC article ?'Visions link' to coffee intake? mentioned previously (BBC, 2009) with cues identified by Pennebaker, Mehl, and Niederhoffer, 2003), tests on academic work (we used 100 scientific abstracts1, which we have no reason to believe are deceptive), and attempting to repeat an analysis of the Enron email corpus including the emails of the executives (Keila, and Skillicorn, 2005a, b, c). The latter of these is made all the more difficult by offering three differing numbers of emails for the analysis without                                                            1 MuchMore Springer Bilingual Corpus, Available at: http://muchmore.dfki.de/resources1.htm ?
9
details of how to obtain such a number from the full collection. Unfortunately, experiments all tended to support the idea that it would be hard to detect deception ?in the wild? reliably, in part because deceptive texts may ?hide? amongst non-deceptive. We can see how this might happen with a simple experiment using the online version of LIWC. We use the 7 LIWC categories, scaled by the maximum of each, for the 100 texts from the MuchMore Springer corpus. We then select the closest matching text (Nearest) from the first 10 to the coffee article (Coffee), and note that values for 5 of these 7 are already close together with differences for social words and cognitive words more marked, but still well within the ranges. A broad grain such as this is unlikely to be revealing.  
  Figure 1: How a deceptive article might hide amongst scientific articles 6 Readability and Deception Given variation in cues and expectations of values for those cues, a question arises of whether it is possible to provide some common, relatively well understood, and static baseline from which it would be possible to consider the variations in the values.  Interestingly, various cues used in relation to deception also feature in Readability research, so might Readability scores offer such a baseline for comparison? Daft and Lengel (1984) argue that more ambiguous texts are more likely to contain deception, and such a claim has been supported in relation to fraudulent financial reports that contained more complex words, while truthful reports attained scores indicating better readability (Moffit and Burns 2009). Historically, readability measures have been used to indicate the proportion of the population that would be able to understand a given text, but it has become apparent that word familiarity, 
cognitive load/complexity, cohesion, and other features of text contribute to its readability (Newbold and Gillam, 2010, Gray and Leary, 1935) and are also features considered in deception research.  Given the apparent overlap, we consider whether we might use readability measures to point more reliably to deceptive texts.  Table 2 shows the cues covered by Gray and Leary (1935) for readability which are also studied as verbal cues for deception, along with expected direction of change in relation to readability and to deception (direction for the latter as suggested in e.g. Burgoon et al, 2003; Qin, Burgoon & Nunamaker, 2004) 2. Not only is there an overlap with readability, but there seems to be a clear suggestion that more difficult texts are more likely to be deceptive.  Could such a clear relationship hold in practice? What would happen with articles such as ?'Visions link' to coffee intake? or the 100 scientific abstracts? Scientific texts, and texts offering a misrepresentation of or as science, will probably both contain Big words, likely Nouns, may contain Rare words in contrast to general language, and possibly have relatively complex sentences. The writing style is also likely to impact on pronoun count. So systematic differences amongst such values might offer an indication of deception.   Cues R D Big words - + Nouns - * + Verbs * + Rare words - + Sentence complexity - - Number of first person pronouns + -  Number of second person pronouns + - Number of third person pronouns + - Average syllables per word and sentence - + * may vary depending of the structure of the sentence and the words before and after them  Table 2: Readability features and their relationship to Deception  Also, the online version of LIWC has a category for Big words (those with more than 6 letters). The values from this follow a similar pattern to that of Grade level for readability. For                                                            2 There are contradictions for expectancy rate for these cues so chosen expectations might conflict with other theories.  
10
Coffee against 10 Springer articles, dividing Big word by Grade level provides the lowest value for the Coffee article. So, it is possible ? indicatively, but not conclusively - that the ratio of Big words to Grade level could offer an indication.  
  Figure 2: How Big words and Grade level tend towards indicating each other.  To explore how such a relationship might hold in practice, we consider a small experiment comparing essentially the same core content, but which results in different readability scores. A document that has been (supposedly equivalently) translated several times, albeit with particular variations, offers such a basis, and a good example of this is the Bible. We selected the Gospel of John (because it contains first person pronouns) from the following four Bibles3; ? New International Version (NIV) ? New King James Bible (NKJB)  ? King James Bible (KJM) and ? New America Standard Bible (NASB) We are not suggesting here that the Gospel of John is general representative for the English language, nor that it should be seen to be deceptive per se, but as translations from a single source it should help to demonstrate any effect.   We choose four Pennebaker categories for our comparison. Since we have yet to find complete lists in Pennebaker?s research, and since Newman et al (2003) does not offer up full lists of words, we make use of the list of 86 words cited by Little & Skillicorn (2008) as being from Pennebaker. If all versions of the Gospel of John essentially contain the same content, and if we can use these categories for ranking purposes, we                                                            3Accessed from link below for stability in structure and sentencing http://www.biblegateway.com  
might expect to either see equal ranks for all four cases or to have the old versions (KJM and NASB) flagged up with higher ranks of deceptiveness.  Table 3 shows the scores for First Person (FP), Negative Words (NW), Exclusive Words (EW) and Motion Verbs (MV) as well as Grade Level and Reading ease score which shows, in terms of readability, NIV and NKJB are the better.    FP NW EW MV Grade Level Reading ease4 NIV 1.96 0.24 0.53 0.47 6.48 78.11 NKJB 3.44 0.12 1.00 0.48 7.24 77.21 KJB 2.29 0.28 0.69 0.37 7.78 74.48 NASB 2.04 0.23 0.65 0.44 8.38 73.88 Newman et al (2003): Light Gray Little and Skillicorn (2008): Dark Gray  Table 3: Variables for Deception and Readability for Gospel of John in 4 Bibles  For Newman et al (2003), the deceptive text will have: ? Decreased frequency of first person singular pronouns ? NIV ? Increased frequency of negative emotion words ? KJB ? Decreased frequency of exclusive words ?NIV ? Increased frequency of action verbs ?NKJB, NIV On the other hand, Little and Skillicorn (2008) expect a deceptive text should show:  ? Increased frequency of first person singular pronouns ? NKJB ? Increased frequency of negative emotion words ? KJB ? Increased frequency of exclusive words ?NKJB ? Increased frequency of action verbs ?NKJB, NIV Interestingly, these results suggest that the New International Version (NIV) and New King James Bible (NKJB) score higher on deception despite both having higher readability values. These results contradict what we would expect in relation to readability, further underlining the                                                            4 Readability values from: http://www.online-utility.org/english/readability_test_and_improve.jsp    
11
difficulty in relying entirely on the existing literature and leading us to question whether even readability offers gain at this grain.  7 Further critique Analysis and experiments presented above suggest that difficulties emerge from present considerations of cues of deception ? at least in relation to verbal deception. However, it is unclear whether this is a consequence of how the cues are being treated, or whether there are other biases which have a telling effect. In much of this research, conclusions have tended to be drawn on specific datasets, many of which are not readily available for inspection or use in repeat experiments by others.    The datasets were usually collected in one of three ways:  Role Playing: some are asked to deceive others (e.g. Hancock et al, 2005; Burgoon et al, 2003; Qin et al, 2005; Qin, Burgoon & Nunamaker, 2004). Diary Keeping: individuals are asked to document their own interactions (e.g. DePaulo et al, 1996; Hancock et al, 2009; Hancock, Thom-Santelli and Ritchie, 2004). In this type of study, participants take time to document, once per day, their lies and to self score them based on dimensions such as seriousness, feelings while lying, and fear of getting caught. Obtained as-is: (e.g. Keila, and Skillicorn, 2005a, b, c). Most such studies adopt Pennebaker?s approach. This means that any classificatory thresholds have to be manually set and evaluated, via the means of the Human Eyeball.  All three approaches suffer from potential experimental effects. For the first two, it would be important to control for the Hawthorne effect which highlights that ?observation and studies can change the behaviour of the participants? regardless of whether they should have really changed anything specifically in diary based studies (Franke, 1978; Jones, 1992). We believe during such studies peoples? behaviours might change, intentionally or otherwise. This might be because they become more cautious about perceptions of them by the researchers, want to avoid fear, shame, and so on, or feel uncomfortable with undertaking or documenting such an act. Indeed, the researchers may even be being deceived about the deceptions by the subjects. The third approach leaves the decision regarding actual deceptiveness of the text or 
statement open to the possibility that the researcher has been ?primed by expectations?. (Doyen et al, 2012). 8 Conclusion and Future Work This paper has outlined a critical review of previous research in deception detection in order to assess whether it is possible to create deception detection. On present evidence, whilst there may be various important findings, there are too many areas open to question to believe that such a system could readily be constructed. We still believe that previous deception detection research has a significant role to play, but many of the difficulties outlined in this paper need to be addressed first. Essentially, this requires a more systematic approach towards both datasets and treatment of cues. The public availability of deception-bearing texts covering different text types and genres would offer an ideal basis for such an approach, and a similar rigour in identifying cues tested, following DePaulo, would be highly beneficial. From this, it may be possible to identify specific cues as worth study in certain genres, whilst of little interest in others ? irrespective of their relative frequency of use. In absence of this, in our own near-future work we intend to explore the extent to which deception cues have also featured in tasks of plagiarism detection. Here, the datasets of PAN, and in particular as relates to authorship attribution and intrinsic plagiarism detection are of interest. Since the act of plagiarism is a deliberate attempt to deceive, such collections ? albeit of a synthetic nature - offer us ready grounds for repeatable explorations and might lead to further insights into the general nature of the cues themselves. 9 References  BBC, (2009). Visions link' to coffee intake. BBC News. Retrieved 10.0d.2011 from http://news.bbc.co.uk/1/hi/health/7827761.stm Buller, D.B. and Burgoon, J.K. (1996). Interpersonal Deception Theory. Communication Theory, 6, 203-242. Burgoon, J.K., Blair, J.P., Qin, T., & Nunamaker, J.F., Jr. (2003). Detecting Deception through Linguistic Analysis. Proceedings of First NSF/NIJ Symposium on Intelligence and Security Informatics (ISI), June 2-3, 2003, Tucson, AZ, 91-101.  
12
Burgoon, J.K. & Qin, T. (2006). The Dynamic Nature of Deceptive Verbal Communication. Journal of Language and Social Psychology, 25(1), 76-96. Cody, M.J., Marston, P.J., & Foster, M. (1984). Deception: Paralinguistic and Verbal Leakage. In Bostrom, R.N. and Westley, B.H. (Eds.). Communication Yearbook 8. Beverly Hills: Sage. 464-490. Daft, R.L. & Lengel, R.H. (1984), Information Richness: a New Approach to Managerial Behavior and Organizational Design. In Cummings, L.L. and Staw, B.M. (Eds.). Research in organizational behaviour. 6, Homewood, IL: JAI Press, 191-233. DePaulo, B.M., Lindsay, J.J., Malone, B.E., Muhlenbruck, L., Charlton, K., & Cooper, H. (2003). Cues to Deception. Psychological Bulletin, 129(1), 74-118. Doyen S , Klein O , Pichon C-L, & Cleeremans A ,  (2012) Behavioral Priming: It's All in the Mind, but Whose Mind? PLoS ONE 7(1): e29081. doi:10.1371/journal.pone.0029081  Ekman, P. (1985). Telling lies, Clues to Deceit in the Marketplace, Politics, and Marriage. New York: W.W. Norton & Company. Franke R.C. & Kaul J.D. (1978). The Hawthorne Experiments: First Statistical Interpretation. American Sociological Review, 43(5), 623-643. Gray, W.S. & Leary, B (1935). What Makes a Book Readable. Chicago: Chicago University Press. Gupta, S. & Skillicorn, D. (2006). Improving a Textual Deception Detection Model, Proceedings of the 2006 Conference of the Center for Advanced Studies on Collaborative Research, October 16-19, 2006, Toronto, Canada, 1-4. Hall, H. V. & Pritchard, D.A. (1996). Detecting Malingering and Deception. Forensic Distortion Analysis (FDA). Boca Raton, FL: St. Lucie Press. Hall, H. V. & Pritchard, D.A. (1996). Detecting Malingering and Deception. Forensic Distortion Analysis (FDA). Boca Raton, FL: St. Lucie Press. Hancock, J.T., Birnholtz, J., Bazarova, N., Guillory, J., Amos, B., & Perlin, J. (2009). Butler Lies: Awareness, Deception and Design. Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI 2009).  Hancock, J.T., Curry, L., Goorha, S. & Woodworth, M.T. (2004). Lies in Conversation: An Examination of Deception Using Automated Linguistic Analysis. Proceedings of Annual Conference of the Cognitive Science Society, 26, 534-540. Hancock, J. T., Curry, L., Goorha, S., & Woodworth, M.T. (2005). Automated linguistic analysis of 
deceptive and truthful synchronous computer-mediated communication. Proceedings of the 38th Annual Hawaii International Conference on System Sciences (HICSS-38), Los Alamitos, CA: IEEE Press. Hancock, J.T., Thom-Santelli, J. & Ritchie, T. (2004). Deception and Design: The Impact of Communication Technology on Lying Behaviour. Proceedings of the Conference on Human Factors in Computing Systems (ACM SIGCHI), 129-134. Jones S.R (1992). Was There a Hawthorne Effect? American Journal of Sociology, 98(3), 451-468. Keila, P.S. & Skillicorn, D.B. (2005a). Detecting Unusual and Deceptive Communication in Email. Centers for Advanced Studies Conference, 17-20.  Keila, P.S. & Skillicorn, D.B. (2005b). Detecting unusual email communication. Proceedings of the 2005 Conference of the Centre for Advanced Studies on Collaborative Research, 117-125. Keila, P.S. & Skillicorn, D.B. (2005c). Structure in the Enron Email Dataset. Computational and Mathematical Organization Theory, 11(3), 183-199. Knapp, M.L., Hart, R.P. & Dennis, H.S. (1974). An exploration of deception as a communication construct. Human Communication Research, 1, 15-29. Little, A. & Skillicorn, B. (2008). Detecting Deception in Testimony. Proceeding of IEEE International Conference of Intelligence and Security Informatics (ISI 2008), June 17 - 20, 2008, Taipei, Taiwan, 13-18. Mahon, J.E. (2007). A Definition of Deceiving. International Journal of Applied Philosophy, 21, 181-194. Mahon, J. E. (2008). Two Definitions of Lying.  International Journal of Applied Philosophy, 22(2), 211-230. Moffitt, K. and Burns, M.B. (2009). What Does that Mean? Investigating Obfuscation and Readability Cues as Indicators of Deception in Fraudulent Financial Reports. Proceedings of Americas Confernece on Information Systems (AMCIS 2009) , 399. Masip, J., Garrido, E. & Herrero, C. (2004).Defining Deception, Anales de Psicologia, 20(1), 147-171. Miller, G.R. & Stiff, J.B. (1993). Deceptive Communication. Newbury Park, CA: Sage. Navarro, J. (2008). ?What Every BODY is Saying: An Ex-FBI Agent's Guide to Speed-Reading People.? New York. Harper-Collins.  
13
Newbold, N. & Gillam, L. (2010). The Linguistics of Readability: The Next Step for Word Processing. Workshop on Computational Linguistics and Writing: Writing Processes and Authoring Aids (CLandW 2010). June 6, 2010, Los Angles, 65-72. Newman, M.L., Pennebaker, J.W., Berry, D.S. & Richards, J.M. (2003). Lying Words: Predicting Deception from Linguistic Styles, Personality and Social Psychology Bulletin, 29(5), 665-675. PAN, (2011), PAN 2011 Lab Uncovering Plagiarism, Authorship and Social Software Misuse, September 19- 22, 2011, Amsterdam. Pennebaker, J.W.,  Francis M.E. & Booth, R.J. (2001) Linguistic inquiry and word count (LIWC). Erlbaum Publishers. Pennebaker, J.W., Mehl, M. & Niederhoffer, K. (2003). Psychological Aspects of Natural Language Use: Our Words, Our Selves. Annual Review of Psychology, 54(1), 547-577. Qin, T., Burgoon, J.K. & Nunamaker, J.F., Jr. (2004). An Exploratory Study on Promising Cues in Deception Detection and Application of Decision Trees. Proceedings of the 37th Hawaii International Conference on System Sciences, January 5-8, 2004, Waikoloa, HI, 23-32. Qin, T., Burgoon, J. K., Blair, J. P., & Nunamaker, J. F. (2005). Modality Effects in Deception Detection and Applications in Automatic-Deception-Detection. Proceedings of the 38th Annual Hawaii International Conference on System Sciences, 23-23. Tausczik, Y.R. & Pennebaker, J.W. (2010). The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods. Journal of Language and Social Psychology, 29, 24-54. Vrij, A. (2000), Detecting Lies and Deceit: The Psychology of Lying and its Implications for Professional Practice. Chichester: John Wiley and Sons. Vrij, A., Edward, K. & Bull, R. (2001). Stereotypical Verbal and Nonverbal Responses while Deceiving Others. Personality and Social Psychology Bulletin, 27, 899-909. Vrij, A., & Mann, S. (2004). Detecting Deception: The Benefit of Looking at a Combination of Behavioral, Auditory and Speech Content Related Cues in a Systematic Manner. Group Decision and Negotiation (special deception issue), 13, 61-79. Zhou, L., Burgoon, J. K., & Twitchell, D. P. (2003). A longitudinal analysis of language behavior of deception in e-mail. Proceedings of Intelligence and Security Informatics, 2665, 102-110. Zhou, L., Burgoon, J. K. Zhang, D. & Nunamaker, J. F., Jr. (2004). Language Dominance in 
Interpersonal Deception in Computer-Mediated Communication, Computers in Human Behavior, 20(3), 381-402. Zhou, L., Twitchell, D.P., Tiantian, Q., Burgoon, J.K. & Nunamaker, J.F., Jr.  (2003). An Exploratory Study into Deception Detection in Text-Based Computer-Mediated Communication. Proceedings of the 36th Annual Hawaii International Conference on System Sciences, January 6-9, 2010, Waikoloa, HI, 10-19. Zuckerman, M, DePaulo, B.M. & Rosenthal, R. (1981). Verbal and Nonverbal Communication of Deception, In Berkowitz, L.(Ed.). Advances in Experimental Social Psychology, 14, 1-59. 
14
