Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 712?724,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Using Syntactic and Semantic Structural Kernels for
Classifying Definition Questions in Jeopardy!
Alessandro Moschitti? Jennifer Chu-Carroll? Siddharth Patwardhan?
James Fan? Giuseppe Riccardi?
?Department of Information Engineering and Computer Science
University of Trento, 38123 Povo (TN), Italy
{moschitti,riccardi}@disi.unitn.it
?IBM T.J. Watson Research Center P.O. Box 704, Yorktown Heights, NY 10598, U.S.A.
{jencc,siddharth,fanj}@us.ibm.com
Abstract
The last decade has seen many interesting ap-
plications of Question Answering (QA) tech-
nology. The Jeopardy! quiz show is certainly
one of the most fascinating, from the view-
points of both its broad domain and the com-
plexity of its language. In this paper, we study
kernel methods applied to syntactic/semantic
structures for accurate classification of Jeop-
ardy! definition questions. Our extensive em-
pirical analysis shows that our classification
models largely improve on classifiers based on
word-language models. Such classifiers are
also used in the state-of-the-art QA pipeline
constituting Watson, the IBM Jeopardy! sys-
tem. Our experiments measuring their impact
on Watson show enhancements in QA accu-
racy and a consequent increase in the amount
of money earned in game-based evaluation.
1 Introduction
Question Answering (QA) is an important research
area of Information Retrieval applications, which re-
quires the use of core NLP capabilities, such as syn-
tactic and semantic processing for a more effective
user experience. While the development of most
existing QA systems are driven by organized eval-
uation efforts such as TREC (Voorhees and Dang,
2006), CLEF (Giampiccolo et al, 2007), and NT-
CIR (Sasaki et al, 2007), there exist efforts that
leverage data from popular quiz shows, such as Who
Wants to be a Millionaire (Clarke et al, 2001; Lam
et al, 2003) and Jeopardy! (Ferrucci et al, 2010), to
demonstrate the generality of the technology.
Jeopardy! is a popular quiz show in the US which
has been on the air for 27 years. In each game, three
contestants compete for the opportunity to answer
60 questions in 12 categories of 5 questions each.
Jeopardy! questions cover an incredibly broad do-
main, from science, literature, history, to popular
culture. We are drawn to Jeopardy! as a test bed
for open-domain QA technology due to its broad do-
main, complex language, as well as the emphasis on
accuracy, confidence, and speed during game play.
While the vast majority of Jeopardy! questions
are factoid questions, we find several other types
of questions in the Jeopardy! data, which can ben-
efit from specialized processing in the QA system.
The additional processing in these questions com-
plements that of the factoid questions to achieve im-
proved overall QA performance. Among the various
types of questions handled by the system are defini-
tion questions shown in the examples below:
(1) GON TOMORROW: It can be the basket
below a hot-air balloon or a flat-bottomed
boat used on a canal (answer: gondola);
(2) I LOVE YOU, ?MIN?: Overbearing (an-
swer: domineering);
(3) INVEST: From the Latin for ?year?, it?s
an investment or retirement fund that pays
out yearly (answer: an annuity)
where the upper case text indicates the Jeop-
ardy! category for each question1.
Several characteristics of this class of questions
warrant special processing: first, the clue (question)
1A Jeopardy! category indicates a theme is common among
its 5 questions.
712
often aligns well with dictionary entries, making
dictionary resources potentially effective. Second,
these clues often do not indicate an answer type,
which is an important feature for identifying cor-
rect answers in factoid questions (in the examples
above, only (3) provided an answer type, ?fund?).
Third, definition questions are typically shorter in
length than the average factoid question. These dif-
ferences, namely the shorter clue length and the lack
of answer types, make the use of a specialized ma-
chine learning model potentially promising for im-
proving the overall system accuracy. The first step
for handling definitions is, of course, the automatic
separation of definitions from other question types,
which is not a simple task in the Jeopardy! domain.
For instance, consider the following example which
is a variation of (3) above:
(4) INVEST: From the Latin for ?year?,
an annuity is an investment or retirement
fund that pays out this often (answer:
yearly)
Even though the clue is nearly identical to (3), the
clue does not provide a definition for the answer
yearly, although at first glance we may have been
misled. The source of complexity is given by the fact
that Jeopardy! clues are not phrased in interrogative
form as questions typically are. This complicates the
design of definition classifiers since we cannot di-
rectly use either typical structural patterns that char-
acterize definition/description questions, or previous
approaches, e.g. (Ahn et al, 2004; Kaisser and Web-
ber, 2007; Blunsom et al, 2006). Given the com-
plexity and the novelty of the task, we found it use-
ful to exploit the kernel methods technology. This
has shown state-of-the-art performance in Question
Classification (QC), e.g. (Zhang and Lee, 2003;
Suzuki et al, 2003; Moschitti et al, 2007) and it
is very well suited for engineering feature represen-
tations for novel tasks.
In this paper, we apply SVMs and kernel meth-
ods to syntactic/semantic structures for modeling
accurate classification of Jeopardy! definition ques-
tions. For this purpose, we use several levels of lin-
guistic information: word and POS tag sequences,
dependency, constituency and predicate argument
structures and we combined them using state-of-
the-art structural kernels, e.g. (Collins and Duffy,
2002; Shawe-Taylor and Cristianini, 2004; Mos-
chitti, 2006). The extensive empirical analysis of
several advanced models shows that our best model,
which combines different kernels, improves the F1
of our baseline model by 67% relative, from 40.37
to 67.48. Surprisingly, with respect to previous find-
ings on standard QC, e.g. (Zhang and Lee, 2003;
Moschitti, 2006), the Syntactic Tree Kernel (Collins
and Duffy, 2002) is not effective whereas the ex-
ploitation of partial tree patterns proves to be es-
sential. This is due to the different nature of Jeop-
ardy! questions, which are not expressed in the usual
interrogative form.
To demonstrate the benefit of our question clas-
sifier, we integrated it into our Watson by coupling
it with search and candidate generation against spe-
cialized dictionary resources. We show that in end-
to-end evaluations, Watson with kernel-based defi-
nition classification and specialized definition ques-
tion processing achieves statistically significant im-
provement compared to our baseline systems.
In the reminder of this paper, Section 2 describes
Watson by focusing on the problem of definition
question classification, Section 3 describes our mod-
els for such classifiers, Section 4 presents our exper-
iments on QC, whereas Section 5 shows the final im-
pact on Watson. Finally, Section 6 discusses related
work and Section 7 derives the conclusions.
2 Watson: The IBM Jeopardy! System
This section gives a quick overview of Watson and
the problem of classification of definition questions,
which is the focus of this paper.
2.1 Overview
Watson is a massively parallel probabilistic
evidence-based architecture for QA (Ferrucci et
al., 2010). It consists of several major stages for
underlying sub-tasks, including analysis of the
question, retrieval of relevant content, scoring and
ranking of candidate answers, as depicted in Figure
1. In the rest of this section, we provide an overview
of Watson, focusing on the task of answering
definitional questions.
Question Analysis: The first stage of the pipeline,
it applies several analytic components to identify
key characteristics of the question (such as answer
713
Figure 1: Overview of Watson
type, question classes, etc.) used by later stages of
the Watson pipeline. Various general purpose NLP
components, such as a parser and named entity de-
tector, are combined with task-specific modules for
this analysis.
The task-specific analytics include several QC
components, which determine if the question be-
longs to one or more broad ?question classes?.
These question classes can influence later stages of
the Watson pipeline. For instance, a question de-
tected as an abbreviation question can invoke spe-
cialized candidate generators to produce possible ex-
pansions of the abbreviated term in the clue. Simi-
larly, the question classes can impact the methods
for answer scoring and the machine learning mod-
els used for ranking candidate answers. The focus
of this paper is on the definition class, which is de-
scribed in the next section.
Hypothesis Generation: Following question anal-
ysis, the Watson pipeline searches its document col-
lection for relevant documents and passages that are
likely to contain the correct answer to the question.
This stage of the pipeline generates search queries
based on question analysis results, and obtains a
ranked list of documents and passages most relevant
to the search queries. A variety of candidate gen-
eration techniques are then applied to the retrieved
results to produce a set of candidate answers.
Information obtained from question analysis can
be used to influence the search and candidate gener-
ation processes. The question classes detected dur-
ing question analysis can focus the search towards
specific subsets of the corpus. Similarly, during can-
didate generation, strategies used to generate the set
of candidate answers are selected based on the de-
tected question classes.
Hypothesis and Evidence Scoring: A wide variety
of answer scorers are then used to gather evidence
supporting each candidate answer as the correct an-
swer to the given question. The scorers include both
context dependent as well as context independent
scorers, relying on various structured and unstruc-
tured resources for their supporting evidence.
Candidate Ranking: Finally, machine learning
models are used to weigh the gathered evidence and
rank the candidate answers. The models generate a
ranked list of answers each with an associated con-
fidence. The system can also choose to refrain from
answering a question if it has low confidence in all
candidates. This stage of the pipeline employs sev-
eral machine learning models specially trained to
handle various types of questions. These models are
trained using selected feature sets based on question
classes and candidate answers are ?routed? to the
appropriate model according to the question classes
detected during question analysis.
2.2 Answering Definition Questions
Among the many question classes that Watson iden-
tifies and leverages for special processing, of partic-
ular interest for this paper is the class we refer to
as definition questions. These are questions whose
clue texts contain one or more definitions of the cor-
rect answer. For instance, in example (3), the main
clause in the question corresponds to a dictionary
definition of the correct answer (annuity). Looking
up this definition in dictionary resources could en-
able us to answer this question correctly and with
high confidence. This suggests that special process-
714
ing of such definition questions could allow us to
hone in on the correct answer through processes dif-
ferent from those used for other types of questions.
This paper explores strategies for definition ques-
tion processing to improve overall question answer-
ing performance. A key challenge we have to ad-
dress is that of accurate recognition of such ques-
tions. Given an input question the Watson question
analysis stage uses a definition question recognizer
to detect this specific class of questions. We explore
several approaches for recognition, including a rule
based approach and a variety of statistical models.
Questions that are recognized as definition ques-
tions invoke search processes targeted towards
dictionary-like sources in our system. We use a va-
riety of such sources, such as standard English dic-
tionaries, Wiktionary, WordNet, etc. After gather-
ing supporting evidence for candidate answers ex-
tracted from these sources, our system routes the
candidates to definition-specific candidate ranking
models, which have been trained with selected fea-
ture sets.
The following sections present a description and
evaluation of our approach for identifying and an-
swering definition questions.
3 Kernel Models for Question
Classification
Previous work (Zhang and Lee, 2003; Suzuki et al,
2003; Blunsom et al, 2006; Moschitti et al, 2007)
as shown that syntactic structures are essential for
QC. Given the novelty of both the domain and the
type of our classification items, we rely on kernel
methods to study and design effective representa-
tions. Indeed, these are excellent tools for auto-
matic feature engineering, especially for unknown
tasks and domains. Our approach consists of using
SVMs and kernels for structured data applied to sev-
eral types of structural lexical, syntactic and shallow
semantic information.
3.1 Tree and Sequence Kernels
Kernel functions are implicit scalar products be-
tween data examples (i.e. questions in our case)
in the very high dimensional space of substructures,
where each of the latter is a component of the im-
plicit vectors associated with the examples.
ROOT
SBARQ
WHADVP
WRB
When
S
VP
VBN
hit
PP
IN
by
NP
NNS
electrons
,
,
NP
DT
a
NN
phosphor
VP
VBZ
gives
PRP
RP
off
NP
NP
JJ
electromagnetic
NN
energy
PP
IN
in
NP
DT
this
NN
form
1
Figure 2: Constituency Tree
PASS
P
A1
phosphor
A0
electron
PR
hit
P
PR
energy
A2
electromag.
A1
phosphor
P
A1
energy
PR
give
AM-TMP
hit
A0
phosphor
ROOT
VBZ
OBJ
NN
NMOD
IN
PMOD
NN
formNMOD
DT
this
in
energyNMOD
JJ
electromag.
PRT
RP
off
givesSBJ
NN
phosphorNMOD
DT
a
P
,
TMP
VBN
LGS
IN
PMOD
NNS
electrons
by
hitTMP
WRB
when
1
Figure 3: Dependency Tree
negative mistake STK, ok PTK
NP
ADJP
JJ
conceited
CC
or
JJ
arrogant
NP
NN
meaning
NN
adjective
NN
5-letter
NN
fowl
positive mistake STK, ok PTK
NP
VP
PP
NP
NN
field
VBG
playing
DT
a
IN
on
VBN
used
NP
NN
grass
JJ
green
JJ
artificial
NP
VP
PP
NP
NN
canal
DT
a
IN
on
VBN
used
NP
NN
boat
JJ
flat-bottomed
DT
a
PASS
P
A1
phosphor
A0
electron
PR
hit
P
PR
energy
AM-MNR
electromag.
A1
phosphor
P
A1
energy
PR
give
AM-TMP
hit
A0
phosphor
1
Figure 4: A tree encoding a Predicate Argument Structure Set
Although several kernels for structured data have
been developed (see Section 6), the main distinc-
tions in terms of feature spaces is given by the fol-
lowing three different kernels:
? Sequence Kernels (SK); we implemented the
discontinuous string kernels described in (Shawe-
Taylor and Cristianini, 2004). This allows for rep-
resenting a string of symbols in terms of its possi-
ble substrings with gaps, i.e. an arbitrary number of
symbols can be skipped during the generation of a
substring. The symbols we used in the sequential de-
scriptions of questions are words and part-of-speech
tags (in two separate sequences). Consequently, all
possible multiwords with gaps are features of the im-
plicitly generated vector space.
715
? Syntactic Tree Kernel (STK) (Collins and Duffy,
2002) applied to constituency parse trees. This gen-
erates all possible tree fragments as features with
the conditions that sibling nodes from the original
trees cannot be separated. In other words, substruc-
tures are composed by atomic building blocks cor-
responding to nodes along with all their direct chil-
dren. These, in case of a syntactic parse tree, are
complete production rules of the associated parser
grammar2.
? Partial Tree Kernel (PTK) (Moschitti, 2006) ap-
plied to both constituency and dependency parse
trees. This generates all possible tree fragments, as
above, but sibling nodes can be separated (so they
can be part of different tree fragments). In other
words, a fragment is any possible tree path, from
whose nodes other tree paths can depart. Conse-
quently, an extremely rich feature space is gener-
ated. Of course, PTK subsumes STK but sometimes
the latter provides more effective solutions as the
number of irrelevant features is smaller as well.
When applied to sequences and tree structures, the
kernels discussed above produce many different
kinds of features. Therefore, the design of appro-
priate syntactic/semantic structures determines the
representational power of the kernels. Hereafter, we
show the models we used.
3.2 Syntactic Semantic Structures
We applied the above kernels to different structures.
These can be divided in sequences of words (WS)
and part of speech tags (PS) and different kinds of
trees. For example, given the non-definition Jeop-
ardy! question:
(5) GENERAL SCIENCE: When hit by elec-
trons, a phosphor gives off electromag-
netic energy in this form. (answer: light
or photons),
we use the following sequences:
WS: [when][hit][by][electrons][,][a][phosphor][gives]
[off][electromagnetic][energy][in][this][form]
PS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in]
[dt][nn]
Additionally, we use constituency trees (CTs), see
2From here the name syntactic tree kernels
Figure 2 and dependency structures converted into
the dependency trees (DTs), e.g. shown in Figure
3. Note that, the POS-tags are central nodes, the
grammatical relation label is added as a father
node and all the relations with the other nodes are
described by means of the connecting edges. Words
are considered additional children of the POS-tag
nodes (in this case the connecting edge just serves
to add a lexical feature to the target POS-tag node).
Finally, we also use predicate argument structures
generated by verbal and nominal relations accord-
ing to PropBank (Palmer et al, 2005) and NomBank
(Meyers et al, 2004). Given the target sentence, the
set of its predicates are extracted and converted into
a forest, then a fake root node, PAS, is used to con-
nect these trees. For example, Figure 4 illustrates a
Predicate Argument Structures Set (PASS) encoding
two relations, give and hit, as well as the nominaliza-
tion energy along with all their arguments.
4 Experiments on Definition Question
Classification
In these experiments, we study the role of kernel
technology for the design of accurate classification
of definition questions. We build several classifiers
based on SVMs and kernel methods. Each classi-
fier uses advanced syntactic/semantic structural fea-
tures and their combination. We carry out an exten-
sive comparison in terms of F1 between the different
models on the Jeopardy! datasets.
4.1 Experimental Setup
Corpus: the data for our QC experiments consists
of a randomly selected set of 33 Jeopardy! games3.
These questions were manually annotated based on
whether or not they are considered definitional. This
resulted in 306 definition and 4964 non-definition
clues. Each test set is stored in a separate file con-
sisting of one line per question, which contains tab-
separated clue information and the Jeopardy! cate-
gory, e.g. INVEST in example (4).
Tools: for SVM learning, we used the SVMLight-
TK software4, which includes structural kernels in
SVMLight (Joachims, 1999)5. For generating con-
3Past Jeopardy! games can be downloaded from
http://www.j-archive.com.
4Available at http://dit.unitn.it/?moschitt
5http://svmlight.joachims.org
716
stituency trees, we used the Charniak parser (Char-
niak, 2000). We also used the syntactic?semantic
parser by Johansson and Nugues (2008) to gener-
ate dependency trees (Mel?c?uk, 1988) and predicate
argument trees according to the PropBank (Palmer
et al, 2005) and NomBank (Meyers et al, 2004)
frameworks.
Baseline Model: the first model that we used as a
baseline is a rule-based classifier (RBC). The RBC
leverages a set of rules that matches against lexical
and syntactic information in the clue to make a bi-
nary decision on whether or not the clue is consid-
ered definitional. The rule set was manually devel-
oped by a human expert, and consists of rules that
attempt to identify roughly 70 different constructs
in the clues. For instance, one of the rules matches
the parse tree structure for ?It?s X or Y?, which will
identify example (1) as a definition question.
Kernel Models: we apply the kernels described
in Section 3 to the structures extracted from Jeop-
ardy! clues. In particular, we design the following
models: BOW, i.e. linear kernel on bag-of-words
from the clues; WSK, PSK and CSK, i.e. SK applied
to the word and POS-tag sequences from the clues,
and the word sequence taken from the question cat-
egories, respectively; STK-CT, i.e. STK applied to
CTs of the clue; PTK-CT and PTK-DT, i.e. PTK
applied to CTs and DTs of the clues, respectively;
PASS, i.e. PTK applied to the Predicate Argument
Structure Set extracted from the clues; and RBC, i.e.
a linear kernel applied to the vector only constituted
by the 1/0 output of RBC.
Learning Setting: there is no particular parameteri-
zation. Since there is an imbalance between positive
and negative examples, we used a Precision/Recall
trade-off parameter in SVM-Light-TK equal to 5.6
Measures: the performance is measured with Pre-
cision, Recall and F1-measure. We estimated them
by means of Leave-One-Out7 (LOO) on the question
set.
4.2 Results and Discussion
Table 1 shows the performance obtained using dif-
ferent kernels (feature spaces) with SVMs. We note
6We have selected 5 as a reasonable value, which kept bal-
anced Precision and Recall on a validation set.
7LOO applied to a corpus ofN instances consists in training
on N ? 1 examples and testing on the single held-out example.
This process is repeated for all instances.
Kernel Space Prec. Rec. F1
RBC 28.27 70.59 40.38
BOW 47.67 46.73 47.20
WSK 47.11 50.65 48.82
STK-CT 50.51 32.35 39.44
PTK-CT 47.84 57.84 52.37
PTK-DT 44.81 57.84 50.50
PASS 33.50 21.90 26.49
PSK 39.88 45.10 42.33
CSK 39.07 77.12 51.86
Table 1: Kernel performance using leave-one-out cross-
validation.
that: first, RBC has good Recall but poor Precision.
This is interesting since, on one hand, these results
validate the complexity of the task: in order to cap-
ture the large variability of the positive examples,
the rules developed by a skilled human designer are
unable to be sufficiently precise to limit the recog-
nition to those examples. On the other hand, RBC,
being a rather different approach from SVMs, can be
successfully exploited in a joint model with them.
Second, BOW yields better F1 than RBC but it
does not generalize well since its F1 is still low.
When n-grams are also added to the model by
means of WSK, the F1 improves by about 1.5 ab-
solute points. As already shown in (Zhang and Lee,
2003; Moschitti et al, 2007), syntactic structures are
needed to improve generalization.
Third, surprisingly with respect to previous work,
STK applied to CT8 provides accuracy lower than
BOW, about 8 absolute points. The reason is due to
the different nature of the Jeopardy! questions: large
syntactic variability reduces the probability of find-
ing general and well formed patterns, i.e. structures
generated by entire production rules. This suggests
that PTK, which can capture patterns derived from
partial production rules, can be more effective. In-
deed, PTK-CT achieves the highest F1, outperform-
ing WSK also when used with a different syntactic
paradigm, i.e. PTK-DT.
Next, PSK and PASS provide a lower accuracy
but they may be useful in kernel combinations as
they can complement the information captured by
the other models. Interestingly, CSK alone is rather
effective for classifying definition questions. We be-
8Applying it to DT does not make much sense as already
pointed out in (Moschitti, 2006).
717
?Figure 5: Similarity according to PTK and STK
lieve this is because definition questions are some-
times clustered into categories such as 4-LETTER
WORDS or BEGINS WITH ?B?.
Moreover, we carried out qualitative error analy-
sis on the PTK and STK outcome, which supported
our initial hypothesis. Let us consider the bottom
tree in Figure 5 in the training set. The top tree is
a test example correctly classified by PTK but in-
correctly classified by STK. The dashed line in the
top tree contains the largest subtree matched by PTK
(against the bottom tree), whereas the dashed line in
the bottom tree indicates the largest subtree matched
by STK (against the top tree). As the figure shows,
PTK can exploit a larger number of partial patterns.
Finally, the above points suggest that different
kernels produce complementary information. It is
thus promising to experiment with their combina-
tions. The joint models can be simply built by
summing kernel functions together. The results are
shown in Table 2. We note that: (i) CSK comple-
ments the WSK information, achieving a substan-
tially better result, i.e. 62.95; (ii) PTK-CT+CSK
performs even better than WSK+CSK (as PTK out-
performs WSK); and (iii) adding RBC improves
further on the above combinations, i.e. 68.11 and
67.32, respectively. This evidently demonstrates
that RBC captures complementary information. Fi-
nally, more complex kernels, especially the overall
kernel summation, do not seem to improve the per-
Kernel Space Prec. Rec. F1
WSK+CSK 70.00 57.19 62.95
PTK-CT+CSK 69.43 60.13 64.45
PTK-CT+WSK+CSK 68.59 62.09 65.18
CSK+RBC 47.80 74.51 58.23
PTK-CT+CSK+RBC 59.33 74.84 65.79
BOW+CSK+RBC 60.65 73.53 66.47
PTK-CT+WSK+CSK+RBC 67.66 66.99 67.32
PTK-CT+PASS+CSK+RBC 62.46 71.24 66.56
WSK+CSK+RBC 69.26 66.99 68.11
ALL 61.42 67.65 64.38
Table 2: Performance of Kernel Combinations using
leave-one-out cross-validation.
formance. This is also confirmed by the PASS re-
sults derived in (Moschitti et al, 2007) on TREC
QC.
5 Experiments on the Jeopardy System
Since the kernel-based classifiers perform substan-
tially better than RBC, we incorporate the PTK-
CT+WSK+CSK model9 into Watson for definition
classification and evaluated the QA performance
against two baseline systems. For the end-to-end ex-
periments, we used Watson?s English Slot Grammar
parser (McCord, 1980) to generate the constituency
trees. The component level evaluation shows that
we achieved comparable performance as previously
discussed with ESG.
5.1 Experimental Setup
We integrated the classifier into the question analy-
sis module, and incorporated additional components
to search against dictionary resources and extract
candidate answers from these search results when a
question is classified as definitional. In the final ma-
chine learning models, a separate model is trained
for definition questions to enable scoring tailored to
the specific characteristics of those questions.
Based on our manually annotated gold standard,
less than 10% of Jeopardy! questions are classified
as definition questions. Due to their relatively low
frequency we conduct two types of evaluations. The
first is definition-only evaluation, in which we apply
our definition question classifier to identify a large
9Since we aim to compare a purely statistical approach to
the rule-based approach, we did not experiment with the model
that uses RBC as a feature in our end-to-end experiments.
718
set of definition questions and evaluate the end-to-
end system?s performance on this large set of ques-
tions. These results enable us to draw statistically
significant conclusions about our approach to ad-
dressing definition questions.
The second type of evaluation is game-based
evaluation, which assesses the impact of our defi-
nition question processing on Watson performance
while preserving the natural distribution of these
question types in Jeopardy! data. Game-based eval-
uations situate the system?s performance on defini-
tion questions relative to other types of questions,
and enable us to gauge the component?s contribu-
tions in a game-based setting.
For both evaluation settings, three configurations
of Watson are used as follows:
? the NoDef system, in which Watson is config-
ured without definition classification and pro-
cessing, thereby treating all definition ques-
tions as regular factoid questions;
? the StatDef system, which leverages the sta-
tistical classifier and subsequent definition spe-
cific search and candidate generation compo-
nents as described above; and
? the RuleDef system, in which Watson adopts
RBC and employs the same additional defini-
tion search and candidate generation compo-
nents as the StatDef system.
For the definition-only evaluation, we selected all
questions recognized as definitional by the statistical
classifier from roughly 1000 unseen games (60000
questions), resulting in a test set of 1606 questions.
Due to the size of the initial set, it is impractical to
manually create a gold standard for measuring Pre-
cision and Recall of the classifier. Instead, we com-
pare the StatDef system against the NoDef on these
1606 questions using two metrics: accuracy, defined
as the percentage of questions correctly answered,
and p@70, the system?s Precision when answering
only the top 70% most confident questions. P@70 is
an important metric in Jeopardy! game play as well
as in real world applications where the system may
refrain from answering a question when it is not con-
fident about any of its answers. Since RBC identifies
significantly more definition questions, we started
NoDef StatDef NoDef RuleDef
# Questions 1606 1606 1875 1875
Accuracy 63.76% 65.57% 56.64% 57.51%
P@70 82.22% 84.53% 72.73% 74.87%
Table 3: Definition-Only Evaluation Results
with an initial set of roughly 300 games, from which
the RBC identified 1875 questions as definitional.
We compared the RuleDef system?s performance on
these questions against the NoDef baseline using the
accuracy and p@70 metrics.
For the game-based evaluation, we randomly se-
lected 66 unseen Jeopardy! games, consisting of
3546 questions after excluding audio/visual ques-
tions.10 We contrast the StatDef system perfor-
mance against that of NoDef and RuleDef along
several dimensions: accuracy and p@70, described
above, as well as earnings, the average amount of
money earned for each game.
5.2 Definition-Only Evaluation
For the definition-only evaluation, we compared the
StatDef system against the NoDef system on a set of
1606 questions that the StatDef system classified as
definitional. The results are shown in the first two
columns in Table 3. To contrast the gain obtained
by the StatDef system against that achieved by the
RuleDef system, we ran the RuleDef system over
the 1875 questions identified as definitional by the
rule-based classifier. We contrast the RuleDef sys-
tem performance with that of the NoDef system, as
shown in the last two columns in Table 3.
Our results show that based on both evaluation
metrics, StatDef improved upon the NoDef baseline
more than RuleDef improved on the same baseline
system. Furthermore, for the accuracy metric where
all samples are paired and independent, the differ-
ence in performance between the StatDef and NoDef
systems is statistically significant at p<0.05, while
that between the RuleDef and NoDef systems is not.
5.3 Game-Based Evaluation
The game-based evaluation was carried out on 66
unseen games (roughly 3500 questions). Of these
10Audio/visual questions are those accompanied by either an
image or an audio clip. The text portions of these questions are
often insufficient for identifying the correct answers.
719
# Def Q?s Accuracy P@70 Earnings
NoDef 0 69.71% 86.79% $24,818
RuleDef 480 69.23% 86.31% $24,397
StatDef 131 69.85% 87.19% $25,109
Table 4: Game-Based Evaluation Results
questions, the StatDef system classified 131 of them
as definitional while the RuleDef system identified
480 definition questions. Both systems were com-
pared against the NoDef system using the accuracy,
p@70, and earnings metric computed over all ques-
tions, as shown in Table 4.
Our results show that even though in the
definition-only evaluation both the RuleDef and
StatDef systems outperformed the NoDef baseline,
in our game-based evaluation, the RuleDef system
performed worse than the NoDef baseline. The low-
ered performance is due to the fact that the Preci-
sion of the RBC is much lower than that of the sta-
tistical classifier, and the special definition process-
ing applied to questions that are erroneously clas-
sified as definitional was harmful. Our evaluation
of this false positive set showed that its accuracy
dropped by 6% compared to the NoDef system. On
the other hand, the StatDef system outperformed the
two other systems, and its accuracy improvement
upon the RuleDef system is statistically significant
at p<0.05.
6 Related Work
Our paper studies the use of advanced representa-
tion for QC in the Jeopardy! domain. As previously
mentioned Jeopardy! questions are stated as affir-
mative sentences, which are different from the typ-
ical QA questions. For the design of our models,
we have carefully taken into account previous work.
This shows that semantics and syntax are essential
to retrieve precise answers, e.g (Hickl et al, 2006;
Voorhees, 2004; Small et al, 2004).
We focus on definition questions, which typically
require more complex processing than factoid ques-
tions (Blair-Goldensohn et al, 2004; Chen et al,
2006; Shen and Lapata, 2007; Bilotti et al, 2007;
Moschitti et al, 2007; Surdeanu et al, 2008; Echi-
habi and Marcu, 2003). For example, language mod-
els were applied to definitional QA in (Cui et al,
2005) to learn soft pattern models based on bigrams.
Other related work, such as (Sasaki, 2005; Suzuki
et al, 2002), was also very tied to bag-of-words
features. Predicate argument structures have been
mainly used for reranking (Shen and Lapata, 2007;
Bilotti et al, 2007; Moschitti et al, 2007; Surdeanu
et al, 2008).
Our work and methods are similar to (Zhang and
Lee, 2003; Moschitti et al, 2007), which achieved
the state-of-the-art in QC by applying SVMs along
with STK-CT. The results were derived by experi-
menting with a TREC dataset11(Li and Roth, 2002),
reaching an accuracy of 91.8%. However, such data
refers to typical instances from QA, whose syntactic
patterns can be easily generalized by STK. In con-
trast, we have shown that STK-CT is not effective
for our domain, as it presents very innovative ele-
ments: questions in affirmative and highly variable
format. Thus, we employed new methods such as
PTK, dependency structures, multiple sequence ker-
nels including category information and many com-
binations.
Regarding the use of Kernel Methods, there is
a considerably large body of work in Natural Lan-
guage Processing, e.g. regarding syntactic parsing
(Collins and Duffy, 2002; Kudo et al, 2005; Shen
et al, 2003; Kudo and Matsumoto, 2003; Titov and
Henderson, 2006; Toutanova et al, 2004), named
entity recognition and chunking (Cumby and Roth,
2003; Daume? III and Marcu, 2004), relation extrac-
tion (Zelenko et al, 2002; Culotta and Sorensen,
2004; Bunescu and Mooney, 2005; Zhang et al,
2005; Bunescu, 2007; Nguyen et al, 2009a), text
categorization (Cancedda et al, 2003), word sense
disambiguation (Gliozzo et al, 2005) and seman-
tic role labeling (SRL), e.g. (Kazama and Torisawa,
2005; Che et al, 2006a; Moschitti et al, 2008).
However, ours is the first study on the use of sev-
eral combinations of kernels applied to several struc-
tures on very complex data from the Jeopardy! do-
main.
7 Final Remarks and Conclusion
In this paper we have experimented with advanced
structural kernels applied to several kinds of syntac-
tic/semantic linguistic structures for the classifica-
tion of questions in a new application domain, i.e.
Jeopardy!. Our findings are summarized hereafter:
11Available at http://cogcomp.cs.illinois.
edu/Data/QA/QC/
720
First, it should be noted that basic kernels, such
as STK, PTK and SK, when applied to new repre-
sentations, i.e. syntactic/semantic structures, con-
stitute new kernels. Thus structural representations
play a major role and, from this perspective, our pa-
per makes a significant contribution.
Second, the experimental results show that the
higher variability of Jeopardy! questions prevents us
from achieving generalization with typical syntactic
patterns even if they are derived by powerful meth-
ods such as STK. In contrast, partial patterns, such
as those provided by PTK applied to constituency
(or dependency) trees, prove to be effective.
In particular, STK has been considered as the best
kernel for exploiting syntactic information in con-
stituency trees, e.g. it is state-of-the-art in: QC
(Zhang and Lee, 2003; Moschitti et al, 2007; Mos-
chitti, 2008); SRL, (Moschitti et al, 2008; Mos-
chitti et al, 2005; Che et al, 2006b); pronominal
coreference resolution (Yang et al, 2006; Versley
et al, 2008) and Relation Extraction (Zhang et al,
2006; Nguyen et al, 2009b). We showed that, in
the complex domain of Jeopardy!, STK surprisingly
provides low accuracy whereas PTK is rather ef-
fective and greatly outperforms STK. We have also
provided an explanation of such behavior by means
of error analysis: in contrast with traditional ques-
tion classification, which focuses on basic syntactic
patterns (e.g. ?what?, ?where?, ?who? and ?how?).
Figure 5 shows that PTK captures partial patterns
that are important for more complex questions like
those in Jeopardy!
Third, we derived other interesting findings for
NLP related to this novel domain, e.g.: (i) the im-
pact of dependency trees is similar to the one of
constituency trees. (ii) A simple computational rep-
resentation of shallow semantics, i.e. PASS (Mos-
chitti, 2008), does not work in Jeopardy!. (iii) Se-
quence kernels on category cues, i.e., higher level of
lexical semantics, improve question classification.
(iv) RBC jointly used with statistical approaches is
helpful to tackle the Jeopardy! complexity.
Next, our kernel models improve up to 20 abso-
lute percent points over n-grams based approaches,
reaching a significant accuracy of about 70%. Wat-
son, exploiting such a classifier, improved previ-
ous versions using RBC and no definition classifica-
tion both in definition-only evaluations and in game-
based evaluations.
Finally, we point out that:
? Jeopardy! has a variety of different special ques-
tion types that are handled differently. We focus on
kernel methods for definition question for two rea-
sons. First, their recognition relies heavily on parse
structures and is therefore more amenable to the ap-
proach proposed in the paper than the recognition
of other question types. Second, definition is by far
the most frequent special question type in Jeopardy!;
therefore, we can obtain sufficient data for training
and testing.
? We were unable to address the whole QC prob-
lem using a statistical model due to the lack of suffi-
cient training data for most special question classes.
Furthermore, we focused only on the definition clas-
sification and its impact on system performance due
to space reasons.
? Our RBC has a rather imbalanced trade-off be-
tween Precision and Recall. This may not be the
best operating point, but the optimal point is diffi-
cult to obtain empirically for an RBC, which is a
strong motivation of the work in this paper. We ex-
perimented with tuning the trade-off between Preci-
sion and Recall with the RBC, but since RBC uses
hand-crafted rules and does not have a parameter for
that, ultimately the statistical approach proved more
effective.
In future work, we plan to extend the current re-
search by investigating models capable of exploit-
ing predicate argument structures for question clas-
sification and answer reranking. The use of syntac-
tic/semantic kernels is a promising research direc-
tion (Basili et al, 2005; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b). In this
perspective kernel learning is a very interesting re-
search line, considering the complexity of represen-
tation and classification problems in which our ker-
nels operate.
Acknowledgements
This work has been supported by the IBM?s Open
Collaboration Research (OCR) awards program. We
are deeply in debt with Richard Johansson, who pro-
duced the earlier syntactic/semantic representations
of the Jeopardy! questions from the text format.
721
References
Kisuh Ahn, Johan Bos, Stephen Clark, James R. Cur-
ran, Tiphaine Dalmas, Jochen L. Leidner, Matthew B.
Smillie, and Bonnie Webber. 2004. Question an-
swering with qed and wee at trec-2004. In E. M.
Voorhees and L. P. Buckland, editors, The Thirteenth
Text REtrieval Conference, TREC 2004, pages 595?
599, Gaitersburg, MD.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of CoNLL-
2005, pages 1?8, Ann Arbor, Michigan. Association
for Computational Linguistics.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of ACM SIGIR.
S. Blair-Goldensohn, K. R. McKeown, and A. H.
Schlaikjer. 2004. Answering definitional questions:
A hybrid approach. In M. Maybury, editor, Proceed-
ings of AAAI 2004. AAAI Press.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ?07.
Phil Blunsom, Krystle Kocik, and James R. Curran.
2006. Question classification with log-linear models.
In SIGIR ?06: Proceedings of the 29th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 615?616, New
York, NY, USA. ACM.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Razvan C. Bunescu. 2007. Learning to extract relations
from the web using minimal supervision. In Proceed-
ings of ACL.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006a. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 73?
80, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006b. A hybrid convolution tree kernel for semantic
role labeling. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ?06,
pages 73?80, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking an-
swers from definitional QA using language models. In
Proceedings of ACL.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358?365.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
H. Cui, M. Kan, and T. Chua. 2005. Generic soft pattern
models for definitional QA. In Proceedings of SIGIR,
Salvador, Brazil. ACM.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423?429, Barcelona, Spain, July.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
A. Echihabi and D. Marcu. 2003. A noisy-channel ap-
proach to question answering. In Proceedings of ACL.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building watson:
An overview of the deepqa project. AI Magazine,
31(3).
Danilo Giampiccolo, Pamela Froner, Anselmo Pen?as,
Christelle Ayache, Dan Cristea, Valentin Jijkoun,
Petya Osenova, Paulo Rocha, Bogdan Sacaleanu, and
Richard Suteliffe. 2007. Overview of the CLEF 2007
multilingual question anwering track. In Proceedings
of the Cross Language Evaluation Forum.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and
B. Rink. 2006. Question answering with lcc chaucer
at trec 2006. In Proceedings of TREC.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods ? Sup-
port Vector Learning, 13.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183?187, Manchester,
United Kingdom.
722
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In Proceedings of
the Workshop on Deep Linguistic Processing, DeepLP
?07, pages 41?48, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2005. Speeding
up training with tree kernels for node relation labeling.
In Proceedings of HLT-EMNLP?05.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Shyong Lam, David Pennock, Dan Cosley, and Steve
Lawrence. 2003. 1 billion pages = 1 milllion dollars?
mining the web to pay ?who wants to be a millionaire?
In Proceedings of the 19th Conference on Uncertainty
in AI.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL.
Michael C. McCord. 1980. Slot grammars. Computa-
tional Linguistics.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York, Albany.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24?31, Boston, United
States.
Alessandro Moschitti, Bonaventura Coppola, Ana-Maria
Giuglea, and Roberto Basili. 2005. Hierarchical se-
mantic role labeling. In CoNLL 2005 shared task.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question/answer clas-
sification. In Proceedings of ACL?07.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of CIKM ?08, NY, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009a. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In Proceedings of EMNLP, pages
1378?1387, Singapore, August.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009b. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1378?1387, Morristown,
NJ, USA. Association for Computational Linguistics.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?105.
Yutaka Sasaki, Chuan-Jie Lin, Kuang-hua Chen, and
Hsin-Hsi Chen. 2007. Overview of the NTCIR-6
cross-lingual question answering (CLQA) task. In
Proceedings of the 6th NTCIR Workshop on Evalua-
tion of Information Access Technologies.
Y. Sasaki. 2005. Question answering as question-biased
term extraction: A new approach toward multilingual
qa. In Proceedings of ACL, pages 215?222.
John Shawe-Taylor and Nello Cristianini. 2004. LaTeX
User?s Guide and Document Reference Manual. Ker-
nel Methods for Pattern Analysis, Cambridge Univer-
sity Press.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL.
L. Shen, A. Sarkar, and A. Joshi. 2003. Using LTAG
Based Features in Parse Reranking. In Proceedings of
EMNLP, Sapporo, Japan.
S. Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin,
N. Shimizu, P. Kantor, D. Kelly, and N. Wacholder.
2004. Hitiqa: Towards analytical question answering.
In Proceedings of COLING.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT, Columbus, Ohio.
J. Suzuki, Y. Sasaki, and E. Maeda. 2002. Svm an-
swer selection for open-domain question answering.
In Proceedings of Coling, pages 974?980.
Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku
Maeda. 2003. Question classification using hdag ker-
nel. In Proceedings of the ACL 2003 Workshop on
Multilingual Summarization and Question Answering,
pages 61?68, Sapporo, Japan, July. Association for
Computational Linguistics.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference sys-
tems based on kernels methods. In The 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing?08), Manchester, England.
723
Ellen M. Voorhees and Hoa Trang Dang. 2006.
Overview of the TREC 2005 question answering track.
In Proceedings of the TREC 2005 Conference.
E. M. Voorhees. 2004. Overview of the trec 2004 ques-
tion answering track. In Proceedings of TREC 2004.
Xiaofeng Yang, Jian Su, and Chewlim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proc. COLING-ACL 06.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26?32. ACM Press.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP?2005, Lecture Notes in Computer Science (LNCS
3651), pages 378?389, Jeju Island, South Korea.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
724
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1426?1436,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Relation Extraction with Relation Topics
Chang Wang James Fan Aditya Kalyanpur David Gondek
IBM T. J. Watson Research Lab
19 Skyline Drive, Hawthorne, New York 10532
{wangchan, fanj, adityakal, dgondek}@us.ibm.com
Abstract
This paper describes a novel approach to the
semantic relation detection problem. Instead
of relying only on the training instances for
a new relation, we leverage the knowledge
learned from previously trained relation detec-
tors. Specifically, we detect a new semantic
relation by projecting the new relation?s train-
ing instances onto a lower dimension topic
space constructed from existing relation de-
tectors through a three step process. First, we
construct a large relation repository of more
than 7,000 relations from Wikipedia. Second,
we construct a set of non-redundant relation
topics defined at multiple scales from the re-
lation repository to characterize the existing
relations. Similar to the topics defined over
words, each relation topic is an interpretable
multinomial distribution over the existing re-
lations. Third, we integrate the relation topics
in a kernel function, and use it together with
SVM to construct detectors for new relations.
The experimental results on Wikipedia and
ACE data have confirmed that background-
knowledge-based topics generated from the
Wikipedia relation repository can significantly
improve the performance over the state-of-the-
art relation detection approaches.
1 Introduction
Detecting semantic relations in text is very useful
in both information retrieval and question answer-
ing because it enables knowledge bases to be lever-
aged to score passages and retrieve candidate an-
swers. To extract semantic relations from text, three
types of approaches have been applied. Rule-based
methods (Miller et al, 2000) employ a number of
linguistic rules to capture relation patterns. Feature-
based methods (Kambhatla, 2004; Zhao and Grish-
man, 2005) transform relation instances into a large
amount of linguistic features like lexical, syntactic
and semantic features, and capture the similarity be-
tween these feature vectors. Recent results mainly
rely on kernel-based approaches. Many of them fo-
cus on using tree kernels to learn parse tree struc-
ture related features (Collins and Duffy, 2001; Cu-
lotta and Sorensen, 2004; Bunescu and Mooney,
2005). Other researchers study how different ap-
proaches can be combined to improve the extraction
performance. For example, by combining tree ker-
nels and convolution string kernels, (Zhang et al,
2006) achieved the state of the art performance on
ACE (ACE, 2004), which is a benchmark dataset for
relation extraction.
Although a large set of relations have been iden-
tified, adapting the knowledge extracted from these
relations for new semantic relations is still a chal-
lenging task. Most of the work on domain adapta-
tion of relation detection has focused on how to cre-
ate detectors from ground up with as little training
data as possible through techniques such as boot-
strapping (Etzioni et al, 2005). We take a differ-
ent approach, focusing on how the knowledge ex-
tracted from the existing relations can be reused to
help build detectors for new relations. We believe by
reusing knowledge one can build a more cost effec-
tive relation detector, but there are several challenges
associated with reusing knowledge.
The first challenge to address in this approach is
how to construct a relation repository that has suffi-
1426
cient coverage. In this paper, we introduce a method
that automatically extracts the knowledge charac-
terizing more than 7,000 relations from Wikipedia.
Wikipedia is comprehensive, containing a diverse
body of content with significant depth and grows
rapidly. Wikipedia?s infoboxes are particularly in-
teresting for relation extraction. They are short,
manually-created, and often have a relational sum-
mary of an article: a set of attribute/value pairs de-
scribing the article?s subject.
Another challenge is how to deal with overlap of
relations in the repository. For example, Wikipedia
authors may make up a name when a new relation
is needed without checking if a similar relation has
already been created. This leads to relation duplica-
tion. We refine the relation repository based on an
unsupervised multiscale analysis of the correlations
between existing relations. This method is parame-
ter free, and able to produce a set of non-redundant
relation topics defined at multiple scales. Similar to
the topics defined over words (Blei et al, 2003), we
define relation topics as multinomial distributions
over the existing relations. The relation topics ex-
tracted in our approach are interpretable, orthonor-
mal to each other, and can be used as basis relations
to re-represent the new relation instances.
The third challenge is how to use the relation top-
ics for a relation detector. We map relation instances
in the new domains to the relation topic space, re-
sulting in a set of new features characterizing the
relationship between the relation instances and ex-
isting relations. By doing so, background knowl-
edge from the existing relations can be introduced
into the new relations, which overcomes the limi-
tations of the existing approaches when the training
data is not sufficient. Our work fits in to a class of re-
lation extraction research based on ?distant supervi-
sion?, which studies how knowledge and resources
external to the target domain can be used to im-
prove relation extraction. (Mintz et al, 2009; Jiang,
2009; Chan and Roth, 2010). One distinction be-
tween our approach and other existing approaches is
that we represent the knowledge from distant super-
vision using automatically constructed topics. When
we test on new instances, we do not need to search
against the knowledge base. In addition, our top-
ics also model the indirect relationship between re-
lations. Such information cannot be directly found
from the knowledge base.
The contributions of this paper are three-fold.
Firstly, we extract a large amount of training
data for more than 7,000 semantic relations from
Wikipedia (Wikipedia, 2011) and DBpedia (Auer
et al, 2007). A key part of this step is how we
handle noisy data with little human effort. Sec-
ondly, we present an unsupervised way to con-
struct a set of relation topics at multiple scales.
This step is parameter free, and results in a non-
redundant, multiscale relation topic space. Thirdly,
we design a new kernel for relation detection by
integrating the relation topics into the relation de-
tector construction. The experimental results on
Wikipedia and ACE data (ACE, 2004) have con-
firmed that background-knowledge-based features
generated from the Wikipedia relation repository
can significantly improve the performance over the
state-of-the-art relation detection approaches.
2 Extracting Relations from Wikipedia
Our training data is from two parts: relation in-
stances from DBpedia (extracted from Wikipedia
infoboxes), and sentences describing the relations
from the corresponding Wikipedia pages.
2.1 Collecting the Training Data
Since our relations correspond to Wikipedia infobox
properties, we use an approach similar to that de-
scribed in (Hoffmann et al, 2010) to collect positive
training data instances. We assume that a Wikipedia
page containing a particular infobox property is
likely to express the same relation in the text of
the page. We further assume that the relation is
most likely expressed in the first sentence on the
page which mentions the arguments of the relation.
For example, the Wikipedia page for ?Albert Ein-
stein? contains an infobox property ?alma mater?
with value ?University of Zurich?, and the first sen-
tence mentioning the arguments is the following:
?Einstein was awarded a PhD by the University of
Zurich?, which expresses the relation. When look-
ing for relation arguments on the page, we go be-
yond (sub)string matching, and use link information
to match entities which may have different surface
forms. Using this technique, we are able to collect a
large amount of positive training instances of DBpe-
1427
dia relations.
To get precise type information for the argu-
ments of a DBpedia relation, we use the DBpedia
knowledge base (Auer et al, 2007) and the asso-
ciated YAGO type system (Suchanek et al, 2007).
Note that for every Wikipedia page, there is a cor-
responding DBpedia entry which has captured the
infobox-properties as RDF triples. Some of the
triples include type information, where the subject
of the triple is a Wikipedia entity, and the object
is a YAGO type for the entity. For example, the
DBpedia entry for the entity ?Albert Einstein? in-
cludes YAGO types such as Scientist, Philosopher,
Violinist etc. These YAGO types are also linked
to appropriate WordNet concepts, providing for ac-
curate sense disambiguation. Thus, for any en-
tity argument of a relation we are learning, we ob-
tain sense-disambiguated type information (includ-
ing super-types, sub-types, siblings etc.), which be-
come useful generalization features in the relation
detection model. Given a common noun, we can
also retrieve its type information by checking against
WordNet (Fellbaum, 1998).
2.2 Extracting Rules from the Training Data
We use a set of rules together with their popular-
ities (occurrence count) to characterize a relation.
A rule representing the relations between two ar-
guments has five components (ordered): argument1
type, argument2 type, noun, preposition and verb. A
rule example of ActiveYearsEndDate relation (about
the year that a person retired) is:
person100007846|year115203791|-|in|retire.
In this example, argument1 type is per-
son100007846, argument2 type is year115203791,
both of which are from YAGO type system. The
key words connecting these two arguments are in
(preposition) and retire (verb). This rule does not
have a noun, so we use a ?-? to take the position of
noun. The same relation can be represented in many
different ways. Another rule example characterizing
the same relation is
person100007846|year115203791|retirement|-|announce.
This paper only considers three types of words:
noun, verb and preposition. It is straightforward to
expand or simplify the rules by including more or
removing some word types. The keywords are ex-
tracted from the shortest path on the dependency
Figure 1: A dependency tree example.
tree between the two arguments. A dependency
tree (Figure 1) represents grammatical relations be-
tween words in a sentence. We used a slot grammar
parser (McCord, 1995) to generate the parse tree of
each sentence. Note that there could be multiple
paths between two arguments in the tree. We only
take the shortest path into consideration. The pop-
ularity value corresponding to each rule represents
how many times this rule applies to the given rela-
tion in the given data. Multiple rules can be con-
structed from one relation instance, if multiple argu-
ment types are associated with the instance, or mul-
tiple nouns, prepositions or verbs are in the depen-
dency path.
2.3 Cleaning the Training Data
To find a sentence on the Wikipedia page that is
likely to express a relation in its infobox, we con-
sider the first sentence on the page that mentions
both arguments of the relation. This heuristic ap-
proach returns reasonably good results, but brings in
about 20% noise in the form of false positives, which
is a concern when building an accurate statistical re-
lation detector. To address this issue, we have devel-
oped a two-step technique to automatically remove
some of the noisy data. In the first step, we extract
popular argument types and keywords for each DB-
pedia relation from the given data, and then use the
combinations of those types and words to create ini-
tial rules. Many of the argument types and keywords
introduced by the noisy data are often not very pop-
ular, so they can be filtered out in the first step. Not
all initial rules make sense. In the second step, we
1428
check each rule against the training data to see if that
rule really exists in the training data or not. If it does
not exist, we filter it out. If a sentence does not have
a single rule passing the above procedure, that sen-
tence will be removed. Using the above techniques,
we collect examples characterizing 7,628 DBpedia
relations.
3 Learning Multiscale Relation Topics
An extra step extracting knowledge from the raw
data is needed for two reasons: Firstly, many DB-
pedia relations are inter-related. For example, some
DBpedia relations have a subclass relationship, e.g.
?AcademyAward? and ?Award?; others overlap in
their scope and use, e.g., ?Composer? and ?Artist?;
while some are equivalent, e.g., ?DateOfBirth? and
?BirthDate?. Secondly, a fairly large amount of the
noisy labels are still in the training data.
To reveal the intrinsic structure of the current DB-
pedia relation space and filter out noise, we car-
ried out a correlation analysis of relations in the
training data, resulting in a relation topic space.
Each relation topic is a multinomial distribution
over the existing relations. We adapted diffusion
wavelets (Coifman and Maggioni, 2006) for this
task. Compared to the other well-known topic ex-
traction methods like LDA (Blei et al, 2003) and
LSI (Deerwester et al, 1990), diffusion wavelets can
efficiently extract a hierarchy of interpretable topics
without any user input parameter (Wang and Ma-
hadevan, 2009).
3.1 An Overview of Diffusion Wavelets
The diffusion wavelets algorithm constructs a com-
pressed representation of the dyadic powers of a
square matrix by representing the associated matri-
ces at each scale not in terms of the original (unit
vector) basis, but rather using a set of custom gener-
ated bases (Coifman and Maggioni, 2006). Figure
2 summarizes the procedure to generate diffusion
wavelets. Given a matrix T , the QR (a modified
QR decomposition) subroutine decomposes T into
an orthogonal matrix Q and a triangular matrix R
such that T ? QR, where |Ti,k ? (QR)i,k| < ?
for any i and k. Columns in Q are orthonormal ba-
sis functions spanning the column space of T at the
finest scale. RQ is the new representation of T with
{[?j ]?0} = DWT (T, ?, J)
//INPUT:
//T : The input matrix.
//?: Desired precision, which can be set to a small
number or simply machine precision.
//J : Number of levels (optional).
//OUTPUT:
//[?j ]?0 : extended diffusion scaling functions at
scale j.
?0 = I;
For j = 0 to J ? 1 {
([?j+1]?j , [T 2j ]?j+1?j )? QR([T 2
j ]?j?j , ?);
[?j+1]?0 = [?j+1]?j [?j ]?0 ;
[T 2j+1 ]?j+1?j+1 = ([T
2j ]?j+1?j [?j+1]?j )
2;
}
Figure 2: Diffusion Wavelets construct multiscale repre-
sentations of the input matrix at different scales. QR is a
modified QR decomposition. J is the max step number
(this is optional, since the algorithm automatically ter-
minates when it reaches a matrix of size 1 ? 1). The
notation [T ]?b?a denotes matrix T whose column space isrepresented using basis ?b at scale b, and row space is
represented using basis ?a at scale a. The notation [?b]?a
denotes basis ?b represented on the basis ?a. At an arbi-
trary scale j, we have pj basis functions, and length of
each function is lj . The number of pj is determined by
the intrinsic structure of the given dataset in QR routine.
[T ]?b?a is a pb ? la matrix, and [?b]?a is an la ? pb matrix.
respect to the space spanned by the columns of Q
(this result is based on the matrix invariant subspace
theory). At an arbitrary level j,DWT learns the ba-
sis functions from T 2j using QR. Compared to the
number of basis functions spanning T 2j ?s original
column space, we usually get fewer basis functions,
since some high frequency information (correspond-
ing to the ?noise? at that level) can be filtered out.
DWT then computes T 2j+1 using the low frequency
representation of T 2j and the procedure repeats.
3.2 Constructing Multiscale Relation Topics
Learning Relation Correlations
Assume we have M relations, and the ith of them
is characterized by mi <rule, popularity> pairs. We
use s(a, b) to represent the similarity between the
ath and bth relations. To compute s(a, b), we first
normalize the popularities for each relation, and then
1429
look for the rules that are shared by both relation a
and b. We use the product of corresponding pop-
ularity values to represent the similarity score be-
tween two relations with respect to each common
rule. s(a, b) is set to the sum of such scores over
all common rules. The relation-relation correlation
matrix S is constructed as follows:
S = [
s(1, 1) ? ? ? s(1,M)
? ? ? ? ? ? ? ? ?
s(M, 1) ? ? ? s(M,M)
]
We have more than 200, 000 argument types, tens
of thousands of distinct nouns, prepositions, and
verbs, so we potentially have trillions of distinct
rules. One rule may appear in multiple relations.
The more rules two relations share, the more related
two relations should be. The rules shared across dif-
ferent relations offer us a novel way to model the
correlations between different relations, and further
allow us to create relation topics. The rules can also
be simplified. For example, we may treat argument1,
argument2, noun, preposition and verb separately.
This results in simple rules that only involve in one
argument type or word. The correlations between
relations are then computed only based on one par-
ticular component like argument1, noun, etc.
Theoretical Analysis
Matrix S models the correlations between rela-
tions in the training data. Once S is constructed, we
adapt diffusion wavelets (Coifman and Maggioni,
2006) to automatically extract the basis functions
spanning the original column space of S at multi-
ple scales. The key strength of the approach is that
it is data-driven, largely parameter-free and can au-
tomatically determine the number of levels of the
topical hierarchy, as well as the topics at each level.
However, to apply diffusion wavelets to S, we first
need to show that S is a positive semi-definite ma-
trix. This property guarantees that all eigenvalues
of S are ? 0. Depending on the way we formal-
ize the rules, the methods to validate this property
are slightly different. When we treat argument1,
argument2, noun, preposition and verb separately, it
is straightforward to see the property holds. In The-
orem 1, we show the property also holds when we
use more complicated rules (using the 5-tuple rule
in Section 2.2 as an example in the proof).
Theorem 1. S is a Positive Semi-Denite matrix.
Proof: An arbitrary rule ri is uniquely characterized
by a five tuple: argument1 type| argument2 type|
noun| preposition| verb. Since the number of dis-
tinct argument types and words are constants, the
number of all possible rules is also a constant: R.
If we treat each rule as a feature, then the set of
rules characterizing an arbitrary relation ri can be
represented as a point [p1i , ? ? ? , pRi ] in a latent R di-
mensional rule space, where pji represents the popu-
larity of rule j in relation ri in the given data.
We can verify that the way to compute s(a, b) is
the same as s(a, b) =< [p1a ? ? ? pRa ], [p1b ? ? ? pRb ] >,
where < ?, ? > is the cosine similarity (kernel). It
follows directly from the definition of positive semi-
definite matrix (PSD) that S is PSD (Scho?lkopf and
Smola, 2002).
In our approach, we construct multiscale re-
lation topics by applying DWT to decompose
S/?max(S), where ?max(S) represents the largest
eigenvalue of S. Theorem 2 shows that this decom-
position will converge, resulting in a relation topic
hierarchy with one single topic at the top level.
Theorem 2. Let ?max(S) represent the largest
eigenvalue of matrix S, then DWT (S/?max(S), ?)
produces a set of nested subspaces of the column
space of S, and the highest level of the resulting sub-
space hierarchy is spanned by one basis function.
Proof: From Theorem 1, we know that S is a PSD
matrix. This means ?max(S) ? [0,+?) (all eigen-
values of S are non-negative). This further implies
that ?(S)/?max(S) ? [0, 1], where ?(S) represents
any eigenvalue of S.
The idea underlying diffusion wavelets is based
on decomposing the spectrum of an input matrix
into various spectral bands, spanned by basis func-
tions (Coifman and Maggioni, 2006). Let T =
S/?max(S). In Figure 2, we construct spectral
bands of eigenvalues, whose associated eigenvectors
span the corresponding subspaces. Define dyadic
spatial scales tj as
tj =
j?
t=0
2t = 2j+1 ? 1, j ? 0 .
At each spatial scale, the spectral band is defined as:
?j(T ) = {? ? ?(T ), ?tj ? ?},
1430
where ?(T ) represents any eigenvalue of T , and ? ?
(0, 1) is a pre-defined threshold in Figure 2. We can
now associate with each of the spectral bands a vec-
tor subspace spanned by the corresponding eigen-
vectors:
Vj = ?{?? : ? ? ?(T ), ?tj ? ?}?, j ? 0 .
In the limit, we obtain
lim
j??
Vj = ?{?? : ? = 1}?
That is, the highest level of the resulting subspace
hierarchy is spanned by the eigenvector associated
with the largest eigenvalue of T .
This result shows that the multiscale analysis of
the relation space will automatically terminate at the
level spanned by one basis, which is the most popu-
lar relation topic in the training data.
3.3 High Level Explanation
We first create a set of rules to characterize each in-
put relation. Since these rules may occur in multi-
ple relations, they provide a way to model the co-
occurrence relationship between different relations.
Our algorithm starts with the relation co-occurrence
matrix and then repeatedly applies QR decomposi-
tion to learn the topics at the current level while at
the same time modifying the matrix to focus more on
low-frequency indirect co-occurrences (between re-
lations) for the next level. Running DWT is equiv-
alent to running a Markov chain on the input data
forward in time, integrating the local geometry and
therefore revealing the relevant geometric structures
of the whole data set at different scales. At scale
j, the representation of T 2j+1 is compressed based
on the amount of remaining information and the de-
sired precision. This procedure is illustrated in Fig-
ure 3. In the resulting topic space, instances with
related relations will be grouped together. This ap-
proach may significantly help us detect new rela-
tions, since it potentially expands the information
brought in by new relation instances from making
use of the knowledge extracted from the existing re-
lation repository.
3.4 Benefits
As shown in Figure 3, the topic spaces at different
levels are spanned by a different number of basis
  
 	
   
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 185?193,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
When Did that Happen? ? Linking Events and Relations to Timestamps
Dirk Hovy*, James Fan, Alfio Gliozzo, Siddharth Patwardhan and Chris Welty
IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532
dirkh@isi.edu, {fanj,gliozzo,siddharth,welty}@us.ibm.com
Abstract
We present work on linking events and flu-
ents (i.e., relations that hold for certain
periods of time) to temporal information
in text, which is an important enabler for
many applications such as timelines and
reasoning. Previous research has mainly
focused on temporal links for events, and
we extend that work to include fluents
as well, presenting a common methodol-
ogy for linking both events and relations
to timestamps within the same sentence.
Our approach combines tree kernels with
classical feature-based learning to exploit
context and achieves competitive F1-scores
on event-time linking, and comparable F1-
scores for fluents. Our best systems achieve
F1-scores of 0.76 on events and 0.72 on flu-
ents.
1 Introduction
It is a long-standing goal of NLP to process natu-
ral language content in such a way that machines
can effectively reason over the entities, relations,
and events discussed within that content. The ap-
plications of such technology are numerous, in-
cluding intelligence gathering, business analytics,
healthcare, education, etc. Indeed, the promise
of machine reading is actively driving research in
this area (Etzioni et al 2007; Barker et al 2007;
Clark and Harrison, 2010; Strassel et al 2010).
Temporal information is a crucial aspect of this
task. For a machine to successfully understand
natural language text, it must be able to associate
time points and temporal durations with relations
and events it discovers in text.
?The first author conducted this research during an in-
ternship at IBM Research.
In this paper we present methods to estab-
lish links between events (e.g. ?bombing? or
?election?) or fluents (e.g. ?spouseOf? or ?em-
ployedBy?) and temporal expressions (e.g. ?last
Tuesday? and ?November 2008?). While previ-
ous research has mainly focused on temporal links
for events only, we deal with both events and flu-
ents with the same method. For example, consider
the sentence below
Before his death in October, Steve Jobs
led Apple for 15 years.
For a machine reading system processing this
sentence, we would expect it to link the fluent
CEO of (Steve Jobs, Apple) to time duration ?15
years?. Similarly we expect it to link the event
?death? to the time expression ?October?.
We do not take a strong ?ontological? position
on what events and fluents are, as part of our
task these distinctions are made a priori. In other
words, events and fluents are input to our tempo-
ral linking framework. In the remainder of this pa-
per, we also do not make a strong distinction be-
tween relations in general and fluents in particu-
lar, and use them interchangeably, since our focus
is only on the specific types of relations that rep-
resent fluents. While we only use binary relations
in this work, there is nothing in the framework
that would prevent the use of n-ary relations. Our
work focuses on accurately identifying temporal
links for eventual use in a machine reading con-
text.
In this paper, we describe a single approach that
applies to both fluents and events, using feature
engineering as well as tree kernels. We show that
we can achieve good results for both events and
fluents using the same feature space, and advocate
185
the versatility of our approach by achieving com-
petitive results on yet another similar task with a
different data set.
Our approach requires us to capture contextual
properties of text surrounding events, fluents and
time expressions that enable an automatic system
to detect temporal linking within our framework.
A common strategy for this is to follow standard
feature engineering methodology and manually
develop features for a machine learning model
from the lexical, syntactic and semantic analysis
of the text. A key contribution of our work in this
paper is to demonstrate a shallow tree-like repre-
sentation of the text that enables us to employ tree
kernel models, and more accurately detect tempo-
ral linking. The feature space represented by such
tree kernels is far larger than a manually engi-
neered feature space, and is capable of capturing
the contextual information required for temporal
linking.
The remainder of this paper goes into the de-
tails of our approach for temporal linking, and
presents empirical evidence for the effectiveness
of our approach. The contributions of this paper
can be summarized as follows:
1. We define a common methodology to link
events and fluents to timestamps.
2. We use tree kernels in combination with clas-
sical feature-based approaches to obtain sig-
nificant gains by exploiting context.
3. Empirical evidence illustrates that our
framework for temporal linking is very ef-
fective for the task, achieving an F1-score of
0.76 on events and 0.72 on fluents/relations,
as well as 0.65 for TempEval2, approaching
state-of-the-art.
2 Related Work
Most of the previous work on relation extraction
focuses on entity-entity relations, such as in the
ACE (Doddington et al 2004) tasks. Temporal
relations are part of this, but to a lesser extent.
The primary research effort in event temporality
has gone into ordering events with respect to one
another (e.g., Chambers and Jurafsky (2008)), and
detecting their typical durations (e.g., Pan et al
(2006)).
Recently, TempEval workshops have focused
on the temporal related issues in NLP. Some of
the TempEval tasks overlap with ours in many
ways. Our task is similar to task A and C of
TempEval-1 (Verhagen et al 2007) in the sense
that we attempt to identify temporal relation be-
tween events and time expressions or document
dates. However, we do not use a restricted set of
events, but focus primarily on a single temporal
relation tlink instead of named relations like BE-
FORE, AFTER or OVERLAP (although we show
that we can incorporate these as well). Part of our
task is similar to task C of TempEval-2 (Verha-
gen et al 2010), determining the temporal rela-
tion between an event and a time expression in
the same sentence. In this paper, we do apply our
system to TempEval-2 data and compare our per-
formance to the participating systems.
Our work is similar to that of Boguraev and
Ando (2005), whose research only deals with
temporal links between events and time expres-
sions (and does not consider relations at all). They
employ a sequence tagging model with manual
feature engineering for the task and achieved
state-of-the-art results on Timebank (Pustejovsky
et al 2003) data. Our task is slightly different be-
cause we include relations in the temporal linking,
and our use of tree kernels enables us to explore a
wider feature space very quickly.
Filatova and Hovy (2001) also explore tempo-
ral linking with events, but do not assume that
events and time stamps have been provided by an
external process. They used a heuristics-based ap-
proach to assign temporal expressions to events
(also relying on the proximity as a base case).
They report accuracy of the assignment for the
correctly classified events, the best being 82.29%.
Our best event system achieves an accuracy of
84.83%. These numbers are difficult to compare,
however, since accuracy does not efficiently cap-
ture the performance of a system on a task with so
many negative examples.
Mirroshandel et al(2011) describe the use of
syntactic tree kernels for event-time links. Their
results on TempEval are comparable to ours. In
contrast to them, we found, though, that syntactic
tree kernels alone do not perform as well as using
several flat tree representations.
3 Problem Definition
The task of linking events and relations to time
stamps can be defined as the following: given a set
of expressions denoting events or relation men-
186
tions in a document, and a set of time expressions
in the same document, find all instances of the
tlink relation between elements of the two input
sets. The existence of a tlink(e, t) means that e,
which is an event or a relation mention, occurs
within the temporal context specified by the time
expression t.
Thus, our task can be cast as a binary rela-
tion classification task: for each possible pair
of (event/relation, time) in a document, decide
whether there exists a link between the two, and
if so, express it in the data.
In addition, we make these assumptions about
the data:
1. There does not exist a timestamp for ev-
ery event/relation in a document. Although
events and relations typically have temporal
context, it may not be explicitly stated in a
document.
2. Every event/relation has at most one time ex-
pression associated with it. This is a simpli-
fying assumption, which in the case of rela-
tions we explore as future work.
3. Each temporal expression can be linked to
one or more events or relations. Since mul-
tiple events or relations may happen for a
given time, it is safe to assume that each tem-
poral expression can be linked to more than
one event/relation.
In general, the events/relations and their associ-
ated timestamps may occur within the same sen-
tence or may occur across different sentences. In
this paper, we focus on our effort and our evalua-
tion on the same sentence linking task.
In order to solve the problem of temporal link-
ing completely, however, it will be important to
also address the links that hold between entities
across sentences. We estimate, based on our data
set, that across sentence links account for 41% of
all correct event-time pairs in a document. For flu-
ents, the ratio is much higher, more than 80% of
the correct fluent-time links are across sentences.
One of the main obstacles for our approach in the
cross-sentence case is the very low ratio of posi-
tive to negative instances (3 : 100) in the set of all
pairs in a document. Most pairs are not linked to
one another.
4 Temporal Linking Framework
As previously mentioned, we approach the tem-
poral linking problem as a classification task. In
the framework of classification, we refer to each
pair of (event/relation, temporal expression) oc-
curring within a sentence as an instance. The goal
is to devise a classifier that separates positive (i.e.,
linked) instances from negative ones, i.e., pairs
where there is no link between the event/relation
and the temporal expression in question. The lat-
ter case is far more frequent, so we have an inher-
ent bias toward negative examples in our data.1
Note that the basis of the positive and nega-
tive links is the context around the target terms.
It is impossible even for humans to determine the
existence of a link based only on the two terms
without their context. For instance, given just two
words (e.g., ?said? and ?yesterday?) there is no
way to tell if it is a positive or a negative example.
We need the context to decide.
Therefore, we base our classification models on
contextual features drawn from lexical and syn-
tactic analyses of the text surrounding the target
terms. For this, we first define a feature-based
approach, then we improve it by using tree ker-
nels. These two subsections, plus the treatment
of fluent relations, are the main contributions of
this paper. In all of this work, we employ SVM
classifiers (Vapnik, 1995) for machine learning.
4.1 Feature Engineering
A manual analysis of development data provided
several intuitions about the kinds of features that
would be useful in this task. Based on this anal-
ysis and with inspiration from previous work (cf.
Boguraev and Ando (2005)) we established three
categories of features whose description follows.
Features describing events or relations. We
check whether the event or relation is phrasal, a
verb, or noun, whether it is present tense, past
tense, or progressive, the type assigned to the
event/relation by the UIMA type system used for
processing, and whether it includes certain trig-
ger words, such as reporting verbs (?said?, ?re-
ported?, etc.).
1Initially, we employed an instance filtering method to
address this, which proved to be ineffective and was subse-
quently left out.
187
Features describing temporal expressions.
We check for the presence of certain trigger words
(last, next, old, numbers, etc.) and the type of
the expression (DURATION, TIME, or DATE) as
specified by the UIMA type system.
Features describing context. We also in-
clude syntactic/structural features, such as testing
whether the relation/event dominates the temporal
expression, which one comes first in the sentence
order, and whether either of them is dominated
by a separate verb, preposition, ?that? (which of-
ten indicates a subordinate sentence) or counter-
factual nouns or verbs (which would negate the
temporal link).
It is not surprising that some of the most in-
formative features (event comes before tempo-
ral expression, time is syntactic child of event)
are strongly correlated with the baselines. Less
salient features include the test for certain words
indicating the event is a noun, a verb, and if so
which tense it has and whether it is a reporting
verb.
4.2 Tree Kernel Engineering
We expect that there exist certain patterns be-
tween the entities of a temporal link, which mani-
fest on several levels: some on the lexical level,
others expressed by certain sequences of POS
tags, NE labels, or other representations. Kernels
provide a principled way of expanding the number
of dimensions in which we search for a decision
boundary, and allow us to easily model local se-
quences and patterns in a natural way (Giuliano et
al., 2009). While it is possible to define a space
in which we find a decision boundary that sepa-
rates positive and negative instances with manu-
ally engineered features, these features can hardly
capture the notion of context as well as those ex-
plored by a tree kernel.
Tree Kernels are a family of kernel functions
developed to compute the similarity between tree
structures by counting the number of subtrees
they have in common. This generates a high-
dimensional feature space that can be handled ef-
ficiently using dynamic programming techniques
(Shawe-Taylor and Christianini, 2004). For our
purposes we used an implementation of the Sub-
tree and Subset Tree (SST) (Moschitti, 2006).
The advantages of using tree kernels are
two-fold: thanks to an existing implementation
(SVMlight with tree kernels, Moschitti (2004)), it
is faster and easier than traditional feature engi-
neering. The tree structure also allows us to use
different levels of representations (POS, lemma,
etc.) and combine their contributions, while at the
same time taking into account the ordering of la-
bels. We use POS, lemma, semantic type, and a
representation that replaces each word with a con-
catenation of its features (capitalization, count-
able, abstract/concrete noun, etc.).
We developed a shallow tree representation that
captures the context of the target terms, without
encoding too much structure (which may prevent
generalization). In essence, our tree structure in-
duces behavior somewhat similar to a string ker-
nel. In addition, we can model the tasks by pro-
viding specific markup on the generated tree. For
example, in our experiment we used the labels
EVENT (or equivalently RELATION) and TIME-
STAMP to mark our target terms. In order to re-
duce the complexity of this comparison, we focus
on the substring between event/relation and time
stamp and the rest of the tree structure is trun-
cated.
Figure 1 illustrates an example of the structure
described so far for both lemmas and POS tags
(note that the lowest level of the tree contains tok-
enized items, so their number can differ form the
actual words, as in ?attorney general?). Similar
trees are produced for each level of representa-
tions used, and for each instance (i.e., pair of time
expressions and event/relation). If a sentence con-
tains more than one event/relation, we create sep-
arate trees for each of them, which differ in the po-
sition of the EVENT/RELATION marks (at level
1 of the tree).
The tree kernel implicitly expands this struc-
ture into a number of substructures allowing us
to capture sequential patterns in the data. As we
will see, this step provides significant boosts to
the task performance.
Curiously, using a full-parse syntactic tree as
input representation did not help performance.
This is in line with our finding that syntactic re-
lations are less important than sequential patterns
(see also Section 5.2). Therefore we adopted the
?string kernel like? representation illustrated in
Figure 1.
188
Scores of supporters of detained Egyptian opposition leader Nur demonstrated outside the attorney general?s
office in Cairo last Saturday, demanding he be freed immediately.
BOW
TIME
TOK
saturday
TOK
last
TERM
TOK
cairo
TERM
TOK
in
TERM
TOK
office
TERM
TOK
attorney general
TERM
TOK
outside
EVENT
TOK
demonstrate
BOP
TIME
TOK
NNP
TOK
JJ
TERM
TOK
NNP
TERM
TOK
IN
TERM
TOK
NN
TERM
TOK
NNP
TERM
TOK
ADV
EVENT
TOK
VBD
Figure 1: Input Sentence and Tree Kernel Representations for Bag of Words (BOW) and POS tags (BOP)
5 Evaluation
We now apply our models to real world data, and
empirically demonstrate their effectiveness at the
task of temporal linking. In this section, we de-
scribe the data sets that were used for evaluation,
the baselines for comparison, parameter settings,
and the results of the experiments.
5.1 Benchmark
We evaluated our approach in 3 different tasks:
1. Linking Timestamps and Events in the IC
domain
2. Linking Timestamps and Relations in the IC
domain
3. Linking Events to Temporal Expressions
(TempEval-2, task C)
The first two data sets contained annotations
in the intelligence community (IC) domain, i.e.,
mainly news reports about terrorism. It com-
prised 169 documents. This dataset has been de-
veloped in the context of the machine reading pro-
gram (MRP) (Strassel et al 2010). In both cases
our goal is to develop a binary classifier to judge
whether the event (or relation) overlaps with the
time interval denoted by the timestamp. Success
of this classification can be measured by precision
and recall on annotated data.
We originally considered using accuracy as a
measure of performance, but this does not cor-
rectly reflect the true performance of the system:
given the skewed nature of the data (much smaller
number of positive examples), we could achieve a
high accuracy simply by classifying all instances
as negative, i.e., not assigning a time stamp at all.
We thus decided to report precision, recall and F1.
Unless stated otherwise, results were achieved via
10-fold cross-validation (10-CV).
The number of instances (i.e., pairs of event
and temporal expression) for each of the differ-
ent cases listed above was (in brackets the ratio of
positive to negative instances).
? events: 2046 (505 positive, 1541 negative)
? relations: 6526 (1847 positive, 4679 nega-
tive)
The size of the relation data set after filtering is
5511 (1847 positive, 3395 negative).
In order to increase the originally lower number
of event instances, we made use of the annotated
event-coreference as a sort of closure to add more
instances: if events A and B corefer, and there
is a link between A and time expression t, then
there is also a link between B and t. This was not
explicitly expressed in the data.
For the task at hand, we used gold standard
annotations for timestamps, events and relations.
The task was thus not the identification of these
objects (a necessary precursor and a difficult task
in itself), but the decision as to which events and
time expressions could and should be linked.
We also evaluated our system on TempEval-
2 (Verhagen et al 2010) for better comparison
189
to the state-of-the-art. TempEval-2 data included
the task of linking events to temporal expressions
(there called ?task C?), using several link types
(OVERLAP, BEFORE, AFTER, BEFORE-OR-
OVERLAP, OVERLAP-OR-AFTER). This is a
bit different from our settings as it required the
implementation of a multi-class classifier. There-
fore we trained three different binary classifiers
(using the same feature set) for the first three of
those types (for which there was sufficient train-
ing data) and we used a one-versus-all strategy to
distinguish positive from negative examples. The
output of the system is the category with the high-
est SVM decision score. Since we only use three
labels, we incur an error every time the gold la-
bel is something else. Note that this is stricter
than the evaluation in the actual task, which left
contestants with the option of skipping examples
their systems could not classify.
5.2 Baselines
Intuitively, one would expect temporal expres-
sions to be close to the event they denote, or even
syntactically related. In order to test this, we ap-
plied two baselines. In the first, each temporal ex-
pression was linked to the closest event (as mea-
sured in token distance). In the second, we at-
tached each temporal expression to its syntactic
head, if the head was an event. Results are re-
ported in Figure 2.
While these results are encouraging for our
task, it seems at first counter-intuitive that the
syntactic baseline does worse than the proximity-
based one. It does, however, reveal two facts:
events are not always synonymous with syntactic
units, and they are not always bound to tempo-
ral expressions through direct syntactic links. The
latter makes even more sense given that the links
can even occur across sentence boundaries. Pars-
ing quality could play a role, yet seems far fetched
to account for the difference.
More important than syntactic relations seem
to be sequential patterns on different levels, a fact
we exploit with the different tree representations
used (POS tags, NE types, etc.).
For relations, we only applied the closest-
relation baseline. Since relations consist of two or
more arguments that occur in different, often sep-
arated syntactic constituents, a syntactic approach
seems futile, especially given our experience with
events. Results are reported in Figure 3.
baseline comparison
Page 1
Precision Recall F1
0
20
40
60
80
100
35.0
63.0
45.048.0
88.0
62.063.0
75.4 68.376.6 76.5 76.2
Evaluation Measures Events
BL-parent BL-closest features +tree kernel
metric
%
Figure 2: Performance on events
System Accuracy
TRIOS 65%
this work 64.5%
JU-CSE, NCSU-indi
TRIPS, USFD2
all 63%
Table 1: Comparison to Best Systems in TempEval-2
5.3 Events
Figure 2 shows the improvements of the feature-
based approach over the two baseline, and the ad-
ditional gain obtained by using the tree kernel.
Both the features and tree kernels mainly improve
precision, while the tree kernel adds a small boost
in recall. It is remarkable, though, that the closest-
event baseline has a very high recall value. This
suggests that most of the links actually do occur
between items that are close to one another. For a
possible explanation for the low precision value,
see the error analysis (Section 5.5).
Using a two-tailed t-test, we compute the sig-
nificance in the difference between the F1-scores.
Both the feature-based and the tree kernel ap-
proach improvements are statistically significant
at p < 0.001 over the baseline scores.
Table 1 compares the performances of our sys-
tem to the state-of-the-art systems on TempEval-2
Data, task C, showing that our approach is very
competitive. The best systems there used sequen-
tial models. We attribute the competitive nature
of our results to the use of tree kernels, which en-
ables us to make use of contextual information.
5.4 Relations
In general, performance for relations is not as high
as for events (see Figure 3). The reason here is
two-fold: relations consist of two (or more) ele-
ments, which can be in various positions with re-
spect to one another and the temporal expression,
and each relation can be expressed in a number of
190
baseline comparison
Page 1
Precision Recall F1
0
10
20
30
40
50
60
70
80
90
100
35.0
24.0 29.0
63.1
80.6
70.470.8 74.0 72.2
Evaluation Metric Relations
BL-closest features +tree kernel
metric
%
Figure 3: Performance on relations/fluents
learning curves
Page 1
0 10 20 30 40 50 60 70 80 90 100
40
45
50
55
60
65
70
75
80
Learning Curves Relations
features w/ tree 
kernel
% of data
F1
 sc
or
e
Figure 4: Learning curves for relation models
different ways.
Again, we perform significance tests on the dif-
ference in F1 scores and find that our improve-
ments over the baseline are statistically significant
at p < 0.001. The improvement of the tree kernel
over the feature-based approach, however, are not
statistically significant at the same value.
The learning curve over parts of the training
data (exemplary shown here for relations, Figure
4)2 indicates that there is another advantage to us-
ing tree kernels: the approach can benefit from
more data. This is conceivably because it allows
the kernel to find more common subtrees in the
various representations the more examples it gets,
while the feature space rather finds more instances
that invalidate the expressiveness of features (i.e.,
it encounters positive and negative instances that
have very similar feature vectors). The curve sug-
gests that tree kernels could yield even better re-
sults with more data, while there is little to no ex-
pected gain using only features.
5.5 Error Analysis
Examining the misclassified examples in our data,
we find that both feature-based and tree-kernel
approaches struggle to correctly classify exam-
2The learning curve for events looks similar and is omit-
ted due to space constraints.
ples where time expression and event/relation are
immediately adjacent, but unrelated, as in ?the
man arrested last Tuesday told the police ...?,
where last Tuesday modifies arrested. It limits
the amount of context that is available to the tree
kernels, since we truncate the tree representations
to the words between those two elements. This
case closely resembles the problem we see in the
closest-event/relation baseline, which, as we have
seen, does not perform too well. In this case, the
incorrect event (?told?) is as close to the time ex-
pression as the correct one (?arrested?), resulting
in a false positive that affects precision. Features
capturing the order of the elements do not seem
help here, since the elements can be arranged in
any order (i.e., temporal expression before or af-
ter the event/relation). The only way to solve this
problem would be to include additional informa-
tion about whether a time expression is already
attached to another event/relation.
5.6 Ablations
To quantify the utility of each tree representation,
we also performed all-but-one ablation tests, i.e.,
left out each of the tree representations in turn, ran
10-fold cross-validation on the data and observed
the effect on F1. The larger the loss in F1, the
more informative the left-out-representation. We
performed ablations for both events and relations,
and found that the ranking of the representations
is the same for both.
In events and relations alike, leaving out POS
trees has the greatest effect on F1, followed by
the feature-bundle representation. Lemma and se-
mantic type representation have less of an impact.
We hypothesize that the former two capture un-
derlying regularities better by representing differ-
ent words with the same label. Lemmas in turn
are too numerous to form many recurring pat-
terns, and semantic type, while having a smaller
label alphabet, does not assign a label to every
word, thus creating a very sparse representation
that picks up more noise than signal.
In preliminary tests, we also used annotated
dependency trees as input to the tree kernel, but
found that performance improved when they were
left out. This is at odds with work that clearly
showed the value of syntactic tree kernels (Mir-
roshandel et al 2011). We identify two poten-
tial causes?either our setup was not capable of
correctly capturing and exploiting the information
191
from the dependency trees, or our formulation of
the task was not amenable to it. We did not inves-
tigate this further, but leave it to future work.
6 Conclusion and Future Work
We cast the problem of linking events and rela-
tions to temporal expressions as a classification
task using a combination of features and tree ker-
nels, with probabilistic type filtering. Our main
contributions are:
? We showed that within-sentence temporal
links for both events and relations can be ap-
proached with a common strategy.
? We developed flat tree representations and
showed that these produce considerable
gains, with significant improvements over
different baselines.
? We applied our technique without great ad-
justments to an existing data set and achieved
competitive results.
? Our best systems achieve F1 score of 0.76
on events and 0.72 on relations, and are ef-
fective at the task of temporal linking.
We developed the models as part of a machine
reading system and are currently evaluating it in
an end-to-end task.
Following tasks proposed in TempEval-2, we
plan to use our approach for across-sentence clas-
sification, as well as a similar model for linking
entities to the document creation date.
Acknowledgements
We would like to thank Alessandro Moschitti for
his help with the tree kernel setup, and the review-
ers who supplied us with very constructive feed-
back. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA
Machine Reading Program.
References
Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw,
James Fan, Noah Friedland, Michael Glass, Jerry
Hobbs, Eduard Hovy, David Israel, Doo Soon Kim,
Rutu Mulkar-Mehta, Sourabh Patwardhan, Bruce
Porter, Dan Tecuci, and Peter Yeh. 2007. Learn-
ing by reading: A prototype system, performance
baseline and lessons learned. In Proceedings of
the 22nd National Conference for Artificial Intelli-
gence, Vancouver, Canada, July.
Branimir Boguraev and Rie Kubota Ando. 2005.
Timeml-compliant text analysis for temporal rea-
soning. In Proceedings of IJCAI, volume 5, pages
997?1003. IJCAI.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. pages
789?797. Association for Computational Linguis-
tics.
Peter Clark and Phil Harrison. 2010. Machine read-
ing as a process of partial question-answering. In
Proceedings of the NAACL HLT Workshop on For-
malisms and Methodology for Learning by Reading,
Los Angeles, CA, June.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion program ? tasks, data and evaluation. In Pro-
ceedings of the LREC Conference, Canary Islands,
Spain, July.
Oren Etzioni, Michele Banko, and Michael Cafarella.
2007. Machine reading. In Proceedings of the
AAAI Spring Symposium Series, Stanford, CA,
March.
Elena Filatova and Eduard Hovy. 2001. Assigning
time-stamps to event-clauses. In Proceedings of
the workshop on Temporal and spatial information
processing, volume 13, pages 1?8. Association for
Computational Linguistics.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and
Carlo Strapparava. 2009. Kernel methods for min-
imally supervised wsd. Computational Linguistics,
35(4).
Seyed A. Mirroshandel, Mahdy Khayyamian, and
Gholamreza Ghassem-Sani. 2011. Syntactic tree
kernels for event-time temporal relation learning.
Human Language Technology. Challenges for Com-
puter Science and Linguistics, pages 213?223.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 335?es. Associa-
tion for Computational Linguistics.
Alessandro Moschitti. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of EACL, volume 6.
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs. 2006.
Learning event durations from event descriptions.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 393?400. Association for Computa-
tional Linguistics.
James Pustejovsky, Patrick Hanks, Roser Saur?, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
192
Ferro, and Marcia Lazo. 2003. The TIMEBANK
Corpus. In Proceedings of Corpus Linguistics
2003, pages 647?656.
John Shawe-Taylor and Nello Christianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA Machine Read-
ing Program-Encouraging Linguistic and Reason-
ing Research with a Series of Reading Tasks. In
Proceedings of LREC 2010.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York, NY.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Puste-
jovsky. 2007. Semeval-2007 task 15: Tempeval
temporal relation identification. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 75?80. Association for Computational
Linguistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62. Association for Computational Linguistics.
193
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 828?838,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Medical Relation Extraction with Manifold Models
Chang Wang
IBM T. J. Watson Research Center
Yorktown Heights, New York, 10598
changwangnk@gmail.com
James Fan
IBM T. J. Watson Research Center
Yorktown Heights, New York, 10598
fanj@us.ibm.com
Abstract
In this paper, we present a manifold model
for medical relation extraction. Our model
is built upon a medical corpus containing
80M sentences (11 gigabyte text) and de-
signed to accurately and efficiently detect
the key medical relations that can facilitate
clinical decision making. Our approach
integrates domain specific parsing and typ-
ing systems, and can utilize labeled as well
as unlabeled examples. To provide users
with more flexibility, we also take label
weight into consideration. Effectiveness
of our model is demonstrated both theo-
retically with a proof to show that the so-
lution is a closed-form solution and exper-
imentally with positive results in experi-
ments.
1 Introduction
There exists a vast amount of knowledge sources
and ontologies in the medical domain. Such in-
formation is also growing and changing extremely
quickly, making the information difficult for peo-
ple to read, process and remember. The combi-
nation of recent developments in information ex-
traction and the availability of unparalleled medi-
cal resources thus offers us the unique opportunity
to develop new techniques to help healthcare pro-
fessionals overcome the cognitive challenges they
face in clinical decision making.
Relation extraction plays a key role in informa-
tion extraction. Using question answering as an
example (Wang et al, 2012): in question analy-
sis, the semantic relations between the question
focus and each term in the clue can be used to
identify the weight of each term so that better
search queries can be generated. In candidate an-
swer generation, relations enable the background
knowledge base to be used for potential candidate
answer generation. In candidate answer scoring,
relation-based matching algorithms can go beyond
explicit lexical and syntactic information to detect
implicit semantic relations shared across the ques-
tion and passages.
To construct a medical relation extraction sys-
tem, several challenges have to be addressed:
? The first challenge is how to identify a set of
relations that has sufficient coverage in the
medical domain. To address this issue, we
study a real-world diagnosis related question
set and identify a set of relations that has a
good coverage of the clinical questions.
? The second challenge is how to efficiently de-
tect relations in a large amount of medical
text. The medical corpus underlying our re-
lation extraction system contains 80M sen-
tences (11 gigabytes pure text). To extract
relations from a dataset at this scale, the re-
lation detectors have to be fast. In this paper,
we speed up relation detectors through pars-
ing adaptation and replacing non-linear clas-
sifiers with linear classifiers.
? The third challenge is that the labeled rela-
tion examples are often insufficient due to the
high labeling cost. When we build a na??ve
model to detect relations, the model tends to
overfit for the labeled data. To address this
issue, we develop a manifold model (Belkin
et al, 2006) that encourages examples (in-
cluding both labeled and unlabeled exam-
ples) with similar contents to be assigned
with similar scores. Our model goes beyond
regular regression models in that it applies
constraints to those coefficients, such that the
topology of the given data manifold will be
respected. Computing the optimal weights
in a regression model and preserving mani-
fold topology are conflicting objectives, we
828
present a closed-form solution to ideally bal-
ance these two goals.
The contributions of this paper on medical rela-
tion extraction are three-fold:
? The problem setup is new. There is a
?fundamental? difference between our prob-
lem setup and the conventional setups, like
i2b2 (Uzuner et al, 2011). In i2b2 rela-
tion extraction task, entity mentions are man-
ually labeled, and each mention has 1 of 3
concepts: ?treatment?, ?problem?, and ?test?.
To resemble real-world medical relation ex-
traction challenges where perfect entity men-
tions do not exist, our new setup requires
the entity mentions to be automatically de-
tected. The most well-known tool to detect
medical entity mentions is MetaMap (Aron-
son, 2001), which considers all terms as en-
tities and automatically associates each term
with a number of concepts from UMLS CUI
dictionary (Lindberg et al, 1993) with more
than 2.7 million distinct concepts (compared
to 3 in i2b2). The huge amount of entity
mentions, concepts and noisy concept assign-
ments provide a tough situation that people
have to face in real-world applications.
? From the perspective of relation extraction
applications, we identify ?super relations?-
the key relations that can facilitate clinical
decision making (Table 1). We also present
approaches to collect training data for these
relations with a small amount of labeling ef-
fort.
? From the perspective of relation extraction
methodologies, we present a manifold model
for relation extraction utilizing both labeled
and unlabeled data. Our approach can also
take the label weight into consideration.
The experimental results show that our relation
detectors are fast and outperform the state-of-the-
art approaches on medical relation extraction by a
large margin. We also apply our model to build a
new medical relation knowledge base as a comple-
ment to the existing knowledge bases.
2 Background
2.1 Medical Ontologies and Sources
Medical domain has a huge amount of natural lan-
guage content found in textbooks, encyclopedias,
guidelines, electronic medical records, and many
other sources. It is also growing at an extremely
high speed. Substantial understanding of the med-
ical domain has already been included in the Uni-
fied Medical Language System (UMLS) (Lind-
berg et al, 1993), which includes medical con-
cepts, relations, definitions, etc. The 2012 version
of the UMLS contains information about more
than 2.7 million concepts from over 160 source
vocabularies. Softwares for using this knowledge
also exist: MetaMap (Aronson, 2001) is able to
identify concepts in text. SEMREP (Rindflesch
and Fiszman, 2003) can detect some relations us-
ing hand-crafted rules.
2.2 Relation Extraction
To extract semantic relations from text, three types
of approaches have been applied. Rule-based
methods (Miller et al, 2000) employ a number
of linguistic rules to capture relation patterns.
Feature-based methods (Kambhatla, 2004; Zhao
and Grishman, 2005) transform relation instances
into a large amount of linguistic features like lex-
ical, syntactic and semantic features, and capture
the similarity between these feature vectors. Re-
cent results mainly rely on kernel-based meth-
ods. Many of them focus on using tree kernels to
learn parse tree structure related features (Collins
and Duffy, 2001; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005).
Other researchers study how different ap-
proaches can be combined to improve the extrac-
tion performance. For example, by combining tree
kernels and convolution string kernels, (Zhang et
al., 2006) achieved the state of the art performance
on ACE data (ACE, 2004). Recently, ?distant su-
pervision? has emerged to be a popular choice for
training relation extractors without using manually
labeled data (Mintz et al, 2009; Jiang, 2009; Chan
and Roth, 2010; Wang et al, 2011; Riedel et al,
2010; Ji et al, 2011; Hoffmann et al, 2011; Sur-
deanu et al, 2012; Takamatsu et al, 2012; Min et
al., 2013).
Various relation extraction approaches have
been adapted to the medical domain, most of
which focus on designing heuristic rules targeted
for diagnosis and integrating the medical ontology
in the existing extraction approaches. Results of
some of these approaches are reported on the i2b2
data (Uzuner et al, 2011).
829
3 Identifying Key Medical Relations
3.1 Super Relations in Medical Domain
The first step in building a relation extraction sys-
tem for medical domain is to identify the relations
that are important for clinical decision making.
Four main clinical tasks that physicians engage
in are discussed in (Demner-Fushman and Lin,
2007). They are Therapy- select treatments to of-
fer a patient, taking consideration of effectiveness,
risk, cost and other factors (prevention is under the
general category of Therapy), Diagnosis (includ-
ing differential diagnosis based on findings and di-
agnostic test), Etiology- identify the factors that
cause the disease and Prognosis- estimate the pa-
tient?s likely course over time. These activities can
be translated into ?search tasks?. For example, the
search for therapy is usually the therapy selection
given a disease.
We did an independent study regarding what
clinical questions usually ask for on a set of 5,000
Doctor Dilemma (DD) questions from the Ameri-
can College of Physicians (ACP). This set includes
questions about diseases, treatments, lab tests, and
general facts1. Our analysis shows that about 15%
of these questions ask for treatments, preventions
or contraindicated drugs for a disease or another
way around, 4% are about diagnosis tests, 6% are
about the causes of a disease, 1% are about the lo-
cations of a disease, 25% are about the symptoms
of a disease, 8% are asking for definitions, 7% are
about guidelines and the remaining 34% questions
either express no relations or some relations that
are not very popular.
Based on the analysis in (Demner-Fushman and
Lin, 2007) and our own results, we decided to fo-
cus on seven key relations in the medical domain,
which are described in Table 1. We call these re-
lations ?super relations?, since they cover most
questions in the DD question set and align well
with the analysis result in (Demner-Fushman and
Lin, 2007).
3.2 Collect Training Data
This section presents how we collect training data
for each relation. The overall procedure is illus-
trated in Figure 1.
1Here?s an example of these questions and its answer:
Question: The syndrome characterized by joint pain, abdom-
inal pain, palpable purpura, and a nephritic sediment. An-
swer: Henoch-Schonlein purpura.
Large Amount of
Noisy Relation
Data
Medical Text
Relation Knowledge in
Medical Domain
Training Data for
Each Relation
For each relation, choose a
small amount of the most
representative examples
Annotation
Unlabeled Data
Labeled Data
Figure 1: Collect Training Data
Our medical corpus has incorporated a set
of medical books/journals2 and MEDLINE ab-
stracts. We also complemented these sources with
Wikipedia articles. In total, the corpus contains
80M sentences (11 gigabyte pure text).
The UMLS 2012 Release contains more than
600 relations and 50M relation instances under
around 15 categories. The RO category (RO
stands for ?has Relationship Other than synony-
mous, narrower, or broader?) is the most inter-
esting one, and covers relations like ?may treat?,
?has finding site?, etc. Each relation has a
certain number of Concept Unique Identifier
(CUI) pairs that are known to bear that rela-
tion. In UMLS, some relation information is
redundant. Firstly, half of these relations are
simply inverse of each other (e.g. the relation
?may treat? and ?may be treated by?). Secondly,
there is a significant amount of redundancy even
among non-inverse relations (e.g. the relation
?has manifestation? and ?disease has finding?).
From UMLS relations, we manually chose a
subset of them that are directly related to the su-
per relations discussed in Section 3.1. The cor-
respondences between them are given in Table 1.
One thing to note is that super relations are more
general than the UMLS relations, and one super
relation might integrate multiple UMLS relations.
Using the CUI pairs in the UMLS relation knowl-
2This is a full list of the books and journals used in
our corpus: ACP-Medical Knowledge Self-Assessment Pro-
gram, EBSCO-Dynamed, EBSCO-Quick Lessons, EBSCO-
EBCS, EBSCO-Clinical Review, Wiley-Essential Evidence
Plus: EBMG Guidelines, Wiley-Essential Evidence Topics,
Wiley-Essential Evidence Plus: EBMG Summaries, Wiley-
POEMs, Wiley-The Breast Journal, New England Journal
of Medicine, Journal Watch, NCCN-CME, NCCN-GUS,
NCCN-Compendium, NCCN-Templates, NCCN-Guidelines
for Patients, NCCN-Physician Guidelines, Merck Manual of
Diagnosis and Therapy, and UpToDate.
830
Table 1: Super relations & their arguments, UMLS sources and noise% in the annotation data
Super Relations Argument 1 Argument 2 UMLS Sources Noise% in Annotation Data
treats disease treatments may treat, treats 16%
prevents disease treatments may prevent 49%
contraindicates disease treatments contraindicated drug 97%
diagnoses disease tests may diagnose 63%
causes disease causes cause of, causative agent of 66%
location of disease locations has finding site 41%
disease has primary anatomic site
symptom of disease symptoms disease has finding 66%
disease may have finding
has manifestation
has definitional manifestation
edge base, we associate each super relation with a
set of CUI pairs.
To collect the training data for each super re-
lation, we need to collect sentences that express
the relation. To achieve this, we parsed all 80M
sentences in our medical corpus, looking for the
sentences containing the terms that are associated
with the CUI pairs in the knowledge base. This
(distant supervision) approach resulted in a huge
amount of sentences that contain the desired rela-
tions, but also brought in a lot of noise in the form
of false positives. For example, we know from
the knowledge base that ?antibiotic drug? may
treat ?Lyme disease?. However the sentence ?This
paper studies the relationship between antibiotic
drug and Lyme disease? contains both terms but
does not express the ?treats? relation.
The most reliable way to clean the training data
is to ask annotators to go through the sentences
and assign the sentences with positive/negative la-
bels. However, it will not work well when we have
millions of sentences to vet. To minimize the hu-
man labeling effort, we ran a K-medoids clustering
on the sentences associated with each super rela-
tion and kept the cluster centers as the most rep-
resentative sentences for annotation. Depending
on the number of the sentences we collected for
each relation, the #clusters was chosen from 3,000
- 6,000. The similarity of two sentences is defined
as the bag-of-words similarity of the dependency
paths connecting arguments. Part of the resulting
data was manually vetted by our annotators, and
the remaining was held as unlabeled data for fur-
ther experiments.
Our relation annotation task is quite straightfor-
ward, since both arguments are given and the de-
cision is a Yes-or-No decision. The noise rate of
each relation (#sentences expressing the relation
/ #sentences) is reported in Table 1 based on the
annotation results. The noise rates differ signifi-
cantly from one relation to another. For ?treats?
relation, only 16% of the sentences are false posi-
tives. For ?contraindicates? relation, the noise rate
is 97%.
To grow the size of the negative training set for
each super relation, we also added a small amount
of the most representative examples (also coming
from K-medoids clustering) from each unrelated
UMLS relation to the training set as negative ex-
amples. This resulted in more than 10,000 extra
negative examples for each relation.
3.3 Parsing and Typing
The most well-known tool to detect medical en-
tity mentions is MetaMap (Aronson, 2001), which
considers all terms as entities and automatically
associates each term with a number of concepts
from UMLS CUI dictionary (Lindberg et al,
1993) with 2.7 million distinct concepts.
The parser used in our system is Medi-
calESG, an adaptation of ESG (English Slot
Grammar) (McCord et al, 2012) to the medical
domain with extensions of medical lexicons inte-
grated in the UMLS 2012 Release. Compared to
MetaMap, MedicalESG is based on the same med-
ical lexicons, 10 times faster and produces very
similar parsing results.
We use the semantic types defined in
UMLS (Lindberg et al, 1993) to categorize
argument types. The UMLS consists of a set
of 133 subject categories, or semantic types,
that provide a consistent categorization of more
than 2M concepts represented in the UMLS
Metathesaurus. Our system assigns each relation
argument with one or more UMLS semantic types
through a two step process. Firstly, we use Med-
icalESG to process the input sentence, identify
segments of text that correspond to concepts in
831
Figure 2: A Parse Tree Example
the UMLS Metathesaurus and associate each of
them with one or more UMLS CUIs (Concept
Unique Identifier). Then we do a CUI lookup in
UMLS to find the corresponding semantic types
for each CUI.
Most relation arguments are associated with
multiple semantic types. For example, the term
?tetracycline hydrochloride? has two types: ?Or-
ganic Chemical? and ?Antibiotic?. Sometimes,
the semantic types are noisy due to ambiguity of
terms. For example, the term ?Hepatitis b? is asso-
ciated with both ?Pharmacologic Substance? and
?Disease or Syndrome? based on UMLS. The rea-
son for this is that people use ?Hepatitis b? to rep-
resent both ?the disease of Hepatitis b? and ?Hep-
atitis b vaccine?, so UMLS assigns both types to it.
This is a concern for relation extraction, since two
types bear opposite meanings. Our current strat-
egy is to integrate all associated types, and rely on
the relation detector trained with the labeled data
to decide how to weight different types based upon
the context.
Here is an illustrative example. Consider the
sentence: ?Antibiotics are the standard therapy
for Lyme disease?: MedicalESG first generates
a dependency parse tree (Figure 2) to represent
grammatical relations between the words in the
sentence, and then associates the words with CUIs.
For example, ?Antibiotics? is associated with CUI
?C0003232? and ?Lyme disease? is associated
with two CUIs: ?C0024198? and ?C0717360?.
CUI lookup will assign ?Antibiotics? with a se-
mantic type ?Antibiotic?, and ?Lyme disease? with
three semantic types: ?Disease or Syndrome?,
?Pharmacologic Substance? and ?Immunologic
Factor?. This sentence expresses a ?treats? rela-
tion between ?Antibiotics? and ?Lyme disease?.
4 Relation Extraction with Manifold
Models
4.1 Motivations
Given a few labeled examples and many unlabeled
examples for a relation, we want to build a re-
lation detector leveraging both labeled and unla-
beled data. Following the manifold regularization
idea (Belkin et al, 2006), our strategy is to learn
a function that assigns a score to each example.
Scores are fit so that examples (both labeled and
unlabeled) with similar content get similar scores,
and scores of labeled examples are close to their
labels. Integration of the unlabeled data can help
solve overfitting problems when the labeled data
is not sufficient.
4.2 Features
We use 8 groups of features to represent each rela-
tion example. These features are commonly used
for relation extraction.
? (1) Semantic types of argument 1, such as
?Antibiotic?.
? (2) Semantic types of argument 2.
? (3) Syntactic features representing the depen-
dency path between two arguments, such as
?subj?, ?pred?, ?mod nprep? and ?objprep?
(between arguments ?antibiotic? and ?lyme
disease?) in Figure 2.
? (4) Features modeling the incoming and out-
going links of both arguments. These fea-
tures are useful to determine if a relation goes
from argument 1 to argument 2 or vice versa.
? (5) Topic features modeling the words in
the dependency path. In the example given
in Figure 2, the dependency path contains
the following words: ?be?, ?standard ther-
apy? and ?for?. These features as well as
the features in (6) are achieved by projecting
the words onto a 100 dimensional LSI topic
space (Deerwester et al, 1990) constructed
from our medical corpus.
? (6) Topic features modeling the words in the
whole sentence.
? (7) Bag-of-words features modeling the de-
pendency path. In (7) and (8), we only con-
sider the words that have occurred in the pos-
itive training data.
832
Notations:
The input dataset X = {x
1
, ? ? ? , x
m
} is repre-
sented as a feature-instance matrix.
The desired label vector Y = {y
1
, ? ? ? , y
l
} repre-
sents the labels of {x
1
, ? ? ? , x
l
}, where l ? m.
W is a weight matrix, where W
i,j
= e
??x
i
?x
j
?
2
models the similarity of x
i
and x
j
.
?x
i
? x
j
? stands for the Euclidean distance be-
tween x
i
and x
j
in the vector space.
D is a diagonal matrix: D
i,i
=
?
j
W
i,j
.
L = D
?0.5
(D ?W )D
?0.5 is called normalized
graph Laplacian matrix.
? is a user defined l ? l diagonal matrix, where
?
i
represents the weight of label y
i
.
A =
(
? 0
0 0
)
is an m?m matrix.
V = [y
1
, ? ? ? y
l
, 0, ? ? ? , 0] is a 1?m matrix.
? is a weight scalar.
()
+ represents pseudo inverse.
Algorithm:
1. Represent each example using features:
X = {x
1
, ? ? ? , x
m
}, where x
i
is the ith ex-
ample.
2. Construct graph Laplacian matrix L
modeling the data manifold.
3. Construct vector V = [y
1
, ? ? ? y
l
, 0, ? ? ? , 0].
4. Compute projection function f for each
relation: f = (X(A+ ?L)XT )+XAV T .
Figure 3: Notations and the Algorithm to Train a
Manifold Model for Relation Extraction
? (8) Bag-of-words features modeling the
whole sentence.
In relation extraction, many recent approaches
use non-linear kernels to get the similarity of two
relation examples. To classify a relation exam-
ple, a lot of dot product computations are required.
This is very time consuming and becomes a bottle-
neck in using relation extraction to facilitate clin-
ical decision making. To speed up the classifier
during the apply time, we decided to use a linear
classifier instead of non-linear classifiers.
We represent all features in a single feature
space. For example, we use a vector of 133 en-
tries (UMLS contains 133 semantic types) to rep-
resent the types of argument 1. If argument 1 is
associated with two types: ?Organic Chemical?
and ?Antibiotic?, we set the two corresponding en-
tries to 1 and all the other entries to 0. Similar ap-
proaches are used to represent the other features.
4.3 The Main Algorithm
The problem we want to solve is formalized as fol-
lows: given a relation dataset X = {x
1
, ? ? ? , x
m
},
and the desired label Y = {y
1
, ? ? ? , y
l
} for
{x
1
, ? ? ? , x
l
}, where l ? m, we want to construct
a mapping function f to project any example x
i
to
a new space, where fTx
i
matches x
i
?s desired la-
bel y
i
. In addition, we also want f to preserve the
manifold topology of the dataset, such that similar
examples (both labeled and unlabeled) get simi-
lar scores. Here, the label is ?+1? for positive ex-
amples, and ?-1? for negative examples. Notations
and the main algorithm to construct f for each re-
lation are given in Figure 3.
4.4 Justification
The solution to the problem defined in Section 4.3
is given by the mapping function f to minimize
the following cost function:
C(f) =
?
i?l
?
i
(f
T
x
i
? y
i
)
2
+ ?
?
i,j
W
i,j
(f
T
x
i
? f
T
x
j
)
2
.
The first term of C(f) is based on labeled ex-
amples, and penalizes the difference between the
mapping result of x
i
and its desired label y
i
. ?
i
is
a user specified parameter, representing the weight
of label y
i
. The second term of C(f) does not take
label information into account. It encourages the
neighborhood relationship (geometry of the man-
ifold) within X to be preserved in the mapping.
When x
i
and x
j
are similar, the corresponding
W
i,j
is big. If f maps x
i
and x
j
to different posi-
tions, f will be penalized. The second term is use-
ful to bound the mapping function f and prevents
overfitting from happening. Here ? is the weight
of the second term. When ? = 0, the model dis-
regards the unlabeled data, and the data manifold
topology is not respected.
Compared to manifold regularization (Belkin
et al, 2006), we do not include the RKHS norm
term. Instead, we associate each labeled example
with an extra weight for label confidence. This
weight is particularly useful when the training
data comes from ?Crowdsourcing?, where we ask
833
multiple workers to complete the same task to
correct errors. In that scenario, weights can be as-
signed to labels based upon annotator agreement.
Theorem 1: f = (X(A + ?L)XT )+XAV T
minimizes the cost function C(f).
Proof:
Given the input X , we want to find the optimal
mapping function f such that C(f) is minimized:
f = argmin
f
C(f).
It can be verified that
?
i?l
?
i
(f
T
x
i
? y
i
)
2
= f
T
XAX
T
f ? 2f
T
XAV
T
+ VAV
T
.
We can also verify that
?
?
i,j
(f
T
x
i
? f
T
x
j
)
2
W
i,j
= ?f
T
XLX
T
f.
So C(f) can be written as
f
T
XAX
T
f ? 2f
T
XAV
T
+ VAV
T
+ ?f
T
XLX
T
f.
Using the Lagrange multiplier trick to differentiate
C(f) with respect to f , we have
2XAX
T
f + 2?XLX
T
f = 2XAV
T
.
This implies that
X(A+ ?L)X
T
f = XAV
T
.
So
f = (X(A+ ?L)X
T
)
+
XAV
T
,
where ?+? represents pseudo inverse.
4.5 Advantages
Our algorithm offers the following advantages:
? The algorithm exploits unlabeled data, which
helps prevent ?overfitting? from happening.
? The algorithm provides users with the flex-
ibility to assign different labels with differ-
ent weights. This feature is useful when the
training data comes from ?crowdsourcing? or
?distant supervision?.
? Different from many approaches in this area,
our algorithm provides a closed-form solu-
tion of the result. The solution is global opti-
mal regarding the cost function C(f).
? The algorithm is computationally efficient at
the apply time (as fast as linear regressions).
5 Experiments
5.1 Cross-Validation Test
We use a cross-validation test3 with the relation
data generated in Section 3.2 to compare our ap-
proaches against the state-of-the-art approaches.
The task is to classify the examples into positive
or negative for each relation. We applied a 5-fold
cross-validation. In each round of validation, we
used 20% of the data for training and 80% for test-
ing. The F
1
scores reported here are the average
of all 5 rounds. We used MedicalESG to process
the input text for all approaches.
5.1.1 Data and Parameters
This dataset includes 7 relations. We do not con-
sider the relation of ?contraindicates? in this test,
since it has too few positive examples. On average,
each relation contains about 800 positive examples
and more than 13,000 negative examples. To elim-
inate the examples that are trivial to classify, we
removed the negative examples that do not bear
the valid argument types. This removed the exam-
ples that can be easily classified by a type filter,
resulting in 3,000 negatives on average per rela-
tion. For each relation, we also collected 5,000
unlabeled examples and put them into two sets:
unlabeled set 1 and 2 (2,500 examples in each set).
No parameter tuning was taken and no relation
specific heuristic rules were applied in all tests. In
all manifold models, ? = 1. In SVM implemen-
tations, the trade-off parameter between training
error and margin was set to 1 for all experiments.
5.1.2 Baseline Approaches
We compare our approaches to three state-of-the-
art approaches including SVM with convolution
tree kernels (Collins and Duffy, 2001), linear re-
gression and SVM with linear kernels (Scho?lkopf
and Smola, 2002). To adapt the tree kernel to med-
ical domain, we followed the approach in (Nguyen
et al, 2009) to take the syntactic structures into
consideration. We also added the argument types
as features to the tree kernel. In the tree kernel im-
plementation, we assigned the tree structure and
the vector corresponding to the argument types
3If we take the perfect entity mentions and the associated
concepts provided by i2b2 (Uzuner et al, 2011) as the input,
our system can directly apply to i2b2 relation extraction data.
However, the i2b2 data has a tough data use agreement. Our
legal team held several rounds of negotiations with the i2b2
data owner and then decided we should not use it due to the
high legal risks. We are not aware of other available medical
relation extraction datasets that fit for our evaluations.
834
Table 2: F
1
Scores from a Five-Fold Cross Validation Experiment
SVM SVM Linear Manifold Manifold Manifold Manifold
Tree Linear Regression Unlabeled Predicted Labels Predicted Labels Unlabeled+Predicted
Kernel Kernel with Weights without Weights Labels with Weights
treats 0.7648 0.7850 0.7267 0.8025 0.8041 0.7884 0.8085
prevents 0.2859 0.3887 0.3922 0.5502 0.5696 0.6349 0.6332
causes 0.3885 0.5024 0.5219 0.5779 0.5088 0.3978 0.5081
location of 0.6113 0.6009 0.4968 0.7275 0.7363 0.6964 0.7454
diagnoses 0.5520 0.4934 0.3202 0.6468 0.6485 0.5720 0.6954
symptom of 0.4398 0.5611 0.5984 0.6347 0.5314 0.4515 0.5968
average 0.5071 0.5553 0.5094 0.6566 0.6331 0.5902 0.6646
with equal weights. The SVM with linear kernels
and the linear regression model used the same fea-
tures as the manifold models.
5.1.3 Settings for the Manifold Models
We tested our manifold model for each relation un-
der three different settings:
(1) Manifold Unlabeled: We combined the la-
beled data and unlabeled set 1 in training. We set
?
i
= 1 for i ? [1, l].
(2) Manifold Predicted Labels: We combined
labeled data and unlabeled set 2 in training. ?
i
=
1 for i ? [1, l]. Different from the previous set-
ting, we gave a label estimation to all the exam-
ples in the unlabeled set 2 based on the noise rate
(Noise%) from Table 1. The label of all unla-
beled examples was set to ?+1? when 100% ? 2 ?
Noise% > 0, or ?-1? otherwise. Two weighting
strategies were applied:
? With Weights: We let label weight ?
i
=
|100%? 2 ?Noise%| for all x
i
coming from
the unlabeled set 2. This setting represents an
empirical rule to estimate the label and con-
fidence of each unlabeled example based on
the sampling result.
? Without Weights: ?
i
is always set to 1.
(3) Manifold UnLabeled+Predicted Labels: a
combination of setting (1) and (2). In this setting,
the data from unlabeled set 1 was used as unla-
beled data and the data from unlabeled set 2 was
used as labeled data (With Weights).
5.1.4 Results
The results are summarized in Table 2.
The tree kernel-based approach and linear re-
gression achieved similar F
1
scores, while linear
SVM made a 5% improvement over them. One
thing to note is that the results from these ap-
proaches vary significantly. The reason for this is
that the labeled training data is not sufficient. So
the approaches that completely depend on the la-
beled data are likely to run into overfitting. Linear
SVM performed better than the other two, since
the large-margin constraint together with the lin-
ear model constraint can alleviate overfitting.
By integrating unlabeled data, the manifold
model under setting (1) made a 15% improvement
over linear regression model on F
1
score, where
the improvement was significant across all rela-
tions.
Under setting (2), the With Weights strategy
achieved a slightly worse F
1
score than the previ-
ous setting but much better result than the baseline
approaches. This tells us that estimating the label
of unlabeled examples based upon the sampling
result is one way to utilize unlabeled data and may
help improve the relation extraction results. The
results also show that the label weight is important
for this setting, since the Without Weights strategy
did not perform very well.
Compared to setting (1) and (2), setting (3)
made use of 2,500 more unlabeled examples,
and achieved the best performance among all ap-
proaches. On one hand, this result shows that
using more unlabeled data can further improve
the result. On the other hand, the insignificant
improvement over (1) and (2) strongly indicates
that how to utilize more unlabeled data to achieve
a significant improvement is non-trivial and de-
serves more attention. To what extensions the un-
labeled data can help the learning process is an
open problem. Generally speaking, when the ex-
isting data is sufficient to characterize the dataset
geometry, adding more unlabeled data will not
help (Singh et al, 2008).
We tested the tree kernel-based approach with-
out integrating the medical types as well. That re-
sulted in very poor performance: the average F
1
score was below 30%. We also applied the rules
used in SEMREP (Rindflesch and Fiszman, 2003)
to this dataset. Since the relations detected by
835
SEMREP rules cannot be perfectly aligned with
super relations, we cannot directly compare the re-
sults. Overall speaking, SEMREP rules are very
conservative and detect very few relations from the
same text.
5.2 Knowledge Base (KB) Construction
The UMLS Metathesaurus (Lindberg et al, 1993)
contains a large amount of manually extracted re-
lation knowledge. Such knowledge is invaluable
for people to collect training data to build new
relation detectors. One downside of using this
KB is its incompleteness. For example, it only
contains the treatments for about 8,000 diseases,
which are far from sufficient. Further, the medical
knowledge is changing extremely quickly, making
people hard to understand it, and update it in the
knowledge base in a timely manner.
To address these challenges, we constructed our
own relation KB as a complement to the UMLS
relation KB. We directly ran our relation detec-
tors (trained with all labeled and unlabeled exam-
ples) on our medical corpus to extract relations.
Then we combined the results and put them in a
new KB. The new KB covers all super relations
and stores the knowledge in the format of (rela-
tion name, argument 1, argument 2, confidence),
where the confidence is computed based on the re-
lation detector confidence score and relation pop-
ularity in the corpus. The most recent version of
our relation KB contains 3.4 million such entries.
We compared this new KB against UMLS KB
using an answer generation task on a set of 742
Doctor Dilemma questions. We first ran our rela-
tion detectors to detect the relation(s) in the ques-
tion clue involving question focus (what the ques-
tion asks for). Then we searched against both KBs
using the relation name and the non-focus argu-
ment for the missing argument. The search re-
sults were then generated as potential answers. We
used the same relations to do KB lookup, so the
results are directly comparable. Since most ques-
tions only have one correct answer, the precision
number is not very important in this experiment.
If we detect multiple relations in the question,
and the same answer is generated from more than
one relations, we sum up all those confidence
scores to make such answers more preferable.
Sometimes, we may generate too many answers
from KBs. For example, if the detected relation
is ?location of? and the non-focus argument is
?skin?, then thousands of answers can be gener-
ated. In this scenario, we sort the answers based
upon the confidence scores and only consider up
to p answers for each question. In our test, we
considered three numbers for p: 20, 50 and 3,000.
From Table 3, we can see that the new KB out-
performs the most popularly-used UMLS KB at
all recall levels by a large margin. This result in-
dicates that the new KB has a much better knowl-
edge coverage. The UMLS KB is manually cre-
ated and thus more precise. In our experiment, the
UMLS KB generated fewer answers than the new
KB. For example, when up to 20 answers were
generated for each question, the UMLS KB gen-
erated around 4,700 answers for the whole ques-
tion set, while the new KB generated about 7,600
answers.
Construction of the new KB cost 16 machines
(using 4?2.8G cores per machine) 8 hours. The
reported computation time is for the whole corpus
with 11G pure text.
Table 3: Knowledge Base Comparison
Recall@20 Recall@50 Recall@3000
Our KB 135/742 182/742 301/742
UMLS KB 42/742 52/742 73/742
6 Conclusions
In this paper, we identify a list of key relations that
can facilitate clinical decision making. We also
present a new manifold model to efficiently extract
these relations from text. Our model is developed
to utilize both labeled and unlabeled examples. It
further provides users with the flexibility to take
label weight into consideration. Effectiveness of
the new model is demonstrated both theoretically
and experimentally. We apply the new model to
construct a relation knowledge base (KB), and use
it as a complement to the existing manually cre-
ated KBs.
Acknowledgments
We thank Siddharth Patwardhan for help on tree
kernels, Sugato Bagchi and Dr. Herbert Chase?s
team for categorizing the Doctor Dilemma ques-
tions. We also thank Anthony Levas, Karen In-
graffea, Mark Mergen, Katherine Modzelewski,
Jonathan Hodax, Matthew Schoenfeld and Adarsh
Thaker for vetting the training data.
836
References
ACE. 2004. The automatic content extraction projects,
http://projects.ldc.upenn.edu/ace/.
A. Aronson. 2001. Effective mapping of biomedical
text to the UMLS metathesaurus: the MetaMap pro-
gram. In Proceedings of the 2001 Annual Sympo-
sium of the American Medical Informatics Associa-
tion.
M. Belkin, P. Niyogi, and V. Sindhwani. 2006.
Manifold regularization: a geometric framework
for learning from labeled and unlabeled exam-
ples. Journal of Machine Learning Research, pages
2399?2434.
R. Bunescu and R. Mooney. 2005. A shortest path de-
pendency kernel for relation extraction. In Proceed-
ings of the Conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing.
Y. Chan and D. Roth. 2010. Exploiting background
knowledge for relation extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 152?160.
M. Collins and N. Duffy. 2001. Convolution ker-
nels for natural language. In Proceedings of the
Advances in Neural Information Processing Systems
(NIPS), pages 625?632.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 423?429.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society
for Information Science, 41(6):391?407.
D. Demner-Fushman and J. Lin. 2007. Answering
clinical questions with knowledge-based and statis-
tical techniques. Journal of Computational Linguis-
tics, 56:63?103.
R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and
D. S. Weld. 2011. Knowledge-based weak supervi-
sion for information extraction of overlapping rela-
tions. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
H. Ji, R. Grishman, and H. T. Dang. 2011. Overview
of the TAC 2011 knowledge base population track.
In Proceedings of the Text Analytics Conference.
J. Jiang. 2009. Multi-task transfer learning for weakly-
supervised relation extraction. In Proceedings of
the Joint Conference of the 47th Annual Meeting
of the Association for Computational Linguistics
(ACL) and the 4th International Joint Conference
on Natural Language Processing (IJCNLP), pages
1012?1020.
N. Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for extracting relations. In Proceedings of the
ACL 2004 on Interactive poster and demonstration
sessions.
D. Lindberg, B. Humphreys, and A. McCray. 1993.
The Unified Medical Language System. Methods of
Information in Medicine, 32:281?291.
M. McCord, J. W. Murdock, and B. K. Boguraev.
2012. Deep parsing in Watson. IBM Journal of Re-
search and Development, 56.
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel.
2000. A novel use of statistical parsing to extract in-
formation from text. In Proceedings of the 1st North
American Chapter of the Association for Computa-
tional Linguistics Conference.
B. Min, R. Grishman, L. Wan, C. Wang, and
D. Gondek. 2013. Distant supervision for relation
extraction with an incomplete knowledge base. In
The 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT).
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association
for Computational Linguistics (ACL) and the 4th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP), pages 1003?1011.
T. Nguyen, A. Moschitti, and G. Riccardi. 2009. Con-
volution kernels on constituent, dependency and se-
quential structures for relation extraction. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
S. Riedel, L. Yao, and A. McCallum. 2010. Mod-
eling relations and their mentions without labeled
text. In Proceedings of the European Conference
on Machine Learning and Knowledge Discovery in
Databases (ECML PKDD).
T. C. Rindflesch and M. Fiszman. 2003. The inter-
action of domain knowledge and linguistic structure
in natural language processing: interpreting hyper-
nymic propositions in biomedical text. Journal of
Biomedical Informatics, 36:462?477.
B. Scho?lkopf and A. J. Smola. 2002. Learning with
Kernels: Support Vector Machines, Regularization,
Optimization, and Beyond. MIT Press.
A. Singh, R. D. Nowak, and X. Zhu. 2008. Unlabeled
data: now it helps, now it doesnot. In Proceedings
of the Advances in Neural Information Processing
Systems (NIPS).
M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D.
Manning. 2012. Multi-instance multilabel learning
for relation extraction. In Proceedings of the 2012
837
Conference on Empirical Methods in Natural Lan-
guage Processing and Natural Language Learning
(EMNLP).
S. Takamatsu, I. Sato, and H. Nakagawa. 2012. Re-
ducing wrong labels in distant supervision for rela-
tion extraction. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
O?. Uzuner, B. R. South, S. Shen, and S. L. DuVall.
2011. 2010 i2b2/VA challenge on concepts, asser-
tions, and relations in clinical text. Journal of Amer-
ican Medical Informatics Association, 18:552?556.
C. Wang, J. Fan, A. Kalyanpur, and D. Gondek. 2011.
Relation extraction with relation topics. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP).
C. Wang, A. Kalyanpur, J. Fan, B. Boguraev, and
D. Gondek. 2012. Relation extraction and scoring
in DeepQA. IBM Journal of Research and Develop-
ment, 56.
M. Zhang, J. Zhang, J. Su, and G. Zhou. 2006. A com-
posite kernel to extract relations between entities
with both flat and structured features. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (ACL).
S. Zhao and R. Grishman. 2005. Extracting relations
with integrated information using kernel methods.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 419?426.
838
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 24?33,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Large Scale Relation Detection?
Chris Welty and James Fan and David Gondek and Andrew Schlaikjer
IBM Watson Research Center ? 19 Skyline Drive ? Hawthorne, NY 10532, USA
{welty, fanj, dgondek, ahschlai}@us.ibm.com
Abstract
We present a technique for reading sentences
and producing sets of hypothetical relations
that the sentence may be expressing. The
technique uses large amounts of instance-level
background knowledge about the relations in
order to gather statistics on the various ways
the relation may be expressed in language, and
was inspired by the observation that half of the
linguistic forms used to express relations oc-
cur very infrequently and are simply not con-
sidered by systems that use too few seed ex-
amples. Some very early experiments are pre-
sented that show promising results.
1 Introduction
We are building a system that learns to read in a new
domain by applying a novel combination of natural
language processing, machine learning, knowledge
representation and reasoning, information retrieval,
data mining, etc. techniques in an integrated way.
Central to our approach is the view that all parts of
the system should be able to interact during any level
of processing, rather than a pipeline view in which
certain parts of the system only take as input the re-
sults of other parts, and thus cannot influence those
results. In this paper we discuss a particular case
of that idea, using large knowledge bases hand in
hand with natural language processing to improve
the quality of relation detection. Ultimately we de-
fine reading as representing natural language text in
? Research supported in part by DARPA MRP Grant
FA8750-09-C0172
a way that integrates background knowledge and in-
ference, and thus are doing the relation detection
to better integrate text with pre-existing knowledge,
however that should not (and does not) prevent us
from using what knowledge we have to influence
that integration along the way.
2 Background
The most obvious points of interaction between NLP
and KR systems are named entity tagging and other
forms of type instance extraction. The second ma-
jor point of interaction is relation extraction, and
while there are many kinds of relations that may
be detected (e.g. syntactic relations such as modi-
fiers and verb subject/object, equivalence relations
like coreference or nicknames, event frame relations
such as participants, etc.), the kind of relations that
reading systems need to extract to support domain-
specific reasoning tasks are relations that are known
to be expressed in supporting knowledge-bases. We
call these relations semantic relations in this paper.
Compared to entity and type detection, extraction
of semantic relations is significantly harder. In our
work on bridging the NLP-KR gap, we have ob-
served several aspects of what makes this task dif-
ficult, which we discuss below.
2.1 Keep reading
Humans do not read and understand text by first rec-
ognizing named entities, giving them types, and then
finding a small fixed set of relations between them.
Rather, humans start with the first sentence and build
up a representation of what they read that expands
and is refined during reading. Furthermore, humans
24
do not ?populate databases? by reading; knowledge
is not only a product of reading, it is an integral part
of it. We require knowledge during reading in order
to understand what we read.
One of the central tenets of our machine reading
system is the notion that reading is not performed on
sentences in isolation. Often, problems in NLP can
be resolved by simply waiting for the next sentence,
or remembering the results from the previous, and
incorporating background or domain specific knowl-
edge. This includes parse ambiguity, coreference,
typing of named entities, etc. We call this the Keep
Reading principle.
Keep reading applies to relation extraction as
well. Most relation extraction systems are imple-
mented such that a single interpretation is forced
on a sentence, based only on features of the sen-
tence itself. In fact, this has been a shortcoming
of many NLP systems in the past. However, when
you apply the Keep Reading principle, multiple hy-
potheses from different parts of the NLP pipeline are
maintained, and decisions are deferred until there is
enough evidence to make a high confidence choice
between competing hypotheses. Knowledge, such
as those entities already known to participate in a
relation and how that relation was expressed, can
and should be part of that evidence. We will present
many examples of the principle in subsequent sec-
tions.
2.2 Expressing relations in language
Due to the flexibility and expressive power of nat-
ural language, a specific type of semantic relation
can usually be expressed in language in a myriad
of ways. In addition, semantic relations are of-
ten implied by the expression of other relations.
For example, all of the following sentences more
or less express the same relation between an actor
and a movie: (1) ?Elijah wood starred in Lord of
the Rings: The Fellowship of the Ring?, (2) ?Lord
of the Rings: The Fellowship of the Ring?s Elijah
Wood, ...?, and(3) ?Elijah Wood?s coming of age
was clearly his portrayal of the dedicated and noble
hobbit that led the eponymous fellowship from the
first episode of the Lord of the Rings trilogy.? No
human reader would have any trouble recognizing
the relation, but clearly this variability of expression
presents a major problem for machine reading sys-
tems.
To get an empirical sense of the variability of nat-
ural language used to express a relation, we stud-
ied a few semantic relations and found sentences
that expressed that relation, extracted simple pat-
terns to account for how the relation is expressed
between two arguments, mainly by removing the re-
lation arguments (e.g. ?Elijah Wood? and ?Lord of
the Rings: The Fellowship of the Ring? above) and
replacing them with variables. We then counted the
number of times each pattern was used to express
the relation, producing a recognizable very long tail
shown in Figure 1 for the top 50 patterns expressing
the acted-in-movie relation in 17k sentences. More
sophisticated pattern generalization (as discussed in
later sections) would significantly fatten the head,
bringing it closer to the traditional 50% of the area
under the curve, but no amount of generalization
will eliminate the tail. The patterns become increas-
ingly esoteric, such as ?The movie Death Becomes
Her features a brief sequence in which Bruce Willis
and Goldie Hawn?s characters plan Meryl Streep?s
character?s death by sending her car off of a cliff
on Mulholland Drive,? or ?The best known Hawk-
sian woman is probably Lauren Bacall, who iconi-
cally played the type opposite Humphrey Bogart in
To Have and Have Not and The Big Sleep.?
2.3 What relations matter
We do not consider relation extraction to be an end
in and of itself, but rather as a component in larger
systems that perform some task requiring interoper-
ation between language- and knowledge-based com-
ponents. Such larger tasks can include question
answering, medical diagnosis, intelligence analysis,
museum curation, etc. These tasks have evaluation
criteria that go beyond measuring relation extraction
results. The first step in applying relation detection
to these larger tasks is analysis to determine what
relations matter for the task and domain.
There are a number of manual and semi-automatic
ways to perform such analysis. Repeating the
theme of this paper, which is to use pre-existing
knowledge-bases as resources, we performed this
analysis using freebase and a set of 20k question-
answer pairs representing our task domain. For each
question, we formed tuples of each entity name in
the question (QNE) with the answer, and found all
25
 0
100
200
300
400
500
600
700
800
900
1000
Figure 1: Pattern frequency for acted-in-movie relation for 17k sentences.
 
0
10
20
30
40
50
60
70
80
Figure 2: Relative frequency for top 50 relations in 20K question-answer pairs.
the relations in the KB connecting the entities. We
kept a count for each relation of how often it con-
nected a QNE to an answer. Of course we don?t ac-
tually know for sure that the relation is the one being
asked, but the intuition is that if the amount of data
is big enough, you will have at least a ranked list of
which relations are the most frequent.
Figure 2 shows the ranking for the top 50 rela-
tions. Note that, even when restricted to the top 50
relations, the graph has no head, it is basically all
tail; The top 50 relations cover about 15% of the do-
main. In smaller, manual attempts to determine the
most frequent relations in our domain, we had a sim-
ilar result. What this means is that supporting even
the top 50 relations with perfect recall covers about
15% of the questions. It is possible, of course, to
narrow the domain and restrict the relations that can
be queried?this is what database systems do. For
reading, however, the results are the same. A read-
ing system requires the ability to recognize hundreds
of relations to have any significant impact on under-
standing.
2.4 Multi-relation learning on many seeds
The results shown in Figure 1 and Figure 2 con-
firmed much of the analysis and experiences we?d
had in the past trying to apply relation extraction in
the traditional way to natural language problems like
26
question answering, building concept graphs from
intelligence reports, semantic search, etc. Either by
training machine learning algorithms on manually
annotated data or by manually crafting finite-state
transducers, relation detection is faced by this two-
fold problem: the per-relation extraction hits a wall
around 50% recall, and each relation itself occurs
infrequently in the data.
This apparent futility of relation extraction led us
to rethink our approach. First of all, the very long
tail for relation patterns led us to consider how to
pick up the tail. We concluded that to do so would
require many more examples of the relation, but
where can we get them? In the world of linked-data,
huge instance-centered knowledge-bases are rapidly
growing and spreading on the semantic web1. Re-
sources like DBPedia, Freebase, IMDB, Geonames,
the Gene Ontology, etc., are making available RDF-
based data about a number of domains. These
sources of structured knowledge can provide a large
number of seed tuples for many different relations.
This is discussed further below.
Furthermore, the all-tail nature of relation cover-
age led us to consider performing relation extraction
on multiple relations at once. Some promising re-
sults on multi-relation learning have already been re-
ported in (Carlson et al, 2009), and the data sources
mentioned above give us many more than just the
handful of seed instances used in those experiments.
The idea of learning multiple relations at once also
fits with our keep reading principle - multiple rela-
tion hypotheses may be annotated between the same
arguments, with further evidence helping to disam-
biguate them.
3 Approach
One common approach to relation extraction is to
start with seed tuples and find sentences that con-
tain mentions of both elements of the tuple. From
each such sentence a pattern is generated using at
minimum universal generalization (replace the tuple
elements with variables), though adding any form of
generalization here can significantly improve recall.
Finally, evaluate the patterns by applying them to
text and evaluating the precision and recall of the tu-
ples extracted by the patterns. Our approach, called
1http://linkeddata.org/
Large Scale Relation Detection (LSRD), differs in
three important ways:
1. We start with a knowledge-base containing a
large number (thousands to millions) of tuples
encoding relation instances of various types.
Our hypothesis is that only a large number of
examples can possibly account for the long tail.
2. We do not learn one relation at a time, but
rather, associate a pattern with a set of relations
whose tuples appear in that pattern. Thus, when
a pattern is matched to a sentence during read-
ing, each relation in its set of associated rela-
tions is posited as a hypothetical interpretation
of the sentence, to be supported or refuted by
further reading.
3. We use the knowledge-base as an oracle to de-
termine negative examples of a relation. As
a result the technique is semi-supervised; it
requires no human intervention but does re-
quire reliable knowledge-bases as input?these
knowledge-bases are readily available today.
Many relation extraction techniques depend on a
prior step of named entity recognition (NER) and
typing, in order to identify potential arguments.
However, this limits recall to the recall of the NER
step. In our approach patterns can match on any
noun phrase, and typing of these NPs is simply an-
other form of evidence.
All this means our approach is not relation extrac-
tion per se, it typically does not make conclusions
about a relation in a sentence, but extracts hypothe-
ses to be resolved by other parts of our reading sys-
tem.
In the following sections, we elaborate on the
technique and some details of the current implemen-
tation.
3.1 Basic pipeline
The two principle inputs are a corpus and a
knowledge-base (KB). For the experiments below,
we used the English Gigaword corpus2 extended
with Wikipedia and other news sources, and IMDB,
DBPedia, and Freebase KBs, as shown. The intent is
2http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2003T05
27
to run against a web-scale corpus and larger linked-
data sets.
Input documents are sentence delimited, tok-
enized and parsed. The technique can benefit dra-
matically from coreference resolution, however in
the experiments shown, this was not present. For
each pair of proper names in a sentence, the names
are looked up in the KB, and if they are related,
a pattern is extracted from the sentence. At min-
imum, pattern extraction should replace the names
with variables. Depending on how patterns are ex-
tracted, one pattern may be extracted per sentence,
or one pattern may be extracted per pair of proper
names in the sentence. Each pattern is associated
with all the relations known in the KB between the
two proper names. If the pattern has been extracted
before, the two are merged by incrementing the as-
sociated relation counts. This phase, called pattern
induction, is repeated for the entire corpus, resulting
in a large set of patterns, each pattern associated with
relations. For each ?pattern, relation? pair, there is a
count of the number of times that pattern appeared
in the corpus with names that are in the relation ac-
cording to the KB.
The pattern induction phase results in positive
counts, i.e. the number of times a pattern appeared
in the corpus with named entities known to be re-
lated in the KB. However, the induction phase does
not exhaustively count the number of times each pat-
tern appears in the corpus, as a pattern may appear
with entities that are not known in the KB, or are not
known to be related. The second phase, called pat-
tern training, goes through the entire corpus again,
trying to match induced patterns to sentences, bind-
ing any noun phrase to the pattern variables. Some
attempt is made to resolve the noun phrase to some-
thing (most obviously, a name) that can be looked
up in the KB, and for each relation associated with
the pattern, if the two names are not in the relation
according to the KB, the negative count for that re-
lation in the matched pattern is incremented. The
result of the pattern training phase is an updated set
of ?pattern, relation? pairs with negative counts.
The following example illustrates the basic pro-
cessing. During induction, this sentence is encoun-
tered:
Tom Cruise and co-star Nicole Kidman
appeared together at the premier.
The proper names ?Tom Cruise? and ?Nicole Kid-
man? are recognized and looked up in the KB. We
find instances in the KB with those names, and the
following relations: coStar(Tom Cruise,
Nicole Kidman); marriedTo(Tom
Cruise, Nicole Kidman). We extract a
pattern p1: ?x and co-star ?y appeared
together at the premier in which all the
names have been replace by variables, and the
associations <p1, costar, 1, 0> and <p1,
marriedTo, 1, 0> with positive counts and
zero negative counts. Over the entire corpus, we?d
expect the pattern to appear a few times and end
up with final positive counts like <p1, coStar,
14, 0> and <p1, marriedTo, 2, 0>, in-
dicating the pattern p1 appeared 14 times in the
corpus between names known to participate in the
coStar relation, and twice between names known
to participate in the marriedTo relation. During
training, the following sentence is encountered that
matches p1:
Tom Hanks and co-star Daryl Hannah ap-
peared together at the premier.
The names ?Tom Hanks? and ?Daryl Hannah?
are looked up in the KB and in this case only
the relation coStar is found between them, so the
marriedTo association is updated with a negative
count: <p1, marriedTo, 2, -1>. Over the
entire corpus, we?d expect the counts to be some-
thing like <p1, costar, 14, -6> and <p1,
marriedTo, 2, -18>.
This is a very simple example and it is difficult to
see the value of the pattern training phase, as it may
appear the negative counts could be collected during
the induction phase. There are several reasons why
this is not so. First of all, since the first phase only
induces patterns between proper names that appear
and are related within the KB, a sentence in the cor-
pus matching the pattern would be missed if it did
not meet that criteria but was encountered before the
pattern was induced. Secondly, for reasons that are
beyond the scope of this paper, having to do with
our Keep Reading principle, the second phase does
slightly more general matching: note that it matches
noun phrases instead of proper nouns.
28
3.2 Candidate-instance matching
An obvious part of the process in both phases is
taking strings from text and matching them against
names or labels in the KB. We refer to the strings in
the sentences as candidate arguments or simply can-
didates, and refer to instances in the KB as entities
with associated attributes. For simplicity of discus-
sion we will assume all KBs are in RDF, and thus
all KB instances are nodes in a graph with unique
identifiers (URIs) and arcs connecting them to other
instances or primitive values (strings, numbers, etc.).
A set of specially designated arcs, called labels, con-
nect instances to strings that are understood to name
the instances. The reverse lookup of entity identi-
fiers via names referred to in the previous section
requires searching for the labels that match a string
found in a sentence and returning the instance iden-
tifier.
This step is so obvious it belies the difficultly of
the matching process and is often overlooked, how-
ever in our experiments we have found candidate-
instance matching to be a significant source of error.
Problems include having many instances with the
same or lexically similar names, slight variations in
spelling especially with non-English names, inflex-
ibility or inefficiency in string matching in KB im-
plementations, etc. In some of our sources, names
are also encoded as URLs. In the case of movie
and book titles-two of the domains we experimented
with-the titles seem almost as if they were designed
specifically to befuddle attempts to automatically
recognize them. Just about every English word is a
book or movie title, including ?It?, ?Them?, ?And?,
etc., many years are titles, and just about every num-
ber under 1000. Longer titles are difficult as well,
since simple lexical variations can prevent matching
from succeeding, e.g. the Shakespeare play, A Mid-
summer Night?s Dream appears often as Midsummer
Night?s Dream, A Midsummer Night Dream, and oc-
casionally, in context, just Dream. When titles are
not distinguished or delimited somehow, they can
confuse parsing which may fail to recognize them as
noun phrases. We eventually had to build dictionar-
ies of multi-word titles to help parsing, but of course
that was imperfect as well.
The problems go beyond the analogous ones in
coreference resolution as the sources and technology
themselves are different. The problems are severe
enough that the candidate-instance matching prob-
lem contributes the most, of all components in this
pipeline, to precision and recall failures. We have
observed recall drops of as much as 15% and preci-
sion drops of 10% due to candidate-instance match-
ing.
This problem has been studied somewhat in the
literature, especially in the area of database record
matching and coreference resolution (Michelson and
Knoblock, 2007), but the experiments presented be-
low use rudimentary solutions and would benefit
significantly from improvements; it is important to
acknowledge that the problem exists and is not as
trivial as it appears at first glance.
3.3 Pattern representation
The basic approach accommodates any pattern rep-
resentation, and in fact we can accommodate non
pattern-based learning approaches, such as CRFs, as
the primary hypothesis is principally concerned with
the number of seed examples (scaling up initial set
of examples is important). Thus far we have only
experimented with two pattern representations: sim-
ple lexical patterns in which the known arguments
are replaced in the sentence by variables (as shown
in the example above), and patterns based on the
spanning tree between the two arguments in a de-
pendency parse, again with the known arguments re-
placed by variables. In our initial design we down-
played the importance of the pattern representation
and especially generalization, with the belief that
very large scale would remove the need to general-
ize. However, our initial experiments suggest that
good pattern generalization would have a signifi-
cant impact on recall, without negative impact on
precision, which agrees with findings in the litera-
ture (Pantel and Pennacchiotti, 2006). Thus, these
early results only employ rudimentary pattern gen-
eralization techniques, though this is an area we in-
tend to improve. We discuss some more details of
the lack of generalization below.
4 Experiment
In this section we present a set of very early proof of
concept experiments performed using drastic simpli-
fications of the LSRD design. We began, in fact, by
29
Relation Prec Rec F1 Tuples Seeds
imdb:actedIn 46.3 45.8 0.46 9M 30K
frb:authorOf 23.4 27.5 0.25 2M 2M
imdb:directorOf 22.8 22.4 0.22 700K 700K
frb:parentOf 68.2 8.6 0.16 10K 10K
Table 1: Precision and recall vs. number of tuples used
for 4 freebase relations.
using single-relation experiments, despite the cen-
trality of multiple hypotheses to our reading system,
in order to facilitate evaluation and understanding of
the technique. Our main focus was to gather data
to support (or refute) the hypothesis that more re-
lation examples would matter during pattern induc-
tion, and that using the KB as an oracle for training
would work. Clearly, no KB is complete to begin
with, and candidate-instance matching errors drop
apparent coverage further, so we intended to explore
the degree to which the KB?s coverage of the relation
impacted performance. To accomplish this, we ex-
amined four relations with different coverage char-
acteristics in the KB.
4.1 Setup and results
The first relation we tried was the acted-in-show
relation from IMDB; for convenience we refer to
it as imdb:actedIn. An IMDB show is a movie,
TV episode, or series. This relation has over 9M
<actor, show> tuples, and its coverage was
complete as far as we were able to determine. How-
ever, the version we used did not have a lot of name
variations for actors. The second relation was the
author-of relation from Freebase (frb:authorOf ),
with roughly 2M <author, written-work>
tuples. The third relation was the director-of-
movie relation from IMDB (imdb:directorOf ), with
700k <director,movie> tuples. The fourth
relation was the parent-of relation from Free-
base (frb:parentOf ), with roughly 10K <parent,
child> tuples (mostly biblical and entertainment).
Results are shown in Table 1.
The imdb:actedIn experiment was performed on
the first version of the system that ran on 1 CPU and,
due to resource constraints, was not able to use more
than 30K seed tuples for the rule induction phase.
However, the full KB (9M relation instances) was
available for the training phase. With some man-
ual effort, we selected tuples (actor-movie pairs) of
popular actors and movies that we expected to ap-
pear most frequently in the corpus. In the other ex-
periments, the full tuple set was available for both
phases, but 2M tuples was the limit for the size of
the KB in the implementation. With these promising
preliminary results, we expect a full implementation
to accommodate up to 1B tuples or more.
The evaluation was performed in decreasing de-
grees of rigor. The imdb:actedIn experiment was run
against 20K sentences with roughly 1000 actor in
movie relations and checked by hand. For the other
three, the same sentences were used, but the ground
truth was generated in a semi-automatic way by re-
using the LSRD assumption that a sentence con-
taining tuples in the relation expresses the relation,
and then spot-checked manually. Thus the evalua-
tion for these three experiments favors the LSRD ap-
proach, though spot checking revealed it is the pre-
cision and not the recall that benefits most from this,
and all the recall problems in the ground truth (i.e.
sentences that did express the relation but were not
in the ground truth) were due to candidate-instance
matching problems. An additional idiosyncrasy in
the evaluation is that the sentences in the ground
truth were actually questions, in which one of the
arguments to the relation was the answer. Since
the patterns were induced and trained on statements,
there is a mismatch in style which also significantly
impacts recall. Thus the precision and recall num-
bers should not be taken as general performance, but
are useful only relative to each other.
4.2 Discussion
The results are promising, and we are continuing the
work with a scalable implementation. Overall, the
results seem to show a clear correlation between the
number of seed tuples and relation extraction recall.
However, the results do not as clearly support the
many examples hypothesis as it may seem. When
an actor and a film that actor starred in are men-
tioned in a sentence, it is very often the case that the
sentence expresses that relation. However, this was
less likely in the case of the parent-of relation, and
as we considered other relations, we found a wide
degree of variation. The borders relation between
two countries, for example, is on the other extreme
from actor-in-movie. Bordering nations often wage
30
war, trade, suspend relations, deport refugees, sup-
port, oppose, etc. each other, so finding the two na-
tions in a sentence together is not highly indicative
of one relation or another. The director-of-movie re-
lation was closer to acted-in-movie in this regard,
and author-of a bit below that. The obvious next step
to gather more data on the many examples hypoth-
esis is to run the experiments with one relation, in-
creasing the number of tuples with each experiment
and observing the change in precision and recall.
The recall results do not seem particularly strik-
ing, though these experiments do not include pat-
tern generalization (other than what a dependency
parse provides) or coreference, use a small corpus,
and poor candidate-instance matching. Further, as
noted above there were other idiosyncrasies in the
evaluation that make them only useful for relative
comparison, not as general results.
Many of the patterns induced, especially for
the acted-in-movie relation, were highly lexical,
using e.g. parenthesis or other punctuation to
signal the relation. For example, a common
pattern was actor-name (movie-name), or
movie-name: actor-name, e.g. ?Leonardo
DiCaprio (Titanic) was considering accepting the
role as Anakin Skywalker,? or ?Titanic: Leonardo
DiCaprio and Kate Blanchett steam up the silver
screen against the backdrop of the infamous disas-
ter.? Clearly patterns like this rely heavily on the
context and typing to work. In general the pattern
?x (?y) is not reliable for the actor-in-movie re-
lation unless you know ?x is an actor and ?y is a
movie. However, some patterns, like ?x appears
in the screen epic ?y is highly indicative
of the relation without the types at all - in fact it is
so high precision it could be used to infer the types
of ?x and ?y if they were not known. This seems
to fit extremely well in our larger reading system,
in which the pattern itself provides one form of evi-
dence to be combined with others, but was not a part
of our evaluation.
One of the most important things to general-
ize in the patterns we observed was dates. If
patterns like, actor-name appears in the
1994 screen epic movie-name could have
been generalized to actor-name appears in
the date screen epic movie-name, re-
call would have been boosted significantly. As it
stood in these experiments, everything but the argu-
ments had to match. Similarly, many relations often
appear in lists, and our patterns were not able to gen-
eralize that away. For example the sentence, ?Mark
Hamill appeared in Star Wars, Star Wars: The Em-
pire Strikes Back, and Star Wars: The Return of the
Jedi,? causes three patterns to be induced; in each,
one of the movies is replaced by a variable in the
pattern and the other two are required to be present.
Then of course all this needs to be combined, so that
the sentence, ?Indiana Jones and the Last Crusade is
a 1989 adventure film directed by Steven Spielberg
and starring Harrison Ford, Sean Connery, Denholm
Elliott and Julian Glover,? would generate a pattern
that would get the right arguments out of ?Titanic
is a 1997 epic film directed by James Cameron and
starring Leonardo DiCaprio, Kate Winslett, Kathy
Bates and Bill Paxon.? At the moment the former
sentence generates four patterns that require the di-
rector and dates to be exactly the same.
Some articles in the corpus were biographies
which were rich with relation content but also with
pervasive anaphora, name abbreviations, and other
coreference manifestations that severely hampered
induction and evaluation.
5 Related work
Early work in semi-supervised learning techniques
such as co-training and multi-view learning (Blum
and Mitchell, 1998) laid much of the ground work
for subsequent experiments in bootstrapped learn-
ing for various NLP tasks, including named entity
detection (Craven et al, 2000; Etzioni et al, 2005)
and document classification (Nigam et al, 2006).
This work?s pattern induction technique also repre-
sents a semi-supervised approach, here applied to
relation learning, and at face value is similar in mo-
tivation to many of the other reported experiments
in large scale relation learning (Banko and Etzioni,
2008; Yates and Etzioni, 2009; Carlson et al, 2009;
Carlson et al, 2010). However, previous techniques
generally rely on a small set of example relation in-
stances and/or patterns, whereas here we explicitly
require a larger source of relation instances for pat-
tern induction and training. This allows us to better
evaluate the precision of all learned patterns across
multiple relation types, as well as improve coverage
31
of the pattern space for any given relation.
Another fundamental aspect of our approach lies
in the fact that we attempt to learn many relations
simultaneously. Previously, (Whitelaw et al, 2008)
found that such a joint learning approach was use-
ful for large-scale named entity detection, and we
expect to see this result carry over to the relation ex-
traction task. (Carlson et al, 2010) also describes
relation learning in a multi-task learning framework,
and attempts to optimize various constraints posited
across all relation classes.
Examples of the use of negative evidence
for learning the strength of associations between
learned patterns and relation classes as proposed
here has not been reported in prior work to our
knowledge. A number of multi-class learning tech-
niques require negative examples in order to prop-
erly learn discriminative features of positive class
instances. To address this requirement, a number of
approaches have been suggested in the literature for
selection or generation of negative class instances.
For example, sampling from the positive instances
of other classes, randomly perturbing known pos-
itive instances, or breaking known semantic con-
straints of the positive class (e.g. positing multiple
state capitols for the same state). With this work,
we treat our existing RDF store as an oracle, and as-
sume it is sufficiently comprehensive that it allows
estimation of negative evidence for all target relation
classes simultaneously.
The first (induction) phase of LSRD is very simi-
lar to PORE (Wang et al, 2007) (Dolby et al, 2009;
Gabrilovich and Markovitch, 2007) and (Nguyen
et al, 2007), in which positive examples were ex-
tracted from Wikipedia infoboxes. These also bear
striking similarity to (Agichtein and Gravano, 2000),
and all suffer from a significantly smaller number of
seed examples. Indeed, its not using a database of
specific tuples that distinguishes LSRD, but that it
uses so many; the scale of the induction in LSRD
is designed to capture far less frequent patterns by
using significantly more seeds
In (Ramakrishnan et al, 2006) the same intu-
ition is captured that knowledge of the structure of
a database should be employed when trying to inter-
pret text, though again the three basic hypotheses of
LSRD are not supported.
In (Huang et al, 2004), a similar phenomenon to
what we observed with the acted-in-movie relation
was reported in which the chances of a protein in-
teraction relation being expressed in a sentence are
already quite high if two proteins are mentioned in
that sentence.
6 Conclusion
We have presented an approach for Large Scale Re-
lation Detection (LSRD) that is intended to be used
within a machine reading system as a source of hy-
pothetical interpretations of input sentences in natu-
ral language. The interpretations produced are se-
mantic relations between named arguments in the
sentences, and they are produced by using a large
knowledge source to generate many possible pat-
terns for expressing the relations known by that
source.
We have specifically targeted the technique at the
problem that the frequency of patterns occurring in
text that express a particular relation has a very long
tail (see Figure 1), and without enough seed exam-
ples the extremely infrequent expressions of the re-
lation will never be found and learned. Further, we
do not commit to any learning strategy at this stage
of processing, rather we simply produce counts, for
each relation, of how often a particular pattern pro-
duces tuples that are in that relation, and how of-
ten it doesn?t. These counts are simply used as ev-
idence for different possible interpretations, which
can be supported or refuted by other components in
the reading system, such as type detection.
We presented some very early results which while
promising are not conclusive. There were many
idiosyncrasies in the evaluation that made the re-
sults meaningful only with respect to other experi-
ments that were evaluated the same way. In addi-
tion, the evaluation was done at a component level,
as if the technique were a traditional relation extrac-
tion component, which ignores one of its primary
differentiators?that it produces sets of hypothetical
interpretations. Instead, the evaluation was done
only on the top hypothesis independent of other evi-
dence.
Despite these problems, the intuitions behind
LSRD still seem to us valid, and we are investing in a
truly large scale implementation that will overcome
the problems discussed here and can provide more
32
valid evidence to support or refute the hypotheses
LSRD is based on:
1. A large number of examples can account for the
long tail in relation expression;
2. Producing sets of hypothetical interpretations
of the sentence, to be supported or refuted by
further reading, works better than producing
one;
3. Using existing, large, linked-data knowledge-
bases as oracles can be effective in relation de-
tection.
References
[Agichtein and Gravano2000] E. Agichtein and L. Gra-
vano. 2000. Snowball: extracting relations from large
plain-text collections. In Proceedings of the 5th ACM
Conference on Digital Libraries, pages 85?94, San
Antonio, Texas, United States, June. ACM.
[Banko and Etzioni2008] Michele Banko and Oren Et-
zioni. 2008. The tradeoffs between open and tradi-
tional relation extraction. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics.
[Blum and Mitchell1998] A. Blum and T. Mitchell. 1998.
Combining labeled and unlabeled data with co-
training. In Proceedings of the 1998 Conference on
Computational Learning Theory.
[Carlson et al2009] A. Carlson, J. Betteridge, E. R. Hr-
uschka Jr., and T. M. Mitchell. 2009. Coupling semi-
supervised learning of categories and relations. In
Proceedings of the NAACL HLT 2009 Workshop on
Semi-supervised Learning for Natural Language Pro-
cessing.
[Carlson et al2010] A. Carlson, J. Betteridge, R. C.
Wang, E. R. Hruschka Jr., and T. M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the 3rd ACM International
Conference on Web Search and Data Mining.
[Craven et al2000] Mark Craven, Dan DiPasquo, Dayne
Freitag, Andrew McCallum, Tom Mitchell, Kamal
Nigam, and Sean Slattery. 2000. Learning to construct
knowledge bases from the World Wide Web. Artificial
Intelligence, 118(1?2):69?113.
[Dolby et al2009] Julian Dolby, Achille Fokoue, Aditya
Kalyanpur, Edith Schonberg, and Kavitha Srinivas.
2009. Extracting enterprise vocabularies using linked
open data. In Proceedings of the 8th International Se-
mantic Web Conference.
[Etzioni et al2005] Oren Etzioni, Michael Cafarella,
Doug Downey, Ana-Maria Popescu, Tal Shaked,
Stephen Soderland, Daniel S. Weld, and Alexander
Yates. 2005. Unsupervised named-entity extraction
from the web: An experimental study. Artificial Intel-
ligence, 165(1):91?134, June.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovich
and Shaul Markovitch. 2007. Computing seman-
tic relatedness using wikipedia-based explicit seman-
tic analysis. In IJCAI.
[Huang et al2004] Minlie Huang, Xiaoyan Zhu, Yu Hao,
Donald G. Payan, Kunbin Qu, and Ming Li. 2004.
Discovering patterns to extract protein-protein interac-
tions from full texts. Bioinformatics, 20(18).
[Michelson and Knoblock2007] Matthew Michelson and
Craig A. Knoblock. 2007. Mining heterogeneous
transformations for record linkage. In Proceedings of
the 6th International Workshop on Information Inte-
gration on the Web, pages 68?73.
[Nguyen et al2007] Dat P. Nguyen, Yutaka Matsuo, ,
and Mitsuru Ishizuka. 2007. Exploiting syntactic
and semantic information for relation extraction from
wikipedia. In IJCAI.
[Nigam et al2006] K. Nigam, A. McCallum, , and
T. Mitchell, 2006. Semi-Supervised Learning, chapter
Semi-Supervised Text Classification Using EM. MIT
Press.
[Pantel and Pennacchiotti2006] Patrick Pantel and Marco
Pennacchiotti. 2006. Espresso: Leveraging generic
patterns for automatically harvesting semantic rela-
tions. In Proceedings of the 21st international Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association For Computational
Linguistics, Sydney, Australia, July.
[Ramakrishnan et al2006] Cartic Ramakrishnan, Krys J.
Kochut, and Amit P. Sheth. 2006. A framework for
schema-driven relationship discovery from unstruc-
tured text. In ISWC.
[Wang et al2007] Gang Wang, Yong Yu, and Haiping
Zhu. 2007. PORE: Positive-only relation extraction
from wikipedia text. In ISWC.
[Whitelaw et al2008] C. Whitelaw, A. Kehlenbeck,
N. Petrovic, , and L. Ungar. 2008. Web-scale named
entity recognition. In Proceeding of the 17th ACM
Conference on information and Knowledge Manage-
ment, pages 123?132, Napa Valley, California, USA,
October. ACM.
[Yates and Etzioni2009] Alexander Yates and Oren Et-
zioni. 2009. Unsupervised methods for determining
object and relation synonyms on the web. Artificial
Intelligence, 34:255?296.
33
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 122?127,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
PRISMATIC: Inducing Knowledge from a Large Scale Lexicalized Relation
Resource?
James Fan and David Ferrucci and David Gondek and Aditya Kalyanpur
IBM Watson Research Lab
19 Skyline Dr
Hawthorne, NY 10532
{fanj, ferrucci, gondek, adityakal}@us.ibm.com
Abstract
One of the main bottlenecks in natural lan-
guage processing is the lack of a comprehen-
sive lexicalized relation resource that contains
fine grained knowledge on predicates. In this
paper, we present PRISMATIC, a large scale
lexicalized relation resource that is automati-
cally created over 30 gb of text. Specifically,
we describe what kind of information is col-
lected in PRISMATIC and how it compares
with existing lexical resources. Our main fo-
cus has been on building the infrastructure and
gathering the data. Although we are still in
the early stages of applying PRISMATIC to
a wide variety of applications, we believe the
resource will be of tremendous value for AI
researchers, and we discuss some of potential
applications in this paper.
1 Introduction
Many natural language processing and understand-
ing applications benefit from the interpretation of
lexical relations in text (e.g. selectional preferences
for verbs and nouns). For example, if one knows that
things being annexed are typically geopolitical enti-
ties, then given the phrase Napoleon?s annexation of
Piedmont, we can infer Piedmont is a geopolitical
entity. Existing linguistic resources such as VerbNet
and FrameNet provide some argument type infor-
mation for verbs and frames. However, since they
are manually built, they tend to specify type con-
straints at a very high level (e.g, Solid, Animate),
?Research supported in part by Air Force Contract FA8750-
09-C-0172 under the DARPA Machine Reading Program
consequently they do not suffice for cases such as
the previous example.
We would like to infer more fine grained knowl-
edge for predicates automatically from a large
amount of data. In addition, we do not want to re-
strict ourselves to only verbs, binary relations, or to
a specific type hierarchy.
In this paper, we present PRISMATIC, a large
scale lexicalized relation resource mined from over
30 gb of text. PRISMATIC is built using a suite of
NLP tools that includes a dependency parser, a rule
based named entity recognizer and a coreference
resolution component. PRISMATIC is composed
of frames which are the basic semantic representa-
tion of lexicalized relation and surrounding context.
There are approximately 1 billion frames in our cur-
rent version of PRISMATIC. To induce knowledge
from PRISMATIC, we define the notion of frame-
cuts, which basically specify a cut or slice operation
on a frame. In the case of the previous Napoleon
annexation example, we would use a noun-phrase
? object type cut to learn the most frequent type
of things being annexed. We believe there are many
potential applications that can utilize PRISMATIC,
such as type inference, relation extraction textual en-
tailment, etc. We discuss some of these applications
in details in section 8.
2 Related Work
2.1 Manually Created Resources
Several lexical resources have been built man-
ually, most notably WordNet (Fellbaum, 1998),
FrameNet(Baker et al, 1998) and VerbNet(Baker et
122
al., 1998). WordNet is a lexical resource that con-
tains individual word synset information, such as
definition, synonyms, antonyms, etc. However, the
amount of predicate knowledge in WordNet is lim-
ited.
FrameNet is a lexical database that describes the
frame structure of selected words. Each frame rep-
resents a predicate (e.g. eat, remove) with a list of
frame elements that constitutes the semantic argu-
ments of the predicate. Different words may map to
the same frame, and one word may map to multiple
frames based on different word senses. Frame ele-
ments are often specific to a particular frame, and
even if two frame elements with the same name,
such as ?Agent?, may have subtle semantic mean-
ings in different frames.
VerbNet is a lexical database that maps verbs to
their corresponding Levin (Levin, 1993) classes, and
it includes syntactic and semantic information of the
verbs, such as the syntactic sequences of a frame
(e.g. NP V NP PP) and the selectional restriction
of a frame argument value must be ANIMATE,
Compared to these resources, in addition to being
an automatic process, PRISMATIC has three major
differences. First, unlike the descriptive knowledge
in WordNet, VerbNet or FrameNet, PRISMATIC of-
fers only numeric knowledge of the frequencies of
how different predicates and their argument values
through out a corpus. The statistical profiles are eas-
ily to produce automatically, and they allow addi-
tional knowledge, such as type restriction (see 8.1),
to be inferred from PRISMATIC easily.
Second, the frames are defined differently. The
frames in PRISMATIC are not abstract concepts
generalized over a set of words. They are defined
by the words in a sentence and the relations between
them. Two frames with different slot values are con-
sidered different even though they may be semanti-
cally similar. For example, the two sentences ?John
loves Mary? and ?John adores Mary? result in two
different frame even though semantically they are
very close. By choosing not to use frame concepts
generalized over words, we avoid the problem of
determining which frame a word belongs to when
processing text automatically. We believe there will
be enough redundancy in a large corpus to produce
valid values for different synonyms and variations.
Third, PRISMATIC only uses a very small set of
slots (see table 1) defined by parser and relation an-
notators to link a frame and its arguments. By using
these slots directly, we avoid the problem of map-
ping parser relations to frame elements.
2.2 Automatically Created Resources
TextRunner (Banko et al, 2007) is an information
extraction system which automatically extracts re-
lation tuples over massive web data in an unsuper-
vised manner. TextRunner contains over 800 mil-
lion extractions (Lin et al, 2009) and has proven
to be a useful resource in a number of important
tasks in machine reading such as hypernym discov-
ery (Alan Ritter and Etzioni, 2009), and scoring in-
teresting assertions (Lin et al, 2009). TextRunner
works by automatically identifying and extracting
relationships using a conditional random field (CRF)
model over natural language text. As this is a rela-
tively inexpensive technique, it allows rapid applica-
tion to web-scale data.
DIRT (Discovering Inference Rules from Text)
(Lin and Pantel, 2001) automatically identifies in-
ference rules over dependency paths which tend to
link the same arguments. The technique consists of
applying a dependency parser over 1 gb of text, col-
lecting the paths between arguments and then cal-
culating a path similarity between paths. DIRT has
been used extensively in recognizing textual entail-
ment (RTE).
PRISMATIC is similar to TextRunner and DIRT
in that it may be applied automatically over mas-
sive corpora. At a representational level it differs
from both TextRunner and DIRT by storing full
frames from which n-ary relations may be indexed
and queried. PRISMATIC differs from TextRun-
ner as it applies a full dependency parser in order
to identify dependency relationships between terms.
In contrast to DIRT and TextRunner, PRISMATIC
also performs co-reference resolution in order to in-
crease coverage for sparsely-occurring entities and
employs a named entity recognizer (NER) and rela-
tion extractor on all of its extractions to better repre-
sent intensional information.
3 System Overview
The PRISMATIC pipeline consists of three phases:
1. Corpus Processing Documents are annotated
123
Figure 1: System Overview
by a suite of components which perform depen-
dency parsing, co-reference resolution, named
entity recognition and relation detection.
2. Frame Extraction Frames are extracted based
on the dependency parses and associated anno-
tations.
3. Frame-Cut Extraction Frame-cuts of interest
(e.g. S-V-O cuts) are identified over all frames
and frequency information for each cut is tabu-
lated.
4 Corpus Processing
The key step in the Corpus Processing stage is the
application of a dependency parser which is used
to identify the frame slots (as listed in Table 1) for
the Frame Extraction stage. We use ESG (McCord,
1990), a slot-grammar based parser in order to fill
in the frame slots. Sentences frequently require co-
reference in order to precisely identify the participat-
ing entity, and so in order to not lose that informa-
tion, we apply a simple rule based co-reference reso-
lution component in this phase. The co-reference in-
formation helps enhance the coverage of the frame-
cuts, which is especially valuable in cases of sparse
data and for use with complex frame-cuts.
A rule based Named Entity Recognizer (NER) is
used to identify the types of arguments in all frame
slot values. This type information is then registered
in the Frame Extraction stage to construct inten-
tional frames.
5 Frame Extraction
Relation Description/Example
subj subject
obj direct object
iobj indirect object
comp complement
pred predicate complement
objprep object of the preposition
mod nprep Bat Cave in Toronto is a tourist attraction.
mod vprep He made it to Broadway.
mod nobj the object of a nominalized verb
mod ndet City?s budget was passed.
mod ncomp Tweet is a word for microblogging.
mod nsubj A poem by Byron
mod aobj John is similar to Steve.
isa subsumption relation
subtypeOf subsumption relation
Table 1: Relations used in a frame and their descriptions
The next step of PRISMATIC is to extract a set of
frames from the parsed corpus. A frame is the basic
semantic unit representing a set of entities and their
relations in a text snippet. A frame is made of a set
of slot value pairs where the slots are dependency
relations extracted from the parse and the values are
the terms from the sentences or annotated types. Ta-
ble 2 shows the extracted frame based on the parse
tree in figure 2.
In order to capture the relationship we are inter-
ested in, frame elements are limited to those that
represent the participant information of a predicate.
Slots consist of the ones listed in table 1. Further-
more, each frame is restricted to be two levels deep
at the most, therefore, a large parse tree may re-
sult in multiple frames. Table 2 shows how two
frames are extracted from the complex parse tree
in figure 2. The depth restriction is needed for two
reasons. First, despite the best efforts from parser
researchers, no parser is perfect, and big complex
parse trees tend to have more wrong parses. By lim-
iting a frame to be only a small subset of a complex
parse tree, we reduce the chance of error parse in
each frame. Second, by isolating a subtree, each
frame focuses on the immediate participants of a
predicate.
Non-parser information may also be included in a
frame. For example, the type annotations of a word
from a named entity recognizer are included, and
such type information is useful for the various ap-
124
Figure 2: The parse tree of the sentence In 1921, Einstein received the Nobel Prize for his original work on the
photoelectric effect.
Frame01
verb receive
subj Einstein
type PERSON / SCIENTIST
obj Nobel prize
mod vprep in
objprep 1921
type YEAR
mod vprep for
objprep Frame02
Frame02
noun work
mod ndet his / Einstein
mod nobj on
objprep effect
Table 2: Frames extracted from Dependency Parse in Fig-
ure 2
plications described in section 8. We also include
a flag to indicate whether a word is proper noun.
These two kinds of information allow us to easily
separate the intensional and the extensional parts of
PRISMATIC.
6 Frame Cut
One of the main reasons for extracting a large
amount of frame data from a corpus is to induce
interesting knowledge patterns by exploiting redun-
dancy in the data. For example, we would like to
learn that things that are annexed are typically re-
gions, i.e., a predominant object-type for the noun-
phrase ?annexation of? is ?Region? where ?Region?
is annotated by a NER. To do this kind of knowledge
induction, we first need to abstract out specific por-
tions of the frame - in this particular case, we need
to isolate and analyze the noun-phrase ? object-
type relationship. Then, given a lot of data, and
frames containing only the above relationship, we
hope to see the frame [noun=?annexation?, prepo-
sition=?of?, object-type=?Region?] occur very fre-
quently.
To enable this induction analysis, we define
frame-cuts, which basically specify a cut or slice op-
eration on a frame. For example, we define an N-P-
OT frame cut, which when applied to a frame only
keeps the noun (N), preposition (P) and object-type
(OT) slots, and discards the rest. Similarly, we de-
fine frame-cuts such as S-V-O, S-V-O-IO, S-V-P-O
etc. (where S - subject, V - verb, O - object, IO -
indirect object) which all dissect frames along dif-
125
ferent dimensions. Continuing with the annexation
example, we can use the V-OT frame cut to learn
that a predominant object-type for the verb ?annex?
is also ?Region?, by seeing lots of frames of the form
[verb=?annex?, object-type=?Region?] in our data.
To make frame-cuts more flexible, we allow them
to specify optional value constraints for slots. For
example, we can define an S-V-O frame cut, where
both the subject (S) and object (O) slot values are
constrained to be proper nouns, thereby creating
strictly extensional frames, i.e. frames containing
data about instances, e.g., [subject=?United States?
verb=?annex? object=?Texas?]. The opposite ef-
fect is achieved by constraining S and O slot val-
ues to common nouns, creating intensional frames
such as [subject=?Political-Entity? verb=?annex?
object=?Region?]. The separation of extensional
from intensional frame information is desirable,
both from a knowledge understanding and an appli-
cations perspective, e.g. the former can be used to
provide factual evidence in tasks such as question
answering, while the latter can be used to learn en-
tailment rules as seen in the annexation case.
7 Data
The corpora we used to produce the initial PRIS-
MATIC are based on a selected set of sources, such
as the complete Wikipedia, New York Times archive
and web page snippets that are on the topics listed in
wikipedia. After cleaning and html detagging, there
are a total of 30 GB of text. From these sources, we
extracted approximately 1 billion frames, and from
these frames, we produce the most commonly used
cuts such as S-V-O, S-V-P-O and S-V-O-IO.
8 Potential Applications
8.1 Type Inference and Its Related Uses
As noted in Section 6, we use frame-cuts to dis-
sect frames along different slot dimensions, and then
aggregate statistics for the resultant frames across
the entire dataset, in order to induce relationships
among the various frame slots, e.g., learn the pre-
dominant types for subject/object slots in verb and
noun phrases. Given a new piece of text, we can
apply this knowledge to infer types for named en-
tities. For example, since the aggregate statistics
shows the most common type for the object of
the verb ?annex? is Region, we can infer from the
sentence ?Napoleon annexed Piedmont in 1859?,
that ?Piedmont? is most likely to be a Region.
Similarly, consider the sentence: ?He ordered a
Napoleon at the restaurant?. A dictionary based
NER is very likely to label ?Napoleon? as a Per-
son. However, we can learn from a large amount
of data, that in the frame: [subject type=?Person?
verb=?order? object type=[?] verb prep=?at? ob-
ject prep=?restaurant?], the object type typically
denotes a Dish, and thus correctly infer the type for
?Napoleon? in this context. Learning this kind of
fine-grained type information for a particular con-
text is not possible using traditional hand-crafted re-
sources like VerbNet or FrameNet. Unlike previ-
ous work in selectional restriction (Carroll and Mc-
Carthy, 2000; Resnik, 1993), PRISMATIC based
type inference does not dependent on a particular
taxonomy or previously annotated training data: it
works with any NER and its type system.
The automatically induced-type information can
also be used for co-reference resolution. For ex-
ample, given the sentence: ?Netherlands was ruled
by the UTP party before Napolean annexed it?, we
can use the inferred type constraint on ?it? (Region)
to resolve it to ?Netherlands? (instead of the ?UTP
Party?).
Finally, typing knowledge can be used for word
sense disambiguation. In the sentence, ?Tom Cruise
is one of the biggest stars in American Cinema?, we
can infer using our frame induced type knowledge
base, that the word ?stars? in this context refers to a
Person/Actor type, and not the sense of ?star? as an
astronomical object.
8.2 Factual Evidence
Frame data, especially extensional data involving
named entities, captured over a large corpus can be
used as factual evidence in tasks such as question
answering.
8.3 Relation Extraction
Traditional relation extraction approach (Zelenko et
al., 2003; Bunescu and Mooney, 2005) relies on the
correct identification of the types of the argument.
For example, to identify ?employs? relation between
?John Doe? and ?XYZ Corporation?, a relation ex-
tractor heavily relies on ?John Doe? being annotated
126
as a ?PERSON? and ?XYZ Corporation? an ?OR-
GANIZATION? since the ?employs? relation is de-
fined between a ?PERSON? and an ?ORGANIZA-
TION?.
We envision PRISMATIC to be applied to rela-
tion extraction in two ways. First, as described in
section 8.1, PRISMATIC can complement a named
entity recognizer (NER) for type annotation. This
is especially useful for the cases when NER fails.
Second, since PRISMATIC has broad coverage of
named entities, it can be used as a database to
check to see if the given argument exist in related
frame. For example, in order to determine if ?em-
ploys? relation exists between ?Jack Welch? and
?GE? in a sentence, we can look up the SVO cut
of PRISMATIC to see if we have any frame that has
?Jack Welch? as the subject, ?GE? as the object and
?work? as the verb, or frame that has ?Jack Welch?
as the object, ?GE? as the subject and ?employs? as
the verb. This information can be passed on as an
feature along with other syntactic and semantic fea-
tures to th relation extractor.
9 Conclusion and Future Work
In this paper, we presented PRISMATIC, a large
scale lexicalized relation resource that is built au-
tomatically over massive amount of text. It provides
users with knowledge about predicates and their ar-
guments. We have focused on building the infras-
tructure and gathering the data. Although we are
still in the early stages of applying PRISMATIC, we
believe it will be useful for a wide variety of AI ap-
plications as discussed in section 8, and will pursue
them in the near future.
References
Stephen Soderland Alan Ritter and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of the 2009 AAAI Spring Symposium
on Learning by Reading and Learning to Read.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Michele Banko, Michael J Cafarella, Stephen Soderl,
Matt Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In In Inter-
national Joint Conference on Artificial Intelligence,
pages 2670?2676.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In HLT ?05: Proceedings of the conference on
Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 724?731,
Morristown, NJ, USA. Association for Computational
Linguistics.
John Carroll and Diana McCarthy. 2000. Word sense
disambiguation using automatically acquired verbal
preferences. Computers and the Humanities Senseval
Special Issue, 34.
Christiane Fellbaum, 1998. WordNet: An Electronic Lex-
ical Database.
Beth Levin, 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323?328.
Thomas Lin, Oren Etzioni, and James Fogarty. 2009.
Identifying interesting assertions from the web. In
CIKM ?09: Proceeding of the 18th ACM conference on
Information and knowledge management, pages 1787?
1790, New York, NY, USA. ACM.
Michael C. McCord. 1990. Slot grammar: A system
for simpler construction of practical natural language
grammars. In Proceedings of the International Sympo-
sium on Natural Language and Logic, pages 118?145,
London, UK. Springer-Verlag.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. J. Mach. Learn. Res., 3:1083?1106.
127
