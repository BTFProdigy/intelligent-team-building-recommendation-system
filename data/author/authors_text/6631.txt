Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 262?271,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Split Utterances in Dialogue: a Corpus Study
Matthew Purver, Christine Howes,
and Patrick G. T. Healey
Department of Computer Science
Queen Mary University of London
Mile End Road, London E1 4NS, UK
{mpurver,chrizba,ph}@dcs.qmul.ac.uk
Eleni Gregoromichelaki
Department of Philosophy
King?s College London
Strand, London WC2R 2LS, UK
eleni.gregor@kcl.ac.uk
Abstract
This paper presents a preliminary English
corpus study of split utterances (SUs), sin-
gle utterances split between two or more
dialogue turns or speakers. It has been
suggested that SUs are a key phenomenon
of dialogue, which this study confirms: al-
most 20% of utterances were found to fit
this general definition, with nearly 3% be-
ing the between-speaker case most often
studied. Other claims/assumptions in the
literature about SUs? form and distribu-
tion are investigated, with preliminary re-
sults showing: splits can occur within syn-
tactic constituents, apparently at any point
in the string; it is unusual for the sepa-
rate parts to be complete units in their own
right; explicit repair of the antecedent does
not occur very often. The theoretical con-
sequences of these results for claims in
the literature are pointed out. The prac-
tical implications for dialogue systems are
mentioned too.
1 Introduction
Split utterances (SUs) ? single utterances split be-
tween two or more dialogue turns/speakers ? have
been claimed to occur regularly in dialogue, espe-
cially according to the observations reported in the
Conversational Analysis (CA) literature, which is
based on the analysis of naturally occurring di-
alogues. SUs are of interest to dialogue theo-
rists as they are a clear sign of how turns cohere
with each other at all levels ? syntactic, seman-
tic and pragmatic. They also indicate the radi-
cal context-dependency of conversational contri-
butions. Turns can, in general, be highly ellip-
tical and nevertheless not disrupt the flow of the
dialogue. SUs are the most dramatic illustration
of this: contributions spread across turns/speakers
rely crucially on the dynamics of the unfolding
context, linguistic and extra-linguistic, in order to
guarantee successful processing and production.
Utterances that are split across speakers also
present a canonical example of participant coor-
dination in dialogue. The ability of one partic-
ipant to continue another interlocutor?s utterance
coherently, both at the syntactic and the seman-
tic level, suggests that both speaker and hearer are
highly coordinated in terms of processing and pro-
duction. The initial speaker must be able to switch
to the role of hearer, processing and integrating the
continuation of their utterance, whereas the ini-
tial hearer must be closely monitoring the gram-
mar and content of what they are being offered
so that they can take over and continue in a way
that respects the constraints set up by the first part
of the utterance. In fact there is (anecdotal) ev-
idence that such constraints are fully respected
across speaker and hearer in such utterances (see
e.g. Gregoromichelaki et al (2009)). A large pro-
portion of the CA literature on SUs tries to iden-
tify the conditions under which SUs usually oc-
cur (see section 2). However, this emphasis seems
to miss the important generalisation, confirmed
by the present study, that, syntactically, a speaker
switch may be able to occur anywhere in a string.
From a theoretical point of view, the implica-
tions of the above are that, if such observations
have an empirical foundation, the grammar em-
ployed by the interlocutors must be able to license
and the semantics interpret chunks much smaller
than the usual sentence/proposition units. More-
over, these observations have implications for the
nature of the grammar itself: dynamic, incremen-
tal formalisms seem more amenable to the mod-
262
elling of this phenomenon as the switch of roles
while syntactic/semantic dependencies are pend-
ing can be taken as evidence for direct involve-
ment of the grammar in the successful process-
ing/production of such utterances. Indeed, Poesio
and Rieser (to appear) claim that ?[c]ollaborative
completions . . . are among the strongest evidence
yet for the argument that dialogue requires coor-
dination even at the sub-sentential level? (italics
original).
From a psycholinguistic point of view, the phe-
nomenon of SUs is compatible with mechanis-
tic approaches as exemplified by the Interactive
Alignment model of Pickering and Garrod (2004)
where it is claimed that it should be as easy to
complete someone else?s sentence as one?s own
(Pickering and Garrod, 2004, p186). According
to this model, speaker and listener ought to be in-
terchangeable at any point. This is also the stance
taken by the grammatical framework of Dynamic
Syntax (DS) (Kempson et al, 2001; Cann et al,
2005). In DS, parsing and production are taken
to employ the same mechanisms, leading to a pre-
diction that split utterances ought to be strikingly
natural (Purver et al, 2006). However, from a
pragmatic point of view, utterance continuation
by another speaker might involve some kind of
guessing1 or preempting the other interlocutor?s
intended content. It has therefore been claimed
that a full account of this phenomenon requires
a complete model of pragmatics that can handle
intention recognition and formation. Indeed, Poe-
sio and Rieser (to appear) claim that ?the study
of sentence completions . . . may be used to com-
pare competing claims about coordination ? i.e.
whether it is best explained with an intentional
model like Clark (1996)?s . . . or with a model
based on simpler alignment models like Pickering
and Garrod (2004)?s.? They conclude that a model
which includes modelling of intentions better cap-
tures the data.
For computational models of dialogue, how-
ever, SUs pose a challenge. While Poesio and
Rieser (to appear) and Purver et al (2006) pro-
vide general foundational models for various parts
of the phenomenon, there are many questions that
remain if we are to begin automatic processing.
A computational dialogue system must be able
to identify SUs, match up their two (or more)
1Note that this says nothing about whether such a contin-
uation is the same as the initial speaker?s intended continua-
tion.
parts (which may not necessarily be adjacent), in-
tegrate them into some suitable syntactic and/or
semantic representation, and determine the over-
all pragmatic contribution to the dialogue context.
SUs also have implications for the organisation of
turn-taking in such models (see e.g. Sacks et al
(1974)), as regards what conditions (if any) allow
or prevent successful turn transfer. Additionally,
from a socio-linguistic point of view, turn-taking
operates (according to Schegloff (1995)) not on
individual conversational participants, but on ?par-
ties?. Lerner (1991) suggests that split utterances
can clarify the formation of such parties in that
they reveal evidence of how syntax can be em-
ployed to organise participants into ?groups?.
Analysis of SUs, when they can or cannot oc-
cur, and what effects they have on the coordina-
tion of agents in dialogue, is therefore an area of
interest not only for conversational analysts wish-
ing to characterise systematic interactions in di-
alogue, but also for linguists trying to formulate
grammars of dialogue, psychologists and sociolin-
guists interested in alignment mechanisms and so-
cial interaction, and those interested in building
automatic dialogue processing systems. In this pa-
per we present and examine empirical corpus data
in order to shed light on some of the questions and
controversies around this phenomenon.
2 Related Work
Most previous work on what we call SUs has ex-
amined specific sub-cases, generally of the cross-
speaker type, and have referred to these vari-
ously as collaborative turn sequences (Lerner,
1996; Lerner, 2004), collaborative completions
(Clark, 1996; Poesio and Rieser, to appear),
co-constructions (Sacks, 1992), joint produc-
tions (Helasvuo, 2004), co-participant comple-
tions (Hayashi, 1999; Lerner and Takagi, 1999),
collaborative productions (Szczepek, 2000) and
anticipatory completions (Fox and others, 2007)
(amongst others). Here we discuss some of these
views.
Conversation Analysis Lerner (1991) identifies
various structures typical of SUs which contain
characteristic split points. Firstly he gives a
number of ?compound? turn-constructional units
(TCUs), i.e., structures that include an initial con-
stituent that hearers can identify as introducing
some later final component. Examples include the
IF X-THEN Y, WHEN X-THEN Y and INSTEAD
263
OF X-Y constructions:
(1) A: Before that then if they were ill
G: They get nothing. [BNC H5H 110-111]
Other cues for potential anticipatory completions
include quotation markers (e.g. SHE SAID), paren-
thetical inserts and lists, as well as non-syntactic
cues such as contrast stress or prefaced disagree-
ments. Ru?hlemann (2007) uses corpus analysis to
examine sentence relatives as typical expansions
of another interlocutor?s turn (see also (16)):
(2) A: profit for the group is a hundred and
ninety thousand pounds.
B: Which is superb. [BNC FUK 2460-2461 ]
Opportunistic Cases Although Lerner focuses
on these projectable turn completions, he also
mentions that splits can occur at other points such
as ?intra-turn silence?, hesitations etc. which he
terms opportunistic completions:
(3) A: Well I do know last week thet=uh Al was
certainly very ? pause 0.5?
B: pissed off [(Lerner, 1996, p260)]
As he makes no claims regarding the frequency
of such devices for SUs, it would be interesting to
know how common these are (insomuch as they
occur at all and can be accordingly classified), es-
pecially as studies on SUs in Japanese (Hayashi,
1999) show that although SUs do occur, they do
not rely on compound TCUs.
Expansions vs. Completions Other classifica-
tions of SUs often distinguish between expansions
and completions (Ono and Thompson, 1993). Ex-
pansions are continuations which add, e.g., an ad-
junct, to an already complete syntactic element:
(4) T: It?ll be an E sharp.
G: Which will of course just be played as an
F. [BNC G3V 262-263]
whilst completions involve the addition of syntac-
tic material which is required to make the whole
utterance complete:
(5) A: . . . and then we looked along one deck, we
were high up, and down below there were
rows of, rows of lifeboats in case you see
B: There was an accident.
A: of an accident [BNC HDK 63-65]
In terms of frequency, the only estimate we
know of is Szczepek (2000), where there are ap-
parently 200 cross-person SUs in 40 hours of En-
glish conversation (there is no mention of the num-
ber of sentences or turns this equates to), of which
75% are completions.2 As briefly outlined above,
CA analyses of SUs tend to be broadly descriptive
of what they reveal for conversational practices.
Because such analyses present real examples they
establish that the phenomenon is a genuine one;
however, there is no discussion of its scale (with
the exception of Szczepek (2000), which offers ex-
tremely limited figures). Even though as a gen-
uine phenomenon it is of theoretical interest, the
lack of frequency statistics prevents generalisabil-
ity. Therefore, any claims that SUs are pervasive
in dialogue need empirical backing.
Linguistic Models Purver et al (2006) present
a grammatical model for split utterances, using an
inherently incremental grammar formalism, Dy-
namic Syntax (Kempson et al, 2001; Cann et al,
2005). This model shows how syntactic and se-
mantic processing can be accounted for no mat-
ter where the split occurs in a sentence; how-
ever, as their interest is in grammatical process-
ing, they give no account of any higher-level in-
ferences which may be required. Poesio and
Rieser (to appear) present a general model for col-
laborative completions based in the PTT frame-
work, using an incremental LTAG-based gram-
mar and an information-state-based approach to
context modelling. While many parts of their
model are compatible with a simple alignment-
based communication model like Pickering and
Garrod (2004)?s, they see intention recognition as
crucial to dialogue management. They conclude
that an intention-based model, more like Clark
(1996)?s, is more suitable. Their primary concern
is to show how such a model can account for the
hearer?s ability to infer a suitable continuation, but
their use of an incremental interpretation method
also allows an explanation of the low-level utter-
ance processing required. Nevertheless, the use
of an essentially head-driven grammar formalism
suggests that some syntactic splits that appear in
our corpus might be more problematic than oth-
ers.
Corpus Studies Skuplik (1999), as reported by
Poesio and Rieser (to appear), collected data from
German two-party task-oriented dialogue, and an-
notated for split utterance phenomena. She found
that expansions (cases where the part before the
split can be considered already complete) were
2However, this could be affected by her decision not to
include what she calls appendor questions in her data which
could also be argued to be expansion SUs.
264
more common than completions (where the first
part is incomplete as it stands). Given that this
study focuses on task-oriented dialogue, it needs
to be shown that its results can be replicated in nat-
urally occurring dialogue. In addition, de Ruiter
and van Dienst (in preparation) are also in the pro-
cess of studying other-initiated completions, in the
above sense, and their effect on the progressivity
of dialogue turns; however no results are available
to us at this point in time.
Dialogue Models We are not aware of any
system/model which treats other-person splits,
but same-person ones are now being looked at.
Skantze and Schlangen (2009) present an incre-
mental system design (for a limited domain) which
can react to user feedback, e.g., backchannels, and
resume with utterance completion if interrupted.
Some related empirical work regarding the issue
of turn-switch addressed here is also presented in
Schlangen (2006) but the emphasis there centered
mostly on prosodic rather than grammar/theory-
based factors.
3 Method
3.1 Terminology
In this paper, as our interest is general, we use the
term split utterances (SUs) to cover all instances
where an utterance is spread across more than one
dialogue contribution ? whether the contributions
are by the same or different speakers. We there-
fore use the term split point to refer to the point at
which the utterance is split (rather than e.g. tran-
sition point which is associated with a speaker
change). Cases where speaker does change across
the split will be called other-person splits; oth-
erwise same-person splits. One of the reasons
for including same-person splits is that there are
claims in the literature that the initial speaker may
strategically continue completing their own utter-
ance, after another person?s intervention, as an al-
ternative to acceptance or rejection of this inter-
vention (delayed completion, (Lerner, 1996)). In
addition, both grammatical formalisms (Purver et
al., 2006) and psycholinguistic models (Picker-
ing and Garrod, 2004) predict that SUs should be
equally natural in both the same- and other- person
conditions.
As not all cases will lead to complete contri-
butions, and not all will be split over exactly two
contributions, we also avoid terms like first-half,
second-half and completion: instead the contri-
butions on either side of a split point will be re-
ferred to as the antecedent and the continuation.
In cases where an utterance has more than one split
point, some portions may therefore act as the con-
tinuation for one split point, and the antecedent for
the next.
3.2 Questions
General Our first interest is in the general statis-
tics regarding SUs: how often do they occur, and
what is the balance between same- and other-
person splits? Do they usually fall into the specific
categories (with specific preferred split points) ex-
amined by e.g. Lerner (1991), or can the split
point be anywhere?
Completeness For a grammatical treatment
of SUs, as well as for implementing pars-
ing/production mechanisms for their processing,
we need to know about the likely completeness
of antecedent and continuation (if they are al-
ways complete in their own right, a standard head-
driven grammar may be suitable; if not, some-
thing more fundamentally incremental may be re-
quired). In addition, CA and other strategic anal-
yses of dialogue phenomena predict that split ut-
terances should occur at turn-transfer points that
are foreseeable by the participants. Complete syn-
tactic units serve this purpose from this point of
view and lack of such completeness will seem
to weaken this general claim. We therefore ask
how often antecedents and continuations are them-
selves complete,3 and look at the syntactic and lex-
ical categories which occur either side of the split.
Repair and Overlap Thirdly, we look at how
often splits involve explicit repair of antecedent
material, and how this depends on antecedent
completeness. Although, sometimes, repair might
be attributed to overlap or speaker uncertainty, it
also might indicate issues regarding preemptive
tactics on the part of the current speaker who needs
to reformulate the original contribution in order
to accommodate their novel offering or take into
account feedback offered while constructing their
utterance. Amount of repair also indicates the de-
gree of attempt the current speaker is making to
3For antecedents, we are more interested in whether they
end in a way that seems complete (they may have started ir-
regularly due to overlap or another split); for continuations,
whether they start in such a way (they may not get finished
for some other reason, but we want to know if they would be
complete if they do get finished).
265
Tag Value Explanation
end-complete y/n For all sentences: does this sentence end in such a way as to
yield a complete proposition or speech act?
continues sentence ID For all sentences: does this sentence continue the proposition
or speech act of a previous sentence? If so, which one?
repairs number of words For continuations: does this continuation explicitly repair
words in the antecedent? If so, how many?
start-complete y/n For continuations: does this continuation start in such a way as
to be able to stand alone as a complete proposition or speech
act?
Table 1: Annotation Tags
integrate syntactically their contribution with the
antecedent. However, we also examine how often
continuations involve overlap, which also has im-
plications for turn-taking management, and how
this depends on antecedent completeness.
3.3 Corpus
For this exercise we used the portion of the
BNC (Burnard, 2000) annotated by Ferna?ndez and
Ginzburg (2002), chosen to maintain a balance be-
tween context-governed dialogue (tutorials, meet-
ings, doctor?s appointments etc.) and general con-
versation. This portion comprises 11,469 sen-
tences taken from 200-turn sections of 53 separate
dialogues.
The BNC transcripts are already annotated for
overlapping speech, for non-verbal noises (laugh-
ter, coughing etc.) and for significant pauses.
Punctuation is included, based on the original au-
dio and the transcribers? judgements; as the au-
dio is not available, we allowed annotators to use
punctuation where it aided interpretation. The
BNC transcription protocol provides a sentence-
level annotation as well as an utterance (turn)-level
one, where turns may be made of several sentences
by the same speaker. We annotated at a sentence-
level, to allow self-continuations within a turn to
be examined. The BNC also forces turns to be
presented in linear order, which is vital if we are
to accurately assess whether turns are continua-
tions of one another; however, this has a side-
effect of forcing long turns to appear split into sev-
eral shorter turns when interrupted by intervening
backchannels. We will discuss this further below.
Annotation Scheme The initial stage of manual
annotation involved 4 tags: start-complete,
end-complete, continues and repairs ?
these are explained in Table 1 above. Sentences
which somehow require continuation (whether
they receive it or not) are therefore those marked
end-complete=n; sentences which act as
continuations are those marked with non-empty
continues tags; and their antecedents are the
values of those continues tags. Further specific
information about the syntactic or lexical nature of
antecedent or continuation components could then
be extracted (semi-)automatically, using the BNC
transcript and part-of-speech annotations.
Inter-Annotator Agreement Three annotators
were used, all linguistically knowledgeable. First,
all three annotators annotated one dialogue inde-
pendently, then compared results and discussed
differences. They then annotated 3 further di-
alogues independently to assess inter-annotator
agreement; kappa statistics (Carletta, 1996) are
shown in Table 2 below.
Tag KND KBG KB0
end-complete .86-.92 .80-1.0 .73-.90
continues (y/n) .89-.81 .76-.85 .77-.89
continues (ant) .90-.82 .74-.85 .76-.86
repairs 1.0-1.0 .55-.81 1.0-1.0
Table 2: Inter-Annotator ? statistic (min-max)
With the exception of the repairs tag for one
annotator pair for one dialogue, all are above 0.7;
the low figure results from a few disagreements
in a dialogue with only a very small number of
repairs instances. The remaining dialogues
were divided evenly between the three annotators.
4 Results and Discussion
The 11,469 sentences annotated yielded 2,228
SUs, of which 1,902 were same-person and 326
other-person splits; 111 examples involved an ex-
plicit repair by the continuation of some part of the
antecedent.
266
person: same other
overlapping 0 17
adjacent 840 260
sep. by overlap 320 10
sep. by backchnl 460 17
sep. by 1 sent 239 16
sep. by 2 sents 31 4
sep. by 3 sents 5 1
sep. by 4 sents 4 0
sep. by 5 sents 1 0
sep. by 6 sents 2 1
Total 1902 326
Table 3: Antecedent/continuation separation
General Same-person splits are much more
common than other-person; however, this is partly
an artefact of the BNC transcription protocol
(which forces contributions to be linearly ordered)
and our choice to annotate at the sentence level.
Around 44% of same-person cases are splits be-
tween sentences within the same-speaker turn;
and a further 17% are separated only by other-
speaker material which entirely overlaps with the
antecedent and therefore does not necessarily ac-
tually interrupt the turn. Both of these might be
considered as single utterances under some views.
However, we believe that splits between same-
turn sentences must be investigated in that the
transcription into separate sentences does indicate
some pause or other separating prosody and, from
a processing/psycholinguistic point of view, it
should be determined whether other-person splits
occur in the same places as same-person split
boundaries. Even in cases of overlap, one can-
not exclude the fact that the shape of the current
speaker?s utterance is influenced by receipt of the
feedback. Nevertheless, we will examine these
issues in further research and hence we exclude
within-turn splits of this type from here on.
Many splits are non-adjacent (see Table 3), with
the antecedent and continuation separated by at
least one intervening sentence. In same-person
cases, once we have excluded the within-turn
splits described above, this must in fact always
be the case; the intervening material is usually a
backchannel (62% of remaning cases) or a sin-
gle other sentence (32%, often e.g. a clarification
question), but two intervening sentences are possi-
ble (4%) with up to six being seen. In other-person
cases, 88% are adjacent or separated only by over-
lapping material, but again up to six intervening
person: same other
and/but/or 748 116
so/whereas 257 39
because 77 3
(pause) 56 5
which/who/etc 26 4
instead of 4 1
said/thought/etc 14 0
if then 1 0
when then 1 1
(other) 783 161
Table 4: Continuation categories
sentences were seen, with a single sentence most
common (10%, in half of which the intervening
sentence was a backchannel).
Many utterances have more than one split. In
same-person cases, a single utterance can be split
over as many as thirteen individual sentence con-
tributions; although such extreme cases occur gen-
erally within one-sided dialogues such as tutori-
als, many multi-split cases are also seen in general
conversation. Only 63% of cases consisted of only
two contributions. Antecedents can also receive
more than one competing continuation, although
this is rare: two continuations are seen in 2% of
cases.
CA Categories We searched for examples
which match CA categories (Lerner, 1991;
Ru?hlemann, 2007) by looking for particular lex-
ical items on either side of the split. Matching was
done loosely, to allow for the ungrammatical na-
ture of dialogue ? for example, an instance was
taken to match the IF X-THEN Y pattern if the con-
tinuation began with ?then? (modulo filled pauses
and non-verbal material) and the antecedent con-
tained ?if? at any point) ? so the counts may be
over-estimates. For Lerner (1996)?s opportunistic
cases, we looked for filled pauses (?er/erm? etc.)
or pauses explicitly annotated in the transcript, so
counts in this case may be underestimates.4 We
also chose some other broad categories based on
our observations of the most common cases. Re-
sults are shown in Table 4.5
The most common of the CA categories can be
4In further research we will examine other features as spe-
cialised laugh tokens, repetitions etc. as well as their particu-
lar positioning
5Note that the categories in Table 4 are not all mutually
exclusive (e.g. an example may have both an ?and?-initial
continuation and an antecedent ending in a pause), so column
sums will not match Table 3.
267
seen to be Lerner (1996)?s hesitation-related op-
portunistic cases, which make up at least 2-3% of
both same- and other-person splits. Ru?hlemann
(2007)?s sentence relative clause cases are next,
with over 1%; the others make up only small pro-
portions.
In contrast, by far the most common pattern (for
both same- and other-) is the addition of an ex-
tending clause, either a conjunction introduced by
?and/but/or/nor? (35-40%), or other clause types
with ?so/whereas/nevertheless/because?. Other
less obviously categorisable cases make up 40-
50% of continuations, with the most common first
words being ?you?, ?it?, ?I?, ?the?, ?in? and ?that?.
Completeness and repair Examination of the
end-complete annotations shows that about
8% of sentences in general are incomplete, but
that (perhaps surprisingly) only 63% of these get
continued. For both same- and other-person con-
tinuations, the vast majority (72% and 74%) con-
tinue an already complete antecedent, with only
26-28% therefore being completions in the sense
of e.g. de Ruiter and van Dienst (in preparation).
This does, however, mean that continuations are
significantly more likely than other sentences to
follow an incomplete antecedent (p < 0.001 us-
ing ?2(1)). Interestingly, though, continuations are
no more likely than other sentences to be complete
themselves.
The frequent clausal categories from Table 4 are
all more likely to continue complete antecedents
than incomplete ones, with the exception of the
(other) category; this suggests that split points
often occur at random points in a sentence, without
regard to particular clausal constructions (see also
A.1 for more examples and context):
(6) D: you know what the actual variations
U: entails
D: entails. you know what the actual quality
of the variations are.
[BNC G4V 114-117]
For the less frequent (e.g. ?if/then?, ?instead of?)
categories, the counts are too low to be sure.
Excluding all the clausal constructions (i.e.
looking only at the general (other) category),
and looking only at other-person cases, we see that
antecedents often end in a complete way (53%) but
that continuations do not often start in a complete
way (24%). Continuations are more than twice
as likely to start in a non-complete as opposed
to complete way, even after complete antecedents.
Explicit repair of some portion of the antecedent
is not common, only occurring in just under 5%
of splits. As might be expected, incomplete an-
tecedents are more likely to be repaired (13% vs.
2%, p < 0.001 using ?2(1)). Other-continuations
are also significantly more likely to repair their an-
tecedents than same-person cases (10% vs. 4%,
p < 0.001 using ?2(1)).
Problematic cases Examination of the data
shows that SUs is not necessarily an autonomous
well-defined category independent of other frag-
ment classifications in the literature. Besides cases
where it is not easy to identify whether a fragment
is a continuation or not or what the antecedent
is (see A.2), there are also cases where, as has
already been pointed out in the literature (Gre-
goromichelaki et al, 2009; Bunt, 2009), fragments
exhibit multifunctionality. This can be illustrated
by the following where the continuation could be
taken also as request for confirmation/question (7)
or a reply to a clarification request (8):
(7) M: It?s generated with a handle and
J: Wound round?
M: Yes [BNC K69 109-112]
(8) S: Quite a good word processor.
J: A word processor?
S: Which is vag- it?s basically a subset of
Word. [BNC H61 37-39]
In this respect, an interesting category is Lerner?s
delayed completions where often the continuation
also serves as some kind of repair or reformulation
(see e.g. (6) and A.3 (26)).
5 Conclusions
Although most of Lerner (1991)?s categories ap-
pear, they are not necessarily the most frequent.
On the other hand, the general results seem to in-
dicate that splits can occur anywhere in a string,
both in the same- or other- conditions. Both these
are consistent with models that advocate highly
coordinated resources between interlocutors and,
moreover, the need for highly incremental means
of processing (Purver et al, 2006; Skantze and
Schlangen, 2009). From a computational mod-
elling point of view, the results also indicate that
start-completeness of continuations is rare, which
means that a dialogue system has a chance of spot-
ting continuations from surface characteristics of
268
the input. This is hampered though by the fact
that the split can occur within any type of syn-
tactic constituent, hence no reliable grammatical
features can be employed securely. On the other
hand, end-incompleteness of antecedents is not as
common as would be expected and long distances
between antecedent and continuation are possible.
In this respect, locating the antecedent is not a
straightforward task for automated systems, espe-
cially again as this can be any type of constituent.
References
H. Bunt. 2009. Multifunctionality and multidimen-
sional dialogue semantics. In Proceedings of Dia-
Holmia, 13th SEMDIAL Workshop.
L. Burnard. 2000. Reference Guide for the British Na-
tional Corpus (World Edition). Oxford University
Computing Services http://www.natcorp.
ox.ac.uk/docs/userManual/.
R. Cann, R. Kempson, and L. Marten. 2005. The Dy-
namics of Language. Elsevier, Oxford.
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?255.
H. Clark. 1996. Using Language. Cambridge Univer-
sity Press.
J. de Ruiter and M. van Dienst. in preparation. Com-
pleting other people?s utterances: evidence for for-
ward modeling in conversation. ms.
R. Ferna?ndez and J. Ginzburg. 2002. Non-sentential
utterances: A corpus-based study. Traitement Au-
tomatique des Langues, 43(2).
A. Fox et al 2007. Principles shaping grammati-
cal practices: an exploration. Discourse Studies,
9(3):299.
E. Gregoromichelaki, Y. Sato, R. Kempson, A. Gargett,
and C. Howes. 2009. Dialogue modelling and the
remit of core grammar. In Proceedings of IWCS.
M. Hayashi. 1999. Where Grammar and Interac-
tion Meet: A Study of Co-Participant Completion in
Japanese Conversation. Human Studies, 22(2):475?
499.
M. Helasvuo. 2004. Shared syntax: the gram-
mar of co-constructions. Journal of Pragmatics,
36(8):1315?1336.
R. Kempson, W. Meyer-Viol, and D. Gabbay. 2001.
Dynamic Syntax: The Flow of Language Under-
standing. Blackwell.
G. Lerner and T. Takagi. 1999. On the place
of linguistic resources in the organization of talk-
in-interaction: A co-investigation of English and
Japanese grammatical practices. Journal of Prag-
matics, 31(1):49?75.
G. Lerner. 1991. On the syntax of sentences-in-
progress. Language in Society, pages 441?458.
G. Lerner. 1996. On the semi-permeable character
of grammatical units in conversation: Conditional
entry into the turn space of another speaker. In
E. Ochs, E. A. Schegloff, and S. A. Thompson,
editors, Interaction and grammar, pages 238?276.
Cambridge University Press.
G. Lerner. 2004. Collaborative turn sequences. In
Conversation analysis: Studies from the first gener-
ation, pages 225?256. John Benjamins.
T. Ono and S. Thompson. 1993. What can conversa-
tion tell us about syntax. In P. Davis, editor, Alterna-
tive Linguistics: Descriptive and Theoretical Modes.
Benjamin.
M. Pickering and S. Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and
Brain Sciences, 27:169?226.
M. Poesio and H. Rieser. to appear. Completions, co-
ordination, and alignment in dialogue. Ms.
M. Purver, R. Cann, and R. Kempson. 2006.
Grammars as parsers: Meeting the dialogue chal-
lenge. Research on Language and Computation,
4(2-3):289?326.
C. Ru?hlemann. 2007. Conversation in context: a
corpus-driven approach. Continuum.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
H. Sacks. 1992. Lectures on Conversation. Blackwell.
E. Schegloff. 1995. Parties and talking together: Two
ways in which numbers are significant for talk-in-
interaction. Situated order: Studies in the social
organization of talk and embodied activities, pages
31?42.
D. Schlangen. 2006. From reaction to prediction: Ex-
periments with computational models of turn-taking.
In Proceedings of the 9th International Conference
on Spoken Language Processing (INTERSPEECH -
ICSLP).
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proceedings
of the 12th Conference of the European Chapter of
the ACL (EACL 2009).
K. Skuplik. 1999. Satzkooperationen. definition und
empirische untersuchung. SFB 360 1999/03, Biele-
feld University.
B. Szczepek. 2000. Formal Aspects of Collaborative
Productions in English Conversation. Interaction
and Linguistic Structures (InLiSt), http://www.
uni-potsdam.de/u/inlist/issues/17/.
269
A Examples
A.1 Split points
(6) D: Yeah I mean if you?re looking at quan-
titative things it?s really you know how
much actual- How much variation hap-
pens whereas qualitative is ?pause? you
know what the actual variations
U: entails
D: entails. you know what the actual quality
of the variations are.
[BNC G4V 114-117]
(9) A: All the machinery was
G: [[All steam.]]6
A: [[operated]] by steam
[BNC H5G 177-179]
(10) K: I?ve got a scribble behind it, oh annual re-
port I?d get that from.
S: Right.
K: And the total number of [[sixth form stu-
dents in a division.]]
S: [[Sixth form stu-
dents in a division.]] Right.
[BNC H5D 123-127]
(11) M: 292 And another sixteen percent is the
other Ne- Nestle coffee ?pause? erm
Blend Thirty Seven which I used to drink
a long time ago and others ?laugh? and
twenty two percent is er ?pause?
U: Maxwell.
M: Maxwell House, which has become the
other local brand now seeing as how
Maxwell House is owned by Kraft, and
Kraft now own Terry?s.
[BNC G3U 292-294]
(12) A: Erm because as Moira said that Kraft is
erm ?pause? now what was she saying,
what was she saying Kraft is the same as
?pause?
M: Craft? [BNC G3U 412-413]
(13) J: And I couldn?t remember whether she
said at the end of the three months or
A: End of the month. [BNC H4P 17-18]
6Overlapping material is shown in double square brackets,
aligned with the material with which it co-occurs.
(14) G: Had their own men
A: unload the boats?
G: unload the boats, yes. [BNC H5H 91-93]
(15) G: That?s right they had to go on a rota.
A: Run by the Dock Commission?
G: Run by the Dock Commission.
[BNC H5H 100-102]
(16) A: So I thought, oh, I think I?ll put lace over
it, it?ll tone the lilac [[down.]]
B: [[down.]] Yes.
Which it is has done
[BNC KBC 3195-3198]
A.2 Uncertain antecedents
(17) C: Look you?re cleaning this ?pause?
[[with erm]]
G: [[That box.]]
C: [[This.]]
G: [[With]] this. [[And this.]]
C: [[And this.]] [[And this.]]
G: [[And this.]]
Whoops! [BNC KSR 9-17]
(18) S: You?re trying to be everything ?pause?
and they?re pushing it away cos it?s not
what they really want ?pause? and they, I
mean, all, all you can get from him is how
marvellous, you?re right, how marvellous
his brothers are ?pause? and yet, what I?ve
heard of the brothers they?re not
C: Not much, [[yeah.]]
S: [[they?re]] not all that marvel-
lous, they?re not really that much to look
[[up]]
C: [[Ah]].
S: to.
C: No [BNC KBG 76-81]
(19) S: Well this is why I think he?d be better
off, hi- his needs ?pause? are not met by a
class teacher. And I don?t think they have
been for this last
C: Mm, we need a support teacher [[to go
there.]]
S: [[for the
last]] year. But yo-, you need somebody
who?s gonna work with him every day
?pause? and ?pause? with an individual
programme and you just can?t offer that
?pause? in a class. [BNC KBG 56-60]
270
(20) M: I might be a bit biased, I think they still
do that but I think erm ?pause?
J: The television has ?pause?
M: the television has made a difference. I
think not only just at fire stations, I think
in the whole of life, hasn?t it?
[BNC K69 51-54]
(21)A5: I?ll definitely use that
U: ?reading?:[ Get a headache ]?
A5: [[in getting to know ]]
A2: [[Year seven ]]
A5: new [[year seven]]
A2: [[Oh yeah]] for year seven
[BNC J8D 190-195]
(22) G: Well a chain locker is where all the spare
chain used to like coil up
A: So it ?unclear? came in and it went round
G: round the barrel about three times round
the barrel then right down into the chain
locker but if you kept, let it ride what we
used to call let it ride well ?unclear? well
now it get so big then you have to run it
all off cos you had one lever, that?s what
you had and the steam valve could have
all steamed. [BNC H5G 174:176]
A.3 Multifunctionality of fragments
(7) Completion and confirmation request:
J: How does it generate?
M: It?s generated with a handle and
J: Wound round?
M: Yes, wind them round and this should,
should generate a charge which rang bells
and sounded bells and then er you lift up a
telephone and plug in a jack and, and take
a message in that way.
[BNC K69 109-112]
(23) Completion and confirmation request:
G: Had their own men
A: unload the boats?
G: unload the boats, yes. [BNC H5H 91-93]
(24) Late completion and (repetitive) confir-
mation:
N: Alistair [last or full name] erm he?s, he?s
made himself er he has made himself co-
ordinator.
U: And section engineer.
N: And section engineer.
N: I didn?t sign it as coordinator.
[BNC H48 141-144]
(25) Completion and clarification reply:
John: If you press N
Sarah: N?
John: N for name, it?ll let you type in the docu
document name. [BNC G4K 84-86]
(26) Expansion and reformulation/repair:
S: Secondly er
J: We guarantee P five.
S: We we are we?re guaranteeing P five plus
a noise level.
J: Yeah. [BNC JP3 167-170]
(27) Expansion and question:
I: I can?t remember exactly who lived on
the right hand side, I?ve forgotten but th
I know the Chief Clerk lived just a little
way down [address], you see, er
A: In one of those little red brick cottages?
[BNC HDK 124-125]
(28) Answer and expansion:
A: We could hear it from outside ?unclear?.
R: Oh you could hear it?
A: Occasionally yeah. [BNC J8D 13-15]
(29) Answer/reformulation and expansion:
G: [address], that was in the middle, more or
less in the middle of the town.
A: And you called that the manual?
G: The manual school, yes.
[BNC H5G 96-98]
271
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 306?309,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Cascaded Lexicalised Classifiers for Second-Person Reference Resolution
Matthew Purver
Department of Computer Science
Queen Mary University of London
London E1 4NS, UK
mpurver@dcs.qmul.ac.uk
Raquel Ferna?ndez
ILLC
University of Amsterdam
1098 XH Amsterdam, Netherlands
raquel.fernandez@uva.nl
Matthew Frampton and Stanley Peters
CSLI
Stanford University
Stanford, CA 94305, USA
frampton,peters@csli.stanford.edu
Abstract
This paper examines the resolution of the
second person English pronoun you in
multi-party dialogue. Following previous
work, we attempt to classify instances as
generic or referential, and in the latter case
identify the singular or plural addressee.
We show that accuracy and robustness can
be improved by use of simple lexical fea-
tures, capturing the intuition that different
uses and addressees are associated with
different vocabularies; and we show that
there is an advantage to treating referen-
tiality and addressee identification as sep-
arate (but connected) problems.
1 Introduction
Resolving second-person references in dialogue is
far from trivial. Firstly, there is the referentiality
problem: while we generally conceive of the word
you1 as a deictic addressee-referring pronoun, it
is often used in non-referential ways, including as
a discourse marker (1) and with a generic sense
(2). Secondly, there is the reference problem: in
addressee-referring cases, we need to know who
the addressee is. In two-person dialogue, this is
not so difficult; but in multi-party dialogue, the ad-
dressee could in principle be any one of the other
participants (3), or any group of more than one (4):
(1) It?s not just, you know, noises like something
hitting.
(2) Often, you need to know specific button
sequences to get certain functionalities done.
(3) I think it?s good. You?ve done a good review.
(4) I don?t know if you guys have any questions.
1We include your, yours, yourself, yourselves.
This paper extends previous work (Gupta et al,
2007; Frampton et al, 2009) in attempting to au-
tomatically treat both problems: detecting refer-
ential uses, and resolving their (addressee) refer-
ence. We find that accuracy can be improved by
the use of lexical features; we also give the first
results for treating both problems simultaneously,
and find that there is an advantage to treating them
as separate (but connected) problems via cascaded
classifiers, rather than as a single joint problem.
2 Related Work
Gupta et al (2007) examined the referentiality
problem, distinguishing generic from referential
uses in multi-party dialogue; they found that 47%
of uses were generic and achieved a classification
accuracy of 75%, using various discourse features
and discriminative classifiers (support vector ma-
chines and conditional random fields). They at-
tempted the reference-resolution problem, using
only discourse (non-visual) features, but accuracy
was low (47%).
Addressee identification in general (i.e. in-
dependent of the presence of you) has been ap-
proached in various ways. Traum (2004) gives
a rule-based algorithm based on discourse struc-
ture; van Turnhout et al (2005) used facial ori-
entation as well as utterance features; and more
recently Jovanovic (2006; 2007) combined dis-
course and gaze direction features using Bayesian
networks, achieving 77% accuracy on a portion of
the AMI Meeting Corpus (McCowan et al, 2005)
of 4-person dialogues.
In recent work, therefore, Frampton et al
(2009) extended Gupta et al?s method to in-
clude multi-modal features including gaze direc-
tion, again using Bayesian networks on the AMI
corpus. This gave a small improvement on the ref-
306
erentiality problem (achieving 79% accuracy), and
a large improvement on the reference-resolution
task (77% accuracy distinguishing singular uses
from plural, and 80% resolving singular individ-
ual addressee reference).
However, they treated the two tasks in isola-
tion, and also broke the addressee-reference prob-
lem into two separate sub-tasks (singular vs. plu-
ral reference, and singular addressee reference). A
full computational you-resolution module would
need to treat all tasks (either simultaneously as one
joint classification problem, or as a cascaded se-
quence) ? with inaccuracy at one task necessar-
ily affecting performance at another ? and we ex-
amine this here. In addition, we examine the ef-
fect of lexical features, following a similar insight
to Katzenmaier et al (2004); they used language
modelling to help distinguish between user- and
robot-directed utterances, as people use different
language for the two ? we expect that the same is
true for human participants.
3 Method
We used Frampton et al (2009)?s AMI corpus
data: 948 ?you?-containing utterances, manu-
ally annotated for referentiality and accompanied
by the AMI corpus? original addressee annota-
tion. The very small number of two-person ad-
dressee cases were joined with the three-person
(i.e. all non-speaker) cases to form a single ?plu-
ral? class. 49% of cases are generic; 32% of
referential cases are plural, and the rest are ap-
proximately evenly distributed between the singu-
lar participants. While Frampton et al (2009) la-
belled singular reference by physical location rel-
ative to the speaker (giving a 3-way classification
problem), our lexical features are more suited to
detecting actual participant identity ? we there-
fore recast the singular reference task as a 4-way
classification problem and re-calculate their per-
formance figures (giving very similar accuracies).
Discourse Features We use Frampton et al
(2009)?s discourse features. These include sim-
ple durational and lexical/phrasal features (includ-
ing mention of participant names); AMI dialogue
act features; and features expressing the simi-
larity between the current utterance and previ-
ous/following utterances by other participants. As
dialogue act features are notoriously hard to tag
automatically, and ?forward-looking? information
about following utterances may be unavailable in
an on-line system, we examine the effect of leav-
ing these out below.
Visual Features Again we used Frampton et al
(2009)?s features, extracted from the AMI corpus
manual focus-of-attention annotations which track
head orientiation and eye gaze. Features include
the target of gaze (any participant or the meet-
ing whiteboard/projector screen) during each ut-
terance, and information about mutual gaze be-
tween participants. These features may also not
always be available (meeting rooms may not al-
ways have cameras), so we investigate the effect
of their absence below.
Lexical Features The AMI Corpus simulates a
set of scenario-driven business meetings, with par-
ticipants performing a design task (the design of
a remote control). Participants are given specific
roles to play, for example that of project manager,
designer or marketing expert. It therefore seems
possible that utterances directed towards particular
individuals will involve the use of different vocab-
ularies reflecting their expertise. Different words
or phrases may also be associated with generic
and referential discussion, and extracting these au-
tomatically may give benefits over attempting to
capture them using manually-defined features. To
exploit this, we therefore added the use of lexical
features: one feature for each distinct word or n-
gram seen more than once in the corpus. Although
such features may be corpus- or domain-specific,
they are easy to extract given a transcript.
4 Results and Discussion
4.1 Individual Tasks
We first examine the effect of lexical features on
the individual tasks, using 10-way cross-validation
and comparing performance with Frampton et al
(2009). Table 1 shows the results for the referen-
tiality task in terms of overall accuracy and per-
class F1-scores; ?MC Baseline? is the majority-
class baseline; results labelled ?EACL? are Framp-
ton et al (2009)?s figures, and are presented for
all features and for reduced feature sets which
might be more realistic in various situations: ?-V?
removes visual features; ?-VFD? removes visual
features, forward-looking discourse features and
dialogue-act tag features.
As can be seen, adding lexical features
(?+words? adds single word features, ?+3grams?
adds n-gram features of lengths 1-3) improves the
307
Features Acc Fgen Fref
MC Baseline 50.9 0 67.4
EACL 79.0 80.2 77.7
EACL -VFD 73.7 74.1 73.2
+words 85.3 85.7 84.9
+3grams 87.5 87.4 87.5
+3grams -VFD 87.2 86.9 87.6
3grams only 85.9 85.2 86.4
Table 1: Generic vs. referential uses
Features Acc Fsing Fplur
MC Baseline 67.9 80.9 0
EACL 77.1 83.3 63.2
EACL -VFD 71.4 81.5 37.1
+words 83.1 87.8 72.5
+3grams 85.9 90.0 76.6
+3grams -VFD 87.1 91.0 77.6
3grams only 86.9 90.8 77.0
Table 2: Singular vs. plural reference.
performance significantly ? accuracy is improved
by 8.5% absolute above the best EACL results,
which is a 40% reduction in error. Robustness to
removal of potentially problematic features is also
improved: removing all visual, forward-looking
and dialogue act features makes little difference.
In fact, using only lexical n-gram features, while
reducing accuracy by 2.6%, still performs better
than the best EACL classifier.
Table 2 shows the equivalent results for the
singular-plural reference distinction task; in this
experiment, we used a correlation-based fea-
ture selection method, following Frampton et al
(2009). Again, performance is improved, this time
giving a 8.8% absolute accuracy improvement, or
38% error reduction; robustness to removing vi-
sual and dialogue act features is also very good,
even improving performance.
For the individual reference task (again using
feature selection), we give a further ?NS baseline?
of taking the next speaker; note that this performs
rather well, but requires forward-looking informa-
tion so should not be compared to ?-F? results.
Results are again improved (Table 3), but the im-
provement is smaller: a 1.4% absolute accuracy
improvement (7% error reduction); we conclude
from this that visual information is most impor-
tant for this part of the task. Robustness to feature
unavailability still shows some improvement: ex-
Features Acc FP1 FP2 FP3 FP4
MC baseline 30.7 0 0 0 47.0
NS baseline 70.7 71.6 71.1 72.7 68.2
EACL 80.3 82.8 79.7 75.9 81.4
EACL -V 73.8 79.2 70.7 74.1 71.4
EACL -VFD 56.6 58.9 55.5 64.0 47.3
+words 81.4 83.9 79.7 79.3 81.8
+3grams 81.7 83.9 80.3 79.3 82.5
+3grams -V 74.8 81.3 71.7 75.2 71.4
+3grams -VFD 60.7 66.3 55.9 66.2 53.0
3grams only 60.7 63.1 58.1 52.9 63.4
3grams +NS 74.5 76.7 73.8 75.0 72.7
Table 3: Singular addressee detection.
cluding all visual, forward-looking and dialogue-
act features has less effect than on the EACL sys-
tem (60.7% vs. 56.6% accuracy), and a system
using only n-grams and the next speaker identity
gives a respectable 74.5%.
Feature Analysis We examined the contribu-
tion of particular lexical features using Informa-
tion Gain methods. For the referentiality task, we
found that generic uses of you were more likely
to appear in utterances containing words related to
the main meeting topic, such as button, channel,
or volume (properties of the to-be-designed remote
control). In contrast, words related to meeting
management, such as presentation, email, project
and meeting itself, were predictive of referential
uses. The presence of first person pronouns and
discourse and politeness markers such as okay,
please and thank you was also indicative of refer-
entiality, as were n-grams capturing interrogative
structures (e.g. do you).
For the plural/singular distinction, we found
that the plural first person pronoun we correlated
with plural references of you. Other predictive n-
grams for this task were you mean and you know,
which were indicative of singular and plural refer-
ences, respectively. Finally, for the individual ref-
erence task, useful lexical features included par-
ticipant names, and items related to their roles.
For instance, the n-grams sales, to sell and make
money correlated with utterances addressed to the
?marketing expert?, while utterances containing
speech recognition and technical were addressed
to the ?industrial designer?.
Discussion The best F-score of the three sub-
tasks is for the generic/referential distinction; the
308
Features Acc Fgen Fplur FP1 FP2 FP3 FP4
MC baseline 49.1 65.9 0 0 0 0 0
EACL 58.3 73.3 24.3 57.6 57.0 36.0 51.1
+3grams 60.9 74.8 42.0 57.7 52.2 35.6 50.2
3grams only 67.5 84.8 61.6 39.1 39.3 30.6 38.6
Cascade +3grams 78.1 87.4 59.1 64.1 76.4 75.0 82.6
Table 4: Combined task: generic vs. plural vs. singular addressee.
worst is for the detection of plural reference (Fplur
in Table 2). This is not surprising: humans find the
former task easy to annotate ? Gupta et al (2007)
report good inter-annotator agreement (? = 0.84)
? but the latter hard. In their analysis of the AMI
addressee annotations, Reidsma et al (2008) ob-
serve that most confusions amongst annotators are
between the group-addressing label and the labels
for individuals; whereas if annotators agree that an
utterance is addressed to an individual, they also
reach high agreement on that addressee?s identity.
4.2 Combined Task
We next combined the individual tasks into one
combined task; for each you instance, a 6-way
classification as generic, group-referring or refer-
ring to one of the 4 participants. This was at-
tempted both as a single classification exercise us-
ing a single Bayesian network; and as a cascaded
pipeline of the three individual tasks; see Table 4.
Both used correlation-based feature selection.
For the single joint classifier, n-grams again im-
prove performance over the EACL features. Using
only n-grams gives a significant improvement, per-
haps due to the reduction in the size of the feature
space on this larger problem. Accuracy is reason-
able (67.5%), but while F-scores are good for the
generic class (above 80%), others are low.
However, use of three cascaded classifiers
improves performance to 78% and gives large
per-class F-score improvements, exploiting
the higher accuracy of the first two stages
(generic/referential, singular/plural), and the fact
that different features are good for different tasks.
5 Conclusions
We have shown that the use of simple lexical fea-
tures can improve performance and robustness for
all aspects of second-person pronoun resolution:
referentiality detection and reference identifica-
tion. An overall 6-way classifier is feasible, and
cascading individual classifiers can help. Future
plans include testing on ASR transcripts, and in-
vestigating different classification techniques for
the joint task.
References
M. Frampton, R. Ferna?ndez, P. Ehlen, M. Christoudias,
T. Darrell, and S. Peters. 2009. Who is ?you?? com-
bining linguistic and gaze features to resolve second-
person references in dialogue. In Proceedings of the
12th Conference of the EACL.
S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky.
2007. Resolving ?you? in multi-party dialog. In
Proceedings of the 8th SIGdial Workshop on Dis-
course and Dialogue.
N. Jovanovic, R. op den Akker, and A. Nijholt. 2006.
Addressee identification in face-to-face meetings. In
Proceedings of the 11th Conference of the EACL.
N. Jovanovic. 2007. To Whom It May Concern -
Addressee Identification in Face-to-Face Meetings.
Ph.D. thesis, University of Twente, The Netherlands.
M. Katzenmaier, R. Stiefelhagen, and T. Schultz. 2004.
Identifying the addressee in human-human-robot in-
teractions based on head pose and speech. In Pro-
ceedings of the 6th International Conference on
Multimodal Interfaces.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-
coln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of the 5th International Conference on
Methods and Techniques in Behavioral Research.
D. Reidsma, D. Heylen, and R. op den Akker. 2008.
On the contextual analysis of agreement scores. In
Proceedings of the LREC Workshop on Multimodal
Corpora.
D. Traum. 2004. Issues in multi-party dialogues. In
F. Dignum, editor, Advances in Agent Communica-
tion, pages 201?211. Springer-Verlag.
K. van Turnhout, J. Terken, I. Bakx, and B. Eggen.
2005. Identifying the intended addressee in mixed
human-humand and human-computer interaction
from non-verbal features. In Proceedings of ICMI.
309
NAACL HLT Demonstration Program, pages 23?24,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
A Conversational In-car Dialog System
Baoshi Yan1 Fuliang Weng1 Zhe Feng1 Florin Ratiu2 Madhuri Raya1 Yao Meng1
Sebastian Varges2 Matthew Purver2 Annie Lien1 Tobias Scheideck1 Badri Raghunathan1
Feng Lin1 Rohit Mishra4 Brian Lathrop4 Zhaoxia Zhang4 Harry Bratt3 Stanley Peters2
Research and Technology Center, Robert Bosch LLC, Palo Alto, California1
Center for the Study of Language and Information, Stanford University, Stanford, California2
Speech Technology and Research Lab, SRI International, Menlo Park, California3
Electronics Research Lab, Volkswagen of America, Palo Alto, California4
Abstract
In this demonstration we present a con-
versational dialog system for automobile
drivers. The system provides a voice-
based interface to playing music, finding
restaurants, and navigating while driving.
The design of the system as well as the
new technologies developed will be pre-
sented. Our evaluation showed that the
system is promising, achieving high task
completion rate and good user satisfation.
1 Introduction
As a constant stream of electronic gadgets such as
navigation systems and digital music players en-
ters cars, it threatens driving safety by increasing
driver distraction. According to a 2005 report by
the National Highway Traffic Safety Administration
(NHTSA) (NHTSA, 2005), driver distraction and
inattention from all sources contributed to 20-25%
of police reported crashes. It is therefore impor-
tant to design user interfaces to devices that mini-
mize driver distraction, to which voice-based inter-
faces have been a promising approach as they keep
a driver?s hands on the wheel and eyes on the road.
In this demonstration we present a conversational
dialog system, CHAT, that supports music selection,
restaurant selection, and driving navigation (Weng
et al, 2006). The system is a joint research effort
from Bosch RTC, VWERL, Stanford CSLI, and SRI
STAR Lab funded by NIST ATP. It has reached a
promising level, achieving a task completion rate of
98%, 94%, 97% on playing music, finding restau-
rants, and driving navigation respectively.
Specifically, we plan to present a number of fea-
tures in the CHAT system, including end-pointing
with prosodic cues, robust natural language under-
standing, error identification and recovery strate-
gies, content optimization, full-fledged reponse gen-
eration, flexible multi-threaded, multi-device dialog
management, and support for random events, dy-
namic information, and domain switching.
2 System Descriptions
The spoken dialog system consists of a number of
components (see the figure on the next page). In-
stead of the hub architecture employed by Commu-
nicator projects (Seneff et al, 1998), it is devel-
oped in Java and uses flexible event-based, message-
oriented middleware. This allows for dynamic regis-
tration of new components. Among the component
modules in the figure, we use the Nuance speech
recognition engine with class-based n-grams and
dynamic grammars, and the Nuance Vocalizer as the
TTS engine. The Speech Enhancer removes noises
and echo. The Prosody module will provide addi-
tional features to the Natural Language Understand-
ing (NLU) and Dialog Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation. Par-
allel to the deep analysis, a topic classifier assigns
n-best topics to the utterance, which are used in the
cases where the dialog manager cannot make any
sense of the parsed structure. The NLU module also
supports dynamic updates of the knowledge base.
The DM module mediates and manages interac-
23
tion. It uses an information-state-update approach to
maintain dialog context, which is then used to inter-
pret incoming utterances (including fragments and
revisions), resolve NPs, construct salient responses,
track issues, etc. Dialog states can also be used to
bias SR expectation and improve SR performance,
as has been performed in previous applications of
the DM. Detailed descriptions of the DM can be
found in (Lemon et al, 2002) (Mirkovic and Cave-
don, 2005).
The Knowledge Manager (KM) controls access
to knowledge base sources (such as domain knowl-
edge and device information) and their updates. Do-
main knowledge is structured according to domain-
dependent ontologies. The current KMmakes use of
OWL, a W3C standard, to represent the ontological
relationships between domain entities.
The Content Optimization module acts as an in-
termediary between the dialog management module
and the knowledge management module and con-
trols the amount of content and provides recommen-
dations to user. It receives queries in the form of se-
mantic frames from the DM, resolves possible ambi-
guities, and queries the KM. Depending on the items
in the query result as well as configurable properties,
the module selects and performs an appropriate op-
timization strategy (Pon-Barry et al, 2006).
The Response Generation module takes query re-
sults from the KM or Content Optimizer and gener-
ates natural language sentences as system responses
to user utterances. The query results are converted
into natural language sentences via a bottom-up ap-
proach using a production system. An alignment-
based ranking algorithm is used to select the best
generated sentence.
The system supports random events and dy-
namic external information, for example, the system
prompts users for the next turn when they drive close
to an intersection and dialogs can be carried out in
terms of the current dynamic situation. The user can
also switch among the three different applications
easily by explicitly instructing the system which do-
main to operate in.
3 Acknowledgement
This work is partially supported by the NIST Ad-
vanced Technology Program.
References
Oliver Lemon, Alex Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in
dialogue systems. In Traitement Automatique des
Langues (TAL), page 43(2).
Danilo Mirkovic and Lawrence Cavedon. 2005. Prac-
tical Plug-and-Play Dialogue Management. In Pro-
ceedings of the 6th Meeting of the Pacific Associa-
tion for Computational Linguistics (PACLING), page
43(2), Tokyo, Japan.
National Highway Traffic Safety Administration
NHTSA. 2005. NHTSA Vehicle Safety Rulemaking
and Supporting Research Priorities: Calendar Years
2005-2009. January.
Heather Pon-Barry, Fuliang Weng, and Sebastian Varges.
2006. Evaluation of content presentation strategies
for an in-car spoken dialogue system. In Proceedings
of the 9th International Conference on Spoken Lan-
guage Processing (Interspeech/ICSLP), pages 1930?
1933, Pittsburgh, PA, September.
Stephanie Seneff, Ed Hurley, Raymond Lau, Chris-
tine Pao, Philipp Schmid, and Victor Zue. 1998.
GALAXY-II: A Reference Architecture for Conversa-
tional System Development. In International Confer-
ence on Spoken Language Processing (ICSLP), page
43(2), Sydney, Australia, December.
Fuliang Weng, Sebastian Varges, Badri Raghunathan,
Florin Ratiu, Heather Pon-Barry, Brian Lathrop,
Qi Zhang, Tobias Scheideck, Harry Bratt, Kui Xu,
Matthew Purver, Rohit Mishra, Annie Lien, Mad-
huri Raya, Stanley Peters, Yao Meng, Jeff Russel,
Lawrence Cavedon, Liz Shriberg, and Hauke Schmidt.
2006. CHAT: A conversational helper for automo-
tive tasks. In Proceedings of the 9th International
Conference on Spoken Language Processing (Inter-
speech/ICSLP), pages 1061?1064, Pittsburgh, PA,
September.
24
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Topic Modelling for Multi-Party Spoken Discourse
Matthew Purver
CSLI
Stanford University
Stanford, CA 94305, USA
mpurver@stanford.edu
Konrad P. Ko?rding
Dept. of Brain & Cognitive Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
kording@mit.edu
Thomas L. Griffiths
Dept. of Cognitive & Linguistic Sciences
Brown University
Providence, RI 02912, USA
tom griffiths@brown.edu
Joshua B. Tenenbaum
Dept. of Brain & Cognitive Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
jbt@mit.edu
Abstract
We present a method for unsupervised
topic modelling which adapts methods
used in document classification (Blei et
al., 2003; Griffiths and Steyvers, 2004) to
unsegmented multi-party discourse tran-
scripts. We show how Bayesian infer-
ence in this generative model can be
used to simultaneously address the prob-
lems of topic segmentation and topic
identification: automatically segmenting
multi-party meetings into topically co-
herent segments with performance which
compares well with previous unsuper-
vised segmentation-only methods (Galley
et al, 2003) while simultaneously extract-
ing topics which rate highly when assessed
for coherence by human judges. We also
show that this method appears robust in
the face of off-topic dialogue and speech
recognition errors.
1 Introduction
Topic segmentation ? division of a text or dis-
course into topically coherent segments ? and
topic identification ? classification of those seg-
ments by subject matter ? are joint problems. Both
are necessary steps in automatic indexing, retrieval
and summarization from large datasets, whether
spoken or written. Both have received significant
attention in the past (see Section 2), but most ap-
proaches have been targeted at either text or mono-
logue, and most address only one of the two issues
(usually for the very good reason that the dataset
itself provides the other, for example by the ex-
plicit separation of individual documents or news
stories in a collection). Spoken multi-party meet-
ings pose a difficult problem: firstly, neither the
segmentation nor the discussed topics can be taken
as given; secondly, the discourse is by nature less
tidily structured and less restricted in domain; and
thirdly, speech recognition results have unavoid-
ably high levels of error due to the noisy multi-
speaker environment.
In this paper we present a method for unsuper-
vised topic modelling which allows us to approach
both problems simultaneously, inferring a set of
topics while providing a segmentation into topi-
cally coherent segments. We show that this model
can address these problems over multi-party dis-
course transcripts, providing good segmentation
performance on a corpus of meetings (compara-
ble to the best previous unsupervised method that
we are aware of (Galley et al, 2003)), while also
inferring a set of topics rated as semantically co-
herent by human judges. We then show that its
segmentation performance appears relatively ro-
bust to speech recognition errors, giving us con-
fidence that it can be successfully applied in a real
speech-processing system.
The plan of the paper is as follows. Section 2
below briefly discusses previous approaches to the
identification and segmentation problems. Sec-
tion 3 then describes the model we use here. Sec-
tion 4 then details our experiments and results, and
conclusions are drawn in Section 5.
2 Background and Related Work
In this paper we are interested in spoken discourse,
and in particular multi-party human-human meet-
ings. Our overall aim is to produce information
which can be used to summarize, browse and/or
retrieve the information contained in meetings.
User studies (Lisowska et al, 2004; Banerjee et
al., 2005) have shown that topic information is im-
portant here: people are likely to want to know
17
which topics were discussed in a particular meet-
ing, as well as have access to the discussion on
particular topics in which they are interested. Of
course, this requires both identification of the top-
ics discussed, and segmentation into the periods of
topically related discussion.
Work on automatic topic segmentation of text
and monologue has been prolific, with a variety of
approaches used. (Hearst, 1994) uses a measure of
lexical cohesion between adjoining paragraphs in
text; (Reynar, 1999) and (Beeferman et al, 1999)
combine a variety of features such as statistical
language modelling, cue phrases, discourse infor-
mation and the presence of pronouns or named
entities to segment broadcast news; (Maskey and
Hirschberg, 2003) use entirely non-lexical fea-
tures. Recent advances have used generative mod-
els, allowing lexical models of the topics them-
selves to be built while segmenting (Imai et al,
1997; Barzilay and Lee, 2004), and we take a sim-
ilar approach here, although with some important
differences detailed below.
Turning to multi-party discourse and meetings,
however, most previous work on automatic seg-
mentation (Reiter and Rigoll, 2004; Dielmann
and Renals, 2004; Banerjee and Rudnicky, 2004),
treats segments as representing meeting phases or
events which characterize the type or style of dis-
course taking place (presentation, briefing, discus-
sion etc.), rather than the topic or subject matter.
While we expect some correlation between these
two types of segmentation, they are clearly differ-
ent problems. However, one comparable study is
described in (Galley et al, 2003). Here, a lex-
ical cohesion approach was used to develop an
essentially unsupervised segmentation tool (LC-
Seg) which was applied to both text and meet-
ing transcripts, giving performance better than that
achieved by applying text/monologue-based tech-
niques (see Section 4 below), and we take this
as our benchmark for the segmentation problem.
Note that they improved their accuracy by com-
bining the unsupervised output with discourse fea-
tures in a supervised classifier ? while we do not
attempt a similar comparison here, we expect a
similar technique would yield similar segmenta-
tion improvements.
In contrast, we take a generative approach,
modelling the text as being generated by a se-
quence of mixtures of underlying topics. The ap-
proach is unsupervised, allowing both segmenta-
tion and topic extraction from unlabelled data.
3 Learning topics and segments
We specify our model to address the problem of
topic segmentation: attempting to break the dis-
course into discrete segments in which a particu-
lar set of topics are discussed. Assume we have a
corpus of U utterances, ordered in sequence. The
uth utterance consists of Nu words, chosen from
a vocabulary of size W . The set of words asso-
ciated with the uth utterance are denoted wu, and
indexed as wu,i. The entire corpus is represented
by w.
Following previous work on probabilistic topic
models (Hofmann, 1999; Blei et al, 2003; Grif-
fiths and Steyvers, 2004), we model each utterance
as being generated from a particular distribution
over topics, where each topic is a probability dis-
tribution over words. The utterances are ordered
sequentially, and we assume aMarkov structure on
the distribution over topics: with high probability,
the distribution for utterance u is the same as for
utterance u?1; otherwise, we sample a new distri-
bution over topics. This pattern of dependency is
produced by associating a binary switching vari-
able with each utterance, indicating whether its
topic is the same as that of the previous utterance.
The joint states of all the switching variables de-
fine segments that should be semantically coher-
ent, because their words are generated by the same
topic vector. We will first describe this generative
model in more detail, and then discuss inference
in this model.
3.1 A hierarchical Bayesian model
We are interested in where changes occur in the
set of topics discussed in these utterances. To this
end, let cu indicate whether a change in the distri-
bution over topics occurs at the uth utterance and
let P (cu = 1) = pi (where pi thus defines the ex-
pected number of segments). The distribution over
topics associated with the uth utterance will be de-
noted ?(u), and is a multinomial distribution over
T topics, with the probability of topic t being ?(u)t .
If cu = 0, then ?(u) = ?(u?1). Otherwise, ?(u)
is drawn from a symmetric Dirichlet distribution
with parameter ?. The distribution is thus:
P (?(u)|cu, ?
(u?1)) =
(
?(?(u), ?(u?1)) cu = 0
?(T?)
?(?)T
QT
t=1(?
(u)
t )
??1 cu = 1
18
Figure 1: Graphical models indicating the dependencies among variables in (a) the topic segmentation
model and (b) the hidden Markov model used as a comparison.
where ?(?, ?) is the Dirac delta function, and ?(?)
is the generalized factorial function. This dis-
tribution is not well-defined when u = 1, so
we set c1 = 1 and draw ?(1) from a symmetric
Dirichlet(?) distribution accordingly.
As in (Hofmann, 1999; Blei et al, 2003; Grif-
fiths and Steyvers, 2004), each topic Tj is a multi-
nomial distribution ?(j) over words, and the prob-
ability of the word w under that topic is ?(j)w . The
uth utterance is generated by sampling a topic as-
signment zu,i for each word i in that utterance with
P (zu,i = t|?(u)) = ?
(u)
t , and then sampling a
word wu,i from ?(j), with P (wu,i = w|zu,i =
j, ?(j)) = ?(j)w . If we assume that pi is generated
from a symmetric Beta(?) distribution, and each
?(j) is generated from a symmetric Dirichlet(?)
distribution, we obtain a joint distribution over all
of these variables with the dependency structure
shown in Figure 1A.
3.2 Inference
Assessing the posterior probability distribution
over topic changes c given a corpus w can be sim-
plified by integrating out the parameters ?, ?, and
pi. According to Bayes rule we have:
P (z, c|w) =
P (w|z)P (z|c)P (c)
P
z,c P (w|z)P (z|c)P (c)
(1)
Evaluating P (c) requires integrating over pi.
Specifically, we have:
P (c) =
R 1
0 P (c|pi)P (pi) dpi
= ?(2?)?(?)2
?(n1+?)?(n0+?)
?(N+2?)
(2)
where n1 is the number of utterances for which
cu = 1, and n0 is the number of utterances for
which cu = 0. Computing P (w|z) proceeds along
similar lines:
P (w|z) =
R
?TW
P (w|z, ?)P (?) d?
=
?
?(W?)
?(?)W
?T QT
t=1
QW
w=1 ?(n
(t)
w +?)
?(n(t)? +W?)
(3)
where ?TW is the T -dimensional cross-product of
the multinomial simplex on W points, n(t)w is the
number of times word w is assigned to topic t in
z, and n(t)? is the total number of words assigned
to topic t in z. To evaluate P (z|c) we have:
P (z|c) =
Z
?UT
P (z|?)P (?|c) d? (4)
The fact that the cu variables effectively divide
the sequence of utterances into segments that use
the same distribution over topics simplifies solving
the integral and we obtain:
P (z|c) =
?
?(T?)
?(?)T
?n1 Y
u?U1
QT
t=1 ?(n
(Su)
t + ?)
?(n(Su)? + T?)
. (5)
19
P (cu|c?u, z,w) ?
8
>
><
>
>:
QT
t=1 ?(n
(S0u)
t +?)
?(n
(S0u)
? +T?)
n0+?
N+2? cu = 0
?(T?)
?(?)T
QT
t=1 ?(n
(S1u?1)
t +?)
?(n
(S1u?1)
? +T?)
QT
t=1 ?(n
(S1u)
t +?)
?(n
(S1u)
? +T?)
n1+?
N+2? cu = 1
(7)
where U1 = {u|cu = 1}, U0 = {u|cu = 0}, Su
denotes the set of utterances that share the same
topic distribution (i.e. belong to the same segment)
as u, and n(Su)t is the number of times topic t ap-
pears in the segment Su (i.e. in the values of zu?
corresponding for u? ? Su).
Equations 2, 3, and 5 allow us to evaluate the
numerator of the expression in Equation 1. How-
ever, computing the denominator is intractable.
Consequently, we sample from the posterior dis-
tribution P (z, c|w) using Markov chain Monte
Carlo (MCMC) (Gilks et al, 1996). We use Gibbs
sampling, drawing the topic assignment for each
word, zu,i, conditioned on all other topic assign-
ments, z?(u,i), all topic change indicators, c, and
all words, w; and then drawing the topic change
indicator for each utterance, cu, conditioned on all
other topic change indicators, c?u, all topic as-
signments z, and all words w.
The conditional probabilities we need can be
derived directly from Equations 2, 3, and 5. The
conditional probability of zu,i indicates the prob-
ability that wu,i should be assigned to a particu-
lar topic, given other assignments, the current seg-
mentation, and the words in the utterances. Can-
celling constant terms, we obtain:
P (zu,i|z?(u,i), c,w) =
n(t)wu,i + ?
n(t)? + W?
n(Su)zu,i + ?
n(Su)? + T?
. (6)
where all counts (i.e. the n terms) exclude zu,i.
The conditional probability of cu indicates the
probability that a new segment should start at u.
In sampling cu from this distribution, we are split-
ting or merging segments. Similarly we obtain the
expression in (7), where S1u is Su for the segmen-
tation when cu = 1, S0u is Su for the segmentation
when cu = 0, and all counts (e.g. n1) exclude cu.
For this paper, we fixed ?, ? and ? at 0.01.
Our algorithm is related to (Barzilay and Lee,
2004)?s approach to text segmentation, which uses
a hiddenMarkov model (HMM) to model segmen-
tation and topic inference for text using a bigram
representation in restricted domains. Due to the
adaptive combination of different topics our algo-
rithm can be expected to generalize well to larger
domains. It also relates to earlier work by (Blei
and Moreno, 2001) that uses a topic representation
but also does not allow adaptively combining dif-
ferent topics. However, while HMM approaches
allow a segmentation of the data by topic, they
do not allow adaptively combining different topics
into segments: while a new segment can be mod-
elled as being identical to a topic that has already
been observed, it can not be modelled as a com-
bination of the previously observed topics.1 Note
that while (Imai et al, 1997)?s HMM approach al-
lows topic mixtures, it requires supervision with
hand-labelled topics.
In our experiments we therefore compared our
results with those obtained by a similar but simpler
10 state HMM, using a similar Gibbs sampling al-
gorithm. The key difference between the twomod-
els is shown in Figure 1. In the HMM, all variation
in the content of utterances is modelled at a single
level, with each segment having a distribution over
words corresponding to a single state. The hierar-
chical structure of our topic segmentation model
allows variation in content to be expressed at two
levels, with each segment being produced from a
linear combination of the distributions associated
with each topic. Consequently, our model can of-
ten capture the content of a sequence of words by
postulating a single segment with a novel distribu-
tion over topics, while the HMM has to frequently
switch between states.
4 Experiments
4.1 Experiment 0: Simulated data
To analyze the properties of this algorithm we first
applied it to a simulated dataset: a sequence of
10,000 words chosen from a vocabulary of 25.
Each segment of 100 successive words had a con-
1Say that a particular corpus leads us to infer topics corre-
sponding to ?speech recognition? and ?discourse understand-
ing?. A single discussion concerning speech recognition for
discourse understanding could be modelled by our algorithm
as a single segment with a suitable weighted mixture of the
two topics; a HMM approach would tend to split it into mul-
tiple segments (or require a specific topic for this segment).
20
Figure 2: Simulated data: A) inferred topics; B)
segmentation probabilities; C) HMM version.
stant topic distribution (with distributions for dif-
ferent segments drawn from a Dirichlet distribu-
tion with ? = 0.1), and each subsequence of 10
words was taken to be one utterance. The topic-
word assignments were chosen such that when the
vocabulary is aligned in a 5?5 grid the topics were
binary bars. The inference algorithm was then run
for 200,000 iterations, with samples collected after
every 1,000 iterations to minimize autocorrelation.
Figure 2 shows the inferred topic-word distribu-
tions and segment boundaries, which correspond
well with those used to generate the data.
4.2 Experiment 1: The ICSI corpus
We applied the algorithm to the ICSI meeting
corpus transcripts (Janin et al, 2003), consist-
ing of manual transcriptions of 75 meetings. For
evaluation, we use (Galley et al, 2003)?s set of
human-annotated segmentations, which covers a
sub-portion of 25 meetings and takes a relatively
coarse-grained approach to topic with an average
of 5-6 topic segments per meeting. Note that
these segmentations were not used in training the
model: topic inference and segmentation was un-
supervised, with the human annotations used only
to provide some knowledge of the overall segmen-
tation density and to evaluate performance.
The transcripts from all 75 meetings were lin-
earized by utterance start time and merged into a
single dataset that contained 607,263 word tokens.
We sampled for 200,000 iterations of MCMC, tak-
ing samples every 1,000 iterations, and then aver-
aged the sampled cu variables over the last 100
samples to derive an estimate for the posterior
probability of a segmentation boundary at each ut-
terance start. This probability was then thresh-
olded to derive a final segmentation which was
compared to the manual annotations. More pre-
cisely, we apply a small amount of smoothing
(Gaussian kernel convolution) and take the mid-
points of any areas above a set threshold to be the
segment boundaries. Varying this threshold allows
us to segment the discourse in a more or less fine-
grained way (and we anticipate that this could be
user-settable in a meeting browsing application).
If the correct number of segments is known for
a meeting, this can be used directly to determine
the optimum threshold, increasing performance; if
not, we must set it at a level which corresponds to
the desired general level of granularity. For each
set of annotations, we therefore performed two
sets of segmentations: one in which the threshold
was set for each meeting to give the known gold-
standard number of segments, and one in which
the threshold was set on a separate development
set to give the overall corpus-wide average number
of segments, and held constant for all test meet-
ings.2 This also allows us to compare our results
with those of (Galley et al, 2003), who apply a
similar threshold to their lexical cohesion func-
tion and give corresponding results produced with
known/unknown numbers of segments.
Segmentation We assessed segmentation per-
formance using the Pk and WindowDiff (WD) er-
ror measures proposed by (Beeferman et al, 1999)
and (Pevzner and Hearst, 2002) respectively; both
intuitively provide a measure of the probability
that two points drawn from the meeting will be
incorrectly separated by a hypothesized segment
boundary ? thus, lower Pk and WD figures indi-
cate better agreement with the human-annotated
results.3 For the numbers of segments we are deal-
ing with, a baseline of segmenting the discourse
into equal-length segments gives both Pk and WD
about 50%. In order to investigate the effect of the
number of underlying topics T , we tested mod-
els using 2, 5, 10 and 20 topics. We then com-
pared performance with (Galley et al, 2003)?s LC-
Seg tool, and with a 10-state HMM model as de-
scribed above. Results are shown in Table 1, aver-
aged over the 25 test meetings.
Results show that our model significantly out-
performs the HMM equivalent ? because the
HMM cannot combine different topics, it places
a lot of segmentation boundaries, resulting in in-
ferior performance. Using stemming and a bigram
2The development set was formed from the other meet-
ings in the same ICSI subject areas as the annotated test meet-
ings.
3WD takes into account the likely number of incorrectly
separating hypothesized boundaries; Pk only a binary cor-
rect/incorrect classification.
21
Figure 3: Results from the ICSI corpus: A) the words most indicative for each topic; B) Probability of a
segment boundary, compared with human segmentation, for an arbitrary subset of the data; C) Receiver-
operator characteristic (ROC) curves for predicting human segmentation, and conditional probabilities
of placing a boundary at an offset from a human boundary; D) subjective topic coherence ratings.
Number of topics T
Model 2 5 10 20 HMM LCSeg
Pk .284 .297 .329 .290 .375 .319
known unknown
Model Pk WD Pk WD
T = 10 .289 .329 .329 .353
LCSeg .264 .294 .319 .359
Table 1: Results on the ICSI meeting corpus.
representation, however, might improve its perfor-
mance (Barzilay and Lee, 2004), although simi-
lar benefits might equally apply to our model. It
also performs comparably to (Galley et al, 2003)?s
unsupervised performance (exceeding it for some
settings of T ). It does not perform as well as their
hybrid supervised system, which combined LC-
Seg with supervised learning over discourse fea-
tures (Pk = .23); but we expect that a similar ap-
proach would be possible here, combining our seg-
mentation probabilities with other discourse-based
features in a supervised way for improved per-
formance. Interestingly, segmentation quality, at
least at this relatively coarse-grained level, seems
hardly affected by the overall number of topics T .
Figure 3B shows an example for one meeting of
how the inferred topic segmentation probabilities
at each utterance compare with the gold-standard
segment boundaries. Figure 3C illustrates the per-
formance difference between our model and the
HMM equivalent at an example segment bound-
ary: for this example, the HMM model gives al-
most no discrimination.
Identification Figure 3A shows the most indica-
tive words for a subset of the topics inferred at the
last iteration. Encouragingly, most topics seem
intuitively to reflect the subjects we know were
discussed in the ICSI meetings ? the majority of
them (67 meetings) are taken from the weekly
meetings of 3 distinct research groups, where dis-
cussions centered around speech recognition tech-
niques (topics 2, 5), meeting recording, annotation
and hardware setup (topics 6, 3, 1, 8), robust lan-
guage processing (topic 7). Others reflect general
classes of words which are independent of subject
matter (topic 4).
To compare the quality of these inferred topics
we performed an experiment in which 7 human
observers rated (on a scale of 1 to 9) the seman-
tic coherence of 50 lists of 10 words each. Of
these lists, 40 contained the most indicative words
for each of the 10 topics from different models:
the topic segmentation model; a topic model that
had the same number of segments but with fixed
evenly spread segmentation boundaries; an equiv-
22
alent with randomly placed segmentation bound-
aries; and the HMM. The other 10 lists contained
random samples of 10 words from the other 40
lists. Results are shown in Figure 3D, with the
topic segmentation model producing the most co-
herent topics and the HMM model and random
words scoring less well. Interestingly, using an
even distribution of boundaries but allowing the
topic model to infer topics performs similarly well
with even segmentation, but badly with random
segmentation ? topic quality is thus not very sus-
ceptible to the precise segmentation of the text,
but does require some reasonable approximation
(on ICSI data, an even segmentation gives a Pk of
about 50%, while random segmentations can do
much worse). However, note that the full topic
segmentation model is able to identify meaningful
segmentation boundaries at the same time as infer-
ring topics.
4.3 Experiment 2: Dialogue robustness
Meetings often include off-topic dialogue, in par-
ticular at the beginning and end, where infor-
mal chat and meta-dialogue are common. Gal-
ley et al (2003) annotated these sections explic-
itly, together with the ICSI ?digit-task? sections
(participants read sequences of digits to provide
data for speech recognition experiments), and re-
moved them from their data, as did we in Ex-
periment 1 above. While this seems reasonable
for the purposes of investigating ideal algorithm
performance, in real situations we will be faced
with such off-topic dialogue, and would obviously
prefer segmentation performance not to be badly
affected (and ideally, enabling segmentation of
the off-topic sections from the meeting proper).
One might suspect that an unsupervised genera-
tive model such as ours might not be robust in the
presence of numerous off-topic words, as spuri-
ous topics might be inferred and used in the mix-
ture model throughout. In order to investigate this,
we therefore also tested on the full dataset with-
out removing these sections (806,026 word tokens
in total), and added the section boundaries as fur-
ther desired gold-standard segmentation bound-
aries. Table 2 shows the results: performance is
not significantly affected, and again is very simi-
lar for both our model and LCSeg.
4.4 Experiment 3: Speech recognition
The experiments so far have all used manual word
transcriptions. Of course, in real meeting pro-
known unknown
Experiment Model Pk WD Pk WD
2 T = 10 .296 .342 .325 .366
(off-topic data) LCSeg .307 .338 .322 .386
3 T = 10 .266 .306 .291 .331
(ASR data) LCSeg .289 .339 .378 .472
Table 2: Results for Experiments 2 & 3: robust-
ness to off-topic and ASR data.
cessing systems, we will have to deal with speech
recognition (ASR) errors. We therefore also tested
on 1-best ASR output provided by ICSI, and re-
sults are shown in Table 2. The ?off-topic? and
?digits? sections were removed in this test, so re-
sults are comparable with Experiment 1. Segmen-
tation accuracy seems extremely robust; interest-
ingly, LCSeg?s results are less robust (the drop in
performance is higher), especially when the num-
ber of segments in a meeting is unknown.
It is surprising to notice that the segmentation
accuracy in this experiment was actually slightly
higher than achieved in Experiment 1 (especially
given that ASR word error rates were generally
above 20%). This may simply be a smoothing ef-
fect: differences in vocabulary and its distribution
can effectively change the prior towards sparsity
instantiated in the Dirichlet distributions.
5 Summary and Future Work
We have presented an unsupervised generative
model which allows topic segmentation and iden-
tification from unlabelled data. Performance on
the ICSI corpus of multi-party meetings is compa-
rable with the previous unsupervised segmentation
results, and the extracted topics are rated well by
human judges. Segmentation accuracy is robust
in the face of noise, both in the form of off-topic
discussion and speech recognition hypotheses.
Future Work Spoken discourse exhibits several
features not derived from the words themselves
but which seem intuitively useful for segmenta-
tion, e.g. speaker changes, speaker identities and
roles, silences, overlaps, prosody and so on. As
shown by (Galley et al, 2003), some of these fea-
tures can be combined with lexical information to
improve segmentation performance (although in a
supervised manner), and (Maskey and Hirschberg,
2003) show some success in broadcast news seg-
mentation using only these kinds of non-lexical
features. We are currently investigating the addi-
tion of non-lexical features as observed outputs in
23
our unsupervised generative model.
We are also investigating improvements into the
lexical model as presented here, firstly via simple
techniques such as word stemming and replace-
ment of named entities by generic class tokens
(Barzilay and Lee, 2004); but also via the use of
multiple ASR hypotheses by incorporating word
confusion networks into our model. We expect
that this will allow improved segmentation and
identification performance with ASR data.
Acknowledgements
This work was supported by the CALO project
(DARPA grant NBCH-D-03-0010). We thank
Elizabeth Shriberg and Andreas Stolcke for pro-
viding automatic speech recognition data for the
ICSI corpus and for their helpful advice; John
Niekrasz and Alex Gruenstein for help with the
NOMOS corpus annotation tool; and Michel Gal-
ley for discussion of his approach and results.
References
Satanjeev Banerjee and Alex Rudnicky. 2004. Using
simple speech-based features to detect the state of a
meeting and the roles of the meeting participants. In
Proceedings of the 8th International Conference on
Spoken Language Processing.
Satanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.
2005. The necessity of a meeting recording and
playback system, and the benefit of topic-level anno-
tations to meeting browsing. In Proceedings of the
10th International Conference on Human-Computer
Interaction.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177?210.
David Blei and Pedro Moreno. 2001. Topic segmenta-
tion with an aspect hidden Markov model. In Pro-
ceedings of the 24th Annual International Confer-
ence on Research and Development in Information
Retrieval, pages 343?348.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Alfred Dielmann and Steve Renals. 2004. Dynamic
Bayesian Networks for meeting structuring. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 562?569.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter, edi-
tors. 1996. Markov Chain Monte Carlo in Practice.
Chapman and Hall, Suffolk.
Thomas Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Science, 101:5228?5235.
Marti A. Hearst. 1994. Multi-paragraph segmenta-
tion of expository text. In Proc. 32nd Meeting of
the Association for Computational Linguistics, Los
Cruces, NM, June.
Thomas Hofmann. 1999. Probablistic latent semantic
indexing. In Proceedings of the 22nd Annual SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 50?57.
Toru Imai, Richard Schwartz, Francis Kubala, and
Long Nguyen. 1997. Improved topic discrimination
of broadcast news using a model of multiple simul-
taneous topics. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pages 727?730.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin,
Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,
and Chuck Wooters. 2003. The ICSI Meeting Cor-
pus. In Proceedings of the IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP), pages 364?367.
Agnes Lisowska, Andrei Popescu-Belis, and Susan
Armstrong. 2004. User query analysis for the spec-
ification and evaluation of a dialogue processing and
retrieval system. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation.
Sameer R. Maskey and Julia Hirschberg. 2003. Au-
tomatic summarization of broadcast news using
structural features. In Eurospeech 2003, Geneva,
Switzerland.
Lev Pevzner and Marti Hearst. 2002. A critique and
improvement of an evaluation metric for text seg-
mentation. Computational Linguistics, 28(1):19?
36.
Stehpan Reiter and Gerhard Rigoll. 2004. Segmenta-
tion and classification of meeting events using mul-
tiple classifier fusion and dynamic programming. In
Proceedings of the International Conference on Pat-
tern Recognition.
Jeffrey Reynar. 1999. Statistical models for topic seg-
mentation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 357?364.
24
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 105?108,
Prague, June 2007. c?2007 Association for Computational Linguistics
Disambiguating Between Generic and Referential ?You? in Dialog?
Surabhi Gupta
Department of Computer Science
Stanford University
Stanford, CA 94305, US
surabhi@cs.stanford.edu
Matthew Purver
Center for the Study
of Language and Information
Stanford University
Stanford, CA 94305, US
mpurver@stanford.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305, US
jurafsky@stanford.edu
Abstract
We describe an algorithm for a novel task: disam-
biguating the pronoun you in conversation. You can
be generic or referential; finding referential you is im-
portant for tasks such as addressee identification or
extracting ?owners? of action items. Our classifier
achieves 84% accuracy in two-person conversations;
an initial study shows promising performance even on
more complex multi-party meetings.
1 Introduction and Background
This paper describes an algorithm for disambiguat-
ing the generic and referential senses of the pronoun
you.
Our overall aim is the extraction of action items
from multi-party human-human conversations, con-
crete decisions in which one (or more) individuals
take on a group commitment to perform a given task
(Purver et al, 2006). Besides identifying the task it-
self, it is crucial to determine the owner, or person
responsible. Occasionally, the name of the responsi-
ble party is mentioned explicitly. More usually, the
owner is addressed directly and therefore referred to
using a second-person pronoun, as in example (1).1
(1)
A: and um if you can get that binding point also
maybe with a nice example that would be helpful
for Johno and me.
B: Oh yeah uh O K.
It can also be important to distinguish between
singular and plural reference, as in example (2)
where the task is assigned to more than one person:
(2)
A: So y- so you guys will send to the rest of us um a
version of um, this, and - the - uh, description -
B: With sugge- yeah, suggested improvements and -
Use of ?you? might therefore help us both in de-
?This work was supported by the CALO project
(DARPA grant NBCH-D-03-0010) and ONR (MURI award
N000140510388). The authors also thank John Niekrasz for
annotating our test data.
1(1,2) are taken from the ICSI Meeting Corpus (Shriberg et
al., 2004); (3,4) from Switchboard (Godfrey et al, 1992).
tecting the fact that a task is being assigned, and in
identifying the owner. While there is an increas-
ing body of work concerning addressee identifica-
tion (Katzenmaier et al, 2004; Jovanovic et al,
2006), there is very little investigating the problem
of second-person pronoun resolution, and it is this
that we address here. Most cases of ?you? do not in
fact refer to the addressee but are generic, as in ex-
ample (3); automatic referentiality classification is
therefore very important.
(3)
B: Well, usually what you do is just wait until you
think it?s stopped,
and then you patch them up.
2 Related Work
Previous linguistic work has recognized that ?you?
is not always addressee-referring, differentiating be-
tween generic and referential uses (Holmes, 1998;
Meyers, 1990) as well as idiomatic cases of ?you
know?. For example, (Jurafsky et al, 2002) found
that ?you know? covered 47% of cases, the referen-
tial class 22%, and the generic class 27%, with no
significant differences in surface form (duration or
vowel reduction) between the different cases.
While there seems to be no previous work investi-
gating automatic classification, there is related work
on classifying ?it?, which also takes various referen-
tial and non-referential readings: (Mu?ller, 2006) use
lexical and syntactic features in a rule-based clas-
sifier to detect non-referential uses, achieving raw
accuracies around 74-80% and F-scores 63-69%.
3 Data
We used the Switchboard corpus of two-party tele-
phone conversations (Godfrey et al, 1992), and an-
notated the data with four classes: generic, referen-
tial singular, referential plural and a reported refer-
ential class, for mention in reported speech of an
105
Training Testing
Generic 360 79
Referential singular 287 92
Referential plural 17 3
Reported referential 5 1
Ambiguous 4 1
Total 673 176
Table 1: Number of cases found.
originally referential use (as the original addressee
may not be the current addressee ? see example (4)).
We allowed a separate class for genuinely ambigu-
ous cases. Switchboard explicitly tags ?you know?
when used as a discourse marker; as this (generic)
case is common and seems trivial we removed it
from our data.
(4)
B: Well, uh, I guess probably the last one I went to I
met so many people that I had not seen in proba-
bly ten, over ten years.
It was like, don?t you remember me.
And I am like no.
A: Am I related to you?
To test inter-annotator agreement, two people an-
notated 4 conversations, yielding 85 utterances con-
taining ?you?; the task was reported to be easy, and
the kappa was 100%.
We then annotated a total of 42 conversations for
training and 13 for testing. Different labelers an-
notated the training and test sets; none of the au-
thors were involved in labeling the test set. Table 1
presents information about the number of instances
of each of these classes found.
4 Features
All features used for classifier experiments were
extracted from the Switchboard LDC Treebank 3
release, which includes transcripts, part of speech
information using the Penn tagset (Marcus et al,
1994) and dialog act tags (Jurafsky et al, 1997).
Features fell into four main categories:2 senten-
tial features which capture lexical features of the
utterance itself; part-of-speech features which cap-
ture shallow syntactic patterns; dialog act features
capturing the discourse function of the current ut-
terance and surrounding context; and context fea-
tures which give oracle information (i.e., the cor-
rect generic/referential label) about preceding uses
2Currently, features are all based on perfect transcriptions.
of ?you?. We also investigated using the presence
of a question mark in the transcription as a feature,
as a possible replacement for some dialog act fea-
tures. Table 2 presents our features in detail.
N Features
Sentential Features (Sent)
2 you, you know, you guys
N number of you, your, yourself
2 you (say|said|tell|told|mention(ed)|mean(t)|sound(ed))
2 you (hear|heard)
2 (do|does|did|have|has|had|are|could|should|n?t) you
2 ?if you?
2 (which|what|where|when|how) you
Part of Speech Features (POS)
2 Comparative JJR tag
2 you (VB*)
2 (I|we) (VB*)
2 (PRP*) you
Dialog Act Features (DA)
46 DA tag of current utterance i
46 DA tag of previous utterance i ? 1
46 DA tag of utterance i ? 2
2 Presence of any question DA tag (Q DA)
2 Presence of elaboration DA tag
Oracle Context Features (Ctxt)
3 Class of utterance i ? 1
3 Class of utterance i ? 2
3 Class of previous utterance by same speaker
3 Class of previous labeled utterance
Other Features (QM)
2 Question mark
Table 2: Features investigated. N indicates the num-
ber of possible values (there are 46 DA tags; context
features can be generic, referential or N/A).
5 Experiments and Results
As Table 1 shows, there are very few occurrences
of the referential plural, reported referential and am-
biguous classes. We therefore decided to model our
problem as a two way classification task, predicting
generic versus referential (collapsing referential sin-
gular and plural as one category). Note that we ex-
pect this to be the major useful distinction for our
overall action-item detection task.
Baseline A simple baseline involves predicting the
dominant class (in the test set, referential). This
gives 54.59% accuracy (see Table 1).3
SVM Results We used LIBSVM (Chang and Lin,
2001), a support vector machine classifier trained
using an RBF kernel. Table 3 presents results for
3Precision and recall are of course 54.59% and 100%.
106
Features Accuracy F-Score
Ctxt 45.66% 0%
Baseline 54.59% 70.63%
Sent 67.05% 57.14%
Sent + Ctxt + POS 67.05% 57.14%
Sent + Ctxt + POS + QM 76.30% 72.84%
Sent + Ctxt + POS + Q DA 79.19% 77.50%
DA 80.92% 79.75%
Sent + Ctxt + POS +
QM + DA 84.39% 84.21%
Table 3: SVM results: generic versus referential
various selected sets of features. The best set of fea-
tures gave accuracy of 84.39% and f-score 84.21%.
Discussion Overall performance is respectable;
precision was consistently high (94% for the
highest-accuracy result). Perhaps surprisingly, none
of the context or part-of-speech features were found
to be useful; however, dialog act features proved
very useful ? using these features alone give us
an accuracy of 80.92% ? with the referential class
strongly associated with question dialog acts.
We used manually produced dialog act tags, and
automatic labeling accuracy with this fine-grained
tagset will be low; we would therefore prefer to
use more robust features if possible. We found that
one such heuristic feature, the presence of ques-
tion mark, cannot entirely substitute: accuracy is
reduced to 76.3%. However, using only the binary
Q DA feature (which clusters together all the dif-
ferent kinds of question DAs) does better (79.19%).
Although worse than performance with a full tagset,
this gives hope that using a coarse-grained set of
tags might allow reasonable results. As (Stolcke et
al., 2000) report good accuracy (87%) for statement
vs. question classification on manual Switchboard
transcripts, such coarse-grained information might
be reliably available.
Surprisingly, using the oracle context features (the
correct classification for the previous you) alone per-
forms worse than the baseline; and adding these fea-
tures to sentential features gives no improvement.
This suggests that the generic/referential status of
each you may be independent of previous yous.
Features Accuracy F-Score
Prosodic only 46.66% 44.31%
Baseline 54.59% 70.63%
Sent + Ctxt + POS +
QM + DA + Prosodic 84.39% 84.21%
Table 4: SVM results: prosodic features
Category Referential Generic
Count 294 340
Pitch (Hz) 156.18 143.98
Intensity (dB) 60.06 59.41
Duration (msec) 139.50 136.84
Table 5: Prosodic feature analysis
6 Prosodic Features
We next checked a set of prosodic features, test-
ing the hypothesis that generics are prosodically re-
duced. Mean pitch, intensity and duration were ex-
tracted using Praat, both averaged over the entire
utterance and just for the word ?you?. Classifi-
cation results are shown in Table 4. Using only
prosodic features performs below the baseline; in-
cluding prosodic features with the best-performing
feature set from Table 3 gives identical performance
to that with lexical and contextual features alone.
To see why the prosodic features did not help, we
examined the difference between the average pitch,
intensity and duration for referential versus generic
cases (Table 5). A one-sided t-test shows no signif-
icant differences between the average intensity and
duration (confirming the results of (Jurafsky et al,
2002), who found no significant change in duration).
The difference in the average pitch was found to be
significant (p=0.2) ? but not enough for this feature
alone to cause an increase in overall accuracy.
7 Error Analysis
We performed an error analysis on our best classi-
fier output on the training set; accuracy was 94.53%,
giving a total of 36 errors.
Half of the errors (18 of 36) were ambiguous even
for humans (the authors), if looking at the sentence
alone without the neighboring context from the ac-
tual conversation ? see (5a). Treating these exam-
ples thus needs a detailed model of dialog context.
The other major class of errors requires detailed
107
knowledge about sentential semantics and/or the
world ? see e.g. (5b,c), which we can tell are ref-
erential because they predicate inter-personal com-
parison or communication.
In addition, as questions are such a useful feature
(see above), the classifier tends to label all question
cases as referential. However, generic uses do occur
within questions (5d), especially if rhetorical (5e):
(5) a. so uh and if you don?t have the money then use a
credit card
b. I?m probably older than you
c. although uh I will personally tell you I used to work
at a bank
d. Do they survive longer if you plant them in the winter
time?
e. my question I guess are they really your peers?
8 Initial Multi-Party Experiments
The experiments above used two-person dialog data:
we expect that multi-party data is more complex. We
performed an initial exploratory study, applying the
same classes and features to multi-party meetings.
Two annotators labeled one meeting from the
AMI corpus (Carletta et al, 2006), giving a total of
52 utterances containing ?you? on which to assess
agreement: kappa was 87.18% for two way clas-
sification of generic versus referential. One of the
authors then labeled a testing set of 203 utterances;
104 are generic and 99 referential, giving a baseline
accuracy of 51.23% (and F-score of 67.65%).
We performed experiments for the same task: de-
tecting generic versus referential uses. Due to the
small amount of data, we trained the classifier on the
Switchboard training set from section 3 (i.e. on two-
party rather than multi-party data). Lacking part-of-
speech or dialog act features (since the dialog act
tagset differs from the Switchboard tagset), we used
only the sentential, context and question mark fea-
tures described in Table 2.
However, the classifier still achieves an accuracy
of 73.89% and F-score of 74.15%, comparable to the
results on Switchboard without dialog act features
(accuracy 76.30%). Precision is lower, though (both
precision and recall are 73-75%).
9 Conclusions
We have presented results on two person and multi-
party data for the task of generic versus referential
?you? detection. We have seen that the problem is
a real one: in both datasets the distribution of the
classes is approximately 50/50, and baseline accu-
racy is low. Classifier accuracy on two-party data is
reasonable, and we see promising results on multi-
party data with a basic set of features. We expect the
accuracy to go up once we train and test on same-
genre data and also add features that are more spe-
cific to multi-party data.
References
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot,
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal,
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post,
D. Reidsma, and P. Wellner. 2006. The AMI meeting cor-
pus. In MLMI 2005, Revised Selected Papers.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library for
Support Vector Machines. Software available at http:
//www.csie.ntu.edu.tw/?cjlin/libsvm.
J. J. Godfrey, E. Holliman, and J. McDaniel. 1992. SWITCH-
BOARD: Telephone speech corpus for research and devel-
opment. In Proceedings of IEEE ICASSP-92.
J. Holmes. 1998. Generic pronouns in the Wellington corpus
of spoken New Zealand English. Ko?tare, 1(1).
N. Jovanovic, R. op den Akker, and A. Nijholt. 2006. Ad-
dressee identification in face-to-face meetings. In Proceed-
ings of the 11th Conference of the EACL.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL shallow-discourse-function annota-
tion coders manual, draft 13. Technical Report 97-02, Uni-
versity of Colorado, Boulder.
D. Jurafsky, A. Bell, and C. Girand. 2002. The role of the
lemma in form variation. In C. Gussenhoven and N. Warner,
editors, Papers in Laboratory Phonology VII, pages 1?34.
M. Katzenmaier, R. Stiefelhagen, and T. Schultz. 2004. Iden-
tifying the addressee in human-human-robot interactions
based on head pose and speech. In Proceedings of the 6th
International Conference on Multimodal Interfaces.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies,
M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn
treebank: Annotating predicate argument structure. In ARPA
Human Language Technology Workshop.
M. W. Meyers. 1990. Current generic pronoun usage. Ameri-
can Speech, 65(3):228?237.
C. Mu?ller. 2006. Automatic detection of nonreferential It in
spoken multi-party dialog. In Proceedings of the 11th Con-
ference of the EACL.
M. Purver, P. Ehlen, and J. Niekrasz. 2006. Detecting action
items in multi-party meetings: Annotation and initial exper-
iments. In MLMI 2006, Revised Selected Papers.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004.
The ICSI Meeting Recorder Dialog Act (MRDA) Corpus. In
Proceedings of the 5th SIGdial Workshop.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Juraf-
sky, P. Taylor, C. V. Ess-Dykema, R. Martin, and M. Meteer.
2000. Dialogue act modeling for automatic tagging and
recognition of conversational speech. Computational Lin-
guistics, 26(3):339?373.
108
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 31?34,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Shallow Discourse Structure for Action Item Detection
Matthew Purver, Patrick Ehlen, and John Niekrasz
Center for the Study of Language and Information
Stanford University
Stanford, CA 94305
{mpurver,ehlen,niekrasz}@stanford.edu
Abstract
We investigated automatic action item
detection from transcripts of multi-party
meetings. Unlike previous work (Gruen-
stein et al, 2005), we use a new hierarchi-
cal annotation scheme based on the roles
utterances play in the action item assign-
ment process, and propose an approach
to automatic detection that promises im-
proved classification accuracy while en-
abling the extraction of useful information
for summarization and reporting.
1 Introduction
Action items are specific kinds of decisions common
in multi-party meetings, characterized by the con-
crete assignment of tasks together with certain prop-
erties such as an associated timeframe and reponsi-
ble party. Our aims are firstly to automatically de-
tect the regions of discourse which establish action
items, so their surface form can be used for a tar-
geted report or summary; and secondly, to identify
the important properties of the action items (such as
the associated tasks and deadlines) that would fos-
ter concise and informative semantically-based re-
porting (for example, adding task specifications to a
user?s to-do list). We believe both of these aims are
facilitated by taking into account the roles different
utterances play in the decision-making process ? in
short, a shallow notion of discourse structure.
2 Background
Related Work Corston-Oliver et al (2004) at-
tempted to identify action items in e-mails, using
classifiers trained on annotations of individual sen-
tences within each e-mail. Sentences were anno-
tated with one of a set of ?dialogue? act classes; one
class Task included any sentence containing items
that seemed appropriate to add to an ongoing to-
do list. They report good inter-annotator agreement
over their general tagging exercise (? > 0.8), al-
though individual figures for the Task class are not
given. They then concentrated on Task sentences,
establishing a set of predictive features (in which
word n-grams emerged as ?highly predictive?) and
achieved reasonable per-sentence classification per-
formance (with f-scores around 0.6).
While there are related tags for dialogue act tag-
ging schema ? like DAMSL (Core and Allen, 1997),
which includes tags such as Action-Directive
and Commit, and the ICSI MRDA schema
(Shriberg et al, 2004) which includes a commit
tag ? these classes are too general to allow iden-
tification of action items specifically. One compa-
rable attempt in spoken discourse took a flat ap-
proach, annotating utterances as action-item-related
or not (Gruenstein et al, 2005) over the ICSI and
ISL meeting corpora (Janin et al, 2003; Burger et
al., 2002). Their inter-annotator agreement was low
(? = .36). While this may have been partly due
to their methods, it is notable that (Core and Allen,
1997) reported even lower agreement (? = .15) on
their Commit dialogue acts. Morgan et al (forth-
coming) then used these annotations to attempt auto-
31
matic classification, but achieved poor performance
(with f-scores around 0.3 at best).
Action Items Action items typically embody the
transfer of group responsibility to an individual.
This need not be the person who actually performs
the action (they might delegate the task to a subor-
dinate), but publicly commits to seeing that the ac-
tion is carried out; we call this person the owner of
the action item. Because this action is a social ac-
tion that is coordinated by more than one person,
its initiation is reinforced by agreement and uptake
among the owner and other participants that the ac-
tion should and will be done. And to distinguish
this action from immediate actions that occur during
the meeting and from more vague future actions that
are still in the planning stage, an action item will be
specified as expected to be carried out within a time-
frame that begins at some point after the meeting and
extends no further than the not-too-distant future. So
an action item, as a type of social action, often com-
prises four components: a task description, a time-
frame, an owner, and a round of agreement among
the owner and others. The related discourse tends to
reflect this, and we attempt to exploit this fact here.
3 Baseline Experiments
We applied Gruenstein et al (2005)?s flat annotation
schema to transcripts from a sequence of 5 short re-
lated meetings with 3 participants recorded as part
of the CALO project. Each meeting was simulated
in that its participants were given a scenario, but
was not scripted. In order to avoid entirely data-
or scenario-specific results (and also to provide an
acceptable amount of training data), we then added
a random selection of 6 ICSI and 1 ISL meetings
from Gruenstein et al (2005)?s annotations. Like
(Corston-Oliver et al, 2004) we used support vec-
tor machines (Vapnik, 1995) via the classifier SVM-
light (Joachims, 1999). Their full set of features are
not available to us, but we experimented with com-
binations of words and n-grams and assessed classi-
fication performance via a 5-fold validation on each
of the CALO meetings. In each case, we trained
classifiers on the other 4 meetings in the CALO se-
quence, plus the fixed ICSI/ISL training selection.
Performance (per utterance, on the binary classifica-
tion problem) is shown in Table 1; overall f-score
figures are poor even on these short meetings. These
figures were obtained using words (unigrams, after
text normalization and stemming) as features ? we
investigated other discriminative classifier methods,
and the use of 2- and 3-grams as features, but no
improvements were gained.
Mtg. Utts AI Utts. Precision Recall F-Score
1 191 22 0.31 0.50 0.38
2 156 27 0.36 0.33 0.35
3 196 18 0.28 0.55 0.37
4 212 15 0.20 0.60 0.30
5 198 9 0.19 0.67 0.30
Table 1: Baseline Classification Performance
4 Hierarchical Annotations
Two problems are apparent: firstly, accuracy is
lower than desired; secondly, identifying utterances
related to action items does not allow us to ac-
tually identify those action items and extract their
properties (deadline, owner etc.). But if the ut-
terances related to these properties form distinct
sub-classes which have their own distinct features,
treating them separately and combining the results
(along the lines of (Klein et al, 2002)) might al-
low better performance, while also identifying the
utterances where each property?s value is extracted.
Thus, we produced an annotation schema which
distinguishes among these four classes. The first
three correspond to the discussion and assignment
of the individual properties of the action item (task
description, timeframe and owner); the fi-
nal agreement class covers utterances which ex-
plicitly show that the action item is agreed upon.
Since the task description subclass ex-
tracts a description of the task, it must include any
utterances that specify the action to be performed,
including those that provide required antecedents for
anaphoric references. The owner subclass includes
any utterances that explicitly specify the responsible
party (e.g. ?I?ll take care of that?, or ?John, we?ll
leave that to you?), but not those whose function
might be taken to do so implicitly (such as agree-
ments by the responsible party). The timeframe
subclass includes any utterances that explicitly refer
to when a task may start or when it is expected to
be finished; note that this is often not specified with
32
a date or temporal expression, but rather e.g. ?by
the end of next week,? or ?before the trip to Aruba?.
Finally, the agreement subclass includes any ut-
terances in which people agree that the action should
and will be done; not only acknowledgements by the
owner themselves, but also when other people ex-
press their agreement.
A single utterance may be assigned to more than
one class: ?John, you need to do that by next
Monday? might count as owner and timeframe.
Likewise, there may be more than one utterance of
each class for a single action item: John?s response
?OK, I?ll do that? would also be classed as owner
(as well as agreement). While we do not require
all of these subclasses to be present for a set of ut-
terances to qualify as denoting an action item, we
expect any action item to include most of them.
We applied this annotation schema to the same
12 meetings. Initial reliability between two anno-
tators on the single ISL meeting (chosen as it pre-
sented a significantly more complex set of action
items than others in this set) was encouraging. The
best agreement was achieved on timeframe utter-
ances (? = .86), with owner utterances slightly
less good (between ? = .77), and agreement and
description utterances worse but still accept-
able (? = .73). Further annotation is in progress.
5 Experiments
We trained individual classifiers for each of the utter-
ance sub-classes, and cross-validated as before. For
agreement utterances, we used a naive n-gram
classifier similar to that of (Webb et al, 2005) for di-
alogue act detection, scoring utterances via a set of
most predictive n-grams of length 1?3 and making a
classification decision by comparing the maximum
score to a threshold (where the n-grams, their scores
and the threshold are automatically extracted from
the training data). For owner, timeframe and
task description utterances, we used SVMs
as before, using word unigrams as features (2- and
3-grams gave no improvement ? probably due to the
small amount of training data). Performance var-
ied greatly by sub-class (see Table 2), with some
(e.g. agreement) achieving higher accuracy than the
baseline flat classifications, but others being worse.
As there is now significantly less training data avail-
able to each sub-class than there was for all utter-
ances grouped together in the baseline experiment,
worse performance might be expected; yet some
sub-classes perform better. The worst performing
class is owner. Examination of the data shows
that owner utterances are more likely than other
classes to be assigned to more than one category;
they may therefore have more feature overlap with
other classes, leading to less accurate classification.
Use of relevant sub-strings for training (rather than
full utterances) may help; as may part-of-speech in-
formation ? while proper names may be useful fea-
tures, the name tokens themselves are sparse and
may be better substituted with a generic tag.
Class Precision Recall F-Score
description 0.23 0.41 0.29
owner 0.12 0.28 0.17
timeframe 0.19 0.38 0.26
agreement 0.48 0.44 0.40
Table 2: Sub-class Classification Performance
Even with poor performance for some of the sub-
classifiers, we should still be able to combine them
to get a benefit as long as their true positives cor-
relate better than their false positives (intuitively, if
they make mistakes in different places). So far we
have only conducted an initial naive experiment, in
which we combine the individual classifier decisions
in a weighted sum over a window (currently set to
5 utterances). If the sum over the window reaches
a given threshold, we hypothesize an action item,
and take the highest-confidence utterance given by
each sub-classifier in that window to provide the
corresponding property. As shown in Table 3, this
gives reasonable performance on most meetings, al-
though it does badly on meeting 5 (apparently be-
cause no explicit agreement takes place, while our
manual weights emphasized agreement).1 Most en-
couragingly, the correct examples provide some use-
ful ?best? sub-class utterances, from which the rele-
vant properties could be extracted.
These results can probably be significantly im-
proved: rather than sum over the binary classifica-
tion outputs of each classifier, we can use their con-
fidence scores or posterior probabilities, and learn
1Accuracy here is currently assessed only over correct de-
tection of an action item in a window, not correct assignment of
all sub-classes.
33
Mtg. AIs Correct False+ False- F-Score
1 3 2 1 1 0.67
2 4 1 0 3 0.40
3 5 2 1 3 0.50
4 4 4 0 0 1.00
5 3 0 1 3 0.00
Table 3: Combined Classification Performance
the combination weights to give a more robust ap-
proach. There is still a long way to go to evaluate
this approach over more data, including the accu-
racy and utility of the resulting sub-class utterance
hypotheses.
6 Discussion and Future Work
So accounting for the structure of action items ap-
pears essential to detecting them in spoken dis-
course. Otherwise, classification accuracy is lim-
ited. We believe that accuracy can be improved, and
the detected utterances can be used to provide the
properties of the action item itself. An interesting
question is how and whether the structure we use
here relates to discourse structure in more general
use. If a relation exists, this would shed light on the
decision-making process we are attempting to (be-
gin to) model, and might allow us to use other (more
plentiful) annotated data.
Our future efforts focus on annotating more meet-
ings to obtain large training and testing sets. We also
wish to examine performance when working from
speech recognition hypotheses (as opposed to the
human transcripts used here), and the best way to in-
corporate multiple hypotheses (either as n-best lists
or word confusion networks). We are actively inves-
tigating alternative approaches to sub-classifier com-
bination: better performance (and a more robust and
trainable overall system) might be obtained by using
a Bayesian network, or a maximum entropy classi-
fier as used by (Klein et al, 2002). Finally, we are
developing an interface to a new large-vocabulary
version of the Gemini parser (Dowding et al, 1993)
which will allow us to use semantic parse informa-
tion as features in the individual sub-class classifiers,
and also to extract entity and event representations
from the classified utterances for automatic addition
of entries to calendars and to-do lists.
References
S. Burger, V. MacLaren, and H. Yu. 2002. The ISLMeet-
ing Corpus: The impact of meeting type on speech
style. In Proceedings of the 6th International Confer-
ence on Spoken Language Processing (ICSLP 2002).
M. Core and J. Allen. 1997. Coding dialogues with
the DAMSL annotation scheme. In D. Traum, edi-
tor, AAAI Fall Symposium on Communicative Action
in Humans and Machines.
S. Corston-Oliver, E. Ringger, M. Gamon, and R. Camp-
bell. 2004. Task-focused summarization of email. In
Proceedings of the Text Summarization Branches Out
ACL Workshop.
J. Dowding, J. M. Gawron, D. Appelt, J. Bear, L. Cherny,
R. Moore, and D. Moran. 1993. Gemini: A natural
language system for spoken language understanding.
In Proc. 31st Annual Meeting of the Association for
Computational Linguistics.
A. Gruenstein, J. Niekrasz, and M. Purver. 2005. Meet-
ing structure annotation: Data and tools. In Proceed-
ings of the 6th SIGdial Workshop on Discourse and
Dialogue.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C.Wooters. 2003. The ICSI Meeting Corpus.
In Proc. IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP 2003).
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods ? Support Vector
Learning. MIT Press.
D. Klein, K. Toutanova, H. T. Ilhan, S. D. Kamvar, and
C. D.Manning. 2002. Combining heterogeneous clas-
sifiers for word-sense disambiguation. In Proceedings
of the ACL Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions.
W. Morgan, S. Gupta, and P.-C. Chang. forthcoming.
Automatically detecting action items in audio meeting
recordings. Ms., under review.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Car-
vey. 2004. The ICSI Meeting Recorder Dialog Act
Corpus. In Proceedings of the 5th SIGdial Workshop
on Discourse and Dialogue.
S. Siegel and J. N. J. Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
N. Webb, M. Hepple, and Y. Wilks. 2005. Dialogue act
classification using intra-utterance features. In Proc.
AAAI Workshop on Spoken Language Understanding.
34
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 156?163,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Modelling and Detecting Decisions in Multi-party Dialogue
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen, Matthew Purver, and Stanley Peters
Center for the Study of Language and Information
Stanford University
{raquel|frampton|ehlen|mpurver|peters}@stanford.edu
Abstract
We describe a process for automatically de-
tecting decision-making sub-dialogues in tran-
scripts of multi-party, human-human meet-
ings. Extending our previous work on ac-
tion item identification, we propose a struc-
tured approach that takes into account the dif-
ferent roles utterances play in the decision-
making process. We show that this structured
approach outperforms the accuracy achieved
by existing decision detection systems based
on flat annotations, while enabling the extrac-
tion of more fine-grained information that can
be used for summarization and reporting.
1 Introduction
In collaborative and organized work environments,
people share information and make decisions exten-
sively through multi-party conversations, usually in
the form of meetings. When audio or video record-
ings are made of these meetings, it would be valu-
able to extract important information, such as the
decisions that were made and the trains of reason-
ing that led to those decisions. Such a capability
would allow work groups to keep track of courses
of action that were shelved or rejected, and could al-
low new team members to get quickly up to speed.
Thanks to the recent availability of substantial meet-
ing corpora?such as the ISL (Burger et al, 2002),
ICSI (Janin et al, 2004), and AMI (McCowan et
al., 2005) Meeting Corpora?current research on the
structure of decision-making dialogue and its use for
automatic decision detection has helped to bring this
vision closer to reality (Verbree et al, 2006; Hsueh
and Moore, 2007b).
Our aim here is to further that research by ap-
plying a simple notion of dialogue structure to the
task of automatically detecting decisions in multi-
party dialogue. A central hypothesis underlying our
approach is that this task is best addressed by tak-
ing into account the roles that different utterances
play in the decision-making process. Our claim is
that this approach facilitates both the detection of
regions of discourse where decisions are discussed
and adopted, and also the identification of important
aspects of the decision discussions themselves, thus
opening the way to better and more concise report-
ing.
In the next section, we describe prior work on re-
lated efforts, including our own work on action item
detection (Purver et al, 2007). Sections 3 and 4 then
present our decision annotation scheme, which dis-
tinguishes several types of decision-related dialogue
acts (DAs), and the corpus used as data (in this study
a section of the AMI Meeting Corpus). Next, in Sec-
tion 5, we describe our experimental methodology,
including the basic conception of our classification
approach, the features we used in classification, and
our evaluation metrics. Section 6 then presents our
results, obtained with a hierarchical classifier that
first trains individual sub-classifiers to detect the dif-
ferent types of decision DAs, and then uses a super-
classifier to detect decision regions on the basis of
patterns of these DAs, achieving an F-score of 58%.
Finally, Section 7 presents some conclusions and di-
rections for future work.
2 Related Work
Recent years have seen an increasing interest in re-
search on decision-making dialogue. To a great
extent, this is due to the fact that decisions have
156
been shown to be a key aspect of meeting speech.
User studies (Lisowska et al, 2004; Banerjee et al,
2005) have shown that participants regard decisions
as one of the most important outputs of a meeting,
while Whittaker et al (2006) found that the develop-
ment of an automatic decision detection component
is critical to the re-use of meeting archives. Identify-
ing decision-making regions in meeting transcripts
can thus be expected to support development of a
wide range of applications, such as automatic meet-
ing assistants that process, understand, summarize
and report the output of meetings; meeting tracking
systems that assist in implementing decisions; and
group decision support systems that, for instance,
help in constructing group memory (Romano and
Nunamaker, 2001; Post et al, 2004; Voss et al,
2007).
Previously researchers have focused on the in-
teractive aspects of argumentative and decision-
making dialogue, tackling issues such as the detec-
tion of agreement and disagreement and the level
of emotional involvement of conversational partic-
ipants (Hillard et al, 2003; Wrede and Shriberg,
2003; Galley et al, 2004; Gatica-Perez et al, 2005).
From a perhaps more formal perspective, Verbree et
al. (2006) have created an argumentation scheme in-
tended to support automatic production of argument
structure diagrams from decision-oriented meeting
transcripts. Only Hsueh and Moore (2007a; 2007b),
however, have specifically investigated the auto-
matic detection of decisions.
Using the AMI Meeting Corpus, Hsueh and
Moore (2007b) attempt to identify the dialogue acts
(DAs) in a meeting transcript that are ?decision-
related?. The authors define these DAs on the ba-
sis of two kinds of manually created summaries: an
extractive summary of the whole meeting, and an
abstractive summary of the decisions made in the
meeting. Those DAs in the extractive summary that
support any of the decisions in the abstractive sum-
mary are then manually tagged as decision-related
DAs. They trained a Maximum Entropy classifier
to recognize this single DA class, using a variety of
lexical, prosodic, dialogue act and topical features.
The F-score they achieved was 0.35, which gives a
good indication of the difficulty of this task.
In our previous work (Purver et al, 2007), we at-
tempted to detect a particular kind of decision com-
mon in meetings, namely action items?public com-
mitments to perform a given task. In contrast to
the approach adopted by Hsueh and Moore (2007b),
we proposed a hierarchical approach where indi-
vidual classifiers were trained to detect distinct ac-
tion item-related DA classes (task description, time-
frame, ownership and agreement) followed by a
super-classifier trained on the hypothesized class la-
bels and confidence scores from the individual clas-
sifiers that would detect clusters of multiple classes.
We showed that this structured approach produced
better classification accuracy (around 0.39 F-score
on the task of detecting action item regions) than a
flat-classifier baseline trained on a single action item
DA class (around 0.35 F-score).
In this paper we extend this approach to the more
general task of detecting decisions, hypothesizing
that?as with action items?the dialogue acts in-
volved in decision-making dialogue form a rather
heterogeneous set, whose members co-occur in par-
ticular kinds of patterns, and that exploiting this
richer structure can facilitate their detection.
3 Decision Dialogue Acts
We are interested in identifying the main conver-
sational units in a decision-making process. We ex-
pect that identifying these units will help in detect-
ing regions of dialogue where decisions are made
(decision sub-dialogues), while also contributing to
identification and extraction of specific decision-
related bits of information.
Decision-making dialogue can be complex, often
involving detailed discussions with complicated ar-
gumentative structure (Verbree et al, 2006). Deci-
sion sub-dialogues can thus include a great deal of
information that is potentially worth extracting. For
instance, we may be interested in knowing what a
decision is about, what alternative proposals were
considered during the decision process, what argu-
ments were given for and against each of them, and
last but not least, what the final resolution was.
Extracting these and other potential decision com-
ponents is a challenging task, which we do not in-
tend to fully address in this paper. This initial study
concentrates on three main components we believe
constitute the backbone of decision sub-dialogues.
A typical decision sub-dialogue consists of three
main components that often unfold in sequence. (a)
157
key DDA class description
I issue utterances introducing the issue or topic under discussion
R resolution utterances containing the decision that is adopted
RP ? proposal ? utterances where the decision adopted is proposed
RR ? restatement ? utterances where the decision adopted is confirmed or restated
A agreement utterances explicitly signalling agreement with the decision made
Table 1: Set of decision dialogue act (DDA) classes
A topic or issue that requires some sort of conclu-
sion is initially raised. (b) One or more proposals are
considered. And (c) once some sort of agreement is
reached upon a particular resolution, a decision is
adopted.
Dialogue act taxonomies often include tags
that can be decision-related. For instance, the
DAMSL taxonomy (Core and Allen, 1997) in-
cludes the tags agreement and commit, as well
as a tag open-option for utterances that ?sug-
gest a course of action?. Similarly, the AMI
DA scheme1 incorporates tags like suggest,
elicit-offer-or-suggestion and assess.
These tags are however very general and do not cap-
ture the distinction between decisions and more gen-
eral suggestions and commitments.2 We therefore
devised a decision annotation scheme that classifies
utterances according to the role they play in the pro-
cess of formulating and agreeing on a decision. Our
scheme distinguishes among three main decision di-
alogue act (DDA) classes: issue (I), resolution (R),
and agreement (A). Class R is further subdivided into
resolution proposal (RP) and resolution restatement
(RR). A summary of the classes is given in Table 1.
Annotation of the issue class includes any utter-
ances that introduce the topic of the decision discus-
sion. For instance, in example (1) below, the utter-
ances ?Are we going to have a backup?? and ?But
would a backup really be necessary?? are tagged as
I. The classes RP and RR are used to annotate those
utterances that specify the resolution adopted?i.e.
the decision made. Annotation with the class RP
includes any utterances where the resolution is ini-
1A full description of the AMI Meeting Corpus DA scheme
is available at http://mmm.idiap.ch/private/ami/
annotation/dialogue acts manual 1.0.pdf, after
free registration.
2Although they can of course be used to aid the identification
process?see Section 5.3.
tially proposed (like the utterance ?I think maybe we
could just go for the kinetic energy. . . ?). Sometimes
decision discussions include utterances that sum up
the resolution adopted, like the utterance ?Okay,
fully kinetic energy? in (1). This kind of utterance
is tagged with the class RR. Finally, the agreement
class includes any utterances in which participants
agree with the (proposed) resolution, like the utter-
ances ?Yeah? and ?Good? as well as ?Okay? in di-
alogue (1).
(1) A: Are we going to have a backup?
Or we do just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.3
Note that an utterance can be assigned to more
than one of these classes. For instance, the utter-
ance ?Okay, fully kinetic energy? is annotated both
as RR and A. Similarly, each decision sub-dialogue
may contain more than one utterance corresponding
to each class, as we saw above for issue. While
we do not a priori require each of these classes to
be present for a set of utterances to be considered
a decision sub-dialogue, all annotated decision sub-
dialogues in our corpus include the classes I, RP and
A. The annotation process and results are described
in detail in the next section.
3This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
158
4 Data: Corpus & Annotation
In this study, we use 17 meetings from the AMI
Meeting Corpus (McCowan et al, 2005), a pub-
licly available corpus of multi-party meetings con-
taining both audio recordings and manual transcrip-
tions, as well as a wide range of annotated infor-
mation including dialogue acts and topic segmenta-
tion. Conversations are all in English, but they can
include native and non-native English speakers. All
meetings in our sub-corpus are driven by an elicita-
tion scenario, wherein four participants play the role
of project manager, marketing expert, interface de-
signer, and industrial designer in a company?s de-
sign team. The overall sub-corpus makes up a total
of 15,680 utterances/dialogue acts (approximately
920 per meeting). Each meeting lasts around 30
minutes.
Two authors annotated 9 and 10 dialogues each,
overlapping on two dialogues. Inter-annotator
agreement on these two dialogues was similar to
(Purver et al, 2007), with kappa values ranging
from 0.63 to 0.73 for the four DDA classes. The
highest agreement was obtained for class RP and the
lowest for class A.4
On average, each meeting contains around 40
DAs tagged with one or more of the DDA sub-
classes in Table 1. DDAs are thus very sparse, cor-
responding to only 4.3% of utterances. When we
look at the individual DDA sub-classes this is even
more pronounced. Utterances tagged as issue make
up less than 0.9% of utterances in a meeting, while
utterances annotated as resolution make up around
1.4%?1% corresponding to RP and less than 0.4%
to RR on average. Almost half of DDA utterances
(slightly over 2% of all utterances on average) are
tagged as belonging to class agreement.
We compared our annotations with the annota-
tions of Hsueh and Moore (2007b) for the 17 meet-
ings of our sub-corpus. The overall number of ut-
terances annotated as decision-related is similar in
the two studies: 40 vs. 30 utterances per meeting on
average, respectively. However, the overlap of the
annotations is very small leading to negative kappa
scores. As shown in Figure 1, only 12.22% of ut-
4The annotation guidelines we used are available on-
line at http://godel.stanford.edu/twiki/bin/
view/Calo/CaloDecisionDiscussionSchema

  









  !"
##On the Means for Clarication in Dialogue
Matthew Purver and Jonathan Ginzburg
Department of Computer Science
King's College London
Strand, London WC2R 2LS, UK
fmatthew.purver, jonathan.ginzburgg@kcl.ac.uk
Patrick Healey
Department of Computer Science
Queen Mary, University of London
Mile End Road, London E1 4NS, UK
ph@dcs.qmw.ac.uk
Abstract
The ability to request clarication
of utterances is a vital part of the
communicative process. In this pa-
per we discuss the range of possi-
ble forms for clarication requests,
together with the range of read-
ings they can convey. We present
the results of corpus analysis which
show a correlation between certain
forms and possible readings, to-
gether with some indication of maxi-
mum likely distance between request
and the utterance being claried.
We then explain the implications of
these results for a possible HPSG
analysis of clarication requests and
for an ongoing implementation of
a clarication-capable dialogue sys-
tem.
1
1 Introduction
Clarication requests (CRs) are common in
human conversation. They can take various
forms and can be intended by the speaker
making the request (the CR initiator) to re-
quest various types of clarication informa-
tion (i.e. they can have various readings),
but have in common the fact that they are
in a sense meta-dialogue acts { they concern
the content or form of a previous utterance
that has failed to be fully comprehended by
the initiator.
1
This research is funded by grant number
GR/R04942/01 from the Engineering and Physical
Research Council of the United Kingdom.
It is not usual for computer dialogue sys-
tems do be able to process CRs produced by
the user. One can see how important this
might be in a negotiative dialogue by consid-
ering the following imagined exchange, which
gives some possible alternative responses to a
CR initiated by the caller:
(1)
System: Would you like to travel via
Paris or Amsterdam?
Caller: Paris?
System: (a) Yes, Paris.
(b) Paris, France.
(c) Paris is the quickest
route, although Amster-
dam is the cheapest.
(d) OK. Your ticket via
Paris will be posted to you.
Goodbye.
Any of responses (a){(c), which correctly
interpret the caller's move as a CR, might be
regarded as useful to the caller: response (d),
which incorrectly interprets it as an answer
to the system's question, would not be ac-
ceptable under any circumstances. Which of
(a){(c) is preferred will depend on the reading
intended. As a rst step towards a full theory
of CR interpretation, we therefore believe it
is important to have information about which
readings are available via which forms.
Previous studies have examined some indi-
vidual CR forms and given possible analyses
for these forms. In this paper we describe an
attempt to exhaustively categorise CR forms
and readings based on corpus work, and dis-
cuss the implications of our results for further
analysis.
The analyses so far proposed require all in-
formation from a previous utterance to be re-
tained in memory (not only propositional con-
tent but syntax and phonology). The reten-
tion of such a large amount of information in-
denitely poses obvious problems for any im-
plementation with nite resources, and seems
at odds with some results from work in psy-
cholinguistics: studies such as (Sachs, 1967;
van Dijk and Kintsch, 1983) have argued that
surface information such as syntax is retained
only in the short term (see (Fletcher, 1994)
for an overview). Our corpus work has there-
fore had the additional aim of identication
of the maximum distance between a CR and
the utterance being claried (the source ut-
terance).
In this section we give a brief overview
of CR forms identied in previous work, to-
gether with the analyses proposed and the
readings that these analyses give rise to. In
sections 2 and 3 we list the possible CR forms
and readings that we have identied from
corpus analysis. In section 4 we describe
this analysis and give detailed results, includ-
ing a discussion of apparent correlations be-
tween certain forms and readings and of max-
imum observed CR-source separation (CSS)
distance. Finally, in section 5 we discuss the
implications of our ndings for an intended
dialogue system implementation.
1.1 Previous Work
(Ginzburg and Sag, 2000) (hereafter G&S)
discuss reprise interrogatives, which they fur-
ther classify into echo questions (those \re-
sulting from mishearing a previous speech
act" { see B's question in example (2)) and
reference questions (those which \ask for clar-
ication of the reference of some element in
the immediately prior utterance" { see exam-
ple (3)).
(2)
A: Did Jill phone?
B: Did Jill phone?
(3)
A: Did Jill phone?
B: Did who phone?
They argue that the content of both read-
ings \contains as a constituent the illocu-
tionary force of the (previous) utterance" be-
ing reprised. In other words, B's utterances
in the examples above both involve query-
ing some feature of A's query. They might
be paraphrased \Are you asking whether Jill
phoned?" and \For which person are you
asking whether that person phoned?", respec-
tively.
They therefore oer a syntactic and seman-
tic analysis which covers both readings: the
reprise is analysed syntactically as an in-situ
interrogative, and semantically as a question
which takes as its propositional content the
perceived content of the previous utterance
being claried. As conversational move type
(CMT) is integrated into utterance content
by their HPSG grammar (see (Ginzburg et
al., 2001b)) this straightforwardly gives rise
to a reading along the lines of \For which
X are you asking/asserting/(etc.) Y about
X?". They give a full derivation for this read-
ing based on the kos dialogue context frame-
work (Ginzburg, 1996; Bohlin (Ljunglof) et
al., 1999).
This analysis is then extended to two el-
liptical forms: reprise sluices and elliptical
literal reprises. Sluices are elliptical wh-
constructions (see (Ross, 1969)) { short wh-
questions which receive a \sentential" inter-
pretation, in this case an interpretation as a
reprise question, as shown in example (4):
(4)
A: Did Jill phone?
B: Who?
(non-elliptical equivalent:
Did who phone?)
Elliptical literal reprises are short polar
questions { bare fragments which receive an
interpretation as a polar reprise question:
(5)
A: Did Jill phone?
B: Jill?
(non-elliptical equivalent:
Did Jill phone?)
Resolution of these elliptical forms is
achieved by allowing a conversational partici-
pant to coerce a clarication question onto the
list of questions under discussion (QUD) in
the current dialogue context. This allows el-
lipsis resolution in the manner of of (Ginzburg
et al, 2001a) to give essentially the same
reading as reprise questions.
(Ginzburg and Cooper, 2001) (hereafter
G&C) give more detailed analysis for the bare
fragment form (therein described as clari-
cation ellipsis) and also give a further read-
ing for this form. They call this reading the
constituent reading to distinguish it from the
clausal reading described above. This con-
stituent reading involves querying the con-
tent of a constituent which the CR initiator
has been unable to ground in context (see
(Traum, 1994; Clark, 1996)), and is along the
lines of \What/who/(etc.) is the reference of
your utterance X?".
A possible lexical identication reading is
also discussed, but no analysis given. They
also raise the issue of whether these specic
readings really exist or could be subsumed
by a single vague reading, but give evidence
that this is not the case: they cite examples
of CR misunderstanding leading to repeated
attempts to elicit the desired claricational
information, showing that a specic reading
was intended; they also point out that some
readings involve dierent parallelism condi-
tions. As will be discussed in detail below,
the results of the work described here also in-
dicate that particular forms may be restricted
to particular sets of specic readings.
2 Clarication Forms
The following forms have been identied as
possible means for CRs. While we cannot
claim that this list is exhaustive, a markup
scheme based on these forms has been shown
to cover the CRs encountered in a corpus of
dialogue, as detailed in section 4 below. In
this section we list the forms identied, and
illustrate them with examples. All examples
have been taken from the British National
Corpus (BNC).
2.1 Non-Reprise Clarications
Unsurprisingly, speakers have recourse to a
non-reprise
2
form of clarication. In this
2
Note that a non-reprise sentence need not be non-
elliptical.
form, the nature of the information being re-
quested by the CR initiator is spelt out for the
addressee. Utterances of this type thus often
contain phrases such as \do you mean. . . ",
\did you say. . . ", as can be seen in exam-
ples (6) and (7).
(6)
3
Cassie: You did get o with him?
Catherine: Twice, but it was totally
non-existent kissing so
Cassie: What do you mean?
Catherine: I was sort of falling asleep.
(7)
4
Leon: Erm, your orgy is a food
orgy.
Unknown: What did you say?
Leon: Your type of orgy is a food
orgy.
2.2 Reprise Sentences
Speakers can form a CR by echoing or repeat-
ing
5
a previous utterance in full, as shown in
example (8). This form corresponds to G&S's
reprise interrogative.
(8)
6
Orgady: I spoke to him on Wednes-
day, I phoned him.
Obina: You phoned him?
Orgady: Phoned him.
This form appears to be divisible into
two sub-categories, literal (as in example (8)
above) and wh-substituted reprise sentences,
as illustrated by example (9).
(9)
7
Unknown: He's anal retentive, that's
what it is.
Kath: He's what?
Unknown: Anal retentive.
2.3 Reprise Sluices
This form is an elliptical wh-construction as
already discussed above and described by
3
BNC le KP4, sentences 521{524
4
BNC le KPL, sentences 524{526
5
Repeats need not be verbatim, due to the possi-
ble presence of phenomena such as anaphora and VP
ellipsis, as well as changes in indexicals as shown in
example (8).
6
BNC le KPW, sentences 463{465
7
BNC le KPH, sentences 412{414
G&S.
(10)
8
Sarah: Leon, Leon, sorry she's
taken.
Leon: Who?
Sarah: Cath Long, she's spoken
for.
There may be a continuum of forms be-
tween wh-substituted reprise sentences and
reprise sluices. Consider the following ex-
change (11):
(11)
9
Richard: I'm opening my own busi-
ness so I need a lot of
money
Anon 5: Opening what?
This form seems to fall between the full wh-
substituted reprise sentence \You're opening
(your own) what?" and the simple reprise
sluice \(Your own) what?". The actual form
employed in this case appears closer to the
sluice and was classied as such.
10
2.4 Reprise Fragments
This elliptical bare fragment form corre-
sponds to that described as elliptical literal
reprise by G&S and clarication ellipsis by
G&C.
(12)
11
Lara: There's only two people in
the class.
Matthew: Two people?
Unknown: For cookery, yeah.
A similar form was also identied in which
the bare fragment is preceded by a wh-
question word:
(13)
12
Ben: No, ever, everything we say
she laughs at.
Frances: Who Emma?
Ben: Oh yeah.
8
BNC le KPL, sentences 347{349
9
BNC le KSV, sentences 363{364
10
While the current exercise has not highlighted it
as an issue, we note that a similar continuum might be
present between literal reprises and reprise fragments.
One approach in the face of this indeterminacy might
be to conate these forms { further analysis of the
results given in this paper may indicate whether this
is desirable.
11
BNC le KPP, sentences 352{354
12
BNC le KSW, sentences 698{700
As these examples appeared to be inter-
changeable with the plain fragment alterna-
tive (in example (13), \Emma?"), they were
not distinguished from fragments in our clas-
sication scheme.
2.5 Gaps
The gap form diers from the reprise forms
described above in that it does not involve a
reprise component corresponding to the com-
ponent being claried. Instead, it consists of
a reprise of (a part of) the utterance imme-
diately preceding this component { see exam-
ple (14).
(14)
13
Laura: Can I have some toast
please?
Jan: Some?
Laura: Toast
Our intuition is that this form is intonation-
ally distinct from the reprise fragment form
that it might be taken to resemble. This ap-
pears to be backed up by the fact that no
misunderstandings of gap-CRs were discov-
ered during our corpus analysis.
2.6 Gap Fillers
The ller form is used by a speaker to ll
a gap left by a previous incomplete utter-
ance. Its use therefore appears to be re-
stricted to such contexts, either because a pre-
vious speaker has left an utterance \hanging"
(as in example (15)) or because the CR ini-
tiator interrupts.
(15)
14
Sandy: if, if you try and do enchi-
ladas or
Katriane: Mhm.
Sandy: erm
Katriane: Tacos?
Sandy: tacos.
2.7 Conventional
A conventional form is available which ap-
pears to indicate a complete breakdown in
13
BNC le KD7, sentences 392{394
14
BNC le KPJ, sentences 555{559
communication. This takes a number of
seemingly conventionalised forms such as
\What?", \Pardon?", \Sorry?", \Eh?":
(16)
15
Anon 2: Gone to the cinema tonight
or summat.
Kitty: Eh?
Anon 2: Gone to the cinema
3 Clarication Readings
This section presents the readings that
have been identied, together with ex-
amples. We follow G&C's proposed
clausal/constituent/lexical split, with an
added reading for corrections.
3.1 Clausal
The clausal reading takes as the basis for
its content the content of the conversational
move made by the utterance being claried.
This reading corresponds roughly to \Are
you asking/asserting that X?", or \For which
X are you asking/asserting that X?". It fol-
lows that the source utterance must have been
partially grounded by the CR initiator, at
least to the extent of understanding the move
being made.
An attribute-value matrix (AVM) skeleton
for the semantic content of an HPSG sign
corresponding to this reading (according to
G&C's analysis) is shown below as AVM [1].
It represents a question
16
, the propositional
content of which is the conversational move
made by the source utterance (shown here as
being of type illoc(utionary)-rel(ation) { pos-
sible subtypes include assert, ask) together
with the message associated with that move
(e.g. the proposition being asserted). The pa-
rameter set being queried can be either a con-
stituent of that message (as would be the case
in a sluice or wh-substituted form, where the
CR question is the wh-question \For which X
are you asserting . . . ") or empty (as would be
15
BNC le KPK, sentences 580{582
16
We adopt here the version of HPSG developed in
G&S, wherein questions are represented as semantic
objects comprising a set of parameters (empty for a
polar question) and a proposition. This is the feature-
structure counterpart of a -abstract wherein the pa-
rameters are abstracted over the proposition.
the case in a fragment or literal reprise form,
where the CR question is the polar question
\Are you asserting . . . ").
[1]
2
6
6
6
6
4
question
params f
2
g or f g
prop j soa
2
4
illoc-rel
uttr
1
msg-arg

. . .
2
. . .

3
5
3
7
7
7
7
5
3.2 Constituent
Another possible reading is a constituent
reading whereby the content of a constituent
of the previous utterance is being claried.
This reading corresponds roughly to
\What/who is X?" or \What/who do you
mean by X?", as shown in AVM [2], a descrip-
tion of the content that would be given by
G&C's analysis. This shows a question whose
propositional content is the relation between
a sign (a constituent of the source utterance),
its speaker, and the intended semantic con-
tent. The abstracted parameter is the con-
tent.
[2]
2
6
6
6
6
6
6
4
question
params f
3
g
prop j soa
2
6
6
4
spkr-meaning-rel
agent
1
sign
2
cont
3
3
7
7
5
3
7
7
7
7
7
7
5
3.3 Lexical
Another possibility appears to be a lexical
reading. This is closely related to the clausal
reading, but is distinguished from it in that
the surface form of the utterance is being clar-
ied, rather than the content of the conversa-
tional move.
This reading therefore takes the form \Did
you utter X?" or \What did you utter?". The
CR initiator is attempting to identify or con-
rm a word in the source utterance, rather
than a part of the semantic content of the
utterance. This poses some interesting ques-
tions if a full analysis for this reading is to
be integrated into the HPSG framework de-
scribed above.
3.4 Corrections
The correction reading appears be along the
lines of \Did you intend to utter X (instead
of Y)?". We do not as yet have a full analysis
for this reading.
17
4 Corpus Analysis
4.1 Aims and Procedure
Our intention was to investigate the forms
and readings for CRs that are present in a cor-
pus of dialogue. For this purpose we used the
BNC, which contains a 10 million word sub-
corpus of English dialogue transcripts. For
this experiment, a sub-portion of the dialogue
transcripts was used consisting of c. 150,000
words. To maintain a spread across dialogue
domain, region, speaker age etc., this sub-
portion was created by taking a 200-speaker-
turn section from 59 transcripts.
All CRs within this sub-corpus were iden-
tied and tagged, using the markup scheme
and decision process described in 4.2 and 4.3
below. At time of writing this process has
been performed by only one (expert) user {
our intention is to conrm results by compar-
ing with those obtained by naive users, using
e.g. the kappa statistic (Carletta, 1996) to
assess reliability.
Initial identication of CRs was performed
using SCoRE (Purver, 2001), a search engine
developed specically for this purpose (in par-
ticular, to allow searches for repeated words
between speaker turns, and to display dia-
logue in an intuitive manner). However, in
order to ensure that all claricational phe-
nomena were captured, the nal search and
markup were performed manually.
4.2 Markup Scheme
The markup scheme used evolved during the
markup process as new CR mechanisms were
identied, and the nal scheme was as de-
scribed here. A multi-layered approach was
17
We suspect that corrections can in fact have
clausal, constituent or lexical sub-type, so this may in
fact not be a separate reading but a particular usage
of those already established. In this case corrections
may be covered by the analyses given for other read-
ings above, with a modied QUD coercion operation
{ see (Ginzburg and Cooper, forthcoming).
taken, along the lines of the DAMSL dialogue
act markup scheme (Allen and Core, 1997) {
this allowed sentences to be marked indepen-
dently for three attributes: form, reading and
source.
The form and reading attributes had -
nite sets of possible values. The possible val-
ues were as described in sections 2 and 3,
plus an extra catch-all category other to deal
with any otherwise uncategorisable phenom-
ena. The source attribute could take any in-
teger value and was set to the number of the
sentence that was being claried (according
to the BNC sentence-numbering scheme).
4.3 Decision Process
Following the methods described in (Allen
and Core, 1997), binary decision trees were
designed to guide the classication process.
The trees are designed so that a naive user can
follow them, but have yet to be tested in this
way. Trees were produced for initial identi-
cation of a CR, for classication of CR form
and for determination of CR source. Due to
space restrictions, the trees are not given here.
In the (common) case of ambiguity of read-
ing, the response(s) of other dialogue par-
ticipants were examined to determine which
reading was chosen by them. The ensuing re-
action of the CR initiator was then used to
judge whether this interpretation was accept-
able. If the CR initiator gave no reaction,
the reading was assumed to have been accept-
able. The following example (17) shows a case
where the other participant's initial (clausal)
reading was incorrect (the initiator is not sat-
ised), as a constituent reading was required.
In such cases, both CRs were marked as con-
stituent.
(17)
18
George: you always had er er say
every foot he had with a
piece of spunyarn in the
wire
Anon 1: Spunyarn?
George: Spunyarn, yes
Anon 1: What's spunyarn?
George: Well that's like er tarred
rope
18
BNC le H5G, sentences 193{196
In example (18), however, the other par-
ticipant's clausal interpretation provokes no
further reaction from the CR initiator, and is
taken to be correct:
(18)
19
Anon 1: you see the behind of Taz
Selassie: Tazmania?
Anon 1: Yeah.
Selassie: Oh this is so rubbish man.
In order to facilitate this process in the case
of CRs near the beginning or end of the 200-
turn section being marked, an additional 10
turns of backward and forward context were
shown (but not themselves marked up).
In the case of ambiguity as to which sen-
tence was being claried, the most recent one
was taken as the source.
4.4 Results
The BNC's SGML markup scheme (see
(Burnard, 2000) for details) allows sub-
corpora to be easily identied according to do-
main. This allowed us to collate results both
over all dialogue domains
20
, and restricted
to dialogue identied as demographic (non-
context-governed).
The distribution of CRs by form and read-
ing are shown in full in table 1 (all dialogue
domains) and table 2 (demographic only).
The distributions are presented as percent-
ages of all CRs found. This allows us to
see the proportion made up by each form
and each reading, together with any correla-
tions between form and reading, as discussed
in full below. Distributions are similar over
both sets, indicating that corpus size is large
enough to give repeatable results.
Separation between CR and source sen-
tence is shown in table 3 and gure 1, and
is discussed below.
4.4.1 Form/Reading Distribution
CRs were found to make up just under 4%
of sentences when calculated over the demo-
19
BNC le KNV, sentences 548{551
20
Domains identied by the BNC as context-
governing for dialogue include educational (school
classes, lectures) and business (meetings, training ses-
sions) { see (Burnard, 2000) for a full list.
graphic portion, or just under 3% when cal-
culated over all domains. This is a signicant
proportion, giving support to our claim that
processing of CRs is important for a dialogue
system.
The most common forms of CR can be seen
to be the conventional and reprise fragment
forms, with each making up over 25% of CRs.
Non-reprise CRs and reprise sluices are also
common, each contributing over 10% of CRs.
Other forms are all around 5% or less.
Nearly 50% of CRs can be successfully
interpreted as having a clausal reading, al-
though both the lexical (about 35%) and con-
stituent (about 15%) readings also make up a
signicant proportion.
This initially suggests that an automated
dialogue system which can deal with frag-
ments, sluices and reprise sentences (the anal-
yses described in section 1), together with
conventional and non-reprise CRs, could give
reasonable coverage of expected dialogue.
Fillers and especially gaps make up only a
small proportion.
However, the high proportion of lexical
readings suggests that a detailed analysis of
this phenomenon will be required.
4.4.2 Coverage
The coverage of the corpus by the forms
and readings listed in this paper is good, with
only 0.5% of CR readings (2 sentences) and
about 1.5% of CR forms (6 sentences) being
classied as other.
The readings not covered were all express-
ing surprise, amusement or outrage at a pre-
vious utterance (rather than requesting clar-
ication directly), and were all of the reprise
fragment or conventional form. Our intuition
is that these readings can be treated as clausal
readings with a further level of illocutionary
force given by use in context.
Of the 2 sentences left unclassied for form,
one appears to be an unusual conventional
reading, and one an interesting example of a
literal reprise of an unuttered but implied sen-
tence.
4.4.3 Form/Reading Correlation
It appears that of the non-conventional
reprise forms, only the reprise fragment re-
quires an analysis that gives a constituent
reading. Even then, this reading is much
less common than the clausal reading, and
we intend further investigation into this fact.
Sluices and reprise sentences appear always to
be satisfactorily interpretable by a clausal or
lexical reading.
21
As few examples of the rarer forms were
observed, it would be dangerous to attempt to
draw any rm conclusions about the readings
they can carry. We can, however, tentatively
suggest that the gap and ller forms might
only be used with a lexical reading.
22
One conclusion that can be safely drawn
is that many readings are available for some
forms (for example, the reprise fragment form
which appears to allow all readings). This
implies that disambiguation between readings
will be important for a dialogue system, and
this is an area we are currently examining.
Possibilities for sources of information that
could be used for disambiguation include di-
alogue context and intonation.
4.4.4 CR-Source Separation
The maximum CSS distance observed was
15 sentences. Only one example of this dis-
tance was observed, and one example of dis-
tance 13 { otherwise all CSS distances were
below 10 sentences. It should be noted that
the two long-distance cases were both seen
in one dialogue which had more than one
speaker present (the dialogue was in a class-
room situation with many people talking and
one speaker attempting to clarify an utter-
ance by the teacher), so may not be entirely
representative of the situation expected with
an automated dialogue system.
The vast majority of CRs had a CSS dis-
tance of one (i.e. were clarifying the immedi-
21
Whether this is desirable is less certain. G&S note
that echo and reference reprise sentences are intona-
tionally distinct, and this seems also true for sluices.
It may be that although the content of both can al-
ways be expressed as clausal, there is good reason not
to do so.
22
This runs contrary to our intuition which is that
the gap form might have a constituent reading.
ately preceding sentence { see gure 1), and
over 96% had a distance of 4 or less.
5 Conclusions
The taxonomy of readings and forms given
in this paper has been shown to cover nearly
99% of CRs within a corpus of dialogue. A
full HPSG analysis has been given elsewhere
for two of the four readings and four of the
eight forms.
Of the remaining readings, we believe that
the lexical reading can be treated by an ex-
tension of the existing analysis. Corrections
will need further research but make up only a
small proportion of CRs.
Of the remaining forms, we believe that two
(non-reprise and conventional) can be accom-
modated relatively smoothly within our cur-
rent HPSG framework. Gaps and llers, how-
ever, present a signicant challenge and will
be the subject of future research.
The measurements of CSS distance show
that an utterance record with length of the
order of ten sentences would be su?cient to
allow a dialogue system to process the vast
majority of CRs.
We are in the process of implement-
ing our existing analyses for the CR
forms and readings described above within
a HPSG/TrindiKit-based dialogue system
which incorporates the ellipsis resolution ca-
pability of SHARDS (Ginzburg et al, 2001a)
and the dialogue move engine of GoDiS (Lars-
son et al, 2000). At time of writing, the sys-
tem can successfully produce both clausal and
constituent readings. As a result of the re-
search outlined in this paper, a lexical reading
is currently being implemented.
Our results also suggest that investigation
into disambiguation of reading, possibly on
the basis of dialogue information state and/or
intonation, will be required.
References
James Allen and Mark Core. 1997. Draft of
DAMSL: Dialog act markup in several layers.
Peter Bohlin (Ljunglof), Robin Cooper, Elisabet
Engdahl, and Staan Larsson. 1999. Informa-
tion states and dialogue move engines. In Jan
Alexandersson, editor, IJCAI-99 Workshop on
Knowledge and Reasoning in Practical Dialogue
Systems.
Lou Burnard. 2000. Reference Guide for the
British National Corpus (World Edition). Ox-
ford University Computing Services.
Jean Carletta. 1996. Assessing agreement on clas-
sication tasks: the kappa statistic. Computa-
tional Linguistics, 22(2):249{255.
Herbert H. Clark. 1996. Using Language. Cam-
bridge University Press.
Charles Fletcher. 1994. Levels of representation
in memory for discourse. In Morton Ann Gerns-
bacher, editor, Handbook of Psycholinguistics.
Academic Press.
Jonathan Ginzburg and Robin Cooper.
2001. Resolving ellipsis in clarication.
In ACL/EACL01 Conference Proceedings.
Association for Computational Linguistics,
July.
Jonathan Ginzburg and Robin Cooper. forthcom-
ing. Clarication, ellipsis and utterance repre-
sentation.
Jonathan Ginzburg and Ivan Sag. 2000. Inter-
rogative Investigations: the Form, Meaning and
Use of English Interrogatives. Number 123 in
CSLI Lecture Notes. CSLI Publications.
Jonathan Ginzburg, Howard Gregory, and Shalom
Lappin. 2001a. SHARDS: Fragment resolu-
tion in dialogue. In Harry Bunt, Ielka van der
Sluis, and Elias Thijsse, editors, Proceedings of
the Fourth International Workshop on Compu-
tational Semantics (IWCS-4), pages 156{172.
ITK, Tilburg University, Tilburg.
Jonathan Ginzburg, Ivan A. Sag, and Matthew
Purver. 2001b. Integrating conversational
move types in the grammar of conversation.
In P. Kuhnlein, H. Rieser, and H. Zeevat, edi-
tors, Proceedings of the Fifth Workshop on For-
mal Semantics and Pragmatics of Dialogue. BI-
DIALOG.
Jonathan Ginzburg. 1996. Interrogatives: Ques-
tions, facts and dialogue. In Shalom Lappin,
editor, The Handbook of Contemporary Seman-
tic Theory, pages 385{422. Blackwell.
Staan Larsson, Peter Ljunglof, Robin Cooper,
Elisabet Engdahl, and Stina Ericsson. 2000.
GoDiS - an accommodating dialogue system. In
Proceedings of ANLP/NAACL-2000 Workshop
on Conversational Systems.
Matthew Purver. 2001. SCoRE: A tool for search-
ing the BNC. Technical report, Department of
Computer Science, King's College London.
John R. Ross. 1969. Guess who? In R. I. Bin-
nick, A. Davison, G. Green, and J. Morgan, ed-
itors, Papers from the Fifth Regional Meeting of
the Chicago Linguistic Society, pages 252{286.
CLS, University of Chicago.
Jacqueline D. Sachs. 1967. Recognition mem-
ory for syntactic and semantic aspects of
connected discourse. Perception and Psy-
chophysics, 2:437{442.
David Traum. 1994. A Computational Theory of
Grounding in Natural Language Conversation.
Ph.D. thesis, University of Rochester.
Teun A. van Dijk and Walter Kintsch. 1983.
Strategies of Discourse Comprehension. Aca-
demic Press.
Non- Literal Wh-sub Reprise Reprise Gap Gap Conve- Other Total
Reprise Reprise Reprise Sluice Fragmt Filler ntional
Clausal 4.3 4.8 1.0 10.7 25.2 0 0 0 0.5 46.5
Constituent 7.6 0 0 0 1.7 0 0 5.3 0 14.5
Lexical 0.7 0 2.6 2.1 0.2 0.5 3.8 25.0 0 35.0
Correction 1.0 0.5 0 0 1.0 0 0 0 0 2.4
Other 0 0 0 0 1.0 0 0 0.5 0 1.4
Total 13.6 5.3 3.6 12.8 29.1 0.5 3.8 30.7 0.5 100.0
Table 1: CR form and type as percentage of CRs { all domains
Non- Literal Wh-sub Reprise Reprise Gap Gap Conve- Other Total
Reprise Reprise Reprise Sluice Fragmt Filler ntional
Clausal 4.1 4.7 1.0 11.3 24.8 0 0 0 0.5 46.5
Constituent 6.2 0 0 0 1.8 0 0 5.7 0 13.6
Lexical 0.8 0 2.6 2.3 0.3 0.5 3.1 26.3 0 35.9
Correction 1.0 0.5 0 0 1.0 0 0 0 0 2.6
Other 0 0 0 0 0.8 0 0 0.5 0 1.3
Total 12.1 5.2 3.6 13.6 28.6 0.5 3.1 32.5 0.5 100.0
Table 2: CR form and type as percentage of CRs { demographic portion
Distance 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
All domains 8 291 36 16 10 3 4 0 2 2 0 0 0 1 0 1
Demographic 7 264 34 16 9 3 4 0 2 2 0 0 0 1 0 1
Table 3: Number of CRs vs. CR-Source Separation Distance
Figure 1: Percentage of CRs vs. CR-Source Separation Distance
 
		Answering Clarification Questions
Matthew Purver1, Patrick G.T. Healey2, James King2, Jonathan Ginzburg1 and Greg J. Mills2
1Department of Computer Science
King?s College, London
London WC2R 2LS, UK
2Department of Computer Science
Queen Mary, University of London
London E1 4NS, UK
Abstract
This paper describes the results of cor-
pus and experimental investigation into
the factors that affect the way clarifica-
tion questions in dialogue are interpreted,
and the way they are responded to. We
present some results from an investigation
using the BNC which show some general
correlations between clarification request
type, likelihood of answering, answer type
and distance between question and an-
swer. We then describe a new experi-
mental technique for integrating manip-
ulations into text-based synchronous dia-
logue, and give more specific results con-
cerning the effect of word category and
level of grounding on interpretation and
response type.
1 Introduction
Requesting clarification is a vital part of the com-
municative process and has received attention from
both the formal semantic (Ginzburg and Cooper,
2001; Ginzburg and Cooper, forthcoming) and con-
versation analytic traditions (Schegloff, 1987), but
little in the computational dialogue system commu-
nity. In theory, a perfect dialogue system should be
able to interpret and deal with clarification requests
(CRs) made by the user in order to elicit clarifica-
tion of some part of a system utterance, and be able
to request clarification itelf of some part of a user ut-
terance. This is no easy task ? CRs may take many
different forms (often highly elliptical), and can be
intended to be interpreted with many different read-
ings which query different aspects of the original ut-
terance. As a result, dialogue system design has tra-
ditionally attempted to avoid the necessity for CR
interpretation by making system utterances as clear
and precise as possible, and avoid having to generate
all but the most simple CRs by using robust shallow
methods of interpretation or by relying on highly
domain-dependent lexicons and grammars. How-
ever, as systems become more human-like, it seems
likely that we will have to cope with user CRs at
some stage; and the ability to generate system CRs
can be useful in order to repair misunderstanding,
disambiguate other utterances, and learn new words
? see (Knight, 1996; Dusan and Flanagan, 2002;
Purver, 2002).
The investigations presented here had two main
aims: to examine (a) how CRs are interpreted, and
(b) how they are responded to. The two are clearly
dependent ? the response must depend on the inter-
pretation ? but there are many other influencing fac-
tors such as CR form, context and level of ground-
ing. Answers to (a) should help us with the follow-
ing questions:
? What factors can help us disambiguate and cor-
rectly interpret user CRs?
? What factors should govern generation of sys-
tem CRs such that they are correctly interpreted
by the user?
Answers to (b) should help with the following re-
lated questions:
? How (and when) should we answer user CRs?
? How (and when) should we expect users to re-
spond to system CRs?
The paper is organised as follows. The next sec-
tion gives a brief overview of CRs in general and
some previous corpus work. Section 3 describes fur-
ther corpus work which gives some general results
concerning response type. Section 4 then describes
a text-based dialogue experiment examining the de-
tailed effects on interpretation and response of part-
of-speech (PoS) type and level of grounding for one
particular CR form, and section 5 then draws some
general conclusions.
2 Clarification Requests
Purver et al (2001; 2002) presented a taxonomy
of CR forms and readings derived from a corpus
study using the British National Corpus (BNC) ?
see (Burnard, 2000). This showed that some forms
showed a high correlation with certain readings, but
that some were highly ambiguous.
Purver et al (2002)?s taxonomy of CR forms is
given in table 1 and CR readings in table 21. Some
CRs (the non-reprise class) explicitly identify the
clarification required, e.g. ?What did you say?? or
?What do you mean??, and some forms (e.g. literal
reprises) appear to favour a particular reading almost
exclusively, but most are more ambiguous. Indeed,
they found that the two most common forms (the
conventional and reprise fragment form) could take
any reading.
Although this corpus study provided informa-
tion about the distribution of different CR forms
and readings, it did not provide any information
about the specific conditions which prompt partic-
ular readings and affect how the CR is answered.
In this paper we concentrate mostly on the reprise
fragment (RF) form, where only a single part of
the problem utterance, possibly a single word, is
reprised2 as in example (1). This form is not only
1They also give a correction reading, which we have ex-
cluded here: such CRs are almost exclusively self-corrections
and as such do not fit well with our discussion here. They are
also very rare compared with the other classes, making up only
about 2% of CRs.
2Such reprises need not be verbatim repeats: users may use
anaphoric terms or use a clearer expression in order to clarify
the fragment in question.
common (approximately 30% of CRs in the previ-
ous study) and can appear with many readings (al-
though biased towards a clausal reading ? 87% of
occurrences), but specifies the problematic element
that it clarifies quite precisely, and therefore should
give us scope for examining the effect of features of
that element.
(1)3
Gary: Aye, but <pause> you
know <pause> like you
se- she mentioned one in
particular, like
Jake: What?
Gary: the word skeilth
Jake: Skeilth?
Lilias: Mm.
Gary: Aha.
Jake: Aye, yeah, yeah, take skeilth.
Intuitively, at least two such features would be
expected to affect the type of reading assigned to
a RF: PoS category and level of grounding.4 The
PoS category of the reprised word should influence
expectations about what is being clarified. For ex-
ample, reprise of a content word (e.g. noun or verb)
should be more likely to signal a constituent problem
than a reprise of a function word (e.g. preposition or
determiner). Dialogue participants would normally
assume that the meaning of function words is well
known in a particular linguistic community and that,
as a result, a reprise of a function word is more likely
to signal clausal or lexical problems. RF interpreta-
tion should also depend on whether a reprised frag-
ment is already considered to have been grounded
by the participants in a conversation. For example,
a reprise of a proper noun would be more likely to
be read as signalling a constituent problem if it oc-
curs on the first mention than on second mention.
All things being equal, the content of a constituent
is already considered to have been established by the
time a second mention occurs.
3 Corpus Investigation
Accordingly we have re-examined the corpus from
the above study in order to add information about
3BNC file KPD, sentences 578?584
4Another is intonation. However, there is no intonational
information in the BNC. In the future we hope to investigate
this using other corpora and experimental methods.
Class Description Example
non Non-Reprise ?What did you say??
wot Conventional ?Pardon??
frg Reprise Fragment ?Paris??
slu Reprise Sluice ?Where??
lit Literal Reprise ?You want to go to Paris??
sub Wh-Subsituted Reprise ?You want to go where??
gap Gap ?You want to go to . . . ??
fil Gap Filler ?. . . Paris??
oth Other Other
Table 1: CR forms
Class Description Paraphrase
cla Clausal ?Are you asking/telling me that . . . X . . . ??
con Constituent ?What/who do you mean by ?X???
lex Lexical ?Did you utter ?X???
oth Other Other
Table 2: CR readings
category, grounding and method of answering.
3.1 Method
The same corpus was re-marked for four attributes:
response type and CR-answer distance, and the PoS
and last mention of the original source element.
The markup scheme used for response type
evolved during the study and is shown in table 3:
it includes classification of apparently unanswered
CRs into those that may have been answered, but
the sentence possibly containing an answer was tran-
scribed in the BNC as <unclear>; those that ap-
pear to have remained unanswered because the CR
initiator continued their turn without pause; and
those that are not answered at all (or at least where
we have no indication of an answer ? eye contact,
head movement etc. are not recorded in the BNC but
could function as answers). In cases where the ini-
tial response was followed by further information,
both were recorded, but the results here are pre-
sented only for the initial response. Further work
later may take both into account, along the lines of
(Hockey et al, 1997) who showed this to be impor-
tant for questions in general.
CR-answer distance was marked in terms of the
sentence numbering scheme in the BNC ? in these
cases it corresponds very closely to distance in
speaker turns, although the correspondence is not
exact.
PoS category and time of last mention of the
source element were marked, but have not currently
been used due to lack of useful data (see below).
Reliability of the markup has not yet been exam-
ined. However, the method is close to that of (Purver
et al, 2002) (and the corpus is identical), where re-
liability was examined and found to be acceptable.
We then examined the correlation between CR type
and response type, between reading and response
type, and the spread of CR-answer distance.
3.2 Results
3.2.1 Response Type
Results for response type are shown in table 4 as
raw numbers, and also in table 5 as percentages for
each CR type, with the none, cont, uncl and qury
classes conflated as one ?unanswered? class, and
only the most common 4 CR forms shown.
The most striking result is perhaps the high over-
all number of CRs that do not receive an answer:
39% of all CRs do not appear to be answered overall,
although this reduces to 17% when taking account
of those marked uncl (possible answers transcribed
none No answer
cont CR initiator continues immediately
uncl Possible answer but transcribed as <unclear>
qury CR explicitly queried
frg Answered with parallel fragment
sent Answered with full sentence
yn Answered with polar particle
Table 3: CR response types
as <unclear>) and cont (the CR-raiser continues
without waiting). The most common forms (conven-
tional and RF) appear to be answered least ? around
45% go unanswered for both. The form which ap-
pears to be most likely to be answered overall is the
explicit non-conventional form.
Some forms appear to have high correlations
with particular response types. As might be ex-
pected, sluices (which are wh-questions) are gen-
erally answered with fragments, and never with a
polar yes/no answer. Yes/no answers also seem to
be unsuitable for the conventional CR form, which
is generally answered with a full sentence. RFs,
conversely, are not often answered with full sen-
tences, but can be responded to either by fragments
or yes/no answers.
Similarly, from tables 6 and 7 (again, percentages
given for each CR reading, with ?unanswered? re-
sponse types conflated and only the most common 3
readings shown) we can see that there is a correla-
tion between reading and response type, but that this
correlation is also not as simple as a direct reading-
answer correspondence. Clausal CRs are unlikely to
be answered with full sentences, but can get either
fragment or yes/no responses. Constituent CRs are
less likely to get yes/no responses but could get ei-
ther other type. Interestingly, constituent CRs seem
to be roughly twice as likely to get a response as
clausal or lexical CRs (even though there are fewer
examples of constituent CRs than the others, this
difference is statistically significant, with a ?2(1) test
showing <0.5% probability of independence).
3.2.2 Answer Distance
Results for CR-answer distance are shown in ta-
ble 8. It is clear that the vast majority (94%) of CRs
that are answered are answered in the immediately
unans frg sent yn
wot 45.6 8.7 44.8 0.8 (100)
frg 43.2 21.1 3.4 32.2 (100)
slu 37.0 50.0 12.9 0 (100)
non 13.4 26.9 26.9 32.6 (100)
Table 5: BNC results: Response type as percentages
for each CR form
unans frg sent yn
cla 39.8 22.2 7.8 30.0 (100)
con 20.0 35.0 33.3 11.6 (100)
lex 42.7 17.2 36.5 3.4 (100)
Table 7: BNC results: Response type as percentages
for each CR reading
1 2 3 >3 Total
Distance 273 14 2 0 289
Table 8: CR-answer distance (sentences)
following sentence, and that none are left longer
than 3 sentences. While we do not yet have concrete
equivalent figures for non-clarificational questions,
a study is in progress and initial indications are that
in general, answers are less immediate: only about
70% have distance 1, with some up to distance 6.5
We therefore expect that (a) answering user CRs
must be done immediately, and that any dialogue
management scheme must take this into account,
and (b) we should expect answers to any system
CRs to come immediately ? interpretation routines
(we are thinking especially of any ellipsis resolution
routines here) should not assume that later turns are
5Thanks to Raquel Ferna?ndez for providing us with these
preliminary figures.
none cont uncl qury frg sent yn Total
wot 21 13 24 0 11 57 1 127
frg 23 22 6 0 25 4 38 118
slu 8 6 5 1 27 7 0 54
non 4 2 1 0 14 14 17 52
lit 5 2 1 0 1 1 10 20
fil 3 0 1 0 7 1 4 16
sub 4 0 3 0 4 4 0 15
gap 1 0 0 0 1 0 0 2
oth 0 0 0 1 0 1 0 2
Total 69 45 41 2 90 89 70 406
Table 4: BNC results: Response type vs. CR form
none cont uncl qury frg sent yn Total
cla 33 31 11 2 43 15 58 193
con 9 3 0 0 21 20 7 60
lex 21 11 30 0 25 53 5 145
oth 5 0 0 0 0 1 0 6
Total 69 45 41 2 90 89 70 406
Table 6: BNC results: Response type vs. CR reading
relevant to the CR.
3.2.3 Further Details
While interesting, we would like to know more
detail than the general trends described above: in
particular we would like to know the effect of
the factors we have mentioned (word category and
grounding) for particular forms. As stated above,
we concentrate here on the reprise fragment form.
Examination of original CR source fragment PoS
category, in order to test the effect of the con-
tent/function distinction, showed that almost all RFs
were of content words or whole phrases: only 6 of
118 RFs were of function words, all of which were
determiners (mostly numbers). This is interesting in
itself: perhaps RFs are unlikely to be used to clarify
uses of e.g. prepositions. However, the effect may
be due to lack of data, and does not provide us with
any way of testing the distinction between clausal
and constituent reading that we expect.
Markup of last mention of the original source
fragment has also not given results in which we can
be confident. For RFs, we have seen that all con-
stituent readings occur on the first mention of the
fragment (as expected) ? but there are too few of
these examples to draw any firm conclusions. It is
also impossible to know whether first mention in the
transcription is really the first mention between the
participants: we do not know what happened before
the tape was turned on, what their shared history is,
or what is said during the frequent portions marked
as <unclear>.
So we need more information than our current
corpus can provide. In order to examine these ef-
fects properly we have therefore designed an exper-
imental technique to allow dialogues to be manipu-
lated directly, with reprises with the desired proper-
ties automatically introduced into the conversation.
The next section describes this technique and the ex-
periment performed.
4 Experimental Work
Empirical analyses of dialogue phenomena have
typically focused either on detailed descriptive anal-
yses of corpora of conversations (Schegloff, 1987)
or on the experimental manipulation of relatively
global parameters of interaction such as task type or
communicative modality (Clark and Wilkes-Gibbs,
1986), (Garrod and Doherty, 1994). These stud-
ies have been used to to motivate a variety of pro-
posals about turn-level mechanisms and procedures
that sustain dialogue co-ordination. Further devel-
opment and testing of these proposals has, how-
ever, been limited by the indirect nature of the avail-
able evidence. Corpus studies provide, retrospec-
tive, correlational data which is susceptible to chal-
lenge and re-interpretation. Current psycholinguis-
tic techniques do not provide ways of integrating ex-
perimental manipulations into interactions in a man-
ner that is sensitive to the linguistic and conversa-
tional context. This section introduces a technique
for carrying out experiments in which text-based in-
teractions can be directly manipulated at the turn
level, and gives the results of an experiment which
uses this approach to investigate the effects of the
factors mentioned above on interpretation and re-
ponse to RFs. We also briefly discuss the range of
potential applications and some of the practical lim-
itations of the approach in the context of the experi-
mental results.
4.1 Manipulating ?Chat? Interactions
The experimental technique presented here draws on
two general developments. Firstly, the increasing
use of text-based forms of synchronous conversa-
tional interaction, for example: chat rooms (MUD?s,
MOO?s etc.), instant messaging, and some online
conferencing tools. Secondly, advances in natural
language processing technology which make some
forms of text processing and transformation fast
enough to be performed on a time scale consistent
with exchanges of turns in synchronous text chat.
The basic paradigm involves pairs of subjects,
seated in different rooms, communicating using a
synchronous text chat tool (see figure 1 for an ex-
ample). However, instead of passing each completed
turn directly to the appropriate chat clients, each turn
is routed via a server. Depending on the specific
goals of the experiment, the server can be used to
systematically modify turns in a variety of ways. For
example, some simple forms of mis-communication
can be introduced into an interaction by transform-
ing the order of characters in some of the input
words or by substituting words with plausible non-
words. Importantly, the server controls which mod-
ifications are broadcast to which participant. So, if
participant A types the word ?table? the sever can
echo back A: table to participant A and a trans-
formed version, say, ?blate? to participant B who
sees A: blate. The ability to set up controlled
asymmetries of this kind between the participants in
a interaction creates a powerful range of experimen-
tal possibilities. Here, we describe an application of
this technique to the investigation of reprise clarifi-
cation requests (CR?s).
A chat-tool experiment was designed to test the
following hypotheses:
1. RFs for function words will normally receive
clausal readings, whereas both clausal and con-
stituent readings will be available for content
words.
2. RFs for content words will receive more con-
stituent readings on first mention than on sec-
ond mention.
3. No difference is predicted for RFs for function
words on first vs. second mention.
4.2 Method
Two tasks were used to elicit dialogue, a balloon
debate and a story-telling task. In the balloon de-
bate subjects are presented with a fictional scenario
in which a balloon is losing altitude and about to
crash. The only way for any of three passengers to
survive is for one of them to jump to a certain death.
The three passengers are; Dr. Nick Riviera, a can-
cer scientist, Mrs. Susie Derkins, a pregnant primary
school teacher, and Mr. Tom Derkins, the balloon
pilot and Susie?s husband. Subjects are asked to de-
cide who should jump. The advantages of this task
are that it is effective at generating debates between
subjects and involves repeated references to particu-
lar individuals.
Following (Bavelas et al, 1992), the second di-
alogue task used was the story-telling task. In this
case subjects are asked to relate a ?near-miss? story
about some experience in which something bad al-
most happened but in the end everything was okay.
This was chosen because, unlike the balloon task,
the topic of the exchange is unrestricted, in effect
a random factor, and the interaction relates to real
events.
4.2.1 Subjects
Twenty-eight subjects were recruited, 20 male
and 8 female, average age 19 years, from computer
science and IT undergraduate students. They were
recruited in pairs to ensure that the members of a
pair were familiar with one another and only sub-
jects who had experience with some form of text
chat such as chat rooms, IRC, ICQ or other mes-
saging systems were used. Each subject was paid
at a rate of ?7.50 per hour for participating in the
experiment.
4.2.2 Materials
A custom experimental chat tool, written in Java
and Perl, was used for the experiment. The user in-
terface is similar to instant messaging applications:
a lower window is used to enter text, and the con-
versation is displayed in the main upper window as
it emerges (see figure 1). The chat clients were run
on two Fujitsu LCD tablet computers with text in-
put via standard external keyboards, with the server
running on a standard PC in a separate room.
User Interface The Chattool client user interface
is written in Java and is designed to be familiar
to subjects experienced with instant messaging/chat
applications. The application window is split into
two panes: a lower pane for text entry and an up-
per pane in which the conversation is displayed (see
figure 1). A status display between the two panes
shows whether the other participant is active (typ-
ing) at any time. This can be artificially controlled
during the generation of artificial turns to make it
appear as if they are generated by the other partici-
pant. The client also has the ability to display an er-
ror message and prevent text entry: this can be used
to delay one participant while the other is engaged
in an artificially-generated turn sequence.
Server Each turn is submitted to a server (also
written in Java) on a separate machine when a ?Send?
button or the ?Return? key is pressed. This server
passes the text to a NLP component for processing
and possible transformation, and then displays the
original version to the originator client, and the pro-
cessed (or artificially generated) version to the other
client. The server records all turns, together with
each key press from both clients, for later analysis.
This data is also used on the fly to control the speed
and capitalisation of artificially generated turns, to
be as realistic a simulation of the relevant subject as
possible.
NLP Component The NLP component consists
of a Perl text-processing module which commu-
nicates with various external NLP modules as re-
quired: PoS tagging can be performed using LT-
POS (Mikheev, 1997), word rarity/frequency tag-
ging using a custom tagger based on the BNC (Kil-
garriff, 1997), and synonym generation using Word-
Net (Fellbaum, 1998).
Experimental parameters are specified as a set of
rules which are applied to each word in turn. Pre-
conditions for the application of the rule can be spec-
ified in terms of PoS, word frequency and the word
itself, together with contextual factors such as the
time since the last artificial turn was generated, and
a probability threshold to prevent behaviour appear-
ing too regular. The effect of the rule can be to
transform the word in question (by substitution with
another word, a synonym or a randomly generated
non-word, or by letter order scrambling) or to trigger
an artificially generated turn sequence (currently a
reprise fragment, followed by an acknowledgement,
although other turn types are possible).
The current experimental setup consists of rules
which generate pairs of RFs and subsequent
acknowledgements6, for proper nouns, common
nouns, verbs, determiners and prepositions, with
probabilities determined during a pilot experiment
to give reasonable numbers of RFs per subject. No
use is made of word rarity or synonyms.
The turn sequences are carried out by (a) present-
ing the artificially-generated RF to the relevant client
only; (b) waiting for a response from that client, pre-
venting the other client from getting too far ahead
by locking the interface if necessary; (c) presenting
an acknowledgement to that response; and (d) pre-
senting any text typed by the other client during the
sequence.
4.2.3 Procedure
Prior to taking part subjects were informed that
the experimenters were carrying out a study of the
effects of a network-based chat tool on the way peo-
6Acknowledgements are randomly chosen amongst: ?ah?,
?oh?, ?oh ok?, ?right?, ?oh right?, ?uh huh?, ?i see?, ?sure?.
ple interact with one another. They were told that
their interaction would be logged, anonymously, and
kept for subsequent analysis. Subjects were advised
that they could also request the log to be deleted af-
ter completion of the interaction. They were not in-
formed of the artificial interventions until afterwards
(see below).
At the start of the experiment subjects were given
a brief demonstration of the operation of the chat
tool.
To prevent concurrent verbal or gestural interac-
tion subjects were seated in separate rooms. Each
pair performed both dialogue tasks and were given
written instructions in each case. The balloon task
was carried out once and the story-telling task twice;
one story for each participant. To control for or-
der effects the order of presentation of the two tasks
was counterbalanced across pairs. A 10-minute time
limit was imposed on both tasks. At the end of
the experiment subjects were fully debriefed and the
intervention using ?artificial? clarifications was ex-
plained to them.
This resulted in a within-subjects design with two
factors; category of reprise fragment and level of
grounding (first vs. second mention).
After the experiment, the logs were manually cor-
rected for the PoS category of the RF and for the
first/second mention clarification. PoS required cor-
rection as the tagger produced incorrect word cate-
gories in approximately 30% of cases. In some in-
stances this was due to typing errors or text-specific
conventions, such as ?k? for ?okay?, that were not
recognised. Detection and classification of proper
nouns was also sensitive to capitalisation. Subjects
were not consistent or conventional in their capitali-
sation of words and this caused some misclassifica-
tions. In addition a small proportion of erroneous
tags were found. Each system-generated CR was
checked and, where appropriate, corrected. Because
pairs completed both tasks together CRs classified
as ?first mentions? were checked to ensure that they
hadn?t already occured in a previous dialogue.
4.3 Results
The readings attributed to each RF were classified
in the same way as the original BNC-based cor-
pus, with the addition of one further category: non-
clarificational, referring to situations in which the
fragment is treated as something other than a CR
(this did not apply when building the original cor-
pus, as only utterances treated as CRs were con-
sidered). In the experimental results, gap, lexical
and non-clarificational readings were low frequency
events (4, 1 and 8 instances respectively) and no in-
stances of correction readings were noted. These fig-
ures are comparable with (Purver et al, 2002)?s ob-
servations for the BNC. For statistical analysis these
three categories of reading were grouped together as
?Other?.
Across the corpus as a whole a total of 215
system-generated RFs were produced. In 50% of
cases the system-generated clarification received no
response from the target participant. This may be
due in part to the medium: unlike verbal exchanges,
participants in text-chat can produce their turns si-
multaneously. This can result in turns getting out
of sequence since users may still be responding to a
prior turn when a new turn arrives. Users must then
trade off the cost of undoing their turn in progress
to respond to the new one, against going ahead any-
way and responding to the new turn later if it seems
necessary. Thus in some cases we observed that the
response to a clarification was displaced to the end
of the turn in progress or to a subsequent turn. How-
ever, comparison with the BNC results from sec-
tion 3 above show similar figures: only 56% of the
frg class received a clear answer. Although the
true figure will be higher (of the 56%, 5% may have
been answered, but the next turn was transcribed as
<unclear>, and we cannot know in how many
cases the reprise may have been answered using
non-verbal signals), it seems likely that a significant
proportion may simply be ignored.
Response Category
Category None Con Cla Other
Cont (1st) 29 14 23 4
Cont (2nd) 43 7 16 9
Func (1st) 6 0 0 6
Func (2nd) 20 0 1 9
Table 9: Frequency of Reading Types By RF Cate-
gory and Mention
The distribution of reading types according to
word category was tested firstly by comparing the
frequency of Clausal, Constituent, and Other read-
ings for content words and function words. This
proved to be reliably different (?2(2) = 35.3, p =
0.00).7 As table 9 shows, RFs of Function words
were almost exclusively interpreted as Other, i.e. ei-
ther Gap, Lexical or Non-clarificational. By contrast
Content word reprises were interpreted as Clausal
CRs 53% of the time, as Constituent CRs 29% of
the time and as Other 18% of the time.
Content word and Function word clarifications
were also compared for the the frequency with
which they received a response. This showed no
reliable difference (?2(1) = 1.95, p = 0.16) indicat-
ing that although the pattern of interpretation for
Content and Function reprises is different they are
equally likely to receive some kind of response.
The influence of grounding on reading type was
assessed firstly by comparing the relative frequency
of Constituent, Clausal and Other readings on first
and second mention. This was reliably different
(?2(2) = 6.28, p = 0.04) indicating that level of
grounding affects the reading assigned. A focussed
comparison of Constituent and Clausal readings on
first and second mention shows no reliable differ-
ence (?2(1) = 0.0, p = 0.92). Together these findings
indicate that, across all word categories, Constituent
and Clausal readings are more likely for RF?s of a
first mention than a second mention and, conversely,
Other readings are less likely for RF?s to a first men-
tion than a second mention.
The effect of grounding on the relative frequency
with which a clarification received a response was
also tested. This indicated a strong effect of mention
(?2(1) = 12.01, p = 0.00); 58% of reprise clarifications
of first mentions recieved a response whereas only
33% of second mention clarifications did.
4.4 Discussion
The experimental results support two basic conclu-
sions. Firstly, people?s interpretation of the type of
CR a reprise fragment is intended to make is influ-
enced both by the category of the reprise fragment
and its level of grounding. Secondly, reprise frag-
ment CRs to first mentions are much more likely to
be responded to than reprise fragment CRs for sec-
7A criterion level of p < 0.05 was adopted for all statistical
tests.
ond mentions.
Text-based and verbal interaction have different
properties as communicative media. Amongst other
things, in text-chat turns take longer to produce,
are normally produced in overlap, and they persist
for longer. However, even given these differences,
the general pattern of clarifications observed in the
experimental task is similar to that noted in ver-
bal dialogue. In particular, Lexical, Gap and Non-
clarificational readings are infrequent and reprise
fragment clarifications are ignored with surprising
frequency. In the present data, the clearest contrast
between text-based and verbal interaction is in the
relative frequency of Constituent and Clausal read-
ings. In the BNC reprise fragments receive Clausal
readings in 87% of cases, and constituent readings in
6% of cases. In the experimental corpus they receive
Clausal readings in 48% of cases and Constituent
readings in 34% of cases.
These findings demonstrate the viability, and
some limitations, of investigating dialogue co-
ordination through the manipulation of chat-tool
based interactions. The chat tool was successful
in producing plausible clarification sequences. Al-
though in some cases participants had difficulty
making sense of the artificial clarifications this did
not make them distinguishable from other, real, but
equally problematic turns from other participants.
The clarifications were mostly successful in creat-
ing realistic exchanges such as those illustrated in
figures 2 and 3. When questioned during debriefing,
no participants reported any suspicions about the ex-
perimental manipulation.
The main practical difficulty encountered in the
present study related to text-chat conventions such
as novel spellings, abbreviations, and use of ?smi-
leys?. This created specific problems for the PoS
tagger which assumes a more standard form of En-
glish. These problems were also compounded by the
noise introduced by typing errors and inconsistency
in spelling and capitalisation.
The experiment presented here exploits only one
possibility for the use of this technique. Other
prossible manipulations include; manipulation of
distance, in turns or time, between target and probe,
substitution of synonyms, hyponyms and hyper-
nyms, introduction of artifical turns, blocking of
certain forms of response. The important potential
it carries, particularly in comparison with corpus-
based techniques, is in the investigation of dialogue
phenomena which for various reasons are infrequent
in existing corpora.
5 Conclusions
The main conclusions we draw from the results pre-
sented here are as follows:
? Reprise CRs appear to go without response far
more often than might be expected, both in the
BNC and in our experimental corpus. Both
may be effects of the media (transcription in
one case, turn sequencing overlap in the other),
but the figures are large enough and similar
enough to warrant further investigation.
? Corpus investigation shows some strong corre-
lations between CR form and expected answer
type. It also shows that responses to CRs, when
they come, come immediately.
? Both word PoS category and first/second men-
tion appear to be reliable indicators of RF read-
ing. This can help us in disambiguating user
CRs, and in choosing forms when generating
system CRs.
? RFs generated on the first mention of a word
have a higher likelihood of receiving a response
than on second mention.
? We have presented a new experimental tech-
nique for manipulating dialogue, which we be-
lieve has many potential uses in dialogue re-
search.
6 Acknowledgments
This work was supported by the EPSRC under the
project ?ROSSINI: Role of Surface Structural Infor-
mation in Dialogue? (GR/R04942/01).
References
J.B. Bavelas, N. Chovil, D. Lawrie, and L. Wade. 1992.
Interactive gestures. Discourse Processes, 15:469?
489.
Lou Burnard. 2000. Reference Guide for the British
National Corpus (World Edition). Oxford University
Computing Services.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22:1?39.
Sorin Dusan and James Flanagan. 2002. Adaptive dialog
based upon multimodal language acquisition. In Pro-
ceedings of the Fourth IEEE International Conference
on Multimodal Interfaces, Pittsburgh, October.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Simon Garrod and Gwyneth Doherty. 1994. Conversa-
tion, co-ordination and convention: an empirical in-
vestigation of how groups establish linguistic conven-
tions. Cognition, 53:181?215.
Jonathan Ginzburg and Robin Cooper. 2001. Resolv-
ing ellipsis in clarification. In Proceedings of the 39th
Meeting of the ACL, pages 236?243. Association for
Computational Linguistics, July.
Jonathan Ginzburg and Robin Cooper. forthcoming.
Clarification, ellipsis, and the nature of contextual up-
dates. Linguistics and Philosophy.
Beth Ann Hockey, Deborah Rossen-Knill, Beverly Spe-
jewski, Matthew Stone, and Stephen Isard. 1997. Can
you predict answers to Yes/No questions? Yes, No and
Stuff. In Proceedings of Eurospeech ?97.
Adam Kilgarriff. 1997. Putting frequencies in the
dictionary. International Journal of Lexicography,
10(2):135?155.
Kevin Knight. 1996. Learning word meanings by in-
struction. In Proceedings of the Thirteenth National
Conference on Artifical Intelligence, pages 447?454.
AAAI/IAAI.
A. Mikheev. 1997. Automatic rule induction for un-
known word guessing. Computational Linguistics,
23(3):405?423.
Matthew Purver, Jonathan Ginzburg, and Patrick Healey.
2001. On the means for clarification in dialogue. In
Proceedings of the 2nd ACL SIGdial Workshop on Dis-
course and Dialogue, pages 116?125. Association for
Computational Linguistics, September.
Matthew Purver, Jonathan Ginzburg, and Patrick Healey.
2002. On the means for clarification in dialogue.
In R. Smith and J. van Kuppevelt, editors, Current
and New Directions in Discourse & Dialogue. Kluwer
Academic Publishers.
Matthew Purver. 2002. Processing unknown words in a
dialogue system. In Proceedings of the 3rd ACL SIG-
dial Workshop on Discourse and Dialogue, pages 174?
183. Association for Computational Linguistics, July.
E. Schegloff. 1987. Some sources of misunderstanding
in talk-in-interaction. Linguistics, 25:201?218.
Figure 1: Chattool Client Interface
Subject A?s View Subject B?s View
A: Obviously the relatives
were coming around like
they do to see me
B: Obviously the relatives
were coming around like
they do to see me
Probe ? A: relatives?
Block B: Yeah just unts and uncles
Ack ? A: ah
A: yeah B: yeah
Figure 2: Story Telling Task Excerpt, Noun Clarification, Subjects 1 & 2
Subject A?s View Subject B?s View
A: so we agree B: so we agree
B: agree? ? Probe
A: yeah to chuck out Susie
derkins
Block
B: uh huh ? Ack
A: yes B: yes
Figure 3: Balloon Task Excerpt, Verb Clarification, Subjects 3 & 4
Incremental Parsing, or Incremental Grammar??
Matthew Purver? and Ruth Kempson?
Departments of ?Computer Science and ?Philosophy,
King?s College London, Strand,
London WC2R 2LS,
UK
{matthew.purver, ruth.kempson}@kcl.ac.uk
Abstract
Standard grammar formalisms are defined with-
out reflection of the incremental and serial na-
ture of language processing, and incremental-
ity must therefore be reflected by independently
defined parsing and/or generation techniques.
We argue that this leads to a poor setup for
modelling dialogue, with its rich speaker-hearer
interaction, and instead propose context-based
parsing and generation models defined in terms
of an inherently incremental grammar formal-
ism (Dynamic Syntax), which allow a straight-
forward model of otherwise problematic dia-
logue phenomena such as shared utterances, el-
lipsis and alignment.
1 Introduction
Despite increasing psycholinguistic evidence of
incrementality in language processing, both in
parsing (see e.g. (Crocker et al, 2000)) and in
production (Ferreira, 1996), there is almost uni-
versal agreement that this should not be re-
flected in grammar formalisms which constitute
the underlying model of language (for rare ex-
ceptions, see (Hausser, 1989; Kempson et al,
2001)). Constraint-based grammar formalisms
are accordingly defined neutrally between either
of these applications, with parsing/generation
systems (whether incremental or not) defined
as independent architectures manipulating the
same underlying system.1 Such assumptions
however lead to formal architectures that are
relatively poorly set up for modelling dialogue,
for they provide no basis for the very rich de-
gree of interaction between participants in dia-
logue. A common phenomenon in dialogue is
? Related papers from the point of view of generation
rather than parsing, and from the point of view of align-
ment rather than incrementality, are to be presented at
INLG?04 and Catalog?04 respectively.
1Authors vary as to the extent to which these archi-
tectures might be defined to be reversible. See (Neu-
mann, 1994).
that of shared utterances (Clark and Wilkes-
Gibbs, 1986), with exchange of roles of parser
and producer midway through an utterance:2
(1)
Daniel: Why don?t you stop mumbling
and
Marc: Speak proper like?
Daniel: speak proper?
(2) Ruth: What did Alex . . .Hugh: Design? A kaleidoscope.
Such utterances clearly show the need for a
strictly incremental model: however, they are
particularly problematic for any overall archi-
tecture in which parsing and generation are in-
dependently defined as applications of a use-
neutral grammar formalism which yields as out-
put the set of well-formed strings, for in these
types of exchange, the string uttered by one and
parsed by the other need not be a wellformed
string in its own right, so will not fall within
the set of data which the underlying formalism
is set up to capture. Yet, with the transition be-
tween interlocutors seen as a shift from one sys-
tem to another, each such substring will have to
be characterised independently. Many other di-
alogue phenomena also show the need for inter-
action between the parsing and generation pro-
cesses, among them cross-speaker ellipsis (e.g.
simple bare fragment answers to wh-questions),
and alignment (Pickering and Garrod, 2004), in
which conversational participants mirror each
other?s patterns at many levels (including lexi-
cal choice and syntactic structure).
The challenge of being able to model these
phenomena, problematic for theorists but ex-
tremely easy and natural for dialogue partici-
pants themselves, has recently been put out by
Pickering and Garrod (2004) as a means of eval-
uating both putative grammar formalisms and
2Example (1) from the BNC, file KNY (sentences
315?317).
models of language use. In response to this chal-
lenge, we suggest that an alternative means of
evaluating parsing implementations is by eval-
uation of paired parsing-generation models and
the dialogue model that results. As an illustra-
tion of this, we show how if we drop the assump-
tion that grammar formalisms are defined neu-
trally between parsing and production in favour
of frameworks in which the serial nature of lan-
guage processing is a central design feature (as
in Dynamic Syntax: (Kempson et al, 2001)),
then we can define a model in which incremental
sub-systems of parsing and generation are nec-
essarily tightly coordinated, and can thus pro-
vide a computational model of dialogue which
directly corresponds with currently emerging
psycholinguistic results (Branigan et al, 2000).
In particular, by adding a shared model of con-
text to previously defined word-by-word incre-
mental parsing and generation models, we show
how the switch in speaker/hearer roles during
a shared utterance can be seen as a switch be-
tween processes which are directed by different
goals, but which share the same incrementally
built data structures. We then show how this
inherent incrementality and structure/context
sharing also allows a straightforward model of
cross-speaker ellipsis and alignment.
2 Background
Dynamic Syntax (DS) (Kempson et al, 2001) is
a parsing-directed grammar formalism in which
a decorated tree structure representing a se-
mantic interpretation for a string is incremen-
tally projected following the left-right sequence
of the words. Importantly, this tree is not a
model of syntactic structure, but is strictly se-
mantic, being a representation of the predicate-
argument structure of the sentence. In DS,
grammaticality is defined as parsability (the
successful incremental construction of a tree-
structure logical form, using all the information
given by the words in sequence): there is no cen-
tral use-neutral grammar of the kind assumed
by most approaches to parsing and/or gener-
ation. The logical forms are lambda terms of
the epsilon calculus (see (Meyer-Viol, 1995) for
a recent development), so quantification is ex-
pressed through terms of type e whose complex-
ity is reflected in evaluation procedures that ap-
ply to propositional formulae once constructed,
and not in the tree itself. With all quantification
expressed as type e terms, the standard grounds
for mismatch between syntactic and semantic
analysis for all NPs is removed; and, indeed, all
syntactic distributions are explained in terms of
this incremental and monotonic growth of par-
tial representations of content. Hence the claim
that the model itself constitutes a NL grammar
formalism.
Parsing (Kempson et al, 2001) defines pars-
ing as a process of building labelled semantic
trees in a strictly left-to-right, word-by-word in-
cremental fashion by using computational ac-
tions and lexical actions defined (for some natu-
ral language) using the modal tree logic LOFT
(Blackburn and Meyer-Viol, 1994). These ac-
tions are defined as transition functions be-
tween intermediate states, which monotonically
extend tree structures and node decorations.
Words are specified in the lexicon to have as-
sociated lexical actions: the (possibly partial)
semantic trees are monotonically extended by
applying these actions as each word is consumed
from the input string. Partial trees may be un-
derspecified: tree node relations may be only
partially specified; node decorations may be de-
fined in terms of unfulfilled requirements and
metavariables; and trees may lack a full set of
scope constraints. Anaphora resolution is a fa-
miliar case of update: pronouns are defined to
project metavariables that are substituted from
context as part of the construction process. Rel-
ative to the same tree-growth dynamics, long-
distance dependency effects are characterised
through restricted licensing of partial trees with
relation between nodes introduced with merely
a constraint on some fixed extension (following
D-Tree grammar formalisms (Marcus, 1987)),
an underspecification that gets resolved within
the left-to-right construction process.3 Quanti-
fying terms are also built up using determiner
and noun to yield a partially specified term e.g.
(, y, Man?(y)) with a requirement for a scope
statement. These scope statements, of the form
x < y (?the term binding x is to be evaluated
as taking scope over the term binding y?), are
added to a locally dominating type-t-requiring
node. Generally, they are added to an accu-
mulating set following the serial order of pro-
cessing in determining the scope dependency,
but indefinites (freer in scope potential) are as-
signed a metavariable as first argument, allow-
3In this, the system is also like LFG, modelling long-
distance dependency in the same terms as functional un-
certainty (Kaplan and Zaenen, 1989), differing from that
concept in the dynamics of update internal to the con-
struction of a single tree.
Figure 1: Parsing ?john likes mary? . . . . . . and generating ?john likes mary?
{}
{john?} {?}
john
{}
{john?} {}
{like?} {?}
likes
{like?(mary?)(john?),?}
{john?} {like?(mary?)}
{like?} {mary?}
mary
{}
{john?} {?}
FAIL FAIL
john
likes mary
{}
{john?} {}
{like?} {?}
FAIL
likes
mary
{like?(mary?)(john?),?}
{john?} {like?(mary?)}
{like?} {mary?}
mary
ing selection from any term already added, in-
cluding temporally-sorted variables associated
with tense/modality specifications. The gen-
eral mechanism is the incremental analogue of
quantifier storage; and once a propositional for-
mula of type t has been derived at a node with
some collection of scope statements, these are
jointly evaluated to yield fully expanded terms
that reflect all relative dependencies within the
restrictor of the terms themselves. For example,
parsing A man coughed yields the pair Si < x,
Cough?(, x, Man?(x)) (Si the index of evalua-
tion), then evaluated as Man?(a) ? Cough?(a)
where a = (, x, Man?(x) ? Cough?(x)).4
Once all requirements are satisfied and all
partiality and underspecification is resolved,
trees are complete, parsing is successful and the
input string is said to be grammatical. Central
to the formalism is the incremental and mono-
tonic growth of labelled partial trees: the parser
state at any point contains all the partial trees
which have been produced by the portion of the
string so far consumed and which remain can-
didates for completion.5
4For formal details of this approach to quantification
see (Kempson et al, 2001) chapter 7; for an early imple-
mentation see (Kibble et al, 2001).
5Figure 1 assumes, simplistically, that linguistic
names correspond directly to scopeless names in the log-
Generation (Otsuka and Purver, 2003;
Purver and Otsuka, 2003) (hereafter O&P)
give an initial method of context-independent
tactical generation based on the same incre-
mental parsing process, in which an output
string is produced according to an input
semantic tree, the goal tree. The generator
incrementally produces a set of corresponding
output strings and their associated partial trees
(again, on a left-to-right, word-by-word basis)
by following standard parsing routines and
using the goal tree as a subsumption check.
At each stage, partial strings and trees are
tentatively extended using some word/action
pair from the lexicon; only those candidates
which produce trees which subsume the goal
tree are kept, and the process succeeds when
a complete tree identical to the goal tree is
produced. Generation and parsing thus use
the same tree representations and tree-building
actions throughout.
3 Contextual Model
The current proposed model (and its imple-
mentation) is based on these earlier definitions
but modifies them in several ways, most signif-
icantly by the addition of a model of context:
ical language that decorate the tree.
while they assume some notion of context they
give no formal model or implementation.6 The
contextual model we now assume is made up not
only of the semantic trees built by the DS pars-
ing process, but also the sequences of words and
associated lexical actions that have been used
to build them. It is the presence of (and as-
sociations between) all three, together with the
fact that this context is equally available to both
parsing and generation processes, that allow our
straightforward model of dialogue phenomena.7
For the purposes of the current implementa-
tion, we make a simplifying assumption that
the length of context is finite and limited to the
result of some immediately previous parse (al-
though information that is independently avail-
able can be represented in the DS tree format,
so that, in reality, larger and only partially or-
dered contexts are no doubt possible): context
at any point is therefore made up of the trees
and word/action sequences obtained in parsing
the previous sentence and the current (incom-
plete) sentence.
Parsing in Context A parser state is there-
fore defined to be a set of triples ?T, W, A?,
where T is a (possibly partial) semantic tree,8
W the sequence of words and A the sequence
of lexical and computational actions that have
been used in building it. This set will initially
contain only a single triple ?Ta, ?, ?? (where Ta
is the basic axiom taken as the starting point of
the parser, and the word and action sequences
are empty), but will expand as words are con-
sumed from the input string and the corre-
sponding actions produce multiple possible par-
tial trees. At any point in the parsing process,
the context for a particular partial tree T in
6There are other departures in the treatment of linked
structures (for relatives and other modifiers) and quan-
tification, and more relevantly to improve the incremen-
tality of the generation process: we do not adopt the
proposal of O&P to speed up generation by use of a re-
stricted multiset of lexical entries selected on the basis
of goal tree features, which prevents strictly incremental
generation and excludes modification of the goal tree.
7In building n-tuples of trees corresponding to
predicate-argument structures, the system is similar to
LTAG formalisms (Joshi and Kulick, 1997). However,
unlike LTAG systems (see e.g. (Stone and Doran, 1997)),
both parsing and generation are not head-driven, but
fully (word-by-word) incremental. This has the ad-
vantage of allowing fully incremental models for all
languages, matching psycholinguistic observations (Fer-
reira, 1996).
8Strictly speaking, scope statements should be in-
cluded in these n-tuples ? for now we consider them as
part of the tree.
this set can then be taken to consist of: (a) a
similar triple ?T0, W0, A0? given by the previous
sentence, where T0 is its semantic tree repre-
sentation, W0 and A0 the sequences of words
and actions that were used in building it; and
(b) the triple ?T, W, A? itself. Once parsing is
complete, the final parser state, a set of triples,
will form the new starting context for the next
sentence. In the simple case where the sentence
is unambiguous (or all ambiguity has been re-
moved) this set will again have been reduced
to a single triple ?T1, W1, A1?, corresponding to
the final interpretation of the string T1 with its
sequence of words W1 and actions A1, and this
replaces ?T0, W0, A0? as the new context; in the
presence of persistent ambiguity there will sim-
ply be more than one triple in the new context.9
Generation in Context A generator state
is now defined as a pair (Tg, X) of a goal tree
Tg and a set X of pairs (S, P ), where S is a
candidate partial string and P is the associated
parser state (a set of ?T, W, A? triples). Ini-
tially, the set X will usually contain only one
pair, of an empty candidate string and the stan-
dard initial parser state, (?, {?Ta, ?, ??}). How-
ever, as both parsing and generation processes
are strictly incremental, they can in theory start
from any state. The context for any partial tree
T is defined exactly as for parsing: the previ-
ous sentence triple ?T0, W0, A0?; and the cur-
rent triple ?T, W, A?. Generation and parsing
are thus very closely coupled, with the central
part of both processes being a parser state: a set
of tree/word-sequence/action-sequence triples.
Essential to this correspondence is the lack of
construction of higher-level hypotheses about
the state of the interlocutor. All transitions
are defined over the context for the individ-
ual (parser or generator). In principle, con-
texts could be extended to include high-level
hypotheses, but these are not essential and are
not implemented in our model (see (Millikan,
2004) for justification of this stance).
4 Shared Utterances
One primary evidence for this close coupling
and sharing of structures and context is the ease
with which shared utterances can be expressed.
O&P suggest an analysis of shared utterances,
9The current implementation of the formalism does
not include any disambiguation mechanism. We simply
assume that selection of some (minimal) context and at-
tendant removal of any remaining ambiguity is possible
by inference.
Figure 2: Transition from hearer to speaker: ?What did Alex . . . / . . . design??
Pt =
?
{+Q}
{WH} {alex?}{?Ty(e ? t),?}
, {what, did, alex}, {a1, a2, a3}
?
Gt =
(
{+Q, design?(WH)(alex?)}
{alex?} {design(WH)}
{WH}{design?}
,
(
?,
?
{+Q}
{WH} {alex?}{?Ty(e ? t),?}
, {what, did, alex}, {a1, a2, a3}
?)
)
G1 =
(
{+Q, design?(WH)(alex?)}
{alex?} {design?(WH)}
{WH}{design?}
,
(
{design},
?
{+Q}
{WH}{alex?} {?Ty(e ? t)}
{?}{design?}
, {. . . , design}, {. . . , a4}
?)
)
and this can now be formalised given the cur-
rent model. As the parsing and generation pro-
cesses are both fully incremental, they can start
from any state (not just the basic axiom state
?Ta, ?, ??). As they share the same lexical en-
tries, the same context and the same semantic
tree representations, a model of the switch of
roles now becomes relatively straightforward.
Transition from Hearer to Speaker Nor-
mally, the generation process begins with
the initial generator state as defined above:
(Tg, {(?, P0)}), where P0 is the standard initial
?empty? parser state {?Ta, ?, ??}. As long as a
suitable goal tree Tg is available to guide gen-
eration, the only change required to generate a
continuation from a heard partial string is to
replace P0 with the parser state (a set of triples
?T, W, A?) as produced from that partial string:
we call this the transition state Pt. The initial
hearer A therefore parses as usual until transi-
tion,10 then given a suitable goal tree Tg, forms
a transition generator state Gt = (Tg, {(?, Pt)}),
from which generation can begin directly ? see
figure 2.11 Note that the context does not
change between processes.
For generation to begin from this transition
state, the new goal tree Tg must be subsumed
by at least one of the partial trees in Pt (i.e.
the proposition to be expressed must be sub-
sumed by the incomplete proposition that has
been built so far by the parser). Constructing
10We have little to say about exactly when transitions
occur. Presumably speaker pauses and the availability
to the hearer of a possible goal tree both play a part.
11Figure 2 contains several simplifications to aid read-
ability, both to tree structure details and by show-
ing parser/generator states as single triples/pairs rather
than sets thereof.
Tg prior to the generation task will often be a
complex process involving inference and/or ab-
duction over context and world/domain knowl-
edge ? Poesio and Rieser (2003) give some idea
as to how this inference might be possible ? for
now, we make the simplifying assumption that
a suitable propositional structure is available.
Transition from Speaker to Hearer At
transition, the initial speaker B?s generator
state G?t contains the pair (St, P ?t), where St is
the partial string output so far, and P ?t is the
corresponding parser state, the transition state
as far as B is concerned.12 In order for B to
interpret A?s continuation, B need only use P ?t
as the initial parser state which is extended as
the string produced by A is consumed.
As there will usually be multiple possible par-
tial trees at the transition point, A may con-
tinue in a way that does not correspond to B?s
initial intentions ? i.e. in a way that does not
match B?s initial goal tree. For B to be able
to understand such continuations, the genera-
tion process must preserve all possible partial
parse trees (just as the parsing process does),
whether they subsume the goal tree or not, as
long as at least one tree in the current state does
subsume the goal tree. A generator state must
therefore rule out only pairs (S, P ) for which P
contains no trees which subsume the goal tree,
rather than thinning the set P directly via the
subsumption check as proposed by O&P.
It is the incrementality of the underlying
grammar formalism that allows this simple
switch: the parsing process can begin directly
12Of course, if both A and B share the same lexical
entries and communication is perfect, Pt = P ?t , but we
do not have to assume that this is the case.
from a state produced by an incomplete gener-
ation process, and vice versa, as their interme-
diate representations are necessarily the same.
5 Cross-Speaker Ellipsis
This inherent close coupling of the two incre-
mental processes, together with the inclusion
of tree-building actions in the model of con-
text, also allows a simple analysis of many cross-
speaker elliptical phenomena.
Fragments Bare fragments (3) may be anal-
ysed as taking a previous structure from con-
text as a starting point for parsing (or genera-
tion). WH -expressions are analysed as partic-
ular forms of metavariables, so parsing a wh-
question yields a type-complete but open for-
mula, which the term presented by a subsequent
fragment can update:
(3) A: What did you eat for breakfast?B: Porridge.
Parsing the fragment involves constructing an
unfixed node, and merging it with the contex-
tually available structure, so characterising the
wellformedness/interpretation of fragment an-
swers to questions without any additional mech-
anisms: the term (, x, porridge?(x)) stands in a
licensed growth relation from the metavariable
WH provided by the lexical actions of what.
Functional questions (Ginzburg and Sag,
2000) with their fragment answers (4) pose
no problem. As the wh-question contains a
metavariable, the scope evaluation cannot be
completed; completion of structure and evalu-
ation of scope can then be effected by merg-
ing in the term the answer provides, identifying
any introduced metavariable in this context (the
genitive imposes narrow scope of the introduced
epsilon term):
(4) A: Who did every student ignore?B: Their supervisor.
{Si < x}
{(?, x, stud?(x))} {}
{WH,?} {ignr?}
? {Si < x, x < y}
{(?, x, stud?(x))} {}
{(, y, sup?(x)(y)}{ignr?}
VP Ellipsis Anaphoric devices such as pro-
nouns and VP ellipsis are analysed as decorating
tree nodes with metavariables licensing update
from context using either established terms, or,
for ellipsis, (lexical) tree-update actions. Strict
readings of VP ellipsis result from taking a suit-
able semantic formula directly from a tree node
in context: any node n ? (T0 ? T ) of suitable
type (e ? t) with no outstanding requirements.
Sloppy readings involve re-use of actions: any
sequence of actions (a1; a2; . . . ; an) ? (A0 ? A)
can be used (given the appropriate elliptical
trigger) to extend the current tree T if this pro-
vides a formula of type e ? t.13 This latter
approach, combined with the representation of
quantified elements as terms, allows a range of
phenomena, including those which are problem-
atic for other (abstraction-based) approaches
(for discussion see (Dalrymple et al, 1991)):
(5)
A: A policeman who arrested Bill read
him his rights.
B: The policeman who arrested Tom
did too.
The actions associated with A?s use of read
him his rights in (5) include the projection of a
metavariable associated with him, and its res-
olution to the term in context associated with
Bill. B?s ellipsis allows this action sequence to
be re-used, again projecting a metavariable and
resolving it, this time (given the new context) to
the term provided by parsing Tom. This leads
to a copy of Tom within the constructed predi-
cate, and a sloppy reading.
This analysis also applies to yield parallellism
effects in scoping (Hirschbu?hler, 1982; Shieber
et al, 1996), allowing narrow scope construal
for indefinites in subject position:
(6) A: A nurse interviewed every patient.B: An orderly did too.
Resolution of the underspecification in the
scope statement associated with an indefinite
can be performed at two points: either at the
immediate point of processing the lexical ac-
tions, or at the later point of compiling the re-
sulting node?s interpretation within the emer-
gent tree.14 In (6), narrow scope can be as-
signed to the subject in A?s utterance via this
late assignment of scope; at this late point in the
13In its re-use of actions provided by context, this ap-
proach to ellipsis is essentially similar to the glue lan-
guage approach (see (Asudeh and Crouch, 2002) and
papers in (Dalrymple, 1999) but, given the lack of in-
dependent syntax /semantics vocabularies, the need for
an intermediate mapping language is removed.
14This pattern parallels expletive pronouns which
equally allow a delayed update (Cann, 2003).
parse process, the term constructed from the ob-
ject node will have been entered into the set of
scope statements, allowing the subject node to
be dependent on the following quantified expres-
sion. The elliptical word did in B?s utterance
will then license re-use of these late actions, re-
peating the procedures used in interpreting A?s
antecedent and so determining scope of the new
subject relative to the object.
Again, these analyses are possible because
parsing and generation processes share incre-
mentally built structures and contextual pars-
ing actions, with this being ensured by the in-
crementality of the grammar formalism itself.
6 Alignment & Routinization
The parsing and generation processes must both
search the lexicon for suitable entries at ev-
ery step (i.e. when parsing or generating each
word). For generation in particular, this is a
computationally expensive process in principle:
every possible word/action pair must be tested ?
the current partial tree extended and the result
checked for goal tree subsumption. As proposed
by O&P (though without formal definitions or
implementation) our model of context now al-
lows a strategy for minimising this effort: as
it includes previously used words and actions,
a subset of such actions can be re-used in ex-
tending the current tree, avoiding full lexical
search altogether. High frequency of elliptical
constructions is therefore expected, as ellipsis
licenses such re-use; the same can be said for
pronouns, as long as they (and their correspond-
ing actions) are assumed to be pre-activated or
otherwise readily available from the lexicon.
As suggested by O&P, this can now lead di-
rectly to a model of alignment phenomena, char-
acterisable as follows. For the generator, if there
is some action a ? (A0 ?A) suitable for extend-
ing the current tree, a can be re-used, generat-
ing the word w which occupies the correspond-
ing position in the sequence W0 or W . This re-
sults in lexical alignment ? repeating w rather
than choosing an alternative word from the lex-
icon. Alignment of syntactic structure (e.g. pre-
serving double-object or full PP forms in the use
of a verb such as give rather than shifting to
the semantically equivalent form (Branigan et
al., 2000)) also follows in virtue of the procedu-
ral action-based specification of lexical content.
A word such as give has two possible lexical
actions a? and a?? despite semantic equivalence
of output, corresponding to the two alternative
forms. A previous use will cause either a? or a??
to be present in (A0 ? A); re-use of this action
will cause the same form to be repeated.15
A similar definition holds for the parser: for a
word w presented as input, if w ? (W0?W ) then
the corresponding action a in the sequence A0 or
A can be used without consulting the lexicon.
Words will therefore be interpreted as having
the same sense or reference as before, modelling
the semantic alignment described by (Garrod
and Anderson, 1987). These characterisations
can also be extended to sequences of words ?
a sub-sequence (a1; a2; . . . ; an) ? (A0 ? A) can
be re-used by a generator, producing the cor-
responding word sequence (w1; w2; . . . ; wn) ?
(W0 ? W ); and similarly the sub-sequence of
words (w1; w2; . . . ; wn) ? (W0 ? W ) will cause
the parser to use the corresponding action se-
quence (a1; a2; . . . ; an) ? (A0 ? A). This will
result in sequences or phrases being repeatedly
associated by both parser and generator with
the same sense or reference, leading to what
Pickering and Garrod (2004) call routinization
(construction and re-use of word sequences with
consistent meanings).
It is notable that these various patterns of
alignment, said by Pickering and Garrod (2004)
to be alignment across different levels, are ex-
pressible without invoking distinct levels of syn-
tactic or lexical structure, since context, content
and lexical actions are all defined in terms of the
same tree configurations.
7 Summary
The inherent left-to-right incrementality and
monotonicity of DS as a grammar formalism al-
lows both parsing and generation processes to
be not only incremental but closely coupled,
sharing structures and context. This enables
shared utterances, cross-speaker elliptical phe-
nomena and alignment to be modelled straight-
forwardly. A prototype system has been im-
plemented in Prolog which reflects the model
given here, demonstrating shared utterances
and alignment phenomena in simple dialogue
sequences. The significance of this direct re-
flection of psycholinguistic data is to buttress
the DS claim that the strictly serial incremen-
tality of processing is not merely essential to
the modelling of natural-language parsing, but
15Most frameworks would have to reflect this via pref-
erences defined over syntactic rules or parallelisms with
syntactic trees in context, both problematic.
to the design of the underlying grammar formal-
ism itself.
Acknowledgements
This paper is an extension of joint work on the
DS framework with Wilfried Meyer-Viol, on ex-
pletives and on defining a context-dependent
formalism with Ronnie Cann, and on DS gen-
eration with Masayuki Otsuka. Each has pro-
vided ideas and input without which the cur-
rent results would have differed, although any
mistakes here are ours. Thanks are also due to
the ACL reviewers. This work was supported
by the ESRC (RES-000-22-0355) and (for the
second author) by the Leverhulme Trust.
References
A. Asudeh and R. Crouch. 2002. Derivational
parallelism and ellipsis parallelism. In Pro-
ceedings of WCCFL 21.
P. Blackburn and W. Meyer-Viol. 1994. Lin-
guistics, logic and finite trees. Bulletin of the
IGPL, 2:3?31.
H. Branigan, M. Pickering, and A. Cleland.
2000. Syntactic co-ordination in dialogue.
Cognition, 75:13?25.
R. Cann. 2003. Semantic underspecification
and the interpretation of copular clauses in
English. In Where Semantics Meets Pragmat-
ics. University of Michigan.
H. H. Clark and D. Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition,
22:1?39.
M. Crocker, M. Pickering, and C. Clifton, ed-
itors. 2000. Architectures and Mechanisms
in Sentence Comprehension. Cambridge Uni-
versity Press.
M. Dalrymple, S. Shieber, and F. Pereira. 1991.
Ellipsis and higher-order unification. Linguis-
tics and Philosophy, 14(4):399?452.
M. Dalrymple, editor. 1999. Syntax and Se-
mantics in Lexical Functional Grammar: The
Resource-Logic Approach. MIT Press.
V. Ferreira. 1996. Is it better to give than to
donate? Syntactic flexibility in language pro-
duction. Journal of Memory and Language,
35:724?755.
S. Garrod and A. Anderson. 1987. Saying what
you mean in dialogue. Cognition, 27:181?218.
J. Ginzburg and I. A. Sag. 2000. Interrogative
Investigations. CSLI Publications.
R. Hausser. 1989. Computation of Language.
Springer-Verlag.
P. Hirschbu?hler. 1982. VP deletion and across-
the-board quantifier scope. In Proceedings of
NELS 12.
A. Joshi and S. Kulick. 1997. Partial proof trees
as building blocks for a categorial grammar.
Linguistics and Philosophy, 20:637?667.
R. Kaplan and A. Zaenen. 1989. Long-
distance dependencies, constituent structure,
and functional uncertainty. In M. Baltin and
A. Kroch, editors, Alternative Conceptions of
Phrase Structure, pages 17?42. University of
Chicago Press.
R. Kempson, W. Meyer-Viol, and D. Gabbay.
2001. Dynamic Syntax: The Flow of Lan-
guage Understanding. Blackwell.
R. Kibble, W. Meyer-Viol, D. Gabbay, and
R. Kempson. 2001. Epsilon terms: a la-
belled deduction account. In H. Bunt and
R. Muskens, editors, Computing Meaning.
Kluwer Academic Publishers.
M. Marcus. 1987. Deterministic parsing and
description theory. In P. Whitelock et al, ed-
itor, Linguistic Theory and Computer Appli-
cations, pages 69?112. Academic Press.
W. Meyer-Viol. 1995. Instantial Logic. Ph.D.
thesis, University of Utrecht.
R. Millikan. 2004. The Varieties of Meaning.
MIT Press.
G. Neumann. 1994. A Uniform Computa-
tional Model for Natural Language Parsing
and Generation. Ph.D. thesis, Universita?t des
Saarlandes.
M. Otsuka and M. Purver. 2003. Incremental
generation by incremental parsing. In Pro-
ceedings of the 6th CLUK Colloquium.
M. Pickering and S. Garrod. 2004. Toward a
mechanistic psychology of dialogue. Behav-
ioral and Brain Sciences, forthcoming.
M. Poesio and H. Rieser. 2003. Coordination in
a PTT approach to dialogue. In Proceedings
of the 7th Workshop on the Semantics and
Pragmatics of Dialogue (DiaBruck).
M. Purver and M. Otsuka. 2003. Incremental
generation by incremental parsing: Tactical
generation in Dynamic Syntax. In Proceed-
ings of the 9th European Workshop in Natural
Language Generation (ENLG-2003).
S. Shieber, F. Pereira, and M. Dalrymple. 1996.
Interactions of scope and ellipsis. Linguistics
and Philosophy, 19:527?552.
M. Stone and C. Doran. 1997. Sentence plan-
ning as description using tree-adjoining gram-
mar. In Proceedings of the 35th Annual Meet-
ing of the ACL, pages 198?205.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 78?89,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Strongly Incremental Repair Detection
Julian Hough1,2
1Dialogue Systems Group
Faculty of Linguistics
and Literature
Bielefeld University
julian.hough@uni-bielefeld.de
Matthew Purver2
2Cognitive Science Research Group
School of Electronic Engineering
and Computer Science
Queen Mary University of London
m.purver@qmul.ac.uk
Abstract
We present STIR (STrongly Incremen-
tal Repair detection), a system that de-
tects speech repairs and edit terms on
transcripts incrementally with minimal la-
tency. STIR uses information-theoretic
measures from n-gram models as its prin-
cipal decision features in a pipeline of
classifiers detecting the different stages of
repairs. Results on the Switchboard dis-
fluency tagged corpus show utterance-final
accuracy on a par with state-of-the-art in-
cremental repair detection methods, but
with better incremental accuracy, faster
time-to-detection and less computational
overhead. We evaluate its performance us-
ing incremental metrics and propose new
repair processing evaluation standards.
1 Introduction
Self-repairs in spontaneous speech are annotated
according to a well established three-phase struc-
ture from (Shriberg, 1994) onwards, and as de-
scribed in Meteer et al. (1995)?s Switchboard cor-
pus annotation handbook:
John [ likes
? ?? ?
reparandum
+ {uh}
? ?? ?
interregnum
loves ]
? ?? ?
repair
Mary (1)
From a dialogue systems perspective, detecting re-
pairs and assigning them the appropriate structure
is vital for robust natural language understanding
(NLU) in interactive systems. Downgrading the
commitment of reparandum phases and assigning
appropriate interregnum and repair phases permits
computation of the user?s intended meaning.
Furthermore, the recent focus on incremental
dialogue systems (see e.g. (Rieser and Schlangen,
2011)) means that repair detection should oper-
ate without unnecessary processing overhead, and
function efficiently within an incremental frame-
work. However, such left-to-right operability on
its own is not sufficient: in line with the princi-
ple of strong incremental interpretation (Milward,
1991), a repair detector should give the best re-
sults possible as early as possible. With one ex-
ception (Zwarts et al., 2010), there has been no
focus on evaluating or improving the incremental
performance of repair detection.
In this paper we present STIR (Strongly In-
cremental Repair detection), a system which ad-
dresses the challenges of incremental accuracy,
computational complexity and latency in self-
repair detection, by making local decisions based
on relatively simple measures of fluency and sim-
ilarity. Section 2 reviews state-of-the-art methods;
Section 3 summarizes the challenges and explains
our general approach; Section 4 explains STIR in
detail; Section 5 explains our experimental set-up
and novel evaluation metrics; Section 6 presents
and discusses our results and Section 7 concludes.
2 Previous work
Qian and Liu (2013) achieve the state of the art in
Switchboard corpus self-repair detection, with an
F-score for detecting reparandum words of 0.841
using a three-step weighted Max-Margin Markov
network approach. Similarly, Georgila (2009)
uses Integer Linear Programming post-processing
of a CRF to achieve F-scores over 0.8 for reparan-
dum start and repair start detection. However nei-
ther approach can operate incrementally.
Recently, there has been increased interest
in left-to-right repair detection: Rasooli and
Tetreault (2014) and Honnibal and Johnson (2014)
present dependency parsing systems with reparan-
dum detection which perform similarly, the latter
equalling Qian and Liu (2013)?s F-score at 0.841.
However, while operating left-to-right, these sys-
tems are not designed or evaluated for their incre-
mental performance. The use of beam search over
78
different repair hypotheses in (Honnibal and John-
son, 2014) is likely to lead to unstable repair label
sequences, and they report repair hypothesis ?jit-
ter?. Both of these systems use a non-monotonic
dependency parsing approach that immediately re-
moves the reparandum from the linguistic anal-
ysis of the utterance in terms of its dependency
structure and repair-reparandum correspondence,
which from a downstream NLU module?s perspec-
tive is undesirable. Heeman and Allen (1999) and
Miller and Schuler (2008) present earlier left-to-
right operational detectors which are less accu-
rate and again give no indication of the incremen-
tal performance of their systems. While Heeman
and Allen (1999) rely on repair structure template
detection coupled with a multi-knowledge-source
language model, the rarity of the tail of repair
structures is likely to be the reason for lower per-
formance: Hough and Purver (2013) show that
only 39% of repair alignment structures appear
at least twice in Switchboard, supported by the
29% reported by Heeman and Allen (1999) on
the smaller TRAINS corpus. Miller and Schuler
(2008)?s encoding of repairs into a grammar also
causes sparsity in training: repair is a general pro-
cessing strategy not restricted to certain lexical
items or POS tag sequences.
The model we consider most suitable for in-
cremental dialogue systems so far is Zwarts et
al. (2010)?s incremental version of Johnson and
Charniak (2004)?s noisy channel repair detector,
as it incrementally applies structural repair anal-
yses (rather than just identifying reparanda) and
is evaluated for its incremental properties. Fol-
lowing (Johnson and Charniak, 2004), their sys-
tem uses an n-gram language model trained on
roughly 100K utterances of reparandum-excised
(?cleaned?) Switchboard data. Its channel model is
a statistically-trained S-TAG parser whose gram-
mar has simple reparandum-repair alignment rule
categories for its non-terminals (copy, delete, in-
sert, substitute) and words for its terminals. The
parser hypothesises all possible repair structures
for the string consumed so far in a chart, before
pruning the unlikely ones. It performs equally
well to the non-incremental model by the end of
each utterance (F-score = 0.778), and can make
detections early via the addition of a speculative
next-word repair completion category to their S-
TAG non-terminals. In terms of incremental per-
formance, they report the novel evaluation met-
ric of time-to-detection for correctly identified re-
pairs, achieving an average of 7.5 words from the
start of the reparandum and 4.6 from the start of
the repair phase. They also introduce delayed ac-
curacy, a word-by-word evaluation against gold-
standard disfluency tags up to the word before the
current word being consumed (in their terms, the
prefix boundary), giving a measure of the stability
of the repair hypotheses. They report an F-score
of 0.578 at one word back from the current prefix
boundary, increasing word-by-word until 6 words
back where it reaches 0.770. These results are the
point-of-departure for our work.
3 Challenges and Approach
In this section we summarize the challenges for
incremental repair detection: computational com-
plexity, repair hypothesis stability, latency of de-
tection and repair structure identification. In 3.1
we explain how we address these.
Computational complexity Approaches to de-
tecting repair structures often use chart storage
(Zwarts et al., 2010; Johnson and Charniak, 2004;
Heeman and Allen, 1999), which poses a com-
putational overhead: if considering all possible
boundary points for a repair structure?s 3 phases
beginning on any word, for prefixes of length n
the number of hypotheses can grow in the order
O(n
4
). Exploring a subset of this space is nec-
essary for assigning entire repair structures as in
(1) above, rather than just detecting reparanda:
the (Johnson and Charniak, 2004; Zwarts et al.,
2010) noisy-channel detector is the only system
that applies such structures but the potential run-
time complexity in decoding these with their S-
TAG repair parser is O(n5). In their approach,
complexity is mitigated by imposing a maximum
repair length (12 words), and also by using beam
search with re-ranking (Lease et al., 2006; Zwarts
and Johnson, 2011). If we wish to include full
decoding of the repair?s structure (as argued by
Hough and Purver (2013) as necessary for full in-
terpretation) whilst taking a strictly incremental
and time-critical perspective, reducing this com-
plexity by minimizing the size of this search space
is crucial.
Stability of repair hypotheses and latency Us-
ing a beam search of n-best hypotheses on a word-
by-word basis can cause ?jitter? in the detector?s
output. While utterance-final accuracy is desired,
79
for a truly incremental system good intermedi-
ate results are equally important. Zwarts et al.
(2010)?s time-to-detection results show their sys-
tem is only certain about a detection after process-
ing the entire repair. This may be due to the string
alignment-inspired S-TAG that matches repair and
reparanda: a ?rough copy? dependency only be-
comes likely once the entire repair has been con-
sumed. The latency of 4.6 words to detection and
a relatively slow rise to utterance-final accuracy up
to 6 words back is undesirable given repairs have
a mean reparandum length of ?1.5 words (Hough
and Purver, 2013; Shriberg and Stolcke, 1998).
Structural identification Classifying repairs
has been ignored in repair processing, despite the
presence of distinct categories (e.g. repeats, sub-
stitutions, deletes) with different pragmatic effects
(Hough and Purver, 2013).1 This is perhaps due to
lack of clarity in definition: even for human anno-
tators, verbatim repeats withstanding, agreement
is often poor (Hough and Purver, 2013; Shriberg,
1994). Assigning and evaluating repair (not just
reparandum) structures will allow repair interpre-
tation in future; however, work to date evaluates
only reparandum detection.
3.1 Our approach
To address the above, we propose an alternative
to (Johnson and Charniak, 2004; Zwarts et al.,
2010)?s noisy channel model. While the model
elegantly captures intuitions about parallelism in
repairs and modelling fluency, it relies on string-
matching, motivated in a similar way to automatic
spelling correction (Brill and Moore, 2000): it as-
sumes a speaker chooses to utter fluent utterance
X according to some prior distribution P (X), but
a noisy channel causes them instead to utter a
noisy Y according to channel model P (Y |X).
Estimating P (Y |X) directly from observed data
is difficult due to sparsity of repair instances, so a
transducer is trained on the rough copy alignments
between reparandum and repair. This approach
succeeds because repetition and simple substitu-
tion repairs are very common; but repair as a psy-
chological process is not driven by string align-
ment, and deletes, restarts and rarer substitution
forms are not captured. Furthermore, the noisy
channel model assumes an inherently utterance-
global process for generating (and therefore find-
1Though see (Germesin et al., 2008) for one approach,
albeit using idiosyncratic repair categories.
ing) an underlying ?clean? string ? much as sim-
ilar spelling correction models are word-global ?
we instead take a very local perspective here.
In accordance with psycholinguistic evidence
(Brennan and Schober, 2001), we assume charac-
teristics of the repair onset allow hearers to detect
it very quickly and solve the continuation prob-
lem (Levelt, 1983) of integrating the repair into
their linguistic context immediately, before pro-
cessing or even hearing the end of the repair phase.
While repair onsets may take the form of inter-
regna, this is not a reliable signal, occurring in
only ?15% of repairs (Hough and Purver, 2013;
Heeman and Allen, 1999). Our repair onset de-
tection is therefore driven by departures from flu-
ency, via information-theoretic features derived
incrementally from a language model in line with
recent psycholinguistic accounts of incremental
parsing ? see (Keller, 2004; Jaeger and Tily, 2011).
Considering the time-linear way a repair is pro-
cessed and the fact speakers are exponentially less
likely to trace one word further back in repair as
utterance length increases (Shriberg and Stolcke,
1998), backwards search seems to be the most ef-
ficient reparandum extent detection method.2 Fea-
tures determining the detection of the reparan-
dum extent in the backwards search can also be
information-theoretic: entropy measures of dis-
tributional parallelism can characterize not only
rough copy dependencies, but distributionally sim-
ilar or dissimilar correspondences between se-
quences. Finally, when detecting the repair end
and structure, distributional information allows
computation of the similarity between reparan-
dum and repair. We argue a local-detection-
with-backtracking approach is more cognitively
plausible than string-based left-to-right repair la-
belling, and using this insight should allow an im-
provement in incremental accuracy, stability and
time-to-detection over string-alignment driven ap-
proaches in repair detection.
4 STIR: Strongly Incremental Repair
detection
Our system, STIR (Strongly Incremental Repair
detection), therefore takes a local incremental ap-
2We acknowledge a purely position-based model for
reparandum extent detection under-estimates prepositions,
which speakers favour as the retrace start and over-estimates
verbs, which speakers tend to avoid retracing back to, prefer-
ring to begin the utterance again, as (Healey et al., 2011)?s
experiments also demonstrate.
80
?John? ?likes?
S
0
S
1
S
2
T0
?John? ?likes? ?uh?
ed
S
0
S
1
S
2
S
3
ed
T1
?John? ?likes? ?uh?
ed
?loves?
rp
start
S
0
S
1
S
2
S
3
ed
?
S
4
rp
start
T2
?John? ?likes?
rm
start
rm
end
?uh?
ed
?loves?
rp
start
S
0
S
1
S
2
rm
start
rm
end
S
3
ed
S
4
rp
start
T3
?John? ?likes?
rm
start
rm
end
?uh?
ed
?loves?
rp
start
rp
sub
end
S
0
S
1
S
2
rm
start
rm
end
S
3
ed
S
4
rp
start
rp
sub
end
T4
?John? ?likes?
rm
start
rm
end
?uh?
ed
?loves?
rp
start
rp
sub
end
?Mary?
S
0
S
1
S
2
rm
start
rm
end
S
3
ed
S
4
rp
start
rp
sub
end
S
5
T5
Figure 1: Strongly Incremental Repair Detection
proach to detecting repairs and isolated edit terms,
assigning words the structures in (2). We in-
clude interregnum recognition in the process, due
to the inclusion of interregnum vocabulary within
edit term vocabulary (Ginzburg, 2012; Hough and
Purver, 2013), a useful feature for repair detection
(Lease et al., 2006; Qian and Liu, 2013).
{
...[rm
start
...rm
end
+ {ed}rp
start
...rp
end
]...
...{ed}...
(2)
Rather than detecting the repair structure in its
left-to-right string order as above, STIR functions
as in Figure 1: first detecting edit terms (possibly
interregna) at step T1; then detecting repair onsets
rp
start
at T2; if one is found, backwards searching
to find rm
start
at T3; then finally finding the re-
pair end rp
end
at T4. Step T1 relies mainly on
lexical probabilities from an edit term language
model; T2 exploits features of divergence from a
fluent language model; T3 uses fluency of hypoth-
esised repairs; and T4 the similarity between dis-
tributions after reparandum and repair. However,
each stage integrates these basic insights via mul-
tiple related features in a statistical classifier.
4.1 Enriched incremental language models
We derive the basic information-theoretic features
required using n-gram language models, as they
have a long history of information theoretic anal-
ysis (Shannon, 1948) and provide reproducible re-
sults without forcing commitment to one partic-
ular grammar formalism. Following recent work
on modelling grammaticality judgements (Clark
et al., 2013), we implement several modifications
to standard language models to develop our basic
measures of fluency and uncertainty.
For our main fluent language models we train
a trigram model with Kneser-Ney smoothing
(Kneser and Ney, 1995) on the words and POS
tags of the standard Switchboard training data
(all files with conversation numbers beginning
sw2*,sw3* in the Penn Treebank III release), con-
sisting of ?100K utterances, ?600K words. We
follow (Johnson and Charniak, 2004) by clean-
ing the data of disfluencies (i.e. edit terms and
reparanda), to approximate a ?fluent? language
model. We call these probabilities plex
kn
, p
pos
kn
be-
low.3
3We suppress the pos and lex superscripts below where we
refer to measures from either model.
81
We then derive surprisal as our principal default
lexical uncertainty measurement s (equation 3) in
both models; and, following (Clark et al., 2013),
the (unigram) Weighted Mean Log trigram prob-
ability (WML, eq. 4)? the trigram logprob of the
sequence divided by the inverse summed logprob
of the component unigrams (apart from the first
two words in the sequence, which serve as the
first trigram history). As here we use a local ap-
proach we restrict the WML measures to single
trigrams (weighted by the inverse logprob of the
final word). While use of standard n-gram prob-
ability conflates syntactic with lexical probability,
WML gives us an approximation to incremental
syntactic probability by factoring out lexical fre-
quency.
s(w
i?2
. . . w
i
) = ? log
2
p
kn
(w
i
| w
i?2
, w
i?1
) (3)
WML(w
0
. . . w
n
) =
?
i=n
i=2
log
2
p
kn
(w
i
| w
i?2
, w
i?1
)
?
?
n
j=2
log
2
p
kn
(w
j
)
(4)
Distributional measures To approximate un-
certainty, we also derive the entropy H(w | c) of
the possible word continuations w given a context
c, from p(w
i
| c) for all words w
i
in the vocabu-
lary ? see (5). Calculating distributions over the
entire lexicon incrementally is costly, so we ap-
proximate this by constraining the calculation to
words which are observed at least once in context
c in training, w
c
= {w|count(c, w) ? 1} , assum-
ing a uniform distribution over the unseen suffixes
by using the appropriate smoothing constant, and
subtracting the latter from the former ? see eq. (6).
Manual inspection showed this approximation
to be very close, and the trie structure of our n-
gram models allows efficient calculation. We also
make use of the Zipfian distribution of n-grams
in corpora by storing entropy values for the 20%
most common trigram contexts observed in train-
ing, leaving entropy values of rare or unseen con-
texts to be computed at decoding time with little
search cost due to their small or empty w
c
sets.
H(w | c) = ?
?
w?V ocab
p
kn
(w | c) log
2
p
kn
(w | c) (5)
H(w | c) ?
[
?
?
w?w
c
p
kn
(w | c) log
2
p
kn
(w | c)
]
? [n? ? log
2
?]
where n = |V ocab| ? |w
c
|
and ? =
1?
?
w?w
c
p
kn
(w | c)
n
(6)
Given entropy estimates, we can also sim-
ilarly approximate the Kullback-Leibler (KL)
divergence (relative entropy) between distribu-
tions in two different contexts c
1
and c
2
, i.e.
?(w|c
1
) and ?(w|c
2
), by pair-wise computing
p(w|c
1
) log
2
(
p(w|c
1
)
p(w|c
2
)
) only for words w ? w
c
1
?
w
c
2
, then approximating unseen values by assum-
ing uniform distributions. Using p
kn
smoothed es-
timates rather than raw maximum likelihood es-
timations avoids infinite KL divergence values.
Again, we found this approximation sufficiently
close to the real values for our purposes. All such
probability and distribution values are stored in
incrementally constructed directed acyclic graph
(DAG) structures (see Figure 1), exploiting the
Markov assumption of n-gram models to allow ef-
ficient calculation by avoiding re-computation.
4.2 Individual classifiers
This section details the features used by the 4 indi-
vidual classifiers. To investigate the utility of the
features used in each classifier we obtain values
on the standard Switchboard heldout data (PTB III
files sw4[5-9]*: 6.4K utterances, 49K words).
4.2.1 Edit term detection
In the first component, we utilise the well-known
observation that edit terms have a distinctive
vocabulary (Ginzburg, 2012), training a bigram
model on a corpus of all edit words annotated in
Switchboard?s training data. The classifier simply
uses the surprisal slex from this edit word model,
and the trigram surprisal slex from the standard
fluent model of Section 4.1. At the current position
w
n
, one, both or none of words w
n
and w
n?1
are
classified as edits. We found this simple approach
effective and stable, although some delayed deci-
sions occur in cases where slex and WMLlex are
high in both models before the end of the edit, e.g.
?I like? ? ?I {like} want...?. Words classified as
ed are removed from the incremental processing
graph (indicated by the dotted line transition in
Figure 1) and the stack updated if repair hypothe-
ses are cancelled due to a delayed edit hypothesis
of w
n?1
.
4.2.2 Repair start detection
Repair onset detection is arguably the most crucial
component: the greater its accuracy, the better the
input for downstream components and the lesser
the overhead of filtering false positives required.
82
i havent had
any
good really
very
good experience with child
care
?1.4
?1.2
?1.0
?0.8
?0.6
?0.4
?0.2
0.0
W
M
L
Figure 2: WMLlex values for trigrams for a repaired utterance exhibiting the drop at the repair onset
We use Section 4.1?s information-theoretic fea-
tures s,WML,H for words and POS, and intro-
duce 5 additional information-theoretic features:
?WML is the difference between the WML val-
ues at w
n?1
and w
n
; ?H is the difference in en-
tropy between w
n?1
and w
n
; InformationGain
is the difference between expected entropy at
w
n?1
and observed s at w
n
, a measure that
factors out the effect of naturally high entropy
contexts; BestEntropyReduce is the best reduc-
tion in entropy possible by an early rough hy-
pothesis of reparandum onsets within 3 words;
and BestWMLBoost similarly speculates on the
best improvement of WML possible by positing
rm
start
positions up to 3 words back. We also in-
clude simple alignment features: binary features
which indicate if the word w
i?x
is identical to the
current word w
i
for x ? {1, 2, 3}. With 6 align-
ment features, 16 N-gram features and a single
logical feature edit which indicates the presence
of an edit word at position w
i?1
, rp
start
detection
uses 23 features? see Table 1.
We hypothesised repair onsets rpstart would
have significantly lower plex (lower lexical-
syntactic probability) and WMLlex (lower syntac-
tic probability) than other fluent trigrams. This
was the case in the Switchboard heldout data
for both measures, with the biggest difference
obtained for WMLlex (non-repair-onsets: -0.736
(sd=0.359); repair onsets: -1.457 (sd=0.359)). In
the POS model, entropy of continuation Hpos was
the strongest feature (non-repair-onsets: 3.141
(sd=0.769); repair onsets: 3.444 (sd=0.899)). The
trigram WMLlex measure for the repaired utter-
ance ?I haven?t had any [ good + really very good
] experience with child care? can be seen in Fig-
ure 2. The steep drop at the repair onset shows the
usefulness of WML features for fluency measures.
To compare n-gram measures against other lo-
cal features, we ranked the features by Informa-
tion Gain using 10-fold cross validation over the
Switchboard heldout data? see Table 1. The lan-
guage model features are far more discriminative
than the alignment features, showing the potential
of a general information-theoretic approach.
4.2.3 Reparandum start detection
In detecting rm
start
positions given a hypothe-
sised rp
start
(stage T3 in Figure 1), we use the
noisy channel intuition that removing the reparan-
dum (from rm
start
to rp
start
) increases fluency
of the utterance, expressed here as WMLboost as
described above. When using gold standard in-
put we found this was the case on the heldout
data, with a mean WMLboost of 0.223 (sd=0.267)
for reparandum onsets and -0.058 (sd=0.224) for
other words in the 6-word history- the negative
boost for non-reparandum words captures the in-
tuition that backtracking from those points would
make the utterance less grammatical, and con-
versely the boost afforded by the correct rm
start
detection helps solve the continuation problem for
the listener (and our detector).
Parallelism in the onsets of rp
start
and
rm
start
can also help solve the continuation
problem, and in fact the KL divergence be-
tween ?pos(w | rm
start
, rm
start?1
) and ?pos(w |
rp
start
, rp
start?1
) is the second most useful fea-
ture with average merit 0.429 (+- 0.010) in cross-
83
validation. The highest ranked feature is ?WML
(0.437 (+- 0.003)) which here encodes the drop in
the WMLboost from one backtracked position to
the next. In ranking the 32 features we use, again
information-theoretic ones are higher ranked than
the logical features.
average merit average rank attribute
0.139 (+- 0.002) 1 (+- 0.00) Hpos
0.131 (+- 0.001) 2 (+- 0.00) WMLpos
0.126 (+- 0.001) 3.4 (+- 0.66) WMLlex
0.125 (+- 0.003) 4 (+- 1.10) spos
0.122 (+- 0.001) 5.9 (+- 0.94) w
i?1
= w
i
0.122 (+- 0.001) 5.9 (+- 0.70) BestWMLBoostlex
0.122 (+- 0.002) 5.9 (+- 1.22) InformationGainpos
0.119 (+- 0.001) 7.9 (+- 0.30) BestWMLBoostpos
0.098 (+- 0.002) 9 (+- 0.00) H lex
0.08 (+- 0.001) 10.4 (+- 0.49) ?WMLpos
0.08 (+- 0.003) 10.6 (+- 0.49) ?Hpos
0.072 (+- 0.001) 12 (+- 0.00) POS
i?1
= POS
i
0.066 (+- 0.003) 13.1 (+- 0.30) slex
0.059 (+- 0.000) 14.2 (+- 0.40) ?WMLlex
0.058 (+- 0.005) 14.7 (+- 0.64) BestEntropyReducepos
0.049 (+- 0.001) 16.3 (+- 0.46) InformationGainlex
0.047 (+- 0.004) 16.7 (+- 0.46) BestEntropyReducelex
0.035 (+- 0.004) 18 (+- 0.00) ?H lex
0.024 (+- 0.000) 19 (+- 0.00) w
i?2
= w
i
0.013 (+- 0.000) 20 (+- 0.00) POS
i?2
= POS
i
0.01 (+- 0.000) 21 (+- 0.00) w
i?3
= w
i
0.009 (+- 0.000) 22 (+- 0.00) edit
0.006 (+- 0.000) 23 (+- 0.00) POS
i?3
= POS
i
Table 1: Feature ranker (Information Gain) for
rp
start
detection- 10-fold x-validation on Switch-
board heldout data.
4.2.4 Repair end detection and structure
classification
For rp
end
detection, using the notion of paral-
lelism, we hypothesise an effect of divergence be-
tween ?lex at the reparandum-final word rm
end
and the repair-final word rp
end
: for repetition re-
pairs, KL divergence will trivially be 0; for substi-
tutions, it will be higher; for deletes, even higher.
Upon inspection of our feature ranking this KL
measure ranked 5th out of 23 features (merit=
0.258 (+- 0.002)).
We introduce another feature encoding paral-
lelism ReparandumRepairDifference : the differ-
ence in probability between an utterance cleaned
of the reparandum and the utterance with its
repair phase substituting its reparandum. In
both the POS (merit=0.366 (+- 0.003)) and word
(merit=0.352 (+- 0.002)) LMs, this was the most
discriminative feature.
4.3 Classifier pipeline
STIR effects a pipeline of classifiers as in Fig-
ure 3, where the ed classifier only permits non
ed words to be passed on to rp
start
classification
and for rp
end
classification of the active repair
hypotheses, maintained in a stack. The rp
start
classifier passes positive repair hypotheses to the
rm
start
classifier, which backwards searches up
to 7 words back in the utterance. If a rm
start
is
classified, the output is passed on for rp
end
clas-
sification at the end of the pipeline, and if not re-
jected this is pushed onto the repair stack. Repair
hypotheses are are popped off when the string is
7 words beyond its rp
start
position. Putting limits
on the stack?s storage space is a way of controlling
for processing overhead and complexity. Embed-
ded repairs whose rm
start
coincide with another?s
rp
start
are easily dealt with as they are added to
the stack as separate hypotheses.4
Classifiers Classifiers are implemented using
Random Forests (Breiman, 2001) and we use dif-
ferent error functions for each stage using Meta-
Cost (Domingos, 1999). The flexibility afforded
by implementing adjustable error functions in a
pipelined incremental processor allows control of
the trade-off of immediate accuracy against run-
time and stability of the sequence classification.
Processing complexity This pipeline avoids an
exhaustive search all repair hypotheses. If we limit
the search to within the ?rm
start
, rp
start
? possibil-
ities, this number of repairs grows approximately
in the triangular number series? i.e. n(n+1)
2
, a
nested loop over previous words as n gets incre-
mented ? which in terms of a complexity class is
a quadratic O(n2). If we allow more than one
?rm
start
, rp
start
? hypothesis per word, the com-
plexity goes up to O(n3), however in the tests that
we describe below, we are able to achieve good de-
tection results without permitting this extra search
space. Under our assumption that reparandum on-
set detection is only triggered after repair onset de-
tection, and repair extent detection is dependent
on positive reparandum onset detection, a pipeline
with accurate components will allow us to limit
processing to a small subset of this search space.
4We constrain the problem not to include embedded
deletes which may share their rp
start
word with another re-
pair ? these are in practice very rare.
84
Figure 3: Classifier pipeline
5 Experimental set-up
We train STIR on the Switchboard data described
above, and test it on the standard Switchboard test
data (PTB III files 4[0-1]*). In order to avoid over-
fitting of classifiers to the basic language models,
we use a cross-fold training approach: we divide
the corpus into 10 folds and use language mod-
els trained on 9 folds to obtain feature values for
the 10th fold, repeating for all 10. Classifiers are
then trained as standard on the resulting feature-
annotated corpus. This resulted in better feature
utility for n-grams and better F-score results for
detection in all components in the order of 5-6%.5
Training the classifiers Each Random Forest
classifier was limited to 20 trees of maximum
depth 4 nodes, putting a ceiling on decoding time.
In making the classifiers cost-sensitive, MetaCost
resamples the data in accordance with the cost
functions: we found using 10 iterations over a re-
sample of 25% of the training data gave the most
effective trade-off between training time and accu-
racy.6 We use 8 different cost functions in rp
start
with differing costs for false negatives and posi-
tives of the form below, where R is a repair ele-
ment word and F is a fluent onset:
(
R
hyp
F
hyp
R
gold
0 2
F
gold
1 0
)
We adopt a similar technique in rm
start
using 5
different cost functions and in rp
end
using 8 dif-
ferent settings, which when combined gives a to-
tal of 320 different cost function configurations.
We hypothesise that higher recall permitted in the
pipeline?s first components would result in better
overall accuracy as these hypotheses become re-
fined, though at the cost of the stability of the hy-
5Zwarts and Johnson (2011) take a similar approach on
Switchboard data to train a re-ranker of repair analyses.
6As (Domingos, 1999) demonstrated, there are only rela-
tively small accuracy gains when using more than this, with
training time increasing in the order of the re-sample size.
potheses of the sequence and extra downstream
processing in pruning false positives.
We also experiment with the number of repair
hypotheses permitted per word, using limits of 1-
best and 2-best hypotheses. We expect that allow-
ing 2 hypotheses to be explored per rp
start
should
allow greater final accuracy, but with the trade-off
of greater decoding and training complexity, and
possible incremental instability.
As we wish to explore the incrementality versus
final accuracy trade-off that STIR can achieve we
now describe the evaluation metrics we employ.
5.1 Incremental evaluation metrics
Following (Baumann et al., 2011) we divide our
evaluation metrics into similarity metrics (mea-
sures of equality with or similarity to a gold stan-
dard), timing metrics (measures of the timing of
relevant phenomena detected from the gold stan-
dard) and diachronic metrics (evolution of incre-
mental hypotheses over time).
Similarity metrics For direct comparison to
previous approaches we use the standard measure
of overall accuracy, the F-score over reparandum
words, which we abbreviate F
rm
(see 7):
precision = rm
correct
rm
hyp
recall = rm
correct
rm
gold
F
rm
= 2 ?
precision ? recall
precision + recall
(7)
We are also interested in repair structural clas-
sification, we also measure F-score over all repair
components (rm words, ed words as interregna
and rp words), a metric we abbreviate F
s
. This
is not measured in standard repair detection on
Switchboard. To investigate incremental accuracy
we evaluate the delayed accuracy (DA) introduced
by (Zwarts et al., 2010), as described in section
2 against the utterance-final gold standard disflu-
ency annotations, and use the mean of the 6 word
F-scores.
85
Input and current repair labels edits
John
John likes
rm rp
(?rm) (?rp)
John likes uh
ed
(?rm) (?rp)?ed
John likes uh loves
rm ed rp
?rm?rp
John likes uh loves Mary
rm ed rp
Figure 4: Edit Overhead- 4 unnecessary edits
Timing and resource metrics Again for com-
parative purposes we use Zwarts et al?s time-to-
detection metrics, that is the two average distances
(in numbers of words) consumed before first de-
tection of gold standard repairs, one from rm
start
,
TD
rm
and one from rp
start
, TD
rp
. In our 1-best
detection system, before evaluation we know a pri-
ori TD
rp
will be 1 token, and TD
rm
will be 1 more
than the average length of rm
start
? rp
start
repair
spans correctly detected. However when we in-
troduce a beam where multiple rm
start
s are pos-
sible per rp
start
with the most likely hypothesis
committed as the current output, the latency may
begin to increase: the initially most probable hy-
pothesis may not be the correct one. In addition
to output timing metrics, we account for intrinsic
processing complexity with the metric processing
overhead (PO), which is the number of classifica-
tions made by all components per word of input.
Diachronic metrics To measure stability of re-
pair hypotheses over time we use (Baumann et al.,
2011)?s edit overhead (EO) metric. EO measures
the proportion of edits (add, revoke, substitute) ap-
plied to a processor?s output structure that are un-
necessary. STIR?s output is the repair label se-
quence shown in Figure 1, however rather than
evaluating its EO against the current gold stan-
dard labels, we use a new mark-up we term the in-
cremental repair gold standard: this does not pe-
nalise lack of detection of a reparandum word rm
as a bad edit until the corresponding rp
start
of that
rm has been consumed. While F
rm
, F
s
and DA
evaluate against what Baumann et al. (2011) call
the current gold standard, the incremental gold
standard reflects the repair processing approach
we set out in 3. An example of a repaired utterance
with an EO of 44% (4
9
) can be seen in Figure 4: of
the 9 edits (7 repair annotations and 2 correct flu-
ent words), 4 are unnecessary (bracketed). Note
Figure 6: Delayed Accuracy Curves
the final ?rm is not counted as a bad edit for the
reasons just given.
6 Results and Discussion
We evaluate on the Switchboard test data; Ta-
ble 2 shows results of the best performing settings
for each of the metrics described above, together
with the setting achieving the highest total score
(TS)? the average % achieved of the best per-
forming system?s result in each metric.7 The set-
tings found to achieve the highest F
rm
(the metric
standardly used in disfluency detection), and that
found to achieve the highest TS for each stage in
the pipeline are shown in Figure 5.
Our experiments showed that different system
settings perform better in different metrics, and
no individual setting achieved the best result in
all of them. Our best utterance-final F
rm
reaches
0.779, marginally though not significantly exceed-
ing (Zwarts et al., 2010)?s measure and STIR
achieves 0.736 on the previously unevaluated F
s
.
The setting with the best DA improves on (Zwarts
et al., 2010)?s result significantly in terms of mean
values (0.718 vs. 0.694), and also in terms of the
steepness of the curves (Figure 6). The fastest av-
erage time to detection is 1 word for TD
rp
and 2.6
words for TD
rm
(Table 3), improving dramatically
on the noisy channel model?s 4.6 and 7.5 words.
Incrementality versus accuracy trade-off We
aimed to investigate how well a system could do
in terms of achieving both good final accuracy and
incremental performance, and while the best F
rm
setting had a large PO and relatively slow DA in-
crease, we find STIR can find a good trade-off set-
7We do not include time-to-detection scores in TS as it
did not vary enough between settings to be significant, how-
ever there was a difference in this measure between the 1-best
stack condition and the 2-best stack condition ? see below.
86
??
?
rp
hyp
start
F
hyp
rp
gold
start
0 64
F
gold
1 0
?
?
?
?
?
?
rm
hyp
start
F
hyp
rm
gold
start
0 8
F
gold
1 0
?
?
?
?
?
?
rp
hyp
end
F
hyp
rp
gold
end
0 2
F
gold
1 0
?
?
?
Stack depth = 2
?
?
?
rp
hyp
start
F
hyp
rp
gold
start
0 2
F
gold
1 0
?
?
?
?
?
?
rm
hyp
start
F
hyp
rm
gold
start
0 16
F
gold
1 0
?
?
?
?
?
?
rp
hyp
end
F
hyp
rp
gold
end
0 8
F
gold
1 0
?
?
?
Stack depth = 1
Figure 5: The cost function settings for the MetaCost classifiers for each component, for the best F
rm
setting (top row) and best total score (TS) setting (bottom row)
F
rm
F
s
DA EO PO
Best Final rm F-score (F
rm
) 0.779 0.735 0.698 3.946 1.733
Best Final repair structure F-score (F
s
) 0.772 0.736 0.707 4.477 1.659
Best Delayed Accuracy of rm (DA) 0.767 0.721 0.718 1.483 1.689
Best (lowest) Edit Overhead (EO) 0.718 0.674 0.675 0.864 1.230
Best (lowest) Processing Overhead (PO) 0.716 0.671 0.673 0.875 1.229
Best Total Score (mean % of best scores) (TS) 0.754 0.708 0.711 0.931 1.255
Table 2: Comparison of the best performing system settings using different measures
F
rm
F
s
DA EO PO TD
rp
TD
rm
1-best rm
start
0.745 0.707 0.699 3.780 1.650 1.0 2.6
2-best rm
start
0.758 0.721 0.701 4.319 1.665 1.1 2.7
Table 3: Comparison of performance of systems with different stack capacities
ting: the highest TS scoring setting achieves an
F
rm
of 0.754 whilst also exhibiting a very good
DA (0.711) ? over 98% of the best recorded score
? and low PO and EO rates ? over 96% of the best
recorded scores. See the bottom row of Table 2.
As can be seen in Figure 5, the cost functions for
these winning settings are different in nature. The
best non-incremental F
rm
measure setting requires
high recall for the rest of the pipeline to work on,
using the highest cost, 64, for false negative rp
start
words and the highest stack depth of 2 (similar to a
wider beam); but the best overall TS scoring sys-
tem uses a less permissive setting to increase in-
cremental performance.
We make a preliminary investigation into the
effect of increasing the stack capacity by com-
paring stacks with 1-best rm
start
hypotheses per
rp
start
and 2-best stacks. The average differences
between the two conditions is shown in Table 3.
Moving to the 2-stack condition results in gain in
overall accuracy in F
rm
and F
s
, but at the cost of
EO and also time-to-detection scores TD
rm
and
TD
rp
. The extent to which the stack can be in-
creased without increasing jitter, latency and com-
plexity will be investigated in future work.
7 Conclusion
We have presented STIR, an incremental repair
detector that can be used to experiment with in-
cremental performance and accuracy trade-offs. In
future work we plan to include probabilistic and
distributional features from a top-down incremen-
tal parser e.g. Roark et al. (2009), and use STIR?s
distributional features to classify repair type.
Acknowledgements
We thank the three anonymous EMNLP review-
ers for their helpful comments. Hough is sup-
ported by the DUEL project, financially supported
by the Agence Nationale de la Research (grant
number ANR-13-FRAL-0001) and the Deutsche
Forschungsgemainschaft. Much of the work was
carried out with support from an EPSRC DTA
scholarship at Queen Mary University of Lon-
don. Purver is partly supported by ConCreTe:
the project ConCreTe acknowledges the financial
support of the Future and Emerging Technologies
(FET) programme within the Seventh Framework
Programme for Research of the European Com-
mission, under FET grant number 611733.
87
References
T. Baumann, O. Bu?, and D. Schlangen. 2011. Eval-
uation and optimisation of incremental processors.
Dialogue & Discourse, 2(1):113?141.
Leo Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5?32.
S.E. Brennan and M.F. Schober. 2001. How listeners
compensate for disfluencies in spontaneous speech.
Journal of Memory and Language, 44(2):274?296.
Eric Brill and Robert C Moore. 2000. An improved er-
ror model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 286?293.
Association for Computational Linguistics.
Alexander Clark, Gianluca Giorgolo, and Shalom Lap-
pin. 2013. Statistical representation of grammat-
icality judgements: the limits of n-gram models.
In Proceedings of the Fourth Annual Workshop on
Cognitive Modeling and Computational Linguistics
(CMCL), pages 28?36, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Pedro Domingos. 1999. Metacost: A general method
for making classifiers cost-sensitive. In Proceed-
ings of the fifth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 155?164. ACM.
Kallirroi Georgila. 2009. Using integer linear pro-
gramming for detecting speech disfluencies. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Papers, pages
109?112. Association for Computational Linguis-
tics.
Sebastian Germesin, Tilman Becker, and Peter Poller.
2008. Hybrid multi-step disfluency detection.
In Machine Learning for Multimodal Interaction,
pages 185?195. Springer.
Jonathan Ginzburg. 2012. The Interactive Stance:
Meaning for Conversation. Oxford University
Press.
P. G. T. Healey, Arash Eshghi, Christine Howes, and
Matthew Purver. 2011. Making a contribution: Pro-
cessing clarification requests in dialogue. In Pro-
ceedings of the 21st Annual Meeting of the Society
for Text and Discourse, Poitiers, July.
Peter Heeman and James Allen. 1999. Speech repairs,
intonational phrases, and discourse markers: model-
ing speakers? utterances in spoken dialogue. Com-
putational Linguistics, 25(4):527?571.
Matthew Honnibal and Mark Johnson. 2014. Joint
incremental disfluency detection and dependency
parsing. Transactions of the Association of Com-
putational Linugistics (TACL), 2:131?142.
Julian Hough and Matthew Purver. 2013. Modelling
expectation in the self-repair processing of annotat-,
um, listeners. In Proceedings of the 17th SemDial
Workshop on the Semantics and Pragmatics of Di-
alogue (DialDam), pages 92?101, Amsterdam, De-
cember.
T Florian Jaeger and Harry Tily. 2011. On language
utility: Processing complexity and communicative
efficiency. Wiley Interdisciplinary Reviews: Cogni-
tive Science, 2(3):323?335.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 33?39,
Barcelona. Association for Computational Linguis-
tics.
Frank Keller. 2004. The entropy rate principle as a
predictor of processing effort: An evaluation against
eye-tracking data. In EMNLP, pages 317?324.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Matthew Lease, Mark Johnson, and Eugene Charniak.
2006. Recognizing disfluencies in conversational
speech. Audio, Speech, and Language Processing,
IEEE Transactions on, 14(5):1566?1573.
W.J.M. Levelt. 1983. Monitoring and self-repair in
speech. Cognition, 14(1):41?104.
M. Meteer, A. Taylor, R. MacIntyre, and R. Iyer. 1995.
Disfluency annotation stylebook for the switchboard
corpus. ms. Technical report, Department of Com-
puter and Information Science, University of Penn-
sylvania.
Tim Miller and William Schuler. 2008. A syntactic
time-series model for parsing fluent and disfluent
speech. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 569?576. Association for Computational Lin-
guistics.
David Milward. 1991. Axiomatic Grammar, Non-
Constituent Coordination and Incremental Interpre-
tation. Ph.D. thesis, University of Cambridge.
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In Proceedings of
NAACL-HLT, pages 820?825.
Mohammad Sadegh Rasooli and Joel Tetreault. 2014.
Non-monotonic parsing of fluent umm I mean dis-
fluent sentences. EACL 2014, pages 48?53.
Hannes Rieser and David Schlangen. 2011. Introduc-
tion to the special issue on incremental processing in
dialogue. Dialogue & Discourse, 2(1):1?10.
88
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 324?333. Association for Com-
putational Linguistics.
Claude E. Shannon. 1948. A mathematical theory
of communication. technical journal. AT & T Bell
Labs.
Elizabeth Shriberg and Andreas Stolcke. 1998. How
far do speakers back up in repairs? A quantitative
model. In Proceedings of the International Confer-
ence on Spoken Language Processing, pages 2183?
2186.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California, Berkeley.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disflu-
ency detection. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 703?711, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Simon Zwarts, Mark Johnson, and Robert Dale. 2010.
Detecting speech repairs incrementally using a noisy
channel approach. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ?10, pages 1371?1378, Stroudsburg, PA,
USA. Association for Computational Linguistics.
89
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708?719,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Evaluating Neural Word Representations in
Tensor-Based Compositional Settings
Dmitrijs Milajevs
1
Dimitri Kartsaklis
2
Mehrnoosh Sadrzadeh
1
Matthew Purver
1
1
Queen Mary University of London
School of Electronic Engineering
and Computer Science
Mile End Road, London, UK
{d.milajevs,m.sadrzadeh,m.purver}@qmul.ac.uk
2
University of Oxford
Department of Computer Science
Parks Road, Oxford, UK
dimitri.kartsaklis@cs.ox.ac.uk
Abstract
We provide a comparative study be-
tween neural word representations and
traditional vector spaces based on co-
occurrence counts, in a number of com-
positional tasks. We use three differ-
ent semantic spaces and implement seven
tensor-based compositional models, which
we then test (together with simpler ad-
ditive and multiplicative approaches) in
tasks involving verb disambiguation and
sentence similarity. To check their scala-
bility, we additionally evaluate the spaces
using simple compositional methods on
larger-scale tasks with less constrained
language: paraphrase detection and di-
alogue act tagging. In the more con-
strained tasks, co-occurrence vectors are
competitive, although choice of composi-
tional method is important; on the larger-
scale tasks, they are outperformed by neu-
ral word embeddings, which show robust,
stable performance across the tasks.
1 Introduction
Neural word embeddings (Bengio et al., 2006;
Collobert and Weston, 2008; Mikolov et al.,
2013a) have received much attention in the dis-
tributional semantics community, and have shown
state-of-the-art performance in many natural lan-
guage processing tasks. While they have been
compared with co-occurrence based models in
simple similarity tasks at the word level (Levy et
al., 2014; Baroni et al., 2014), we are aware of
only one work that attempts a comparison of the
two approaches in compositional settings (Blacoe
and Lapata, 2012), and this is limited to additive
and multiplicative composition, compared against
composition via a neural autoencoder.
The purpose of this paper is to provide a more
complete picture regarding the potential of neu-
ral word embeddings in compositional tasks, and
meaningfully compare them with the traditional
distributional approach based on co-occurrence
counts. We are especially interested in investi-
gating the performance of neural word vectors in
compositional models involving general mathe-
matical composition operators, rather than in the
more task- or domain-specific deep-learning com-
positional settings they have generally been used
with so far (for example, by Socher et al. (2012),
Kalchbrenner and Blunsom (2013) and many oth-
ers).
In particular, this is the first large-scale study
to date that applies neural word representations in
tensor-based compositional distributional models
of meaning similar to those formalized by Coecke
et al. (2010). We test a range of implementations
based on this framework, together with additive
and multiplicative approaches (Mitchell and Lap-
ata, 2008), in a variety of different tasks. Specif-
ically, we use the verb disambiguation task of
Grefenstette and Sadrzadeh (2011a) and the tran-
sitive sentence similarity task of Kartsaklis and
Sadrzadeh (2014) as small-scale focused experi-
ments on pre-defined sentence structures. Addi-
tionally, we evaluate our vector spaces on para-
phrase detection (using the Microsoft Research
Paraphrase Corpus of Dolan et al. (2005)) and di-
alogue act tagging using the Switchboard Corpus
(see e.g. (Stolcke et al., 2000)).
In all of the above tasks, we compare the neural
word embeddings of Mikolov et al. (2013a) with
two vector spaces both based on co-occurrence
counts and produced by standard distributional
techniques, as described in detail below. The gen-
eral picture we get from the results is that in almost
all cases the neural vectors are more effective than
the traditional approaches.
We proceed as follows: Section 2 provides a
concise introduction to distributional word repre-
sentations in natural language processing. Section
708
3 takes a closer look to the subject of composi-
tionality in vector space models of meaning and
describes the range of compositional operators ex-
amined here. In Section 4 we provide details about
the vector spaces used in the experiments. Our ex-
perimental work is described in detail in Section 5,
and the results are discussed in Section 6. Finally,
Section 7 provides conclusions.
2 Meaning representation
There are several approaches to the representation
of word, phrase and sentence meaning. As nat-
ural languages are highly creative and it is very
rare to see the same sentence twice, any practical
approach dealing with large text segments must
be compositional, constructing the meaning of
phrases and sentences from their constituent parts.
The ideal method would therefore express not
only the similarity in meaning between those con-
stituent parts, but also between the results of their
composition, and do this in ways which fit with
linguistic structure and generalisations thereof.
Formal semantics Formal approaches to the
semantics of natural language have long built
upon the classical idea of compositionality ?
that the meaning of a sentence is a function
of the meanings of its parts (Frege, 1892). In
compositional type-logical approaches, predicate-
argument structures representing phrases and sen-
tences are built from their constituent parts by ?-
reduction within the lambda calculus framework
(Montague, 1970): for example, given a represen-
tation of John as john
?
and sleeps as ?x.sleep
?
(x),
the meaning of the sentence ?John sleeps?
can be constructed as ?x.sleep
?
(x)(john
?
) =
sleep
?
(john
?
). Given a suitable pairing between
words and semantic representations of them, this
method can produce structured sentential repre-
sentations with broad coverage and good gener-
alisability (see e.g. (Bos, 2008)). The above logi-
cal approach is extremely powerful because it can
capture complex aspects of meaning such as quan-
tifiers and their interaction (see e.g. (Copestake et
al., 2005)), and enables inference using well stud-
ied and developed logical methods (see e.g. (Bos
and Gabsdil, 2000)).
Distributional hypothesis However, such for-
mal approaches are less able to express similar-
ity in meaning. We would like to capture the
intuition that while John and Mary are distinct,
they are rather similar to each other (both of them
are humans) and dissimilar to words such as dog,
pavement or idea. The same applies at the phrase
and sentence level: ?dogs chase cats? is similar in
meaning to ?hounds pursue kittens?, but less so to
?cats chase dogs? (despite the lexical overlap).
Distributional methods provide a way to address
this problem. By representing words and phrases
as vectors or tensors in a (usually highly dimen-
sional) vector space, one can express similarity
in meaning via a suitable distance metric within
that space (usually cosine distance); furthermore,
composition can be modelled via suitable linear-
algebraic operations.
Co-occurrence-based word representations
One way to produce such vectorial representa-
tions is to directly exploit Harris (1954)?s intuition
that semantically similar words tend to appear in
similar contexts. We can construct a vector space
in which the dimensions correspond to contexts,
usually taken to be words as well. The word
vector components can then be calculated from
the frequency with which a word has co-occurred
with the corresponding contexts in a window of
words, with a predefined length.
Table 1 shows 5 3-dimensional vectors for the
words Mary, John, girl, boy and idea. The words
philosophy, book and school signify vector space
dimensions. As the vector for John is closer to
Mary than it is to idea in the vector space?a di-
rect consequence of the fact that John?s contexts
are similar to Mary?s and dissimilar to idea?s?we
can infer that John is semantically more similar to
Mary than to idea.
Many variants of this approach exist: perfor-
mance on word similarity tasks has been shown
to be improved by replacing raw counts with
weighted values (e.g. mutual information)?see
(Turney et al., 2010) and below for discussion, and
(Kiela and Clark, 2014) for a detailed comparison.
philosophy book school
Mary 0 10 22
John 4 60 59
girl 0 19 93
boy 0 12 164
idea 10 47 39
Table 1: Word co-occurrence frequencies ex-
tracted from the BNC (Leech et al., 1994).
709
Neural word embeddings Deep learning tech-
niques exploit the distributional hypothesis dif-
ferently. Instead of relying on observed co-
occurrence frequencies, a neural language model
is trained to maximise some objective function re-
lated to e.g. the probability of observing the sur-
rounding words in some context (Mikolov et al.,
2013b):
1
T
T
?
t=1
?
?c?j?c,j 6=0
log p(w
t+j
|w
t
) (1)
Optimizing the above function, for example, pro-
duces vectors which maximise the conditional
probability of observing words in a context around
the target word w
t
, where c is the size of the
training window, and w
1
w
2
, ? ? ?w
T
a sequence of
words forming a training instance. Therefore, the
resulting vectors will capture the distributional in-
tuition and can express degrees of lexical similar-
ity.
This method has an obvious advantage com-
pared to co-occurrence method: since now the
context is predicted, the model in principle can
be much more robust in data sparsity prob-
lems, which is always an important issue for co-
occurrence word spaces. Additionally, neural vec-
tors have also proven successful in other tasks
(Mikolov et al., 2013c), since they seem to en-
code not only attributional similarity (the degree to
which similar words are close to each other), but
also relational similarity (Turney, 2006). For ex-
ample, it is possible to extract the singular:plural
relation (apple:apples, car:cars) using vector sub-
traction:
????
apple ?
?????
apples ?
??
car ?
???
cars
Perhaps even more importantly, semantic relation-
ships are preserved in a very intuitive way:
???
king ?
???
man ?
????
queen ?
?????
woman
allowing the formation of analogy queries similar
to
???
king ?
???
man +
?????
woman = ?, obtaining
????
queen as
the result.
1
Both neural and co-occurrence-based ap-
proaches have advantages over classical formal
approaches in their ability to capture lexical se-
mantics and degrees of similarity; their success at
1
Levy et al. (2014) improved Mikolov et al. (2013c)?s
method of retrieving relational similarities by changing the
underlying objective function.
extending this to the sentence level and to more
complex semantic phenomena, though, depends
on their applicability within compositional mod-
els, which is the subject of the next section.
3 Compositional models
Compositional distributional models represent
meaning of a sequence of words by a vector, ob-
tained by combining meaning vectors of the words
within the sequence using some vector composi-
tion operation. In a general classification of these
models, one can distinguish between three broad
cases: simplistic models which combine word
vectors irrespective of their order or relation to one
another, models which exploit linear word order,
and models which use grammatical structure.
The first approach combines word vectors
by vector addition or point-wise multiplication
(Mitchell and Lapata, 2008)?as this is indepen-
dent of word order, it cannot capture the differ-
ence between the two sentences ?dogs chase cats?
and ?cats chase dogs?. The second approach has
generally been implemented using some form of
deep learning, and captures word order, but not by
necessarily caring about the grammatical structure
of the sentence. Here, one works by recursively
building and combining vectors for subsequences
of words within the sentence using e.g. autoen-
coders (Socher et al., 2012) or convolutional fil-
ters (Kalchbrenner et al., 2014). We do not con-
sider this approach in this paper. This is because,
as mentioned in the introduction, their vectors and
composition operators are task-specific. These are
trained directly to achieve specific objectives in
certain pre-determined tasks. We are interested
in vector and composition operators that work for
any compositional task, and which can be com-
bined with results in linguistics and formal se-
mantics to provide generalisable models that can
canonically extend to complex semantic phenom-
ena. The third (i.e. the grammatical) approach
promises a way to achieve this, and has been in-
stantiated in various ways in the work of Baroni
and Zamparelli (2010),Grefenstette and Sadrzadeh
(2011a), and Kartsaklis et al. (2012).
General framework Formally, we can spec-
ify the vector representation of a word sequence
w
1
w
2
? ? ?w
n
as the vector
??
s =
??
w
1
?
??
w
2
? ? ? ??
??
w
n
,
where ? is a vector operator, such as addition +,
point-wise multiplication , tensor product ?, or
matrix multiplication ?.
710
In the simplest compositional models (the first
approach described above), ? is + or , e.g. see
(Mitchell and Lapata, 2008). Grammar-based
compositional models (the third approach) are
based on a generalisation of the notion of vectors,
known as tensors. Whereas a vector
??
v is an ele-
ment of an atomic vector space V , a tensor z is an
element of a tensor space V ?W ? ? ? ? ? Z. The
number of tensored spaces is referred to by the or-
der of the space. Using a general duality theorem
from multi-linear algebra (Bourbaki, 1989), it fol-
lows that tensors are in one-one correspondence
with multi-linear maps, that is we have:
z ? V ?W?? ? ??Z
?
=
f
z
: V ?W ? ? ? ? ? Z
In such a tensor-based formalism, meanings of
nouns are vectors and meanings of predicates such
as adjectives and verbs are tensors. Meaning of a
string of words is obtained by applying the compo-
sitions of multi-linear map duals of the tensors to
the vectors. For the sake of demonstration, take
the case of an intransitive sentence ?Sbj Verb?;
the meaning of the subject is a vector
??
Sbj ? V
and the meaning of the intransitive verb is a ten-
sor Verb ? V ?W . Meaning of the sentence is
obtained by applying f
V erb
to
??
Sbj, as follows:
??????
Sbj Verb = f
V erb
(
??
Sbj)
By tensor-map duality, the above becomes
equivalent to the following, where composition
has now become the familiar notion of matrix mul-
tiplication, that is ? is ?:
Verb?
??
Sbj
In general and for words with tensors of order
higher than two, ? becomes a generalisation of ?,
referred to by tensor contraction, see e.g. Kartsak-
lis and Sadrzadeh (2013). Since the creation and
manipulation of tensors of order higher than 2 is
difficult, one can work with simplified versions of
tensors, faithful to their underlying mathematical
basis; these have found intuitive interpretations,
e.g. see Grefenstette and Sadrzadeh (2011a), Kart-
saklis and Sadrzadeh (2014). In such cases, ? be-
comes a combination of a range of operations such
as ?, ?, , and +.
Specific models In the current paper we will ex-
periment with a variety of models. In Table 2, we
present these models in terms of their composi-
tion operators and a reference to the main paper in
which each model was introduced. For the sim-
ple compositional models the sentence is a string
of any number of words; for the grammar-based
models, we consider simple transitive sentences
?Sbj Verb Obj? and introduce the following abbre-
viations for the concrete method used to build a
tensor for the verb:
1. Verb is a verb matrix computed using the for-
mula
?
i
???
Sbj
i
?
???
Obj
i
, where
???
Sbj
i
and
???
Obj
i
are
the subjects and objects of the verb across the
corpus. These models are referred to by rela-
tional (Grefenstette and Sadrzadeh, 2011a);
they are generalisations of predicate seman-
tics of transitive verbs, from pairs of individ-
uals to pairs of vectors. The models reduce
the order 3 tensor of a transitive verb to an
order 2 tensor (i.e. a matrix).
2.
?
Verb is a verb matrix computed using the for-
mula
???
Verb ?
???
Verb, where
???
Verb is the distri-
butional vector of the verb. These models are
referred to by Kronecker, which is the term
sometimes used to denote the outer prod-
uct of tensors (Grefenstette and Sadrzadeh,
2011b). This models also reduces the order
3 tensor of a transitive verb to an order 2 ten-
sor.
3. The models of the last five lines of the table
use the so-called Frobenius operators from
categorical compositional distributional se-
mantics (Kartsaklis et al., 2012) to expand
the relational matrices of verbs from order 2
to order 3. The expansion is obtained by ei-
ther copying the dimension of the subject into
the space provided by the third tensor, hence
referred to by Copy-Sbj, or copying the di-
mension of the object in that space, hence re-
ferred to by Copy-Obj; furthermore, we can
take addition, multiplication, or outer product
of these, which are referred to by Frobenius-
Add, Frobenius-Mult, and Frobenius-Outer
(Kartsaklis and Sadrzadeh, 2014).
4 Semantic word spaces
Co-occurrence-based vector space instantiations
have received a lot of attention from the scientific
community (refer to (Kiela and Clark, 2014; Pola-
jnar and Clark, 2014) for recent studies). We in-
stantiate two co-occurrence-based vectors spaces
with different underlying corpora and weighting
schemes.
711
Method Sentence Linear algebraic formula Reference
Addition w
1
w
2
? ? ?w
n
??
w
1
+
??
w
2
+ ? ? ?+
??
w
n
Mitchell and Lapata (2008)
Multiplication w
1
w
2
? ? ?w
n
??
w
1

??
w
2
 ? ? ? 
??
w
n
Mitchell and Lapata (2008)
Relational Sbj Verb Obj Verb (
??
Sbj?
??
Obj) Grefenstette and Sadrzadeh (2011a)
Kronecker Sbj Verb Obj V?erb (
??
Sbj?
??
Obj) Grefenstette and Sadrzadeh (2011b)
Copy object Sbj Verb Obj
??
Sbj (Verb?
??
Obj) Kartsaklis et al. (2012)
Copy subject Sbj Verb Obj
??
Obj (Verb
T
?
??
Sbj) Kartsaklis et al. (2012)
Frob. add. Sbj Verb Obj (
??
Sbj (Verb?
??
Obj)) + (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Frob. mult. Sbj Verb Obj (
??
Sbj (Verb?
??
Obj)) (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Frob. outer Sbj Verb Obj (
??
Sbj (Verb?
??
Obj))? (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Table 2: Compositional methods.
GS11 Our first word space is based on a typ-
ical configuration that has been used in the past
extensively for compositional distributional mod-
els (see below for details), so it will serve as a
useful baseline for the current work. In this vec-
tor space, the co-occurrence counts are extracted
from the British National Corpus (BNC) (Leech et
al., 1994). As basis words, we use the most fre-
quent nouns, verbs, adjectives and adverbs (POS
tags SUBST, VERB, ADJ and ADV in the BNC
XML distribution
2
). The vector space is lemma-
tized, that is, it contains only ?canonical? forms of
words.
In order to weight the raw co-occurrence counts,
we use positive point-wise mutual information
(PPMI). The component value for a target word
t and a context word c is given by:
PPMI(t, c) = max
(
0, log
p(c|t)
p(c)
)
where p(c|t) is the probability of word c given t
in a symmetric window of length 5 and p(c) is the
probability of c overall.
Vector spaces based on point-wise mutual in-
formation (or variants thereof) have been success-
fully applied in various distributional and compo-
sitional tasks; see e.g. Grefenstette and Sadrzadeh
(2011a), Mitchell and Lapata (2008), Levy et al.
(2014) for details. PPMI has been shown to
achieve state-of-the-art results (Levy et al., 2014)
and is suggested by the review of Kiela and Clark
(2014). Our use here of the BNC as a corpus
and the window length of 5 is based on previ-
ous use and better performance of these param-
eters in a number of compositional experiments
(Grefenstette and Sadrzadeh, 2011a; Grefenstette
2
http://www.natcorp.ox.ac.uk/
and Sadrzadeh, 2011b; Mitchell and Lapata, 2008;
Kartsaklis et al., 2012).
KS14 In this variation, we train a vector space
from the ukWaC corpus
3
(Ferraresi et al., 2008),
originally using as a basis the 2,000 content words
with the highest frequency (but excluding a list of
stop words as well as the 50 most frequent content
words since they exhibit low information content).
The vector space is again lemmatized. As context
we consider a 5-word window from either side of
the target word, while as our weighting scheme we
use local mutual information (i.e. point-wise mu-
tual information multiplied by raw counts). In a
further step, the vector space was normalized and
projected onto a 300-dimensional space using sin-
gular value decomposition (SVD).
In general, dimensionality reduction produces
more compact word representations that are robust
against potential noise in the corpus (Landauer and
Dumais, 1997; Sch?utze, 1997). SVD has been
shown to perform well on a variety of tasks similar
to ours (Baroni and Zamparelli, 2010; Kartsaklis
and Sadrzadeh, 2014).
Neural word embeddings (NWE) For our neu-
ral setting, we used the skip-gram model of
Mikolov et al. (2013b) trained with negative sam-
pling. The specific implementation that was tested
in our experiments was a 300-dimensional vec-
tor space learned from the Google News corpus
and provided by the word2vec
4
toolkit. Fur-
thermore, the gensim library (
?
Reh?u?rek and So-
jka, 2010) was used for accessing the vectors.
On the contrary with the previously described co-
3
http://wacky.sslmit.unibo.it/
4
https://code.google.com/p/word2vec/
712
occurrence vector spaces, this version is not lem-
matized.
The negative sampling method improves the ob-
jective function of Equation 1 by introducing neg-
ative examples to the training algorithm. Assume
that the probability of a specific (c, t) pair of words
(where t is a target word and c another word in
the same context with t), coming from the training
data, is denoted as p(D = 1|c, t). The objective
function is then expressed as follows:
?
(c,t)?D
p(D = 1|c, t) (2)
That is, the goal is to set the model parameters in
a way that maximizes the probability of all obser-
vations coming from the training data. Assume
now that D
?
is a set of randomly selected incorrect
(c
?
, t
?
) pairs that do not occur in D, then Equation
2 above can be recasted in the following way:
?
(c,t)?D
p(D = 1|c, t)
?
(c
?
,t
?
)?D
?
p(D = 0|c
?
, t
?
)
(3)
In other words, the model tries to distinguish a tar-
get word t from random draws that come from a
noise distribution. In the implementation we used
for our experiments, c is always selected from
a 5-word window around t. More details about
the negative sampling approach can be found in
(Mikolov et al., 2013b); the note of Goldberg and
Levy (2014) also provides an intuitive explanation
of the underlying setting.
5 Experiments
Our experiments explore the use of the vector
spaces above, together with the compositional op-
erators described in Section 3, in a range of tasks
all of which require semantic composition: verb
sense disambiguation; sentence similarity; para-
phrasing; and dialogue act tagging.
5.1 Disambiguation
We use the transitive verb disambiguation dataset
described in Grefenstette and Sadrzadeh (2011a)
5
.
This dataset consists of ambiguous transitive verbs
together with their arguments, landmark verbs
that identify one of the verb senses, and human
judgements that specify how similar is the disam-
biguated sense of the verb in the given context to
5
This and the sentence similarity dataset are avail-
able at http://www.cs.ox.ac.uk/activities/
compdistmeaning/
one of the landmarks. This is similar to the in-
transitive dataset described in (Mitchell and Lap-
ata, 2008). Consider the sentence ?system meets
specification?; here, meets is the ambiguous tran-
sitive verb, and system and specification are its ar-
guments in this context. Possible landmarks for
meet are satisfy and visit; for this sentence, the
human judgements show that the disambiguated
meaning of the verb is more similar to the land-
mark satisfy and less similar to visit.
The task is to estimate the similarity of the sense
of a verb in a context with a given landmark. To
get our similarity measures, we compose the verb
with its arguments using one of our compositional
models; we do the same for the landmark and then
compute the cosine similarity of the two vectors.
We evaluate the performance by averaging the hu-
man judgements for the same verb, argument and
landmark entries, and calculating the Spearman?s
correlation between the average values and the co-
sine scores. As a baseline, we compare this with
the correlation produced by using only the verb
vector, without composing it with its arguments.
Table 3 shows the results of the experiment.
NWE copy-object composition yields the best cor-
relation with the human judgements, and top per-
formance across all vector spaces and models with
a Spearman ? of 0.456. For the KS14 space, the
best result comes from Frobenius outer (0.350),
Method GS11 KS14 NWE
Verb only 0.212 0.325 0.107
Addition 0.103 0.275 0.149
Multiplication 0.348 0.041 0.095
Kronecker 0.304 0.176 0.117
Relational 0.285 0.341 0.362
Copy subject 0.089 0.317 0.131
Copy object 0.334 0.331 0.456
Frobenius add. 0.261 0.344 0.359
Frobenius mult. 0.233 0.341 0.239
Frobenius outer 0.284 0.350 0.375
Table 3: Spearman ? correlations of models with
human judgements for the word sense disam-
biguation task. The best result (NWE Copy ob-
ject) outperforms the nearest co-occurrence-based
competitor (KS14 Frobenius outer) with a statisti-
cally significant difference (p < 0.05, t-test).
713
while the best operator for the GS11 space is
point-wise multiplication (0.348).
For simple point-wise composition, only mul-
tiplicative GS11 and additive NWE improve over
their corresponding verb-only baselines (but both
perform worse than the KS14 baseline). With
tensor-based composition in co-occurrence based
spaces, copy subject yields lower results than
the corresponding baselines. Other composition
methods, except Kronecker for KS14, improve
over the verb-only baselines. Finally we should
note that, despite the small training corpus, the
GS11 vector space performs comparatively well:
for instance, Kronecker model improves the pre-
viously reported score of 0.28 (Grefenstette and
Sadrzadeh, 2011b).
5.2 Sentence similarity
In this experiment we use the transitive sen-
tence similarity dataset described in Kartsaklis and
Sadrzadeh (2014). The dataset consists of transi-
tive sentence pairs and a human similarity judge-
ment
6
. The task is to estimate a similarity measure
between two sentences. As in the disambiguation
task, we first compose word vectors to obtain sen-
tence vectors, then compute cosine similarity of
them. We average the human judgements for iden-
tical sentence pairs to compute a correlation with
cosine scores.
Table 4 shows the results. Again, the best
performing vector space is KS14, but this time
with addition: the Spearman ? correlation score
with averaged human judgements is 0.732. Addi-
tion was the means for the other vector spaces to
achieve top performance as well: GS11 and NWE
got 0.682 and 0.689 respectively.
None of the models in tensor-based composi-
tion outperformed addition. KS14 performs worse
with tensor-based methods here than in the other
vector spaces. However, GS11 and NWE, except
copy subject for both of them and Frobenius multi-
plication for NWE, improved over their verb-only
baselines.
5.3 Paraphrasing
In this experiment we evaluate our vector spaces
on a mainstream paraphrase detection task.
6
The textual content of this dataset is the same as that of
(Kartsaklis and Sadrzadeh, 2013), the difference is that the
dataset of (Kartsaklis and Sadrzadeh, 2014) has updated hu-
man judgements whereas the previous dataset used the orig-
inal annotations of the intransitive dataset of (Mitchell and
Lapata, 2010).
Method GS11 KS14 NWE
Verb only 0.491 0.602 0.561
Addition 0.682 0.732 0.689
Multiplication 0.597 0.321 0.341
Kronecker 0.581 0.408 0.561
Relational 0.558 0.437 0.618
Copy subject 0.370 0.448 0.405
Copy object 0.571 0.306 0.655
Frobenius add. 0.566 0.460 0.585
Frobenius mult. 0.525 0.226 0.387
Frobenius outer 0.560 0.439 0.622
Table 4: Results for sentence similarity. There
is no statistically significant difference between
KS14 addition and NWE addition (the second best
result).
Specifically, we get classification results on the
Microsoft Research Paraphrase Corpus paraphrase
corpus (Dolan et al., 2005) working in the follow-
ing way: we construct vectors for the sentences
of each pair; if the cosine similarity between the
two sentence vectors exceeds a certain threshold,
the pair is classified as a paraphrase, otherwise as
not a paraphrase. For this experiment and that of
Section 5.4 below, we investigate only the addi-
tion and point-wise multiplication compositional
models, since at their current stage of development
tensor-based models can only efficiently handle
sentences of fixed structure. Nevertheless, the
simple point-wise compositional models still al-
low for a direct comparison of the vector spaces,
which is the main goal of this paper.
For each vector space and model, a number of
different thresholds were tested on the first 2000
pairs of the training set, which we used as a de-
velopment set; in each case, the best-performed
threshold was selected for a single run of our
?classifier? on the test set (1726 pairs). Addition-
ally, we evaluate the NWE model with a lemma-
tized version of the corpus, so that the experimen-
tal setup is maximally similar for all vector spaces.
The results are shown in the first part of Table 5.
Additive NWE gives the highest performance,
with both lemmatized and un-lemmatized versions
outperforming the GS11 and KS14 spaces. In
the un-lemmatized case, the accuracy of our sim-
ple ?classifier? (0.73) is close to state-of-the-art
range. The state-of-the art result (0.77 accuracy
714
Co-occurrence Neural word embeddings
Baseline GS11 KS14 Unlemmatized Lemmatized
Model Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score
MSR addition
0.65 0.75
0.62 0.79 0.70 0.80 0.73 0.82 0.72 0.81
MSR multiplication 0.52 0.58 0.66 0.80 0.42 0.34 0.41 0.36
SWDA addition
0.60 0.58
0.35 0.35 0.40 0.35 0.63 0.60 0.44 0.40
SWDA multiplication 0.32 0.16 0.39 0.33 0.58 0.53 0.43 0.38
Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results
significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, ?
2
test.
and 0.84 F-score
7
) by the time of this writing has
been obtained using 8 machine translation metrics
and three constituent classifiers (Madnani et al.,
2012).
The multiplicative model gives lower results
than the additive model across all vector spaces.
The KS14 vector space shows the steadiest per-
formance, with a drop in accuracy of only 0.04
and no drop in F-score, while for the GS11 and
NWE spaces both accuracy and F-score experi-
enced drops by more than 0.20.
5.4 Dialogue act tagging
As our last experiment, we evaluate the word
spaces on a dialogue act tagging task (Stolcke et
al., 2000) over the Switchboard corpus (Godfrey
et al., 1992). Switchboard is a collection of ap-
proximately 2500 dialogs over a telephone line by
500 speakers from the U.S. on predefined topics.
8
The experiment pipeline follows (Milajevs and
Purver, 2014). The input utterances are prepro-
cessed so that the parts of interrupted utterances
are concatenated (Webb et al., 2005). Disfluency
markers and commas are removed from the utter-
ance raw texts. For GS11 and KS14 the utterance
tokens are POS-tagged and lemmatized; for NWE,
we test the vectors in both a lemmatized and an
un-lemmatized version of the corpus.
9
We split
the training and testing utterances as suggested by
Stolcke et al. (2000). Utterance vectors are then
obtained as in the previous experiments; they are
reduced to 50 dimensions using SVD and a k-
nearest-neighbour classifier is trained on these re-
duced utterance vectors (the 5 closest neighbours
by Euclidean distance are retrieved to make a clas-
7
F-scores use the standard definition F = 2(precision ?
recall)/(precision + recall).
8
The dataset and a Python interface to it are available
at http://compprag.christopherpotts.net/
swda.html
9
We use WordNetLemmatizer of the NLTK library
(Bird, 2006).
sification decision). The results are shown in the
second part of Table 5.
Un-lemmatized NWE addition gave the best ac-
curacy (0.63) and F-score (0.60) (averaged over
tag classes), i.e. similar results to (Milajevs and
Purver, 2014)?although note that the dimension-
ality of our NWE vectors is 10 times lower than
theirs. Multiplicative NWE outperformed the cor-
responding model in (Milajevs and Purver, 2014).
In general, addition consistently outperforms mul-
tiplication for all the models. Lemmatization
dramatically lowers tagging accuracy: the lem-
matized GS11, KS14 and NWE models perform
much worse than un-lemmatized NWE, suggest-
ing that morphological features are important for
this task.
6 Discussion
Previous comparisons of co-occurrence-based and
neural word vector representations vary widely
in their conclusions. While Baroni et al. (2014)
conclude that ?context-predicting models obtain
a thorough and resounding victory against their
count-based counterparts?, this seems to contra-
dict, at least at the first consideration, the more
conservative conclusion of Levy et al. (2014) that
?analogy recovery is not restricted to neural word
embeddings [. . . ] a similar amount of relational
similarities can be recovered from traditional dis-
tributional word representations? and the findings
of Blacoe and Lapata (2012) that ?shallow ap-
proaches are as good as more computationally in-
tensive alternatives? on phrase similarity and para-
phrase detection tasks.
It seems clear that neural word embeddings
have an advantage when used in tasks for which
they have been trained; our main questions here
are whether they outperform co-occurrence based
alternatives across the board; and which ap-
proach lends itself better to composition using
general mathematical operators. To partially an-
715
swer this question, we can compare model be-
haviour against the baselines in isolation.
For the disambiguation and sentence similarity
tasks the baseline is the similarity between verbs
only, ignoring the context?see above. For the
paraphrase task, we take the global vector-based
similarity reported in (Mihalcea et al., 2006): 0.65
accuracy and 0.75 F-score. For the dialogue act
tagging task the baseline is the accuracy of the
bag-of-unigrams model in (Milajevs and Purver,
2014): 0.60.
Sections 5.1 and 5.2 show that although the best
choice of vector representation might vary, for
small-scale tasks all methods give fairly compet-
itive results. The choice of compositional oper-
ator seems to be more important and more task-
specific: while a tensor-based operation (Frobe-
nius copy-object) performs best for verb disam-
biguation, the best result for sentence similarity
is achieved by a simple additive model, with all
other compositional methods behaving worse than
the verb-only baseline in the KS14 case. GS11 and
NWE, on the other hand, outperform their base-
lines with a number of compositional methods, al-
though both of them achieve lower performance
than KS14 overall.
Based on only small-scale experiment results,
one could conclude that there is little significant
difference between the two ways of obtaining vec-
tors. GS11 and NWE show similar behaviour in
comparison to their baselines, while it is possible
to tune a co-occurrence based vector space (KS14)
and obtain the best result. Large scale tasks reveal
another pattern: the GS11 vector space, which be-
haves stably on the small scale, drags behind the
KS14 and NWE spaces in the paraphrase detec-
tion task. In addition, NWE consistently yields
best results. Finally, only the NWE space was able
to provide adequate results on the dialogue act tag-
ging task. Table 6 summarizes model performance
with regard to baselines.
7 Conclusion
In this work we compared the performance of two
co-occurrence-based semantic spaces with vectors
learned by a neural network in compositional set-
tings. We carried out two small-scale tasks (word
sense disambiguation and sentence similarity) and
two large-scale tasks (paraphrase detection and di-
alogue act tagging).
Task GS11 KS14 NWE
Disambiguation + + +
Sentence similarity + ? +
Paraphrase ? + +
Dialog act tagging ? ? +
Table 6: Summary of vector space performance
against baselines. General improvement (cases
where more than a half of the models perform bet-
ter) and decrease with regard to a corresponding
baseline is respectively marked by + and ?. A
bold value means that the model gave the best re-
sult in the task.
On small-scale tasks, where the sentence struc-
tures are predefined and relatively constrained,
NWE gives better or similar results to count-based
vectors. Tensor-based composition does not al-
ways outperform simple compositional operators,
but for most of the cases gives results within the
same range.
On large-scale tasks, neural vectors are more
successful than the co-occurrence based alterna-
tives. However, this study does not reveal whether
this is because of their neural nature, or just be-
cause they are trained on a larger amount of data.
The question of whether neural vectors outper-
form co-occurrence vectors therefore requires fur-
ther detailed comparison to be entirely resolved;
our experiments suggest that this is indeed the case
in large-scale tasks, but the difference in size and
nature of the original corpora may be a confound-
ing factor. In any case, it is clear that the neural
vectors of word2vec package perform steadily
off-the-shelf across a large variety of tasks. The
size of the vector space (3 million words) and the
available code-base that simplifies the access to
the vectors, makes this set a good and safe choice
for experiments in the future. Of course, even bet-
ter performances can be achieved by training neu-
ral language models specifically for a given task
(see e.g. Kalchbrenner et al. (2014)).
The choice of compositional operator (tensor-
based or a simple point-wise operation) depends
strongly on the task and dataset: tensor-based
composition performed best with the verb dis-
ambiguation task, where the verb senses depend
strongly on the arguments of the verb. However, it
seems to depend less on the nature of the vectors
itself: in the disambiguation task, tensor-based
716
composition proved best for both co-occurrence-
based and neural vectors; in the sentence similar-
ity task, where point-wise operators proved best,
this was again true across vector spaces.
Acknowledgements
We would like to thank the three anonymous
reviewers for their fruitful comments. Sup-
port by EPSRC grant EP/F042728/1 is grate-
fully acknowledged by Milajevs, Kartsaklis and
Sadrzadeh. Purver is partly supported by Con-
CreTe: the project ConCreTe acknowledges the fi-
nancial support of the Future and Emerging Tech-
nologies (FET) programme within the Seventh
Framework Programme for Research of the Eu-
ropean Commission, under FET grant number
611733.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, volume 1.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Steven Bird. 2006. NLTK: the natural language
toolkit. In Proceedings of the COLING/ACL on In-
teractive presentation sessions, pages 69?72. Asso-
ciation for Computational Linguistics.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546?556. Association for Compu-
tational Linguistics.
Johan Bos and Malte Gabsdil. 2000. First-order infer-
ence and the interpretation of questions and answers.
Proceedings of Gotelog, pages 43?50.
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
N. Bourbaki. 1989. Commutative Algebra: Chapters
1-7. Srpinger Verlag, Berlin/New York.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. CoRR,
abs/1003.4394.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 3(2-3):281?332.
Bill Dolan, Chris Brockett, and Chris Quirk. 2005. Mi-
crosoft research paraphrase corpus. Retrieved May,
29:2013.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47?54.
Gottlob Frege. 1892. On sense and reference. Ludlow
(1997), pages 563?584.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517?520. IEEE.
Yoav Goldberg and Omer Levy. 2014. word2vec
Explained: deriving Mikolov et al.?s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1394?1404.
Association for Computational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of the GEMS 2011 Work-
shop on GEometrical Models of Natural Language
Semantics, pages 62?66, Edinburgh, UK, July. As-
sociation for Computational Linguistics.
Z.S. Harris. 1954. Distributional structure. Word.
717
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, pages 119?126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, June.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNL), pages 1590?1601, Seat-
tle, USA, October. Association for Computational
Linguistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A
study of entanglement in a categorical framework of
natural language. In Proceedings of the 11th Work-
shop on Quantum Physics and Logic (QPL), Kyoto,
Japan, June.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for
categorical distributional-compositional semantics:
Theory and experiments. In Proceedings of COL-
ING 2012: Posters, pages 549?558, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Douwe Kiela and Stephen Clark. 2014. A systematic
study of semantic vector space model parameters.
In Proceedings of the 2nd Workshop on Continu-
ous Vector Space Models and their Compositionality
(CVSC), pages 21?30, Gothenburg, Sweden, April.
Association for Computational Linguistics.
T. Landauer and S. Dumais. 1997. A Solution
to Plato?s Problem: The Latent Semantic Analysis
Theory of Acquision, Induction, and Representation
of Knowledge. Psychological Review.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Omer Levy, Yoav Goldberg, and Israel Ramat-Gan.
2014. Linguistic regularities in sparse and explicit
word representations. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 182?190. Asso-
ciation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775?780.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746?751.
Dmitrijs Milajevs and Matthew Purver. 2014. Inves-
tigating the contribution of distributional semantic
information for dialogue act classification. In Pro-
ceedings of the 2nd Workshop on Continuous Vector
Space Models and their Compositionality (CVSC),
pages 40?47, Gothenburg, Sweden, April. Associa-
tion for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244. Association for
Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1439.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36(3):373?398.
Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 230?
238, Gothenburg, Sweden, April. Association for
Computational Linguistics.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45?
50, Valletta, Malta, May. ELRA. http://is.
muni.cz/publication/884893/en.
Hinrich Sch?utze. 1997. Ambiguity resolution in natu-
ral language learning. csli. Stanford, CA, 4:12?36.
718
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Carol Van Ess-Dykema, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Peter D Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Nick Webb, Mark Hepple, and Yorick Wilks. 2005.
Dialogue act classification based on intra-utterance
features. In Proceedings of the AAAI Workshop on
Spoken Language Understanding. Citeseer.
719
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 482?491,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Experimenting with Distant Supervision for Emotion Classification
Matthew Purver? and Stuart Battersby?
?
Interaction Media and Communication Group
?
Chatterbox Analytics
School of Electronic Engineering and Computer Science
Queen Mary University of London
Mile End Road, London E1 4NS, UK
m.purver@qmul.ac.uk stuart@cbanalytics.co.uk
Abstract
We describe a set of experiments using au-
tomatically labelled data to train supervised
classifiers for multi-class emotion detection
in Twitter messages with no manual inter-
vention. By cross-validating between mod-
els trained on different labellings for the
same six basic emotion classes, and testing
on manually labelled data, we conclude that
the method is suitable for some emotions
(happiness, sadness and anger) but less able
to distinguish others; and that different la-
belling conventions are more suitable for
some emotions than others.
1 Introduction
We present a set of experiments into classify-
ing Twitter messages into the six basic emotion
classes of (Ekman, 1972). The motivation behind
this work is twofold: firstly, to investigate the pos-
sibility of detecting emotions of multiple classes
(rather than purely positive or negative sentiment)
in such short texts; and secondly, to investigate
the use of distant supervision to quickly bootstrap
large datasets and classifiers without the need for
manual annotation.
Text classification according to emotion and
sentiment is a well-established research area. In
this and other areas of text analysis and classifica-
tion, recent years have seen a rise in use of data
from online sources and social media, as these
provide very large, often freely available datasets
(see e.g. (Eisenstein et al 2010; Go et al 2009;
Pak and Paroubek, 2010) amongst many others).
However, one of the challenges this poses is that
of data annotation: given very large amounts of
data, often consisting of very short texts, written
in unconventional style and without accompany-
ing metadata, audio/video signals or access to the
author for disambiguation, how can we easily pro-
duce a gold-standard labelling for training and/or
for evaluation and test? One possible solution
that is becoming popular is crowd-sourcing the la-
belling task, as the easy access to very large num-
bers of annotators provided by tools such as Ama-
zon?s Mechanical Turk can help with the problem
of dataset size; however, this has its own attendant
problems of annotator reliability (see e.g. (Hsueh
et al 2009)), and cannot directly help with the in-
herent problem of ambiguity ? using many anno-
tators does not guarantee that they can understand
or correctly assign the author?s intended interpre-
tation or emotional state.
In this paper, we investigate a different ap-
proach via distant supervision (see e.g. (Mintz
et al 2009)). By using conventional markers of
emotional content within the texts themselves as
a surrogate for explicit labels, we can quickly re-
trieve large subsets of (noisily) labelled data. This
approach has the advantage of giving us direct
access to the authors? own intended interpreta-
tion or emotional state, without relying on third-
party annotators. Of course, the labels themselves
may be noisy: ambiguous, vague or not having
a direct correspondence with the desired classi-
fication. We therefore experiment with multiple
such conventions with apparently similar mean-
ings ? here, emoticons (following (Read, 2005))
and Twitter hashtags ? allowing us to examine the
similarity of classifiers trained on independent la-
bels but intended to detect the same underlying
class. We also investigate the precision and cor-
respondence of particular labels with the desired
emotion classes by testing on a small set of man-
482
ually labelled data.
We show that the success of this approach de-
pends on both the conventional markers chosen
and the emotion classes themselves. Some emo-
tions are both reliably marked by different con-
ventions and distinguishable from other emotions;
this seems particularly true for happiness, sadness
and anger, indicating that this approach can pro-
vide not only the basic distinction required for
sentiment analysis but some more finer-grained
information. Others are either less distinguishable
from short text messages, or less reliably marked.
2 Related Work
2.1 Emotion and Sentiment Classification
Much research in this area has concentrated on the
related tasks of subjectivity classification (distin-
guishing objective from subjective texts ? see e.g.
(Wiebe and Riloff, 2005)); and sentiment classifi-
cation (classifying subjective texts into those that
convey positive, negative and neutral sentiment ?
see e.g. (Pang and Lee, 2008)). We are interested
in emotion detection: classifying subjective texts
according to a finer-grained classification of the
emotions they convey, and thus providing richer
and more informative data for social media anal-
ysis than simple positive/negative sentiment. In
this study we confine ourselves to the six basic
emotions identified by Ekman (1972) as being
common across cultures; other finer-grained clas-
sifications are of course available.
2.1.1 Emotion Classification
The task of emotion classification is by nature
a multi-class problem, and classification experi-
ments have therefore achieved lower accuracies
than seen in the binary problems of sentiment and
subjectivity classification. Danisman and Alpko-
cak (2008) used vector space models for the same
six-way emotion classification we examine here,
and achieved F-measures around 32%; Seol et al
(2008) used neural networks for an 8-way clas-
sification (hope, love, thank, neutral, happy, sad,
fear, anger) and achieved per-class accuracies of
45% to 65%. Chuang and Wu (2004) used su-
pervised classifiers (SVMs) and manually defined
keyword features over a seven-way classification
consisting of the same six-class taxonomy plus a
neutral category, and achieved an average accu-
racy of 65.5%, varying from 56% for disgust to
74% for anger. However, they achieved signifi-
cant improvements using acoustic features avail-
able in their speech data, improving accuracies up
to a maximum of 81.5%.
2.2 Conventions
As we are using text data, such intonational and
prosodic cues are unavailable, as are the other
rich sources of emotional cues we obtain from
gesture, posture and facial expression in face-to-
face communication. However, the prevalence of
online text-based communication has led to the
emergence of textual conventions understood by
the users to perform some of the same functions
as these acoustic and non-verbal cues. The most
familiar of these is the use of emoticons, either
Western-style (e.g. :), :-( etc.) or Eastern-style
(e.g. (?_?), (>_<) etc.). Other conventions
have emerged more recently for particular inter-
faces or domains; in Twitter data, one common
convention is the use of hashtags to add or em-
phasise emotional content ? see (1).
(1) a. Best day in ages! #Happy :)
b. Gets so #angry when tutors don?t email
back... Do you job idiots!
Linguistic and social research into the use of
such conventions suggests that their function is
generally to emphasise or strengthen the emo-
tion or sentiment conveyed by a message, rather
than to add emotional content which would not
otherwise be present. Walther and D?Addario
(2001) found that the contribution of emoticons
towards the sentiment of a message was out-
weighed by the verbal content, although nega-
tive ones tended to shift interpretation towards the
negative. Ip (2002) experimented with emoticons
in instant messaging, with the results suggesting
that emoticons do not add positivity or negativ-
ity but rather increase valence (making positive
messages more positive and vice versa). Similarly
Derks et al(2008a; 2008b) found that emoticons
are used in strengthening the intensity of a ver-
bal message (although they serve other functions
such as expressing humour), and hypothesized
that they serve similar functions to actual non-
verbal behavior; Provine et al(2007) also found
that emoticons are used to ?punctuate? messages
rather than replace lexical content, appearing in
similar grammatical locations to verbal laughter
and preserving phrase structure.
483
2.3 Distant Supervision
These findings suggest, of course, that emoticons
and related conventional markers are likely to be
useful features for sentiment and emotion classifi-
cation. They also suggest, though, that they might
be used as surrogates for manual emotion class la-
bels: if their function is often to complement the
verbal content available in messages, they should
give us a way to automatically label messages ac-
cording to emotional class, while leaving us with
messages with enough verbal content to achieve
reasonable classification.
This approach has been exploited in several
ways in recent work; Tanaka et al(2005) used
Japanese-style emoticons as classification labels,
and Go et al(2009) and Pak and Paroubek (2010)
used Western-style emoticons to label and classify
Twitter messages according to positive and nega-
tive sentiment, using traditional supervised clas-
sification methods. The highest accuracies ap-
pear to have been achieved by Go et al(2009),
who used various combinations of features (un-
igrams, bigrams, part-of-speech tags) and clas-
sifiers (Na??ve Bayes, maximum entropy, and
SVMs), achieving their best accuracy of 83.0%
with unigram and bigram features and a maxi-
mum entropy; using only unigrams with a SVM
classifier achieved only slightly lower accuracy at
82.2%. Ansari (2010) then provides an initial in-
vestigation into applying the same methods to six-
way emotion classification, treating each emotion
independently as a binary classification problem
and showing that accuracy varied with emotion
class as well as with dataset size. The highest ac-
curacies achieved were up to 81%, but these were
on very small datasets (e.g. 81.0% accuracy on
fear, but with only around 200 positive and nega-
tive data instances).
We view this approach as having several ad-
vantages; apart from the ease of data collection
it allows by avoiding manual annotation, it gives
us access to the author?s own intended interpeta-
tions, as the markers are of course added by the
authors themselves at time of writing. In some
cases such as the examples of (1) above, the emo-
tion conveyed may be clear to a third-party anno-
tator; but in others it may not be clear at all with-
out the marker ? see (2):
(2) a. Still trying to recover from seeing the
#bluewaffle on my TL #disgusted #sick
b. Leftover ToeJams with Kettle Salt and
Vinegar chips. #stress #sadness #comfort
#letsturnthisfrownupsidedown
3 Methodology
We used a collection of Twitter messages, all
marked with emoticons or hashtags correspond-
ing to one of Ekman (1972)?s six emotion classes.
For emoticons, we used Ansari (2010)?s taxon-
omy, taken from the Yahoo messenger classifica-
tion. For hashtags, we used emotion names them-
selves together with the main related adjective ?
both are used commonly on Twitter in slightly
different ways as shown in (3); note that emo-
tion names are often used as marked verbs as well
as nouns. Details of the classes and markers are
given in Table 1.
(3) a. Gets so #angry when tutors don?t email
back... Do you job idiots!
b. I?m going to say it, Paranormal Activity
2 scared me and I didn?t sleep well last
night because of it. #fear #demons
c. Girls that sleep w guys without even fully
getting to know them #disgust me
Messages with multiple conventions (see (4))
were collected and used in the experiments, ensur-
ing that the marker being used as a label in a par-
ticular experiment was not available as a feature in
that experiment. Messages with no markers were
not collected. While this prevents us from exper-
imenting with the classification of neutral or ob-
jective messages, it would require manual anno-
tation to distinguish these from emotion-carrying
messages which are not marked. We assume that
any implementation of the techniques we investi-
gate here would be able to use a preliminary stage
of subjectivity and/or sentiment detection to iden-
tify these messages, and leave this aside here.
(4) a. just because people are celebs they dont
reply to your tweets! NOT FAIR #Angry
:( I wish They would reply! #Please
Data was collected from Twitter?s Streaming
API service.1 This provides a 1-2% random sam-
ple of all tweets with no constraints on language
1See http://dev.twitter.com/docs/
streaming-api.
484
Table 1: Conventional markers used for emotion
classes.
happy :-) :) ;-) :D :P 8) 8-| <@o
sad :-( :( ;-( :-< :?(
anger :-@ :@
fear :| :-o :-O
surprise :s :S
disgust :$ +o(
happy #happy #happiness
sad #sad #sadness
anger #angry #anger
fear #scared #fear
surprise #surprised #surprise
disgust #disgusted #disgust
or location. These are collected in near real time
and stored in a local database. An English lan-
guage selection filter was applied; scripts collect-
ing each conventional marker set were alternated
throughout different times of day and days of the
week to avoid any bias associated with e.g. week-
ends or mornings. The numbers of messages col-
lected varied with the popularity of the markers
themselves: for emoticons, we obtained a max-
imum of 837,849 (for happy) and a minimum
of 10,539 for anger; for hashtags, a maximum
of 10,219 for happy and a minimum of 536 for
disgust. 2
Classification in all experiments was using sup-
port vector machines (SVMs) (Vapnik, 1995) via
the LIBSVM implementation of Chang and Lin
(2001) with a linear kernel and unigram features.
Unigram features included all words and hashtags
(other than those used as labels in relevant exper-
iments) after removal of URLs and Twitter user-
names. Some improvement in performance might
be available using more advanced features (e.g.
n-grams), other classification methods (e.g. maxi-
mum entropy, as lexical features are unlikely to be
independent) and/or feature weightings (e.g. the
variant of TFIDF used for sentiment classification
by Martineau (2009)). Here, our interest is more
in the difference between the emotion and con-
vention marker classes - we leave investigation of
2One possible way to increase dataset sizes for the rarer
markers might be to include synonyms in the hashtag names
used; however, people?s use and understanding of hashtags is
not straightforwardly predictable from lexical form. Instead,
we intend to run a longer-term data gathering exercise.
absolute performance for future work.
4 Experiments
Throughout, the markers (emoticons and/or hash-
tags) used as labels in any experiment were re-
moved before feature extraction in that experi-
ment ? labels were not used as features.
4.1 Experiment 1: Emotion detection
To simulate the task of detecting emotion classes
from a general stream of messages, we first built
for each convention type C and each emotion
class E a dataset DCE of size N containing (a)
as positive instances, N/2 messages containing
markers of the emotion class E and no other
markers of type C, and (b) as negative instances,
N/2 messages containing markers of type C of
any other emotion class. For example, the posi-
tive instance set for emoticon-marked anger was
based on those tweets which contained :-@ or
:@, but none of the emoticons from the happy,
sad, surprise, disgust or fear classes;
any hashtags were allowed, including those as-
sociated with emotion classes. The negative in-
stance set contained a representative sample of
the same number of instances, with each having
at least one of the happy, sad, surprise,
disgust or fear emoticons but not containing
:-@ or :@.
This of course excludes messages with no emo-
tional markers; for this to act as an approximation
of the general task therefore requires a assump-
tion that unmarked messages reflect the same dis-
tribution over emotion classes as marked mes-
sages. For emotion-carrying but unmarked mes-
sages, this does seem intuitively likely, but re-
quires investigation. For neutral objective mes-
sages it is clearly false, but as stated above we as-
sume a preliminary stage of subjectivity detection
in any practical application.
Performance was evaluated using 10-fold
cross-validation. Results are shown as the bold
figures in Table 2; despite the small dataset
sizes in some cases, a ?2 test shows all to be
significantly different from chance. The best-
performing classes show accuracies very similar
to those achieved by Go et al(2009) for their bi-
nary positive/negative classification, as might be
expected; for emoticon markers, the best classes
are happy, sad and anger; interestingly the
best classes for hashtag markers are not the same
485
Table 2: Experiment 1: Within-class results. Same-
convention (bold) figures are accuracies over 10-fold
cross-validation; cross-convention (italic) figures are
accuracies over full sets.
Train
Convention Test emoticon hashtag
emoticon happy 79.8% 63.5%
emoticon sad 79.9% 65.5%
emoticon anger 80.1% 62.9%
emoticon fear 76.2% 58.5%
emoticon surprise 77.4% 48.2%
emoticon disgust 75.2% 54.6%
hashtag happy 67.7% 82.5%
hashtag sad 67.1% 74.6%
hashtag anger 62.8% 74.7%
hashtag fear 60.6% 77.2%
hashtag surprise 51.9% 67.4%
hashtag disgust 64.6% 78.3%
? happy performs best, but disgust and fear
outperform sad and anger, and surprise
performs particularly badly. For sad, one reason
may be a dual meaning of the tag #sad (one emo-
tional and one expressing ridicule); for anger
one possibility is the popularity on Twitter of the
game ?Angry Birds?; for surprise, the data
seems split between two rather distinct usages,
ones expressing the author?s emotion, but one ex-
pressing an intended effect on the audience (see
(5)). However, deeper analysis is needed to estab-
lish the exact causes.
(5) a. broke 100 followers. #surprised im glad
that the HOFF is one of them.
b. Who?s excited for the Big Game? We
know we are AND we have a #surprise
for you!
To investigate whether the different conven-
tion types actually convey similar properties (and
hence are used to mark similar messages) we then
compared these accuracies to those obtained by
training classifiers on the dataset for a different
convention: in other words, for each emotion
class E, train a classifier on dataset DC1E and test
on DC2E . As the training and testing sets are dif-
ferent, we now test on the entire dataset rather
than using cross-validation. Results are shown as
the italic figures in Table 2; a ?2 test shows all
to be significantly different from the bold same-
convention results. Accuracies are lower overall,
but the highest figures (between 63% and 68%)
are achieved for happy, sad and anger; here
perhaps we can have some confidence that not
only are the markers acting as predictable labels
themselves, but also seem to be labelling the same
thing (and therefore perhaps are actually labelling
the emotion we are hoping to label).
4.2 Experiment 2: Emotion discrimination
To investigate whether these independent clas-
sifiers can be used in multi-class classification
(distinguishing emotion classes from each other
rather than just distinguishing one class from a
general ?other? set), we next cross-tested the clas-
sifiers between emotion classes: training models
on one emotion and testing on the others ? for
each convention type C and each emotion class
E1, train a classifier on dataset DCE1 and test on
DCE2, D
C
E3 etc. The datasets in Experiment 1 had
an uneven balance of emotion classes (including a
high proportion of happy instances) which could
bias results; for this experiment, therefore, we cre-
ated datasets with an even balance of emotions
among the negative instances. For each conven-
tion type C and each emotion class E1, we built
a dataset DCE1 of size N containing (a) as pos-
itive instances, N/2 messages containing mark-
ers of the emotion class E1 and no other mark-
ers of type C, and (b) as negative instances, N/2
messages consisting of N/10 messages contain-
ing only markers of class E2, N/10 messages
containing only markers of class E3 etc. Results
were then generated as in Experiment 1.
Within-class results are shown in Table 3 and
are similar to those obtained in Experiment 1;
again, differences between bold/italic results are
statistically significant. Cross-class results are
shown in Table 4. The happy class was well
distinguished from other emotion classes for both
convention types (i.e. cross-class classification
accuracy is low compared to the within-class fig-
ures in italics and parentheses). The sad class
also seems well distinguished when using hash-
tags as labels, although less so when using emoti-
cons. However, other emotion classes show a sur-
prisingly high cross-class performance in many
cases ? in other words, they are producing dis-
appointingly similar classifiers.
This poor discrimination for negative emotion
classes may be due to ambiguity or vagueness in
the label, similarity of the verbal content associ-
486
Table 4: Experiment 2: Cross-class results. Same-class figures from 10-fold cross-validation are shown in
(italics) for comparison; all other figures are accuracies over full sets.
Train
Convention Test happy sad anger fear surprise disgust
emoticon happy (78.1%) 17.3% 39.6% 26.7% 28.3% 42.8%
emoticon sad 16.5% (78.9%) 59.1% 71.9% 69.9% 55.5%
emoticon anger 29.8% 67.0% (79.7%) 74.2% 76.4% 67.5%
emoticon fear 27.0% 69.9% 64.4% (75.3%) 74.0% 61.2%
emoticon surprise 25.4% 69.9% 67.7% 76.3% (78.1%) 66.4%
emoticon disgust 42.2% 54.4% 61.1% 64.2% 64.1% (73.9%)
hashtag happy (81.1%) 10.7% 45.3% 47.8% 52.7% 43.4%
hashtag sad 13.8% (77.9%) 47.7% 49.7% 46.5% 54.2%
hashtag anger 44.6% 45.2% (74.3%) 72.0% 63.0% 62.9%
hashtag fear 45.0% 50.4% 68.6% (74.7%) 63.9% 60.7%
hashtag surprise 51.5% 45.7% 67.4% 70.7% (70.2%) 64.2%
hashtag disgust 40.4% 53.5% 74.7% 71.8% 70.8% (74.2%)
Table 3: Experiment 2: Within-class results. Same-
convention (bold) figures are accuracies over 10-fold
cross-validation; cross-convention (italic) figures are
accuracies over full sets.
Train
Convention Test emoticon hashtag
emoticon happy 78.1% 61.2%
emoticon sad 78.9% 60.2%
emoticon anger 79.7% 63.7%
emoticon fear 75.3% 55.9%
emoticon surprise 78.1% 53.1%
emoticon disgust 73.9% 51.5%
hashtag happy 68.7% 81.1%
hashtag sad 65.4% 77.9%
hashtag anger 63.9% 74.3%
hashtag fear 58.9% 74.7%
hashtag surprise 51.8% 70.2%
hashtag disgust 55.4% 74.2%
ated with the emotions, or of genuine frequent co-
presence of the emotions. Given the close lex-
ical specification of emotions in hashtag labels,
the latter reasons seem more likely; however, with
emoticon labels, we suspect that the emoticons
themselves are often used in ambiguous or vague
ways.
As one way of investigating this directly, we
tested classifiers across labelling conventions as
well as across emotion classes, to determine
whether the (lack of) cross-class discrimination
holds across convention marker types. In the
case of ambiguity or vagueness of emoticons,
we would expect emoticon-trained models to fail
to discriminate hashtag-labelled test sets, but
hashtag-trained models to discriminate emoticon-
labelled test sets well; if on the other hand the
cause lies in the overlap of verbal content or the
emotions themselves, the effect should be simi-
lar in either direction. This experiment also helps
determine in more detail whether the labels used
label similar underlying properties.
Table 5 shows the results. For the three classes
happy, sad and perhaps anger, models trained
using emoticon labels do a reasonable job of dis-
tinguishing classes in hashtag-labelled data, and
vice versa. However, for the other classes, dis-
crimination is worse. Emoticon-trained mod-
els appear to give (undesirably) higher perfor-
mance across emotion classes in hashtag-labelled
data (for the problematic non-happy classes).
Hashtag-trained models perform around the ran-
dom 50% level on emoticon-labelled data for
those classes, even when tested on nominally
the same emotion as they are trained on. For
both label types, then, the lower within-class and
higher cross-class performance with these nega-
tive classes (fear, surprise, disgust) sug-
gests that these emotion classes are genuinely
hard to tell apart (they are all negative emotions,
and may use similar words), or are simply of-
ten expressed simultaneously. The higher perfor-
mance of emoticon-trained classifiers compared
to hashtag-trained classifiers, though, also sug-
gests vagueness or ambiguity in emoticons: data
labelled with emoticons nominally thought to be
487
Table 5: Experiment 2: Cross-class, cross-convention results (train on hashtags, test on emoticons and vice
versa). All figures are accuracies over full sets. Accuracies over 60% are shown in bold.
Train
Convention Test happy sad anger fear surprise disgust
emoticon happy 61.2% 40.4% 44.1% 47.4% 52.0% 45.9%
emoticon sad 38.3% 60.2% 55.1% 51.5% 47.1% 53.9%
emoticon anger 47.0% 48.0% 63.7% 56.2% 50.9% 56.6%
emoticon fear 39.8% 57.7% 57.1% 55.9% 50.8% 56.1%
emoticon surprise 43.7% 55.2% 59.2% 58.4% 53.1% 54.0%
emoticon disgust 51.5% 48.0% 53.5% 55.1% 53.1% 51.5%
hashtag happy 68.7% 32.5% 43.6% 32.1% 35.4% 50.4%
hashtag sad 33.8% 65.4% 53.2% 65.0% 61.8% 48.8%
hashtag anger 43.9% 55.5% 63.9% 59.6% 60.4% 53.0%
hashtag fear 44.3% 54.6% 56.1% 58.9% 61.5% 54.3%
hashtag surprise 54.2% 45.3% 49.8% 49.9% 51.8% 52.3%
hashtag disgust 41.5% 57.6% 61.6% 62.2% 59.3% 55.4%
associated with surprise produces classifiers
which perform well on data labelled with many
other hashtag classes, suggesting that those emo-
tions were present in the training data. Con-
versely, the more specific hashtag labels produce
classifiers which perform poorly on data labelled
with emoticons and which thus contains a range
of actual emotions.
4.3 Experiment 3: Manual labelling
To confirm whether either (or both) set of auto-
matic (distant) labels do in fact label the under-
lying emotion class intended, we used human an-
notators via Amazon?s Mechanical Turk to label
a set of 1,000 instances. These instances were all
labelled with emoticons (we did not use hashtag-
labelled data: as hashtags are so lexically close to
the names of the emotion classes being labelled,
their presence may influence labellers unduly)3
and were evenly distributed across the 6 classes,
in so far as indicated by the emoticons. Labellers
were asked to choose the primary emotion class
(from the fixed set of six) associated with the mes-
sage; they were also allowed to specify if any
other classes were also present. Each data in-
stance was labelled by three different annotators.
Agreement between labellers was poor over-
all. The three annotators unanimously agreed in
only 47% of cases overall; although two of three
agreed in 83% of cases. Agreement was worst
3Although, of course, one may argue that they do the
same for their intended audience of readers ? in which case,
such an effect is legitimate.
for the three classes already seen to be prob-
lematic: surprise, fear and disgust. To
create our dataset for this experiment, we there-
fore took only instances which were given the
same primary label by all labellers ? i.e. only
those examples which we could take as reliably
and unambiguously labelled. This gave an un-
balanced dataset, with numbers varying from 266
instances for happy to only 12 instances for
each of surprise and fear. Classifiers were
trained using the datasets from Experiment 2. Per-
formance is shown in Table 6; given the imbal-
ance between class numbers in the test dataset,
evaluation is given as recall, precision and F-score
for the class in question rather than a simple accu-
racy figure (which is biased by the high proportion
of happy examples).
Table 6: Experiment 3: Results on manual labels.
Train Class Precision Recall F-score
emoticon happy 79.4% 75.6% 77.5%
emoticon sad 43.5% 73.2% 54.5%
emoticon anger 62.2% 37.3% 46.7%
emoticon fear 6.8% 63.6% 12.3%
emoticon surprise 15.0% 90.0% 25.7%
emoticon disgust 8.3% 25.0% 12.5%
hashtag happy 78.9% 51.9% 62.6%
hashtag sad 47.9% 81.7% 60.4%
hashtag anger 58.2% 76.0% 65.9%
hashtag fear 10.1% 81.8% 18.0%
hashtag surprise 5.9% 60.0% 10.7%
hashtag disgust 6.7% 66.7% 11.8%
488
Again, results for happy are good, and cor-
respond fairly closely to the levels of accuracy
reported by Go et al(2009) and others for the
binary positive/negative sentiment detection task.
Emoticons give significantly better performance
than hashtags here. Results for sad and anger
are reasonable, and provide a baseline for fur-
ther experiments with more advanced features and
classification methods once more manually anno-
tated data is available for these classes. In con-
trast, hashtags give much better performance with
these classes than the (perhaps vague or ambigu-
ous) emoticons.
The remaining emotion classes, however, show
poor performance for both labelling conventions.
The observed low precision and high recall can be
adjusted using classifier parameters, but F-scores
are not improved. Note that Experiment 1 shows
that both emoticon and hashtag labels are to some
extent predictable, even for these classes; how-
ever, Experiment 2 shows that they may not be
reliably different to each other, and Experiment 3
tells us that they do not appear to coincide well
with human annotator judgements of emotions.
More reliable labels may therefore be required;
although we do note that the low reliability of
the human annotations for these classes, and the
correspondingly small amount of annotated data
used in this evaluation, means we hesitate to draw
strong conclusions about fear, surprise and
disgust. An approach which considers multi-
ple classes to be associated with individual mes-
sages may also be beneficial: using majority-
decision labels rather than unanimous labels im-
proves F-scores for surprise to 23-35% by in-
cluding many examples also labelled as happy
(although this gives no improvements for other
classes).
5 Survey
To further detemine whether emoticons used
as emotion class labels are ambiguous or vague
in meaning, we set up a web survey to exam-
ine whether people could reliably classify these
emoticons.
5.1 Method
Our survey asked people to match up which of
the six emotion classes (selected from a drop-
down menu) best matched each emoticon. Each
drop-down menu included a ?Not Sure? option.
To avoid any effect of ordering, the order of the
emoticon list and each drop-down menu was ran-
domised every time the survey page was loaded.
The survey was distributed via Twitter, Facebook
and academic mailing lists. Respondents were not
given the opportunity to give their own definitions
or to provide finer-grained classifications, as we
wanted to establish purely whether they would re-
liably associate labels with the six emotions in our
taxonomy.
5.2 Results
The survey was completed by 492 individuals;
full results are shown in Table 7. It demonstrated
agreement with the predefined emoticons for sad
and most of the emoticons for happy (people
were unsure what 8-| and <@o meant). For all
the emoticons listed as anger, surprise and
disgust, the survey showed that people are reli-
ably unsure as to what these mean. For the emoti-
con :-o there was a direct contrast between the
defined meaning and the survey meaning; the def-
inition of this emoticon following Ansari (2010)
was fear, but the survey reliably assigned this to
surprise.
Given the small scale of the survey, we hesi-
tate to draw strong conclusions about the emoti-
con meanings themselves (in fact, recent conver-
sations with schoolchildren ? see below ? have in-
dicated very different interpretations from these
adult survey respondents). However, we do con-
clude that for most emotions outside happy and
sad, emoticons may indeed be an unreliable la-
bel; as hashtags also appear more reliable in the
classification experiments, we expect these to be
a more promising approach for fine-grained emo-
tion discrimination in future.
6 Conclusions
The approach shows reasonable performance at
individual emotion label prediction, for both
emoticons and hashtags. For some emotions (hap-
piness, sadness and anger), performance across
label conventions (training on one, and testing on
the other) is encouraging; for these classes, per-
formance on those manually labelled examples
where annotators agree is also reasonable. This
gives us confidence not only that the approach
produces reliable classifiers which can predict the
labels, but that these classifiers are actually de-
tecting the desired underlying emotional classes,
489
Table 7: Survey results showing the defined emotion, the most popular emotion from the survey, the percentage
of votes this emotion received, and the ?2 significance test for the distribution of votes. These are indexed by
emoticon.
Emoticon Defined Emotion Survey Emotion % of votes Significance of votes distribution
:-) Happy Happy 94.9 ?2 = 3051.7 (p < 0.001)
:) Happy Happy 95.5 ?2 = 3098.2 (p < 0.001)
;-) Happy Happy 87.4 ?2 = 2541 (p < 0.001)
:D Happy Happy 85.7 ?2 = 2427.2 (p < 0.001)
:P Happy Happy 59.1 ?2 = 1225.4 (p < 0.001)
8) Happy Happy 61.9 ?2 = 1297.4 (p < 0.001)
8-| Happy Not Sure 52.2 ?2 = 748.6 (p < 0.001)
<@o Happy Not Sure 84.6 ?2 = 2335.1 (p < 0.001)
:-( Sad Sad 91.3 ?2 = 2784.2 (p < 0.001)
:( Sad Sad 89.0 ?2 = 2632.1 (p < 0.001)
;-( Sad Sad 67.9 ?2 = 1504.9 (p < 0.001)
:-< Sad Sad 56.1 ?2 = 972.59 (p < 0.001)
:?( Sad Sad 80.7 ?2 = 2116 (p < 0.001)
:-@ Anger Not Sure 47.8 ?2 = 642.47 (p < 0.001)
:@ Anger Not Sure 50.4 ?2 = 691.6 (p < 0.001)
:s Surprise Not Sure 52.2 ?2 = 757.7 (p < 0.001)
:$ Disgust Not Sure 62.8 ?2 = 1136 (p < 0.001)
+o( Disgust Not Sure 64.2 ?2 = 1298.1 (p < 0.001)
:| Fear Not Sure 55.1 ?2 = 803.41 (p < 0.001)
:-o Fear Surprise 89.2 ?2 = 2647.8 (p < 0.001)
without requiring manual annotation. We there-
fore plan to pursue this approach with a view to
improving performance by investigating training
with combined mixed-convention datasets, and
cross-training between classifiers trained on sepa-
rate conventions.
However, this cross-convention performance is
much better for some emotions (happiness, sad-
ness and anger) than others (fear, surprise and dis-
gust). Indications are that the poor performance
on these latter emotion classes is to a large de-
gree an effect of ambiguity or vagueness of the
emoticon and hashtag conventions we have used
as labels here; we therefore intend to investi-
gate other conventions with more specific and/or
less ambiguous meanings, and the combination
of multiple conventions to provide more accu-
rately/specifically labelled data. Another possi-
bility might be to investigate approaches to anal-
yse emoticons semantically on the basis of their
shape, or use features of such an analysis ? see
(Ptaszynski et al 2010; Radulovic and Milikic,
2009) for some interesting recent work in this di-
rection.
Acknowledgements
The authors are supported in part by the Engi-
neering and Physical Sciences Research Council
(grants EP/J010383/1 and EP/J501360/1) and the
Technology Strategy Board (R&D grant 700081).
We thank the reviewers for their comments.
References
Saad Ansari. 2010. Automatic emotion tone detection
in twitter. Master?s thesis, Queen Mary University
of London.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for Support Vector Machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Ze-Jing Chuang and Chung-Hsien Wu. 2004. Multi-
modal emotion recognition from speech and text.
Computational Linguistics and Chinese Language
Processing, 9(2):45?62, August.
Taner Danisman and Adil Alpkocak. 2008. Feeler:
Emotion classification of text using vector space
model. In AISB 2008 Convention, Communication,
Interaction and Social Intelligence, volume 2, pages
53?59, Aberdeen.
490
Daantje Derks, Arjan Bos, and Jasper von Grumbkow.
2008a. Emoticons and online message interpreta-
tion. Social Science Computer Review, 26(3):379?
388.
Daantje Derks, Arjan Bos, and Jasper von Grumbkow.
2008b. Emoticons in computer-mediated commu-
nication: Social motives and social context. Cy-
berPsychology & Behavior, 11(1):99?101, Febru-
ary.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 1277?1287,
Cambridge, MA, October. Association for Compu-
tational Linguistics.
Paul Ekman. 1972. Universals and cultural differ-
ences in facial expressions of emotion. In J. Cole,
editor, Nebraska Symposium on Motivation 1971,
volume 19. University of Nebraska Press.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervi-
sion. Master?s thesis, Stanford University.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: A study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 27?35, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Amy Ip. 2002. The impact of emoticons on affect in-
terpretation in instant messaging. Carnegie Mellon
University.
Justin Martineau. 2009. Delta TFIDF: An improved
feature space for sentiment analysis. Artificial In-
telligence, 29:258?261.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP 2009.
Alexander Pak and Patrick Paroubek. 2010. Twitter
as a corpus for sentiment analysis and opinion min-
ing. In Proceedings of the 7th conference on Inter-
national Language Resources and Evaluation.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Robert Provine, Robert Spencer, and Darcy Mandell.
2007. Emotional expression online: Emoticons
punctuate website text messages. Journal of Lan-
guage and Social Psychology, 26(3):299?307.
M. Ptaszynski, J. Maciejewski, P. Dybala, R. Rzepka,
and K Araki. 2010. CAO: A fully automatic emoti-
con analysis system based on theory of kinesics. In
Proceedings of The 24th AAAI Conference on Arti-
ficial Intelligence (AAAI-10), pages 1026?1032, At-
lanta, GA.
F. Radulovic and N. Milikic. 2009. Smiley ontology.
In Proceedings of The 1st International Workshop
On Social Networks Interoperability.
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for sen-
timent classification. In Proceedings of the 43rd
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguis-
tics.
Young-Soo Seol, Dong-Joo Kim, and Han-Woo Kim.
2008. Emotion recognition from text using knowl-
edge based ANN. In Proceedings of ITC-CSCC.
Y. Tanaka, H. Takamura, and M. Okumura. 2005. Ex-
traction and classification of facemarks with kernel
methods. In Proceedings of IUI.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
Joseph Walther and Kyle D?Addario. 2001. The
impacts of emoticons on message interpretation in
computer-mediated communication. Social Science
Computer Review, 19(3):324?347.
J. Wiebe and E. Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated
texts . In Proceedings of the 6th International Con-
ference on Computational Linguistics and Intelli-
gent Text Processing (CICLing-05), volume 3406 of
Springer LNCS. Springer-Verlag.
491
Incremental Semantic Construction in a Dialogue System?
Matthew Purver, Arash Eshghi, Julian Hough
Interaction, Media and Communication
School of Electronic Engineering and Computer Science, Queen Mary University of London
{mpurver, arash, jhough}@eecs.qmul.ac.uk
Abstract
This paper describes recent work on the DynDial project? towards incremental semantic inter-
pretation in dialogue. We outline our domain-general grammar-based approach, using a variant of
Dynamic Syntax integrated with Type Theory with Records and a Davidsonian event-based seman-
tics. We describe a Java-based implementation of the parser, used within the Jindigo framework to
produce an incremental dialogue system capable of handling inherently incremental phenomena such
as split utterances, adjuncts, and mid-sentence clarification requests or backchannels.
1 Introduction
Many dialogue phenomena seem to motivate an incremental view of language processing: for example,
a participant?s ability to change hearer/speaker role mid-sentence to produce or interpret backchannels,
or complete or continue an utterance (see e.g. Yngve, 1970; Lerner, 2004, amongst many others). Much
recent research in dialogue systems has pursued this line, resulting in frameworks for incremental di-
alogue processing (Schlangen and Skantze, 2009) and systems capable of mid-utterance backchannels
(Skantze and Schlangen, 2009) or utterance completions (DeVault et al, 2009; Bu? et al, 2010).
However, to date there has been little focus on semantics, with the systems produced either operating
in domains in which semantic representation is not required (Skantze and Schlangen, 2009), or using
variants of domain-specific canned lexical or phrasal matching (Bu? et al, 2010). Our intention is to
extend this work to finer-grained and more domain-general notions of grammar and semantics, by using
an incremental grammatical framework, Dynamic Syntax (DS, Kempson et al, 2001) together with the
structured semantic representation provided by Type Theory with Records (TTR, see e.g. Cooper, 2005).
(a)
A: I want to go to . . .
B: Uh-huh
A: . . . Paris by train.
(b)
A: I want to go to Paris . . .
B: Uh-huh
A: . . . by train.
(c)
A: I want to go to Paris.
B: OK. When do you . . .
A: By train.
Figure 1: Examples of motivating incremental dialogue phenomena
One aim is to deal with split utterances, both when the antecedent is inherently incomplete (see Fig-
ure 1(a)) and potentially complete (even if not intended as such ? Figure 1(b)). This involves producing
representations which are as complete as possible ? i.e contain all structural and semantic information
so far conveyed ? on a word-by-word basis, so that in the event of an interruption or a hesitation, the
system can act accordingly (by producing backchannels or contentful responses as above); but that can
be further incremented in the event of a continuation by the user.
Importantly, this ability should be available not only when an initial contribution is intended and/or
treated as incomplete (as in Figure 1(b)), but also when it is in fact complete, but is still subsequently
extended (Figure 1(c)). Treating A?s two utterances as distinct, with separate semantic representations,
must require high-level processes of ellipsis reconstruction to interpret the final fragment ? for example,
treating it as the answer to an implicit question raised by A?s initial sentence (Ferna?ndez et al, 2004). If,
?The authors were supported by the Dynamics of Conversational Dialogue project (DynDial ? ESRC-RES-062-23-0962).
We thank Shalom Lappin, Tim Fernando, Yo Sato, our project colleagues and the anonymous reviewers for helpful comments.
365
instead, we can treat such fragments as continuations which merely add directly to the existing represen-
tation, the task is made easier and the relevance of the two utterances to each other becomes explicit.
2 Dynamic Syntax (DS) and Type Theory with Records (TTR)
Our approach is a grammar-based one, as our interest is in using domain-general techniques that are
capable of fine-grained semantic representation. Dynamic Syntax (DS) provides an inherently incre-
mental grammatical framework which dispenses with an independent level of syntax, instead expressing
grammaticality via constraints on the word-by-word monotonic growth of semantic structures. In DS?s
original form, these structures are trees with nodes corresponding to terms in the lambda calculus; nodes
are decorated with labels expressing their semantic type and formula, and beta-reduction determines the
type and formula at a mother node from those at its daughters (Figure 2(a)). Trees can be partial, with
nodes decorated with requirements for future development; lexical actions (corresponding to words) and
computational actions (general capabilities) are defined as operations on trees which satisfy and/or add
requirements; and grammaticality of a word sequence is then defined as satisfaction of all requirements
(tree completeness) via the application of its associated actions ? see Kempson et al (2001) for details.
Previous work in DS has shown how this allows a treatment of split utterances and non-sentential
fragments (e.g. clarifications) as extensions of the semantic trees so far constructed, either directly or via
the addition of ?linked? trees (Purver and Kempson, 2004; Gargett et al, 2009).
(a) Ty(t),arrive(john)
Ty(e),
john
Ty(e ? t),
?x.arrive(x)
(b)
[
x=john : e
p=arrive(x) : t
]
[
x=john : e
]
?r :
[
x : e
]
[
x=r.x : e
p=arrive(x) : t
]
(c)
?
?
e=now : es
x=john : e
p=arrive(e,x) : t
?
?
[
x=john : e
]
?r :
[
x : e
]
?
?
e=now : es
x=r.x : e
p=arrive(e,x) : t
?
?
Figure 2: A simple DS tree for ?john arrives?: (a) original DS, (b) DS+TTR, (c) event-based
2.1 Extensions
More recent work in DS has started to explore the use of TTR to extend the formalism, replacing the
atomic semantic type and FOL formula node labels with more complex record types, and thus providing
a more structured semantic representation. Purver et al (2010) provide a sketch of one way to achieve
this and explain how it can be used to incorporate pragmatic information such as participant reference
and illocutionary force. As shown in Figure 2(b) above, we use a slightly different variant here: node
record types are sequences of typed labels (e.g. [x : e] for a label x of type e), with semantic content
expressed by use of manifest types (e.g. [x=john : e] where john is a singleton subtype of e).
We further adopt an event-based semantics along Davidsonian lines (Davidson, 1980). As shown
in Figure 2(c), we include an event term (of type es) in the representation: this allows tense and aspect
to be expressed (although Figure 2(c) shows only a simplified version using the current time now).
It also permits a straightforward analysis of optional adjuncts as extensions of an existing semantic
representation; extensions which predicate over the event term already in the representation. Adding
fields to a record type results in a more fully specified record type which is still a subtype of the original:
?
?
?
e=now : es
x=john : e
p=arrive(e,x) : t
?
?
?
7?
?
?
?
?
?
e=now : es
x=john : e
p=arrive(e,x) : t
p?=today(e) : t
?
?
?
?
?
?john arrives? 7? ?john arrives today?
Figure 3: Optional adjuncts as leading to TTR subtypes
366
3 Implementation
The resulting framework has been implemented in Java, following the formal details of DS as per (Kemp-
son et al, 2001; Cann et al, 2005, inter alia). This implementation, DyLan,1 includes a parser and gener-
ator for English, which take as input a set of computational actions, a lexicon and a set of lexical actions
(instructions for partial tree update); these are specified separately in text files in the IF-THEN-ELSE
procedural (meta-)language of DS, allowing any pre-written grammar to be loaded. Widening or chang-
ing its coverage, i.e. extending the system with new analyses of various linguistic phenomena, thus do
not involve modification or extension of the Java program, but only the lexicon and action specifications.
The current coverage includes a small lexicon, but a broad range of structures: complementation, relative
clauses, adjuncts, tense, pronominal and ellipsis construal, all in interaction with quantification.
3.1 The parsing process
Given a sequence of words (w1, w2, ..., wn), the parser starts from the axiom tree T0 (a requirement
to construct a complete tree of type t), and applies the corresponding lexical actions (a1, a2, . . . , an),
optionally interspersing general computational actions (which can apply whenever their preconditions
are met). More precisely: we define the parser state at step i as a set of partial trees Si. Beginning with
the singleton axiom state S0 = {T0}, for each word wi:
1. Apply all lexical actions ai corresponding to wi to each partial tree in Si?1. For each application
that succeeds (i.e. the tree satisfies the action preconditions), add resulting (partial) tree to Si.
2. For each tree in Si, apply all possible sequences of computational actions and add the result to Si.
If at any stage the state Si is empty, the parse has failed and the string is deemed ungrammatical. If the
final state Sn contains a complete tree (all requirements satisfied), the string is grammatical and its root
node will provide the full sentence semantics; partial trees provide only partial semantic specifications.2
3.2 Graph representations
Sato (2010) shows how this procedure can be modelled as a directed acyclic graph, rooted at T0, with
individual partial trees as nodes, connected by edges representing single actions. While Sato uses this to
model the search process, we exploit it (in a slightly modified form) to represent the linguistic context
available during the parse ? important in DS for ellipsis and pronominal construal. Details are given in
(Cann et al, 2007; Gargett et al, 2009), but three general mechanisms are available: 1) copying formulae
from some tree in context (used for e.g. anaphora and strict VP ellipsis); 2) rerunning actions in context
(for e.g. sloppy VP-ellipsis and fragment corrections); and 3) directly extending/augmenting the current
tree (used for most fragment types in (Ferna?ndez, 2006)). For any partial tree, then, the context available
to the parser must include not only the tree itself, but the sequence of actions and previous partial trees
which have gone into its construction. The parse graph (which we call the tree graph) provides exactly
this information, via the shortest path back to the root from the current node.
However, we can also take a coarser-grained view via a graph which we term the state graph; here,
nodes are states Si and edges the sets of action sequences connecting them. This subsumes the tree graph,
with state nodes containing possibly many tree-graph nodes; and here, nodes have multiple outgoing
edges only when multiple word hypotheses are present. This corresponds directly to the input word graph
(often called a word lattice) available from a speech recognizer, allowing close integration in a dialogue
system ? see below. We also see this as a suitable structure with which to begin to model phenomena
such as hesitation and self-repair: as edges are linear action sequences, intended to correspond to the
time-linear psycholinguistic processing steps involved, such phenomena may be analysed as building
further edges from suitable departure points earlier in the graph.3
1DyLan is short for Dynamics of Language. Available from http://dylan.sourceforge.net/.
2Note that only a subset of possible computational actions can apply to any given tree; together with a set of heuristics on
possible application order, and the merging of identical trees produced by different sequences, this helps reduce complexity.
3There are similarities to chart parsing here: the tree graph edges spanning a state graph edge could be seen as corresponding
to chart edges spanning a substring, with the tree nodes in the state Si as the agenda. However, the lack of a notion of syntactic
constituency means no direct equivalent for the active/passive edge distinction; a detailed comparison is still to be carried out.
367
4 Dialogue System
The DyLan parser has now been integrated into a working dialogue system by implementation as an
Interpreter module in the Java-based incremental dialogue framework Jindigo (Skantze and Hjal-
marsson, 2010). Jindigo follows Schlangen and Skantze (2009)?s abstract architecture specification and
is specifically designed to handle units smaller than fully sentential utterances; one of its specific imple-
mentations is a travel agent system, and our module integrates semantic interpretation into this.
As set out by Schlangen and Skantze (2009)?s specification, our Interpreter?s essential compo-
nents are a left buffer (LB), processor and right buffer (RB). Incremental units (IUs) of various types are
posted from the RB of one module to the LB of another; for our module, the LB-IUs are ASR word hy-
potheses, and after processing, domain-level concept frames are posted as RB-IUs for further processing
by a downstream dialogue manager. The input IUs are provided as updates to a word lattice, and new
edges are passed to the DyLan parser which produces a state graph as described above in 3.1 and 3.2:
new nodes are new possible parse states, with new edges the sets of DS actions which have created them.
These state nodes are then used to create Jindigo domain concept frames by matching against the TTR
record types available (see below), and these are posted to the RB as updates to the state graph (lattice
updates in Jindigo?s terminology).
Crucial in Schlangen and Skantze (2009)?s model is the notion of commitment: IUs are hypotheses
which can be revoked at any time until they are committed by the module which produces them. Our
module hypothesizes both parse states and associated domain concepts (although only the latter are
outputs); these are committed when their originating word hypotheses are committed (by ASR) and a
type-complete subtree is available; other strategies are possible and are being investigated.
4.1 Mapping TTR record types to domain concepts incrementally
Our Interpreter module matches TTR record types to domain concept frames via a simple XML
matching specification; TTR fields map to particular concepts in the domain depending on their se-
mantic type (e.g. go events map to Trip concepts, and the entity of manifest type paris maps to the
City[paris] concept). As the tree and parse state graphs are maintained, incremental sub-sentential
extensions can produce TTR subtypes and lead to enrichment of the associated domain concept.
User: I want to go to Paris . . .
?
?
?
?
?
?
?
?
?
?
?
?
e=,e17,P resentState : es
e1=,e19,FutureAccomp : es
x1=Paris : e
p2=to(e1,x1) : t
x=speaker : e
p1=go(e1,x) : t
p?=want(e,x,p1) : t
?
?
?
?
?
?
?
?
?
?
?
?
Trip(to : City[paris])
User: . . . from London
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
e=,e17,P resentState : es
e1=,e19,FutureAccomp : es
x1=Paris : e
p2=to(e1,x1) : t
x2=London : e
p3=from(e1,x2) : t
x=speaker : e
p1=go(e1,x) : t
p?=want(e,x,p1) : t
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Trip(from : City[london],
to : City[paris])
Figure 4: Incremental construction of a TTR record type over two turns
Figure 4 illustrates this process for a user continuation; the initial user utterance is parsed to produce
a TTR record type, with a corresponding domain concept ? a valid incremental unit to post in the RB.
The subsequent user continuation ?from London? extends the parser state graph, producing a new TTR
subtype (in this case via the DS apparatus of an adjoining linked tree (Cann et al, 2005)), and a more
368
fully specified concept (with a further argument slot filled) as output.
System behaviour between these two user contributions will depend on the committed status of the
input, and perhaps some independent prosody-based judgement of whether a turn is finished (Skantze
and Schlangen, 2009). An uncommitted input might be responded to with a backchannel (Yngve, 1970);
commitment might lead to the system beginning processing and starting to respond more substantively.
However, in either case, the maintenance of the parse state graph allows the user continuation to be
treated as extending a parse tree, subtyping the TTR record type, and finally mapping to a fully satisfied
domain concept frame that can be committed.
5 Conclusions
We have implemented an extension of the Dynamic Syntax framework, integrated with Type Theory with
Records, which provides structured semantic representations suitable for use in a dialogue system, and
which does so incrementally, producing well-defined partial representations on a word-by-word basis.
This has been integrated into a working Jindigo dialogue system, capable of incremental behaviour such
as mid-sentence backchannels and utterance continuations, which will be demonstrated at the conference.
The coverage of the parser is currently limited, but work is in progress to widen it; the possibility of using
grammar induction to learn lexical actions from real corpora is also being considered for future projects.
We are also actively pursuing possbilities for tighter integration of TTR and DS, with the aim of unifying
syntactic and semantic incremental construction.
References
Bu?, O., T. Baumann, and D. Schlangen (2010). Collaborating on utterances with a spoken dialogue system using
an ISU-based approach to incremental dialogue management. In Proceedings of the SIGDIAL 2010 Conference.
Cann, R., R. Kempson, and L. Marten (2005). The Dynamics of Language. Oxford: Elsevier.
Cann, R., R. Kempson, and M. Purver (2007). Context and well-formedness: the dynamics of ellipsis. Research
on Language and Computation 5(3), 333?358.
Cooper, R. (2005). Records and record types in semantic theory. Journal of Logic and Computation 15(2), 99?112.
Davidson, D. (1980). Essays on Actions and Events. Oxford, UK: Clarendon Press.
DeVault, D., K. Sagae, and D. Traum (2009). Can I finish? learning when to respond to incremental interpretation
results in interactive dialogue. In Proceedings of the SIGDIAL 2009 Conference.
Ferna?ndez, R. (2006). Non-Sentential Utterances in Dialogue: Classification, Resolution and Use. Ph. D. thesis,
King?s College London, University of London.
Ferna?ndez, R., J. Ginzburg, H. Gregory, and S. Lappin (2004). SHARDS: Fragment resolution in dialogue. In
H. Bunt and R. Muskens (Eds.), Computing Meaning, Volume 3. Kluwer Academic Publishers. To appear.
Gargett, A., E. Gregoromichelaki, R. Kempson, M. Purver, and Y. Sato (2009). Grammar resources for modelling
dialogue dynamically. Cognitive Neurodynamics 3(4), 347?363.
Kempson, R., W. Meyer-Viol, and D. Gabbay (2001). Dynamic Syntax: The Flow of Language Understanding.
Blackwell.
Lerner, G. H. (2004). Collaborative turn sequences. In Conversation analysis: Studies from the first generation,
pp. 225?256. John Benjamins.
Purver, M., E. Gregoromichelaki, W. Meyer-Viol, and R. Cann (2010). Splitting the ?I?s and crossing the ?You?s:
Context, speech acts and grammar. In Aspects of Semantics and Pragmatics of Dialogue. SemDial 2010, 14th
Workshop on the Semantics and Pragmatics of Dialogue.
Purver, M. and R. Kempson (2004). Incremental context-based generation for dialogue. In Proceedings of the 3rd
International Conference on Natural Language Generation (INLG04).
Sato, Y. (2010). Local ambiguity, search strategies and parsing in Dynamic Syntax. In E. Gregoromichelaki,
R. Kempson, and C. Howes (Eds.), The Dynamics of Lexical Interfaces. CSLI. to appear.
Schlangen, D. and G. Skantze (2009). A general, abstract model of incremental dialogue processing. In Proceed-
ings of the 12th Conference of the European Chapter of the ACL (EACL 2009).
Skantze, G. and A. Hjalmarsson (2010). Towards incremental speech generation in dialogue systems. In Proceed-
ings of the SIGDIAL 2010 Conference.
Skantze, G. and D. Schlangen (2009). Incremental dialogue processing in a micro-domain. In Proceedings of the
12th Conference of the European Chapter of the ACL (EACL 2009).
Yngve, V. H. (1970). On getting a word in edgewise. In Papers from the 6th regional meeting of the Chicago
Linguistic Society.
369
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 79?83,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Predicting Adherence to Treatment for Schizophrenia from Dialogue
Transcripts
Christine Howes, Matthew Purver, Rose McCabe, Patrick G. T. Healey, Mary Lavelle
Queen Mary University of London
Mile End Road, London E1 4NS
c.howes@qmul.ac.uk
Abstract
Recent work on consultations between out-
patients with schizophrenia and psychiatrists
has shown that adherence to treatment can
be predicted by patterns of repair ? specifi-
cally, the pro-activity of the patient in check-
ing their understanding, i.e. patient clarifi-
cation. Using machine learning techniques,
we investigate whether this tendency can be
predicted from high-level dialogue features,
such as backchannels, overlap and each partic-
ipant?s proportion of talk. The results indicate
that these features are not predictive of a pa-
tient?s adherence to treatment or satisfaction
with the communication, although they do
have some association with symptoms. How-
ever, all these can be predicted if we allow
features at the word level. These preliminary
experiments indicate that patient adherence is
predictable from dialogue transcripts, but fur-
ther work is necessary to develop a meaning-
ful, general and reliable feature set.
1 Introduction
How conversational partners achieve and maintain
shared understanding is of crucial importance in
the understanding of dialogue. One such mecha-
nism, other initiated repair (Schegloff, 1992), where
one conversational participant queries or corrects
the talk of another, has been well documented in
both general and task-based dialogues (Colman and
Healey, 2011). However, how such shared under-
standing impacts beyond the level of the conversa-
tion has not typically been examined. Exceptions to
this have highlighted the role of shared understand-
ing in schizophrenia (McCabe et al, 2002; Themis-
tocleous et al, 2009) and the association between
psychiatrist-patient communication and adherence.
McCabe et al (in preparation) found that more pa-
tient clarification (i.e. other initiated repair) of the
psychiatrist?s talk was associated with better treat-
ment adherence six months later. Clarification con-
sists mainly of asking questions to clarify the mean-
ing of the psychiatrist?s utterance (checking under-
standing) and correcting something that the psychi-
atrist has said (getting the facts straight). Example 1,
taken from a consultation, shows the patient request-
ing clarification of something the psychiatrist has
just said about a possible side effect.
(1) Dr: Yep, well that is a possible side effect
Pat: Side effect?
Dr: Of the er haloperidol
The patient?s request leads to additional explana-
tion by the psychiatrist about the medication which
can cause the possible side effect. More patient clar-
ification reflects greater effort to reach a shared un-
derstanding. McCabe et al (in preparation) found
that for each unit increase in the patient clarification
factor,1 the odds of good (versus poor) adherence
were increased by 5.8 (95% CI 1.3 to 25.8, p=0.02).
Explaining the link between communicative pat-
terns of patients and adherence may create the pos-
sibility for new interventions to improve adherence,
and has both clinical and theoretical implications.
1A regression factor weighted heavily towards patient clar-
ifcations (as in e.g. 1).
79
However, there is no evidence regarding what fac-
tors influence patient clarification and may explain
the link with adherence. If patient clarification is
a measure of greater communicational effort, or en-
gagement, then wemight expect other dialogue mea-
sures, such as the amount of acknowledgements or
other grounding cues (Traum and Allen, 1992), or
the proportion of talk per person, to be correlated
with other initiated repair and therefore similarly
predictive of subsequent adherence behaviour. This
is of particular importance if we wish to build a sys-
tem to automatically predict possible (lack of) ad-
herence from dialogue transcripts, especially given
that the types of patient clarification which carry
the highest weight in the patient clarification factor
(next-turn repair initiators, Schegloff, 1992) are rare,
occurring on average only 1.2 times per dialogue.
Further, although certain types of repair were
shown to affect how patients reported they felt the
conversation went, self-reports of symptoms and
communicational factors are not predictive of adher-
ence. Although micro-communicational behaviour
(in the form of other initiated repair) does have a
bearing on subsequent adherence behaviour, patients
are unaware of this. Additional questions therefore
concern whether we can predict patient?s symptom
levels and subjective analyses of the communication
based only on overview dialogue factors.
2 Hypotheses
Factors which we would expect to index patient en-
gagement, and thus be predictive of adherence to
treatment are the amount of backchannel responses
patients make, and the proportion of questions pa-
tients ask, both of which ought to be higher for the
more engaged patients. We might also expect that
such patients have a greater proportion of the talk
overall, and/or longer turns on average, though note
that this conversational pattern might also be one in
which the patient is not engaged, as they might not
be responding to the feedback from their consultant.
For the symptom scores (see below for details),
we should expect that patients with high levels
of negative symptoms (which includes loss of af-
fect and poverty of speech) would produce less
talk overall, and in general produce shorter turns.
There should also be more noticeable gaps in the
dialogues (defined as greater than approximately
200ms, (Heldner and Edlund, 2010)). Contrarily,
for positive symptoms, (including hallucinations and
delusions) patients ought to produce longer turns
and have a greater proportion of the talk.
We also expect to see effects on how patients felt
the conversation went from the amount of overlap,
though as overlap can be both intended and inter-
preted as either interruptive or collaborative (as with
e.g. overlapping backchannels) it is unclear which
direction such a prediction should take.
3 Method
131 dialogues from outpatient consultations be-
tween patients and psychiatrists were analysed ac-
cording to a number of factors. Each of these fac-
tors, detailed in table 1, below, is calculated for each
dialogue participant (with the exception of pauses).
Each patient featured in only one of the dialogues
however, there were only 29 doctors in the study,
so the same clinician may have featured in several
of the dialogues with different patients. The con-
sultations varied in length, with the shortest con-
sisting of 61 turns (438 words) and the longest
881 turns (13178 words), with an average of 320.5
turns (2706.4 words). In addition, a third party was
present in 47 of the consultations.
Following the consultation, each patient was
asked questions from standard questionnaires to as-
certain their level of symptoms, and their evalua-
tion of aspects of the consultation. The positive
and negative syndrome scale (PANSS) (Kay et al,
1987) assesses positive, negative and general symp-
toms on a 7-point scale of severity (1=absent ? 7=ex-
treme). Positive symptoms represent a change in
the patients? behaviour or thoughts and include sen-
sory hallucinations and delusional beliefs. Negative
symptoms represent a withdrawal or reduction in
functioning, including blunted affect, and emotional
withdrawal and alogia (poverty of speech). Positive
and negative subscale scores ranged from 7 (absent)
? 49 (extreme), general symptoms (such as anxiety)
scores ranged from 16 (absent) ? 112 (extreme).
Patient satisfaction with the communication was
assessed using the Patient Experience Questionnaire
(PEQ) (Steine et al, 2001). Three of the five sub-
scales (12 questions) were used as the others were
80
not relevant, having been developed for primary
care. The three subscales were ?communication ex-
periences?, ?communication barriers? and ?emotions
immediately after the visit?. For the communication
subscales, items were measured on a 5-point Lik-
ert scale, with 1=disagree completely and 5=agree
completely. The four items for the emotion scale
were measured on a 7-point visual analogue scale,
with opposing emotions were at either end. A higher
score indicates a better experience.
Adherence to treatment was rated by the clini-
cians as good (> 75%), average (25  75%) or poor
(< 25%) six months after the consultation. Due to
the low incidence of poor ratings (only 8 dialogues),
this was converted to a binary score of 1 for good ad-
herence (91 patients), and 0 otherwise (37). Ratings
were not available for the remaining dialogues.
Measure Description
Turns Total number of turns
Words Total number of words spoken
Proportion Proportion of total talk in words
(by each participant)
WordsPerTurn Average length of turn in words
WhPerWord Proportion of wh-words (e.g.
what? who?) per word
OCRPerWord Proportion of open class repair ini-
tiators (e.g. pardon? huh?) per
word
BackchannelPerWord Proportion of backchannels (e.g.
uh-huh, yeah) per word
RepeatPerWord Proportion of words repeated from
preceding turn by other person
OverlapAny Proportion of turns containing any
overlapping talk
OverlapAll Proportion of turns entirely over-
lapping another turn
QMark Proportion of turns containing a
question mark
TimedPause Pause of more than approx 200ms,
as marked on the transcripts
Table 1: Measures from outpatient consultations
3.1 Classification Experiments
We performed a series of classification experiments
using the Weka machine learning toolkit (Hall et
al., 2009) to predict each of the outcome mea-
sures outlined above (symptom measures, satisfac-
tion measures, and adherence to treatment). In each
case, outcome measures were converted to binary
high/low scores on an equal frequency basis (i.e.
providing approximately equal numbers of high and
low instances). Features used were the high-level
measures given in Table 1, and/or all unigrams ex-
tracted from the transcript; in both cases, features
from doctor and patient were treated separately. Un-
igrams were produced by tokenising the lower-cased
transcripts on white space; no stemming or stop-
word removal was performed, and feature values
were binary i.e. indicating only presence or ab-
sence of the word spoken by the given speaker in
the given dialogue.2 Given the small size of our
dataset (131 instances) and the large feature space
resulting (> 6500 features), we selected features
based on their predictive ability across the entire
dataset (using Weka?s CfsSubsetEval selector), re-
ducing the number of features to 50-100. In order
to avoid biasing towards doctor-specific features, we
used only words spoken by patients in these exper-
iments ? each patient only features in one dialogue,
so patient-specific vocabulary cannot help perfor-
mance across dialogues. All unigram features thus
selected were used in at least 3 dialogues.3
4 Results
Experiments including unigram features used Lib-
SVM?s support vector machine implementation
(Chang and Lin, 2001) with a radial basis func-
tion kernel; experiments with only high-level fea-
tures used J48 decision trees. In each case, experi-
ments used 5-fold cross-validation.4 In experiments
predicting adherence, the distribution between pos-
itive and negative (i.e. good and bad adherence)
made it impossible to balance the dataset - as this
can be problematic for decision tree classifiers, we
also present results for a downsampled dataset with
only 71 instances but which provides balance. Per-
formance is shown in Table 2 as overall percentage
accuracy, and is compared to a majority-class base-
line throughout; results which are significantly dif-
ferent at the 5% level according to a  2 test from a
2Experiments with frequency counts did not affect the re-
sults as reported.
3Bi- and tri-gram features were not extracted from this data
because of the small amount of data available which we felt
would result in models that suffered from overfitting (note that
the same concern holds for the unigram features).
4Classifiers were trained on 80% and tested on 20% of the
sample, with this was repeated 5 times over each possible 80/20
combination so as to test the whole dataset.
81
random distribution and the majority class distribu-
tion are shown marked with *.
Baseline Words High-level
PANSS positive 51.1 87.0* 56.5*
PANSS negative 49.6 87.8* 56.5*
PANSS general 48.4 91.1* 54.0
PEQ emotions 51.9 89.1* 53.5
PEQ communication 50.8 79.8* 52.4
PEQ comm. barriers 51.6 90.6* 51.6
PEQ overall 50.8 90.6* 53.9
Adherence 73.2 91.1* 63.4
Adherence (balanced) 53.5 93.0* 52.1
Table 2: Percentage accuracies vs feature set
Results show good performance for all experi-
ments when including lexical features, with all fac-
tors being predictable with around 90% accuracy
with the exception of PEQ communication at just be-
low 80%. However, using high-level features alone
gives negligible performance, except for a small
benefit on the PANSS negative and positive symp-
tom measures, though contrary to our hypotheses
the most important high-level features were OCR-
PerWord by the doctor (negative) and WhWords by
an other participant (positive).
Examination of the most predictive unigrams
shows that sets selected for different outcome mea-
sures are different: for example, the 54 fea-
tures selected for adherence and the 73 selected
for PEQ overall have only 1 word in com-
mon (?mates?). Adherence-related words in-
clude words related to conditions, treatment and
medication (?schizophrenic?, ?sickness?, ?symp-
toms?, ?worse?, ?pains?, ?flashbacks?, ?sodium?,
?chemical?, ?monthly?); PEQ-related words in-
clude those related to personal life (?sundays?,
?thursdays?, ?television?, ?sofa?, ?wine?, ?per-
sonally?, ?played?), and filled pauses (?eerrmm?,
?uhhm?) ? although more investigation is required
to draw any firm conclusions from these. Table 3
shows the full lists for adherence and PEQ overall.
5 Discussion and Conclusions
The results show that although we can weakly pre-
dict symptoms at levels above chance using only
high-level dialogue factors, we cannot do so for ad-
herence, or satisfaction measures. Despite the link
between patient other initiated repair and adherence,
this is also not an effective predictor for our machine
learning approach because of the scarcity of the phe-
nomenon, and the fact that many of the consulta-
tions for which the patients subsequently exhibited
good adherence behaviour do not feature a single
patient clarification, which may be linked to psychi-
atrist clarity rather than lack of effort or engagement
on the patient?s part.
The high accuracies with lexical features show
that some aspects of the consultations do enable ac-
curate prediction of adherence, PEQ measures and
symptoms. However, as the features which allow us
to achieve such good results rely on specific words
used, it is unclear how generalisable or interpretable
such results are. The lexical features chosen do gen-
eralise over our dataset (in which individual patients
appear only once), and exclude doctor talk, so can-
not be simply picking out unique unigram signatures
relating to individual patients or doctors; however,
given the small size of the dataset used for this ini-
tial investigation with its constrained domain, genre
and topics, and the use of the whole dataset to select
predictive words, it is unclear whether these results
will scale up to a larger dataset.
We therefore suspect that more general, higher-
level dialogue features such as specific interac-
tion phenomena (repair, question-answering) and/or
more general models of topic may be required.
While unigrams are too low-level to be explanatory
and may not generalise, the dialogue features dis-
cussed are too high-level to be useful; we are there-
fore examining mid-level phenomena and models
to capture the predictability while remaining gen-
eral and providing more interpretable features and
results. Although the word lists offer clues as to
the relevance of specific words for the overall pre-
dictability, we would not like to leave it at that.
Further experiments are therefore underway to in-
vestigate whether we can find a level of appropri-
ate explanatory power and maximal predictivity us-
ing an interim level of analysis, for example with n-
gram and part-of-speech-based models, topic mod-
els based on word distributions, and turn-taking phe-
nomena. Additional experiments also look at the
turn-level data to see if the patient led clarification
factor can be directly extracted from the transcripts.
82
Adherence PEQ overall
air grass schizophrenic 20th electric onto sometime
anyone grave sensation ages energy overweight son
balanced guitar sickness angry environment oxygen standing
bleach h simply anxiety experiencing packed stomach
build hahaha sodium background facilities percent suddenly
building lager stable bladder friendly personally sundays
busy laying stock booked helps picture suppose
challenge lifting symptoms boy ignore played table
chemical lucky talks broken immediately programs team
complaining mates teach bus increased progress television
cup monthly terminology certificate irritated provide thursdays
dates mouse throat dead kick public troubles
en nowhere virtually deep later quid uhhm
fill pains was drunk lee radio upsetting
finished possibly wave earn loose realised walks
fish pr weve eeerrrr low reply watchers
flashbacks recent worse eerrmm march sat wine
removed writing eerrrmm mates shaky
ri moments sofa
Table 3: Most predictive unigram features
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for Support Vector Machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
M. Colman and P. G. T. Healey. 2011. The distribution of
repair in dialogue. In Proceedings of the 33rd Annual
Meeting of the Cognitive Science Society, pages 1563?
1568, Boston, MA.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGDKDD Explorations, 11(1):10?18.
M. Heldner and J. Edlund. 2010. Pauses, gaps and
overlaps in conversations. Journal of Phonetics,
38(4):555?568.
S.R. Kay, A. Fiszbein, and L.A. Opfer. 1987. The
positive and negative syndrome scale (PANSS) for
schizophrenia. Schizophrenia bulletin, 13(2):261.
R. McCabe, C. Heath, T. Burns, S. Priebe, and J. Skel-
ton. 2002. Engagement of patients with psychosis in
the consultation: conversation analytic study. British
Medical Journal, 325(7373):1148?1151.
R. McCabe, M. Lavelle, S. Bremner, D. Dodwell, P. G. T.
Healey, R. Laugharne, S. Priebe, and A. Snell. in
preparation. Shared understanding in psychiatrist-
patient communication: Association with treatment
adherence in schizophrenia.
E.A. Schegloff. 1992. Repair after next turn: The last
structurally provided defense of intersubjectivity in
conversation. American Journal of Sociology, pages
1295?1345.
S. Steine, A. Finset, and E. Laerum. 2001. A new,
brief questionnaire (PEQ) developed in primary health
care for measuring patients? experience of interaction,
emotion and consultation outcome. Family practice,
18(4):410?418.
M. Themistocleous, R. McCabe, N. Rees, I. Hassan,
P. G. T. Healey, and S. Priebe. 2009. Establishing mu-
tual understanding in interaction: An analysis of con-
versational repair in psychiatric consultations. Com-
munication & Medicine, 6(2):165?176.
D.R. Traum and J.F. Allen. 1992. A speech acts ap-
proach to grounding in conversation. In Second Inter-
national Conference on Spoken Language Processing.
83
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 94?103,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Incremental Grammar Induction from Child-Directed
Dialogue Utterances?
Arash Eshghi
Interaction Lab
Heriot-Watt University
Edinburgh, United Kingdom
eshghi.a@gmail.com
Julian Hough and Matthew Purver
Cognitive Science Research Group
Queen Mary University of London
London, United Kingdom
{julian.hough, mpurver}@eecs.qmul.ac.uk
Abstract
We describe a method for learning an in-
cremental semantic grammar from data in
which utterances are paired with logical
forms representing their meaning. Work-
ing in an inherently incremental frame-
work, Dynamic Syntax, we show how
words can be associated with probabilistic
procedures for the incremental projection
of meaning, providing a grammar which
can be used directly in incremental prob-
abilistic parsing and generation. We test
this on child-directed utterances from the
CHILDES corpus, and show that it results
in good coverage and semantic accuracy,
without requiring annotation at the word
level or any independent notion of syntax.
1 Introduction
Human language processing has long been
thought to function incrementally, both in pars-
ing and production (Crocker et al, 2000; Fer-
reira, 1996). This incrementality gives rise to
many characteristic phenomena in conversational
dialogue, including unfinished utterances, inter-
ruptions and compound contributions constructed
by more than one participant, which pose prob-
lems for standard grammar formalisms (Howes et
al., 2012). In particular, examples such as (1) sug-
gest that a suitable formalism would be one which
defines grammaticality not in terms of licensing
strings, but in terms of constraints on the semantic
construction process, and which ensures this pro-
cess is common between parsing and generation.
(1) A: I burnt the toast.
? We are grateful to Ruth Kempson for her support and
helpful discussions throughout this work. We also thank
the CMCL?2013 anonymous reviewers for their constructive
criticism. This work was supported by the EPSRC, RISER
project (Ref: EP/J010383/1), and in part by the EU, FP7
project, SpaceBook (Grant agreement no: 270019).
B: But did you burn . . .
A: Myself? Fortunately not.
[where ?did you burn myself?? if uttered by
the same speaker is ungrammatical]
One such formalism is Dynamic Syntax (DS)
(Kempson et al, 2001; Cann et al, 2005); it
recognises no intermediate layer of syntax, but
instead reflects grammatical constraints via con-
straints on the word-by-word incremental con-
struction of meaning, underpinned by attendant
concepts of underspecification and update.
Eshghi et al (2013) describe a method for in-
ducing a probabilistic DS lexicon from sentences
paired with DS semantic trees (see below) repre-
senting not only their meaning, but their function-
argument structure with fine-grained typing infor-
mation. They apply their method only to an ar-
tificial corpus generated using a known lexicon.
Here, we build on that work to induce a lexi-
con from real child-directed utterances paired with
less structured Logical Forms in the form of TTR
Record Types (Cooper, 2005), thus providing less
supervision. By assuming only the availability of a
small set of general compositional semantic opera-
tions, reflecting the properties of the lambda calcu-
lus and the logic of finite trees, we ensure that the
lexical entries learnt include the grammatical con-
straints and corresponding compositional seman-
tic structure of the language. Our method exhibits
incrementality in two senses: incremental learn-
ing, with the grammar being extended and refined
as each new sentence becomes available; resulting
in an inherently incremental, probabilistic gram-
mar for parsing and production, suitable for use
in state-of-the-art incremental dialogue systems
(Purver et al, 2011) and for modelling human-
human dialogue.
94
?Ty(t)
?Ty(e),
?
?Ty(e ? t)
??
?john?
?Ty(t)
Ty(e),
john
?Ty(e ? t),
?
??
?upset?
?Ty(t)
Ty(e),
john ?Ty(e ? t)
?Ty(e),
?
Ty(e ? (e ? t)),
?y?x.upset?(x)(y)
??
?mary?
Ty(t),?,
upset?(john?)(mary?)
Ty(e),
john
Ty(e ? t),
?x.upset?(x)(mary?)
Ty(e),
mary?
Ty(e ? (e ? t)),
?y?x.upset?(x)(y)
Figure 1: Incremental parsing in DS producing semantic trees: ?John upset Mary?
2 Background
2.1 Grammar Induction and Semantics
We can view existing grammar induction meth-
ods along a spectrum from supervised to unsu-
pervised. Fully supervised methods take a parsed
corpus as input, pairing sentences with syntactic
trees and words with their syntactic categories, and
generalise over the phrase structure rules to learn
a grammar which can be applied to a new set of
data. Probabilities for production rules sharing a
LHS category can be estimated, producing a gram-
mar suitable for probabilistic parsing and disam-
biguation e.g. a PCFG (Charniak, 1996). While
such methods have shown great success, they pre-
suppose detailed prior linguistic information and
are thus inadequate as human grammar learning
models. Fully unsupervised methods, on the other
hand, proceed from unannotated raw data; they
are thus closer to the human language acquisition
setting, but have seen less success. In its pure
form ?positive data only, without bias? unsu-
pervised learning is computationally too complex
(?unlearnable?) in the worst case (Gold, 1967).
Successful approaches involve some prior learning
or bias (see (Clark and Lappin, 2011)) e.g. a set
of known lexical categories, a probability distri-
bution bias (Klein and Manning, 2005) or a semi-
supervised method with shallower (e.g. POS-tag)
annotation (Pereira and Schabes, 1992).
Another point on the spectrum is lightly su-
pervised learning: providing information which
constrains learning but with little or no lexico-
syntactic detail. One possibility is the use of se-
mantic annotation, using sentence-level proposi-
tional Logical Forms (LF). It seems more cogni-
tively plausible, as the learner can be said to be
able to understand, at least in part, the meaning
of what she hears from evidence gathered from
(1) her perception of her local, immediate environ-
ment given appropriate biases on different patterns
of individuation of entities and relationships be-
tween them, and (2) helpful interaction, and joint
focus of attention with an adult (see e.g. (Saxton,
1997)). Given this, the problem she is faced with
is one of separating out the contribution of each
individual linguistic token to the overall meaning
of an uttered linguistic expression (i.e. decompo-
sition), while maintaining and generalising over
several such hypotheses acquired through time as
she is exposed to more utterances involving each
token.
This has been successfully applied in Combi-
natorial Categorial Grammar (CCG) (Steedman,
2000), as it tightly couples compositional seman-
tics with syntax (Zettlemoyer and Collins, 2007;
Kwiatkowski et al, 2010; Kwiatkowski et al,
2012); as CCG is a lexicalist framework, grammar
learning involves inducing a lexicon assigning to
each word its syntactic and semantic contribution.
Moreover, the grammar is learnt incrementally, in
the sense that the learner collects data over time
and does the learning sentence by sentence.
Following this approach, Eshghi et al (2013)
outline a method for inducing a DS grammar
from semantic LFs. This brings an added di-
mension of incrementality: not only is learning
sentence-by-sentence incremental, but the gram-
mar learned is inherently word-by-word incre-
mental (see section 2.2 below). However, their
method requires a higher degree of supervision
than (Kwiatkowski et al, 2012): the LFs assumed
are not simply flat semantic formulae, but full DS
semantic trees (see e.g. Fig. 1) containing infor-
mation about the function-argument structure re-
95
quired for their composition, in addition to fine
grained type and formula annotations. Further,
they test their method only on artificial data cre-
ated using a known, manually-specified DS gram-
mar. In contrast, in this paper we provide an
approach which can learn from LFs without any
compositional structure information, and test it on
real language data; thus providing the first prac-
tical learning system for an explicitly incremental
grammar that we are aware of.
2.2 Dynamic Syntax (DS)
Dynamic Syntax (Kempson et al, 2001; Cann et
al., 2005) is a parsing-directed grammar formal-
ism, which models the word-by-word incremental
processing of linguistic input. Unlike many other
formalisms, DS models the incremental building
up of interpretations without presupposing or in-
deed recognising an independent level of syntactic
processing. Thus, the output for any given string
of words is a purely semantic tree representing
its predicate-argument structure; tree nodes cor-
respond to terms in the lambda calculus, deco-
rated with labels expressing their semantic type
(e.g. Ty(e)) and formula, with beta-reduction de-
termining the type and formula at a mother node
from those at its daughters (Figure 1).
These trees can be partial, containing unsatis-
fied requirements for node labels (e.g. ?Ty(e) is a
requirement for future development to Ty(e)), and
contain a pointer ? labelling the node currently
under development. Grammaticality is defined as
parsability: the successful incremental construc-
tion of a tree with no outstanding requirements (a
complete tree) using all information given by the
words in a sentence. The complete sentential LF
is then the formula decorating the root node ? see
Figure 1. Note that in these trees, leaf nodes do
not necessarily correspond to words, and may not
be in linear sentence order; syntactic structure is
not explicitly represented, only the structure of se-
mantic predicate-argument combination.
2.2.1 Actions in DS
The parsing process is defined in terms of condi-
tional actions: procedural specifications for mono-
tonic tree growth. These include general structure-
building principles (computational actions), puta-
tively independent of any particular natural lan-
guage, and language-specific actions associated
with particular lexical items (lexical actions). The
latter are what we learn from data here.
Computational actions These form a small,
fixed set, which we assume as given here. Some
merely encode the properties of the lambda cal-
culus and the logical tree formalism itself, LoFT
(Blackburn and Meyer-Viol, 1994) ? these we
term inferential actions. Examples include THIN-
NING (removal of satisfied requirements) and
ELIMINATION (beta-reduction of daughter nodes
at the mother). These actions are language-
independent, cause no ambiguity, and add no new
information to the tree; as such, they apply non-
optionally whenever their preconditions are met.
Other computational actions reflect the fun-
damental predictivity and dynamics of the DS
framework. For example, *-ADJUNCTION in-
troduces a single unfixed node with underspec-
ified tree position (replacing feature-passing or
type-raising concepts for e.g. long-distance depen-
dency); and LINK-ADJUNCTION builds a paired
(?linked?) tree corresponding to semantic con-
junction (licensing relative clauses, apposition and
more). These actions represent possible parsing
strategies and can apply optionally whenever their
preconditions are met. While largely language-
independent, some are specific to language type
(e.g. INTRODUCTION-PREDICTION in the form
used here applies only to SVO languages).
Lexical actions The lexicon associates words
with lexical actions; like computational actions,
these are sequences of tree-update actions in an
IF..THEN..ELSE format, and composed of ex-
plicitly procedural atomic tree-building actions
such as make (creates a new daughter node),
go (moves the pointer), and put (decorates the
pointed node with a label). Figure 2 shows an ex-
ample for a proper noun, John. The action checks
whether the pointed node (marked as ?) has a re-
quirement for type e; if so, it decorates it with type
e (thus satisfying the requirement), formula John?
and the bottom restriction ???? (meaning that the
node cannot have any daughters). Otherwise the
action aborts, i.e. the word ?John? cannot be parsed
in the context of the current tree.
Graph-based Parsing & Generation These ac-
tions define the parsing process. Given a sequence
of words (w1, w2, ..., wn), the parser starts from
the axiom tree T0 (a requirement to construct a
complete propositional tree, ?Ty(t)), and applies
the corresponding lexical actions (a1, a2, . . . , an),
optionally interspersing computational actions.
96
Action Input tree Output tree
John
IF ?Ty(e)
THEN put(Ty(e))
put(Fo(John?)
put(????)
ELSE ABORT
?Ty(t)
?Ty(e),
?
?Ty(e ? t)
?John??? ?Ty(t)
Ty(e), ?Ty(e)
John?, ????,?
?Ty(e ? t)
Figure 2: Lexical action for the word ?John?
T0
T1intro
T2pred
T3
link-adj
T4*-adj
T5
john
abort
T6
john
?john?
T7
thin
T8
comp
T9
pred
T10
link-adj
T11
thin
T12
comp
T13
likes
abort
abort
?likes?
Figure 3: DS parsing as a graph: actions (edges) are transitions between partial trees (nodes).
This parsing process can be modelled as a di-
rected acyclic graph (DAG) rooted at T0, with par-
tial trees as nodes, and computational and lexi-
cal actions as edges (i.e. transitions between trees)
(Sato, 2011). Figure 3 shows an example: here,
intro, pred and *adj correspond to the computa-
tional actions INTRODUCTION, PREDICTION and
*-ADJUNCTION respectively; and ?john? is a lex-
ical action. Different DAG paths represent dif-
ferent parsing strategies, which may succeed or
fail depending on how the utterance is continued.
Here, the path T0?T3 will succeed if ?John? is the
subject of an upcoming verb (?John upset Mary?);
T0 ? T4 will succeed if ?John? turns out to be a
left-dislocated object (?John, Mary upset?).
This incrementally constructed DAG makes up
the entire parse state at any point. The right-
most nodes (i.e. partial trees) make up the current
maximal semantic information; these nodes with
their paths back to the root (tree-transition actions)
make up the linguistic context for ellipsis and
pronominal construal (Purver et al, 2011). Given
a conditional probability distribution P (a|w, T )
over possible actions a given a word w and (some
set of features of) the current partial tree T , we can
parse probabilistically, constructing the DAG in a
best-first, breadth-first or beam parsing manner.
Generation uses exactly the same actions and
structures, and can be modelled on the same DAG
with the addition only of a goal tree; partial
trees are checked for subsumption of the goal
at each stage. The framework therefore inher-
ently provides both parsing and generation that
are word-by-word incremental and interchange-
able, commensurate with psycholinguistic results
(Lombardo and Sturt, 1997; Ferreira and Swets,
2002) and suitable for modelling dialogue (Howes
et al, 2012). While standard grammar formalisms
can of course also be used with incremental pars-
ing or generation algorithms (Hale, 2001; Collins
and Roark, 2004; Clark and Curran, 2007), their
string-based grammaticality and lack of inherent
parsing-generation interoperability means exam-
ples such as (1) remain problematic.
3 Method
Our task here is to learn an incremental DS gram-
mar; following Kwiatkowski et al (2012), we
assume as input a set of sentences paired with
their semantic LFs. Eshghi et al (2013) outline a
method for inducing DS grammars from semantic
DS trees (e.g. Fig. 1), in which possible lexical en-
tries are incrementally hypothesized, constrained
by subsumption of the target tree for the sentence.
Here, however, this structured tree information is
not available to us; our method must therefore con-
strain hypotheses via compatibility with the sen-
tential LF, represented as Record Types of Type
Theory with Records (TTR).
3.1 Type Theory with Records (TTR)
Type Theory with Records (TTR) is an exten-
sion of standard type theory shown useful in se-
mantics and dialogue modelling (Cooper, 2005;
Ginzburg, 2012). It is also used for representing
97
non-linguistic context such as the visual percep-
tion of objects (Dobnik et al, 2012), suggesting
potential for embodied learning in future work.
Some DS variants have incorporated TTR as the
semantic LF representation (Purver et al, 2011;
Hough and Purver, 2012; Eshghi et al, 2012).
Here, it can provide us with the mechanism we
need to constrain hypotheses in induction by re-
stricting them to those which lead to subtypes of
the known sentential LF.
In TTR, logical forms are specified as record
types (RTs), sequences of fields of the form [ l : T ]
containing a label l and a type T . RTs can be wit-
nessed (i.e. judged true) by records of that type,
where a record is a sequence of label-value pairs
[ l = v ], and [ l = v ] is of type [ l : T ] just in case
v is of type T .
R1 :
?
?
l1 : T1
l2=a : T2
l3=p(l2) : T3
?
? R2 :
[
l1 : T1
l2 : T2?
]
R3 : []
Figure 4: Example TTR record types
Fields can be manifest, i.e. given a singleton
type e.g. [ l : Ta ] where Ta is the type of which
only a is a member; here, we write this using the
syntactic sugar [ l=a : T ]. Fields can also be de-
pendent on fields preceding them (i.e. higher) in
the record type ? see R1 in Figure 4. Importantly
for us here, the standard subtyping relation ? can
be defined for record types: R1 ? R2 if for all
fields [ l : T2 ] in R2, R1 contains [ l : T1 ] where
T1 ? T2. In Figure 4, R1 ? R2 if T2 ? T2? , and
both R1 and R2 are subtypes of R3.
Following Purver et al (2011), we assume
that DS tree nodes are decorated not with simple
atomic formulae but with RTs, and correspond-
ing lambda abstracts representing functions from
RT to RT (e.g. ?r : [ l1 : T1 ].[ l2=r.l1 : T1 ] where
r.l1 is a path expression referring to the label l1
in r) ? see Figure 5. The equivalent of conjunc-
tion for linked trees is now RT extension (concate-
nation modulo relabelling ? see (Cooper, 2005;
Ferna?ndez, 2006)). TTR?s subtyping relation now
allows a record type at the root node to be in-
ferred for any partial tree, and incrementally fur-
ther specified via subtyping as parsing proceeds
(Hough and Purver, 2012).
We assume a field head in all record types, with
this corresponding to the DS tree node type. We
also assume a neo-Davidsonian representation of
?, T y(t),
?
?
?
x=john : e
e=arrive : es
p=subj(e,x) : t
head=p : t
?
?
?
Ty(e),
[
x=john : e
head=x : e
]
Ty(e ? t),
?r :
[
head : e
]
.
?
?
?
x=r.head : e
e=arrive : es
p=subj(e,x) : t
head=p : t
?
?
?
Figure 5: DS-TTR tree
predicates, with fields corresponding to the event
and to each semantic role; this allows all available
semantic information to be specified incrementally
via strict subtyping (e.g. providing the subj() field
when subject but not object has been parsed) ? see
Figure 5 for an example.
3.2 Problem Statement
Our induction procedure now assumes as input:
? a known set of DS computational actions.
? a set of training examples of the form
?Si, RTi?, where Si = ?w1 . . . wn? is a sen-
tence of the language and RTi ? henceforth
referred to as the target RT ? is the record
type representing the meaning of Si.
The output is a grammar specifying the possi-
ble lexical actions for each word in the corpus.
Given our data-driven approach, we take a prob-
abilistic view: we take this grammar as associat-
ing each word w with a probability distribution ?w
over lexical actions. In principle, for use in pars-
ing, this distribution should specify the posterior
probability p(a|w, T ) of using a particular action
a to parse a word w in the context of a particular
partial tree T . However, here we make the sim-
plifying assumption that actions are conditioned
solely on one feature of a tree, the semantic type
Ty of the currently pointed node; and that actions
apply exclusively to one such type (i.e. ambiguity
of type implies multiple actions). This simplifies
our problem to specifying the probability p(a|w).
In traditional DS terms, this is equivalent to as-
suming that all lexical actions have a simple IF
clause of the form IF ?Ty(X); this is true of
most lexical actions in existing DS grammars (see
Fig. 2), but not all. Our assumption may there-
fore lead to over-generation ? inducing actions
which can parse some ungrammatical strings ? we
must rely on the probabilities learned to make such
98
parses unlikely, and evaluate this in Section 4.
Given this, our focus here is on learning the THEN
clauses of lexical actions: sequences of DS atomic
actions such as go, make, and put (Fig. 2), but now
with attendant posterior probabilities. We will
henceforth refer to these sequences as lexical hy-
potheses. We first describe how we construct lexi-
cal hypotheses from individual training examples;
we then show how to generalise over these, while
incrementally estimating corresponding probabil-
ity distributions.
3.3 Hypothesis construction
DS is strictly monotonic: actions can only extend
the current (partial) tree Tcur, deleting nothing ex-
cept satisfied requirements. Thus, we can hypoth-
esise lexical actions by incrementally exploring
the space of all monotonic, well-formed exten-
sions T of Tcur, whose maximal semantics R is
a supertype of (extendible to) the target RT (i.e.
R ? RT ). This gives a bounded space described
by a DAG equivalent to that of section 2.2.1: nodes
are trees; edges are possible extensions; paths start
from Tcur and end at any tree with LF RT . Edges
may be either known computational actions or
new lexical hypotheses. The space is further con-
strained by the properties of the lambda-calculus
and the modal tree logic LoFT (not all possible
trees and extensions are well-formed).1
Hypothesising increments In purely semantic
terms, the hypothesis space at any point is the pos-
sible set of TTR increments from the current LF
R to the target RT . We can efficiently compute
and represent these possible increments using a
type lattice (see Figure 6),2 which can be con-
structed for the whole sentence before processing
each training example. Each edge is a RTR repre-
senting an increment from one RT, Rj , to another,
Rj+1, such that Rj ? RI = Rj+1 (where ? rep-
resents record type intersection (Cooper, 2005));
possible parse DAG paths must correspond to
some path through this lattice.
Hypothesising tree structure These DAG paths
can now be hypothesised with the lattice as a con-
straint: hypothesising possible sequences of ac-
1We also prevent arbitrary type-raising by restricting the
types allowed, taking the standard DS assumption that noun
phrases have semantic type e (rather than a higher type as in
Generalized Quantifier theory) and common nouns their own
type cn, see Cann et al (2005), chapter 3 for details.
2Clark (2011) similarly use a concept lattice relating
strings to their contexts in syntactic grammar induction.
Ri : []
R11 :
[
a : b
]
R12 :
[
c : d
]
R12 :
[
e : f
]
R21 :
[
a : b
c : d
]
R22 :
[
a : b
e : f
]
R22 :
[
c : d
e : f
]
RT :
?
?
a : b
c : d
e : f
?
?
Figure 6: RT extension hypothesis lattice
tions which extend the tree to produce the required
semantic increment, while the increments them-
selves constitute a search space of their own which
we explore by traversing the lattice.
The lexical hypotheses comprising these DAG
paths are divide into two general classes: (1) tree-
building hypotheses, which hypothesise appropri-
ately typed daughters to compose a given node;
and (2) content hypotheses, which decorate leaf
nodes with appropriate formulae from Ri (non-
leaf nodes then receive their content via beta-
reduction/extension of daughters).
Tree-building can be divided into two general
options: functional decomposition (corresponding
to the addition of daughter nodes with appropri-
ate types and formulae which will form a suitable
mother node by beta-reduction); and type exten-
sion (corresponding to the adjunction of a linked
tree whose LF will extend that of the current tree,
see Sec. 3.1 above). The availability of the former
is constrained by the presence of suitable depen-
dent types in the LF (e.g. in Fig. 5, p = subj(e, x)
depends on the fields with labels x and e, and
could therefore be hypothesised as the body of a
function with x and/or e as argument). The latter is
more generally available, but constrained by shar-
ing of a label between the resulting linked trees.
Figure 7 shows an example: a template for
functional decomposition hypotheses, extending a
node with some type requirement ?Ty(X) with
daughter nodes which can combine to satisfy that
requirement ? here, of types Y and Y ? X.
Specific instantiations are limited to a finite set of
types: e.g. X = e ? t and Y = e is allowed,
but higher types for Y are not. We implement
these constraints by packaging together permitted
sequences of tree updates as macros, and using
these macros to hypothesise DAG paths commen-
surate with the lattice.
Finally, semantic content decorations (as se-
99
IF ?Ty(X)
THEN make(??0?); go(??0?)
put(?Ty(Y )); go(???)
make(??1?); go(??1?)
put(?Ty(Y ? X)); go(?)
ELSE ABORT
Figure 7: Tree-building hypothesis
quences of put operations) are hypothesised for
the leaf nodes of the tree thus constructed; these
are now determined entirely by the tree structure
so far hypothesised and the target LF RT .
3.4 Probabilistic Grammar Estimation
This procedure produces, for each training sen-
tence ?w1 . . . wn?, all possible sequences of ac-
tions that lead from the axiom tree T0 to a tree
with the target RT as its semantics. These must
now be split into n sub-sequences, hypothesising
a set of word boundaries to form discrete word hy-
potheses; and a probability distribution estimated
over this (large) word hypothesis space to provide
a grammar that can be useful in parsing. For this,
we apply the procedure of Eshghi et al (2013).
For each training sentence S = ?w1 . . . wn?,
we have a set HT of possible Hypothesis Tuples
(sequences of word hypotheses), each of the form
HTj = ?hj1 . . . h
j
n?, where hji is the word hypoth-
esis for wi in HTj . We must estimate a prob-
ability distribution ?w over hypotheses for each
word w, where ?w(h) is the posterior probability
p(h|w) of a given word hypothesis h being used to
parse w. Eshghi et al (2013) define an incremen-
tal version of Expectation-Maximisation (Demp-
ster et al, 1977) for use in this setting.
Re-estimation At any point, the Expectation
step assigns each hypothesis tuple HTj a proba-
bility based on the current estimate ??w:
p(HTj|S) =
n
?
i=1
p(hji |wi) =
n
?
i=1
??wi(h
j
i ) (2)
The Maximisation step then re-estimates
p(h|w) as the normalised sum of the probabilities
of all observed tuples HTj which contain h,w:
???w(h) =
1
Z
?
{j|h,w?HTj}
n
?
i=1
??wi(h
j
i ) (3)
where Z is the appropriate normalising constant
summed over all the HTj?s.
Incremental update The estimate of ?w is now
updated incrementally at each training example:
the new estimate ?Nw is a weighted average of the
previous estimate ?N?1w and the new value from
the current example ???w from equation (3):
?Nw (h) =
N ? 1
N ?
N?1
w (h) +
1
N ?
??
w(h) (4)
?e.not(aux|do(v|have(pro|he, det|a(x,n|hat(x)), e), e), e)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
e=have : es
p3=not(e) : t
p2=do-aux(e) : t
r :
?
?
x : e
p=hat(x) : t
head=x : e
?
?
x2=?(r.head,r) : e
x1=he : e
p1=object(e,x2) : t
p=subject(e,x1) : t
head=e : es
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 8: Conversion of LFs from FOL to TTR.
For the first training example, a uniform distribu-
tion is assumed; when subsequent examples pro-
duce new previously unseen hypotheses these are
assigned probabilities uniformly distributed over a
held-out probability mass.
4 Experimental Setup
Corpus We tested our approach on a section
of the Eve corpus within CHILDES (MacWhin-
ney, 2000), a series of English child-directed ut-
terances, annotated with LFs by Kwiatkowski et
al. (2012) following Sagae et al (2004)?s syntactic
annotation. We convert these LFs into semanti-
cally equivalent RTs; e.g. Fig 8 shows the conver-
sion to a record type for ?He doesn?t have a hat?.
Importantly, our representations remove all
part-of-speech or syntactic information; e.g. the
subject, object and indirect object predicates func-
tion as purely semantic role information express-
ing an event?s participants. This includes e.g.
do-aux(e) in (8), which is taken merely to rep-
resent temporal/aspectual information about the
event, and could be part of any word hypothesis.
From this corpus we selected 500 short
utterance-record type pairs. The minimum utter-
ance length in this set is 1 word, maximum 7,
mean 3.7; it contains 1481 word tokens of 246
types, giving a type:token ratio of 6.0). We use the
first 400 for training and 100 for testing; the test
set alo has a mean utterance length of 3.7 words,
and contains only words seen in training.
Evaluation We evaluate our learner by compar-
ing the record type semantic LFs produced using
the induced lexicon against the gold standard LFs,
calculating precision, recall and f-score using a
method similar to Allen et al (2008).
100
Coverage % Precision Recall F-Score
Top-1 59 0.548 0.549 0.548
Top-2 85 0.786 0.782 0.782
Top-3 92 0.854 0.851 0.851
Table 1: Results: parse coverage & accuracy using
the top N hypotheses induced in training.
Each field has a potential score in the range
[0,1]. A method maxMapping(R1, R2) con-
structs a mapping from fields in R1 to those in R2
to maximise alignment, with fields that map com-
pletely scoring a full 1, and partially mapped fields
receiving less, depending on the proportion of the
R1 field?s representation that subsumes its mapped
R2 field;e.g. a unary predicate field in RT2 such
as
[
p=there(e) : t
]
could score a maximum of
3 - 1 for correct type t, 1 for correct predicate
there and 1 for the subsumption of its argument
e; we use the total to normalise the final score.
The potential maximum for any pair is therefore
the number of fields in R1 (including those in em-
bedded record types). So, for hypothesis H and
goal record type G, with NH and NG fields re-
spectively:
(5) precision = maxMapping(H,G)/NH
recall = maxMapping(H,G)/NG
5 Results
Table 1 shows that the grammar learned achieves
both good parsing coverage and semantic accu-
racy. Using the top 3 lexical hypotheses induced
from training, 92% of test set utterances receive a
parse, and average LF f-score reaches 0.851.
We manually inspected the learned lexicon for
instances of ambiguous words to assess the sys-
tem?s ability to disambiguate (e.g. the word ??s?
(is) has three different senses in our corpus: (1)
auxiliary, e.g. ?the coffee?s coming?; (2) verb
predicating NP identity, e.g. ?that?s a girl?; and
(3) verb predicating location, e.g. ?where?s the
pencil?). From these the first two were in the top
3 hypotheses (probabilities p=0.227 and p=0.068).
For example, the lexical entry learned for (2) is
shown in Fig. 9.
However, less common words fared worse: e.g.
the double object verb ?put?, with only 3 tokens,
had no correct hypothesis in the top 5. Given suffi-
cient frequency and variation in the token distribu-
tions, our method appears successful in inducing
the correct incremental grammar. However, the
complexity of the search space also limits the pos-
sibility of learning from larger record types, as the
space of possible subtypes used for hypothesising
IF ?Ty(e ? t)
THEN make(??0?); go(??0?)
put(?Ty(e))
go(??0?)
make(??1?); go(??1?)
put(Ty(e ? (e ? t)))
put(Fo(
?r1 :
[
head : e
]
?r2 :
[
head : e
]
.
?
?
?
?
?
?
?
?
x1=r1.head : e
x2=r2.head : e
e=eq : es
p1=subj(e,x2) : t
p2=obj(e,x1) : t
head=e : t
?
?
?
?
?
?
?
?
))
put(????)
ELSE ABORT
Figure 9: Action learned for second sense of ?is?
tree structure grows exponentially with the num-
ber of fields in the type. Therefore, when learning
from longer, more complicated sentences, we may
need to bring in further sources of bias to constrain
our hypothesis process further (e.g. learning from
shorter sentences first).
6 Conclusions
We have outlined a novel method for the induc-
tion of a probabilistic grammar in an inherently in-
cremental and semantic formalism, Dynamic Syn-
tax, compatible with dialogue phenomena such
as compound contributions and with no indepen-
dent level of syntactic phrase structure. Assum-
ing only general compositional mechanisms, our
method learns from utterances paired with their
logical forms represented as TTR record types.
Evaluation on a portion of the CHILDES corpus
of child-directed dialogue utterances shows good
coverage and semantic accuracy, which lends sup-
port to viewing it as a plausible, yet idealised, lan-
guage acquisition model.
Future work planned includes refining the
method outlined above for learning from longer
utterances, and then from larger corpora e.g. the
Groningen Meaning Bank (Basile et al, 2012),
which includes more complex structures. This will
in turn enable progress towards large-scale incre-
mental semantic parsers and allow further investi-
gation into semantically driven language learning.
101
References
James F. Allen, Mary Swift, and Will de Beaumont.
2008. Deep Semantic Analysis of Text. In Johan
Bos and Rodolfo Delmonte, editors, Semantics in
Text Processing. STEP 2008 Conference Proceed-
ings, volume 1 of Research in Computational Se-
mantics, pages 343?354. College Publications.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196?3200, Istan-
bul, Turkey.
Patrick Blackburn and Wilfried Meyer-Viol. 1994.
Linguistics, logic and finite trees. Logic Journal
of the Interest Group of Pure and Applied Logics,
2(1):3?29.
Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005.
The Dynamics of Language. Elsevier, Oxford.
Eugene Charniak. 1996. Statistical Language Learn-
ing. MIT Press.
Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Alexander Clark and Shalom Lappin. 2011. Linguistic
Nativism and the Poverty of the Stimulus. Wiley-
Blackwell.
Alexander Clark. 2011. A learnable representation for
syntax using residuated lattices. In Philippe Groote,
Markus Egg, and Laura Kallmeyer, editors, Formal
Grammar, volume 5591 of Lecture Notes in Com-
puter Science, pages 183?198. Springer Berlin Hei-
delberg.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42ndMeeting of the ACL, pages 111?118,
Barcelona.
Robin Cooper. 2005. Records and record types in se-
mantic theory. Journal of Logic and Computation,
15(2):99?112.
Matthew Crocker, Martin Pickering, and Charles
Clifton, editors. 2000. Architectures and Mecha-
nisms in Sentence Comprehension. Cambridge Uni-
versity Press.
A.P. Dempster, N.M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety. Series B (Methodological), 39(1):1?38.
Simon Dobnik, Robin Cooper, and Staffan Larsson.
2012. Modelling language, action, and perception in
type theory with records. In Proceedings of the 7th
International Workshop on Constraint Solving and
Language Processing (CSLP12), pages 51?63.
Arash Eshghi, Julian Hough, Matthew Purver, Ruth
Kempson, and Eleni Gregoromichelaki. 2012. Con-
versational interactions: Capturing dialogue dynam-
ics. In S. Larsson and L. Borin, editors, From Quan-
tification to Conversation: Festschrift for Robin
Cooper on the occasion of his 65th birthday, vol-
ume 19 of Tributes, pages 325?349. College Publi-
cations, London.
Arash Eshghi, Matthew Purver, and Julian Hough.
2013. Probabilistic induction for an incremental se-
mantic grammar. In Proceedings of the 10th In-
ternational Conference on Computational Seman-
tics (IWCS 2013) ? Long Papers, pages 107?118,
Potsdam, Germany, March. Association for Compu-
tational Linguistics.
Raquel Ferna?ndez. 2006. Non-Sentential Utterances
in Dialogue: Classification, Resolution and Use.
Ph.D. thesis, King?s College London, University of
London.
Fernanda Ferreira and Benjamin Swets. 2002. How
incremental is language production? evidence from
the production of utterances requiring the compu-
tation of arithmetic sums. Journal of Memory and
Language, 46:57?84.
Victor Ferreira. 1996. Is it better to give than to do-
nate? Syntactic flexibility in language production.
Journal of Memory and Language, 35:724?755.
Jonathan Ginzburg. 2012. The Interactive Stance:
Meaning for Conversation. Oxford University
Press.
E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 10(5):447?474.
John Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter of
the Association for Computational Linguistics, Pitts-
burgh, PA.
Julian Hough and Matthew Purver. 2012. Process-
ing self-repairs in an incremental type-theoretic di-
alogue system. In Proceedings of the 16th SemDial
Workshop on the Semantics and Pragmatics of Di-
alogue (SeineDial), pages 136?144, Paris, France,
September.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL 2012 Confer-
ence), pages 79?83, Seoul, South Korea, July. Asso-
ciation for Computational Linguistics.
Ruth Kempson,WilfriedMeyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow of Language Un-
derstanding. Blackwell.
102
Dan Klein and Christopher D. Manning. 2005. Nat-
ural language grammar induction with a genera-
tive constituent-context mode. Pattern Recognition,
38(9):1407?1419.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, andMark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1223?1233, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettle-
moyer, and Mark Steedman. 2012. A proba-
bilistic model of syntactic and semantic acquisition
from child-directed utterances and their meanings.
In Proceedings of the Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL).
Vincenzo Lombardo and Patrick Sturt. 1997. Incre-
mental processing and infinite local ambiguity. In
Proceedings of the 1997 Cognitive Science Confer-
ence.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Lawrence Erlbaum As-
sociates, Mahwah, New Jersey, third edition.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting
of the Association for Computational Linguistics,
pages 128?135, Newark, Delaware, USA, June. As-
sociation for Computational Linguistics.
Matthew Purver, Arash Eshghi, and Julian Hough.
2011. Incremental semantic construction in a di-
alogue system. In J. Bos and S. Pulman, editors,
Proceedings of the 9th International Conference on
Computational Semantics, pages 365?369, Oxford,
UK, January.
Kenji Sagae, Brian MacWhinney, and Alon Lavie.
2004. Adding syntactic annotations to transcripts of
parent-child dialogs. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 1815?1818, Lisbon.
Yo Sato. 2011. Local ambiguity, search strate-
gies and parsing in Dynamic Syntax. In E. Gre-
goromichelaki, R. Kempson, and C. Howes, editors,
The Dynamics of Lexical Interfaces. CSLI Publica-
tions.
Matthew Saxton. 1997. The contrast theory of nega-
tive input. Journal of Child Language, 24(1):139?
161.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
103
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 80?88,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Probabilistic Type Theory for Incremental Dialogue Processing
Julian Hough and Matthew Purver
Cognitive Science Research Group
School of Electronic Engineering and Computer Science
Queen Mary University of London
{j.hough,m.purver}@qmul.ac.uk
Abstract
We present an adaptation of recent work
on probabilistic Type Theory with Records
(Cooper et al., 2014) for the purposes of
modelling the incremental semantic pro-
cessing of dialogue participants. After
presenting the formalism and dialogue
framework, we show how probabilistic
TTR type judgements can be integrated
into the inference system of an incremen-
tal dialogue system, and discuss how this
could be used to guide parsing and dia-
logue management decisions.
1 Introduction
While classical type theory has been the predomi-
nant mathematical framework in natural language
semantics for many years (Montague, 1974, in-
ter alia), it is only recently that probabilistic type
theory has been discussed for this purpose. Sim-
ilarly, type-theoretic representations have been
used within dialogue models (Ginzburg, 2012);
and probabilistic modelling is common in dia-
logue systems (Williams and Young, 2007, inter
alia), but combinations of the two remain scarce.
Here, we attempt to make this connection, taking
(Cooper et al., 2014)?s probabilistic Type Theory
with Records (TTR) as our principal point of de-
parture, with the aim of modelling incremental in-
ference in dialogue.
To our knowledge there has been no practi-
cal integration of probabilistic type-theoretic in-
ference into a dialogue system so far; here we dis-
cuss computationally efficient methods for imple-
mentation in an extant incremental dialogue sys-
tem. This paper demonstrates their efficacy in sim-
ple referential communication domains, but we ar-
gue the methods could be extended to larger do-
mains and additionally used for on-line learning
in future work.
2 Previous Work
Type Theory with Records (TTR) (Betarte and
Tasistro, 1998; Cooper, 2005) is a rich type the-
ory which has become widely used in dialogue
models, including information state models for
a variety of phenomena such as clarification re-
quests (Ginzburg, 2012; Cooper, 2012) and non-
sentential fragments (Ferna?ndez, 2006). It has also
been shown to be useful for incremental semantic
parsing (Purver et al., 2011), incremental genera-
tion (Hough and Purver, 2012), and recently for
grammar induction (Eshghi et al., 2013).
While the technical details will be given in sec-
tion 3, the central judgement in type theory s ? T
(that a given object s is of type T ) is extended
in TTR so that s can be a (potentially complex)
record and T can be a record type ? e.g. s could
represent a dialogue gameboard state and T could
be a dialogue gameboard state type (Ginzburg,
2012; Cooper, 2012). As TTR is highly flexible
with a rich type system, variants have been con-
sidered with types corresponding to real-number-
valued perceptual judgements used in conjunction
with linguistic context, such as visual perceptual
information (Larsson, 2011; Dobnik et al., 2012),
demonstrating its potential for embodied learning
systems. The possibility of integration of per-
ceptron learning (Larsson, 2011) and naive Bayes
classifiers (Cooper et al., 2014) into TTR show
how linguistic processing and probabilistic con-
ceptual inference can be treated in a uniform way
within the same representation system.
Probabilistic TTR as described by Cooper et al.
(2014) replaces the categorical s ? T judgement
with the real number valued p(s ? T ) = v where
v ? [0,1]. The authors show how standard proba-
bility theoretic and Bayesian equations can be ap-
plied to TTR judgements and how an agent might
learn from experience in a simple classification
game. The agent is presented with instances of
80
a situation and it learns with each round by updat-
ing its set of probabilistic type judgements to best
predict the type of object in focus ? in this case
updating the probability judgement that something
is an apple given its observed colour and shape
p(s ? T
apple
? s ? T
Shp
, s ? T
Col
) where Shp ?{shp1, shp2} and Col ? {col1, col2}. From a
cognitive modelling perspective, these judgements
can be viewed as probabilistic perceptual informa-
tion derived from learning. We use similar meth-
ods in our toy domain, but show how prior judge-
ments could be constructed efficiently, and how
classifications can be made without exhaustive it-
eration through individual type classifiers.
There has also been significant experimental
work on simple referential communication games
in psycholinguistics, computational and formal
modelling. In terms of production and genera-
tion, Levelt (1989) discusses speaker strategies
for generating referring expressions in a simple
object naming game. He showed how speakers
use informationally redundant features of the ob-
jects, violating Grice?s Maxim of Quantity. In
natural language generation (NLG), referring ex-
pression generation (REG) has been widely stud-
ied (see (Krahmer and Van Deemter, 2012) for
a comprehensive survey). The incremental algo-
rithm (IA) (Dale and Reiter, 1995) is an iterative
feature selection procedure for descriptions of ob-
jects based on computing the distractor set of ref-
erents that each adjective in a referring expression
could cause to be inferred. More recently Frank
and Goodman (2012) present a Bayesian model
of optimising referring expressions based on sur-
prisal, the information-theoretic measure of how
much descriptions reduce uncertainty about their
intended referent, a measure which they claim cor-
relates strongly to human judgements.
The element of the referring expression do-
main we discuss here is incremental processing.
There is evidence from (Brennan and Schober,
2001)?s experiments that people reason at an in-
credibly time-critical level from linguistic infor-
mation. They demonstrated self-repair can speed
up semantic processing (or at least object refer-
ence) in such games, where an incorrect object
being partly vocalized and then repaired in the
instructions (e.g. ?the yell-, uh, purple square?)
yields quicker response times from the onset of
the target (?purple?) than in the case of the flu-
ent instructions (?the purple square?). This exam-
ple will be addressed in section 5. First we will
set out the framework in which we want to model
such processing.
3 Probabilistic TTR in an incremental
dialogue framework
In TTR (Cooper, 2005; Cooper, 2012), the princi-
pal logical form of interest is the record type (?RT?
from here), consisting of sequences of fields of the
form [ l ? T ] containing a label l and a type T .1
RTs can be witnessed (i.e. judged as inhabited)
by records of that type, where a record is a set of
label-value pairs [ l = v ]. The central type judge-
ment in TTR that a record s is of (record) type
R, i.e. s ? R, can be made from the component
type judgements of individual fields; e.g. the one-
field record [ l = v ] is of type [ l ? T ] just in case
v is of type T . This is generalisable to records and
RTs with multiple fields: a record s is of RT R if
s includes fields with labels matching those occur-
ring in the fields of R, such that all fields in R are
matched, and all matched fields in s must have a
value belonging to the type of the corresponding
field in R. Thus it is possible for s to have more
fields than R and for s ? R to still hold, but not
vice-versa: s ? R cannot hold if R has more fields
than s.
R
1
?
??????????
l
1
? T
1
l
2
? T
2
l
3
? T
3
(l
1
)
?????????? R2 ? [
l
1
? T
1
l
2
? T
2
?
] R
3
? []
Figure 1: Example TTR record types
Fields can have values representing predicate
types (ptypes), such as T
3
in Figure 1, and conse-
quently fields can be dependent on fields preced-
ing them (i.e. higher) in the RT, e.g. l
1
is bound in
the predicate type field l
3
, so l
3
depends on l
1
.
Subtypes, meets and joins A relation between
RTs we wish to explore is ? (?is a subtype of?),
which can be defined for RTs in terms of fields as
simply: R
1
? R
2
if for all fields [ l ? T
2
] in R
2
,
R
1
contains [ l ? T
1
] where T
1
? T
2
. In Figure 1,
both R
1
? R
3
and R
2
? R
3
; and R
1
? R
2
iff
T
2
? T
2
? . The transitive nature of this relation (if
R
1
? R
2
and R
2
? R
3
then R
1
? R
3
) can be used
effectively for type-theoretic inference.
1We only introduce the elements of TTR relevant to the
phenomena discussed below. See (Cooper, 2012) for a de-
tailed formal description.
81
We also assume the existence of manifest (sin-
gleton) types, e.g. T
a
, the type of which only a is
a member. Here, we write manifest RT fields such
as [ l ? T
a
] where T
a
? T using the syntactic sugar[ l=a ? T ]. The subtype relation effectively allows
progressive instantiation of fields (as addition of
fields to R leads to R? where R? ? R), which is
practically useful for an incremental dialogue sys-
tem as we will explain.
We can also define meet and join types of two
or more RTs. The representation of the meet type
of two RTs R
1
and R
2
is the result of a merge
operation (Larsson, 2010), which in simple terms
here can be seen as union of fields. A meet type
is also equivalent to the extraction of a maxi-
mal common subtype, an operation we will call
MaxSub(R
i
..R
n
):2
if R
1
= [ l1 ? T1
l
2
? T
2
] and R
2
= [ l2 ? T2
l
3
? T
3
]
R
1
?R
2
=
?????????
l
1
? T
1
l
2
? T
2
l
3
? T
3
?????????
= MaxSub(R
1
,R
2
)
R
1
and R
2
here are common supertypes of the
resulting R
1
? R
2
. On the other hand, the join of
two RTs R
1
and R
2
, the type R
1
? R
2
cannot be
represented by field intersection. It is defined in
terms of type checking, in that s ? R
1
? R
2
iff
s ? R
1
or s ? R
2
. It follows that if R
1
? R
2
then
s ? R
1
?R
2
iff s ? R
1
, and s ? R
1
? R
2
iff s ? R
2
.
While technically the maximally common su-
pertype of R
1
and R
2
is the join type R
1
? R
2
,
here we introduce the maximally common simple
(non disjunctive) supertype of two RTs R
1
and R
2
as field intersection:
if R
1
= [ l1 ? T1
l
2
? T
2
] and R
2
= [ l2 ? T2
l
3
? T
3
]
MaxSuper(R
1
, R
2
) = [ l
2
? T
2
]
We will explore the usefulness of this new op-
eration in terms of RT lattices in sec. 4.
3.1 Probabilistic TTR
We follow Cooper et al. (2014)?s recent extension
of TTR to include probabilistic type judgements of
the form p(s ? R) = v where v ? [0,1], i.e. the real
valued judgement that a record s is of RT R. Here
2Here we concern ourselves with simple examples that
avoid label-type clashes between two RTs (i.e. where R
1
con-
tains l
1
? T1 and R
2
contains l
1
? T2); in these cases the op-
erations are more complex than field concatenation/sharing.
we use probabilistic TTR to model a common psy-
cholinguistic experimental set up in section 5. We
repeat some of Cooper et al.?s calculations here
for exposition, but demonstrate efficient graphical
methods for generating and incrementally retriev-
ing probabilities in section 4.
Cooper et al. (2014) define the probability of the
meet and join types of two RTs as follows:
p(s ? R
1
?R
2
) = p(s ? R
1
)p(s ? R
2
? s ? R
1
)
p(s ? R
1
?R
2
) = p(s ? R
1
) + p(s ? R
2
) ? p(s ? R
1
?R
2
)
(1)
It is practically useful, as we will describe be-
low, that the join probability can be computed in
terms of the meet. Also, there are equivalences be-
tween meets, joins and subtypes in terms of type
judgements as described above, in that assuming
if R
1
? R
2
then p(s ? R
2
? s ? R
1
) = 1, we have:
if R
1
? R
2
p(s ? R
1
?R
2
) = p(s ? R
1
)
p(s ? R
1
?R
2
) = p(s ? R
2
)
p(s ? R
1
) ? p(s ? R
2
)
(2)
The conditional probability of a record being of
type R
2
given it is of type R
1
is:
p(s ? R
2
? s ? R
1
) = p(s ? R1 ? s ? R2)
p(s ? R
1
) (3)
We return to an explanation for these classical
probability equations holding within probabilistic
TTR in section 4.
Learning and storing probabilistic judgements
When dealing with referring expression games, or
indeed any language game, we need a way of stor-
ing perceptual experience. In probabilistic TTR
this can be achieved by positing a judgement set J
in which an agent stores probabilistic type judge-
ments.3 We refer to the sum of the value of proba-
bilistic judgements that a situation has been judged
to be of type R
i
within J as ?R
i
?
J
and the sum of
all probabilistic judgements in J simply as P (J );
thus the prior probability that anything is of type
R
i
under the set of judgements J is ?Ri?J
P (J ) . The
conditional probability p(s ? R
1
? s ? R
2
) un-
der J can be reformulated in terms of these sets
of judgements:
p
J
(s ? R
1
? s ? R
2
) = { ?R1?R2?J?R2?J iff ?R2?J ? 0
0 otherwise
(4)
3(Cooper et al., 2014) characterise a type judgement as an
Austinian proposition that a situation is of a given type with
a given probability, encoded in a TTR record.
82
where the sample spaces ?R
1
? R
2
?
J
and ?R
2
?
J
constitute the observations of the agent so far. J
can have new judgements added to it during learn-
ing. We return to this after introducing the incre-
mental semantics needed to interface therewith.
3.2 DS-TTR and the DyLan dialogue system
In order to permit type-theoretic inference in a
dialogue system, we need to provide suitable
TTR representations for utterances and the cur-
rent pragmatic situation from a parser, dialogue
manager and generator as instantaneously and ac-
curately as possible. For this purpose we use
an incremental framework DS-TTR (Eshghi et
al., 2013; Purver et al., 2011) which integrates
TTR representations with the inherently incre-
mental grammar formalism Dynamic Syntax (DS)
(Kempson et al., 2001).
?, T y(t),
????????????
x=john ? e
e=arrive ? es
p=subj(e,x) ? t
head=p ? t
????????????
Ty(e),
[ x=john ? e
head=x ? e ]
Ty(e ? t),
?r ? [ head ? e ] .????????????
x=r.head ? e
e=arrive ? es
p=subj(e,x) ? t
head=p ? t
????????????
Figure 2: DS-TTR tree
DS produces an incrementally specified, partial
logical tree as words are parsed/generated; follow-
ing Purver et al. (2011), DS tree nodes are dec-
orated not with simple atomic formulae but with
RTs, and corresponding lambda abstracts repre-
senting functions of type RT ? RT (e.g. ?r ?[ l
1
? T
1
].[ l
2=r.l
1
? T
1
] where r.l
1
is a path ex-
pression referring to the label l
1
in r) ? see Fig-
ure 2. Using the idea of manifestness of fields
as mentioned above, we have a natural represen-
tation for underspecification of leaf node content,
e.g. [x ? e ] is unmanifest whereas [x=john ? e ]4
is manifest and the latter is a subtype of the for-
mer. Functional application can apply incremen-
tally, allowing a RT at the root node to be com-
piled for any partial tree, which is incrementally
further specified as parsing proceeds (Hough and
Purver, 2012). Within a given parse path, due to
4This is syntactic sugar for [ x ? e
john
] and the = sign is
not the same semantically as that in a record.
DS-TTR?s monotonicity, each maximal RT of the
tree?s root node is a subtype of the parser?s previ-
ous maximal output.
Following (Eshghi et al., 2013), DS-TTR tree
nodes include a field head in all RTs which cor-
responds to the DS tree node type. We also as-
sume a neo-Davidsonian representation of predi-
cates, with fields corresponding to an event term
and to each semantic role; this allows all available
semantic information to be specified incrementally
in a strict subtyping relation (e.g. providing the
subj() field when subject but not object has been
parsed) ? see Figure 2.
We implement DS-TTR parsing and genera-
tion mechanisms in the DyLan dialogue system5
within Jindigo (Skantze and Hjalmarsson, 2010),
a Java-based implementation of the incremental
unit (IU) framework of (Schlangen and Skantze,
2009). In this framework, each module has input
and output IUs which can be added as edges be-
tween vertices in module buffer graphs, and be-
come committed should the appropriate condi-
tions be fulfilled, a notion which becomes im-
portant in light of hypothesis change and repair
situations. Dependency relations between differ-
ent graphs within and between modules can be
specified by groundedIn links (see (Schlangen and
Skantze, 2009) for details).
The DyLan interpreter module (Purver et al.,
2011) uses Sato (2011)?s insight that the context of
DS parsing can be characterized in terms of a Di-
rected Acyclic Graph (DAG) with trees for nodes
and DS actions for edges. The module?s state is
characterized by three linked graphs as shown in
Figure 3:
? input: a time-linear word graph posted by the
ASR module, consisting of word hypothesis
edge IUs between vertices W
n
? processing: the internal DS parsing DAG,
which adds parse state edge IUs between ver-
tices S
n
groundedIn the corresponding word
hypothesis edge IU
? output: a concept graph consisting of domain
concept IUs (RTs) as edges between vertices
C
n
, groundedIn the corresponding path in the
DS parsing DAG
Here, our interest is principally in the parser out-
put, to support incremental inference; a DS-TTR
generator is also included which uses RTs as goal
concepts (Hough and Purver, 2012) and uses the
5Available from http://dylan.sourceforge.net/
83
same parse graph as the interpreter to allow self-
monitoring and compound contributions, but we
omit the details here.
Figure 3: Normal incremental parsing in Dylan
4 Order theoretic and graphical methods
for probabilistic TTR
RT lattices to encode domain knowledge To
support efficient inference in DyLan, we represent
dialogue domain concepts via partially ordered
sets (posets) of RT judgements, following similar
insights used in inducing DS-TTR actions (Eshghi
et al., 2013). A poset has several advantages over
an unordered list of un-decomposed record types:
the possibility of incremental type-checking; in-
creased speed of type-checking, particularly for
pairs of/multiple type judgements; immediate use
of type judgements to guide system decisions; in-
ference from negation; and the inclusion of learn-
ing within a domain. We leave the final challenge
for future work, but discuss the others here.
We can construct a poset of type judgements
for any single RT by decomposing it into its con-
stituent supertype judgements in a record type lat-
tice. Representationally, as per set-theoretic lat-
tices, this can be visualised as a Hasse diagram
such as Figure 4, however here the ordering arrows
show ? (?subtype of?) relations from descendant to
ancestor nodes.
To characterize an RT lattice G ordered by ?,
we adapt Knuth (2005)?s description of lattices in
line with standard order theory: for a pair of RT
elements R
x
and R
y
, their lower bound is the set
of all R
z
? G such that R
z
? R
x
and R
z
? R
y
.
In the event that a unique greatest lower bound ex-
ists, this is their meet, which in G happily corre-
sponds to the TTR meet type R
x
? R
y
. Dually, if
their unique least upper bound exists, this is their
R
1200
= [] = ?
R
120
= [ a ? b ] R
121
= [ c ? d ] R
110
= [ e ? f ]
R
10
= [ a ? b
c ? d ] R11 = [ a ? be ? f ] R12 = [ c ? de ? f ]
R
1
=
?????????
a ? b
c ? d
e ? f
????????? = ?
Figure 4: Record Type lattice ordered by the sub-
type relation
join and in TTR terms is MaxSuper(R
x
, R
y
) but
not necessarily their join type R
x
? R
y
as here
we concern ourselves with simple RTs. One el-
ement covers another if it is a direct successor to
it in the subtype ordering relation hierarchy. G
has a greatest element (?) and least element (?),
with the atoms being the elements that cover ?;
in Figure 4 if R
1
is viewed as ? , the atoms are
R{10,11,12}. An RT element Rx has a comple-
ment if there is a unique element ?R
x
such that
MaxSuper(R
x
,?R
x
) = ? and R
x
? ?R
x
= ?
(the lattice in Figure 4 is complemented as this
holds for every element).
Graphically, the join of two elements can be
found by following the connecting edges upward
until they first converge on a single RT, giving us
MaxSuper(R
10
, R
12
) = R
121
in Figure 4, and the
meet can be found by following the lines down-
ward until they connect to give their meet type,
i.e. R
10
?R
12
= R
1
.
If we consider R
1
to be a domain concept in
a dialogue system, we can see how its RT lattice
G can be used for incremental inference. As in-
crementally specified RTs become available from
the interpreter they are matched to those in G to
determine how far down towards the final domain
concept R
1
our current state allows us to be. Dif-
ferent sequences of words/utterances lead to dif-
ferent paths. However, any practical dialogue sys-
tem must entertain more than one possible domain
concept as an outcome; G must therefore contain
multiple possible final concepts, constituting its
atoms, each with several possible dialogue move
sequences, which correspond to possible down-
ward paths ? e.g. see the structure of Figure 5.
Our aim here is to associate each RT in G with a
probabilistic judgement.
Initial lattice construction We define a simple
bottom-up procedure in Algorithm 1 to build a RT
84
lattice G of all possible simple domain RTs and
their prior probabilistic judgements, initialised by
the disjunction of possible final state judgements
(the priors),6 along with the absurdity ?, stipu-
lated a priori as the least element with probability
0 and the meet type of the atomic priors. The al-
gorithm recursively removes one field from the RT
being processed at a time (except fields referenced
in a remaining dependent ptype field), then orders
the new supertype RT in G appropriately.
Each node in G contains its RT R
i
and a sum
of probability judgements {?R
k
?
J
+ .. + ?R
n
?
J
}
corresponding to the probabilities of the priors it
stands in a supertype relation to. These sums are
propagated up from child to parent node as it is
constructed. It terminates when all simple maxi-
mal supertypes7 have been processed, leaving the
maximally common supertype as ? (possibly the
empty type [ ]), associated with the entire proba-
bility mass P (J ), which constitutes the denomina-
tor to all judgements- given this, only the numer-
ator of equation ?Ri?J
P (J ) needs to be stored at each
node.
Algorithm 1 Probabilistic TTR record type lattice
construction algorithm
INPUT: priors ? use the initial prior judgements for G?s atoms
OUTPUT: G
G = newGraph(priors) ? P(J) set to equal sum of prior probs
agenda = priors ? Initialise agenda
while not agenda is empty do
RT = agenda.pop()
for field ? RT do
if field ? RT.paths then ? Do not remove bound fields
continue
superRT = RT - field
if superRT ? G then ? not new? order w.r.t. RT and inherit RT?s priors
G.order(RT.address,G.getNode(superRT),?)
else ? new?
superNode = G.newNode(superRT) ? create new node w. empty priors
for node ? G do ? order superNode w.r.t. other nodes in G
if superRT.fields ? node.fields then
G.order(node,superNode,?) ? superNode inherits node?s priors
agenda.append(superRT) ? add to agenda for further supertyping
Direct inference from the lattice To explain
how our approach models incremental inference,
we assume Brennan and Schober (2001)?s experi-
mental referring game domain described in section
2: three distinct domain situation RTs R
1
, R
2
and
R
3
correspond to a purple square, a yellow square
and a yellow circle, respectively.
The RT lattice G constructed initially upon ob-
servation of the game (by instructor or instructee)
shown in Figure 5 uses a uniform distribution for
6Although the priors? disjunctive probability sums to 1 af-
ter G is constructed, i.e. in Figure 5 ?R1?J+?R2?J+?R3?J
P (J ) = 1,
the real values initially assigned to them need not sum to
unity, as they form the atoms of G (see (Knuth, 2005)).
7Note that it does not generate the join types but maximal
common supertypes defined by field intersection.
the three disjunctive final situations. Each node
shows an RT R
i
on the left and the derivation of
its prior probability p
J
(R
i
) that any game situa-
tion record will be of type R
i
on the right, purely
in terms of the relevant priors and the global de-
nominator P (J ).
G can be searched to make inferences in light
of partial information from an ongoing utterance.
We model inference as predicting the likelihood
of relevant type judgements R
y
? G of a situa-
tion s, given the judgement s ? R
x
we have so far.
To do this we use conditional probability judge-
ments following Knuth?s work on distributive lat-
tices, using the ? relation to give a choice function:
p
J
(s ? R
y
? s ? R
x
) =
???????????
1 if R
x
? R
y
0 if R
x
?R
y
= ?
p otherwise, where 0 ? p ? 1
(5)
The third case is the degree of inclusion of R
y
in R
x
, and can be calculated using the conditional
probability calculation (4) in sec. 3. For nega-
tive RTs, a lattice generated from Algorithm 1 will
be distributive but not guaranteed to be comple-
mented, however we can still derive p
J
(s ? R
y
?
s ? ?R
x
) by obtaining p
J
(s ? R
y
) in G modulo the
probability mass of R
x
and that of its subtypes:
p
J
(s ? R
y
? s ? ?R
x
) = {0 if Ry ? Rx
p
J
(s?R
y
)?p
J
(s?R
x
?R
y
)
p
J
(s??)?p
J
(s?R
x
) otherwise
(6)
The subtype relations and atomic, join and meet
types? probabilities required for (1) - (6) can be
calculated efficiently through graphical search al-
gorithms by characterising G as a DAG: the re-
verse direction of the subtype ordering edges can
be viewed as reachability edges, making ? the
source and ? the sink. With this characterisation,
if R
x
is reachable from R
y
then R
x
? R
y
.
In DAG terms, the probability of the meet of
two RTs R
x
and R
y
can be found at their highest
common descendant node ? e.g. p
J
(R
4
? R
5
) in
Figure 5 can be found as 1
3
directly at R
1
. Note if
R
x
is reachable from R
y
, i.e. R
x
? R
y
, then due
to the equivalences listed in (2), p
J
(R
x
? R
y
) can
be found directly at R
x
. If the meet of two nodes
is ? (e.g. R
4
and R
3
in Figure 5), then their meet
probability is 0 as p
J
(?)=0.
While the lattice does not have direct access to
the join types of its elements, a join type prob-
ability p
J
(R
x
? R
y
) can be calculated in terms
of p
J
(R
x
? R
y
) by the join equation in (1),
which holds for all probabilistic distributive lat-
85
PRIORS:?R
1
?
J
= 1
3?R
2
?
J
= 1
3?R
3
?
J
= 1
3
R
8
= [ x ?ind ] ?R1?J +?R2?J +?R3?J
P (J ) = ? = 1
R
4
= [ x ? ind
shp
sq
? square(x) ] ?R1?J +?R2?JP (J ) R5 = [ x ? indcol
p
? purple(x) ] ?R1?JP (J ) R6 = [ x ? indcol
y
? yellow(x) ] ?R2?J +?R3?JP (J ) R7 = [ x ? indshp
c
? circle(x) ] ?R3?JP (J )
R
1
=
?????????
x ? ind
col
p
? purple(x)
shp
sq
? square(x)
?????????
?R
1
?
J
P (J ) R2 =
?????????
x ? ind
col
y
? yellow(x)
shp
sq
? square(x)
?????????
?R
2
?
J
P (J ) R3 =
?????????
x ? ind
col
y
? yellow(x)
shp
c
? circle(x)
?????????
?R
3
?
J
P (J )
R
0
= ? = 0
Figure 5: Record type lattice with initial uniform prior probablities
tices (Knuth, 2005).8 As regards efficiency, worst
case complexity for finding the meet probability at
the common descendant of R
x
and R
y
is a linear
O(m+ n) where m and n are the number of edges
in the downward (possibly forked) paths R
x
? ?
and R
y
? ?.9
5 Simulating incremental inference and
self-repair processing
Interpretation in DyLan and its interface to the
RT lattice G follows evidence that dialogue agents
parse self-repairs efficiently and that repaired di-
alogue content (reparanda) is given special sta-
tus but not removed from the discourse context.
To model Brennan and Schober (2001)?s findings
of disfluent spoken instructions speeding up ob-
ject recognition (see section 2), we demonstrate
a self-repair parse in Figure 6 for ?The yell-, uh,
purple square? in the simple game of predicting
the final situation from {R
1
, R
2
, R
3
} continuously
given the type judgements made so far. We de-
scribe the stages T1-T4 in terms of the current
word being processed- see Figure 6:
At T1:?the? the interpreter will not yield a sub-
type checkable in G so we can only condition on
R
8
(?), giving us p
J
(s ? R
i
? s ? R
8
) = 1
3
for
i ? {1, 2, 3}, equivalent to the priors. At T2:
8The search for the meet probability is generalisable to
conjunctive types by searching for the conjuncts? highest
common descendant. The join probability is generalisable to
the disjunctive probability of multiple types, used, albeit pro-
gramatically, in Algorithm 1 for calculating a node?s proba-
bility from its child nodes.
9While we do not give details here, simple graphical
search algorithms for conjunctive and disjunctive multiple
types are linear in the number of conjuncts and disjuncts, sav-
ing considerable time in comparison to the algebraic calcula-
tions of the sum and product rules for distributive lattices.
?yell-?, the best partial word hypothesis is now
?yellow?;10 the interpreter therefore outputs an RT
which matches the type judgement s ? R
6
(i.e. that
the object is a yellow object). Taking this judge-
ment as the conditioning evidence using function
(5) we get p
J
(s ? R
1
? s ? R
6
) = 0 and us-
ing (4) we get p
J
(s ? R
2
? s ? R
6
) = 0.5 and
p
J
(s ? R
3
? s ? R
6
) = 0.5 (see the schematic
probability distribution at stage T2 in Figure 6 for
the three objects). The meet type probabilities
required for the conditional probabilities can be
found graphically as described above.
At T3:?uh purple?, low probability in the in-
terpreter output causes a self-repair to be recog-
nised, enforcing backtracking on the parse graph
which informally operates as follows (see Hough
and Purver (2012)) :
Self-repair:
IF from parsing word W the edge SE
n
is in-
sufficiently likely to be constructed from ver-
tex S
n
OR IF there is no sufficiently likely
judgement p(s ? R
x
) for R
x
? G
THEN parse word W from vertex S
n?1. IF
successful add a new edge to the top path,
without removing any committed edges be-
ginning at S
n?1; ELSE set n=n?1 and repeat.
This algorithm is consistent with a local model
for self-repair backtracking found in corpora
(Shriberg and Stolcke, 1998; Hough and Purver,
2013). As regards inference in G, upon detection
of a self-repair that revokes s ? R
6
, the type judge-
ment s ? ?R
6
, i.e. that this is not a yellow object,
10In practice, ASR modules yielding partial results are less
reliable than their non-incremental counterparts, but progress
is being made here (Schlangen and Skantze, 2009).
86
Figure 6: Incremental DS-TTR self-repair parsing. Inter-graph groundedIn links go top to bottom.
is immediately available as conditioning evidence.
Using (6) our distribution of RT judgements now
shifts: p
J
(s ? R
1
? s ? ?R
6
) = 1, p
J
(s ? R
2
?
s ? ?R
6
) = 0 and p
J
(s ? R
3
? s ? ?R
6
) = 0 be-
fore ?purple? has been parsed ? thus providing a
probabilistic explanation for increased subsequent
processing speed. Finally at T4: ?square? given
p
J
(s ? R
1
? s ? R
1
) = 1 and R
1
?R
2
= R
1
?R
3
= ?,
the distribution remains unchanged.
The system?s processing models how listen-
ers reason about the revocation itself rather than
predicting the outcome through positive evidence
alone, in line with (Brennan and Schober, 2001)?s
results.
6 Extensions
Dialogue and self-repair in the wild To move
towards domain-generality, generating the lattice
of all possible dialogue situations for interesting
domains is computationally intractable. We in-
tend instead to consider incrementally occurring
issues that can be modelled as questions (Lars-
son, 2002). Given one or more issues manifest in
the dialogue at any time, it is plausible to gener-
ate small lattices dynamically to estimate possible
answers, and also assign a real-valued relevance
measure to questions that can be asked to resolve
the issues. We are exploring how this could be
implemented using the inquiry calculus (Knuth,
2005), which defines information theoretic rele-
vance in terms of a probabilistic question lattice,
and furthermore how this could be used to model
the cause of self-repair as a time critical trade-off
between relevance and accuracy.
Learning in a dialogue While not our focus
here, lattice G?s probabilities can be updated
through observations after its initial construction.
If a reference game is played over several rounds,
the choice of referring expression can change
based on mutually salient functions from words
to situations- see e.g. (DeVault and Stone, 2009).
Our currently frequentist approach to learning is:
given an observation of an existing RT R
i
is made
with probability v, then ?R
i
?
J
, the overall denom-
inator P (J ) , and the nodes in the upward path
from R
i
to ? are incremented by v. The approach
could be converted to Bayesian update learning by
using the prior probabilities in G for calculating v
before it is added. Furthermore, observations can
be added to G that include novel RTs: due to the
DAG structure of G, their subtype ordering and
probability effects can be integrated efficiently.
7 Conclusion
We have discussed efficient methods for construct-
ing probabilistic TTR domain concept lattices or-
dered by the subtype relation and their use in
incremental dialogue frameworks, demonstrating
their efficacy for realistic self-repair processing.
We wish to explore inclusion of join types, the
scalability of RT lattices to other domains and
their learning capacity in future work.
Acknowledgements
We thank the two TTNLS reviewers for their com-
ments. Purver is supported in part by the European
Community?s Seventh Framework Programme un-
der grant agreement no 611733 (ConCreTe).
87
References
G. Betarte and A. Tasistro. 1998. Extension of Martin-
Lo?f type theory with record types and subtyping. In
G. Sambin and J. Smith, editors, 25 Years of Con-
structive Type Theory. Oxford University Press.
S. Brennan and M. Schober. 2001. How listeners
compensate for disfluencies in spontaneous speech.
Journal of Memory and Language, 44(2):274?296.
R. Cooper, S. Dobnik, S. Lappin, and S. Larsson. 2014.
A probabilistic rich type theory for semantic inter-
pretation. In Proceedings of the EACL Workshop
on Type Theory and Natural Language Semantics
(TTNLS).
R. Cooper. 2005. Records and record types in se-
mantic theory. Journal of Logic and Computation,
15(2):99?112.
R. Cooper. 2012. Type theory and semantics in flux.
In R. Kempson, N. Asher, and T. Fernando, edi-
tors, Handbook of the Philosophy of Science, vol-
ume 14: Philosophy of Linguistics, pages 271?323.
North Holland.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233?
263.
D. DeVault and M. Stone. 2009. Learning to interpret
utterances using dialogue history. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL).
S. Dobnik, R. Cooper, and S. Larsson. 2012. Mod-
elling language, action, and perception in type the-
ory with records. In Proceedings of the 7th Inter-
national Workshop on Constraint Solving and Lan-
guage Processing (CSLP12).
A. Eshghi, J. Hough, and M. Purver. 2013. Incre-
mental grammar induction from child-directed di-
alogue utterances. In Proceedings of the 4th An-
nual Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL).
R. Ferna?ndez. 2006. Non-Sentential Utterances in Di-
alogue: Classification, Resolution and Use. Ph.D.
thesis, King?s College London, University of Lon-
don.
M. C. Frank and N. D. Goodman. 2012. Predicting
pragmatic reasoning in language games. Science,
336(6084):998?998.
J. Ginzburg. 2012. The Interactive Stance: Meaning
for Conversation. Oxford University Press.
J. Hough and M. Purver. 2012. Processing self-repairs
in an incremental type-theoretic dialogue system. In
Proceedings of the 16th SemDial Workshop on the
Semantics and Pragmatics of Dialogue (SeineDial).
J. Hough and M. Purver. 2013. Modelling expectation
in the self-repair processing of annotat-, um, listen-
ers. In Proceedings of the 17th SemDial Workshop
on the Semantics and Pragmatics of Dialogue (Di-
alDam).
R. Kempson, W. Meyer-Viol, and D. Gabbay. 2001.
Dynamic Syntax: The Flow of Language Under-
standing. Blackwell.
K. H. Knuth. 2005. Lattice duality: The origin of prob-
ability and entropy. Neurocomputing, 67:245?274.
E. Krahmer and K. Van Deemter. 2012. Computa-
tional generation of referring expressions: A survey.
Computational Linguistics, 38(1):173?218.
S. Larsson. 2002. Issue-based Dialogue Management.
Ph.D. thesis, Go?teborg University. Also published
as Gothenburg Monographs in Linguistics 21.
S. Larsson. 2010. Accommodating innovative mean-
ing in dialogue. Proc. of Londial, SemDial Work-
shop, pages 83?90.
S. Larsson. 2011. The TTR perceptron: Dynamic
perceptual meanings and semantic coordination. In
Proceedings of the 15th Workshop on the Semantics
and Pragmatics of Dialogue (SemDial 2011 - Los
Angelogue).
W. Levelt. 1989. Speaking: From Intention to Articu-
lation. MIT Press.
R. Montague. 1974. Formal Philosophy: Selected Pa-
pers of Richard Montague. Yale University Press.
M. Purver, A. Eshghi, and J. Hough. 2011. Incremen-
tal semantic construction in a dialogue system. In
J. Bos and S. Pulman, editors, Proceedings of the
9th International Conference on Computational Se-
mantics.
Y. Sato. 2011. Local ambiguity, search strate-
gies and parsing in Dynamic Syntax. In E. Gre-
goromichelaki, R. Kempson, and C. Howes, editors,
The Dynamics of Lexical Interfaces. CSLI Publica-
tions.
D. Schlangen and G. Skantze. 2009. A general, ab-
stract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009).
E. Shriberg and A. Stolcke. 1998. How far do speakers
back up in repairs? A quantitative model. In Pro-
ceedings of the International Conference on Spoken
Language Processing.
G. Skantze and A. Hjalmarsson. 2010. Towards incre-
mental speech generation in dialogue systems. In
Proceedings of the SIGDIAL 2010 Conference.
J. Williams and S. Young. 2007. Scaling POMDPs
for spoken dialog management. IEEE Transac-
tions on Audio, Speech, and Language Processing,
15(7):2116?2129.
88
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 40?47,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Investigating the Contribution of Distributional Semantic Information for
Dialogue Act Classification
Dmitrijs Milajevs
Queen Mary University of London
d.milajevs@qmul.ac.uk
Matthew Purver
Queen Mary University of London
m.purver@qmul.ac.uk
Abstract
This paper presents a series of experiments
in applying compositional distributional
semantic models to dialogue act classifica-
tion. In contrast to the widely used bag-of-
words approach, we build the meaning of
an utterance from its parts by composing
the distributional word vectors using vec-
tor addition and multiplication. We inves-
tigate the contribution of word sequence,
dialogue act sequence, and distributional
information to the performance, and com-
pare with the current state of the art ap-
proaches. Our experiment suggests that
that distributional information is useful for
dialogue act tagging but that simple mod-
els of compositionality fail to capture cru-
cial information from word and utterance
sequence; more advanced approaches (e.g.
sequence- or grammar-driven, such as cat-
egorical, word vector composition) are re-
quired.
1 Introduction
One of the fundamental tasks in automatic dia-
logue processing is dialogue act tagging: labelling
each utterance with a tag relating to its function
in the dialogue and effect on the emerging con-
text: greeting, query, statement etc (see e.g. (Core,
1998)). Although factors such as intonation also
play a role (see e.g. (Jurafsky et al., 1998)), one
of the most important sources of information in
this task is the semantic meaning of an utterance,
and this is reflected in the fact that people use
similar words when they perform similar utterance
acts. For example, utterances which state opinion
(tagged sv in the standard DAMSL schema, see
below) often include words such as ?I think?, ?I
believe?, ?I guess? etc. Hence, a similarity-based
model of meaning ? for instance, a distributional
semantic model ? should provide benefits over
a purely word-based model for dialogue act tag-
ging. However, since utterances generally con-
sist of more than one word, one has to be able
to extend such similarity-based models from sin-
gle words to sentences and/or complete utterances.
Hence, we consider here the application of compo-
sitional distributional semantics for this task.
Here, we extend bag-of-word models com-
mon in previous approaches (Serafin et al., 2003)
with simple compositional distributional opera-
tions (Mitchell and Lapata, 2008) and examine the
improvements gained. These improvements sug-
gest that distributional information does improve
performance, but that more sophisticated compo-
sitional operations such as matrix multiplication
(Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011) should provide further benefits.
The state of the art is a supervised method
based on Recurrent Convolutional Neural Net-
works (Kalchbrenner and Blunsom, 2013). This
method learns both the sentence model and the
discourse model from the same training corpus,
making it hard to understand how much of the
contribution comes from the inclusion of distribu-
tional word meaning, and how much from learn-
ing patterns specific to the corpus at hand. Here,
in contrast, we use an external unlabeled resource
to obtain a model of word meaning, composing
words to obtain representations for utterances, and
rely on training data only for discourse learning
for the tagging task itself.
We proceed as follows. First, we discuss related
work by introducing distributional semantics and
describe common approaches for dialogue act tag-
ging in Section 2. Section 3 proposes several mod-
els for utterance representation based on the bag of
words approach and word vector composition. We
describe the experiment and discuss the result in
Section 4. Finally, Section 5 concludes the work.
40
2 Related work
Distributional semantics The aim of natural
language semantics is to provide logical represen-
tations of meaning for information in textual form.
Distributional semantics is based on the idea that
?You shall know a word by the company it keeps?
(Firth, 1957) ? in other words, the meaning of a
word is related to the contexts it appears in. Fol-
lowing this idea, word meaning can be represented
as a vector where its dimensions correspond to the
usage contexts, usually other words observed to
co-occur, and the values are the co-occurrence fre-
quencies. Such a meaning representation is easy
to build from raw data and does not need rich an-
notation.
Methods based on this distributional hypothe-
sis have recently been applied to many tasks, but
mostly at the word level: for instance, word sense
disambiguation (Zhitomirsky-Geffet and Dagan,
2009) and lexical substitution (Thater et al., 2010).
They exploit the notion of similarity which corre-
lates with the angle between word vectors (Turney
et al., 2010). Compositional distributional seman-
tics goes beyond the word level and models the
meaning of phrases or sentences based on their
parts. Mitchell and Lapata (2008) perform com-
position of word vectors using vector addition and
multiplication operations. The limitation of this
approach is the operator associativity, which ig-
nores the argument order, and thus word order. As
a result, ?John loves Mary? and ?Mary loves John?
get assigned the same meaning.
To capture word order, various approaches
have been proposed. Grefenstette and Sadrzadeh
(2011) extend the compositional approach by us-
ing non-associative linear algebra operators as
proposed in the theoretical work of (Coecke et
al., 2010). Socher et al. (2012) present a recur-
sive technique to build compositional meaning of
phrases from their constituents, where the non-
linear composition operators are learned by Neural
Networks.
Dialogue act tagging There are many ways to
approach the task of dialogue act tagging (Stol-
cke et al., 2000). The most successful approaches
combine intra-utterance features, such as the (se-
quences of) words and intonational contours used,
together with inter-utterance features, such as the
sequence of utterance tags being used previously.
To capture both of these aspects, sequence models
such as Hidden Markov Models are widely used
(Stolcke et al., 2000; Surendran and Levow, 2006).
The sequence of words is an observable variable,
while the sequence of dialogue act tags is a hidden
variable.
However, some approaches have shown com-
petitive results without exploiting features of inter-
utterance context. Webb et al. (2005) concentrate
only on features found inside an utterance, identi-
fying ngrams that correlate strongly with particu-
lar utterance tags, and propose a statistical model
for prediction which produces close to the state of
the art results.
The current state of the art (Kalchbrenner and
Blunsom, 2013) uses Recurrent Convolutional
Neural Networks to achieve high accuracy. This
model includes information about word identity,
intra-utterance word sequence, and inter-utterance
tag sequence, by using a vector space model of
words with a compositional approach. The words
vectors are not based on distributional frequencies
in this case, however, but on randomly initialised
vectors, with the model trained on a specific cor-
pus. This raises several questions: what is the con-
tribution of word sequence and/or utterance (tag)
sequence; and might further gains be made by ex-
ploiting the distributional hypothesis?
As our baseline, we start with an approach
which uses only word information, and excludes
word sequence, tag sequence and word distribu-
tions. Serafin et al. (2003) use Latent Semantic
Analysis for dialogue act tagging: utterances are
represented using a bag-of-words representation
in a word-document matrix. The rows in the ma-
trix correspond to words, the columns correspond
to documents and each cell in the matrix contains
the number of times a word occurs in a document.
Singular Value Decomposition (SVD) is then ap-
plied to reduce the number of rows in the matrix,
with the number of components in the reduced
space set to 50. To predict the tag of an unseen
utterance, the utterance vector is mapped to the re-
duced space and the tag of the closest neighbor is
assigned to it (using cosine similarity as a similar-
ity measure). The reported accuracy on the Span-
ish Call Home corpus for predicting 37 different
utterance tags is 65.36%.
3 Utterance models
In this paper, we investigate the extent to which
distributional representations, word order infor-
41
mation, and utterance order information can im-
prove this basic model, by choosing different ways
to represent an utterance in a vector space. We de-
sign three basic models. The first model is based
directly on the bag-of-words model which serves
as the baseline in our experiment, following (Ser-
afin et al., 2003); and extends this to investigate the
effect of word order information by moving from
word unigrams to bigrams. The second model
investigates distributional information, by calcu-
lating word vector representations from a general
corpus, and obtaining utterance representations by
composing the word vectors using simple opera-
tors. The third model extends this idea to inves-
tigate the role of utterance order information, by
including the information about the previous ut-
terance.
Bag of words The first model represents an ut-
terance as a vector where each component corre-
sponds to a word. The values of vector compo-
nents are the number of times the corresponding
words occured in the utterance. The model is sim-
ilar to (Serafin et al., 2003), but the matrix is trans-
posed. We refer to it as bag of unigrams in Table 1.
However, this bag of words approach does not
preserve any word order information. As it has
been said previously, for the dialogue act tagging
word order may be crucial. Consider these utter-
ances:
? John, are there cookies
? John, there are cookies
One of the utterances is a question (or request)
while the other is a statement. However, the bag
of words model will extract the same vector repre-
sentation for both.
To overcome this problem we also represent an
utterance as a bag of bigrams. When bigrams are
used in place of single words, the utterance rep-
resentation will differ. The question contains the
bigram ?are there?, while the statement contains
the bigram ?there are?.
Simple composition Our second model ex-
ploits the distributional hypothesis, by represent-
ing words not as atomic types (i.e. individual di-
mensions in the utterance matrix, as above), but
as vectors encoding their observed co-occurrence
distributions. We estimate these from a large cor-
pus of general written English (the Google Books
Ngrams corpus ? see below).
However, this raises the question of how to
compose these word vectors into a single repre-
sentation for an utterance. Various approaches to
compositional vector space modelling have been
successfully applied to capture the meaning of a
phrase in a range of tasks (Mitchell and Lapata,
2008; Grefenstette and Sadrzadeh, 2011; Socher
et al., 2013). In this work, we follow (Mitchell and
Lapata, 2008) and apply vector addition and point-
wise multiplication to obtain the vector of an ut-
terance from the words it consists of. This has the
advantage of simplicity and domain-generality, re-
quiring no sentence grammar (problematic for the
non-canonical language in dialogue) or training on
a specific corpus to obtain the appropriate compo-
sitionality operators or associative model; but has
the disadvantage of losing word order information.
The corresponding models are referred as addition
and multiplication in Table 1 and Table 2.
Previous utterance A conversation is a se-
quence of utterances, and the tag of an utter-
ance often depends on the previous utterance
(e.g. answers tend to follow questions). Hid-
den Markov Models (Surendran and Levow, 2006;
Stolcke et al., 2000) are often used to cap-
ture these dependencies; Recurrent Convolutional
Neural Networks (Kalchbrenner and Blunsom,
2013) have been used to simultaneously capture
the intra-utterance sequence of words and the
inter-utterance sequence of dialog tags in a con-
versation.
In this model, we are interested specifically in
the effect of inter-utterance (tag) sequence. We
provide previous addition and previous multipli-
cation models as simple attempts to capture this
phenomenon: the vector of an utterance is the con-
catenation of its vector obtained in the correspond-
ing compositional model (addition or multiplica-
tion) and the vector of the previous utterance.
4 Predicting dialogue acts
4.1 The resources
This section describes the resources we use to
evaluate and compare the proposed models.
Switchboard corpus The Switchboard corpus
(Godfrey et al., 1992) is a corpus of telephone con-
versations on selected topics. It consists of about
2500 conversations by 500 speakers from the U.S.
The conversations in the corpus are labeled with
42 unique dialogue act tags and split to 1115 train
42
A o : Okay. /
A qw : {D So, }
B qy?d: [ [I guess, +
A + : What kind of experience
[ do you, + do you ] have,
then with child care? /
B + : I think, ] + {F uh, }
I wonder if that worked. /
(a) A conversation with interrupted utterances.
A o : Okay.
A qw : So What kind of experience
do you do you have then
with child care?
B qy?d: I guess I think uh I wonder
if that worked.
(b) A preprocessed conversation.
Figure 1: A example of interrupted utterances from Switchboard and their transformation.
and 19 test conversations (Jurafsky et al., 1997;
Stolcke et al., 2000).
In addition to the dialog act tags, utterances
interrupted by the other speaker (and thus split
into two or more parts) have their continuations
marked with a special tag ?+?. Tag prediction of
one part of an interrupted utterance in isolation is
a difficult task even for a human; for example, it
would not be clear why the utterance ?So,? should
be assigned the tag qw (wh-question) in Figure 1a
without the second part ?What kind of experience
do you have [. . . ]?. Following (Webb et al., 2005)
we preprocess Switchboard by concatenating the
parts of an interrupted utterance together, giving
the result the tag of the first part and putting it in
its place in the conversation sequence. We also
remove commas and disfluency markers from the
raw text. Figure 1b illustrates the transformation
we do as preprocessing.
We split the utterances between training and
testing as suggested in (Stolcke et al., 2000).
Google Books Ngram Corpus The Google
Books Ngram Corpus (Lin et al., 2012) is a col-
lection of n-gram frequencies over books written
in 8 languages. The English part of the corpus is
based on more than 4.5 million books and contains
more than four thousand billion tokens. The re-
source provides frequencies of n-grams of length
1 to 5. For our experiments we use 5-grams from
the English part of the resource.
4.2 Word vector spaces
In distributional semantics, the meanings of words
are captured by a vector space model based on a
word co-occurrence matrix. Each row in the ma-
trix represents a target word, and each column rep-
resents a context word; each element in the matrix
is the number of times a target word co-occured
with a corresponding context word. The frequency
counts are typically normalized, or weighted us-
ing tf-idf or log-likelihood ratio to obtain better re-
sults, see (Mitchell and Lapata, 2008; Agirre et al.,
2009) for various approaches. It is also common
to apply dimensionality reduction to get higher
performance (Dinu and Lapata, 2010; Baroni and
Zamparelli, 2010).
As target words we select all the words in our
(Switchboard) training split. As context words
we choose the 3000 most frequent words in the
Google Ngram Corpus, excluding the 100 most
frequent. To obtain co-occurrence frequencies
from ngrams we sum up the frequency of a 5-gram
over the years, treat the word in the middle as a
target, and the other words as its contexts.
For normalization, we experiment with a vec-
tor space based on raw co-occurrences; a vector
space where frequencies are weighted using tf-idf;
and another one with the number of dimensions
reduced to 1000 using Non-negative Matrix Fac-
torization (NMF) (Hoyer, 2004).
We use the NMF and tf-idf implementations
provided by scikit-learn version 0.14 (Pe-
dregosa et al., 2011). For tf-idf, the term vectors
are L
2
normalized. For NMF, NNDSVD initial-
ization (Boutsidis and Gallopoulos, 2008) is used,
and the tolerance value for stopping conditions is
set to 0.001. The co-occurrence matrix is line-
normalized, so the sum of the values in each row
is 1 before applying NMF.
1
4.3 Evaluation
To evaluate these possible models we follow (Ser-
afin et al., 2003). Once we have applied a model
to extract features from utterances and build a vec-
tor space, the dimensionality of the vector space
is reduced using SVD to 50 dimensions. Then a
k-nearest neighbours (KNN) classifier is trained
and used for utterance tag prediction. In contrast
to (Serafin et al., 2003), we use Euclidean dis-
tance as a distance metric and choose the most
1
The co-occurrence matrix and the information about the
software used in the experiment are available at
http://www.eecs.qmul.ac.uk/
?
dm303/cvsc14.html
43
Method Accuracy
(Kalchbrenner and Blunsom, 2013) 0.739
(Webb et al., 2005) 0.719
(Stolcke et al., 2000) 0.710
(Serafin et al., 2003) 0.654
Bag of unigrams 0.602
Bag of bigrams 0.621
Addition 0.639
Multiplication 0.572
Previous addition 0.569
Previous multiplication 0.497
Table 1: Comparison with previous work. Note
that (Serafin et al., 2003) do not use Switchboard
and therefore their results are not directly compa-
rable to others.
frequent label among the 5 closest neighbors.
The SVD and KNN classifier implementations in
scikit-learn are used.
Baseline In our experiments, the bag of uni-
grams model accuracy of 0.602 is lower than the
accuracy of 0.654 reported in (Serafin et al., 2003),
see Table 1. The lower performance may be due
to the differences between Switchboard and Call-
Home37 corpora, in particular the tag distribu-
tion.
2
In CallHome37, 42.7% of utterances are la-
beled with the most frequent dialogue act, while
the figure in Switchboard is 31.5%; the more even
distribution in Switchboard is likely to make over-
all average accuracy levels lower.
Word order As Table 1 shows, the bag of bi-
grams model improves over unigrams. This con-
firms that word order provides important informa-
tion for predicting dialogue act tags.
Distributional models Performance of compo-
sitional distributional models depends both on
compositional operator and weighting. Table 2
demonstrates accuracy of the models. We instan-
tiate 3 vector spaces from Google Ngrams: one
space with raw co-occurrence frequencies, a tf-idf
weighted space and a reduced space using NMF.
Addition outperforms multiplication in our ex-
periments, although for other tasks multiplication
has been shown to perform better (Grefenstette
and Sadrzadeh, 2011; Mitchell and Lapata, 2008).
Lower multiplication performance here might be
2
The CallHome37 corpus is not currently available to us.
Space
Model Raw tf-idf NMF
Addition without SVD 0.592
Addition 0.610 0.639 0.620
Multiplication 0.572 0.568 0.525
Previous addition 0.569
Previous multiplication 0.497
Table 2: Accuracy results for different composi-
tional models and vector spaces.
due to the fact that some utterances are rather long
(for example, more than 70 tokens), and the result-
ing vectors get many zero components.
Selection of the optimal weighting method
could be crucial for overall model performance.
The 3 weighting schemes we use give a broad va-
riety of results; more elaborate weighting and con-
text selection might give higher results.
Figure 2 illustrates dialog tag assignment us-
ing addition and the tf-idf weighted vector space.
As we do not use any inter-utterance features, the
first two statements, which consist only of the
word Okay, got assigned wrong tags. However,
the Wh-question in the conversation got classified
as a Yes-No-question, probably because what did
not influence the classification decision strongly
enough and could have been classified correctly
using only intra-utterance features. Also, the ex-
ample shows how important grammatical features
are: the verb think appears in many different con-
text, and its presence does not indicate a certain
type of an utterance.
In addition, we observed that SVD improves
classification accuracy. The accuracy of KNN
classification without prior dimensionality reduc-
tion drops from 0.610 to 0.592 for vector addition
on the raw vector space.
Utterance sequence To solve the issue of utter-
ances that can be tagged correctly only by consid-
ering inter-utterance features, we included previ-
ous utterance. However, in our experiment, such
inclusion by vector concatenation does not im-
prove tagging accuracy (Table 2). The reason for
this could be that after concatenation the dimen-
sionality of the space doubles, and SVD can not
handle it properly. We evaluated only dimension-
ally reduced spaces because of the memory limit.
44
B**
(b) : Okay.
A b?m (b) : Okay.
B qw (qy): Well what do you think about the idea of uh kids having to do public
service work for a year?
B qy (sd): Do you think it?s a <breathing>
A sv (sv): Well I I think it?s a pretty good idea.
A sv (sd): I think they should either do that or or afford some time to the military
or or helping elderly people.
B aa (aa): Yes
B aa (b) : yes
B % (%) : def
A sv (sv): I I you know I think that we have a bunch of elderly folks in the country
that could use some help
Figure 2: The beginning of the conversation 2151 from the test split of Switchboard. In brackets the
tags predicted using vector addition as a composition method on the tf-idf space are given. We mark
fo o fw " by bc as
**
.
Summary Our accuracy is lower compared to
other work. Webb et al. (2005)?s method, based
only on intra-utterance lexical features, but incor-
porating longer ngram sequences and feature se-
lection, yields accuracy of 0.719. Advanced treat-
ment of both utterance and discourse level features
yields accuracy of 0.739 (Kalchbrenner and Blun-
som, 2013). However, our experiments allow us to
evaluate the contribution of various kinds of infor-
mation: vector spaces based on word bigrams and
on co-occurrence distributions both outperformed
the bag of words approach; but incorporation of
previous utterance information did not.
5 Conclusions and future work
In this work we evaluated the contribution of
word and utterance sequence, and of distributional
information using simple compositional vector
space models, for dialogue act tagging. Our exper-
iments show that information about intra-utterance
word order (ngrams), and information about word
co-occurence distributions, outperforms the bag of
words models, although not competitive with the
state of the art given the simplistic compositional
approach used here. Information about utterance
tag sequence, on the other hand, did not.
The usage of an external, large scale resource
(here, the Google Ngram Corpus) to model word
senses improves the tagging accuracy in compari-
son to the bag of word model, suggesting that the
dialogue act tag of an utterance depends on its se-
mantics.
However, the improvements in performance of
the bag of bigrams model in comparison to bag of
unigrams, and the much higher results of Webb et
al. (2005)?s intra-utterance approach, suggest that
the sequence of words inside an utterance is cru-
cial for the dialogue act tagging task. This sug-
gests that our simplistic approaches to vector com-
position (addition and multiplication) are likely
to be insufficient: more advanced, sequence- or
grammar-driven composition, such as categorical
composition (Coecke et al., 2010), might improve
the tagging accuracy.
In addition, our results show that the perfor-
mance of distributional models depends on many
factors, including compositional operator selec-
tion and weighting of the initial co-occurrence ma-
trix. Our work leaves much scope for improve-
ments in these factors, including co-occurrence
matrix instantiation. For example, the window
size of 2, which we used to obtain co-occurrence
counts, is lower than the usual size of 5 (Dinu and
Lapata, 2010), or the sentence level (Baroni and
Zamparelli, 2010). Word representation in a vec-
tor space using neural networks might improve ac-
curacy as well (Mikolov et al., 2013).
Previous approaches to dialogue act tagging
have shown utterance/tag sequence to be a use-
ful source of information for improved accuracy
(Stolcke et al., 2000). We therefore conclude that
the lower accuracy we obtained using models that
include information about the previous utterance
is due again to our simplistic method of compo-
sition (vector concatenation); models which re-
flect dialogue structure or sequence explicitly are
likely to be more suited. Kalchbrenner and Blun-
som (2013) give one way in which this can be
achieved by learning from a specific corpus, and
the question of possible alternatives and more gen-
eral models remains for future research.
45
Acknowledgments
We thank Mehrnoosh Sadrzadeh for her helpful
advice and valuable discussion. We would like
to thank anonymous reviewers for their effective
comments. Milajevs is supported by the EP-
SRC project EP/J002607/1. Purver is supported
in part by the European Community?s Seventh
Framework Programme under grant agreement no
611733 (ConCreTe).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27. Association for Computational Lin-
guistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
Christos Boutsidis and Efstratios Gallopoulos. 2008.
Svd based initialization: A head start for nonneg-
ative matrix factorization. Pattern Recognition,
41(4):1350?1362.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. CoRR,
abs/1003.4394.
Mark Core. 1998. Analyzing and predicting patterns
of damsl utterance tags. In Proceedings of the AAAI
spring symposium on Applying machine learning to
discourse processing.
G. Dinu and M. Lapata. 2010. Measuring distribu-
tional similarity in context. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172. Associa-
tion for Computational Linguistics.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517?520. IEEE.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394?1404. Asso-
ciation for Computational Linguistics.
Patrik O Hoyer. 2004. Non-negative matrix factor-
ization with sparseness constraints. The Journal of
Machine Learning Research, 5:1457?1469.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca.
1997. Switchboard swbd-damsl shallow-discourse-
function annotation coders manual, draft 13. Tech-
nical Report 97-02, University of Colorado, Boul-
der. Institute of Cognitive Science.
Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox, and
Traci Curl. 1998. Lexical, prosodic, and syn-
tactic cues for dialog acts. In Proceedings of the
ACL-COLING Workshop on Discourse Relations
and Discourse Markers.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector SpaceModels and their Composition-
ality, pages 119?126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the Google Books ngram
corpus. In Proceedings of the ACL 2012 System
Demonstrations, pages 169?174. Association for
Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244. Association for
Computational Linguistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Riccardo Serafin, Barbara Di Eugenio, and Michael
Glass. 2003. Latent semantic analysis for dia-
logue act classification. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology: companion volume of
the Proceedings of HLT-NAACL 2003?short papers-
Volume 2, pages 94?96. Association for Computa-
tional Linguistics.
46
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Carol Van Ess-Dykema, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Dinoj Surendran and Gina-Anne Levow. 2006. Dialog
act tagging with support vector machines and hidden
markov models. In INTERSPEECH.
Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?10, pages
948?957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Nick Webb, Mark Hepple, and Yorick Wilks. 2005.
Dialogue act classification based on intra-utterance
features. In Proceedings of the AAAI Workshop on
Spoken Language Understanding. Citeseer.
M. Zhitomirsky-Geffet and I. Dagan. 2009. Bootstrap-
ping distributional feature vector quality. Computa-
tional Linguistics, 35(3):435?461.
47
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 7?16,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Linguistic Indicators of Severity and Progress in Online Text-based
Therapy for Depression
?
Christine Howes, Matthew Purver
Cognitive Science Research Group
School of Electronic Engineering and Computer Science
Queen Mary University of London
London, UK
{c.howes,m.purver}@qmul.ac.uk
Rose McCabe
University of Exeter Medical School
Exeter, UK
r.mccabe@exeter.ac.uk
Abstract
Mental illnesses such as depression and
anxiety are highly prevalent, and therapy
is increasingly being offered online. This
new setting is a departure from face-to-
face therapy, and offers both a challenge
and an opportunity ? it is not yet known
what features or approaches are likely to
lead to successful outcomes in such a dif-
ferent medium, but online text-based ther-
apy provides large amounts of data for lin-
guistic analysis. We present an initial in-
vestigation into the application of compu-
tational linguistic techniques, such as topic
and sentiment modelling, to online ther-
apy for depression and anxiety. We find
that important measures such as symptom
severity can be predicted with compara-
ble accuracy to face-to-face data, using
general features such as discussion topic
and sentiment; however, measures of pa-
tient progress are captured only by finer-
grained lexical features, suggesting that
aspects of style or dialogue structure may
also be important.
1 Introduction
Mental illnesses such as depression and anxiety
have been called ?the biggest causes of misery
in Britain today? (Layard, 2012). The main av-
enue of treatment for such conditions is talking
therapies, such as Cognitive Behavioural Therapy
(CBT); however, there is far greater demand than
can currently be met, and currently only 25% of
sufferers in the UK receive treatment. Therapy is
therefore increasingly being delivered online: this
?
This work was partly supported by the ConCreTe project.
The project ConCreTe acknowledges the financial support
of the Future and Emerging Technologies (FET) programme
within the Seventh Framework Programme for Research of
the European Commission, under FET grant number 611733.
helps to improve access and reduce waiting times,
and is just as effective as standard therapy (Kessler
et al., 2009). However, this new online setting
provides a challenge of evaluation and optimisa-
tion (Hanley and Reynolds, 2009; Beattie et al.,
2009). Online therapy is a significant departure
from face-to-face therapy, and it is not yet known
exactly what features or approaches are likely to
lead to successful outcomes, or help identify neg-
ative outcomes such as risk to the patient or oth-
ers. Current methods (e.g. controlled studies) are
expensive and time-consuming; we need fast, ac-
curate methods to ensure treatment can be made
effective and efficient in this new context.
Professional communication varies widely
(McCabe et al., 2013b) and aspects of doctor-
patient interaction and language are known to
influence outcomes such as patient satisfaction,
treatment adherence and health status (Ong et
al., 1995; McCabe et al., 2013a). For therapists,
automated methods to analyse therapist-client
communication are of interest as there is little
known about how the quality of communication
influences patient outcome. Identifying patterns
of effective communication ? both in terms
of what is spoken about and how it is spoken
about ? would help guide training of therapists.
Moreover, it may assist in identifying successful
therapy and perhaps, more importantly, where
communication is not therapeutic and patients are
failing to improve. This may warrant a different or
more intensive therapeutic intervention. Applying
computational linguistic techniques to therapy
data could therefore offer potential to produce
tools which can aid clinicians in predicting out-
comes, diagnosing severity of symptoms and/or
evaluating progress. Recent work on spoken
therapy dialogue has shown promising results in a
range of mental health tasks, including diagnosis
of post-traumatic stress disorder (PTSD) and
depression (DeVault et al., 2013; Yu et al., 2013),
7
and prediction of outcomes in schizophrenia
treatment (Howes et al., 2013).
Online therapy data provides a new challenge
? language and interaction styles differ to face-
to-face ? but also an opportunity in the availabil-
ity of large amounts of text data without the need
for automatic speech recognition or manual tran-
scription. Here, we present an initial investiga-
tion into the application of computational linguis-
tic techniques to online therapy for depression and
anxiety. We find that important measures such as
symptom severity can be predicted with compara-
ble accuracy to face-to-face data, and that general
aspects such as discussion topic and sentiment are
useful predictors; and suggest some ways in which
techniques can be adapted for improved perfor-
mance in future.
2 Background
2.1 Computational analysis & mental health
Research into computer-based diagnosis in mental
health goes back at least to the 1960s ? see (Over-
all and Hollister, 1964; Hirschfeld et al., 1974)
amongst others ? but most systems rely on doctor-
or patient-reported data rather than naturally oc-
curring language. Much recent work similarly
uses self-reported clinical and socio-demographic
data, e.g. to predict treatment resistance in depres-
sion (Perlis, 2013). Some recent natural language
processing (NLP) research examines features of
the language used by patients when discussing
conditions or treatment, e.g. discovering topics
and opinions from online doctor ratings (Paul et
al., 2013) or social media (Paul and Drezde, 2011).
However, aspects of the communication dur-
ing treatment itself are also associated with pa-
tient outcomes (Ong et al., 1995). In the mental
health domain, recent work suggests that, for pa-
tients with schizophrenia both conversation struc-
ture (how communication proceeds in therapy),
and content (what is talked about), can affect out-
comes (McCabe et al., 2013a; John et al., under
review). NLP research has now begun to examine
both. Wallace et al. (2013) model speech acts to
characterise doctor-patient consultations on medi-
cation adherence; Angus et al. (2012) use unsuper-
vised topic models to visualise shared content in
clinical dialogue; Cretchley et al. (2010) use a sim-
ilar approach for a qualititative analysis of topic
and communication style between patients with
schizophrenia and carers. DeVault et al. (2013)
use features of speech, and Yu et al. (2013) mul-
timodal features, from video-mediated dialogue to
detect depression and PTSD with promising accu-
racies (0.66 to 0.74 depending on condition and
task). In face-to-face therapy for schizophrenia,
Howes et al. (2012; 2013) use a combination of
supervised and unsupervised approaches to pre-
dict a range of diagnostic and outcome measures,
including future adherence to treatment (accuracy
0.70); fine-grained lexical features gave reason-
able accuracy, with more general topic features
giving weaker prediction of some outcomes.
2.2 Topic modelling
One focus of research for mental health is there-
fore on methods for analysing content (what is
talked about). Traditional methods, while ef-
fective, involve time-consuming hand-coding of
data (Beattie et al., 2009; John et al., under re-
view); NLP techniques can reduce this require-
ment. Unsupervised probabilistic models (e.g. La-
tent Dirichlet Allocation (LDA) Blei et al. (2003)
and variants) have been widely applied to learn
topics (word distributions) from the data itself,
connecting words with similar meanings and even
distinguishing between uses of words with mul-
tiple meanings (Steyvers and Griffiths, 2007).
Such techniques have been applied successfully
to structured dialogue e.g. meetings and tuto-
rials (Purver et al., 2006; Eisenstein and Barzi-
lay, 2008), and more recently to dialogues in the
clinical domain (Cretchley et al., 2010; Howes et
al., 2013), with topics found to identify important
themes within therapy conversation such as medi-
cation, symptoms, family and social issues, and to
correlate with outcomes.
2.3 Sentiment and emotion analysis
One aspect of conversation process and style is
the affect or emotion present. NLP research has
generally approached this via the task of senti-
ment detection, distinguishing positive from neg-
ative (and sometimes neutral) stance (Pang and
Lee, 2008). Methods generally take either a
knowledge-rich approach (relying on e.g. dictio-
naries of sentiment-carrying words (Pennebaker
et al., 2007)), or a data-rich approach via (usu-
ally supervised) machine learning over datasets of
sentiment-carrying text (e.g. Socher et al. (2013)).
The former can provide deeper insights, but are
less robust in the face of unexpected vocabulary,
unusual or errorful spelling; the latter are more ro-
8
bust but require training from large datasets. Re-
cent research has attempted finer-grained distinc-
tions, e.g. detecting specific emotions such as
anger, surprise, fear etc; again, approaches can
be characterised as dictionary-based or machine-
learning-based (Chuang and Wu, 2004; Seol et al.,
2008; Purver and Battersby, 2012; De Choudhury
et al., 2012). The resulting sentiment or emotion
ratings have been widely used to determine as-
pects of personality and mental state in various do-
mains. In social media text, Quercia et al. (2011;
2012) found correlations between sentiment and
levels of popularity, influence and general well-
being; O?Connor et al. (2010) with measures of
public opinion. Closer to our application, Liakata
et al. (2012) show that these methods can be ap-
plied to analyse emotion in suicide notes.
2.4 Research questions
Here, similar to (DeVault et al., 2013; Howes et
al., 2013), our primary question is whether these
approaches can be usefully applied to diagnose
conditions and predict outcomes, but in a new
modality ? online text-based therapy ? which may
require different and/or more robust methods. In
addition, we would like to gain some insight into
which features of language and interaction might
be predictive, in order to help clinicians improve
therapeutic methods, and to assess how general
and transferable any model might be. Our main
questions here are therefore:
? What features of text-based online therapy di-
alogue might help predict symptoms and/or
outcomes? Specifically, how predictive are
conversation topic and emotional content?
? Can we detect them accurately and reli-
ably, using approaches generalisable to large
datasets, across different subjects and condi-
tions?
? Can the features provide any insights into the
treatment process and/or the online modality?
3 Method
3.1 Data
The data used in this study consisted of the tran-
scripts from 882 Cognitive Behavioural Ther-
apy (CBT) treatment dialogues between patients
with depression and/or anxiety and their ther-
apists using an online text-based chat system.
The transcripts are from online CBT provided
by Psychology Online, who deliver ?live? therapy
from a qualified psychologist accessed via the in-
ternet (http://www.psychologyonline.
co.uk). Of the 882 transcripts, 837 are between
therapists and patients who were in an ongoing
treatment program or had completed their treat-
ment by the time our sample was collected. There
are 167 patients in this sample (125 females and
42 males), with 35 different therapists (for 2 pa-
tients the identity of the therapist is unknown).
The number of transcripts per patient ranges from
1 to 14, with a mean of 5.011 (s.d. 2.73). For all
of the measures based on the transcripts, as out-
lined below, we included all text typed by both the
therapist and the patient. In addition to the tran-
scripts themselves, each patient normally filled out
two questionnaires prior to each session with their
therapist. These are described below.
3.2 Outcomes
Patient Health Questionnaire (PHQ-9) This
is a self-administered diagnostic instrument for
common mental disorders (Kroenke and Spitzer,
2002). The PHQ-9 is the depression module,
which scores each of the 9 DSM-IV criteria as ?0?
(not at all) to ?3? (nearly every day). A higher
score indicates higher levels of depression, with
scores ranging from 0-27. It has been validated
for use (Martin et al., 2006).
Generalised Anxiety Disorder scale (GAD-7)
Similarly, the GAD-7 (Spitzer et al., 2006) is a
brief self-report scale of generalised anxiety disor-
der. This is a 7-item scale which scores each of the
items as ?0? (not at all) to ?3? (nearly every day).
A higher score indicates higher levels of anxiety.
Outcome measures For the data in our sam-
ple, PHQ-9 and GAD-7 were highly correlated
(r = 0.811, p < 0.001) so for the results re-
ported below we focus on PHQ-9. As each patient
filled in the PHQ-9 before each consultation, we
used two different outcome measures: PHQ now
? the PHQ-9 score of the patient for the question-
naire completed immediately prior to the consulta-
tion; and PHQ start-now ? the difference between
the PHQ-9 score prior to any treatment and PHQ
now, i.e. a measure of progress (how much bet-
ter or worse the patient is since the start of their
treatment). Although these two measures are nu-
merical, one of the general aims of our research
is to identify patients at risk. We therefore bina-
rised the outcome measures and treated our task
9
as a categorisation problem to identify the group
of interest. For PHQ now, these were patients with
moderate to severe symptoms; for PHQ start-now,
patients whose PHQ score had not improved.
3.3 Topics
The transcripts from the 882 treatment consulta-
tions were analysed using an unsupervised proba-
bilistic topic model, using MALLET (McCallum,
2002) to apply standard Latent Dirichlet Alloca-
tion (Blei et al., 2003), with the notion of docu-
ment corresponding to a single consultation ses-
sion, represented as the sequence of words typed
by any speaker. Stop words (common words
which do not contribute to the content, e.g. ?the?,
?to?) were removed as usual (Salton and McGill,
1986), but the word list had to be augmented for
text chat conventions and spellings (e.g. unpunc-
tuated ?ive?). Additionally, common mispellings
were mapped to their correctly spelled equivalents
using Microsoft Excel?s in-built spellchecker. This
was due to the nature of text chat, in contrast to
transcribed speech or formal text ? the word ?ques-
tionnaire?, for example, was found to have been
typed in 21 different ways. Following (Howes et
al., 2013) we set the number of topics to 20,
1
used
the default setting of 1000 Gibbs sampling itera-
tions, and enabled automatic hyperparameter opti-
misation to allow an uneven distribution of topics
via an asymmetric prior over the document-topic
distributions (Wallach et al., 2009).
As Howes et al. (2013) did in face-to-face ther-
apy, we found most topics were composed of co-
herent word lists, with many corresponding to
common themes in therapy e.g. family (Topic 12),
symptoms (16), treatment process (2, 14), and is-
sues in work and social life (19, 5) ? see Table 5.
3.4 Sentiment and emotion analysis
Each turn in the transcripts was then annotated for
strength of positive and negative sentiment, and
level of anger. We compared three approaches: the
dictionary-based LIWC (Pennebaker et al., 2007)
and two machine learning approaches, the Stan-
ford classifier based on deep neural nets and parse
structure trained on standard text (Socher et al.,
2013), and one based on distant supervision over
social media text, Sentimental (Purver and Bat-
1
An arbitrary decision, but Howes et al. (2013) chose it
to match the number defined by manual coders in a therapy
domain.
tersby, 2012).
2
None are specifically designed for
therapy dialogue data; however, given the unortho-
dox spelling and vocabulary used in text chat, we
expect machine-learning based approaches, and
training on ?noisy? social media text, to provide
more robustness.
We used each to provide a posi-
tive/negative/neutral sentiment value; for LIWC,
we took this from the relative magnitudes of the
posemo and negemo categories. Two human
judges then rated the 85 utterances in one tran-
script independently. Inter-annotator agreement
was good, with Cohen?s kappa = 0.66. Agreement
with LIWC was poor (0.43-0.45); with Stanford
better (0.51-0.54); but best with Sentimental
(0.63-0.80). For anger, LIWC gave only one
utterance a non-zero rating, while Sentimental
provided a range of values. We therefore used
Sentimental in our experiments. Raw values
per turn were scaled to [-1,+1] for sentiment
(-1 representing strong negative sentiment, +1
strong positive), and [0,1] for anger; we then
derived minimum, maximum, mean and standard
deviation values per transcript.
3.5 Classification experiments
We performed a series of experiments, to inves-
tigate whether various features of the transcripts
could enable automatic detection of patient re-
sponses to the PHQ-9. The full range of possible
features were calculated for each transcript ? see
Table 1. As well as topic, sentiment and emotion
features as detailed above, we include raw lexi-
cal features to characterise details of content, and
some high-level features (amount of talk; patient
demographics; and therapist identity, known to af-
fect outcomes).
In each case, we used the Weka machine learn-
ing toolkit (Hall et al., 2009) to pre-process
data, and a decision tree classifier (J48), a logis-
tic regression model and the support vector ma-
chine implementation LibLINEAR (Chang and
Lin, 2001) as classifiers. PHQ now was bina-
rised based on the classification in Kroenke and
Spitzer (2002), whereby scores of 10 or over are
moderate to severe and scores of less than 10 are
mild. PHQ start-now was binarised according to
whether there was an improvement (reduction) in
the PHQ score or not. Positive scores indicate
2
Available from liwc.net, nlp.stanford.edu
and sentimental.co respectively.
10
Feature set Description
AgentID Identity of the therapist
High level Client gender; client age group; session
number; client/agent number of words and
turns used; proportion of all words per par-
ticipant
Topic Probability distribution of topics per tran-
script (one value per topic per transcript)
Sentiment Overall sentiment mean, standard devi-
ation, minimum and maximum; overall
anger mean, standard deviation, minimum
and maximum
Word Unigrams, for all words that appeared in
at least 20 of the transcripts, regardless of
speaker; the features were the normalised
counts of each word
N-gram As word, but including unigrams, bigrams
and trigrams
Table 1: Feature sets for classification experiments
an improvement; scores of 0 or lower indicate no
change or a worsening of PHQ score. Each out-
come indicator was tested with different feature
sets using 10-fold cross-validation.
3
4 Results
4.1 Correlations
First, we examined statistical associations be-
tween our four outcome measures and our avail-
able features (see Section 3). R-values are shown
for all significant correlations (at the p < 0.05
level) in Tables 2-4. For the PHQ now measure,
a positive correlation means a greater value of the
feature is associated with a greater value of the
PHQ score (i.e. a higher level of symptoms). For
the PHQ start-now measures, a positive correla-
tion means that a greater value of the feature is as-
sociated with a greater improvement in the PHQ
score since the start of treatment. Correlations
greater than ?0.2 are shown in bold.
High-level With patients with a worse (higher)
PHQ score (PHQ now), more words and turns are
typed by both participants. Better overall progress
scores are also weakly associated with the amount
of talk, with fewer turns typed by both participants
if patients? PHQ score has improved by a greater
3
We partition the data into 10 equal subsamples, and use
each subsample as the test data for a model trained on the
remaining 90%. This is repeated for each subsample (the 10
folds), and the test predictions collated to give the overall re-
sults. This partitioning is done by transcript: different tran-
scripts from the same patient may therefore appear in training
and test data within the same fold; our use of low-dimensional
topic/sentiment features should minimise over-fitting, but fu-
ture work will investigate the extent of this effect.
amount since the start of their treatment program
(see Table 2).
Sentiment As shown in Table 3, more negative
sentiment expressed in the transcripts (mean and
minimum), a higher variability of sentiment be-
tween negative and positive (s.d.), and greater lev-
els of anger (mean and maximum) are associated
with worse PHQ scores. More positive sentiments
(mean and maximum) are also associated with bet-
ter progress.
Topic Topics 2, 6, 9, 10, 16 and 17 are neg-
atively correlated with PHQ scores, i.e. higher
levels of these topics are associated with better
PHQ (see Table 4). Some of these topics involve
words related to assessing the patient?s progress
and feedback, e.g. topic 2 includes session, goals
and questionnaires, and topic 17 includes good,
work and positive. Others relate to specific con-
cerns of the patient, e.g. topic 6 (worry, worrying
and problem) and topic 16 (anxiety, fear and sick).
The top twenty words assigned to each topic by
LDA, and the direction of significant correlations
are shown in Table 5.
Conversely, topics 4, 5, 7, 8, 11 and 18 are
positively correlated with PHQ scores, meaning
more talk assigned to these topics is associated
with worse PHQ. Several of these topics relate to
specific issues, such as topic 5 (sleep, bed, night)
and topic 18 (eating, food, weight). Some of these
topics display overlap with the previous group
(e.g. topics 2 and 4 both contain words reviewing
progress such as session, week, next and last); this
suggests that some topics (e.g. progress or particu-
lar issues) are discussed in importantly (and recog-
nisably) different ways or contexts (possibly dif-
ferent emotional valences ? see below), and these
differences are being identified by the automatic
topic modelling.
Similarly, greater amounts of talk in topics 2, 15
and 17 are weakly associated with better progress.
These are the topics identified above as involving
words related to assessing progress, and feedback.
Greater amounts of talk in topic 8 (checking, OCD,
anxiety, rituals) is associated with worse progress.
Cross-correlations between topic and senti-
ment features Previous work has hypothesised
that automatically derived topics may differ from
hand-coded topics in picking up additional factors
of the communication such as valence (Howes et
al., 2013). To explore this on a global level (i.e.
11
Measure PHQ now PHQ start-now
Agent number of words 0.231
Client number of words 0.195
Agent number of turns 0.149 -0.080
Client number of turns 0.193 -0.071
Table 2: Significant correlations of high-level features and outcomes
Measure PHQ now PHQ start-now
Sentiment mean -0.237 0.119
Sentiment s.d. 0.161
Sentiment minimum -0.167
Sentiment maximum 0.074
Anger mean 0.185
Anger s.d. 0.074
Anger minimum
Anger maximum 0.192
Table 3: Significant correlations of sentiment features and outcomes
at the level of the transcript, rather than at the
finer-grained level of the turn) we examined cross-
correlations between sentiment and topic. This
initial exploration offers support for this hypoth-
esis, as can be seen in Table 6. For example, top-
ics 3 and 4 both contain words relating to feel-
ings and thoughts, but topic 3 is positively corre-
lated with sentiment, while topic 4 is negatively
correlated. These correlations indicate a complex
relationship between topic and sentiment which
should be explored further in future research; a
joint topic-sentiment model might be appropriate
e.g. (Paul et al., 2013). Although some topics
pattern consistently with sentiment (e.g. topic 12,
with words about relatives and relationships, is as-
sociated with negative sentiments and higher lev-
els of anger) some do not (e.g. topic 19 is asso-
ciated with more positive sentiment, but greater
anger). Examination suggests that this topic in-
volves discussions about feelings of anger, but not
necessarily expressing anger, and also may include
talk on how to deal with such feelings (with words
like assertive). These observations may indicate
that in this domain, in which people explicitly talk
about their feelings, fully accurate sentiment and
emotion analysis may require a different approach
than in domains such as social media analysis.
4.2 Classification experiments
Results of classification experiments on different
feature sets are shown in Tables 7-9. For each ex-
periment, the weighted average f-score is shown,
with the f-score for the class of interest shown
in brackets. For PHQ now the class of interest
is patients with high (moderate to severe) PHQ-9
scores; for PHQ start-now we are concerned with
patients who are not getting better. As a baseline,
the proportion of the data in the class of interest in
each case is shown in the first column in Table 7 ?
note that these are not exactly 50%, but reflect the
actual proportions in the data (see Section 3.5).
High-level As can be seen in Table 7, if we use
a feature set consisting of high-level features and
AgentID, we are able to predict PHQ now and
PHQ start-now reasonably well (> 0.7). How-
ever, given the nature of the data, it is uncommon
for a therapist to have many clients of the same
age group and gender; these features can therefore
act as a reasonable proxy for identifying individ-
ual patients, meaning that this result is somewhat
spurious. Also, although identity of therapist is an
important factor in therapeutic outcomes (McCabe
et al., 2013a; McCabe et al., 2013b), we would
like to identify aspects of the communication that
explain why particular therapists are more success-
ful than others, and generalise our findings to new
therapists. AgentID was therefore removed in all
subsequent experiments.
Sentiment and topic As shown in Table 8, us-
ing the proportions of derived topics by transcript
as features does allow us to predict whether a pa-
tient has a high PHQ now score reasonably well;
but sentiment alone performs poorly. Combining
sentiment and topic features, however, allows us
to predict PHQ now with scores of around 0.7 (i.e.
approaching the accuracy achieved using high-
level and AgentID features above). Prediction of
the progress measure is less effective.
Words and n-grams For the symptom mea-
sure, using words and n-grams gives f-scores in
12
Measure PHQ now PHQ start-now
Topic 2 -0.157 0.112
Topic 4 0.124
Topic 5 0.176
Topic 6 -0.117
Topic 7 0.217
Topic 8 0.093 -0.126
Topic 9 -0.077
Topic 10 -0.149
Topic 11 0.140
Topic 12 0.080
Topic 15 0.072
Topic 16 -0.112
Topic 17 -0.211 0.079
Topic 18 0.121
Table 4: Significant correlations of topic features and outcomes
Topic P
H
Q
+
/
-
S
e
n
t
i
m
e
n
t
+
/
-
A
n
g
e
r
+
/
-
keywords
Topic 0 - + good thought re well also mindfulness hw thoughts now vc maybe prob message neg just wk one self bit
Topic 1 people good others self evidence thought enough wrong negative esteem thinking say confidence beliefs person true someone belief situation
Topic 2 - + - session send goals next week last sent read great think questionnaires also homework goal appointment set time cbt able
Topic 3 + thoughts thinking unhelpful helpful look thought behaviours go feelings may think anxiety negative try aware behaviour agenda start self
Topic 4 + - feel think like just good really week now know last session next say felt people thoughts going feeling bit
Topic 5 + - + sleep bed day week work get night mood time diary see better much sleeping activity house routine done activities
Topic 6 - worry worrying worries bit stop train worried problem go example idea control hierarchy driving exposure home happen worst car
Topic 7 + - help feel gp depression thank understand therapy now feeling life today think problems able little message medication sorry make
Topic 8 + check checking ocd thoughts anxiety try something difficult danger brain week sense threat helpful away rituals anxious elephant images
Topic 9 - - think time like much way sure see though know look lot sounds well also right thing sorry sense different
Topic 10 - + thought thoughts anxiety really situation situations one week next example social experience record great emotions thanks notice see make
Topic 11 + + things get time go need like want now just something feel know one work good day going give next
Topic 12 + - + mum relationship husband life family dad parents never love feelings children said years mother much hard way told sister
Topic 13 really week think appointment homework however lets teeth questions great just ready start may dentist set end sure therapy
Topic 14 + - great right sure appointment just thank well tonight loo lol good say really cool get going sorry transcript absolutely
Topic 15 + - things like get bit good sounds feeling also something really great today think idea send week useful anything make
Topic 16 - - anxiety panic breathing get anxious feeling going go attack fear physical control try happen sick symptoms times cope distraction
Topic 17 - + - good work well positive back help really time still last much weeks use thanks session better keep done things
Topic 18 + eating eat food weight day week meal lunch dinner pie energy good mum put table public walk believe ate
Topic 19 + + work job anger angry school stress thanks wife team stuff issues also boss year assertiveness assertive meeting kids times
Table 5: Top 20 words per topic
line with those using only the reduced dimen-
sionality of sentiment and topic. This is surpris-
ing; one might expect finer-grained lexical fea-
tures (which provide more information via a much
higher-dimensional feature space) to increase pre-
dictivity, as per Howes et al. (2013); on the other
hand, it is also promising as it suggests that mean-
ingful generalisations can be drawn out of this data
using NLP techniques.
For the progress measure, on the other hand, n-
gram features perform better than topic/sentiment
(though not as well as on the symptom measures);
this suggests that there are aspects of the com-
munication that can assist in predicting patient
progress, but that they are not captured by the topic
and sentiment information as currently defined.
This suggests that dialogue structure or style may
play a role; one possibility for exploration is to
look at topic and/or sentiment at a finer-grained
level and examine their dynamics (e.g. are posi-
tive sentiments expressed near the start or end of a
consultation linked to better progress)?
5 Discussion
Standard topic, sentiment and emotion modelling
can be usefully applied to online text therapy dia-
logue, although care is needed choosing and ap-
plying a technique suitable for the idiosyncratic
language and spelling. The resulting information
allows us to predict aspects of symptom sever-
ity and patient progress with reasonable degrees
of accuracy (similar to those achieved with face-
to-face data (DeVault et al., 2013; Howes et al.,
2012)), without requiring knowledge of thera-
pist identity. However, some measures of patient
progress are predicted better with fine-grained,
high-dimensional lexical features, suggesting that
insight into style and/or dialogue structure is re-
quired, beyond simple topic or sentiment analysis.
13
Sentiment Anger
Measure mean s.d. min max mean s.d. min max
Topic 0 -0.083 0.189 -0.234 0.206 0.329 0.343 -0.144 0.267
Topic 1 0.087 0.083
Topic 2 0.245 -0.180 0.202 -0.135 -0.175 -0.109 0.076 -0.176
Topic 3 0.113 -0.213 0.159 -0.135 -0.123 0.110 0.095
Topic 4 -0.350 0.324 -0.201 0.099 0.074
Topic 5 -0.079 0.119
Topic 6 0.068
Topic 7 -0.083 -0.167 -0.109 0.110
Topic 8 0.078 0.123 -0.104
Topic 9 -0.072 -0.071 -0.075
Topic 10 0.100 -0.167 0.133 -0.073
Topic 11 0.086 0.161 0.132 0.121
Topic 12 -0.338 0.182 -0.156 0.233 0.092 -0.087 0.146
Topic 13 -0.111 -0.112 -0.243 0.077 -0.089
Topic 14 0.112 0.156 -0.183 0.186 -0.087 0.225 -0.116 0.204
Topic 15 0.140 -0.179 0.072 -0.064 -0.161 -0.156 -0.070
Topic 16 -0.090 -0.089 0.073 -0.115
Topic 17 0.385 -0.156 0.267 -0.116 -0.408 -0.139 0.078 -0.288
Topic 18 -0.071
Topic 19 0.177 0.209
Table 6: Significant correlations between topic and sentiment features
Baseline Agent High-level (H/L)
Measure Proportion OneR (Worse) inc Agent J48 exc Agent J48
PHQ Now 40.5% 0.584 (0.360) 0.738 (0.637) 0.640 (0.561)
PHQ Start-now 38.1% 0.639 (0.446) 0.707 (0.611) 0.545 (0.299)
Table 7: Weighted average f-scores of outcomes using different high-level feature groups (figures in
brackets are the f-scores for the class of interest; i.e. PHQ Now ? patients with higher/more symptomatic
PHQ; PHQ Start-now ? patients showing no change or a worsening in PHQ)
Sentiment Topic Sentiment + Topic
inc H/L exc H/L inc H/L exc H/L inc H/L exc H/L
J48
PHQ Now 0.625 (0.528) 0.610 (0.437) 0.642 (0.548) 0.650 (0.512) 0.641 (0.544) 0.638 (0.522)
PHQ Start-now 0.630 (0.412) 0.508 (0.094) 0.628 (0.479) 0.477 (0.024) 0.619 (0.474) 0.526 (0.147)
Logistic PHQ Now 0.626 (0.497) 0.610 (0.432) 0.689 (0.585) 0.658 (0.537) 0.707 (0.613) 0.674 (0.559)
Regr. PHQ Start-now 0.532 (0.218) 0.605 (0.025) 0.593 (0.369) 0.569 (0.283) 0.591 (0.377) 0.557 (0.295)
Table 8: Weighted average f-scores using sentiment/topic features (figures in brackets are the f-scores
for the class of interest)
Words N-grams
Measure inc H/L exc H/L inc H/L exc H/L
PHQ NOW 0.655 (0.575) 0.676 (0.614) 0.696 (0.615) 0.686 (0.616)
PHQ Start-now 0.616 (0.528) 0.623 (0.506) 0.626 (0.459) 0.645 (0.532)
Table 9: Weighted average f-scores using raw lexical features (words/ngrams) using LibLINEAR (figures
in brackets are the f-scores for the class of interest)
14
References
D. Angus, B. Watson, A. Smith, C. Gallois, and
J. Wiles. 2012. Visualising conversation structure
across time: Insights into effective doctor-patient
consultations. PLoS ONE, 7(6):1?12.
A. Beattie, A. Shaw, S. Kaur, and D. Kessler. 2009.
Primary-care patients? expectations and experiences
of online cognitive behavioural therapy for depres-
sion: a qualitative study. Health Expectations,
12(1):45?59.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a
library for Support Vector Machines. Soft-
ware available at http://www.csie.ntu.
edu.tw/
?
cjlin/libsvm.
Z.-J. Chuang and C.-H. Wu. 2004. Multi-modal emo-
tion recognition from speech and text. Computa-
tional Linguistics and Chinese Language Process-
ing, 9(2):45?62, August.
J. Cretchley, C. Gallois, H. Chenery, and A. Smith.
2010. Conversations between carers and peo-
ple with schizophrenia: a qualitative analysis us-
ing Leximancer. Qualitative Health Research,
20(12):1611?1628.
M. De Choudhury, M. Gamon, and S. Counts. 2012.
Happy, nervous or surprised? Classification of hu-
man affective states in social media. In Proceed-
ings of the Sixth International Conference on We-
blogs and Social Media (ICWSM).
D. DeVault, K. Georgila, R. Artstein, F. Morbini,
D. Traum, S. Scherer, A. S. Rizzo, and L.-P.
Morency. 2013. Verbal indicators of psychologi-
cal distress in interactive dialogue with a virtual hu-
man. In Proceedings of the SIGDIAL 2013 Confer-
ence, pages 193?202.
J. Eisenstein and R. Barzilay. 2008. Bayesian unsu-
pervised topic segmentation. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 334?343.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA data
mining software: An update. SIGDKDD Explo-
rations, 11(1):10?18.
T. Hanley and D. Reynolds. 2009. Counselling psy-
chology and the internet: A review of the quanti-
tative research into online outcomes and alliances
within text-based therapy. Counselling Psychology
Review, 24(2):4?13.
R. Hirschfeld, R. L. Spitzer, and M. R.G. 1974. Com-
puter diagnosis in psychiatry: A Bayes approach.
Journal of Nervous and Mental Disease, 158:399?
407.
C. Howes, M. Purver, R. McCabe, P. G. T. Healey, and
M. Lavelle. 2012. Helping the medicine go down:
Repair and adherence in patient-clinician dialogues.
In Proceedings of the 16th Workshop on the Seman-
tics and Pragmatics of Dialogue (SemDial 2012).
C. Howes, M. Purver, and R. McCabe. 2013. Using
conversation topics for predicting therapy outcomes
in schizophrenia. Biomedical Informatics Insights,
6(Suppl. 1):39?50, July.
P. John, M. Lavelle, S. Mehnaz, and R. McCabe. un-
der review. What do psychiatrists and patients with
schizophrenia talk about and does it matter? Psy-
chiatric Bulletin.
D. Kessler, G. Lewis, S. Kaur, N. Wiles, M. King,
S. Weich, D. Sharp, R. Araya, S. Hollinghurst, and
T. Peters. 2009. Therapist-delivered internet psy-
chotherapy for depression: a randomised controlled
trial in primary care. Lancet, 374:628?634.
K. Kroenke and R. L. Spitzer. 2002. The PHQ-9:
a new depression diagnostic and severity measure.
Psychiatr Ann, 32(9):1?7.
R. e. a. Layard. 2012. How mental illness loses out
in the NHS. Technical report, Mental Health Policy
Group, Centre for Economic Performance, London
School of Economics, June.
M. Liakata, J.-H. Kim, S. Saha, J. Hastings, and
D. Rebholz-Schuhmann. 2012. Three hybrid classi-
fiers for the detection of emotions in suicide notes.
Biomedical Informatics Insights, 5(1):175?184.
A. Martin, W. Rief, A. Klaiberg, and E. Braehler.
2006. Validity of the brief patient health question-
naire mood scale (PHQ-9) in the general population.
General hospital psychiatry, 28(1):71?77.
R. McCabe, P. G. T. Healey, S. Priebe, M. Lavelle,
D. Dodwell, R. Laugharne, A. Snell, and S. Brem-
ner. 2013a. Shared understanding in psychiatrist-
patient communication: Association with treatment
adherence in schizophrenia. Patient Education and
Counselling.
R. McCabe, H. Khanom, P. Bailey, and S. Priebe.
2013b. Shared decision-making in ongoing outpa-
tient psychiatric treatment. Patient education and
counseling, 91(3):326?328.
A. K. McCallum. 2002. MALLET: A machine learn-
ing for language toolkit. http://mallet.cs.umass.edu.
B. O?Connor, R. Balasubramanyan, B. R. Routledge,
and N. A. Smith. 2010. From tweets to polls: Link-
ing text sentiment to public opinion time series. In
Proceedings of the 4th AAAI International Confer-
ence on Weblogs and Social Media, pages 122?129.
L. Ong, J. De Haes, A. Hoos, and F. Lammes. 1995.
Doctor-patient communication: a review of the liter-
ature. Social science & medicine, 40(7):903?918.
15
J. Overall and L. Hollister. 1964. Computer proce-
dures for psychiatric classification. Journal of the
American Medical Association, 187:583?585.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1?2):1?135.
M. Paul and M. Drezde. 2011. You are what you
tweet: Analyzing twitter for public health. In Pro-
ceedings of the 5th International AAAI Conference
on Weblogs and Social Media (ICWSM).
M. Paul, B. Wallace, and M. Dredze. 2013. What
affects patient (dis)satisfaction? Analyzing online
doctor ratings with a joint topic-sentiment model.
In Proceedings of the AAAI Workshop on Expand-
ing the Boundaries of Health Informatics Using AI
(HIAI).
J. W. Pennebaker, R. J. Booth, and M. E. Francis.
2007. Linguistic inquiry and word count (LIWC):
A computerized text analysis program. Austin, TX:
LIWC.net.
R. H. Perlis. 2013. A clinical risk stratification tool
for predicting treatment resistance in major depres-
sive disorder. Biological Psychiatry, 74(1):7?14.
Sources of Treatment Resistance in Depression: In-
flammation and Functional Connectivity.
M. Purver and S. Battersby. 2012. Experimenting
with distant supervision for emotion classification.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), pages 482?491.
M. Purver, K. K?ording, T. Griffiths, and J. Tenenbaum.
2006. Unsupervised topic modelling for multi-party
spoken discourse. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages
17?24.
D. Quercia, J. Ellis, L. Capra, and J. Crowcroft. 2011.
In the Mood for Being Influential on Twitter. In
Proceedings of the 3
rd
IEEE Conference on Social
Computing (SocialCom).
D. Quercia, J. Crowcroft, J. Ellis, and L. Capra. 2012.
Tracking ?gross community happiness? from tweets.
In Proceedings of the ACMConference on Computer
Supported Cooperative Work (CSCW), pages 965?
968.
G. Salton and M. McGill. 1986. Introduction to mod-
ern information retrieval. McGraw-Hill, Inc.
Y.-S. Seol, D.-J. Kim, and H.-W. Kim. 2008. Emotion
recognition from text using knowledge based ANN.
In Proceedings of ITC-CSCC.
R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Man-
ning, A. Ng, and C. Potts. 2013. Recursive deep
models for semantic compositionality over a senti-
ment treebank. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1631?1642.
R. L. Spitzer, K. Kroenke, J. B. Williams, and B. L?owe.
2006. A brief measure for assessing generalized
anxiety disorder: the GAD-7. Archives of internal
medicine, 166(10):1092?1097.
M. Steyvers and T. Griffiths. 2007. Probabilistic
topic models. Handbook of latent semantic analy-
sis, 427(7):424?440.
B. C. Wallace, T. A. Trikalinos, M. B. Laws, I. B. Wil-
son, and E. Charniak. 2013. A generative joint, ad-
ditive, sequential model of topics and speech acts
in patient-doctor communication. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1765?1775.
H. M. Wallach, D. M. Mimno, and A. McCallum.
2009. Rethinking LDA: Why priors matter. In
NIPS, volume 22, pages 1973?1981.
Z. Yu, S. Scherer, D. Devault, J. Gratch, G. Stratou, L.-
P. Morency, and J. Cassell. 2013. Multimodal pre-
diction of psychological disorder: Learning nonver-
bal commonality in adjacency pairs. In Proceedings
of the SemDial 2013 Workshop, pages 193?202.
16
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 155?160,
Dublin, Ireland, August 23 2014.
A Simple Baseline for Discriminating Similar Languages
Matthew Purver
Cognitive Science Research Group
School of Electronic Engineering and Computer Science
Queen Mary University of London
m.purver@qmul.ac.uk
Abstract
This paper describes an approach to discriminating similar languages using word- and character-
based features, submitted as the Queen Mary University of London entry to the Discriminating
Similar Languages shared task. Our motivation was to investigate how well a simple, data-
driven, linguistically naive method could perform, in order to provide a baseline by which more
linguistically complex or knowledge-rich approaches can be judged. Using a standard supervised
classifier with word and character n-grams as features, we achieved over 90% accuracy in the test;
on fixing simple file handling and feature extraction bugs, this improved to over 95%, comparable
to the best submitted systems. Similar accuracy is achieved using only word unigram features.
1 Introduction and Approach
Most approaches to written language detection use character or byte ngram features to capture charac-
teristic orthographic sequences ? see e.g. (Cavnar and Trenkle, 1994) to (Lui et al., 2014) and many
in between, as well as implementations such as the widely used open-source Chromium Compact Lan-
guage Detector.
1
Some approaches determine these characteristic features from linguistic properties of
the language (e.g. (Lins and Gonc?alves, 2004)), while some determine them from data (e.g. (Cavnar and
Trenkle, 1994)). A wide range of approaches to modelling and classification can be used, ranging from
simple Na??ve Bayes models (Grefenstette, 1995) to more complex generative mixture models for tasks
with multilingual texts (Lui et al., 2014). Our interest in this task was to see how well a naive, entirely
data-driven baseline method would perform in the task of discriminating similar languages (DSL) as
posed by the DSL Shared Task (Zampieri et al., 2014).
Our approach was intended to capture two basic insights into variation between similar languages.
First, that closely related languages often use quite different words for the same concept: e.g. US English
elevator vs UK English lift; Croatian tjedan vs Serbian nedelja vs Bosnian sedmica. Second, that there
are often regular variations in the details of a word?s orthographic or phonological form: e.g. US English
color, favorite vs UK English colour, favourite; Croatian/Bosnian rijeka, htjeti vs Serbian reka, hteti.
The former insight can be approximated by use of word ngrams; the latter via character ngrams. While
such ngram features cannot capture similarity of meaning or non-sequential dependencies, they may do
a reasonable job of capturing similarity of sentential context (often taken to be an indicator of lexical
meaning) and sequential phenomena.
Together with simplicity in method, speed and simplicity of implementation was also an objective.
We therefore used only the training and development data available in the shared task ? see (Tan et
al., 2014) ? together with a standard freely available discriminative SVM classifier and common text
pre-processing methods.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
https://code.google.com/p/chromium-compact-language-detector/
155
2 Background and Related Work
Shared Task The Discriminating Similar Languages (DSL) Shared Task was established as part of the
2014 VarDial workshop.
2
The task provided datasets for 13 different languages in 6 groups of closely
related languages, shown in Table 1. Data was divided into training, development and test sets: for each
language, 18,000 labelled training instances and 2,000 labelled development instances were provided;
an unlabelled and previously unseen test set containing 1,000 instances per language was then used for
evaluation by the organisers ? see (Tan et al., 2014) for full details of the dataset, and (Zampieri et al.,
2014) for the task and evaluation.
Group A Bosnian (bs), Croatian (hr), Serbian (sr)
Group B Indonesian (id), Malaysian (my)
Group C Czech (cz), Slovakian (sk)
Group D Brazilian Portuguese (pt-BR), European Portuguese (pt-PT)
Group E Peninsular Spain (es-ES), Argentine Spanish (es-AR)
Group F American English (en-US), British English (en-GB)
Table 1: Languages and groups in the DSL Shared Task.
However, problems were discovered in labelling the languages in Group F, and an evaluation for
groups A-E was therefore performed separately; we discuss only this latter task and evaluation here.
Related Work Classification approaches based on character or byte sequences have shown success in
providing general models of language identification; see e.g. (Lui et al., 2014). In more specific exper-
iments into discriminating between pairs or triples of similar languages, many researchers have found
that word-based features can aid accuracy; but classification method and feature choice vary widely.
When distinguishing Malay from Indonesian, Ranaivo-Malanc?on (2006) combines character n-gram
frequencies with heuristics based on number format and lists of words unique to each language. Ljube?si?c
et al. (2007) use a character trigram-based probabilistic language model, again in combination with a
unique-word list, to distinguish between Croatian, Serbian and Slovenian, achieving high accuracies
(over 99%); Tiedemann and Ljube?si?c (2012) extend this task to include Bosnian and improve perfor-
mance by using a Naive Bayes classifier with unigram word features to achieve accuracies over 95%.
Some research suggests that word-based features can even outperform character-based approaches.
For Brazilian vs European Portuguese, Zampieri and Gebre (2012) found that word unigrams gave very
similar performance to character n-gram features when used in a probabilistic language model; Zampieri
et al. (2013) then showed that word 1- or 2-grams outperformed character ngrams of any length from 1
to 5 (and that both outperformed features based purely on syntactic part-of-speech), when distinguishing
different varieties of Spanish. Lui and Cook (2013) likewise found that bag-of-words features generally
outperformed features based on syntax or character sequences when distinguishing between Canadian,
Australian and UK English. However, Zampieri (2013) found that in some cases (e.g French) character
n-grams might give benefits above simple word unigram features.
In this work, then our interest was to investigate whether these simple, knowledge-poor approaches can
generalise and apply across several language groups, using a single integrated approach to classification
incorporating character- and word-based features within one model; and to compare the utility of word
and character features.
3 Methods
Processing and training We tokenise the training texts from (Tan et al., 2014) based on transitions
between alphanumeric and non-alphanumeric characters, and remove URLs, email addresses, Twitter
usernames and emoticons. We then form feature vectors with entries for all observed word (token) uni-
grams, and character ngrams of lengths 1-3; feature values are counts (raw term frequencies) normalised
2
http://corporavm.uni-koeln.de/vardial/
156
by the text length in tokens or characters respectively. We then train a single multi-class linear-kernel sup-
port vector machine using LIBLINEAR (Fan et al., 2008) with the language identifiers (en-US, en-UK,
hr, bs, sr etc.) as labels. SVMs are well-suited to high-dimensional feature spaces; and SVMs with
ngrams of these lengths have shown good performance in other language identification work (Baldwin
and Lui, 2010). Features were given numerical indices corresponding to the unique ngram type (i.e.
we used a feature dictionary with no hashing). No feature selection or frequency cutoff was used. No
part-of-speech tagging or grammatical analysis was attempted; no external language resources or tools
were used other than described above.
Development and testing Development and test set texts were tokenised and featurised using the same
process; feature indices were taken from the dictionary generated during training, with unseen ngram
types ignored. LIBLINEAR was then used to predict the most likely language identifier label.
By re-using a standard set of in-house utilities for tokenisation and featurisation,
3
the code for train-
ing and parameter testing (see below) was written and tested for functionality in around 30 minutes.
Pre-processing, featurisation and vectorisation then took around 25 minutes over the training and devel-
opment sets, and writing out LIBLINEAR format files around 15 minutes, running on a MacBook Air
with 1.7GHz Intel Core i7 processor and 8Gb memory. Classifier training then takes around 1 minute,
depending on exact parameter settings. Testing on the development set or test set takes around 1 second
per language group.
4 Experiments and Results
Development We used 10-fold cross-validation on the training set, and testing on the development set,
to choose a suitable SVM cost parameter (tradeoff between error and maximum margin criterion). We
cross-validated over the training set to check overall multi-class accuracy while varying the cost over
a range from 1 to 100 ? see Table 2. We then trained on the full training set, and tested accuracy on
the development across each language group ? see Table 3. Given reported problems with the group F
dataset (en-UK/en-US), we focussed on groups A-E.
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Overall A-E 91.36 93.24 94.44 94.83 94.86 94.85
Table 2: 10-fold cross-validation accuracy on training set with varying SVM cost.
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Group A bs/hr/sr 88.93 91.96 93.20 93.56 93.46 93.26
Group B id/my 97.11 97.72 98.14 98.28 98.31 98.42
Group C cz/sk 99.90 99.92 99.95 99.97 99.97 99.95
Group D pt-BR/pt-PT 89.83 91.99 93.52 94.12 94.00 94.05
Group E es-AR/es-ES 82.72 85.82 87.78 89.26 89.24 89.01
Overall A-E 91.34 93.28 94.34 94.86 94.81 94.73
Table 3: Accuracy on development set with varying SVM cost.
A cost parameter value of 30 to 50 appeared to perform best across all groups, so these two values
were used for separate runs in the shared task test. Note though that performance appears relatively
stable over a cost range of 10-100 (perhaps 30-100 for group E). The classifier performs worst for group
E (es-AR/es-ES), with only this language group failing to reach 90% accuracy. Group C (cz/sk)
performs best with almost perfect accuracy; this may be due to the existence of characters which are
highly discriminative on their own (e.g. ?o is used in Slovak, but not in Czech, ?u in Czech but not in
Slovak ? although a few dozen examples appear labelled as Slovak in this dataset).
3
Tools with equivalent functionality are widely available e.g. as part of NLTK, http://www.nltk.org/.
157
Test ? Shared Task A blind run on the test set was then performed and submitted as part of the shared
task. Overall accuracy was 90.61% (macro-averaged F-score 92.51%), placing us 5
th
amongst the task
entrants; results per group are shown in Table 4.
Cost: 30.0
Group A bs/hr/sr 87.87
Group B id/my 93.50
Group C cz/sk 96.20
Group D pt-BR/pt-PT 90.45
Group E es-AR/es-ES 86.45
Overall A-E 90.61
Table 4: Accuracy on test set as submitted for the shared task.
Corrected Test However, after submission of the test run, a bug was discovered in the code which
paired test sentences with predictions; predictions had been omitted for about 500 of the 11,000 test
texts (i.e. 4.5% of the data) due to an unfortunate combination of unpaired double-quote characters in the
test data with the use of a standard CSV-file handling library. After release of the gold-standard test set
labels, the classifier was therefore re-run, with resulting accuracies as shown in Table 5.
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Group A bs/hr/sr 87.97 90.70 92.40 92.90 93.00 93.17
Group B id/my 98.30 98.85 99.05 99.15 99.15 99.15
Group C cz/sk 99.90 99.95 99.95 99.95 99.95 99.95
Group D pt-BR/pt-PT 88.50 91.45 93.25 93.95 93.90 93.80
Group E es-AR/es-ES 83.65 86.70 88.45 89.35 89.45 89.45
Overall A-E 91.33 93.27 94.42 94.86 94.90 94.93
Table 5: True accuracy on test set after restoring omitted predictions.
Accuracies are very similar to those on the development set. Overall accuracy at the chosen cost
parameter range of 30-50 is 94.9%, slightly worse than the 1
st
and slightly better than the 2
nd
-placed
systems in the official test (95.71% and 94.68% respectively). Increasing the cost parameter setting
could perhaps give a very slight boost to performance. Again, group E performs worst, and Group C
best; per-group and overall accuracies are very similar to those achieved on the development set.
A second unintended feature of the feature generation code was subsequently discovered: character
n-grams were being extracted spanning word boundaries (including the whitespace characters separating
words). These were removed, leaving only the intended character n-grams within words, and accuracies
are shown in Table 6. Again, overall performance increases slightly, now to over 95%, although Group
A accuracy shows a slight decrease (0.1%). Group E accuracy improves by over 1% and is now over
90% at the chosen cost parameter.
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Group A bs/hr/sr 89.83 92.13 92.73 92.77 92.63 92.67
Group B id/my 98.55 99.15 99.25 99.35 99.35 99.35
Group C cz/sk 99.95 99.95 99.95 99.95 99.95 99.95
Group D pt-BR/pt-PT 91.00 93.25 94.80 95.15 95.10 95.15
Group E es-AR/es-ES 86.10 88.35 90.30 90.85 90.95 91.15
Overall A-E 92.79 94.35 95.16 95.35 95.33 95.37
Table 6: Accuracy on test set after removing spurious character n-grams.
158
Effect of features To investigate the utility of our chosen feature sets and their insights into lexical and
orthographic distinctions, we then compared the overall performance to that achieved when removing
certain features. Table 7 shows the accuracies achieved without word unigram features (i.e. using only
character ngrams of lengths 1-3); Table 8 shows accuracies without character ngram features (i.e. using
only word unigrams).
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Group A bs/hr/sr 81.43 85.40 88.03 89.77 90.10 90.70
Group B id/my 91.80 94.15 96.05 96.95 97.20 97.50
Group C cz/sk 99.90 99.95 99.95 99.95 99.95 99.95
Group D pt-BR/pt-PT 82.75 86.70 89.15 90.80 91.50 91.80
Group E es-AR/es-ES 77.80 80.95 83.50 85.20 85.10 85.65
Overall A-E 86.25 89.06 91.04 92.28 92.53 92.90
Table 7: Accuracy on test set without word unigrams.
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Group A bs/hr/sr 86.83 89.63 91.73 92.20 92.43 92.07
Group B id/my 97.70 98.65 98.85 99.10 99.15 99.10
Group C cz/sk 99.70 99.70 99.80 99.90 99.90 99.90
Group D pt-BR/pt-PT 86.65 90.55 92.55 93.25 93.40 93.20
Group E es-AR/es-ES 85.10 87.10 88.35 89.35 89.35 89.45
Overall A-E 90.80 92.81 94.02 94.53 94.63 94.50
Table 8: Accuracy on test set using only word unigrams.
Neither system performs as well as the classifier with the full, combined feature set (Table 6). How-
ever, the system with only word unigrams does almost as well as the full system, losing a maximum of
2% performance at the extreme range of cost parameter values, and less than 1% at the chosen optimal
values. The system with only character ngrams, however, loses noticably more performance, with around
3% lost even at optimal cost values.
5 Conclusions
A simple approach using ngram features and discriminative classification achieves competitive results
on the task of discriminating similar languages, and the availability of existing language processing and
machine learning tools makes setting up and training such a system easy and extremely quick. Simple
word unigram features perform well on their own, although combination with character n-gram features
improves performance; the choice of classifier parameters is important but seems to generalise well
across different languages. Future extensions of this work could include features which take into account
longer word or character sequences and/or more flexible characterisations and combinations of those
features, for example via the convolutional neural network approach of (Kalchbrenner et al., 2014).
References
Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 229?237, Los Angeles, California, June. Association for Computational
Linguistics.
William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Proceedings of the 3rd
Symposium on Document Analysis and Information Retrieval, pages 161?175, Las Vegas.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learning Research, 9:1871?1874.
159
Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of Analisi Statistica
dei Dati Testuali (JADT), pages 263?268, Rome.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling
sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pages 655?665, Baltimore, Maryland, June. Association for Computational Linguistics.
Rafael Dueire Lins and Paulo Gonc?alves. 2004. Automatic language identification of written texts. In Proceedings
of the 2004 ACM Symposium on Applied Computing, SAC ?04, pages 1128?1133, New York, NY, USA. ACM.
Nikola Ljube?si?c, Nives Mikeli?c, and Damir Boras. 2007. Language identification: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces (ITI),
pages 541?546.
Marco Lui and Paul Cook. 2013. Classifying english documents by national dialect. In Proceedings of the
Australasian Language Technology Association Workshop 2013 (ALTA 2013), pages 5?15, Brisbane, Australia.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multi-
lingual documents. Transactions of the Association for Computational Linguistics, 2:27?40.
Bali Ranaivo-Malanc?on. 2006. Automatic identification of close languages - case study: Malay and Indonesian.
ECTI Transactions on Computer and Information Technology, 2(2):126?134, November.
Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann. 2014. Merging comparable data sources for
the discrimination of similar languages: The DSL Corpus Collection. In Proceedings of the 7th Workshop on
Building and Using Comparable Corpora (BUCC).
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
Proceedings of COLING 2012, pages 2619?2634, Mumbai, India, December.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS2012, pages 233?237, Vienna, September.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580?587, Sables
d?Olonne, France.
Marcos Zampieri, Liling Tan, Nikola Ljube?si?c, and J?org Tiedemann. 2014. A report on the DSL Shared Task
2014. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects
(VarDial).
Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they? In
Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics (CINTI),
pages 37?41, Budapest, November.
160
