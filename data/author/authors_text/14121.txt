Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 181?184,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Evolving new lexical association measures using genetic programming
Jan ?Snajder Bojana Dalbelo Bas?ic? Sas?a Petrovic? Ivan Sikiric?
Faculty of Electrical Engineering and Computing, University of Zagreb
Unska 3, Zagreb, Croatia
{jan.snajder, bojana.dalbelo, sasa.petrovic, ivan.sikiric}@fer.hr
Abstract
Automatic extraction of collocations from
large corpora has been the focus of many re-
search efforts. Most approaches concentrate
on improving and combining known lexical
association measures. In this paper, we de-
scribe a genetic programming approach for
evolving new association measures, which is
not limited to any specific language, corpus,
or type of collocation. Our preliminary experi-
mental results show that the evolved measures
outperform three known association measures.
1 Introduction
A collocation is an expression consisting of two or
more words that correspond to some conventional
way of saying things (Manning and Schu?tze, 1999).
Related to the term collocation is the term n-gram,
which is used to denote any sequence of n words.
There are many possible applications of colloca-
tions: automatic language generation, word sense
disambiguation, improving text categorization, in-
formation retrieval, etc. As different applications
require different types of collocations that are of-
ten not found in dictionaries, automatic extraction
of collocations from large textual corpora has been
the focus of much research in the last decade; see,
for example, (Pecina and Schlesinger, 2006; Evert
and Krenn, 2005).
Automatic extraction of collocations is usually
performed by employing lexical association mea-
sures (AMs) to indicate how strongly the words
comprising an n-gram are associated. However, the
use of lexical AMs for the purpose of collocation
extraction has reached a plateau; recent research
in this field has focused on combining the existing
AMs in the hope of improving the results (Pecina
and Schlesinger, 2006). In this paper, we propose
an approach for deriving new AMs for collocation
extraction based on genetic programming. A simi-
lar approach has been usefully applied in text min-
ing (Atkinson-Abutridy et al, 2004) as well as in
information retrieval (Gordon et al, 2006).
Genetic programming is an evolutionary compu-
tational technique designed to mimic the process of
natural evolution for the purpose of solving complex
optimization problems by stochastically searching
through the whole space of possible solutions (Koza,
1992). The search begins from an arbitrary seed
of possible solutions (the initial population), which
are then improved (evolved) through many iterations
by employing the operations of selection, crossover,
and mutation. The process is repeated until a termi-
nation criterion is met, which is generally defined by
the goodness of the best solution or the expiration of
a time limit.
2 Genetic programming of AMs
2.1 AM representation
In genetic programming, possible solutions (in our
case lexical AMs) are mathematical expressions rep-
resented by a tree structure (Koza, 1992). The leaves
of the tree can be constants, or statistical or linguistic
information about an n-gram. A constant can be any
real number in an arbitrarily chosen interval; our ex-
periments have shown that variation of this interval
does not affect the performance. One special con-
stant that we use is the number of words in the cor-
pus. The statistical information about an n-gram can
be the frequency of any part of the n-gram. For ex-
181
ample, for a trigram abc the statistical information
can be the frequency f(abc) of the whole trigram,
frequencies f(ab) and f(bc) of the digrams, and
the frequencies of individual words f(a), f(b), and
f(c). The linguistic information about an n-gram is
the part-of-speech (POS) of any one of its words.
Inner nodes in the tree are operators. The binary
operators are addition, subtraction, multiplication,
and division. We also use one unary operator, the
natural logarithm, and one ternary operator, the IF-
THEN-ELSE operator. The IF-THEN-ELSE node
has three descendant nodes: the left descendant is
the condition in the form ?i-th word of the n-gram
has a POS tag T,? and the other two descendants are
operators or constants. If the condition is true, then
the subexpression corresponding to the middle de-
scendant is evaluated, otherwise the subexpression
corresponding to the right descendant is evaluated.
The postfix expression of an AM can be obtained
by traversing its tree representation in postorder.
Figure 1 shows the representation of the Dice co-
efficient using our representation.
2.2 Genetic operators
The crossover operator combines two parent solu-
tions into a new solution. We defined the crossover
operator as follows: from each of the two parents,
one node is chosen randomly, excluding any nodes
that represent the condition of the IF-THEN-ELSE
operator. A new solution is obtained by replacing
the subtree of the chosen node of the first parent with
the subtree of the chosen node of the second parent.
This method of defining the crossover operator is the
same as the one described in (Gordon et al, 2006).
The mutation operator introduces new ?genetic
material? into a population by randomly changing
a solution. In our case, the mutation operator can
do one of two things: either remove a randomly se-
lected inner node (with probability of 25%), or insert
an inner node at a random position in the tree (with
probability of 75%). If a node is being removed
from the tree, one of its descendants (randomly cho-
sen) takes its place. An exception to this rule is the
IF-THEN-ELSE operator, which cannot be replaced
by its condition node. If a node is being inserted,
a randomly created operator node replaces an exist-
ing node that then becomes a descendant of the new
node. If the inserted node is not a unary operator,
the required number of random leaves is created.
The selection operator is used to copy the best so-
lutions into the next iteration. The goodness of the
solution is determined by the fitness function, which
assigns to each solution a number indicating how
good that particular solution actually is. We mea-
sure the goodness of an AM in terms of its F1 score,
obtained from the precision and recall computed on
a random sample consisting of 100 positive n-grams
(those considered collocations) and 100 negative n-
grams (non-collocations). These n-grams are ranked
according to the AM value assigned to them, after
which we compute the precision and recall by con-
sidering first n best-ranked n-grams as positives and
the rest as negatives, repeating this for each n be-
tween 1 and 200. The best F1 score is then taken as
the AM?s goodness.
Using the previous definition of the fitness func-
tion, preliminary experiments showed that solutions
soon become very complex in terms of number of
nodes in the tree (namely, on the order of tens
of thousands). This is a problem both in terms
of space and time efficiency; allowing unlimited
growth of the tree quickly consumes all computa-
tional resources. Also, it is questionable whether
the performance benefits from the increased size of
the solution. Thus, we modified the fitness func-
tion to also take into account the size of the tree
(that is, the less nodes a tree has, the better). Fa-
voring shorter solutions at the expense of some loss
in performance is known as parsimony, and it has
already been successfully used in genetic program-
ming (Koza, 1992). Therefore, the final formula for
the fitness function we used incorporates the parsi-
mony factor and is given by
fitness(j) = F1(j) + ?
Lmax ? L(j)
Lmax
, (1)
where F1(j) is the F1 score (ranging from 0 to 1) of
the solution j, ? is the parsimony factor, Lmax is the
maximal size (measured in number of nodes), and
L(j) is the size of solution j. By varying ? we can
control how much loss of performance we will tol-
erate in order to get smaller, more elegant solutions.
Genetic programming algorithms usually iterate
until a termination criterion is met. In our case, the
algorithm terminates when a certain number, k, of
iterations has passed without an improvement in the
182
Dice(a, b, c) = f(abc)f(a)+f(b)+f(c)
Figure 1: Dice coefficient for digrams represented by tree
results. To prevent the overfitting problem, we mea-
sure this improvement on another sample (valida-
tion sample) that also consists of 100 collocations
and 100 non-collocations.
3 Preliminary results
3.1 Experimental setting
We use the previously described genetic program-
ming approach to evolve AMs for extracting collo-
cations consisting of three words from a corpus of
7008 Croatian legislative documents. Prior to this,
words from the corpus were lemmatized and POS
tagged. Conjunctions, propositions, pronouns, in-
terjections, and particles were treated as stop-words
and tagged with a POS tag X . N-grams starting or
ending with a stopword, or containing a verb, were
filtered out. For evaluation purposes we had a hu-
man expert annotate 200 collocations and 200 non-
collocations, divided into the evaluation and valida-
tion sample. We considered an n-gram to be a collo-
cation if it is a compound noun, terminological ex-
pression, or a proper name. Note that we could have
adopted any other definition of a collocation, since
this definition is implicit in the samples provided.
In our experiments, we varied a number of ge-
netic programming parameters. The size of the ini-
tial population varied between 50 and 50 thousand
randomly generated solutions. To examine the ef-
fects of including some known AMs on the perfor-
mance, the following AMs had a 50% chance of
being included in the initial population: pointwise
mutual information (Church and Hanks, 1990), the
Dice coefficient, and the heuristic measure defined
in (Petrovic? et al, 2006):
H(a, b, c) =
?
?
?
2 log f(abc)f(a)f(c) if POS (b) = X,
log f(abc)f(a)f(b)f(c) otherwise.
For the selection operator we used the well-known
three-tournament selection. The probability of mu-
tation was chosen from the interval [0.0001, 0.3],
and the parsimony factor ? from the interval
[0, 0.05], thereby allowing a maximum of 5% loss
of F1 in favor of smaller solutions. The maximal
size of the tree in nodes was chosen from the inter-
val [20, 1000]. After the F1 score for the validation
sample began dropping, the algorithm would con-
tinue for another k iterations before stopping. The
parameter k was chosen from the interval [104, 107].
The experiments were run with 800 different random
combinations of the aforementioned parameters.
3.2 Results
Around 20% of the evolved measures (that is, the so-
lutions that remained after the algorithm terminated)
achieved F1 scores of over 80% on both the evalu-
ation and validation samples. This proportion was
13% in the case when the initial population did not
include any known AMs, and 23% in the case when
it did, thus indicating that including known AMs in
the initial population is beneficial. The overall best
solution had 205 nodes and achieved an F1 score of
88.4%. In search of more elegant AMs, we singled
out solutions that had less than 30 nodes. Among
these, a solution that consisted of 13 nodes achieved
the highest F1. This measure is given by
M13(a, b, c) =
?
?
?
?0.423f(a)f(c)f2(abc) if POS (b) = X,
1 ? f(b)f(abc) otherwise.
The association measure M13 is particularly inter-
esting because it takes into account whether the
middle word in a trigram is a stopword (denoted
by the POS tag X). This supports the claim laid
out in (Petrovic? et al, 2006) that the trigrams con-
taining stopwords (e.g., cure for cancer) should be
treated differently, in that the frequency of the stop-
word should be ignored. It is important to note that
the aforementioned measure H was not included in
the initial population from which M13 evolved. It
is also worthwhile noting that in such populations,
out of 100 best evolved measures, all but four of
them featured a condition identical to that of M13
(POS (b) = X). In other words, the majority of
the measures evolved this condition completely in-
dependently, without H being included in the initial
population.
183
1 2 3 4 5 6 7 8 9 10
0
10
20
30
40
50
60
70
80
90
100
Number of n?grams (? 105)
F 1
 
sc
o
re
 
 
Dice
PMI
H
M13
M205
Figure 2: Comparison of association measures on a cor-
pus of 7008 Croatian documents
Figure 2 shows the comparison of AMs in terms
of their F1 score obtained on the corpus of 7008
documents. The x axis shows the number of n
best ranked n-grams that are considered positives
(we show only the range of n in which all the AMs
achieve their maximum F1; all measures tend to per-
form similarly with increasing n). The maximum
F1 score is achieved if we take 5 ? 105 n-grams
ranked best by the M205 measure. From Fig. 2 we
can see that the evolved AMs M13 and M205 outper-
formed the other three considered AMs. For exam-
ple, collocations kosilica za travu (lawn mower) and
digitalna obrada podataka (digital data processing)
were ranked at the 22th and 34th percentile accord-
ing to Dice, whereas they were ranked at the 97th
and 87th percentile according to M13.
4 Conclusion
In this paper we described a genetic programming
approach for evolving new lexical association mea-
sures in order to extract collocations.
The evolved association measure will perform at
least as good as any other AM included in the initial
population. However, the evolved association mea-
sure may be a complex expression that defies inter-
pretation, in which case it may be treated as a black-
box suitable for the specific task of collocation ex-
traction. Our approach only requires an evaluation
sample, thus it is not limited to any specific type of
collocation, language or corpus.
The preliminary experiments, conducted on a cor-
pus of Croatian documents, showed that the best
evolved measures outperformed other considered as-
sociation measures. Also, most of the best evolved
association measures took into account the linguis-
tic information about an n-gram (the POS of the in-
dividual words).
As part of future work, we intend to apply our ap-
proach to corpora in other languages and compare
the results with existing collocation extraction sys-
tems. We also intend to apply our approach to collo-
cations consisting of more than three words, and to
experiment with additional linguistic features.
Acknowledgments
This work has been supported by the Government
of Republic of Croatia, and Government of Flan-
ders under the grant No. 036-1300646-1986 and
KRO/009/06.
References
John Atkinson-Abutridy, Chris Mellish, and Stuart
Aitken. 2004. Combining information extraction with
genetic algorithms for text mining. IEEE Intelligent
Systems, 19(3):22?30.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Stephan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statisti-
cal evaluation measures. Computer Speech and Lan-
guage, 19(4):450?466.
Michael Gordon, Weiguo Fan, and Praveen Pathak.
2006. Adaptive web search: Evolving a program
that finds information. IEEE Intelligent Systems,
21(5):72?77.
John R. Koza. 1992. Genetic programming: On the pro-
gramming of computers by means of natural selection.
MIT Press.
Christopher Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA, USA.
Pavel Pecina and Pavel Schlesinger. 2006. Combin-
ing association measures for collocation extraction. In
Proc. of the COLING/ACL 2006, pages 651?658.
Sas?a Petrovic?, Jan ?Snajder, Bojana Dalbelo Bas?ic?, and
Mladen Kolar. 2006. Comparison of collocation ex-
traction measures for document indexing. J. of Com-
puting and Information Technology, 14(4):321?327.
184
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 181?189,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Streaming First Story Detection with application to Twitter
Sas?a Petrovic?
School of Informatics
University of Edinburgh
sasa.petrovic@ed.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Victor Lavrenko
School of Informatics
University of Edinburgh
vlavrenk@inf.ed.ac.uk
Abstract
With the recent rise in popularity and size of
social media, there is a growing need for sys-
tems that can extract useful information from
this amount of data. We address the prob-
lem of detecting new events from a stream of
Twitter posts. To make event detection feasi-
ble on web-scale corpora, we present an algo-
rithm based on locality-sensitive hashing which
is able overcome the limitations of traditional
approaches, while maintaining competitive re-
sults. In particular, a comparison with a state-
of-the-art system on the first story detection
task shows that we achieve over an order of
magnitude speedup in processing time, while
retaining comparable performance. Event de-
tection experiments on a collection of 160 mil-
lion Twitter posts show that celebrity deaths
are the fastest spreading news on Twitter.
1 Introduction
In the recent years, the microblogging service Twit-
ter has become a very popular tool for express-
ing opinions, broadcasting news, and simply com-
municating with friends. People often comment on
events in real time, with several hundred micro-blogs
(tweets) posted each second for significant events.
Twitter is not only interesting because of this real-
time response, but also because it is sometimes ahead
of newswire. For example, during the protests fol-
lowing Iranian presidential elections in 2009, Iranian
people first posted news on Twitter, where they were
later picked up by major broadcasting corporations.
Another example was the swine flu outbreak when
the US Centre for disease control (CDC) used Twit-
ter to post latest updates on the pandemic. In ad-
dition to this, subjective opinion expressed in posts
is also an important feature that sets Twitter apart
from traditional newswire.
New event detection, also known as first story de-
tection (FSD)1 is defined within the topic detection
and tracking as one of the subtasks (Allan, 2002).
Given a sequence of stories, the goal of FSD is to
identify the first story to discuss a particular event.
In this context, an event is taken to be something
that happens at some specific time and place, e.g.,
an earthquake striking the town of L?Aquila in Italy
on April 6th 2009. Detecting new events from tweets
carries additional problems and benefits compared
to traditional new event detection from newswire.
Problems include a much higher volume of data to
deal with and also a higher level of noise. A major
benefit of doing new event detection from tweets is
the added social component ? we can understand the
impact an event had and how people reacted to it.
The speed and volume at which data is coming
from Twitter warrants the use of streaming algo-
rithms to make first story detection feasible. In
the streaming model of computation (Muthukrish-
nan, 2005), items (tweets in our case) arrive contin-
uously in a chronological order, and we have to pro-
cess each new one in bounded space and time. Recent
examples of problems set in the streaming model in-
clude stream-based machine translation (Levenberg
and Osborne, 2009), approximating kernel matrices
of data streams (Shi et al, 2009), and topic mod-
elling on streaming document collections (Yao et al,
2009). The traditional approach to FSD, where each
new story is compared to all, or a constantly grow-
ing subset, of previously seen stories, does not scale
to the Twitter streaming setting. We present a FSD
system that works in the streaming model and takes
constant time to process each new document, while
also using constant space. Constant processing time
is achieved by employing locality sensitive hashing
(LSH) (Indyk and Motwani, 1998), a randomized
technique that dramatically reduces the time needed
1We will be using the terms first story detection and new
event detection interchangeably.
181
to find a nearest neighbor in vector space, and the
space saving is achieved by keeping the amount of
stories in memory constant.
We find that simply applying pure LSH in a FSD
task yields poor performance and a high variance in
results, and so introduce a modification which vir-
tually eliminates variance and significantly improves
performance. We show that our FSD system gives
comparable results as a state-of-the-art system on
the standard TDT5 dataset, while achieving an order
of magnitude speedup. Using our system for event
detection on 160 million Twitter posts shows that i)
the number of users that write about an event is more
indicative than the volume of tweets written about
it, ii) spam tweets can be detected with reasonable
precision, and iii) news about deaths of famous peo-
ple spreads the fastest on Twitter.
2 First Story Detection
2.1 Traditional Approach
The traditional approach to first story detection is to
represent documents as vectors in term space, where
coordinates represent the (possibly IDF-weighted)
frequency of a particular term in a document. Each
new document is then compared to the previous ones,
and if its similarity to the closest document (or cen-
troid) is below a certain threshold, the new document
is declared to be a first story. For example, this ap-
proach is used in the UMass (Allan et al, 2000) and
the CMU system (Yang et al, 1998). Algorithm 1
shows the exact pseudocode used by the UMass sys-
tem. Note that dismin(d) is the novelty score as-
signed to document d. Often, in order to decrease
the running time, documents are represented using
only n features with the highest weights.
Algorithm 1: Traditional FSD system based on
nearest-neighbor search.
1 foreach document d in corpus do
2 foreach term t in d do
3 foreach document d? that contains t do
4 update distance(d, d?)
5 end
6 end
7 dismin(d) = mind?{distance(d, d?)}
8 add d to inverted index
9 end
2.2 Locality Sensitive Hashing
The problem of finding the nearest neighbor to a
given query has been intensively studied, but as the
dimensionality of the data increases none of the cur-
rent solutions provide much improvement over a sim-
ple linear search (Datar et al, 2004). More recently,
research has focused on solving a relaxed version of
the nearest neighbor problem, the approximate near-
est neighbor, where the goal is to report any point
that lies within (1 + ?)r distance of the query point,
where r is the distance to the nearest neighbor. One
of the first approaches to solving the approximate-
NN problem in sublinear time was described in Indyk
and Motwani (1998), where the authors introduced a
new method called locality sensitive hashing (LSH).
This method relied on hashing each query point into
buckets in such a way that the probability of collision
was much higher for points that are near by. When a
new point arrived, it would be hashed into a bucket
and the points that were in the same bucket were
inspected and the nearest one returned.
Because we are dealing with textual documents,
a particularly interesting measure of distance is the
cosine between two documents. Allan et al (2000)
report that this distance outperforms the KL diver-
gence, weighted sum, and language models as dis-
tance functions on the first story detection task. This
is why in our work we use the hashing scheme pro-
posed by Charikar (2002) in which the probability
of two points colliding is proportional to the cosine
of the angle between them. This scheme was used,
e.g., for creating similarity lists of nouns collected
from a web corpus in Ravichandran et al (2005). It
works by intersecting the space with random hyper-
planes, and the buckets are defined by the subspaces
formed this way. More precisely, the probability of
two points x and y colliding under such a hashing
scheme is
Pcoll = 1? ?(x, y)pi , (1)
where ?(x, y) is the angle between x and y. By us-
ing more than one hyperplane, we can decrease the
probability of collision with a non-similar point. The
number of hyperplanes k can be considered as a num-
ber of bits per key in this hashing scheme. In par-
ticular, if x ? ui < 0, i ? [1 . . . k] for document x and
hyperplane vector ui, we set the i-th bit to 0, and
1 otherwise. The higher k is, the fewer collisions
we will have in our buckets but we will spend more
time computing the hash values.2 However, increas-
ing k also decreases the probability of collision with
the nearest neighbor, so we need multiple hash ta-
bles (each with k independently chosen random hy-
perplanes) to increase the chance that the nearest
neighbor will collide with our point in at least one of
2Probability of collision under k random hyperplanes will
be Pkcoll .
182
them. Given the desired number of bits k, and the
desired probability of missing a nearest neighbor ?,
one can compute the number of hash tables L as
L = log1?Pkcoll ?. (2)
2.3 Variance Reduction Strategy
Unfortunately, simply applying LSH for nearest
neighbor search in a FSD task yields poor results
with a lot of variance (the exact numbers are given in
Section 6). This is because LSH only returns the true
near neighbor if it is reasonably close to the query
point. If, however, the query point lies far away
from all other points (i.e., its nearest neighbor is far
away), LSH fails to find the true near neighbor. To
overcome this problem, we introduce a strategy by
which, if the LSH scheme declares a document new
(i.e., sufficiently different from all others), we start a
search through the inverted index, but only compare
the query with a fixed number of most recent doc-
uments. We set this number to 2000; preliminary
experiments showed that values between 1000 and
3000 all yield very similar results. The pseudocode
shown in algorithm 2 summarizes the approach based
on LSH, with the lines 11 and 12 being the variance
reduction strategy.
Algorithm 2: Our LSH-based approach.
input: threshold t
1 foreach document d in corpus do
2 add d to LSH
3 S ? set of points that collide with d in LSH
4 dismin(d) ? 1
5 foreach document d? in S do
6 c = distance(d, d?)
7 if c < dismin(d) then
8 dismin(d) ? c
9 end
10 end
11 if dismin(d) >= t then
12 compare d to a fixed number of most
recent documents as in Algorithm 1 and
update dismin if necessary
13 end
14 assign score dismin(d) to d
15 add d to inverted index
16 end
3 Streaming First Story Detection
Although using LSH in the way we just described
greatly reduces the running time, it is still too expen-
sive when we want to deal with text streams. Text
streams naturally arise on the Web, where millions
of new documents are published each hour. Social
media sites like Facebook, MySpace, Twitter, and
various blogging sites are a particularly interesting
source of textual data because each new document
is timestamped and usually carries additional meta-
data like topic tags or links to author?s friends. Be-
cause this stream of documents is unbounded and
coming down at a very fast rate, there is usually a
limit on the amount of space/time we can spend per
document. In the context of first story detection,
this means we are not allowed to store all of the pre-
vious data in main memory nor compare the new
document to all the documents returned by LSH.
Following the previous reasoning, we present the
following desiderata for a streaming first story de-
tection system: we first assume that each day we
are presented with a large volume of documents
in chronological order. A streaming FSD system
should, for each document, say whether it discusses a
previously unseen event and give confidence in its de-
cision. The decision should be made in bounded time
(preferably constant time per document), and using
bounded space (also constant per document). Only
one pass over the data is allowed and the decision
has to be made immediately after a new document
arrives. A system that has all of these properties can
be employed for finding first stories in real time from
a stream of stories coming down from the Web.
3.1 A constant space and time approach
In this section, we describe our streaming FSD sys-
tem in more depth. As was already mentioned in
Section 2.2, we use locality sensitive hashing to limit
our search to a small number of documents. How-
ever, because there is only a finite number of buck-
ets, in a true streaming setting the number of docu-
ments in any bucket will grow without a bound. This
means that i) we would use an unbounded amount
of space, and ii) the number of comparisons we need
to make would also grow without a bound. To alle-
viate the first problem, we limit the number of doc-
uments inside a single bucket to a constant. If the
bucket is full, the oldest document in the bucket is
removed. Note that the document is removed only
from that single bucket in one of the L hash tables
? it may still be present in other hash tables. Note
that this way of limiting the number of documents
kept is in a way topic-specific. Luo et al (2007) use
a global constraint on the documents they keep and
show that around 30 days of data needs to be kept
in order to achieve reasonable performance. While
using this approach also ensures that the number of
comparisons made is constant, this constant can be
183
rather large. Theoretically, a new document can col-
lide with all of the documents that are left, and this
can be quite a large number (we have to keep a suffi-
cient portion of the data in memory to make sure we
have a representative sample of the stream to com-
pare with). That is why, in addition to limiting the
number of documents in a bucket, we also limit our-
selves to making a constant number of comparisons.
We do this by comparing each new document with
at most 3L documents it collided with. Unlike Datar
et al (2004), where any 3L documents were used, we
compare to the 3L documents that collide most fre-
quently with the new document. That is, if S is the
set of all documents that collided with a new doc-
ument in all L hash tables, we order the elements
of S according to the number of hash tables where
the collision occurred. We take the top 3L elements
of that ordered set and compare the new document
only to them.
4 Detecting Events in Twitter Posts
While doing first story detection on a newspaper
stream makes sense because all of the incoming doc-
uments are actual stories, this is not the case with
Twitter posts (tweets). The majority of tweets are
not real stories, but rather updates on one?s personal
life, conversations, or spam. Thus, simply running a
first story detection system on this data would yield
an incredible amount of new stories each day, most
of which would be of no interest to anyone but a few
people. However, when something significant hap-
pens (e.g., a celebrity dies), a lot of users write about
this either to share their opinion or just to inform
others of the event. Our goal here is to automati-
cally detect these significant events, preferably with
a minimal number of non-important events.
Threading. We first run our streaming FSD
system and assign a novelty score to each tweet. In
addition, since the score is based on a cosine dis-
tance to the nearest tweet, for each tweet we also
output which other tweet it is most similar to. This
way, we can analyze threads of tweets, i.e., a subset
of tweets which all discuss the same topic (Nallap-
ati et al, 2004). To explain how we form threads
of tweets, we first introduce the links relation. We
say that tweet a links to tweet b if b is the nearest
neighbor of a and 1? cos(a, b) < t, where t is a user-
specified threshold. Then, for each tweet a we either
assign it to an existing thread if its nearest neighbor
is within distance t, or say that a is the first tweet in
a new thread. If we assign a to an existing thread,
we assign it to the same thread to which its nearest
neighbor belongs. By changing t we can control the
granularity of threads. If t is set very high, we will
have few very big and broad threads, whereas setting
t very low will result in many very specific and very
small threads. In our experiments, we set t = 0.5.
We experimented with different values of t and found
that for t ? [0.5, 0.6] results are very much the same,
whereas setting t outside this interval starts to im-
pact the results in the way we just explained.
Once we have threads of tweets, we are interested
in which threads grow fastest, as this will be an indi-
cation that news of a new event is spreading. There-
fore, for each time interval we only output the fastest
growing threads. This growth rate also gives us a way
to measure a thread?s impact.
5 Related Work
In the recent years, analysis of social media has at-
tracted a lot of attention from the research commu-
nity. However, most of the work that uses social
media focuses on blogs (Glance et al, 2004; Bansal
and Koudas, 2007; Gruhl et al, 2005). On the other
hand, research that uses Twitter has so far only
focused on describing the properties of Twitter it-
self (Java et al, 2007; Krishnamurthy et al, 2008).
The problem of online new event detection in
a large-scale streaming setting was previously ad-
dressed in Luo et al (2007). Their system used the
traditional approach to FSD and then employed var-
ious heuristics to make computation feasible. These
included keeping only the first stories in memory,
limiting the number of terms per document, limiting
the number of total terms kept, and employing par-
allel processing. Our randomized framework gives us
a principled way to work out the errors introduced
and is more general than the previously mentioned
approach because we could still use all the heuris-
tics used by Luo et al (2007) in our system. Fi-
nally, while Luo et al (2007) achieved considerable
speedup over an existing system on a TDT corpus,
they never showed the utility of their system on a
truly large-scale task.
The only work we are aware of that analyzes so-
cial media in a streaming setting is Saha and Getoor
(2009). There, the focus was on solving the maxi-
mum coverage problem for a stream of blog posts.
The maximum coverage problem in their setting,
dubbed blog watch, was selecting k blogs that maxi-
mize the cover of interests specified by a user. This
work differs from Saha and Getoor (2009) in many
ways. Most notably, we deal with the problem of
detecting new events, and determining who was the
first to report them. Also, there is a difference in the
type and volume of data ? while Saha and Getoor
(2009) use 20 days of blog data totalling two million
posts, we use Twitter data from a timespan of six
184
months, totalling over 160 million posts.
6 Experiments
6.1 TDT5 Experimental Setup
Baseline. Before applying our FSD system on
Twitter data, we first compared it to a state-of-the-
art FSD system on the standard TDT5 dataset. This
way, we can test if our system is on par with the best
existing systems, and also accurately measure the
speedup that we get over a traditional approach. In
particular, we compare our system with the UMass
FSD system (Allan et al, 2000). The UMass system
has participated in the TDT2 and TDT3 competi-
tions and is known to perform at least as well as other
existing systems who also took part in the competi-
tion (Fiscus, 2001). Note that the UMass system
uses an inverted index (as shown in Algorithm 1)
which optimizes the system for speed and makes sure
a minimal number of comparisons is made. We com-
pare the systems on the English part of the TDT5
dataset, consisting of 221, 306 documents from a time
period spanning April 2003 to September 2003. To
make sure that any difference in results is due to
approximations we make, we use the same settings
as the UMass system: 1-NN clustering, cosine as a
similarity measure, and TFIDF weighted document
representation, where the IDF weights are incremen-
tally updated. These particular settings were found
by Allan et al (2000) to perform the best for the
FSD task. We limit both systems to keeping only
top 300 features in each document. Using more than
300 features barely improves performance while tak-
ing significantly more time for the UMass system.3
LSH parameters. In addition, our system has
two LSH parameters that need to be set. The num-
ber of hyperplanes k gives a tradeoff between time
spent computing the hash functions and the time
spent computing the distances. A lower k means
more documents per bucket and thus more distance
computations, whereas a higher k means less doc-
uments per bucket, but more hash tables and thus
more time spent computing hash functions. Given k,
we can use equation (2) to compute L. In our case,
we chose k to be 13, and L such that the probability
of missing a neighbor within the distance of 0.2 is
less than 2.5%. The distance of 0.2 was chosen as a
reasonable estimate of the threshold when two docu-
ments are very similar. In general, this distance will
depend on the application, and Datar et al (2004)
suggest guessing the value and then doing a binary
search to set it more accurately. We set k to 13 be-
3In other words, using more features only increases the
advantage of our system over the UMass system.
cause it achieved a reasonable balance between time
spent computing the distances and the time spent
computing the hash functions.
Evaluation metric. The official TDT evalua-
tion requires each system to assign a confidence score
for its decision, and this assignment can be made
either immediately after the story arrives, or after
a fixed number of new stories have been observed.
Because we assume that we are working in a true
streaming setting, systems are required to assign a
confidence score as soon as the new story arrives.
The actual performance of a system is measured
in terms of detection error tradeoff (DET) curves
and the minimal normalized cost. Evaluation is car-
ried out by first sorting all stories according to their
scores and then performing a threshold sweep. For
each value of the threshold, stories with a score above
the threshold are considered new, and all others are
considered old. Therefore, for each threshold value,
one can compute the probability of a false alarm, i.e.,
probability of declaring a story new when it is actu-
ally not, and the miss probability, i.e., probability
of declaring a new story old (missing a new story).
Having computed all the miss and false alarm prob-
abilities, we can plot them on a graph showing the
tradeoff between these two quantities ? such graphs
are called detection error tradeoff curves. The nor-
malized cost Cdet is computed as
Cdet = Cmiss ?Pmiss ?Ptarget+CFA?PFA?Pnon?target ,
where Cmiss and CFA are costs of miss and false
alarm, Pmiss and PFA are probabilities of a miss and
false alarm, and Ptarget and Pnon?target are the prior
target and non-target probabilities. Minimal nor-
malized cost Cmin is the minimal value of Cdet over
all threshold values (a lower value of Cmin indicates
better performance).
6.2 TDT5 Results
All the results on the TDT5 dataset are shown in
Table 1. In this section, we go into detail in explain-
ing them. As was mentioned in Section 2.2, simply
using LSH to find a nearest neighbor resulted in poor
performance and a high variance of results. In par-
ticular, the mean normalized cost of ten runs of our
system without the variance reduction strategy was
0.88, with a standard deviation of 0.046. When us-
ing the strategy explained in Section 2.2, the mean
result dropped to 0.70, with a standard deviation of
0.004. Therefore, the results were significantly im-
proved, while also reducing standard deviation by an
order of magnitude. This shows that there is a clear
advantage in using our variance reduction strategy,
185
Table 1: Summary of TDT5 results. Numbers next to LSH?ts indicate the maximal number of documents in a bucket,
measured in terms of percentage of the expected number of collisions.
Baseline Unbounded Bounded
Pure Variance Red. Time Space and Time
System UMass LSH LSH? LSH?t LSH?ts 0.5 LSH?ts 0.3 LSH?ts 0.1
Cmin 0.69 0.88 0.70 0.71 0.76 0.75 0.73
1
2
5
10
20
40
60
80
90
.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90
Miss 
proba
bility 
(in %)
False Alarms probability (in %)
Random PerformanceOur systemUMass system
Figure 1: Comparison of our system with the UMass FSD
system.
and all the following results we report were obtained
from a system that makes use of it.
Figure 1 shows DET curves for the UMass and for
our system. For this evaluation, our system was not
limited in space, i.e., buckets sizes were unlimited,
but the processing time per item was made constant.
It is clear that UMass outperforms our system, but
the difference is negligible. In particular, the min-
imal normalized cost Cmin was 0.69 for the UMass
system, and 0.71 for our system. On the other hand,
the UMass system took 28 hours to complete the
run, compared to two hours for our system. Figure 2
shows the time required to process 100 documents
as a function of number of documents seen so far.
We can see that our system maintains constant time,
whereas the UMass system processing time grows
without a bound (roughly linear with the number
of previously seen documents).
The last three columns in Table 1 show the effect
that limiting the bucket size has on performance.
Bucket size was limited in terms of the percent of
expected number of collisions, i.e., a bucket size of
0.5 means that the number of documents in a bucket
cannot be more than 50% of the expected number
of collisions. The expected number of collisions can
 0
 20
 40
 60
 80
 100
 120
 0  50000  100000  150000  200000  250000
Time 
per 10
0 doc
umen
ts (sec)
Number of documents processed
UMass systemOur system
Figure 2: Comparison of processing time per 100 docu-
ments for our and the UMass system.
be computed as n/2k, where n is the total number
of documents, and k is the LSH parameter explained
earlier. Not surprisingly, limiting the bucket size re-
duced performance compared to the space-unlimited
version, but even when the size is reduced to 10% of
the expected number of collisions, performance re-
mains reasonably close to the UMass system. Fig-
ure 3 shows the memory usage of our system on a
month of Twitter data (more detail about the data
can be found in Section 6.3). We can see that most
of the memory is allocated right away, after which
the memory consumption levels out. If we ran the
system indefinitely, we would see the memory usage
grow slower and slower until it reached a certain level
at which it would remain constant.
6.3 Twitter Experimental Setup
Corpus. We used our streaming FSD system to
detect new events from a collection of Twitter data
gathered over a period of six months (April 1st 2009
to October 14th 2009). Data was collected through
Twitter?s streaming API.4 Our corpus consists of
163.5 million timestamped tweets, totalling over 2
billion tokens. All the tweets in our corpus contain
4http://stream.twitter.com/
186
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0  500  1000  1500  2000  2500  3000  3500  4000
Perce
nt of m
emor
y use
d
Minutes
Figure 3: Memory usage on a month of Twitter data.
X-axis shows how long the system has been running for.
only ASCII characters and we additionally stripped
the tweets of words beginning with the @ or # sym-
bol. This is because on Twitter words beginning with
@ indicate a reply to someone, and words beginning
with # are topic tags. Although these features would
probably be helpful for our task, we decided not to
use them as they are specific to Twitter and our ap-
proach should be independent of the stream type.
Gold standard. In order to measure how well
our system performs on the Twitter data, we em-
ployed two human experts to manually label all the
tweets returned by our system as either Event, Neu-
tral, or Spam. Note that each tweet that is returned
by our system is actually the first tweet in a thread,
and thus serves as the representative of what the
thread is about. Spam tweets include various ad-
vertisements, automatic weather updates, automatic
radio station updates, etc. For a tweet to be la-
beled as an event, it had to be clear from the tweet
alone what exactly happened without having any
prior knowledge about the event, and the event refer-
enced in the tweet had to be sufficiently important.
Important events include celebrity deaths, natural
disasters, major sports, political, entertainment, and
business events, shootings, plane crashes and other
disasters. Neutral tweets include everything not la-
beled as spam or event. Because the process of man-
ual labeling is tedious and time-consuming, we only
labeled the 1000 fastest growing threads from June
2009. Rate of growth of a thread is measured by the
number of tweets that belong to that thread in a win-
dow of 100,000 tweets, starting from the beginning
of the thread. Agreement between our two annota-
tors, measured using Cohen?s kappa coefficient, was
substantial (kappa = 0.65). We use 820 tweets on
which both annotators agreed as the gold standard.
Evaluation. Evaluation is performed by com-
puting average precision (AP) on the gold standard
sorted according to different criteria, where event
tweets are taken to be relevant, and neutral and spam
tweets are treated as non-relevant documents. Aver-
age precision is a common evaluation metric in tasks
like ad-hoc retrieval where only the set of returned
documents and their relevance judgements are avail-
able, as is the case here (Croft et al, 2009). Note
that we are not evaluating our FSD system here.
There are two main reasons for this: i) we already
have a very good idea about the first story detection
performance from the experiments on TDT5 data,
and ii) evaluating a FSD system on this scale would
be prohibitively expensive as it would involve hu-
man experts going through 30 million tweets looking
for first stories. Rather, we are evaluating different
methods of ranking threads which are output from a
FSD system for the purpose of detecting important
events in a very noisy and unstructured stream such
as Twitter.
6.4 Twitter Results
Results for the average precisions are given in Ta-
ble 2. Note that we were not able to compare our
system with the UMass FSD system on the Twit-
ter data, as the UMass system would not finish in
any reasonable amount of time. Different rows of
Table 2 correspond to the following ways of ranking
the threads:
? Baseline ? random ordering of threads
? Size of thread ? threads are ranked according to
number of tweets
? Number of users ? threads are ranked according
to number of unique users posting in a thread
? Entropy + users ? if the entropy of a thread is
< 3.5, move to the back of the list, otherwise
sort according to number of unique users
Results show that ranking according to size of thread
performs better than the baseline, and ranking ac-
cording to the number of users is slightly better.
However, a sign test showed that neither of the two
ranking strategies is significantly better than the
baseline. We perform the sign test by splitting the
labeled data into 50 stratified samples and ranking
each sample with different strategies. We then mea-
sure the number of times each strategy performed
better (in terms of AP) and compute the significance
levels based on these numbers. Adding the informa-
tion about the entropy of the thread showed to be
187
Table 2: Average precision for Events vs. Rest and for
Events and Neutral vs. Spam.
Ranking method events vs. rest spam vs. rest
Baseline 16.5 84.6
Size of thread 24.1 83.5
Number of users 24.5 83.9
Entropy + users 34.0 96.3
Table 3: Average precision as a function of the entropy
threshold on the Events vs. Rest task.
Entropy 2 2.5 3 3.5 4 4.5
AP 24.8 27.6 30.0 34.0 33.2 29.4
very beneficial. Entropy of a thread is computed as
Hthread = ?
?
i
ni
N log
ni
N ,
where ni is the number of times word i appears in
a thread, and N = ?i ni is the total number of
words in a thread. We move the threads with low
entropy (< 3.5) to the back of the list, while we or-
der other threads by the number of unique users.
A sign test showed this approach to be significantly
better (p ? 0.01) than all of the previous ranking
methods. Table 3 shows the effect of varying the en-
tropy threshold at which threads are moved to the
back of the list. We can see that adding informa-
tion about entropy improves results regardless of the
threshold we choose. This approach works well be-
cause most spam threads have very low entropy, i.e.,
contain very little information.
We conducted another experiment where events
and neutral tweets are considered relevant, and spam
tweets non-relevant documents. Results for this ex-
periment are given in the third column of Table 2.
Results for this experiment are much better, mostly
due to the large proportion of neutral tweets in the
data. The baseline in this case is very strong and
neither sorting according to the size of the thread
nor according to the number of users outperforms
the baseline. However, adding the information about
entropy significantly (p ? 0.01) improves the perfor-
mance over all other ranking methods.
Finally, in Table 4 we show the top ten fastest
growing threads in our data (ranked by the number
of users posting in the thread). Each thread is repre-
sented by the first tweet. We can see from the table
that events which spread the fastest on Twitter are
Table 4: Top ten fastest growing threads in our data.
# users First tweet
7814 TMZ reporting michael jackson has had a heart
attack. We r checking it out. And pulliing
video to use if confirmed
7579 RIP Patrick Swayze...
3277 Walter Cronkite is dead.
2526 we lost Ted Kennedy :(
1879 RT BULLETIN ? STEVE MCNAIR
HAS DIED.
1511 David Carradine (Bill in ?Kill Bill?)
found hung in Bangkok hotel.
1458 Just heard Sir Bobby Robson has died. RIP.
1426 I just upgraded to 2.0 - The professional
Twitter client. Please RT!
1220 LA Times reporting Manny Ramirez tested
positive for performance enhancing drugs.
To be suspended 50 games.
1057 A representative says guitar legend
Les Paul has died at 94
mostly deaths of famous people. One spam thread
that appears in the list has an entropy of 2.5 and
doesn?t appear in the top ten list when using the
entropy + users ranking.
7 Conclusion
We presented an approach to first story detection in a
streaming setting. Our approach is based on locality
sensitive hashing adapted to the first story detection
task by introducing a backoff towards exact search.
This adaptation greatly improved performance of the
system and virtually eliminated variance in the re-
sults. We showed that, using our approach, it is pos-
sible to achieve constant space and processing time
while maintaining very good results. A comparison
with the UMass FSD system showed that we gain
more than an order of magnitude speedup with only a
minor loss in performance. We used our FSD system
on a truly large-scale task of detecting new events
from over 160 million Twitter posts. To the best of
our knowledge, this is the first work that does event
detection on this scale. We showed that our system
is able to detect major events with reasonable preci-
sion, and that the amount of spam in the output can
be reduced by taking entropy into account.
Acknowledgments
The authors would like to thank Donnla Osborne for
her work on annotating tweets.
188
References
James Allan, Victor Lavrenko, Daniella Malin, and Rus-
sell Swan. 2000. Detections, bounds, and timelines:
Umass and tdt-3. In Proceedings of Topic Detection
and Tracking Workshop, pages 167?174.
James Allan. 2002. Topic detection and tracking: event-
based information organization. Kluwer Academic
Publishers.
Nilesh Bansal and Nick Koudas. 2007. Blogscope: a
system for online analysis of high volume text streams.
In VLDB ?07: Proceedings of the 33rd international
conference on Very large data bases, pages 1410?1413.
VLDB Endowment.
Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In STOC ?02: Pro-
ceedings of the thiry-fourth annual ACM symposium on
Theory of computing, pages 380?388, New York, NY,
USA. ACM.
W.B. Croft, D. Metzler, and T. Strohman. 2009. Search
Engines: Information Retrieval in Practice. Addison-
Wesley Publishing.
Mayur Datar, Nicole Immorlica, Piotr Indyk, and Va-
hab Mirrokni. 2004. Locality-sensitive hashing scheme
based on p-stable distributions. In SCG ?04: Proceed-
ings of the twentieth annual symposium on Computa-
tional geometry, pages 253?262, New York, NY, USA.
ACM.
J. Fiscus. 2001. Overview of results (nist). In Proceed-
ings of the TDT 2001 Workshop.
N. Glance, M. Hurst, and T. Tomokiyo. 2004. BlogPulse:
Automated Trend Discovery for Weblogs. WWW 2004
Workshop on the Weblogging Ecosystem: Aggregation,
Analysis and Dynamics, 2004.
Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak,
and Andrew Tomkins. 2005. The predictive power
of online chatter. In KDD ?05: Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 78?87, New
York, NY, USA. ACM.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In STOC ?98: Proceedings of the thirti-
eth annual ACM symposium on Theory of computing,
pages 604?613, New York, NY, USA. ACM.
Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng.
2007. Why we twitter: understanding microblogging
usage and communities. In WebKDD/SNA-KDD ?07:
Proceedings of the 9th WebKDD and 1st SNA-KDD
2007 workshop on Web mining and social network
analysis, pages 56?65, New York, NY, USA. ACM.
Balachander Krishnamurthy, Phillipa Gill, and Martin
Arlitt. 2008. A few chirps about twitter. In WOSP
?08: Proceedings of the first workshop on Online social
networks, pages 19?24, New York, NY, USA. ACM.
Abby Levenberg and Miles Osborne. 2009. Stream-based
randomised language models for smt. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 756?764.
Gang Luo, Chunqiang Tang, and Philip S. Yu. 2007.
Resource-adaptive real-time new event detection. In
SIGMOD ?07: Proceedings of the 2007 ACM SIG-
MOD international conference on Management of
data, pages 497?508, New York, NY, USA. ACM.
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Now Publishers Inc.
Ramesh Nallapati, Ao Feng, Fuchun Peng, and James
Allan. 2004. Event threading within news topics. In
CIKM ?04: Proceedings of the thirteenth ACM interna-
tional conference on Information and knowledge man-
agement, pages 446?453, New York, NY, USA. ACM.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In ACL ?05: Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
622?629, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Barna Saha and Lise Getoor. 2009. On maximum cov-
erage in the streaming model & application to multi-
topic blog-watch. In 2009 SIAM International Con-
ference on Data Mining (SDM09), April.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, Alex Strehl, and Vishy Vish-
wanathan. 2009. Hash kernels. In Proceedings of
the 12th International Conference on Artificial Intelli-
gence and Statistics (AISTATS), pages 496?503.
Yiming Yang, Tom Pierce, and Jaime Carbonell. 1998.
A study of retrospective and on-line event detection.
In SIGIR ?98: Proceedings of the 21st annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 28?36, New
York, NY, USA. ACM.
Limin Yao, David Mimno, and Andrew McCallum. 2009.
Efficient methods for topic model inference on stream-
ing document collections. In KDD ?09: Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 937?946,
New York, NY, USA. ACM.
189
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228?232,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Unsupervised joke generation from big data
Sas?a Petrovic?
School of Informatics
University of Edinburgh
sasa.petrovic@ed.ac.uk
David Matthews
School of Informatics
University of Edinburgh
dave.matthews@ed.ac.uk
Abstract
Humor generation is a very hard problem.
It is difficult to say exactly what makes a
joke funny, and solving this problem al-
gorithmically is assumed to require deep
semantic understanding, as well as cul-
tural and other contextual cues. We depart
from previous work that tries to model this
knowledge using ad-hoc manually created
databases and labeled training examples.
Instead we present a model that uses large
amounts of unannotated data to generate I
like my X like I like my Y, Z jokes, where
X, Y, and Z are variables to be filled in.
This is, to the best of our knowledge, the
first fully unsupervised humor generation
system. Our model significantly outper-
forms a competitive baseline and gener-
ates funny jokes 16% of the time, com-
pared to 33% for human-generated jokes.
1 Introduction
Generating jokes is typically considered to be a
very hard natural language problem, as it implies
a deep semantic and often cultural understanding
of text. We deal with generating a particular type
of joke ? I like my X like I like my Y, Z ? where X
and Y are nouns and Z is typically an attribute that
describes X and Y. An example of such a joke is
I like my men like I like my tea, hot and British ?
these jokes are very popular online.
While this particular type of joke is not interest-
ing from a purely generational point of view (the
syntactic structure is fixed), the content selection
problem is very challenging. Indeed, most of the
X, Y, and Z triples, when used in the context of
this joke, will not be considered funny. Thus, the
main challenge in this work is to ?fill in? the slots
in the joke template in a way that the whole phrase
is considered funny.
Unlike the previous work in humor generation,
we do not rely on labeled training data or hand-
coded rules, but instead on large quantities of
unannotated data. We present a machine learning
model that expresses our assumptions about what
makes these types of jokes funny and show that by
using this fairly simple model and large quantities
of data, we are able to generate jokes that are con-
sidered funny by human raters in 16% of cases.
The main contribution of this paper is, to the
best of our knowledge, the first fully unsupervised
joke generation system. We rely only on large
quantities of unlabeled data, suggesting that gener-
ating jokes does not always require deep semantic
understanding, as usually thought.
2 Related Work
Related work on computational humor can be di-
vided into two classes: humor recognition and hu-
mor generation. Humor recognition includes dou-
ble entendre identification in the form of That?s
what she said jokes (Kiddon and Brun, 2011),
sarcastic sentence identification (Davidov et al,
2010), and one-liner joke recognition (Mihalcea
and Strapparava, 2005). All this previous work
uses labeled training data. Kiddon and Brun
(2011) use a supervised classifier (SVM) trained
on 4,000 labeled examples, while Davidov et al
(2010) and Mihalcea and Strapparava (2005) both
use a small amount of training data followed by a
bootstrapping step to gather more.
Examples of work on humor generation include
dirty joke telling robots (Sjo?bergh and Araki,
2008), a generative model of two-liner jokes (Lab-
utov and Lipson, 2012), and a model of punning
riddles (Binsted and Ritchie, 1994). Again, all this
work uses supervision in some form: Sjo?bergh and
Araki (2008) use only human jokes collected from
various sources, Labutov and Lipson (2012) use a
supervised approach to learn feasible circuits that
connect two concepts in a semantic network, and
228
ZYX
?1(Z)
?(Y, Z)?(X, Z)
?(X, Y)
?2(Z)
Figure 1: Our model presented as a factor graph.
Binsted and Ritchie (1994) have a set of six hard-
coded rules for generating puns.
3 Generating jokes
We generate jokes of the form I like my X like I like
my Y, Z, and we assume that X and Y are nouns,
and that Z is an adjective.
3.1 Model
Our model encodes four main assumptions about
I like my jokes: i) a joke is funnier the more often
the attribute is used to describe both nouns, ii) a
joke is funnier the less common the attribute is, iii)
a joke is funnier the more ambiguous the attribute
is, and iv) a joke is funnier the more dissimilar
the two nouns are. A graphical representation of
our model in the form of a factor graph is shown
in Figure 1. Variables, denoted by circles, and fac-
tors, denoted by squares, define potential functions
involving the variables they are connected to.
Assumption i) is the most straightforward, and
is expressed through ?(X,Z) and ?(Y, Z) factors.
Mathematically, this assumption is expressed as:
?(x, z) = p(x, z) = f(x, z)?
x,z f(x, z)
, (1)
where f(x, z)1 is a function that measures the co-
occurrence between x and z. In this work we sim-
ply use frequency of co-occurrence of x and z in
some large corpus, but other functions, e.g., TF-
IDF weighted frequency, could also be used. The
same formula is used for ?(Y,Z), only with dif-
ferent variables. Because this factor measures the
1We use uppercase to denote random variables, and low-
ercase to denote random variables taking on a specific value.
similarity between nouns and attributes, we will
also refer to it as noun-attribute similarity.
Assumption ii) says that jokes are funnier if the
attribute used is less common. For example, there
are a few attributes that are very common and can
be used to describe almost anything (e.g., new,
free, good), but using them would probably lead
to bad jokes. We posit that the less common the
attribute Z is, the more likely it is to lead to sur-
prisal, which is known to contribute to the funni-
ness of jokes. We express this assumption in the
factor ?1(Z):
?1(z) = 1/f(z) (2)
where f(z) is the number of times attribute z ap-
pears in some external corpus. We will refer to this
factor as attribute surprisal.
Assumption iii) says that more ambiguous at-
tributes lead to funnier jokes. This is based on the
observation that the humor often stems from the
fact that the attribute is used in one sense when
describing noun x, and in a different sense when
describing noun y. This assumption is expressed
in ?2(Z) as:
?2(z) = 1/senses(z) (3)
where senses(z) is the number of different senses
that attribute z has. Note that this does not exactly
capture the fact that z should be used in different
senses for the different nouns, but it is a reason-
able first approximation. We refer to this factor as
attribute ambiguity.
Finally, assumption iv) says that dissimilar
nouns lead to funnier jokes. For example, if the
two nouns are girls and boys, we could easily find
many attributes that both nouns share. However,
since the two nouns are very similar, the effect of
surprisal would diminish as the observer would ex-
pect us to find an attribute that can describe both
nouns well. We therefore use ?(X,Y ) to encour-
age dissimilarity between the two nouns:
?(x, y) = 1/sim(x, y), (4)
where sim is a similarity function that measures
how similar nouns x and y are. We call this fac-
tor noun dissimilarity. There are many similar-
ity functions proposed in the literature, see e.g.,
Weeds et al (2004); we use the cosine between
the distributional representation of the nouns:
sim(x, y) =
?
z p(z|x)p(z|y)??
z p(z|x)2 ?
?
z p(z|y)2
(5)
229
Equation 5 computes the similarity between the
nouns by representing them in the space of all at-
tributes used to describe them, and then taking the
cosine of the angle between the noun vectors in
this representation.
To obtain the joint probability for an (x, y, z)
triple we simply multiply all the factors and nor-
malize over all the triples.
4 Data
For estimating f(x, y) and f(z), we use Google
n-gram data (Michel et al, 2010), in particular the
Google 2-grams. We tag each word in the 2-grams
with the part-of-speech (POS) tag that corresponds
to the most common POS tag associated with that
word in Wordnet (Fellbaum, 1998). Once we have
the POS-tagged Google 2-gram data, we extract
all (noun, adjective) pairs and use their counts to
estimate both f(x, z) and f(y, z). We discard 2-
grams whose count in the Google data is less than
1000. After filtering we are left with 2 million
(noun, adjective) pairs. We estimate f(z) by sum-
ming the counts of all Google 2-grams that con-
tain that particular z. We obtain senses(z) from
Wordnet, which contains the number of senses for
all common words.
It is important to emphasize here that, while we
do use Wordnet in our work, our approach does not
crucially rely on it, and we use it to obtain only
very shallow information. In particular, we use
Wordnet to obtain i) POS tags for Google 2-grams,
and ii) number of senses for adjectives. POS tag-
ging could be easily done using any one of the
readily available POS taggers, but we chose this
approach for its simplicity and speed. The number
of different word senses for adjectives is harder to
obtain without Wordnet, but this is only one of the
four factors in our model, and we do not depend
crucially on it.
5 Experiments
We evaluate our model in two stages. Firstly, using
automatic evaluation with a set of jokes collected
from Twitter, and secondly, by comparing our ap-
proach to human-generated jokes.
5.1 Inference
As the focus of this paper is on the model, not the
inference methods, we use exact inference. While
this is too expensive for estimating the true proba-
bility of any (x, y, z) triple, it is feasible if we fix
one of the nouns, i.e., if we deal with P (Y, Z|X =
x). Note that this is only a limitation of our infer-
ence procedure, not the model, and future work
will look at other ways (e.g., Gibbs sampling) to
perform inference. However, generating Y and
Z given X , such that the joke is funny, is still a
formidable challenge that a lot of humans are not
able to perform successfully (cf. performance of
human-generated jokes in Table 2).
5.2 Automatic evaluation
In the automatic evaluation we measure the effect
of the different factors in the model, as laid out in
Section 3.1. We use two metrics for this evalua-
tion. The first is similar to log-likelihood, i.e., the
log of the probability that our model assigns to a
triple. However, because we do not compute it on
all the data, just on the data that contains the Xs
from our development set, it is not exactly equal
to the log-likelihood. It is a local approximation
to log-likelihood, and we therefore dub it LOcal
Log-likelihood, or LOL-likelihood for short. Our
second metric computes the rank of the human-
generated jokes in the distribution of all possible
jokes sorted decreasingly by their LOL-likelihood.
This Rank OF Likelihood (ROFL) is computed
relative to the number of all possible jokes, and
like LOL-likelihood is averaged over all the jokes
in our development data. One advantage of ROFL
is that it is designed with the way we generate
jokes in mind (cf. Section 5.3), and thus more di-
rectly measures the quality of generated jokes than
LOL-likelihood. For measuring LOL-likelihood
and ROFL we use a set of 48 jokes randomly sam-
pled from Twitter that fit the I like my X like I like
my Y, Z pattern.
Table 1 shows the effect of the different fac-
tors on the two metrics. We use a model with
only noun-attribute similarity (factors ?(X,Z)
and ?(Y, Z)) as the baseline. We see that the sin-
gle biggest improvement comes from the attribute
surprisal factor, i.e., from using rarer attributes.
The best combination of the factors, according to
automatic metrics, is using all factors except for
the noun similarity (Model 1), while using all the
factors is the second best combination (Model 2).
5.3 Human evaluation
The main evaluation of our model is in terms of
human ratings, put simply: do humans find the
jokes generated by our model funny? We compare
four models: the two best models from Section 5.2
230
Model LOL-likelihood ROFL
Baseline -225.3 0.1909
Baseline + ?(X,Y ) -227.1 0.2431
Baseline + ?1(Z) -204.9 0.1467
Baseline + ?2(Z) -224.6 0.1625
Baseline + ?1(Z) + ?2(Z) (Model 1) -198.6 0.1002
All factors (Model 2) -203.7 0.1267
Table 1: Effect of different factors.
(one that uses all the factors (Model 2), and one
that uses all factors except for the noun dissimilar-
ity (Model 1)), a baseline model that uses only the
noun-attribute similarity, and jokes generated by
humans, collected from Twitter. We sample a fur-
ther 32 jokes from Twitter, making sure that there
was no overlap with the development set.
To generate a joke for a particular x we keep the
top n most probable jokes according to the model,
renormalize their probabilities so they sum to one,
and sample from this reduced distribution. This al-
lows our model to focus on the jokes that it consid-
ers ?funny?. In our experiments, we use n = 30,
which ensures that we can still generate a variety
of jokes for any given x.
In our experiments we showed five native En-
glish speakers the jokes from all the systems in a
random, per rater, order. The raters were asked
to score each joke on a 3-point Likert scale: 1
(funny), 2 (somewhat funny), and 3 (not funny).
Naturally, the raters did not know which approach
each joke was coming from. Our model was used
to sample Y and Z variables, given the same Xs
used in the jokes collected from Twitter.
Results are shown in Table 2. The second col-
umn shows the inter-rater agreement (Randolph,
2005), and we can see that it is generally good, but
that it is lower on the set of human jokes. We in-
spected the human-generated jokes with high dis-
agreement and found that the disagreement may
be partly explained by raters missing cultural ref-
erences in the jokes (e.g., a sonic screwdriver is
Doctor Who?s tool of choice, which might be lost
on those who are not familiar with the show).
We do not explicitly model cultural references,
and are thus less likely to generate such jokes,
leading to higher agreement. The third column
shows the mean joke score (lower is better), and
we can see that human-generated jokes were rated
the funniest, jokes from the baseline model the
least funny, and that the model which uses all the
Model ? Mean % funny jokes
Human jokes 0.31 2.09 33.1
Baseline 0.58 2.78 3.7
Model 1 0.52 2.71 6.3
Model 2 0.58 2.56 16.3
Table 2: Comparison of different models on the
task of generating Y and Z given X.
factors (Model 2) outperforms the model that was
best according to the automatic evaluation (Model
1). Finally, the last column shows the percentage
of jokes the raters scored as funny (i.e., the num-
ber of funny scores divided by the total number of
scores). This is a metric that we are ultimately
interested in ? telling a joke that is somewhat
funny is not useful, and we should only reward
generating a joke that is found genuinely funny
by humans. The last column shows that human-
generated jokes are considered funnier than the
machine-generated ones, but also that our model
with all the factors does much better than the other
two models. Model 2 is significantly better than
the baseline at p = 0.05 using a sign test, and
human jokes are significantly better than all three
models at p = 0.05 (because we were testing mul-
tiple hypotheses, we employed Holm-Bonferroni
correction (Holm, 1979)). In the end, our best
model generated jokes that were found funny by
humans in 16% of cases, compared to 33% ob-
tained by human-generated jokes.
Finally, we note that the funny jokes generated
by our system are not simply repeats of the human
jokes, but entirely new ones that we were not able
to find anywhere online. Examples of the funny
jokes generated by Model 2 are shown in Table 3.
6 Conclusion
We have presented a fully unsupervised humor
generation system for generating jokes of the type
231
I like my relationships like I like my source, open
I like my coffee like I like my war, cold
I like my boys like I like my sectors, bad
Table 3: Example jokes generated by Model 2.
I like my X like I like my Y, Z, where X, Y, and Z are
slots to be filled in. To the best of our knowledge,
this is the first humor generation system that does
not require any labeled data or hard-coded rules.
We express our assumptions about what makes a
joke funny as a machine learning model and show
that by estimating its parameters on large quanti-
ties of unlabeled data we can generate jokes that
are found funny by humans. While our experi-
ments show that human-generated jokes are fun-
nier more of the time, our model significantly im-
proves upon a non-trivial baseline, and we believe
that the fact that humans found jokes generated by
our model funny 16% of the time is encouraging.
Acknowledgements
The authors would like to thank the raters for their
help and patience in labeling the (often not so
funny) jokes. We would also like to thank Micha
Elsner for this helpful comments. Finally, we
thank the inhabitants of offices 3.48 and 3.38 for
putting up with our sniggering every Friday after-
noon.
References
Kim Binsted and Graeme Ritchie. 1994. An imple-
mented model of punning riddles. In Proceedings
of the twelfth national conference on Artificial intel-
ligence (vol. 1), AAAI ?94, pages 633?638, Menlo
Park, CA, USA. American Association for Artificial
Intelligence.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 107?116.
Christiane Fellbaum. 1998. Wordnet: an electronic
lexical database. MIT Press.
Sture Holm. 1979. A simple sequentially rejective
multiple test procedure. Scandinavian journal of
statistics, pages 65?70.
Chloe? Kiddon and Yuriy Brun. 2011. That?s what she
said: double entendre identification. In Proceedings
of the 49th Annual Meeting of the ACL: Human Lan-
guage Technologies: short papers - Volume 2, pages
89?94.
Igor Labutov and Hod Lipson. 2012. Humor as cir-
cuits in semantic networks. In Proceedings of the
50th Annual Meeting of the ACL (Volume 2: Short
Papers), pages 150?155, July.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hol-
berg, Dan Clancy, Peter Norvig, Jon Orwant, Steven
Pinker, Martin A. Nowak, and Erez Lieberman
Aiden. 2010. Quantitative analysis of culture using
millions of digitized books. Science.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: investigations in automatic humor
recognition. In Proceedings of the conference on
Human Language Technology and EMNLP, pages
531?538.
Justus J. Randolph. 2005. Free-marginal multi-
rater kappa (multirater free): An alternative to fleiss
fixed- marginal multirater kappa. In Joensuu Uni-
versity Learning and Instruction Symposium.
Jonas Sjo?bergh and Kenji Araki. 2008. A com-
plete and modestly funny system for generating and
performing japanese stand-up comedy. In Coling
2008: Companion volume: Posters, pages 111?114,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th international
conference on Computational Linguistics, COLING
?04, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
232
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 25?26,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The Edinburgh Twitter Corpus
Sas?a Petrovic?
School of Informatics
University of Edinburgh
sasa.petrovic@ed.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Victor Lavrenko
School of Informatics
University of Edinburgh
vlavrenk@inf.ed.ac.uk
Abstract
We describe the first release of our corpus of
97 million Twitter posts. We believe that this
data will prove valuable to researchers working
in social media, natural language processing,
large-scale data processing, and similar areas.
1 Introduction
In the recent years, the microblogging service Twit-
ter has become a popular tool for expressing opin-
ions, broadcasting news, and simply communicating
with friends. People often comment on events in
real time, with several hundred micro-blogs (tweets)
posted each second for significant events. Despite
this popularity, there still does not exist a publicly
available corpus of Twitter posts. In this paper we
describe the first such corpus collected over a period
of two months using the Twitter streaming API.1
Our corpus contains 97 million tweets, and takes up
14 GB of disk space uncompressed. The corpus is
distributed under a Creative Commons Attribution-
NonCommercial-ShareAlike license2 and can be ob-
tained at http://demeter.inf.ed.ac.uk/. Each
tweet has the following information:
? timestamp ? time (in GMT) when the tweet was
written
? anonymized username ? the author of the tweet,
where the author?s original Twitter username
is replaced with an id of type userABC. We
anonymize the usernames in this way to avoid
malicious use of the data (e.g., by spammers).
Note that usernames are anonymized consis-
tently, i.e., every time user A is mentioned in
the stream, he is replaced with the same id.
1http://stream.twitter.com/
2http://creativecommons.org/licenses/by-nc-sa/3.0/
legalcode
Table 1: N-gram statistics.
N-grams tokens unique
Unigrams 2,263,886,631 31,883,775
Bigrams 2,167,567,986 174,785,693
Trigrams 2,072,595,131 948,850,470
4-grams 1,980,386,036 1,095,417,876
? posting method ? method used to publish the
tweet (e.g., web, API, some Twitter client).
Given that there are dozen of Twitter clients in
use today, we believe this information could be
very useful in determining, e.g., any differences
in content that comes through different clients.
The format of our data is very simple. Each line
has the following format:
timestamp \t username \t tweet \t client
where \t is the tab character, and client is the pro-
gram used for posting the tweet. Note that the ad-
ditional whitespaces seen above are only added for
readability, and don?t exist in the corpus.
2 Corpus statistics
We collected the corpus from a period spanning
November 11th 2009 until February 1st 2010. As was
already mentioned, the data was collected through
Twitter?s streaming API and is thus a representa-
tive sample of the entire stream. Table 1 shows the
basic n-gram statistics ? note that our corpus con-
tains over 2 billion words. We made no attempt to
distinguish between English and non-English tweets,
as we believe that a multilingual stream might be of
use for various machine translation experiments.
Table 2 shows some basic statistics specific to the
Twitter stream. In particular, we give the number
of users that posted the tweets, the number of links
(URLs) in the corpus, the number of topics and the
number of replies. From the first two rows of Table 2
25
Table 2: Twitter-specific statistics.
Unique Total
tweets - 96,369,326
users 9,140,015 -
links - 20,627,320
topics 1,416,967 12,588,431
replies 5,426,030 54,900,387
clients 33,860 -
Table 3: Most cited Twitter users
Username number of replies
@justinbieber 279,622
@nickjonas 95,545
@addthis 56,761
@revrunwisdom 51,203
@ 50,565
@luansantanaevc 49,106
@donniewahlberg 46,126
@eduardosurita 36,495
@fiuk 33,570
@ddlovato 32,327
we can see that the average number of tweets per user
is 10.5. Topics are defined as single word preceded by
a # symbol, and replies are single words preceded by
a @ symbol. This is the standard way Twitter users
add metadata to their posts. For topics and replies,
we give both the number of unique tokens and the
total number of tokens.
Table 3 shows a list of 10 users which received the
most replies. The more replies a user receives, more
influential we might consider him. We can see that
the two top ranking users are Justin Bieber and Nick
Jonas, two teenage pop-stars who apparently have a
big fan base on Twitter. In fact, six out of ten users
on the list are singers, suggesting that many artists
have turned to Twitter as a means of communicating
with their fans. Note also that one of the entries is
an empty username ? this is probably a consequence
of mistakes people make when posting a reply.
Similarly to Table 3, Table 4 shows the ten most
popular topics in our corpus. We can see that the
most popular topics include music (#nowplaying,
#mm ? music monday), jobs ads, facebook updates
(#fb), politics (#tcot ? top conservatives on Twit-
ter), and random chatter (#ff ? follow friday, #tiny-
chat, #fail, #formspringme). The topic #39;s is an
error in interpreting the apostrophe sign, which has
the ascii value 39 (decimal).
Table 4: Most popular topics on Twitter
Topic number of occurences
#nowplaying 255,715
#ff 220,607
#jobs 181,205
#fb 144,835
#39;s 110,150
#formspringme 85,775
#tcot 77,294
#fail 56,730
#tinychat 56,174
#mm 52,971
Figure 1: Different sources of tweets.
Figure 1 shows the ten most popular clients used
for posting to Twitter. Despite the large amount
of different Twitter clients used (over 33 thousand,
cf. Table 2), figure 1 shows that almost 80% of all the
tweets in our corpus were posted using one of the top
ten most popular clients. We can see that traditional
posting through the Twitter web site is still by far
the most popular method, while UberTwitter and
TweetDeck seem to be the next most popular choices.
3 Conclusion
In this paper we presented a corpus of almost 100
million tweets which we made available for public
use. Basic properties of the corpus are given and a
simple analysis of the most popular users and topics
revealed that Twitter is in large part used to talk
about music by communicating both with artists and
other fans. We believe that this corpus could be
a very useful resource for researchers dealing with
social media, natural language processing, or large-
scale data processing.
26
