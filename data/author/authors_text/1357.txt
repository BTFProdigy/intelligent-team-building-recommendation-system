Proceedings of the 12th Conference of the European Chapter of the ACL, pages 273?281,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Who is ?You?? Combining Linguistic and Gaze Features to Resolve
Second-Person References in Dialogue?
Matthew Frampton1, Raquel Ferna?ndez1, Patrick Ehlen1, Mario Christoudias2,
Trevor Darrell2 and Stanley Peters1
1Center for the Study of Language and Information, Stanford University
{frampton, raquelfr, ehlen, peters}@stanford.edu
2International Computer Science Institute, University of California at Berkeley
cmch@icsi.berkeley.edu, trevor@eecs.berkeley.edu
Abstract
We explore the problem of resolving the
second person English pronoun you in
multi-party dialogue, using a combination
of linguistic and visual features. First, we
distinguish generic and referential uses,
then we classify the referential uses as ei-
ther plural or singular, and finally, for the
latter cases, we identify the addressee. In
our first set of experiments, the linguistic
and visual features are derived from man-
ual transcriptions and annotations, but in
the second set, they are generated through
entirely automatic means. Results show
that a multimodal system is often prefer-
able to a unimodal one.
1 Introduction
The English pronoun you is the second most fre-
quent word in unrestricted conversation (after I
and right before it).1 Despite this, with the ex-
ception of Gupta et al (2007b; 2007a), its re-
solution has received very little attention in the lit-
erature. This is perhaps not surprising since the
vast amount of work on anaphora and reference
resolution has focused on text or discourse - medi-
ums where second-person deixis is perhaps not
as prominent as it is in dialogue. For spoken di-
alogue pronoun resolution modules however, re-
solving you is an essential task that has an impor-
tant impact on the capabilities of dialogue summa-
rization systems.
?We thank the anonymous EACL reviewers, and Surabhi
Gupta, John Niekrasz and David Demirdjian for their com-
ments and technical assistance. This work was supported by
the CALO project (DARPA grant NBCH-D-03-0010).
1See e.g. http://www.kilgarriff.co.uk/BNC_lists/
Besides being important for computational im-
plementations, resolving you is also an interesting
and challenging research problem. As for third
person pronouns such as it, some uses of you are
not strictly referential. These include discourse
marker uses such as you know in example (1), and
generic uses like (2), where you does not refer to
the addressee as it does in (3).
(1) It?s not just, you know, noises like something
hitting.
(2) Often, you need to know specific button se-
quences to get certain functionalities done.
(3) I think it?s good. You?ve done a good review.
However, unlike it, you is ambiguous between sin-
gular and plural interpretations - an issue that is
particularly problematic in multi-party conversa-
tions. While you clearly has a plural referent in
(4), in (3) the number of its referent is ambigu-
ous.2
(4) I don?t know if you guys have any questions.
When an utterance contains a singular referen-
tial you, resolving the you amounts to identifying
the individual to whom the utterance is addressed.
This is trivial in two-person dialogue since the cur-
rent listener is always the addressee, but in conver-
sations with multiple participants, it is a complex
problem where different kinds of linguistic and vi-
sual information play important roles (Jovanovic,
2007). One of the issues we investigate here is
2In contrast, the referential use of the pronoun it (as well
as that of some demonstratives) is ambiguous between NP
interpretations and discourse-deictic ones (Webber, 1991).
273
how this applies to the more concrete problem of
resolving the second person pronoun you.
We approach this issue as a three-step prob-
lem. Using the AMI Meeting Corpus (McCowan
et al, 2005) of multi-party dialogues, we first dis-
criminate between referential and generic uses of
you. Then, within the referential uses, we dis-
tinguish between singular and plural, and finally,
we resolve the singular referential instances by
identifying the intended addressee. We use multi-
modal features: initially, we extract discourse fea-
tures from manual transcriptions and use visual in-
formation derived from manual annotations, but
then we move to a fully automatic approach, us-
ing 1-best transcriptions produced by an automatic
speech recognizer (ASR) and visual features auto-
matically extracted from raw video.
In the next section of this paper, we give a brief
overview of related work. We describe our data in
Section 3, and explain how we extract visual and
linguistic features in Sections 4 and 5 respectively.
Section 6 then presents our experiments with man-
ual transcriptions and annotations, while Section
7, those with automatically extracted information.
We end with conclusions in Section 8.
2 Related Work
2.1 Reference Resolution in Dialogue
Although the vast majority of work on reference
resolution has been with monologic text, some re-
cent research has dealt with the more complex
scenario of spoken dialogue (Strube and Mu?ller,
2003; Byron, 2004; Arstein and Poesio, 2006;
Mu?ller, 2007). There has been work on the iden-
tification of non-referential uses of the pronoun it:
Mu?ller (2006) uses a set of shallow features au-
tomatically extracted from manual transcripts of
two-party dialogue in order to train a rule-based
classifier, and achieves an F-score of 69%.
The only existing work on the resolution of you
that we are aware of is Gupta et al (2007b; 2007a).
In line with our approach, the authors first disam-
biguate between generic and referential you, and
then attempt to resolve the reference of the ref-
erential cases. Generic uses of you account for
47% of their data set, and for the generic vs. ref-
erential disambiguation, they achieve an accuracy
of 84% on two-party conversations and 75% on
multi-party dialogue. For the reference resolution
task, they achieve 47%, which is 10 points over
a baseline that always classifies the next speaker
as the addressee. These results are achieved with-
out visual information, using manual transcripts,
and a combination of surface features and manu-
ally tagged dialogue acts.
2.2 Addressee Detection
Resolving the referential instances of you amounts
to determining the addressee(s) of the utterance
containing the pronoun. Recent years have seen
an increasing amount of research on automatic
addressee detection. Much of this work focuses
on communication between humans and computa-
tional agents (such as robots or ubiquitous com-
puting systems) that interact with users who may
be engaged in other activities, including interac-
tion with other humans. In these situations, it
is important for a system to be able to recognize
when it is being addressed by a user. Bakx et
al. (2003) and Turnhout et al (2005) studied this
issue in the context of mixed human-human and
human-computer interaction using facial orienta-
tion and utterance length as clues for addressee
detection, while Katzenmaier et al (2004) inves-
tigated whether the degree to which a user utter-
ance fits the language model of a conversational
robot can be useful in detecting system-addressed
utterances. This research exploits the fact that hu-
mans tend to speak differently to systems than to
other humans.
Our research is closer to that of Jovanovic
et al (2006a; 2007), who studied addressing in
human-human multi-party dialogue. Jovanovic
and colleagues focus on addressee identification in
face-to-face meetings with four participants. They
use a Bayesian Network classifier trained on sev-
eral multimodal features (including visual features
such as gaze direction, discourse features such as
the speaker and dialogue act of preceding utter-
ances, and utterance features such as lexical clues
and utterance duration). Using a combination of
features from various resources was found to im-
prove performance (the best system achieves an
accuracy of 77% on a portion of the AMI Meeting
Corpus). Although this result is very encouraging,
it is achieved with the use of manually produced
information - in particular, manual transcriptions,
dialogue acts and annotations of visual focus of at-
tention. One of the issues we aim to investigate
here is how automatically extracted multimodal
information can help in detecting the addressee(s)
of you-utterances.
274
Generic Referential Ref Sing. Ref Pl.
49.14% 50.86% 67.92% 32.08%
Table 1: Distribution of you interpretations
3 Data
Our experiments are performed using the AMI
Meeting Corpus (McCowan et al, 2005), a collec-
tion of scenario-driven meetings among four par-
ticipants, manually transcribed and annotated with
several different types of information (including
dialogue acts, topics, visual focus of attention, and
addressee). We use a sub-corpus of 948 utterances
containing you, and these were extracted from 10
different meetings. The you-utterances are anno-
tated as either discourse marker, generic or refer-
ential.
We excluded the discourse marker cases, which
account for only 8% of the data, and of the refer-
ential cases, selected those with an AMI addressee
annotation.3 The addressee of a dialogue act can
be unknown, a single meeting participant, two
participants, or the whole audience (three partici-
pants in the AMI corpus). Since there are very few
instances of two-participant addressee, we distin-
guish only between singular and plural addressees.
The resulting distribution of classes is shown in
Table 1.4
We approach the reference resolution task as a
two-step process, first discriminating between plu-
ral and singular references, and then resolving the
reference of the singular cases. The latter task re-
quires a classification scheme for distinguishing
between the three potential addressees (listeners)
for the given you-utterance.
In their four-way classification scheme,
Gupta et al (2007a) label potential addressees in
terms of the order in which they speak after the
you-utterance. That is, for a given you-utterance,
the potential addressee who speaks next is labeled
1, the potential addressee who speaks after that is
2, and the remaining participant is 3. Label 4 is
used for group addressing. However, this results
in a very skewed class distribution because the
next speaker is the intended addressee 41% of
the time, and 38% of instances are plural - the
3Addressee annotations are not provided for some dia-
logue act types - see (Jovanovic et al, 2006b).
4Note that the percentages of the referential singular and
referential plural are relative to the total of referential in-
stances.
L1 L2 L3
35.17% 30.34% 34.49%
Table 2: Distribution of addressees for singular you
remaining two classes therefore make up a small
percentage of the data.
We were able to obtain a much less skewed class
distribution by identifying the potential addressees
in terms of their position in relation to the current
speaker. The meeting setting includes a rectangu-
lar table with two participants seated at each of
its opposite longer sides. Thus, for a given you-
utterance, we label listeners as either L1, L2 or
L3 depending on whether they are sitting opposite,
diagonally or laterally from the speaker. Table 2
shows the resulting class distribution for our data-
set. Such a labelling scheme is more similar to Jo-
vanovic (2007), where participants are identified
by their seating position.
4 Visual Information
4.1 Features from Manual Annotations
We derived per-utterance visual features from the
Focus Of Attention (FOA) annotations provided
by the AMI corpus. These annotations track meet-
ing participants? head orientation and eye gaze
during a meeting.5 Our first step was to use the
FOA annotations in order to compute what we re-
fer to as Gaze Duration Proportion (GDP) values
for each of the utterances of interest - a measure
similar to the ?Degree of Mean Duration of Gaze?
described by (Takemae et al, 2004). Here a GDP
value denotes the proportion of time in utterance u
for which subject i is looking at target j:
GDPu(i, j) =
?
j
T (i, j)/Tu
were Tu is the length of utterance u in millisec-
onds, and T (i, j), the amount of that time that i
spends looking at j. The gazer i can only refer to
one of the four meeting participants, but the tar-
get j can also refer to the white-board/projector
screen present in the meeting room. For each utter-
ance then, all of the possible values of i and j are
used to construct a matrix of GDP values. From
this matrix, we then construct ?Highest GDP? fea-
tures for each of the meeting participants: such
5A description of the FOA labeling scheme is avail-
able from the AMI Meeting Corpus website http://corpus.
amiproject.org/documentations/guidelines-1/
275
For each participant Pi
? target for whole utterance
? target for first third of utterance
? target for second third of utterance
? target for third third of utterance
? target for -/+ 2 secs from you start time
? ratio 2nd hyp. target / 1st hyp. target
? ratio 3rd hyp. target / 1st hyp. target
? participant in mutual gaze with speaker
Table 3: Visual Features
features record the target with the highest GDP
value and so indicate whom/what the meeting par-
ticipant spent most time looking at during the ut-
terance.
We also generated a number of additional fea-
tures for each individual. These include firstly,
three features which record the candidate ?gazee?
with the highest GDP during each third of the ut-
terance, and which therefore account for gaze tran-
sitions. So as to focus more closely on where par-
ticipants are looking around the time when you
is uttered, another feature records the candidate
with the highest GDP -/+ 2 seconds from the start
time of the you. Two further features give some
indication of the amount of looking around that
the speaker does during an utterance - we hypoth-
esized that participants (especially the speaker)
might look around more in utterances with plu-
ral addressees. The first is the ratio of the sec-
ond highest GDP to the highest, and the second
is the ratio of the third highest to the highest. Fi-
nally, there is a highest GDP mutual gaze feature
for the speaker, indicating with which other indi-
vidual, the speaker spent most time engaged in a
mutual gaze.
Hence this gives a total of 29 features: seven
features for each of the four participants, plus one
mutual gaze feature. They are summarized in Ta-
ble 3. These visual features are different to those
used by Jovanovic (2007) (see Section 2). Jo-
vanovic?s features record the number of times that
each participant looks at each other participant
during the utterance, and in addition, the gaze di-
rection of the current speaker. Hence, they are not
highest GDP values, they do not include a mutual
gaze feature and they do not record whether par-
ticipants look at the white-board/projector screen.
4.2 Automatic Features from Raw Video
To perform automatic visual feature extraction, a
six degree-of-freedom head tracker was run over
each subject?s video sequence for the utterances
containing you. For each utterance, this gave 4 se-
quences, one per subject, of the subject?s 3D head
orientation and location at each video frame along
with 3D head rotational velocities. From these
measurements we computed two types of visual
information: participant gaze and mutual gaze.
The 3D head orientation and location of each
subject along with camera calibration information
was used to compute participant gaze information
for each video frame of each sequence in the form
of a gaze probability matrix. More precisely, cam-
era calibration is first used to estimate the 3D head
orientation and location of all subjects in the same
world coordinate system.
The gaze probability matrix is a 4 ? 5 matrix
where entry i, j stores the probability that subject
i is looking at subject j for each of the four sub-
jects and the last column corresponds to the white-
board/projector screen (i.e., entry i, j where j = 5
is the probability that subject i is looking at the
screen). Gaze probability G(i, j) is defined as
G(i, j) = G0e
??i,j2/?2
where ?i,j is the angular difference between the
gaze of subject i and the direction defined by the
location of subjects i and j. G0 is a normalization
factor such that
?
j G(i, j) = 1 and ? is a user-
defined constant (in our experiments, we chose
? = 15 degrees).
Using the gaze probability matrix, a 4 ? 1 per-
frame mutual gaze vector was computed that for
entry i stores the probability that the speaker and
subject i are looking at one another.
In order to create features equivalent to those
described in Section 4.1, we first collapse the
frame-level probability matrix into a matrix of bi-
nary values. We convert the probability for each
frame into a binary judgement of whether subject
i is looking at target j:
H(i, j) = ?G(i, j)
? is a binary value to evaluate G(i, j) > ?, where
? is a high-pass thresholding value - or ?gaze prob-
ability threshold? (GPT) - between 0 and 1.
Once we have a frame-level matrix of binary
values, for each subject i, we compute GDP val-
ues for the time periods of interest, and in each
case, choose the target with the highest GDP as the
candidate. Hence, we compute a candidate target
for the utterance overall, for each third of the ut-
terance, and for the period -/+ 2 seconds from the
276
you start time, and in addition, we compute a can-
didate participant for mutual gaze with the speaker
for the utterance overall.
We sought to use the GPT threshold which pro-
duces automatic visual features that agree best
with the features derived from the FOA annota-
tions. Hence we experimented with different GPT
values in increments of 0.1, and compared the re-
sulting features to the manual features using the
kappa statistic. A threshold of 0.6 gave the best
kappa scores, which ranged from 20% to 44%.6
5 Linguistic Information
Our set of discourse features is a simplified ver-
sion of those employed by Galley et al (2004) and
Gupta et al (2007a). It contains three main types
(summarized in Table 4):
? Sentential features (1 to 13) encode structural,
durational, lexical and shallow syntactic patterns
of the you-utterance. Feature 13 is extracted us-
ing the AMI ?Named Entity? annotations and in-
dicates whether a particular participant is men-
tioned in the you-utterance. Apart from this fea-
ture, all other sentential features are automatically
extracted, and besides 1, 8, 9, and 10, they are all
binary.
? Backward Looking (BL)/Forward Looking (FL)
features (14 to 22) are mostly extracted from ut-
terance pairs, namely the you-utterance and the
BL/FL (previous/next) utterance by each listener
Li (potential addressee). We also include a few
extra features which are not computed in terms of
utterance pairs. These indicate the number of par-
ticipants that speak during the previous and next 5
utterances, and the BL and FL speaker order. All
of these features are computed automatically.
? Dialogue Act (DA) features (23 to 24) use the
manual AMI dialogue act annotations to represent
the conversational function of the you-utterance
and the BL/FL utterance by each potential ad-
dressee. Along with the sentential feature based
on the AMI Named Entity annotations, these are
the only discourse features which are not com-
puted automatically. 7
6The fact that our gaze estimator is getting any useful
agreement with respect to these annotations is encouraging
and suggests that an improved tracker and/or one that adapts
to the user more effectively could work very well.
7Since we use the manual transcripts of the meetings, the
transcribed words and the segmentation into utterances or di-
alogue acts are of course not given automatically. A fully
automatic approach would involve using ASR output instead
of manual transcriptions? something which we attempt in
(1) # of you pronouns
(2) you (say|said|tell|told| mention(ed)|mean(t)|
sound(ed))
(3) auxiliary you
(4) wh-word you
(5) you guys
(6) if you
(7) you know
(8) # of words in you-utterance
(9) duration of you-utterance
(10) speech rate of you-utterance
(11) 1st person
(12) general case
(13) person Named Entity tag
(14) # of utterances between you- and BL/FL utt.
(15) # of speakers between you- and BL/FL utt.
(16) overlap between you- and BL/FL utt. (binary)
(17) duration of overlap between you- and BL/FL utt.
(18) time separation between you- and BL/FL utt.
(19) ratio of words in you- that are in BL/FL utt.
(20) # of participants that speak during prev. 5 utt.
(21) # of participants that speak during next 5 utt.
(22) speaker order BL/FL
(23) dialogue act of the you-utterance
(24) dialogue act of the BL/FL utterance
Table 4: Discourse Features
6 First Set of Experiments & Results
In this section we report our experiments and re-
sults when using manual transcriptions and anno-
tations. In Section 7 we will present the results
obtained using ASR output and automatically ex-
tracted visual information. All experiments (here
and in the next section) are performed using a
Bayesian Network classifier with 10-fold cross-
validation.8 In each task, we give raw overall ac-
curacy results and then F-scores for each of the
classes. We computed measures of information
gain in order to assess the predictive power of the
various features, and did some experimentation
with Correlation-based Feature Selection (CFS)
(Hall, 2000).
6.1 Generic vs. Referential Uses of You
We first address the task of distinguishing between
generic and referential uses of you.
Baseline. A majority class baseline that classi-
fies all instances of you as referential yields an ac-
curacy of 50.86% (see Table 1).
Results. A summary of the results is given in Ta-
ble 5. Using discourse features only we achieve
an accuracy of 77.77%, while using multimodal
Section 7.
8We use the the BayesNet classifier implemented in the
Weka toolkit http://www.cs.waikato.ac.nz/ml/weka/.
277
Features Acc F1-Gen F1-Ref
Baseline 50.86 0 67.4
Discourse 77.77 78.8 76.6
Visual 60.32 64.2 55.5
MM 79.02 80.2 77.7
Dis w/o FL 78.34 79.1 77.5
MM w/o FL 78.22 79.0 77.4
Dis w/o DA 69.44 71.5 67.0
MM w/o DA 72.75 74.4 70.9
Table 5: Generic vs. referential uses
(MM) yields 79.02%, but this increase is not sta-
tistically significant.
In spite of this, visual features do help to dis-
tinguish between generic and referential uses -
note that the visual features alone are able to beat
the baseline (p < .005). The listeners? gaze is
more predictive than the speaker?s: if listeners
look mostly at the white-board/projector screen in-
stead of another participant, then the you is more
likely to be referential. More will be said on this
in Section 6.2.1 in the analysis of the results for
the singular vs. plural referential task.
We found sentential features of the you-
utterance to be amongst the best predictors, es-
pecially those that refer to surface lexical proper-
ties, such as features 1, 11, 12 and 13 in Table 4.
Dialogue act features provide useful information
as well. As pointed out by Gupta et al (2007b;
2007a), a you pronoun within a question (e.g.
an utterance tagged as elicit-assess or
elicit-inform) is more likely to be referen-
tial. Eliminating information about dialogue acts
(w/o DA) brings down performance (p < .005),
although accuracy remains well above the baseline
(p < .001). Note that the small changes in perfor-
mance when FL information is taken out (w/o FL)
are not statistically significant.
6.2 Reference Resolution
We now turn to the referential instances of you,
which can be resolved by determining the ad-
dressee(s) of the given utterance.
6.2.1 Singular vs. Plural Reference
We start by trying to discriminate singular vs. plu-
ral interpretations. For this, we use a two-way
classification scheme that distinguishes between
individual and group addressing. To our knowl-
edge, this is the first attempt at this task using lin-
guistic information.9
9But see e.g. (Takemae et al, 2004) for an approach that
uses manually extracted visual-only clues with similar aims.
Baseline. A majority class baseline that consid-
ers all instances of you as referring to an individual
addressee gives 67.92% accuracy (see Table 1).
Results. A summary of the results is shown in
Table 6. There is no statistically significant differ-
ence between the baseline and the results obtained
when visual features are used alone (67.92% vs.
66.28%). However, we found that visual informa-
tion did contribute to identifying some instances of
plural addressing, as shown by the F-score for that
class. Furthermore, the visual features helped to
improve results when combined with discourse in-
formation: using multimodal (MM) features pro-
duces higher results than the discourse-only fea-
ture set (p < .005), and increases from 74.24% to
77.05% with CFS.
As in the generic vs. referential task, the white-
board/projector screen value for the listeners? gaze
features seems to have discriminative power -
when listeners? gaze features take this value, it is
often indicative of a plural rather than a singular
you. It seems then, that in our data-set, the speaker
often uses the white-board/projector screen when
addressing the group, and hence draws the listen-
ers? gaze in this direction. We should also note
that the ratio features which we thought might be
useful here (see Section 4.1) did not prove so.
Amongst the most useful discourse features
are those that encode similarity relations between
the you-utterance and an utterance by a potential
addressee. Utterances by individual addressees
tend to be more lexically cohesive with the you-
utterance and so if features such as feature 19 in
Table 4 indicate a low level of lexical similarity,
then this increases the likelihood of plural address-
ing. Sentential features that refer to surface lexical
patterns (features 6, 7, 11 and 12) also contribute
to improved results, as does feature 21 (number of
speakers during the next five utterances) - fewer
speaker changes correlates with plural addressing.
Information about dialogue acts also plays a
role in distinguishing between singular and plu-
ral interpretations. Questions tend to be addressed
to individual participants, while statements show a
stronger correlation with plural addressees. When
no DA features are used (w/o DA), the drop in per-
formance for the multimodal classifier to 71.19%
is statistically significant (p < .05). As for the
generic vs. referential task, FL information does
not have a significant effect on performance.
278
Features Acc F1-Sing. F1-Pl.
Baseline 67.92 80.9 0
Discourse 71.19 78.9 54.6
Visual 66.28 74.8 48.9
MM* 77.05 83.3 63.2
Dis w/o FL 72.13 80.1 53.7
MM w/o FL 72.60 79.7 58.1
Dis w/o DA 68.38 78.5 40.5
MM w/o DA 71.19 78.8 55.3
Table 6: Singular vs. plural reference; * = with Correlation-
based Feature Selection (CFS).
6.2.2 Detection of Individual Addressees
We now turn to resolving the singular referential
uses of you. Here we must detect the individual
addressee of the utterance that contains the pro-
noun.
Baselines. Given the distribution shown in Ta-
ble 2, a majority class baseline yields an accu-
racy of 35.17%. An off-line system that has access
to future context could implement a next-speaker
baseline that always considers the next speaker to
be the intended addressee, so yielding a high raw
accuracy of 71.03%. A previous-speaker base-
line that does not require access to future context
achieves 35% raw accuracy.
Results. Table 7 shows a summary of the re-
sults, and these all outperform the majority class
(MC) and previous-speaker baselines. When all
discourse features are available, adding visual in-
formation does improve performance (74.48% vs.
60.69%, p < .005), and with CFS, this increases
further to 80.34% (p < .005). Using discourse or
visual features alone gives scores that are below
the next-speaker baseline (60.69% and 65.52% vs.
71.03%). Taking all forward-looking (FL) infor-
mation away reduces performance (p < .05), but
the small increase in accuracy caused by taking
away dialogue act information is not statistically
significant.
When we investigated individual feature contri-
bution, we found that the most predictive features
were the FL and backward-looking (BL) speaker
order, and the speaker?s visual features (including
mutual gaze). Whomever the speaker spent most
time looking at or engaged in a mutual gaze with
was more likely to be the addressee. All of the vi-
sual features had some degree of predictive power
apart from the ratio features. Of the other BL/FL
discourse features, features 14, 18 and 19 (see Ta-
ble 4) were more predictive. These indicate that
utterances spoken by the intended addressee are
Features Acc F1-L1 F1-L2 F1-L3
MC baseline 35.17 52.0 0 0
Discourse 60.69 59.1 60.0 62.7
Visual 65.52 69.1 63.5 64.0
MM* 80.34 80.0 82.4 79.0
Dis w/o FL 52.41 50.7 51.8 54.5
MM w/o FL 66.55 68.7 62.7 67.6
Dis w/o DA 61.03 58.5 59.9 64.2
MM w/o DA 73.10 72.4 69.5 72.0
Table 7: Addressee detection for singular references; * =
with Correlation-based Feature Selection (CFS).
often adjacent to the you-utterance and lexically
similar.
7 A Fully Automatic Approach
In this section we describe experiments which
use features derived from ASR transcriptions and
automatically-extracted visual information. We
used SRI?s Decipher (Stolcke et al, 2008)10 in or-
der to generate ASR transcriptions, and applied
the head-tracker described in Section 4.2 to the
relevant portions of video in order to extract the
visual information. Recall that the Named Entity
features (feature 13) and the DA features used in
our previous experiments had been manually an-
notated, and hence are not used here. We again
divide the problem into the same three separate
tasks: we first discriminate between generic and
referential uses of you, then singular vs. plural
referential uses, and finally we resolve the ad-
dressee for singular uses. As before, all exper-
iments are performed using a Bayesian Network
classifier and 10-fold cross validation.
7.1 Results
For each of the three tasks, Figure 7 compares
the accuracy results obtained using the fully-
automatic approach with those reported in Section
6. The figure shows results for the majority class
baselines (MCBs), and with discourse-only (Dis),
and multimodal (MM) feature sets. Note that the
data set for the automatic approach is smaller,
and that the majority class baselines have changed
slightly. This is because of differences in the ut-
terance segmentation, and also because not all of
the video sections around the you utterances were
processed by the head-tracker.
In all three tasks we are able to significantly
outperform the majority class baseline, but the vi-
sual features only produce a significant improve-
10Stolcke et al (2008) report a word error rate of 26.9% on
AMI meetings.
279
Figure 1: Results for the manual and automatic systems; MCB = majority class baseline, Dis = discourse features, MM =
multimodal, * = with Correlation-based Feature Selection (CFS), FL = forward-looking, man = manual, auto = automatic.
ment in the individual addressee resolution task.
For the generic vs. referential task, the discourse
and multimodal classifiers both outperform the
majority class baseline (p < .001), achieving
accuracy scores of 68.71% and 68.48% respec-
tively. In contrast to when using manual transcrip-
tions and annotations (see Section 6.1), removing
forward-looking (FL) information reduces perfor-
mance (p < .05). For the referential singular
vs. plural task, the discourse and multimodal with
CFS classifier improve over the majority class
baseline (p < .05). Multimodal with CFS does
not improve over the discourse classifier - indeed
without feature selection, the addition of visual
features causes a drop in performance (p < .05).
Here, taking away FL information does not cause
a significant reduction in performance. Finally,
in the individual addressee resolution task, the
discourse, visual (60.78%) and multimodal clas-
sifiers all outperform the majority class baseline
(p < .005, p < .001 and p < .001 respec-
tively). Here the addition of visual features causes
the multimodal classifier to outperform the dis-
course classifier in raw accuracy by nearly ten per-
centage points (67.32% vs. 58.17%, p < .05), and
with CFS, the score increases further to 74.51%
(p < .05). Taking away FL information does
cause a significant drop in performance (p < .05).
8 Conclusions
We have investigated the automatic resolution of
the second person English pronoun you in multi-
party dialogue, using a combination of linguistic
and visual features. We conducted a first set of
experiments where our features were derived from
manual transcriptions and annotations, and then a
second set where they were generated by entirely
automatic means. To our knowledge, this is the
first attempt at tackling this problem using auto-
matically extracted multimodal information.
Our experiments showed that visual informa-
tion can be highly predictive in resolving the ad-
dressee of singular referential uses of you. Visual
features significantly improved the performance of
both our manual and automatic systems, and the
latter achieved an encouraging 75% accuracy. We
also found that our visual features had predictive
power for distinguishing between generic and ref-
erential uses of you, and between referential sin-
gulars and plurals. Indeed, for the latter task,
they significantly improved the manual system?s
performance. The listeners? gaze features were
useful here: in our data set it was apparently the
case that the speaker would often use the white-
board/projector screen when addressing the group,
thus drawing the listeners? gaze in this direction.
Future work will involve expanding our data-
set, and investigating new potentially predictive
features. In the slightly longer term, we plan to
integrate the resulting system into a meeting as-
sistant whose purpose is to automatically extract
useful information from multi-party meetings.
280
References
Ron Arstein and Massimo Poesio. 2006. Identifying
reference to abstract objects in dialogue. In Pro-
ceedings of the 10th Workshop on the Semantics and
Pragmatics of Dialogue (Brandial?06), pages 56?
63, Potsdam, Germany.
Ilse Bakx, Koen van Turnhout, and Jacques Terken.
2003. Facial orientation during multi-party inter-
action with information kiosks. In Proceedings of
INTERACT, Zurich, Switzerland.
Donna Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, University
of Rochester, Department of Computer Science.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007a. Resolving ?you? in multi-
party dialog. In Proceedings of the 8th SIGdial
Workshop on Discourse and Dialogue, Antwerp,
Belgium, September.
Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.
2007b. Disambiguating between generic and refer-
ential ?you? in dialog. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Mark Hall. 2000. Correlation-based Feature Selection
for Machine Learning. Ph.D. thesis, University of
Waikato.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006a. Addressee identification in face-to-
face meetings. In Proceedings of the 11th Confer-
ence of the European Chapter of the ACL (EACL),
pages 169?176, Trento, Italy.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006b. A corpus for studying addressing
behaviour in multi-party dialogues. Language Re-
sources and Evaluation, 40(1):5?23. ISSN=1574-
020X.
Natasa Jovanovic. 2007. To Whom It May Concern -
Addressee Identification in Face-to-Face Meetings.
Ph.D. thesis, University of Twente, Enschede, The
Netherlands.
Michael Katzenmaier, Rainer Stiefelhagen, and Tanja
Schultz. 2004. Identifying the addressee in human-
human-robot interactions based on head pose and
speech. In Proceedings of the 6th International
Conference on Multimodal Interfaces, pages 144?
151, State College, Pennsylvania.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Christoph Mu?ller. 2006. Automatic detection of non-
referential It in spoken multi-party dialog. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 49?56, Trento, Italy.
Christoph Mu?ller. 2007. Resolving it, this, and that
in unrestricted multi-party dialog. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 816?823, Prague,
Czech Republic.
Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?r
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The icsi-sri spring
2007 meeting and lecture recognition system. In
Proceedings of CLEAR 2007 and RT2007. Springer
Lecture Notes on Computer Science.
Michael Strube and Christoph Mu?ller. 2003. A ma-
chine learning approach to pronoun resolution in
spoken dialogue. In Proceedings of ACL?03, pages
168?175.
Yoshinao Takemae, Kazuhiro Otsuka, and Naoki
Mukawa. 2004. An analysis of speakers? gaze
behaviour for automatic addressee identification in
multiparty conversation and its application to video
editing. In Proceedings of IEEE Workshop on Robot
and Human Interactive Communication, pages 581?
586.
Koen van Turnhout, Jacques Terken, Ilse Bakx, and
Berry Eggen. 2005. Identifying the intended
addressee in mixed human-humand and human-
computer interaction from non-verbal features. In
Proceedings of ICMI, Trento, Italy.
Bonnie Webber. 1991. Structure and ostension in
the interpretation of discourse deixi. Language and
Cognitive Processes, 6(2):107?135.
281
A Salience-Based Approach to Gesture-Speech Alignment
Jacob Eisenstein and C. Mario Christoudias
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar Street
Cambridge, MA 02139
{jacobe+cmch}@csail.mit.edu
Abstract
One of the first steps towards understanding
natural multimodal language is aligning ges-
ture and speech, so that the appropriate ges-
tures ground referential pronouns in the speech.
This paper presents a novel technique for
gesture-speech alignment, inspired by salience-
based approaches to anaphoric pronoun reso-
lution. We use a hybrid between data-driven
and knowledge-based mtehods: the basic struc-
ture is derived from a set of rules about gesture
salience, but the salience weights themselves
are learned from a corpus. Our system achieves
95% recall and precision on a corpus of tran-
scriptions of unconstrained multimodal mono-
logues, significantly outperforming a competi-
tive baseline.
1 Introduction
In face to face communication, speakers frequently use
gesture to supplement speech (Chovil, 1992), using the
additional modality to provide unique, non-redundant in-
formation (McNeill, 1992). In the context of pen/speech
user interfaces, Oviatt finds that ?multimodal ... language
is briefer, syntactically simpler, and less disfluent than
users? unimodal speech.? (Oviatt, 1999)
One of the simplest and most direct ways in which ges-
ture can supplement verbal communication is by ground-
ing references, usually through deixis. For example, it is
impossible to extract the semantic content of the verbal
utterance ?I?ll take this one? without an accompanying
pointing gesture indicating the thing that is desired. The
problem of gesture-speech alignment involves choosing
the appropriate gesture to ground each verbal utterance.
This paper describes a novel technique for this problem.
We evaluate our system on a corpus of multimodal mono-
logues with no fixed grammar or vocabulary.
1.1 Example
[This]_1 thing goes over [here]_2 so
that it goes back ...
--------------------
1. Deictic: Hand rests on latch mechanism
2. Iconic: Hand draws trajectory from
right to left
In this example, there are three verbal references. The
word ?this? refers to the latch mechanism, which is indi-
cated by the rest position of the hand. ?Here? refers to the
endpoint of the trajectory indicated by the iconic gesture.
?It? is an anaphoric reference to a noun phrase defined
earlier in the sentence; there is no accompanying gesture.
The word ?that? does not act as a reference, although it
could in other cases. Not every pronoun keyword (e.g.,
this, here, it, that, etc.) will act as a reference in all cases.
In addition, there will be many gestures that do not re-
solve any keyword.
2 Related Work
This research draws mainly from two streams of re-
lated work. Researchers in human-computer interaction
have worked towards developing multimodal user inter-
faces, which allow spoken and gestural input. These sys-
tems often feature powerful algorithms for fusing modal-
ities; however, they also restrict communication to short
grammatically-constrained commands over a very lim-
ited vocabulary. Since our goal is to handle more com-
plex linguistic phenomena, these systems were of little
help in the design of our algorithm. Conversely, we found
that the problem of anaphora resolution faces a very sim-
ilar set of challenges as gesture-speech alignment. We
were able to apply techniques from anaphora resolution
to gesture-speech alignment.
2.1 Multimodal User Interfaces
Discussion of multimodal user interfaces begins with the
seminal ?Put-That-There? system (Bolt, 1980), which al-
lowed users to issue natural language commands and use
deictic hand gestures to resolve references from speech.
Commands were subject to a strict grammar and align-
ment was straightforward: keywords created holes in the
semantic frame, and temporally-aligned gestures filled
the holes.
More recent systems have extended this approach
somewhat. Johnston and Bangalore describe a multi-
modal parsing algorithm that is built using a 3-tape, finite
state transducer (FST) (Johnston and Bangalore, 2000).
The speech and gestures of each multimodal utterance
are provided as input to an FST whose output is a se-
mantic representation conveying the combined meaning.
A similar system, based on a graph-matching algorithm,
is described in (Chai et al, 2004). These systems per-
form mutual disambiguation, where each modality helps
to correct errors in the others. However, both approaches
restrict users to a predefined grammar and lexicon, and
rely heavily on having a complete, formal ontology of
the domain.
In (Kettebekov et al, 2002), a co-occurrence model
relates the salient prosodic features of the speech (pitch
variation and pause) to characteristic features of gestic-
ulation (velocity and acceleration). The goal was to im-
prove performance of gesture recognition, rather than to
address the problem of alignment directly. Their ap-
proach also differs from ours in that they operate at the
level of speech signals, rather than recognized words.
Potentially, the two approaches could compliment each
other in a unified system.
2.2 Anaphora Resolution
Anaphora resolution involves linking an anaphor to its
corresponding antecedent in the same or previous sen-
tence. In many cases, speech/gesture multimodal fusion
works in a very similar way, with gestures grounding
some of the same anaphoric pronouns (e.g., ?this?, ?that?,
?here?).
One approach to anaphora resolution is to assign a
salience value to each noun phrase that is a candidate
for acting as a grounding referent, and then to choose
the noun phrase with the greatest salience (Lappin and
Leass, 1994). Mitkov showed that a salience-based ap-
proach can be applied across genres and without com-
plex syntactic, semantic, and discourse analysis (Mitkov,
1998). Salience values are typically computed by apply-
ing linguistic knowledge; e.g., recent noun phrases are
more salient, gender and number should agree, etc. This
knowledge is applied to derive a salience value through
the application of a set of predefined salience weights
on each feature. Salience weights may be defined by
hand, as in (Lappin and Leass, 1994), or learned from
data (Mitkov et al, 2002).
Anaphora resolution and gesture-speech alignment are
very similar problems. Both involve resolving ambigu-
ous words which reference other parts of the utterance.
In the case of anaphora resolution, pronomial references
resolve to previously uttered noun phrases; in gesture-
speech alignment, keywords are resolved by gestures,
which usually precede the keyword. The salience-based
approach works for anaphora resolution because the fac-
tors that contribute to noun-phrase salience are well un-
derstood. We define a parallel set of factors for evaluating
the salience of gestures.
3 Our Approach
The most important goal of our system is the ability to
handle natural, human-to-human language usage. This
includes disfluencies and grammatically incorrect utter-
ances, which become even more problematic when con-
sidering that the output of speech recognizers is far from
perfect. Any approach that requires significant parsing
or other grammatical analysis may be ill-suited to meet
these goals.
Instead, we identify keywords that are likely to require
gestural referents for resolution. Our goal is to produce
an alignment ? a set of bindings ? that match at least some
of the identified keywords with one or more gestures.
There are several things that are known to contribute to
the salience of candidate gesture-speech bindings:
? The relevant gesture is usually close in time to the
keyword (Oviatt et al, 1997; Cohen et al, 2002)
? The gesture usually precedes the keyword (Oviatt et
al., 1997).
? A one-to-one mapping is preferred. Multiple key-
words rarely align with a single gesture, and mul-
tiple gestures almost never align with a single key-
word (Eisenstein and Davis, 2003).
? Some types of gestures, such as deictic pointing ges-
tures, are more likely to take part in keyword bind-
ings. Other gestures (i.e., beats) do not carry this
type of semantic content, and instead act to moder-
ate turn taking or indicate emphasis. These gestures
are unlikely to take part in keyword bindings (Cas-
sell, 1998).
? Some keyword/gesture combinations may be partic-
ularly likely; for example, the keyword ?this? and a
deictic pointing gesture.
These rules mirror the salience weighting features em-
ployed by the anaphora resolution methods described in
the previous section. We define a parameterizable penalty
function that prefers alignments that adhere to as many of
these rules as possible. Given a set of verbal utterances
and gestures, we then try to find the set of bindings with
the minimal penalty. This is essentially an optimization
approach, and we use the simplest possible optimization
technique: greedy hill-climbing. Of course, given a set
of penalties and the appropriate representation, any op-
timization technique could be applied. In the evaluation
section, we discuss whether and how much our system
would benefit from using a more sophisticated optimiza-
tion technique. Later in this section, we formalize the
problem and our proposed solution.
3.1 Leveraging Empirical Data
One of the advantages of the salience-based approach is
that it enables the creation of a hybrid system that ben-
efits both from our intuitions about multimodal commu-
nication and from a corpus of annotated data. The form
of the salience metric, and the choice of features that fac-
tor into it, is governed by our knowledge about the way
speech and gesture work. However, the penalty func-
tion also requires parameters that weigh the importance
of each factor. These parameters can be crafted by hand
if no corpus is available, but they can also be learned from
data. By using knowledge about multimodal language to
derive the form and features of the salience metric, and
using a corpus to fine-tune the parameters of this metric,
we can leverage the strengths of both knowledge-based
and data-driven approaches.
4 Formalization
We define a multimodal transcript M to consist of a set
of spoken utterances S and gestures G. S contains a set
of references R that must be ground by a gestural ref-
erent. We define a binding, b ? B, as a tuple relating
a gesture, g ? G, to a corresponding speech reference,
r ? R. Provided G and R, the set B enumerates all
possible bindings between them. Formally, each gesture,
reference, and binding are defined as
g = ?tgs , t
g
e , Gtype?
r = ?trs, t
r
e, w?
b = ?g, r?
(1)
where ts, te describe the start and ending time of a gesture
or reference, w ? S is the word corresponding to r, and
Gtype is the type of gesture (e.g. deictic or trajectory).
An alternative, useful description of the set B is as the
function b(g) which returns for each gesture a set of cor-
responding references. This function is defined as
b(g) = {r|?g, r? ? B} (2)
4.1 Rules
In this section we provide the analytical form for the
penalty functions of Section 3. We have designed these
functions to penalize bindings that violate the preferences
that model our intuitions about the relationship between
speech and gesture. We begin by presenting the analytical
form for the binding penalty function, ?b.
It is most often the case that verbal references closely
follow the gestures that they refer to; the verbal reference
rarely precedes the gesture. To reflect this knowledge,
we parameterize ?b using a time gap penalty, ?tg, and a
wrong order penalty, ?wo as follows,
?b(b) = ?tgwtg(b) + ?wowwo(b) (3)
where,
wwo(b) =
{
0 trs ? t
g
s
1 trs < t
g
s
and wtg = |trs ? tgs |
In addition to temporal agreement, specific words or
parts-of-speech have varying affinities for different types
of gestures. We incorporate these penalties into ?b by in-
troducing a binding agreement penalty, ?(b), as follows:
?b(b) = ?tgwwo(b) + ?(b) (4)
The remaining penalty functions model binding fertil-
ity. Specifically, we assign a penalty for each unassigned
gesture and reference, ?g(g) and ?r(r) respectively, that
reflect our desire for the algorithm to produce bindings.
Certain gesture types (e.g., deictics) are much more likely
to participate in bindings than others (e.g., beats). An
unassigned gesture penalty is associated with each ges-
ture type, given by ?g(g). Similarly, we expect refer-
ences to have a likelihood of being bound that is condi-
tioned on their word or part-of-speech tag. However, we
currently handle all keywords in the same way, with a
constant penalty ?r(r) for unassigned keywords.
4.2 Minimization Algorithm
GivenG andR we wish to find aB? ? B that minimizes
the penalty function ?(B,G,R):
B? = arg min
B?
?(B?,G,R) (5)
Using the penalty functions of Section 4.1 ?(B?,G,R) is
defined as,
?(B?,G,R) =
?
b?B?
?b(b) + ?g(Ga) + ?r(Ra) (6)
where
Ga = {g|b(g) = ?}
Ra = {r|b(r) = ?}
.
Although there are numerous optimization techniques
that may be applied to minimize Equation 5, we have
chosen to implement a naive gradient decent algorithm
presented below as Algorithm 1. Observing the prob-
lem, note we could have initialized B? = B; in other
Algorithm 1 Gradient Descent
Initialize B? = ? and B? = B
repeat
Let b0 be the first element in B?
?max = ?(B?, G,R)? ?({B?, b0}, G,R)
bmax = b0
for all b ? B?, b 6= b0 do
? = ?(B?, G,R)? ?({B?, b}, G,R)
if ? > ?max then
bmax = b
?max = ?
end if
end for
if ?max > 0 then
B? = {B?, bmax}
B? = B? ? bmax
end if
Convergence test: is ?max < limit?
until convergence
words, start off with all possible bindings, and gradu-
ally prune away the bad ones. But it seems likely that
|B?| ? min(|R|, |G|); thus, starting from the empty set
will converge faster. The time complexity of this algo-
rithm is given by O(|B?||B|). Since |B| = |G||R|, and
assuming |B?| ? |G| ? |R|, this simplifies to O(|B?|3),
cubic in the number of bindings returned.
4.3 Learning Parameters
We explored a number of different techniques for find-
ing the parameters of the penalty function: setting them
by hand, gradient descent, simulated annealing, and a ge-
netic algorithm. A detailed comparison of the results with
each approach is beyond the scope of this paper, but the
genetic algorithm outperformed the other approaches in
both accuracy and rate of convergence.
The genome representation consisted of a thirteen bit
string for each penalty parameter; three bits were used
for the exponent, and the remaining ten were used for
the base. Parameters were allowed to vary from 10?4
to 103. Since there were eleven parameters, the overall
string length was 143. A population size of 200 was used,
and training proceeded for 50 generations. Single-point
crossover was applied at a rate of 90%, and the mutation
rate was set to 3% per bit. Tournament selection was used
rather than straightforward fitness-based selection (Gold-
berg, 1989).
5 Evaluation
We evaluated our system by testing its performance
on a set of 26 transcriptions of unconstrained human-
to-human communication, from nine different speak-
Baseline Training Test
Recall 84.2% 94.6% 95.1%
? n/a 1.2% 5.1%
Precision 82.8% 94.5% 94.5%
? n/a 1.2% 5.0%
Table 1: Performance of our system versus a baseline
ers (Eisenstein and Davis, 2003). Of the four women and
five men who participated, eight were right-handed, and
one was a non-native English speaker. The participants
ranged in age from 22 to 28. All had extensive computer
experience, but none had any experience in the task do-
main, which required explaining the behavior of simple
mechanical devices.
The participants were presented with three conditions,
each of which involved describing the operation of a me-
chanical device based on a computer simulation. The
conditions were shown in order of increasing complexity,
as measured by the number of moving parts: a latchbox, a
piston, and a pinball machine. Monologues ranged in du-
ration from 15 to 90 seconds; the number of gestures used
ranged from six to 58. In total, 574 gesture phrases were
transcribed, of which 239 participated in gesture-speech
bindings.
In explaining the devices, the participants were al-
lowed ? but not instructed ? to refer to a predrawn di-
agram that corresponded to the simulation. Vocabulary,
grammar, and gesture were not constrained in any way.
The monologues were videotaped, transcribed, and an-
notated by hand. No gesture or speech recognition was
performed. The decision to use transcriptions rather than
speech and gesture recognizers will be discussed in detail
below.
5.1 Empirical Results
We averaged results over ten experiments, in which 20%
of the data was selected randomly and held out as a test
set. Entire transcripts were held out, rather than parts of
each transcript. This was necessary because the system
considers the entire transcript holistically when choosing
an alignment.
For a baseline, we evaluated the performance of choos-
ing the temporally closest gesture to each keyword.
While simplistic, this approach is used in several imple-
mented multimodal user interfaces (Bolt, 1980; Koons
et al, 1993). Kettebekov and Sharma even reported that
93.7% of gesture phrases were ?temporally aligned? with
the semantically associated keyword in their corpus (Ket-
tebekov and Sharma, 2001). Our results with this base-
line were somewhat lower, for reasons discussed below.
Table 1 shows the results of our system and the base-
line on our corpus. Our system significantly outperforms
the baseline on both recall and precision on this corpus
(p < 0.05, two-tailed). Precision and recall differ slightly
because there are keywords that do not bind to any ges-
ture. Our system does not assume a one-to-one mapping
between keywords and gestures, and will refuse to bind
some keywords if there is no gesture with a high enough
salience. One benefit of our penalty-based approach is
that it allows us to easily trade off between recall and
precision. Reducing the penalties for unassigned ges-
tures and keywords will cause the system to create fewer
alignments, increasing precision and decreasing recall.
This could be useful in a system where mistaken ges-
ture/speech alignments are particularly undesirable. By
increasing these same penalties, the opposite effect can
also be achieved.
Both systems perform worse on longer monologues.
On the top quartile of monologues by length (measured
in number of keywords), the recall of the baseline system
falls to 75%, and the recall of our system falls to 90%.
For the baseline system, we found a correlation of -0.55
(df = 23, p < 0.01) between F-measure and monologue
length.
This may help to explain why Kettebekov and Sharma
found such success with the baseline algorithm. The mul-
timodal utterances in their corpus consisted of relatively
short commands. The longer monologues in our corpus
tended to be more grammatically complex and included
more disfluency. Consequently, alignment was more dif-
ficult, and a relatively na??ve strategy, such as the baseline
algorithm, was less effective.
6 Discussion
To our knowledge, very few multimodal understanding
systems have been evaluated using natural, unconstrained
speech and gesture. One exception is (Quek et al, 2002),
which describes a system that extracts discourse struc-
ture from gesture on a corpus of unconstrained human-
to-human communication; however, no quantitative anal-
ysis is provided. Of the systems that are more relevant
to the specific problem of gesture-speech alignment (Co-
hen et al, 1997; Johnston and Bangalore, 2000; Kette-
bekov and Sharma, 2001), evaluation is always conducted
from an HCI perspective, in which participants act as
users of a computer system and communicate in short,
grammatically-constrained multimodal commands. As
shown in Section 5.1, such commands are significantly
easier to align than the natural multimodal communica-
tion found in our corpus.
6.1 The Corpus
A number of considerations went into gathering this cor-
pus.1 One of our goals was to minimize the use of
discourse-related ?beat? gestures, so as to better focus on
the deictic and iconic gestures that are more closely re-
lated to the content of the speech; that is why we focused
on monologues rather than dialogues. We also wanted the
corpus to be relevant to the HCI community. That is why
we provided a diagram to gesture at, which we believe
serves a similar function to a computer display, providing
reference points for deictic gestures. We used a predrawn
diagram ? rather than letting participants draw the dia-
gram themselves ? because interleaved speech, gesture,
and sketching is a much more complicated problem, to
be addressed only after bimodal speech-gesture commu-
nication is better understood.
For a number of reasons, we decided to focus on tran-
scriptions of speech and gesture, rather than using speech
and gesture recognition systems. Foremost is that we
wanted the language in our corpus to be as natural as pos-
sible; in particular, we wanted to avoid restricting speak-
ers to a finite list of gestures. Building a recognizer that
could handle such unconstrained gesture would be a sub-
stantial undertaking and an important research contribu-
tion in its own right. However, we are sensitive to the con-
cern that our system should scale to handle possibly erro-
neous recognition data. There are three relevant classes of
errors that our system may need to handle: speech recog-
nition, gesture recognition, and gesture segmentation.
? Speech Recognition Errors
The speech recognizer could fail to recognize a key-
word; in this case, a binding would simply not be
created. If the speech recognizer misrecognized
a non-keyword as a keyword, a spurious binding
might be created. However, since our system does
not require that all keywords have bindings, we feel
that our approach is likely to degrade gracefully in
the face of this type of error.
? Gesture Recognition Errors
This type of error would imply a gestural misclas-
sification, e.g., classifying a deictic pointing gesture
as an iconic. Again, we feel that a salience-based
system will degrade gracefully with this type of er-
ror, since there are no hard requirements on the type
of gesture for forming a binding. In contrast, a sys-
tem that required, say, a deictic gesture to accom-
pany a certain type of command would be very sen-
sitive to a gesture misclassification.
1We also considered using the recently released FORM2
corpus from the Linguistic Data Consortium. However, this
corpus is presently more focused on the kinematics of hand and
upper body movement, rather than on higher-level linguistic in-
formation relating to gestures and speech.
? Gesture Segmentation Errors
Gesture segmentation errors are probably the most
dangerous, since this could involve incorrectly
grouping two separate gestures into a single gesture,
or vice versa. It seems that this type of error would
be problematic for any approach, and we have no
reason to believe that our salience-based approach
would fare differently from any other approach.
6.2 Success Cases
Our system outperformed the baseline by more than 10%.
There were several types of phenomena that the base-
line failed to handle. In this corpus, each gesture pre-
cedes the semantically-associated keyword 85% of the
time. Guided by this fact, we first created a baseline sys-
tem that selected the nearest preceding gesture for each
keyword; clearly, the maximum performance for such a
baseline is 85%. Slightly better results were achieved by
simply choosing the nearest gesture regardless of whether
it precedes the keyword; this is the baseline shown in Ta-
ble 1. However, this baseline incorrectly bound several
cataphoric gestures. The best strategy is to accept just a
few cataphoric gestures in unusual circumstances, but a
na??ve baseline approach is unable to do this.
Most of the other baseline errors came about when
the mapping from gesture to speech was not one-to-one.
For example, in the utterance ?this piece here,? the two
keywords actually refer to a single deictic gesture. In
the salience-based approach, the two keywords were cor-
rectly bound to a single gesture, but the baseline insisted
on finding two gestures. The baseline similarly mishan-
dled situations where a keyword was used without refer-
ring to any gesture.
6.3 Failure Cases
Although the recall and precision of our system neared
95%, investigating the causes of error could suggest
potential improvements. We were particularly interested
in errors on the training set, where overfitting could not
be blamed. This section describes two sources of error,
and suggests some potential improvements.
6.3.1 Disfluencies
We adopted a keyword-based approach so that our sys-
tem would be more robust to disfluency than alternative
approaches that depended on parsing. While we were
able to handle many instances of disfluent speech, we
found that disfluencies occasionally disturbed the usual
relationship between gesture and speech. For example,
consider the following utterance:
It has this... this spinning thing...
Our system attempted to bind gestures to each occur-
rence of ?this?, and ended up binding each reference to a
different gesture. Moreover, both references were bound
incorrectly. The relevant gesture in this case occurs after
both references. This is an uncommon phenomenon, and
as such, was penalized highly. However, anecdotally
it appears that the presence of a disfluency makes this
phenomenon more likely. A disfluency is frequently
accompanied by an abortive gesture, followed by the
full gesture occurring somewhat later than the spoken
reference. It is possible that a system that could detect
disfluency in the speech transcript could account for this
phenomenon.
6.3.2 Greedy Search
Our system applies a greedy hill-climbing opti-
mization to minimize the penalty. While this greedy
optimization performs surprisingly well, we were able
to identify a few cases of errors that were caused by the
greedy nature of our optimization, e.g.
...once it hits this, this thing is blocked.
In this example, the two references are right next to
each other. The relevant gestures are also very near each
other. The ideal bindings are shown in Figure 1a. The
earlier ?this? is considered first, but from the system?s
perspective, the best possible binding is the second ges-
ture, since it overlaps almost completely with the spoken
utterance (Figure 1b). However, once the second gesture
is bound to the first reference, it is removed from the list
of unassigned gestures. Thus, if the second gesture were
also bound to the second utterance, the penalty would
still be relatively high. Even though the earlier gesture
is farther away from the second reference, it is still on the
list of unassigned gestures, and the system can reduce the
overall penalty considerably by binding it. The system
ends up crisscrossing, and binding the earlier gesture to
the later reference, and vice versa (Figure 1c).
7 Future Work
The errors discussed in the previous section suggest some
potential improvements to our system. In this section, we
describe four possible avenues of future work: dynamic
programming, deeper syntactic analysis, other anaphora
resolution techniques, and user adaptation.
7.1 Dynamic Programming
Algorithm 1 provides only an approximate solution to
Equation 5. As demonstrated in Section 6.3.2, the greedy
choice is not always optimal. Using dynamic program-
ming, an exhaustive search of the space of bindings can
be performed within polynomial time.
(a) (b) (c)
Figure 1: The greedy binding problem. (a) The correct binding, (b) the greedy binding, (c) the result.
We define m[i, j] to be the penalty of the optimal sub-
set B? ? {bi, ..., bj} ? B, i ? j. m[i, j] is implemented
as a k ? k lookup table, where k = |B| = |G||R|. Each
entry of this table is recursively defined by preceding ta-
ble entries. Specifically, m[i, j] is computed by perform-
ing exhaustive search on its subsets of bindings. Using
this lookup table, an optimal solution to Equation 5 is
therefore found as ?(B?, G,R) = m[1, k]. Again as-
suming |B?| ? |G| ? |R|, the size of the lookup table is
given by O(|B?|4). Thus, it is possible to find the glob-
ally optimal set of bindings, by moving from an O(n3)
algorithm to O(n4). The precise definition of a recur-
rence relation for m[i, j] and a proof of correctness will
be described in a future publication.
7.2 Syntactic Analysis
One obvious possibility for improvement would be to in-
clude more sophisticated syntactic information beyond
keyword spotting. However, we require that our system
remain robust to disfluency and recognition errors. Part
of speech tagging is a robust method of syntactic anal-
ysis which could allow us to refine the penalty function
depending on the usage case. Consider that there at least
three relevant uses of the keyword ?this.?
1. This movie is better than A.I.
2. This is the bicycle ridden by E.T.
3. The wheel moves like this.
When ?this? is followed by a noun (case 1), a deic-
tic gesture is likely, although not strictly necessary. But
when ?this? is followed by a verb (case 2), a deictic
gesture is usually crucial for understanding the sentence.
Thus, the penalty for not assigning this keyword should
be very high. Finally, in the third case, when the keyword
follows a preposition, a trajectory gesture is more likely,
and the penalty for any such binding should be lowered.
7.3 Other Anaphora Resolution Techniques
We have based this research on salience values, which
is just one of several possible alternative approaches to
anaphora resolution. One such alternative is the use of
constraints: rules that eliminate candidates from the list
of possible antecedents (Rich and Luperfoy, 1988). An
example of a constraint in anaphora resolution is a rule
requiring the elimination of all candidates that disagree
in gender or number with the referential pronoun. Con-
straints may be used in combination with a salience met-
ric, to prune away unlikely choices before searching.
The advantage is that enforcing constraints could be sub-
stantially less computationally expensive than searching
through the space of all possible bindings for the one with
the highest salience. One possible future project would be
to develop a set of constraints for speech-gesture align-
ment, and investigate the effect of these constraints on
both accuracy and speed.
Ge, Hale, and Charniak propose a data-driven ap-
proach to anaphora resolution (Ge et al, 1998). For a
given pronoun, their system can compute a probability
for each candidate antecedent. Their approach of seek-
ing to maximize this probability is similar to the salience-
maximizing approach that we have described. However,
instead of using a parametric salience function, they learn
a set of conditional probability distributions directly from
the data. If this approach could be applied to gesture-
speech alignment, it would be advantageous because the
binding probabilities could be combined with the output
of probabilistic recognizers to produce a pipeline archi-
tecture, similar to that proposed in (Wu et al, 1999). Such
an architecture would provide multimodal disambigua-
tion, where the errors of each component are corrected
by other components.
7.4 Multimodal Adaptation
Speakers have remarkably entrenched multimodal com-
munication patterns, with some users overlapping ges-
ture and speech, and others using each modality sequen-
tially (Oviatt et al, 1997). Moreover, these multimodal
integration patterns do not seem to be malleable, sug-
gesting that multimodal user interfaces should adapt to
the user?s tendencies. We have already shown how the
weights of the salience metric can adapt for optimal per-
formance against a corpus of user data; this approach
could also be extended to adapt over time to an individual
user.
8 Conclusions
This work represents one of the first efforts at aligning
gesture and speech on a corpus of natural multimodal
communication. Using greedy optimization and only a
minimum of linguistic processing, we significantly out-
perform a competitive baseline, which has actually been
implemented in existing multimodal user interfaces. Our
approach is shown to be robust to spoken English, even
with a high level of disfluency. By blending some of the
benefits of empirical and knowledge-based approaches,
our system can learn from a large corpus of data, but de-
grades gracefully when limited data is available.
Obviously, alignment is only one small component of
a comprehensive system for recognizing and understand-
ing multimodal communication. Putting aside the issue
of gesture recognition, there is still the problem of de-
riving semantic information from aligned speech-gesture
units. The solutions to this problem will likely have to be
specially tailored to the application domain. While our
evaluation indicates that our approach achieves what ap-
pears to be a high level of accuracy, the true test will be
whether our system can actually support semantic infor-
mation extraction from multimodal data. Only the con-
struction of such a comprehensive end-to-end system will
reveal whether the algorithm and features that we have
chosen are sufficient, or whether a more sophisticated ap-
proach is required.
Acknowledgements
We thank Robert Berwick, Michael Collins, Trevor Darrell,
Randall Davis, Tracy Hammond, Sanshzar Kettebekov, ?Ozlem
Uzuner, and the anonymous reviewers for their helpful com-
ments on this paper.
References
Richard A. Bolt. 1980. Put-That-There: Voice and gesture at
the graphics interface. Computer Graphics, 14(3):262?270.
Justine Cassell. 1998. A framework for gesture generation and
interpretation. In Computer Vision in Human-Machine Inter-
action, pages 191?215. Cambridge University Press.
Joyce Y. Chai, Pengyu Hong, , and Michelle X. Zhou. 2004. A
probabilistic approach to reference resolution in multimodal
user interfaces. In Proceedings of 2004 International Con-
ference on Intelligent User Intefaces (IUI?04), pages 70?77.
Nicole Chovil. 1992. Discourse-oriented facial displays in con-
versation. Research on Language and Social Interaction,
25:163?194.
Philip R. Cohen, M. Johnston, D. McGee, S. Oviatt, J. Pittman,
I. Smith, L. Chen, and J. Clow. 1997. Quickset: Multimodal
interaction for distributed applications. In ACM Multime-
dia?97, pages 31?40. ACM Press.
Philip R. Cohen, Rachel Coulston, and Kelly Krout. 2002.
Multimodal interaction during multiparty dialogues: Initial
results. In IEEE Conference on Multimodal Interfaces.
Jacob Eisenstein and Randall Davis. 2003. Natural gesture in
descriptive monologues. In UIST?03 Supplemental Proceed-
ings, pages 69?70.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical
approach to anaphora resolution. In Proceedings of the Sixth
Workshop on Very Large Corpora, pages 161?171.
David E. Goldberg. 1989. Genetic Algorithms in Search, Opti-
mization, and Machine Learning. Addison-Wesley.
Michael Johnston and Srinivas Bangalore. 2000. Finite-state
multimodal parsing and understanding. In Proceedings of
COLING-2000. ICCL.
Sanshzar Kettebekov and Rajeev Sharma. 2001. Toward natu-
ral gesture/speech control of a large display. In Engineering
for Human-Computer Interaction (EHCI?01). Lecture Notes
in Computer Science. Springer Verlag.
Sanshzar Kettebekov, Mohammed Yeasin, and Rajeev Sharma.
2002. Prosody based co-analysis for continuous recognition
of coverbal gestures. In International Conference on Mul-
timodal Interfaces (ICMI?02), pages 161?166, Pittsburgh,
USA.
David B. Koons, Carlton J. Sparrell, and Kristinn R. Thorisson.
1993. Integrating simultaneous input from speech, gaze, and
hand gestures. In Intelligent Multimedia Interfaces, pages
257?276. AAAI Press.
Shalom Lappin and Herbert J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational Linguistics,
20(4):535?561.
David McNeill. 1992. Hand and Mind. The University of
Chicago Press.
Ruslan Mitkov, Richard Evans, and Constantin Ora?san. 2002.
A new, fully automatic version of mitkov?s knowledge-poor
pronoun resolution method. In Intelligent Text Processing
and Computational Linguistics (CICLing?02), Mexico City,
Mexico, February, 17 ? 23.
Ruslan Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In COLING-ACL, pages 869?875.
Sharon L. Oviatt, Antonella DeAngeli, and Karen Kuhn. 1997.
Integration and synchronization of input modes during mul-
timodal human-computer interaction. In Human Factors in
Computing Systems (CHI?97), pages 415?422. ACM Press.
Sharon L. Oviatt. 1999. Ten myths of multimodal interaction.
Communications of the ACM, 42(11):74?81.
Francis Quek, David McNeill, Robert Bryll, Susan Duncan,
Xin-Feng Ma, Cemil Kirbas, Karl E. McCullough, and
Rashid Ansari. 2002. Multimodal human discourse: gesture
and speech. Transactions on Computer-Human Interaction,
9(3):171?193.
Elaine Rich and Susann Luperfoy. 1988. An architecture for
anaphora resolution. In Proceedings of the Second Confer-
ence on Applied Natural Language Processing (ANLP-2),
pages 18?24, Texas, USA.
Lizhong Wu, Sharon L. Oviatt, and Philip R. Cohen. 1999.
Multimodal integration - a statistical view. IEEE Transac-
tions on Multimedia, 1(4):334?341.
