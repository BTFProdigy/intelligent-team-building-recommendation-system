Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 42?46,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Combining Multi-Engine Translations with Moses
Yu Chen1, Michael Jellinghaus1, Andreas Eisele1,2,Yi Zhang1,2,
Sabine Hunsicker1, Silke Theison1, Christian Federmann2, Hans Uszkoreit1,2
1: Universita?t des Saarlandes, Saarbru?cken, Germany
2: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
{yuchen,micha,yzhang,sabineh,sith}@coli.uni-saarland.de
{eisele,cfedermann,uszkoreit}@dfki.de
Abstract
We present a simple method for generating
translations with the Moses toolkit (Koehn
et al, 2007) from existing hypotheses pro-
duced by other translation engines. As
the structures underlying these translation
engines are not known, an evaluation-
based strategy is applied to select sys-
tems for combination. The experiments
show promising improvements in terms of
BLEU.
1 Introduction
With the wealth of machine translation systems
available nowadays (many of them online and
for free), it makes increasing sense to investigate
clever ways of combining them. Obviously, the
main objective lies in finding out how to integrate
the respective advantages of different approaches:
Statistical machine translation (SMT) and rule-
based machine translation (RBMT) systems of-
ten have complementary characteristics. Previous
work on building hybrid systems includes, among
others, approaches using reranking, regeneration
with an SMT decoder (Eisele et al, 2008; Chen
et al, 2007), and confusion networks (Matusov et
al., 2006; Rosti et al, 2007; He et al, 2008).
The approach by (Eisele et al, 2008) aimed
specifically at filling lexical gaps in an SMT sys-
tem with information from a number of RBMT
systems. The output of the RBMT engines was
word-aligned with the input, yielding a total of
seven phrase tables which where simply concate-
nated to expand the phrase table constructed from
the training corpus. This approach differs from the
confusion network approaches mainly in that the
final hypotheses do not necessarily follow any of
the input translations as the skeleton. On the other
hand, it emphasizes that the additional translations
should be produced by RBMT systems with lexi-
cons that cannot be learned from the data.
The present work continues on the same track
as the paper mentioned above but implements a
number of important changes, most prominently
a relaxation of the restrictions on the number and
type of input systems. These differences are de-
scribed in more detail in Section 2. Section 3 ex-
plains the implementation of our system and Sec-
tion 4 its application in a number of experiments.
Finally, Section 5 concludes this paper with a sum-
mary and some thoughts on future work.
2 Integrating Multiple Systems of
Unknown Type and Quality
When comparing (Eisele et al, 2008) to the
present work, our proposal is more general in a
way that the requirement for knowledge about the
systems is minimum. The types and the identities
of the participated systems are assumed unknown.
Accordingly, we are not able to restrict ourselves
to a certain class of systems as (Eisele et al, 2008)
did. We rely on a standard phrase-based SMT
framework to extract the valuable pieces from the
system outputs. These extracted segments are also
used to improve an existing SMT system that we
have access to.
While (Eisele et al, 2008) included translations
from all of a fixed number of RBMT systems
and added one feature to the translation model for
each system, integrating all given system outputs
in this way in our case could expand the search
space tremendously. Meanwhile, we cannot rely
on the assumption that all candidate systems ac-
tually have the potential to improve our baseline.
This implies the need for a first step of system se-
lection where the best candidate systems are iden-
tified and a limited number of them is chosen to be
included in the combination. Our approach would
not work without a small set of tuning data being
available so that we can evaluate the systems for
later selection and adjust the weights of our sys-
tems. Such tuning data is included in this year?s
42
task.
In this paper, we use the Moses decoder to con-
struct translations from the given system outputs.
We mainly propose two slightly different ways:
One is to construct translation models solely from
the given translations and the other is to extend
an existing translation model with these additional
translations.
3 Implementation
Despite the fact that the output of current MT sys-
tems is usually not comparable in quality to hu-
man translations, the machine-generated transla-
tions are nevertheless ?parallel? to the input so
that it is straightforward to construct a translation
model from data of this kind. This is the spirit
behind our method for combining multiple trans-
lations.
3.1 Direct combination
Clearly, for the same source sentence, we expect
to have different translations from different trans-
lation systems, just like we would expect from hu-
man translators. Also, every system may have its
own advantages. We break these translations into
smaller units and hope to be able to select the best
ones and form them into a better translation.
One single translation of a few thousand sen-
tences is normally inadequate for building a re-
liable general-purpose SMT system (data sparse-
ness problem). However, in the system combina-
tion task, this is no longer an issue as the system
only needs to translate sentences within the data
set.
When more translation engines are available,
the size of this set becomes larger. Hence,
we collect translations from all available systems
and pair them with the corresponding input text,
thus forming a medium-sized ?hypothesis? cor-
pus. Our system starts processing this corpus
with a standard phrase-based SMT setup, using the
Moses toolkit (Koehn et al, 2007).
The hypothesis corpus is first tokenized and
lowercased. Then, we run GIZA++ (Och and
Ney, 2003) on the corpus to obtain word align-
ments in both directions. The phrases are extracted
from the intersection of the alignments with the
?grow? heuristics. In addition, we also generate
a reordering model with the default configuration
as included in the Moses toolkit. This ?hypothe-
sis? translation model can already be used by the
Moses decoder together with a language model to
perform translations over the corresponding sen-
tence set.
3.2 Integration into existing SMT system
Sometimes, the goal of system combination is not
only to produce a translation but also to improve
one of the systems. In this paper, we aim at incor-
porating the additional system outputs to improve
an out-of-domain SMT system trained on the Eu-
roparl corpus (Koehn, 2005). Our hope is that the
additional translation hypotheses could bring in
new phrases or, more generally, new information
that was not contained in the Europarl model. In
order to facilitate comparisons, we use in-domain
LMs for all setups.
We investigate two alternative ways of integrat-
ing the additional phrases into the existing SMT
system: One is to take the hypothesis translation
model described in Section 3.1, the other is to
construct system-specific models constructed with
only translations from one system at a time.
Although the Moses decoder is able to work
with two phrase tables at once (Koehn and
Schroeder, 2007), it is difficult to use this method
when there is more than one additional model.
The method requires tuning on at least six more
features, which expands the search space for the
translation task unnecessarily. We instead inte-
grate the translation models from multiple sources
by extending the phrase table. In contrast to the
prior approach presented in (Chen et al, 2007) and
(Eisele et al, 2008) which concatenates the phrase
tables and adds new features as system markers,
our extension method avoids duplicate entries in
the final combined table.
Given a set of hypothesis translation models
(derived from an arbitrary number of system out-
puts) and an original large translation model to be
improved, we first sort the models by quality (see
Section 3.3), always assigning the highest priority
to the original model. The additional phrase tables
are appended to the large model in sorted order
such that only phrase pairs that were never seen
before are included. Lastly, we add new features
(in the form of additional columns in the phrase ta-
ble) to the translation model to indicate each pair?s
origin.
3.3 System evaluation
Since both the system translations and the ref-
erence translations are available for the tuning
43
set, we first compare each output to the reference
translation using BLEU (Papineni et al, 2001)
and METEOR (Banerjee and Lavie, 2005) and a
combined scoring scheme provided by the ULC
toolkit (Gimenez and Marquez, 2008). In our ex-
periments, we selected a subset of 5 systems for
the combination, in most cases, based on BLEU.
On the other hand, some systems may be de-
signed in a way that they deliver interesting unique
translation segments. Therefore, we also measure
the similarity among system outputs as shown in
Table 2 in a given collection by calculating aver-
age similarity scores across every pair of outputs.
de-en fr-en es-en en-de en-fr en-es
Num. 20 23 28 15 16 9
Median 19.87 26.55 22.50 13.78 24.76 23.70
Range 16.37 17.06 9.74 4.75 11.05 13.94
Top 5 de-en fr-en es-en en-de en-fr en-es
Median 22.26 27.93 26.43 15.21 26.62 26.61
Range 4.31 4.76 5.71 1.71 0.68 5.56
Table 1: Statistics of system outputs? BLEU scores
The range of BLEU scores cannot indicate the
similarity of the systems. The direction with the
most systems submitted is Spanish-English but
their respective performances are very close to
each other. As for the selected subset, the English-
French systems have the most similar performance
in terms of BLEU scores. The French-English
translations have the largest range in BLEU but the
similarity in this group is not the lowest.
de-en fr-en es-en en-de en-fr en-es
All 34.09 46.48 61.83 31.74 44.95 38.11
Selected 36.65 56.16 56.06 33.92 52.78 57.25
Table 2: Similarity of the system outputs
Ideally, we should select systems with highest
quality scores and lowest similarity scores. For
German-English, we selected the three with the
highest METEOR scores and another two with
high METEOR scores but low similarity scores to
the first three. For the other language directions,
we chose five systems from different institutions
with the highest scores.
3.4 Language models
We use a standard n-gram language model for
each target language using the monolingual train-
ing data provided in the translation task. These
LMs are thus specific to the same domain as the
input texts. Moreover, we also generate ?hypoth-
esis? LMs solely based on the given system out-
puts, that is, LMs that model how the candidate
systems convey information in the target language.
These LMs do not require any additional training
data. Therefore, we do not require any training
data other than the given system outputs by using
the ?hypothesis? language model and the ?hypoth-
esis? translation model.
3.5 Tuning
After building the models, it is essential to tune
the SMT system to optimize the feature weights.
We use Minimal Error Rate Training (Och, 2003)
to maximize BLEU on the complete development
data. Unlike the standard tuning procedure, we do
not tune the final system directly. Instead, we ob-
tain the weights using models built from the tuning
portion of the system outputs.
For each combination variant, we first train
models on the provided outputs corresponding to
the tuning set. This system, called the tuning sys-
tem, is also tuned on the tuning set. The initial
weights of any additional features not included in
the standard setting are set to 0. We then adapt the
weights to the system built with translations cor-
responding to the test set. The procedure and the
settings for building this system must be identical
to that of the tuning system.
4 Experiments
The purpose of this exercise is to understand the
nature of the system combination task in prac-
tice. Therefore, we restrict ourselves to the train-
ing data and system translations provided by the
shared task. The types of the systems that pro-
duced the translations are assumed to be unknown.
We report results for six translation directions be-
tween four languages.
4.1 Data and baseline
We build an SMT system from release v4 of the
Europarl corpus (Koehn, 2005), following a stan-
dard routine using the Moses toolkit. The sys-
tem also includes 5-gram language models trained
on in-domain corpora of the respective target lan-
guages using SRILM (Stolcke, 2002).
The systems in this paper, including the base-
line, are all tuned on the same 501-sentence tuning
set. Note also that the provided n-best outputs are
excluded in our experiments.
44
4.2 Results
The experiments include three different setups for
direct system combination, involving only hypoth-
esis translation models. System S0, the baseline
for this group, uses a hypothesis translation model
built with all available system translations and a
hypothesis LM (also from the machine-generated
outputs). S1 differs from S0 in that the LM in S1 is
generated from a large news corpus. S2 consists of
translation models built with only the five selected
systems. The BLEU scores of these systems are
shown in Table 3.
de-en fr-en es-en en-de en-fr en-es
Top 1 21.16 30.91 28.54 14.96 26.55 27.84
Mean 17.29 23.78 21.39 12.76 22.96 21.43
S0 20.46 27.50 23.35 13.95 27.29 25.59
S1 21.76 28.05 25.49 15.16 27.70 26.09
S2 21.71 24.98 27.26 15.62 24.28 25.22
Table 3: BLEU scores of direct system combina-
tion
When all outputs are included, the combined
system can always produce translations better than
most of the systems. When only a hypothesis LM
is used, the BLEU scores are always higher than
the average BLEU scores of the outputs. It even
outperforms the top system for English-French.
This simple setup (S0) is certainly a feasible so-
lution when no additional data is available and no
system evaluation is possible. This approach ap-
pears to be more effective on typically difficult
language pairs that involve German.
As for the systems with normal language mod-
els, neither of the systems ensure better transla-
tions. The translation quality is not completely
determined by the number of included translations
and their quality. On the other hand, the output
set with higher diversity (Table 2) usually leads
to better combination results. This observation is
consistent with the results from the system inte-
gration experiments shown in Table 4.
de-en fr-en es-en en-de en-fr en-es
Bas 19.13 25.07 24.55 13.59 23.67 23.67
Med 17.99 24.56 20.70 13.19 24.19 22.12
All 21.40 28.00 27.75 15.21 27.20 26.41
Top5 21.70 26.01 28.53 15.52 27.87 27.92
Table 4: BLEU scores of integrated SMT systems
(Bas: Baseline, Med: Median)
There are two variants in our experiments on
system integration. All in Table 4 represents the
system that integrates the complete hypothesis
translation model with the Europarl model, while
Top 5 refers to the system that incorporates the five
system-specific models separately. Both setups re-
sult in an improvement over the baseline Europarl-
based SMT system. BLEU scores increase by up
to 4.25 points. The integrated SMT system some-
times produces translations better than the best
system (7 out of 12 cases).
5 Conclusion
This work uses the Moses toolkit to combine
translations from multiple engines in a simple way.
The experiments on six translation directions show
interesting results: The final translations are al-
ways better than the majority of the given systems,
while the combination performs better than the
best system in half the cases. A similar approach
was applied to improve an existing SMT system
which was built in a domain different from the test
task. We achieved improvements in all cases.
There are many possible future directions to
continue this work. As we have shown, the qual-
ity of the combined system is more related to the
diversity of the involved systems than to the num-
ber of the systems or their quality. Hand-picked
systems lead to better combinations than those se-
lected by BLEU scores. It would be interesting
to develop a more comprehensive system selection
strategy.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison.
2007. Multi-engine machine translation with an
open-source SMT decoder. In Proceedings of
45
WMT07, pages 193?196, Prague, Czech Republic,
June. Association for Computational Linguistics.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to integrate mul-
tiple rule-based machine translation engines into a
hybrid system. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 179?
182, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Jesus Gimenez and Lluis Marquez. 2008. A smor-
gasbord of features for automatic MT evaluation.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 195?198, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 98?107, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbs. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of Annual meet-
ing of the Association for Computation Linguis-
tics (acl), demonstration session, pages 177?180,
Prague, Czech, June.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit 2005.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33?40, Trento, Italy, April.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Antti-Veikko I. Rosti, Spyridon Matsoukas, and
Richard M. Schwartz. 2007. Improved word-level
system combination for machine translation. In
ACL.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
46
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 70?74,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Translation Combination using Factored Word Substitution
Christian Federmann1, Silke Theison2, Andreas Eisele1,2, Hans Uszkoreit1,2,
Yu Chen2, Michael Jellinghaus2, Sabine Hunsicker2
1: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
2: Universita?t des Saarlandes, Saarbru?cken, Germany
{cfedermann,eisele,uszkoreit}@dfki.de, {sith,yuchen,micha,sabineh}@coli.uni-sb.de
Abstract
We present a word substitution approach
to combine the output of different machine
translation systems. Using part of speech
information, candidate words are deter-
mined among possible translation options,
which in turn are estimated through a pre-
computed word alignment. Automatic
substitution is guided by several decision
factors, including part of speech, local
context, and language model probabili-
ties. The combination of these factors
is defined after careful manual analysis
of their respective impact. The approach
is tested for the language pair German-
English, however the general technique it-
self is language independent.
1 Introduction
Despite remarkable progress in machine transla-
tion (MT) in the last decade, automatic translation
is still far away from satisfactory quality. Even the
most advanced MT technology as summarized by
(Lopez, 2008), including the best statistical, rule-
based and example-based systems, produces out-
put rife with errors. Those systems may employ
different algorithms or vary in the linguistic re-
sources they use which in turn leads to different
characteristic errors.
Besides continued research on improving MT
techniques, one line of research is dedicated to bet-
ter exploitation of existing methods for the com-
bination of their respective advantages (Macherey
and Och, 2007; Rosti et al, 2007a).
Current approaches for system combination in-
volve post-editing methods (Dugast et al, 2007;
Theison, 2007), re-ranking strategies, or shal-
low phrase substitution. The combination pro-
cedure applied for this pape tries to optimize
word-level translations within a ?trusted? sentence
frame selected due to the high quality of its syntac-
tic structure. The underlying idea of the approach
is the improvement of a given (original) translation
through the exploitation of additional translations
of the same text. This can be seen as a simplified
version of (Rosti et al, 2007b).
Considering our submission from the shared
translation task as the ?trusted? frame, we add
translations from four additional MT systems that
have been chosen based on their performance in
terms of automatic evaluation metrics. In total, the
combination system performs 1,691 substitutions,
i.e., an average of 0.67 substitutions per sentence.
2 Architecture
Our system combination approach computes a
combined translation from a given set of machine
translations. Below, we present a short overview
by describing the different steps in the derivation
of a combined translation.
Compute POS tags for translations. We apply
part-of-speech (POS) tagging to prepare the
selection of possible substitution candidates.
For the determination of POS tags we use the
Stuttgart TreeTagger (Schmid, 1994).
Create word alignment. The alignment between
source text and translations is needed to
identify translation options within the differ-
ent systems? translations. Word alignment
is computed using the GIZA++ toolkit (Och
and Ney, 2003), only one-to-one word align-
ments are employed.
Select substitution candidates. For the shared
task, we decide to substitute nouns, verbs
and adjectives based on the available POS
tags. Initially, any such source word is con-
sidered as a possible substitution candidate.
As we do not want to require substitution can-
70
didates to have exactly the same POS tag as
the source, we use groups of ?similar? tags.
Compute decision factors for candidates. We
define several decision factors to enable an
automatic ranking of translation options.
Details on these can be found in section 4.
Evaluate the decision factors and substitute.
Using the available decision factors we
compute the best translation and substitute.
The general combination approach is language
independent as it only requires a (statistical) POS
tagger and GIZA++ to compute the word align-
ments. More advanced linguistic resources are not
required. The addition of lexical resources to im-
prove the extracted word alignments has been con-
sidered, however the idea was then dropped as we
did not expect any short-term improvements.
3 System selection
Our system combination engine takes any given
number of translations and enables us to compute
a combined translation out of these. One of the
given system translations is chosen to provide the
?sentence skeleton?, i.e. the global structure of the
translation, thus representing the reference system.
All other systems can only contribute single words
for substitution to the combined translation, hence
serve as substitution sources.
3.1 Reference system
Following our research on hybrid translation try-
ing to combine the strengths of rule-based MT
with the virtues of statistical MT, we choose our
own (usaar) submission from the shared task to
provide the sentence frame for our combination
system. As this translation is based upon a rule-
based MT system, we expect the overall sentence
structure to be of a sufficiently high quality.
3.2 Substitution sources
For the implementation of our combination sys-
tem, we need resources of potential substitution
candidates. As sources for possible substitution,
we thus include the translation results of the fol-
lowing four systems:
? Google (google)1
1The Google submission was translated by the Google
MT production system offered within the Google Language
Tools as opposed to the qualitatively superior Google MT
research system.
? University of Karlsruhe (uka)
? University of Maryland (umd)
? University of Stuttgart (stuttgart)
The decision to select the output of these par-
ticular MT systems is based on their performance
in terms of different automatic evaluation metrics
obtained with the IQMT Framework by (Gime?nez
and Amigo?, 2006). This includes BLEU, BLEU1,
TER, NIST, METEOR, RG, MT06, and WMT08.
The results, listing only the three best systems per
metric, are given in table 1.
metric best three systems
BLEU1 google uka systran
0.599 0.593 0.582
BLEU google uka umd
0.232 0.231 0.223
TER umd rwth.c3 uka
0.350 0.335 0.332
NIST google umd uka
6.353 6.302 6.270
METEOR google uka stuttgart
0.558 0.555 0.548
RG umd uka google
0.527 0.525 0.520
MT06 umd google stuttgart
0.415 0.413 0.410
WMT08 stuttgart rbmt3 google
0.344 0.341 0.336
Table 1: Automatic evaluation results.
On grounds of these results we anticipate the
four above named translation engines to perform
best when being combined with our hybrid ma-
chine translation system. We restrict the substi-
tution sources to the four potentially best systems
in order to omit bad substitutions and to reduce
the computational complexity of the substitution
problem. It is possible to choose any other num-
ber of substitution sources.
4 Substitution
As mentioned above, we consider nouns, verbs
and adjectives as possible substitution candidates.
In order to allow for automatic decision making
amongst several translation options we define a set
of factors, detailed in the following. Furthermore,
we present some examples in order to illustrate the
use of the factors within the decision process.
71
4.1 Decision factors
The set of factors underlying the decision proce-
dure consists of the following:
A: Matching POS. This Boolean factor checks
whether the target word POS tag matches the
source word?s POS category. The factor com-
pares the source text to the reference trans-
lation as we want to preserve the sentential
structure of the latter.
B: Majority vote. For this factor, we compute
an ordered list of the different translation op-
tions, sorted by decreasing frequency. A con-
sensus between several systems may help to
identify the best translation.
Both the reference system and the Google
submission receive a +1 bonus, as they ap-
peared to offer better candidates in more
cases within the small data sample of our
manual analysis.
C: POS context. Further filtering is applied de-
termining the words? POS context. This is
especially important as we do not want to de-
grade the sentence structure maintained by
the translation output of the reference system.
In order to optimize this factor, we conduct
trials with the single word, the ?1 left, and
the +1 right context. To reduce complex-
ity, we shorten POS tags to a single character,
e.g. NN ? N or NPS ? N .
D: Language Model. We use an English lan-
guage model to score the different translation
options. As the combination system only re-
places single words within a bi-gram context,
we employ the bi-gram portion of the English
Gigaword language model.
The language model had been estimated us-
ing the SRILM toolkit (Stolcke, 2002).
4.2 Factor configurations
To determine the best possible combination of our
different factors, we define four potential factor
configurations and evaluate them manually on a
small set of sentences. The configurations differ
in the consideration of the POS context for factor
C (strict including ?1 left context versus relaxed
including no context) and in the usage of factor A
Matching POS (+A). Table 2 shows the settings of
factors A and C for the different configurations.
configuration Matching POS POS context
strict disabled ?1 left
strict+A enabled ?1 left
relaxed disabled single word
relaxed+A enabled single word
Table 2: Factor configurations for combination.
Our manual evaluation of the respective substi-
tution decisions taken by different factor combi-
nation is suggestive of the ?relaxed+A? configura-
tion to produce the best combination result. Thus,
this configuration is utilized to produce sound
combined translations for the complete data set.
4.3 Factored substitution
Having determined the configuration of the dif-
ferent factors, we compute those for the complete
data set, in order to apply the final substitution step
which will create the combined translation.
The factored substitution algorithm chooses
among the different translation options in the fol-
lowing way:
(a) Matching POS? If factor A is activated for
the current factor configuration (+A), sub-
stitution of the given translation options can
only be possible if the factor evaluates to
True. Otherwise the substitution candidate is
skipped.
(b) Majority vote winner? If the majority vote
yields a unique winner, this translation option
is taken as the final translation.
Using the +1 bonuses for both the reference
system and the Google submission we intro-
duce a slight bias that was motivated by man-
ual evaluation of the different systems? trans-
lation results.
(c) Language model. If several majority vote
winners can be determined, the one with the
best language model score is chosen.
Due to the nature of real numbers this step
always chooses a winning translation option
and thus the termination of the substitution
algorithm is well-defined.
Please note that, while factors A, B, and D are
explicitly used within the substitution algorithm,
factor C POS context is implicitly used only when
computing the possible translation options for a
given substitution candidate.
72
configuration substitutions ratio
strict 1,690 5.714%
strict+A 1,347 4.554%
relaxed 2,228 7.532%
relaxed+A 1,691 5.717%
Table 3: Substitutions for 29,579 candidates.
Interestingly we are able to obtain best results
without considering the ?1 left POS context, i.e.
only checking the POS tag of the single word
translation option for factor C.
4.4 Combination results
We compute system combinations for each of the
four factor configurations defined above. Table
3 displays how many substitutions are conducted
within each of these configurations.
The following examples illustrate the perfor-
mance of the substitution algorithm used to pro-
duce the combined translations.
?Einbruch?: the reference translation for ?Ein-
bruch? is ?collapse?, the substitution sources
propose ?slump? and ?drop?, but also ?col-
lapse?, all three, considering the context,
forming good translations. The majority vote
rules out the suggestions different to the ref-
erence translation due to the fact that 2 more
systems recommend ?collapse? as the correct
translation.
?Ru?ckgang?: the reference system translates this
word as ?drop? while all of the substitution
sources choose ?decline? as the correct trans-
lation. Since factor A evaluates to True, i.e.
the POS tags are of the same nature, ?de-
cline? is clearly selected as the best transla-
tion by factor B Majority vote and thus re-
places ?drop? in the final combined transla-
tion result.
?Tagesgescha?fte?: our reference system trans-
lates ?Tagesgescha?fte? with ?requirements?,
while two of the substitution systems indi-
cate ?business? to be a better translation. Due
to the +1 bonus for our reference translation
a tie between the two possible translations
emerges, leaving the decision to the language
model score, which is higher for ?business?.
4.5 Evaluation results
Table 4 shows the results of the manual evaluation
campaign carried out as part of the WMT09 shared
task. Randomly chosen sentences are presented
to the annotator, who then has to put them into
relative order. Note that each annotator is shown a
random subset of the sentences to be evaluated.
system relative rank data points
google -2.74 174
uka -3.00 217
umd -3.03 170
stuttgart -2.89 163
usaar -2.78 186
usaar-combo -2.91 164
Table 4: Relative ranking results from the WMT09
manual evalution campaign.
Interestingly, our combined system is not able
to outperform the baseline, i.e., additional data
did not improve translation results. However the
evaluation is rather intransparent since it does not
allow for a strict comparison between sentences.
5 Conclusion
Within the system described in this paper, we ap-
proach a hybrid translation technique combining
the output of different MT systems. Substituting
particular words within a well-structured transla-
tion frame equips us with considerably enhanced
translation output. We obtain promising results
providing substantiated proof that our approach is
going in the right direction.
Further steps in the future will include machine
learning methods to optimize the factor selection.
This was, due to limited amount of time and data,
not feasible thus far. We will also investigate the
potential of phrase-based substitution taking into
account multi-word alignments instead of just sin-
gle word mappings. Additionally, we would like
to continue work on the integration of lexical re-
sources to post-correct the word alignments ob-
tained by GIZA++ as this will directly improve the
overall system performance.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
73
References
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
220?223, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
framework for automatic machine translation eval-
uation. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC?06).
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):1?49.
Wolfgang Macherey and Franz J. Och. 2007. An em-
pirical study on computing consensus translations
from multiple machine translation systems. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 986?995, Prague, Czech Republic,
June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007a. Combining outputs from multiple
machine translation systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 228?235, Rochester, New York, April.
Association for Computational Linguistics.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system
combination for machine translation. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 312?319,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, September.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
Silke Theison. 2007. Optimizing rule-based machine
translation output with the help of statistical meth-
ods. Master?s thesis, Saarland University, Computa-
tional Linguistics department.
74
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 77?81,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Further Experiments with Shallow Hybrid MT Systems
Christian Federmann1, Andreas Eisele1, Hans Uszkoreit1,2,
Yu Chen1, Sabine Hunsicker1, Jia Xu1
1: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
2: Universita?t des Saarlandes, Saarbru?cken, Germany
{cfedermann,eisele,uszkoreit,yuchen,sabine.hunsicker,jia.xu}@dfki.de
Abstract
We describe our hybrid machine trans-
lation system which has been developed
for and used in the WMT10 shared task.
We compute translations from a rule-
based MT system and combine the re-
sulting translation ?templates? with par-
tial phrases from a state-of-the-art phrase-
based, statistical MT engine. Phrase sub-
stitution is guided by several decision
factors, a continuation of previous work
within our group. For the shared task,
we have computed translations for six lan-
guage pairs including English, German,
French and Spanish. Our experiments
have shown that our shallow substitu-
tion approach can effectively improve the
translation result from the RBMT system;
however it has also become clear that a
deeper integration is needed to further im-
prove translation quality.
1 Introduction
In recent years the quality of machine translation
(MT) output has improved greatly, although each
paradigm suffers from its own particular kind of
errors: statistical machine translation (SMT) of-
ten shows poor syntax, while rule-based engines
(RBMT) experience a lack in vocabulary. Hybrid
systems try to avoid these typical errors by com-
bining techniques from both paradigms in a most
useful manner.
In this paper we present the improved version of
the hybrid system we developed last year?s shared
task (Federmann et al, 2009). We take the out-
put from an RBMT engine as basis for our hybrid
translations and substitute noun phrases by trans-
lations from an SMT engine. Even though a gen-
eral increase in quality could be observed, our sys-
tem introduced errors of its own during the substi-
tution process. In an internal error analysis, these
degradations were classified as follows:
- the translation by the SMT engine is incorrect
- the structure degrades through substitution
(because of e.g. capitalization errors, double
prepositions, etc.)
- the phrase substitution goes astray (caused by
alignment problems, etc.)
Errors of the first class cannot be corrected, as
we have no way of knowing when the translation
by the SMT engine is incorrect. The other two
classes could be eliminated, however, by introduc-
ing additional steps for pre- and post-processing
as well as improving the hybrid algorithm itself.
Our current error analysis based on the results of
this year?s shared task does not show these types
of errors anymore.
Additionally, we extended our coverage to also
include the language pairs English?French and
English?Spanish in both directions as well as
English?German, compared to last year?s initial
experiments for German?English only. We were
able to achieve an increase in translation quality
for this language set, which shows that the substi-
tution method works for different language config-
urations.
2 Architecture
Our hybrid translation system takes translation
output from a) the Lucy RBMT system (Alonso
and Thurmair, 2003) and b) a Moses-based SMT
system (Koehn et al, 2007). We then identify
noun phrases inside the rule-based translation and
compute the most likely correspondences in the
statistical translation output. For these, we apply a
factored substitution method that decides whether
the original RBMT phrase should be kept or rather
be replaced by the Moses phrase. As this shallow
substitution process may introduce problems at
77
phrase boundaries, we afterwards perform several
post-processing steps to cleanup and finalize the
hybrid translation result. A schematic overview
of our hybrid system and its main components is
given in figure 1.
Figure 1: Schematic overview of the hybrid MT
system architecture.
2.1 Input to the Hybrid System
Lucy RBMT System We obtain the translation
as well as linguistic structures from the RBMT
system. An internal evaluation has shown that
these structures are usually of a high quality which
supports our initial decision to consider the RBMT
output as an appropriate ?template? for our hybrid
translation approach. The Lucy translation output
can include additional markup that allows to iden-
tify unknown words or other, local phenomena.
The Lucy system is a transfer-based MT system
that performs translation in three phases, namely
analysis, transfer, and generation. Intermediate
tree structures for each of the translation phases
can be extracted from the Lucy system to guide
the hybrid system. Sadly, only the 1-best path
through these three phases is given, so no alterna-
tive translation possibilities can be extracted from
the given data; a fact that clearly limits the poten-
tial for more deeply integrated hybrid translation
approaches. Nevertheless, the availability of the
1-best trees already allows to improve the transla-
tion quality of the RBMT system as we will show
in this paper.
Moses SMT System We used a state-of-the-art
Moses SMT system to create statistical phrase-
based translations of our input text. Moses has
been modified so that it returns the translation re-
sults together with the bidirectional word align-
ments between the source texts and the transla-
tions. Again, we make use of markup which helps
to identify unknown words as these will later guide
the factored substitution method. Both of the
translation models and the language models within
our SMT systems were only trained with lower-
cased and tokenized Europarl training data. The
system used sets of feature weights determined us-
ing data sets also from Europarl (test2008). In
addition, we used LDC gigaword corpus to train
large scale n-gram language models to be used in
our hybrid system. We tokenized the source texts
using the standard tokenizers available from the
shared task website. The SMT translations are re-
cased before being fed into the hybrid system to-
gether with the word alignment information.The
hybrid system can easily be adapted to support
other statistical translation engines. If the align-
ment information is not available, a suitable align-
ment tool would be necessary to compute it as the
alignment is a key requirement for the hybrid sys-
tem.
2.2 Aligning RBMT and SMT Output
We compute alignment in several components of
the hybrid system, namely:
source-text-to-tree: we first find an alignment
between the source text and the correspond-
ing analysis tree(s). As Lucy tends to sub-
divide large sentences into several smaller
units, it sometimes becomes necessary to
align more than one tree structure to a given
source sentence.
analysis-transfer-generation: for each of the
analysis trees, we re-construct the path from
its tree nodes, via the transfer tree, and their
corresponding generation tree nodes.
tree-to-target-text: similarly to the first align-
ment process, we find a mapping between
generation tree nodes and the actual transla-
tion output of the RBMT system.
source-text-to-tokenized: as the Lucy RBMT
system works on non-tokenized input text
and our Moses system takes tokenized input,
78
we need to align the source text to its tok-
enized form.
Given the aforementioned alignments, we can then
correlate phrases from the rule-based translation
with their counterparts from the statistical trans-
lation, both on source or target side. As our
hybrid approach relies on the identification of
such phrase pairs, the computation of the different
alignments is critical to obtain good combination
performance.
Please note that all these tree-based alignments
can be computed with a very high accuracy. How-
ever, due to the nature of statistical word align-
ment, the same does not hold for the alignment
obtained from the Moses system. If the alignment
process has produced erroneous phrase tables, it is
very likely that Lucy phrases and their ?aligned?
SMT matches simply will not fit. Or put the other
way round: the better the underlying SMT word
alignment, the greater the potential of the hybrid
substitution approach.
2.3 Factored Substitution
Given the results of the alignment process, we can
then identify ?interesting? phrases for substitution.
Following our experimental setup from last year?s
shared task, we again decided to focus on noun
phrases as these seem to be best-suited for in-place
swapping of phrases. Our initial assumption is that
SMT phrases are better on a lexical level, hence
we aim to replace Lucy?s noun phrases by their
Moses counterparts.
Still, we want to perform the substitution in a
controlled manner in order to avoid problems or
non-matching insertions. For this, we have (man-
ually) derived a set of factors that are checked for
each of the phrase pairs that are processed. The
factors are described briefly below:
identical? simply checks whether two candidate
phrases are identical.
too complex? a Lucy phrase is ?too complex?
to substitute if it contains more than 2
embedded noun phrases.
many-to-one? this factor checks if a Lucy phrase
containing more than one word is mapped to
a Moses phrase with only one token.
contains pronoun? checks if the Lucy phrase
contains a pronoun.
contains verb? checks if the Lucy phrase con-
tains a verb.
unknown? checks whether one of the phrases is
marked as ?unknown?.
length mismatch computes the number of words
for both phrases and checks if the absolute
difference is too large.
language model computes language model
scores for both phrases and checks which is
more likely according to the LM.
All of these factors have been designed and ad-
justed during an internal development phase using
data from previous shared tasks.
2.4 Post-processing Steps
After the hybrid translation has been computed,
we perform several post-processing steps to clean
up and finalize the result:
cleanup first, we perform basic cleanup opera-
tions such as whitespace normalization, cap-
italizing the first word in each sentence, etc.
multi-words then, we take care of proper han-
dling of multi-word expressions. Using the
tree structures from the RBMT system we
eliminate superfluous whitespace and join
multi-words, even if they were separated in
the SMT phrase.
prepositions finally, we give prepositions a spe-
cial treatment. Experience from last year?s
shared task had shown that things like double
prepositions contributed to a large extent to
the amount of avoidable errors. We tried to
circumvent this class of error by identifying
the correct prepositions; erroneous preposi-
tions are removed.
3 Hybrid Translation Analysis
We evaluated the intermediate outputs using
BLEU (Papineni et al, 2001) against human refer-
ences as in table 3. The BLEU score is calculated
in lower case after the text tokenization. The trans-
lation systems compared are Moses, Lucy, Google
and our hybrid system with different configura-
tions:
Hybrid: we use the language model with case
information and substitute some NPs in Lucy
outputs by Moses outputs.
Hybrid LLM: same as Hybrid but we use a
larger language model.
79
Table 1: Intermediate results of BLEU[%] scores for WMT10 shared task.
System de?en en?de fr?en en?fr es?en en?es
Moses 18.32 12.66 22.26 20.06 24.28 24.72
Lucy 16.85 12.38 18.49 17.61 21.09 20.85
Google 25.64 18.51 28.53 28.70 32.77 32.20
Hybrid 17.29 13.05 18.92 19.58 22.53 23.55
Hybrid LLM 17.37 13.73 18.93 19.76 22.61 23.66
Hybrid SG 17.43 14.40 19.67 20.55 24.37 24.99
Hybrid NCLM 17.38 14.42 19.56 20.55 24.41 24.92
Hybrid SG: same as Hybrid but the NP substitu-
tions are based on Google output instead of
Moses translations.
Hybrid NCLM: same as Hybrid but we use the
language model without case information.
We participated in the translation evaluation in
six language pairs: German to English (de?en),
English to German (en?de), French to English
(fr?en), English to French (en?fr), Spanish to
English (es?en) and English to Spanish (en?es).
As shown in table 3, the Moses translation sys-
tem achieves better results overall than the Lucy
system does. Google?s system outperforms other
systems in all language pairs. The hybrid transla-
tion as described in section 2 improves the Lucy
translation quality with a BLEU score up to 2.7%
absolutely.
As we apply a larger language model or a lan-
guage model without case information, the trans-
lation performance can be improved further. One
major problem in the hybrid translation is that the
Moses outputs are still not good enough to replace
the Lucy outputs, therefore we experimented on
a hybrid translation of Google and Lucy systems
and substitute some unrelaible NP translations by
the Google?s translations. The results in the line
of ?Hybrid SG? shows that the hybrid translation
quality can be enhanced if the translation system
where we select substitutions is better.
4 Internal Evaluation of Results
In the analysis of the remaining issues, the fol-
lowing main sources of problems can be distin-
guished:
- Lucy?s output contains structural errors that
cannot be fixed by the chosen approach.
- Lucy results contain errors that could have
been corrected by alternative expressions
from SMT, but the constraints in our system
were too restrictive to let that happen.
- The SMT engine we use generates subopti-
mal results that find their way into the hybrid
result.
- SMT results that are good are incorporated
into the hybrid results in a wrong way.
We have inspected a part of the results and classi-
fied the problems according to these criteria. As
this work is still ongoing, it is too early to report
numerical results for the relative frequencies of the
different causes of the error. However, we can
already see that three of these four cases appear
frequently enough to justify further attention. We
observed several cases in which the parser in the
Lucy system was confused by unknown expres-
sions and delivered results that could have been
significantly improved by a more robust parsing
approach. We also encountered several cases in
which an expression from SMT was used although
the original Lucy output would have been better.
Also we still observe problems finding to correct
correspondences between Lucy output and SMT
output, which leads to situations where material is
inserted in the wrong place, which can lead to the
loss of content words in the output.
5 Conclusion and Outlook
In our contribution to the shared task we have ap-
plied the hybrid architecture from (Federmann et
al., 2009) to six language pairs. We have identi-
fied and fixed many of the problems we had ob-
served last year, and we think that, in addition to
the increased coverage in laguage pairs, the overall
quality has been significantly increased.
However, in the last section we characterized
three main sources of problems that will require
further attention. We will address these problems
in the near future in the following way:
80
1. We will investigate in more detail the align-
ment issue that leads to occasional loss of
content words, and we expect that a careful
inspection and correction of the code will in
all likelihood give us a good remedy.
2. The problem of picking expressions from the
SMT output that appear more probable to the
language model although they are inferior to
the original expression from the RBMT sys-
tem is more difficult to fix. We will try to find
better thresholds and biases that can at least
reduce the number of cases in which this type
of degradation happen.
3. Finally, we will also address the robustness
issue that leads to suboptimal structures from
the RBMT engine caused by parsing failures.
Our close collaboration with Lucy enables us to
address these issues in a very effective way via the
inspection and classification of intermediate struc-
tures and, if these structures indicate parsing prob-
lems, the generation of variants of the input sen-
tence that facilitate correct parsing.
Acknowledgments
This work was supported by the EuroMatrixPlus
project (IST-231720) which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme. The authors want to thank Michael
Jellinghaus and Bastian Simon for help with the
inspection of intermediate results and classifica-
tion of errors.
References
Juan A. Alonso and Gregor Thurmair. 2003. The Com-
prendium Translator system. In Proc. of the Ninth
MT Summit.
Christian Federmann, Silke Theison, Andreas Eisele,
Hans Uszkoreit, Yu Chen, Michael Jellinghaus, and
Sabine Hunsicker. 2009. Translation combination
using factored word substitution. In Proceedings of
the Fourth Workshop on Statistical Machine Transla
tion, pages 70?74, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL Demo and Poster Sessions, pages 177?
180, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report
RC22176(W0109-022), IBM.
81
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 351?357,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Stochastic Parse Tree Selection for an Existing RBMT System
Christian Federmann
DFKI GmbH
Language Technology Lab
Saarbru?cken, Germany
cfedermann@dfki.de
Sabine Hunsicker
DFKI GmbH
Language Technology Lab
Saarbru?cken, Germany
sabine.hunsicker@dfki.de
Abstract
In this paper we describe our hybrid machine
translation system with which we participated
in the WMT11 shared translation task for the
English?German language pair. Our system
was able to outperform its RBMT baseline and
turned out to be the best-scored participating
system in the manual evaluation. To achieve
this, we extended an existing, rule-based MT
system with a module for stochastic selection
of analysis parse trees that allowed to better
cope with parsing errors during the system?s
analysis phase. Due to the integration into the
analysis phase of the RBMT engine, we are
able to preserve the benefits of a rule-based
translation system such as proper generation
of target language text. Additionally, we used
a statistical tool for terminology extraction to
improve the lexicon of the RBMT system.
We report results from both automated metrics
and human evaluation efforts, including exam-
ples which show how the proposed approach
can improve machine translation quality.
1 Introduction
Rule-based machine translation (RBMT) systems
that employ a transfer-based translation approach,
highly depend on the quality of their analysis phase
as it provides the basis for its later processing
phases, namely transfer and generation. Any parse
failures encountered in the initial analysis phase will
proliferate and cause further errors in the following
phases. Very often, bad translation results can be
traced back to incorrect analysis trees that have been
computed for the respective input sentences. Hence-
forth, any improvements that can be achieved for
the analysis phase of a given RBMT system directly
lead to improved translation output which makes this
an interesting topic in the context of hybrid MT.
In this paper we present a study how the rule-
based analysis phase of a commercial RBMT system
can be supplemented by a stochastic parser. The
system under investigation is the rule-based engine
Lucy LT. This software uses a sophisticated RBMT
transfer approach with a long research history; it is
explained in more detail in (Alonso and Thurmair,
2003).
The output of its analysis phase is a parse forest
containing a small number of tree structures. For
our hybrid system we investigated if the existing rule
base of the Lucy LT system chooses the best tree
from the analysis forest and how the selection of this
best tree out of the set of candidates can be improved
by adding stochastic knowledge to the rule-based
system.
The remainder of this paper is structured in the
following way: in Section 2 we first describe the
transfer-based architecture of the rule-based Lucy
LT engine, giving special focus to its analysis phase
which we are trying to optimize. Afterwards,
we provide details on the implementation of the
stochastic selection component, the so-called ?tree
selector? which allows to integrate knowledge from
a stochastic parser into the analysis phase of the
rule-based system. Section 3 reports on the results
of both automated metrics and manual evaluation
efforts, including examples which show how the
proposed approach has improved or degraded MT
quality. Finally, we conclude and provide an outlook
on future work in this area.
351
S$
$
CLS
CLS
NP
NO
PRN
They
ADVP
ADVB
ADV
also
PRED
VB
VST
were
VB
VST
protesting
PP
PREPP
PREP
against
NP
AP
A
AST
bad
NO
NO
NST
pay
NO
NST
conditions
CONJP
CONJ
and
CLS
NP
NO
PRN
They
PRED
VB
VST
alleged
NP
NO
NST
persecution
$
PNCT
.
Figure 1: Original analysis tree from the rule-based MT system
2 System Architecture
2.1 Lucy LT Architecture
The Lucy LT engine is a renowned RMBT system
which follows a ?classical?, transfer-based machine
translation approach. The system first analyses the
given source sentence creating a forest of several
analysis parse trees. One of these parse trees is then
selected (as ?best? analysis) and transformed in the
transfer phase into a tree structure from which the
target text (i.e. the translation) can be generated.
It is clear that any errors that occur during the
initial analysis phase proliferate and cause negative
side effects on the outcome of the final translation
result. As the analysis phase is thus of very special
importance, we have investigated it in more detail.
The Lucy LT analysis consists of several phases:
1. The input is tokenised with regards to the
system?s source language lexicon.
2. The resulting tokens undergo a morphological
analysis, which is able to identify possible
combinations of allomorphs for a token.
3. This leads to a chart which forms the basis for
the actual parsing, using a head-driven strat-
egy1. Special handling is performed for the
analysis of multi-word expressions and also for
verbal framing.
At the end of the analysis, there is an extra phase
named phrasal analysis which is called whenever
1grammar formalism + number of rules
the grammar was not able to construct a legal con-
stituent from all the elements of the input. This hap-
pens in several different scenarios:
? The input is ungrammatical according to the
LT analysis grammar.
? The category of the derived constituent is not
one of the allowed categories.
? A grammatical phenomenon in the source
sentence is not covered.
? There are missing lexical entries for the input
sentence.
During the phrasal analysis, the LT engine collects
all partial trees and greedily constructs an overall in-
terpretation of the chart. Based on our findings from
many experiments with the Lucy LT engine, phrasal
analyses are performed for more than 40% of the
sentences from our test sets and very often result in
bad translations.
Each resulting analysis parse tree, independent
of whether it is a grammatical or a result from the
phrasal analysis, is also assigned an integer score by
the grammar. The tree with the highest score is then
handed over to the transfer phase, thus pre-defining
the final translation output.
2.2 The ?Tree Selector?
An initial evaluation of the translation quality based
on the tree selection of the analysis phase showed
that there is potential for improvement. The integer
score assigned by the analysis grammar provides a
352
S$
$
CLS
NP
NO
PRN
They
ADVP
ADVB
ADV
also
PRED
VB
VST
were
VB
VST
protesting
PP
PREPP
PREP
against
NP
NP
AP
A
AST
bad
NO
NO
NST
pay
NO
NST
conditions
CONJP
CONJ
and
NP
AP
A
AST
alleged
NO
NST
persecution
$
PNCT
.
Figure 2: Improved analysis tree resulting from stochastic parse selection
good indication of which trees lead to good transla-
tions, as is depicted in Table 1. Still, in many cases
an alternative tree would have lead to a better trans-
lation.
As additional feature, we chose to use the tree
edit distance of each analysis candidate to a stochas-
tic parse tree. An advantage of stochastic parsing
lies in the fact that parsers from this class can deal
very well even with ungrammatical or unknown out-
put, which we have seen is problematic for a rule-
base parser. We decided to make use of the Stanford
Parser as described in (Klein and Manning, 2003),
which uses an unlexicalised probabilistic context-
free grammar that was trained on the Penn Tree-
bank2. We parse the original source sentence with
this PCFG grammar to get a stochastic parse tree that
can be compared to the trees from the Lucy analysis
forest.
In our experiments, we compare the stochastic
parse tree with the alternatives given by Lucy LT.
Tree comparison is implemented based on the Tree
Edit Distance, as originally defined in (Zhang and
Shasha, 1989). In analogy to the Word Edit or Lev-
2Further experiments with different grammars are currently
on-going.
Best Analysis Tree Percentage
Default (id=1) 42 (61.76%)
Alternative (id=2-7) 26 (38.24%)
Table 1: Evaluation of Analysis Forests
enshtein Distance, the distance between two trees
is the number of editing actions that are required to
transform the first tree into the second tree. The Tree
Edit Distance knows three actions:
? Insertion
? Deletion
? Renaming (substitution in Levenshtein Distance)
Since the Lucy LT engine uses its own tag set,
a mapping between this proprietary and the Penn
Treebank tag set was created. Our implementation,
called ?Tree Selector? uses a normalised version of
the Tree Edit Distance to estimate the quality of the
trees from the Lucy analysis forest, possibly over-
riding the analysis decision taken by the unmodified
RBMT engine. The integration of the Tree Selector
has been possible by using an adapted version of the
rule-based MT system which allowed to communi-
cate the selection result from our external process to
the Lucy LT kernel which would then load the re-
spective parse tree for all further processing steps.
2.3 LiSTEX Terminology Extraction
The LiSTEX extension of the Lucy RBMT engine
allows to improve the system?s lexicon; the approach
is described in more detail in (Federmann et al,
2011). To extend the lexicon, terminology lists are
extracted from parallel corpora. These lists are then
enriched with linguistic information, such as part-of-
speech tag, internal structure of multi-word expres-
353
sions and frequency. For English and German, about
26,000 terms were imported using this procedure.
2.4 Named Entity Handling
Named entities are often handled incorrectly and
wrongly translated, such as George Bush? George
Busch. To reduce the frequency of such errors, we
added a pre- and post-processing modules to deal
with named entities. Before translation, the input
text is scanned for named entities. We use both
HeiNER (Wolodja Wentland and Hartung (2008))
and the OpenNLP toolkit3. HeiNER is a dictionary
containing named entities extracted from Wikipedia.
This provides us with a wide range of well-translated
entities. To increase the coverage, we also use the
named entity recogniser in OpenNLP. These entities
have to be translated using the RBMT engine. We
save the named entity translations and insert place-
holders for all NEs. The modified text is translated
using the hybrid set-up described above. After the
translation is finished, the placeholders are replaced
by their respective translations.
3 Evaluation
3.1 Shared Task Setup
For the WMT11 shared translation task, we submit-
ted three different runs of our hybrid MT system:
1. Hybrid Transfer (without the Tree Selector, but
with the extended lexicon)
2. Full Hybrid (with both the Tree Selector and
the extended lexicon)
3. Full Hybrid+Named Entities (full hybrid and
named entity handling)
Our primary submission was run #3. All three runs
were evaluated using BLEU (Papineni et al (2001))
and TER (Snover et al (2006)). The results from
these automated metrics are reported in Table 2.
Table 2: Automatic metric scores for WMT11
System BLEU TER
Hybrid Transfer 13.4 0.792
Full Hybrid 13.1 0.796
Full Hybrid+Named Entities 12.8 0.800
3http://incubator.apache.org/opennlp/
Table 3 shows that we were able to outperform
the original Lucy version. Furthermore, it turned out
that our hybrid system was the best-scoring system
from all shared task participants.
Table 3: Manual evaluation scores for WMT11
System Normalized Score
Full Hybrid+Named Entities 0.6805
Original Lucy 0.6599
3.2 Error Analysis
The selection process following the decision factors
as explained in Section 2.2 may fail due to wrong
assumptions in two areas:
1. The tree with the lowest distance does not
result in the best translation.
2. There are several trees associated with the low-
est distance, but the tree with the highest score
does not result in the best translation.
To calculate the error rate of the Tree Selector, we
ran experiments on the test set of the WMT10 shared
task and evaluated a sample of 100 sentences with
regards to translation quality. To do so, we created
all seven possible translations for each of the phrasal
analyses and checked whether the Tree Selector re-
turned a tree that led to exactly this translation. In
case it did not, we investigated the reasons for this.
Sentences for which all trees created the same trans-
lation were skipped. This sample contains both
examples in which the translation changed and in
which the translation stayed the same.
Table 4 shows the error rate of the Tree Selector
while Table 5 contains the error analysis. As one
can see, the optimal tree was chosen for 56% of the
sentences. We also see that the minimal tree edit
distance seems to be a good feature to use for com-
parisons, as it holds for 71% of the trees, including
those examples where the best tree was not scored
highest by the LT engine. This also means that addi-
tional features for choosing the tree out of the group
of trees with the minimal edit distance are required.
Even for the 29% of sentences, in which the opti-
mal tree was not chosen, little quality was lost: in
75.86% of those cases, the translations didn?t change
354
Best Translation Returned 56%
Other Translation Returned 44%
Best Tree has Minimal Edit Distance 71%
Best Tree has Higher Distance 29%
Table 4: Error Rate of the Tree Selector
at all (obviously the trees resulted in equal transla-
tion output). In the remaining cases the translations
were divided evenly between slight degradations and
and equal quality.
Other Translation: Selected Tree
Tree 1 (Default) 31
Tree 2-7 (Alternatives) 13
Reasons for Selection
Source contained more than 50 tokens 16
Time-out before best tree is reached 13
Chosen tree had minimal distance 15
Table 5: Evaluation of Tree Selector Errors
In the cases when the best tree was not chosen,
the first tree (which is the default tree) was selected
in 70.45% . This is due to a combinations of ro-
bustness factors that are implemented in the RBMT
system and have been beyond our control in the ex-
periments. The LT engine has several different indi-
cators which may throw a time-out exception, if, for
example, the analysis phase takes too long to pro-
duce a result. To avoid getting time-out errors, only
sentences with up to 50 tokens are treated with the
Tree Selector. Additionally the Tree Selector itself
checks the processing time and returns intermediate
results, if this limit is reached. This ensures that we
receive a proper translation for all sentences.4
3.3 Examples
Using our stochastic selection component, we are
able to fix errors which can be found in translation
output generated by the original Lucy engine.
Table 6 shows several examples including source
text, reference text, and translations from both the
original Lucy engine (A) and our hybrid system (B).
We will briefly discuss our observations for these
examples in the following section.
4We are currently working on eliminating this time-out issue
as it prevents us from driving our approach to its full potential.
1. Translation A is the default translation. The
parse tree for this translation can be seen in
Figure 1. Here the adjective alleged is wrongly
parsed as a verb. By contrast, Figure 2 shows
the tree selected by our hybrid implementation,
which contains the correct analysis of alleged
and results in a correct translation.
2. Word order is improved in the Example 2.
3. Lexical items are associated with a domain area
in the lexicon of the rule-based system. Items
that are contained within a different domain
than the input text are still accessible, but items
in the same domain are preferred. In Exam-
ple 3, this may lead to the incorrect disam-
biguation of multi-word expressions: the trans-
lation of to blow up as in die Luft fliegen was
not preferred in Translation A due to the cho-
sen domain and a more superficial translation
was chosen. This problem is fixed in Transla-
tion B. Our system chose a tree leading to the
correct idiomatic translation.
4. Something similar happens in Example 4
where the choice of preposition is improved.
5. These changes remain at a rather local scope,
but we also have instances where the sentence
improves globally: Example 5 illustrates this
well. In translation A, the name of the book,
?After the Ice?, has been moved to an entirely
different place in the sentence, removing it
from its original context.
6. The same process can be observed in Exam-
ple 6, where the translation of device was
moved from the main clause to the sub clause
in Translation A.
7. An even more impressive example is Exam-
ple 7. Here, translation A was not even a gram-
matically correct sentence. This is due to the
heuristics of the Lucy engine, although they
could also create a correct translation B.
These examples show that our initial goal of
improving the given RMBT system has been
reached and that a hybrid MT system with an
architecture similar to what we have described in
this paper does in fact perform quite well.
355
Table 6: Translation Examples for Original (A) and Improved (B) Lucy
1 Source: They were also protesting against bad pay conditions and alleged persecution.
Reference: Sie protestierten auch gegen die schlechten Zahlungsbedingungen und angebliche Schikanen.
Translation A: Sie protestierten auch gegen schlechte Soldbedingungen und behaupteten Verfolgung.
Translation B: Sie protestierten auch gegen schlechte Soldbedingungen und angebliche Verfolgung.
2 Source: If the finance minister can?t find the money elsewhere, the project will have to be aborted and
sanctions will be imposed, warns Janota.
Reference: Sollte der Finanzminister das Geld nicht anderswo finden, mu?sste das Projekt gestoppt wer-
den und in diesem Falle kommen Sanktionen, warnte Janota.
Translation A: Wenn der Finanzminister das Geld nicht anderswo finden kann, das Projekt abgebrochen
werden mu?ssen wird und Sanktionen auferlegt werden werden, warnt Janota.
Translation B: Wenn der Finanzminister das Geld nicht anderswo finden kann, wird das Projekt abgebrochen
werden mu?ssen und Sanktionen werden auferlegt werden, warnt Janota.
3 Source: Apparently the engine blew up in the rocket?s third phase.
Reference: Vermutlich explodierte der Motor in der dritten Raketenstufe.
Translation A: Offenbar blies der Motor hinauf die dritte Phase der Rakete in.
Translation B: Offenbar flog der Motor in der dritten Phase der Rakete in die Luft.
4 Source: As of January, they should be paid for by the insurance companies and not compulsory.
Reference: Ab Januar soll diese von den Versicherungen bezahlt und freiwillig sein.
Translation A: Ab Januar sollten sie fu?r von den Versicherungsgesellschaften und nicht obligatorisch bezahlt
werden.
Translation B: Ab Januar sollten sie von den Versicherungsgesellschaften und nicht obligatorisch gezahlt
werden.
5 Source: In his new book, ?After the Ice?, Alun Anderson, a former editor of New Scientist, offers
a clear and chilling account of the science of the Arctic and a gripping glimpse of how the
future may turn out there.
Reference: In seinem neuen Buch ?Nach dem Eis? (Originaltitel ?After the Ice?) bietet Alun Anderson,
ein ehemaliger Herausgeber des Wissenschaftsmagazins ?New Scientist?, eine klare und be-
unruhigende Beschreibung der Wissenschaft der Arktis und einen packenden Einblick, wie
die Zukunft sich entwickeln ko?nnte.
Translation A: In seinem neuen Buch bietet Alun Anderson, ein fru?herer Redakteur von Neuem Wis-
senschaftler, ?Nach dem Eis? einen klaren und kalten Bericht u?ber die Wissenschaft der
Arktis und einen spannenden Blick davon an, wie die Zukunft sich hinaus dort drehen kann.
Translation B: In seinem neuen Buch, ?Nach dem Eis?, bietet Alun Anderson, ein fru?herer Redakteur von
Neuem Wissenschaftler, einen klaren und kalten Bericht u?ber die Wissenschaft der Arktis
und einen spannenden Blick davon an, wie die Zukunft sich hinaus dort drehen kann.
6 Source: If he does not react, and even though the collision is unavoidable, the device exerts the maxi-
mum force to the brakes to minimize damage.
Reference: Falls der Fahrer nicht auf die Warnung reagiert und sogar wenn der Zusammenstoss schon
unvermeidlich ist, u?bt der Bremsassistent den maximalen Druck auf die Bremsen aus, um auf
diese Weise die Scha?den so gering wie mo?glich zu halten.
Translation A: Wenn er nicht reagiert, und das Gera?t auch wenn der Zusammensto? unvermeidlich ist, die
gro??tmo?gliche Kraft zu den Bremsen ausu?bt, um Schaden zu bagatellisieren.
Translation B: Wenn er nicht reagiert, und auch wenn der Zusammensto? unvermeidlich ist, u?bt das Gera?t
die gro??tmo?gliche Kraft zu den Bremsen aus, um Schaden zu bagatellisieren.
7 Source: For the second year, the Walmart Foundation donated more than $150,000 to purchase, and
transport the wreaths.
Reference: Die Walmart-Stiftung spendete zum zweiten Mal mehr als 150.000 Dollar fu?r Kauf und
Transport der Kra?nze.
Translation A: Fu?r das zweite Jahr, die Walmart-Gru?ndung, mehr gespendet al $150,000, um die Kra?nze zu
kaufen, und zu transportieren.
Translation B: Fu?r das zweite Jahr spendete die Walmart-Gru?ndung mehr als $150,000, um die Kra?nze zu
kaufen, und zu transportieren.
356
4 Conclusion and Outlook
The analysis phase proves to be crucial for the over-
all quality of the translation in rule-based machine
translation systems. Our hybrid approach indicates
that it is possible to improve the analysis results
of such a rule-based engine by a better selection
method of the trees created by the grammar. Our
evaluation shows that the selection itself is no trivial
task, as our initial experiments deliver results of
varying quality. The degradations we have observed
in our own manual evaluation can be fixed by a
more fine-grained selection mechanism, as we al-
ready know that better trees exist, i.e. the default
translations.
While the work reported on in this paper is a
dedicated extension of a specific rule-based machine
translation system, the overall approach can be used
with any transfer-based RBMT system. Future work
will concentrate on the circumvention of e.g. the
time-out errors that prevented a better performance
of the stochastic selection module. Also, we will
more closely investigate the issue of decreased trans-
lation quality and experiment with other decision
factors that may help to alleviate the negative effects.
The LiSTEX module provides us with high qual-
ity entries for the lexicon, increasing the coverage
of the lexicon and fluency of the translation. As a
side-effect, the new terms also help to reduce parsing
errors, as formerly unknown multiword expressions
are now properly recognised and treated. Further
work is being carried out to increase the precision
of the extracted terminology lists.
The addition of stochastic knowledge into an
existing rule-based machine translation system is
an example of a successful, hybrid combination of
different MT paradigms into a joint system. Our
system turned out to be the winning system for
the English?German language pair of the WMT11
shared task.
Acknowledgements
The work described in this paper was supported
by the EuroMatrixPlus project (IST-231720) which
is funded by the European Community under the
Seventh Framework Programme for Research and
Technological Development.
References
Juan A. Alonso and Gregor Thurmair. 2003. The Com-
prendium Translator system. In Proceedings of the
Ninth Machine Translation Summit.
Christian Federmann, Sabine Hunsicker, Petra Wolf, and
Ulrike Bernardi. 2011. From statistical term extrac-
tion to hybrid machine translation. In Proceedings of
the 15th Annual Conference of the European Associa-
tion for Machine Translation.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the ACL, pages 423?430.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report
RC22176(W0109-022), IBM.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
Carina Silberer Wolodja Wentland, Johannes Knopp and
Matthias Hartung. 2008. Building a multilingual lexi-
cal resource for named entity disambiguation, transla-
tion and transliteration. In European Language Re-
sources Association (ELRA), editor, Proceedings of
the Sixth International Language Resources and Eval-
uation (LREC?08), Marrakech, Morocco, may.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related prob-
lems. SIAM J. Comput., 18:1245?1262, December.
357
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 312?316,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Machine Learning for Hybrid Machine Translation
Sabine Hunsicker
DFKI GmbH
Language Technology Lab
Saarbru?cken, Germany
sabine.hunsicker@dfki.de
Chen Yu
DFKI GmbH
Language Technology Lab
Saarbru?cken, Germany
yu.chen@dfki.de
Christian Federmann
DFKI GmbH
Language Technology Lab
Saarbru?cken, Germany
cfedermann@dfki.de
Abstract
We describe a substitution-based system for
hybrid machine translation (MT) that has been
extended with machine learning components
controlling its phrase selection. The approach
is based on a rule-based MT (RBMT) system
which creates template translations. Based
on the rule-based generation parse tree and
target-to-target algnments, we identify the set
of ?interesting? translation candidates from
one or more translation engines which could
be substituted into our translation templates.
The substitution process is either controlled by
the output from a binary classifier trained on
feature vectors from the different MT engines,
or it is depending on weights for the decision
factors, which have been tuned using MERT.
We are able to observe improvements in terms
of BLEU scores over a baseline version of the
hybrid system.
1 Introduction
In recent years, machine translation (MT) systems
have achieved increasingly better translation quality.
Still each paradigm has its own challenges: while
statistical MT (SMT) systems suffer from a lack of
grammatical structure, resulting in ungrammatical
sentences, RBMT systems have to deal with a lack
of lexical coverage. Hybrid architectures intend to
combine the advantages of the individual paradigms
to achieve an overall better translation.
Federmann et al (2010) and Federmann and Hun-
sicker (2011) have shown that using a substitution-
based approach can improve the translation quality
of a baseline RBMT system. Our submission to
WMT12 is a new, improved version following these
approaches. The output of an RBMT engine serves
as our translation backbone, and we substitute noun
phrases by translations mined from other systems.
2 System Architecture
Our hybrid MT system combines translation output
from:
a) the Lucy RBMT system, described in more
detail in (Alonso and Thurmair, 2003);
b) the Linguatec RBMT system (Aleksic and
Thurmair, 2011);
c) Moses (Koehn et al, 2007);
d) Joshua (Li et al, 2009).
Lucy provides us with the translation skeleton,
which is described in more detail in Section 2.2
while systems b)?d) are aligned to this translation
template and mined for substitution candidates. We
give more detailed information on these systems in
Section 2.3.
2.1 Basic Approach
We first identify ?interesting? phrases inside the
rule-based translation and then compute the most
probable correspondences in the translation output
from the other systems. For the resulting phrases,
we apply a factored substitution method that decides
whether the original RBMT phrase should be kept or
rather be replaced by one of the candidate phrases.
A schematic overview of our hybrid system and its
main components is given in Figure 1.
312
Figure 1: Schematic overview of the architecture of our
substitution-based, hybrid MT system.
In previous years, it turned out that the alignment
of the candidate translations to the source contained
too many errors. In this version of our system, we
thus changed the alignment method that connects the
other translations. Only the rule-based template is
aligned to the source. As we make use of the Lucy
RBMT analysis parse trees, this alignment is very
good. The other translations are now connected to
the rule-based template using a confusion network
approach. This also reduces computational efforts,
as we now can compute the substitution candidates
directly from the template without detouring over
the source. During system training and tuning, this
new approach has resulted in a reduced number of
erroneous alignment links.
Additionally, we also changed our set of decision
factors, increasing their total number. Whereas an
older version of this system only used four factors,
we now consider the following twelve factors:
1. frequency: frequency of a given candidate
phrase compared to total number of candidates
for the current phrase;
2. LM(phrase): language model (LM) score of
the phrase;
3. LM(phrase+1): phrase with right-context;
4. LM(phrase-1): phrase with left-context;
5. Part-of-speech match?: checks if the part-of-
speech tags of the left/right context match the
current candidate phrase?s context;
6. LM(pos) LM score for part-of-speech (PoS);
7. LM(pos+1) PoS with right-context;
8. LM(pos-1) PoS with left-context;
9. Lemma checks if the lemma of the candidate
phrase fits the reference;
10. LM(lemma) LM score for the lemma;
11. LM(lemma+1) lemma with right-context;
12. LM(lemma-1) lemma with left-context.
The language model was trained using the SRILM
toolkit (Stolcke, 2002), on the EuroParl (Koehn,
2005) corpus, and lemmatised or part-of-speech
tagged versions, respectively. We used the Tree-
Tagger (Schmid, 1994) for lemmatisation as well as
part-of-speech tagging.
The substitution algorithm itself was also adapted.
We investigated two machine learning approaches.
In the previous version, the system used a hand-
written decision tree to perform the substitution:
1. the first of the two new approaches consisted
of machine learning this decision tree from
annotated data;
2. the second approach was to assign a weight to
each factor and using MERT tuning of these
weights on a development set.
Both approaches are described in more detail later in
Section 2.4.
2.2 Rule-Based Translation Templates
The Lucy RBMT system provides us with parse tree
structures for each of the three phases of its transfer-
based translation approach: analysis, transfer and
generation. Out of these structures, we can extract
linguistic phrases which later represent the ?slots?
for substitution. Previous work has shown that these
structures are of a good grammatical quality due to
the grammar Lucy uses.
313
2.3 Substitution Candidate Translations
Whereas in our previous work, we solely relied on
candidates retrieved from SMT systems, this time
we also included an additional RBMT system into
the architecture. Knowing that statistical systems
make similar errors, we hope to balance out this fact
by exploiting also a system of a different paradigm,
namely RBMT.
To create the statistical translations, we used state-
of-the-art SMT systems. Both our Moses and Joshua
systems were trained on the EuroParl corpus and
News Commentary1 training data. We performed
tuning on the ?newstest2011? data set using MERT.
We compile alignments between translations
with the alignment module of MANY (Barrault,
2010). This module uses a modified version of
TERp (Snover et al, 2009) and a set of different
costs to create the best alignment between any two
given sentences. In our case, each single candidate
translation is aligned to the translation template that
has been produced by the Lucy RBMT system. As
we do not use the source in this alignment tech-
nique, we can use any translation system, regardless
of whether this system provides us with a source-to-
target algnment.
In earlier versions of this system, we compiled the
source-to-target algnments for the candidate trans-
lations using GIZA++ (Och and Ney, 2003), but
these alignments contained many errors. By using
target-to-target algnments, we are able to reduce the
amount of those errors which is, of course, preferred.
2.4 Substitution Approaches
Using the parse tree structures provided by Lucy, we
extract ?interesting? phrases for substitution. This
includes noun phrases of various complexity, then
simple verb phrases consisting of only the main
verb, and finally adjective phrases. Through the
target-to-target algnments we identify and collect
the set of potential substitution candidates. Phrase
substitution can be performed using two methods.
2.4.1 Machine-Learned Decision Tree
Previous work used hand-crafted rules. These are
now replaced by a classifier which was trained on
annotated data. Our training set D can formally be
1Available at http://www.statmt.org/wmt12/
represented as
D = {(xi, yi)|xi ? Rp, yi ? {?1, 1}}ni=1 (1)
where each xi represents the feature vector for some
sentence i while the yi value contains the annotated
class information. We use a binary classification
scheme, simply defining 1 as ?good? and ?1 as
?bad? translations.
In order to make use of machine (ML) learn-
ing methods such as decision trees (Breiman et al,
1984), Support Vector Machines (Vapnik, 1995),
or the Perceptron (Rosenblatt, 1958) algorithm, we
have to prepare our training set with a sufficiently
large amount of annotated training instances.
To create the training data set, we computed the
feature vectors and all possible substitution candi-
dates for the WMT12 ?newstest2011? development
set. Human annotators were then given the task to
assign to each candidate whether it was a ?good? or
a ?bad? substitution. We used Appraise (Federmann,
2010) for the annotation, and collected a set of
24,996 labeled training instances with the help of six
human annotators. Table 1 gives an overview of the
data sets characteristics. The decision tree learned
from this data replaces the hand-crafted rules.
2.4.2 Weights Tuned with MERT
Another approach we followed was to assign
weights to the chosen decision factors and to use
Minimal Error Rate Training to get the best weights.
Using the twelve factors described in Section 2.1,
we assign uniformly distributed weights and create
n-best lists. Each n-best lists contains a total of
n+2 hypotheses, with n being the number of candi-
date systems. It contains the Lucy template trans-
lations, the hybrid translation using the best can-
didates as well as a hypothesis for each candidate
system. In the latter translation, each potential can-
didate for substitution is selected and replaces the
original sub phrase in the baseline. The n-best list is
Translation Candidates
Total ?good? ?bad?
Count 24,996 10,666 14,330
Table 1: Training data set characteristics
314
Hybrid Systems Baseline Systems
Baseline +Decision Tree +MERT Lucy Linguatec Joshua Moses
BLEU 13.9 14.2 14.3 14.0 14.7 14.6 15.9
BLEU-cased 13.5 13.8 13.9 13.7 14.2 13.5 14.9
TER 0.776 0.773 0.768 0.774 0.775 0.772 0.774
Table 2: Experimental results for all component and hybrid systems applied to the WMT12 ?newstest2012? test set
data for language pair English?German.
sorted by the final score of the feature vectors mak-
ing up each hypothesis. We used Z-MERT (Zaidan,
2009) to optimise the set of feature weights on the
?newstest2011? development set.
3 Evaluation
Using the ?newstest2012? test set, we created base-
line translations for the four MT systems used in our
hybrid system. Then we performed three runs of our
hybrid system:
a) a baseline run, using the factors and uniformly
distributed weights;
b) a run using the weights trained on the develop-
ment set;
c) a run using the decision tree learned from an-
notated data.
Table 2 shows the results for automatic metrics?
scores. Besides BLEU (Papineni et al, 2001), we
also report its case-sensitive variant, BLEU-cased,
and TER (Snover et al, 2006) scores.
Comparing the scores, we see that both advanced
hybrid methods perform better than the original,
baseline hybrid as well as the Lucy baseline system.
The MERT approach performs slightly better than
the decision tree. This proves that using machine-
learning to adapt the substitution approach results in
better translation quality.
Other baseline systems, however, still outperform
the hybrid systems. In part this is due to the fact that
we are preserving the basic structure of the RBMT
translation and do not reorder the new hybrid trans-
lation. To improve the hybrid approach further, there
is more research required.
4 Conclusion and Outlook
In this paper, we have described how machine-
learning approaches can be used to improve the
phrase substitution component of a hybrid machine
translation system.
We reported on two different approaches, the first
using a binary classifier learned from annotated data,
and the second using feature weights tuned with
MERT. Both systems achieved improved automatic
metrics? scores on the WMT12 ?newstest2012? test
set for the language pair English?German.
Future work will have to investigate ways how to
achieve a closer integration of the individual base-
line translations. This might be done by also taking
into account reordering of the linguistic phrases as
shown in the tree structures. We will also need to
examine the differences between the classifier and
MERT approach, to see whether we can integrate
them to improve the selection process even further.
Also, we have to further evaluate the machine
learning performance via, e.g., cross-validation-
based tuning, to improve the prediction rate of the
classifier model. We intend to explore other machine
learning techniques such as SVMs as well.
Acknowledgments
This work has been funded under the Seventh
Framework Programme for Research and Techno-
logical Development of the European Commission
through the T4ME contract (grant agreement no.:
249119). It was also supported by the EuroMatrix-
Plus project (IST-231720). We are grateful to the
anonymous reviewers for their valuable feedback.
Special thanks go to Herve? Saint-Amand for help
with fixing the automated metrics scores.
315
References
Vera Aleksic and Gregor Thurmair. 2011. Personal
Translator at WMT 2011. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
303?308, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Juan A. Alonso and Gregor Thurmair. 2003. The Com-
prendium Translator System. In Proceedings of the
Ninth Machine Translation Summit.
Lo??c Barrault. 2010. MANY : Open Source Machine
Translation System Combination. Prague Bulletin
of Mathematical Linguistics, Special Issue on Open
Source Tools for Machine Translation, 93:147?155,
January.
L. Breiman, J. Friedman, R. Olshen, and C. Stone. 1984.
Classification and Regression Trees. Wadsworth and
Brooks, Monterey, CA.
Christian Federmann and Sabine Hunsicker. 2011.
Stochastic parse tree selection for an existing rbmt sys-
tem. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 351?357, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Christian Federmann, Andreas Eisele, Yu Chen, Sabine
Hunsicker, Jia Xu, and Hans Uszkoreit. 2010. Fur-
ther experiments with shallow hybrid mt systems. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 77?81,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Christian Federmann. 2010. Appraise: An Open-Source
Toolkit for Manual Phrase-Based Evaluation of Trans-
lations. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of ACL Demo and Poster Sessions, pages
177?180. Association for Computational Linguistics,
June.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
MT Summit 2005.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An Open
Source Toolkit for Parsing-Based Machine Transla-
tion. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation, pages 135?139, Athens,
Greece, March. Association for Computational Lin-
guistics.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Multi-
ple Machine Translation Systems Using Enhanced Hy-
potheses Alignment. In Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 33?40, Stroudsburg, PA, USA, April. Asso-
ciation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic Eval-
uation of Machine Translation. IBM Research Report
RC22176(W0109-022), IBM.
F. Rosenblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psychological Review, 65:386?408.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, UK.
Toby Segaran. 2007. Programming Collective In-
telligence: Building Smart Web 2.0 Applications.
O?Reilly, Beijing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with
a Tunable MT Metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation at the
12th Meeting of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL-2009),
Athens, Greece, March.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 257?286, November.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer, New York.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88, January.
316
