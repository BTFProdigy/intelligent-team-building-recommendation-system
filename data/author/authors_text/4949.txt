A niche at the nexus: situating an NLP curriculum interdisciplinarily
Deryle Lonsdale
Department of Linguistics
Brigham Young University
Provo, UT, USA, 84602
lonz@byu.edu
Abstract
This paper discusses the establishment and
implementation of a curriculum for teach-
ing NLP. At the core are two classes which
involve some theoretical background, ex-
tensive hands-on experience with state-of-
the-art technologies, and practical applica-
tion in the form of an intensive program-
ming project. Issues involving interdis-
ciplinary coordination, curriculum design,
and challenges in teaching this discipline
are discussed.
1 Institutional context
Our university initiated teaching of an NLP curricu-
lum in 1998. At the core are two classes which
both include some theoretical background, extensive
hands-on experience with state-of-the-art technolo-
gies, and practical application in the form of an in-
tensive programming project. They are meant to be
comparable in quality and scope to the best NLP
courses taught elsewhere. Each semester that these
classes were taught, the university administered an
anonymous survey to students to gauge their sat-
isfaction with the course and its contents. Issues
involving interdisciplinary coordination, curriculum
design, student satisfaction, and challenges unique to
teaching this discipline are presented in this paper.
First, though, necessary relevant background on the
campus and student demographics is presented.
The university is one of the largest private univer-
sities in the U.S. with an enrollment of about 30,000
(with 2,000 graduate students). Uncharacteristically,
almost three-fourths of the student body speaks a
non-English language.
The College of Humanities houses Linguistics,
Philosophy, and several language and literature de-
partments. Over 60 languages are taught within the
college to students from all disciplines. The Linguis-
tics Department offers undergraduate and master de-
grees (but no Ph.D.); over 150 undergraduate majors
are currently enrolled. The richness of the linguistic
environment in this college has a great bearing on the
results discussed in this paper. The College of Physi-
cal and Mathematical Sciences houses the Computer
Science (CS) Department (and several others teach-
ing the ?hard? sciences). CS faculty strive to meet
the demands of flourishing undergrad and graduate
programs, and by necessity offer courses primarily
in the core areas of CS.
Until 1998 there was almost no interaction
between the CS and Linguistics departments, no
courses were taught in NLP or CL, and no research
brought together students across college boundaries.
Recently, however, the situation has improved. For
several reasons, NLP classes were initiated at the
university in 1998: faculty hires into Linguistics
held such interests, campus research projects needed
students with this type of experience, an NLP
infrastructure was necessary for internal work of the
university and its sponsor, and the improvement of
job prospects for Linguistics students was targeted.
Creating learning opportunities in NLP would help
on all of these accounts. This paper discusses the
two new NLP classes that have been developed and
taught during the last four years in response to these
                     July 2002, pp. 46-53.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
needs.
Both courses are referred to as NLP classes rather
than as CL classes. One commonly expressed dis-
tinction between the two areas is that CL is more the-
oretical and cognitive in orientation, whereas NLP
has a more engineering and applied focus1. By this
criterion the two classes discussed in this paper are
clearly NLP-oriented.
2 NLP for the humanities
For several years a program called Computers and
the Humanities (CHum) in the College of Human-
ities has provided a computer background for hu-
manities undergrads; this is necessary because CS
courses have traditionally been unavailable to non-
CS majors. The CHum track can lead to a minor
specialization for any humanities degree and attracts
students from all corners of the college. The CHum
offerings include a dozen classes meant to take stu-
dents from the most basic contexts (computer end-
user) through computer research tools use, basic pro-
gramming, and programming projects. More re-
cently the programming language taught has been
Visual Basic (though see below). In the last ten years
enrollment in this popular program has increased
tenfold.
Computer support for humanities students is laud-
able. Several labs are available, supplied with good
computational resources and adequate supervision
by student consultants. Many students also choose
to use their own computers at home or at work to
fulfill assignments. Occasionally off-campus com-
puter use for homework is not possible when propri-
etary software or corpora acquired by the university
for classroom use cannot be distributed off-campus.
2.1 A parsing class
In recent years CHum course content has involved
increasingly more language processing. Twice
now, an intermediate-level class has been taught in
natural-language parsing and grammar engineering.
The class was taught the Unix and Emacs environ-
ments, basic LISP programming, and lexicon and
phrase-structure grammar development techniques;
all of these were new areas of focus for a CHum
1See, for example, Hans Uszkoreit?s website at
www.coli.uni-sb.de/~hansu/what is cl.html.
class. One text was used for this class (Allen,
1995), and the associated parsers available from its
author were actively used by the students in their
coursework. The textbook was followed rather
closely and was quite accessible even to humanities
students at this level. Instruction involved two
80-minute lecture classes per week and was given in
a technologically enhanced (TELE) classroom fea-
turing an ethernet-connected instructional computer,
overhead LCD projection, multimedia devices, and
a touch-sensitive projection wall (smart-screen).
Lectures were delivered primarily via PowerPoint,
running parsing engines, and accessing commercial
and research project websites for materials and
discussion.
CHum parsing course evaluations were very good
to excellent (around 5.5-6.0 on a 7-point scale), ex-
ceeding college and department means across-the-
board in 30 categories. Lowest (i.e. average satis-
faction) ratings went to the text book used. The only
critical comments mentioned a desire for more in-
class lab time (a carry-over from prerequisite classes
which are held in computer lab classrooms with
a computer for every student). Whereas in lower
classes the focus was on learning particular program-
ming constructs, the parsing class content was more
abstract and required less classroom-time program-
ming demonstrations.
2.2 A speech programming class
Though the parsing class was popular and success-
ful, it has not been taught in the last year and a
half. Instead, the intermediate-level CHum class has
focused on teaching speech applications program-
ming, again to meet infrastructure and pedagogical
needs. In this class2 the first third of the semester in-
cludes intensive instruction in phonetics, phonology,
and speech phenomena (using (J.M.Pickett, 1999)),
as well as in TCL programming. The balance of
the semester involves instruction on manipulating
a speech toolkit, developing and leveraging associ-
ated data resources (lexicons and corpora, phrase-
structure grammars, discourse moves), and under-
standing applications implementation (file formats,
speaker adaptation, interaction, agents, v-commerce,
speech translation).
2See humanities.byu.edu/classes/chum387dl/homepage.html.
Sample homework assignments include: running,
writing, and debugging simple TCL programs; de-
veloping and extending speech corpora and lexicons;
manipulating speech data with toolkits (e.g. PRAAT
(Boersma and Weenink, 1996) and SFS3); extending
a simple recognizer; and creating new speech-based
dialogue scenarios. For the latter tasks the OGI
toolkit (Cole, 1999) has proven particularly helpful,
and the students find the environment stimulating,
interesting, and versatile enough for a wide array of
experimentation from both the linguistics and toolkit
programming sides.
A final programming project is required; its de-
liverables include a proposal, interim report on
progress, final presentation and class demonstration,
and a final write-up including commented source
code and a paper discussing related work, the ap-
proach taken, and results. Sample final projects in-
clude speech interfaces for ubiquitous task-specific
access (e.g. intelligent kitchen appliances, automo-
bile navigation, and large XML-marked document
access), a call-center application for a dentist?s of-
fice, and a spoken-language identifier.
2.3 Future CHum prospects
Computers and the Humanities course offerings are
dynamic and flexible, and more NLP-related content
is being offered beyond the parsing and speech tech-
nologies classes already mentioned. For example,
currently an advanced seminar class is (for the first
time) teaching XML and the annotation of linguistic
data.
It should be noted that recent CHum efforts have
been attracting CS students as well, who don?t cur-
rently have an outlet for NLP-related coursework in
their home department. This introduces a slight chal-
lenge for the instructor since an asymmetry exists
between humanities and CS students? programming
abilities. To date, though, this has not proved very
problematic since the CS students were willing to
rely on their own initiative to apply classroom con-
cepts to more complex programming projects than
their humanities peers could attempt. Presumably in
the future if this becomes a problem a speech class
might be offered in the CS department, or a in higher
section in CHum.
3See www.phon.ucl.ac.uk/resource/sfs/.
An important and very recent development will
ensure further strengthening of NLP-related courses
within the College of Humanities. With the intro-
duction of a new campus-wide interdisciplinary un-
dergraduate minor in Computing and Information
(CpIn), the CS department has secured several new
tenure-track slots to be hosted externally by depart-
ments across campus. The College of Humani-
ties has been allocated one of these slots, and the
new faculty member will be housed within the Lin-
guistics Department. More course offerings at the
CS/Linguistics nexus will be possible in the near fu-
ture as a result. In turn, these classes will serve as
electives for CpIn students.
3 An advanced NLP course
The linchpin of the NLP curriculum is the advanced
NLP course4. Hosted in the Linguistics department,
it also has been cross-listed as a CS course. It
is intended primarily for graduate students, though
it is open to advanced undergrads with requisite
background. Proficiency is assumed in at least one
programming language; in addition, a background
in algorithms, data structures, and some basic dis-
crete math is also required. Linguistics students and
CHum students with a solid background in linguis-
tics and good programming skills are accepted with
the instructor?s approval.
3.1 Course goals and student demographics
The course?s goals are: to teach how computational
techniques are used successfully in various areas of
NLP; to demonstrate by hands-on experience how to
use NLP tools in the performance of linguistically in-
teresting tasks; to demonstrate the application of a
novel, nontrivial approach to solving some aspect of
NLP-related computation; and to read, understand,
and assess current research literature and trends in
the relevant areas. The class is by design very broad
in scope, trying to address as many areas of NLP
as possible in a semester. The breadth of coverage
entails that the depth in any one area cannot be ad-
dressed fully; still, each topic is addressed at some
nontrivial level of detail. The topics are sequenced
in such a way as to build upon previously introduced
topics. For example, units on part-of-speech tagging
4See humanities.byu.edu/classes/ling581dl/homepage.html.
and lexicons precede those on parsing, which in turn
precede those on speech understanding.
The class has been taught four times so far, with
an average of ten students per semester (plus inter-
ested faculty who sat in on classes without regis-
tering). Each class had an equal three-way balance
of students from CS, Linguistics, and other areas of
campus (physics, engineering, and even modern lan-
guages). Half of the students are undergrads and
half are graduates. Without exception, every student
had knowledge of at least one non-English language.
One of the challenges, but also unique opportunities,
of this class is to bring their disparate backgrounds
together in class discussions. For example class dis-
cussion, homework assignments, and final projects
often center around the students? linguistic knowl-
edge and their application of principles learned to the
processing of non-English languages.
3.2 Course content
Materials: Class lectures, discussions, and demon-
strations are based primarily on the content of two
NLP texts (Manning and Schu?tze, 1999; Cole et al,
1997) and several supplementary readings from the
Web5. The class is held thrice weekly in one-hour
sessions; it too is held in a TELE room. Each student
is required to ?adopt? a lecture topic from the cur-
riculum: researching intensively this particular field,
preparing a lecture in consultation with the instruc-
tor, and teaching the class. Often students choose an
area that reflects the strengths of their background,
and as a result their lectures, materials and discus-
sions are of high quality.
Coursework: Students are generally free to do
their homework in any of the labs on campus or
on their own machines elsewhere. In some cases,
however, this is not possible due to licensing con-
straints on software needed for work in the course:
several resources require that the data or programs
only be used on an on-campus computer licensed to
the CS and/or Linguistics departments. For this rea-
son a Unix server has been acquired by the Linguis-
tics department and set up with the requisite software
to act as a classwork project server. Students can
also access the machine remotely to do their work
5Particularly useful are researchers? personal and project
pages worldwide, the ACL NLP/CL Universe, and the arXiv
Computation and Language archive.
within these constraints. Students from the CS de-
partment have access to CS and Linguistics servers
where class-related resources can be used. Students
also have access to the campus supercomputer when
necessary for NLP projects, under the instructor?s
supervision.
Sample non-trivialhands-on and programming as-
signments are given weekly. They include such
topics as: work with various corpus manipulation
and annotation tools, use of various POS taggers
and their comparison (Brill, 1992; Tufis and Mason,
1998), development of morphophonological rules
in PC-Kimmo (Antworth, 1990), understanding and
manipulating content from WordNet databases (Fell-
baum, 1998), aligning bitext, using and evaluating
a machine translation system, developing a phrase-
structure grammar for syntactic and then semantic
chart parsing, experimenting with information re-
trieval, working with a speech toolkit to develop a
simple application, or developing knowledge for a
text generation engine (Tomita and Nyberg, 1988).
Tutorials are provided for for any necessary remedial
work that the student might need or desire in such
topics as using the Emacs editor, using Unix shell
scripts, or writing Perl or Tcl scripts.
Final project: A final programming project is
required, similar in scope to that described above
for the humanities course: close coordination with
the instructor, meeting milestones, documenting and
demonstrating the final product, and producing a
write-up of the significance and contributions of the
result. Of course, a much higher standard is required
of these advanced students. The student is free to
choose any relevant project, the programming lan-
guage(s) to be used, and the theoretical approach to
be taken. Sample final projects cover almost as wide
a range of topics as those covered in the curriculum6.
Linguisticsstudentsoften focus on the programmatic
development of knowledge sources whereas CS stu-
dents tend to engineer large-scale integrations of sev-
eral components in novel ways to address multi-
faceted issues. The most common tendency with all
students is to scope their work a little too ambitiously
at first; close consultation with the instructor is cru-
cial throughout the process. Teamwork is permitted,
and often a Linguistics student will pair up with a CS
6See humanities.byu.edu/classes/ling581dl/egnlpprojs.htm.
one; this usually results in good knowledge and skill
transfer for both parties.
Evaluations: A three-hour (non-programming)
final exam is given which tests a knowledge of con-
cepts, algorithms, tools, procedures, and approaches
learned throught the semester. Class evaluation rat-
ings by students have improved over time, from
very good (5.1/7.0, first time offered) to exceptional
(6.7/7.0, last semester). The most frequent com-
plaints concerned amount of background that the
textbook assumes, and the lack of a midterm exami-
nation to help students gauge their progress.7
4 Other courses
The infrastructure developed for teaching the two
courses mentioned above has also been successfully
applied in other classes as well. This section explains
how other classes have benefited from the NLP in-
frastructure being put in place.
A linguistics major undergrad survey course cov-
ers all of the core areas of linguistics (phonet-
ics, phonology, morphology, syntax, semantics, and
pragmatics) as well as several application areas.
Interestingly, one chapter of the textbook used in
this class even contains a very cursory overview of
computational linguistics (Klavans, 2001). Several
already-mentioned tools supporting the NLP classes
have also been used in the undergrad survey class: a
speech toolkit for sound wave manipulation, Word-
Net for lexical semantics, and a morphology engine
for English.
The Linguistics department offers a translation
theory and practice class, which traditionally attracts
up to 40 students with advanced capabilities in as
many as 25 languages per class section. With the
NLP infrastructure recently developed, more techno-
logical exercises have been added to the curriculum
involving WordNet, bitext alignment, corpus and
lexicography tools, software localization (l10n) and
internationalization (i18n), machine-assisted trans-
lation, and machine translation systems (standalone
and web-based).
Other Linguistics classes also have recently lever-
aged the NLP infrastructure: a graduate seman-
tics class uses WordNet, a grad phonology class
7The instructor as a general rule does not give graduate
classes midterms; this is being rethought for the NLP class be-
cause of student comment.
works with a speech toolkit, a grad morphology class
uses a morphology engine, and a language modeling
seminar uses machine learning and other exemplar-
based methods. In the CS department, a series of
two 90-minute lectures in l10n and i18n has been
developed and is regularly presented to the grad
class in human-computer interation. Finally, sev-
eral foreign-language classes outside of the Linguis-
tics/CS area have used recently-assembled tools such
as corpora, part-of-speech taggers, and WordNets in
their own instruction and student assignments.
5 Extracurricular opportunities
As with any field of endeavor, chances to apply NLP
principles acquired in the classroom enrich greatly a
student?s learning experience and solidify the con-
cepts taught. Various outlets are provided on our
campus for experiencing the field.
5.1 Research opportunities
Several research projects in both the Linguistics and
CS departments welcome undergrad and graduate
students. Weekly meetings involve keeping abreast
of current research literature, discussing project de-
liverables, and practicing conference paper presen-
tations. Groups where NLP-related work is done fo-
cus on: a data-driven, exemplar-based approach to
language modeling, integrating speech tools and dis-
course engines for task-oriented language learning,
extraction and integration of web-based document
content, technologies in human and machine trans-
lation, and cognitive modeling of natural-language
processing. Attendance is voluntary and no course
credit is given, but generally participation in project
work is enthusiastic and consistent, especially since
the NLP class was initiated.
One relevant change in the NLP offerings is wor-
thy of note: at first, the advanced NLP class was of-
fered in Winter semester, with many students treat-
ing it like a capstone class in their last semester.
They would then leave the school for further grad-
uate work or employment. Consequently the class
was recently moved from Winter to Fall, keeping the
students? experience on campus for at least another
semester?s worth of project participation.
The university sponsors a mechanism for funding
undergraduate research projects. Proposals are re-
viewed by faculty, and the very competitive program
offers cash awards to the winners, who are required
to submit a written report synthesizing results ob-
tained. NLP students have had phenomenal success
in winning these awards and have gained valuable
experience with such projects as: morphological en-
gines for Cebuano and Farsi, cognitive modeling of
word-sense disambiguation, modeling task-oriented
discourse structure, and developing exemplar-based
machine translation. One advantage for NLP stu-
dents in this competition is that interdisciplinary re-
search is more likely to win funding.
5.2 Beyond the campus
In its short time, the NLP environment has also pro-
vided several students with the requisite skills to be
placed in summer internships, most of them compet-
itive and paid. Students have been placed in an Euro-
pean MT project, a U.S. industrial NLP research lab,
another university?s language acquisition research
project, and a defense-funded research institute. Par-
ticularly appealing to their sponsors was the combi-
nation of proven foreign-language aptitudes with a
computational background and an understanding of
NLP techniques and tools.
Students whose project work is promising are
encouraged to present their work at conferences,
and several have presented their work at local, na-
tional, and international venues. Particularly note-
worthy projects have also served as the founda-
tion for peer-reviewed conference papers, under-
grad honors theses, and master?s theses. Success-
fully defended or ongoing theses in six departments
(Linguistics, Spanish/Portuguese, Asian Languages,
Language Acquisition, CS, and Physics) were initi-
ated in the NLP class.
6 Reflections on issues
Naturally, communication and cooperation across
department and college lines offers continual chal-
lenges for an interdisciplinary NLP curriculum.
Still, both sides recognize the unique linguistic skills
present in our students and the need to develop
an environment fostering wider NLP expertise on-
campus. Students, for their part, are attracted to such
offerings and seem satisfied.
6.1 NLP and CL
One as-yet unrealized goal is to develop and offer
a class in computational linguistics. Since Linguis-
tics doesn?t offer a Ph.D. degree, advanced grads are
not available to the program; current students typ-
ically do not have, in this teacher?s estimation, ap-
propriate background in computational complexity,
algorithmic analysis, AI techniques, formal logic,
math/stats, and formal linguistics to be adequately
prepared for an intensive CL class. To be sure, many
students have a background in some of these areas,
but not across a wide enough base to prove necessary
for theoretical CL work. This may change over time,
if a Ph.D. program is adopted; in the meantime, the
NLP courses do fill a necessary niche.
Another promising recent development might
help stimulate progress in this area: a newly hired
CS faculty member with a computational linguistics
background will begin teaching in that department
next year. Initially, it has been decided to offer two
somewhat complementary NLP classes. The CS
Department will offer one class, which will resem-
ble the advanced class discussed above, including
using that textbook. The other class, hosted by
Linguistics, will use a different text (Jurafsky and
Martin, 2000) with its content focused more on
the lexicon, morphology, speech, semantics, and
deep parsing. Overlap between the two courses
will be minimized as much as possible, with the
goal of broadening NLP content offerings. Whether
students will be attracted to a two-course sequence
of NLP remains an open question.
6.2 Resource issues
A few obstacles and difficulties have been experi-
enced in spite of the overall positive aspects of im-
plementing an NLP curriculum mentioned in this pa-
per. A few of these are sketched in this section.
Texts: A frequent complaint from Linguistics (but
not CS) students enrolled in the advanced NLP class
is that textbook discussions almost invariably focus
on English. Since these students have all studied lin-
guistics intensively, including exposure to a large va-
riety of language typologies and nontrivial issues of
language analysis and complexity, these discussions
seem overly narrow and simplistic in many cases.
As mentioned earlier, classroom discussion can to
some extent elicit the wider-scope issues that interest
them. Certainly the vast array of Web-published re-
search findings also helps to fill this void. Still, sev-
eral students have voiced the desire for a comprehen-
sive NLP textbook that would address typologically
diverse issues.
Support: One clear disadvantage to hosting an
NLP class in the College of Humanities? Linguis-
tics Department is one of comparatively limited re-
sources. CS resources for coursework computers,
research facilities, and student labs are noticeably
superior. Software acquisition and licensing proce-
dures, resolution of networking issues, and computer
support are more problematic in a non-CS context on
our campus. This is primarily a problem for CS stu-
dents, who occasionally chafe at the need to use non-
CS computers for coursework.
Tools: Tools accessibility is the greatest difficulty
perceived by this author in trying to develop co-
hesive and coherent course content. With its ad-
mittedly fast-paced progress, the field of NLP has
seen the development of a plethora of systems, tools,
modules, and development environments for a wide
array of tasks. Keeping track of these products,
and assessing their availability and appropriateness
for student use, is a daunting task. Several help-
ful resources assist this teacher besides those already
mentioned. Listservs like LinguistList8, Corpora9,
and LN10 notify subscribers of new NLP software.
There are even a few hierarchically-structured meta-
indexes for NLP tools: among others, the ACL Uni-
verse (mentioned above), Colibri11, and especially
the Natural Language Software Registry12. These
repositories, while helpful and greatly appreciated,
tend to be spotty and idiosyncratic at best in their
consideration of resources. Certainly a more system-
atic and comprehensive clearinghouse of NLP tools
would be a boon to educators and researchers, partic-
ularly if its contents could be individually annotated
for pedagogical applicability.
8See www.linguistlist.org.
9See www.hit.uib.no/corpora/.
10See www.biomath.jussieu.fr/LN/.
11See colibri.let.uu.nl/.
12See registry.dfki.de.
6.3 Teacher background
In a recent analysis of several years? worth of linguis-
tics job announcements, Richard Sproat offered in-
teresting conclusions relevant to employment in lin-
guistics13. He notes ?a prejudice that linguists are
not typically well trained in computational skills?,
and the fact that ?relatively few linguistics programs
have got serious about providing their students with
extensive computational training.? In most current
CL courses, he claims, ?there is little emphasis on
practical applications?. If these observations hold,
the NLP offerings discussed in this paper serve a
valuable purpose in providing Linguistics (and CS)
students much-needed practical experience in lan-
guage computation technologies.
Sproat also detects a trend in linguistics showing
?little effort to hire faculty members who have had
extensive industrial experience?, whereas in CS such
experience is often desired, valued, and sought. He
concludes that: ?Departments thinking of building
up [CL] programs would be well advised to consider
people with industrial experience.? The present au-
thor?s 11-year experience in the NLP industry be-
fore pursuing graduate CL studies has proven invalu-
able in administering an NLP curriculum, facilitat-
ing such tasks as: advising students in their pro-
gramming projects; directing them in job searches
and internship opportunities; helping them propose
and establish research agendas; collaborating with
commercial and governmental sponsors; and deal-
ing with issues of software licensing and technology
transfer.
7 Conclusion
Our experience has shown that a small core of NLP
classes housed in a Linguistics department and serv-
ing well-prepared students from other fields can
greatly enhance the research and pedagogical infras-
tructure across many disciplines on-campus, while
also preparing students for further grad studies or
careers in the industrial NLP sector. Though chal-
lenges and issues remain, NLP courses are enjoying
good enrollment, high satisfaction ratings, and ap-
preciable learning outcomes.
13See www.research.att.com/~rws/lingjobs.
References
James Allen. 1995. Natural Language Understanding.
Benjamin/Cummings Publishing.
Evan Antworth. 1990. PC-KIMMO: a two-level proec-
ssor for morphological analysis. Number 16 in Oc-
casional Publications in Academic Computing. Sum-
mer Institute of Linguistics, Dallas, TX. See also
www.sil.org/pckimmo/.
P. Boersma and D. J. M. Weenink. 1996. Praat, a system
for doing phonetics by computer, version 3.4. Techni-
cal Report 132, Institute of Phonetic Sciences, Amster-
dam. See also fonsg3.let.uva.nl/praat/.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the DARPA speech and natural
language workshop.
R. Cole, J. Mariani, H. Uszkoreit, A. Zaenen, and V. Zue,
editors. 1997. Survey of the State of the Art in Human
Language Technology. Cambridge University Press.
Ron Cole. 1999. Tools for research and education in
speech science. In Proceedings of the International
Conference of Phonetic Sciences, San Francisco, CA,
August. See also cslu.cse.ogi.edu/toolkit/index.html.
Christiane Fellbaum. 1998. WordNet: An electronic lex-
ical database. MIT Press, Cambridge, MA. See also
www.cogsci.princeton.edu/~wn/.
J.M.Pickett. 1999. The Acoustics of Speech Communica-
tion. Allyn & Bacon.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and
Speech Recognition. Prentice-Hall.
Judith Klavans. 2001. Computational linguistics. In
William O?Grady, John Archibald, Mark Aronoff, and
Janie Rees-Miller, editors, Contemporary Linguistics:
an Introduction. Bedford/St. Martin?s.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press.
Masaru Tomita and Eric H. Nyberg, 3rd. 1988. Gener-
ation Kit and Transformation Kit: Version 3.2 user?s
manual. Technical Report CMU-CMT-88-MEMO,
Carnegie Mellon Center for Machine Translation, Oc-
tober.
Dan Tufis and Oliver Mason. 1998. Tagging Romanian
texts: a case study for QTAG, a language-independent
probabilistic tagger. In Proceedings of the First Inter-
national Conference on Language Resources and Eval-
uation (LREC), Grenada, Spain, May.
Automated Rating of ESL Essays
Deryle Lonsdale
BYU Linguistics and English Language
lonz@byu.edu
Diane Strong-Krause
BYU Linguistics and English Language
diane strong-krause@byu.edu
Abstract
To date, traditional NLP parsers have
not been widely successful in TESOL-
oriented applications, particularly in scor-
ing written compositions. Re-engineering
such applications to provide the neces-
sary robustness for handling ungrammat-
ical English has proven a formidable ob-
stacle. We discuss the use of a non-
traditional parser for rating compositions
that attenuates some of these difficulties.
Its dependency-based shallow parsing ap-
proach provides significant robustness in
the face of language learners? ungrammat-
ical compositions. This paper discusses
how a corpus of L2 essays for English was
rated using the parser, and how the auto-
matic evaulations compared to those ob-
tained by manual methods. The types of
modifications that were made to the sys-
tem are discussed. Limitations to the cur-
rent system are described, future plans for
developing the system are sketched, and
further applications beyond English essay
rating are mentioned.
1 Introduction
Rating constructed response items, particularly es-
says, is a time-consuming effort. This is true in
rating essays written by second-language speakers.
To make this process more manageable, researchers
have investigated how to involve computers in the
grading process. Several factors suggest why au-
tomating scoring might be desirable: (i) practical-
ity: essay grading is costly and time-consuming;
(ii) consistency: essay grading is somewhat subjec-
tive in nature, and consistency may sometimes suf-
fer; and (iii) feedback: Providing feedback to a stu-
dent is important, and automated scoring can pro-
vide ways of generating specific suggestions tailored
to the needs of the author.
However, computerized rating of essays written
by second-language speakers poses unique dilem-
mas, particularly for responses written by exami-
nees at low levels of language proficiency. Where
we expect generally well-formed sentences from na-
tive English speaker responses, we find that the ma-
jority of the responses by lower proficiency second-
language English speakers will be made up of ill-
formed sentences.
Previous work in automated essay grading and
related technologies has been surveyed and dis-
cussed in several different forums (Burstein and
Chodorow, 1999; Thompson, 1999; Hearst, 2000;
Williams, 2001; Rudner and Gagne, 2001), and
a thorough survey of the field has recently been
published (Shermis and Burstein, 2003). Typi-
cally these approaches have borrowed techniques
and tools from several natural language processing
(NLP) fields. For example, the knowledge-based en-
gines have been used for analyzing essays: parsers
(Carbonell and Hayes, 1984; Schneider and McCoy,
1998), grammar and spelling checkers (Park et al,
1997), discourse processing analyzers (Miltsakaki
and Kukich, 2000), and other hand-crafted linguistic
knowledge sources.
Linkage 1, cost vector = (UNUSED=0 DIS=1 AND=0 LEN=20)
+-------------------------------Xp------------------------------+
+--------------Wd--------------+ |
| +----------CO---------+ |
| +--------Xc--------+ | |
| +-----Jp----+ | | +------Op-----+ |
| | +--Dmu-+ | +-Sp*i+--PPf-+ +--Dmc-+ |
| | | | | | | | | | |
LEFT-WALL during my schooling.n , I.p have.v taken.v many classes.n .
Figure 1: Sample link-parsed sentence with associated cost vector.
On the other hand, much work has leveraged sta-
tistical methods in detecting properties of student es-
says via stylometrics (Aaronson, 2001)1, latent se-
mantic indexing (Wiemer-Hastings et al, 1998), and
feature analysis.
Finally, mirroring noteworthy progress in other
NLP fields involving data-driven methods, recent
work has involved essay grading via exemplar-based
machine learning techniques (Chodorow and Lea-
cock, 2000).
The most visible systems implement one (or
more) of these approaches. The Project Essay
Grade (PEG) system, for example, uses lexically-
based metrics in scoring (Page, 2003). The Intel-
ligent Essay Assessor (IEA) uses latent semantic
analysis in calculating its metrics (Landauer et al,
2003). The E-Rater system by Educational Testing
Services uses syntactic, discourse, and topical (i.e.
vocabulary-based) data analysis (Burstein, 2003).
Several criticisms have been aimed at automatic
scoring systems on both theoretical and implementa-
tional grounds. For example, many systems exhibit
an inherent Achilles? heel since it is possible to trick
them into evaluating a nonsensical text purely by
reverse-engineering the scoring mechanism and de-
signing a text that responds to the criteria. Another
problem is the cost of development, which can be
substantial. In addition, most systems are designed
around certain specific topics in order to focus ter-
minology and vocabulary in limited subdomains; in-
troducing new subject areas requires building a new
model, often a nontrivial process. Thus, many sys-
tems are often not adaptable enough to meet the par-
ticular needs of an individual, class, teacher, or in-
stitution.
1This work also uses the Link Grammar parser.
The purpose of our research is to explore the use
of a particular natural language processing (NLP)
approach for automated scoring of essays written
by ESL students at lower levels of language pro-
ficiency Our goal for the system reflects common-
sense (though ambitious) criteria: to have the sys-
tem?s scores agree with those assigned by human
raters at least as often as human raters agree among
themselves.
2 The parser
As mentioned above, one approach to grading is
to use a parsing system, along with its associated
knowledge sources, to analyze the correctness of
a text. The NLP field has produced a wide range
of parsers and grammars to support them. Most
of the widely used and highly accurate parsers are
closely (or even inalienably) tied to a particular syn-
tactic theory: XTAG with Tree-Adjoining Gram-
mar2, LFG-WB with Lexical-Functional Grammar3,
ALE with HPSG or Categorial Grammar4, and so
on. Principled coverage of grammatical phenomena
can thus be tied closely to the linguistic theory in
question. Some parsers are designed to skip over
ungrammatical and disfluent portions of input and
have been successfully applied to speech and dia-
logue processing (Rose?, 1997), with perhaps possi-
ble future application to rating ESL essays. There
are disadvanges to traditional parsers, though, which
offset their usefulness for automated grading.
Consider, for example, that the encoding of any
parser?s phrase-structure component is costly, com-
plex, and dependent on significant lexical resources.
This precludes involvement of the uninitiated. Even
2See http://www.cis.upenn.edu/?xtag/
3See http://www2.parc.com/istl/groups/nltt/medley/
4See http://www.cs.toronto.edu/?gpenn/ale.html
more serious is the lack of robustness that most
parsers entail. Most linguistic formalisms focus pre-
cisely on what is grammatical, and not on what
is ungrammatical. This often becomes an archi-
tectural assumption in the way parsers are imple-
mented. The result is that such systems are rather
inflexible, particularly in the face of ungrammatical
input?ungrammaticaly is almost always avoided in
both the theory and in its implementation. Yet cru-
cially for the essay grading ungrammatical input is
frequent and expected.
One method used to sidestep the robustness issue
is to explicitly encode rules reflecting ungrammati-
cality, called ?mal-rules? (McCoy et al, 1996). For
example, the following is a possible mal-rule for an
LFG parser:
S --> NP (agr ?a) VP (agr ??a)
This rule says that a sentence can consist of an NP
and a VP whose respective agreement features do
NOT agree. While such a technique allows for de-
tection of ungrammatical sentences, it introduces
two problems. First, the computational complex-
ity of a parsing system increases as such rules are
added to the phrase-structure component. Second,
maintaining a knowledge base of such information
is a complicated and never-ending proposition, as
student errors vary in a seemingly infinite number
of ways.
In our work we chose to use a different kind of
parser, the link grammer parser (Sleator and Tem-
perley, 1993). This parser has been developed for
robust, efficient processing of dependency-style syn-
tax (Grinberg et al, 1995). Freely available for re-
search purposes, it is more robust than traditional
parsers and has been widely used in such NLP appli-
cations as information retrieval, speech recognition,
and machine translation5. Written in the C program-
ming language, it is comparatively fast and efficient.
The link grammar parser does not seek to
construct constituents in the traditional linguistic
sense?instead, it calculates simple, explicit rela-
tions between pairs of words. A link is a targeted
relationship between two words and has two parts:
a left side and a right side. For example, links asso-
ciate such word pairs as: subject + verb, verb + ob-
ject, preposition + object, adjective + adverbial mod-
5For a bibliography see http://link.cs.cmu.edu/link/papers/
ifier, and auxiliary + main verb. Each link has a label
that expresses the nature of the relationship mediat-
ing the two words. Potential links are specified by a
set of technical rules that constrain and permit word-
pair associations. In addition, it is possible to score
individual linkages and to penalize unwanted links.
Figure 1 shows an example link parse of a sen-
tence from a student essay. Ten links of various
types span the various relationships observable in
the sentence.
When parses are not possible, the system?s robust-
ness allows it to discard words (or alternatively posit
spelling corrections) in order to arrive at a tenable
description of the input. Figure 2 shows two un-
grammatical sentences that the parser has nonethe-
less coped with. In the first, it skips over words that
don?t seem to fit into any grammatical pattern, pars-
ing instead a core sentence ?The class is mathemat-
ical.? The cost vector for this sentence records the
fact that there were 4 unused words. In the second
example, only one word must be discarded to arrive
at a reasonable parse.
The LG parser as distributed was not completely
suited to handle the grading of ESL students? es-
says, so some modifications had to be made. Lex-
ical items had to be added to the system?s lexicon
to cover terms frequently used by students, such
as acronyms: E.L.C. (the English Language Cen-
ter), R.O.C. (Republic of China), and so on. Other
constructions not supported in the standard release
were also added, for example variant ordering within
dates (e.g. 24 May as well as May 24). The grammar
as originally distributed did not allow for optional
commas where unexpected. It also did not penalize
certain ungrammatical constructions (e.g. missing
determiners, as in ?I am student of English.?) since
such constructions were not anticipated.
With the system slightly modified as described
above, it was well suited to parsing ESL essays. Two
more example parses of student essay sentences are
illustrated in Figure 3. In the next section we discuss
how it was used to score such essays.
3 The corpus
Our study involved using the LG parser to rate es-
says based on the results of a link parse for each sen-
tence. We used ESL essays written by Intensive En-
Linkage 1, cost vector = (UNUSED=4 DIS=0 AND=0 LEN=11)
+--------------------------------Xp-------------------------------+
+-----Wd-----+ |
| +-D*u-+------------Ss-----------+---Ost--+ |
| | | | | |
LEFT-WALL the class.n [most] [important] is.v Mathematical [for] [my] .
Linkage 1, cost vector = (UNUSED=1 DIS=0 AND=0 LEN=17)
+----------------------------------Xp----------------------------------+
| +--------------MVp-------------+ |
| +----I----+------MVp------+ +----Js----+ |
+------Wi-----+-Ox-+ +---Op--+ +--Jp--+ | +--Ds-+ |
| | | | | | | | | | |
LEFT-WALL [it] help.v me make.v friends.n with people.p around the world.n .
Figure 2: Link parser results for highly ungrammatical sentences. Note the discarded words indicated in
square brackets.
glish students who spanned a range of ability from
Novice-mid to Intermediate-high. The essays were
on a variety of assigned topics, and each had to be
written within a 30-minute time limit.
Our corpus consists of 301 human-rated essays in
total consisting of some 50,000 words and 3400 sen-
tences. These were sub-divided further by semester
into 5 sub-corpora. The essays exhibited the follow-
ing characteristics: each had (on average) 165 words
and 11.2 sentences, and each sentence had on aver-
age 14.75 words. Note the wide variety of errors in
this typical sentence from one essay:
Iwork really hard and occacionally
I don?t have time for have fun
whith mt friens but i don?t mind
becausse i knew ,when i grow up
i will have a profesion and have
a good job and i will be very
happy.
Each essay was given a holistic rating by two
human judges. Different raters participated each
semester, though there was likely a small degree of
overlap among raters across subsequent semesters.
Scores ranged from 1 to 5 with half-points possi-
ble (i.e. essays could receive 1, 1.5, 2, 2.5, 3, 3.5,
4, 4.5, and 5). Occasionally a judge gave a rating of
0 indicating that no comprehensible language was
present. Inter-rater agreement, where each human
assigned a score within one point of the other, was
98% over the corpora.
The following categories describe scoring levels:
1. Demonstrates limited ability to write English
words and sentences. Sentences and para-
graphs may be incomplete and difficult to fol-
low.
2. Writes a simple paragraph with a fair control
of basic, not complex, sentences structures. Er-
rors occur in almost all sentences except for the
most basic, formula-type (memorized) struc-
tures. Little detail is present.
3. Writes a fairly long paragraph with relatively
simple sentence structures. Personal experi-
ences and some emotions can be expressed,
but much detail is missing. Frequent errors in
grammar and word use make reading somewhat
burdensome.
4. Writes long groups of paragraphs with some
complex sentence patterns. Some transitions
are used effectively. Vocabulary is broaden-
ing, but some wrong word use. Grammar er-
rors may detract from meaning. Some ideas
are supported with detail. Some notion of an
introduction and conclusion is included.
5. Writes complex thoughts using complex sen-
tence patterns with effective transitions be-
tween ideas and sentences. Errors in gram-
mar exist but do not obscure meaning. A va-
riety of advanced vocabulary words are used
but some wrong use occurs, including problems
with prepositions and articles. Ideas are clearly
supported with details. Effective introduction
and conclusion are included.
Although judges gave a holistic rating based on
a number of features of the essay, the category de-
scriptions hint that syntax (and to some degree vo-
cabulary use) is the focus for at least categories 1
through 3, and even much of category 4.
4 Results and analysis
The corpus was partitioned into two classes: the de-
velopment set and the test set. The former was used
to develop and tune the system, and consisted of the
most recent set of essays (60 in number) dating from
Winter 2002 semester. The other 4 (earlier) corpora
were used for testing the system.
Each essay was sent through the LG parser a sen-
tence at a time, and each sentence was given a 5-
point score based on the parse?s cost vector. An
overall score for each essay was computed as the
average across sentence scores, after discarding the
lowest and highest ones. The overall score was
then compared with those of the human raters. For
the development set, the system agreed 67% of the
time with human raters, where agreement follows
the standardly accepted definition of falling within
1.0 of the closest human?s score. Note that since hu-
mans only gave ratings involving integers and half-
steps between them, all computer-generated scores
were rounded to the nearest integer or half-step as
appropriate.
The system was then run on the previously
unseen test corpus, actually consisting of essays
from four separate semesters. The test corpus scores
were as follows:
Fall01: 82 students 69.5% agreement
Summer01: 58 students 62.1% agreement
Winter01: 36 students 66.7% agreement
Winter92: 75 students 62.2% agreement
Hence over a corpus of some 300 essays and five
sub-corpora, system agreement with human raters
was achieved about 66% of the time. We now turn
to a brief analysis of interesting results that emerged
from the system?s performance.
Generally, the system tended to over-score essays
with very low human scores (i.e. those in the 1-
2 point range). It also tended to under-score es-
says with high human scores and complex run-on
sentences. This reflects the observation that run-on
sentences, which were very plentiful, were penal-
ized by the system but largely forgiven by human
raters. Also, the system?s scoring matched human
values better for midrange-scored essays, and worse
for extreme examples (i.e. with average score < 2
or > 4.5). Finally, system panics (when the system
ran out of allotted time without successfully parsing
a sentence) occurred most frequently when several
conjunctions appeared in a single sentence.
It is informative to look at an essay that reflects
one of the most extreme mismatches between human
and computer ratings. In this case, the two human
raters gave the essay scores of 1 and 2 respectively,
whereas the computer scored the essay at 4.40.
My free time is very fun. Because
I meet my friends. We goes to
play. For exmple, I went to
movies, recreation ground, trip
and shopping with them. I can?t
write English.
Another observation from the present work is that
performing a purely syntactic parse does not al-
ways assure appropriate ratings. The current sys-
tem?s scoring mechanism occasionally results in ar-
tificially high scores. Consider, for example, the
sentence in Figure 4. Even though there are no
egregious syntactic errors, violations of selectional
restriction, collocation, determiner selection, and
verbal aspect render the sentence highly unnatural,
though this is undetected by the current parser. Ad-
dition of hand-coded postprocessor rules may help
avoid such situations, and is possible with the parser.
5 Future work
There are several ways in which the base system de-
scribed in this paper can be improved. For instance,
sentence and essay scores are currently based on
straightforward values from the cost vector, whereas
more sophisticated measures can be implemented.
Future work will involve using statistical smooth-
ing to improve performance in the extreme (high-
scoring and low-scoring) situations.
Linkage 1, cost vector = (UNUSED=0 DIS=2 AND=0 LEN=23)
+-----------------------------------------Xp----------------------------------------+
| +-----------------------MVp-----------------------+ |
| +---------------MVp--------------+ | |
| | +-------Jp-------+ +----Js---+ | |
+--Wd--+Sp*+-PPf-+--Pg*b--+--MVp-+ +----AN----+ | +---D--+ +-Js+ |
| | | | | | | | | | | | | |
LEFT-WALL I.p ?ve been.v majoring.v in Material engineering.n at my University in Korea .
Linkage 1, cost vector = (UNUSED=0 DIS=2 AND=0 LEN=27)
+----------------------------------------------Xp----------------------------------------------+
| +-----------Wdc-----------+ +------------------Opt-----------------+ |
| | +--------CO--------+ | +--------------AN-------------+ |
| | | +-----D*u----+-------Ss------+ | +-------AN-------+ |
+--Wc--+ | +--La-+ +--Mp--+--J-+ | | | +----AN---+ |
| | | | | | | | | | | | | |
LEFT-WALL but probably the best.a class.n for.p me was.v medicine.n and first.n aid.n principles.n .
Figure 3: Sample link-parsed sentences from the student essay corpus.
Linkage 1, cost vector = (UNUSED=0 DIS=1 AND=0 LEN=13)
+--------------------------------Xp--------------------------------+
+------Wd------+---------Ss---------+ +---Jp---+ |
| +--D*u--+--Mp--+--Jp-+ +--Pg*b--+---MVp--+ +-D*u-+ |
| | | | | | | | | | |
LEFT-WALL the practice.n in English.n is.v progressing.v in the life.n .
Figure 4: Sentence illustrating collocational and selectional problems.
The system could also achieve more human-like
scoring by integrating data-driven, exemplar-based
approaches. Training the system to relate salient fea-
tures and vector costs of the essays with the corre-
sponding human scores can be done using any of
a variety of available techniques, such as memory-
based learning or analogical modeling.
Finally, further linguistic processing can improve
the system. Some syntactic improvements can be
made, including specifying licit structures that are
not recognized as well as unacceptable structures
with their associated coses. As mentioned above,
pushing the analysis beyond a syntactic LG parse
will be necessary. This might involve leveraging
resources such as WordNet (Fellbaum, 1998) for
providing lexical semantic information which could
prove useful in scoring compounding strategies, col-
locations, and verb argument structure formation.
Current research in discourse processing using such
devices as centering, anaphor/coreference, coher-
ence, and topic continuity can also be integrated into
the system as has been done in other scoring pro-
grams.
In addition, the LG parser is also being developed
for other languages (e.g. French and Spanish) with
the eventual goal of providing a scoring engine for
learners of these languages as well.
6 Conclusions
We have shown how the output from a non-
traditional syntactic parser can be used to grade ESL
essays. With a robust enough parser, reasonable re-
sults can be obtained, even for highly ungrammati-
cal text. We anticipate that this foundation can be
improved upon by using other commonly adopted
NLP techniques. Its applicability should extend to
other languages besides English given a comparable
LG parser implementation.
References
Scott Aaronson. 2001. Stylometric clustering: a compar-
ison of data-driven and syntactic features. Manuscript.
http://www.cs.berkeley.edu/?aaronson/sc.doc.
Jill Burstein and Martin Chodorow. 1999. Automated
essay scoring for nonnative English speakers. In Com-
puter Mediated Language Assessment and Evaluation
in Natural Language Processing, pages 68?75. Asso-
ciation for Computational Linguistics.
Jill Burstein. 2003. The E-rater scoring engine: Auto-
mated essay scoring with natural language processing.
In Mark D. Shermis and Jill C. Burstein, editors, Auto-
mated Essay Scoring: A Cross-Disciplinary Perspec-
tive. Lawrence Erlbaum, Mahwah, NJ.
Jaime G. Carbonell and Phillip J. Hayes. 1984. Coping
with extragrammaticality. In Proceedings of COLING
?84, pages 437?443. Association for Computational
Linguistics.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proceedings of ANLP-NAACL 2000, pages 140?147.
Morgan Kaufmann Publishers.
Christiane Fellbaum. 1998. WordNet: An electronic lex-
ical database. MIT Press, Cambridge, MA.
Dennis Grinberg, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for Link Grammars.
Technical Report CMU-CS-95-125, School of Com-
puter Science, August.
Marti A. Hearst. 2000. The debate on automated
essay grading. IEEE Intelligent Systems, Septem-
ber/October 2000:22?37.
Thomas Landauer, Darrell Laham, and Peter Foltz. 2003.
Automated scoring and annotation of essays with the
Intelligent Essay Assessor. In Mark D. Shermis and
Jill C. Burstein, editors, Automated Essay Scoring:
A Cross-Disciplinary Perspective. Lawrence Erlbaum,
Mahwah, NJ.
Kathleen F. McCoy, Christopher A. Pennington, and
Linda Z. Suri. 1996. English error correction: A syn-
tactic user model based on principled ?mal-rule? scor-
ing. In Proceedings of the Fifth International Confer-
ence on User Modeling, pages 59?66. User Modeling,
Inc.
Eleni Miltsakaki and Karen Kukich. 2000. The role of
centering theory?s rough-shift in the teaching and eval-
uation of writing skills. In Proceedings of ACL-2000.
Association for Computational Linguistics.
Ellis Batten Page. 2003. Project Essay Grade: PEG. In
Mark D. Shermis and Jill C. Burstein, editors, Auto-
mated Essay Scoring: A Cross-Disciplinary Perspec-
tive. Lawrence Erlbaum, Mahwah, NJ.
Jong C. Park, Martha Palmer, and Gay Washburn. 1997.
An English grammar checker as a writing aid for stu-
dents of English as a Second Language. In Proceed-
ings of the Conference of Applied Natural Language
Processing (ANLP).
Carolyn Penstein Rose?. 1997. Robust Interactive Di-
alogue Interpretation. Ph.D. thesis, School of Com-
puter Science, Carnegie Mellon University.
Lawrence Rudner and Phill Gagne. 2001. An overview
of three approaches to scoring written essays by com-
puter. Practical Assessment, Research & Evaluation,
7(26).
David Schneider and Kathleen McCoy. 1998. Recogniz-
ing syntactic errors in the writing of second language
learners. In Proceedings of COLING-ACL 1998, pages
1198?1204. Morgan Kaufmann Publishers.
Mark D. Shermis and Jill C. Burstein, editors. 2003.
Automated Essay Scoring: A Cross-Disciplinary Per-
spective. Lawrence Erlbaum, Mahwah, NJ.
Daniel Sleator and Davy Temperley. 1993. Parsing En-
glish with a Link Grammar. In Third International
Workshop on Parsing Technologies.
Clive Thompson. 1999. New word order: The attack
of the incredible grading machine. Linguafranca: The
Review of Academic Life, 9(5).
Peter Wiemer-Hastings, Arthur C. Graesser, and Derek
Harter. 1998. The foundations and architecture
of AutoTutor. Lecture Notes in Computer Science,
1452:334?343.
Robert Williams. 2001. Automated essay grading: An
evaluation of four conceptual models. In Expand-
ing Horizons in Teaching and Learning: Proceedings
of the 10th Annual Teaching Learning Forum. Curtin
University of Technology.
Proceedings of the Linguistic Annotation Workshop, pages 101?108,
Prague, June 2007. c?2007 Association for Computational Linguistics
Active Learning for Part-of-Speech Tagging: 
Accelerating Corpus Annotation 
 
Eric Ringger*, Peter McClanahan*, Robbie Haertel*, George Busby*, Marc Carmen**, 
James Carroll*, Kevin Seppi*, Deryle Lonsdale** 
*Computer Science Department; **Linguistics Department 
Brigham Young University 
Provo, Utah, USA 84602 
 
Abstract 
In the construction of a part-of-speech an-
notated corpus, we are constrained by a 
fixed budget. A fully annotated corpus is 
required, but we can afford to label only a 
subset. We train a Maximum Entropy Mar-
kov Model tagger from a labeled subset 
and automatically tag the remainder. This 
paper addresses the question of where to 
focus our manual tagging efforts in order to 
deliver an annotation of highest quality. In 
this context, we find that active learning is 
always helpful. We focus on Query by Un-
certainty (QBU) and Query by Committee 
(QBC) and report on experiments with sev-
eral baselines and new variations of QBC 
and QBU, inspired by weaknesses particu-
lar to their use in this application. Experi-
ments on English prose and poetry test 
these approaches and evaluate their robust-
ness. The results allow us to make recom-
mendations for both types of text and raise 
questions that will lead to further inquiry. 
1 Introduction 
We are operating (as many do) on a fixed budget 
and need annotated text in the context of a larger 
project. We need a fully annotated corpus but can 
afford to annotate only a subset. To address our 
budgetary constraint, we train a model from a ma-
nually annotated subset of the corpus and automat-
ically annotate the remainder. At issue is where to 
focus manual annotation efforts in order to produce 
a complete annotation of highest possible quality. 
A follow-up question is whether these techniques 
work equally well on different types of text. 
In particular, we require part-of-speech (POS) 
annotations. In this paper we employ a state-of-the-
art tagger on both prose and poetry, and we ex-
amine multiple known and novel active learning 
(or sampling) techniques in order to determine 
which work best in this context. We show that the 
results obtained by a state-of-the-art tagger trained 
on a small portion of the data selected through ac-
tive learning can approach the accuracy attained by 
human annotators and are on par with results from 
exhaustively trained automatic taggers. 
In a study based on English language data pre-
sented here, we identify several active learning 
techniques and make several recommendations that 
we hope will be portable for application to other 
text types and to other languages. In section 2 we 
briefly review the state of the art approach to POS 
tagging. In section 3, we survey the approaches to 
active learning employed in this study, including 
variations on commonly known techniques. Sec-
tion 4 introduces the experimental regime and 
presents results and their implications. Section 5 
draws conclusions and identifies opportunities for 
follow-up research. 
2 Part of Speech Tagging 
Labeling natural language data with part-of-speech 
tags can be a complicated task, requiring much 
effort and expense, even for trained annotators. 
Several efforts, notably the Alembic workbench 
(Day et al, 1997) and similar tools, have provided 
interfaces to aid annotators in the process.  
Automatic POS tagging of text using probabilis-
tic models is mostly a solved problem but requires 
supervised learning from substantial amounts of 
training data. Previous work demonstrates the sui-
tability of Hidden Markov Models for POS tagging 
(Kupiec, 1992; Brants, 2000). More recent work 
has achieved state-of-the-art results with Maxi-
101
mum entropy conditional Markov models (MaxEnt 
CMMs, or MEMMs for short) (Ratnaparkhi, 1996; 
Toutanova & Manning, 2000; Toutanova et al, 
2003). Part of the success of MEMMs can be attri-
buted to the absence of independence assumptions 
among predictive features and the resulting ease of 
feature engineering. To the best of our knowledge, 
the present work is the first to present results using 
MEMMs in an active learning framework.  
An MEMM is a probabilistic model for se-
quence labeling. It is a Conditional Markov Model 
(CMM as illustrated in Figure 1) in which a Max-
imum Entropy (MaxEnt) classifier is employed to 
estimate the probability distribution
1.. 1 1 2( | , ) ( | , , , )i i ME i i i i ip t w t p t w f t t? ? ?? over 
possible labels it  for each element in the se-
quence?in our case, for each word iw  in a sen-
tence w . The MaxEnt model is trained from la-
beled data and has access to any predefined 
attributes (represented here by the collection if ) of 
the entire word sequence and to the labels of pre-
vious words ( 1.. 1it ? ). Our implementation employs 
an order-two Markov assumption so the classifier 
has access only to the two previous tags 1 2,i it t? ? . 
We refer to the features 1 2( , , , )i i i iw f t t? ? from 
which the classifier predicts the distribution over 
tags as ?the local trigram context?. 
A Viterbi decoder is a dynamic programming 
algorithm that applies the MaxEnt classifier to 
score multiple competing tag-sequence hypotheses 
efficiently and to produce the best tag sequence, 
according to the model. We approximate Viterbi 
very closely using a fast beam search. Essentially, 
the decoding process involves sequential classifi-
cation, conditioned on the (uncertain) decisions of 
the previous local trigram context classifications. 
The chosen tag sequence t? is the tag sequence 
maximizing the following quantity: 
1 2
1..
? arg max ( | )
arg max ( | , , , )
t
t ME i i i i i
i n
t P t w
p t w f t t? ?
=
=
= ?  
The features used in this work are reasonably 
typical for modern MEMM feature-based POS 
tagging and consist of a combination of lexical, 
orthographic, contextual, and frequency-based in-
formation. In particular, for each word the follow-
ing features are defined: the textual form of the 
word itself, the POS tags of the preceding two 
words, and the textual form of the following word. 
Following Toutanova and Manning (2000) approx-
imately, more information is defined for words that 
are considered rare (which we define here as words 
that occur fewer than fifteen times). We consider 
the tagger to be near-state-of-the-art in terms of 
tagging accuracy. 
 
Figure 1. Simple Markov order 2 CMM, with focus on 
the i-th hidden label (or tag). 
3 Active Learning 
The objective of this research is to produce more 
high quality annotated data with less human anno-
tator time and effort. Active learning is an ap-
proach to machine learning in which a model is 
trained with the selective help of an oracle. The 
oracle provides labels on a sufficient number of 
?tough? cases, as identified by the model. Easy 
cases are assumed to be understood by the model 
and to require no additional annotation by the 
oracle. Many variations have been proposed in the 
broader active learning and decision theory litera-
ture under many names, including ?active sam-
pling? and ?optimal sampling.? 
In active learning for POS tagging, as in other 
applications, the oracle can be a human. For expe-
rimental purposes, a human oracle is simulated 
using pre-labeled data, where the labels are hidden 
until queried. To begin, the active learning process 
requires some small amount of training data to 
seed the model. The process proceeds by identify-
ing the data in the given corpus that should be 
tagged first for maximal impact. 
3.1 Active Learning in the Language Context 
When considering the role of active learning, we 
were initially drawn to the work in active learning 
for classification. In a simple configuration, each 
instance (document, image, etc.) to be labeled can 
be considered to be independent. However, for ac-
tive learning for the POS tagging problem we con-
sidered the nature of human input as an oracle for 
the task. As an approximation, people read sen-
tences as propositional atoms, gathering contextual 
cues from the sentence in order to assemble the 
102
meaning of the whole. Consequently, we thought it 
unreasonable to choose the word as the granularity 
for active learning. Instead, we begin with the as-
sumption that a human will usually require much 
of the sentence or at least local context from the 
sentence in order to label a single word with its 
POS label. While focusing on a single word, the 
human may as well label the entire sentence or at 
least correct the labels assigned by the tagger for 
the sentence. Consequently, the sentence is the 
granularity of annotation for this work. (Future 
work will question this assumption and investigate 
tagging a word or a subsequence of words at a 
time.) This distinguishes our work from active 
learning for classification since labels are not 
drawn from a fixed set of labels. Rather, every sen-
tence of length n can be labeled with a tag se-
quence drawn from a set of size nT , where T  is 
the size of the per-word tag set. Granted, many of 
the options have very low probability. 
To underscore our choice of annotating at the 
granularity of a sentence, we also note that a max-
imum entropy classifier for isolated word tagging 
that leverages attributes of neighboring words?
but is blind to all tags?will underperform an 
MEMM that includes the tags of neighboring 
words (usually on the left) among its features. Pre-
vious experiments demonstrate the usefulness of 
tags in context on the standard Wall Street Journal 
data from the Penn Treebank (Marcus et al, 1999). 
A MaxEnt isolated word tagger achieves 93.7% on 
words observed in the training set and 82.6% on 
words unseen in the training set. Toutanova and 
Manning (2000) achieves 96.9% (on seen) and 
86.9% (on unseen) with an MEMM. They sur-
passed their earlier work in 2003 with a ?cyclic 
dependency network tagger?, achieving 
97.2%/89.05% (seen/unseen) (Toutanova et al, 
2003). The generally agreed upon upper bound is 
around 98%, due to label inconsistencies in the 
Treebank. The main point is that effective use of 
contextual features is necessary to achieve state of 
the art performance in POS tagging. 
In active learning, we employ several sets of 
data that we refer to by the following names: 
? Initial Training: the small set of data used 
to train the original model before active 
learning starts 
? Training: data that has already been la-
beled by the oracle as of step i in the learn-
ing cycle 
? Unannotated: data not yet labeled by the 
oracle as of step i 
? Test (specifically Development Test): la-
beled data used to measure the accuracy of 
the model at each stage of the active learn-
ing process. Labels on this set are held in 
reserve for comparison with the labels 
chosen by the model. It is the accuracy on 
this set that we report in our experimental 
results in Section 4. 
Note that the Training set grows at the expense of 
the Unannotated set as active learning progresses. 
Active Learning for POS Tagging consists of the 
following steps: 
1. Train a model with Initial Training data 
2. Apply model to Unannotated data 
3. Compute potential informativeness of 
each sentence 
4. Remove top n sentences with most po-
tential informativeness from Unanno-
tated data and give to oracle 
5. Add n sentences annotated (or corrected) 
by the oracle to Training data 
6. Retrain model with Training data 
7. Return to step 2 until stopping condition 
is met. 
There are several possible stopping conditions, 
including reaching a quality bar based on accuracy 
on the Test set, the rate of oracle error corrections 
in the given cycle, or even the cumulative number 
of oracle error corrections. In practice, the exhaus-
tion of resources, such as time or money, may 
completely dominate all other desirable stopping 
conditions. 
Several methods are available for determining 
which sentences will provide the most information. 
Expected Value of Sample Information (EVSI) 
(Raiffa & Schlaiffer, 1967) would be the optimal 
approach from a decision theoretic point of view, 
but it is computationally prohibitive and is not con-
sidered here. We also do not consider the related 
notion of query-by-model-improvement or other 
methods (Anderson & Moore, 2005; Roy & 
McCallum, 2001a, 2001b). While worth exploring, 
they do not fit in the context of this current work 
and should be considered in future work. We focus 
here on the more widely used Query by Committee 
(QBC) and Query by Uncertainty (QBU), includ-
ing our new adaptations of these. 
Our implementation of maximum entropy train-
ing employs a convex optimization procedure 
known as LBFGS. Although this procedure is rela-
tively fast, training a model (or models in the case 
103
of QBC) from scratch on the training data during 
every round of the active learning loop would pro-
long our experiments unnecessarily. Instead we 
start each optimization search with a parameter set 
consisting of the model parameters from the pre-
vious iteration of active learning (we call this ?Fast 
MaxEnt?). In practice, this converges quickly and 
produces equivalent results. 
3.2 Query by Committee 
Query by Committee (QBC) was introduced by 
Seung, Opper, and Sompolinsky (1992). Freund, 
Seung, Shamir, and Tishby (1997) provided a care-
ful analysis of the approach. Engelson and Dagan 
(1996) experimented with QBC using HMMs for 
POS tagging and found that selective sampling of 
sentences can significantly reduce the number of 
samples required to achieve desirable tag accura-
cies. Unlike the present work, Engelson & Dagan 
were restricted by computational resources to se-
lection from small windows of the Unannotated set, 
not from the entire Unannotated set. Related work 
includes learning ensembles of POS taggers, as in 
the work of Brill and Wu (1998), where an ensem-
ble consisting of a unigram model, an N-gram 
model, a transformation-based model, and an 
MEMM for POS tagging achieves substantial re-
sults beyond the individual taggers. Their conclu-
sion relevant to this paper is that different taggers 
commit complementary errors, a useful fact to ex-
ploit in active learning. QBC employs a committee 
of N models, in which each model votes on the 
correct tagging of a sentence. The potential infor-
mativeness of a sentence is measured by the total 
number of tag sequence disagreements (compared 
pair-wise) among the committee members. Possi-
ble variants of QBC involve the number of com-
mittee members, how the training data is split 
among the committee members, and whether the 
training data is sampled with or without replace-
ment. 
A potential problem with QBC in this applica-
tion is that words occur with different frequencies 
in the corpus. Because of the potential for greater 
impact across the corpus, querying for the tag of a 
more frequent word may be more desirable than 
querying for the tag of a word that occurs less fre-
quently, even if there is greater disagreement on 
the tags for the less frequent word. We attempted 
to compensate for this by weighting the number of 
disagreements by the corpus frequency of the word 
in the full data set (Training and Unannotated). 
Unfortunately, this resulted in worse performance; 
solving this problem is an interesting avenue for 
future work. 
3.3 Query by Uncertainty 
The idea behind active sampling based on uncer-
tainty appears to originate with Thrun and Moeller 
(1992). QBU has received significant attention in 
general. Early experiments involving QBU were 
conducted by Lewis and Gale (1994) on text classi-
fication, where they demonstrated significant bene-
fits of the approach. Lewis and Catlett (1994) ex-
amined its application for non-probabilistic learn-
ers in conjunction with other probabilistic learners 
under the name ?uncertainty sampling.? Brigham 
Anderson (2005) explored QBU using HMMs and 
concluded that it is sometimes advantageous. We 
are not aware of any published work on the appli-
cation of QBU to POS tagging. In our implementa-
tion, QBU employs a single MEMM tagger. The 
MaxEnt model comprising the tagger can assess 
the probability distribution over tags for any word 
in its local trigram context, as illustrated in the ex-
ample in Figure 2. 
Figure 2. Distribution over tags for the word ?hurdle? in 
italics. The local trigram context is in boldface. 
In Query by Uncertainty (QBU), the informa-
tiveness of a sample is assumed to be the uncer-
tainty in the predicted distribution over tags for 
that sample, that is the entropy of 
1 2( | , , , )ME i i i i ip t w f t t? ? . To determine the poten-
tial informativeness of a word, we can measure the 
entropy in that distribution. Since we are selecting 
sentences, we must extend our measure of uncer-
tainty beyond the word. 
3.4 Adaptations of QBU 
There are several problems with the use of QBU in 
this context: 
? Some words are more important; i.e., they 
contain more information perhaps because 
they occur more frequently. 
   NN 0 .85 
   VB  0.13 
   ... 
RB    DT JJS CD  2.0E-7 
 
Perhaps     the biggest   hurdle ? 
104
? MaxEnt estimates per-word distributions 
over tags, not per-sentence distributions 
over tag sequences. 
? Entropy computations are relatively costly. 
We address the first issue in a new version of QBU 
which we call ?Weighted Query by Uncertainty? 
(WQBU). In WQBU, per-word uncertainty is 
weighted by the word's corpus frequency. 
To address the issue of estimating per-sentence 
uncertainty from distributions over tag sequences, 
we have considered several different approaches. 
The per-word (conditional) entropy is defined as 
follows: 
 
 
 
 
 
 
where iT  is the random variable for the tag it  on 
word iw , and the features of the context in which 
iw  occurs are denoted, as before, by the collection 
if  and the prior tags 1 2,i it t? ? . It is straightforward 
to calculate this entropy for each word in a sen-
tence from the Unannotated set, if we assume that 
previous tags 1 2,i it t? ?  are from the Viterbi (best) 
tag sequence (for the entire sentence) according to 
the model. 
For an entire sentence, we estimate the tag-
sequence entropy by summing over all possible tag 
sequences. However, computing this estimate ex-
actly on a 25-word sentence, where each word can 
be labeled with one of 35 tags, would require 3525 
= 3.99*1038 steps. Instead, we approximate the per-
sentence tag sequence distribution entropy by 
summing per-word entropy: 
 
 
This is the approach we refer to as QBU in the 
experimental results section. We have experi-
mented with a second approach that estimates the 
per-sentence entropy of the tag-sequence distribu-
tion by Monte Carlo decoding. Unfortunately, cur-
rent active learning results involving this MC POS 
tagging decoder are negative on small Training set 
sizes, so we do not present them here. Another al-
ternative approximation worth pursuing is compu-
ting the per-sentence entropy using the n-best POS 
tag sequences. Very recent work by Mann and 
McCallum (2007) proposes an approach in which 
exact sequence entropy can be calculated efficient-
ly. Further experimentation is required to compare 
our approximation to these alternatives. 
An alternative approach that eliminates the 
overhead of entropy computations entirely is to 
estimate per-sentence uncertainty with ?1 ( )P t? , 
where t? is the Viterbi (best) tag sequence. We call 
this scheme QBUV. In essence, it selects a sample 
consisting of the sentences having the highest 
probability that the Viterbi sequence is wrong. To 
our knowledge, this is a novel approach to active 
learning. 
4 Experimental Results 
In this section, we examine the experimental setup, 
the prose and poetry data sets, and the results from 
using the various active learning algorithms on 
these corpora. 
4.1 Setup 
The experiments focus on the annotation scenario 
posed earlier, in which budgetary constraints af-
ford only some number x of sentences to be anno-
tated. The x-axis in each graph captures the num-
ber of sentences. For most of the experiments, the 
graphs present accuracies on the (Development) 
Test set. Later in this section, we present results for 
an alternate metric, namely number of words cor-
rected by the oracle. 
In order to ascertain the usefulness of the active 
learning approaches explored here, the results are 
presented against a baseline in which sentences are 
selected randomly from the Unannotated set. We 
consider this baseline to represent the use of a 
state-of-the-art tagger trained on the same amount 
of data as the active learner. Due to randomization, 
the random baseline is actually distinct from expe-
riment to experiment without any surprising devia-
tions. Also, each result curve in each graph 
represents the average of three distinct runs. 
Worth noting is that most of the graphs include 
active learning curves that are run to completion; 
namely, the rightmost extent of all curves 
represents the exhaustion of the Unannotated data. 
At this extreme point, active learning and random 
sample selection all have the same Training set. In 
the scenarios we are targeting, this far right side is 
not of interest. Points representing smaller amounts 
of annotated data are our primary interest. 
In the experiments that follow, we address sev-
eral natural questions that arise in the course of 
applying active learning. We also compare the va-
1 2
1 2
1 2
( | , , , )
( | , , , )
log ( | , , , )
i
i i i i i
ME i i i i i
t Tagset
ME i i i i i
H T w f t t
p t w f t t
p t w f t t
? ?
? ?
?
? ?
= ?
?
?
1 2
? ( | ) ( | , , , )
i
i i i i i
w w
H T w H T w f t t? ?
?
? ??
105
riants of QBU and QBC. For QBC, committee 
members divide the training set (at each stage of 
the active learning process) evenly. All committee 
members and final models are MEMMs. Likewise, 
all variants of QBU employ MEMMs. 
4.2 Data Sets 
The experiments involve two data sets in search 
of conclusions that generalize over two very dif-
ferent kinds of English text. The first data set con-
sists of English prose from the POS-tagged one-
million-word Wall Street Journal text in the Penn 
Treebank (PTB) version 3. We use a random sam-
ple of the corpus constituting 25% of the tradition-
al training set (sections 2?21). Initial Training data 
consists of 1% of this set. We employ section 24 as 
the Development Test set. Average sentence length 
is approximately 25 words. 
Our second experimental set consists of English 
poetry from the British National Corpus (BNC) 
(Godbert & Ramsay, 1991; Hughes, 1982; Raine, 
1984). The text is also fully tagged with 91 parts of 
speech from a different tag set than the one used 
for the PTB. The BNC XML data was taken from 
the files B1C.xml, CBO.xml, and H8R.xml. This 
results in a set of 60,056 words and 8,917 sen-
tences. 
4.3 General Results 
To begin, each step in the active learning process 
adds a batch of 100 sentences from the Unanno-
tated set at a time. Figure 3 demonstrates (using 
QBU) that the size of a query batch is not signifi-
cant in these experiments.  
The primary question to address is whether ac-
tive learning helps or not. Figure 4 demonstrates 
that QBU, QBUV, and QBC all outperform the 
random baseline in terms of total, per-word accu-
racy on the Test set, given the same amount of 
Training data. Figure 5 is a close-up version of 
Figure 4, placing emphasis on points up to 1000 
annotated sentences. In these figures, QBU and 
QBUV vie for the best performing active learning 
algorithm. These results appear to give some useful 
advice captured in Table 1. The first column in the 
table contains the starting conditions. The remain-
ing columns indicate that for between 800-1600 
sentences of annotation, QBUV takes over from 
QBU as the best selection algorithm. 
The next question to address is how much initial 
training data should be used; i.e., when should we 
start using active learning? The experiment in Fig-
ure 6 demonstrates (using QBU) that one should 
use as little data as possible for Initial Training 
Data. There is always a significant advantage to 
starting early. In the experiment documented in  
 
Figure 3. Varying the size of the query batch in active 
learning yields identical results after the first query batch.  
 
Figure 4. The best representatives of each type of active 
learner beat the baseline. QBU and QBUV trade off the 
top position over QBC and the Baseline. 
Figure 5. Close-up of the low end of the graph from Figure 
4. QBUV and QBU are nearly tied for best performance. 
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
Batch Query Size of 10 Sentences
Batch Query Size of 100 Sentences
Batch Query Size of 500 Sentences
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
ur
ac
y 
(%
)
Number of Sentences in Training Set
QBUV 
QBU
QBC
Baseline
 76
 78
 80
 82
 84
 86
 88
 90
 92
 100  1000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBUV 
QBU
QBC
Baseline
106
this figure, a batch query size of one was employed 
in order to make the point as clearly as possible. 
Larger batch query sizes produce a graph with sim-
ilar trends as do experiments involving larger Un-
annotated sets and other active learners. 
 
 100 200 400 800 1600 3200 6400 
QBU 76.26 86.11 90.63 92.27 93.67 94.65 95.42 
QBUV 76.65 85.09 89.75 92.24 93.72 94.96 95.60 
QBC 76.19 85.77 89.37 91.78 93.49 94.62 95.36 
Base 76.57 82.13 86.68 90.12 92.49 94.02 95.19 
Table 1. The best models (on PTB WSJ data) with various 
amounts of annotation (columns). 
 
Figure 6. Start active learning as early as possible for a 
head start. 
4.4 QBC Results 
An important question to address for QBC is 
what number of committee members produces the 
best results? There was no significant difference in 
results from the QBC experiments when using be-
tween 3 and 7 committee members. For brevity we 
omit the graph. 
4.5 QBU Results 
For Query by Uncertainty, the experiment in Fig-
ure 7 demonstrates that QBU is superior to QBUV 
for low counts, but that QBUV slightly overtakes 
QBU beyond approximately 300 sentences. In fact, 
all QBU variants, including the weighted version, 
surpassed the baseline. WQBU has been omitted 
from the graph, as it was inferior to straight-
forward QBU. 
4.6 Results on the BNC 
Next we introduce results on poetry from the Brit-
ish National Corpus. Recall that the feature set 
employed by the MEMM tagger was optimized for 
performance on the Wall Street Journal. For the 
experiment presented in Figure 8, all data in the 
Training and Unannotated sets is from the BNC, 
but we employ the same feature set from the WSJ 
experiments. This result on the BNC data shows 
first of all that tagging poetry with this tagger 
leaves a final shortfall of approximately 8% from 
the WSJ results. Nonetheless and more importantly, 
the active learning trends observed on the WSJ still 
hold. QBC is better than the baseline, and QBU 
and QBUV trade off for first place. Furthermore, 
for low numbers of sentences, it is overwhelmingly 
to one?s advantage to employ active learning for 
annotation. 
 
 
Figure 7. QBUV is superior to QBU overall, but QBU is 
better for very low counts. Both are superior to the ran-
dom baseline and the Longest Sentence (LS) baseline. 
 
Figure 8. Active learning results on the BNC poetry data. 
Accuracy of QBUV, QBU, and QBC against the random 
baseline. QBU and QBUV are nearly indistinguishable. 
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 10  100
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
1%
5%
10%
25%
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBU
QBUV
LS
Baseline 
 40
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBU
QBUV
Baseline
QBC
107
4.7 Another Perspective 
Next, briefly consider a different metric on the ver-
tical axis. In Figure 9, the metric is the total num-
ber of words changed (corrected) by the oracle. 
This quantity reflects the cumulative number of 
differences between the tagger?s hypothesis on a 
sentence (at the point in time when the oracle is 
queried) and the oracle?s answer (over the training 
set). It corresponds roughly to the amount of time 
that would be required for a human annotator to 
correct the tags suggested by the model. This fig-
ure reveals that QBUV makes significantly more 
changes than QBU, QBC, or LS (the Longest Sen-
tence baseline). Hence, the superiority of QBU 
over QBUV, as measured by this metric, appears to 
outweigh the small wins provided by QBUV when 
measured by accuracy alone. That said, the random 
baseline makes the fewest changes of all. If this 
metric (and not some combination with accuracy) 
were our only consideration, then active learning 
would appear not to serve our needs. 
This metric is also a measure of how well a par-
ticular query algorithm selects sentences that espe-
cially require assistance from the oracle. In this 
sense, QBUV appears most effective. 
 
Figure 9. Cumulative number of corrections made by the 
oracle for several competitive active learning algorithms. 
QBU requires fewer corrections than QBUV. 
5 Conclusions 
Active learning is a viable way to accelerate the 
efficiency of a human annotator and is most effec-
tive when done as early as possible. We have pre-
sented state-of-the-art tagging results using a frac-
tion of the labeled data. QBUV is a cheap approach 
to performing active learning, only to be surpassed 
by QBU when labeling small numbers of sentences. 
We are in the midst of conducting a user study to 
assess the true costs of annotating a sentence at a 
time or a word at a time. We plan to incorporate 
these specific costs into a model of cost measured 
in time (or money) that will supplant the metrics 
reported here, namely accuracy and number of 
words corrected. As noted earlier, future work will 
also evaluate active learning at the granularity of a 
word or a subsequence of words, to be evaluated 
by the cost metric. 
References 
Anderson, B., and Moore, A. (2005). ?Active Learning for HMM: 
Objective Functions and Algorithms.? ICML, Germany. 
Brants, T., (2000). ?TnT -- a statistical part-of-speech tagger.? ANLP, 
Seattle, WA. 
Brill, E., and Wu, J. (1998). ?Classifier combination for improved 
lexical disambiguation.? Coling/ACL, Montreal, Quebec, Canada. 
Pp. 191-195.  
Day, D., et al (1997). ?Mixed-Initiative Development of Language 
Processing Systems.? ANLP, Washington, D.C. 
Engelson, S. and Dagan, I. (1996). ?Minimizing manual annotation 
cost in supervised training from corpora.? ACL, Santa Cruz, Cali-
fornia. Pp. 319-326. 
Freund, Y., Seung, H., Shamir, E., and Tishby, N. (1997). ?Selective 
sampling using the query by committee algorithm.? Machine 
Learning, 28(2-3):133-168.  
Godbert, G. and Ramsay, J. (1991). ?For now.? In the British National 
Corpus file B1C.xml. London: The Diamond Press (pp. 1-108).  
Hughes, T. (1982). ?Selected Poems.? In the British National Corpus 
file H8R.xml. London: Faber & Faber Ltd. (pp. 35-235).  
Kupiec, J. (1992). ?Robust part-of-speech tagging using a hidden 
Markov model.? Computer Speech and Language 6, pp. 225-242. 
Lewis, D., and Catlett, J. (1994). ?Heterogeneous uncertainty sam-
pling for supervised learning.? ICML. 
Lewis, D., and Gale, W. (1995). ?A sequential algorithm for training 
text classifiers: Corrigendum and additional data.? SIGIR Forum, 
29 (2), 13--19. 
Mann, G., and McCallum, A. (2007). "Efficient Computation of En-
tropy Gradient for Semi-Supervised Conditional Random Fields". 
NAACL-HLT. 
Marcus, M. et al (1999). ?Treebank-3.? Linguistic Data Consortium, 
Philadelphia, PA. 
Raiffa, H. and Schlaiffer, R. (1967). Applied Statistical Decision 
Theory. New York: Wiley Interscience.  
Raine, C. (1984). ?Rich.? In the British National Corpus file CB0.xml. 
London: Faber & Faber Ltd. (pp. 13-101).  
Ratnaparkhi, A. (1996). ?A Maximum Entropy Model for Part-Of-
Speech Tagging.? EMNLP. 
Roy, N., and McCallum, A. (2001a). ?Toward optimal active learning 
through sampling estimation of error reduction.? ICML. 
Roy, N. and McCallum, A. (2001b). ?Toward Optimal Active Learn-
ing through Monte Carlo Estimation of Error Reduction.? ICML, 
Williamstown. 
Seung, H., Opper, M., and Sompolinsky, H. (1992). ?Query by com-
mittee?.  COLT. Pp. 287-294. 
Thrun S., and Moeller, K. (1992). ?Active exploration in dynamic 
environments.? NIPS.  
Toutanova, K., Klein, D., Manning, C., and Singer, Y. (2003). ?Fea-
ture-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-
work.? HLT-NAACL. Pp. 252-259. 
Toutanova, K. and Manning, C. (2000). ?Enriching the Knowledge 
Sources Used in a Maximum Entropy Part-of-Speech Tagger.? 
EMNLP, Hong Kong. Pp. 63-70. 
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
 10000
 100  1000  10000
N
u
m
b
er
 o
f 
C
h
an
g
ed
 W
o
rd
s
Number of Sentences in Training Set
QBUV 
QBU 
QBC 
Baseline 
LS 
108
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 810?820,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Probabilistic Morphological Analyzer for Syriac
Peter McClanahan, George Busby, Robbie Haertel, Kristian Heal ?,
Deryle Lonsdale?, Kevin Seppi, Eric Ringger
Department of Computer Science, ?Department of Linguistics,
?Center for the Preservation of Ancient Religious Texts (CPART)
Brigham Young University
Provo, Utah 84604 USA
http://nlp.cs.byu.edu/
Abstract
We define a probabilistic morphological ana-
lyzer using a data-driven approach for Syriac in
order to facilitate the creation of an annotated
corpus. Syriac is an under-resourced Semitic
language for which there are no available lan-
guage tools such as morphological analyzers.
We introduce novel probabilistic models for
segmentation, dictionary linkage, and morpho-
logical tagging and connect them in a pipeline
to create a probabilistic morphological analyzer
requiring only labeled data. We explore the per-
formance of models with varying amounts of
training data and find that with about 34,500
labeled tokens, we can outperform a reason-
able baseline trained on over 99,000 tokens and
achieve an accuracy of just over 80%. When
trained on all available training data, our joint
model achieves 86.47% accuracy, a 29.7% re-
duction in error rate over the baseline.
1 Introduction
Our objective is to facilitate the annotation of a large
corpus of classical Syriac (referred to simply as ?Syr-
iac? throughout the remainder of this work). Syr-
iac is an under-resourced Western Semitic language
of the Christian Near East and a dialect of Aramaic.
It is currently employed almost entirely as a liturgi-
cal language but was a true spoken language up un-
til the eighth century, during which time many pro-
lific authors wrote in Syriac. Even today there are
texts still being composed in or translated into Syr-
iac. By automatically annotating these texts with lin-
guistically useful information, we will facilitate sys-
tematic study by scholars of Syriac, the Near East,
and Eastern Christianity. Furthermore, languages
that are linguistically similar to Syriac (e.g., Arabic
and Hebrew) may benefit from the methodology pre-
sented here.
Our desired annotations include morphological
segmentation, links to dictionary entries, and mor-
phological attributes. Typically, annotations of this
kind are made with the assistance of language tools,
such as morphological analyzers, segmenters, or
part-of-speech (POS) taggers. Such tools do not
exist for Syriac, but some labeled data does exist:
Kiraz (1994) compiled an annotated version of the
Peshitta New Testament (1920) and a concordance
thereof. We aim to replicate this kind of annota-
tion on a much larger scale with more modern tools,
building up from the labeled New Testament data,
our only resource. Motivated by this state of affairs,
our learning and annotation framework requires only
labeled data.
We approach the problem of Syriac morphological
annotation by creating five probabilistic sub-models
that can be trained in a supervised fashion and com-
bined in a joint model of morphological annota-
tion. We introduce novel algorithms for segmenta-
tion, dictionary linkage, and morphological tagging.
We then combine these sub-models into a joint n-
best pipeline. This joint model outperforms a strong,
though na?ve, baseline for all amounts of training
data over about 9,900 word tokens.
1.1 Syriac Background
Since Syriac is an abjad, its writing system does
not require vowels. As a dialect of Aramaic, it
is an inflected language with a templatic (non-
concatenative) morphology, based on a system of
triliteral consonantal roots, with prefixes, suffixes,
infixes, and enclitic particles. Syriac is written from
810
right to left. For the purposes of this work, all Syr-
iac is transliterated according to the Kiraz (1994)
transliteration1 and is written left-to-right whenever
transliterated; the Syriac appearing in the Serto script
in this paper is shown right-to-left.
Since there is no standardized nomenclature for
the parts of a Syriac word, we define the following
terms to facilitate the definitions of segmentation,
dictionary linkage, and morphological tagging:
? word token - contiguous characters delimited by
whitespace and/or punctuation
? stem - an inflected form of the baseform and
the main part of the word to which prefixes and
suffixes can be attached; the affixes do not in-
flect the stem but include prepositions, object
suffixes, and enclitic pronouns
? baseform - the dictionary citation form; also
known as a lexeme or lemma
? root - the form from which the baseform is de-
rived
To clarify, we will use an example word token
???????, LMLCCON, which means ?to your (mas-
culine plural) king?. For this word, the stem is ???,
MLC; the baseform is ????, MLCA ?king?; and the
root is ???,MLC. To clarify, note that the word token
(including the stem) can be spoken and written with
vowels as diacritics; however, since the vowels are
not written in common practice and since most text
does not include them, this work omits any indica-
tion of vowels. Furthermore, the stem is an inflected
baseform and does not necessarily form a word on
its own. Also, the (unvocalized) stem and root are
not necessarily identical. In Syriac, the same root
???, MLC is the foundation for other words such as
promise, counsel, deliberate, reign, queen, kingdom,
and realm.
1.2 Sub-tasks
Segmentation, or tokenization as it is sometimes
called (e.g., Habash and Rambow, 2007), is the pro-
cess of dividing a word token into its prefix(es) (if
any), a stem, and a suffix (if any). For Syriac, each
1According to this transliteration all capital letters including
A (?, olaph) and O (?, waw) are consonants. Additionally, the
semi-colon (;), representing (?, yod), is also a consonant.
word token consists of exactly one stem, from zero
to three prefixes, and zero or one suffix. Each pre-
fix is exactly one character in length. Segmenta-
tion does not include the process of parsing the stem
for its inflectional morphology; that step is handled
separately in subsequent processes described below.
While segmenting a Syriac word, we can handle all
prefixes as a single unit. It is trivial to segment a
prefix cluster into its individual prefixes (one charac-
ter per prefix). Suffixes may be multiple characters
in length and encode the morphological attributes of
the suffix itself (not of the stem); the suffix usually
encodes the object of the stem and has its own gram-
matical attributes, which we list later. As an example
of token segmentation, for the word token ???????,
LMLCCON, the prefix is ?, L ?to?, the stem is ???,
MLC ?king?, and the suffix is ???, CON ?(masculine
plural) your?.
Dictionary linkage is the process of linking a stem
to its associated baseform and root. In most Syriac
dictionaries, all headwords are either baseforms or
roots, and for a given word these are the only rele-
vant entries in the dictionary. Each Syriac stem is
derived from a baseform, and each baseform is de-
rived from a root. There is ambiguity in this cor-
respondence which can be caused by, among other
things, homographic stems generated from different
roots or even from homographic roots. As such, link-
age may be thought of as two separate processes: (1)
baseform linkage, where the stem is mapped to its
most likely baseform; and (2) root linkage, where
the baseform is mapped to its most likely root. For
our example ???????, LMLCCON, baseform linkage
would map stem ???,MLC to baseform ????,MLCA,
and root linkage would map baseform ????,MLCA to
root ???, MLC.
Morphological tagging is the process of labeling
each word token with its morphological attributes.
Morphological tagging may be thought of as two
separate tagging tasks: (1) tagging the stem and (2)
tagging the suffix. For Syriac, scholars have defined
for this task a set of morphological attributes con-
sisting of twelve attributes for the stem and four at-
tributes for the suffix. The attributes for the stem
are as follows: grammatical category, verb conju-
gation, aspect, state, number, person, gender, pro-
noun type, demonstrative category, noun type, nu-
meral type, and participle type. The morphological
811
Attribute Value
Grammatical Category noun
Verb Conjugation N/A
Aspect N/A
State emphatic
Number singular
Person N/A
Gender masculine
Pronoun Type N/A
Demonstrative Category N/A
Noun Type common
Numeral Type N/A
Participle Type N/A
Table 1: The values for the morphological attributes of
the stem ???,MLC, ?king?.
Attribute Value
Gender masculine
Person second
Number plural
Contraction normal suffix
Table 2: The values for the morphological attributes of
the suffix ???, CON, ?(masculine plural) your?.
attributes for the suffix are gender, person, number,
and contraction. The suffix contraction attribute en-
codes whether the suffix is normal or contracted, a
phonological process involving the attachment of an
enclitic pronoun to a participle. These morphologi-
cal attributes were heavily influenced by those used
by Kiraz (1994), but were streamlined in order to fo-
cus directly on grammatical function. During mor-
phological tagging, each stem is labeled for each of
the stem attributes, and each suffix is labeled for each
of the suffix attributes. For a given grammatical cat-
egory (or POS), only a subset of the morphological
attributes is applicable. For those morphological at-
tributes (both of the stem and of the suffix) that do
not apply, the correct label is ?N/A? (not applicable).
Tables 1 and 2 show the correct stem and suffix tags
for the word ???????, LMLCCON.
The remainder of the paper will proceed as fol-
lows: Section 3 outlines our approach. In Section 4,
we describe our experimental setup; we present re-
sults in Section 5. Section 6 contrasts previous work
with our approach. Finally, in Section 7 we briefly
conclude and offer directions for future work.
2 The Syromorph Approach
Since lack language tools, we focus on automatically
annotating Syriac text in a data-driven fashion based
on the labeled data we have available. Since seg-
mentation, linkage, and morphological tagging are
not mutually independent tasks, we desire models
for the sub-tasks to influence each other. To accom-
modate these requirements, we use a joint pipeline
model (Finkel et al, 2006). In this section, we will
first discuss this joint pipeline model, which we call
syromorph. We then examine each of the individual
sub-models.
2.1 Joint Pipeline Model
Our approach is to create a joint pipeline model con-
sisting of a segmenter, a baseform linker, a root
linker, a suffix tagger, and a stem tagger. Figure 1
shows the dependencies among the sub-models in
the pipeline for a single word. Each sub-model
(oval) has access to the data and predictions (rect-
angles) indicated by the arrows. For example, for a
given word, the stem tagger has access to the previ-
ously predicted stem, baseform, root, and suffix tag.
The baseform linker has access to the segmentation,
most importantly the stem.
The training of syromorph is straightforward.
Each of the individual sub-models is trained sepa-
rately on the true labeled data. Features are extracted
from the local context in the sentence. The local con-
text consists first of predictions for the entire sen-
tence from earlier sub-tasks (those sub-tasks upon
which the sub-task in question depends). We cre-
ated the dependencies shown in Figure 1 taking into
account the difficulty of the tasks and natural depen-
dencies in the language. In addition to the predic-
tions for the entire sentence from previous sub-tasks,
the local context also includes the previous o tags of
the current sub-task, as the standard order o Markov
model does. For example, when the stem tagger is
being trained on a particular sentence, the local con-
text consists of the words in the sentence, the pre-
dicted segmentation, baseform, root, and suffix tags
for each word in the sentence, and additionally the
labels for the previous o stems. To further elaborate
812
Figure 1: The syromorph model. Each rectangle is an
input or output and each oval is a process employing a
sub-model.
on the example, since features are extracted from the
local context, for stem tagging we extract features
such as current stem, previous stem, current base-
form, previous baseform, current root, previous root,
current suffix tags, and previous suffix tags. (Here,
?previous? refers to labels on the immediately pre-
ceding word token.)
2.2 Segmentation
The syromorph segmentation model is a hybrid
word- and consonant-level model, based on the
model of Haertel et al (2010) for data-driven dia-
critization. Each of our probabilistic sequence mod-
els is a maximum entropy Markov model (MEMM).
Haertel et al (2010) showed that the distribution
over labels is different for known and words and rare
words. In this work, we only consider words not
seen in training (i.e., ?unknown?) to be rare. Follow-
ing Haertel et al?s (2010) model, a separate model
is trained for each word type seen in training with
the intent of choosing the best segmentation given
that word. This approach is closely related to the
idea of ambiguity classes mentioned in Haji? and
Hladk? (1998).
To handle unknown words, we back off to a
consonant-level model. Our consonant-level seg-
mentation model uses the notion of BI (Beginning
and Inside) tags, which have proven successful in
named-entity recognition. Since there are three
labels in which we are interested (prefix, stem, and
suffix), we apply the beginning and inside notion
to each of them to create six tags: BEGINNING-
PREFIX, INSIDE-PREFIX, BEGINNING-STEM,
INSIDE-STEM, BEGINNING-SUFFIX, and
INSIDE-SUFFIX. We train an MEMM to predict
one of these six tags for each consonant. Further-
more, we constrain the decoder to allow only legal
possible transitions given the current prediction,
so that prefixes must come before stems and stems
before suffixes. In order to capture the unknown
word distributions, we train the consonant-level
model on words occurring only once during training.
We call this word- and consonant-level segmenta-
tion model hybrid. As far as we are aware, this is a
novel approach to segmentation.
2.3 Dictionary Linkage
For dictionary linkage, we divide the problem into
two separate tasks: baseform linkage and root link-
age. For both of these tasks, we use a hybrid model
similar to that used for segmentation, consisting of
a collection of separate MEMMs for each word type
(either a stem or baseform, depending on the linker)
and amodel for unknown (or rare) words. For the un-
known words, we compare two distinct approaches.
The first approach for unknown words is based
on the work of Chrupa?a (2006), including the Mor-
fette system. Instead of predicting a baseform given
a stem, we predict what Chrupa?a calls a lemma-
class. A lemma-class is the transformation specified
by the minimum edit distance between the baseform
(which he calls a lemma) and the stem. The trans-
formation is a series of tuples, where each tuple in-
cludes (1) whether it was an insertion or deletion,
(2) the letter inserted or deleted, and (3) the position
of the insertion or deletion in the string (positions
begin at zero). All operations are assumed to oc-
cur sequentially, as in Morfette. For example, the
transformation of XE;N to XEA would proceed as
follows: delete ; from position 2, insert A into po-
sition 2, delete N from position 3.
813
In hybrid-morfette baseform linkage (respec-
tively, root linkage), we predict a lemma-class (i.e.,
transformation) for each baseform (respectively,
root). The predicted transformation is then applied
to the stem (respectively, baseform) in order to con-
struct the actual target baseform (respectively, root).
The advantage to this method is that common trans-
formations are grouped into a single class, thereby
allowing the model to generalize and adequately
predict baseforms (and roots) that have not been
seen during training, but whose transformations have
been seen. This model is trained on all words in or-
der to capture as many transformations as possible.
The second approach for unknown words, called
hybrid-maxent, uses an MEMM trained on all
words seen in training. Given a stem (respectively,
baseform), this approach predicts only baseforms
(respectively, roots) that were observed in training
data. Thus, this method has a distinct disadvan-
tage when it comes to predicting new forms. This
approach corresponds directly to the approach to
handling unknown -words by Toutanova and Man-
ning (2000) for POS tagging.
With regard to baseform and root linkage, we do
not use the dictionary to constrain possible base-
forms or roots, since we make no initial assumptions
about the completeness of a dictionary.
2.4 Morphological Tagging
For morphological tagging, we break the task into
two separate tasks: tagging the suffix and tagging
the stem. Since there are a number of values that
need to be predicted, we define two ways to ap-
proach the problem. We call the first approach the
monolithic approach, in which the label is the con-
catenation of all the morphological attribute values.
Table 3 illustrates the tagging of an example sen-
tence: the stem tag and suffix tag columns contain
the monolithic tags for stem tagging and suffix tag-
ging. We use an MEMM to predict a monolithic tag
for each stem or suffix and call this model maxent-
mono. No co-occurrence restrictions among related
or complementary morphological tags are directly
enforced. Co-occurrence patterns are observed in
the data, learned, and encoded in the models of the
tagging process. It is worth noting further that con-
straints provided by the baseforms ? predicted by
dictionary linkage ? on the morphological attributes
are likewise not directly enforced. Enforcement of
such constraints would require an infusion of expert
knowledge into the system.
The second approach is to assume that morpho-
logical attributes are independent of each other. We
call this the independent approach. Here, each tag
is predicted by a tagger for a single morphological
attribute. For example, the gender model is ignorant
of the other 11 sub-tags during stem tagging. Using
its local context (which does not include other stem
sub-tags), the model predicts the best gender for a
given word. The top prediction of each of these tag-
gers (12, for stem tagging) is then combined na?vely
with no notion of what combinations may be valid
or invalid. We use MEMMs for each of the single-
attribute taggers. This model is calledmaxent-ind.
2.5 Decoding
Our per-task decoders are beam decoders, with
beam-size b. In particular, we limit the number of
per-stage back-pointers to b due to the large size of
the tagset for some of our sub-models. Although
Viterbi decoding produces the most probable label
sequence given a sequence of unlabeled words, it is
potentially intractible on our hybrid models due to
the unbounded dependence on previous consonant-
level decisions. Our beam decoders produce a good
approximation when tuned properly.
Decoding in syromorph consists of extending the
per-task decoders to allow transitions from each sub-
model to the next sub-model in the pipe. For exam-
ple, in our pipeline, the first sub-model is segmen-
tation. We predict the top n segmentations for the
sentence (i.e., sequences of segmentations), where n
is the number of transitions tomaintain between each
sub-task. Then, we run the remaining sub-tasks with
each of the n sequences as a possible context. After
each sub-task is completed, we narrow the number
of possible contexts back to n.
We swept b and n for various values, and found
b = 5 and n = 5 to be good values that balanced
between accuracy and time; larger values saw only
minute gains in accuracy.
814
Word Transliteration Pre. Stem Suffix Baseform Root Suff. Tags Stem Tags
????? OEBDT O EBDT EBD EBD 0000 011012200000
???? ANON ANON HO HO 0000 300023222000
???? LALHN L ALH N ALHA ALH 1011 200310200200
?????? MLCOTA MLCOTA MLCOTA MLC 0000 200310300200
????? OCHNA O CHNA CHNA CHN 0000 200320200200
?????? OMLCA O MLCA MLCA MLC 0000 200320200200
Table 3: Part of a labeled Syriac sentence ?????? ????? ?????? ???? ???? ?????, ?And you have made them a kingdom and
priests and kings for our God.? (Revelation 5:10)
3 Experimental Setup
We are using the Syriac Peshitta New Testament in
the form compiled by Kiraz (1994).2 This data is
segmented, annotated with baseform and root, and
labeled with morphological attributes. Kiraz and
others in the Syriac community refined and corrected
the original annotation while preparing a digital and
print concordance of the New Testament. We aug-
mented Kiraz?s version of the data by segmenting
suffixes and by streamlining the tagset. The dataset
consists of 109,640 word tokens.
Table 3 shows part of a tagged Syriac sentence us-
ing this tagset. The suffix and stem tags consist of
indices representing morphological attributes. In the
example sentence, the suffix tag 1011 represents the
values ?masculine?, ?N/A?, ?plural?, ?normal suf-
fix? for the suffix attributes of gender, person, num-
ber, and contraction. Each value of 0 for each stem
and suffix attribute represents a value of ?N/A?, ex-
cept for that of grammatical category, which always
must have a value other than ?N/A?. Therefore, the
suffix tag 0000 means there is no suffix.
For the stem tags, the attribute order is the same
as that shown in Table 1 from top to bottom. The
following describes the interpretation of the stem
values represented in Table 3. Grammatical cate-
gory values 0, 2, and 3 represent ?verb?, ?noun?,
and ?pronoun?, respectively. (Grammatical cate-
gory has no ?N/A? value.) The verb conjugation
value 1 represents ?peal conjugation?. Aspect value
1 represents ?perfect?. State value 3 represents ?em-
phatic?. Number values 1 and 2 represent ?singular?
and ?plural?. Person values 2 and 3 represent ?sec-
2The Way International, a Biblical research ministry, anno-
tated this version of the New Testament by hand and required
15 years to do so.
ond? and ?third? person. Gender values 2 and 3 rep-
resent ?masculine? and ?feminine?. Pronoun type
value 2 represents ?demonstrative?. Demonstrative
category value 2 represents ?far?. Finally, noun type
2 represents ?common?. The last two columns of 0
represent ?N/A? for numeral type and particle type.
We implement five sub-tasks: segmentation, base-
form linkage, root linkage, suffix tagging, and stem
tagging. We compare each sub-task to a na?ve ap-
proach as a baseline. In addition to desiring good
sub-models, we also want a joint pipeline model that
significantly outperforms the na?ve joint approach,
which is formed by using each of the following base-
lines in the pipeline framework.
The baseline implementation of segmentation is to
choose the most-frequent label: for a given word,
the baseline predicts the segmentation with which
that word appeared most frequently during training.
For unknown words, it chooses the largest prefix
and largest suffix that is possible for that word from
the list of prefixes and suffixes seen during train-
ing. (This na?ve baseline for unknown words does
not take into account the fact that the stem is often at
least three characters in length.)
For dictionary linkage, the baseline is similar:
both baseform linkage and root linkage use the most-
frequent label approach. Given a stem, the baseline
baseform linker predicts the baseform with which
the stem was seen most frequently during training;
likewise, the baseline root linker predicts the root
from the baseform in a similar manner. For the un-
known stem case, the baseline baseform linker pre-
dicts the baseform to be identical to the stem. For
the unknown baseform case, the baseline root linker
predicts a root identical to the first three consonants
of the baseform, since for Syriac the root is exactly
815
three consonants in a large majority of the cases.
The baselines for stem and suffix tagging are the
most-frequent label approaches. These baselines
are similar to maxent-mono and maxent-ind, us-
ing the monolithic and independent approaches used
by maxent-mono and maxent-ind. The difference
is that instead of using maximum entropy, the na?ve
most-frequent approach is used in its place.
The joint baseline tagger uses each of the compo-
nent baselines in then-best joint pipeline framework.
Because this framework is modular, we can trivially
swap in and out different models for each of the sub-
tasks.
4 Experimental Results
Since we are focusing on under-resourced circum-
stances, we sweep the amount of training data and
produce learning curves to better understand how
our models perform in such circumstances. For each
point in our learning curves and for all other eval-
uations, we employ ten-fold cross-validation. The
learning curves use the chosen percentage of the data
for training and a fixed-size test set from each fold
and report the average accuracy.
The reported task accuracy requires the entire out-
put for that task to be correct in order to be counted as
correct. For example, during stem tagging, if one of
the sub-tags is incorrect, then the entire tag is said to
be incorrect. Furthermore, for syromorph, the out-
puts of every sub-task must be correct in order for
the word token to be counted as correct.
Moving beyond token-level metrics, in order to
understand performance of the system at the level
of individual decisions (including N/A decisions),
we compute decision-level accuracy: we call this
metric total-decisions. For the syromorph method
reported here, there are a total of 20 decisions: 2
for segmentation (prefix and suffix boundaries), 1
for baseform linkage, 1 for root linkage, 4 for suf-
fix tagging, and 12 for stem tagging. This accuracy
helps us to assess the number of decisions a human
annotator would need to correct, if data were pre-
annotated by a given model. Excluding N/A deci-
sions, we compute per-decision coverage and accu-
racy. These metrics are called applicable-coverage
and applicable-accuracy.
We show results on both the individual sub-tasks
and the entire joint task. Since previous sub-
tasks can adversely affect tasks further down in
the pipeline, we evaluate the sub-models by plac-
ing them in the pipeline with other (simulated) sub-
models that correctly predict every instance. For
example, when testing a root linker, we place the
root linker to be evaluated in the pipeline with a
segmenter, baseform linker, and taggers that return
the correct label for every prediction. This gives an
upper-bound for the individual model, removes the
possibility of error propagation, and shows how well
that model performs without the effects of the other
models in the pipeline.
For our results, unknown accuracy is the accuracy
of unknown instances, specific to the task, at training
time. In the case of baseform linkage, for example,
a stem is considered unknown if that stem was not
seen during training. It is therefore possible to have
a known word with an unknown stem and vice versa.
As in other NLP problems, unknown instances are a
manifestation of training data sparsity.
4.1 Baseline Results
Table 4 is grouped by sub-task and reports the results
of each of the baseline sub-tasks in the first row of
each group. Each of the baselines performs surpris-
ingly well. The accuracies of the baselines for most
of the tasks are high because the ambiguity of the
labels given the instance is quite low: the average
ambiguity across word types for segmentation, base-
form linkage, root linkage, suffix tagging, and stem
tagging are 1.01, 1.05, 1.02, 1.35, and 1.47, respec-
tively.
Preliminary experiments indicated that if we had
trained a baseline model using a single prediction (a
monolithic concatenation of the predictions for all
tasks) per token rather than separating the tasks, the
baseline tagging accuracy would have been lower.
Note that the unknown tagging accuracy for the
monolithic suffix tagger is not applicable, because
there were no test suffixes that were not seen during
training.
4.2 Individual Model Results
Table 4 also shows the results for the individual
models. In the table, SEG, BFL, RTL, SUFFIX,
and STEM represent segmentation, baseform link-
age, root linkage, suffix tagging, and stem tagging,
816
Model Total Known Unk
SE
G baseline 96.75 99.64 69.11
hybrid 98.87 99.70 90.83
BF
L baseline 95.64 98.45 22.28
hybrid-morfette 96.19 98.05 78.40
hybrid-maxent 96.19 99.15 67.86
RT
L baseline 98.84 99.56 80.20
hybrid-morfette 99.05 99.44 88.86
hybrid-maxent 98.34 99.45 69.30
SU
FF
IX
mono. baseline 98.75 98.75 N/A
ind. baseline 96.74 98.78 0.01
maxent-mono 98.90 98.90 N/A
maxent-ind 98.90 98.90 N/A
ST
EM
mono. baseline 83.08 86.26 0.01
ind. baseline 53.24 86.90 0.00
maxent-mono 89.48 92.87 57.04
maxent-ind 88.43 90.26 40.59
Table 4: Word-level accuracies for the individual sub-
models used in the syromorph approach.
respectively. Even though the baselines were high,
each individual model outperformed its respective
baseline, with the exception of the root linker. Two
of the most interesting results are the known ac-
curacy of the baseform linkers hybrid-maxent and
hybrid-morfette. As hybrid models, the difference
between them lies only in the treatment of unknown
words; however, the known accuracy of the mor-
fette model drops fairly significantly. This is due
to the unknown words altering the weights for fea-
tures in which those words occur. For instance, if
the previous word is unknown and a baseform that
was never seen was predicted, then the weights on
the next word for all features that contain that un-
known word will be quite different than if that pre-
vious word were a known word.
It is also worth noting that the stem tagger is by
far the worst model in this group of models, but it is
also the most difficult task. The largest gains in im-
proving the entire systemwould come from focusing
attention on that task.
4.3 Joint Model Results
Table 5 shows the accuracies for the joint mod-
els. The joint model incorporating ?maxent? vari-
ants performs best overall and on known cases. The
Model Total Known Unk
Baseline 80.76 85.74 28.07
Morfette Monolithic 85.96 89.85 44.86
Maxent Monolithic 86.47 90.77 40.93
Table 5: Word-level accuracies for various joint syro-
morph models.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  10  20  30  40  50  60  70  80  90  100
To
ta
l A
cc
ur
ac
y
Percentage of Training Data
baseline
hybrid / hybrid-maxent / maxent-mono
hybrid / hybrid-morfette / maxent-mono
Figure 2: The total accuracy of the joint model.
joint model incorporating the ?morfette? variants
performs best on unknown cases.
Decision-level metrics for the SEG:hybrid /
BFL and RTL:hybrid-maxent / SUFFIX and
STEM:maxent-mono model are as follows: for
total-decisions, the model achieves an accuracy
of 97.08%, compared to 95.50% accuracy for the
baseline, amounting to a 35.11% reduction in error
rate over the baseline; for applicable-coverage and
applicable-accuracy this model achieved 93.45%
and 93.81%, respectively, compared to the baseline?s
90.03% and 91.44%.
Figures 2, 3, and 4 show learning curves for to-
tal, known, and unknown accuracies for the joint
pipeline model. As can be seen in Figure 2, by the
time we reach 10% of the training data, syromorph
is significantly better than the baseline. In fact, at
35% of the training data, our joint pipeline model
outperforms the baseline trained with all available
training data.
Figure 3 shows the baseline performing quite well
on known words with very low amounts of data.
Since the x-axis varies the amount of training data,
the meaning of ?known? and ?unknown? evolves as
we move to the right of the graph; consequently, the
817
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0  10  20  30  40  50  60  70  80  90  100
Kn
ow
n 
Ac
cu
ra
cy
Percentage of Training Data
baseline
hybrid / hybrid-maxent / maxent-mono
hybrid / hybrid-morfette / maxent-mono
Figure 3: The accuracy of the joint model on known
words.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0  10  20  30  40  50  60  70  80  90  100
Un
kn
ow
n 
Ac
cu
ra
cy
Percentage of Training Data
baseline
hybrid / hybrid-maxent / maxent-mono
hybrid / hybrid-morfette / maxent-mono
Figure 4: The accuracy of the joint model on unknown
words.
left and right sides of the graph are incomparable.
When the percentage of training data is very low,
the percentage of unknown words is high, and the
number of known words is relatively low. On this
dataset, the more frequent words tend to be less am-
biguous, giving the most-frequent taggers an advan-
tage in a small random sample. For this reason, the
baseline performs very well on known accuracy with
lower amounts of training data.
Figure 4 clearly shows that hybrid-morfette link-
ers outperform hybrid-maxent linkers on unknown
words. However, Figures 2- 4 show that hybrid-
morfette?s advantage on unknown words is coun-
teracted by its lower performance on known words;
therefore, it has slightly lower overall accuracy than
hybrid-maxent.
5 Related Work
The most closely related work to our approach is
the Morfette tool for labeling inflectional morphol-
ogy (Chrupa?a et al, 2008). Chrupa?a et al cre-
ated a tool that labels Polish, Romanian, and Span-
ish with morphological information as well as base-
forms. It is a supervised learning approach that
requires data labeled with both morphological tags
and baseforms. This approach creates two separate
models (a morphological tagger and a lemmatizer)
and combines the decoding process in order to cre-
ate a joint model that predicts both morphological
tags and the baseform. Morfette uses MEMMs for
both models and has access to predicted labels in
the feature set. Reported accuracy rates are 96.08%,
93.83%, and 81.19% for joint accuracy on datasets
trained with fewer than 100,000 tokens for Roma-
nian, Spanish, and Polish, respectively. The major
difference between this work and ours is the degree
of morphological analysis required by the languages.
Chrupa?a et al neglect segmentation, a task not as
intuitive for their languages as it is for Syriac. These
languages also require only linkage to a baseform, as
no root exists.
Also closely related is the work of Daya, Roth, and
Wintner (2008) on Hebrew. The authors use the no-
tion of patterns into which root consonants are in-
jected to compose Semitic words. They employ lin-
guistic knowledge (specifically, lists of prefixes, suf-
fixes, and ?knowledge of word-formation processes?
combined with SNoW, a multi-class classifier that
has been shown to work well in other NLP tasks.
The major difference between this approach and the
method presented in this paper is that this method
does not require the extra knowledge required to en-
code word-formation processes. A further point of
difference is our use of hybrid word- and consonant-
level models, after Haertel et al (2010). Their work
builds on the work of Shacham and Wintner (2007),
which is also related to that of Habash and Rambow,
described below.
Work by Lee et al (2003) is themost relevant work
for segmentation, since they segment Arabic, closely
related to Syriac, with a data-driven approach. Lee
et al use an unsupervised algorithm bootstrapped
with manually segmented data to learn the segmen-
tation for Arabic without any additional language re-
818
sources. At the heart of the algorithm is a word-level
trigram language model, which captures the correct
weights for prefixes and suffixes. They report an ac-
curacy of 97%. We opted to use our own segmenter
because we felt we could achieve higher accuracy
with the hybrid segmenter.
Mohamed and K?bler (2010a, 2010b) report on
closely related work for morphological tagging.
They use a data-driven approach to find the POS tags
for Arabic, using both word tokens and segmented
words as inputs for their system. Although their seg-
mentation performance is high, they report that ac-
curacy is lower when first segmenting word tokens.
They employ TiMBL, a memory-based learner, as
their model and report an accuracy of 94.74%.
Habash and Rambow (2005) currently have the
most accurate approach for Arabic morphological
analysis using additional language tools. They focus
on morphological disambiguation (tagging), given
morphological segmentation in the output of the
morphological analyzer. For each word, they first
run it through the morphological analyzer to reduce
the number of possible outputs. They then train a
separate Support Vector Machine (SVM) for each
morphological attribute (ten in all). They look at dif-
ferent ways of combining these outputs to match an
output from the morphological analyzer. For their
best model, they report an overall tag accuracy of
97.6%.
Others have used morphological analyzers and
other language tools for morphological disambigua-
tion coupled with segmentation. The following
works exemplify this approach: Diab et al (2004)
use a POS tagger to jointly segment, POS tag, and
chunk base-phrases for Arabic with SVMs. Kudo
et al (2004) use SVMs to morphologically tag
Japanese. Smith et al (2005) use SVMs for seg-
mentation, lemmatization, and POS tagging for Ara-
bic, Korean, and Czech. Petkevi? (2001) use a mor-
phological analyzer and additional simple rules for
morphological disambiguation of Czech. Mansour
et al (2007) and Bar-haim et al (2008) both use hid-
denMarkov models to POS tag Hebrew, with the lat-
ter including segmentation as part of the task.
For Syriac, a morphological analyzer is not avail-
able. Kiraz (2000) created a Syriac morphological
analyzer using finite-state methods; however, it was
developed on outdated and now inaccessible equip-
ment and is no longer working or available to us.
6 Conclusions and Future Work
We have shown that we can effectively model seg-
mentation, linkage to headwords in a dictionary, and
morphological tagging using a joint model called sy-
romorph. We have introduced novel approaches for
segmentation, dictionary linkage, and morphologi-
cal tagging, and each of these approaches has out-
performed its corresponding na?ve baseline. Further-
more, we have shown that for Syriac, a data-driven
approach seems to be an appropriate way to solve
these problems in an under-resourced setting.
We hope to use this combined model for pre-
annotation in an active learning setting to aid anno-
tators in labeling a large Syriac corpus. This corpus
will contain data spanning multiple centuries and a
variety of authors and genres. Future work will re-
quire addressing issues encountered in this corpus.
In addition, there is much to do in getting the over-
all tag accuracy closer to the accuracy of individual
decisions. We leave further feature engineering for
the stem tagger and the exploration of possible new
morphological tagging techniques for future work.
Finally, future work includes the application of the
syromorph methodology to other under-resourced
Semitic languages.
Acknowledgments
We would like to thank David Taylor of the Oriental
Institute at Oxford University for collaboration on
the design of the simplified tagset. We also recog-
nize the assistance of Ben Hansen of BYU on a sub-
set of the experimental results. Finally, we would
like to thank the anonymous reviewers for helpful
guidance.
References
Roy Bar-haim, Khalil Sima?an, and Yoad Winter. 2008.
Part-of-speech tagging ofmodern hebrew text. Natural
Language Engineering, 14(2):223?251.
British and Foreign Bible Society, editors. 1920. The
New Testament in Syriac. Oxford: Frederick Hall.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08).
819
Grzegorz Chrupa?a. 2006. Simple data-driven
context-sensitive lemmatization. In Procesamiento del
Lenguaje Natural, volume 37, pages 121 ? 127.
Ezra Daya, Dan Roth, and Shuly Wintner. 2008.
Identifying Semitic roots: Machine learning with
linguistic constraints. Computational Linguistics,
34(3):429?448.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic tagging of Arabic text: From
raw text to base phrase chunks. In Proceedings of
the 5th Meeting of the North American Chapter of
the Association for Computational Linguistics/Human
Language Technologies Conference (HLT-NAACL04),
pages 149?152.
Jenny Rose Finkel, Christopher D. Manning, and An-
drew Y. Ng. 2006. Solving the problem of cascading
errors: Approximate Bayesian inference for linguistic
annotation pipelines. In EMNLP ?06: Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing, pages 618?626. Association for
Computational Linguistics.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 573?580. Asso-
ciation for Computational Linguistics.
Nizar Habash and Owen Rambow. 2007. Arabic diacriti-
zation through full morphological tagging. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 53?56. Association for Computational Lin-
guistics.
Robbie Haertel, Peter McClanahan, and Eric K. Ring-
ger. 2010. Automatic diacritization for low-resource
languages using a hybrid word and consonant cmm.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 519?527.
Association for Computational Linguistics.
Jan Haji? and Barbora Hladk?. 1998. Tagging inflective
languages: Prediction of morphological categories for
a rich, structured tagset. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics,
pages 483?490. Association for Computational Lin-
guistics.
George Kiraz. 1994. Automatic concordance generation
of Syriac texts. In R. Lavenant, editor, VI Symposium
Syriacum 1992, pages 461?.
George Anton Kiraz. 2000. Multitiered nonlinear mor-
phology using multitape finite automata: a case study
on Syriac and Arabic. Computational Linguistics,
26:77?105.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to japanese
morphological analysis. In Proceedings of EMNLP,
pages 230?237.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 399?406. Associ-
ation for Computational Linguistics.
Saib Mansour, Khalil Sima?an, and Yoad Winter. 2007.
Smoothing a lexicon-based pos tagger for Arabic and
Hebrew. In Semitic ?07: Proceedings of the 2007
Workshop on Computational Approaches to Semitic
Languages, pages 97?103. Association for Computa-
tional Linguistics.
Emad Mohamed and Sandra K?bler. 2010a. Arabic
part of speech tagging. In Proceedings of the Sev-
enth International Language Resources and Evalua-
tion (LREC?10).
Emad Mohamed and Sandra K?bler. 2010b. Is Arabic
part of speech tagging feasible without word segmen-
tation? In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
705?708. Association for Computational Linguistics.
Vladim?r Petkevi?. 2001. Grammatical agreement
and automatic morphological disambiguation of inflec-
tional languages. In TSD ?01: Proceedings of the
4th International Conference on Text, Speech and Dia-
logue, pages 47?53. Springer-Verlag.
Danny Shacham and Shuly Wintner. 2007. Morpholog-
ical disambiguation of Hebrew: a case study in clas-
sifier combination. In Proceedings of EMNLP-CoNLL
2007, the Conference on Empirical Methods in Natural
Language Processing and the Conference on Compu-
tational Natural Language Learning. Association for
Computational Linguistics.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 475?482. Association for Computational Lin-
guistics.
K. Toutanova and C. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proceedings of EMNLP, pages
63?70.
820
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 30?37,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Elicited Imitation for Prediction of OPI Test Scores
Kevin Cook
Brigham Young University
Department of Computer Science
kevincook13@gmail.com
Jeremiah McGhee, Deryle Lonsdale
Brigham Young University
Department of Linguistics
{jlmcghee,lonz}@byu.edu
Abstract
Automated testing of spoken language is the
subject of much current research. Elicited
Imitation (EI), or sentence repetition, is well
suited for automated scoring, but does not di-
rectly test a broad range of speech communi-
cation skills. An Oral Proficiency Interview
(OPI) tests a broad range of skills, but is not as
well suited for automated scoring. Some have
suggested that EI can be used as a predictor of
more general speech communication abilities.
We examine EI for this purpose. A fully au-
tomated EI test is used to predict OPI scores.
Experiments show strong correlation between
predicted and actual OPI scores. Effective-
ness of OPI score prediction depends upon at
least two important design decisions. One of
these decisions is to base prediction primar-
ily on acoustic measures, rather than on tran-
scription. The other of these decisions is the
choice of sentences, or EI test items, to be re-
peated. It is shown that both of these design
decisions can greatly impact performance. It
is also shown that the effectiveness of individ-
ual test items can be predicted.
1 Introduction
1.1 Background
Learning to speak a second language is an impor-
tant objective for many people. Assessing progress
in oral proficiency is often expensive and time-
consuming. The development of automated systems
promises to significantly lower costs and increase
accessibility.
Elicited imitation (EI) has been used for nearly
half a century to measure abnormal language devel-
opment (Fujiki and Brinton, 1987) and the perfor-
mance of second language learners (Chaudron et al,
2005; Vinther, 2002). As a method for assessing oral
proficiency it consists of a person listening to a test
item, typically a full sentence, and then doing their
best to repeat it back correctly. This method is also
referred to as sentence repetition, or more simply as
repeats. One motivation for using EI, as opposed to
some other form of test, is that it is relatively inex-
pensive to administer. An EI test can be effectively
scored by non-experts in a relatively short amount
of time. It is also well suited for automated scoring
(Graham et al, 2008), since correct responses are
predictable.
1.2 Motivation
The language skills directly measured by an EI test
are those involved in repeating back what one has
just heard. In order to directly measure a broader set
of language skills, other tests must be used. One of
these is the Oral Proficiency Interview (OPI).
The OPI is face-to-face interview conducted to as-
sess language proficiency. The interview tests dif-
ferent types of relevant skills and lasts for about 30
minutes. Additionally, a validated OPI requires a
second review of a recording created during the ini-
tial interview with arbitration if necessary. This pro-
cess is expensive ( $150 U.S.) and time-consuming
with a turn-around of several weeks before finalized
results are received.
A fully automated OPI test does not seem to be
practical. This is especially the case when the in-
30
terpersonal aspects of a face-to-face interview are
considered. There have been several efforts to au-
tomatically score the type of speech which might be
spoken by an OPI test-taker, spontaneous non-native
speech (Zechner and Xi, 2008). It has been shown
that current automatic speech recognition (ASR)
systems, used to transcribe such speech, have error
rates which make it challenging to use transcripts for
testing purposes.
The argument has been made that although EI
does not directly measure communicative skills,
such as the ability to converse with another person,
it can be used to infer such skills (Henning, 1983).
Part of the theory behind EI is that people typically
are not able to memorize the sounds of an utterance
the length of a full sentence. Rather, people build
a mental model of the meaning of an utterance, and
are then able to remember the model. People who
cannot understand the utterance are not able to build
a mental model, and are therefore unable to remem-
ber or repeat the utterance. If it is true that EI can
be used to infer more general speech communica-
tion abilities, even if only to a limited extent, then
EI may be useful for predicting test scores which are
designed to directly measure that ability.
Bernstein et al (2000) describe a system which
elicits short predictable responses, such as readings,
repeats (EI), opposites, and short answers, for auto-
mated testing. A similar system is discussed later
in Bernstein et al (2010). It is evident that EI is
used in these systems, as part of a greater whole.
The argument is made that although the skills di-
rectly tested are limited, the scores produced may be
useful for inferring more general language abilities.
It is shown that automated scores correlate well with
scores from conventional tests, such as the OPI. One
aspect which may not be as clear is the role that EI
plays as compared to other methods used in the au-
tomated test.
We are interested in the use of a fully automated
EI test as a means to predict more general ability
in spoken language communication. Since the OPI
test is specifically designed to measure such general
ability we use it as a gold standard, in spite of the
fact that we do not expect it to be a perfect measure.
We are interested in learning the extent to which OPI
scores can be predicted using an EI test. We are also
interested in learning how to design an automated
system such that prediction of OPI scores is most
effective. We evaluate system performance based on
how highly correlated OPI score predictions are with
actual OPI scores.
Several design decisions must be made in the de-
velopment of such a system. One, is which method
to use for converting spoken responses to OPI score
predictions. Another, is the choice of sentences, or
EI test items, to be repeated. We address both of
these issues.
There are at least two approaches to scoring spo-
ken responses. One, is to score based on tran-
scriptions, generated by a speech recognizer. An-
other, is to score based on acoustic measures alone,
such as pronunciation and fluency (Cincarek et al,
2009). The primary difference between these two
approaches is what is assumed about the textual con-
tent of a spoken response. Acoustic measures are
based on the assumption that the textual content of
each spoken response is known. Speech recognition
is based on the assumption that the content is not
known. We explore the effect of this assumption on
OPI prediction.
The selection of effective EI test items has been
the subject of some research. Tomita et al (2009)
outline principles for creating effective EI test items.
Christensen et al (2010) present a tool for test item
creation. We explore the use of OPI scores as a
means to evaluate the effectiveness of individual test
items.
2 Related Work
The system described by Bernstein et al (2010) uses
EI as part of the automated test. Sentences range
in length from two to twenty or more syllables. If
fewer than 90% of natives can repeat the sentence
verbatim, then the item is not used. An augmented
ASR system is used which has been optimized for
non-native speech. The ASR system is used to tran-
scribe test-taker responses. Transcriptions are com-
pared to the word string recited in the prompt. Word
errors are counted and used to calculate a score. Flu-
ency and pronunciation of spoken responses are also
scored.
Graham et al (2008) report on a system which
uses EI for automated assessment. Results show that
automated scores are strongly correlated with man-
31
ual EI scores. ASR grammars are specific to each
test item. Our work is based on this system.
Mu?ller et al (2009) compare the effectiveness of
reading and repeating (EI) tasks for automated test-
ing. Automated scores are compared with manual
scores for the same task. It is found that repeating
tasks provide a better means of automatic assess-
ment than reading tasks.
3 Experiments
In this section we describe experiments, including
both an OPI test and an automated EI test. We de-
tail the manner of automated scoring of the EI test,
together with the method used to predict OPI scores.
3.1 Setup
We administer an ACTFL-OPI (see www.actfl.org)
and an automated EI test to each of 85 English as
a Foreign Language learners of varying proficiency
levels. This group of speakers (test-takers) is ran-
domly divided into a 70%/30% training/testing split,
with 60 speakers forming the training set and the re-
maining 25 forming the test set. Training data con-
sists of OPI scores and EI responses for each speaker
in the training set. Test data consists of OPI scores
and EI responses for each speaker in the test set.
An OPI is a face-to-face interview conducted by
a skilled, certified human evaluator. (We do not ex-
pect that this interview results in an ideal evaluation
of oral proficiency. We use the OPI because it is de-
signed to directly test speech communication skills
which are not directly tested by EI.) OPI proficiency
levels range across a 10-tiered nominal scale from
Novice Low to Superior. We convert these levels to
an integer score from 1 to 10 (NoviceLow = 1,
Superior = 10).
The EI test consists of 59 items, each an English
sentence. An automated system plays a recording of
each sentence and then records the speaker?s attempt
to repeat the sentence verbatim. A fixed amount of
time is allotted for the speaker to repeat the sentence.
After that fixed time, the next item is presented, until
all items are presented and all responses recorded.
The choice of which items to include in the test is
somewhat arbitrary; we select those items which we
believe might work well, given past experimentation
with EI. We expect that improvement could be made
in both the manner of administration of the test, and
in the selection of test items.
Responses are scored using a Sphinx 4 (Walker
et al, 2004) ASR system, version 1.0 beta 4, to-
gether with the supplied 30-6800HZ WSJ acoustic
model. ASR performance is affected by various sys-
tem parameters. For our experiments, we generally
use default parameters found in configuration files
for Sphinx demos. The ASR system has not been
adapted for non-native speech.
3.2 Language Models
We vary the language model component of the ASR
system in order to evaluate the merit of assum-
ing that the content of spoken responses is known.
Speech recognizers use both an acoustic model and
a language model, to transcribe text. The acoustic
model is used to estimate a probability correspond-
ing to how well input speech sounds like output text.
The language model is used to estimate a probabil-
ity corresponding to how well output text looks like
a target language, such as English. Output text is
determined based on a joint probability, using both
the acoustic and the language models. We vary the
degree to which it is assumed that the content of spo-
ken responses is known. This is done by varying the
degree to which the language model is constrained
to the text of the expected response.
When the language model is fully constrained, the
assumption is made that the content of each spoken
response is known. The language model assigns all
probability to the text of the expected response. All
other output text has zero probability. The acoustic
model estimates a probability for this word sequence
according to how well the test item is pronounced. If
the joint probability of the word sequence is below
a certain rejection threshold, then there is no out-
put from the speech recognizer. Otherwise, the text
of the test item is the output of the speech recog-
nizer. With this fully constrained language model,
the speech recognizer is essentially a binary indica-
tor of pronunciation quality.
When the language model is fully unconstrained,
there is no relationship between the language model
and test items, except that test items belong to the
English language. In this case, the speech recognizer
functions normally, as a means to transcribe spoken
responses. Output text is the best guess of the ASR
32
system as to what was said.
A partially constrained language model is one that
is based on test items, but also allows variation in
output text.
We perform experiments using the following five
language models:
1. WSJ20K The 20K word Wall Street Journal
language model, supplied with Sphinx.
2. WSJ5K The 5K word Wall Street Journal lan-
guage model, supplied with Sphinx.
3. EI Items A custom language model created
from the corpus of all test items.
4. Item Selection A custom language model con-
straining output to any one of the test items.
5. Forced Alignment A custom language model
constraining output to only the current test
item.
The first two language models, WSJ20K and
WSJ5K, are supplied with Sphinx and have no spe-
cial relationship to the test items. The training cor-
pus used to build these models is drawn from issues
of the Wall Street Journal. These models are fully
unconstrained.
The third model, EI Items, is a conventional lan-
guage model with the exception that the training cor-
pus is very limited. The training corpus consists of
all test items; no other text is included in the train-
ing corpus. The fourth model, Item Selection, is not
a conventional language model. It assigns a set prob-
ability to each test item as a whole. That probability
is equal to one divided by the total number of test
items. Such a simple language model is sometimes
referred to as a grammar (Walker et al, 2004; Gra-
ham et al, 2008). Both the EI Items and Item Selec-
tion models are partially constrained. The Item Se-
lection model is much more highly constrained than
the EI Items model.
The last model, Forced Alignment, is fully con-
strained. It assigns all probability to item text. These
five language models are chosen for the purpose of
evaluating the effectiveness of constraining the lan-
guage model to the text of the expected response.
i Item
I Number of items
s Speaker (test-taker)
S Number of speakers
xis Score for item i, speaker s
y s Predicted OPI score for speaker s
os Actual OPI score for speaker s
MSE i Mean squared error for item i
Figure 1: Notation used in this paper.
3.3 Scoring
Each response is scored using a two-step process.
First, the spoken response is transcribed by the ASR
system. Second, word error rate (WER) is calcu-
lated by comparing the transcription to the item text.
WER is converted to an item score xis for item i and
speaker s in the range of 0 to 1 using the following
formula:
xis =
{
1? WER100 if WER < 100%
0 otherwise
(1)
A list of notation used in this paper is shown in
Figure 1.
3.4 Prediction
In order to avoid over-fitting, a simple linear model
is trained (Witten and Frank, 2005) to predict an OPI
score ys, given items scores xis together with model
parameters a and b. The mean of item scores for
speaker s is multiplied by parameter a. This product
plus parameter b is the OPI score prediction: (I is
the total number of items.)
ys =
1
I
?
i
xis ? a + b (2)
Correlation is calculated between predicted and
actual OPI scores for all speakers in the test set.
4 Results
Correlation for each of the language models using
all 59 test items is shown in Figure 2. Correlation for
both of the unconstrained language models was rel-
atively poor. Performance improved significantly as
the language model was constrained to the expected
response. These results suggest that it is effective
33
to assume that the content of spoken responses is
known.
Fully constraining the language model to the text
of the expected response results in an item score
which is a binary indicator (because, in this case,
WER is either 100% or 0%) of how well the spoken
response sounds like the expected response. In this
case, prediction is based on the output of the acous-
tic model of the speech recognizer, an acoustic mea-
sure. Prediction is not based on transcription, since
a specific transcription is assumed prior to process-
ing the spoken response. When the language model
is fully unconstrained, an item score is an indica-
tor of how well ASR transcription matches the text
of the expected response. In this case, prediction is
based on transcription, the speech recognizer?s best
guess of which words were spoken. Results indicate
that correlation between predicted and actual OPI
scores improves as prediction is based on acoustic
measures, rather than on transcription.
Language Model Constrained Corr.
WSJ20K Not 0.633
WSJ5K Not 0.600
EI Items Partial 0.737
Item Selection Partial 0.805
Forced Alignment Full 0.799
Figure 2: Correlation with OPI scores, for all 5 language
models, using all 59 test items. Language models are
unconstrained, partially constrained, or fully constrained
to the text of the expected response.
4.1 Item MSE
The effectiveness of individual test items is explored
by defining a measure of item quality. If each item
score xis were ideally linearly correlated with the
actual OPI score os for speaker s then the equality
shown below would hold: (os is an integer from 1 to
10. xis is a real number from 0 to 1.)
IDEAL =? os = xis ? 9 + 1 (3)
We calculate the difference between this ideal and
the actual OPI score:
(xis ? 9 + 1)? os (4)
This difference can be seen as a measure of how
useful the item is as a predictor OPI scores. For bet-
ter items, this difference is closer to zero. The mean
of the squares of these differences for a particular
item, over all S speakers in the training set, is a mea-
sure of item quality MSEi:
MSEi =
1
S
?
s
((xis ? 9 + 1)? os)
2 (5)
Because we expect improved results by assuming
that the content of expected responses is known, we
use the Forced Alignment language model to cal-
culate an MSE score for each test item. A sample
of items and their associated MSE are listed in Fig-
ure 3.
MSE Item text
9.28 He should have walked away before
the fight started.
10.48 We should have eaten breakfast by
now.
. . .
14.53 She dove into the pool gracefully, and
with perfect form.
14.68 If her heart were to stop beating, we
might not be able to help her.
. . .
25.78 She ought to learn Spanish.
26.09 Sometimes they go to town.
Figure 3: Sample EI items with corresponding MSE
scores.
Item MSE scores are used to define various sub-
sets of test items, better items, worse items, and so
on. Better items have lower MSE scores. These sub-
sets are used to compute a series of correlations for
each of the five language models. First, correlation
is computed using only one test item. That item is
the item with the lowest (best) MSE score. Then,
correlation is computed again using only two test
items, the two items with the lowest MSE scores.
This process is repeated until correlation is com-
puted using all test items. Results are shown in Fig-
ure 4. These results show even more convincingly
that OPI prediction improves by assuming that the
content of spoken responses is known.
4.2 OPI Prediction
Figure 4 also gives an idea of how effectively EI can
be used to predict OPI scores. Correlation over 0.80
is achieved using the Forced Alignment language
34
Figure 4: Correlation with OPI scores, for all 5 language
models, using varying numbers of test items.
Figure 5: Plot of predicted OPI scores as a function of
actual OPI scores, using the Forced Alignment language
model and the best 24 test items.
model for all but 7 of the 59 subsets of test items.
Correlation is over 0.84 for 11 of the subsets (best
20 - best 31). Correlation is above 0.85 for 3 subsets
(best 23 - best 25). Predicted OPI scores correlate
strongly with actual OPI scores.
Figure 5 shows a plot of predicted OPI scores as
a function of actual OPI scores, using the Forced
Alignment language model and only the best 24 test
items. Correlation is 0.856. Interestingly, two of the
outliers (OPI=5, predicted OPI=2.3) and (OPI=4,
predicted OPI=2.3) were for speakers whose re-
sponses contained only silence, indicating those par-
ticipants may have experienced technical difficulties
or may have been uncooperative during their test
session. The inferred model used to calculate OPI
predictions for Figure 5 is shown below:
ys =
1
I
?
i
xis ? 6.8 + 2.3 (6)
(Given this particular model, the lowest possible
predicted OPI score is 2.3, and the highest possible
predicted score is 9.1. The ability to predict OPI
scores 1 and 10 is lost, but the objective is to improve
overall correlation.)
4.3 Item Selection
To see more clearly the effect that the choice of test
items has on OPI prediction, we compute a series
of correlations similar to before, except that the or-
der of test items is reversed: First, correlation is
computed using only the test item with the high-
est (worst) MSE score. Then, correlation is com-
puted again using only the two worst items, and so
on. This series of correlations is computed for the
Forced Alignment language model only. It is shown
together with the original ordering for the Forced
Alignment language model from Figure 4.
These two series are shown in Figure 6. The se-
ries with generally high correlation is computed us-
ing best items first. The series with generally low
correlation is computed using worst items first. At
the end of both series all items are used, and corre-
lation is the same. As mentioned earlier, correlation
using only the best 24 items is 0.856. By contrast,
correlation using only the worst 24 items is 0.679.
The choice of test items can have a significant im-
pact on OPI score prediction.
Figure 6 also shows that the effectiveness of in-
dividual test items can be predicted. MSE scores
were calculated using only training data. Correla-
tions were calculated for test data.
4.4 Rejection Threshold
Since the Forced Alignment language model is
found to be so effective, we experiment further to
learn more about its behavior. Using this language
model, item scores are either zero or one, depending
upon whether ASR output text is the same as item
text, or there is no output text. If joint probability,
for a spoken response, is below a certain rejection
threshold, no text is output. We perform experiments
35
Figure 6: Correlation with OPI scores, showing the dif-
ference between best and worst items, using the Forced
Alignment language model.
Figure 7: Correlation with OPI scores versus rejection
threshold.
to see how sensitive OPI predictions are to the set-
ting of this threshold.
Any ASR system parameter which affects prob-
ability estimates of word sequences can affect the
rejection threshold. We make the arbitrary deci-
sion to vary the Sphinx relativeBeamWidth pa-
rameter. For all previous experiments, the value
of this parameter was fixed at 1E ? 90. The
wordInsertionProbability parameter, which also
affects the rejection threshold, was fixed at 1E?36.
Correlation is computed for various values of
the relativeBeamWidth parameter. Results are
shown in Figure 7. Good results are obtained over
a wide range of rejection thresholds. Correlation
peaks at 1E? 80. OPI prediction does not appear to
be overly sensitive to the setting of this threshold.
5 Discussion
We conclude that a fully-automated EI test can be
used to effectively predict more general language
ability than those abilities which are directly tested
by EI. Such an EI test is used to predict the OPI
scores of 25 test-takers. Correlation between pre-
dicted and actual OPI scores is strong.
Effectiveness of OPI score prediction depends
upon at least two important design decisions. One
of these decisions is to base prediction primarily on
acoustic measures, rather than on transcription. The
other of these decisions is the choice of sentences,
or EI test items, to be repeated. It is shown that both
of these design decisions can greatly impact perfor-
mance. It is also shown that the effectiveness of in-
dividual test items can be predicted.
We quantify the effectiveness of individual test
items using item MSE. It may be possible to use
item MSE to learn more about the characteristics
of effective EI test items. Developing more effec-
tive test items may lead to improved prediction of
OPI test scores. In this paper, we do not attempt
to address how linguistic factors (such as sentence
length, syntactic complexity, lexical difficulty, and
morphology) affect test item effectiveness for OPI
prediction. However, others have discussed simi-
lar questions (Tomita et al, 2009; Christensen et al,
2010).
It may be possible that a test-taker could learn
strategies for doing well on an EI test, without de-
veloping more general speech communication skills.
If test-takers were able to learn such strategies, it
may affect the usefulness of EI tests. Bernstein et al
(2010) suggest that, as yet, no conclusive evidence
has been presented on this issue, and that automated
test providers welcome such research.
It is possible that other automated systems are
found to be more effective as a means for testing
speech communication skills, or as a means for pre-
dicting OPI scores. We expect this to be the case.
The purpose of this research is not to design the best
possible system. Rather, it is to improve understand-
ing of how such a system might be designed. It is
shown that an EI test can be used as a key compo-
nent of such a system. Strong correlation between
actual and predicted OPI scores is achieved without
using any other language testing method.
36
Acknowledgments
We would like to thank the Brigham Young Uni-
versity English Language Center for their support.
We also appreciate assistance from the Pedagogical
Software and Speech Technology research group,
Casey Kennington, and Dr. C. Ray Graham.
References
Jared Bernstein, John De Jong, David Pisoni, and Brent
Townshend. 2000. Two experiments on automatic
scoring of spoken language proficiency. In P. Del-
cloque, editor, Proceedings of InSTIL2000 (Integrat-
ing Speech Technology in Learning), pages 57?61.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010. Validating automated speaking tests. Language
Testing, 27(3):355?377.
Craig Chaudron, Matthew Prior, and Ulrich Kozok.
2005. Elicited imitation as an oral proficiency mea-
sure. Paper presented at the 14th World Congress of
Applied Linguistics, Madison, WI.
Carl Christensen, Ross Hendrickson, and Deryle Lons-
dale. 2010. Principled construction of elicited imita-
tion tests. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta. European Language Re-
sources Association (ELRA).
Tobias Cincarek, Rainer Gruhn, Christian Hacker, Elmar
Nth, and Satoshi Nakamura. 2009. Automatic pro-
nunciation scoring of words and sentences indepen-
dent from the non-native?s first language. Computer
Speech and Language, 23(1):65 ? 88.
Martin Fujiki and Bonnie Brinton. 1987. Elicited imi-
tation revisited: A comparison with spontaneous lan-
guage production. Language, Speech, and Hearing
Services in the Schools, 18(4):301?311.
C. Ray Graham, Deryle Lonsdale, Casey Kennington,
Aaron Johnson, and Jeremiah McGhee. 2008. Elicited
Imitation as an Oral Proficiency Measure with ASR
Scoring. In Proceedings of the 6th International Con-
ference on Language Resources and Evaluation, pages
1604?1610, Paris, France. European Language Re-
sources Association.
Grant Henning. 1983. Oral proficiency testing: compar-
ative validities of interview, imitation, and completion
methods. Language Learning, 33(3):315?332.
Pieter Mu?ller, Febe de Wet, Christa van der Walt, and
Thomas Niesler. 2009. Automatically assessing the
oral proficiency of proficient L2 speakers. In Proceed-
ings of the ISCA Workshop on Speech and Language
Technology in Education (SLaTE), Warwickshire, UK.
Yasuyo Tomita, Watuaru Suzuki, and Lorena Jessop.
2009. Elicited imitation: Toward valid procedures to
measure implicit second language grammatical knowl-
edge. TESOL Quarterly, 43(2):345?349.
Thora Vinther. 2002. Elicited imitation: a brief
overview. International Journal of Applied Linguis-
tics, 12(1):54?73.
Willie Walker, Paul Lamere, Philip Kwok, Bhiksha Raj,
Rita Singh, Evandro Gouvea, Peter Wolf, and Joe
Woelfel. 2004. Sphinx-4: A flexible open source
framework for speech recognition.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco.
Klaus Zechner and Xiaoming Xi. 2008. Towards auto-
matic scoring of a test of spoken language with het-
erogeneous task types. In Proceedings of the Third
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 98?106. Association for
Computational Linguistics.
37
